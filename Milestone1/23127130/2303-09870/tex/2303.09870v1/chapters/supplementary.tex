\appendix
\noindent
\textbf{Overview.} This Appendix provides important additional details about our proposed method \textbf{TeSLA}. In Appendix~\ref{appendix:hyperparameters}, we provide hyperparameters details for each test-time adaptation experiment on the \textbf{common image corruptions}, \textbf{synthetic-to-real}, and \textbf{medical measurement shift} datasets. In Appendix~\ref{appendix:additional_results}, we provide additional quantitative results, including class top-1 accuracies for the VisDA-C \cite{visda2017} and Kather-16 \cite{kather_2016_53169} datasets and corruption-wise error rates on the CIFAR-10-C/CIFAR-100-C \cite{hendrycks2018benchmarking}, and ImageNet-C \cite{hendrycks2018benchmarking} datasets. In addition, we also provide segmentation class-wise mean Intersection over Union (mIoU) for the VisDA-S dataset \cite{visda2017} and class average Dice score for different sites of the target test domain on the spinal cord \cite{prados2017spinal} and prostate dataset \cite{liu2020ms} for the competing test time adaptation methods. All quantitative results are included for both one-pass (\textbf{O}) and multi-pass (\textbf{M}) protocols. Appendix~\ref{appendix:runtime} provides an overall runtime computation cost of TeSLA along with other Test Time adaptation methods on VisDA-C \cite{visda2017} dataset, while Appendix~\ref{appendix:other_tt_obj} discusses TeSLA's equivalence to TENT \cite{wang2021tent} and \cite{liang2020we} without mean teacher and adversarial augmentations. We include additional ablation experiments and hyperparameter sensitivity tests in Appendix~\ref{appendix:sensitivity}. Finally, we provide other qualitative results, including a sanity check on TeSLA's adversarial augmentations, uncertainty evaluation, and segmentation visualization in Appendix~\ref{appendix:qualitative_results}. 

\section{Hyperparameter Settings}\label{appendix:hyperparameters}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
Table \ref{tab:hyperparameters_cls} and Table \ref{tab:hyperparameters_seg}  present the hyperparameters' values of TeSLA used for individual experiments on different classification and segmentation datasets, respectively. These hyperparameters include the batch size $B$, learning rate, optimizer, EMA momentum coefficient $\alpha$, number of epochs for test-time adaptation (M protocol), the number of weak augmentations $|\rho_w|$; the number of nearest neighbors $n$; class-wise queue size $N_Q$ used by soft pseudo-label refinement (PLR) module, number of image operations for augmentation sub-policy $N$ used by the adversarial augmentation module, the augmentation severity controller coefficient $\lambda_1$ and the knowledge distillation coefficient $\lambda_2$ for $\mathcal{L}_\text{kd}$ loss term.

\begin{table}[ht]
\centering
\caption{\textbf{Hyperparameter setting} used for the proposed methods TeSLA/TeSLA-s on different classification datasets.}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|cccccccccc@{}}

    \toprule
     \multirow{2}[2]{*}{\begin{tabular}[c]{@{}c@{}}Hyperparameters\end{tabular}}
     & \multicolumn{2}{c}{CIFAR10-C}
     & \multicolumn{2}{c}{CIFAR100-C}
     & \multicolumn{2}{c}{ImageNet-C}
     & \multicolumn{2}{c}{VisDA-C}
     & \multicolumn{2}{c}{Kather-16}\\
     
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
     
     & O & M & O & M & O & M & O & M & O & M  \\
     
     \midrule
     
     Batch size $B$ & 128 & 128 & 128 & 128 & 128 & 128 & 128 & 128 & 32 & 32  \\
     
     \midrule
     
     Learning rate & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.005 & 0.005 \\
     
     \midrule
     
     Optimizer & Adam & Adam & Adam & Adam & SGD & SGD & SGD & SGD & Adam & Adam \\
     
     \midrule
     
    \begin{tabular}[c]{@{}c@{}}Momentum\\coefficient $\alpha$\end{tabular} & 0.99 & 0.999 & 0.99 & 0.999 & 0.9 & 0.996 & 0.9 & 0.996 & 0.9 & 0.96 \\
    
     \midrule
     
    \begin{tabular}[c]{@{}c@{}}Number of\\epochs\end{tabular} & 1 & 70 & 1 & 70 & 1 & 5 & 1 & 5 & 1 & 70 \\
     
     \midrule
     
     \begin{tabular}[c]{@{}c@{}}Number of weak\\augmented views $|\rho_w|$\end{tabular}  & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 \\
     
     \midrule
     
     \begin{tabular}[c]{@{}c@{}}Number of nearest\\neighbors $n$\end{tabular}  & 1 & 4 & 1 & 1 & 1 & 1 & 10 & 10 & 8 & 8 \\
     
     \midrule
     
    \begin{tabular}[c]{@{}c@{}}Class-wise\\queue size $N_\mathbf{Q}$\end{tabular}  & 1 & 256 & 1 & 1 & 1 & 1 & 256 & 256 & 32 & 256\\
    
    \midrule
    
    \begin{tabular}[c]{@{}c@{}}Sub-policy\\dimension $N$\end{tabular} & 2 & 2 & 2 & 2 & 2 & 2 & 4 & 3 & 2 & 2 \\
    
    \midrule
    
    \begin{tabular}[c]{@{}c@{}}Augmentation severity\\controller $\lambda_1$\end{tabular} & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    
    \midrule
    
    \begin{tabular}[c]{@{}c@{}}Knowledge\\distillation weight $\lambda_2$\end{tabular} & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\
    
    \bottomrule
\end{tabular}}
\label{tab:hyperparameters_cls}
\end{table}

\begin{table}[ht]
\centering
\caption{\textbf{Hyperparameter setting} used for the proposed method TeSLA on different segmentation datasets.}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|cccccc@{}}

    \toprule
     \multirow{2}[2]{*}{\begin{tabular}[c]{@{}c@{}}Hyperparameters\end{tabular}}
     & \multicolumn{2}{c}{VisDA-S}
     & \multicolumn{2}{c}{Spinal Cord}
     & \multicolumn{2}{c}{Prostate} \\
     
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
     
     & O & M & O & M & O & M \\
     
     \midrule
     
     Batch size $B$ & 8 & 8 & 16 & 16 & 16 & 16 \\
     
     \midrule
     
     Learning rate  & 0.001 & 0.001  & 0.0002 & 0.0002 & 0.0002 & 0.0002  \\
     
     \midrule
     
      Optimizer & AdamW & AdamW & AdamW & AdamW & AdamW & AdamW \\

     
     \midrule
     
    \begin{tabular}[c]{@{}c@{}}Momentum\\coefficient $\alpha$\end{tabular} & 0.996 & 0.999 & 0.996 & 0.996 & 0.996 & 0.996\\
    
     \midrule
     
    \begin{tabular}[c]{@{}c@{}}Number of\\epochs\end{tabular} & 1 & 3 & 1 & 5 & 1 & 3\\
     
     \midrule
     
     \begin{tabular}[c]{@{}c@{}}Number of weak\\augmented views $|\rho_w|$\end{tabular} & 3 & 3 & 5 & 5 & 5 & 5 \\
     
     \midrule
     
     \begin{tabular}[c]{@{}c@{}}Number of nearest\\neighbors $n$\end{tabular} & 1 & 1 & 1 & 1 & 1 & 1\\
     
     \midrule
     
    \begin{tabular}[c]{@{}c@{}}Class-wise\\queue size $N_\mathbf{Q}$\end{tabular} & 1 & 1 & 1 & 1 & 1 & 1 \\
    
    \midrule
    
    \begin{tabular}[c]{@{}c@{}}Sub-policy\\dimension $N$\end{tabular} & 3 & 3 & 3 & 3 & 3 & 3\\
    
    \midrule
    
    \begin{tabular}[c]{@{}c@{}}Augmentation severity\\ controller $\lambda_1$\end{tabular} & 1 & 1 & 1 & 1 & 1 & 1 \\
    
    \midrule
    
    \begin{tabular}[c]{@{}c@{}}Knowledge\\distillation weight $\lambda_2$\end{tabular} & 1 & 1 & 1 & 1 & 1 & 1 \\

    \bottomrule
\end{tabular}}
\label{tab:hyperparameters_seg}
\end{table}

% \newpage
\section{Additional Quantitative Results}\label{appendix:additional_results}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
In Table \ref{tab:kather}, we compare TeSLA against state-of-the-art test-time adaptation methods for the classification task on the Kather-16 dataset. We present the class top-1 accuracies (\%) for each of the four tissue categories of \textbf{tumor}, \textbf{stroma}, \textbf{lymphocyte}, and \textbf{mucosa}. In addition, we report the class average accuracy (Avg.). 

Furthermore, in Table \ref{tab:cifar_c}, we present the corruption-wise average class error rates for different competing test time adaptation baselines, including the proposed TesLA and TeSLA-s on the  CIFAR10-C,
CIFAR100-C and ImageNet-C.  We use the following image corruptions for the evaluation at the maximum severity level of  5: [{\footnotesize\textbf{\textsc{Gaussian Noise, Shot Noise, Impulse Noise, Defocus Blur, Glass Blur, Motion Blur, Zoom Blur, Snow, Frost, Fog, Brightness, Contrast, Elastic Transformation, Pixelate, JPEG Compression}}}]. We also use the ResNet-50 backbone for all experiments.


In Table \ref{tab:visda}, we include the overall and class-wise accuracies for test time adaptation of ResNet-101 trained on synthetic vehicle images (\textbf{training}) and tested on the photo-realistic vehicle images (\textbf{validation}) of the VisDA-C dataset. The photo-realistic images are classified into 12 categories: \textbf{plane}, \textbf{bicycle}, \textbf{bus}, \textbf{car}, \textbf{horse}, \textbf{knife}, \textbf{motor-cycle}, \textbf{person}, \textbf{plant}, \textbf{skate-board}, \textbf{train}, and \textbf{truck}.

In Table \ref{tab:mri}, we present segmentation results (class Avg. volume-wise mean \%Dice score)  for test-time adaptation baselines on two multi-site magnetic resonance imaging (MRI) benchmarks - spinal cord \cite{prados2017spinal} and prostate dataset \cite{liu2020ms}. For the spinal cord dataset, we report results for test-time adaptation of the U-Net segmentation model trained on \textbf{site 1} to \textbf{site 2}, \textbf{site 3}, and \textbf{site 4}. Similarly, we report results of the U-Net segmentation model trained on the sites \textbf{A and B}, which are adapted on the sites \textbf{D}, \textbf{site E}, and \textbf{site F}.


Table \ref{tab:visdas} presents the results of competing test-time adaptation methods applied to the segmentation adaptation task from the synthetic images of GTA \cite{Richter_2016_ECCV} to the photo-realistic images of Cityscapes \cite{Cordts2016Cityscapes} dataset. We report the class-wise mean Intersection over Union (mIoU) over 19 classes: \textbf{road}, \textbf{side-walk}, \textbf{building}, \textbf{wall}, \textbf{fence}, \textbf{pole}, \textbf{light}, \textbf{sign}, \textbf{vegetation}, \textbf{terrain}, \textbf{sky}, \textbf{person}, \textbf{rider}, \textbf{car}, \textbf{truck}, \textbf{bus}, \textbf{train}, \textbf{motor-cycle}, and \textbf{bicycle}. 



\begin{table}[t]
\centering
\caption{\textbf{Comparison of state-of-the-art TTA methods under different protocols} on the Kather-16  dataset. We report the class top-1 accuracies (\%) for each of the four classes and the per-class average accuracy (Avg.). Each result is averaged over ten seeds.}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|cccc|c}
\toprule
     Method & \rotatebox[origin=c]{90}{Protocol} & tumor & stroma & lymphocyte & mucosa & Avg. \\
     
\midrule

Source & N & 84.5\std{4.0} & 91.6\std{3.0} & 0.9\std{1.2} & 95.0\std{1.3} & 68.0\std{1.3}\\

\midrule

BN & N-O & 89.3\std{2.5} & 85.5\std{2.6} & 61.7\std{2.2} & 90.2\std{0.6} & 81.7\std{1.0}\\

Tent & N-O  & 89.8\std{4.0} & 89.3\std{3.4} & 67.2\std{2.2} & 88.9\std{1.0} & 83.8\std{1.8}\\

SHOT & N-O  & 84.7\std{5.7} & 95.7\std{2.0} & 67.9\std{3.9} & 92.8\std{0.8} & 85.3\std{2.5}\\
 
\midrule

TTT++ & Y-O & 82.8\std{8.5} & 85.1\std{7.6} & 73.7\std{3.8} & 91.4\std{1.7} & 83.3\std{2.7}\\

TTAC & Y-O & 92.6\std{2.6} & 96.6\std{1.9} & \textbf{78.3\std{4.3}} & 93.9\std{1.0} & 90.4\std{1.1}\\


\midrule

TeSLA & N-O  & \textbf{93.5\std{1.8}} & \textbf{98.2\std{1.2}} & 77.3\std{4.1} & \textbf{94.3\std{0.7}} & \textbf{90.8\std{1.1}} \\

TeSLA-s & Y-O & 90.7\std{4.6} & 98.0\std{1.0} & 77.9\std{5.6} & 94.0\std{1.7} & 90.1\std{1.4} \\

\midrule
\midrule

BN & N-M  & 86.3\std{3.6} & 86.1\std{1.9} & 66.9\std{1.2} & 87.7\std{0.7} & 81.8\std{1.0}\\

Tent & N-M & 96.4\std{4.2} & 99.5\std{0.4} & 62.6\std{9.0} & 93.7\std{1.5} & 88.0\std{3.3} \\

SHOT & N-M & 84.6\std{4.4} & 98.5\std{0.6} & 77.1\std{5.2} & 91.7\std{0.8} & 88.0\std{2.4}  \\

\midrule

TTT++ & Y-M & 95.6\std{1.4} & 93.9\std{2.8} & 85.2\std{5.3} & 93.6\std{1.8} & 92.1\std{2.0} \\

TTAC & Y-M & 92.9\std{12.6} & 98.1\std{1.1} & 92.4\std{4.7} & 94.4\std{1.8} & 94.5\std{4.7} \\

\midrule

TeSLA & N-M   & 97.1\std{1.0} & \textbf{99.6\std{0.3}} & 94.4\std{2.0} & 95.6\std{0.9} & 96.7\std{0.5} \\

TeSLA-s & Y-M & \textbf{97.4\std{0.4}} & 99.5\std{0.3} & \textbf{95.1\std{2.0}} & \textbf{95.7\std{1.0}} & \textbf{96.9\std{0.6}} \\

\bottomrule
\end{tabular}}
\label{tab:kather}
\end{table}

\begin{table*}
\centering
\caption{\textbf{Comparison of state-of-the-art TTA methods under different protocols} on common image corruptions datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C. We report the error rates (\%) on 15 test images' corruptions.}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|c|ccccccccccccccc|c}
\toprule
     Method & \begin{sideways}Protocol\end{sideways} & Gaus. & Shot & Impu. & Defo. & Glas. & Moti. & Zoom & Snow & Fros. & Fog & Brig. & Cont. & Elas. & Pixe. & Jpeg & Avg. \\

\midrule
\multicolumn{18}{c}{\cellcolor[HTML]{EFEFEF}CIFAR10-C} \\
\midrule

Source & N & 48.7 & 44.0 & 57.0 & 11.8 & 50.8 & 23.4 & 10.8 & 21.9 & 28.2 & 29.4 & 7.0 & 13.3 & 23.4 & 47.9 & 19.5 & 29.1 \\

\midrule
%%%%%%%%%%%%%%%% CIFAR10C %%%%%%%%%%%%%%%%

BN & N-O & 18.2 & 17.2 & 28.1 & 9.8 & 26.6 & 14.2 & 8.0 & 15.5 & 13.8 & 20.2 & 7.9 & 8.3 & 19.3 & 13.3 & 13.8 & 15.6 \\

Tent & N-O & 16.0 & 14.8 & 24.5 & 9.2 & 23.8 & 13.1 & 7.7 & 14.9 & 13.0 & 16.5 & 8.2 & 8.3 & 17.9 & 10.9 & 13.3 & 14.1 \\

SHOT & N-O & 16.5 & 15.3 & 23.6 & 9.0 & 23.4 & 12.7 & 7.5 & 14.0 & 12.4 & 16.1 & 7.5 & 8.0 & 17.4 & 12.5 & 13.1 & 13.9 \\

\midrule

TTT++ & Y-O & 18.0 & 17.1 & 30.8 & 10.4 & 29.9 & 13.0 & 9.9 & 14.8 & 14.1 & 15.8 & 7.0 & 7.8 & 19.3 & 12.7 & 16.4 & 15.8\\

TTAC & Y-O & 17.9 & 15.8 & 22.5 & \textbf{8.5} & 23.5 & \textbf{11.2} & 7.6 & \textbf{11.9} & 12.9 & \textbf{13.3} & \textbf{6.9} & 7.6 & 17.3 & 12.3 & 12.6 & 13.4 \\

\midrule

TeSLA & N-O & 13.3 & 12.5 & 20.8 & 8.8 & 21.1 & 11.8 & 7.3 & 12.6 & 11.2 & 15.6 & 7.6 & 7.6 & 16.2 & 9.7 & 11.6 & 12.5 \\

TeSLA-s & Y-O & \textbf{13.0} & \textbf{12.2} & \textbf{20.3} & \textbf{8.5} & \textbf{20.8} & \textbf{11.2} & \textbf{7.2} & 12.0 & \textbf{11.0} & 15.5 & 7.3 & \textbf{7.2} & \textbf{15.6} & \textbf{9.1} & \textbf{11.3} & \textbf{12.1} \\

\midrule
\midrule

BN & N-M & 17.3 & 16.2 & 28.0 & 9.8 & 26.1 & 14.0 & 7.9 & 16.1 & 13.7 & 20.4 & 8.3 & 8.3 & 19.6 & 11.8 & 14.0 & 15.4 \\

Tent & N-M & 15.1 & 13.7 & 22.2 & 8.5 & 22.4 & 11.8 & 7.1 & 12.7 & 11.9 & 12.9 & 7.6 & 7.6 & 16.9 & 9.8 & 12.6 & 12.9 \\

SHOT & N-M & 15.8 & 14.8 & 24.9 & 9.2 & 23.6 & 13.2 & 7.5 & 14.5 & 12.8 & 17.5 & 8.1 & 8.2 & 18.1 & 10.8 & 13.4 & 14.2 \\

\midrule

TTT++ & Y-M & 13.2 & 11.8 & \textbf{11.1} & 7.9 & 16.5 & 8.9 & 6.6 & 9.5 & 9.7 & 8.6 & \textbf{5.2} & \textbf{5.6} & 13.1 & 8.8 & 11.1 & 9.8 \\

TTAC & Y-M & 11.6 & 10.3 & 15.8 & \textbf{6.8} & 15.9 & \textbf{7.5} & \textbf{5.8} & \textbf{8.7} & 9.0 & \textbf{8.5} & 5.6 & 5.7 & \textbf{12.7} & 8.0 & 9.7 & \textbf{9.4} \\

\midrule

TeSLA & N-M & 10.7 & \textbf{9.8} & 15.2 & 7.0 & \textbf{15.8} & 9.1 & 6.1 & 10.0 & \textbf{8.9} & 10.9 & 6.0 & 6.2 & 13.0 & \textbf{7.9} & 9.6 & 9.7 \\

TeSLA-s & Y-M & \textbf{10.4} & \textbf{9.8} & 14.9 & 7.3 & 16.1 & 9.0 & 6.2 & 9.5 & 9.1 & 11.5 & 5.9 & 5.8 & 12.9 & \textbf{7.9} & \textbf{9.5} & 9.7 \\

\midrule
\multicolumn{18}{c}{\cellcolor[HTML]{EFEFEF}CIFAR100-C} \\
\midrule

%%%%%%%%%%%%%%%% CIFAR100C %%%%%%%%%%%%%%%%


Source & N & 80.8 & 77.8 & 87.8 & 39.6 & 82.3 & 54.2 & 38.4 & 54.6 & 60.2 & 68.1 & 28.9 & 50.9 & 59.5 & 72.3 & 50.0 & 60.4 \\

\midrule

BN & N-O & 48.2 & 46.4 & 61.1 & 33.8 & 58.2 & 41.4 & 31.9 & 46.1 & 42.5 & 54.7 & 31.3 & 33.3 & 48.4 & 39.0 & 39.6 & 43.7\\

Tent & N-O & 43.3 & 41.2 & 52.7 & 31.2 & 50.8 & 36.1 & 29.3 & 41.9 & 38.9 & 43.6 & 30.1 & 31.0 & 43.5 & 34.4 & 36.5 & 39.0 \\

SHOT & N-O & 44.1 & 41.8 & 53.3 & 31.5 & 50.6 & 36.0 & 29.6 & 40.7 & 40.1 & 41.9 & 29.5 & 33.6 & 44.0 & 34.9 & 36.6 & 39.2\\

\midrule

TTT++ & Y-O & 50.2 & 47.7 & 66.1 & 35.8 & 61.0 & 38.7 & 35.0 & 44.6 & 43.8 & 48.6 & 28.8 & 30.8 & 49.9 & 39.2 & 45.5 & 44.4\\

TTAC & Y-O & 47.7 & 45.7 & 58.1 & 32.5 & 55.3 & 36.6 & 31.2 & 40.3 & 40.8 & \textbf{44.7} & 30.0 & 39.9 & 47.1 & 37.8 & 38.3 & 41.7 \\

\midrule

TeSLA & N-O & 40.0 & 38.9 & 51.5 & 32.2 & 49.1 & 36.9 & 29.7 & 40.4 & 37.4 & 46.0 & 29.3 & 30.7 & 42.7 & 32.9 & 34.6 & 38.2 \\

TeSLA-s & Y-O & \textbf{39.1} & \textbf{38.5} & \textbf{50.0} & \textbf{30.6} & \textbf{48.6} & \textbf{35.9} & \textbf{29.1} & \textbf{38.9} & \textbf{36.4} & 46.2 & \textbf{28.3} & \textbf{29.7} & \textbf{41.9} & \textbf{32.1} & \textbf{33.9} & \textbf{37.3} \\

\midrule
\midrule

BN & N-M & 47.4 & 45.5 & 60.0 & 33.9 & 56.9 & 40.8 & 31.8 & 46.4 & 42.6 & 54.2 & 32.3 & 33.1 & 48.5 & 37.2 & 39.4 & 43.3 \\

Tent& N-M & 41.0 & 38.4 & 49.2 & 30.0 & 47.4 & 33.1 & 28.1 & 38.1 & 38.0 & 37.5 & 28.3 & 29.0 & 41.1 & 32.8 & 35.6 & 36.5 \\

SHOT & N-M& 41.6 & 40.6 & 51.7 & 31.4 & 49.5 & 36.2 & 29.3 & 42.4 & 38.4 & 45.4 & 29.9 & 31.3 & 43.1 & 33.5 & 36.0 & 38.7 \\

\midrule

TTT++ & N-M & 38.4 & 37.7 & \textbf{41.3} & 29.1 & 44.1 & 32.9 & 27.8 & 34.3 & 34.4 & \textbf{34.7} & 25.4 & 26.6 & 39.2 &  32.3 & 33.6 & 34.1\\

TTAC & N-M & 37.8 & 36.8 & 45.1 & 28.2 & 45.3 & \textbf{30.7} & 26.6 & 35.3 & 35.7 & 36.7 & 26.8 & 27.4 & 39.6 & 30.6 & 34.2 & 33.6 \\

\midrule

TeSLA & N-M & 34.4 & 33.5 & 42.2 & 28.0 & \textbf{41.9} & 32.1 & \textbf{25.9} & 35.1 & 32.6 & 38.3 & 25.0 & 27.4 & 37.5 & 28.6 & 30.6 & 32.9\\

TeSLA-s & Y-M & \textbf{33.9} & \textbf{33.0} & 42.1 & \textbf{27.5} & 42.0 & 31.6 & 26.1 & \textbf{34.2} & \textbf{32.2} & 39.4 & \textbf{24.8} & \textbf{26.3} & \textbf{36.8} & \textbf{28.1} & \textbf{30.3} & \textbf{32.6} \\


\midrule
\multicolumn{18}{c}{\cellcolor[HTML]{EFEFEF}ImageNet-C} \\
\midrule

%%%%%%%%%%%%%%%% ImageNetC %%%%%%%%%%%%%%%%


Source & N & 97.0 & 96.3 & 97.4 & 82.1 & 90.3 & 85.3 & 77.5 & 83.4 & 76.9 & 76.0 & 40.9 & 94.6 & 83.5 & 79.1 & 67.4 & 81.8 \\

\midrule

BN & N-O & 83.5 & 82.6 & 82.9 & 84.4 & 84.2 & 73.1 & 60.5 & 65.1 & 66.3 & 51.5 & 34.0 & 82.6 & 55.3 & 50.3 & 58.7 & 67.7 \\

Tent & N-O & 70.8 & 68.7 & 69.1 & 72.5 & 73.3 & 59.3 & 50.8 & 53.0 & 59.1 & 42.7 & 32.6 & \textbf{74.5} & 45.5 & 41.6 & 47.8 & 57.4 \\

SHOT & N-O &77.0 & 74.6 & 76.4 & 81.2 & 79.3 & 72.5 & 61.7 & 65.7 & 66.3 & 55.6 & 56.0 & 92.7 & 57.1 & 56.3 & 58.2 & 68.7\\

\midrule

TTAC & Y-O & 71.5& 67.7 &70.3 &81.2 &77.3 &64 &54.4 &51.1 &56.9 &45.4 &32.6 &79.1 &46.0 & 43.7& 48.6& 59.3 \\

TTT++ & Y-O & 69.4 & 66 & 69.7 &84.2 &81.7 &65.2 &53.2 &49.3 &56.2 &44.4 &32.8 &75.7 &43.9 &41.6 &46.9 & 58.7\\

\midrule

TeSLA & N-O & 65.0 & 62.9 & 63.5 & 69.4 & 69.2 & 55.4 & 49.5 & 49.1 & 56.6 & 41.8 & 33.7 & 77.9 & 43.3 & 40.4 & 46.6 & 55.0\\

TeSLA-s & Y-O & \textbf{61.4} & \textbf{58.8} & \textbf{60.3} & \textbf{67.3} & \textbf{66.2} & \textbf{54.0} & \textbf{48.2} & \textbf{46.9} & \textbf{53.1} & \textbf{40.9} & \textbf{32.4} & 81.2 & \textbf{41.1} & \textbf{39.2} & \textbf{44.8} & \textbf{53.1} \\

\midrule
\midrule

BN & N-M & 83.4 & 82.6 & 82.8 & 84.4 & 84.2 & 73.2 & 60.3 & 64.9 & 66.4 & 51.2 & 34.0 & 82.6 & 54.9 & 49.9 & 58.8 & 67.6\\

Tent & N-M & 66.1 & 63.7 & 64.2 & 68.9 & 69.6 & 52.6 & 47.4 & 48.4 & 58.4 & 39.8 & \textbf{31.6} & 77.9 & 41.7 & \textbf{28.7} & 44.5 & 54.2 \\

SHOT & N-M & 75.8 & 73.7 & 73.7 & 78.3 & 77.1 & 71.8 & 60.9 & 64.2 & 66.1 & 55.4 & 59.8 & 95.5 & 56.1 & 57.3 & 58.1 & 68.2\\


\midrule

TeSLA & N-M & \textbf{62.3} & \textbf{60.9} & \textbf{60.6} & \textbf{64.3} & \textbf{65.7} & \textbf{50.4} & \textbf{46.2} & \textbf{46.1} & \textbf{54.7} & \textbf{39.1} & 32.2 & \textbf{68.5} & \textbf{40.9} & 37.5 & \textbf{43.5} & \textbf{51.5}\\

\bottomrule
\end{tabular}}
\label{tab:cifar_c}
\end{table*}

\begin{table*}[htb]
\centering
\caption{\textbf{Comparison of state-of-the-art TTA methods under different protocols} on the VisDA-C dataset. We report the class top-1 accuracies (\%) for each of the 12 classes. We also report the overall accuracy (Acc.) and the per-class average accuracy (Avg.). Each result is averaged over three seeds.}
%\textbf{Comparison of SOTA TTA methods under different protocols} on VisDA-C (train $\rightarrow$ val) dataset. We report the accuracy (\%) for each of the 12 classes. We also also report the overall Top1. accuracy (Acc.) as well as the per-class average accuracy (Avg.). Each result is averaged over 3 seeds.
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|c|cccccccccccc|cc}
\toprule
     Method & \rotatebox[origin=c]{90}{Protocol} & plane & bicycle & bus & car & horse & knife & mcycl & person & plant & sktbrd & train & truck & Acc. & Avg. \\
     
\midrule

Source & N & 3.8\std{4.5} & 23.3\std{0.7} & 56.0\std{3.9} & 82.5\std{0.9} & 70.8\std{3.1} & 1.6\std{0.3} & 84.4\std{1.3} & 9.1\std{2.2} & 78.0\std{5.4} & 22.1\std{3.7} & 79.3\std{2.4} & 1.6\std{0.7} & 55.6\std{0.7} & 48.5\std{1.0} \\

\midrule

BN & N-O & 86.9\std{2.2} & 57.8\std{2.3} & 75.4\std{1.2} & 52.9\std{1.3} & 86.7\std{0.6} & 54.2\std{4.0} & 85.5\std{0.9} & 55.4\std{2.0} & 64.9\std{2.7} & 41.6\std{2.3} & 85.7\std{1.2} & 28.8\std{2.5} & 64.5\std{0.3} & 64.6\std{0.5} \\

Tent & N-O  & 86.9\std{2.2} & 57.7\std{3.0} & 77.4\std{1.4} & 56.8\std{1.5} & 87.3\std{0.8} & 62.4\std{3.8} & 86.6\std{0.8} & 62.9\std{2.9} & 71.2\std{1.7} & 39.9\std{2.8} & 84.8\std{1.2} & 24.7\std{3.4} & 66.3\std{0.3} & 66.5\std{0.6} 
\\

SHOT & N-O  & 90.5\std{1.0} & 77.0\std{0.9} & 76.2\std{0.7} & 47.5\std{0.5} & 87.9\std{0.2} & 62.1\std{4.0} & 75.9\std{0.2} & 74.4\std{1.1} & 83.3\std{0.3} & 47.0\std{6.6} & 84.2\std{0.9} & 41.6\std{0.4} & 68.6\std{0.6} & 70.6\std{1.0} 
 \\
 
AdaContrast & N-O  & 95.2\std{0.3} & 78.2\std{0.3} & 81.8\std{0.1} & 67.9\std{1.2} & 94.9\std{0.5} & 87.4\std{3.3} & \textbf{87.9\std{0.6}} & 82.0\std{1.5} & 90.7\std{0.7} & 36.8\std{16.1} & \textbf{88.6\std{0.1}} & 31.5\std{3.6} & 76.2\std{0.7} & 76.9\std{1.4} \\

\midrule

TTT++ & Y-O & 86.4\std{1.5} & 60.5\std{2.6} & 75.7\std{2.2} & 51.7\std{3.6} & 86.5\std{0.9} & 55.3\std{2.1} & 85.2\std{2.7} & 55.8\std{1.1} & 64.5\std{2.7} & 41.3\std{2.1} & 86.4\std{1.9} & 28.4\std{2.6} & 64.4\std{0.8} & 64.8\std{0.7} \\

TTAC & Y-O & 90.0\std{1.2} & 64.7\std{12.5} & 69.7\std{0.9} & 48.5\std{1.7} & 84.3\std{1.8} & 82.8\std{3.6} & 84.7\std{4.1} & 64.7\std{7.2} & 72.1\std{1.3} & 40.2\std{6.3} & 86.5\std{1.2} & 25.5\std{5.6} & 65.5\std{1.6} & 67.8\std{2.1} \\


\midrule

TeSLA & N-O  & \textbf{95.4\std{0.2}} & \textbf{87.4\std{0.2}} & \textbf{83.8\std{0.6}} & \textbf{70.1\std{0.8}} & \textbf{95.1\std{0.1}} & 90.0\std{1.0} & 84.8\std{3.1} & \textbf{83.2\std{1.3}} & \textbf{93.6\std{0.1}} & \textbf{67.9\std{19.9}} & 85.4\std{0.8} & \textbf{49.3\std{1.2}} & \textbf{80.3\std{1.3}} & \textbf{82.2\std{1.9}} \\

TeSLA-s & Y-O & 92.0\std{0.2} & 81.2\std{2.0} & 77.1\std{1.9} & 56.5\std{0.9} & 90.2\std{0.4} & \textbf{91.0\std{0.9}} & 82.9\std{1.8} & 79.8\std{0.8} & 91.3\std{0.1} & 48.9\std{3.5} & 81.2\std{1.5} & 40.1\std{2.4} & 73.5\std{0.3} & 76.0\std{0.3} \\

\midrule
\midrule

BN & N-M  & 87.2\std{1.4} & 58.0\std{1.1} & 76.4\std{1.4} & 53.7\std{1.9} & 87.2\std{1.3} & 54.2\std{3.6} & 86.2\std{0.3} & 55.5\std{1.6} & 64.9\std{2.3} & 42.1\std{2.7} & 85.6\std{1.3} & 29.3\std{2.2} & 64.9\std{0.1} & 65.0\std{0.4}\\

Tent & N-M   & 89.1\std{2.0} & 56.4\std{5.9} & 82.4\std{1.0} & 69.2\std{0.5} & 89.3\std{1.3} & 95.2\std{0.5} & \textbf{91.4\std{0.5}} & 79.5\std{1.0} & 86.1\std{0.3} & 16.3\std{1.9} & 84.7\std{0.4} & 8.4\std{3.5} & 70.9\std{0.4} & 70.7\std{0.6}  \\

SHOT & N-M   & 93.9\std{0.5} & 82.6\std{0.7} & 76.6\std{0.8} & 49.7\std{1.8} & 92.0\std{0.2} & 79.0\std{21.6} & 75.3\std{2.0} & 80.9\std{2.4} & 89.5\std{0.6} & 50.5\std{19.0} & 83.8\std{0.9} & 52.2\std{1.1} & 72.7\std{1.8} & 75.5\std{3.4} \\

AdaContrast & N-M   & 95.6\std{0.6} & 82.8\std{1.0} & 76.5\std{2.4} & \textbf{72.4\std{5.3}} & \textbf{96.7\std{0.3}} & 91.3\std{2.2} & 88.6\std{1.2} & \textbf{85.4\std{0.8}} & 95.3\std{0.5} & 30.1\std{51.3} & \textbf{93.6\std{0.7}} & 48.9\std{2.1} & 79.7\std{1.3} & 79.8\std{3.9} \\

\midrule

TTT++ & Y-M & 87.2\std{2.0} & 61.8\std{2.0} & 74.7\std{1.3} & 52.7\std{3.6} & 86.1\std{1.7} & 65.0\std{7.0} & 84.9\std{2.3} & 62.1\std{6.0} & 67.2\std{1.6} & 36.6\std{1.3} & 86.2\std{0.1} & 27.1\std{3.6} & 65.3\std{0.3} & 65.9\std{1.0}\\

TTAC & Y-M &  86.8\std{4.2} & 73.5\std{1.3} & 69.3\std{2.1} & 44.2\std{2.5} & 78.8\std{5.1} & 73.1\std{6.7} & 84.7\std{1.6} & 67.3\std{8.6} & 78.6\std{5.6} & 52.9\std{4.1} & 84.7\std{2.6} & 33.2\std{3.9} & 66.0\std{2.0} & 68.9\std{2.4}\\

\midrule

TeSLA & N-M   & \textbf{96.6\std{0.2}} & \textbf{91.3\std{0.1}} & \textbf{85.1\std{1.0}} & 69.3\std{0.0} & \textbf{96.7\std{0.3}} & \textbf{97.1\std{0.8}} & 88.0\std{0.9} & 85.2\std{0.4} & \textbf{96.3\std{0.2}} & \textbf{87.7\std{9.3}} & 87.4\std{0.2} & \textbf{57.3\std{0.8}} & \textbf{83.4\std{0.6}} & \textbf{86.5\std{0.9}} \\

TeSLA-s & Y-M & 96.1\std{0.4} & 89.4\std{0.4} & 83.0\std{0.4} & 62.4\std{0.5} & 94.4\std{0.1} & 94.5\std{1.1} & 87.3\std{0.3} & 83.3\std{0.5} & 95.5\std{0.2} & 63.9\std{14.9} & 85.7\std{0.7} & 49.4\std{3.3} & 79.3\std{1.0} & 82.1\std{1.5} \\


\bottomrule
\end{tabular}}
\label{tab:visda}
\end{table*}



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[htb]
\caption{\textbf{Segmentation results} for test-time adaptation methods (class Avg. volume-wise mean Dice score in \%) on the spinal cord dataset (site $\left \{\texttt{1}  \right \}\rightarrow \texttt{2,3,4}$) and prostate dataset (sites $\left \{ \texttt{A,B} \right \}\rightarrow \texttt{D,E,F}$), respectively.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{@{}c|c|ccc|c||c|c|c|c@{}}
\toprule
Method              & \rotatebox[origin=c]{0}{Protocol}             & \multicolumn{4}{c||}{Spinal Cord}                                                                                    & \multicolumn{4}{c}{Prostate}                                  \\
\midrule
\multicolumn{2}{c|}{\multirow{2}{*}{Sites}} & $\left \{ \texttt{1} \right \}\rightarrow\left \{\texttt{2}  \right \}$ & $\left \{ \texttt{1} \right \}\rightarrow\left \{\texttt{3}  \right \}$ & $\left \{ \texttt{1} \right \}\rightarrow\left \{\texttt{4}  \right \}$ & $\left \{ \texttt{1} \right \}\rightarrow\left \{\texttt{2,3,4}  \right \}$ & $\left \{ \texttt{A,B} \right \}\rightarrow\left \{\texttt{D}  \right \}$ & $\left \{ \texttt{A,B} \right \}\rightarrow\left \{\texttt{E}  \right \}$ & $\left \{ \texttt{A,B} \right \}\rightarrow\left \{\texttt{F}  \right \}$ & $\left \{ \texttt{A,B} \right \}\rightarrow\left \{\texttt{D,E,F}  \right \}$ \\
\multicolumn{2}{c|}{}  & Class Avg.      & Class Avg.  & Class Avg.     &   Avg.   & Class Avg.         & Class Avg.         & Class Avg.         &   Avg.   \\

\midrule
Source & N  & 77.4\std{6.6} & 64.8\std{11.7} & 85.9\std{3.8} & 76.0 \std{11.8} & 75.8\std{8.9} & 65.9\std{18.5} & 38.4\std{32.3} & 60.5\std{27.0} \\

\midrule

BN     & N-O  & 85.2\std{2.1}  & 70.6\std{3.6}  & 88.9\std{1.7} & 81.6\std{8.3} & 75.9\std{9.4} & 74.4\std{7.4} & 65.7\std{22.4} & 72.1\std{15.2} \\
TENT   & N-O  & 85.7\std{1.8}  & 68.7\std{2.8}  & 88.9\std{1.7} & 81.1\std{9.1} & 78.8\std{6.2} & 77.9\std{6.9} & 67.0\std{28.4} & 74.7\std{17.9}\\
PL     & N-O  & 85.3\std{2.1}  & 71.0\std{3.6}  & 88.9\std{1.7} & 81.7\std{8.6} & 76.1\std{9.4} & 74.8\std{7.5} & 66.2\std{22.4} & 72.4\std{15.2}  \\
OptTTA & N-O  & 84.4\std{2.3}  &  80.2\std{5.1}  & 87.5\std{2.0}  & 84.1\std{4.8}     & 84.9\std{6.9} & \textbf{80.3\std{8.4}} & 84.0\std{6.6} & 83.1\std{7.7}\\
TeSLA  & N-O  & \textbf{86.3\std{1.5}}  & \textbf{80.3\std{7.3}} & \textbf{89.3\std{1.4}} & \textbf{85.3\std{5.8}} & \textbf{86.1\std{3.3}} &  79.8\std{7.5} & \textbf{84.3\std{6.3}} & \textbf{83.5\std{6.5}} \\
\midrule \midrule
BN  & N-M & 85.5\std{1.6} & 78.5\std{3.2} & 88.8\std{1.5} & 84.3\std{4.8} & 77.8\std{9.6} & 77.3\std{7.2} & 63.8\std{26.7} & 73.1\std{18.0}  \\
TENT& N-M  & 85.5\std{1.6} & 79.0\std{3.3} & 88.8\std{1.5} & 84.4\std{4.7} & 81.6\std{7.7} & 79.0\std{10.4} & 82.8\std{9.2} & 81.2\std{9.3} \\
PL  & N-M  & 85.5\std{1.7} & 78.8\std{3.3} & 88.8\std{1.5} & 84.3\std{4.7} & 81.2\std{7.9} & 79.1\std{10.1} & 82.8\std{9.1} & 81.1\std{9.2}\\
OptTTA & N-M  & 84.3\std{2.5} & \textbf{80.7\std{4.9}} & 87.7\std{2.0} & 84.3\std{4.4}  & \textbf{86.2\std{5.2}} & 78.6\std{8.6} & 85.0\std{6.7} & 83.4\std{7.7}\\
TeSLA  & N-M  & \textbf{86.4\std{1.7}}  & 80.4\std{3.2} & \textbf{89.3\std{1.7}} & \textbf{85.4\std{4.4}} & 85.9\std{4.0} & \textbf{81.2\std{6.7}} & \textbf{85.6\std{5.4}} & \textbf{84.3\std{5.8}} \\
\bottomrule
\end{tabular}%
}
\label{tab:mri}
\end{table*}
\begin{table*}[t!]
\centering
\caption{\textbf{Segmentation results} for test-time adaptation methods (mIoU\%)  for adaptation from synthetic GTA5 dataset to Cityscapes dataset (O and M protocols). }
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}c|c|ccccccccccccccccccc|c@{}}
\toprule
Method & \begin{sideways}Protocol\end{sideways} & \begin{sideways}road\end{sideways} & \begin{sideways}sidewalk\end{sideways} & \begin{sideways}building\end{sideways} & \begin{sideways}wall\end{sideways} & \begin{sideways}fence\end{sideways} & \begin{sideways}pole\end{sideways} & \begin{sideways}light\end{sideways} & \begin{sideways}sign\end{sideways} & \begin{sideways}vegetation\end{sideways} & \begin{sideways}terrain\end{sideways} & \begin{sideways}sky\end{sideways} & \begin{sideways}person\end{sideways} & \begin{sideways}rider\end{sideways} & \begin{sideways}car\end{sideways} & \begin{sideways}truck\end{sideways} & \begin{sideways}bus\end{sideways} & \begin{sideways}train\end{sideways} & \begin{sideways}motocycle\end{sideways} & \begin{sideways}bicycle\end{sideways} & mIoU \\
\midrule
Source & N & 72.9 & 20.0 & 81.4 & 21.7 & 22.9 & 19.2 & 25.3 & 10.6 & 78.9 & 26.4 & 85.9 & 54.9 & 20.7 & 53.0 & 30.6 & 16.2 &  1.9 & 20.0 &  7.5 & 35.3 \\
\midrule
BN & N-O & 84.3 & 31.8 & 79.2 & 24.1 & 20.5 & 21.5 & 23.5 & 10.5 & 74.6 & 32.0 & 75.3 & 52.1 & 14.4 & 77.6 & 28.1 & 20.6 &  6.0 & 14.9 &  6.2 & 36.7\\
PL & N-O & 84.0 & 31.1 & 80.6 & 25.5 & 20.7 & 21.5 & 24.7 & 11.4 & 77.3 & 34.0 & \textbf{79.4} & 54.2 & 17.1 & 78.3 & 30.1 & 21.7 &  9.7 & 18.5 &  8.1 & 38.3\\
Tent & N-O & 88.0 & 34.3 & 80.7 & 27.7 & 17.8 & 19.3 & 22.1 & 10.0 & \textbf{80.1} & \textbf{40.5} & 77.6 & 51.8 & 15.7 & 81.8 & \textbf{32.6} & 24.0 &  8.9 & 18.8 &  5.8 & 38.8\\
CoTTA  & N-O &  85.8 & 35.3 & 79.1 & 26.5 & 20.3 & 19.8 & 21.7 &  9.9 & 76.7 & 36.2 & 74.6 & 53.2 & 14.4 & 77.8 & 29.0 & 19.3 &  3.6 & 13.2 &  5.8 & 37.0 \\
TeSLA & N-O &  \textbf{90.4} & \textbf{52.2} & \textbf{82.5} & \textbf{29.6} & \textbf{25.5} & \textbf{28.1} & \textbf{32.5} & \textbf{29.7} & 79.7 & 39.0 & 75.2 & \textbf{59.0} & \textbf{21.3} & \textbf{84.0} & 29.3 & \textbf{24.6} & \textbf{14.5} & \textbf{23.0} & \textbf{26.1} & \textbf{44.5}\\
\midrule
\midrule
BN & N-M & 84.3 & 31.1 & 80.7 & 25.4 & 21.0 & 22.6 & 25.6 & 11.8 & 76.7 & 32.7 & 77.6 & 54.8 & 17.2 & 79.7 & 29.7 & 21.7 &  9.3 & 18.7 &  8.5 & 38.4\\
PL & N-M & 85.1 & 30.6 & 80.9 & 25.7 & 20.6 & 21.5 & 24.7 & 11.3 & 77.9 & 33.9 & \textbf{80.2} & 54.4 & 17.4 & 80.0 & 30.0 & 21.9 &  9.2 & 19.0 &  8.3 & 38.6\\
Tent & N-M & 89.0 & 35.1 & 81.0 & 28.4 & 17.0 & 19.5 & 22.9 &  9.8 & \textbf{80.8} & \textbf{41.8} & 76.7 & 52.4 & 16.3 & 83.6 & \textbf{33.0} & 24.8 &  7.1 & 20.6 &  5.7 & 39.2 \\
CoTTA & N-M & 88.6 & 40.2 & 80.6 & \textbf{30.0} & 20.4 & 19.2 & 25.9 & 16.0 & 77.1 & 32.7 & 75.3 & 55.7 & 23.1 & 82.8 & 30.1 & 19.4 &  9.8 & 19.9 & 11.3 & 39.9 \\
TeSLA & N-M & \textbf{90.1} & \textbf{51.4} & \textbf{83.1} & 29.0 & \textbf{27.7} & \textbf{28.7} & \textbf{34.8} & \textbf{34.0} & 78.7 & 35.7 & 73.0 & \textbf{62.0} & \textbf{26.5} & \textbf{83.9} & 28.5 & \textbf{25.0} & \textbf{25.7} & \textbf{27.3} & \textbf{29.4} & \textbf{46.0} \\

\bottomrule
\end{tabular}}
\label{tab:visdas}
\end{table*}


% \begin{table}
%     \centering
%     \caption{Ablation study of Pseudo Label Refinement Module of proposed method on VisDA-C measured by classification accuracy (mean \std{std}) over three seeds with Resnet-101. NN denotes Nearest Neighbors and Ens. denotes the ensembling prediction protocol. In this experiment, our method is optimized with $\mathcal{L}_{pl}$ only.
% }
%     {
%     \begin{tabular}{ccc|cc}
    
%     \toprule
    
%     \# & NN & Ens. & Acc. & Avg.\\
    
%     \midrule
    
%     0 & - & - & 70.6\std{0.7} & 72.3\std{1.1} \\
    
%     1 & \cmark & - & 76.6\std{1.2} & 80.2\std{1.9} \\
    
%     1 & - & \cmark & 73.0\std{0.8} & 75.1\std{1.3} \\
    
%     2 & \cmark & \cmark & 77.9\std{1.4} & 81.6\std{2.1} \\
    
%     \bottomrule
%     \end{tabular}}
%     \label{tab:ablation_plr}
% \end{table}

% \begin{table}
%     \centering
%     \caption{Analysis of positive effects of the proposed loss compared to cross entropy (CE) and flipped cross-entropy (f-CE). Results on VisDA-C measured by classification accuracy (mean \std{std}) over three seeds with Resnet-101.
% }
%     {
%     \begin{tabular}{ccc}
    
%     \toprule
    
%     Objective &  Acc. & Avg.\\
    
%     \midrule
    
%     Source & 55.6\std{0.7} & 48.5\std{1.0} \\
    
%     CE & 64.9\std{0.5} & 65.1\std{0.6} \\
    
%     f-CE & 71.4\std{0.5} & 71.6\std{1.0} \\
     
%     $\mathcal{L}_{pl}$ & 70.6\std{0.7} & 72.3\std{1.1} \\
    
%     \bottomrule
%     \end{tabular}}
%     \label{tab:ablation_pl_loss}
% \end{table}

\section{Runtime Analysis}\label{appendix:runtime}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
We compare the computational runtime cost of several test-time adaptation methods, including BN \cite{ioffe2015batch, nado2020evaluating}, TTAC \cite{su2022revisiting}, SHOT-IM \cite{liang2020we}, TENT \cite{wang2021tent}, AdaContrast \cite{chen2022contrastive} and our proposed method TeSLA in Table \ref{tab:computation_efficiency}. We also include overall TesLA runtimes using static, pre-optimized RandAugment (RA) / AutoAugment (AA) augmentation policies instead of the proposed Adversarial Augmentations.


\begin{table}[H]
    \centering
    \caption{Runtime (GPU hours) per epoch on GeForce RTX-3090 for ResNet-101 with batch size of 128 on the VisDA-C. RA implies RandAugment [10], AA implies AutoAugment [9].}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}ccccc|ccc@{}}
    \midrule
         BN & TTAC & SHOT-IM & TENT & AdaContrast & \textbf{TeSLA} & \textbf{TeSLA (RA)} & \textbf{TeSLA (AA)} \\
         % & & & & & & (RA) & (AA)\\
    \midrule
        % Run with batch size 64
        %  0.04 & 0.14 & 0.15 & 0.05 & 0.20 & 0.34\\
        
        % Run with batch size 128
        0.04 & 0.14 & 0.16 & 0.05 & 0.22 & \cellcolor{cyan!20}\textbf{0.38} & \cellcolor{cyan!20}\textbf{0.27} & \cellcolor{cyan!20}\textbf{0.28} \\
    \midrule
    \end{tabular}}
    \label{tab:computation_efficiency}
\end{table}

\section{Equivalence to other Test-Time Objectives}\label{appendix:other_tt_obj}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
Our proposed flipped cross-entropy loss \textit{f-}$\mathbb{CE}$ of Eq. \ref{eq1} without soft-pseudo labels from the teacher is equivalent to entropy minimization of TENT \cite{wang2021tent}, while our final objective $\mathcal{L}_\text{TeSLA}$ of Eq. \ref{eq:loss_TeSLA} without the knowledge distillation from adversarial augmentation is equivalent to SHOT-IM \cite{liang2020we} as
$\mathcal{D}_{\text{KL}}\left (\mathbf{Y}\parallel\hat{\mathbf{Y}}\mid \mathbf{X}\right ) = 0$ when the teacher network is an instant update of the student (momentum $\alpha$ is 0). Thus, without the mean-teacher and adversarial augmentation, our method would have similar shortcomings as that of TENT and SHOT. Incorporating the soft-pseudo labels from the mean teacher alone improves TeSLA's accuracy on VisDA-C from 82.0\% to 86.5\%.

\section{Sensitivity Tests and Aditional Ablations}\label{appendix:sensitivity}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
\subsection{Sensitivity Tests}
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Ablation_lmbnorm.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Ablation_N_subpolicy.pdf}
        \caption{}
    \end{subfigure}
    \caption{\textbf{Sensitivity test for adversarial augmentation hyperparameters} of TeSLA on the VisDA-C dataset for classification task on various TTA protocols. We plot the class Avg. accuracy (\%) on the VisDA-C dataset for (a) augmentation severity controller $\lambda_1 \in \{0, 0.1, 1, 10\}$ and (b) sub-policy dimension $N \in [1,5]$.}
    \label{fig:ablation_augmentation}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Ablation_lmbnorm_seg.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Ablation_N_subpolicy_seg.pdf}
        \caption{}
    \end{subfigure}
    \caption{\textbf{Sensitivity test for adversarial augmentation hyperparameters} of TeSLA on the VisDA-S dataset for segmentation task on various TTA protocols. We plot the mIoU (\%) on the VisDA-S dataset for (a) augmentation severity controller $\lambda_1 \in \{0, 0.1, 1, 10\}$ and (b) sub-policy dimension $N \in [1,5]$, respectively.}
    \label{fig:ablation_augmentation_seg}
\end{figure}

\paragraph{Automatic Adversarial Augmentation.}
We additionally provide the sensitivity test results for the hyperparameters of the automatic adversarial augmentation module ($\lambda_1$ and sub-policy dimension $N$) on the VisDA-C and VisDA-S datasets. In Fig. \ref{fig:ablation_augmentation}, we show how the class average (Avg.) accuracy (\%) varies with the hyperparameter $\lambda_1$ controlling the severity of augmentations and the sub-policy dimension $N$ on the VisDA-C dataset. Similarly, Fig. \ref{fig:ablation_augmentation_seg} shows the effect of changing $\lambda_1$ and $N$ on the segmentation scores measured by mIoU (\%) on the VisDA-S dataset. We observe that the performance of our method TeSLA is stable over a wide range of $\lambda_1$ and $N$ for both classification and segmentation tasks.

\begin{figure}
    \centering
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Ablation_nn_visda.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/Ablation_qs_visda.pdf}
        \caption{}
    \end{subfigure}
    \caption{\textbf{Sensitivity test for PLR hyperparameters}: (a) the number of nearest neighbors $n$ and (b) the class memory queue size $N_\mathbf{Q}$ on the VisDA-C dataset. We report the overall accuracy (Acc.) and the class average accuracy (Avg.) in \% on the ViSDA-C under the N-O protocol.}
    \label{fig:ablation_nn}
\end{figure}

\paragraph{PLR hyperparameters.}
We present sensitivity tests for the hyperparameters of the soft pseudo-label refinement (PLR) module. In Fig. \ref{fig:ablation_nn},  we show the test-time adaptation classification performance of TeSLA on the VisDA-C (N-O) for varying numbers of nearest neighbors $n\in\{1, 4, 10, 32, 64, 128\}$, and class memory queue size $N_\mathbf{Q}\in\{16, 32, 64, 128, 256, 512, 1024\}$. TeSLA outperforms competing baselines under a wide range of choices. Moreover, the number of examples in the queue can be as small as less than 0.5\% of the dataset size and still maintains on-par performance.




%While we opt $n=10$ and $N_\mathbf{Q}$=256 for the final model, TeSLA still outperforms competing baselines under a wide range of choices. Moreover, the number of examples in the queue can be as small as less than 0.5\% of the dataset size and still maintains on-par performance.



Fig. \ref{fig:views_ablation_cifar100} shows the classification performance of TeSLA on the CIFAR10-C and CIFAR100-C and various corruptions [{\footnotesize\textbf{\textsc{Gaussian Blur, Spatter, Speckle Noise, Saturate}}}] under multiple protocols and the number of weak augmentation for ensembling $|\rho_w|\in\{2,3,5,9\}$. These plots show that increasing the number of views decreases the average error rate in both protocols. While we report the results with $|\rho_w|=5$ in the main, we observe we could further decrease the error rate with $|\rho_w|=9$. However, this choice would multiply the computational cost by two as the number of forward passes is linearly proportional to this hyperparameter. For this reason, we opt $|\rho_w|=5$, which gathers the benefit of ensembling and reasonable computational cost.

\begin{figure}
\centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=0.905\linewidth]{figures/Ablation_views_cifar10.pdf}
        \caption{CIFAR10-C}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=0.905\linewidth]{figures/Ablation_views_cifar100.pdf}
        \caption{CIFAR100-C}
    \end{subfigure}
    \caption{\textbf{Sensitivity test on the number of weak augmentation} ($|\rho_w|$) for ensembling soft-pseudo labels for Soft-Pseudo Label Refinement (PLR) on (a) CIFAR10-C and (b) CIFAR100-C datasets. We report, for each case, the average error rate over 4 validation corruptions.}
    \label{fig:views_ablation_cifar100}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/Ablation_lmb2_cifar10.pdf}
    \caption{CIFAR10-C}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/Ablation_lmb2_cifar100.pdf}
    \caption{CIFAR100-C}
\end{subfigure}
    \caption{\textbf{Sensitivity test on the scalar coefficient} $\lambda_2$ of $\mathcal{L}_\text{kd}$ term of the test time loss $\mathcal{L}_\text{TeSLA}$. We report, for each case, the average error rate over the 4 validation corruptions.}
    \label{fig:lmb2_ablation}
\end{figure}

\paragraph{Knowledge distillation coefficient.}
Finally, we report the sensitivity test results for the knowledge distillation weight $\lambda_2$. In Fig. \ref{fig:lmb2_ablation}, we show the classification adaptation performance of TeSLA on the CIFAR10-C and CIFAR100-C datasets is not very sensitive to the selection of $\lambda_2 \in \{0.1, 1, 10\}$.

\subsection{Ablations}
\paragraph{EMA coefficient of teacher model.}
In Fig. \ref{fig:alpha_ablation_cifar10}, we show the effect of changing the EMA coefficient $\alpha$ used for updating the teacher model from the student model for the one-pass (O protocol) and multi-pass (M protocol) on the CIFAR10-C and CIFAR100-C datasets. We observe that for the multi-pass protocol (M), decreasing $\alpha$ leads to better performance, while for the one-pass protocol (O), optimal $\alpha$ depends on the number of test images observed in one epoch. If $\alpha$ is large (close to 1.0), the teacher is updated very slowly and thus requires more updates to reach better performance. Therefore, for a one-pass online evaluation, the accuracy decreases. On the other hand, if we set $\alpha$ to a minimal value, it results in unstable convergence.

\begin{figure}
\centering
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/Ablation_alpha_cifar10.pdf}
    \caption{CIFAR10-C}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/Ablation_alpha_cifar100.pdf}
    \caption{CIFAR100-C}
\end{subfigure}
    \caption{\textbf{Sensitivity test on the EMA coefficient of the teacher model} $\alpha$ on the (a) CIFAR10-C and (b) CIFAR100-C datasets. We report the average error rate over four corruptions for each dataset.}
    \label{fig:alpha_ablation_cifar10}
\end{figure}

\paragraph{Batch size and learning rate.}
In Fig. \ref{fig:bs_lr}, we show the effect of batch size and learning rate on the proposed method TeSLA along with TENT\cite{wang2021tent}, SHOT\cite{liang2020we}, and TTAC\cite{su2022revisiting} on CIFAR10-C for N-O protocol. We observe that increasing batch size helps reduce test time error rates, and the model performs best with the same batch size used during source model training. Similarly, increasing the learning rate reduces the error rate until it becomes too large for unstable gradient model updates.

\begin{figure}
 \centering
 \begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/Ablation_BS_cifar.pdf}
    \caption{}
\end{subfigure}
\begin{subfigure}{0.48\linewidth}
    \includegraphics[width=\linewidth]{figures/Ablation_LR_cifar.pdf}
    \caption{}
 \end{subfigure}
    \caption{\textbf{Ablation study} for the roles played by the (a) batch size $B$ and (b) the learning rate scale with respect to the default value. We report, for each baseline, the average error rate (in \%) over four validation corruption sets of the CIFAR10-C under the N-O protocol.}
    \label{fig:bs_lr}
\end{figure}



% \begin{figure*}
% \centering
%     \begin{subfigure}{0.48\textwidth}
%         \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_alpha_cifar10.pdf}
%         \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_lmb2_cifar10.pdf}
%         \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_views_cifar10.pdf}
%         \caption{\textbf{CIFAR10-C}}
%     \end{subfigure}
%     \begin{subfigure}{0.48\textwidth}
%         \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_alpha_cifar100.pdf}
%         \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_lmb2_cifar100.pdf}
%         \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_views_cifar100.pdf}
%         \caption{\textbf{CIFAR100-C}}
%     \end{subfigure}
%     \caption{Sensitivity test on EMA Coefficient of the Teacher model $\alpha$, scalar coefficient $\lambda_2$ of $\mathcal{L}_\text{kd}$ term of the test time loss $\mathcal{L}_\text{TeSLA}$, and the number of weak augmentation ($|\rho_w|$) for ensembling soft-pseudo labels for Soft-Pseudo Label Refinement (PLR).}
%     \label{fig:overall_ablation}
% \end{figure*}
\section{Additional Qualitative Results}\label{appendix:qualitative_results}
\renewcommand\thefigure{\thesection.\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/sanity_check.pdf}
    \caption{\textbf{Adversarial augmentation sanity check.} We report the per-class average accuracy (\%) of TeSLA's student model on the VisDA-C augmented by the ten most adversarial sub-policies optimized by our automatic augmentation module.}
    \label{fig:sanity_check}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/callibration.pdf}
    \caption{\textbf{Calibration performance comparison} of TeSLA (with and without adversarial augmentations) against other baselines via a reliability diagram on the VisDA-C dataset for N-O protocol.}
    \label{fig:calibration}
\end{figure}
\subsection{Sanity Check for Adversarial Augmentation}
To assess the adversarial effect of the proposed automatic augmentations, we conduct a sanity check for the optimized sub-policies. in particular, in Fig. \ref{fig:sanity_check}, we rank the sub-policies optimized by our automatic augmentation module on the VisDA-C for N-M protocol after one epoch by decreasing the order of sampling probability. Then, we evaluate the performance of the student model on the test-test images from VisDA-C that are augmented using the above sub-policies. We observe that reducing the hardness level of sub-policies, the more the student model is accurate in recognizing the images. This is supported by the Pearson correlation of 0.7 between the sub-policy rank and the accuracy $(p=0.02)$, demonstrating our module's capability to optimize and sample adversarial examples.

\begin{figure}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_spinal_cord_segfir_gt.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_spinal_cord_segfir_deepall.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_spinal_cord_segfir_bn.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_spinal_cord_segfir_tent.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_spinal_cord_segfir_pl.png}
        \caption{}
    \end{subfigure}
    % \begin{subfigure}{0.15\linewidth}
    %     \includegraphics[width=\linewidth]{latex/figures/mri_spinal_cord_segfir_opttta.png}
    %     \caption{}
    % \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_spinal_cord_segfir_tesla.png}
        \caption{}
    \end{subfigure}}
    \caption{\textbf{Qualitative segmentation results} of test-time adaptation methods trained on \textbf{site 1} and tested on \textbf{site 3} of the spinal cord dataset. From left to right: (a) Ground Truth, (b) Source Model, (c) BN\cite{nado2020evaluating}, (d) TENT\cite{wang2021tent}, (e) PL, and (f) TeSLA, respectively.}
    \label{fig:spinal_qulatitative}
\end{figure}

\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_prostate_segfir_gt.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_prostate_segfir_deepall.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_prostate_segfir_bn.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_prostate_segfir_tent.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_prostate_segfir_pl.png}
        \caption{}
    \end{subfigure}
    % \begin{subfigure}{0.15\linewidth}
    %     \includegraphics[width=\linewidth]{latex/figures/mri_prostate_segfir_opttta.png}
    %     \caption{}
    % \end{subfigure}
    \begin{subfigure}{0.15\linewidth}
        \includegraphics[width=\linewidth]{figures/mri_prostate_segfir_tesla.png}
        \caption{}
    \end{subfigure}}
    \caption{\textbf{Qualitative segmentation results} of test-time adaptation method trained on \textbf{site A} and \textbf{site B} and tested on \textbf{site F} of the prostate dataset. From left to right: (a) Ground Truth, (b) Source Model, (c) BN\cite{nado2020evaluating}, (d) TENT\cite{wang2021tent}, (e) PL, and (f) TeSLA, respectively.}
    \label{fig:prostate_qulatitative}
\end{figure}

\subsection{Uncertainty Evaluation}
We evaluate the model reliability on the ViSDA-C classification adaptation task (N-O protocol). In Fig. \ref{fig:calibration}, we show the reliability diagram (dividing the probability range [0, 1.0] into ten bins) and report the expected calibration error (ECE) \cite{niculescu2005predicting} for the Source model without adaptation, SHOT \cite{liang2020we}, and TeSLA with and without adversarial augmentations. The proposed TeSLA gives the lowest calibration error with an 8.71\% improvement over SHOT. It is interesting to observe that the ECE of TeSLA without adversarial augmentations is on-par with the SHOT method. The adversarial augmentation module improves the TeSLA's ECE by 8\%, which shows the benefit of test-time adversarial augmentation on the model's reliability.

\subsection{Qualitative Segmentation Results}
In Fig. \ref{fig:spinal_qulatitative} and Fig. \ref{fig:prostate_qulatitative}, we show the qualitative segmentation results of TeSLA for test-time adaptation on the spinal cord and prostate MRI datasets and compare it with TENT\cite{wang2021tent}, BN\cite{nado2020evaluating}, and Pseudo Labeling (PL). Compared to other baselines, TeSLA outputs more accurate segmentation results closer to the provided ground truth.
