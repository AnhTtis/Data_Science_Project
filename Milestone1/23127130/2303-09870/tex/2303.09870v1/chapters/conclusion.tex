\section{Conclusion}
\label{sec:limitation}
We introduced TeSLA, a novel self-learning algorithm for test-time adaptation that utilizes automatic adversarial augmentation. TeSLA is agnostic to the model architecture and source training strategies and gives better model calibration than other TTA methods. Through extensive experiments on various domain shifts (measurement, common corruption, synthetic to real), we show TeSLA's superiority over previous TTA methods on classification and segmentation tasks. Note that TeSLA assumes class uniformly when implicitly maximizing mutual information and can be improved by incorporating prior, e.g., class label distribution statistics.

%Note that TeSLA assumes class uniformly when implicitly maximizing mutual information, and our future work focuses on improving this by incorporating prior, e.g., class label distribution statistics.





%leave it for future work to investigate other mechanisms to address class imbalance streaming test data.




%As TeSLA implicitly maximizes mutual information, we assume an even class distribution of the test images, which may be a minor limitation of our work.  

%We introduce TeSLA, a novel test-time adaptation method in image classification and segmentation for various types of domain shifts.


%\Devavrat{In summary, we propose a new test time self-learning algorithm that utilizes adversarial data augmentation at test time to better adapt and generalize on the test images. TeSLA is agnostic to the model architecture and source training strategies and gives better model calibration. 
%Through extensive experiments on a wide range of distribution shifts (medical, corruption, synthetic to real), we show TeSLA's superiority over previous methods on classification and segmentation tasks. As TeSLA implicitly maximizes mutual information, we assume an even class distribution of the test images, which may be a minor limitation of our work.}

% \Devavrat{Even though TeSLA works well over different model architecture designs and tasks (classification and segmentation) on a wide range of test time distribution shifts including medical imaging, there are a few limitations. One major limitation of the current work is its applicability when the test images are not I.I.D. distributed with the labels in the online stream for the classification task. To counter such scenarios, the test time loss function should also incorporate temporal consistency of the predicted labels. Another limitation of TeSLA is its applicability when the distribution shift changes continuously with time. For such scenarios, learning data augmentations as a proxy for remembering the previous target distribution while the model adapts to the current target distribution can be helpful. As Tesla implicitly maximizes mutual information, we assume that the test dataset follows even class distribution..}

% \section{Conclusion}
% \label{sec:conclusion}