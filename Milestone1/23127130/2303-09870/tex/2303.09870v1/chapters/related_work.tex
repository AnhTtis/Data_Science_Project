\section{Related Work}
\label{sec:rel_work}
In general, domain adaptation methods aim to distill knowledge from source data that are well-generalizable to target data and relax the assumption of i.i.d. between source and target datasets. To circumvent expensive and cumbersome annotation of new target data, \textbf{unsupervised domain adaptation (UDA)} emerges, and there has been a large corpus of UDAs\cite{peng2019moment,zhang2020collaborative,hoffman2018cycada,ganin2015unsupervised} to match the distribution of domains on both labeled source data and unlabeled target data.  Nonetheless, the above-mentioned methods demand source data to achieve the domain adaptation process, which is often impractical in real-world scenarios, e.g., due to privacy restrictions. The above issue motivates research into \textbf{source-free domain adaptation (SFDA)} \cite{liang2020we,li2020model,agarwal2022unsupervised,kundu2020universal} and \textbf{test-time training} or \textbf{adaptation (TTA)} \cite{su2022revisiting,iwasawa2021test,liu2021ttt,wang2021tent}, which are more closely relevant to our problem setup.

SFDA approaches formulate domain adaptation through pseudo labeling \cite{liang2020we,kundu2020universal}, target feature clustering \cite{yang2021generalized}, synthesizing extra training samples \cite{kundu2020towards}, or feature restoration \cite{eastwood2022sourcefree}. SHOT \cite{liang2020we} proposed feature clustering via information maximization while incorporating pseudo-labeling as additional supervision. BAIT \cite{yang2020casting} leverages the fixed source classifier as source anchors and uses them to achieve feature alignment between the source and target domain. CPGA \cite{Qiu2021CPGA} proposed the SFDA method based on matching feature prototypes between source and target domains. AaD-SFDA \cite{yang2022local} proposed to optimize an objective by encouraging prediction consistency of local neighbors in feature space. Nevertheless, SFDA methods require apriori access to all target data in advance, and the current pseudo-labeling approach, e.g., SHOT \cite{liang2020we}, used offline pseudo-label refinement on a per-epoch basis. In a more realistic domain adaptation scenario, SFDA will still be incompetent to perform inference and adaptation simultaneously.  

TTA methods \cite{wang2021tent,chen2022contrastive} propose alleviating the domina shift by online  (or streaming) adapting a model at test time. Still, we argue that there are ambiguities over the problem setup of TTA in the literature, particularly on whether sequential inference on target test data is feasible upon arrival \cite{iwasawa2021test,wang2021tent} or whether training objectives must be adapted \cite{liu2021ttt,sun2020test}. TTT \cite{sun2020test} proposed fine-tuning the model parameters via rotation classification task as a proxy. On-target adaptation \cite{wang2021target} used pseudo-labeling and contrastive learning to initialize the target-domain feature; each performed independently in their method. T3A \cite{iwasawa2021test} utilized pseudo-labeling to adjust the classifier prototype. More recently, TTAC \cite{su2022revisiting} leveraged the clustering scheme to match target domain clusters to source domain categories. Nevertheless, existing TTA methods rely on specialized neural network architectures or only update a fraction of model weights yielding limited performance gain on the target data. For instance, TENT \cite{wang2021tent} proposed updating affine parameters in the batchnorm layers of convolutional neural networks (CNN), while AdaContrast \cite{chen2022contrastive} and SHOT \cite{liang2020we} used an additional weight normalization classification layer with a projection head. In contrast, we show our methodsâ€™ superiority for various neural network architectures, including CNNs and vision transformers (ViTs) \cite{dosovitskiy2020vit}, without additional architectural requirements and different source model training strategies. Moreover, we update all model parameters, and our method is stable over a wide range of hyper-parameters, e.g., learning rate.

\textbf{Test time augmentation} methods \cite{lyzhov2020greedy,ashukha2019pitfalls} are another popular line of domain adaptation research. %Instead of adapting the model on the target domain,  the optimal augmentation policies are explored that yield better performance on the target domain. The final prediction is made during the inference by ensembling the predictions over the optimal augmentations. 
GPS \cite{lyzhov2020greedy} learns optimal augmentation sub-policies by combining image transformations of RandAugment \cite{cubuk2020randaugment} that minimize calibrated log-likelihood loss \cite{ashukha2019pitfalls} on the validation set. OptTTA \cite{tomar2022opttta} optimized the magnitudes of augmentation sub-policies using gradient descent to maximize mutual information and match feature statistics over augmented images. Though effective, these methods \cite{lyzhov2020greedy, tomar2022opttta, wang2019aleatoric} are computationally expensive as all augmentation sub-policies need to be evaluated, making their real-time application difficult. Instead, our proposed adversarial augmentation policies can be learned online and are several orders faster than the existing learnable test-time augmentation strategies, e.g., \cite{tomar2022opttta}. 



%On the other hand, TeSLA optimizes both the magnitude and selection probabilities of augmentation sub-policies using unbiased sample gradient estimates and thus is much more efficient for real-time applications.

%\behzad{ %Current augmentation policy learning methods \cite{cubuk2018autoaugment, cubuk2020randaugment, lim2019fast, li2020dada, hataya2022meta} could not emulate all possible test-time distribution shifts during model training. On the other hand, recent test-time augmentation policy learning methods \cite{lyzhov2020greedy, tomar2022opttta, wang2019aleatoric} are computationally expensive, and the cost requirement has to increase when learning new augmentation policies during model adaptation at test time. Instead, our proposed adversarial augmentation policies can be learned online and are several orders faster than the existing learnable test-time augmentation strategies, e.g., \cite{tomar2022opttta}. } 


%During \cm{the model }evaluation, they select and finetune top-performing optimized sub-policies individually for subsequent test images. 

%\paragraph{Self Training and Pseudo Labels}