\section{Experiments}\label{sec:exps}
%In this section, we first ...

\subsection{Datasets and Experimental Settings}\label{sec:datasets}
We evaluate and compare TeSLA against state-of-the-art (SOTA) test-time adaptation algorithms for both classification and segmentation tasks under three types of test-time distribution shifts resulting from (1) \textbf{common image corruption}, (2) \textbf{synthetic to real data transfer}, and (3) \textbf{measurement shifts} on medical images. The latter is characterized by a change in medical imaging systems, e.g., different scanners across hospitals or various staining techniques.  

%\vspace{0.5em}
\noindent\textbf{TTA protocols.} We adopt the TTA protocols in \cite{su2022revisiting} and categorize competing methods based on two factors: (i) source training objective and (ii) sequential or non-sequential inference. First, unlike \cite{su2022revisiting}, we use \textbf{Y} to indicate if access to the source domain's information, e.g., feature statistics is possible or if the source training objective is allowed to be modified; otherwise, we use \textbf{N}. Next, we use \textbf{O} to indicate a one-pass adaptation and evaluation protocol (one epoch) for sequential test data and \textbf{M} to show a multi-pass adaptation on all test data and inference after several epochs.
% With the above criterion,
Thus, we end up with four possible TTA protocols, namely \textbf{N-M}, \textbf{Y-M}, \textbf{N-O}, and \textbf{Y-O}. Unlike previous works \cite{liu2021ttt,su2022revisiting}, TeSLA does not rely on any source domain feature distribution, yielding two realistic TTA protocols: \textbf{N-M} and \textbf{N-O}.  


%\vspace{0.5em}
\noindent\textbf{Hyperparameters for all experiments.} We set $\lambda _{1}=\lambda _{2}=1.0$ for all experiments (cf. ablation in \textbf{Appendix~\apxE}). For the adversarial augmentation module, we use sub-policy dimension $N=2$ (except for VisDA-C \cite{visda2017}, $N=4/3$ for N-O/N-M protocols) (cf. \textbf{Appendix~\apxE}) and Adam optimizer \cite{kingma2014adam}. For pseudo-label refinement (PLR), we use five weak augmentations composed of random flips and resize crop.
More details of hyperparameters used for each experiment can be found in \textbf{Appendix~\apxA}.

%\vspace{0.5em}
\noindent\textbf{Common image corruptions.}
To evaluate TeSLA's efficacy for the classification task, we use \textbf{CIFAR10-C/CIFAR100-C} \cite{hendrycks2018benchmarking} and large-scale \textbf{ImageNet-C} \cite{hendrycks2018benchmarking} datasets each containing 19 types of corruptions applied to the clean test set with five levels of severity. We perform validation on 4 out of 19 types of corruption {\footnotesize\textsc{{(Spatter, Gaussian Blur, Speckle Noise, Saturate)}}} to select hyperparameters and test on the remaining 15 corruptions at the maximum severity level of 5. Following \cite{liu2021ttt}, we train the ResNet50 \cite{he2016deep} on the clean CIFAR10/CIFAR100/ImageNet training set and adapt it to classify the unlabeled corrupted test set.
%Unlike previous works \cite{liu2021ttt}, since we remove that sentence we should say unlike ...
%\cm{To evaluate TeSLA's efficacy for the classification task on standard corrupted images, we use \textbf{CIFAR10-C/CIFAR100-C} \cite{hendrycks2018benchmarking} and large-scale \textbf{ImageNet-C} \cite{hendrycks2018benchmarking}, each containing 19 types of corruptions with five levels of severity.
%Each CIFAR10-C/CIFAR100-C consists of 50,000 training images of clean data, and the corruptions are applied to 10,000 clean test images \cite{krizhevsky2009learning}. Similarly, ImageNet-C contains 50,000 corrupted testing samples. Unlike previous works \cite{liu2021ttt} that identify optimal hyperparameters directly on the test set, we perform validation on 4 out of 19 types of corruption corruptions {\footnotesize\textsc{{(Spatter, Gaussian Blur, Speckle Noise, Saturate)}}} to select hyperparameters and test on the remaining 15 corruptions at the maximum severity level of 5. Following \cite{liu2021ttt}, we use the ResNet50 \cite{he2016deep} trained on the clean CIFAR10/CIFAR100/ImageNet and test it on the CIFAR10-C/CIFAR100-C/ImageNet-C.}



% \cm{For the \textbf{N-M} protocol on CIFAR10-C/CIFAR100-C, we adapt the model for 70 epochs using the Adam optimizer \cite{kingma2014adam}  at a learning rate of $0.001$. We also use a batch size (BS) of 128 and an EMA coefficient $\alpha$ of 0.999 for updating the teacher model. We use the same setup for the \textbf{N-O} protocol except $\alpha=0.996$. The number of nearest neighbors $n$ is set to 4 on CIFAR10-C and one on CIFAR100-C. For ImageNet-C, we use SGD optimizer at a learning rate of $2.5 \times  10^{-4}$, and we adapt the model with the $\alpha$ value of 0.996 and BS of 128 for five epochs.} 

%\vspace{0.5em}
\noindent\textbf{Synthetic to real data adaption.} We use challenging and large-scale \textbf{VisDA-C}, and \textbf{VisDA-S} \cite{visda2017} datasets for evaluating synthetic-to-real data adaptation at test-time for classification and segmentation tasks, respectively. Following \cite{chen2022contrastive, liang2020we}, we adapt the ResNet101 network pre-trained on synthetic images to classify 12 vehicle classes on the photo-realistic images of VisDA-C. While for VisDA-S, we adapt the DeepLab-v3 \cite{chen2017rethinking} backbone pre-trained on synthetic GTA5 \cite{Richter_2016_ECCV} to the Cityscapes \cite{Cordts2016Cityscapes} to segment 19 classes.


%\cm{\noindent\textbf{Synthetic to real data transfer.} We use a challenging large-scale \textbf{VisDA-C} \cite{visda2017} for evaluating TeSLA on synthetic-to-real data transfer for the object classification task. Following \cite{chen2022contrastive, liang2020we}, we adapt the ResNet101 network to recognize 12 classes of various vehicles on the VisDA-C. Similarly, we evaluate TeSLA on the \textbf{VisDA-S} (synthetic \textbf{GTA} \cite{Richter_2016_ECCV} $\rightarrow$ \textbf{Cityscapes} dataset \cite{Cordts2016Cityscapes}) for the segmentation task. We adopt the DeepLab-v3 \cite{chen2017rethinking} backbone trained on the GTA5 dataset to segment 19 classes on the Cityscapes dataset.}


% \cm{For the \textbf{N-M} setting used in the classification task, we adapt the model using Adam optimizer at a learning rate of $0.001$ for five epochs and BS of 128. We set momentum $\alpha$ to 0.999 and the number of nearest neighbors $n$ to 1. For the \textbf{N-O} protocol, the same setup is used for one epoch but with $\alpha=0.996$. For the segmentation task (\textbf{N-M} protocol),  we use Adam optimizer with a learning rate of $0.0001$ and batch size of 8, the model is adapted for five epochs, and we set $\alpha$ to 0.999, while in the \textbf{N-O} protocol, we set the learning rate to $2\times10^{-4}$ and $\alpha$ to 0.996, respectively.} 


%\Devavrat{
%We use datasets from Visual Domain Adaptation Challenge \cite{visda2017} for test time adaptation of the source model trained on synthetic 2D renderings of 3D models to photo-realistic image for classification task (VisDA-C) and synthetic dashcam renderings from GTA-5 \cite{Richter_2016_ECCV} to the real-world collection of dashcam images from CityScapes \cite{Cordts2016Cityscapes} (VisDA-S) for the segmentation task. Following the setup of \cite{chen2022contrastive, liang2020we}, we adapt the ResNet-101 network to recognize 12 classes of various vehicles on the photo-realistic set of VisDA-C. 
%For the offline setting, we use Adam optimizer at a learning rate of $10^{-3}$ for 5 epochs and a batch size of 128. We set EMA momentum $\alpha$ to 0.999 and the number of nearest neighbors $n$ to 1. For the online setting, the same setup is used for one epoch but with $\alpha$ equal to 0.996. For the segmentation task on VisDA-S dataset, we adopt DeeplabV3 \cite{chen2017rethinking} network trained on the GTA5 dataset to segment 19 classes on the Cityscapes dataset. In the offline setting, we use Adam \cite{kingma2014adam} optimizer with a learning rate of $10^{-4}$ and batch size of 8 for 5 epochs and set $\alpha$ to 0.999, while in the online setting we set learning rate to $2*10^{-4}$ and $\alpha$ to 0.996.}

%\vspace{0.5em}
\noindent\textbf{Measurement shifts on medical images.} We access TeSLA on staining variations for the tissue type classification of hematoxylin \&  eosin (H\&E) stained patches from colorectal cancer tissue slides. We use MobileNetV2 \cite{sandler2018mobilenetv2} trained on the source \textbf{Kather-19} dataset \cite{kather2019predicting} and adapt it to the target \textbf{Kather-16} dataset \cite{kather_2016_53169} on four tissue categories: tumor, stroma, lymphocyte, and mucosa. We also evaluate TeSLA for variations in scanners and imaging protocols in multi-site medical images on two magnetic resonance imaging (MRI) datasets for the segmentation task, namely the \textbf{multi-site prostate MRI} \cite{liu2020ms} and \textbf{spinal cord grey matter segmentation (SCGM)} \cite{prados2017spinal}. Following \cite{tomar2022opttta}, we adapt the U-Net \cite{ronneberger2015u} from site 1 to sites \{2,3,4\} of the spinal cord dataset, and  from sites \{A,B\} to sites \{D,E,F\} of the prostate dataset.

%\cm{\noindent\textbf{Measurement
%shifts on medical images.} We evaluate our method on measurement shift (staining variations) for the tissue type classification task of stained patches from hematoxylin \&  eosin (H\&E) colorectal cancer tissue slides. We use MobileNetV2 \cite{sandler2018mobilenetv2} trained on the source \textbf{Kather-19} dataset \cite{kather2019predicting} and adapt it to the target \textbf{Kather-16} dataset \cite{kather_2016_53169} on four tissue categories: tumor, stroma, lymphocyte, and mucosa. We also evaluate TeSLA for variations in scanners and imaging protocols in multi-site medical images on two magnetic resonance imaging (MRI) datasets for the segmentation task, namely the \textbf{multi-site prostate MRI} \cite{liu2020ms} and \textbf{spinal cord grey matter segmentation (SCGM)} dataset \cite{prados2017spinal}. Following \cite{tomar2022opttta}, we adapt the U-Net \cite{ronneberger2015u} that is trained on site 1 and tested on sites 2, 3, and 4 for the spinal cord dataset. Similarly, we used the trained U-Net on sites A and B for the prostate dataset and adapted to sites D, E, and F.}

% \cm{We adapt each model using Adam optimizer with a learning rate of 10 and BS of 16 for five epochs and set $\alpha$ to 0.999 in the \textbf{N-M} setting. We use the same setup for the  \textbf{N-O} protocol except for $\alpha=0.996$.}

%\vspace{0.5em}
\noindent\textbf{Competing baselines.} We compare TeSLA with the following SFDA and TTA baselines, including direct inference of the trained source model on the target test data without adaptation (\textbf{Source}). We also implement the SFDA-based pseudo-labeling baselines from a basic pseudo-labeling approach (\textbf{PL}) to the \textbf{SHOT} approach~\cite{liang2020we} based on training the feature extraction module by adopting the information maximization loss to make globally diverse but individually certain predictions on the target domain. For recent TTA methods, we compare our method against \textbf{TTT++}~\cite{liu2021ttt}, \textbf{AdaContrast}~\cite{chen2022contrastive}, \textbf{CoTTA}~\cite{wang2022continual}, basic \textbf{TENT}~\cite{wang2021tent} and \textbf{BN}~\cite{ioffe2015batch} as representative methods based on feature distribution alignment via stored statistics, contrastive learning, augmentation-averaged predictions, entropy minimization, and batch normalization statistic. Furthermore, we compare TeSLA to more recent methods based on test-time augmentation policy learning, \textbf{OptTTA} \cite{tomar2022opttta}, and anchor clustering \textbf{TTAC}~\cite{su2022revisiting}. By default, ~\cite{liang2020we,ioffe2015batch,chen2022contrastive} follow the N-M protocol, and we adapt them to the N-O protocol, while ~\cite{wang2021tent,tomar2022opttta,wang2022continual}, by default, follow the N-O protocol and adapt them to the N-M protocol setting. Using source features distribution, TTAC and TTT++ follow Y-O and Y-M.
 % and we adapt them for other TTA protocols
We report error rates on classification tasks and Mean Intersection over Union (mIoU)/ Dice scores on segmentation tasks.
% and Expected Calibration Error (ECE).
For consistency across baselines and TTA protocols, we do not use any specialized model architectures (e.g., projection head, weight-normalized classifier layer) for SHOT~\cite{liang2020we} and AdaContrast~\cite{chen2022contrastive}.

\begin{figure*}[t]
      \centering
      \begin{subfigure}{0.4\textwidth}
          \centering
          \includegraphics[width=\linewidth]{figures/radar_plot_2.pdf}
          \caption{}
          \label{fig:radar_plot}
      \end{subfigure}
      \hspace{1em}
      \begin{subfigure}{0.4\textwidth}
          \centering
          \includegraphics[width=\linewidth]{figures/Tsne_2.pdf}
          \caption{}
          \label{fig:tsne_plot}
      \end{subfigure}
    %     \begin{subfigure}{0.55\textwidth}
    %     \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_plloss.pdf}
    %     \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_PLR.pdf}
    %     \includegraphics[width=0.32\linewidth]{latex/figures/Ablation_global.pdf}
    %     \caption{}
    %     \label{fig:ablation_components}
    %   \end{subfigure}
    %   \begin{subfigure}{0.35\textwidth}
    %     % \includegraphics[width=0.48\textwidth]{latex/figures/Ablation_BS_cifar.pdf}
    %     % \includegraphics[width=0.48\textwidth]{latex/figures/Ablation_LR_cifar.pdf}
    %     \includegraphics[width=0.48\textwidth]{latex/figures/Ablation_lmbnorm.pdf}
    %     \includegraphics[width=0.48\textwidth]{latex/figures/Ablation_N_subpolicy.pdf}
    %     \caption{}
    %     \label{fig:ablation_augmentation}
    %   \end{subfigure}
    \label{fig:results_callibration_tsne}
    \caption{\textbf{Ablation experiments on the VisDA-C}. \textbf{(a) Source training strategy and model architecture.} TeSLA outperforms the competing TTA methods across varied source training strategies (Supervised, SimCLRV2\cite{chen2020big}, and DINO\cite{caron2021emerging}) and model architectures (ResNet50, ResNet101 \cite{he2016deep}, and ViT-B/16\cite{dosovitskiy2020vit}) using the same set of hyperparameters for both protocols (N-O/N-M). Each vertex represents the mean class avg accuracy over 3 seeds. \textbf{(b) t-SNE visualization} \cite{van2008visualizing} comparison of feature embeddings for AdaContrast \cite{chen2022contrastive} and TeSLA.}
    % \textbf{(c) (\textit{left to right})} (i) Ablation on Self-Learning Loss functions including \textit{f-$\mathbb{CE}$}, $\mathbb{CE}$, and $\mathcal{L}_\text{pl}$; (ii) Ablation on different PLR methods: Ensembling predictions on weak augmentations, Averaging predictions of the Nearest Neighbours in the feature space; (iii) Ablation on different components of TeSLA's objective function: $\mathcal{L}_\text{pl}$, PLR, and $\mathcal{L}_\text{kd}$. \textbf{(d) Ablation on sub-policy dimension $N$ and augmentation severity.} Average class accuracy of TeSLA with augmentation severity $\lambda_1=0, 0.1, 1.0, 10.0$ and sub-policy dimension $N\in{1,..., 5}$.
    % \cm{\textbf{(d) Ablation on Batch Size and Learning Rate.} Average Class error rates of TeSLA compared with other TTA methods for different test time batch size and different optimizer learning rates on CIFAR-10-C dataset.}
    \vspace{-0.5em}
\end{figure*}


\begin{table}[t]
    \centering
    \caption{\textbf{Comparison of SOTA TTA methods under different protocols} evaluated on CIFAR-10/100-C, ImageNet-C, VisDA-C and Kather-16 datasets. We report the average error computed over 15 test corruptions for the common image corruption shifts. We also report \textit{Class Avg.} in \% error rates for synthetic-to-real and measurement shifts over 3 and 10 seeds, respectively.}
    
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}c|c|cccccc|cc|cc@{}}
    
    \toprule
    
       \multirow{3}[3]{*}{Method} &  \multirow{3}[3]{*}{\rotatebox[origin=c]{90}{Protocol}} & \multicolumn{6}{c|}{\cellcolor[HTML]{FFFFE0}\textbf{Common Image Corruptions}} & \multicolumn{2}{c|}{\cellcolor[HTML]{FFE0FF}\textbf{Syn-to-Real}} & \multicolumn{2}{c}{\cellcolor[HTML]{FED8B1}\makecell{\textbf{Measurement}\\\textbf{Shift}}} \\
       
      \cmidrule(lr){3-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
       
      \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} & \multicolumn{2}{|c}{CIFAR10-C} & \multicolumn{2}{c}{CIFAR100-C} & 
      \multicolumn{2}{c|}{ImageNet-C} &
      \multicolumn{2}{c|}{VisDA-C} & \multicolumn{2}{c}{Kather-16} \\
      
      \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10} \cmidrule(lr){11-12}
      
       & & \multicolumn{1}{c}{O} & M & O & M & O & \multicolumn{1}{c|}{M} & O & M & O & M \\ %\multicolumn{1}{c|}{Test Protocol}
      
    
    \midrule

    Source & N & \multicolumn{2}{c}{29.1} & \multicolumn{2}{c}{60.4} & \multicolumn{2}{c|}{81.8} & \multicolumn{2}{c|}{51.5} & \multicolumn{2}{c}{32.0} \\
    
    \midrule
    
    BN~\cite{nado2020evaluating,ioffe2015batch}& N & 15.6 & 15.4 & 43.7 & 43.3 & 67.7 & 67.6 & 35.4 & 35.0 & 18.3 & 18.2\\
    TENT~\cite{wang2021tent}& N & 14.1 & 12.9 & 39.0 & 36.5 & 57.4 & 54.2 & 33.5 & 29.3 & 16.2 & 12.0 \\
    SHOT~\cite{liang2020we} & N & 13.9 & 14.2 & 39.2 & 38.7 & 68.7 & 68.2 & 29.4 & 24.5 & 14.7 & 12.0 \\
    AdaContrast~\cite{chen2022contrastive} & N & \multicolumn{6}{c|}{-} & 23.1 & 20.2 & \multicolumn{2}{c}{-}\\
    
    \midrule
    
    TTT++ \cite{liu2021ttt} & Y & 15.8 & 9.8 & 44.4 & 34.1 & 59.3 & - & 35.2 & 34.1 & 16.7 & 7.9\\
    TTAC~\cite{su2022revisiting} & Y & 13.4 & \textbf{9.4} & 41.7 & 33.6 & 58.7 & - &  32.2 & 31.1 & 9.6 & 5.5\\ %\multicolumn{2}{c|}{-}
    
    \midrule
    
    \rowcolor[HTML]{E0FFFF}
     \textbf{TeSLA} & N & 12.5 & 9.7 & 38.2 & 32.9 & 55.0 & \textbf{51.5} & \textbf{17.8} & \textbf{13.5} & \textbf{9.2} & 3.3 \\
     
     \rowcolor[HTML]{E0FFFF}
     \textbf{TeSLA-s} & Y & \textbf{12.1} & 9.7 & \textbf{37.3} & \textbf{32.6}
     & \textbf{53.1} & - & 24.0 & 17.9 & 9.9 & \textbf{3.1} \\

    \bottomrule
    
    \end{tabular}}
    \label{tab:results_classification}
    \vspace{-0.5em}
\end{table}

\subsection{Results}\label{subsec:results}
\noindent \textbf{Classification task.} In Table \ref{tab:results_classification}, we summarize the average (Avg) classification error rates (\%) of TeSLA against several SOTA \textbf{SFDA} and \textbf{TTA} baselines on the \textbf{common image corruptions} of CIFAR10-C, CIFAR100-C, ImageNet-C; \textbf{synthetic to real data} domain shifts of VisDA-C; and \textbf{measurement shifts} on the Kather-16 dataset. We also report per-class top-1 accuracies for VisDA-C and Kather-16 datasets and corruption-wise error rates on CIFAR-10-C/CIFAR-100-C and ImageNet-C datasets in \textbf{Appendix~\apxB}. TeSLA easily surpasses all competing methods under the protocols N-O and N-M on all the datasets. In particular, TeSLA outperforms the second-best baseline under N-O/N-M protocol by a (\%) margin 1.4/3.2 on CIFAR10-C, 0.8/3.6 on CIFAR100-C, 2.4/2.7 on ImageNet-C, 5.3/6.7 on VisDA-C, and 5.5/8.7 on the Kather-16 datasets. Despite not utilizing any source feature statistic, TeSLA (N-O/N-M) is either competitive or surpasses TTAC\cite{su2022revisiting} and TTT++\cite{liu2021ttt} (Y-O/Y-M) that benefit from the source dataset feature statistics during adaptation.
% We surpass the second-best TTA baseline under \textbf{N-O} protocol by 1.4\% on CIFAR10-C, 0.8\% on CIFAR100-C, -1.0\% on ImageNetC, 5.3\% on VisDA-C and 2.1\% on Kather; while under \textbf{N-O} protocol we,
Including the global source feature alignment module of TTAC with TeSLA (\textbf{TeSLA-s}) could further improve its performance under (Y-O/Y-M) protocols for slight domain shifts (e.g., common image corruptions). We report the results of AdaContrast for VisDA-C only as the implementation for other datasets are not provided. We do not report TeSLA-s results on ImageNet-C under the Y-M protocol, as previous baselines were evaluated only Y-O protocol.

%\Guillaume{Additonally, we do not report results on ImageNet-C under the Y-M protocol as previous baselines evaluated only under the Y-O protocol.}
%We include the class-wise error rates on VisDA-C and Kather datasets and corruption-wise error rates on CIFAR-10-C/CIFAR-100-C and ImageNet-C datasets in the Appendix.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/uncertainty.pdf}
  % \includegraphics[height=0.42\linewidth]{figures/reliability_diagram.pdf}
  % \hspace{1em}
  % \includegraphics[height=0.42\linewidth]{figures/brier_nll.pdf}
  \caption{\textbf{Model calibration and uncertainty estimation.} Model calibration using reliability diagrams \cite{niculescu2005predicting} and model uncertainty estimation using Brier Score and Negative Log Likelihood (NLL) of the Source Model, TENT\cite{wang2021tent}, OptTTA\cite{tomar2022opttta} and, TeSLA on the prostate dataset for test-time adaptation. \vspace{-0.5em}}
  \label{fig:reliability_brier_plot}
\end{figure}

\begin{table}[t]
    \centering
    \caption{\textbf{Semantic segmentation results} (\textit{Class Avg.} mIoU in \%) on the VisDA-S (GTA5 $\rightarrow $ Cityscapes) test-time adaptation task. }
    \vspace{-0.5em}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|c|c|ccccc}
    
    \toprule
    
    \multicolumn{2}{c|}{Protocol} & Source & BN\cite{nado2020evaluating} & TENT\cite{wang2021tent} & PL & CoTTA\cite{wang2022continual} & \cellcolor[HTML]{E0FFFF}\textbf{TeSLA} \\
    
    \midrule
    
    N & O & \multirow{2}[2]{*}{35.3}& 36.7 & 38.3 & 38.8 & 37.0 & \cellcolor[HTML]{E0FFFF}\textbf{44.5} \\
    
    N & M & & 38.4 & 39.2 & 38.6 & 39.9 & \cellcolor[HTML]{E0FFFF}\textbf{46.0} \\
    
    \bottomrule
    
    \end{tabular}}
    \vspace{-0.5em}
    \label{tab:results_gta_cityscape}
\end{table}

\begin{table}[ht]
    \centering
    \caption{\textbf{Semantic segmentation results} (\textit{Class Avg.} volume-wise \textit{mean} Dice  in \%) on the cross-site spinal cord and prostate MRI test-time adaptation tasks. }
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|c|ccccc}
    
    \toprule
    
    \multicolumn{2}{c|}{Protocol} & Source & BN\cite{nado2020evaluating} & TENT\cite{wang2021tent} & PL & OptTTA\cite{tomar2022opttta} & \cellcolor[HTML]{E0FFFF}\textbf{TeSLA} \\
    
    \midrule
    \multicolumn{8}{l}{\textbf{Spinal Cord}\hspace{1em}$\texttt{Site}\left \{ \texttt{1} \right \}\rightarrow \texttt{Sites}\left \{\texttt{2,3,4}  \right \}$ } \\
    \midrule
    
    N & O & \multirow{2}[2]{*}{76.0\std{11.8}} & 81.6\std{8.3} & 81.1\std{9.1} & 81.7\std{8.6} & 84.1\std{4.8} & \cellcolor[HTML]{E0FFFF}\textbf{85.3\std{5.8}} \\
    
    N & M & & 84.3\std{4.8} & 84.4\std{4.7} & 84.3\std{4.7} & 84.3\std{4.4} & \cellcolor[HTML]{E0FFFF}\textbf{85.4\std{4.4}} \\
    
    \midrule
    \multicolumn{8}{l}{\textbf{Prostate}\hspace{1em}$\texttt{Sites}\left \{ \texttt{A,B} \right \}\rightarrow \texttt{Sites}\left \{\texttt{D,E,F}  \right \}$} \\
    \midrule
    
        
    N & O & \multirow{2}[2]{*}{60.5\std{27.0}} & 72.1\std{15.2} & 74.7\std{17.9} & 72.4\std{15.2} & 83.1\std{7.7} & \cellcolor[HTML]{E0FFFF}\textbf{83.5\std{6.5}} \\ %\textbf{83.5}
    N & M & & 73.1\std{18.0} & 81.2\std{9.3} & 81.1\std{9.2} & 83.4\std{7.7} & \cellcolor[HTML]{E0FFFF}\textbf{84.3\std{5.8}} \\
    
    \bottomrule
    
    \end{tabular}}
    \label{tab:results_mri}
    \vspace{-0.5em}
\end{table}


%\vspace{0.5em} 
\noindent \textbf{Segmentation task.} Unlike TeSLA, several TTA methods e.g., AdaContrast\cite{chen2022contrastive}, SHOT\cite{liang2020we}, TTT++\cite{liu2021ttt}, TTAC\cite{su2022revisiting}, have \textit{only} been evaluated on the classification task. Therefore, we compare TeSLA against the current SOTA \textbf{TTA} methods applied to the segmentation task on \textbf{synthetic-to-real data transfer} of the VisDA-S dataset in Table \ref{tab:results_gta_cityscape}. TeSLA significantly outperforms the competing methods and achieves the best mIOU scores for both N-O and N-M protocols, beating the second-best baseline by a margin of +5.7\% and +6.1\%, respectively. In Table \ref{tab:results_mri}, we compare TeSLA against the recent SOTA test-time augmentation policy method, OptTTA\cite{tomar2022opttta} and other TTA baselines for the inter-site adaptation on two challenging MRI datasets mentioned in Sec. \ref{sec:datasets}. TeSLA convincingly outperforms all other methods on severe measurement shifts across sites under $\textbf{N-O}$ and $\textbf{N-M}$ protocols.
 

%\vspace{0.5em}
\noindent \textbf{Computational cost for adversarial augmentations.} As shown in Table \ref{tab:computation_cost}, TeSLA learns the adversarial augmentation in an online manner, and thus its runtime is several orders faster than learnable test-time augmentation OptTTA \cite{tomar2022opttta}, while comparable to that of static augmentation policies with an additional overhead of only 0.10 GPU hours/epoch on the VisDA-C dataset. 





%Also, in Fig. \ref{fig:sanity_check}, we show the accuracy of the current teacher model on the images augmented by the sub-policies arranged in descending order of their probabilities of selection. The sub-policy with the highest probability of selection gives the most adversarial augmented view (corresponding to the lowest accuracy).

%\vspace{0.5em}
\noindent \textbf{Test-time feature visualization.} Fig.~\ref{fig:tsne_plot} compares t-SNE projection \cite{van2008visualizing} of the encoder's features of AdaContrast\cite{chen2022contrastive} with TeSLA on the VisDA-C. TeSLA shows better inter-class separation for features than AdaContrast, as supported by an improved silhouette score from $0.149$ to $0.271$.
% ,  signifying an improvement in classification accuracy.

%\vspace{0.5em}
\noindent \textbf{Model calibration and uncertainty estimation.} Entropy minimization-based TTA methods \cite{wang2021tent, liang2020we} explicitly make the model confident in their predictions, resulting in poor model calibration. For trusting the adapted model, it should output reliable confidence estimates matching its true underlying performance on the test images. Such a well-behaved model is characterized by expected \textbf{calibration error (ECE)} through \textbf{reliability diagram} \cite{niculescu2005predicting}, and uncertainty metrics of \textbf{brier score} and \textbf{negative log-likelihood (NLL)} \cite{gomariz2021probabilistic}. In Fig. \ref{fig:reliability_brier_plot}, we plot the reliability diagram (dividing the probability range $\left [ 0,0.5 \right ]$ into 10 bins), and uncertainty metrics of the Source model, TENT\cite{wang2021tent} and OptTTA\cite{tomar2022opttta} against TeSLA for the inter-site model adaptation on the prostate dataset \cite{liu2020ms}. We observe that TeSLA achieves the best model calibration (lowest ECE of 1.80\%) and lowest Brier and NLL scores of 0.12 and 0.24 ($p$ $<$ 0.006 against TENT).

%We observe that TeSLA achieves the best model calibration (lowest ECE) of 1.80\% and lowest uncertainty scores (Brier and NLL scores) of 0.12 and 0.24 respectively ($p < 0.006$ and $<0.005$ against TENT respectively.)


 %In Fig. \ref{fig:reliability_brier_plot}, we plot the reliability diagram and uncertainty metrics of the Source Model, TENT\cite{wang2021tent} and OptTTA\cite{tomar2022opttta} against TeSLA for the inter-site model adaptation on the Prostate \cite{liu2020ms} dataset. We observe that TeSLA achieves the best ECE, Brier, and NLL scores of 1.80\%, 0.12, 0.24 against 3.95\%, 0.13, 0.34 of the second best baseline OptTTA respectively .}
% \Devavrat{Describe the results in table 1. Describe results with several backbones describing calibration plots and overall computation cost for augmentation policy search. Sanity check. T-SNE plot.}


% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/callibration.pdf}
%     \caption{Evaluating calibration of test time adaptation in the order: Source Model, TENT\cite{wang2021tent}, OptTTA\cite{tomar2022opttta}, and TeSLA on the Prostate dataset : (a) Reliability diagrams \cite{niculescu2005predicting} of with Expected Calibration Error (ECE) 17.4\%, 8.3\%, 3.9\%, and 2.9\% respectively. (b) Brier Score of 0.26, 0.17, 0.14, 0.13; Negative Log Likelihood of 1.19, 0.67, 0.53, 0.50; and Dice Score of 60.5\%, 74.7\%, 83.1\%, 81.8\% respectively.}
%     \label{fig:reliability_diagram}
% \end{figure*}

\begin{table}[t]
    \centering
    \caption{\textbf{Performance comparison of adversarial augmentation} learned by TeSLA during adaptation against the prior art augmentation methods on runtime (GPU hours/epoch on {NVIDIA GeForce RTX 3090}) and \textit{Class Avg.} accuracy in \% on the VisDA-C dataset.}
    \vspace{-0.5em}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccc|c}
    \toprule
         Augmentation Type & OptTTA & \makecell{TeSLA\\RA\cite{cubuk2020randaugment}} & \makecell{TeSLA\\AA\cite{cubuk2018autoaugment}} & \cellcolor[HTML]{E0FFFF}\makecell{TeSLA\\$N=2$} & \cellcolor[HTML]{E0FFFF}\makecell{TeSLA\\$N=3$} \\
         \midrule
         Runtime & 4.50 & 0.05 & 0.06 & \cellcolor[HTML]{E0FFFF}0.16 & \cellcolor[HTML]{E0FFFF}0.18 \\
        \textit{Class Avg.} Acc. (N-O) & - & 80.2 & 81.2 & \cellcolor[HTML]{E0FFFF}81.5 & \cellcolor[HTML]{E0FFFF}82.2 \\
        \textit{Class Avg.} Acc. (N-M) & - & 84.7 & 85.7 & \cellcolor[HTML]{E0FFFF}86.3 & \cellcolor[HTML]{E0FFFF}86.5 \\
     \bottomrule
    \end{tabular}
    }
    \vspace{-0.5em}
    \label{tab:computation_cost}
\end{table}

\subsection{Ablation Studies}\label{sec:ablations}
In this section, we scrutinize the roles played by different components of TeSLA. All ablations are performed on the synthetic-to-real test data adaptation of the VisDA-C dataset.

%\vspace{0.5em}
%\noindent\textbf{Loss functions for self-learning with pseudo labels.}
\noindent\textbf{Loss functions for self-learning.} We compare TTA performance of our self-learning objective $\mathcal{L}_\text{pl}$ with the proposed flipped cross-entropy loss \textit{f}-$\mathbb{CE}$ and the basic cross-entropy loss $\mathbb{CE}$ in Fig. \ref{fig:ablation_components}-(1). We do not use the proposed augmentation module and pseudo-label refinement in this experiment. We observe that \textit{f}-$\mathbb{CE}$ alone improves the accuracy of the Source model in the N-M protocol by +23.1\% compared to +16.6\% improvement by $\mathbb{CE}$. Using $\mathcal{L}_\text{pl}$ (Eq. \ref{eq:loss_pl}) further improves the accuracy to a margin of +23.8\%. 

%\vspace{0.5em}
\noindent\textbf{Contribution of individual components: PLR, $\mathcal{L}_\text{pl}$, and $\mathcal{L}_\text{kd}$.} Our test-time objective $\mathcal{L}_\text{TeSLA}$ has three components-- (i) self-learning loss $\mathcal{L}_\text{pl}$, (ii) pseudo-label refinement (PLR), and (iii) knowledge distillation $\mathcal{L}_\text{kd}$ with adversarial augmentation. In Fig. \ref{fig:ablation_components}-(3), we study the effect of accruing individual components to $\mathcal{L}_\text{TeSLA}$  (starting from source trained model) for the test-time adaptation accuracy. We observe that $\mathcal{L}_\text{pl}$ alone improves the model's accuracy from 48.5\% to 72.3\% while adding PLR and $\mathcal{L}_\text{kd}$ further boosts it to 81.6\% (+9.3) and 86.3\% (+4.7), respectively.

%\vspace{0.5em}
\noindent\textbf{PLR: ensembling on weak augmentations (Ens) and nearest neighbor averaging (NN).} We also conduct an ablation on the two pseudo-label refinement approaches -- (i) ensembling the teacher model's output on five weak augmentations. (ii) averaging soft pseudo labels of $n=10$ nearest neighbors in the feature space (cf. sensitivity test in \textbf{Appendix~\apxE}). Fig. \ref{fig:ablation_components}-(2) shows that combining both approaches gives better results than using them individually.

%\vspace{0.5em}
\noindent\textbf{Adversarial augmentation.} Table \ref{tab:computation_cost} shows the benefit of using our adversarial augmentation ($N=2,3$) with TeSLA against (i) \textbf{RandAugment (RA)} \cite{cubuk2020randaugment}, (ii) \textbf{AutoAugment (AA)} \cite{cubuk2019autoaugment} (optimized for ImageNet), and (iii) OptTTA \cite{tomar2022opttta}. We achieve far better \textit{Class Avg.} accuracy on the VisDA-C at the cost of slight runtime overhead compared to static augmentation policies for protocols (N-O, N-M). In addition, TeSLA augmentation (TeAA) consistently improves the performance of other TTA methods (Table \ref{tab:tesla_shot}).
\begin{table}[ht]
    \centering
    \caption{\textit{Class Avg. accuracy (\%)} of \textbf{TeSLA augmentation (TeAA)} with other TTA objectives under N-O protocol on VisDA-C dataset.}
    \vspace{-0.5em}
    \resizebox{0.9\linewidth}{!}{\begin{tabular}{c|c||c|c||c|c}
    \toprule
         TENT & \textbf{+TeAA} & SHOT & \textbf{+TeAA} & AdaContrast & \textbf{+TeAA}\\ 
    \midrule
         66.5 & \textbf{72.0} & 70.6 & \textbf{73.1} & 76.9 & \textbf{79.1} \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
    \label{tab:tesla_shot}
\end{table}
% SHOT \cite{liang2020we} (+3.7\%), AdaContrast \cite{chen2022contrastive} (+0.8\%) by replacing their default augmentation method.


%In addition, the usage of adversarial augmentation with the sub-policy dimensions of $N=2,3$ surpasses other augmentation policy replacements: \textbf{RandAugment (RA)} \cite{cubuk2020randaugment}, and \textbf{AutoAugment (AA)} \cite{cubuk2019autoaugment}, during adaptation by TeSLA (\textit{Class Avg.} accuracies) on the VisDA-C dataset for both protocols (N-O, N-M). 



%\vspace{0.5em}
\noindent\textbf{Source training strategies and model architectures.} The ablation results (Fig. \ref{fig:radar_plot}) on the VisDA-C show that TeSLA surpasses competing TTA baselines for different source training strategies, including \textbf{Supervised} and self-supervised (\textbf{SimCLRV2} \cite{chen2020big}, \textbf{DINO}\cite{caron2021emerging}). Moreover, we ablate TeSLA over different architectures, ranging from ResNet-50/101\cite{he2016deep} to the recent vision transformer ViT-B \cite{dosovitskiy2020vit} using the same set of hyperparameters, showing the merits of no reliance on the network architecture or source training strategy. 

%Finally, in \textbf{Appendix~\apxE}, we provide detailed sensitivity results with respect to all hyperparameters.\vspace{-0.5em}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{figures/Ablation_plloss.pdf}
    \includegraphics[width=0.32\linewidth]{figures/Ablation_PLR.pdf}
    \includegraphics[width=0.32\linewidth]{figures/Ablation_global.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Ablation experiments on the VisDA-C.} (\textit{left to right}) (1) Ablation results for self-learning loss functions, including \textit{f-$\mathbb{CE}$}, $\mathbb{CE}$, and $\mathcal{L}_\text{pl}$; (2) ablation on different PLR methods: ensembling predictions on 5 weak augmentations, averaging predictions of the 10 nearest neighbors in the feature space; (3) ablation on different components of TeSLA's objective function: $\mathcal{L}_\text{pl}$, PLR, and $\mathcal{L}_\text{kd}$.}
    \label{fig:ablation_components}
    \vspace{-0.5em}
\end{figure}

% \vspace{0.5em}\noindent\textbf{Effect of sub-policy dimension.} \Devavrat{We perform sensitivity tests on a wide range of hyperparameter values to show the robustness of our method SLug. Fig.}


% \subsection{Analysis of the Augmentation Module}

% \Guillaume{
% \begin{itemize}
%     \item Optimal Hard Augmentations: We compare with (i) no augmentation (Done), (ii) RandAugment policy (Done), (iii) AutoAugment policy (Done) and (iv) TeachAugment adapted (Todo) -> Table \behzad{(Adding plot for e.g., a sanity check to show that our SLug can learn reasonable augmentation for specific dataset that is perhaps close to corruption. Reporting search cost in GPU hours or similar experiments to show efficiency (linked to Introduction))}
%     \item augmentation: we evaluate our method with policy dimension from 1 to 5 (Done) + weight of regularization term [0,0.1,1,10] (Done) -> Figure
%     \item We analyze in more details the optimal augmentation module: computation cost (in medical part), qualitative results (image examples, vs teachaugment and randaugment), what augmentation are selected (make plot)? Maybe find a metric to compute the expansion (diversity)
%     \item On VisDA (To repeat Online)
%     \item \behzad{(Adding calibration plots, t-sne, you can use two colÃ©umns format to place four figures))}
% \end{itemize}}

% \subsection{Sensitivity to Hyper-parameters}
% \Guillaume{We evaluate the model independence along the axis of hyper-parameters on CIFAR10-C/Cifar100-C validation corruptions:
% \begin{itemize}
%     \item learning rate (1e-2, 1e-3, 1e-4) (TODO) (On CIFAR on test set (to compare with other methods)) 
%     \item batch size (64, 128, 256, compare with TTAC) (On CIFAR on test set (to compare with other methods))
%     \item lambdas (0.1, 1, 10) (Sone) (On CIFAR)
%     \item effect of queue size [1,16,256,2048,55000] (Online on CIFAR) + number neighbors [1,4,10,32,64] (Ongoing CIFAR) -> Make Figure (Done)
%     \item number of epochs and momentum (0.9, 0.96, 0.996, 0.999) (Done) (On CIFAR)
%     \item effect of the number of weak augmentations to ensemble (Ongoing CIFAR)
% \end{itemize}}

% \begin{table}[t]
%     \centering
%     \caption{Evaluating importance of our test time objective's components ($\mathcal{L}_\text{SLug}$) and Pseudo Label Refinement (PLR) on VisDA-C dataset with ResNet-101 network.}
    
%     \resizebox{0.7\linewidth}{!}{\begin{tabular}{@{}ccc|cc@{}}
    
%     \toprule
    
%     $\mathcal{L}_{pl}$ & PLR & $\mathcal{L}_{kd}$ & N-O & N-M\\
    
%     \midrule
    
%      - & - & - & \multicolumn{2}{c}{48.5} \\
    
%     \cmark & - & - & 73.6 & 72.3 \\
    
%     \cmark & \cmark & - & 77.5 & 81.6 \\
    
%     \cmark & \cmark & \cmark & 82.2 & 86.5 \\

%     \bottomrule
%     \end{tabular}}
%     \label{tab:ablation_global}
% \end{table}

% \begin{figure*}
%   \centering

%   \caption{Ablation Study.}
%   \label{fig:ablation}
% \end{figure*}
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=0.9\linewidth]{latex/figures/Ablation_plloss.pdf}
%     \caption{Ablation on $\mathcal{L}_{pl}$}
%     \end{subfigure}
    
%     \hfill
    
%     \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=0.9\linewidth]{latex/figures/Ablation_PLR.pdf}
%     \caption{Ablation on PLR}
%     \end{subfigure}
% \caption{Test}
% \label{fig:test}
% \end{figure*}
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/ablations.pdf}
%     \caption{Ablations Study on VisDA-C. \Devavrat{Maybe switch A and B or use a sub-figure for easy reference.}}
%     \label{fig:ablation_pl_PLR}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1.01\linewidth]{latex/figures/radar_plot.pdf}
%     \caption{Classification accuracy (\%) of several test time adaptation methods using different training strategies and network architectures on VisDA-C dataset.}
%     \label{fig:radar_plot_backbones}
% \end{figure}


% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \begin{table}[ht]
% \centering
% \caption{Adaptation classification accuracy (\%) on VisDA-C dataset using ResNet-50 in \textbf{N-O} protocol for \textbf{TTA} methods SHOT\cite{liang2020we} and AdaContrast\cite{chen2022contrastive} when using default augmentation and the proposed adversarial augmentation.}
% \begin{tabular}{@{}c|ccc@{}}
% \toprule
% Augmentation & SHOT          & AdaContrast   & TeSLA         \\ \midrule
% Default      & 70.1\std{0.2} & 75.5\std{0.1} & /             \\
% TeSLA        & 73.8\std{0.2} & 75.8\std{0.2} & 79.6\std{1.3} \\ \bottomrule
% \end{tabular}
% \label{tab:augeffect}
% \end{table}


% \begin{table}[ht]
%     \centering
%     \caption{Adaptation classification accuracy (\%) of several test-time augmentation modules including Vanilla Augmentations (VA) (random color jitter, flip, resize-crop), RandAugment (RA)\cite{cubuk2020randaugment}, AutoAugment (AA)\cite{cubuk2019autoaugment}, and SLug when used with different test-time adaptation methods including SHOT\cite{liang2020we}, AdaContrast\cite{chen2022contrastive} and SLug. We use ResNet-50 network with \textbf{N-O} protocol on VisDA-C dataset.}
    
%     \resizebox{0.48\textwidth}{!}{
%     \begin{tabular}{c|ccc|c}
%     \toprule
%   \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Test-Time\\ Method\end{tabular}}} &  \multicolumn{4}{c}{Test Time Augmentation Module} \\
   
%   \cmidrule{2-5}
   
%   & VA & RA & AA & SLug \\
   
%   \midrule
   
%   SHOT & 70.1\std{0.2} & 72.8\std{0.1} & 72.8\std{0.1} & 73.8\std{0.2}\\
   
%   AdaContrast & 75.5\std{0.1} & 75.3\std{0.2} & 75.6\std{0.3} & 75.8\std{0.2} \\
   
%   \midrule
   
%   SLug & 75.3\std{0.1} & 75.8\std{0.2} & 75.9\std{0.1} & 76.7\std{0.1}\\
% \bottomrule
% \end{tabular}}
% \label{tab:augeffect}
% \end{table}

