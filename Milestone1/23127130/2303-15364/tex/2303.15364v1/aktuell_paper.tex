\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage[table,xcdraw]{xcolor}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{amsmath}

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\interfootnotelinepenalty=10000

\begin{document}

% ToDo: Choose title
% 1.0: Improving Inflation Forecasts with Transformer-based Time Series Models
% 1.1: Study on Transformer Models for Inflation Forecasts
% 1.2: Inflation Forecasts using Attention based Transformer Neural Networks
\title{Inflation forecasting with attention based transformer neural networks}
% \title{Inflation forecasting with attention based transformer neural networks}
% keywords
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

%\author{BLIND\inst{1}\orcidID{0000-0001-0002-0003} \and
%BLIND\inst{2}\orcidID{0000-0001-0002-0003} \and
%BLIND\inst{1}\orcidID{0000-0001-0002-0003} \and
%BLIND\inst{1}\orcidID{0000-0001-0002-0003}}

\author{Maximilian Tschuchnig\inst{1}\orcidID{0000-0002-1441-4752} \and
Petra Tschuchnig\inst{2}\orcidID{0000-0001-5593-0043} \and
Cornelia Ferner\inst{1}\orcidID{0000-0003-1721-0453} \and
Michael Gadermayr\inst{1}\orcidID{0000-0003-1450-9222}}

\authorrunning{M. Tschuchnig et al.}
%\authorrunning{BLIND}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{BLIND, BLIND, BLIND \and 
%BLIND, BLIND, BLIND
%\email{BLIND.BLIND@BLIND.BLIND}}
\institute{Salzburg University of Applied Sciences, Puch bei Hallein, Austria \and 
University of Salzburg, Salzburg, Austria
\email{maximilian.tschuchnig@fh-salzburg.ac.at}}
%
\maketitle              % typeset the header of the contribution

\begin{abstract}
Inflation is a major determinant for allocation decisions and its forecast is a fundamental aim of governments and central banks. However, forecasting inflation is not a trivial task, as its prediction relies on low frequency, highly fluctuating data with unclear explanatory variables. While classical models show some possibility of predicting inflation, reliably beating the random walk benchmark remains difficult. Recently, (deep) neural networks have shown impressive results in a multitude of applications, increasingly setting the new state-of-the-art. This paper investigates the potential of the transformer deep neural network architecture to forecast different inflation rates. The results are compared to a study on classical time series and machine learning models. We show that our adapted transformer, on average, outperforms the baseline in $6$ out of $16$ experiments, showing best scores in two out of four investigated inflation rates. Our results demonstrate that a transformer based neural network can outperform classical regression and machine learning models in certain inflation rates and forecasting horizons.\blfootnote{This project is partially funded by the Science and Innovation Strategy Salzburg project ”IDA-Lab Salzburg”, grant number 20102-F1901166-KZP.}%\blfootnote{This project is partially funded by BLIND BLIND BLIND.}

\keywords{Inflation Forecasting \and Deep Neural Networks \and Transformer.}
\end{abstract}
%
%
%
\section{Introduction} 
\vspace{-0.15cm}
% ---
% Intro to topic (inflation)
% ---
Inflation is defined as the permanent increase in the price level of the economy, which leads to a decline in the value of money. Inflation can be explained as a monetary and non-monetary phenomenon. Monetary inflation determines the price level due to the money stock, velocity of money and real production, so that e.g. the expansion of money supply causes inflation by central banks. Non-monetary inflation can be divided into demand-pull inflation (demand increases faster than production can be adjusted) and cost-push inflation (costs increase and no substitutes are available). Therefore, inflation is a major determinant of allocation decisions and its forecast is a fundamental aim of governments and central banks. 

% ---
% Related work - Economics
% ---
Forecasting inflation is not a trivial task, as its prediction relies on low frequency, highly fluctuating data, and it is unclear which macroeconomic indicators are suitable as explanatory variables. Further, the relationship between inflation and potential explanatory variables, such as the unemployment rate, are not necessary stable over time~\cite{atkeson2001phillips,inoue2008useful,stock2010modeling,faust2013forecasting,manzan2013macroeconomic}. A common benchmark for inflation forecasting is the random walk model (RW)~\cite{malkiel1989efficient}, which assumes that the best inflation prediction for the next point in time is today's inflation without taking other macroeconomic variables into account. This naive model is a special form of the autoregressive model (AR), which in general predicts inflation based on a constant and $p$ lags of inflation. Multivariate autoregressive models such as the autoregressive distributed lag model (ARDL)~\cite{pesaran1995autoregressive} and the vector autoregressive regression models (VAR)~\cite{sims1972money} are based on the assumption that inflation is not only determined by the lags of inflation but also other macroeconomic indicators and include these as explanatory variables into the regression problem~\cite{ulke2018comparison}. 

So far, these parameter estimation-based time series (TS) models dominate inflation forecasting, and machine learnling (ML) models are only used to a limited extent. 
However, since ML models are used to forecast other macroeconomic indicators, several studies evaluate the potential of ML models for inflation forecasting~\cite{inoue2008useful,garcia2017real,ulke2018comparison,medeiros2021forecasting}. Ülke et al.~\cite{ulke2018comparison} compare the forecasting precision of classical TS models RW, AR, ARDL, and VAR with ML models k-nearest neighbours (k-NN), artificial neural networks (NN) and support vector machines (SVM). They conclude that the preferred model depends on the type of inflation measurement and the forecasting horizon, but that ML models tend to perform better than classical TS models for "more volatile and irregular series" with respect to root-mean-squared-error (RMSE).

Choudhary and Haider~\cite{choudhary2012neural} also show that NN-based methods are especially useful when modeling non-linear, noisy data, beating RW in short-term horizons. In regard to the temporal component, Nakamura~\cite{nakamura2005inflation} comes to the same result. The NN used by Moshiri and Cameron~\cite{moshiri2000neural} is able to forecast inflation at least as well as classical TS models. % and stresses the importance of fitting the model to the task. 
Most NN-based inflation studies restrict themselves to basic NN models and besides some promising results~\cite{choudhary2012neural,nakamura2005inflation,moshiri2000neural}, are often outperformed by non-NN models~\cite{ulke2018comparison,medeiros2021forecasting,sari2016backpropagation,thakur2016artificial}. To better differentiate NNs, we arbitrarily define basic NNs to have less than $10^6$ trainable parameters and not using modern deep NN techniques like parameter sharing, skip connections or attention architectures~\cite{he2016deep,vaswani2017attention,sagheer2019time}. %alexnet: 62mio. resnet: 11mio. Alex net cite: krizhevsky20122012
We argue that the potential of NNs for inflation forecasting should be reinvestigated with modern models, especially the attention based transformer neural networks, which perform extremely well on related task~\cite{li2019enhancing}.

% Current NN subtypes for sequence prediction use recurrent, convolutional and/or attention mechanisms to encode the intra-dependencies of sequences~\cite{vaswani2017attention}. % The attention mechanism~\cite{vaswani2017attention} shows for each element in a sequence, a weighted mapping, representing the importance to perform a task, to each element in another sequence (the same sequence in the case of self attention). 

Since transformers were originally designed for language translation, applying them to regress TS data is not obvious. Li et al.~\cite{li2019enhancing} provide some evidence, that it is possible by applying transformers to predict long-term, real valued TS. They experimentally show that transformers achieve state-of-the-art (sota) results compared to long-short-term-memory NNs, recurrent NNs as well as methods based on matrix factorization and classical statistical methods.

\textbf{Contribution}
This paper investigates the suitability of a modern deep NN, the transformer, to forecast inflation. A transformer model is adapted and the results are compared to a baseline, established by a reference paper~\cite{ulke2018comparison}, that investigates classical TS and ML based models for inflation forecasting. It is shown that the proposed method can reliably beat the baseline in $6$ out of $16$ scenarios (four measurements of inflation and four time horizons), while staying competitive in most other cases. To the best of our knowledge, we are the first that apply transformers to forecast inflation rates.


\vspace{-0.15cm}
\section{Method}
\vspace{-0.15cm}
% ---
% General Methodology
% ---
% The methodology used to investigate the usefulness of transformers for inflation prediction was to perform a study on parts of FRED-MD data~\cite{mccracken2016fred} and evaluate it based on results of the study performed by Ülke et al.~\cite{ulke2018comparison} using macro-indicators defined by Manzan and Zerom~\cite{manzan2013macroeconomic}. 
We investigate the potential of transformers to forecast US inflation by evaluating and comparing our results with those obtained from Ülke et al.~\cite{ulke2018comparison}, using classical TS (RW, AR, ARDL, VAR) and ML (k-NN, NN, SVM) models. Like Ülke et al.~\cite{ulke2018comparison}, we use four different measurements of inflation and six further macroeconomic indicators provided by the Federal Reserve Bank of Saint Louis monthly dataset (FRED-MD)~\cite{mccracken2016fred}. 
%The obtained data was transformed in the same way as stated in~\cite{ulke2018comparison}. This transformation was shown in Sec.~\ref{sec:Dataset}. 
Inflation rates are predicted monthly for up to 12 months. Forecasting precision is measured by RMSE and goodness of fit by $R^2$. 
%Following Ülke et al.~\cite{ulke2018comparison}, we report $R^2$ for the 12-month horizon and RMSE for the 3-, 6-, 9-, and 12-month horizon, defined as
Both are reported for the 3-, 6-, 9-, and 12-month horizon $H = \{3,6,9,12\}$, with observed $Y$ and forecasted inflation $\hat{Y}$, defined as
%The encoder part of a transformer is adapted for regression and applied to the transformed data to predict different timeframe horizons (hor3, hor6, hor9, hor12) defined as
% \vspace{-0.5cm}
$$
RMSEhor_{h \in H} = \frac{1}{h} \cdot \sum_{i=1}^h{RMSE(y_i, \hat{y_i})}, \quad R^2hor_{h \in H} = \frac{1}{h} \cdot \sum_{i=1}^h{R^2(y_i, \hat{y_i})}.
$$
% with $H = \{3,6,9,12\}$. % These horizons were evaluated using the root-mean-squared-error (RMSE) and $R^2$ correlation and compared to the results presented by Ülke et al.~\cite{ulke2018comparison}.
\subsection{Dataset}
\label{sec:Dataset}
% ---
% Dataset used
% ---
To achieve comparable results, we use the same dataset as Ülke et al.~\cite{ulke2018comparison} (1984:1 to 2014:12), based on Manzan and Zerom~\cite{manzan2013macroeconomic} and sourced from FRED-MD~\cite{mccracken2016fred}. Our data set includes four different measurements of inflation as target variables: Consumer Price Index (CPI), CPI excluding food and energy (Core-CPI), Personal Consumption Expenditure deflator (PCE), PCE excluding food and energy (Core-PCE) and six macroeconomic indicators as potential explanatory variables: index of industrial production (IP), civilian unemployment rate (UNEM), real personal consumption expenditure (INC), employees on non-farm payrolls (WORK), housing starts (HS), term spread (SPREAD). We use a different (as similar as possible) Core-PCE because we could not reproduce it due to a misreported FRED code and some of the reference times slightly changed\footnote{For more details, see Ülke et al.~\cite{ulke2018comparison}, which provide a definition for each variable. See corresponding FRED codes in parentheses: CPI (CPIAUCSL), Core-CPI (USACPICORMINMEI\_NBD20100601), PCE (PCE), Core-PCE (PCEPILFE\_NBD20100601), IP (INDPRO\_NBD20070601), UNEM (UNRATE), INC (DPCERL1M225NBEA), WORK (PAYEMS), HS (HOUST), SPREAD (GS5, TB3MS).}.
% ---
% Data preprocessing
% ---
Following Ülke et al.~\cite{ulke2018comparison} and Manzan and Zerom~\cite{manzan2013macroeconomic}, we define the one-month annualized inflation by $y_t=1200(log(p_t) - log(p_{t-1}))$, where $p_t$ is the price index level in month $t$ and use the gap form\footnote{Difference between observed values and their long-run trend~\cite{hodrick1997postwar,manzan2013macroeconomic,ulke2018comparison}.} of the nonstationary indicators WORK, UNEM, IP, and HS. Table \ref{descriptiveStat} reports the corresponding descriptive statistics.
\begin{table}[ht]
\setlength{\tabcolsep}{5pt}
\caption{Descriptive statistics in reference to Ülke et al.~\cite{ulke2018comparison}. }
\centering
%\resizebox{0.8\textwidth}{!}{%
\resizebox{!}{1.7cm}{
\begin{tabular}{clrrrrrrrr}
  \hline
&& \textbf{Mean} & \textbf{Median} & \textbf{Max.} & \textbf{Min.} & \textbf{s.d.} & \textbf{Skew.} & \textbf{Kurt.} & \textbf{Obs.} \\ 
  \hline
%  Inflation rates $y$ \\
\multirow{4}*{y} & CPI & 2.71 & 2.70 & 16.41 & -21.44 & 3.08 & -1.49 & 15.20 & 371 \\ 
& Core-CPI & 2.74 & 2.74 & 9.90 & -3.27 & 2.43 & 0.18 & 2.78 & 371 \\ 
& PCE & 5.20 & 4.99 & 34.75 & -24.92 & 6.10 & -0.12 & 8.26 & 371 \\ 
& Core-PCE & 2.32 & 2.17 & 8.42 & -6.82 & 1.50 & 0.13 & 7.33 & 371 \\ \hline
%  Explanatory variables $x$ \\
\multirow{6}*{x} & IP & 0.00 & 0.06 & 4.57 & -7.22 & 1.47 & -1.02 & 9.54 & 372 \\ 
& UNEM & 0.00 & -0.00 & 1.34 & -1.18 & 0.37 & 0.51 & 5.15 & 372 \\ 
& INC & 0.24 & 0.20 & 2.40 & -2.60 & 0.48 & -0.20 & 8.89 & 372 \\ 
& WORK & -0.00 & -65.06 & 2108.49 & -2206.26 & 721.48 & 0.06 & 4.43 & 372 \\ 
& HS & -0.00 & -0.63 & 449.17 & -341.21 & 97.79 & 0.32 & 5.02 & 372 \\ 
& SPREAD & 1.44 & 1.45 & 3.61 & -0.60 & 0.87 & -0.04 & 2.23 & 372 \\ 
   \hline
\end{tabular}}
\label{descriptiveStat}
\end{table}
\vspace{-0.5cm}

Our sample includes 372 observations. CPI has a mean of 2.71$\pm$3.08 and varies between -21.44 and 16.41. Mean and median of CPI, Core-CPI and Core-PCE lie between 2.17$\pm$1.50 and 2.74$\pm$2.43, while PCE has a mean of 5.20$\pm$6.10 and a median of 4.99. As expected, CPI and PCE have a wider range and a higher standard deviation, than Core-CPI and Core-PCE. 
While CPI (-1.49) and PCE (-0.12) are skewed left, Core-CPI (0.18) and Core-PCE (0.13) are skewed right. Kurtosis indicates for Core-CPI a light-tailed distribution (2.78) and for the other inflation rates a heavy-tailed distribution (CPI: 15.20, PCE: 8.26, Core-PCE: 7.33). That implies that the distribution of Core-CPI most closely follows a normal distribution, while that of CPI has the largest deviation. 

In addition to the six explanatory variables $x_1, \ldots, x_{6}$ described in Table~\ref{descriptiveStat}, we use the year and month of each data point as additional features, i.e. $x_{7}$ and $x_{8}$. These two features serve as positional encoding such that the attention-based model described in section~\ref{sec:Transformer} can establish a notion of the temporal sequence.
To get bias free train-test splits, data from 1984:1 to 2012:12 is separated into $N$ randomly sampled sequences($X_{i \in 0 < i < N}$) with a window size of $12$, resulting in the train-dataset
$$
X = (X_1, X_2, ..., X_N), \quad
X_{i \in N} =
\begin{bmatrix}
y_{2} & x_{1,1} & x_{1,2} & ... & x_{1,m} \\
y_{2} & x_{2,1} & x_{2,2} & ... & x_{2,m} \\
% y_{3} & x_{3,1} & x_{3,2} & ... & x_{3,m} \\
... & ... & ... & ... & ... \\
y_{12} & x_{12,1} & x_{12,2} & ... & x_{12,m} \\
\end{bmatrix}
$$
with $N = \{0, \dots, 1023\}$ and $m = 8$ as the amount of explanatory variables.
From this data, we use $80\%$ to train the model and $20\%$ to validate inter-training. Data from 2013:1 to 2014:12 is used to test the model.


\subsection{Transformer encoder based regressions model}
\label{sec:Transformer}
% ---
% Regressor
% ---
% For the proposed transformer regression model (TF), the transformer architecture is used, due to its recent, impressive results in a variety of natural language tasks and especially sequence modeling~\cite{li2019enhancing}.
The transformer~\cite{vaswani2017attention} is a deep sequence-to-sequence model that encodes an input sequence to a latent space and decodes it to produce the target sequence. The  main novelty of this architecture is the self-attention mechanism, allowing the model's encoder to read sequences of arbitrary length at once, without recurrence, and putting individual weights at each time step. 
%The proposed transformer regression model (TF)\footnote{On successful publication, the model and experiment will be uploaded to github and made available as an url here.} adapts this transformer model for regression, and is shown in Fig.~\ref{fig:model}. 
We adapt this transformer model for regression\footnote{This adaptation is based on \url{https://keras.io/examples/timeseries/timeseries_transformer_classification/}. On successful publication, the model and experiment will be uploaded to github and made available as an url here.}. Our proposed transformer regression model (TF) is shown in Fig.~\ref{fig:model}.
It consists of a transformer's encoder only, and maps the latent representations to a single-valued output through a NN structure. %To some extent, this encoder-only structure is similar to the BERT model~\cite{devlin2019bert} that is also known from natural language processing tasks.

TF consists of $4$ identical encoder blocks including layernorm, multi-head attention, residual connections and 1d convolutions (Conv 1d).
%The chosen structure consist of input embedding (see Sec.~\ref{sec:Dataset}), $N\times$ Transformer Encoder blocks, encoding the sequential monthly information into a feature space using sota methods such as Multi-Head Attention, residual connections and convolutions. 
Pooling %\footnote{For natural language processing tasks, average pooling of the transformer encoder's output has been shown to be superior to other pooling techniques when the pooled output is used as feature vector in downstream networks~\cite{reimers2019sentence}.}
is applied to reduce the multidimensional tensor back to a 1d feature vector followed by two dense NN layers with dropout. In the last layer, consisting of only one neuron, the value is regressed using a linear activation function. The network is trained using the Adam optimizer with a learning rate of $10^{-4}$, minimizing the mean-squared-error and a batch size of $4$. To show the stability of results, each network is trained and evaluated $10$ times.
\vspace{-0.35cm}
\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{figs/Model.pdf}
\caption{Proposed TF model. It consists of a transformer-encoder, encoding the sequential data into representations in a latent space, and a regression network, facilitated by a deep NN with a final neuron using linear activation to establish regression.}
\label{fig:model}
\end{figure}
\vspace{-0.5cm}

In detail, the Multi-head attention headsize is set to $32$ with $4$ heads. Layernorm uses an $\epsilon=10^{-6}$. All Conv 1d and dense layers (except for the last) use the Relu activation function. Dropout during training is set to $0.1$. The filters of the Conv 1d blocks are set to $\{4, 12\}$ respectively with $1 \times 1$ receptive fields, facilitating transformer projection layers. The number of neurons in the dense layers are $\{32, 16, 12, 1\}$ in the corresponding layers along the stream direction.


\vspace{-0.15cm}
\section{Results}
\vspace{-0.15cm}
% ---
% Results RMSD
% ---
% \vspace{-0.15cm}
Table~\ref{tab:res} presents the best forecasting RMSE per model for four measurements of inflation and four horizons of the year 2014. Results for the first seven models (AR, RW, ARDL, VAR, NN, k-NN, SVM) from Ülke et al.~\cite{ulke2018comparison} are compared to TF\textsubscript{b} and TF\textsubscript{m}, with TF\textsubscript{b} showing the best and TF\textsubscript{m} the mean (10 experiment runs) RMSE.
The best results from Ülke et al.~\cite{ulke2018comparison} are presented in bold. If TF beats the best reference scores, the corresponding scores are highlighted.

%\vspace{-0.2cm}
\begin{table}[h] % t oder b würde evtl. Platz sparen
%\vspace{-0.3cm}
\setlength{\tabcolsep}{3.5pt}
\caption{Forecasting RMSE per model for 2014 in reference to Ülke et al.~\cite{ulke2018comparison} %for four measurements of inflation and four horizons. AR to SVM are the reported values from the baseline paper~\cite{ulke2018comparison}. TF are the results from our transformer network. Results improving on the baseline are highlighted in green.
}
\centering
%\rowcolors{2}{gray!25}{white}
%\resizebox{0.8\textwidth}{!}{%
\resizebox{!}{3cm}{
\begin{tabular}{cl|lllllllll}
\hline
%\rowcolor{white!100}
\multirow{2}{4.5em}{\textbf{Horizon}}        & \multirow{2}{5em}{\textbf{Inflation}}  & \multicolumn{8}{c}{\textbf{Forecasting RMSE (2014)}}  \\
%\rowcolor{white!100} %\cline{3-10} 
& & \textbf{AR}                                     & \textbf{RW}                                & \textbf{ARDL}                                   & \textbf{VAR}                           & \textbf{NN}                          & \textbf{k-NN}                                   & \textbf{SVM}                                    & \textbf{TF\textsubscript{b}} & \textbf{TF\textsubscript{m}}                                                              \\ \hline
\multirow{4}*{3} & CPI & 1.33  & 2.30 & 1.10 & 12.48 & 1.01 & \textbf{0.38} & 0.97 & 0.64 & 1.33 \\
           & Core-CPI & \textbf{0.84} & 0.94 & 0.98 & 12.65 & 1.29 & 1.82 & 1.42 & 0.99 & 1.84 \\
           & PCE & 5.39 & 6.12 & \textbf{0.81} & 14.96 & 1.47 & 3.89 & 1.03 & 1.52 & 4.41 \\
           & Core-PCE & 6.93 & 7.26 & 2.10 & 15.89 & 2.35 & 5.05 & \textbf{1.32} & \cellcolor[HTML]{4472C4}{\color[HTML]{FFFFFF} \textbf{0.44}} & \cellcolor[HTML]{70AD47}{\color[HTML]{FFFFFF} \textbf{0.70}} \\ \hline
\multirow{4}*{6} & CPI & 1.06 & 2.49 & 1.29 & 11.25 & 0.99 & 1.19 & \textbf{0.89} & \cellcolor[HTML]{4472C4}\color[HTML]{FFFFFF}{\textbf{0.73}} & 1.29 \\
           & Core-CPI & 1.18 & 1.28 & \textbf{1.08} & 10.05 & 1.53 & 2.28 & 1.29 & \cellcolor[HTML]{4472C4}{\color[HTML]{FFFFFF} \textbf{0.98}} & 1.55 \\
           & PCE & 4.11 & 4.71 & \textbf{0.62} & 13.36 & 1.05 & 2.77 & 0.77 & 2.34 & 3.40 \\
           & Core-PCE & 4.94 & 5.52 & 2.13 & 13.79 & 1.89 & 3.67 & \textbf{1.77} & \cellcolor[HTML]{4472C4}{\color[HTML]{FFFFFF} \textbf{0.42}} & \cellcolor[HTML]{70AD47}{\color[HTML]{FFFFFF} \textbf{0.62}} \\ \hline
\multirow{4}*{9} & CPI & 1.69 & 2.13 & 1.58 & 10.05 & 1.67 & \textbf{1.38} & 1.61 & \cellcolor[HTML]{4472C4}\color[HTML]{FFFFFF}\textbf{0.79} & \cellcolor[HTML]{70AD47}{\color[HTML]{FFFFFF} \textbf{1.19}} \\
           & Core-CPI & 1.20 & 1.18 & \textbf{1.08} & 8.68 & 1.90 & 2.00 & 1.49 & 1.22 & 1.51\\
           & PCE & 3.67 & 4.36 & \textbf{1.2} & 12.53 & 1.72 & 2.52 & 1.41 & 2.53 & 3.33 \\
           & Core-PCE & 4.25 & 5.16 & 2.27 & 13.00 & 1.96 & 3.31 & \textbf{1.73} & \cellcolor[HTML]{4472C4}{\color[HTML]{FFFFFF} \textbf{0.50}} & \cellcolor[HTML]{70AD47}{\color[HTML]{FFFFFF} \textbf{0.67}} \\ \hline
\multirow{4}*{12} & CPI & 3.10 & 2.62 & \textbf{2.56} & 8.89  & 2.66 & 2.72 & 3.11 & \cellcolor[HTML]{4472C4}{\color[HTML]{FFFFFF} \textbf{1.47}} & \cellcolor[HTML]{70AD47}{\color[HTML]{FFFFFF} \textbf{1.97}} \\
           & Core-CPI & 1.25 & \textbf{1.18} & 1.20 & 7.70 & 2.60 & 2.26 & 2.19 & 1.68 & 1.91 \\
           & PCE & 3.82 & 3.98 & \textbf{1.85} & 11.56 & 2.64 & 2.77 & 2.29 & 2.58 & 3.25 \\
           & Core-PCE & 3.77 & 4.77 & 2.29 & 12.28 & 1.92 & 3.01 & \textbf{1.66} & \cellcolor[HTML]{4472C4}{\color[HTML]{FFFFFF} \textbf{0.46}} & \cellcolor[HTML]{70AD47}{\color[HTML]{FFFFFF} \textbf{0.62}} \\
           \hline
\end{tabular}}
\label{tab:res}
\end{table}
\vspace{-0.5cm}

% The highest precision is reached by TF with a RMSE of 0.44 for Core-PCE in horizon 3, while the highest RMSE (15.89) results from VAR for Core-PCE in horizon 3. 
In half of the cases (8), TF\textsubscript{b} shows forecasts with the smallest RMSE, outperforming all baseline~\cite{ulke2018comparison} models. It is followed by ARDL with the lowest RMSE in five cases and AR, RW and k-NN with one best RMSE each. 
CPI and Core-PCE are best forecasted by TF, PCE by ARDL, while the best model for Core-CPI varies per horizon.  
Horizon 3 is best predicted by four different models (AR, ARDL, k-NN, TF), horizon 6 by TF\textsubscript{b} (three out of four), horizon 9 equally by ARDL and TF (two each) and horizon 12 again by TF (two out of four).

% ---
% Results correlation
% ---


%To further investigate the results, the $R^2$ regression is analyzed. The best baseline $R^2$ scores (horizon 12) are $R^2_{hor12}=\{0.91, 0.23, 0.92, 0.78\}$, compared to our scores $R^2_{hor12}=\{0.55, 0.42, 0.64, 0.43\}$ with the scores corresponding to the inflation measurements \{CPI, Core-CPI, PCE, Core-PCE\}.
The model fit of TF\textsubscript{b} is almost perfect for all inflation measurements in horizon three ($R^2_{hor3} \sim 1$) and decreases with the number of months to forecast: $R^2_{hor6}=[0.85, 0.92]$, $R^2_{hor9}=[0.54, 0.80]$, $R^2_{hor12}=[0.42, 0.64]$. 
In comparison the best baseline~\cite{ulke2018comparison} $R^2_{hor12}$ scores for CPI, Core-CPI, PCE, and Core-PCE are 0.23 (ARDL), 0.78 (RW), 0.91 (ARDL), and 0.92 (k-NN, SVM). 
%Fig.~\ref{fig:regressions} shows hand-picked regression forecasts, with the red graphs showing the observed inflation, while the blue-dashed graphs show the auto-regressed forecast of our models. The x-axis shows the months of the predicted year of 2014 and the y-axis the change of the current measurement of inflation.
To visualise the forecasting fit, Fig.~\ref{fig:regressions} shows monthly TF\textsubscript{b} forecasts (blue) in comparison to the observed inflation rates (red) for each inflation measurement in 2014. 

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figs/predictionsDirtyFix.pdf}
\caption{TF forecasts (blue) and observed change in inflation (red) per month (2014). %Picked results showing the resulting regressions. The red plot shows the real inflation, while the blue-dashed plot shows the auto-regressed forecast of our models.
}
\vspace{-0.5cm}
\label{fig:regressions}
\end{figure}

\vspace{-0.15cm}
\section{Discussion}
\vspace{-0.15cm}
% ---
% Discussion Results
% ---
\vspace{-0.15cm}
RMSE results show that TF is well suited to forecast inflation rates, as in terms of absolute cases no other model examined is better than TF\textsubscript{b} ($8$ out of $16$) or able to outperform TF\textsubscript{m} (TF\textsubscript{m} and ARDL: $6$ out of $16$). TF provides the best forecasts for two (CPI and Core-PCE) of four inflation measurements. ARDL is the only other model with systematically good forecasts, especially for PCE. Since ARDL and TF provide complementary results, a further investigation into why this is the case could improve both models.

Another point of discussion is cherry picking of ML results. It is theoretically possible that a ML model learns a perfect fit on random, if enough test runs are performed. For this reason it makes sense to give the most weight to ML results reported in mean values over multiple experimental runs with the same (initial) parametrization, not best scores. Since the presented baseline results of NN, k-NN and SVM are the best scores of unreported parameters and possibly multiple training runs~\cite{ulke2018comparison}, these scores as well as the TF\textsubscript{b} scores should be analyzed with caution. Additional analysis of TF\textsubscript{m} however shows, that the TF results are very stable.

Based on the success of CPI forecasting, which has the largest deviation from a normal distribution, we can support the potential of ML models for irregular series~\cite{ulke2018comparison,choudhary2012neural}. 
Regarding the $R^2$, as with Haider~\cite{choudhary2012neural} and Nakamura~\cite{nakamura2005inflation}, we find that TF fits particularly well for short-term horizons but this fit decreases in further horizons and is not supported in terms of RMSE. 
Based on Fig.~\ref{fig:regressions}, we assume that the divergence between RMSE and $R^2$ results from TF underfitting monthly outliers in more volatile time periods, leading to a greater focus on the general trend, improving RMSE but leading to lower $R^2$. 

%\section{Conclusion}
%\textbf{Conclusion}
To conclude, this paper investigates the potential of a sota deep NN, the TF, for inflation forecasting. We adapt a transformer for sequential data and apply it to FRED-MD, forecasting four measurements of monthly US inflation up to $12$ months. The results are compared to reference TS and ML model forecasts from Ülke et al.~\cite{ulke2018comparison}. 
The proposed TF model is able to improve forecasting results in overal cases (TF\textsubscript{b}: $8$ out of $16$, TF\textsubscript{m}: $6$ out of $16$) and for $2$ (CPI, Core-PCE) out of $4$ inflation measurements, showing high potential for further investigations. Major future research directions would be the extension of uncertainty measurements and data augmentation, e.g. simulating or removing the business cycle or synthetically adding and removing financial crises.
% Impact to macro

% \newpage
\bibliographystyle{splncs04} 
\bibliography{mybibliography.bib}
\end{document}
