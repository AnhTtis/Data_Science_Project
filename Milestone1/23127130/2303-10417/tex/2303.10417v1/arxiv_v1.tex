\documentclass[letterpaper,10pt]{IEEEtran}
\usepackage{cite}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{enumerate}

\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage{xcolor}
\usepackage{hyperref} 
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


%\pagestyle{empty}

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
%\overrideIEEEmargins

\title{\huge On the Benefit of Nonlinear Control for Robust
Logarithmic\\ Growth: Coin Flipping Games as a Demonstration Case  }

\author{\Large Anton V. Proskurnikov and B. Ross Barmish
%\thanks{The work of the first author was in part supported by...}%
\thanks{Anton V. Proskurnikov is associate professor with Department of Electronics and Telecommunications at Politecnico di Torino, Turin, Italy and B. Ross Barmish is an emeritus professor in ECE at University of Wisconsin, Madison, and CEO of Robust Trading Solutions, LLC.}
\thanks{Emails: \texttt{anton.p.1982@ieee.org, bob.barmish@gmail.com}}
}


\def\be{\begin{equation}}
\def\ee{\end{equation}}
\def\ben{\begin{equation*}}
\def\een{\end{equation*}}

\newtheorem{corollary}{Corollary}%[section]
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}{Lemma}%[section]
%\newtheorem{algorithm}{Algorithm}[section]
\newtheorem{assumption}{Assumption}
%\newtheorem{statement}{Statement}
%\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}%[section]
\newtheorem{proposition}{Proposition}%[section]

\def\boldP{\mathbb{P}}
\def\boldE{\mathbb{E}}
\def\boldI{\mathbb{I}}

\def\r{\mathbb{R}}
\def\ve{\varepsilon}

\newcommand\ap[1]{\color{blue}#1 \color{black}} %Anton's text
\newcommand\bb[1]{\color{red}#1 \color{black}} %Bob's
\newcommand\td[1]{\color{magenta}TODO: #1 \color{black}} %TODO

\newcommand\AS{\emph{a.s.} } %almost sure

\begin{document}

\vskip -1in
\maketitle

\begin{abstract}
The takeoff point for this paper is the voluminous body of literature addressing recursive betting games with  expected logarithmic growth of wealth being the performance criterion. Whereas almost all existing papers involve use of linear feedback, the use of nonlinear control is conspicuously absent. This is epitomized by the large subset of this literature dealing with Kelly Betting. With this as the high-level motivation, we study the potential for use of nonlinear control in this framework. To this end, we consider a ``demonstration case'' which is one of the simplest scenarios encountered in this line of research: repeated flips of a biased coin with probability of heads~$p$ and even-money payoff on each flip. First, we formulate a new robust nonlinear control problem which we believe is both simple to understand and apropos for dealing with concerns about distributional robustness; i.e., instead of assuming that~$p$ is perfectly known as in the case of the classical Kelly formulation, we begin with a bounding set ~${\cal P} \subseteq [0,1]$ for this probability. Then, we provide a theorem, our main result, which gives a closed-form description of the optimal robust nonlinear controller and a corollary which establishes that it robustly outperforms linear controllers such as those found in the literature. A second contribution of this paper bears upon the computability of our solution. For an $n$-flip game, whereas an admissible controller has~$2^n-1$
parameters, at the optimum only~$O(n^2)$ of them turn out to be distinct.
Finally, we provide some illustrations comparing robust performance with what is possible when working with the so-called {\it perfect-information Kelly optimum}.
%%%% We need to add index terms
\end{abstract}
%\begin{IEEEkeywords}
%Robust Control, Finance, Markov Processes
%\end{IEEEkeywords}



\section{Introduction}
This paper addresses a large class of betting games described by discrete-time Markov processes. In this setting, the bettor begins with initial account value~$V_0 > 0$ and at each stage~$k$, the controller~$u_k$, alternatively called the {\it betting strategy}, determines the size of the~$k$-th wager. Then, over~$n$ steps, these bets, typically assuming independent and identically distributed random variables~$X_k$ as the {\it returns} with a known probability distribution, the stochastic gambling dynamics determine the resulting account value trajectory~$V_1,V_2,...,V_n$, emanating from initial condition~$V_0 > 0$. That is, $V_k$ are obtained recursively by
$$
V_{k+1} = V_k + u_kX_k
$$
Within this context, the literature most closely related to this paper concentrates  
on the design of a causal controller~$u$ maximizing the resulting  {\it Expected Logarithmic Growth}~(ELG) given by
$$
ELG_u = \frac{1}{n} \boldE\log\left(\frac{V_n}{V_0}\right)
$$
subject to satisfaction of the {\it budget constraint}~$u_k \leq V_k$ for~$k = 0,1,...,n-1$.

Perhaps, the most celebrated work along the lines above is the seminal paper by Kelly~\cite{Kelly_1956}; see also the early recognition of the power of the ELG approach  in \cite{Breiman_1961}, \cite{Thorp_1961}, \cite{Thorp_1969} and~\cite{Cover_1984}. Over the decades  to follow, we see a voluminous body of literature, comprised of hundreds of papers, dealing with extensions, generalizations, applications of Kelly's result in various directions, and, we also see many papers providing rationale for the use of the logarithmic growth criterion versus other performance metrics. A selection of highlights from this work includes the detailed coverage of these topics in textbooks such as~\cite{Cover_Thomas_1991} and~\cite{Luenberger_1998} and the extensive collection of papers in~\cite{Maclean_et_al_2011}. It is also important to point out that this body of literature being cited includes major results on various properties of the ELG-maximizing controller over and above optimal logarithmic growth. That is, many authors provide results bearing on the ``asymptotic superiority'' of the ELG maximizer and other topics such as the relaxation the~i.i.d. assumption on the~$X_k$ are covered. In this regard, some good starting points for the uninitiated reader are~\cite{Hakansson_1971},\cite{Algoet_and_Cover_1988} and the more recent paper~\cite{Obrien_et_al_2021}. Finally, we draw attention to the 2019 doctoral dissertation of Hsieh~\cite{Hsieh_2019} which not only includes a comprehensive review of the earlier literature but also details and citations of his contributions and those of others over the preceding years.

To complete this brief perspective of the literature related to this paper, it is also important to mention the body of work dealing with ``distributional robustness'' issues arising in stochastic optimization. This terminology, introduced around the new millennium in \cite{Barmish_and_Shcherbakov_1999} and~\cite{Lagoa_and_Barmish_2002} and carried forward  in papers such as  \cite{Delage_and_Ye_2010}, is central to the important 2016 paper~\cite{Rujeerapaiboon_et_al_2016}, dealing with ELG considerations ---  with distributional robustness considerations arising due to factors such as  ambiguity in the specification of the probabilistic description of the returns~$X_k$ above, poor out-of-sample performance, shortness of time horizons and reduction of the probability of ruin; see also~\cite{Li_2023} for ongoing work along these lines.

Given the research setting above, the primary motivation for this paper is the fact that in the existing ELG literature, a problem formulation as one of nonlinear control is conspicuously absent; i.e., only linear control is considered. Whereas it is arguable, based on some of the results in existing work, that a nonlinear control cannot outperform a linear feedback when the probability distribution for the~$X_k$ is perfectly known, our main contention in this paper is that the same does not hold true when uncertainty in the underlying probability distributions is in play. Said another way, our main results provide compelling evidence that there are a large number of scenarios, involving distributional robustness considerations for which a nonlinear controller can outperform the ``best'' linear controller; e.g., see~\cite{Rujeerapaiboon_et_al_2016} and~\cite{Sun_and_Boyd_2018}. To this end, our analysis to follow demonstrates the potential of robust nonlinear control by considering one of the simplest possible ELG scenarios: a coin-flipping game with uncertainty in the probability of heads~$p$. Instead of taking~$p$ to be perfectly known as in the case of the classical Kelly formulation, we begin with  a bounding set~${\cal P} \subseteq [0,1]$ for this  probability. In this setting, the main result in this paper is a theorem which provides a complete closed-form solution of an optimal robust nonlinear control problem.  Furthermore, to support the claim in the title that there is a ``benefit'' associated with nonlinear control, as a corollary of the theorem, we prove that our nonlinear controller robustly outperforms any linear controller.

A second contribution of this paper bears on the computability of our new solution. Whereas an admissible controller has~$2^n-1$ design parameters associated with the sample path points for an $n$-flip game, surprisingly, at the optimum, many of them turn out to be the same with the resulting number of ``free parameters'' being of~$O(n^2)$. Finally, we provide some initial illustrations bearing on the ``cost of imprecision'' in our knowledge of the probability~$p$. To this end, some comparisons are made between the expected logarithmic growth associated with the optimal robust nonlinear controller and the so-called {\it perfect-information Kelly optimum}. Perhaps the main implication of our results is that future study of nonlinear control with more general problem formulations, such as those found in the field of finance, is likely to bear fruit.

\subsection*{Our Demonstration Case: Coin-Flipping}

To demonstrate the potential for the use of nonlinear control, we analyze
one of the simplest and most fundamental problems in the logarithmic growth literature: Making bets on~$n$ consecutive flips of a biased coin with probability of heads being~$p$ and even-money payoff. This simplified framework enables us to explain the key ideas behind our new nonlinear control formulation without being encumbered by all the technical details associated with a more general framework.

Indeed, we begin with the widely celebrated betting scheme of Kelly~\cite{Kelly_1956}. That is, starting with account value~$V_0 >0$, at each stage~$k$, the controller generates the bet size as a linear feedback. That is,~$u_k= KV_k$ with  the understanding that~$u_k > 0$ and $u_k < 0$  corresponds to bets on heads and tails respectively. Consistent with this, with~$X_k = 1$ being the outcome heads and
$X_k = -1$ being tails, the account value dynamics are described by
\begin{equation*}
V_{k+1} =  (1 + KX_k)V_k.
\end{equation*}
% is represented as a basic linear feedback system in the figure below.
%\vskip -.0.5in
%\begin{figure}[htb]
%\centerline{\includegraphics[width=2.5in]{block_diagram.eps}}
%%\vspace*{-1.5in}
%\caption{\bf  Linear Feedback Realization of Coin-Flipping Strategy}
%\end{figure}

Then, with probability of heads~$p$ assumed to be perfectly known and budget constraint $|u(k)| \leq V_k$ imposed, a straightforward calculation leads to  Kelly's optimal ELG maximizing feedback gain~$K^* = 2p-1$. Finally, it is important to note that when the probability~$p$ is perfectly known, this simple linear controller can be proven to be optimal for many cases other than the simple scenario described above; e.g., in the widely cited 1971 paper by Hakansson~\cite{Hakansson_1971}, the control (bet size)~$u_k$ may depend on the entirety of the past history~$V_0,V_1,...,V_{k-1},V_k$. However, as indicated earlier, our goal in this paper is to demonstrate the importance on nonlinear control when~$p$ is imperfectly known with robustness being a concern.

%\hskip -.5in
\section{Admissible Nonlinear Controllers and\\
 Resulting Expected Logarithmic Growth}

For the coin-flipping game at hand with sample path space
$$
{\cal X} \doteq \{-1,1\}^n,
$$
a mapping~$K: {\cal X} \rightarrow \mathbb{R}^n$ is said to define an {\it admissible nonlinear controller} if the following conditions are satisfied: First, this mapping is {\it causal}; that is, given any~ sample path~$X = (X_0,X_1,...,X_{n-1}) \in {\cal X}$, for~$k = 0,1,...,n-1$, the~$k$-th component of~$K(X)$, denoted by~$K_k(X)$, depends only on~$X_0,X_1,...,X_{k-1}$ with resulting control, being the bet size at stage~$k$, given by
$$
u_k(K,X) = K_{_{k}}(X)V_k(K,X)
$$
Second, this controller should satisfy the {\it budget constraint}
$$
|u_k(K,X)| \leq V_k(K,X).
$$
Equivalently, $|K_{_{k}}(X)|\leq 1$. In the sequel, We use notation~${\cal K}$ to denote the set of all admissible controllers.

It is important to note that~${\cal K}$ includes linear controllers as  special case
but in general, it can be highly nonlinear.  Accordingly, whenever appropriate,~$K_{_{k}}(X)$ we refer to it as a {\it nonlinear feedback gain}.  Now, along sample path~$X \in {\cal X}$, the resulting account value is obtained recursively as
\begin{eqnarray*}
V_{k+1}(K,X) = (1 +  K_{_{k}}(X)X_k)V_k(K,X).
%V_{k+1}(K,X) & = & V_k(K,X) + u_k(K,X)V_k(K,X)\\
%& = &  (1 +  K_{_{k}}(X)X_k)V_k(K,X).
\end{eqnarray*}
In Figure 1, the binary tree associated with the state transitions above are shown. We also draw attention to the color scheme used for the nodes: At any given stage~$k$, two nodes with the same color represent sample pathes with the same number of heads over the prior stages~$0,1,2,...,k-1$. Moreover as seen in the main result to follow, at such nodes, the \emph{optimal} robust nonlinear control, has the same nonlinear gain\footnote{Hence, as mentioned in the introduction the computation burden associated with finding this control is quadratic rather than exponential.}~$K_k(X)$.
Now, continuing with the analysis, the final account value is
$$
V_n(K,X) = \left(\prod_{k = 0}^{n-1}(1 +  K_{_{k}}(X)X_k)\right)V_0,
$$
and, therefore, the final logarithmic growth is given by
$$
\frac{1}{n}\log\left(\frac{V_n(K,X)}{V_0}\right)
       = \frac{1}{n} \sum_{k = 0}^{n-1}\log\left((1 +  K_{_{k}}(X)X_k\right).
$$
\begin{figure}[htb]
\centering
\includegraphics[width=1.8in]{picture-tree1.pdf}
%\vspace*{-1.5in}
\caption{\bf Causal Controllers and Random Walks on a Binary Tree.
%Nodes of same color correspond to equal values of the \emph{optimal} controller $K^*$.
}
\label{fig.tree}
\end{figure}

We now turn our attention to the starting point for much of the analysis to follow: the simple formula for the Expected Logarithmic Growth (ELG) as a function of the controller nonlinear gains~$K \in {\cal K}$ and the probability of heads~$p$. Indeed, we begin with the fact that  probability of a sample path $X \in {\cal X}$ is
$$
P(X) \doteq p^{n_h(X)}(1-p)^{n-n_h(X)}
$$
where~$n_h(X)$ above denotes the number of heads; i.e.,
$$
n_h(X)\doteq\#\{i=0,\ldots,n-1:X_i=1\}.
$$
Therefore, the ELG as a function of $K$ and $p$ is found as
\begin{eqnarray*}
ELG_K(p)&\doteq&\frac{1}{n}\sum_{X \in {\cal X}} P(X)\log \frac{V_n(K,X)}{V_0}\\
& = & \frac{1}{n}\sum_{X \in {\cal X}}P(X)\sum_{k=0}^{n-1}\log(1 +  K_k(X)X_k).
\end{eqnarray*}
%\]

When $p=0$ or $p=1$, there is a possibility that $P(X)=0$ and $V_n(K,X) = 0$ for some sample paths $X\in\mathcal{X}$.
For such cases, to insure the integrity of our mathematical analysis, we use the transfinite arithmetic convention $0\cdot(-\infty) = 0$ when calculating the sum.
This takes cae of the possibility that the combination of a feedback~$K\in {\cal K}$ and a sample path~$X \in {\cal X}$ leads to $ELG_K(p)=-\infty$.


%\ap{
%\begin{remark}\label{rem.budget}
%Using induction on $k=0,\ldots,n-1$, it can be shown that any controller $\theta\in\Theta$ ensures the inequalities
%\[
%0\leq V_{k+1}(\theta,X)\leq 2^{k+1},
%\]
%which inequalities are strict if all inequalities~\eqref{eq:budget} are strict.
%
%If the equality is met in~\eqref{eq:budget} for some $k$ and some sequence $X_0,\ldots,X_{k-1}$, then either for $X_k=1$ or for $X_k=-1$ one has $V_{k+1}(\theta,X)=0$. For such a trajectory $X$, one has .
%Hence, if some of inequalities~\eqref{eq:budget} turns into equality, then $ELG_\theta(p) = -\infty$ for all $0<p<1$.
%\end{remark}
%}

\section{Robustness Formulation}
Per earlier discussion, we now formulate a Robust Expected Logarithmic Growth problem involving uncertainty in the probability of heads. To this end, let~${\cal P} \subseteq [0,1]$ denote a Lebesgue measurable set of the possible values for $p$ against which we seek robustness and let~$\mu({\cal P})$ be its corresponding Lebesgue measure. For example, if the only a priori information we have about the probability of heads are bounds
$$
0\leq p_{min} \leq p \leq p_{max} \leq 1,
$$
then with~${\cal P} = [p_{min},p_{max}]$, we have~$\mu({\cal P})= p_{max}-p_{min}$. In the sequel, to avoid trivialities, we assume~$\mu({\cal P})> 0$. Unlike Kelly's perfect-information scenario, we cannot take~$K \in {\cal K}$ depending on the unknown probability of heads~$p$; i.e., it becomes a function of the known bound~${\cal P}$ and   
we can  emphasize the concern for robustness, whenever convenient, by writing~$K = K({\cal P})$.


\subsection*{Comparison With Kelly's Perfect-Information Optimum}
To assess the robustness of any particular controller, we compare it's expected logarithmic growth, as a function of $p\in{\cal P}$, with that of Kelly's {\it perfect-information ELG optimum}~$K_p^*= 2p-1$ described in the Introduction.
In this regard, we view~$K_p^*$ as a member the admissible set~${\cal K}$ and a straightforward calculation leads to optimal performance level
$$
ELG^*(p) = p\log(2p) + (1-p)\log(2(1-p)).
$$
This quantity, serves as our ``gold standard'' against which we assess the robust performance of controllers with {\it imperfect information}. That is, given
any~$K\in {\cal K}$, we first observe that the inequality
$
ELG^*(p) \geq ELG_K(p)
$
must hold for all $p\in {\cal P}$. Hence, the associated error integral
$$
Err(K) \doteq \int_{p \in {\cal P }}\left(ELG^*(p) - ELG_K(p)\right)dp
$$
is minimized by finding the supremum
$$
ELG^*_{\cal K} \doteq  \sup_{K \in {\cal K}} \int_{p \in {\cal P }} ELG_K(p)dp.
$$
One way to interpret the optimization over~$K \in {\cal K}$ above 
is by viewing~$p$ as a random variable uniformly distributed\footnote{The uniform distribution here is being used solely for the sake of simplicity; the results to follow are easily modified to address a more general distributional robustness problem  with~$p$ being drawn from a general probability distribution on $\mathcal{P}$.
} over~$\mathcal{P}$. Then one seeks an admissible nonlinear controller which maximizes the expected value 
\[
\mathbb{E}_p(ELG_K(p))=\frac{1}{\mu(\mathcal{P})}\int_{p \in {\cal P }} ELG_K(p)dp.
\]

In the theorem to follow, we prove that a maximizing element~$K^* \in {\cal K}$ attaining the supremum above exists and is unique, and, we provide a simple and efficient formula to compute it.
%
\section{The Subclass of Static Linear Controllers}

By way of preliminaries, we say that an admissible~$K \in{\cal K}$ defines an
{\it  admissible static linear controller} if there exists a constant~$K_0\in[-1,1]$ such that
$K(X) \equiv K_0$ for all~$X \in {\cal X}$.
%In the sequel, we use the notation~${\cal K}_0$ to represent this subclass of~${\cal K}$.
In order to study the robust performance over this subclass of controllers, we work with the function
\[
\begin{split}
f(K_0)&\doteq \int_{p\in\mathcal{P}}ELG_{{K_0}}(p)dp\\
&=\int_{p\in\mathcal{P}}(p\log(1+K_0)+(1-p)\log(1-K_0))\,dp.
\end{split}
\]
\noindent{\bf Lemma:} {\it The admissible static feedback controller 
 maximizing~$f(K_0)$ with respect to all~$K_0\in[-1,1]$ is unique and given by
$$
K_0^* =2\bar p-1,\quad \bar p\doteq\frac{2}{\mu({\cal P})}\int_{p \in {\cal P }}pdp.
$$
}
\\
{\bf Proof}: A straightforward computation indicates  that for admissible $K_0\in(-1,1)$,~$f$ has the first derivative
\[
f'(K_0) = \frac{2\int_{\cal P}pdp-\mu(\mathcal{P})-K_0\mu(\mathcal{P})}{1-K_0^2}=\frac{\mu(P)(2\bar p-1-K_0)}{1-K_0^2},
\]
whereas $f(\pm 1)=-\infty$. Since this derivative is positive when $K_0<K_0^*$, negative when $K_0>K_0^*$ and zero at~$K_0=K_0^*$, it follows that~$K_0^*$ 
maximizes $f(K)$ over $K_0\in[-1,1]$. $\square$

\subsubsection*{Observations and Important Special Cases}
Notice that the optimal robust static linear control coincides with the Kelly's formula~$K^*_p$ for the perfect-information case with probability
of heads~$p$ being the \emph{centroid} $\bar p$ of uncertainty set $\mathcal{P}$. It is also interesting that we can obtain Kelly's formula~$K^*_p$ as a special case of our robustness analysis if we relax the positive measure assumption on~${\cal P}$ by using a limiting argument. That is, suppose~$p \in (0,1)$ and~${\cal P} = [p, p +\delta]$ with parameter~$\delta$ satisfying~$0 < \delta \leq 1-p$. Then, taking the limit as~$\delta \rightarrow 0$, it is straightforward to verify that the optimal robust linear feedback gain reduces to~$K_0^*= 2p-1$. Next, we consider the case when~${\cal P}$ is a union positive-length disjoint intervals
$$
{\cal P} = \bigcup_{{i =1}}^m[p_{min,i},p_{max,i}].
$$
Applying the lemma, a straightforward calculation leads to
$$
K_0^* = \frac{\sum_{i=1}^m ({p_{max,i}^2 - p_{min,i}^2})}
          {\sum_{i=1}^m ({p_{max,i} - p_{min,i}})}-1
$$
which specializes further: If all differences~$p_{max,i} - p_{min,i}$ are the same, then, the formula above reduces to
$$
K_0^* = \frac{1}{m} \sum_{i =1}^m (p_{min,i} + p_{max,i}) - 1
$$
which, for the single-interval case~${\cal P} = [p_{min},p_{max}]$ becomes
$$
K_0^* = p_{min} + p_{max} -1.
$$
\section{Examples: Optimal Nonlinear Controller}
To motivate the key ideas underlying the general result in the theorem to follow, we calculate the optimal robust nonlinear control for the simpler special cases~$n =2$ and~$n = 3$. As seen below each of these optima can be found in a very simple manner.
That is, each of the desired nonlinear feedback gains comprising the optimum~$K^* \in {\cal K}$ is found via single-variable maximization whose solution
admits a closed form. 
%In the two exmples below, for simplicity, we take~$V_0 = 1$.
%and consider two illustrative cases for the bounding set~${\cal P}$.

\subsubsection*{Example~1} Beginning with the $n=2$ and ${\cal P} = [0,1]$, to simplify calculations, we first represent the nonlinear controller components~$K_k(X)$ employing the shorthand notation~$a = K_0(X)$ for all~$X \in {\cal X}$, $b = K_1(1,1) = K_1(1,-1)$ and~$c = K_1(-1,1) = K(-1,-1)$, we first calculate
\[
\begin{aligned}
ELG_K(p)= & \;\;\;\;\;p^2\log(1+a)(1+b)\\
          & \;\;\;\;\; +\; p(1-p)\log(1+a)(1-b)\\
          &\;\;\;\;\; +\; p(1-p)\log(1-a)(1+c)\\
          & \;\;\;\;\; +\; (1-p)^2\log(1-a)(1-c).
\end{aligned}
\]
Then, upon expanding the logarithms above and integrating with respect to~$p\in{\cal P}$, we obtain
\[
\begin{aligned}
ELG_K  = & \;\;\;\; \tfrac{1}{2}\log(1+a) + \tfrac{1}{2}\log(1-a)\\
         & \;\;\;\; +\;\tfrac{1}{3}\log(1+b) + \tfrac{1}{6}\log(1-b)\\
         & \;\;\;\; +\;\tfrac{1}{6}\log(1+c) + \tfrac{1}{3}\log(1-c).
\end{aligned}
\]
Next, we note that the desired optimization with respect $(a,b,c)$ above can be solved by maximizing three separate single-variable strictly concave functions; each of 
functions is of the form~$f(x) = \alpha\log(1+x) + \beta\log(1-x)$ with unconstrained maximum, obtained by setting the derivative to zero, given by~$x^* = (\alpha - \beta)/(\alpha + \beta)$. Then, we simply observe that the budget-constrained optimum is also~$x^*$ because in all three cases above,~$\alpha$ and~$\beta$ are such that~$|x^*| \leq 1$\footnote{It is important to note this simple argument for this 2-step problem is also important in the proof for the general case of~$n > 2$ in the theorem to follow}. Based on these considerations, we immediately arrive at a unique maximizing solution~$a = 0; b = \frac{1}{3};c =-\frac{1}{3}$. In other words, it is optimal to skip the first bet, and  bet $1/3$ of the account value on heads if the first toss comes up to be heads ($X_0=1$), and bet $1/3$ of the account value on tails if the first toss comes up to be tails ($X_0=-1$). Furthermore,
via a straightforward substitution we obtain~$ELG_{K^*}= \frac{1}{3}\log\frac{32}{27}\approx 0.0566.$ For comparison purposes, thee optimal static linear controller is degenerate; i.e.,~ $K_0^*=0$; i.e., no bets are made resulting in~$ELG_{K_0^*}=0$.

\subsubsection*{Example~2} Now proceeding to the analysis for~$n = 3$, we consider the case~${\cal P} = [0.25,0.95]$ and again represent the nonlinear controller components~$K_k(X)$ using a shorthand notation~$a = K_0(X)$ for all~$X$, $b = K_1(X)$ for all~$X$ with~$X_0 = 1$, $c =K_2(1,1))$, $d =K_2(1,-1))$, $e = K_1(X)$ for all~$X$ with~$X_0 = -1$, $c =K_2(1,1))$, $f =K_2(-1,1))$ and $g =K_2(-1,1))$, we again carry out the straightforward sum-of-logarithms computation and integrate~$ELG_K(p)$ with respect to~$p \in {\cal P}$ to arrive at
\[\begin{aligned}
ELG  = &\;\; 0.09333\log(1 - a) + 0.1400\log(1+a)\\
     & +0.04647\log(1-b) + 0.09353\log(1+b)\\
   &+ 0.02598\log(1-c) + 0.06755\log(1+c)\\
   & + 0.02049\log(1-d)+ 0.02598\log(1+d)\\
  & + 0.04686\log(1-e) + 0.04647\log(1+e)\\
 & + 0.02049\log(1-f)+ 0.02598\log(1+f)\\
  & + 0.02637\log(1 - g) + 0.02049\log(1+g).
\end{aligned}\]
Now, proceeding as is the case~$n = 2$, we maximize separately with respect
to each of the seven control parameters above and arrive at unique optimum~$a = 0.2;\; b = 0.3361;\;c = 0.4445;\\ d = 0.118; e = -0.004167; f = 0.118; g = -0.007429$. Finally, for robust performance comparison purposes, we also calculate the optimal static gain~$K_0^* = p_{min} + p_{max} -1 = 0.2$.
%\textcolor{green}{In the first step of our analysis, we analyze how the degree to which use of~$K^*$ leads to robust outperformance with respect to other feedback gains~$K \in [-1,1]$. To this end, we consider the function
%\begin{eqnarray*}
% f(K) &= & \int_{p_{min}}^{p_{max}}ELG_K(p)dp\\
% & &\\
%      & = & \int_{0.25}^{0.95} p\log(1 + K) + (1-p)\log(1-K)dp\\
%      & &\\
%      & = & 0.42\log(1+K) + 0.28\log(1-K)
%\end{eqnarray*}
%which is a strictly concave single-variable function with maximum~$K^* = 0.2$ and associated optimal cost~ $f(K^*)\approx 0.0141$.}

%\vskip -.25in
%\begin{figure}[htb]
%\includegraphics[width=3.5in]{f_of_K_plot_better.eps}
%%\vspace*{-1.5in}
%\caption{\bf Optimization of Feedback Gain~$K$}
%\end{figure}

It is interesting to compare the robust performance for the optimal nonlinear controller~$ELG_{K^*}(p)$ with that of the optimal static linear controller~$ELG_{K_0^*}(p)$, as functions of the probability~$p \in {\cal P}$ and benchmarked with Kelly's perfect-information optimum~$ELG^*(p)$ as the best-possible upper bound. In Figure~2 below, where these quantities are plotted, the superior robust performance of the nonlinear controller over the static linear controller is readily apparent.
\begin{figure}[htb]
\centering
\includegraphics[width=2.5in]{performance_fixed.pdf}
%\vspace*{-1.5in}
\caption{\bf Robust Performance Plots for Comparison Purposes}
\end{figure}


\section{Main Result on Robust Optimal Control}

The theorem below, establishes the existence and uniqueness of an optimal robust nonlinear controller~$K^* \in {\cal K}$ and characterizes it with an explicit formula. Although there are~$2^n-1$ nonlinear controllers gains associated with the nodes, the theorem tells us that at each stage~$k$, there are only~$k+1$ possible values of for the optimal robust nonlinear gain~$K_k^*(X)$. Summing up these numbers across all stages, we see that  the total number of nonlinear gains to be calculated is~$1+2+\ldots+n=n(n+1)/2$; i.e., the computational burden increases quadratically in~$n$ rather than exponentially. It should also be noted that the time between coin flips is suitability large, the controller in the theorem below can be implemented ``dynamically'' with no need to pre-compute the optimal nonlinear gains~$K_k^*(\bar X)$ below.\\
\\
%\vskip1mm
\noindent
{\bf Theorem}: {\it The integrated expected logarithmic growth
$$
f(K) \doteq  \int_{p \in {\cal P }} ELG_K(p)dp,
$$
defined over on the set of admissible controllers characterized by their nonlinear gains $K\in\mathcal{K}$ has a unique maximizer~$K^*$, which, at stage~$k$ for a  sample path $\bar X\in\mathcal{X}$, is given by
\[
\begin{gathered}
K_k^*(\bar X)=\frac{\int_{p\in\mathcal{P}}p^{q_k}(1-p)^{k-q_k}(2p-1)\,dp}
{\int_{p\in\mathcal{P}}p^{q_k}(1-p)^{k-q_k}\,dp},%,\\
%q=q_k(X)\doteq\#\{i=0,1,\ldots,k-1:X_i=1\}
\end{gathered}
\]
where $q_k=q_k(\bar X)=\#\{i=0,\ldots,k-1:\bar X_i=1\}\leq k$ is the number of heads occurring over the first $k$ coin flips}\footnote{By definition, $q_0(\bar X)=0$. Thus, at stage~$k= 0$, the optimal  nonlinear controller reduces to $K^*_0(X)\equiv K^*_0$, the optimal static linear linear controller; }.
\\
%\bb{some of the steps in the proof below require attention to avoid ill-defined
%sets, empty sets, quantities etc for $k= 0$ and~$n = 1$}

\subsubsection*{Proof}\label{sec.proof}
In the arguments to follow, sample path $\bar X \in {\cal X}$ and stage number~$k \in \{0,1,...,n-1\}$ are assumed to be fixed, and we
let ${\cal X}_k(\bar X) \doteq {\cal X}_k^+(\bar X)\cup {\cal X}^-_k(\bar X)$, defining sets ${\cal X}_k^{\pm}$ by\footnote{For $k=0$, sets~${\cal X}_k^+$ and ${\cal X}_k^-$ consist of all sample paths starting from $X_0=1$ and $X_0=-1$, respectively.}
\[
\begin{gathered}
{\cal X}_k^+(\bar X) \doteq \{X \in {\cal X}:
 X_i = \bar X_i\; \forall i = 0,1,...,k-1;\, X_k = 1\},\\
{\cal X}_k^-(\bar X) \doteq \{X \in {\cal X}:
 X_i = \bar X_i\; \forall i = 0,1,...,k-1;\, X_k = -1\},
\end{gathered}
\]
%wih sets~${\cal X}_0^+$ and ${\cal X}_0^-$ consisting of all sample paths starting from $X_0=1$ and $X_0=-1$, respectively.
the expected logarithmic growth function can be written as
\begin{equation*}
\begin{aligned}
ELG_K(p)& = &\frac{1}{n}\sum_{X \in {\cal X}_k^+(\bar X)}\hskip -.15in P(X)\sum_{i = 0}^{n-1}
\log(1 + K_i(X)X_i)\\
& & \;\;+\frac{1}{n}\sum_{X \in {\cal X}_k^-(\bar X)}\hskip -.15in P(X)\sum_{i = 0}^{n-1}\log(1 + K_i(X)X_i)\\
& & \;\;+\frac{1}{n}\sum_{X \notin {\cal X}_k(\bar X)}
\hskip -.15in P(X)\sum_{i = 0}^{n-1}\log(1 + K_i(X)X_i).
\end{aligned}
\end{equation*}
Next we note that all of the terms above involving~$K_k(\bar X)$ can be isolated by setting~$i = k$ in the first two terms above. As far as the third term is concerned, it is independent of~$K_k(\bar X)$ because  admissibility of the controller forces~$K_k(X)= K_k(\bar X)$ for all~$X\in {\cal X}_k(\bar X)$. Now integrating $ELG_K(p)$ over $p\in\cal P$, it is straightforward to see that maximization of~$f(K)$ over $K\in{\cal K}$ reduces to maximization of
the single-variable function
$$
g_{k,\bar X}(K_k)\doteq\alpha_k(\bar X)\log(1 + K_k)+ \beta_k(\bar X) \log(1 - K_k)
$$
over the interval $K_k\in[-1,1]$. Here coefficients $\alpha_k,\beta_k$ are defined as follows
\[
\begin{gathered}
%g_k(\bar X,K_k)\doteq\alpha_k(\bar X)\log(1 + K_k)+ \beta_k(\bar X) \log(1 - K_k),\\
\alpha_k(\bar X)\doteq  \int_{p\in\cal P }
 \sum_{X \in {\cal X}_k^+(\bar X)}\hskip-.15in P(X)\,dp=\int_{p\in\cal P}P(X\in\mathcal{X}_k^+(\bar X))\,dp,\\
 \beta_k(\bar X)\doteq\int_{p\in{\cal P }}
 \sum_{X \in {\cal X}_k^-(\bar X)} \hskip -.15in P(X)\,dp=\int_{p\in\cal P}P(X\in\mathcal{X}_k^-(\bar X))\,dp.
\end{gathered}
\]
%function $f$ is found as
%\[
%f(K)=\frac{1}{n}\sum_{k=0}^{n-1}\sum_{\bar X\in\cal X}g_k(\bar X,K_k(\bar X));
%\]
%where the function $g_k(\bar X,K_k)$ is readily obtained as
%
%are defined as follows
%\[
%\begin{gathered}
%g_k(\bar X,K_k)\doteq\alpha_k(\bar X)\log(1 + K_k)+ \beta_k(\bar X) \log(1 - K_k),\\
%\alpha_k(\bar X)\doteq  \int_{p\in\cal P }
% \sum_{X \in {\cal X}_k^+(\bar X)}\hskip-.15in P(X)\,dp=\int_{p\in\cal P}P(X\in\mathcal{X}_k^+(\bar X))\,dp,\\
% \beta_k(\bar X)\doteq\int_{p\in{\cal P }}
% \sum_{X \in {\cal X}_k^-(\bar X)} \hskip -.15in P(X)\,dp=\int_{p\in\cal P}P(X\in\mathcal{X}_k^-(\bar X))\,dp.
%\end{gathered}
%\]
To find $\alpha_k$, notice that event $X\in\mathcal{X}_k^{+}$ is the intersection of
$k+1$ events $X_i=\bar X_i$ (where $i=0,\ldots,k-1$) and $X_k=1$; these events are mutually independent.
Among these events, there are $1+q_k(\bar X)$ events of type $X_j=1$ and probability $p$ and $k-q_k(\bar X)$ events of type
$X_j=-1$ and probability $1-p$. Multiplying these probabilities, one has
\[
\alpha_k(\bar X)=\int_{p \in {\cal P }}p^{q_k(\bar X)+1}(1-p)^{k-q_k(\bar X)}dp.
\]
To find coefficient~$\beta_k$, by a very similar argument yields
%
% $X\in\mathcal{X}_k^{-}$ is the intersection of $k+1$ independent events $X_i=\bar X_i$ (where $i=0,\ldots,k-1$) and $X_k=-1$, among which
%$q_k(\bar X)$ events have probability $p$ and $k+1-q_k(\bar X)$ events have probability $1-p$. Hence,
\[
\beta_k(\bar X)=\int_{p \in {\cal P }}p^{q_k(\bar X)}(1-p)^{k+1-q_k(\bar X)}dp.
\]
%Now, to complete the proof, we note that
%A controller $K^*$ maximizes $f(K)$ over all $K\in\mathcal{K}$ if and only if, for each sample path $\bar X$, $K_k^*(\bar X)$ is a maximizer
%of function $g(\bar X,K_k)$ over all $K_k\in[-1,1]$.
Since $g_{k,\bar X}(K_k)$ is strictly concave on interval $K_k\in[-1,1]$ and $g_{k,\bar X}(\pm 1)=-\infty$, the optimum is found by setting the derivative with respect to~$K_k$ to zero; i.e., we obtain
%$%\partial g_k(\bar X,K_k)/\partial K_k$ to zero, that is,
\[
\begin{aligned}
K_k^*(\bar X) &= \frac{\alpha_k(\bar X)-\beta_k(\bar X)}{\alpha_k(\bar X)+\beta_k(\bar X)}=\\
&=\frac{\int_{p\in\mathcal{P}}p^{q_k(\bar X)}(1-p)^{k-q_k(\bar X)}(2p-1)\,dp}
{\int_{p\in\mathcal{P}}p^{q_k(\bar X)}(1-p)^{k-q_k(\bar X)}\,dp},
\end{aligned}
\]
which satisfies $|K_k^*(\bar X)|\leq 1$ as required. $\square$
\subsection*{Nonlinear Versus Linear Control}
We are now prepared to address one of our main contentions articulated the title and abstract. That is, except for the trivial case of single-flip game~($n = 1$), we establish, as a corollary of the theorem, that the optimal nonlinear controller~$K^* \in {\cal K}$ robustly outperforms the optimal static linear feedback~$K_0^*$.
\vskip1mm
\noindent
{\bf Corollary}: { \it For $n > 1$ steps, it follows that}
$$
\int_{p\in\mathcal{P}} ELG_{_{K^*}}(p)\,dp >   \int_{p\in\mathcal{P}} ELG_{_{K_0^*}}\,dp.
$$

To facilitate the proof,  we first provide a preliminary lemma.
\vskip1mm\noindent
{\bf Preliminary Lemma}: {\it Let ${\cal P} \subseteq [0,1]$ be a Lebesgue measurable set with~$\mu({\cal P}) > 0$. Then, for all $n>1$, it follows that }
$$
\int_{p \in{\cal P}} p^{n-1}\,dp \int_{p \in {\cal P}} p\,dp < \mu({\cal P})\int_{p \in {\cal P}} p^n\,dp.
$$
\\
{\bf Proof.} Denote for brevity $I_k=\int_{p \in{\cal P}} p^k\,dp$, where $k\geq 0$. We need to prove that $I_{n-1}I_1<I_0I_n$. We proceed in three steps.
\\
\textbf{Step 1.} We apply H\"older's inequality to the pair of functions $f(p)=p$, and $g(p)=1$ with $m=n/(n-1)$ being the conjugate exponent. Since also~$1/n + 1/m = 1$, we obtain
\[
\begin{split}
I_1=\|fg\|_{L_1(\cal P)}\leq \|f\|_{L_n(\cal P)}\|g\|_{L_{m}(\cal P)}=I_n^{1/n}I_0^{1/m}.\\
\end{split}
\]
\textbf{Step 2} We now play the same ``trick'' as in Step~1  with $h(p)=p^{n-1}$ and $g(p)=1$ and the roles of $m$ and $n$ being swapped. Now,
also using fact that~$h^m=p^{(n-1)m}=p^n$, we obtain
\[
\begin{split}
I_{n-1}=\|hg\|_{L_1(\cal P)}\leq \|h\|_{L_m(\cal P)}\|g\|_{L_{n}(\cal P)}=I_n^{1/m}I_0^{1/n}.
\end{split}
\]
\textbf{Step 3.} To complete the proof, we now multiply the estimates for $I_1$ and $I_{n-1}$ in the previous two steps to obtain
\[
I_1I_{n-1}<I_n^{1/n}I_0^{1/m}I_{n}^{1/m}I_0^{1/n}=I_nI_0. \square
\]

\subsubsection*{Proof of Corollary} Recalling the theorem, $K^*$ is the unique maximizer of the robust ELG function over all admissible nonlinear gains~${K \in \cal K}$. Since the optimal linear static gain $K_0^*$ is admissible, it suffices to show that $K^*_k(X)\ne K^*_0$ for at least one $k \in \{0,1,\ldots,n-1\}$ and at least
one sample path $X \in {\cal X}$. Inded, considering the distinguished sample path corresponding to all heads; i.e., $X_0=\ldots=X_{n-1}=1$, we first note that~$q_k(X)=k$ for all $k$. Now recalling that $K^*_0=2\bar p-1$, where $\bar p$ is the centroid of $\mathcal{P}$, using the theorem, it is easily shown that $K^*_{k}(X)=2\hat p_k-1$ with
\[
\hat p_k\doteq \frac{\int_{p\in\mathcal{P}}p^{n}\,dp}{\int_{p\in\mathcal{P}}p^{n-1}\,dp}>\bar p\quad\forall k=1,\ldots,n-1
\]
with the latter inequality implied by our Preliminary Lemma. In particular, $K^*_k(X)>K_0^*$ for all $k=1,\ldots,n-1$. $\square$

\section{Conclusion and Future Research}

In this paper, our main objective was to demonstrate that nonlinear control has an important role to play in large classes of betting games described by discrete-time Markov processes. To this end, we considered a simple coin-flipping game as a demonstration case to convey our main ideas. Whereas a static linear control is ``unbeatable'' with a perfectly known probability of heads~$p$, this does not hold true when robustness with respect to variations in~$p$ is of concern. For this situation, we showed that the optimal controller with its nonlinear gains~$K^*$ the robustly outperforms optimal static linear controller with its gain~$K_0^*$.

By way of future research, our belief is that it should be possible to significantly generalize the results given here for the simple coin-flipping scenario to the case when the returns~$X_k$ taking on multiple or even a continuum of values governed by rather general probability distributions. A second possible generalization begins with ``vector sample paths'' $X$ in lieu of the scalar ones considered here. Such a formulation can be viewed as a robust portfolio balancing problem with results along these lines serving  as a stepping stone to applications such as algorithmic stock trading in financial markets.

Finally, we mention one additional continuation of this research which is motivated by the following observation: An adaptive controller aimed at maximizing expected logarithmic growth, say along the lines of those given in recent papers such as~\cite{Despons_et_al_2022} and \cite{Dettu_et_al_2022}, should rightfully be viewed as a member of our admissible control set~${\cal K}$. Accordingly, our plan for future research involves exploring the connection between results in adaptive and nonlinear control which have traditionally been viewed as rather separate areas. In this regard, further motivation for such work is provided by the simple example provided for ~$n = 2$. For this low-dimensional example, our optimal three-gain robust nonlinear controller turns out to be the same as the one provided in~\cite{Despons_et_al_2022}.
\vspace{2mm}
\normalfont

\begin{thebibliography}{99}
		\vspace{-1mm}
		
\bibitem{Kelly_1956}
J. L. Kelly, ``A New Interpretation of Information Rate,'' {\it Bell System Technical Journal,}~vol.~35.4,~pp. 917--926,~1956.

\bibitem{Breiman_1961}
L. Breiman, ``Optimal Gambling Systems for Favourable Games,'' {\it Fourth Berkeley Symposium on Mathematical Statistics and Probability}, University of California Press, pp.~65-78, 1961.

\bibitem{Thorp_1961}
E. O. Thorp, ``Fortune's Formula: The Game of Blackjack,'' {\it American Mathematical Society}, 1961.

\bibitem{Thorp_1969}
E. O. Thorp, ``Optimal Gambling Systems for Favorable Games,'' {\it Review of the International Statistical Institute}, vol. 37, pp. 273-293, 1969.

\bibitem{Cover_1984}
T. M. Cover, ``Algorithm for Maximizing Expected Log Investment Return,''{\it  IEEE Transactions on Information Theory}, IT-30, pp. 369-373,~1984.

\bibitem{Cover_Thomas_1991}
T. M. Cover and T., J. Thomas, Elements of Information Theory, Wiley Series in Telecommunications, 1991.

\bibitem{Luenberger_1998}
D. G. Luenberger, Investment Science, Oxford University Press, 1998.

\bibitem{Maclean_et_al_2011}
L. C. MacLean, E. O. Thorp, and W. T. Ziemba, The Kelly Capital Growth Investment
Criterion: Theory and Practice. World Scientic, 2011.

\bibitem{Hakansson_1971}
N. H. Hakansson, ``On Optimal Myopic Portfolio Policies With and Without Serial
Correlation of Yields,'' {\it Journal of Business},$\;\;$ vol. 44, pp. 324-334, 1971.

\bibitem{Algoet_and_Cover_1988}
P. H. Algoet and T. M. Cover, ``Asymptotic Optimality and Asymptotic Equipartition Properties of Log-Optimum Investment,'' {\it The Annals of Probability}, vol. 16, pp.~876-898, 1988.

\bibitem{Obrien_et_al_2021}
J. D. O'Brien, K. Burke, M. E. Burke, and B. R. Barmish, ``A Generalization of the Classical Kelly Betting Formula to the Case of Temporal Correlation,'' {\it IEEE Control Sytems Letters}, vol.5, pp. 623-628, 2021.

\bibitem{Hsieh_2019} C. H. Hsieh, ``Contributions to the Theory of Kelly Betting with
Applications to Stock Trading: A Control-Theoretic Approach,'' Doctoral Dissertation,
ECE Department, University of Wisconsin, 2019.

\bibitem{Barmish_and_Shcherbakov_1999}
B. R. Barmish and P. S. Shcherbakov, ''Distributionally Robust Least Squares,'' {\it Proceedings of SPAS'99}, St. Petersburg, Russia, 1999.

\bibitem{Lagoa_and_Barmish_2002}
C. M. Lagoa and and B.R. Barmish, ''Distributionally Robust Monte Carlo Simulation: A Tutorial Survey,'' Proceedings of the IFAC World Congress, vol.~35, pp.~151-162, Barcelona Spain, 2002.

\bibitem{Delage_and_Ye_2010}
E. Y. Delage and Ye, ``Distributionally Robust Optimization Under Moment Uncertainty with Application to Data-Rriven Problems,'' {\it Operations Research}, vol.~58,pp.~595-612,2010.

\bibitem{Rujeerapaiboon_et_al_2016}
N. Rujeerapaiboon, D. Kuhn and W. Wiesemann, ``Robust Growth-Optimal Portfolios,''
{\it Management Science}, vol. 62, pp. 2090-2109, 2016.

\bibitem{Li_2023}
J. Y. Li, ``Wasserstein-Kelly Portfolios: A Robust Data-Driven Solution to Optimize Portfolio Growth,'' https://arxiv.org/abs/2302.13979, 2023.
		
\bibitem{Sun_and_Boyd_2018}
Q. Sun and S. Boyd, ``Distributional Robust Kelly Strategy: Optimal
Strategy under Uncertainty in the Long-Run,'' arXiv preprint: 1812.10371, 2018.

%\bibitem{Krantz_1999}
%S.G. Krantz, Handbook of Complex Variables, Birkh{\"a}user, 1999.

\bibitem{Despons_et_al_2022}
A. Despons, L. Peliti and D. Lacoste, ``Adaptive Strategies in Kelly's Races Model,'' {\it Journal of Statistical Mechanics: Theory and Experiment}, pp.~1-17,~2022,


\bibitem{Dettu_et_al_2022}
F. Dettu, F. Abbracciavento and S. Formentin, ``Kelly-Based Stock Trading via Feedback Control,'' {\it Proceedings of the IEEE Conference on Decision and Control,} pp.~5574-5579, Cancun, Mexico, 2022.

\end{thebibliography}

\end{document}
%R. M. Bell and T. M. Cover, \Competitive Optimality of Logarithmic Investment,"
%Mathematics of Operations Research, vol. 5, pp. 161{166, 1980.
%[7]
%[8] T. M. Cover and E. Ordentlich, \Universal Portfolios with Side Information," IEEE
%Transactions on Information Theory, IT-42, pp. 348{363, 1996.
%[9] T. M. Cover and J. A. Thomas, Elements of Information Theory. Wiley, 2012.
%[10] L. C. Maclean, W. T. Ziemba, and G. Blazenko, \Growth Versus Security in Dynamic
%Investment Analysis," Management Science, vol. 38, pp. 1562{1585, 1992.
%[11] L. C. Maclean and W. T. Ziemba, \Growth Versus Security Tradeos in Dynamic
%Investment Analysis," Annals of Operations Research, vol. 85, pp. 193{227, 1999.
%160
%
%[12] L. C. Maclean, E. O. Thorp, and W. T. Ziemba, \Long-Term Capital Growth: The
%Good and Bad Properties of The Kelly and Fractional Kelly Capital Growth Criteria,"
%Quantitative Finance, vol. 10, pp. 681{687, 2010.
%[13]

%MYOPIC
%
%[15] J. Mossin, \Optimal Multiperiod Portfolio Policies," Journal of Business, vol. 41,
%pp. 215{229, 1968
%[17] L. Breiman, \Optimal Gambling Systems for Favorable Games," Fourth Berkeley Sym-
%posium, vol. 1, pp. 65{78, 1961.
%
%26] Y. Lv and B. K. Meister, \Application of the Kelly Criterion to Ornstein-Uhlenbeck
%Processes," in International Conference on Complex Sciences, pp. 1051{1062, 2009.

% Kelly Refs: Not used or look
%
%Thorp, E. O. (1969). Optimal gambling systems for favorable games. Review of the International Statistical Institute, 37(3), 273–293.
%Cover, T. M., & Thomas, J. A. (1991). Elements of Information Theory. New York: Wiley.
%MacLean, L. C., Thorp, E., & Ziemba, W. T. (2011). The Kelly criterion in blackjack, sports betting, and the stock market. Handbook of Asset and Liability Management, 1, 385–428.
%Ziemba, W. T., & Vickson, R. G. (1989). Kelly criterion and gambling problems. The American Mathematical Monthly, 96(3), 185–194.
%Daniel, K. (2012). A Generalization of the Kelly Criterion. Journal of Applied Probability, 49(3), 712–724.
%Ma, J., & Wang, Q. (2018). Kelly criterion and its applications in finance and economics. Quantitative Finance, 18(11), 1779–1789.
%Chan, L. (2018). Kelly criterion for portfolio management. Journal of Investment Strategies, 7(1), 91–111.


%



    .
%[34] C. H. Hsieh and B. R. Barmish, \On Kelly Betting: Some Limitations," in Proceedings
%%of Allerton Conf. on Communication, Control, and Computing, pp. 165{172, 2015.
%%[35] C. H. Hsieh, B. R. Barmish, and J. A. Gubner, \Kelly Betting Can be Too Conservative,"
%%in Proceedings of IEEE Conf. on Decision and Control, pp. 3695{3701, 2016.
%%[36] M.-E. Wu and W.-H. Chung, \A Novel Approach of Option Portfolio Construction
%%Using the Kelly Criterion," IEEE Access, vol. 6, pp. 53044{53052, 2018.
%%[37] M.-E. Wu, W.-H. Chung, and C.-J. Lee, \On the Analysis of Kelly Criterion and Its
%%Application," in Asian Conference on Intelligent Information and Database Systems,
%%pp. 165{172, Springer, 2019.
%[38] D. Kuhn and D. G. Luenberger, \Analysis of the Rebalancing Frequency in Log-
%Optimal Portfolio Selection," Quantitative Finance, vol. 10, no. 2, pp. 221{234, 2010.
%[39] M.-E. Wu and P.-J. Hung, \A Framework of Option Buy-Side Strategy with Simple
%Index Futures Trading Based on Kelly Criterion," in 2018 5th International Conference
%on Behavioral, Economic, and Socio-Cultural Computing, pp. 210{212, IEEE, 2018.
%[40] A. W. Lo, H. A. Orr, and R. Zhang, \The Growth of Relative Wealth and the Kelly
%Criterion," Journal of Bioeconomics, vol. 20, no. 1, pp. 49{67, 2018.
%[41] H. Markowitz, \Portfolio Selection," The Journal of Finance, vol. 7, pp. 77{91, 1952.

%
%C. H. Hsieh, B. R. Barmish, and J. A. Gubner, \The Impact of Execution Delay on
%Kelly-Based Stock Trading: High-Frequency Versus Buy and Hold," Proceedings of the
%IEEE Conference on Decision and Control, pp. 2580{2585, 2019.
%
%
%
%



%\section*{Optimality of  Kelly Betting}
%
%In this section we show (Theorem~\ref{thm.kelly-best}) that the Kelly betting strategy~\cite{Kelly_1956} is unbeatable in the case of a known probability $p$, that is, no
%gambling strategy (including those using memory and/or randomization) can provide a larger log-growth.
%
%The proof is based on the following technical lemmas.
%\begin{lemma}\label{lem.entropy}
%For $p\in[0,1]$, the maximum of the function\footnote{We formally define $f_0(-1)=0$ and $f_1(1)=0$ in order to resolve the uncertainty $0\cdot(-\infty)$ in the two degenerate cases.}
%\begin{equation}\label{eq.fp}
%f_p(\xi)=p\log(1+\xi)+(1-p)\log(1-\xi)
%\end{equation}
%on the interval $\xi\in[-1,1]$ is attained at $\xi_p^*=2p-1$, that is,
%\begin{equation}\label{eq.fp-ineq}
%f_p(\xi)\leq f_p(\xi_p^*)=1-H(p)\quad\forall \xi\in [-1,1]
%\end{equation}
%The equality here is met if and only if $\xi=\xi_p^*$.
%\end{lemma}
%\begin{proof}
%For $p\in\{0,1\}$, the statement of Lemma is straightforward, since $f_0$ (respectively, $f_1$) is decreasing (respectively, increasing) on the interval
%$\xi\in[-1,1]$. Otherwise, one may easily notice that $f_p$ is concave ($f_p''(\xi)>0$ for $\xi\in[-1,1]$) with
%$f_p'(\xi_p^*)=0$. Hence, $\xi_p^*$ is the point of (unique) strict maximum for $f_p$.
%\end{proof}
%\begin{lemma}\label{lem.log-growth}
%Let $V\geq 0,U\in\r,X\in\{-1,1\}$ be three random variables on a common probability space satisfying the following conditions:
%\begin{enumerate}
%\item $X$ has the Bernoulli distribution $X\sim B_{\pm 1}(p)$ and is independent of the random vector $(V,U)$;
%\item $\boldP(|U|\leq V)=1$;
%\item the expectation $\boldE\log V\in [-\infty,\infty)$ exists.\footnote{Recall that the expectation is well defined for a random variable
%$f$ provided that either $f^+=\max(f,0)$ or $f^-=\max(-f,0)$ has a finite expectation. In this case,
%$\boldE f=\boldE f^+-\boldE f^-$. Thus, $\boldE f<\infty$ if and only if $\boldE f^+<\infty$.}
%\end{enumerate}
%Then, the random variable $V^+=V+UX$ obeys the inequality
%\begin{equation}\label{eq.kelly-best1}
%\boldE\log V^+\leq\boldE\log V+1-H(p)
%\end{equation}
%where the equality is met if and only if $\boldP(U=(2p-1)V)=1$.
%\end{lemma}
%\begin{proof}
%Notice first that $0\leq V^+\leq 2V$, and hence $\log V^+\leq 1+\log V$ \AS Using condition 3), it can be easily seen that $\boldE\log V^+$ is well
%defined. To prove~\eqref{eq.kelly-best1}, we introduce an auxiliary random variable as follows
%\[
%\Xi=
%\begin{cases}
%U/V,&V\ne 0,\\
%0,&V=0.
%\end{cases}
%\]
%Condition 2) ensures that $|\Xi|\leq 1$ \AS
%
%Thanks to condition 1), $X$ and $\Xi$ are independent. Hence $\boldE\log(1+\Xi X|\Xi)=p\log(1+\Xi)+(1-p)\log(1-\Xi)=f_p(\Xi)$ with $f_p$ defined by~\eqref{eq.fp}.
%With probability $1$, one has $\boldE\log(1+\Xi X|\Xi)\leq 1-H(p)$ thanks to~\eqref{eq.fp-ineq}; therefore,
%\begin{equation}\label{eq.aux1}
%  \boldE\log(1+\Xi X)=\boldE\log(1+\Xi X|\Xi)\leq 1-H(p).
%\end{equation}
%
%To prove~\eqref{eq.kelly-best1}, it remains to notice that $V^+=V(1+\Xi X)$,
%whence $\log V^+=\log V+\log(1+\Xi X)$ (if $V=0$, then $V^+=0$ and both sides are $-\infty$).
%Taking the expectation, the inequality~\eqref{eq.kelly-best1} follows from~\eqref{eq.aux1}. Furthermore,~\eqref{eq.kelly-best1} is strict
%unless either $\boldE\log V=-\infty$ (in which case one also has $U=0$ \AS) or~\eqref{eq.aux1} turns into equality (that is, $\Xi=\xi_p^*=2p-1$ \AS).
%In both cases, one has $U=(2p-1)V$ \AS
%\end{proof}
%
%Lemma~\ref{lem.log-growth} entails the following theorem.
%\td{Check the notation, use $\theta(X)$ etc.}
%\begin{theorem}[\textbf{Kelly's strategy is unbeatable}]\label{thm.kelly-best}
%Consider the coin-flip game~\eqref{eq.state} and assume that $X_k\sim B_{\pm 1}(p)$
%(return of the $k$th flip) is independent of
%the random vector $(V_k,u_k)$ at each period $k=0,\ldots,n-1$.
%Then the inequality holds
%\[
%\boldE\log V_N\leq \boldE\log V_0+N(1-H(p)),
%\]
%turning into equality if and only if $u_k=(2p-1)V_k$ at each step $k$ with probability $1$.
%In particular, for $V(0)=1$ one has
%\[
%\frac{1}{N}\boldE\log V_N\leq 1-H(p).
%\]
%\end{theorem}
%\begin{proof}
%The proof is immediate by substituting $U=u_k,X=X_k,V=V_k$ and $V^+=V_{k+1}$ into~\eqref{eq.kelly-best1}:
%\[
%\boldE\log V_{k+1}\leq \boldE\log V_k+(1-H(p))\;\;\forall k=0,1,\ldots,n-1
%\]
%and recalling that the equality at each period $k$ can be met if and only if $u_k=(2p-1)V_k$.
%\end{proof}
%
%Notice that the gambler's decisions $u_k$ can be an arbitrary functions of the system's history
%$u_k=F_k(V_0,\ldots,V_{k-1},V_k,X_0,\ldots,X_{k-1})$, furthermore,
%these functions may even be random. As soon as the coin is ``memoryless'' ($X_k$ does not depend on the system's trajectory), the Kelly's betting strategy cannot be
%beaten.
%
%Theorem~\ref{thm.kelly-best} entails the following simple corollary, showing that it is impossible to provide the log-growth of the wealth in case of the ideal coin.
%\begin{corollary}\label{cor.negative}
%Consider the coin-flip game~\eqref{eq.state} and assume that $X_k\sim B_{\pm 1}\big(\frac{1}{2}\big)$
%(the coin is ideal) is independent of
%the vector $(V_k,u_k)$ at each period $k=0,\ldots,n-1$. Then
%\[
%\boldE\log V_n\leq 0,
%\]
%which inequality is strict unless the gambler declines to bet, i.e., $u_k=0$, $k=0,\ldots,n-1$ \AS
%\end{corollary}

