% 1) What problem the paper tries to address?
% 2) Why the problem is important?

% Large-scale graphs with billions of edges are ubiquitous in many industry, science, and engineering fields such as recommendation systems, social graph analysis,  knowledge base,  material science,  and biology.
% Graph machine learning (ML) on these real-world graphs will bring huge practical impact such as improving the quality of recommendations and web search results for economical growth and enabling new opportunities for enhanced AI.  
% In these fields, Researchers and practitioners are increasingly adopting graph neural networks (GNN), an emerging class of deep neural networks tailored to graph-structured data, due to their superior performance in various graph analytics tasks. 

Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to analyze graph-structured data due to their superior performance in various graph analytics tasks. 
An input graph for a GNN can have millions of vertices and billions of edges~\cite{hu2021ogblsc}.
GNNs are broadly adopted in companies such as Pinterest~\cite{pinsage} and Twitter~\cite{sign_icml_grl2020} to improve user experiences and in engineering and science domains for computer vision~\cite{qi20173d}, natural language understanding~\cite{goldberg2017neural}, quantum chemistry~\cite{gilmer2017neural}, material design~\cite{zitnick2020introduction}, and drug discovery~\cite{gaudelet2020utilising}. 

\begin{figure*}[h]
    \vspace{3mm}
  \begin{minipage}[b]{0.35\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/mini-batch.pdf}
    \caption{Example of a mini-batch.}
    \label{fig:minibatch}
  \end{minipage}
  \begin{minipage}[b]{0.30\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/data-parallel.pdf}
    \caption{Data parallelism.}
    \label{fig:intro-data-parallel}
  \end{minipage}
  \begin{minipage}[b]{0.34\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cache-parallel.pdf}
    
    \caption{Split parallelism.}
    \label{fig:intro-cache-parallel}
  \end{minipage}
  % \caption{Alternative approaches to parallel mini-batch GNN training (one training iteration, sampling and gradient aggregation not depicted).
  % \emph{Data parallelism} is adopted by existing GNN tools. \emph{\Tname} is the approach introduced by this paper.} 
\end{figure*}

A common approach to train a GNN on large-scale graphs
% , and more generally any deep neural network (DNN) on a large dataset, 
is to use \emph{mini-batch} gradient descent. 
This approach partitions the training data into subsets called mini-batches.
Each training \emph{iteration} calculates updates to the model parameters, called gradients, based on a different mini-batch. 
In GNN training, we want to learn how to compute the features of the vertices in the training set, which are called \emph{target vertices}, based on the input features of the vertices in their k-hop neighborhood. 
A mini-batch consists of a subset of target vertices and a \emph{sample} of their k-hop neighborhood, which could be excessively large if sampling was not used (see Figure~\ref{fig:minibatch}).
%\ms{Cite papers saying that mini-batch can have better accuracy?}

Mini-batch training is commonly used in production and research GNN training systems such as DGL~\cite{dgl}, 
PyTorch Geometric~\cite{pytorch-geometric}, Quiver~\cite{quiver}, AliGraph~\cite{aligraph},
PaGraph~\cite{pagraph}, 
and GNN Lab~\cite{gnnlab}.
These systems use \emph{data parallelism} to execute each training iteration across multiple GPUs (see Figure~\ref{fig:intro-data-parallel}). 
At each iteration, data parallelism partitions the mini-batch into \emph{micro-batches}, which consist of a partition of the target vertices in the mini-batch and their sampled k-hop neighbors, and assigns each micro-batch to one GPU.
The entire GNN model is replicated at each GPU.
Each GPU then loads the input features of all the vertices in its micro-batch and computes gradients locally and independently from other GPUs.
At the end of the iteration, all GPUs aggregate and apply the gradients they computed. 

% 3) Why existing solutions are insufficient?
\mypar{Limitations of Existing Work} 
Unfortunately, data parallel training for GNNs is \emph{inherently redundant} due to the \emph{overlaps} between the k-hop neighborhoods of target vertices that are assigned to different micro-batches (gray area in Figure~\ref{fig:minibatch}).
This creates both data loading and computation overheads.

Loading training data to GPUs at each iteration is known to be a significant overhead in data-parallel training since each GPU must gather the input features of all the vertices in its micro-batch, which can be large~\cite{pagraph}.
%If the features are stored in the CPU memory, gathering can take up to 74\% of the training time even if the GPU does not have to share the bus with other GPUs~\cite{pagraph}.
This overhead is exacerbated by the overlaps across micro-batches, since several vertices are likely to appear in micro-batches assigned to multiple GPUs.
The input features of these vertices are sent to multiple GPUs, which increases the communication cost.


% PaGraph and GNNlab have each GPU keep an independent cache~\cite{lin2020pagraph} \TODO{GNNlab}.
% This approach avoids does not scale well to larger graphs, since a single GPU can cache only a limited amount of data and will observe more and more cache misses as the size of the graph grows.
% Quiver uses a global cache that is partitioned across GPUs and scales with the number of GPUs.
% It relies on GPU-GPU interconnects such as NVLink, which are faster than PCIe, to perform input feature gathering at each GPU. \TODO{Quiver}
% However, input feature gathering remains an important bottleneck even when using caching, as our evaluation shows, because a GPU must still gather data every time there is a \emph{local cache miss} in its micro-batch.
% \TODO{Have experiment in the motivation section}.
% Even though the throughput of CPU-GPU and GPU-GPU buses keeps increasing, the computational capacity of GPUs is growing at a comparable rate, so input feature gathering will likely remain an important overhead. 

%To limit this problem, systems such as PaGraph~\cite{pagraph} and GNNLab~\cite{gnnlab} use overlapping caches that replicate the features of frequently-accessed vertices at multiple GPUs.
%This, however, reduces the total number of vertices whose features can be cached by GPUs in the system.

An orthogonal problem caused by overlaps across micro-batches is \emph{redundant computation}.
GNN models organize the vertices of a micro-batch sample into layers.
They compute the hidden features of the vertices in a layer by aggregating the features of their neighbors in the lower layer.
If the same vertex appears in multiple micro-batches assigned to multiple GPUs, these will compute their hidden features redundantly, which can be a significant overhead.


% 4) What is the basic idea of our solution? What are the distinctive features of our solution?
\mypar{Proposed Approach}
In this paper, rather than  patching the data-parallel pipeline incrementally, we introduce a new paradigm for parallel mini-batch GNN training called \emph{\tname} (see Figure~\ref{fig:intro-cache-parallel}).
At each iteration, split parallelism splits the vertices of the mini-batch into multiple partitions.
Unlike micro-batches, partitions do not have overlaps.
Each partition is assigned to a GPU for computation and partitions are created in a way that maximizes local computation.
During training, GPUs cooperate with each other to exchange lightweight intermediate results in each iteration.

\Tname solves the two fundamental problems of data parallelism.
It eliminates redundant computation because each vertex is uniquely associated to one GPU that is responsible for computing its hidden features.
It also eliminates redundant data loading because each vertex belongs to the split of exactly one GPU, which is the only one responsible for loading its input features.

% 5) What are the challenges to make our solution work well?
% 6) How do we address those challenges?
\Tname builds on some ideas used in systems for full-graph training, which are designed to train the GNN on the entire graph at each iteration~\cite{roc,neugraph,md2021distgnn}.
To scale to large graphs, these systems partition the input graph and exchange intermediate data across GPUs.
Mini-batch training, however, presents two unique challenges and opportunities to make \tname efficient.

First, \emph{mini-batches change at each iteration}, since they are randomly sampled.
\Tname splits each mini-batch into non-overlapping partitions, each corresponding to one GPU, to eliminate redundancies.
We call this novel scheduling problem \emph{split-parallel online scheduling}.
The scheduling algorithm is executed on the critical path of each iteration so it must be very fast.
We show how to achieve this goal by combining offline partitioning and online scheduling.
In full-graph training, online scheduling is not necessary since each iteration operates on the same graph using the same schedule, which is computed offline.
Data parallelism also uses the same schedule at each iteration (see Figure~\ref{fig:intro-data-parallel}).


% \hui{This paragraph do not connect well. Will revise it later. The logic should be, since we partition a mini-batch, we introduce communication during training. And thus propose the cooperative training} 
The consequence of splitting a mini-batch into non-overlapping partitions is that \tname requires inter-GPU communications to complete one iteration of training, because some local partitions may have a few edges that require data from other GPUs.  
We implement \emph{cooperative training} that invokes \emph{split-aware} kernels and transparently supports GNN models written using the standard high-level SAGA operators offered by existing GNN training systems.
% We design \emph{split-aware implementations} of some of these operators that are executed by all GPUs cooperatively in a tightly integrated manner through direct GPU-GPU communication.
Our implementation leverages fast GPU-GPU buses such as NVLink, when available, to speed up coordination.
We present the first experimental evaluation of cooperative training in the context of mini-batch training.
Our evaluation demonstrates that the inter-GPU communication required for cooperative training represents a much smaller cost than the redundancies in input data loading and computation inherent in data parallelism.
% and outperform NVLink-based training systems like Quiver~\cite{quiver}. \ms{Also speed up PCIe}
%These implementations pipeline intermediate data across GNN layers and GPUs without materializing it in the CPU memory, as done by many scalable full-graph training systems (e.g.~\cite{neugraph}).
% \hui{show that the communication overhead is trivial compared to the savings of redundant computation.}



Second, \emph{mini-batches are only subsets of the whole graph and some vertices are much more likely to be included in a mini-batch than others.}
This offers caching opportunities to make \tname efficient. 
Existing work on mini-batch training has shown that \emph{caching in the GPU memory} the input features of frequently-loaded vertices can reduce data transfers and significantly speed up training~\cite{pagraph,gnnlab, quiver}.
In full-graph training, caching is not used because the input features of \emph{all} vertices of a potentially large graph must be loaded to the GPUs at every iteration.

Although caching is used in some data-parallel--based GNN training systems~\cite{pagraph,gnnlab, quiver}, 
\tname is much better suited than data parallelism to leverage in-GPU input feature caches.
Data parallelism cannot take full advantage of caches because it moves input data (the micro-batches) to computation (the GPUs training independently).
A GPU still needs to load all the input features in its micro-batch that are not cached locally.
\Tname, instead, uses online scheduling to split mini-batches so that if a GPU caches the input features of a vertex, it is given the responsibility of processing those features locally.
This follows the well-established principle of moving computation to data, not data to computation. 

By moving computation to data, \tname enables multiple GPUs to cache \emph{non-overlapping subsets} of input features and form a \emph{distributed GPU cache}, whose size scales with the number of GPUs in the system.
Input features only need to be transferred from the CPU memory to a GPU when there are \emph{global} cache misses, that is, a feature is not cached by \emph{any} GPU.
Data parallelism can take advantage of a distributed GPU cache only in systems with fast GPU-GPU buses, such as NVLink.
When such a bus is available and a GPU has a local cache miss, it is faster to load input features from another GPU memory than from the CPU memory, as done by systems like Quiver~\cite{quiver} and WholeGraph~\cite{wholegraph}.
Without a fast GPU-GPU bus, a GPU running data-parallel training must load data through the PCIe bus regardless of whether the features are cached by another GPU or stored in the host memory, so a distributed GPU cache is not effective.
\Tname improves data loading time independent of the system configuration because it does not require transferring cached features across GPUs.



% the splitting algorithm must \emph{maximize data access locality for input data} based on the caches at each GPU.
% Third, splitting creates a \emph{tight integration} between GPUs, which need to perform blocking communication.
% Minimizing communication by \emph{maximizing data access locality for intermediate} data is extremely important.
% The latter two problems are related: by scheduling computation so that it maximizes data access locality for input data, we constraint the location of intermediate data and thus the scheduling of the computation that access it.

% 7) What are the experimental results and conclusions?
\mypar{Results}
We implement the proposed solutions in \name, a novel \tadj GNN training framework targeting \emph{synchronous multi-GPU} training over large graphs on a single host. 
%\name is implemented on top of DGL 0.8.2 and Torch 1.8.0 with CUDA 11.1. 
We compare \name against state-of-the-art single-host multi-GPU systems for mini-batch training: DGL~\cite{dgl}, PaGraph~\cite{pagraph}, and Quiver~\cite{quiver}.
\name consistently and significantly outperforms all these baselines, sometimes by more than one order of magnitude.
\Tname reduces data loading time, one of the main bottlenecks in the end-to-end training pipeline, by a large margin.
Its training time is competitive with data-parallel training because the communication cost of cooperative training is balanced by the gains of eliminating computational redundancy. 
Online scheduling is fast enough to not become a bottleneck, yet it produces partitions that are good enough to deliver consistent speedups over the baselines.



% 8) What are the list of contributions this work makes?
This paper makes the following key contributions: 
\begin{itemize}
    \item We introduce split parallelism, a novel paradigm for mini-batch GNN training that maximizes data access locality and avoids the redundant computation and data transfer overheads of data parallelism.
    \item We design a fast online scheduling algorithm to obtain per-GPU computation graphs from each mini-batch at each iteration. 
    \item We implement cooperative training to support GNN models written using standard SAGA operators. We present the first experimental comparison of cooperative mini-batch training and data parallel training.
    \item We implement \name, an end-to-end multi-GPU GNN training system based on \tname that \name implements an efficient split-aware operator.
\end{itemize}