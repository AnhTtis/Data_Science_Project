\label{sec:eval}

\begin{table}[t]
    \centering
    \begin{tabular}{|l||c|c|c|} \hline
         \textbf{Dataset} & \textbf{\# Nodes} & \textbf{\# Edges} & \textbf{\# Feat} \\ \hline \hline
         ogbn-products (PR) & 2.4M & 62M & 100 \\ 
        %  ogbn-arxiv & .16M & 1.1M & 128 \\ \hline
        ogbn-papers100M (PA) & 111M & 1.6B & 128 \\ 
        %  Yelp & .716M & 6.9M &300 \\ \hline
         Amazon (AM) & 1.56M & 168M & 200 \\ \hline
    \end{tabular}
    \caption{Datasets used for the evaluation}
    \label{tab:dataset}
\end{table}

\subsection{Experiment Settings}
\mypar{Hardware setup} We run our experiments on two types of hosts, both with 4 GPUs but with different GPU-GPU interconnects.
The first host type, which we call \texttt{PCIe}, has four NVIDIA GeForce RTX 3090 Ti GPUs, each with 24 GB memory, and four Intel(R) Xeon(R) Silver 4214R CPU @ 2.40GHz, each with 12 cores and 192 GB RAM.
The GPUs in host are connected using a PCIe 3.0 bus. 

The second host type, which we call \texttt{NVLink}, is an AWS EC2 P3.8xlarge instance.
It has four NVIDIA V100 GPUs with 16GB memory and Xeon E5-2686 v4  @ 2.70GHz,  with 18 cores and 244 GB RAM. 
GPUs are connected to the CPU with a PCIe 3.0 bus and with each other via NVLink.


\mypar{Datasets} 
We use three datasets listed in Table~\ref{tab:dataset}.
Two of the datasets are from the Open Graph Benchmark (OGB), a standard benchmark for GNN training~\cite{ogb-node-dataset}.
We use the two largest graphs in the benchmark: products and papers100M.
We also use the  Amazon dataset from~\cite{graphsaint}.

\mypar{GNN models}
We consider two popular and diverse GNN models, which we described in Section~\ref{sec:example-GNN}: GraphSage~\cite{graphsage} and GAT~\cite{velivckovic2017graph}.
Both GraphSage and GAT perform up to two shuffles per layer.
% GAT is a more complex model because it computes per-edge attention, which requires up to eight shuffles per layer.
We use the standard neighbour sampling algorithm, with a fanout of 20 and three hops.
We use a default hidden size of 16, as used in~\cite{graphsage,velivckovic2017graph}, and a batch size of 4096.
% \ms{check hidden size}

\mypar{Baselines}
All state-of-the-art systems for mini-batch multi-GPU GNN training use data-parallel training.
We use three systems as baselines: DGL, PaGraph, and Quiver.

\begin{itemize}
\item \textbf{DGL} (Deep Graph Library) is a standard library for GNN training~\cite{dgl-repo}.
We use DGL version 0.8.2, the same we use as a component of \name.
DGL does not use GPU caching.

\item \textbf{PaGraph} uses GPU caching to reduce data transfer overheads~\cite{pagraph}.
PaGraph always loads missing input features from the host memory through the CPU-GPU bus (PCIe).
We run the publicly-available implementation of PaGraph, updated to  run with DGL 0.8.2.

\item \textbf{Quiver} is a recent GNN training system that leverages fast direct GPU-GPU buses like NVLink~\cite{quiver} to reduce data loading overhead.
Quiver partitions the input features across GPU caches.
It loads missing features from other GPUs' caches whenever possible.
\end{itemize}

All systems use CPU-based sampling to scale to graphs that do not fit in GPU memory.
They all run 32 sampler processes in parallel with training.
\name uses the same degree-based caching policy as PaGraph and Quiver for consistency.

