\subsection{Old text}
\TODO{Here we discuss the splitting algorithm. It relies on the API to perform local and global scatter and gather. This part takes the workload assignment and storage assignment as input. Then the next section describes why we use Metis for both.}
We first define the necessary data structures for split parallelism. 
We create new function calls to use this data structure to express Graph operators.
Further we define the cost function of our approach and derive a partitioning which optimizes for it. 

\paragraph{APIs}
\TODO{This is the part to put the high-level AP} 
While current frameworks work on gpu independent bipartite graphs, which do not communicate with each other. 
We create a new data structure GPU Local Bipartite Graph which contains meta data required for intermediate computation. 
Current frameworks describe graph operators as functions of these bipartite graphs. 
We extend these operators for our new data structure of independent graphs. 
A key challenge in zero redundancy is data placement and computation placement which has to decrease both communication and be a simple on the fly algorithm. 
These challenges are addressed below. 
We need to extend complicated GCN computation to span multiple GPUs while taking advantage of intermediate computation. 
We provide the following api constructs to express traditional neural networks. 
(1) slice owned nodes: Not all nodes owned in a bipartite graph proceed to the next layer. 
(2) Shuffle scatter and reduce. As intermediate nodes must be gathered from multiple GPUs. Similary in attention computation, the src node values must be scatter to remote GPUs. 

\subsection{Local Bipartite Graph}

\TODO{Define a local bp from global sample graph}
We continue our formulation from the sample expressed in the background. 
Given a sampled computation graph $G(V,E)$ we aim to derive a gpu specific computation graph $G_{g}(V_g, E_g)$.
The key component of our formulation is the partitioning function $P$ which maps every node in the sample to a gpu.
This partition function allows us to extract edges to construct the gpu specific information. 
The nodes in a layer are further marked into owned nodes $O^l_g$ and remote nodes $R^l$.
The owned nodes represent the final resting value (i.e final accumulated value for a node) while the remote nodes represent locations of partial aggregations. 
To facilitate interchange of data , we create 2 functions per gpu, which are used to represent data flowing in and out of a gpu.
The $to$ function is used to extract node data from remote gpu and send it to the owner gpu.

\begin{align*} 
    P &= \{V \to gpu\} \\
   E^l_g &= \{e(u,v): u \in V^{l-1} \land v \in V^{l} \land P[u] = g\} \\
   V^l_g &= \{v: e(u,v) \in E^l_g\}    
\end{align*}
The set of $V^l_g$ can be further classified into owned nodes and remote nodes. 
\begin{align*}
    O^l_g &= \{v: v \in V^l_g \land P[v] = g\}  \\
    R^l_g &= \{v: v \in V^l_g \land p[v] \neq g\} \\
    to_{src_{gpu}} &= \{dest_{gpu} \to v: v \in R^l_{src_{gpu}} \land v \in O^l_{dest_{gpu}} \} \\
    from_{dest_{{gpu}}} &= \{src_{gpu} \to v: v \in O^l_{dest_{{gpu}}} \land v \in R^l_{src_{gpu}} \} \\
\end{align*}
