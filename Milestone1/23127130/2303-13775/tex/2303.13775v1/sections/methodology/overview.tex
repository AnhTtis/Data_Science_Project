\subsection{Overview}

Each \tadj training iteration consists of four main steps: sampling, online scheduling, data transfer, and cooperative training.
Figure~\ref{fig:overview} shows an overview of one iteration, excluding the preliminary sampling step that produces a mini-batch and the final synchronous gradient aggregation and parameter update step.

In \tname, \textit{sampling} returns a single mini-batch sample for all the target vertices in the mini-batch.
\textit{\Tadj online scheduling} (Step 1 in Figure~\ref{fig:overview}) splits the mini-batch into \emph{local splits} (e.g., $S_1$ and $S_2$ in Figure~\ref{fig:overview}), one for each GPU.
Unlike micro-batches in data-parallel training, the local splits are non-overlapping partitions of the mini-batch sample.
% and associate each GPU with a different non-overlapping set of vertices and edges.
% They are used by the \tadj online scheduler to decide how to load the input features onto GPUs and determine the computation graph of each GPU.
Layer-$0$ vertices are assigned based on the content of the GPU caches.
Each GPU is responsible for loading and processing the input and hidden features for the vertices in its assigned split.
The hidden features for vertices at the other layers are computed by only one GPU, which is selected to maximize data access locality.
Splitting eliminates all redundant input data loading and hidden feature computation.
The scheduling algorithm must be very fast because it is run online at each iteration on a different mini-batch.


The step following scheduling is \textit{data transfer} (Step 2 in Figure~\ref{fig:overview}). 
\Tname optimizes this step because it only transfers the features that are missing from the combined caches of all GPUs, and it transfers them to only one GPU.
After data transfer, the system invokes training kernels of each local split on its corresponding GPU. 

Finally, \textit{cooperative training} (Step 3 in Figure~\ref{fig:overview}) executes the local splits to train model parameters.
This step could involve inter-GPU communications to complete one iteration of training because some local splits may have a few edges that require data from other GPUs.
Cooperative training uses \emph{split-aware} implementations of the message-passing functions of Section~\ref{sec:gnn-background} to 
transparently supports GNN models written using the standard high-level SAGA operators offered by existing GNN training systems.
% minimize communication cost in mini-batch training.


% The GPUs compute aggregations on the data they have locally available and exchange hidden features and partial gradients in multiple stages.
The GNN model in Figure~\ref{fig:overview} has two layers, each having edges across splits. Both layers require inter-GPU communication (Steps~3.1-2), which we will discuss more in detail in Section~\ref{sec:example-GNN}.
After this coordination, each GPU is able to compute an embedding for the target vertices in their local split, compute the loss, and start propagating partial gradients backward.
This requires GPUs to send back the partial gradients over the hidden features they received in the forward pass (Step~3.3-4).
At the end of the backward pass, each GPU has gradients for the model parameters ($g_1, g_2$).

As we will discuss shortly and show in the evaluation, the inter-GPU communication required for cooperative training is a cost that is comparable to, and typically lower than, the redundant input data loading and redundant computation costs of data parallelism.


% An iteration ends with the aggregation of the gradients and the update of the model parameters (not depicted in Figure~\ref{fig:overview}).
%The size of the k-hop neighborhood explodes and quickly includes the entire graph as k grows.

%(ii)~when available, GPUs can transfer data using direct GPU-to-GPU links (e.g., NVLink~\cite{NVLink}), which are much faster than CPU-to-GPU buses. % (up to 300-600 GB/s vs. 8-128 GB/s).

% Current frameworks partition target vertices and each gpu independently constructs a computation graph from a minibatch of vertices. 
% We instead partition the training vertices and instead sample a global computation graph for the minibatch of training vertices. 
% This computation graph is split into local graphs which will be executed on individual GPU. 
% Our system uses zero redundancy approach both for computation and data loading. 
% The system proceeds to refresh the cache of each individual GPU based on the local computation graph assigned to a GPU. 
% After refreshing the GPU caches, the system proceeds with the forward and backward pass of the system.
% However, as each layer is split amongst different GPUs, we have shuffle step which allows each local graph to gather data from the neighbouring GPU. 
% This phase in co-operative training proceeds with each GCN layer exchanging data with the other GPU.

