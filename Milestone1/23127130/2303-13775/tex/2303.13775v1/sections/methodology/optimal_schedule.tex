\TODO{Sandeep: This an attempt to proof why metis works. If this is unconvincing we can skip it}
\section{Split-Parallel Optimal Scheduling}

\TODO{1st paragraph should summarize this subsection in high-level: 1 sentences: What is goal of split-parallel online  scheduling; 2nd sentence: what is the basic idea/insight we use in the split-parallel online scheduling}
We attempt to justify our use of partitioning the training graph. 
We rely on the insight that sampling in graphs even with a few hops quickly span multiple partitions and that we can use properties of the training graph to generate efficient partitions of the sample graph. 

\TODO{Define Cost Function}
\TODO{1st paragraph explains the goal of this assignment: why we do this one different from the input vertices: two principles - fast; reduce edge cuts (reduce GPU-GPU communication) --> offline ensures fast; metis algorithm works pretty well.} 
Given a sampled global graph $G_s(V,E)$, with $L$ layer  we can construct the split graphs using a partitioning function $P(v^l)$ which maps for every layer, a vertex to a gpu. 
In our approach we incur 2 costs, the cost of cache refresh and the cost of inter layer communication. 
To achieve load balancing in both computation and communication with caching, we would like to have each gpu to have approximately the same number of vertices mapped to it per layer. 
While we efficiently manage our cache by ensuring that the vertices in $v^0$ are equally distributed, we optimize cost for inner nodes in our approach due to intermediate shuffling through the partitioning function. 
As we have have local gather and scatter which aggregate messages, the total cost of shuffling for a node $v^l$ is defined as the number of partitions which contain atleast one source vertex which has an incident edge to it. 
Formally, We define a cost function for a vertex as below:
\begin{equation*}
    C[v^l] = |\{p: E(u,v) \in G_s \land P(u) = p \land p \neq P(v)\}|
\end{equation*}
This is due to the local gathering and scattering in our system.
We also want the partitioning function to be computationally tractable as this function must be run online during training. 
Since we do not constrain $P(v^m_i) \neq P(v^n_j)$, we can express a wide array of schedules. 
To make our cost function, cache percentage agnostic, we achieve optimal cache utilization (i.e communication) in the first layer by ensuring that all the vertices are equally partitioned.

\TODO{flattening}

\textit{lemma:} The cost for a sampled graph does not change when a vertex v mapped to partition $i$ is mapped to $j$,if there exists atleast one vertex $u$ in both partitions i,j with an edge to v. 
\textit{Proof:} While moving $v$ to $j$, might at decrease one partition cross edge to j, we will also add a cross edge to $i$. \TODO{remove this if its trivial}
Thus since every vertex has a self edge with its lower layer, we can move the move the vertex to same gpu partition as its lower layer such that $P(v^{l-1}) = P(v^l)$.
This is a form of graph coarsening commonly used in graph partitioning. \cite{karypis1997metis}

\TODO{union upper bound}

\textit{lemma:} The optimal cost of the sampled graph is upper bounded by the cost of the training graph partitioned to minimize communication costs while balancing vertices. 

\textit{proof by contradiction: } We assume that the partitioned training algorithm has a  lower cost has a lower cost than the sampled graph. 
We can construct induced sampled graph from the training graph using the vertices and edges in the sample, while keeping its original partitioning.
From our experiments in motivation we show that a random partition of training vertices can easily cover the training graph with 3 hops even with a highly balanced partitioning such as pagraph. 
Thus this induced graph can be assumed to be balanced. 
If the cost of this induced graph is lower, implies that sampled graph function is not optimal.

Thus a partitioning function which optimizes communication in the training graph allows us to 
 to generate the optimal partitioning of the sampled graph.
Thus we therefore use Metis to split partition the training graph while minimizing the communication costs. 
As the partitioning function through metis can be performed as a preprocessing function, the split partitioning can be quickly performed online. 


