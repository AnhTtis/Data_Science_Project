\subsection{Cooperative Training} 
\TODO{1st paragraph: explain the goal of cooperative training; basic idea to achieve the goal}
The key challenge of co-operative training with gpu local graph the input features of a layer are not self-sufficient. 
The result of a gather operation might be incomplete as some neighbours might be resident in another gpu, similarly 
the destinations nodes hidden value needed for the scatter operation might be not be available on the same gpu. 
To facilitate co-operative training, we must exchange hidden feature data during training. 
We define two forms of exchange. 
\begin{itemize}
    \item Push to owner: partial data from each remote node on a gpu is pushed to its owner gpu.
    \item Push from owner: data from owner node on a gpu is pushed to all gpus which have this node as a remote node. 
\end{itemize}
Using the above two communication patterns allow us to construct the standard gather and scatter layers. 
The forward pass of the gather function first performs a local gather and then synchronization with remote. 
The backward pass proceeds in the reverse faction where data is pushed from owner and then further pushed to src. 
We similary, design the forward and backward pass of scatter. 
We note that communication happens through cross edges.
The receiving gpu should have necessary information on how much data is expected from each sending gpu. 
Note, that this communication is blocking till all gpus finish the exchange. 

\input{algorithms/api}

\TODO{how to do the forward and backward during training using GCN /GAT example} 
We show the computation of the following function using our API.
\TODO{Is this example required. The algorithm seems to be straightforward}
\begin{equation*}
     H^{l+1} = \sum(H^{l}(v) * H^{l}(u))
\end{equation*}

% %     \begin{verbatim}    
% %     pull_from_self
% %     global_scatter
% %     edge_wise_local_computation
% %     node_wise_local_gather
% %     global_gather
% % \end{verbatim} &

\subsection{Execution Model}
\TODO{Caching and other details}

We use the partitioning function for execution management and input data management. 
Every node in $V^0$ is assigned to a gpu partition. 
In case of cache miss $V^0$ is moved only to the cache partition.
After loading the cache, each layer proceeds to exchange data such that the final value of node, lies on the owner GPU. 
The training proceeds as such till the training node vectors are predicted. 

