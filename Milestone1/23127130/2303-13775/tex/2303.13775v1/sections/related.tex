Our work focuses on single-host multi-GPU mini-batch training.
Distributed mini-batch training is another important area of research for GNN training systems.
Systems like DistDGL~\cite{distdgl} and AliGraph~\cite{aligraph} use data-parallel  mini-batch training to scale to large graphs.
$P^3$ introduces pipelined push-pull hybrid parallelism for distributed mini-batch training~\cite{gandhi2021p3}.
% Each host keeps a partition of the input features of all vertices and executes the first layer of all micro-batches on its local partition, in parallel.
% The system then shuffles the intermediate data by micro-batch.
% Each host then processes each micro-batch locally to one server, as in data parallelism.
% The declared goal of $P^3$ is to minimize distributed CPU-to-CPU communication, not single-host CPU-to-GPU or GPU-to-GPU data transfers, which is the focus of this paper.
% $P^3$ is not designed to run on single-host multi-GPU systems, but we evaluate the communication cost of applying the partitioning and shuffling strategy of $P^3$ in that context and show that it is much higher than what we can achieve using \tname.
% \TODO{see if we get the numbers}
% We discussed P3's hybrid parallelism approach for distributed training in Section~\ref{sec:background-limitations}.
Marius++ runs data-parallel GNN training on large graphs using a single GPU and out-of-core approach rather than a distributed system~\cite{waleffe2022marius++}.
For a survey on distributed GNN training, we refer the reader to~\cite{shao2022distributed}


Many systems focus on full-graph GNN training, which is a different problem than mini-batch training as discussed in Sections~\ref{sec:intro} and~\ref{sec:scheduling}.
Other notable but less directly related work in full-graph training includes Dorylus~\cite{thorpe2021dorylus}, which uses serverless functions, DGCL~\cite{cai2021dgcl}, which optimizes communication in distributed training, and FlexGraph, which support aggregation from indirect neighbors~\cite{flexgraph}.
NeutronStar is a distributed full-graph training system that uses standard full-graph training for some target vertices and fetches the k-hop neighborhood similar to data parallelism for others~\cite{neutronstar}.

Other work has explored other types of bottlenecks than the ones discussed in this paper.
Mini-batch sampling and extraction can become bottlenecks in some cases.
Existing work has explored using in-GPU sampling to alleviate this bottleneck. 
NextDoor proposes a programming API and a runtime to speed up in-GPU sampling~\cite{nextdoor}.
C-SAW is another system with similar goals~\cite{c-saw}.
GPU sampling in these systems does not scale to large graphs that cannot be stored in the GPU memory.
GNNLab is a multi-GPU mini-batch training system that uses a factorized approach to share GPU resources between sampling and training to leave more GPU memory for sampling~\cite{gnnlab}.
Adding support for in-GPU sampling and splitting to \name is an interesting avenue of future work.
GNNLab also proposes a pre-sampling technique to select vertices to cache based on sampling probability, using an independent per-GPU cache.
\name can leverage this and other caching policies to increase its cache hit rate, also thanks to its distributed cache design.
%Other prior work proposes locality-aware execution to build training pipelines alleviate these bottlenecks~\cite{kim2021accelerating}.
Finally, some work proposes dedicating a subset of the GPU threads to load data directly from the CPU memory concurrently with training~\cite{UVA-GNN-load}.
This speeds up loading but it requires a double in-GPU buffer to load features.
None of these systems explores different paradigms to parallelize GNN training.

Work on optimizing GPU kernels for GNN training (for example SeaStar~\cite{wu2021seastar}) is complementary to the work described in this work.
CPU-GPU data transfers can also be limited by biasing the sampling algorithms to prioritize sampling from graph partitions that store on GPUs~\cite{dong2021global, ramezani2020gcn}. 
However, this approach is not transparent as it relies on ML practitioners to change the model design to be cache-aware, which affects training convergence and accuracy. 