
\input{sections/experiments/setup}

%\input{sections/results/table1_pagraph}
\input{sections/results/cache-size-PCIe}

\input{sections/results/cache-size-NVLink}


\subsection{Running Time Comparison}
We compare the running times per-epoch for all systems.
All systems except DGL make use of the local cache.
Sampling, and slicing for \name, happen in parallel with training.
Therefore, we break down the epoch time of a trainer process into two sequential steps: loading time and training time.

%OLD text
% In our evaluation, we found that sampling and online splitting is never a bottleneck. 
% Trainer processes, which perform loading and training, do not wait for data from samplers.
% Therefore, we break down the epoch time of a trainer process into two sequential steps: loading time and training time.
% Sampling, and slicing for \name, happen in parallel with training.
We compare the performance of different GNN training systems on our two hosts, which use different buses.

\subsubsection{Performance using PCIe}


We first consider the \texttt{PCIe} host.
We use DGL as the baseline in configurations without a GPU cache a PaGraph in configurations with a cache.
We do not run Quiver in this setup since its distributed caching strategy requires NVLink.
Figure~\ref{fig:cache-PCIe} reports the epoch time comparison. 
Overall, \name outperforms both baselines significantly, sometimes by one order of magnitude.
We now discuss the impact of \tname in terms of loading and training time.

\mypar{Loading time}
The most important factor in this speedup is the reduction in loading time, which is a significant bottleneck in baselines.
When each GPU caches 25\% of the graph or more, the combined distributed cache contains all the features.
The scheduler schedules computation based on the placement of the cached features, so there are no cache misses.
\name only has to load the computation graph and this results in the lowest loading time.

DGL has the highest loading time because it does not use caching.
PaGraph's loading time is lower than DGL thanks to caching.
However, it still has a much higher loading overhead than \name for two reasons.
First, PaGraph maintains an independent cache at each GPU, with large overlaps among the caches.
Unlike \name, it can only cache a subset of the input features.
Second, it loads data redundantly because it uses data parallelism, an overhead \name avoids.

Figure~\ref{fig:cache-PCIe} also shows the effect of using different cache sizes.
We consider a cache size of up to 25\%, which is where \name achieves full caching.
\name can reduce loading time compared to both baselines even when the distributed cache does not contain all the features.
Consider for example the 0\% case, where caching is not used.
In this case, all the input features of the mini-batch need to be loaded from the CPU memory.
However, \name loads each feature vector to only one GPU, whereas the other data parallel systems need to perform redundant loads.

\mypar{Training time}
\name does not show large speedups in terms of training time on the \texttt{PCIe} host.
On the one hand, it has a lower computation cost than the other systems because it eliminates redundant computation.
The effect is particularly visible for the PA and AM datasets, which have larger overlaps among micro-batches (see Table~\ref{tab:overlap-edges}). 
% \ms{check}
On the other hand, it has the additional communication and synchronization cost of cooperative training, which the baseline systems do not have.
Eliminating redundant computation does not always entirely offset this cost in the \texttt{PCIe} host, which has a slower inter-GPU communication.
%This additional cost is particularly heavy in the GAT model, which requires more rounds of shuffling.
However, the training time of \name is still in line with the baseline systems.
As expected, the cache size has no significant impact on training time, as shown in Figure~\ref{fig:cache-PCIe}.

\subsubsection{Performance using NVLink}


In the experiments on the \texttt{NVLink} host, we include Quiver since its distributed cache is designed to leverage the NVLink bus and is a superior baseline than PaGraph.
Figure~\ref{fig:cache-nvlink} reports the epoch time comparison. 
\name shows large speedups of up to one order of magnitude in this configuration too, this time for both loading and training.

\mypar{Loading time}
As before, the loading time is strongly influenced by the size of the GPU caches.
In the 25\% cache case, both Quiver and \name can cache the input features of the entire graph thanks to their distributed cache.
Neither system needs to load input features from the CPU memory.
However, Quiver replaces loads from the CPU memory with loads from other GPUs' memory, as shown in Table~\ref{tab:overlap-features}.
These data transfers are faster thanks to NVLink, so data loading time is significantly reduced compared to DGL.
However, they are still not free and represent a low but not negligible cost.
\name can take advantage of the caches much more effectively than Quiver since it does not require loading input features at all.

The drawback of redundant data transfers is particularly evident when the cache size is lower than 25\%. In this case, both Quiver and \name need to load features from the CPU memory.
The loading time of Quiver grows significantly and becomes a major bottleneck.
In contrast, \name's loading time also grows, but much less significantly.

\mypar{Training time}
\name shows significant speedups compared to DGL and Quiver in terms of training time because it eliminates redundant computation.
% , as shown in Table~\ref{tab:speedup-NVLink}.
The additional communication cost of cooperative training is greatly reduced thanks to NVLink, leading to a clearly positive net gain, particularly for PA and AM, the two datasets with larger computational redundancy.
%   These gains are reduced for GAT due to the additional coordination overhead required by cooperative training.
As in the previous case, the training time of \name is not affected by the size of the cache, as shown in Figure~\ref{fig:cache-nvlink}.

% \subsubsection{Effect of Cache Size}
% We now evaluate how the size of the per-GPU cache impacts training time.
% As in the previous experiments, we measure the size of each GPU cache relative to the total size of all the input features in the dataset.

% \mypar{PCIe host}
% The results for the \texttt{PCIe} setup are shown in Table~\ref{tab:cache-PCIe}.
% We don't report DGL since it does not cache data on GPUs.
% As expected, the loading time of PaGraph decreases as the size of the cache grows.
% However, it is still consistently higher than \name for two reasons.
% This is in part due to the fact that caches are overlapping.
% The limitation of using overlapping caches shows clearly for the 25\% cache case.
% The system has 4 GPUs so total size of the caches of all GPU is sufficient to store all the input features.
% PaGraph does not manage to cache the entire input because it caches data redundantly at multiple GPUs.
% This is a reasonable choice for data-parallel training since gathering the missing features from other GPUs is as expensive as loading them from the CPU.
% With smaller cache sizes than 25\%, PaGraph performance becomes worse because each GPU experiences even more cache misses.

% \name caches all the input features across the GPUs in the 25\% case.
% The scheduler schedules computation based on the placement of the cached data, so there are no cache misses.
% The small loading time \name observes in the 25\% case is because each GPU still needs to load the structure of its local split.
% With smaller cache percentages, the loading time is still low because there are fewer cache misses, thanks to the distributed cache, and cache misses are cheaper, because there are no redundant loads.

% Another reason why PaGraph has a higher loading time is that features may need to be loaded redundantly by multiple GPUs.
% This never happens in \name since each vertex is logically associated with only one partition, which corresponds to the GPU that is responsible for loading it.

% \mypar{NVLink host}
% The results for the NVLink host are reported in Table~\ref{tab:cache-NVLink}.
% The results show that, as expected, both Quiver and \name can take advantage of a distributed cache to minimize the loading time in the 25\% case, where all the input features in the dataset can be cached by the 4 GPUs in the host.
% Quiver still has a larger loading time than \name because even if all features can be cached on the GPUs, each GPU still needs to perform input feature gathering and to transfer data from other GPUs.
% NVLink makes these data transfer faster but not free.
% When the cache fraction is less than 25\%, Quiver has much larger loading times due to the higher cost of cache misses, as discussed previously.

% \name is much faster than Quiver, primarily because of its much lower loading time. 
% Quiver caches 40\% of the input features, since each of the 4 GPUs has a non-overlapping cache.
% However, it shows a much larger loading time than \name.
% This is partly because of the redundant loads it performs.
% Another reason is that Quiver uses unified memory to load data into a GPU in presence of a cache miss.
% Unified memory determines whether to load the data is available in another GPU's memory on in the CPU memory.
% When only part of the features are cached on the distributed GPU cache, unified memory generates a sizable amount of random accesses to the CPU memory, which need to be served over PCIe.

% \name's feature loading is more efficient.
% The online scheduler extracts the missing features to be sent to each GPUs using the local slices it produces.
% The CPU sends all the features missing to each GPU in a single contiguous tensor.

% Also in this case, cooperative training pays a performance cost because of its need to coordinate.
% This overhead is mitigated in this setup because coordination now occurs over NVLink.
% This is particularly beneficial in the GAT model, which requires more shuffling rounds.


%\input{sections/results/table1_ornl}

\input{sections/results/batch_size_PCIE.tex}
\input{sections/results/batch_size_NVLINK.tex}


\subsubsection{Effect of Mini-Batch Size}
We now consider the effect of the mini-batch size on the running time.
For these experiments, we consider a cache size of 25\%.
The results are shown in Figures~\ref{fig:batch-pcie} and~\ref{fig:batch-nvlink}.

As the size of the mini-batch grows, the epoch time for all systems decreases since the system performs fewer training iterations.
\name achieves consistent speedups regardless of the mini-batch size and on both hosts.
It tends to do slightly better on larger batch sizes since they present larger overlaps among micro-batches.


\begin{table*}[h]
\centering
\begin{tabular}{|c||ccc|ccc|ccc|}
\hline
                     &      & \textbf{PR}   &       &      & \textbf{PA}   &       &      & \textbf{AM}   &       \\
\textbf{Algorithm}                     & \textbf{1024} & \textbf{4096} & \textbf{16384} & \textbf{1024} & \textbf{4096} & \textbf{16384} & \textbf{1024} & \textbf{4096} & \textbf{16384} \\
                     \hline \hline
DGL sample   & 101.701 & 98.230 & 62.723 & 88.844 & 71.453 & 59.916 & 190.542 & 128.012 & 64.928 \\
GSplit sample+split  & 141.485 & 91.448 & 44.958 & 139.557 & 123.972 & 94.477 & 266.657 & 143.772 & 66.641 \\      
\hline
\end{tabular}
\caption{Average time (in seconds) to produce the mini-batch, or the micro-batches, for one training epoch using a single sampling thread, with different datasets and batch sizes (i.e., 1024, 4086, and 16384).}
\label{tab:online-split}
\end{table*}

\subsection{Online Scheduling Evaluation}

\mypar{Cost of Online Splitting}
\Tname requires an additional step in the training pipeline executed at each iteration: online splitting, which occurs after a mini-batch sample is produced and before training starts.
It is important to ensure that this step does not become a performance bottleneck.
We now perform a micro-benchmark to evaluate the throughput of sampling and splitting in \name compared to DGL.

We run a single thread for both DGL and \name on the \texttt{PCIe} host and measure the time it takes to produce all the data required for a training iteration, considering a system with four GPUs.
A data-parallel system like DGL consumes four micro-batch samples per training iteration, each for one fourth of the target vertices in the mini-batch. 
A \tadj system like \name consumes four splits of a single mini-batch.
Even though different samples are sampled in parallel, our single-thread experiment estimates the throughput at which sampling can operate.

The results are shown in Table~\ref{tab:online-split}.
Splitting adds some overhead to sampling: for each vertex, we need to check which offline partition it belongs to and whether it is cached.
DGL does not perform splitting or caching, so it does not have this overhead.
However, sampling for \tadj training also benefits from the elimination of redundancy.
Instead of sampling four independent and potentially overlapping micro-batches, a \tadj sampler only needs to sample one larger mini-batch.
The combined effect of these two factors is that the overhead of online splitting is at most 74\% of the time spent on sampling. 



\begin{table}[h]
    \centering
        \begin{tabular}{|c|c||c|c|} \hline
        Graph & Batch Size & Edge Skew & Local Edges  \\ \hline \hline
         & 1024 & 0.997 & 0.93 \\
        PR & 4096 & 0.999 & 0.93 \\
         & 16384 & 1.012 & 0.94 \\
        \hline
         & 1024 & 1.326 & 0.96 \\
        PA & 4096 & 1.323 & 0.96\\
        & 16384 & 1.315 & 0.95\\
        \hline
        & 1024 & 0.514 & 0.83 \\
       AM & 4096 & 0.517 & 0.84 \\
        & 16384 & 0.541 & 0.83 \\
        \hline
        \end{tabular}
    \caption{Edge skew and percentage of local edges using the offline partitioning/online splitting approach in GSplit.}
    \label{tab:offline-online}
\end{table}

\mypar{Offline-Online Split Quality}
\name uses a combination of offline partitioning and online splitting to reduce the edge cut.
Previous experiments have shown that this solution achieves both high training performance and low splitting time. Here, we examine the split quality by measuring \textit{edge skew} and \textit{percentage of local edges}. 

Specifically, \textit{edge skew} is the maximum number of edges assigned to a split in an iteration minus the minimum divided by the average. It aims to capture whether the computation workloads are balanced across GPUs. A large edge skew means the local splits are not balanced. Splitting a mini-batch in a random uniform way (called \textit{random uniform partition}) would result in zero edge skew and thus perfect load balancing. However, it could lead to many edge cuts and thus high inter-GPU communication overhead. 

A \textit{local edge} is an edge whose source and destination vertex are in the same split. Otherwise it is remote and introduce a reference vertex. \textit{Percentage of local edges} is the ratio between the number of local edges and the total edges in a mini-batch. 
Higher percentage of local edges are better since it indicates lower inter-GPU communication costs. 
A \textit{random uniform partition} would result in low percentage of local edges (e.g., an average of 25\% when splitting a mini-batch into four local splits), as explained in Section~\ref{sec:scheduling}. 


Table~\ref{tab:offline-online} reports the edge skew (column ``Edge Skew'') and percentage of local edges (column ``Local Edges'') of our offline-partitioning online-splitting approach using different datasets and batch sizes, when producing 4 splits per mini-batch.
% We measure \textit{edge skew} as 
% We also measure the fraction of local edges, where both endpoints are in the same split.
Compared to a uniform random partition approach, our approach achieves higher skew but much higher percentage of local edges (83\% - 96\%), indicating much lower inter-GPU communication cost. 


% Mini-batches include vertices from the combined k-hop neighborhood of a random set of target vertices, so they can potentially sample vertices from most or all the input graph.
% Some imbalance across splits is still possible, especially at the upper layers of the mini-batch since they typically have fewer vertices than the lower layers.
% However, the computation cost of the upper layers is also lower, so an imbalance at those layers is a less significant performance problem.


% \subsection{Comparison between schedules}
% Compare \name with source-to-destination scatter with regular \name using GATs.

% \begin{tabular}{|c|c|c|} \hline
% Graph & Partition by Src & Partition by dest \\ \hline
% ogbn-products & 1.66 & 2.78 \\
% reorder-papers100M & 12.56 & 18.02 \\
% amazon & 10.49 & 15.88 \\ \hline
% \end{tabular}

% This table captures the training time (i.e forward and backward time) when edge tensors are partitioned by src vs partitioned by destionation. 
% GAT has multiple rounds of communication within itself, therefore is more sensitive to this kind of optimization.
% For the cache scenario of 25\% we measure the scattering the edge tensors by src vs by destination. 
% However the overall gains in training time are marginal.
% This is due the gains made in training time, being offset by the sampling bottlenec.
% \Sandeep{This is the push vs pull optimization. }



% \subsubsection{Pre-Processing time}

% \begin{tabular}{ |c|c|c|}
% \hline 
% Dataset & \name & Pagraph \\ \hline
% ogbn-products & XX & 3767 seconds \\ \hline
% papers100M & XX & 940.52 \\ \hline
% amazon & XX & >24 Hours \\ \hline
% \end{tabular}

% Pagraph partitions training vertices, considering locality and balancing partitions both in terms of training vertices and target vertices.
% This approach is quite expensive as PaGraph processes one vertex at a time in a blocking manner.
% We utilize the multi level graph partitioning algorithm METIS to balance the training vertices.

\begin{comment}
   \subsubsection{Effect of Feature Partitioning}

\begin{table}[t]
    \centering
        \begin{tabular}{|c|c|c|}
        \hline
        Graph & Batch & Savings \\ \hline 
        \multirow{3}{*}{ PR }  & 1024 & 8.2 \\
        & 4096 & 9.0 \\
         & 16384 & 10.1 \\ \hline
        \multirow{3}{*}{ PA } & 1024 & 48.0 \\
         & 4096 & 47.0 \\
         & 16384 & 44.2 \\  \hline
        \multirow{3}{*}{ AM } & 1024 & XX \\
         & 4096 & 9.0 \\
         & 16384 & XX \\  \hline
        \end{tabular}
    \caption{Data Transfer Volume with Feature Partitioning}
    \label{tab:my_label}
\end{table}

\name partitions mini-batches by vertex, and assigns the entire feature vector of a vertex to one GPU.
We now consider a different partitioning and scheduling approach inspired by $P^3$, a system for distributed mini-batch training we discussed in Section~\ref{sec:background-limitations}.
We partition the input graph along the feature dimension:
each feature is assigned to one GPU, which caches that feature for all the vertices in the input graph.
We compare this approach with \name.
In both cases, the entire input graph is cached using a distributed GPU cache, but it is partitioned along different dimensions.

We then assume that the 

Even though the system is not designed to run on a multi-GPU system, we des

P4 is primarily built for distributed data parallel setting.
P4 slices, the feature dimension of data and places it on each gpu.
For a 4 GPU setting, P4 places 25 percent of the feature data on each gpu.
The data of the first layer is shuffled similar to model parallelism.
We measure the ration of data moved using P4 and data moved in \name as a metric of potential savings.
 
\end{comment}


% \subsubsection{GPU scalability}
% For after the submission

\subsection{ Accuracy Evaluation}

To validate the correctness of \name, we compare its test accuracy with DGL.
Test accuracy is the accuracy of the GNN model on the test dataset, which is not seen during training. 


Figure~\ref{fig:eval-accuracy} compares the accuracy at different epochs for the two OGB datasets, PR, and PA, using GraphSage and GAT respectively.
\name's accuracy at each epoch matches DGL's accuracy, as shown in the Figure.
\name's convergence speed, however, is much faster since each epoch takes a much shorter time to complete.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/test_accuracy.pdf}
    \caption{Test Accuracy}
    \label{fig:eval-accuracy}
\end{figure}

