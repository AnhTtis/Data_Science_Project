This paper discusses some inherent performance limitations of the common data-parallel approach to mini-batch GNN training.
It proposes \tname, a novel parallel training paradigm that eliminates the computation and data transfers redundancies of data parallelism.
\Tname requires new approaches for online scheduling and cooperative training.
We implement \name, a multi-GPU mini-batch training system based on \tname, and show that it outperforms state-of-the-art systems in a variety of scenarios.

By moving computation to data, \tname significantly reduces data loading time.
While this result was expected given the premise of \tadj training, showing that cooperative training can match and sometimes surpass the performance of data parallel training is an interesting empirical result.
Similarly, we show that offline pre-processing can effectively support fast online splitting while achieving good training performance.
We believe that \tname represents an interesting new avenue to scale GNN training to large graphs and systems.
