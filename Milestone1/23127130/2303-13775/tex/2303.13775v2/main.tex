% \documentclass[sigconf, nonacm]{acmart}
\documentclass[sigplan,nonacm]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
 June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}
% enable page numbers
\settopmatter{printfolios=true}

\usepackage{cite}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{xspace}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} 
\usepackage{hyperref}
\hypersetup{ 
    colorlinks=true, 
    linkcolor=blue,
    anchorcolor=black,
    citecolor=purple,
    filecolor=cyan,
    menucolor=red,
    runcolor=cyan,
    urlcolor=magenta
} 
\usepackage{multirow}
\usepackage{bm}
\usepackage{comment}
\usepackage{apptools}
% \usepackage{subfig}
\usepackage{xspace}
\usepackage[ruled,noline,noend,linesnumbered]{algorithm2e}
\usepackage{amsmath}

% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
% \usepackage{authblk}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\name}{GSplit\xspace}
\newcommand{\TName}{Split Parallelism\xspace}
\newcommand{\Tname}{Split parallelism\xspace}
\newcommand{\tname}{split parallelism\xspace}
\newcommand{\Tadj}{Split-parallel\xspace}
\newcommand{\TAdj}{Split-Parallel\xspace}
\newcommand{\tadj}{split-parallel\xspace}
\newcommand{\ds}{mixed frontier\xspace}
\newcommand{\dss}{mixed frontiers\xspace}
\newcommand{\DS}{Mixed Frontier\xspace}
\newcommand{\DSs}{Mixed Frontiers\xspace}
\newcommand{\Ds}{Mixed frontier\xspace}
\newcommand{\Dss}{Mixed frontiers\xspace}

\newcommand{\mypar}[1]{\vspace*{0.05in}\noindent\textbf{#1.}}
\newcommand{\outline}[1]{{\color{blue}\bfseries\textit{\{#1\}}}}

\newcommand{\hui}[1]{{\color{blue}\bfseries\textit{\{Hui: #1 \}}}}
\newcommand{\ms}[1]{{\color{red}\bfseries\textit{\{Marco: #1\}}}}
\newcommand{\sand}[1]{{\color{teal}\bfseries\textit{\{Sandeep: #1 \}}}}
\newcommand{\juelin}[1]{{\color{olive}\bfseries\textit{\{Juelin: #1 \}}}}
\newcommand{\todo}[1]{{\color{red}\bfseries\textit{\{TODO: #1 \}}}}
\newcommand{\TODO}[1]{{\color{red}\bfseries\textit{\{TODO: #1\}}}}

\DeclareMathOperator*{\E}{\mathbb{E}} % expected value \E{}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% make references clickable 
\usepackage[]{hyperref}

\begin{document}
\title{\name: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism}
\input{arxiv-sections/0-Authors}

\begin{abstract}
    Graph neural networks (GNNs), an emerging class of machine learning models for graphs, have gained popularity for their superior performance in various graph analytical tasks. Mini-batch training is commonly used to train GNNs on large graphs, and data parallelism is the standard approach to scale mini-batch training across multiple GPUs. One of the major performance costs in GNN training is the loading of input features, which prevents GPUs from being fully utilized.
    In this paper, we argue that this problem is exacerbated by redundancies that are inherent to the data parallel approach. To address this issue, we introduce a hybrid parallel mini-batch training paradigm called \tname. \Tname avoids redundant data loads and splits the sampling and training of each mini-batch across multiple GPUs online, at each iteration, using a lightweight splitting algorithm. We implement \tname in \name and show that it outperforms state-of-the-art mini-batch training systems like DGL, Quiver, and $P^3$.
\end{abstract}

\maketitle
\pagestyle{plain} % should come right after \maketitle

\input{arxiv-sections/1-Introduction}
\input{arxiv-sections/2-Background}
\input{arxiv-sections/3-Opportunities}
\input{arxiv-sections/4-Overview}
\input{arxiv-sections/5-Partition}
\input{arxiv-sections/6-API}
\input{arxiv-sections/7-Evaluations}
\input{arxiv-sections/8-relwork}
\input{arxiv-sections/9-Conclusion}
% use the plain bibliography style
\bibliographystyle{plain}
\bibliography{ref}

\end{document}