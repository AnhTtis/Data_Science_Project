    
    ``                                                                  ``  @inproceedings{ye2023sparsetir,
  title={SparseTIR: Composable abstractions for sparse compilation in deep learning},
  author={Ye, Zihao and Lai, Ruihang and Shao, Junru and Chen, Tianqi and Ceze, Luis},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={660--678},
  year={2023}
}

@inproceedings{mtmetis2013ipdps,
  title={Multi-threaded graph partitioning},
  author={LaSalle, Dominique and Karypis, George},
  booktitle={Parallel \& Distributed Processing (IPDPS), 2013 IEEE 27th International Symposium on},
  pages={225--236},
  year={2013},
  organization={IEEE}
}

@inproceedings{fu2022tlpgnn,
  title={TLPGNN: A lightweight two-level parallelism paradigm for graph neural network computation on GPU},
  author={Fu, Qiang and Ji, Yuede and Huang, H Howie},
  booktitle={Proceedings of the 31st International Symposium on High-Performance Parallel and Distributed Computing},
  pages={122--134},
  year={2022}
}

@article{balin2023cooperative,
  title={Cooperative Minibatching in Graph Neural Networks},
  author={Balin, Muhammed Fatih and LaSalle, Dominique and {\c{C}}ataly{\"u}rek, {\"U}mit V},
  journal={arXiv preprint arXiv:2310.12403},
  month={Oct.},
  year={2023}
}

@article{gsplit-arxiv,
  title={GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism},
  author={Polisetty, Sandeep and Liu, Juelin and Falus, Kobi and Fung, Yi Ren and Lim, Seung-Hwan and Guan, Hui and Serafini, Marco},
  journal={arXiv preprint arXiv:2303.13775},
  month={Mar.},
  year={2023}
}

@article{gsplit-arxiv-anon,
  title={Anonymous (omitted due to double-anonymous review)},
  journal={arXiv},
  month={Mar.},
  year={2023}
}

@inproceedings{liu2023bgl,
  title={{BGL}: {GPU-Efficient} {GNN} Training by Optimizing Graph Data {I/O} and Preprocessing},
  author={Liu, Tianfeng and Chen, Yangrui and Li, Dan and Wu, Chuan and Zhu, Yibo and He, Jun and Peng, Yanghua and Chen, Hongzheng and Chen, Hongzhi and Guo, Chuanxiong},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={103--118},
  year={2023}
}

@article{song2023adgnn,
  title={ADGNN: Towards Scalable GNN Training with Aggregation-Difference Aware Sampling},
  author={Song, Zhen and Gu, Yu and Li, Tianyi and Sun, Qing and Zhang, Yanfeng and Jensen, Christian S and Yu, Ge},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={4},
  pages={1--26},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{wan2022bns,
  title={Bns-gcn: Efficient full-graph training of graph convolutional networks with partition-parallelism and random boundary node sampling},
  author={Wan, Cheng and Li, Youjie and Li, Ang and Kim, Nam Sung and Lin, Yingyan},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={673--693},
  year={2022}
}

@article{hongtu,
  title={HongTu: Scalable Full-Graph GNN Training on Multiple GPUs},
  author={Wang, Qiange and Chen, Yao and Wong, Weng-Fai and He, Bingsheng},
  journal={Proceedings of the ACM on Management of Data},
  volume={1},
  number={4},
  pages={1--27},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@inproceedings{neutronstar,
author = {Wang, Qiange and Zhang, Yanfeng and Wang, Hao and Chen, Chaoyi and Zhang, Xiaodong and Yu, Ge},
title = {NeutronStar: Distributed GNN Training with Hybrid Dependency Management},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3526134},
doi = {10.1145/3514221.3526134},
abstract = {GNN's training needs to resolve issues of vertex dependencies, i.e., each vertex representation's update depends on its neighbors. Existing distributed GNN systems adopt either a dependencies-cached approach or a dependencies-communicated approach. Having made intensive experiments and analysis, we find that a decision to choose one or the other approach for the best performance is determined by a set of factors, including graph inputs, model configurations, and an underlying computing cluster environment. If various GNN trainings are supported solely by one approach, the performance results are often suboptimal. We study related factors for each GNN training before its execution to choose the best-fit approach accordingly. We propose a hybrid dependency-handling approach that adaptively takes the merits of the two approaches at runtime. Based on the hybrid approach, we further develop a distributed GNN training system called NeutronStar, which makes high performance GNN trainings in an automatic way. NeutronStar is also empowered by effective optimizations in CPU-GPU computation and data processing. Our experimental results on 16-node Aliyun cluster demonstrate that NeutronStar achieves 1.81X-14.25X speedup over existing GNN systems including DistDGL and ROC.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {1301–1315},
numpages = {15},
keywords = {hybrid dependency management, graph neural networks, gpu, distributed training},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@article{bytegnn,
author = {Zheng, Chenguang and Chen, Hongzhi and Cheng, Yuxuan and Song, Zhezheng and Wu, Yifan and Li, Changji and Cheng, James and Yang, Hao and Zhang, Shuai},
title = {ByteGNN: efficient graph neural network training at large scale},
year = {2022},
issue_date = {February 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3514061.3514069},
doi = {10.14778/3514061.3514069},
abstract = {Graph neural networks (GNNs) have shown excellent performance in a wide range of applications such as recommendation, risk control, and drug discovery. With the increase in the volume of graph data, distributed GNN systems become essential to support efficient GNN training. However, existing distributed GNN training systems suffer from various performance issues including high network communication cost, low CPU utilization, and poor end-to-end performance. In this paper, we propose ByteGNN, which addresses the limitations in existing distributed GNN systems with three key designs: (1) an abstraction of mini-batch graph sampling to support high parallelism, (2) a two-level scheduling strategy to improve resource utilization and to reduce the end-to-end GNN training time, and (3) a graph partitioning algorithm tailored for GNN workloads. Our experiments show that ByteGNN outperforms the state-of-the-art distributed GNN systems with up to 3.5--23.8 times faster end-to-end execution, 2--6 times higher CPU utilization, and around half of the network communication cost.},
journal = {Proc. VLDB Endow.},
month = {feb},
pages = {1228–1242},
numpages = {15}
}

@INPROCEEDINGS{fastkernel,
  author={Fan, Ruibo and Wang, Wei and Chu, Xiaowen},
  booktitle={2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Fast Sparse GPU Kernels for Accelerated Training of Graph Neural Networks}, 
  booktitle={2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  year={2023}
}

@article{adaptgear,
  title={AdaptGear: Accelerating GNN Training via Adaptive Subgraph-Level Kernels on GPUs},
  author={Yangjie Zhou and Yaoxu Song and Jingwen Leng and Zihan Liu and Weihao Cui and Zhendong Zhang and Cong Guo and Quan Chen and Li Li and Minyi Guo},
  journal={Proceedings of the 20th ACM International Conference on Computing Frontiers},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258960480}
}

@INPROCEEDINGS{twolevel,
  author={Zhang, Zhe and Luo, Ziyue and Wu, Chuan},
  booktitle={IEEE INFOCOM 2023 - IEEE Conference on Computer Communications}, 
  title={Two-level Graph Caching for Expediting Distributed GNN Training}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/INFOCOM53939.2023.10228911}}


@article{G3,
author = {Wan, Xinchen and Xu, Kaiqiang and Liao, Xudong and Jin, Yilun and Chen, Kai and Jin, Xin},
title = {Scalable and Efficient Full-Graph GNN Training for Large Graphs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589288},
doi = {10.1145/3589288},
abstract = {Graph Neural Networks (GNNs) have emerged as powerful tools to capture structural information from graph-structured data, achieving state-of-the-art performance on applications such as recommendation, knowledge graph, and search. Graphs in these domains typically contain hundreds of millions of nodes and billions of edges. However, previous GNN systems demonstrate poor scalability because large and interleaved computation dependencies in GNN training cause significant overhead in current parallelization methods. We present G3, a distributed system that can efficiently train GNNs over billion-edge graphs at scale. G3 introduces GNN hybrid parallelism which synthesizes three dimensions of parallelism to scale out GNN training by sharing intermediate results peer-to-peer in fine granularity, eliminating layer-wise barriers for global collective communication or neighbor replications as seen in prior works. G3 leverages locality-aware iterative partitioning and multi-level pipeline scheduling to exploit acceleration opportunities by distributing balanced workload among workers and overlapping computation with communication in both inter-layer and intra-layer training processes. We show via a prototype implementation and comprehensive experiments that G3 can achieve as much as 2.24x speedup in a 16-node cluster, and better final accuracy over prior works.},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {143},
numpages = {23},
keywords = {GPU, distributed training, hybrid parallelism, graph neural network}
}

@inproceedings{betty,
author = {Yang, Shuangyan and Zhang, Minjia and Dong, Wenqian and Li, Dong},
title = {Betty: Enabling Large-Scale GNN Training with Batch-Level Graph Partitioning},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575725},
doi = {10.1145/3575693.3575725},
abstract = {The Graph Neural Network (GNN) is showing outstanding results in improving the performance of graph-based applications. Recent studies demonstrate that GNN performance can be boosted via using more advanced aggregators, deeper aggregation depth, larger sampling rate, etc. While leading to promising results, the improvements come at a cost of significantly increased memory footprint, easily exceeding GPU memory capacity.  
In this paper, we introduce a method, Betty, to make GNN training more scalable and accessible via batch-level partitioning. Different from DNN training, a mini-batch in GNN has complex dependencies between input features and output labels, making batch-level partitioning difficult. Betty introduces two noveltechniques, redundancy-embedded graph (REG) partitioning and memory-aware partitioning, to effectively mitigate the redundancy and load imbalances issues across the partitions. Our evaluation of large-scale real-world datasets shows that Betty can significantly mitigate the memory bottleneck, enabling scalable GNN training with much deeper aggregation depths, larger sampling rate, larger training batch sizes, together with more advanced aggregators, with a few as a single GPU.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {103–117},
numpages = {15},
keywords = {Redundancy elimination, Efficient training method, Graph neural network, Load balancing, Heterogeneous memory, Graph partition},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings {mgg,
author = {Yuke Wang and Boyuan Feng and Zheng Wang and Tong Geng and Kevin Barker and Ang Li and Yufei Ding},
title = {{MGG}: Accelerating Graph Neural Networks with {Fine-Grained} {Intra-Kernel} {Communication-Computation} Pipelining on {Multi-GPU} Platforms},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {779--795},
url = {https://www.usenix.org/conference/osdi23/presentation/wang-yuke},
publisher = {USENIX Association},
month = jul
}

@inproceedings{pipegcn,
  title={{PipeGCN}: Efficient Full-Graph Training of Graph Convolutional Networks with Pipelined Feature Communication},
  author={Wan, C and Li, Y and Wolfe, Cameron R and Kyrillidis, A and Kim, Nam S and Lin, Y},
  booktitle={The Tenth International Conference on Learning Representations (ICLR 2022)},
  year={2022}
}

@inproceedings {legion,
author = {Jie Sun and Li Su and Zuocheng Shi and Wenting Shen and Zeke Wang and Lei Wang and Jie Zhang and Yong Li and Wenyuan Yu and Jingren Zhou and Fei Wu},
title = {Legion: Automatically Pushing the Envelope of {Multi-GPU} System for {Billion-Scale} {GNN} Training},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {165--179},
url = {https://www.usenix.org/conference/atc23/presentation/sun},
publisher = {USENIX Association},
month = jul
}

@inproceedings{wholegraph,
author = {Yang, Dongxu and Liu, Junhong and Qi, Jiaxing and Lai, Junjie},
title = {WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Graph neural networks (GNNs) are prevalent to deal with graph-structured datasets, encoding graph data into low dimensional vectors. In this paper, we present a fast training graph neural network framework, i.e., WholeGraph, based on a multi-GPU distributed shared memory architecture. Whole-Graph consists of partitioning the graph and corresponding node or edge features to multi-GPUs, eliminating the bottleneck of communication between CPU and GPUs during the training process. And the communication between different GPUs is implemented by GPUDirect Peer-to-Peer (P2P) memory access technology. Furthermore, WholeGraph provides several optimized computing operators. Our evaluations show that on large-scale graphs WholeGraph outperforms state-of-the-art GNN frameworks, such as Deep Graph Library (DGL) and Pytorch Geometric (PyG). The speedups of WholeGraph are up to 57.32x and 242.98x compared with DGL and PyG on a single machine multi-GPU node, respectively. The ratio of GPU utilization can sustain above 95\% during GNN training process.},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {54},
numpages = {14},
keywords = {GPU, graph neural network, shared memory architecture},
location = {Dallas, Texas},
series = {SC '22}
}

@Inproceedings{gsampler,
 author = {Ping Gong and Renjie Liu and Zunyao Mao and Zhenkun Cai and Xiao Yan and Cheng Li and Minjie Wang and Zhuozhao Li},
 title = {gSampler: General and efficient GPU-based graph sampling for graph learning},
 year = {2023},
 booktitle = {ACM 2023 Symposium on Operating Systems Principles (SOSP)},
}

@article{ducati,
author = {Zhang, Xin and Shen, Yanyan and Shao, Yingxia and Chen, Lei},
title = {DUCATI: A Dual-Cache Training System for Graph Neural Networks on Giant Graphs with the GPU},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589311},
doi = {10.1145/3589311},
abstract = {Recently Graph Neural Networks (GNNs) have achieved great success in many applications. The mini-batch training has become the de-facto way to train GNNs on giant graphs. However, the mini-batch generation task is extremely expensive which slows down the whole training process. Researchers have proposed several solutions to accelerate the mini-batch generation, however, they (1) fail to exploit the locality of the adjacency matrix, (2) cannot fully utilize the GPU memory, and (3) suffer from the poor adaptability to diverse workloads. In this work, we propose DUCATI, aDual-Cache system to overcome these drawbacks. In addition to the traditionalNfeat-Cache, DUCATI introduces a newAdj-Cache to further accelerate the mini-batch generation and better utilize GPU memory. DUCATI develops a workload-awareDual-Cache Allocator which adaptively finds the best cache allocation plan under different settings. We compare DUCATI with various GNN training systems on four billion-scale graphs under diverse workload settings. The experimental results show that in terms of training time, DUCATI can achieve up to 3.33 times speedup (2.07 times on average) compared to DGL and up to 1.54 times speedup (1.32 times on average) compared to the state-of-the-artSingle-Cache systems. We also analyze the time-accuracy trade-offs of DUCATI and four state-of-the-art GNN training systems. The analysis results offer users some guidelines on system selection regarding different input sizes and hardware resources.},
journal = {Proc. ACM Manag. Data},
month = {jun},
articleno = {166},
numpages = {24},
keywords = {mini-batch training, cache, graph neural networks}
}

@inproceedings{ugache,
author = {Song, Xiaoniu and Zhang, Yiwen and Chen, Rong and Chen, Haibo},
title = {UGACHE: A Unified GPU Cache for Embedding-Based Deep Learning},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613169},
doi = {10.1145/3600006.3613169},
abstract = {This paper presents UGache, a unified multi-GPU cache system for embedding-based deep learning (EmbDL). UGache is primarily motivated by the unique characteristics of EmbDL applications, namely read-only, batched, skewed, and predictable embedding accesses. UGache introduces a novel factored extraction mechanism that avoids bandwidth congestion to fully exploit high-speed cross-GPU interconnects (e.g., NVLink and NVSwitch). Based on a new hotness metric, UGache also provides a near-optimal cache policy that balances local and remote access to minimize the extraction time. We have implemented UGache and integrated it into two representative frameworks, TensorFlow and PyTorch. Evaluation using two typical types of EmbDL applications, namely graph neural network training and deep learning recommendation inference, shows that UGache outperforms state-of-the-art replication and partition designs by an average of 1.93\texttimes{} and 1.63\texttimes{} (up to 5.25\texttimes{} and 3.45\texttimes{}), respectively.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {627–641},
numpages = {15},
keywords = {GPU cache, GPU interconnect, embedding},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@article{uvafeat,
author = {Min, Seung Won and Wu, Kun and Huang, Sitao and Hidayeto\u{g}lu, Mert and Xiong, Jinjun and Ebrahimi, Eiman and Chen, Deming and Hwu, Wen-mei},
title = {Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476249.3476264},
doi = {10.14778/3476249.3476264},
abstract = {Graph Convolutional Networks (GCNs) are increasingly adopted in large-scale graph-based recommender systems. Training GCN requires the minibatch generator traversing graphs and sampling the sparsely located neighboring nodes to obtain their features. Since real-world graphs often exceed the capacity of GPU memory, current GCN training systems keep the feature table in host memory and rely on the CPU to collect sparse features before sending them to the GPUs. This approach, however, puts tremendous pressure on host memory bandwidth and the CPU. This is because the CPU needs to (1) read sparse features from memory, (2) write features into memory as a dense format, and (3) transfer the features from memory to the GPUs.In this work, we propose a novel GPU-oriented data communication approach for GCN training, where GPU threads directly access sparse features in host memory through zero-copy accesses without much CPU help. By removing the CPU gathering stage, our method significantly reduces the consumption of the host resources and data access latency. We further present two important techniques to achieve high host memory access efficiency by the GPU: (1) automatic data access address alignment to maximize PCIe packet efficiency, and (2) asynchronous zero-copy access and kernel execution to fully overlap data transfer with training. We incorporate our method into PyTorch and evaluate its effectiveness using several graphs with sizes up to 111 million nodes and 1.6 billion edges. In a multi-GPU training setup, our method is 65--92\% faster than the conventional data transfer method, and can even match the performance of all-in-GPU-memory training for some graphs that fit in GPU memory.},
journal = {Proc. VLDB Endow.},
month = {jul},
pages = {2087–2100},
numpages = {14}
}

@inproceedings{dsp,
author = {Cai, Zhenkun and Zhou, Qihui and Yan, Xiao and Zheng, Da and Song, Xiang and Zheng, Chenguang and Cheng, James and Karypis, George},
title = {DSP: Efficient GNN Training with Multiple GPUs},
year = {2023},
isbn = {9798400700156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572848.3577528},
doi = {10.1145/3572848.3577528},
abstract = {Jointly utilizing multiple GPUs to train graph neural networks (GNNs) is crucial for handling large graphs and achieving high efficiency. However, we find that existing systems suffer from high communication costs and low GPU utilization due to improper data layout and training procedures. Thus, we propose a system dubbed Distributed Sampling and Pipelining (DSP) for multi-GPU GNN training. DSP adopts a tailored data layout to utilize the fast NVLink connections among the GPUs, which stores the graph topology and popular node features in GPU memory. For efficient graph sampling with multiple GPUs, we introduce a collective sampling primitive (CSP), which pushes the sampling tasks to data to reduce communication. We also design a producer-consumer-based pipeline, which allows tasks from different mini-batches to run congruently to improve GPU utilization. We compare DSP with state-of-the-art GNN training frameworks, and the results show that DSP consistently outperforms the baselines under different datasets, GNN models and GPU counts. The speedup of DSP can be up to 26x and is over 2x in most cases.},
booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {392–404},
numpages = {13},
keywords = {graph neural networks, GPU, model training},
location = {Montreal, QC, Canada},
series = {PPoPP '23}
}

@inproceedings{teixeira2015arabesque,
  title={Arabesque: A System for Distributed Graph Mining},
  author={Teixeira, Carlos HC and Fonseca, Alexandre J and Serafini, Marco and Siganos, Georgos and Zaki, Mohammed J and Aboulnaga, Ashraf},
  booktitle={SOSP},
  year={2015}
}

@inproceedings{yang2021random,
  title={Random Walks on Huge Graphs at Cache Efficiency},
  author={Yang, Ke and Ma, Xiaosong and Thirumuruganathan, Saravanan and Chen, Kang and Wu, Yongwei},
  booktitle={Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
  pages={311--326},
  year={2021}
}

@article{zhu2020livegraph,
  title={LiveGraph: a transactional graph storage system with purely sequential adjacency list scans},
  author={Zhu, Xiaowei and Feng, Guanyu and Serafini, Marco and Ma, Xiaosong and Yu, Jiping and Xie, Lei and Aboulnaga, Ashraf and Chen, Wenguang},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={7},
  pages={1020--1034},
  year={2020},
  publisher={VLDB Endowment}
}

@article{nodehi2018tigr,
  title={Tigr: Transforming irregular graphs for gpu-friendly graph processing},
  author={Nodehi Sabet, Amir Hossein and Qiu, Junqiao and Zhao, Zhijia},
  journal={ACM SIGPLAN Notices},
  volume={53},
  number={2},
  pages={622--636},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{wang2017gunrock,
  title={Gunrock: GPU graph analytics},
  author={Wang, Yangzihao and Pan, Yuechao and Davidson, Andrew and Wu, Yuduo and Yang, Carl and Wang, Leyuan and Osama, Muhammad and Yuan, Chenshan and Liu, Weitang and Riffel, Andy T and others},
  journal={ACM Transactions on Parallel Computing (TOPC)},
  volume={4},
  number={1},
  pages={1--49},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{jangda2021accelerating,
  title={Accelerating graph sampling for graph machine learning using {GPUs}},
  author={Jangda, Abhinav and Polisetty, Sandeep and Guha, Arjun and Serafini, Marco},
  booktitle={Eurosys},
  year={2021}
}
@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{liu2021bgl,
  title={BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing},
  author={Liu, Tianfeng and Chen, Yangrui and Li, Dan and Wu, Chuan and Zhu, Yibo and He, Jun and Peng, Yanghua and Chen, Hongzheng and Chen, Hongzhi and Guo, Chuanxiong},
  journal={arXiv preprint arXiv:2112.08541},
  year={2021}
}

@inproceedings{wang2021skywalker,
  title={Skywalker: Efficient Alias-Method-Based Graph Sampling and Random Walk on GPUs},
  author={Wang, Pengyu and Li, Chao and Wang, Jing and Wang, Taolei and Zhang, Lu and Leng, Jingwen and Chen, Quan and Guo, Minyi},
  booktitle={2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  pages={304--317},
  year={2021},
  organization={IEEE}
}

@misc{ogb-large,
  title = {Open Graph Benchmark: Large-Scale Challenge},
  howpublished = {\url{https://ogb.stanford.edu/docs/lsc/}}
}

@misc{ogb-large-issue1,
  title = {OGB LSC WikiKG90M-LSC: Memory Issue when running baseline models},
  howpublished = {\url{https://github.com/snap-stanford/ogb/discussions/198}}
}

@misc{thrust,
  title = {Thrust library},
  howpublished = {\url{https://github.com/NVIDIA/thrust}}
}

@misc{ogb-large-issue2,
  title = {OGB LSC MAG240M-LSC: Infra requirements},
  howpublished = {\url{https://github.com/snap-stanford/ogb/discussions/121}}
}

@misc{ogb-large-issue3,
  title = {OGB LSC MAG240M-LSC: rgnn.py --in-memory option error},
  howpublished = {\url{https://github.com/snap-stanford/ogb/discussions/192?sort=new}}
}

@misc{ogb-large-kdd,
  title = {OGB LSC: KDD'21 Cup},
  howpublished = {\url{https://ogb.stanford.edu/kddcup2021/}}
}

@misc{wikidata,
  title = {Wikidata Knowledge Base},
  howpublished = {\url{https://www.wikidata.org}}
}

@inproceedings{serafini2017qfrag,
  title={Qfrag: Distributed graph search via subgraph isomorphism},
  author={Serafini, Marco and De Francisci Morales, Gianmarco and Siganos, Georgos},
  booktitle={SoCC},
  year={2017}
}

@misc{nextdoor-github,
  title = {NextDoor - A System and DSL for In-GPU Graph Sampling},
  howpublished = {\url{https://github.com/plasma-umass/nextdoor}}
}

@misc{livegraph-github,
  title = {LiveGraph - A Transactional Graph Storage System},
  howpublished = {\url{https://github.com/thu-pacman/LiveGraph}}
}

@misc{pytorch-geometric-repo,
  title = {PyTorch Geometric},
  howpublished = {\url{https://pytorch-geometric.readthedocs.io/en/latest/}}
}

@article{wang2019deep,
  title={Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs.},
  author={Wang, Minjie and Yu, Lingfan and Zheng, Da and Gan, Quan and Gai, Yu and Ye, Zihao and Li, Mufei and Zhou, Jinjing and Huang, Qi and Ma, Chao and others},
  year={2019}
}

@inproceedings{pagraph,
author = {Lin, Zhiqi and Li, Cheng and Miao, Youshan and Liu, Yunxin and Xu, Yinlong},
title = {PaGraph: Scaling GNN Training on Large Graphs via Computation-Aware Caching},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3419111.3421281},
abstract = {Emerging graph neural networks (GNNs) have extended the successes of deep learning techniques against datasets like images and texts to more complex graph-structured data. By leveraging GPU accelerators, existing frameworks combine both mini-batch and sampling for effective and efficient model training on large graphs. However, this setup faces a scalability issue since loading rich vertices features from CPU to GPU through a limited bandwidth link usually dominates the training cycle. In this paper, we propose PaGraph, a system that supports general and efficient sampling-based GNN training on single-server with multi-GPU. PaGraph significantly reduces the data loading time by exploiting available GPU resources to keep frequently accessed graph data with a cache. It also embodies a lightweight yet effective caching policy that takes into account graph structural information and data access patterns of sampling-based GNN training simultaneously. Furthermore, to scale out on multiple GPUs, PaGraph develops a fast GNN-computation-aware partition algorithm to avoid cross-partition access during data parallel training and achieves better cache efficiency. Evaluations on two representative GNN models, GCN and GraphSAGE, show that PaGraph achieves up to 96.8\% data loading time reductions and up to 4.8X performance speedup over the state-of-the-art baselines. Together with preprocessing optimization, PaGraph further delivers up to 16.0X end-to-end speedup.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {401–415},
numpages = {15},
keywords = {cache, multi-GPU, large graph, graph partition, graph neural network},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@inproceedings{mvs,
author = {Cong, Weilin and Forsati, Rana and Kandemir, Mahmut and Mahdavi, Mehrdad},
title = {Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks},
year = {2020},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
series = {KDD '20}
}

@article{hu2021ogb,
  title={Ogb-lsc: A large-scale challenge for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Ren, Hongyu and Nakata, Maho and Dong, Yuxiao and Leskovec, Jure},
  journal={arXiv preprint arXiv:2103.09430},
  year={2021}
}



@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{multi-dimensional-rw,
author = {Ribeiro, Bruno and Towsley, Don},
title = {Estimating and Sampling Graphs with Multidimensional Random Walks},
year = {2010},
booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
series = {IMC '10}
}

@inproceedings{fastgcn,
title={Fast{GCN}: Fast Learning with Graph Convolutional Networks via Importance Sampling},
author={Jie Chen and Tengfei Ma and Cao Xiao},
booktitle={International Conference on Learning Representations},
year={2018},
series = {ICLR'18}
}

@article{asgcn,
  title={Adaptive sampling towards fast graph representation learning},
  author={Huang, Wenbing and Zhang, Tong and Rong, Yu and Huang, Junzhou},
  journal={arXiv preprint arXiv:1809.05343},
  year={2018}
}

@inproceedings{asap,
author = {Iyer, Anand Padmanabha and Liu, Zaoxing and Jin, Xin and Venkataraman, Shivaram and Braverman, Vladimir and Stoica, Ion},
title = {{ASAP: Fast, Approximate Graph Pattern Mining at Scale}},
year = {2018},
booktitle = {{Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation}},
series = {OSDI’18}
}

@inproceedings{rstream,
author = {Wang, Kai and Zuo, Zhiqiang and Thorpe, John and Nguyen, Tien Quang and Xu, Guoqing Harry},
title = {{RStream: Marrying Relational Algebra with Streaming for Efficient Graph Mining on a Single Machine}},
year = {2018},
booktitle = {{Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation}},
series = {OSDI’18}
}

@inproceedings{fractal,
author = {Dias, Vinicius and Teixeira, Carlos H. C. and Guedes, Dorgival and Meira, Wagner and Parthasarathy, Srinivasan},
title = {Fractal: A General-Purpose Graph Pattern Mining System},
year = {2019},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
series = {SIGMOD '19}
}

@inproceedings{gminer,
  title={G-miner: an efficient task-oriented graph mining system},
  author={Chen, Hongzhi and Liu, Miao and Zhao, Yunjian and Yan, Xiao and Yan, Da and Cheng, James},
  booktitle={Proceedings of the Thirteenth EuroSys Conference},
  pages={1--12},
  year={2018}
}

@inproceedings{peregrine,
  title={Peregrine: a pattern-aware graph mining system},
  author={Jamshidi, Kasra and Mahadasa, Rakesh and Vora, Keval},
  booktitle={Proceedings of the Fifteenth European Conference on Computer Systems},
  pages={1--16},
  year={2020}
}

@article{pangolin,
author = {Chen, Xuhao and Dathathri, Roshan and Gill, Gurbinder and Pingali, Keshav},
title = {{Pangolin: An Efficient and Flexible Graph Mining System on CPU and GPU}},
year = {2020},
journal = {Proc. VLDB Endow.},

}

@inproceedings{automine,
author = {Mawhirter, Daniel and Wu, Bo},
title = {{AutoMine: Harmonizing High-Level Abstraction and High Performance for Graph Mining}},
year = {2019},
booktitle = {{Proceedings of the 27th ACM Symposium on Operating Systems Principles}},
series = {SOSP ’19}
}


@article{bai2020ripple,
  title={Ripple Walk Training: A Subgraph-based training framework for Large and Deep Graph Neural Network},
  author={Bai, Jiyang and Ren, Yuxiang and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2002.07206},
  year={2020}
}

@inproceedings{perozzi2014deepwalk,
  title={Deepwalk: Online learning of social representations},
  author={Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
  booktitle={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={701--710},
  year={2014}
}

@inproceedings{dong2021global,
  title={Global Neighbor Sampling for Mixed CPU-GPU Training on Giant Graphs},
  author={Dong, Jialin and Zheng, Da and Yang, Lin F and Karypis, Geroge},
  booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages = {289–299},
  numpages = {11},
  series = {KDD '21},
  year={2021}
}
@inproceedings{grover2016node2vec,
  title={node2vec: Scalable feature learning for networks},
  author={Grover, Aditya and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={855--864},
  year={2016}
}
@article{nscale,
  title={NScale: neighborhood-centric large-scale graph analytics in the cloud},
  author={Quamar, Abdul and Deshpande, Amol and Lin, Jimmy},
  journal={The VLDB Journal},
  year={2016},
}
@article{samplingsurvey,
  title={Sampling methods for efficient training of graph convolutional networks: A survey},
  author={Liu, Xin and Yan, Mingyu and Deng, Lei and Li, Guoqi and Ye, Xiaochun and Fan, Dongrui},
  journal={arXiv preprint arXiv:2103.05872},
  year={2021}
}
@article{bai2020ripple,
  title={Ripple Walk Training: A Subgraph-based training framework for Large and Deep Graph Neural Network},
  author={Bai, Jiyang and Ren, Yuxiang and Zhang, Jiawei},
  journal={arXiv preprint arXiv:2002.07206},
  year={2020}
}
@article{asgcn,
  title={Adaptive sampling towards fast graph representation learning},
  author={Huang, Wenbing and Zhang, Tong and Rong, Yu and Huang, Junzhou},
  journal={arXiv preprint arXiv:1809.05343},
  year={2018}
}
@inproceedings{c-saw,
  title={C-SAW: a framework for graph sampling and random walk on GPUs},
author={Pandey, Santosh and Li, Lingda and Hoisie, Adolfy and Li, Xiaoye S. and Liu, Hang},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  year={2020}
}
@article{vrgcn,
  title={Stochastic training of graph convolutional networks with variance reduction},
  author={Chen, Jianfei and Zhu, Jun and Song, Le},
  journal={arXiv preprint arXiv:1710.10568},
  year={2017}
}
@inproceedings{sse,
  title={Learning steady-states of iterative algorithms over graphs},
  author={Dai, Hanjun and Kozareva, Zornitsa and Dai, Bo and Smola, Alex and Song, Le},
  booktitle={International conference on machine learning},
  pages={1106--1114},
  year={2018},
  organization={PMLR}
}
@article{aligraph,
  title={Aligraph: A comprehensive graph neural network platform},
  author={Zhu, Rong and Zhao, Kun and Yang, Hongxia and Lin, Wei and Zhou, Chang and Ai, Baole and Li, Yong and Zhou, Jingren},
  journal = {Proc. VLDB Endow.},
  volume = {12},
  number = {12},
  pages = {2094–2105},
  year={2019}
}
@inproceedings{nextdoor,
  title={Accelerating Graph Sampling for Graph Machine Learning using GPUs},
  author={Jangda, Abhinav and Polisetty, Sandeep and Guha, Arjun and Serafini, Marco},
  booktitle={European Conference on Computer Systems (EuroSys)},
  year={2021}
}
@inproceedings{pinsage,
  title={Graph convolutional neural networks for web-scale recommender systems},
  author={Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L and Leskovec, Jure},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={974--983},
  year={2018}
}
@article{gnn-survey-comprehensive,
  title={A comprehensive survey on graph neural networks},
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  year={2020},
  publisher={IEEE}
}

@article{gnn-survey-review,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI Open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}
@article{roc,
  title={Improving the accuracy, scalability, and performance of graph neural networks with roc},
  author={Jia, Zhihao and Lin, Sina and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={187--198},
  year={2020}
}
@article{lux,
  title={A distributed multi-gpu system for fast graph processing},
  author={Jia, Zhihao and Kwon, Yongkee and Shipman, Galen and McCormick, Pat and Erez, Mattan and Aiken, Alex},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={3},
  pages={297--310},
  year={2017},
  publisher={VLDB Endowment}
}
@article{distdgl,
  title={DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs},
  author={Zheng, Da and Ma, Chao and Wang, Minjie and Zhou, Jinjing and Su, Qidong and Song, Xiang and Gan, Quan and Zhang, Zheng and Karypis, George},
  journal={arXiv preprint arXiv:2010.05337},
  year={2020}
}
@article{ldbc-graphalytics,
  title={LDBC Graphalytics: A benchmark for large-scale graph analysis on parallel and distributed platforms},
  author={Iosup, Alexandru and Hegeman, Tim and Ngai, Wing Lung and Heldens, Stijn and Prat-P{\'e}rez, Arnau and Manhardto, Thomas and Chafio, Hassan and Capot{\u{a}}, Mihai and Sundaram, Narayanan and Anderson, Michael and others},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={13},
  pages={1317--1328},
  year={2016},
  publisher={VLDB Endowment}
}
@inproceedings{fractal,
author = {Dias, Vinicius and Teixeira, Carlos H. C. and Guedes, Dorgival and Meira, Wagner and Parthasarathy, Srinivasan},
title = {Fractal: A General-Purpose Graph Pattern Mining System},
year = {2019},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
series = {SIGMOD '19}
}
@article{graph-GPU-survey,
  title={Graph processing on GPUs: A survey},
  author={Shi, Xuanhua and Zheng, Zhigao and Zhou, Yongluan and Jin, Hai and He, Ligang and Liu, Bo and Hua, Qiang-Sheng},
  journal={ACM Computing Surveys (CSUR)},
  year={2018},
}

@inproceedings{gemini,
  title={Gemini: A computation-centric distributed graph processing system},
  author={Zhu, Xiaowei and Chen, Wenguang and Zheng, Weimin and Ma, Xiaosong},
  booktitle={12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
  year={2016}
}

@inproceedings{clustergcn,
author = {Chiang, Wei-Lin and Liu, Xuanqing and Si, Si and Li, Yang and Bengio, Samy and Hsieh, Cho-Jui},
title = {Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks},
year = {2019},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
series = {KDD '19}
}

@inproceedings{compressed-graphs-gpu,
author = {Sha, Mo and Li, Yuchen and Tan, Kian-Lee},
title = {GPU-Based Graph Traversal on Compressed Graphs},
year = {2019},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
series = {SIGMOD ’19}
}  

@INPROCEEDINGS{sc15-bfs,
  author={H. {Liu} and H. H. {Huang}},
  booktitle={SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Enterprise: breadth-first graph traversal on GPUs}, 
  year={2015},
}

@inproceedings{ma2019neugraph,
  title={Neugraph: parallel deep neural network computation on large graphs},
  author={Ma, Lingxiao and Yang, Zhi and Miao, Youshan and Xue, Jilong and Wu, Ming and Zhou, Lidong and Dai, Yafei},
  booktitle={2019 {USENIX} Annual Technical Conference ({USENIX ATC} 19)},
  year={2019}
}

@article{jia2020improving,
  title={Improving the accuracy, scalability, and performance of graph neural networks with roc},
  author={Jia, Zhihao and Lin, Sina and Gao, Mingyu and Zaharia, Matei and Aiken, Alex},
  journal={Proceedings of Machine Learning and Systems},
  year={2020}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{hybrid-radix-sort,
author = {Stehle, Elias and Jacobsen, Hans-Arno},
title = {A Memory Bandwidth-Efficient Hybrid Radix Sort on GPUs},
year = {2017},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
series = {SIGMOD ’17}
}

@article{word2vec,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{pinterest,
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
year = {2018},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
series = {KDD '18}
}

@inproceedings{subway,
author = {Sabet, Amir Hossein Nodehi and Zhao, Zhijia and Gupta, Rajiv},
title = {Subway: Minimizing Data Transfer during out-of-GPU-Memory Graph Processing},
year = {2020},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
series = {EuroSys ’20}
}

@inproceedings{tigr,
author = {Nodehi Sabet, Amir Hossein and Qiu, Junqiao and Zhao, Zhijia},
title = {Tigr: Transforming Irregular Graphs for GPU-Friendly Graph Processing},
year = {2018},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
series = {ASPLOS ’18}
}

@misc{graphsage-repo,
      author       = {William Hamilton},
      title        = {GraphSAGE repository},
      howpublished = {\url{https://github.com/williamleif/GraphSAGE}},
      year         = 2017
}

@misc{graphsage-website,
      author       = {William Hamilton et al.},
      title        = {GraphSAGE website},
      howpublished = {\url{http://snap.stanford.edu/graphsage/}},
}

@article{hamilton2017representation,
  title={Representation learning on graphs: Methods and applications},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  journal={arXiv preprint arXiv:1709.05584},
  year={2017}
}

@article{chami2020machine,
  title={Machine Learning on Graphs: A Model and Comprehensive Taxonomy},
  author={Chami, Ines and Abu-El-Haija, Sami and Perozzi, Bryan and R{\'e}, Christopher and Murphy, Kevin},
  journal={arXiv preprint arXiv:2005.03675},
  year={2020}
}

@article{goh2007human,
  title={The human disease network},
  author={Goh, Kwang-Il and Cusick, Michael E and Valle, David and Childs, Barton and Vidal, Marc and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  journal={Proceedings of the National Academy of Sciences},
  year={2007},
}

@inproceedings{wu2021seastar,
  title={Seastar: vertex-centric programming for graph neural networks},
  author={Wu, Yidi and Ma, Kaihao and Cai, Zhenkun and Jin, Tatiana and Li, Boyang and Zheng, Chenguang and Cheng, James and Yu, Fan},
  booktitle={Proceedings of the Sixteenth European Conference on Computer Systems},
  pages={359--375},
  year={2021}
}

@inproceedings{node2vec,
author = {Grover, Aditya and Leskovec, Jure},
title = {{Node2vec: Scalable Feature Learning for Networks}},
year = {2016},
booktitle = {{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}},
series = {KDD ’16}
}


@inproceedings{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{ivanov2018anonymous,
  title={Anonymous walk embeddings},
  author={Ivanov, Sergey and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:1805.11921},
  year={2018}
}

@article{li2015gated,
  title={Gated graph sequence neural networks},
  author={Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  journal={arXiv preprint arXiv:1511.05493},
  year={2015}
}

@inproceedings{duvenaud2015convolutional,
  title={Convolutional networks on graphs for learning molecular fingerprints},
  author={Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'a}n and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  year={2015}
}

@inproceedings{bordes2013translating,
  title={Translating embeddings for modeling multi-relational data},
  author={Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
  booktitle={Advances in neural information processing systems},
  year={2013}
}

@inproceedings{layersampling,
  title={Large-Scale Learnable Graph Convolutional Networks},
  author={Gao, Hongyang and Wang, Zhengyang and Ji, Shuiwang},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2018},
  series={KDD' 18}
}

@inproceedings{SympleGraph,
author = {Zhuo, Youwei and Chen, Jingji and Luo, Qinyi and Wang, Yanzhi and Yang, Hailong and Qian, Depei and Qian, Xuehai},
title = {SympleGraph: Distributed Graph Processing with Precise Loop-Carried Dependency Guarantee},
year = {2020},
booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
series = {PLDI 2020}
}

@misc{nvidia-cub,
  title           = {NVIDIA CUB},
  url       = {https://nvlabs.github.io/cub/},
  year = {Accessed in Feb 2021}
}
@article{gunrock,
author = {Wang, Yangzihao and Davidson, Andrew and Pan, Yuechao and Wu, Yuduo and Riffel, Andy and Owens, John D.},
title = {{Gunrock: A High-Performance Graph Processing Library on the GPU}},
year = {2016},
journal = {SIGPLAN Not.},

}
@misc{snapnets,
      author       = {Jure Leskovec and Andrej Krevl},
      title        = {{SNAP Datasets}: {Stanford} Large Network Dataset Collection},
      howpublished = {\url{http://snap.stanford.edu/data}},     
      year         = 2014
}

@techreport{page-rank,           
          author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
           title = {The PageRank Citation Ranking: Bringing Order to the Web.},
            type = {Technical Report},
            year = {1999},
     institution = {Stanford InfoLab},
}


@inproceedings{personalized-page-rank,
author = {Haveliwala, Taher H.},
title = {Topic-Sensitive PageRank},
year = {2002},
booktitle = {Proceedings of the 11th International Conference on World Wide Web},
series = {WWW ’02}
}
@article{pytorch-geometric,
  title={Fast graph representation learning with PyTorch Geometric},
  author={Fey, Matthias and Lenssen, Jan Eric},
  journal={arXiv preprint arXiv:1903.02428},
  year={2019}
}
@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={arXiv preprint arXiv:1912.01703},
  year={2019}
}
@inproceedings{tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)},
  pages={265--283},
  year={2016}
}
@article{dgl,
      title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks}, 
      author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},
      year={2020},
      eprint={1909.01315},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@article{allreduce,
  title={Bandwidth optimal all-reduce algorithms for clusters of workstations},
  author={Patarasuk, Pitch and Yuan, Xin},
  journal={Journal of Parallel and Distributed Computing},
  volume={69},
  number={2},
  pages={117--124},
  year={2009},
  publisher={Elsevier}
}
@inproceedings{parameter-server,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  pages={583--598},
  year={2014}
}
@article{owt,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}
@inproceedings{pipedream,
  title={PipeDream: generalized pipeline parallelism for DNN training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}
@inproceedings{gminer,
  title={G-miner: an efficient task-oriented graph mining system},
  author={Chen, Hongzhi and Liu, Miao and Zhao, Yunjian and Yan, Xiao and Yan, Da and Cheng, James},
  booktitle={Proceedings of the Thirteenth EuroSys Conference},
  pages={1--12},
  year={2018}
}
@inproceedings{asap,
author = {Iyer, Anand Padmanabha and Liu, Zaoxing and Jin, Xin and Venkataraman, Shivaram and Braverman, Vladimir and Stoica, Ion},
title = {{ASAP: Fast, Approximate Graph Pattern Mining at Scale}},
year = {2018},
booktitle = {{Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation}},
series = {OSDI’18}
}

@inproceedings{automine,
author = {Mawhirter, Daniel and Wu, Bo},
title = {{AutoMine: Harmonizing High-Level Abstraction and High Performance for Graph Mining}},
year = {2019},
booktitle = {{Proceedings of the 27th ACM Symposium on Operating Systems Principles}},
series = {SOSP ’19}
}

@inproceedings{simd-x,
author = {Liu, Hang and Huang, H. Howie},
title = {{SIMD-X: Programming and Processing of Graph Algorithms on {GPU}s}},
year = {2019},
booktitle = {{Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference}},
series = {USENIX ATC ’19}
}

@inproceedings{xbfs,
author = {Gaihre, Anil and Wu, Zhenlin and Yao, Fan and Liu, Hang},
title = {{XBFS: EXploring Runtime Optimizations for Breadth-First Search on GPUs}},
year = {2019},
booktitle = {{Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing}},
series = {HPDC ’19}
}

@ARTICLE{medusa,
author={J. {Zhong} and B. {He}},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Medusa: Simplified Graph Processing on GPUs},
year={2014},
}

@inproceedings{cusha,
author = {Khorasani, Farzad and Vora, Keval and Gupta, Rajiv and Bhuyan, Laxmi N.},
title = {{CuSha: Vertex-Centric Graph Processing on GPUs}},
year = {2014},
booktitle = {{Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing}},
series = {HPDC ’14}
}

@inproceedings{map-graph,
author = {Fu, Zhisong and Personick, Michael and Thompson, Bryan},
title = {{MapGraph: A High Level API for Fast Development of High Performance Graph Analytics on GPUs}},
year = {2014},
booktitle = {{Proceedings of Workshop on GRAph Data Management Experiences and Systems}},
series = {GRADES’14}
}

@inproceedings{pregel,
author = {Malewicz, Grzegorz and Austern, Matthew H. and Bik, Aart J.C and Dehnert, James C. and Horn, Ilan and Leiser, Naty and Czajkowski, Grzegorz},
title = {{Pregel: A System for Large-Scale Graph Processing}},
year = {2010},
booktitle = {{Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data}},
series = {SIGMOD ’10}
}

@inproceedings{graphlab,
author = {Low, Yucheng and Gonzalez, Joseph and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos and Hellerstein, Joseph},
title = {{GraphLab: A New Framework for Parallel Machine Learning}},
year = {2010},
booktitle = {{Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence}},
series = {UAI’10}
}

@inproceedings{powergraph,
author = {Gonzalez, Joseph E. and Low, Yucheng and Gu, Haijie and Bickson, Danny and Guestrin, Carlos},
title = {{PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs}},
year = {2012},
booktitle = {{Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation}},
series = {OSDI’12}
}

@article{ligra,
author = {Shun, Julian and Blelloch, Guy E.},
title = {{Ligra: A Lightweight Graph Processing Framework for Shared Memory}},
year = {2013},
journal = {SIGPLAN Not.},
}

@inproceedings{galois,
author = {Nguyen, Donald and Lenharth, Andrew and Pingali, Keshav},
title = {{A Lightweight Infrastructure for Graph Analytics}},
year = {2013},
booktitle = {{Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles}},
series = {SOSP ’13}
}

@article{PowerLyra,
author = {Chen, Rong and Shi, Jiaxin and Chen, Yanzhe and Zang, Binyu and Guan, Haibing and Chen, Haibo},
title = {PowerLyra: Differentiated Graph Computation and Partitioning on Skewed Graphs},
year = {2019},
journal = {ACM Trans. Parallel Comput.},
rticleno = {13},
}

@inproceedings{mvs,
author = {Cong, Weilin and Forsati, Rana and Kandemir, Mahmut and Mahdavi, Mehrdad},
title = {Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks},
year = {2020},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
series = {KDD '20}
}

  
@inproceedings{graphsaint,
  title={GraphSAINT: Graph Sampling Based Inductive Learning Method},
  author={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},
  booktitle={International Conference on Learning Representations},
  year={2020},
  series={ICLR '20}
}

@inproceedings{multi-dimensional-rw,
author = {Ribeiro, Bruno and Towsley, Don},
title = {Estimating and Sampling Graphs with Multidimensional Random Walks},
year = {2010},
booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
series = {IMC '10}
}

@inproceedings{fastgcn,
title={Fast{GCN}: Fast Learning with Graph Convolutional Networks via Importance Sampling},
author={Jie Chen and Tengfei Ma and Cao Xiao},
booktitle={International Conference on Learning Representations},
year={2018},
series = {ICLR'18}
}

@inproceedings{ladies,
 author = {Zou, Difan and Hu, Ziniu and Wang, Yewen and Jiang, Song and Sun, Yizhou and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks},
 year = {2019},
 series={Nuerips '19}
}


@inproceedings{help,
author = {Salihoglu, Semih and Widom, Jennifer},
title = {{HelP: High-Level Primitives For Large-Scale Graph Processing}},
year = {2014},
booktitle = {{Proceedings of Workshop on GRAph Data Management Experiences and Systems}},
series = {GRADES’14}
}

@article{green-marl,
author = {Hong, Sungpack and Chafi, Hassan and Sedlar, Edic and Olukotun, Kunle},
title = {{Green-Marl: A DSL for Easy and Efficient Graph Analysis}},
year = {2012},
journal = {SIGARCH Comput. Archit. News},
}

@inproceedings{graphsage,
author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
title = {{Inductive Representation Learning on Large Graphs}},
year = {2017},
booktitle = {{Proceedings of the 31st International Conference on Neural Information Processing Systems}},
series = {NIPS’17}
}

@inproceedings{deepwalk,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {{DeepWalk: Online Learning of Social Representations}},
year = {2014},
booktitle = {{Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}},
series = {KDD ’14}
}
@inproceedings{peregrine,
  title={Peregrine: a pattern-aware graph mining system},
  author={Jamshidi, Kasra and Mahadasa, Rakesh and Vora, Keval},
  booktitle={Proceedings of the Fifteenth European Conference on Computer Systems},
  pages={1--16},
  year={2020}
}

@inproceedings{rstream,
author = {Wang, Kai and Zuo, Zhiqiang and Thorpe, John and Nguyen, Tien Quang and Xu, Guoqing Harry},
title = {{RStream: Marrying Relational Algebra with Streaming for Efficient Graph Mining on a Single Machine}},
year = {2018},
booktitle = {{Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation}},
series = {OSDI’18}
}

@article{pangolin,
author = {Chen, Xuhao and Dathathri, Roshan and Gill, Gurbinder and Pingali, Keshav},
title = {{Pangolin: An Efficient and Flexible Graph Mining System on CPU and GPU}},
year = {2020},
journal = {Proc. VLDB Endow.},

}

@inproceedings{arabesque,
author = {Teixeira, Carlos H. C. and Fonseca, Alexandre J. and Serafini, Marco and Siganos, Georgos and Zaki, Mohammed J. and Aboulnaga, Ashraf},
title = {{Arabesque: A System for Distributed Graph Mining}},
year = {2015},
booktitle = {{Proceedings of the 25th Symposium on Operating Systems Principles}},
series = {SOSP ’15}
}

@inproceedings{scalemine,
author = {Abdelhamid, Ehab and Abdelaziz, Ibrahim and Kalnis, Panos and Khayyat, Zuhair and Jamour, Fuad},
title = {{Scalemine: Scalable Parallel Frequent Subgraph Mining in a Single Large Graph}},
year = {2016},
booktitle = {{Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}},
series = {SC ’16}
}

@INPROCEEDINGS{distgraph,
author={N. {Babu} and A. {John}},
booktitle={2016 International Conference on Emerging Technological Trends (ICETT)},
title={A distributed approach to weighted frequent Subgraph mining},
year={2016},
}

@inproceedings{knightking,
author = {Yang, Ke and Zhang, MingXing and Chen, Kang and Ma, Xiaosong and Bai, Yang and Jiang, Yong},
title = {{KnightKing: A Fast Distributed Graph Random Walk Engine}},
year = {2019},
booktitle = {{Proceedings of the 27th ACM Symposium on Operating Systems Principles}},
series = {SOSP ’19}
}

@article{karypis1997metis,
  title={METIS: A software package for partitioning unstructured graphs, partitioning meshes, and computing fill-reducing orderings of sparse matrices},
  author={Karypis, George and Kumar, Vipin},
  year={1997}
}
@inproceedings{tripathy2020reducing,
  title={Reducing communication in graph neural network training},
  author={Tripathy, Alok and Yelick, Katherine and Bulu{\c{c}}, Ayd{\i}n},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2020},
  organization={IEEE}
}
@inproceedings{thorpe2021dorylus,
  title={Dorylus: affordable, scalable, and accurate GNN training with distributed CPU servers and serverless threads},
  author={Thorpe, John and Qiao, Yifan and Eyolfson, Jonathan and Teng, Shen and Hu, Guanzhou and Jia, Zhihao and Wei, Jinliang and Vora, Keval and Netravali, Ravi and Kim, Miryung and others},
  booktitle={15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
  pages={495--514},
  year={2021}
}
@article{allamanisgraph,
  title={Graph Neural Networks on Program Analysis},
  author={Allamanis, Miltiadis}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{sign_icml_grl2020,
    title={SIGN: Scalable Inception Graph Neural Networks},
    author={Fabrizio Frasca and Emanuele Rossi and Davide Eynard and Benjamin Chamberlain and Michael Bronstein and Federico Monti},
    booktitle={ICML 2020 Workshop on Graph Representation Learning and Beyond},
    year={2020}
}

@inproceedings{you2020graph,
  title={Graph structure of neural networks},
  author={You, Jiaxuan and Leskovec, Jure and He, Kaiming and Xie, Saining},
  booktitle={International Conference on Machine Learning},
  pages={10881--10891},
  year={2020},
  organization={PMLR}
}
@inproceedings{ramezani2020gcn,
  title={GCN meets GPU: Decoupling" When to Sample" from" How to Sample".},
  author={Ramezani, Morteza and Cong, Weilin and Mahdavi, Mehrdad and Sivasubramaniam, Anand and Kandemir, Mahmut T},
  booktitle={NeurIPS},
  year={2020}
}
@article{gaudelet2020utilising,
  title={Utilising graph machine learning within drug discovery and development},
  author={Gaudelet, Thomas and Day, Ben and Jamasb, Arian R and Soman, Jyothish and Regep, Cristian and Liu, Gertrude and Hayter, Jeremy BR and Vickers, Richard and Roberts, Charles and Tang, Jian and others},
  journal={arXiv preprint arXiv:2012.05716},
  year={2020}
}
@article{zitnick2020introduction,
  title={An introduction to electrocatalyst design using machine learning for renewable energy storage},
  author={Zitnick, C Lawrence and Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Lavril, Thibaut and Palizhati, Aini and Riviere, Morgane and others},
  journal={arXiv preprint arXiv:2010.09435},
  year={2020}
}
@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}
@article{sahu2017ubiquity,
  title={The ubiquity of large graphs and surprising challenges of graph processing},
  author={Sahu, Siddhartha and Mhedhbi, Amine and Salihoglu, Semih and Lin, Jimmy and {\"O}zsu, M Tamer},
  journal={Proceedings of the VLDB Endowment},
  volume={11},
  number={4},
  pages={420--431},
  year={2017},
  publisher={VLDB Endowment}
}
@inproceedings{huang2021understanding,
  title={Understanding and bridging the gaps in current GNN performance optimizations},
  author={Huang, Kezhao and Zhai, Jidong and Zheng, Zhen and Yi, Youngmin and Shen, Xipeng},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={119--132},
  year={2021}
}
@inproceedings{wang2021gnnadvisor,
  title={GNNAdvisor: An Adaptive and Efficient Runtime System for {GNN} Acceleration on GPUs},
  author={Wang, Yuke and Feng, Boyuan and Li, Gushu and Li, Shuangchen and Deng, Lei and Xie, Yuan and Ding, Yufei},
  booktitle={15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
  pages={515--531},
  year={2021}
}
@article{kernighan1970efficient,
  title={An efficient heuristic procedure for partitioning graphs},
  author={Kernighan, Brian W and Lin, Shen},
  journal={The Bell system technical journal},
  volume={49},
  number={2},
  pages={291--307},
  year={1970},
  publisher={Nokia Bell Labs}
}
@article{chen2017stochastic,
  title={Stochastic training of graph convolutional networks with variance reduction},
  author={Chen, Jianfei and Zhu, Jun and Song, Le},
  journal={arXiv preprint arXiv:1710.10568},
  year={2017}
}
@article{fey2021gnnautoscale,
  title={GNNAutoScale: Scalable and Expressive Graph Neural Networks via Historical Embeddings},
  author={Fey, Matthias and Lenssen, Jan E and Weichert, Frank and Leskovec, Jure},
  journal={arXiv preprint arXiv:2106.05609},
  year={2021}
}
@article{serafini2021scalable,
  title={Scalable Graph Neural Network Training: The Case for Sampling},
  author={Serafini, Marco and Guan, Hui},
  journal={ACM SIGOPS Operating Systems Review},
  volume={55},
  number={1},
  pages={68--76},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@inproceedings{chen2020fusegnn,
  title={fuseGNN: accelerating graph convolutional neural network training on GPGPU},
  author={Chen, Zhaodong and Yan, Mingyu and Zhu, Maohua and Deng, Lei and Li, Guoqi and Li, Shuangchen and Xie, Yuan},
  booktitle={2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)},
  pages={1--9},
  year={2020},
  organization={IEEE}
}
@article{lin2021accelerating,
  title={Accelerating SpMM Kernel with Cache-First Edge Sampling for GNN Inference},
  author={Lin, Chien-Yu and Luo, Liang and Ceze, Luis},
  journal={arXiv preprint arXiv:2104.10716},
  year={2021}
}
@article{angerd2020distributed,
  title={Distributed Training of Graph Convolutional Networks using Subgraph Approximation},
  author={Angerd, Alexandra and Balasubramanian, Keshav and Annavaram, Murali},
  journal={arXiv preprint arXiv:2012.04930},
  year={2020}
}
@article{das2021case,
  title={Case-based Reasoning for Natural Language Queries over Knowledge Bases},
  author={Das, Rajarshi and Zaheer, Manzil and Thai, Dung and Godbole, Ameya and Perez, Ethan and Lee, Jay-Yoon and Tan, Lizhen and Polymenakos, Lazaros and McCallum, Andrew},
  journal={arXiv preprint arXiv:2104.08762},
  year={2021}
}
@article{das2020probabilistic,
  title={Probabilistic Case-based Reasoning for Open-World Knowledge Graph Completion},
  author={Das, Rajarshi and Godbole, Ameya and Monath, Nicholas and Zaheer, Manzil and McCallum, Andrew},
  journal={arXiv preprint arXiv:2010.03548},
  year={2020}
}
@article{das2020simple,
  title={A simple approach to case-based reasoning in knowledge bases},
  author={Das, Rajarshi and Godbole, Ameya and Dhuliawala, Shehzaad and Zaheer, Manzil and McCallum, Andrew},
  journal={arXiv preprint arXiv:2006.14198},
  year={2020}
}
@article{li2021training,
  title={Training Graph Neural Networks with 1000 Layers},
  author={Li, Guohao and M{\"u}ller, Matthias and Ghanem, Bernard and Koltun, Vladlen},
  journal={arXiv preprint arXiv:2106.07476},
  year={2021}
}
@article{jiang2021communication,
  title={Communication-Efficient Sampling for Distributed Training of Graph Convolutional Networks},
  author={Jiang, Peng and Rumi, Masuma Akter},
  journal={arXiv preprint arXiv:2101.07706},
  year={2021}
}

@misc{holykecodes,
 title = {About Holyoke Codes},
 author = {Holyoke Codes},
 howpublished = "\url{https://holyokecodes.org/about}",
 year = {2020},
}

@misc{departmentbpc,
 title = {UMass College of Information and Computer Science Departmental BPC Plan},
 author = {UMASS CICS},
 howpublished = "\url{https://plans.bpcnet.org/UniversityOfMassachusettsAmherst_Information&ComputerScience_DepartmentalBPCPlan.pdf}",
 year = {2020},
}



@inproceedings{md2021distgnn,
  title={DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks},
  author={Md, Vasimuddin and Misra, Sanchit and Ma, Guixiang and Mohanty, Ramanarayan and Georganas, Evangelos and Heinecke, Alexander and Kalamkar, Dhiraj and Ahmed, Nesreen K and Avancha, Sasikanth},
    booktitle = {SC'21: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    year={2021}
}
@article{ramezani2021learn,
  title={Learn Locally, Correct Globally: A Distributed Algorithm for Training Graph Neural Networks},
  author={Ramezani, Morteza and Cong, Weilin and Mahdavi, Mehrdad and Kandemir, Mahmut T and Sivasubramaniam, Anand},
  journal={arXiv preprint arXiv:2111.08202},
  year={2021}
}

@misc{ogb-node-dataset,
      title={Open Graph Benchmark: Datasets for Machine Learning on Graphs}, 
      author={Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
      year={2021},
      eprint={2005.00687},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings {yuje2023tcgnn,
author = {Yuke Wang and Boyuan Feng and Zheng Wang and Guyue Huang and Yufei Ding},
title = {{TC-GNN}: Bridging Sparse {GNN} Computation and Dense Tensor Cores on {GPUs}},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {149--164},
url = {https://www.usenix.org/conference/atc23/presentation/wang-yuke},
publisher = {USENIX Association},
month = jul
}

@inproceedings{cai2021dgcl,
author = {Cai, Zhenkun and Yan, Xiao and Wu, Yidi and Ma, Kaihao and Cheng, James and Yu, Fan},
title = {DGCL: An Efficient Communication Library for Distributed GNN Training},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3447786.3456233},
abstract = {Graph neural networks (GNNs) have gained increasing popularity in many areas such as e-commerce, social networks and bio-informatics. Distributed GNN training is essential for handling large graphs and reducing the execution time. However, for distributed GNN training, a peer-to-peer communication strategy suffers from high communication overheads. Also, different GPUs require different remote vertex embeddings, which leads to an irregular communication pattern and renders existing communication planning solutions unsuitable. We propose the distributed graph communication library (DGCL) for efficient GNN training on multiple GPUs. At the heart of DGCL is a communication planning algorithm tailored for GNN training, which jointly considers fully utilizing fast links, fusing communication, avoiding contention and balancing loads on different links. DGCL can be easily adopted to extend existing single-GPU GNN systems to distributed training. We conducted extensive experiments on different datasets and network configurations to compare DGCL with alternative communication schemes. In our experiments, DGCL reduces the communication time of the peer-to-peer communication by 77.5\% on average and the training time for an epoch by up to 47\%.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {130–144},
numpages = {15},
keywords = {graph neural networks, distributed and parallel training, network communication},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}
@article{hu2021ogblsc,
  title={OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs},
  author={Hu, Weihua and Fey, Matthias and Ren, Hongyu and Nakata, Maho and Dong, Yuxiao and Leskovec, Jure},
  journal={arXiv preprint arXiv:2103.09430},
  year={2021}
}
@article{li2020deepergcn,
  title={Deepergcn: All you need to train deeper gcns},
  author={Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
  journal={arXiv preprint arXiv:2006.07739},
  year={2020}
}
@inproceedings{li2019deepgcns,
  title={Deepgcns: Can gcns go as deep as cnns?},
  author={Li, Guohao and Muller, Matthias and Thabet, Ali and Ghanem, Bernard},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9267--9276},
  year={2019}
}
@article{bianco2018benchmark,
  title={Benchmark analysis of representative deep neural network architectures},
  author={Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
  journal={IEEE Access},
  volume={6},
  pages={64270--64277},
  year={2018},
  publisher={IEEE}
}
@inproceedings{hu2020featgraph,
  title={Featgraph: A flexible and efficient backend for graph neural network systems},
  author={Hu, Yuwei and Ye, Zihao and Wang, Minjie and Yu, Jiali and Zheng, Da and Li, Mu and Zhang, Zheng and Zhang, Zhiru and Wang, Yida},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--13},
  year={2020},
  organization={IEEE}
}
@article{goldberg2017neural,
  title={Neural network methods for natural language processing},
  author={Goldberg, Yoav},
  journal={Synthesis lectures on human language technologies},
  volume={10},
  number={1},
  pages={1--309},
  year={2017},
  publisher={Morgan \& Claypool Publishers}
}
@inproceedings{qi20173d,
  title={3d graph neural networks for rgbd semantic segmentation},
  author={Qi, Xiaojuan and Liao, Renjie and Jia, Jiaya and Fidler, Sanja and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={5199--5208},
  year={2017}
}
@article{bulucc2016recent,
  title={Recent advances in graph partitioning},
  author={Bulu{\c{c}}, Ayd{\i}n and Meyerhenke, Henning and Safro, Ilya and Sanders, Peter and Schulz, Christian},
  journal={Algorithm engineering},
  pages={117--158},
  year={2016},
  publisher={Springer}
}
@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}
@inproceedings{li2014scaling,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  pages={583--598},
  year={2014}
}
@article{jia2018beyond,
  title={Beyond data and model parallelism for deep neural networks},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={arXiv preprint arXiv:1807.05358},
  year={2018}
}
@inproceedings{gandhi2021p3,
  title={P3: Distributed Deep Graph Learning at Scale},
  author={Gandhi, Swapnil and Iyer, Anand Padmanabha},
  booktitle={15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
  pages={551--568},
  year={2021}
}
@article{zhang2020agl,
  title={Agl: a scalable system for industrial-purpose graph machine learning},
  author={Zhang, Dalong and Huang, Xin and Liu, Ziqi and Hu, Zhiyang and Song, Xianzheng and Ge, Zhibang and Zhang, Zhiqiang and Wang, Lin and Zhou, Jun and Shuang, Yang and others},
  journal={arXiv preprint arXiv:2003.02454},
  year={2020}
}

@misc{Pytorch-repo,
  author       = {},
  title        = {Pytorch},
  howpublished = {\url{https://pytorch.org/}},
}

@article{quiver,
  title={Quiver: Supporting gpus for low-latency, high-throughput gnn serving with workload awareness},
  author={Tan, Zeyuan and Yuan, Xiulong and He, Congjie and Sit, Man-Kit and Li, Guo and Liu, Xiaoze and Ai, Baole and Zeng, Kai and Pietzuch, Peter and Mai, Luo},
  journal={arXiv preprint arXiv:2305.10863},
  year={2023}
}

@misc{NVLink,
  author       = {},
  title        = {NVLink},
  howpublished = {\url{https://www.nvidia.com/en-us/data-center/nvlink/}},
  year = 2022
}

@misc{Tensorflow-repo,
  author       = {},
  title        = {Tensorflow},
  howpublished = {\url{https://www.tensorflow.org/}}
}

@inproceedings{hamilton2017inductive,
  title={Inductive representation learning on large graphs},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={1025--1035},
  year={2017}
}
@article{hu2020open,
  title={Open Graph Benchmark: Datasets for machine learning on graphs},
  author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  journal={arXiv preprint arXiv:2005.00687},
  year={2020}
}



@article{waleffe2022marius++,
  title={Marius++: Large-Scale Training of Graph Neural Networks on a Single Machine},
  author={Waleffe, Roger and Mohoney, Jason and Rekatsinas, Theodoros and Venkataraman, Shivaram},
  journal={arXiv preprint arXiv:2202.02365},
  year={2022}
}

@inproceedings{neugraph,
  title={Neugraph: parallel deep neural network computation on large graphs},
  author={Ma, Lingxiao and Yang, Zhi and Miao, Youshan and Xue, Jilong and Wu, Ming and Zhou, Lidong and Dai, Yafei},
  booktitle={2019 {USENIX} Annual Technical Conference ({USENIX ATC} 19)},
  pages={443--458},
  year={2019}
}
@article{zeng2019graphsaint,
  title={Graphsaint: Graph sampling based inductive learning method},
  author={Zeng, Hanqing and Zhou, Hongkuan and Srivastava, Ajitesh and Kannan, Rajgopal and Prasanna, Viktor},
  journal={arXiv preprint arXiv:1907.04931},
  year={2019}
}
@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={103--112},
  year={2019}
}

@CONTROL{REVTEX42Control}
@CONTROL{apsrev42Control,author="00",pages="1",title="0",year="0"}

@article{fleet,
  title={FLEET: Flexible Efficient Ensemble Training for Heterogeneous Deep Neural Networks},
  author={Guan, Hui and Mokadam, Laxmikant Kishor and Shen, Xipeng and Lim, Seung-Hwan and Patton, Robert},
  booktitle={Third Conference on Machine Learning and Systems (MLSys)},
  year={2020}
}
@inproceedings{ning2019adaptive,
  title={Adaptive deep reuse: Accelerating cnn training on the fly},
  author={Ning, Lin and Guan, Hui and Shen, Xipeng},
  booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)},
  pages={1538--1549},
  year={2019},
  organization={IEEE}
}
@inproceedings{pittman2018exploring,
  title={Exploring flexible communications for streamlining DNN ensemble training pipelines},
  author={Pittman, Randall and Guan, Hui and Shen, Xipeng and Lim, Seung-Hwan and Patton, Robert M},
  booktitle={International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  pages={807--818},
  year={2018},
  organization={IEEE}
}
@article{guan2021cocopie,
  title={CoCoPIE: enabling real-time AI on off-the-shelf mobile devices via compression-compilation co-design},
  author={Guan, Hui and Liu, Shaoshan and Ma, Xiaolong and Niu, Wei and Ren, Bin and Shen, Xipeng and Wang, Yanzhi and Zhao, Pu},
  journal={Communications of the ACM},
  volume={64},
  number={6},
  pages={62--68},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@inproceedings{guan2019wootz,
  title={Wootz: A compiler-based framework for fast CNN pruning via composability},
  author={Guan, Hui and Shen, Xipeng and Lim, Seung-Hwan},
  booktitle={Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  pages={717--730},
  year={2019}
}
@inproceedings{ding2017generalizations,
  title={Generalizations of the theory and deployment of triangular inequality for compiler-based strength reduction},
  author={Ding, Yufei and Ning, Lin and Guan, Hui and Shen, Xipeng},
  booktitle={Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages={33--48},
  year={2017}
}
@inproceedings{guan2021rnn,
  title={Recurrent Neural Networks Meet Context-Free Grammar: Two Birds with One Stone},
  author={Guan, Hui  and  Chaudhary, Umang and  Xu, Yuanchao and Ning, Lin  and Zhang, Lijun  and  Shen, Xipeng},
  booktitle={IEEE International Conference on Data Mining},
  year={2021}
}
@inproceedings{shaurya2021freelunch,
  title={FreeLunch: Compression-based GPU Memory Management for Convolutional Neural Networks},
  author={ Patel, Shaurya and  Liu, Tongping and  Guan, Hui},
  booktitle={Workshop on Memory Centric High Performance Computing},
  year={2021}
}

@inproceedings{pkg,
        Author = {{Uddin Nasir}, Muhammad Anis and {De Francisci Morales}, Gianmarco and Garcia-Soriano, David and Kourtellis, Nicolas and Serafini, Marco},
        Booktitle = {ICDE '15: 31st International Conference on Data Engineering},
        Date-Added = {2015-01-17 18:34:48 +0000},
        Date-Modified = {2015-01-17 18:35:46 +0000},
        Title = {{The Power of Both Choices: Practical Load Balancing for Distributed Stream Processing Engines}},
        Year = {2015}
}

@inproceedings{pkg-2,
        Author = {{Uddin Nasir}, Muhammad Anis and {De Francisci Morales}, Gianmarco and Kourtellis, Nicolas and Serafini, Marco},
        Booktitle = {ICDE '16: 32nd International Conference on Data Engineering},
        Title = {{When Two Choices Are not Enough: Balancing at Scale in Distributed Stream Processing}},
        Year = {2016}
}

@misc{zookeeper-website,
  author       = {},
  title        = {Apache Zookeeper},
  howpublished = {\url{https://zookeeper.apache.org}},
}

@misc{storm-website,
  author       = {},
  title        = {Apache Storm},
  howpublished = {\url{https://storm.apache.org}},
}

@misc{pkg-storm,
  author       = {},
  title        = {Apache Storm - Concepts},
  howpublished = {\url{https://storm.apache.org/releases/current/Concepts.html}},
}

@misc{spaulding,
  author       = {},
  title        = {Spaulding-Smith Fellows},
  howpublished = {\url{https://www.umass.edu/graduate/inclusion/spaulding-smith-fellows}},
}

@inproceedings{zab,
  title={Zab: High-performance broadcast for primary-backup systems},
  author={Junqueira, Flavio P and Reed, Benjamin C and Serafini, Marco},
  booktitle={Dependable Systems \& Networks (DSN), 2011 IEEE/IFIP 41st International Conference on},
  pages={245--256},
  year={2011},
  organization={IEEE}
}

@article{accordion,
  title={Accordion: Elastic Scalability for Database Systems Supporting Distributed Transactions},
  author={Marco Serafini and Essam Mansour and Ashraf Aboulnaga and Kenneth Salem and Taha Rafiq and Umar Farooq Minhas},
  journal={PVLDB},
  year={2014}
}

@article{estore,
  title={E-store: Fine-grained elastic partitioning for distributed transaction processing systems},
  author={Taft, Rebecca and Mansour, Essam and Serafini, Marco and Duggan, Jennie and Elmore, Aaron J and Aboulnaga, Ashraf and Pavlo, Andrew and Stonebraker, Michael},
  journal={PVLDB},
  year={2014}
}

@article{clay,
  title={Clay: fine-grained adaptive partitioning for general database schemas},
  author={Serafini, Marco and Taft, Rebecca and Elmore, Aaron J and Pavlo, Andrew and Aboulnaga, Ashraf and Stonebraker, Michael},
  journal={Proceedings of the VLDB Endowment},
  volume={10},
  number={4},
  pages={445--456},
  year={2016},
  publisher={VLDB Endowment}
}

@article{flexpushdown,
  title={FlexPushdownDB: Hybrid Pushdown and Caching in a Cloud DBMS},
  author={Yifei Yang and Matt Youill and Matthew Woicik and Yizhou Liu and Xiangyao Yu and Marco Serafini and Ashraf Aboulnaga and Michael Stonebraker},
  journal={Proceedings of the VLDB Endowment},
  year={2021},
  publisher={VLDB Endowment}
}

@inproceedings{gnnlab,
  title={GNNLab: a factored system for sample-based GNN training over GPUs},
  author={Yang, Jianbang and Tang, Dahai and Song, Xiaoniu and Wang, Lei and Yin, Qiang and Chen, Rong and Yu, Wenyuan and Zhou, Jingren},
  booktitle={Proceedings of the Seventeenth European Conference on Computer Systems},
  pages={417--434},
  year={2022}
}

@inproceedings{flexflow,
  title={Flexflow: A flexible dataflow accelerator architecture for convolutional neural networks},
  author={Lu, Wenyan and Yan, Guihai and Li, Jiajun and Gong, Shijun and Han, Yinhe and Li, Xiaowei},
  booktitle={2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={553--564},
  year={2017},
  organization={IEEE}
}

@inproceedings{kim2021accelerating,
  title={Accelerating gnn training with locality-aware partial execution},
  author={Kim, Taehyun and Hwang, Changho and Park, KyoungSoo and Lin, Zhiqi and Cheng, Peng and Miao, Youshan and Ma, Lingxiao and Xiong, Yongqiang},
  booktitle={Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems},
  pages={34--41},
  year={2021}
}

@article{UVA-GNN-load,
author = {Min, Seung Won and Wu, Kun and Huang, Sitao and Hidayeto\u{g}lu, Mert and Xiong, Jinjun and Ebrahimi, Eiman and Chen, Deming and Hwu, Wen-mei},
title = {Large Graph Convolutional Network Training with GPU-Oriented Data Communication Architecture},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {11},
journal = {Proc. VLDB Endow.},
month = {oct},
pages = {2087–2100},
numpages = {14}
}

@inproceedings{flexgraph,
  title={FlexGraph: a flexible and efficient distributed framework for GNN training},
  author={Wang, Lei and Yin, Qiang and Tian, Chao and Yang, Jianbang and Chen, Rong and Yu, Wenyuan and Yao, Zihang and Zhou, Jingren},
  booktitle={Proceedings of the Sixteenth European Conference on Computer Systems},
  pages={67--82},
  year={2021}
}

@article{shao2022distributed,
  title={Distributed Graph Neural Network Training: A Survey},
  author={Shao, Yingxia and Li, Hongzheng and Gu, Xizhi and Yin, Hongbo and Li, Yawen and Miao, Xupeng and Zhang, Wentao and Cui, Bin and Chen, Lei},
  journal={arXiv preprint arXiv:2211.00216},
  year={2022}
}