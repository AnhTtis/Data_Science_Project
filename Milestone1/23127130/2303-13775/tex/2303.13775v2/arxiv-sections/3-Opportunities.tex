\section{\name: Overview} \label{sec:opportunities}
Our work is based on the observation that it is possible to eliminate redundant data loads \emph{and} reduce end-to-end training times by using the right type of hybrid parallelism.
Instead of sampling micro-batches that have overlapping input vertices, one micro-batch for each GPU, the sampling phase can prepare non-overlapping \textit{splits} and assign each split to a specific GPU.
The splits are obtained running an online \emph{splitting algorithm} while sampling each mini-batch at each iteration.
Each GPU loads only the features of the vertices within its assigned splits.
Splits must be consistent with the location of cached features to benefit from them.
During training, the hidden features of a vertex are computed only by one GPU and then shuffled to other GPUs.
We refer to this parallelism technique as \textit{\tname}.
% The key challenge in making \tname effective is to devise a lightweight splitting algorithm that minimizes shuffling cost and balances load among GPUs.


Figure~\ref{fig:comparison}(c) illustrates how \tadj training reduces input feature loads. 
Data parallelism requires loading eight feature vectors into the GPUs before the training phase, four for each GPU, of which 3 are loaded redundantly.
\Tname eliminates the redundant loads and requires loading only five feature vectors (three for GPU 1 and two for GPU 2). 
Push-pull parallelism, illustrated in Figure~\ref{fig:comparison}(b), requires no data loads if the graph is fully cached.
However, it shuffles partial activations across GPUs for eight edges (the number of edges in the bottom layer of the GNN), whereas \tname shuffles vertex features over three edges (the number of vertices that lie in the boundaries between splits). 

\Tname also avoids the redundant computation of hidden features that occurs in data parallelism.
This reduction in computational cost helps compensate for the cost of shuffling and sometimes offsets it.
For example the hidden features of vertex $c$ in Figure~\ref{fig:comparison}(a) are computed by both GPUs with data parallelism, but with split parallelism, they are computed only by GPU 1 (see Figure~\ref{fig:comparison}(c)).
When used for sampling, \tname also avoids redundant computation in the sampling step.

In the following, we describe how we embodied \tname into the \name system (Section~\ref{sec:overview}).
We describe \name's novel lightweight online splitting algorithm, which provides probabilistic performance guarantees, in Section~\ref{sec:workload}.
\name also offers a programming API that supports efficient single-GPU kernels for sampling and training, as discussed in Section~\ref{sec:api}. 

% Next, we illustrate how we faced the technical challenges involved in implementing \name. 
% The first challenge is to devise a lightweight online splitting algorithm that can provide probabilistic performance guarantees (Section~\ref{sec:workload}).
% The second is to find an appropriate programming API to support efficient single-GPU kernels for sampling and training (Section~\ref{sec:api}).

\begin{comment}
Materializing split parallelism, however, faces two research challenges. 
\begin{itemize}
    \item[RQ1] 
    Recent research has developed optimized single-GPU kernels for GNN sampling \ms{refs} and training \ms{refs}.
    Data-parallelism runs single-GPU code and kernels on multiple GPUs as-is, in an embarrassingly parallel manner.
    Split parallelism introduces coordination across multiple GPUs.
    An appropriate API to abstract away coordination can facilitate running existing optimized kernels with split parallelism. 
    \item[RQ2] 
    The second challenge is the efficient implementation of cross-GPU coordination, which requires the design of new data structures to enable effective mapping between vertices IDs in splits assigned to different GPUs. 
    % The second challenge is to minimize the run-time overhead caused by split parallelism during sampling and training.  
    % \hui{fill in the research challenge, add 1-2 sentences to elaborate on why this is important to address}
\end{itemize}
This work addresses the two challenges and developed a GNN training system called \name{} that implements split parallelism. We next provide an overview of the system \name{} in Section~\ref{sec:overview} and then elaborate on proposed solutions to address the two challenges in Section~\ref{sec:api}. 
\end{comment}
