\section{Related Work}

\mypar{Mini-batch data-parallel systems}
DistDGL~\citep{distdgl} and AliGraph~\citep{aligraph} use data-parallel mini-batch training to scale to large graphs.
ByteGNN optimizes distributed sampling for CPU-based data-parallel training~\citep{bytegnn}.
% 
Another research direction is taken by Marius++, which runs data-parallel GNN training on large graphs using a single GPU and out-of-core approach rather than a distributed system~\citep{waleffe2022marius++}.

Some prior work has focused on speeding up sampling.
Some systems propose programming APIs and runtimes to speed up single-GPU sampling~\citep{nextdoor,c-saw,wang2021skywalker,gsampler}.
GNNLab uses a factorized approach, where sampler GPUs cache the entire graph topology while other trainer GPUs only cache input features~\citep{gnnlab}.
Other systems propose caching both topological and feature data on each GPU~\citep{legion,ducati,dsp}.
DSP supports distributed caching of the graph structure but performs two all-to-all shuffles for each sampled layer due to its use of data parallelism~\citep{dsp}.
\Tadj sampling requires only one all-to-all shuffle per sampled layer.

Other works propose complementary optimizations to our work.
Some systems process multiple mini-batches in parallel using pipelining to increase resource utilization and reduce training time~\citep{UVA-GNN-load,dsp}.
Pipelining reduces the amount of GPU memory available per mini-batch and for caching since each pipelined mini-batch needs its own GPU memory space to store samples, input features, and activations.
This can be a challenge with complex models like GAT, which can require a large amount of memory even for a single mini-batch.
UGache proposes techniques to optimize NVLink communication and determine the content of the caches~\citep{ugache}. 

\mypar{Sampling algorithms}
In our work, we consider general systems that support arbitrary sampling algorithms rather than imposing a specific algorithm, with its specific performance and accuracy tradeoffs, to the user.
A different line of work has focused on designing specific sampling algorithms that speed up GNN training and reduce data transfers at the cost of potentially biasing accuracy~\citep{dong2021global,twolevel, ramezani2020gcn,liu2023bgl}. 
After \tname and cooperative training were introduced in a preliminary version of this paper~\citep{gsplit-arxiv},
% After \tname and cooperative training were first introduced in a pre-print~\citep{gsplit-arxiv-anon},
subsequent work proposed sampling algorithms to increase temporal feature access locality~\citep{balin2023cooperative}.
MariusGNN~\citep{waleffe2022marius++} proposes a sampling algorithm to improve the efficiency of out-of-core sampling.

\mypar{Hybrid parallelism in full-graph training}
Hybrid parallelism has been used in many full-graph training systems, which have different workloads and optimization opportunities than mini-batch training systems~\citep{roc, neugraph, cai2021dgcl, md2021distgnn, pipegcn, mgg, betty, G3, neutronstar, hongtu}.
In full-graph training, each epoch operates on the same static input graph, which is leveraged by performing offline optimizations.
% Some systems deal with the limited memory of the GPUs, which cannot store the entire full graph, and process only some chunks of the graph at a time~\citep{roc, neugraph, betty, G3, hongtu}.
% Others mitigate the large communication cost of transferring the hidden features for the entire graph by overlapping communication with computation~\citep{pipegcn,mgg,G3} or minimizing the communication cost in distributed multi-host systems~\citep{cai2021dgcl,neutronstar,G3}.
These systems organize the computation and communication of each epoch into much larger chunks than mini-batch training systems.
They can thus amortize the cost of more complex scheduling algorithms that overlap computation and communication.
Other works propose specific sampling algorithms to reduce the communication cost of distributed full-graph training~\citep{wan2022bns,song2023adgnn}.
This sampling occurs once per epoch, much less frequently than in mini-batch training where sampling occurs at every iteration.
