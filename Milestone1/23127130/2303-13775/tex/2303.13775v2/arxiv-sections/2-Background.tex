\section{Background and Motivation}
This section first introduces mini-batch GNN training, 
% in Section~\ref{sec:background}.
and then 
% discusses the data loading-induced performance bottleneck and 
elaborates on the limitations of existing optimizations. 
% , such as caching and hybrid parallelism, 
% in Section~\ref{sec:problem}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/mini-batch.pdf}
    \caption{Example of a mini-batch.}
    \label{fig:minibatch}
\end{figure}

\subsection{Mini-Batch GNN Training and Data Parallelism} 
A GNN model is defined as a sequence of \emph{GNN layers}.\footnote{In the following, the term ``layer'' will refer to GNN layers, not to neural network layers unless otherwise stated.}
During each mini-batch training iteration, there are three phases: \textit{sampling}, \textit{loading}, and \textit{training}.
The sampling phase randomly selects a mini-batch starting from the target vertices.
A mini-batch with two target vertices is shown in Figure~\ref{fig:comparison}(a). 
In the loading phase, the input features of the vertices in the bottom layer of the mini-batch are loaded into the GPUs.
During forward propagation, each GNN layer $l > 0$ aggregates and transforms the features of the vertices in the layer $l-1$ of the sample and produces the features of the vertices in the layer $l$ (see Figure~\ref{fig:minibatch}).
The last GNN layer computes the features of the target vertices, which are then used to calculate the loss. 
During backward propagation, the layers are executed in reverse order to compute gradients. 
Finally, all GPUs aggregate and apply the computed gradients.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{results/breakdown/orkut_epoch_breakdown.pdf}
        \caption{The fraction of sampling, loading, and training time per epoch of DGL, P3* and Quiver on Orkut with the GAT model.}
        \label{fig:orkut_epoch_breakdown}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\textwidth]{results/breakdown/quiver_epoch_breakdown.pdf}
        \caption{The percentage of sampling, loading, and training time per epoch of Quiver on Orkut and Papers100M with the GraphSage model. }
        \label{fig:quiver_epoch_breakdown}
    \end{subfigure}
    \caption{Epoch time breakdown of existing systems on a four-GPU host with NVLink.}
    \label{fig:epoch_breakdown}
\end{figure}

% \begin{figure}[h!]
%     \centering
%     \advance\leftskip-1cm
% % \advance\rightskip-3cm
% \includegraphics[keepaspectratio=true,width=.5\textwidth]{results/epoch_breakdown/sage_epoch_breakdown_arxiv.png}
%     % \includegraphics[width=0.85\columnwidth]{results/epoch_breakdown/sage_epoch_breakdown.png}
%      \caption{The fraction of sampling, loading, and training time per epoch of existing systems when training the GraphSage model on a four GPU host with NVLink. \ms{New figures. This figure is out of bounds. Larger fonts. Edit text as needed}
%      %DGL and P3 cache no feature data in all experiments. Quiver caches 44\% for Papers100M, and 12\% percent for Friendster.  All systems use the GAT model with fanouts [15, 15, 15] and batch size 1024. 
%      }
%     \label{fig:epoch_breakdown}
% \end{figure}

Data parallelism is the most commonly used training strategy for mini-batch GNN training. 
In data parallel training, the target vertices are partitioned among GPUs, where each partition corresponds to a separate \textit{micro-batch} (see Figure~\ref{fig:comparison}(a)). 
Each GPU independently loads the input features of all the vertices in the bottom layer of its micro-batch and trains on it.
This approach has two limitations: a high cost of data loading and a high degree of computational and data loading redundancy.

% Instead of creating multiple independent and overlapping micro-batches as done by data parallelism, \tname generates a single mini-batch for all the target vertices and splits it without overlaps, avoiding redundant computation and loads.

\mypar{Data loading bottleneck}
Input feature loading is a major overhead in data-parallel GNN training, which contributes to a large fraction of the total training time and prevents a good utilization of the GPUs.
% When training a GNN using a mini-batch with N target vertices, the GPUs need to load the feature vectors of the vertices in the k-hop neighborhood of these target vertices into their memory. 
% 
Figure \ref{fig:orkut_epoch_breakdown} shows the time breakdown of sampling, feature loading, and forward/backward pass per epoch with three GNN systems: DGL, Quiver, and P3*.
We will initially focus on DGL, which is a standard data-parallel baseline, and discuss optimizations shortly.
% We observe that data loading can take up to 69\% of the epoch time for DGL, 48\% for P3*, and 79\% for Quiver. 
We observe that data loading can take more than 60\% of the epoch time for DGL. 
This data loading-induced performance bottleneck is also reported in other GNN training literature~\citep{quiver, pagraph, wholegraph,ugache}. 

\mypar{Redundant loading and computation}
Table~\ref{tab:redundancy} further reports the degree of computational and data loading redundancy in data-parallel training.
With 4 GPUs, data parallelism creates 4 separate micro-batches (``Micro''), causing up to $1.2\times$ compute and $2.5\times$ feature loading compared to having only a single mini-batch (``Mini'').

\input{tables/compute_redundancy}



% For example, in Figure~\ref{fig:comparison}(a), although there are only two target vertices in that mini-batch, a total of eight input features need to be loaded into the GPU memory for forward and backward propagation on a 2-layer GNN. 

% The data loading phase can take up a significant portion of GNN training time and even become the performance bottleneck, depending on the system, the graph characteristics, and the GNN model.


\subsection{Limitations of Existing Optimizations} \label{sec:problem}

\begin{comment}
Table~\ref{tab:loading-bottleneck} shows the time spent on the three phases, sampling, data loading, and training, when training two GraphSAGE and GAT models on two graph datasets, Papers100M and Orkut, using DGL~\citep{dgl} on a four-GPU server with NVLink.
DGL is one of the well-established GNN training frameworks that support mini-batch data parallel training. 
Details on the experiment settings are in Section~\ref{sec:settings}.
% DGL doesn't support caching only a part of a graph in GPU memory and both graphs we consider are too big to fit. 
From the table, we observe that data loading can take up to XX\% of the epoch time. \ms{update number}
This data loading-induced performance bottleneck is also reported in other GNN training literature~\citep{quiver, pagraph}. 

\begin{table*}[t]
\small 
\begin{tabular}{|c||c|c||c|c|c|c||c|c|c|c|}
\hline 
\multirow{2}{*}{ Graph }  & \multirow{2}{*}{ System}  & \multirow{2}{*}{ Cache \%} &\multicolumn{4}{|c||}{SAGE} & \multicolumn{4}{|c|}{GAT} \\
\cline{4-11}
 & & & S  & L  & FB & Total & S  & L  & FB  & Total  \\  
\hline \hline  
 & DGL & no cache & 6.57 & 14.46 & 11.89 & 32.92 & 6.36 & 14.42 & 31.36 & 52.14\\
Papers100M & Quiver & SAGE: 51\% - GAT: 44\%  &11.17 & 17.23 & 8.99 & 37.38 & 11.19 & 22.09 & 25.06 & 58.33 \\
& $P^{3*}$ & OOM & OOM & OOM & OOM & OOM & OOM & OOM & OOM & OOM \\
\hline
& DGL & no cache &0.96 & 96.59 & 13.29 & 110.84 & 0.98 & 96.32 & 17.55 & 114.85 \\
Orkut & Quiver & 100\% & 4.08 & 6.85 & 12.81 & 23.74 & 3.83 & 6.91 & 17.44 & 28.18\\
& $P^{3*}$ & 100\% & 0.97 & 2 & 15.72 & 18.68 & 1.01 & 1.83 & 25.89 & 28.73 \\
\hline
\end{tabular}
\caption{Data loading bottleneck on a system with NVLink. 
\ms{Explain S/L/T once finalized. make it look less similar to the evaluation results (since they are the same numbers). }
}
\label{tab:loading-bottleneck}
\end{table*}
% 
\end{comment}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.49\columnwidth]{results/epoch_breakdown/Papers100M_epoch_breakdown.png}
%     \includegraphics[width=0.49\columnwidth]{results/epoch_breakdown/Friendster_epoch_breakdown.png}

%      \caption{The fraction of sampling, loading, and training time per epoch of existing systems when training GAT on a 4 GPU host with NVLink. 
%      %DGL and P3 cache no feature data in all experiments. Quiver caches 44\% for Papers100M, and 12\% percent for Friendster.  All systems use the GAT model with fanouts [15, 15, 15] and batch size 1024. 
%      }
%     \label{fig:epoch_breakdown}
% \end{figure}


% \subsection{Existing Optimizations} \label{sec:limitations}
% \mypar{Existing optimizations}
Many approaches have been proposed to address the data loading-induced performance bottleneck of mini-batch training. 
These approaches fall broadly into three categories: caching~\citep{pagraph, gnnlab, quiver, wholegraph}, hybrid parallelism~\citep{gandhi2021p3}, and algorithmic optimizations~\citep{dong2021global,twolevel, ramezani2020gcn,liu2023bgl}. 
In this work, 
% we do not consider algorithmic optimizations that impose approximation choices onto the users and impact model accuracy. 
we focus on optimizations that can be used in general-purpose GNN training systems that scale to multiple GPUs without imposing modeling choices that can impact the model accuracy or semantics, such as using specific sampling algorithms or relaxing synchrony.

We now discuss caching and hybrid parallelism approaches 
% for single-host mini-batch training 
and show that they still suffer from high data loading costs.
Besides caching, none of these solutions addresses the fundamental problem of redundant loading and computation that is inherent in data parallelism.

\mypar{Limitations of data-parallel caching}
To reduce data loading time, several systems maintain a static cache in the main memory of the GPUs. 
This cache is populated offline with frequently-accessed input features~\citep{pagraph, gnnlab}.
The latest systems use a distributed shared memory to enable GPUs to fetch features from other GPUs' memory using fast GPU-to-GPU interconnects like NVLink \citep{quiver, dsp, ugache}.
As shown in Figure~\ref{fig:orkut_epoch_breakdown}, the distributed shared-memory caching mechanism in Quiver~\citep{quiver} can reduce loading time for the Orkut graph, whose features are too large to fit in a single GPU memory but can be fully cached across multiple GPUs. 

Distributed caching, however, does not fully address the performance issue of data loading, which can still take a significant fraction of the epoch time as shown in Figure~\ref{fig:quiver_epoch_breakdown}.
For the GraphSage model, data loading over NVLink can be relatively expensive for Quiver with a graph that can be fully cached like Orkut.
For larger graphs such as Papers100M, only a part of input features can be cached on GPUs. 
Data loading can still stress the PCIe bus between the host and devices, resulting in unsatisfactory training performance. 
With Papers100M, only up to 60\% of the input features can be cached and the data loading time remains high.
%We fixed this bug of Quiver and termed the improved system DistCache. DGL can still outperform DistCache}
%Even with Orkut, which can be fully cached in the distributed GPU memory, Quiver still needs to spend up to XX\% of the time loading data.\ms{update numbers}

\begin{comment}
Table~\ref{tab:loading-bottleneck} reports the maximum percentage of features that can be cached on a four-GPU server with NVLink interconnect. 
With Friendster, only up to XX\% of the input features can be cached and the data loading time remains high (up to 14\% of the epoch time).  
Even with Orkut, which can be fully cached in the distributed GPU memory, Quiver still needs to spend up to XX\% of the time loading data.\ms{update numbers}
\end{comment}


\begin{comment}    
These experiments consider a system equipped with NVLink.
Without NVLink, data loads from the host memory or other GPUs always need to occur over PCIe.
This makes distributed GPU caching not effective, as we will show in our experiments.
\ms{Review this later. PCIe numbers are still inconclusive.}
\end{comment}

% \input{tables/compute_redundancy}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/overview.drawio.pdf}
    \vspace{-5mm}
    \caption{Overview of the \name training pipeline.}
    \label{fig:overview}
\end{figure*}

\mypar{Limitations of existing hybrid parallelism} 
The alternative to data-parallelism for mini-batch GNN training is called \emph{push-pull parallelism} proposed by the P3 system~\citep{gandhi2021p3}.
P3 targets distributed multi-host systems and aims to avoid transferring input features among hosts.
Each host keeps a slice of the feature vector of each vertex in host memory.
Like in data-parallel training, each GPU is associated with a micro-batch.
However, in push-pull parallelism, each GPU computes the input layer of \emph{all} micro-batches on its feature slice.
GPUs then exchange partial activations and continue the iteration in a data-parallel fashion.

$P^3$ is a multi-host system that does not use GPUs for caching, so it was not previously evaluated in a single-host multi-GPU setting.
To fill this gap, we have implemented its push-pull parallelism approach in a single-host multi-GPU setting.
We call this implementation P3*.
Figure~\ref{fig:comparison}(b) illustrates how P3* applies hybrid parallelism. 
Like the original $P^3$, this implementation partitions the cached features among GPUs at the cost of a cross-GPU push-pull shuffle.
For the Orkut graph, which can be fully cached, P3* does not exchange features among GPUs during the loading time, as Quiver does.
Instead, it pushes the bottom layer of all micro-batches to all GPUs, which induces a much lower loading cost.
The additional overhead of push-pull shuffling outweighs the gains in terms of loading time during the forward and backward pass, which results in higher overall training costs (see Figure~\ref{fig:orkut_epoch_breakdown}).
For other graphs that cannot be fully cached, P3* loads all the features in the mini-batch.
The training time still is higher than the other two systems due to the cost of shuffling.

\begin{comment}
\mypar{Implications for Training Cost}
One of the most important reasons for optimizing the training time of ML models is to reduce the dollar cost of training.
This depends not only on the training time but also on the type of server that is used for training.
GNN models are very small and they can be easily replicated at each GPU, unlike for example large language models that need to be partitioned among multiple GPUs.
For example, the models we consider range around XXX \ms{report size}.
The computation in GNN training is also relatively lightweight.
Therefore, the benefit of using high-end servers with NVLink mainly comes from mitigating the data loading bottleneck.
However, are these speedups sufficient to justify the additional cost of using high-end GPU servers for GNN training?

To answer this question, we consider the cost of the two AWS instance types we used for our evaluation and evaluate the cost per epoch of each system.
We consider a p3.8xlarge instance with NVLink, which currently costs \$12.24 per hour and a g4dn.12xlarge without NVLink, which costs \$3.912 per hour.
The cost per epoch is a good metric to measure the overall training cost because all the systems we consider run the models unmodified using synchronous training and have similar convergence rates per epoch, as we validated experimentally.

Figure XXX shows the cost per epoch achieved by different systems. 
For the Papers100M graph, which cannot be entirely cached in the distributed GPU cache, running DGL on an instance \emph{without} NVLink results in a much lower cost per epoch than using a caching-based system on NVLink.
Using a cheaper instance is preferable unless minimizing the absolute training time is the main goal.

The Orkut graph can be entirely cached in the distributed GPU cache.
In this case, using NVLink has a similar cost per epoch as PCIe, so using a high-end server is preferable since it can speed up training at no extra cost.
\end{comment}

% \mypar{Summary and motivation}
% Data-parallel training suffers from a fundamental problem, which is \emph{redundant data loading}. 
% In each training iteration of data-parallel training, two micro-batches (prepared for two GPUs separately) can share the same input vertices due to the interconnected nature of graphs. 
% The input features of these overlapped vertices need to be loaded twice, one for each GPU. 
% Caching speeds up some of these data loads but it does not fundamentally solve the redundant loading problem.
% The push-pull parallelism approach proposed by P3 eliminates redundant data loads but it introduces an expensive shuffle during training that adds a significant cost to the end-to-end training time.
% This cost often outweighs its benefits compared to data parallelism with caching.
