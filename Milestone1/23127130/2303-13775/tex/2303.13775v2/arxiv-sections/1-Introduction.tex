\section{Introduction}
Graph neural networks (GNNs) are a rapidly emerging class of machine learning models that have demonstrated superior performance in various graph analytics tasks. 
% GNNs often handle input graphs that comprise millions of vertices and billions of edges~\citep{pinsage,hu2021ogblsc,sign_icml_grl2020, liu2023bgl}. 
%Notably, companies like Pinterest~\citep{pinsage} and Twitter~\citep{sign_icml_grl2020} have widely adopted GNNs to enhance user experiences. 
%Additionally, GNNs find applications in engineering and scientific domains such as computer vision~\citep{qi20173d}, natural language understanding~\citep{goldberg2017neural}, quantum chemistry~\citep{gilmer2017neural}, material design~\citep{zitnick2020introduction}, and drug discovery~\citep{gaudelet2020utilising}.
Widely used systems like DGL~\citep{dgl}, PyTorch Geometric~\citep{pytorch-geometric}, and AliGraph~\citep{aligraph} employ mini-batch training to scale GNN training on large graphs. 
%This approach involves dividing the training data into subsets known as mini-batches. 
%During each training \emph{iteration}, the model parameters (gradients) are updated based on a different mini-batch. 
To accelerate GNN model training, these systems utilize \emph{data parallelism} to execute each training iteration across multiple GPUs. 
At each iteration, multiple independent \textit{micro-batches} are sampled, one per GPU. 
The GNN model is replicated on each GPU, which independently load its micro-batch and compute gradients locally.
These micro-batches consist of a partition of the target vertices in the mini-batch along with a sample of their k-hop neighbors. 

% Sampling prevents the k-hop neighborhood from becoming excessively large. 


% % 3) Why existing solutions are insufficient?
One drawback of data-parallel training for GNNs is redundant data movements and computations (see Figure~\ref{fig:comparison}(a)).
This is because the k-hop neighbors of target vertices in different micro-batches overlap, so the same vertices can appear in multiple micro-batches. 
This redundancy leads to overheads in all three stages of a data-parallel training iteration: sampling, loading, and training. 
In particular, loading is known to be a significant overhead in data-parallel GNN training since each GPU must load the input features of all the vertices in its micro-batch, which can be large~\citep{pagraph}. 
%This overhead is exacerbated by the overlaps across micro-batches since the same vertices appear in micro-batches assigned to multiple GPUs.
%Redundancy also occurs during the sampling stage, where the neighbors of the vertices in the overlap are redundantly sampled to create independent micro-batches, and in the training stage, where the hidden features and gradients of these vertices are computed redundantly. 

Prior work on data-parallel training has proposed caching features in GPU memory to avoid data loads~\citep {pagraph,quiver,wholegraph}.
However, GPUs have limited memory, so only a subset of the features can be cached with larger graphs.
% Some prior work proposes sharding the cached features among multiple GPUs and loading cached data using fast GPU-to-GPU interconnects like NVLink that are available in high-end servers~\citep{quiver,wholegraph}.
% As in other data-parallel systems, GPUs still need to load data redundantly, which makes loading a major bottleneck even when all input features can be fetched using NVLink only.
%We show that due to this bottleneck, it is not necessarily cost-effective to use high-end NVLink servers to perform GNN training.
The $P^3$ system proposed the only alternative approach to data parallelism for mini-batch training, a form of hybrid parallelism called push-pull parallelism~\citep{gandhi2021p3}. 
The main idea is to eliminate input feature transfers among hosts by having each GPU store a slice of each input feature vector (see Figure~\ref{fig:comparison}(b)).
%$P^3$ targets distributed multi-host systems but a similar approach can be used in a multi-GPU setting if we assume that we can cache all input features in GPU memory.
The drawback of this approach is that all GNN layers except the bottom one are still executed in a data-parallel manner, which requires an expensive push-pull shuffle among all GPUs.
The cost of this shuffle increases the training time and often outweighs the gains in data loading.

% 4) What is the basic idea of our solution? What are the distinctive features of our solution?
In this paper, we propose using a different hybrid parallelism approach tailored to mini-batch training that eliminates redundant data loading and computation by sending computation to data instead of sending data to computation, as done by data parallelism.
During each training iteration, the sampling step samples a single mini-batch for all GPUs, divides it on the fly into non-overlapping partitions called \emph{splits} and assigns each split to a specific GPU (see Figure~\ref{fig:comparison}(c)).
Now, only one GPU is responsible for loading the input features of the split, which avoids redundant loads.
GPUs then cooperatively execute the training of an iteration on the same mini-batch. 
Each GPU operates only on the vertices within its assigned split and shuffles intermediate results with other GPUs at each GNN layer. 
The additional shuffling overhead is lower than in push-pull parallelism in $P^3$ and is counterbalanced, and sometimes offset, by eliminating redundant computation.
We refer to this parallelism technique as \emph{\tname}. 

% 5) What are the challenges to make our solution work well?
% 6) How do we address those challenges?
We implement \tname in \name, a scalable multi-GPU GNN training system.
\name delivers state-of-the-art performance in mini-batch GNN training by eliminating redundancy.
Applying \tname to mini-batch GNN training, however, requires solving several challenges.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/comparison2.drawio.pdf}
    \caption{Comparison between data parallel, push-pull parallel, and the proposed split parallel training.}
    \label{fig:comparison}
\end{figure*}


The primary novel technical challenge in split parallelism is devising a \emph{splitting algorithm} that satisfies three key requirements: (i) minimize the cost of shuffles by minimizing the volume of transferred data and balancing load to avoid stragglers (ii) be fast enough to be executed on-the-fly at each iteration without becoming a performance bottleneck, (iii) split vertices consistently with the location of their cached input features in GPU, if present.
A straightforward solution to the first requirement would be to run a min-cut graph partitioning algorithm online on each sampled mini-batch.
This, however, would not satisfy the second requirement since the partitioning problem is NP-hard and difficult to parallelize, nor the third, since it would not take into account where input features are cached.
Many full-graph training systems shuffle intermediate data among GPUs at each GNN layer and use sophisticated scheduling algorithms to minimize communication and balance load~\citep{roc, neugraph, cai2021dgcl, md2021distgnn, betty, pipegcn, mgg, G3}.
These algorithms, however, do not directly apply to splitting.
In full-graph training, the work to be performed at each epoch is known since the batch is static and training occurs on the whole graph.
A splitting algorithm, in contrast, must split sampled mini-batches on the fly at each iteration, which have a much shorter time frame than full-graph epochs.

%Compared to full-graph training systems~\citep{roc, neugraph, cai2021dgcl, md2021distgnn, betty, pipegcn, mgg, G3}, which schedule the same large batch at each epoch and can amortize the cost of complex scheduling algorithms, mini-batch training systems need to operate in much shorter iterations, with mini-batches that change randomly at each iteration, and with much lower computation and communication costs per iteration.
% Mini-batches must be split at each iteration, unlike in full-graph training, and the associated cost cannot be easily amortized during training, 

% A fast splitting algorithm for mini-batch training should ideally be embarrassingly parallel while minimizing communication costs. 
% assign each vertex in the mini-batch to its split independently from each other with constant time complexity.

To solve this problem, we propose a splitting algorithm that achieves a negligible online overhead by providing \emph{probabilistic} splitting guarantees: on a randomly sampled mini-batch, it \emph{provably} minimizes the \emph{expected} communication cost among splits and balances the \emph{expected} load per split.
The algorithm pushes its main computational complexity offline:
%, outside of the critical path of training, when 
it uses pre-sampling to obtain a weighted version of the input graph and then partitions it.
Online, the splitting algorithm uses the partitions to split the mini-batch on the fly in an embarrassingly parallel manner while it is sampled.
The partitions are also used to determine the location of the cached input features, ensuring that caches are consistent with splitting.
We prove that this approach achieves its desired probabilistic guarantees.
Compared to using a standard offline graph partitioning algorithm that does not provide probabilistic guarantees, our algorithm speeds up the end-to-end training time of our \name system by up to 1.7$\times$.
\name scales to distributed multi-host multi-GPU training and avoids the cost of expensive cross-host communication.
At each iteration, it partitions the target vertices in the mini-batch among the hosts.
It then uses \tname within each host and data parallelism across hosts.

% \ms{Main findings from our evaluation.}
% Data parallel systems balance load by assigning each micro-batch an equal number of target vertices. \juelin{We observe that this strategy leads to a skewed workload during \tname training. Instead, we assign vertices to each partition based on the likelihood of the vertices being sampled in a mini-batch. We empirically show that this strategy leads to an even workload for \tname training.}

% \Tname is a form of hybrid parallelism tailored to the sample-load-train iteration pipeline of mini-batch training, unlike hybrid parallel GNN training strategies for full-graph training on large graphs~\citep{roc, neugraph, cai2021dgcl, md2021distgnn, betty, pipegcn, mgg, G3}.
% Full-graph training systems operate on a single large batch and can run more complex scheduling algorithms to organize computation and communication into larger chunks, amortize their cost, and overlap computation and communication.
% Mini-batch training systems need to operate on a much shorter time scale, with much lower computation and communication costs per iteration, and on mini-batches that change randomly at each iteration.

Another challenge is to preserve the programming abstractions of data-parallel training systems.
% In data-parallel systems, each GPU performs sampling and training locally, without cooperation with other GPUs, running kernels in an embarrassingly parallel manner.
Besides simplifying development, preserving the programming abstractions of data-parallel training allows us to directly leverage work on optimizing single-GPU kernels for GNN sampling and training~\citep{adaptgear, fastkernel, wu2021seastar, nextdoor, gsampler, fu2022tlpgnn, ye2023sparsetir}.
% Single-GPU optimizations operate on one GNN layer at a time, where a layer is the set of vertices at the same depth in the mini- or micro-batch (see Figure~\ref{fig:minibatch}).
In data-parallelism, vertices in a micro-batch layer are always local, whereas in \tname a layer may contain a mix of local and remote vertices.
\name provides a \emph{split/shuffle} API that hides distribution and data shuffles.
Unlike the edge-centric API of $P^3$, \name's layer-centric API enables reusing existing single-GPU kernels as black boxes, including the ones of state-of-the-art attention-based models like GAT.
% \name's API automatically generates a data structure, called the shuffle index, to optimize shuffles at each iteration.
% The index is built using efficient dedicated kernels during sampling and then reused throughout the iteration.

% \name facilitates development and provides a \emph{split/shuffle} API that hides distribution.
% The API delegates optimizing the intra-layer parallelism to single-GPU sampling and training kernels and only controls data shuffles between layers.
% In data-parallelism, vertices in a micro-batch layer are always local, whereas in \tname a layer may contain a mix of local and remote vertices.
% Using \name's split/shuffle API, sampling and training code runs as if all the vertices in a layer were local, like in the single-GPU case.

% 7) What are the experimental results and conclusions?
Our evaluation across multiple large graphs and GNN models shows that \name outperforms the state-of-the-art systems like DGL~\citep{dgl} and Quiver~\citep{quiver} by up to 4.4$\times$ and 1.9$\times$ respectively (2.4$\times$ and 1.4$\times$ on average). 
We also implement and evaluate 
%for the first time 
the push-pull parallelism approach of $P^3$~\citep{gandhi2021p3} in a single-host multi-GPU system and show that \name outperforms it by up to 4.1$\times$ (2.4$\times$ on average).
\name's splitting algorithm is the key to achieving these speedups, and it is effective in balancing load among splits and reducing the cost of shuffles.
%We will make our source code publicly available before this work is published.

% 8) What are the list of contributions this work makes?
Overall, we make the following contributions:
\begin{itemize}
    \item We characterize the cost of data loading in mini-batch GNN training, including also $P^3$'s push-pull parallelism (Section~\ref{sec:problem}). 
    \item We propose \tname to eliminate redundant input feature loading and computation (Section~\ref{sec:opportunities}) and discuss how we embodied it in the \name system (Section~\ref{sec:overview}). 
    \item We propose a lightweight online splitting algorithm that uses a probabilistic approach to minimize the expected communication cost and balance the expected load per split at each iteration (Section~\ref{sec:workload}).
    \item We develop \name's split/shuffle API, which supports optimized single-GPU kernels for sampling and training (Section~\ref{sec:api}). 
\end{itemize}