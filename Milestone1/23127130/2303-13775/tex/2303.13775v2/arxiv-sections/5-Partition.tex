\section{The Splitting Algorithm} \label{sec:workload}
% Balancing load among GPUs is important in multi-GPU training.
% Data-parallel systems partition a mini-batch and balance load between GPUs simply by assigning a similar number of target vertices to each micro-batch before sampling starts.
% The hybrid push-pull parallelism proposed by $P^3$ pushes each micro-batch to each GPU at each iteration.
% It balances the load by partitioning the input features along the feature dimension so that each partition includes a slice of the features of all vertices.

\Tadj training must split each sampled mini-batch among multiple GPUs in a way that minimizes communication costs during shuffles and balances load across GPUs.
A na\"{i}ve approach would be to run a min-edge-cut graph partitioning algorithm on each mini-batch we sample.
However, this would be too computationally expensive, since splitting must be executed during the sampling step of each iteration, and is hard to parallelize across multiple GPUs.
\name uses an embarrassingly parallel online splitting algorithm that maps each vertex to a split independently from each other in constant time. 
It does so by providing \emph{probabilistic} guarantees, rather than deterministic ones: given a random mini-batch sample, it minimizes the \emph{expected} communication cost while balancing the \emph{expected} load per split.
Formally, the splitting algorithm solves the following problem.


% This problem is analogous to classic min-cut partitioning, but two aspects make the mini-batch splitting peculiar. 
% First, splitting must be fast as it is executed on each mini-batch.
% Running a graph partitioning algorithm online at each training iteration can introduce a performance bottleneck. 
% On the other hand, partitioning the entire input graph offline does not necessarily result in good splits for the mini-batches, which are randomly sampled online.
% Second, mini-batch splitting should be cache-aware to reduce data loading time. 
% Vertices at the bottom layer of the current mini-batch should be placed where their features are cached, which is determined offline before training.


\noindent\textbf{Problem definition.} (Mini-batch splitting problem)
\emph{
Let $M(V_M,E_M)$ be a sampled mini-batch of a graph $G(V_G,E_G)$, $S$ a set of splits, and $f_M:\: V_M \rightarrow S$ a splitting function that assigns each vertex in $V_M$ to a split.
Let $S_i$ be the set of vertices assigned to split $i$ by $f_M$, $X_i$ a random variable expressing the number of vertices at layer $l > 0$ in $S_i$, $Y$ a random variable expressing the number of edges in $E_M$ having endpoints in two different splits, and $\epsilon \geq 0$ a tunable constant.
The mini-batch splitting problem involves finding the splitting function $f_M$ that solves the following minimization problem:}

\begin{equation}
\begin{aligned}
\min_{f_M }\quad & \E[Y]\\
\textrm{s.t.} \quad & \forall i:\: \E[X_i] \leq (1 + \epsilon) \cdot \sum_{i \in [1,|S|]} \E[X_i] /|S|
\end{aligned}
\label{eqn:prob}
\end{equation}

The problem is to find a \emph{splitting function} $f_M$ that can be used online by the splitting algorithm to map each sampled vertex to a split, each corresponding to a different GPU device.
The random variables $X_i$ and $Y$ represent the computation and communication cost of the splits, respectively.
When the algorithm assigns a vertex at layer $l>0$ to a split, the corresponding GPU must sample its neighbors during the sampling phase and compute its hidden feature during the training phase.
Edges connecting vertices in different splits induce communication costs during the shuffle phases of both sampling and training. 
This problem is NP-hard since it can be reduced to min-edge-cut graph partitioning by selecting an appropriate sampling function.

\mypar{Splitting algorithm}
To avoid solving this NP-hard problem online, our approach is to reduce it to a problem that can be solved offline.
Therefore, we propose using a splitting algorithm that has an offline and an online part.
Offline, the algorithm finds a \emph{global partitioning function} $f_G:\: V_G \rightarrow D$ that statically maps each vertex in the whole input graph to a GPU device.
Then, online, the splitting algorithm uses $f_G$ as a substitute to $f_M$ to map each vertex to a device and thus to its corresponding split at each iteration.
Online splitting is embarrassingly parallel since $f_G$ maps each vertex to a split independently and does it in constant time.

\name uses the global partitioning $f_G$ also to determine the GPU where it statically caches input features of a vertex.
This ensures that the caches are consistent with the splits.

\mypar{Finding the global partitioning function}
We now describe the details of the offline part of the splitting algorithm, which finds the global partitioning function.
The first stage of the offline algorithm is pre-sampling, which weighs the vertices and edges of the input graph.
Weights represent the computational and communication costs incurred by GPUs during \tadj sampling and training.
The second stage uses a weighted min-edge-cut graph partitioning algorithm to find the global partitioning function.

Given an input graph $G$, the pre-sampling stage assigns weights to the vertices and edges of $G$ to obtain a weighted graph $G_w$.
It runs the same sampling algorithm used during training for a fixed number of epochs.
At each iteration, the algorithm samples the k-hop neighborhood of the training (target) vertices in the mini-batch. 
It then assigns to each vertex $v$ a weight $w_V(v) = k_v/N$, where $k_v$ is the total number of times $v$ is sampled at a layer $l > 0$ across all samples and $N$ is the number of samples.
The weight of each edge $e$ is $w_E(e) = k_e/N$, where $k_e$ is the total number of times $e$ is sampled across all samples.

After completing the pre-sampling stage and obtaining the weighted graph $G_w$, the offline algorithm runs a weighted min-edge-cut graph partitioning algorithm on $G_w$ to obtain partitions.
The algorithm outputs partitions that minimize the sum of the weights of the edges in the cut and ensure that the load per partition, which is the sum of the weights of their vertices, is balanced. 
% Any vertex feature that needs to be cached is now assigned to a GPU based on the partitions.
Formally, given a graph $G_w(V,E,w_V,w_E)$ and the number of partitions $d = |D|$, where $D$ is the set of GPU devices, a weighted min-edge-cut graph partition algorithm outputs the set of partitions $P=\{P_1,\ldots,P_d\}$ of $V$ that solves the following problem:

\begin{equation}
\begin{aligned}
\min_P \quad & \sum_{e \in C} w_E(e)\\
\textrm{s.t.} \quad & \forall i:\: L_i \leq (1 + \epsilon) \cdot L /d
\end{aligned}
\label{eqn:optim}
\end{equation}
where $w_E(e) = k_e/N$ is the edge weight of $e$, $C = \{\langle u, v \rangle \in E: u \in P_i, v \in P_j, i \neq j\}$ is the edge cut set, $L_i = \sum_{v \in P_i} w_V(v) = \sum_{v \in P_i} k_v/N$ is the load of each partition $P_i$, $L = \sum_{v \in V} w_V(v)$ is the total load across all partitions, and $\epsilon \geq 0$ is a tunable constant.
% The first equation defines the minimization function as the sum of the weights of the edges in the cut.
% The second equation defines the optimization constraint, which balances the load of each partition $P_i$ defined as the sum of the weights of its vertices 
Note that since the minimization problem of Eq.~\ref{eqn:optim} is NP-hard, we use heuristics to solve it in practice, for example using Metis~\citep{karypis1997metis}.
The partitions $P$ found by the graph partitioning algorithm determine the global partitioning function $f_G$ from vertices to GPUs. %, which in turns determines the splitting function $f_M$ used by the online splitting algorithm.

\begin{comment}
Let $X_i$ be a random variable expressing the load on the $i$-th split of $S$ as the number of vertices \ms{number of incoming vertices} at layer $l>0$ in the split and let $Y$ be a random variable expressing the communication cost of $S$ as the number of edges across different splits.
In the following, we show that $\E[X_i] = L_i$ and $\E[Y] = \sum_{e \in C} k_e/N$.
After doing that, we can conclude based on Eq.~\ref{eqn:optim} that $\E[Y]$ is minimized and $\E[X_i]$ across different splits is balanced.
\end{comment}

\mypar{Analysis}
We now show that our splitting algorithm finds a solution to the mini-batch splitting problem of Eq.~\ref{eqn:prob} by reducing it to the optimization problem of Eq.~\ref{eqn:optim} with $d=|S|$, which we can solve with a heuristic.
We show that $\E[X_i] = L_i$ and $\E[Y] = \sum_{e \in C} k_e/N$.
After doing that, we can conclude that Eq.~\ref{eqn:optim} minimizes $\E[Y]$ and constraints $\E[X_i]$ as in Eq.~\ref{eqn:prob}.

We start by showing that $\E[X_i] = L_i$.
% Given a vertex $v \in V$, let the random variable $Z_v$ be 1 if the vertex appears at layer $l>0$ in the sample $S$ and 0 otherwise. 
Given a vertex $v \in V$, let the random variable $Z_v$ be the number of layers $l > 0$ where the vertex appears in the sample $S$. 
% \ms{$Z_v$ is the number of incoming edges in the split}
We have that $X_i = \sum_{v \in P_i} Z_v$, which implies that $\E[X_i]= \sum_{v \in P_i} \E[{Z_v}]$.
If we assign vertex weights using a sufficiently large number of samples $N$, according to the law of large numbers, we have $\E[{Z_v}] = k_v/N$, so $\E[X_i] = L_i$.

We use a similar argument to show that $\E[Y] = \sum_{e \in C} k_e/N$.
Given an edge $e \in E$, let the random variable $Z_e$ be the number of times $e$ is sampled (at different layers) in $S$. 
The set of cross-split edges in the sample $S$ is a subset of the cross-partition edges $C$ in the whole graph, so $Y = \sum_{e \in C} Z_e$.
The expected value of $Y$ is given by $\E[Y] = \sum_{e \in C} \E[Z_e]$.
If we use a sufficiently large number of samples $N$ to assign edge weights, according to the law of large numbers, we have that $\E[Z_e] = k_e/N$, so $\E[Y] = \sum_{e \in C} k_e/N$.

\begin{comment}
\mypar{Analysis}
To minimize communication overhead during training, we assign a score to each edge and aim to minimize the weighted sum of edge cuts. This score represents the probability of an edge being sampled in a mini-batch. Our analysis demonstrates that by minimizing the weighted sum of edge cuts, we effectively reduce the communication overhead in a sample. Although our focus in this analysis is on a single layer, the results apply to all layers within the sample.

Let $e$ be an edge, and define the random variable $Z_e$ as 1 if the edge appears in a sampled layer, and 0 otherwise. To estimate the expected value $\E[Z_e]$, we compute the score of each edge using the same k-hop sampling algorithm used in the training. According to the law of large numbers, the expected mean of a random variable will converge as we average its outcomes a sufficiently large number of times. \juelin{We can also use k-hop neighbor gathering (including all k-hop neighbors) to compute the exact probability. Yet this approach is not scalable as it goes out-of-memory on many graphs}

Consider a set $S$ of edges across all partitions, and let $X$ be the count of edges from $S$ that appear in a sampled layer. We denote this as $X = \sum_{e \in S} Z_e$. Assuming independence among all $Z_e$ values, the expected value of $X$ is given by $\E[X] = \sum_{e \in S} \E[Z_e]$. The edge-cut partitioner minimizes the sum of edge weights, which is equivalent to minimizing $\sum_{e \in S} \E[Z_e]$. Hence, it reduces the expected number of cross-edges in a sample.
\ms{We have never described how we assign edge weights}

A similar argument can be made for the load-balancing constraint. In this case, we consider the computation cost as the number of destination vertices in the partition (or the number of edges to aggregate in the graph convolution network). Let $v$ be a vertex, and let $Z_v$ represent the number of times the vertex appears in the layer.
\ms{Why number of times? Isn't this binary, 1 if it appears and 0 if it does not?}
Let $T$ denote the set of vertices in a given partition, and $Y$ be the total count of vertices from $T$ that appear in a sample. We express this as $Y = \sum_{v \in T} Z_v$. Assuming independence among all $Z_v$ values, the expected value of $Y$ is given by $\E[Y] = \sum_{v \in T} \E[Z_v]$. In this case, we aim to balance the sum of vertex weights within each partition, which is $\sum_{v \in T} \E[Z_v]$, to achieve an evenly distributed workload in a sample.
\end{comment}