\section{Evaluation}
\label{sec:eval}
In this section, we evaluate \name by answering the following questions: 
What are the end-to-end speedups that can be achieved by \name relative to the baselines (ยง~\ref{sec:speedups})?
What is the impact of using \name's splitting algorithm, which provides probabilistic performance guarantees (ยง~\ref{sec:eval-split})?
How does \name scale to a larger number of GPUs within one host and across hosts (ยง~\ref{sec:eval-scalability})? 
How does performance vary when we vary the hyperparameters (\S~\ref{sec:ablation})?

% We first describe the experiment settings (machines, baselines, etc.) in Section~\ref{sec:settings} and then report our experiment
% results in Sections~\ref{sec:results}-\ref{} to answer the questions. \hui{fill in}

\subsection{Experiment Settings}
\label{sec:settings}

\mypar{GNN models}
We consider two popular and diverse GNN models: GraphSage~\citep{graphsage} and GAT~\citep{velivckovic2017graph}.
We use the standard neighborhood sampling algorithm.
Its low computational complexity makes it less likely to hide the cost of shuffling during \tadj sampling.
By default, we use a sampling fanout of 15, 3 GNN layers, a default hidden size of 256 as used in \citep{graphsage}, and a batch size of 1024. 
% \ms{Describe implementations a bit, including P3*}
% We implemented two GNN models
% , GraphSage and GAT, on top of \name. 
\name's \tadj implementations use the same sampling and training kernels as DGL's data-parallel one.


\mypar{Datasets}
We use three large datasets listed in Table~\ref{tab:dataset}.
The Papers100M dataset is the largest homogeneous dataset from the Open Graph Benchmark (OGB), a standard benchmark for GNN training ~\citep{ogb-node-dataset}.
We use two more large synthetic graphs from the SNAP repository \citep{snapnets} which are commonly used for GNN training evaluation.
\begin{table}[ht]
    \centering
    \begin{tabular}{|l||c|c|c|} \hline
         \textbf{Dataset} & \textbf{\# Nodes} & \textbf{\# Edges} & \textbf{\# Feat} \\ \hline \hline
         %  ogbn-arxiv & .16M & 1.1M & 128 \\ \hline
         % Products & 2.4M & 62M & 100 \\ 
         Orkut & 3.1M & 120M & 512 \\ 
         Papers100M & 111M & 1.6B & 128 \\ 
         Friendster &  65M & 1.9B & 128\\ \hline
    \end{tabular}
    \caption{Datasets used for the evaluation}
    \label{tab:dataset}
    \vspace{-.4cm}
\end{table}

\mypar{Hardware setup} 
By default, our experiments use an AWS EC2 p3.8xlarge instance with 4 NVIDIA V100 GPUs (16GB memory) and Xeon E5-2686 v4  @ 2.70GHz,  with 32 CPU cores and 244 GB RAM. 
GPUs are connected to the CPU with a PCIe 3.0  16 bus and with each other via NVLink.
For experiments with 8 GPUs, we use a similar p3.16xlarge instance having 64 CPU cores and 488 GB RAM.

\input{tables/main_experiment_v3}


\mypar{Baselines}
We consider the systems described in Section~\ref{sec:problem} as baselines.
All systems perform synchronous training to avoid biasing model accuracy and use GPU-based sampling.

\begin{itemize}
\item \textbf{DGL} is a standard production library for data-parallel GNN training~\citep{dgl}.
We use DGL version 1.1.3, the same one we use as a component of \name. %, which was released in January 2024.
DGL only supports caching input features and the graph topology when they fully fit into one GPU.

\item \textbf{Quiver} is a recent data-parallel GNN training system that uses distributed caches and leverages fast direct GPU-GPU buses like NVLink~\citep{quiver}.
We use version 0.1.1.
Quiver supports distributed and partial caching across multiple GPUs.

\item \bm{$P^3$} is a distributed GNN training system that uses hybrid push-pull parallelism.
Its source code is not publicly available, so we adapt the push-pull parallelism approach to a single-host multi-GPU system and refer to our implementation as \textbf{P3*}. 

%\item \textbf{DistCache} is a baseline we implemented to further evaluate distributed caching with data parallelism. It is a minor improvement of Quiver that optimizes feature loading from host memory as done in DGL.

\item \textbf{Edge} is a variant of \name used to investigate the impact of using a na\"ive offline splitting algorithm that does not weigh vertices and edges using pre-sampling (see Section~\ref{sec:eval-split}). It uses min-cut partitioning and balances the number of edges and target vertices in each partition, as commonly done in data-parallel GNN training systems~\citep{distdgl}, while minimizing the number of edges across partitions.

\end{itemize}

We configure all systems to maximize the memory available for caching the graph structure and input features while allocating sufficient memory to sample and train without going out of memory.
We configure Quiver
and \name to use the same sampling frequency criterion to rank the input features to cache as proposed in~\citep{gnnlab}.
%\name supports distributed caching to store the graph structure, unlike the other systems.
P3* cannot cache input features for only a subset of the vertices, so it only uses caching for the Orkut graph.

% \mypar{Cache configuration}
% All systems cache the graph structure and input features in the GPU if feasible, otherwise they store it on the host-pinned memory, using UVA memory access.
% DGL cannot use input feature caching for the graphs in Table~\ref{tab:dataset} since they exceed the memory capacity of a single GPU.
% Quiver, DistCache, and \name cache as many feature vectors as possible while allocating sufficient memory to sample and train without going out of memory.
% We configure Quiver, DistCache, and \name to use the same sampling frequency criterion to rank the input features to cache as proposed in~\citep{gnnlab}.
% P3* cannot cache input features for only a subset of the vertices, so it only uses caching for the Orkut graph.
% \name supports distributed caching to store the structure of all the graphs, unlike the other systems.

\subsection{End-to-End Performance}
\label{sec:speedups}
\mypar{Overview}
We now compare the performance of \name with existing work: DGL, Quiver, and P3*. 
The comparison is based on epoch time only because none of these systems biases the accuracy of the GNN models they train.
We measure the total epoch time and break it down for the three steps of the mini-batch training iterations: sampling a subgraph, which also includes splitting for \name, loading the input features into each GPU, and performing the forward and backward pass.

We report the results in Table~\ref{tab:main_nvlink}.
Overall, \name outperforms DGL by up to 4.4x (2.5x on average), P3* by up to 4.1x (2.4x on average), and Quiver by up to 1.9x (1.4x on average). 
%It also outperforms our DistCache baseline by up to 1.7x (1.3x on average).
\name consistently reduces the loading time compared to the other systems.
By avoiding redundant computation, \name can reduce its sampling and training costs, mitigate the additional cost of shuffling, and in some cases be even faster than some data-parallel systems in those steps.
Overall, the speedups tend to be higher for larger graphs such as Papers100M and Friendster, which cannot be fully cached on GPU and have higher loading costs.

\mypar{Sampling time comparison}
The sampling step in \name entails not only sampling the mini-batch, as in the other systems, but also splitting the mini-batch, constructing the shuffle indexes, and shuffling vertices.
The evaluation shows that these additional costs are balanced, and sometimes offset, by the elimination of redundant work and by the use of distributed caching for the graph structure.
\name's online splitting is not a performance bottleneck because it is embarrassingly parallel and fast.

% Compared to DGL and P3*, 
% \name has a higher sampling time in most cases. \ms{Update}
% Still, its sampling is faster than Quiver, which 
% has a less efficient sampler implementation.
% The Orkut graph is interesting since its entire topology is cached at each GPU.
% This minimizes the cost of sampling and magnifies \name's additional overheads, such as splitting and shuffling.
% Nonetheless, even in this case, the performance of \name is comparable with the baselines.
% In summary, sampling is more costly in \name than in some data-parallel systems, due to the extra splitting and shuffling cost, but cheaper than in others.
% The additional sampling overhead of \name is outweighed by its gains in terms of loading time, as we will show. 

\mypar{Loading time comparison}
Reducing the input feature loading time by avoiding redundant loads is one of the main goals of \name.
The results show that \name achieves this goal and consistently has the lowest loading time among all systems, graphs, and models.
This is because \name avoids redundant input feature loads from host memory or other GPUs.

DGL has a high data loading costs overall since it does not support distributed caching and none of the graphs we consider fits in a single GPU's memory.

P3* and Quiver perform much better than DGL in the Orkut graph because its input features can be fully cached across GPUs.
The input features for the Papers100M and Friendster graphs cannot be fully cached, even when using a distributed cache.
Quiver has lower loading times than DGL and P3*  for Friendster because it supports caching only a subset of the features in the GPUs, but it cannot leverage its cache effectively for Papers100M because of the high cost of loading cache misses from the host memory.
%DistCache is faster than Quiver on Papers100M thanks to our optimizations, but it still does not outperform \name.
% For Friendster, using a more complex model like GAT impacts the performance of caching.
% GAT requires more memory per mini-batch, which reduces the size of the caches and increases the data loading time due to cache misses.
%This can be seen by comparing the loading times of the GraphSage and GAT models in Quiver and DistCache.

% P3 partitions the the graph feature data along the feature dimension.
% Since P3 does not have in built caching, we can either put the partitioned graph features on GPU or fetch the from host using UVA. 
% For orkut the graph features are partitioned and stored on GPU whereas for the other graphs, P3 fetches these features stored on host using UVA.
% P3 has similar data loading costs to DGL, but for orkut the graph can be cached using distributed caching across all GPUs. 
% For the systems with \textit{NVLINK} the gpus have fast interconnects which allow Quiver to store a global cache fully distributed across all GPUs, on systems with \textit{PCI-E} the each GPU has a replicated cache. 
% To configure the cache size we store the maximum which allows training to complete without out of memory.
% All cache misses are accessed using zero-copy memory similar to DGL.
% Quiver feature loading time is better than DGL and P3, when the graph features are stored on the CPU, as it caches a portion of the features on the GPU while accessing them using the same UVA.
% P3 feature loading is faster than Quiver as P3 loads the partitioned feature data from the GPU where as Quiver data is loaded from distributed GPU memory.
% \name loading time is lower than quiver but as remote gpu memory access is faster than uva based host memory access, this reduction contributes lower to overall speed up.
% Across all the graphs we have lower loading time as vertex data only has to be loaded only one gpu. 
% We see a variance in loading times across models SAGE and GAT for quiver, as GAT consumes higher memory therefore has lesser amount of data available to be cached, causing more cache misses from the distributed gpu cache. 
% \input{tables/partitioning}

\mypar{Forward/backward pass time comparison}
Compared to the data-parallel systems, 
%each GPU performs the forward and backward (FB) passes for each iteration independently, without coordinating with others except to synchronize model parameters.
hybrid parallel systems such as P3* and \name introduce shuffles during training, which typically result in larger FB times.
When training the GraphSage model, P3* can partially compensate for its push-pull shuffle overhead by pushing some of the computation of each micro-batch to all GPUs.
Thanks to this optimization, P3* achieves the lowest training time of all systems on the Orkut graph.
In all the other cases, however, P3* has the largest FB time.
This is because the GPUs must shuffle all the partial activations for all micro-batches.
In particular, more complex models like GAT tend to have large partial activations.

\name mitigates the shuffle overhead by avoiding computing hidden features redundantly.
Its FB times, though, are still larger than in the other data-parallel systems.
The gap is smaller for GAT, which is a more computationally complex model because \name benefits more from avoiding redundant computation.
Compared to P3*, \name always has lower training times except in the case of the Orkut graph with the GraphSage model.
This is thanks to the avoidance of redundant computation and its less expensive shuffles.

\subsection{Evaluation of the Splitting Algorithm}
\label{sec:eval-split}
\mypar{End-to-end performance impact}
\name relies on an offline graph partitioning algorithm to provide probabilistic performance guarantees: balancing the expected load across splits and minimizing the expected communication costs.
We evaluate the impact of using this algorithm by combining \name's online splitting with three alternative offline partitioning algorithms that do not provide these guarantees. 

The \emph{\name} is the pre-sampling-based algorithm with probabilistic guarantees described in Section~\ref{sec:workload}.
The \emph{Node} algorithm partitions the graph using only the pre-sampled node weights. Comparing it to \emph{\name} shows the impact of using edge weights during graph partitioning.
The \emph{Edge} algorithm uses min-cut partitioning but it does not assign weights to vertices and edges using pre-sampling. It balances the number of edges and target vertices in each partition, as commonly done in data-parallel GNN training systems~\citep{distdgl} while minimizing the number of edge cuts across partitions.
Finally, \emph{Rand} partitioning algorithm randomly assigns each vertex to a partition. 

Table~\ref{tab:main_nvlink} shows the end-to-end performance benefit of using \name's splitting algorithm compared to the Edge baseline.
\name helps improve the end-to-end training performance by up to 1.5$\times$ on Orkut, 1.7$\times$ on Papers100M, and 1.4$\times$ on Friendster.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.47\linewidth}
        \centering
        \includegraphics[width=\textwidth]{results/simulation/papers100M_edge_epoch10_gsplit_vs_base.pdf}
        % \caption{Workload imbalance of \name vs. others}
        \label{fig:gsplit_partition_vs_others_workload}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=\textwidth]{results/simulation/papers100M_crs_epoch10_gsplit_vs_base.pdf}
        % \caption{Communication overhead of \name vs. other partitioning algorithms}
        \label{fig:gsplit_partition_vs_others_comm}
    \end{subfigure}
    \vspace{-20pt}
    \caption{\name vs. other offline partitioning algorithms.}
    \label{fig:gsplit_partition_vs_others}
\end{figure}

We analyze the reasons for these speedups more in-depth in Figure~\ref{fig:gsplit_partition_vs_others}, which compares the workload imbalance and communication costs of different partition strategies using the Papers100M graph. 
We quantify the \textit{workload imbalance} among the splits in each iteration as the maximum number of edges at layer $l >0 $ per split divided by the average, and the \textit{communication cost} as the percentage of cross-edges among splits over all the edges in the mini-batch.

As shown in Figure~\ref{fig:gsplit_partition_vs_others}, the \emph{Rand} baseline leads to the most evenly distributed computation cost across partitions. 
Yet, it results in a high communication overhead with 75\% of the edges crossing two partitions in most iterations. 
The \emph{Edge} baseline achieves a much lower edge cut and reduces the communication overhead. However, balancing the target vertices alone does not guarantee that the splits of the sampled mini-batches will be balanced.
%Even though we constraint the maximum load imbalance among graph partitions to be up to $1.05$, the actual imbalance among the splits of the sampled mini-batches is notably higher than that, as shown in Figure~\ref{fig:friendster_bal}.

The splitting algorithm of \name achieves the benefit of both approaches. 
It has a lower communication overhead compared to the random partition algorithm and a more balanced workload than simply balancing the number of target vertices in each partition. 
This is thanks to its offline pre-sampling approach.
% In particular, we can see that the splitting algorithm benefits from the probabilistic load balancing guarantees mandated in Eq.~\ref{eqn:prob}, since the load imbalance among splits is now around $1.05$ as expected.
% This lower load imbalance is key to speeding up end-to-end training, even though it constraints the solution space and results in a higher communication cost than the Edge baseline.

In addition, we observe that assigning weights to edges using \name effectively reduces the communication overhead in a mini-batch.
Compared to Node, which does not weigh edges, the average ratio of cross edges over total edges is reduced from 9\% to 5\% for Papers100M as shown in Figure~\ref{fig:gsplit_partition_vs_others}. Better yet, the reduction in communication costs does not significantly impact workload imbalance. We observe a similar trend in Orkut and Friendster. 

\mypar{Cost of the splitting algorithm}
The splitting algorithm has two offline steps: pre-sampling and graph partitioning.
Empirically, we found that running ten epochs of sampling during the pre-sampling stage is sufficient.
Using a larger number of sampling epochs has little impact on load balancing and communication costs. 
When using $30$ and $100$ pre-sampling epochs, the difference in average load imbalance per mini-batch remains within 2\% for all the graphs, while the percentage of cross edges over the total number of edges per mini-batch remains within 7\% for Orkut, 2\% for Papers100M and Friendster. 
Pre-sampling is fast relative to the overall training time.
Using a machine with four RTX 3090 GPUs, it is 19s for Orkut, 20s for Papers100M, and 288s for Friendster. 

The final offline step of \name is graph partitioning, which is commonly used in many distributed mini-batch GNN training systems.
We use METIS~\citep{mtmetis2013ipdps} to partition the graphs on an AWS r7a.x24large instance, which has 48 cores (96 threads) and 768GB of memory. The partitioning time is 14s for Orkut, 78s for Papers100M, and 534s for Friendster.
Both pre-sampling and partitioning are one-time costs that can be amortized by training over the same dataset multiple times. 

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=0.85 \textwidth]{results/abalations.pdf}
%     \caption{Scalability and ablation studies on the Papers100M graph. }
%     \label{fig:ablation}
%     \vspace{-.2cm}
% \end{figure*}


% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width= \textwidth]{results/world_size.pdf}
%     \caption{varying number of gpus}
%     \label{fig:varygpus}
% \end{figure*}

% \hui{add scalability study here}


% When varying the number of GPUs from 2 to 8, \name's forward/backward (FB) time scales well because each GPU must process smaller splits. 
% In our 8 GPU host, GPUs are organized in two NVLink cliques of 4 GPUs each.
% The two cliques are connected by an NVSwitch. 
% In this configuration, DistCache (and Quiver) replicates the content of the cache to avoid communication over the NVSwitch.
% This explains why its loading time does not change much when going from 4 to 8 GPUs.



% \begin{figure}[t]
% %    \advance\leftskip-2.2cm
% % \advance\rightskip2cm
% % \includegraphics[keepaspectratio=true,width=.5\textwidth]{results/abalations_scalability.pdf}
% \hspace*{-1cm}
% \includegraphics[keepaspectratio=true,width=.5\textwidth]{results/abalations_FR_scalability.pdf}

%   \caption{Scalability on the Friendster graph 
%   \juelin{How about merge this into Fig 7}
%   \sand{We cant merge as they use a different hidden size,}
%   \ms{use "(a) \# GPUs, single host" and "(b) \# Hosts, 4 GPUs each" as x axis titles. }
%   }
%   \label{fig:friendster_scalabiity}
% \end{figure}

\begin{figure*}[t!]
    \centering
  \includegraphics[width=\textwidth]{results/arxiv_abalations_friendster_combined.pdf}
  \caption{Scalability and ablation study. The reported speedups are the epoch time of other systems relative to \name.}
\label{fig:ablation}
\end{figure*}
\subsection{Scalablity}
\label{sec:eval-scalability}
We show how \name scales to a varying number of GPUs and hosts in Figure~\ref{fig:ablation}.
\textbf{Single-host.} 
We first evaluate using a single host and varying the number of GPUs in Figure~\ref{fig:ablation}(a). 
\name scales better than the other systems with a larger number of GPUs because it has more opportunities to avoid redundant loads and computation.
It can also make more efficient use of the GPU caches thanks to its use of collective communication primitives.
Quiver relies on direct remote memory access to transfer cached input features across GPUs efficiently.
This, however, is only possible between GPUs that have direct NVLink connections.
In our 8 GPU host, not all GPUs are directly connected. Quiver circumvents this problem by replicating cached features across GPUs that have no direct links.
\name, by contrast, does not need to cache features redundantly.
\textbf{Multi-hosts.} We also run distributed multi-host experiments where each host has 4 GPUs and show the results in Figure~\ref{fig:ablation}(b). 
\name uses a hybrid approach to scale to multiple hosts, which uses data parallelism across hosts and \tname within each host. 
All hosts cache the same input features on their GPUs.
% We observe the hybrid approach outperforms pure split parallelism as there is less coordination between 8 GPUs. 
We observe that \name shows consistent speedups in all configurations and models. 



% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.85 \textwidth]{results/abalations_new.pdf}
%     \caption{Ablation study on the Friendster graph.}
%     \label{fig:ablation}
% %    \vspace{-.2cm}
% \end{figure*}
% \begin{figure*}[t]
%     \centering
%   \includegraphics[width=0.85\textwidth]{figures/abalations_papers100M.pdf}
%   \caption{Ablation study on the papers100M graph. \sand{The training time is slightly more for quiver then dgl, speedups need to be made consistent with main table.These numbers look too inconsistent for us to use. }}
%     \label{fig:ablation}
% %    \vspace{-.2cm}
% \end{figure*}
\subsection{Ablation Study}
\label{sec:ablation}

We now estimate how consistent \name's speedups are when 
% the number of GPUs in the system or 
the model and training hyperparameters change.
The results for the Friendster graph are reported in Figure~   \ref{fig:ablation}.
\textbf{Hidden size.} Increasing the hidden feature size impacts the FB time of \name negatively, increasing the overall volume of data shuffled. However, it also increases the gains of avoiding redundant computation, especially for complex models such as GAT. 
The two factors balance out and \name shows consistent speedups over the baselines, as shown in Figure~\ref{fig:ablation}(c).
\textbf{Batch size.}
We vary the mini-batch size while keeping a hidden size of 128 to avoid going out of memory. 
Larger mini-batches increase the relative cost of shuffling during the FB phase but also offer more opportunities to save on redundant data loading.
Overall, \name always outperforms the data-parallel baselines, as shown in Figure~\ref{fig:ablation}(d).
\textbf{Number of GNN Layers.}
In this experiment, we use a hidden size of 128 and pick the largest sampling fanout that avoids going out of memory for each number of layers.
The results are reported in Figure~\ref{fig:ablation}(e).
The GNNs are most commonly trained with 2 or 3 layers, i.e., 2 or 3 hops from the target vertices.
\name consistently outperforms the baselines in these settings.
Eventually, adding more layers increases the number of shuffles, making \tname more expensive.
We validate this intuition by evaluating a very deep GNN with 4 layers (4 hops) and observe that the \name is only slightly faster than Quiver for GAT and slower for GraphSage, which is a simpler GNN model where the relative cost of shuffling over computation is higher.
These results suggest that for very deep GNNs a hybrid approach that uses \tname only for the bottom layers and data parallelism for the higher layers would be a promising avenue for future research.

% \begin{figure*}[!h]
%     \centering
%     \includegraphics[width=\textwidth]{results/depth.pdf}
%     \caption{Varying depth}
%     \label{fig:depth}
% \end{figure*}