\section{Conclusion}
This paper presented \tname, a new hybrid parallelism approach for mini-batch training.
Compared to data parallelism and push-pull parallelism, \tname achieves the dual goals of reducing the cost of data loading, by avoiding redundant loads, and keeping the sampling and training cost low, by avoiding redundant computation. 
%Our \name system outperforms baselines using other forms of mini-batch GNN training parallelism by up to 1.9-4.4$\times$. 
%Future work includes extending \tname to optimize its hybrid parallelism strategy given any GNN training program and heterogeneous hardware platform.  
%We embody these ideas in \name, introduce novel splitting algorithms, and a programming API that supports the reuse of efficient sampling and training kernels.