{
    "arxiv_id": "2303.13775",
    "paper_title": "GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism",
    "authors": [
        "Sandeep Polisetty",
        "Juelin Liu",
        "Kobi Falus",
        "Yi Ren Fung",
        "Seung-Hwan Lim",
        "Hui Guan",
        "Marco Serafini"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.DC",
        "cs.LG"
    ],
    "abstract": "Large-scale graphs with billions of edges are ubiquitous in many industries, science, and engineering fields such as recommendation systems, social graph analysis, knowledge base, material science, and biology. Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to learn on these graphs due to their superior performance in various graph analytics tasks. Mini-batch training is commonly adopted to train on large graphs, and data parallelism is the standard approach to scale mini-batch training to multiple GPUs. In this paper, we argue that several fundamental performance bottlenecks of GNN training systems have to do with inherent limitations of the data parallel approach. We then propose split parallelism, a novel parallel mini-batch training paradigm. We implement split parallelism in a novel system called gsplit and show that it outperforms state-of-the-art systems such as DGL, Quiver, and PaGraph.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13775v1"
    ],
    "publication_venue": null
}