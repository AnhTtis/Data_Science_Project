%\documentclass{midl} % Include author names
\documentclass[anon]{midl} % Anonymized submission
\usepackage{mwe} % to get dummy images
\usepackage{booktabs}
\usepackage{amsmath}
\jmlrvolume{}
\jmlryear{2023}
\jmlrworkshop{}
\editors{}
\usepackage{tabu}
\def\authorBlock{
    Wei Zhu$^{1}~$~~
    Runtao Zhou$^1$~~
    Yuan Yao$^1$~~ 
    Campbell Timothy$^2$~~
    Rajat Jain$^2$~~
    Jiebo Luo$^1$\\
    $^1$~University of Rochester~$^2$~University of Rochester Medical Center\\

}

%%%%%%%%% TITLE - PLEASE UPDATE

\title[SegPrompt]{SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification}
% Prompt Tune Pretrained Models with Segmentation Map for Kidney Stone Classification

\begin{document}
\author{\authorBlock}
\maketitle

\begin{abstract}
Recently, deep learning has produced encouraging results for kidney stone classification using endoscope images. However, the shortage of annotated training data poses a severe problem in improving the performance and generalization ability of the trained model. It is thus crucial to fully exploit the limited data at hand. In this paper, we propose SegPrompt to alleviate the data shortage problems by exploiting segmentation maps from two aspects. First, SegPrompt integrates segmentation maps to facilitate classification training so that the classification model is aware of the regions of interest. The proposed method allows the image and segmentation tokens to interact with each other to fully utilize the segmentation map information. Second, we use the segmentation maps as prompts to tune the pretrained deep model, resulting in much fewer trainable parameters than vanilla finetuning. We perform extensive experiments on the collected kidney stone dataset. The results show that SegPrompt can achieve an advantageous balance between the model fitting ability and the generalization ability, eventually leading to an effective model with limited training data.
\end{abstract}

\begin{keywords}
Kidney Stone Classification, Prompt Tuning, Parameter Efficient Finetuning 
\end{keywords}
% motivations: prevent overfitting with few training data; better utilize the segmentation map;
\section{Introduction}
%The formation of kidney stones is a complex biological process that involves physicochemical changes and supersaturation of urine\cite{alelign2018kidney}. Some of the risk factors for kidney stone formation include obesity, diabetes, hypertension, etc. \cite{alelign2018kidney}.
% Nephrolithiasis, commonly referred to as kidney stones, are crystal concretions formed in the kidney and leave the body through the urethra without discomfort. However, larger stones typically cause severe pain and require surgical intervention ~\cite{Nephrolithias}.  In the United States, it is estimated that about 5\% of women and 12\% of men will develop kidney stones at some point in their lifetime, and the prevalence of kidney stone development has been rising in both genders ~\cite{coe2005kidney}. One study has shown that the relapse rate of kidney stones after 2, 5, 10, and 15 years was 11\%, 20\%, 31\%, and 39\%, respectively ~\cite{rule2014roks}. Therefore, understanding the type of kidney stones will help determine the cause and may give physicians clues on reducing the risks of relapse through personalized treatments~\cite{scales2012prevalence}. The standard way of classifying kidney stone types involves kidney stone fragment extraction and subsequent visual examination through microscopes. This approach typically takes 1-2 months to get the classification result back to the physician, even though the patients can be in a critical condition and suffer from great pain ~\cite{ochoa2022vivo}. 

Kidney stone disease (KSD) affects 10\% of the US population during their lifetime and results in billions of dollars of the annual cost to society~\cite{chewcharat2021trends}. The recurrent nature of KSD often causes multiple emergency room visits, hospital admissions, and surgical procedures for the patient. Over the last ten years, laser technology has evolved significantly~\cite{kronenberg2018advances,elhilali2017use,ibrahim2020double}. The most common procedure for KSD is ureteroscopy with laser lithotripsy~\cite{heers2016trends}.  In this procedure, a semi-rigid or flexible 2-3mm ureteroscope is navigated into the urinary tract to the stone.  It is then fragmented using a holmium:YAG laser.  This type of laser has been the mainstay of KSD procedures for over 30 years~\cite{zarrabi2011evolution}. 
In the past, it has been standard to fragment the stone into small pieces, which are then removed from the body using a small basket that is passed through the scope.  This method allows the urologist to collect small pieces, which can then be sent to a laboratory for chemical analysis. However,
this approach typically takes 1-2 months to get the classification result back to the physician, even though the patients can be in a critical condition and suffer from great pain ~\cite{ochoa2022vivo}. In this paper, we focus on real-time stone-type prediction with the deep neural network~\cite{ochoa2022vivo}. 

Recently, deep learning-based methods have been developed to perform efficient diagnosis using endoscope images, and these methods often directly fine-tune the whole model~\cite{ochoa2022vivo,estrade2022towards}. However, similar to most medically related tasks~\cite{zhu2020alleviating}, the limited training data makes it hard to obtain a robust deep model that could generalize to unseen cases with vanilla finetuning~\cite{zang2022unified}. In this paper, inspired by the recent progress on Visual Prompt Tuning (VPT)~\cite{jia2022visual}, we propose SegPrompt for kidney stone classification by taking segmentation maps as prompts to tune the pretrained model. On the one hand, SegPrompt integrates the segmentation map into the training process to make the model aware of the regions of interest, which intuitively benefits the classification training process~\cite{khan2019construction}. The segmentation map is obtained with a pretrained Unet~\cite{ronneberger2015u}. On the other hand, as a prompt tuning-based method, SegPrompt does not update the backbone model and thus has much fewer trainable parameters than finetuning. In this way, we can avoid the overfitting problem that often comes with small-scale training data. Moreover, our model allows the image and segmentation tokens to interact mutually with each other so that the model can make full use of the segmentation map.

We highlight our contributions as follows:
\begin{enumerate}
    \item We propose SegPrompt, which regards segmentation maps as prompts to tune the pretrained deep model for kidney stone classification with limited training data.
    \item SegPrompt incorporates the segmentation maps to facilitate the classification training process and only prompt-tunes a small part of the model, thus alleviating the overfitting problem and improving the classification performance.
    \item We conduct thorough experiments on the collected dataset to validate the effectiveness of SegPrompt.
\end{enumerate}

\section{Related Work} 
\subsection{Kidney Stone Classification}
Both traditional machine learning and deep learning approaches have been used for kidney stone classification. Serrat \textit{et al.} adopt random forest to classify kidney stone images with hand-crafted texture and color feature vectors~\cite {serrat2017mystone}. Motivated by the encouraging results of deep medical image analysis~\cite{ronneberger2015u}, Amado and Alejandro~\cite{torrell2018metric} exploit deep metric learning approaches such as Siamese Networks and Triplet Networks to learn the embedding for kidney stone images and use the k-nearest neighbor algorithm to classify testing images. Black \textit{et al.}~\cite{black2020deep} incorporate ex-vivo kidney stones images  to finetune a pre-trained deep neural network and obtain reasonable results. Estrade \textit{et al.} leverage transfer learning to classify mixed stones using surface and cross-section images of kidney stones~\cite{estrade2022towards}. In Manoj \textit{et al.}'s work ~\cite{manoj2022automated}, they present the visualization analysis of the well-trained kidney stone classifier with Grad-CAM. Finally, Ochoa-Ruiz \textit{et al.}~\cite{ochoa2022vivo} use deep neural networks to classify kidney stones with in-vivo images from medical endoscopes. However, most of these methods finetune the whole model, potentially leading to an overfitting problem. In contrast, SegPrompt has fewer trainable parameters to enhance the generalization ability and incorporates the segmentation map to facilitate the training. 

\subsection{Visual Prompt Tuning}
Visual Prompt Tuning (VPT) was recently proposed to adjust a pretrained vision transformer for specific tasks with few trainable parameters and shows advantages in generalization ability over vanilla fine-tuning, particularly with limited training data~\cite{jia2022visual}. VPT simply adds learnable tokens to the input of vision transformers~\cite{jia2022visual}. Visual Prompting pads the original images with learnable pixels~\cite{bahng2022visual}. NOAH performs a neural architecture search to learn optimal prompt structure~\cite{zhang2022neural}. Unified vision and language prompt tuning are proposed to jointly tune the VL models~\cite{zang2022unified}. S-Prompt is proposed to handle domain incremental learning with prompt tuning~\cite{wang2022s}. The prompts used by these methods are simply trainable parameters. In contrast, SegPrompt learns to generate prompts based on the segmentation map, and achieve a better balance between fitting and generalization ability.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{blockdiag.jpg}
    \caption{Block diagram of SegPrompt. We first extract the segmentation map with a pretrained Unet. The segmentation map is encoded into segmentation embeddings by the first two blocks of a pretrained ResNet18. We add the position embedding and segmentation indicator to the segmentation embedding to obtain segmentation tokens. Finally, we concatenate the image tokens, segmentation tokens, and extra learnable tokens and feed all tokens to the transformer backbone. We only update the segmentation map encoder and the last classifier during training.}
    \label{fig:my_label}
\end{figure}
\section{Methodology}
In this section, we present the proposed segmentation map-based prompt tuning framework for kidney stone classification. It is designed to improve the model's performance and generalization ability by better exploiting the knowledge of segmentation maps with fewer trainable parameters.
%propose to utilize the segmentation map to prompt-tune pre-trained models for kidney stone classification. 

\subsection{Overview}
Our method contains a frozen ViT backbone~\cite{dosovitskiy2020image}, a segmentation map encoder, and a linear classifier. Similar to most medical image tasks, we also suffer from the scarcity of annotated kidney stone images, and it is expensive to collect more samples~\cite{zhu2020alleviating}. The shortage of training data makes it hard to obtain an effective model which could generalize to unseen cases. To alleviate the problem, on the one hand, we involve the segmentation map in the classification training so that the model is aware of the regions of interest. On the other hand, we tokenize the segmentation map into prompts to finetune the backbone model and only update the segmentation map encoder and the last linear classifier. Consequently, the much fewer trainable parameters empower our model with better generalization abilities to avoid the overfitting problem~\cite{jia2022visual}. Moreover, since we perform self-attention on both image and segmentation tokens~\cite{vaswani2017attention}, our method allows the model to exploit the knowledge of the segmentation map more comprehensively and flexibly. We show the block diagram of our method in Fig. (\ref{fig:my_label}).

\subsection{Tokenize the Segmentation Map}
We first show how to convert the segmentation map into prompt tokens with the proposed segmentation map encoder $h$. The encoder $h$ consists of the first two blocks of an ImageNet pre-trained ResNet18 followed by a projector, where the projector is composed of a 1x1 convolutional layer and an adaptive pooling layer~\cite{he2016deep}. The convolutional layer of the projector is used to match the dimension of Resnet output to that of ViT while the pooling layer reduces the segmentation tokens to a desirable length. Moreover, the encoder $h$ also contains learnable tokens $P_s$, $Z_e$, and $r$, which will be introduced later. Given a training image $X$, we first obtain the binarized pixel-wise segmentation map $O \in \lbrace -1, 1 \rbrace$ from a pretrained Unet~\cite{ronneberger2015u}, where $1$ denotes foreground regions with kidney stones, and $-1$ denotes background regions, the segmentation map embeddings $M=\lbrace m^i \rbrace_{i=1}^{l_m} \in R^{l_m \times d}$ can be obtained by flattening the output of segmentation map encoder as
\begin{equation}
    M = flatten(h(O)),
\end{equation}
where $d$ is the dimension of the backbone model. Then, to convert the embedding $M$ to tokens, we first add the learnable position embedding $P_s=\lbrace p_s^i \rbrace_{i=1}^{l_m} \in R^{l_m \times d}$ to retain the position information and then add a learnable indicator token $r \in R^{d}$ to enable the model to distinguish the segmentation tokens from the image tokens. Specifically, we obtain the segmentation tokens $Z_s=\lbrace z_s^i \rbrace_{i=1}^{l_m} \in R^{l_m \times d}$ by 
\begin{equation}
    z_s^i = m^i + p^i_s + r \;\; for \;\; i = 1, \ldots, l_m.
\end{equation}

% a binarized pixel-wise segmentation map $M \in \lbrace -1, 1 \rbrace$, where $1$ denotes foreground regions with kidney stones and $-1$ denotes background regions. 
% The segmentation masks were processed through the network composed of the first two blocks of a pre-trained ResNet18. The output mask patches from the ResNet had to go through an additional linear layer to match the embedding dimension of the image patches. 

%The original segmentation maps were converted to masks containing only values of -1 and 1 ( -1 for regions without kidney stones and 1 for regions containing kidney stones). The segmentation masks were processed separately in a network composed of the first two blocks of a pre-trained ResNet18. The output mask patches from the ResNet had to go through an additional linear layer in order to match the embedding dimension of the image patches. Unlike the 16x16 patch size for the images, masks have a larger patch size of 32x32 in return for fewer patches and fewer training parameters. Thus, additional pooling layers were added to the ResNet in order to control the mask patch size. Similar to image patches, positional embedding was added to the mask patches. In order to distinguish the image patches from the mask patches, we added one additional learnable “mask tokens” to the all mask patches. We then concatenated all the patches ( image patches and mask patches) together before feeding them into the transformer encoders. The original mask patches will be used later in the transformer blocks, thus they will be stored separately. 

\subsection{Prompt Tuning with Segmentation Tokens}
To enable the model to better interact with and exploit the segmentation map, we propose to prompt-tune the model with the segmentation tokens~\cite{jia2022visual}. In particular, we concatenate the classification token $z_{cls}$, image tokens $Z_x=\lbrace z_x^i \rbrace_{i=1}^{l} \in R^{l \times d}$, the segmentation tokens $Z_s$, and some extra learnable tokens $Z_e = \lbrace z_e^i \rbrace_{i=1}^{l_e} \in R^{l_e \times d}$ as input $Z$ to the transformer backbone
\begin{equation}
    Z = [z_{cls}, z_x^0, \ldots, z_x^l, z_s^0, \ldots, z_x^{l_s}, z_e^0, \ldots, z_e^{l_e}]
\end{equation}
The extra learnable tokens make the pretrained model better adapt to kidney stone classification. The classification token $z_{cls}$ and image tokens $Z_{x}$ are frozen during training~\cite{dosovitskiy2020image}. We perform multi-head self-attention on the input tokens $Z$ and take the $z_{cls}$ from the last layer as the output, which will be further processed by the classifier to get the final prediction. We adopt cross-entropy loss to train the model. During training, we keep the transformer backbone frozen and only update the segmentation map encoder (including the corresponding position embedding $P_s$ and the indicator token $r$), the last classifier, and the introduced extra tokens $Z_e$. 

We discuss several important features of SegPrompt as follows. Existing tuning methods strive to balance the generalization and fitting abilities to adapt a pretrained model for small-scale tasks. For example, finetuning suffers from the overfitting problem with over-abundant learnable parameters, while VPT may underfit the target dataset, which deviates significantly from the pretrained dataset~\cite{wortsman2022robust}. Moreover, it is not trivial to integrate additional knowledge (e.g., segmentation map) into the training process for these methods. We empirically find that SegPrompt leads to a powerful model without severe overfitting and can effectively utilize extra knowledge. Specifically, compared with vanilla fine-tuning, the much fewer trainable parameters of SegPrompt significantly alleviate the overfitting problem. Compared with VPT, SegPrompt, equipped with the learnable segmentation map encoder, has a more powerful learning capacity and also does not severely distort the pretrained model because the number of newly introduced tokens (default set to 51) is much fewer than that of the original tokens (197 for ViT-B/16)~\cite{dosovitskiy2020image}. To make use of additional knowledge, i.e., the segmentation map, SegPrompt allows image tokens and segmentation tokens to interact with and extract information from each other to improve classification performance. {Compared with the joint classification and segmentation model, our framework is more flexible and can directly use human annotations when a good segmentation model cannot be obtained with the current data.} Last but not least, one can simply extend SegPrompt to integrate other kinds of knowledge, such as patient demographic information~\cite{daneshjouskincon}, device/scanner information~\cite{ji2022amos}, text descriptions of the symptoms~\cite{qin2022medical}, and medical record. We leave these as future directions.

%To train the semantic segmentation neural network, the kidney stone areas of 2082 images containing kidney stones from 9 different videos. 

\section{Experiments}
\subsection{Dataset}
We collect 1496 kidney stone images from 5 different videos. We filter out low-quality and background images and obtain 867 images from 3 videos with COM (calcium oxalate monohydrate) stones and 629 images from 2 videos with CAP(calcium phosphate) stones. We split the dataset into training and validation sets in a video-wise fashion to prevent any data leaks. {We perform 6-fold cross-validation, and the averaged accuracy, precision, recall, and F1 scores with standard deviation are reported on the validation sets composed of images from two hold-out videos.}

%The segmentation maps for each of the training or validation images will be generated by a semantic segmentation network mentioned above. 
%Images from the same videos have been assigned, either all in the training set or validation set.


% \subsection{Network Structure and }
% % seg model training, do we use same data to train seg model and cls model?

% % The images from the original dataset for classification purposes were then passed into the training segmentation network and 

%We changed the output classes of the last linear layer from 1000 to 2 in order to suit our binary classification purposes.The hyperparameters setting for all models is the following. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{result_final.png}
     \caption{Illustration of segmentation results. Each column represents one image sample. The images come from the validation set of different folds.}
    \label{fig:a}
\end{figure}
\vspace{-2mm}
\subsection{Implementation of Segmentation Model}
The kidney stone regions are labeled {by an undergraduate student advised by a specialist}, and we train a  Unet~\cite{ronneberger2015u} implemented by the MMsegmentation framework~\cite{mmseg2020} to perform the segmentation. Besides the training data from different folds, we also leverage some extra images without kidney stone labels to facilitate the segmentation model training. In total, there are eight additional kidney stone videos without kidney stone labels, which provide 1860 additional images. {We manually annotate the segmentation maps for these images and only include them in the segmentation model training.} Finally, we obtain segmentation models with the pixel-level accuracy averaged over different folds as 96.7\% and the dice score as 92.93\%. The outputs are binarized to obtain final segmentation maps. We visualize segmentation results from the validation set in Fig. (\ref{fig:a}).

\subsection{Baseline Methods and Implementation Details}
Three finetuning-based models are included as the baselines to validate the superiority of the proposed SegPrompt for the kidney stone classification task as FT (Finetuning), FT-crop (Finetuning-crop), FT-concat (Finetuning-concat). FT uses raw images without segmentation maps. FT-crop takes the cropped regions of interest as input. As for FT-concat, we channel-wisely concatenate the segmentation maps to the images as the input. The concatenated images then passed through a 1x1 convolution layer to reduce their channel size to 3 before feeding them into the ViT backbone. We also implement FT-based ResNet for comparison. Moreover, we also compare our method with Visual Prompt Tuning (VPT) and VPT-Deep~\cite{jia2022visual}. VPT adds the learnable tokens to the input, while VPT-deep adds different tokens for each layer. Similar to VPT and VPT-deep, we also implement two variants of SegPrompt as SegPrompt and SegPrompt-Deep. 

For all methods, the standard image preprocessing steps, such as resizing and normalization, have been applied to the training images. The batch size, number of epochs, and learning rate are 16, 20, and 0.001, respectively. We adopt an ImageNet pretrained ViT-B/16 as the backbone, which contains 196 image tokens and one classification token~\cite{dosovitskiy2020image}. As for the proposed SegPrompt (Deep), we set the number of segmentation tokens to $l_s=49$, and the extra learnable tokens to $l_e=2$. The number of learnable tokens for VPT (Deep) is searched from $\lbrace 8, 16, 32, 51 \rbrace$.


\begin{table}[]
\centering
\caption{Kidney Stone Classification Results averaged over 6 Folds (corresponding to all possible combinations of videos). The best results are highlighted in bold. (\%)}
\resizebox{0.8\columnwidth}{!}{%
\begin{tabu}{@{}l|ccccc@{}}
\toprule
Methods        & Accuracy        & Precision       & Recall          & F1       &AUC              \\ \midrule
FT             & $95.07 \pm 3.2$ & $94.21 \pm 4.7$ & $95.46 \pm 2.7$ & $93.73 \pm 5.9$          &$95.37\pm 3.4$\\
FT-crop        & $94.34 \pm 4.0$ & $94.14 \pm 4.2$ & $94.19 \pm 3.9$ & $92.96 \pm 5.6$          &$94.17\pm 3.4$\\
FT-concat      & $95.18 \pm 2.5$ & $94.91 \pm 2.5$ & $95.13 \pm 2.3$ & $94.53 \pm 3.0$          &$95.10\pm 2.9$\\ \hline
ResNet50             & $94.75 \pm 2.7$ & $94.06 \pm 4.1$ & $95.49 \pm 2.0$ & $93.55 \pm 5.1$          &$95.18\pm 1.6$\\
 ResNet50-crop        & $95.28 \pm 3.8$ & $95.71 \pm 3.3$ & $94.66 \pm 3.7$ & $94.32 \pm 4.4$          &$94.58\pm 3.7$\\
ResNet50-concat      & $96.72 \pm 2.6$ & $96.44 \pm 3.3$ & $96.93 \pm 2.5$ & $96.44 \pm 2.8$          &$96.84\pm 2.5$\\ \hline
VPT            & $96.87 \pm 1.1$ & $96.60 \pm 1.7$ & $96.65 \pm 1.3$ & $96.07 \pm 2.8$          &$96.52\pm 1.8$\\
VPT-Deep       & $95.85 \pm 0.8$ & $95.49 \pm 1.2$ & $95.60 \pm 1.2$ & $95.13 \pm 2.4$          &$95.32\pm 1.7$\\ \hline
SegPrompt      & $\mathbf{99.56 \pm 0.3}$ & $\mathbf{99.45\pm 0.4}$  & $\mathbf{99.60\pm 0.3}$  & $\mathbf{99.45\pm 0.5}$   & $\mathbf{99.57\pm 0.4}$\\
SegPrompt-Deep & $99.19 \pm 0.3$ & $99.06 \pm 0.3$ & $99.24 \pm 0.3$ & $99.26 \pm 0.2$      & $99.23 \pm 0.5$\\ \bottomrule
\end{tabu}
}
\label{tab:results}
\end{table}
\vspace{-2mm}

\subsection{Experimental Results}
We present the experimental results in Table \ref{tab:results}, and draw several interesting conclusions according to the results. First, FT outperforms FT-crop, suggesting that the cropped clean kidney stone images do not benefit the classification performance, which in turn suggests that the surrounding background regions contain critical information. Second, FT-concat utilizes the segmentation map without removing the background regions and it slightly outperforms FT for 0.8\% in terms of F1 score. This shows that the classification performance could be boosted by properly exploiting the information of the segmentation map. {The performance of FT-ResNet has consistent results.} Third, the overfitting problem is crucial in adapting the pre-trained models to the kidney stone classification with limited training data. VPT and VPT-deep outperform all FT-based methods with much fewer trainable parameters~\cite{jia2022visual}. In particular, VPT obtains a 1.54\% improvement over FT-concat in terms of the F1 score and also surpasses its direct counterpart VPT-deep which has much more trainable parameters. Finally, the proposed SegPrompt makes better use of the segmentation map with few trainable parameters and achieves the best overall performance. For example, SegPrompt improves the F1 score from 96.07\% to 99.45\% compared with the second-best method VPT. We also note that SegPrompt-deep only slightly degrades the performance compared with SegPrompt, which shows the possibility of applying our method to handle large-scale tasks that require more learnable parameters to fit the training set.
%Since all FT variants update the whole model, they may suffer from the overfitting problem because of the few available training data and eventually lead to suboptimal performance. 

%we could conclude that while the segmentation map can potentially benefit the kidney stone classification performance, 

% The accuracy of the first baseline model after 6 fold cross validation was 95.07\% with standard deviation of 3.19\%. The precision, recall and f1 for the first baseline model were 0.942, 0.955, and 0.937 respectively. The second baseline model after 6 fold cross validation was 94.34\% with standard deviation of 4.00\%. The precision, recall and f1 score for the second baseline model were 0.941, 0.942, and 0.930 respectively. All the performance metrics for second baseline model were lower than the first baseline model. We argue that this was because the second baseline model was trained on the cropped kidney stone regions which completely ignored the images' background which might contain relavant information about the target kidney stones. The third baseline model after 6 fold cross validation was 95.18\% with standard deviation of 2.53\%. The precision, recall and f1 score for the third baseline model were 0.941, 0.951, and 0.945 respectively. The third baseline model shows a slight improvement in terms of classification accuracy compared to the second baseline model and the reason for the improvement was because the extra concatenated segmentation maps helped the model recognize the kidney stone region for more accuracy classification. The better performance of the third baseline model demonstrated the importance of the segmentation mask usage on the task of kindey stone classificiation. The SegPrompt model's average accuracy after 6 fold cross validation was 96.86\% with standard deviation of 1.07\%. The precision, recall and f1 score for the SegPrompt model were 0.965, 0.967, and 0.961 respectively. The SegPrompt model has out performed all three baseline models with by least 1.68\%. The reason for the significant improvement because the SegPrompt model effectively utilized the segmentation maps to generate segmentation map embeddings. The embeddings provide the model with the relavent information in terms of the segmentation map patch positions in the original images and distinguish the segmentaion map tokens apart from the image tokens using segmentation embeddings.   

\begin{table}[]
\centering
\caption{Ablation Studies. (\%)}
\resizebox{0.75\columnwidth}{!}{%
\begin{tabular}{@{}l|ccccccccccc@{}}
\toprule
Methods   & Accuracy & Precision  & Recall & F1 & AUC \\ \midrule
SegPrompt       &$99.56 \pm 0.3$          &$99.45\pm 0.4$               &$99.60\pm 0.3$                 &$99.45\pm 0.5$              &$99.57\pm 0.4$ \\ \hline
% SegPrompt w/o $l_e=0$ &          &               &                 &            \\
SegPrompt w/o $r$    &$99.07\pm 0.6$          &$98.90\pm 0.7$               &$99.11\pm 0.5$                 &$98.87\pm 0.8$               &$99.08\pm 0.4$\\
SegPrompt w/o $z_e$      &$99.38\pm 0.5$          &$99.34\pm 0.5$               &$99.36\pm 0.8$                 &$99.30\pm 0.3$   &$99.42\pm0.5$\\

\bottomrule
\end{tabular}
}
\label{tab:ablation}
\end{table}
\vspace{-3mm}
\begin{table}[]
\centering
\caption{Performance with the different number of segmentation tokens. (\%)}
\resizebox{0.7\columnwidth}{!}{%
\begin{tabular}{@{}l|ccccccccccc@{}}
\toprule
\# $l_s$   & Accuracy & Precision  & Recall & F1  &AUC\\ \midrule
% 16       & $98.74\pm 0.6$         &$98.61\pm 0.7$               & $98.69\pm 0.6$                &$99.49\pm 0.9$               \\
25       & $98.81\pm 1.0$         &  $98.75\pm 0.9$             & $98.73\pm 1.0$                &$98.69\pm 0.9$         &$98.70\pm 0.8$      \\
36       & $99.13\pm 0.6$         &  $99.16\pm 0.4$             & $98.87\pm 1.2$                & $98.78\pm 1.4$             &$98.92\pm 0.6$ \\
49       & $\mathbf{99.56 \pm 0.3}$         & $\mathbf{99.45\pm 0.4}$            &  $\mathbf{99.60\pm 0.3}$               &$\mathbf{99.45\pm 0.5}$     &$\mathbf{99.57\pm 0.4}$        \\
64       & $99.28\pm 0.2$          & $99.32\pm 0.1$               & $99.22\pm 0.3$                 & $99.18\pm 0.4$           & $99.24\pm 0.3$    \\
81       &$99.22\pm 0.6$           & $99.14\pm 0.7$               & $99.25\pm 0.6$                 &$99.13\pm 0.7$       &$99.23\pm 0.8$    \\
\bottomrule
\end{tabular}
}
\label{tab:segtoken}
\end{table}
\vspace{-3mm}
% \begin{table}[]
% \centering
% \caption{Performance with the different number of extra learnable tokens. (\%)}
% \begin{tabular}{@{}lccccccccc@{}}
% \toprule
% \# Extra Tokens $l_e$   & Accuracy & Precision  & Recall & F1 \\ \midrule
% 0       &$99.38\pm 0.5$          &$99.34\pm 0.5$               &$99.36\pm 0.8$                 &$99.30\pm 0.3$               \\
% 2       &$99.56 \pm 0.3$          &$99.45\pm 0.4$               &$99.60\pm 0.3$                 &$99.45\pm 0.5$               \\
% 4       &$99.32\pm 0.5$          &   $99.14\pm 0.6$            & $99.38\pm 0.5$                & $99.15\pm 0.6$              \\
% 8       &$99.33\pm 0.6$          & $99.28\pm 0.6$              & $99.35\pm 0.6$                & $99.27\pm 0.6$              \\
% \bottomrule
% \end{tabular}
% \end{table}

\subsection{Ablation Studies}
Extensive ablation studies have been performed to verify the effectiveness of different components of SegPrompts. All variants are developed based on SegPrompt instead of SegPrompt-Deep due to its superior performance and simplicity. We first study the importance of indicator token $r$ and the extra learnable tokens $z_e$. The indicator token $r$ enables the model to distinguish the segmentation map tokens from other tokens, and the extra learnable tokens could make the pretrained model better adapt to our task. The two variants are denoted as SegPrompt w/o $r$ and SegPrompt w/o $z_e$, respectively. The experimental results are shown in Table \ref{tab:ablation}. According to the results, we find that the indicator token is essential for SegPrompt while the extra learnable tokens further slightly improve the performance. We also conduct experiments to study the influence of different numbers of segmentation tokens $l_s$, and select $l_s \in \lbrace 25, 26, 49, 64, 81 \rbrace$. Based on the results shown in Table \ref{tab:segtoken}, the increasing number of segmentation tokens benefits the final performance, while over-large values may lead to a slight decrease. We default set $l_s=49$.
% The first ablation test we conducted was removing the mask tokens from the mask patches. By removing the mask tokens from test if adding mask tokens can improve the model classification accuracy by helping the model distinguish image patches apart from the mask patches. After removing the segmentation tokens, the average validation accuracy  became 96.68\% which decreased by 0.2\% compared to the original SegPrompt model. Since the model performance only decreased by 0.2\%, we argue that adding the mask tokens won't improve the model accuracy significantly. However, since adding the mask token won't increase the training parameters, we should still keep the mask token in the model. 

% % The second ablation test we did was to keep all the configuration of the original segmentation prompt model the same, but keep output mask patches from each transformer block instead of replacing them with the original mask patches. The reason for doing it is to test if replacing the output mask patches of each transformer block by the original mask patches will have a prompting effect on the SegPrompt model. The average validation accuracy for the second test was 98.5\% with a standard deviation of 0.56, which outperformed the original segmentation prompt model by 1.7\%. The increase in model performance indicates replacing the output mask patches from each transformer block with the original mask patches will not necessary help the SegPrompt model increase the classification accuracy. 

% The third ablation test was to freeze the weight of the pre-trained ViT model and only train the ResNet 16 layers for mask embedding, as well as keeping the mask patches after each of the transformer blocks. The motivation for the third ablation test is to decrease the trainable parameters in the original model and maintain good overall model performance. The average accuracy after the third ablation test was 99.2\% with a standard deviation of 0.64\%, which outperformed the original SegPrompt model by 2.36\%. The third ablation test demonstrated that freeze the weight of the pre-trained ViT model will not hurt your model's performance instead it will increase the overall model performance. 

% The fourth ablation test was to change the number of mask patches which out put from the ResNet embedding layers while freeze the pre-trained ViT model. The motivation of this ablation test is to find the optimal number of the mask patches for the kidney stone classification task. The result shows the 49 mask patches gave the best overall accuracy of 99.3\% with standard deviation of 0.52\%. 

% The last ablation test is to add additional tokens besides the tokens from the image patches and mask patches before the first transformer block while freezing all the weight of the pre-trained ViT model. The motivation for the ablation test is to find out if adding the additional tokens will assist model on the learning. We want to find out if the additional tokens have the assisting effect, how many tokens will be optimal for the SegPrompt model's learning processing. We found 2 positional patches give the optimal accuracy of 99.6\% with standard deviation of 0.34\%. The individual ablation test result can be found in the fig x.

\section{Conclusions and Future Work}
In this paper, we present a novel segmentation map-based prompt tuning method for kidney stone classification with limited data, named SegPrompt. We first employ a well-trained Unet to extract the segmentation maps, which are further converted to segmentation tokens by a segmentation map encoder. Then SegPrompt takes the concatenation of image, segmentation, and some extra tokens as input to the transformer. During training, we only prompt-tune the segmentation map encoder and the linear classifier. SegPrompt can better exploit the knowledge of segmentation maps with few trainable parameters and significantly outperforms existing methods for kidney stone classification. The main limitation of our work is the small-scale training dataset, and we will collect more data to improve our model for more types of kidney stones. Moreover, we plan to extend our work to other medical tasks to further validate the effectiveness of SegPrompt~\cite{daneshjouskincon,qin2022medical}.

\noindent \textbf{Acknwledgments} This work is supported in part by NSF \#2050842, NIH 1P50NS108676-01, and NIH 1R21DE030251-01. 

\bibliography{midl-samplebibliography}

\end{document}
