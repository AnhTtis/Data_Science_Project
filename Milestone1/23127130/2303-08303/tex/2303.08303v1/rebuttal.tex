\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}

\title{Rebuttal}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\section{Reviews}
\subsection{Reviewer 1}
\section{Rebuttal}

[Reviewer twWC]

Q1. First, one clear downside of the proposed approach is that it heavily relies on segmentation annotations. In addition, it is not clear how are the segmentation annotations collected.

The motivation of our paper is to better exploit the limited training data at hand since it is expensive and even prohibitive to collect more samples for many medical tasks. That is, we admitted that more efforts need to be made when applying our method to other tasks. Their performance could be boosted with the existing data as suggested by our experimental results. Moreover, our method intuitively does not require precise segmentation annotation since approximated segmentation maps are good enough to make the classifier aware of the ROI regions, which will greatly reduce the cost of labeling segmentation maps. For our paper, a non-medical undergraduate student annotated the segmentation map advised by a specialist. 

Q2. What are the numbers of training/validation samples? Which set are the experiment results obtained from? 

We perform 6-fold cross-validation. We select 2 out of 5 videos for each round. That is, the number of validation samples is approximately 2/5 of the training samples in each rotation, and the averaged results on 6 rotating validation sets are reported.

Q3: What's the advantage of the proposed two-stage method against a simplified one-stage baseline which jointly performs classification and segmentation?

Our method is more flexible compared with the multi-task one-stage model.
\begin{enumerate}
    \item A segmentation model is not required for our method. Our method can directly work with the human-annotated segmentation maps when it is hard to train an effective segmentation model with the training data. 
    \item It is simple to extend our framework to include additional information, e.g., medical record, demographic information, ROI boxes, etc. Unlike the simple joint segmentation and classification model, it is nontrivial to design a general model to conduct these tasks simultaneously. 
    \item It is not easy to balance the performance for the multi-task one-stage model, considering the fact that only one of the tasks (classification) really matters.
\end{enumerate}

[Reviewer BE5B]

Q1. The reporting of the results is not clear. Results by fold would be useful, and clarifying if the authors are reporting training or validation performances would help.

We perform 6-fold cross-validation, and the averaged results on 6 rotating validation sets are reported. This is a common practice. 

Q2. While I do not know much about transformers at the moment, I remain very surprised by the results (99.5+\% on all 4 metrics), which could be explained by either: 1) a very trivial classification problem 2) the sheer awesomeness of transformers, in which case I should probably drop my current research project.

Our current dataset only contains two different types of kidney stones with the same endoscope, which leads to a relatively simple task. We continuously collect images with more stone types and different endoscopes to make our settings more realistic. In addition, transformers have been proven effective for many image recognition tasks. 


Q3. To get a point of comparison, how would a standard CNN (with and without the segmentation map added as an extra channel), trained from scratch, perform? 

Thanks for your suggestion. We conduct experiments by adopting the popular Resnet50 as the backbone. According to the results, while finetune-based Resnet models slightly outperform ViT models without prompt, the performance is still limited since they cannot effectively utilize segmentation maps and also suffer from the overfitting problem with finetuning. Note that the proposed SegPrompt models remain the best by a good margin. 

Q5. Maybe I missed the information, but how many classes are there in this task?

Our dataset currently has two types of kidney stone images. Please refer to the Section on the dataset for details.

Q6. The authors claim that the base methods they compare to suffer from "over-fitting", yet they achieve a stunning 95\% accuracy/precision/recall/f1. Is that really supposed to demonstrate a massive over-fitting?

We observe a performance decrease on the validation set when we finetune the model during training. Moreover, even the cherry-picking results (around 97\% in terms of F1 for FT-concat) of the finetuned variants are worse than Segprompt since our method can avoid severe overfitting problems (more so when the training set is small) and can better utilize the segmentation map to facilitate training.

Q7. Why not compare to related work on kidney stone classification?

We would love to. However, the related works on kidney stone classification that we properly cited in our paper adopt different private datasets, and they also do not release their code. That is, we cannot directly compare our results with them. On the other hand, we regard the proposed method SegPrompt as the main contribution of our paper. We validate the superiority of SegPrompt by comparing it with existing Finetune-based and VPT-based methods. This is a fair comparison for the proposed SegPrompt. 

[Reviewer CYQw]

Q1. Motivation is not really clear as the reader only learns at the setting of the experiments what are the two subtypes of kidney stones to classify, but there is no explanation as to why these two subtypes need to be separated.

Kidney stone classification is a well-established problem in the literature because it can allow doctors to choose the best treatment and also prevent relapse (see the Introduction). Our current dataset only contains these two different types of kidney stones, and we continuously collect images with more stone types to construct a realistic dataset.   

Q2. There are some statements of outperformance but these are not supported by any statistical test. Having a ROC AUC for the classification performance would have been useful.

The ROC-AUC scores are shown in the table. The results are consistent with other evaluation metrics.

\begin{table}[]
\centering
\caption{Kidney stone classification results (\%) averaged over 6 folds (corresponding to all possible combinations of videos). The best results are highlighted in bold. }
\begin{tabular}{@{}l|ccccc@{}}
\toprule
Methods        & Accuracy        & Precision       & Recall          & F1       &AUC              \\ \midrule
FT             & $95.07 \pm 3.2$ & $94.21 \pm 4.7$ & $95.46 \pm 2.7$ & $93.73 \pm 5.9$          &$95.37\pm 3.4$\\
FT-crop        & $94.34 \pm 4.0$ & $94.14 \pm 4.2$ & $94.19 \pm 3.9$ & $92.96 \pm 5.6$          &$94.17\pm 3.4$\\
FT-concat      & $95.18 \pm 2.5$ & $94.91 \pm 2.5$ & $95.13 \pm 2.3$ & $94.53 \pm 3.0$          &$95.10\pm 2.9$\\ \hline
ResNet50             & $94.75 \pm 2.7$ & $94.06 \pm 4.1$ & $95.49 \pm 2.0$ & $93.55 \pm 5.1$          &$95.18\pm 1.6$\\
ResNet50-crop        & $95.28 \pm 3.8$ & $95.71 \pm 3.3$ & $94.66 \pm 3.7$ & $94.32 \pm 4.4$          &$94.58\pm 3.7$\\
ResNet50-concat      & $96.72 \pm 2.6$ & $96.44 \pm 3.3$ & $96.93 \pm 2.5$ & $96.44 \pm 2.8$          &$96.84\pm 2.5$\\ \hline
VPT            & $96.87 \pm 1.1$ & $96.60 \pm 1.7$ & $96.65 \pm 1.3$ & $96.07 \pm 2.8$          &$96.52\pm 1.8$\\
VPT-Deep       & $95.85 \pm 0.8$ & $95.49 \pm 1.2$ & $95.60 \pm 1.2$ & $95.13 \pm 2.4$          &$95.32\pm 1.7$\\ \hline
SegPrompt      & $\mathbf{99.56 \pm 0.3}$ & $\mathbf{99.45\pm 0.4}$  & $\mathbf{99.60\pm 0.3}$  & $\mathbf{99.45\pm 0.5}$   & $\mathbf{99.57\pm 0.4}$\\
SegPrompt-Deep & $99.19 \pm 0.3$ & $99.06 \pm 0.3$ & $99.24 \pm 0.3$ & $99.26 \pm 0.2$      & $99.23 \pm 0.5$\\ \bottomrule
\end{tabular}
\label{tab:results}
\end{table}

Q3. The authors state the relevance of the proposed method when learning with fewer examples, but there are no experiments to support that aspect of the paper.

Our current dataset is rather small and only contains 1496 samples from 5 different videos, which is essentially a learning task with fewer samples.

Q4. It is not clear how the additional images are properly "leveraged" - Do they require kidney segmentation if not the full annotation?

These additional images have no labels on the types of kidney stones. We manually annotate the segmentation maps for them and include them only in the segmentation model training.

\end{document}
