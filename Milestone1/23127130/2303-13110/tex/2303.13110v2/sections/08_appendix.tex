\section*{Appendix}
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}

\noindent\textbf{Note:} We use \textcolor{blue}{blue} color to refer to section numbers \textbf{in the main paper}. All \textcolor{red}{red} and \textcolor{green}{green} characters refer to figures, tables, and citations in this supplementary material.
\vspace{0.3cm}

\noindent\textbf{Overview.} This supplementary material includes further information regarding the implementation details, results, and datasets discussed in the main paper, and summarized as,

\begin{itemize}
    \item We detail how the cell detection task is posed as a segmentation task, and how cell detection and tissue segmentation tasks are handled simultaneously. 

    \item We show tissue segmentation results to provide more insights on how large FoVs and the corresponding tissue label improve cell detection performance, as discussed in \textcolor{blue}{Subsection 6.4} and \textcolor{blue}{Tab. 7}.
    \item We share qualitative results comparing the \textit{Cell-only} and \textit{Pred-to-inter-2} models in OCELOT and CARP.

    \item Finally, regarding the datasets, we provide the amount of annotated cells and tissue pixels per data subset of OCELOT in \autoref{tab:label_stats_ocelot2}. For TIGER \cite{tiger}, we describe how the dataset is pre-processed in order to be used in our experiments.
\end{itemize}

\section{Annotation Protocol \textcolor{blue}{(Section 3.1)}}

\paragraph{Annotation rules.} For cell patches, annotators were asked to annotate the center point of each cell. For tissue patches, annotators drew contours as accurately as possible.

\paragraph{Consensus strategy.} All data were annotated by board-certified pathologists. Each tissue patch is annotated by a single pathologist. Each cell patch is annotated by three pathologists with the following consensus strategy. First, two pathologists annotate the same cell patch independently. Then, the third pathologist merges the two annotations taking the discrepancies into account. This strategy was specifically designed to reduce the naturally high inter-rater variability when annotating cells.
\input{sections/tab_ocelot_countings_v2}

\section{Implementation Details \textcolor{blue}{(Section 5.1)}}

\paragraph{Cell detection as segmentation.}
We define the cell detection task as a segmentation one, similarly to \cite{SwiderskaChadaj2019LearningTD}. At training time, we provide the cell labels as a segmentation map by drawing a disk centered on each cell point annotation. We use a fixed radius of 1.4 $\mu\text{m}$, corresponding to 7 pixels at a resolution of 0.2 Microns-per-Pixel (MPP). Then, we assign the value of each pixel within each disk to the corresponding cell label, e.g., 1 for TC and 2 for BC in OCELOT; 0 for the remaining background pixels. We utilize the Dice loss \cite{dice} for both cell and tissue branches, which is a widely used loss function for semantic segmentation.

At inference time, we post-process the probabilistic cell segmentation map, i.e., the output of the cell branch, to obtain a set of points, corresponding to the detection of the cells.
To that end, we apply \verb|skimage.feature.peak_local_max|\footnote{\scriptsize\url{https://scikit-image.org/docs/stable/api/skimage.feature.html\#skimage.feature.peak\_local\_max}} on the cell segmentation map to get the set of predicted points (cells). 
Lastly, we retrieve the class probability values of each cell from the segmentation maps and determine their class through \verb|argmax|. The class probability is used as the confidence score. 

\paragraph{Data augmentation.} 
During training, five data augmentations are randomly applied, including three photometric (gaussian blur, gaussian noise, color jitter) and two geometric (horizontal flipping, rotation by a multiple of 90$^{\circ}$) transformations.
In the case of geometric transformations, we apply the same transformation for cell and tissue patches within a pair to maintain the physical alignment between them (e.g. 90$^{\circ}$ for both cell and tissue patches).

\paragraph{Learning rate and dropout for cell and tissue branches.}
During experiments, we find that the convergence speeds of the cell detection and tissue segmentation tasks are different. The cell branch starts overfitting while the tissue branch is still learning. 
To address this behavior, we use different dropout probabilities and learning rates (LRs) for each branch.
In the case of dropout, a fixed probability value of 0.1 is used for the tissue branch. 
Conversely, we tune the cell branch by performing a grid search with 3 dropout probability values: 0.1, 0.3, and 0.5.
Note that the dropout layer is added at the end of each ResNet block. We use spatial dropout \cite{tompson2015efficient}.
In the case of the LR, while searching for the best hyper-parameter values, we constrain the LR of the cell branch to be the same or half of the LR of the tissue branch. This constraint is applied to reduce the search space. 

\section{More Cell Detection Baselines \textcolor{blue}{(Section 6.1)}}
We provide more cell detection baselines (U-Net \cite{unet} and MFoVCENet \cite{10.1007/978-3-030-59722-1_37}) on the OCELOT dataset. MFoVCE-Net is a strong baseline that further utilizes a large FoV patch as an input, but not a corresponding tissue annotation. \autoref{tab:more-baseline} shows that the proposed \textit{Pred-to-inter-2} model still outperforms all the baselines by a large margin. This emphasizes the importance of the additional larger FoV input and associated tissue label. In addition, the U-Net architecture shows lower performance than DeepLabV3+ \cite{deeplabv3plus2018}.

\setcounter{section}{3}
\setcounter{table}{0}
\input{sections/tab_more_baselines}

\setcounter{section}{4}
\setcounter{table}{0}
\input{sections/tab_ablation_supple}
\setcounter{section}{3}
\section{Ablation Study: Tissue Performance \textcolor{blue}{(Section 6.4)}}

Through the ablation study in the \textcolor{blue}{Tab. 7} of the main document, we observe improvements in cell detection performance by utilizing a large FoV or tissue segmentation label. Moreover, utilizing both components simultaneously shows synergy, leading to an even better performance improvement. 
In \autoref{tab:ablation_supple}, we investigate the tissue segmentation performance to better understand the reason for such synergy. 
By comparing the second and last rows in \autoref{tab:ablation_supple}, we observe that training with large input/label FoV tissue results in a better tissue model, which achieves higher mIoU in both validation and test sets. Therefore, the cell detection performance boost can be justified by the fact that the tissue model shares more accurate tissue information to the cell branch.

\section{Qualitative Results \textcolor{blue}{(Section 6.5)}}
We provide more examples for qualitative comparison between \textit{Cell-only} and \textit{Pred-to-inter-2} models. Visualizations of OCELOT can be found in \autoref{fig:supple_ours} and CARP in \autoref{fig:supple_carp}. We use a different color scheme for each figure since each dataset is based on different staining methods. The color scheme can be found in the captions.

% Qualitative figure
\begin{figure*}[t]
    \centering
    \vspace*{-2mm}
    \includegraphics[width=0.85\linewidth]{figures/supple_ocelot.pdf}
    \vspace{-3mm}
    \caption{\textbf{Qualitative results - OCELOT}. 
        The \emph{Pred-to-inter-2} model can correct the mistakes of the \emph{Cell-only} model by incorporating tissue prediction information during cell prediction.
        The colors represent the following classes:
        \textcolor{cyan}{$\mdlgblkcircle$} Tumor Cells (TC), 
        \textcolor{yellow}{$\mdlgblkcircle$} Background Cells (BC), and
        \textcolor{ForestGreen}{$\mdlgblksquare$} Cancer Area (CA).
    }
\label{fig:supple_ours}
\vspace{-4mm}
\end{figure*}

% Qualitative figure
\begin{figure*}[t]
    \centering
    \vspace*{-2mm}
    \includegraphics[width=0.85\linewidth]{figures/supple_carp.pdf}
    \vspace{-3mm}
    \caption{\textbf{Qualitative results - CARP}.
        The \emph{Pred-to-inter-2} model can correct the mistakes of the \emph{Cell-only} model by incorporating tissue prediction information during cell prediction.
        The colors represent the following classes:
        \textcolor{Orchid}{$\mdlgblkcircle$} PD-L1 positive tumor cells (TC+),
        \textcolor{orange}{$\mdlgblkcircle$} PD-L1 negative tumor cells (TC-), and
        \textcolor{ForestGreen}{$\mdlgblksquare$} Cancer Area (CA).
    }
\label{fig:supple_carp}
\vspace{-4mm}
\end{figure*}


\section{Details about TIGER \textcolor{blue}{(Section 6.1)}}
\paragraph{Annotations.} There is a single class annotation for the cell task, namely, lymphocyte cells. In contrast, 7 classes are considered for the tissue task: \textit{Invasive Tumor}, \textit{Tumor-associated Stroma}, \textit{In-situ Tumor}, \textit{Healthy Glands}, \textit{Necrosis not in-situ}, \textit{Inflamed Stroma}, and \textit{Rest}. In addition, TIGER considers the tissue class \textit{Excluded}, which has the same role as \textit{Unknown} in OCELOT. 

Based on the statistics in \autoref{tab:tiger_class_ratio}, we observe that most of the lymphocyte cells are located within stroma tissue areas, i.e., \textit{Tumor-associated Stroma} and \textit{Inflamed Stroma.} 
Also, the tissue annotations suffer from severe class imbalance. In fact, the frequencies of 4 out of 7 classes are lower than 5\%. 
To make the tissue task more straightforward, while maintaining the interrelation between lymphocyte cells and stroma tissue, we remap the tissue classes as follows: \textit{Tumor-associated Stroma} and \textit{Inflamed Stroma} are grouped into the \textit{Stroma} (\textit{ST}) class, and the remaining labels are remapped to \textit{BG} class.
Note that the main goal of this work is to explore cell-tissue relationships for improving the cell detection task, not to tackle the tissue segmentation task explicitly.

\input{sections/tab_tiger_class}
\input{sections/tab_supple_dataset_comparison}
\paragraph{Data pre-processing.} A pre-processing step is necessary for the TIGER dataset due to the inconsistent annotated ROI sizes for both cell and tissue samples. We can identify two different subsets in TIGER: 1) the sample pairs from the TCGA \cite{HUTTER2018283} database, and 2) the pairs from other sources, which we denote as non-TCGA pairs. On one hand, TCGA samples are composed of large annotated tissue patches that contain several smaller cell annotated ROIs within their region.
The number of cell ROIs per sample highly varies, reaching up to 58.
These cell ROIs are variable in size and most of them are smaller than 256 $\times$ 256 pixels. On the other hand, non-TCGA samples have a complete overlap between the cell and tissue patches, and the size of these patches is larger than 512 $\times$ 512 pixels. 

To maximize the amount of usable cell-tissue sample pairs, while maintaining the 4 times FoV difference across the cell and tissue tasks (as done in OCELOT and CARP), we define the cell and tissue FoVs to be 128 $\times$ 128 and 512 $\times$ 512 pixels, respectively. Note that the image patch size is considerably smaller than in OCELOT and CARP mainly because of the limited size of cell ROIs in TCGA samples. In addition, the pre-processing step is implemented differently according to the data source; TCGA samples (see \autoref{alg:tcga_preproc}) and non-TCGA samples (see \autoref{alg:nontcga_preproc}). As a result of this pre-processing step, each non-TCGA tissue patch is paired to $4^2$ different cell sub-patches. In contrast, for each cell ROI in TCGA, there can be up to $4^2$ surrounding tissue patches. Please, refer to \autoref{tab:supple_dataset_comparison} for a comparison of the statistics across OCELOT, CARP, and the pre-processed TIGER datasets.
\vspace{-1mm}

\input{sections/algo_tiger_preproc_tcga_v2}
\input{sections/algo_tiger_preproc_nontcga_v2}