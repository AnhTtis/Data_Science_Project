\section{Method}
\label{sec:approach}
In this section, we propose to utilize cell-tissue relationships through multi-task learning. 
First, we propose a set of approaches inspired by the \textit{tissue-label leaking} model described in \autoref{poc-exp}. These models replace the annotated tissue labels with predictions from an auxiliary tissue segmentation branch.
Second, we design a bi-directional information-sharing approach that shares features in both tissue-to-cell and cell-to-tissue directions. The proposed approaches are described in \autoref{subsec:pred-to-x} and \autoref{subsec:arch-search}

\subsection{Preliminary} \label{subsec:preliminary}
To deal with cell-tissue sample pairs, i.e., $(x_s, y_s^c)$ and $(x_l, y_l^t)$, we build a dual-branch architecture containing separate networks for the cell and tissue tasks. Similarly to \cite{SwiderskaChadaj2019LearningTD}, we define cell detection as a segmentation task. Specifically, the cell labels are provided as a segmentation map by drawing a fixed-radius circle centered on each cell point annotation and filled with the corresponding class label.
At inference time, we find local peaks within the predicted cell probability maps and output them as point predictions.
More details are provided in the supplementary material.
Treating cell detection as a segmentation task enables us to use the same architecture for both cell and tissue branches, which largely simplifies the training and tuning of the model and reduces the range of possible decisions, such as neural network architecture, or hyper-parameters.
We use DeepLabV3+~\cite{deeplabv3plus2018} as a base architecture for both branches and single-task models. 

\subsection{Tissue-prediction Injection Models} \label{subsec:pred-to-x}
These models are a simple and practical extension of the \textit{tissue-label leaking} model, where we inject the predicted tissue probabilities into the cell detection branch instead of leaking the tissue labels. We consider only one injection point in the cell detection branch, but explore four possible alternatives: (a) at the input (\textit{Pred-to-input}), (b) after the encoder (\textit{Pred-to-inter-1}), (c) after the ASPP module (\textit{Pred-to-inter-2}), and (d) after the decoder (\textit{Pred-to-output}). \autoref{fig:pred_to_x} depicts this family of models, denominated as \textit{Tissue-prediction injection}.
Since the tissue and cell patches represent different regions, we need to align the content between the tissue and cell feature maps before concatenation. Therefore, we crop the cell corresponding region from the tissue predictions, upsample them, and, finally, concatenate them in the channel dimensions of the feature maps of the cell branch, as illustrated in \autoref{fig:tissue_to_cell}.

\begin{figure}[t]
	\centering
    \includegraphics[width=\linewidth]{figures/fig4_font.png}
	\caption{\textbf{Tissue-prediction injection model} injects the tissue segmentation prediction into 1 out of 4 locations of the cell branch: (a) input, (b) after encoder, (c) after ASPP, and (d) after decoder.}
	\label{fig:pred_to_x}
    \vspace{-0.3cm}
\end{figure}

\subsection{Cell-Tissue Feature Sharing Model} \label{subsec:arch-search}
\textit{Tissue-prediction injection} models share the tissue prediction in a single location and direction, i.e., tissue-to-cell. To enable a more diverse and flexible cell-tissue information flow, we also explore bi-directional feature map sharing from cell-to-tissue (\autoref{fig:info_exchange}, left) and tissue-to-cell (\autoref{fig:info_exchange}, right). Considering these two operations, we conduct an architecture search procedure to find the optimal feature map sharing configuration between both branches. To limit the search space, we constrain it to only 3 positions in the architecture: after the encoder, after the ASPP module, or after the decoder. Furthermore, we also exclusively allow the branches to inject feature maps at the same depth or position.
Finally, we consider only the best-performing model among the $4^3$ candidates, which is presented in \autoref{fig:our_model}. 
We name this model as \textit{cell-tissue feature sharing}.


\begin{figure}[t]
    \centering
    \hfill
    \begin{subfigure}{0.44\linewidth}
        \includegraphics[width=\linewidth]{figures/tissue_to_cell_v2.png}
        \caption{Tissue to Cell}
        \label{fig:tissue_to_cell}
        \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/cell_to_tissue_v2.png}
        \caption{Cell to Tissue}
        \label{fig:cell_to_tissue}
    \end{subfigure}
    \hfill
	\caption{Information is shared between cell and tissue branches via channel-wise concatenation preceded by a shallow convolutional layer with $3\times3$ kernel size. Cropping and upsampling (in \autoref{fig:tissue_to_cell}) or downsampling and zero-padding (in \autoref{fig:cell_to_tissue}) is applied to match the patch sizes and pixel-alignment between two feature maps from different FoVs. The cell and tissue feature maps are represented in orange and blue, respectively. The red contour denotes the cell patch-associated region in the tissue patch, and the gray regions represent zero padding.}
    \label{fig:info_exchange}
\end{figure}

\begin{figure}[t]
	\centering
    \includegraphics[width=\linewidth]{figures/arch_search_new2.pdf}
	\caption{\textbf{Cell-Tissue Feature Sharing Model} has two branches for tissue segmentation and cell detection.  Information exchange occurs multiple times between the two branches, indicated by the red vertical arrows. Details regarding the information exchange procedures are described in \autoref{fig:info_exchange}.}
	\label{fig:our_model}
\vspace{-4mm}
\end{figure}
