\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[demo]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{cuted}
\usepackage{capt-of}
\usepackage{comment}
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{blindtext}% for dummy text only
\usepackage{graphicx}
\usepackage{caption}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1925} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}
% \onecolumn
%%%%%%%%% TITLE
\title{Supplementary Material: CLIP goes 3D}

% \maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT

% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle


% }]
\maketitle
% \thispagestyle{empty}

% \onecolumn
\section{Scene querying with language}
We provide further qualitative results that demonstrate the language-based querying capabilities of our proposed framework on point clouds of indoor scenes. Note that in the main paper, we pre-trained on ShapeNet which is not a real-world dataset. In order to imporve performance on scene-querying on real-world datasets, we perform additional pre-training on the ScanObjectNN dataset for querying on a collection of meshed indoor scenes from the S3DIS  \cite{2017arXiv170201105A} and ScanNet \cite{dai2017scannet} datasets.  

% \begin{e*}figure*}
%     \centering
%     \includegraphics[width=\linewidth]{iccv2023AuthorKit/figures/loss_supp.drawio.pdf}
%     \caption{Caption}
%     \label{fig:loss}
% \end{figur
\begin{figure*}[!htp]
    \centering
    \includegraphics[width=0.8\linewidth]{iccv2023AuthorKit/figures/supp_scene_query_s3dis.drawio.pdf}
    \caption{Qualitative results of scene-querying samples from the S3DIS \cite{2017arXiv170201105A} dataset.}
    \label{fig:s3dis}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{iccv2023AuthorKit/figures/supp_scene_query_scannet.drawio.pdf}
    \caption{Qualitative results of scene-querying samples from the ScanNet \cite{dai2017scannet} dataset.}
    \label{fig:scan}
\end{figure*}

% \subsection{Pre-training on ScanObjectNN}

During standard CG3D pre-training, we render the textured CAD models of ShapeNet to use as inputs to our visual encoder. Such rendering is not possible with ScanObjectNN, so we project each point cloud to a depth map in a random view. The text caption is curated in the standard procedure. 




\subsection{Scene querying on S3DIS}
We perform language-based scene querying on the indoor scene dataset S3DIS \cite{2017arXiv170201105A}. Each scene (disregarding the floor and ceiling regions, as is standard in semantic segmentation tasks) is clustered into regions, each of which is passed to CG3D along with a query containing the object to be localized. Some qualitative results may be seen in Figure \ref{fig:s3dis}. Each row shows an input indoor scene, the result of clustering, and the final result of the language query. We query the same scene for two different objects. In the second row, it can be observed that several instances of the queried category ``chair" are correctly identified. 

\subsection{Scene querying on ScanNet}
We also demonstrate the performance of the 3D encoder pre-trained on real data on indoor scene samples from the ScanNet \cite{dai2017scannet} dataset. Figure \ref{fig:scan} shows the result of querying on two samples. The quality of the results depends on the clustering accuracy, which can cause spurious results. 
 However, both instances of the queried object are correctly identified in both examples.
\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{Method}&\multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Overall Acc} \\ \cline{3-4} 
                &        & ShapeNet \cite{shapenet2015}     & ModelNet40 \cite{wu20153d} (ZS)    \\ \hline
PointCLIP \cite{zhang2021pointclip}   &    ViT-B       &   33.6           &           10.1       \\
CG3D-render  &      ViT-B + prompt     &      77.8        &     24.8             \\
CG3D-depth   &   ViT-B + prompt        &     37.0         &        34.4          \\ \hline

\end{tabular}
}
\caption{Comparison of classification performance of CLIP visual encoder and CG3D visual encoder on the ShapeNet and ModelNet datasets. CG3D is pretrained on ShapeNet.}
\label{tab:img}

\end{table}



\begin{table}[]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc}
\hline
\multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Linear probe acc} \\ \cline{3-4} 
                        &                           & ShapeNet       & ModelNet40       \\ \hline
PointCLIP               & ViT-B                     &      70.31          &      43.31               \\
CG3D-render             & ViT-B + prompt            &    82.47            &   46.15                  \\
CG3D-depth              & ViT-B + prompt            & 70.38          & 68.80               \\ \hline
\end{tabular}
}
\caption{Comparison of linear probe classification performance of CLIP visual encoder and CG3D visual encoder on the ShapeNet and ModelNet datasets.}
\label{tab:lp}

\end{table}

\section{Leveraging prompt-tuning for images}
% Training CLIP's visual encoder through the tuning of prompt tokens enables improved zero-shot capabilities for images in addition to the established zero-shot capabilities of the 3D encoder. In order to leverage the visual encoder on 3D shapes, we project each point cloud to a 2D depth map. In Table \ref{tab:} we compare the .... [TODO]
In the main paper, we showed how we can train a 3D encoder in the CLIP framework and illustrated its benefits. We also introduced prompt tuning in the 2D encoder of CG3D to tune it towards rendered 3D shapes and objects. Performing contrastive visual prompt tuning not only aids in pre-training the 3D encoder, but also allows us to leverage CLIP's visual encoder for 3D shapes. Using visual prompts allows us to train the visual encoder without forgetting the already existing weights of CLIP. Thus, this helps us obtain a new visual encoder that has a capability of performing better than normal CLIP encoder on image-based 3D tasks. By image-based 3D tasks, we mean applications such as PointCLIP \cite{zhang2021pointclip} where  zero-shot classification is performed by forwarding depth maps of each point cloud directly to the visual encoder.

 % Methods that apply CLIP to 3D shapes such as PointCLIP \cite{zhang2021pointclip} perform zero-shot classification by forwarding depth maps of each point cloud directly to the visual encoder. Since CLIP is trained on natural images, there exists a gap in distribution from the test data, which affects performance. By injecting learnable prompt tokens in each transformer layer of the visual encoder, we tune CLIP to operate on images of 3D shapes. We now have a CLIP image encoder specific to 3D shapes along with the 3D encoder.
% \begin{table}[]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{cccc}
% \hline
% \multirow{2}{*}{Image backbone} & \multicolumn{3}{c}{ZS acc} \\ \cline{2-4} 
%                                 & MN10 & MN40 & ScanObjectNN \\ \hline
% ViT-B + shallow prompt          &   64.2   &   43.2   &        20.5      \\
% ViT-B + deep prompt             &  67.3    &   50.6   &     25.6         \\ \hline
% \end{tabular}
% }
% \caption{Ablation study on visual prompting.}
% \label{tab:vpt}

% \end{table}

%\multicolumn{1}{c|}{67.3}       & \multicolumn{1}{c|}{50.6}      & \multicolumn{1}{c}{25.6}

We examine the effectiveness of tuning the visual encoder using our method by comparing the classification and zero-shot classification performance on the ShapeNet and ModelNet datasets respectively. In column 3 of Table \ref{tab:img}, we compare the overall classification accuracy of single-view PointCLIP \cite{zhang2021pointclip} against the prompt-tuned visual encoder on rendered images of ShapeNet objects. Column 4 compares zero-shot classfication performance on the 40-class split of ModelNet. CG3D-render denotes that the framework uses rendered images of ShapeNet objects during pre-training. CG3D-depth denotes that depth projections are used as inputs to the visual encoder during pre-training. 



We observe that performing visual prompt tuning of the 2D encoder significantly improves the performance of CLIP visual encoder in CG3D for 3D shape datasets. Classification performance on rendered ShapeNet images improves by 44.2\% points and and zero-shot performance on ModelNet depth images improves by 24.3\% points. The best performance is observed when the modality of the pre-training image dataset aligns with the modality during evaluation. Nonetheless, we observe improvements even when the modalities do not match.  


As a further demonstration of the quality of image features obtained after visual prompt tuning, we perform linear probing as done in \cite{radford2021learning} by performing logistic regression on the learned image features from the visual encoder. In Table \ref{tab:lp}, we compare the linear probe performance using image features from the out-of-the-box vision encoder of CLIP (as used in \cite{zhang2021pointclip}) with the visual encoder with tuned prompt tokens. A trend similar to Table \ref{tab:img} can be observed, where prompt tuning results in improved classification performance, with the best performance occurring when the modalities of the train and test data match. This means that the visual encoder is able to effectively learn image features specific to 3D shapes. This opens up numerous possibilities in multi-modal learning between 3D shapes and images. 


% \section{Ablations on prompt-tuning}
% We examine further the role prompt tokens have on zero-shot performance. We implement two variations of visual prompt tuning on transformers \cite{jia2022visual}, namely shallow and deep prompts. While using shallow prompts, learnable prompt tokens are injected only at the input to the visual encoder. In deep prompting, a series of learnable tokens are input at the input of each encoder in the transformer block. Table \ref{tab:vpt} shows a comparison of zero-shot performance of the PointTransformer 3D encoder when trained under the CG3D framework using each prompting strategy. The use of deep prompts clearly aids 3D zero shot performance, particularly in the case of ScanObjectNN \cite{scanobjectnn}, where there is a 5.1\% point increase from shallow prompting. This means that more learnable parameters at every stage of the transformer encoder results in a visual encoder that provides more meaningful image features for contrastive training with shape features. This observation leads us to choose deep prompting as the design for tuning the visual encoder. 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
