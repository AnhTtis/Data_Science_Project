\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage[misc]{ifsym}

\newcommand{\tww}[1]{\textcolor{red}{[tww]: #1}}
\newcommand{\hy}[1]{\textcolor{blue}{[HY]: #1}}
\newcommand{\hm}[1]{\textcolor{cyan}{[hm]: #1}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}
  \endgroup
}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage{color}
\definecolor{mycitecolor}{rgb}{0, 0.4, 0.7}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=true,citecolor=mycitecolor]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2397} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
% \title{Revisiting NeRF for 3D Data Augmentation in Autonomous Driving}
% \title{Revisiting Data Augmentation for 3D Perception}
\title{3D Data Augmentation for Driving Scenes on Camera}

\author{
% Wenwen Tong\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
Wenwen Tong$^{1*}$\quad
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
Jiangwei Xie$^{1*}$\quad
Tianyu Li$^{2*}$\quad
Hanming Deng$^{1*}$
% \thanks{
% % \normalsize{
% % $^{*}$
% % indicates 
% Equal Contributions. 
% % \quad
% \\
% \textsuperscript{\Letter}Corresponding author at
% \texttt{lihongyang@pjlab.org.cn}
% % }
% }
\quad 
Xiangwei Geng$^{2}$ \quad \\ 
Ruoyi Zhou$^{1}$\quad
Dingchen Yang$^{2}$ \quad
Bo Dai$^{2}$ \quad
Lewei Lu$^{1}$ \quad
Hongyang Li$^{2}$\textsuperscript{\Letter}
\\
[2mm]
$^{1}$SenseTime Research \quad
% $^{2}$Beihang University \\
% $^{2}$OpenDriveLab, Shanghai AI Laboratory \\
$^{2}$Shanghai AI Laboratory \\
% \quad $^{4}$The Chinese University of Hong Kong
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Driving scenes are extremely diverse and complicated that it is impossible to collect all 
% the scenes 
cases
with %only 
human effort alone. 
% Data augmentation is an effective technique to enrich the training data. 
% But 
% Existing methods have limited effectiveness in autonomous driving applications since they are often confined to the 2D image plane, which may not optimally increase data diversity in 3D real-world scenarios.
While data augmentation is an effective technique to enrich the training data, existing methods for camera data in autonomous driving applications are confined to the 2D image plane, which may not optimally increase data diversity in 3D real-world scenarios.
% such as 
% and thus unable to rotate the vehicles. 
% \hy{lack transition}
To this end, 
% are mostly limited to augmenting images by moving objects with depth constraint in the image plane, which lack the flexibility to rotate objects 
% or using synthetic data from simulators, which usually have a domain gap with real data. 
we propose a 3D data augmentation approach termed Drive-3DAug, aiming at
augmenting the driving scenes on camera in the 3D space.
% 
We first utilize Neural Radiance Field (NeRF) to reconstruct the 3D models of background and foreground objects. Then, augmented driving scenes can be obtained by placing the 3D objects with adapted location and orientation at the pre-defined valid region of backgrounds. 
As such, 
the training database could be effectively scaled up.
% new images are generated 
% by rendering these scenes.
%The depth supervision is introduced to speed up the model training.
% Since object NeRF is modelled based on extract object pixel, the noise mask reduce the model quality. Thus we introduce the geometrical rectified loss to help improve the reconstruction quality of object model.
%We decompose every scene in the data into several backgrounds and objects. Then we use voxel-based NeRF to build their 3D models from images and combine them  of objects. The images are generated through scene rendering.
% \hy{lack transition. maybe here}
% For points
However, the 3D object modeling is constrained to the image quality and the limited viewpoints.
% driving scenes are complex, which occluded and have limited views of objects.}
% To ensure quality of 3D models and diversity of generated driving scenes, 
% especially for vehicles, 
To overcome these problems,
we modify the original NeRF by introducing a geometric rectified loss and a symmetric-aware training strategy.
% so that we can rotate and render the object in a large viewpoint.
% The constructed 3D models serves as the model assets, then we can random choose bachground model and object models with manipulation of object to augment data and generate new driving scene.
%to be able to rotate the objects with a larger degree, we design a symmetric-aware sampling strategy for training of object models because of the limited viewpoints of objects in the driving data. 
We evaluate our method for the camera-only monocular 3D detection task on the Waymo and nuScences datasets. The proposed data augmentation approach contributes to a gain of $1.7\%$ and $1.4\%$ 
% improvement 
in terms of detection accuracy, on Waymo and nuScences respectively.
%
Furthermore, the constructed 3D models serve as digital driving assets and could be recycled for different detectors or other 3D perception tasks. 
% Code 
% % and models 
% would be available at \url{https://github.com/opendrivelab/Drive-3DAug}.

% increase the range of available viewpoints of objects so that we can
% 3D data augmentation is essential for 3D perception systems in autonomous driving while being seldom explored, especially for the camera-based tasks. Existing methods are limited to augmenting images in the 2D image plane or using synthetic data from simulators, which either lacks the flexibility for augmenting 3D objects or does not match the real data in terms of styles and contents. In this work, we revisit the camera-based 3D data augmentation problem and propose the first 3D data augmentation method, named xxx, that can generate new driving scenes by transforming objects at arbitrary pose across different backgrounds in the 3D space. Considering NeRF has shown potential in 3D reconstruction and rendering photographical images, we use voxel-based NeRF to efficiently convert each scene into a background voxel field (BVF) and a batch of object voxel fields (OVF). For the training of OVF, we also develop a modified NeRF with a geometric rectified loss and a symmetric-aware sampling strategy to enhance its quality and also broaden the range of available views. Given the BVFs and OVFs, we can easily manipulate the objects and place them in the plausible positions of different backgrounds to render new images. We evaluate our method for the monocular 3D detection task on the Waymo and nuScences datasets and achieves up to $1.7\%$ and $2.5\%$ improvements for different detectors. Moreover, we also demonstrate the effects of each components and different augmentation strategies through extensive ablation studies.

\end{abstract}

\blfootnote{*: Equal contribution.}
\blfootnote{\Letter: Corresponding author at
\texttt{lihongyang@pjlab.org.cn}.}

%%%%%%%%% BODY TEXT
\section{Introduction}
% Camera-based 3D 
3D perception system, particularly 3D object detection, plays a vital role for autonomous driving. Despite recent progress \cite{brazil2019m3d, li2019gs3d, xu2018multi, li2020rtm3d,  mousavian20173d, qin2019monogrnet,  qin2019triangulation,  shi2021geometry, zhang2021objects, zhang2021learning, chen20153d, park2021pseudo, wang2021fcos3d, wang2022detr3d, weng2019monocular}, the current perception system still suffers from the hard case challenge due to the long-tail driving scenes on the road, \textit{e.g.} trucks with diversified pose on the road. 
% It is expensive to collect all scenarios to improve the perception system, since most of them are rare and dangerous to obtain, especially for vehicles. 
To overcome this challenge, data augmentation has been proven to be an effective technique to enrich training data. For LiDAR-based 3D perception, different data augmentation methods \cite{reuse2021ambiguity,fang2021lidar,abs-2004-01643} have made great achievements by generating new drive scenes. 
% However, existing image data augmentation can only generate limited number of new driving scenes. 
However, it is still under-explored how to augment the driving scenes for camera-based 3D perception with data augmentation.
% How to address this issue remains one of the core challenges in autonomous driving.

\begin{figure}
  \centering
  \includegraphics[width=.47\textwidth]{fig/motivation_07-20.pdf}
  % \caption{\textbf{Performance Gain} Monocular 3D detection results of different data augmentation with visualization on Waymo. Each bubbleâ€™s area is proportional to the theoretical data amount could be generated. Compared with previous 2D Aug or 2.5D Aug, our Drive-3DAug method, which is 3D data augmentation, can generate more diverse data with the ability to rotate the object from the camera view and also achieve more performance gain+ for camera-based 3D perception tasks.
  % \hy{dramatic refine please!}
  % }
    \caption{\textbf{Visualization of different data augmentation techniques and their \emph{performance gain} of LET-AP \cite{hung2022let3dap} for monocular 3D detection on Waymo \cite{sun2020scalability}} with 100 scenes augmented. Compared with previous 2D image data augmentations, our Drive-3DAug method, modifies the driving scenes in the 3D space. This can generate more diverse driving scenes and contribute larger performance gain to camera-based 3D perception tasks. Baseline is the FCOS3D \cite{wang2021fcos3d} method.
  }
  \label{fig:motivation}
\end{figure}

% As depicted in Figure \ref{fig:motivation}, data augmentation is regarded as an effective technique  \cite{shorten2019survey, zoph2020learning} to enrich training data. A simple solution \cithe te{autoaug} is to utilize transformations 
% (scale, flip, crop, \textit{etc}.)
% to augment the image as a whole. It cannot generate new driving scenes, limiting its effectiveness. 
% %
% Copy-paste \cite{copypaste} and depth-aware copy-paste \cite{lian2022exploring} can create a new driving scene by moving object pixels from one image to another with optional depth constraints. Since these pixel-based approaches are limited to the image plane, it is challenging to generate data with diverse viewpoints of objects, such as rotating objects in accordance to the 3D-imaging. 
% %
% By contrast, 3D data augmentation manipulates objects in 3D space and obsesses more degree of freedom to create a wide span of scenes.
% % and can contribute to larger improvement of the 3D perception system. 
% There are some 3D data augmentation approaches \cite{fang2021lidar,abs-2004-01643,abs-2208-00223} curated for LiDAR-based 3D perception tasks -  manipulating points of objects in the LiDAR coordinate system.
% %
% Although they demonstrate impressive improvement, these options cannot be directly applied to
% generate novel images in our setting, \hy{due to the modality discrepancy between LiDAR and camera. why?}
% % images because of the limited views and rich semantics of image data. 
% % Because image has much richer semantics than point cloud, which means purely editing pixels may make the image unnatural and flawed. 
% One might argue that
% % There are other methods using 
% simulator \cite{carla,ShahDLK17} is a powerful tool to generate synthetic 3D imagery to supplement the database. However, the sim2real bottleneck is a long-standing issue to be resolved.
% % But this approach often leads to domain gaps as the synthetic data do not match the real-world, thus being an issue to address.

As illustrated in Figure \ref{fig:motivation}, existing image data augmentation approaches are mostly restricted to the 2D image plane, such as image transformations \cite{autoaug} and copy-paste \cite{copypaste, lian2022exploring}. These techniques face challenges in changing the view of the components in the scene, such as rotating objects within the image, thereby limiting the diversity of generated driving scenes. By contrast, the data augmentation approaches \cite{fang2021lidar,abs-2004-01643,abs-2208-00223} for point clouds are applied in the 3D space, offering more degrees of freedom to change the driving scenes. Although existing data augmentation approaches for image data have achieved some performance gains for the 3D detection task, such gains are still limited compared with the improvement brought by the 3D data augmentation approaches \cite{reuse2021ambiguity} for the LiDAR-based 3D perception tasks. 
This means the diversity of generated scenes is essential to improve the performance of 3D perception tasks. The manner of augmenting data in the 3D space is an effective way to create diverse scenes.
% data augmentation is an effective technique  \cite{shorten2019survey, zoph2020learning} to enrich training data and improve the performance. 
% data augmentation \cite{} demonstrates impressive improvement for LiDAR-based 3D perception tasks by manipulating points of objects in the 3D space. 
% - manipulating points of objects in the 3D space to create new driving scenes. 
% However, these options cannot be directly applied to generate new images due to the modality discrepancy between the point cloud and images. 
% For camera-based 3D perception, 
% a commonly used solution \cite{autoaug} is to utilize transformations (scale, flip, crop, \textit{etc}.) to augment the image as a whole. It cannot generate new driving scenes, limiting their effectiveness. Copy-paste \cite{copypaste, lian2022exploring} can create new driving scene through moving object pixels among images with optional depth constraints. 
% However, these pixel-based 
% , but it is still limited compared with the 3D data augmentation for point cloud. These approaches are mostly restricted to the image plane. \tww{It is challenging to generate data with diverse new driving scenes as those for point cloud, such as rotating objects in accordance to the 3D-imaging.} 
One might argue that simulator \cite{carla,ShahDLK17} is a powerful tool to generate synthetic 3D imagery to supplement the database. However, the sim2real bottleneck is a long-standing issue to be resolved. 



% To achieve the 3D data augmentation method for image data, the challenge lies on two aspects. The first is how to edit the existing data to create new scenes. To solve this problem, we decompose one scene into a set of backgrounds and foreground objects and create new scenes by combining them arbitrarily. The second is how to convert the scenes from pixels to the 3D space. Considering recent progress of NeRF that can reconstruct 3D models from multi-view images, we adopt NeRF for the purpose of 3D data augmentation.

% \hy{Add more intuition as to why refined NeRF is a necessity; a favorable / desirable solution towards camera-based 3d aug is to XXXX (motivation for our novel contribution, aka, loss and training strategy)} 
% However, these options cannot be directly applied to generate new images due to the modality discrepancy between point cloud and images. 
In this work, we have pioneered research into 3D data augmentation for camera-based 3D percpetion for in autonomous driving. 
% Because image has rich semantics and limited views, operations, such as rotating objects on the image plane, will generate flawed images. 
% Manipulating object on 2D image plane satisfying 3D-imaging constraint, such as rotating objects pixels, will generate flawed images.
% To overcome this, we need to convert the scenes to the 3D space so that more operations can be implemented. 
To implement 3D data augmentation for image data, we need to convert the scenes to the 3D space. This is because manipulating objects on 2D image plane satisfying 3D-imaging constraint, such as rotating pixels of objects, will generate flawed images.
One desirable solution is using Neural Radiance Field (NeRF)~\cite{mildenhall2020nerf,park2021nerfies,yu2022plenoxels} to reconstruct the 3D models of background and foreground objects obtained by decomposing the scene, then we can compose them for data augmentation.
% How to convert the scenes in the 3D space for scenes editing
However, it is hard to achieve perfect decomposition without ground truth to extract objects. Pixels of the object will be mixed with the background pixels near the object edge.
In addition, the limited viewpoints of objects in driving scenes make it difficult to apply NeRF for generating objects of novel views by large rotation, further limiting the diversity of generated scenes.
% Furthermore, it is hard to achieve perfect decomposition due to the complexity of driving scenes.
% % to 3D image data augmentation. 
% % For the convenience of scene editing, decomposing scenes is a preferable approach. 
% However, it is hard to achieve perfect decomposition due to the complexity of driving scenes. The extracted objects are likely to be mixed with the background pixels near the object edge.
% Furthermore, reconstruction from limited object viewpoints restricts the range of rotation, further limiting the diversity of generated scenes.

% which aims at creating more driving scenes by editing real scenes in the 3D space. For the convenience of scene editing, we need to decompose one scene into a set of backgrounds and foreground objects. Then, to have more degrees of freedom for creating more diverse new scenes, it is required to convert these backgrounds and objects from pixels to 3D models so that we can manipulate the scenes in the 3D space. After these, we can combine objects and backgrounds arbitrarily in the 3D space to generate various driving scenes. To alleviate the domain gap, we should create new scenes based on the training data with insurance of their reasonability. 
 
% Such a 3D data augmentation method is able to cover more data than previous 2D or 2.5D methods and contribute to better improving the performance of 3D perception systems. In this work, we have pioneered research into the 3D data augmentation problem, which involves enhancing data by editing real scenes of images in 3D space. The key challenge lies in building the 3D models of the foreground objects and backgrounds from images with low cost and fast speed. Once we obtain these models, we can combine them arbitrarily and freely create novel scenes by adjusting the poses of the foreground objects and placing them in the plausible positions of any background. we adopt NeRF for the purpose of 3D data augmentation.


% Details
To this end, we present a novel 3D data augmentation approach for 2D images, named Drive-3DAug. Our approach has two stages. The first stage is to build the 3D models. We decompose the driving scenes into multiple backgrounds and foreground objects and use voxel-based NeRF \cite{sun2021direct} to turn them into 3D models.
% use the predicted masks of an off-the-shelf instance segmentation model to extract foreground objects from the backgrounds. 
%
% Inspired by NeRF \cite{mildenhall2020nerf,tancik2022block,deng2022depth} that can reconstruct 3D models from multi-view images and generate photo-realistic novel view images,
% we use NeRF to reconstruct the driving scenes including the background and foreground objects models with multiple consecutive frames.
To overcome the difficulties of reconstructing the driving scenes, 
% we modify the NeRF to generate more diverse photo-realistic images.
% First,  To improve the quality of object models
we propose a geometric rectified loss to weaken the effect of noisy edges
% and prevent the leaking of background pixels into object models. 
% Note that the extracted object mask is mixed up the background pixels near object edge, using the noisy mask for object model training can not represent accurate object geometry.
% Thus we propose a geometric rectified loss to improve the object models by weakening the effect of noisy mask near object edges and prevent the leaking of background pixels into object models. 
% Due to imperfect object extraction, we modify the voxel-based NeRF by adding a geometric rectified loss that xxx to preventing object models from interfering with background pixels. 
% Moreover, because of the limited viewpoints of objects in driving data, we design 
and a symmetric-aware training strategy for objects with symmetry, such as vehicles, 
% to construct a symmetric 3D model 
to broaden the available rendering viewpoints.
% making it possible to rotate the object with much larger angles. 
% Moreover, considering the limited viewpoints of objects in driving data, we also design a symmetric-aware sampling strategy to create the symmetric-view image. In this way, we can model the object with large viewpoints, making it possible to rotate the object with much larger angles. 
%
After the first stage, we re-compose the 3D models of backgrounds and foreground objects to create new driving scenes. Considering the physical constraints in the real world, we adopt a strategy to identify the valid region of the backgrounds. The augmented images are then generated by rendering the constructed scenes and can be further applied to the training of 3D perception tasks. Compared with existing approaches\cite{lian2022exploring}, our method contributes to a larger performance gain of the 3D detection task, as described in Figure \ref{fig:motivation}. 


% (1) The first stage is to construct the 3D models. Given a driving scene, we first employ an off-the-shelf instance segmentation model to separate foreground objects from the background. Then we use voxel-based NeRF to build both the background voxel field and object voxel field from the images as the 3D models. To improve the reconstruction speed and quality for complex driving scenes, we extend voxel-based NeRF with additional supervision using LiDAR data to regularize depth. Moreover, we modify the voxel-based NeRF approach for the objects models by adding a geometric rectified loss and a symmetric-aware sampling strategy in training, preventing object models from interfering with background pixels due to imperfect object extraction and increasing the range of available viewpoints which are limited in driving data.



% (2) The second stage involves generating augmented data from 3D models. 
% We can choose an object model randomly, and then place it on the identified plausible positions of any background model with different location and rotation to generate new driving scenes.
% Through placing object models on the background models with different locations and orientations in the 3D space, we can easily create plenty of 3D driving scenes, satisfying occlusion, depth and other constraints.
% Given the OVFs and BVFs, we can manipulate the objects with arbitrary poses and identify the plausible positions on the road. 
% We also design various data augmentation strategies to place the objects, 
% The augmented images are first generated by rendering the constructed scenes and then applied to the training of 3D perception tasks.

% Experiments
We evaluate Drive-3DAug for the monocular 3D detection task, as it is one of the most important 3D perception tasks in autonomous driving, on the Waymo~\cite{sun2020scalability} and nuScenes~\cite{caesar2020nuscenes} datasets with different detectors. 
% We focus on the vehicles as the foreground objects in this work because it is the most essential objects in driving scenes. 
Our method is able to achieve $1.7\%$ improvement of performance on Waymo, especially $2.2\%$ for vehicles with rare orientations, and $1.4\%$ on nuScenes on their corresponding metrics. 
% Compared with NeRF that requires days for reconstructing one scene, our method only needs half an hour. 
Moreover, once the 3D models of the backgrounds and foreground objects are reconstructed, they can serve as a digital driving asset, which can be used repeatedly for different tasks.


% Contribution
To sum up, our contributions are three-fold:
\begin{itemize}
    \item We have pioneered research into the 3D data augmentation problem for camera-based 3D perception in autonomous driving.
    \item We propose a 3D data augmentation approach based on NeRF including improvements by a geometrically rectified loss and a symmetric-aware training strategy to generate more natural and diverse images.
    \item We evaluate our method on the Waymo and nuScenes dataset, demonstrating that it can improve the performance of 3D object detection, and the 3D models can be further used as digital driving assets.
\end{itemize}

\begin{figure*}[t]
  \centering
  \includegraphics[width=.99\textwidth]{fig/pipeline2.pdf}
  \caption{\textbf{Overview of Drive-3DAug} for 3D data augmentation. Driving scenes are decomposed into multiple backgrounds and objects. For each background and object, we use multi-frame views to reconstruct them separately by voxel-based NeRF \cite{sun2021direct}. To further improve the reconstruction quality, we introduce the symmetry constraint, geometry rectification, and depth supervision for the NeRF. We edit the scene in 3D space with the manipulation of trained 3D models, and the images are generated by rendering the composed new scenes for the following 3D perception tasks.}
  \label{fig:pipeline}
\end{figure*}

% ----------------------------------Related Work---------------------------------------

\section{Related Work}

\paragraph{Data Augmentation for 3D perception.} % Image-based 3D data augmentation
Data augmentation is a powerful technique to improve performances of perception algorithms~\cite{shorten2019survey, zoph2020learning}. For 3D perception, augmentation methods vary in data modalities. For point cloud data, flipping, rotation and translation of objects and backgrounds are common techniques~\cite{cheng2020improving, hahner2020quantifying, yan2018second, choi2021part}. For image data, 2D augmentation methods can be lifted to 3D with geometry constraints. \cite{lian2022exploring} improve random scale, crop and copy-paste from 2D to 3D with 2D--3D geometry relationship. For multi-modality data, \cite{zhang2020exploring, wang2021pointaugmenting} are proposed to keep a consistency between images and point clouds. 
Although these methods improve the performance of 3D perception tasks, the operations on the image plane are much less flexible than the 3D data augmentation for point clouds, \textit{e.g.}, unable to rotate vehicles. Besides, aggressive augmentation techniques on images always violate geometry constraints and lead to unnatural, flawed data. In comparison, the approach we proposed can generate images of diverse driving scenes with more degrees of freedom by object manipulation in the 3D space, which takes geometry constraints and occlusions into account at the same time.

\paragraph{NeRF for Scene Generation.}
% NeRF \cite{mildenhall2020nerf} is a breakthrough in novel view synthesis task, which represents a scene with a fully connected neural network and optimizes it with differentiable volume rendering. As a powerful tool to reconstruct 3D models from images, NeRF has been applied to autonomous driving scenarios, which are large and unbounded~\cite{tancik2022block, li2022read, muller2022autorf, fu2022panoptic}. Block--NeRF~\cite{tancik2022block} reconstructs a whole city by merging multiple block-NeRFs with predicted visibility. READ~\cite{li2022read} constructs realistic autonomous driving scenarios, utilizing point--based NeRF. Auto--RF~\cite{muller2022autorf} focus on vehicle reconstruction with single image in autonomous driving scenes. However, autonomous driving scene reconstruction is still computation and time consuming since accelerated NeRFs (\textit{e.g.}, voxel based NeRF~\cite{yu2022plenoxels, sun2021direct, mueller2022instant}, depth supervised NeRF~\cite{deng2022depth}) has not been adopted. Recently, some researchers begin to explore how to edit scenes with NeRF~\cite{yang2021objectnerf, zhang2021stnerf, yuan2022nerf}. Object--NeRF~\cite{yang2021objectnerf} reconstructs scenes in a decoupled manner, modeling background and objects separately, which enable manipulation on objects in indoor scenes. ST-NeRF~\cite{zhang2021stnerf} utilizes multi--layer NeRF to model different objects, achieving editable scenes. READ~\cite{li2022read} realizes autonomous driving scenes editing with point based NeRF. However, previous scene editing works mostly focus on scene image generation, which do not extend to 3D perception tasks and produce corresponding 3D labels.

NeRF \cite{mildenhall2020nerf} is a powerful tool for novel view synthesis, which represents a scene with a fully connected neural network and optimizes it with differentiable volume rendering. It has been recently applied to autonomous driving scenarios \cite{tancik2022block, li2022read, muller2022autorf, fu2022panoptic}. Block--NeRF~\cite{tancik2022block} reconstructs a whole city by merging multiple block-NeRFs with predicted visibility. 
% READ~\cite{li2022read} constructs realistic autonomous driving scenarios, utilizing point--based NeRF. 
Auto-RF~\cite{muller2022autorf} focuses on vehicle reconstruction in autonomous driving scenes. 
% with single image of multiple vehicles
Considering the majority of NeRF methods are per-scene fitting, how to quickly reconstruct a scene is an important challenge because of the large scale of driving scenes. To overcome this, voxel-based NeRF~\cite{yu2022plenoxels, sun2021direct, mueller2022instant} and depth-supervised NeRF~\cite{deng2022depth}, which are adopted in our method, have been proposed to accelerate the training of NeRF. Moreover, some works begin to explore how to edit scenes with NeRF~\cite{OstMTKH21,KunduGYFPGTDF22,li2022read} for autonomous driving. 
However, these works are not extended to augmenting the data for 3D perception tasks. PNF~\cite{KunduGYFPGTDF22} does not consider driving data only has few views for most objects, which restricts the quality of novel view synthesis.
READ~\cite{li2022read} realizes autonomous driving scene editing with larger viewpoints of objects by point-based NeRF. However, this method relies on LiDAR data which limits its application scope. By contrast, the LiDAR data is not necessary for our method.

% Object--NeRF~\cite{yang2021objectnerf} reconstructs the in-door scenes in a decoupled manner, modeling background and objects separately, which enable manipulation on objects. ST-NeRF~\cite{zhang2021stnerf} utilizes multi-layer NeRF to model different objects, achieving editable in-door scenes.


% ----------------------------------Method---------------------------------------



\section{Method}

Our goal is to create diverse driving scenes for improving 3D perception tasks, especially for camera-based 3D detection, by aid of 3D data augmentation. To this end, we propose 
% our method -- 
Drive-3DAug. 
% We first give an overview of our method in Section \ref{overview}. 
% % including the NeRF training process and data augmentation process.
% Then we describe the construction of 3D model assets in Section \ref{reconstruct} and how to improve their quality and diversity in Section \ref{improve}.
% At last, we discuss how to use the 3D model assets to augment the training data in Section \ref{generation}.

\subsection{Overview}~\label{overview}
As demonstrated in Figure \ref{fig:pipeline}, Drive-3DAug has two stages for implementing 3D data augmentation. 
\paragraph{Stage 1 - Training.} The first stage is to construct the 3D models from images for the following generation of new driving scenes. To achieve this, we decompose the scenes into backgrounds and foregrounds and build the 3D models of them by training the NeRF. Then we can edit the scenes in the 3D space. Considering the characteristics of driving data, we further develop several techniques to improve the training process. 
\paragraph{Stage 2 - Generation.} The second stage is to augment the training data through the 3D models. 
To create new driving scenes, we combine the models of foreground objects and background scenes in the 3D space with the manipulation of objects including the location and orientation jittering. 
In order to make the generated scenes close to the real-world driving scene, e.g., we cannot place vehicles on a tree, we design a strategy to identify the valid region of the background scene where we place the foreground object. Then we use volume rendering to generate new images of these scenes for the training of camera-based 3D perception tasks. 

\subsection{3D Model Training}~\label{reconstruct}
To edit driving scenes for 3D data augmentation, we need to construct a set of 3D models for backgrounds and foregrounds. To achieve this, we first utilize an off-the-shelf instance segmentation model to extract the objects from the backgrounds. Then we use the Intersection of Union (IoU) of the object masks and the projection of 3D boxes on images to match the extracted object and the 3D annotation.
After this, we use NeRF to reconstruct the 3D models from images. 
We reconstruct the 3D objects based on the extracted masks in consecutive frames by matching the object masks with IoU constraint, and we only consider the totally visible objects with intact masks for 3D reconstruction. 
% Existing NeRF methods model the background and foreground objects jointly based on multi-view images.  Because the number of available views for the backgrounds and foregrounds are often few,
% we obtain the multiple frames of one object by matching the object masks with IoU constraint in consecutive frames to and choose the unoccluded objects with complete masks for 3D modeling. 
To model the static backgrounds, the moving object masks in the images are filtered.
% we manually decide how many consecutive frames are used for reconstruction and build the static background model by filtering the moving objects with the matched 3D box information.
% For the static or moving foreground objects, we choose the unoccluded objects with complete masks in the dataset for 3D modeling.  Moreover, We can classify the extracted mask into static object mask and moving object mask with the matched 3D box information.

To efficiently reconstruct the backgrounds and objects, we use voxel-based NeRF \cite{yu2022plenoxels,sun2021direct,mueller2022instant} instead of MLP-based NeRF \cite{mildenhall2020nerf,park2021nerfies}. Voxel-based NeRF has a density voxel grid $\boldsymbol{V}_{\text{density}}$, and a feature voxel grid $\boldsymbol{V}_{\text{color}}$ with shallow MLPs to represent the geometry and appearance respectively. 
% NeRF can reconstruct the 3D modelling of the scene given multi-view images by rendering the image pixel along the ray. 
% Given input query of point $\boldsymbol{x} $ and direction $\boldsymbol{d}$, the query density $\sigma$ and color $\boldsymbol{c}$ are obtained with the interpolation of voxel grid
% \begin{equation}
% \begin{aligned}
%  \sigma &= \text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{density}})  \\
%  \boldsymbol{c} &= \text{MLP}_{\theta}(\text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{color}}), \boldsymbol{x} , \boldsymbol{d}) 
%   \label{eq:important}
%   \end{aligned}
% \end{equation}
The NeRF model is trained by minimizing the loss between the rendered pixel color $\boldsymbol{C}(\boldsymbol{r})$ and observed pixel color $\hat{\boldsymbol{C}}(\boldsymbol{r})$ along the ray $\boldsymbol{r}$ given by
\begin{equation}
\mathcal{L}_{\text {Color }}=\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})}\|\hat{\boldsymbol{C}}(\boldsymbol{r})-\boldsymbol{C}(\boldsymbol{r})\|_2^2,
\end{equation}
where $\mathcal{R}(\mathbf{P})$ is the set of rendered rays in a batch. 
% To regularize the scene geometry and accelerate the model training,
To accelerate the model training, we introduce depth supervision to optimize the voxel field.
Then the NeRF model is trained by minimizing the loss
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text {color}} + \mathcal{L}_{\text {Depth }},
\end{equation}
where $\mathcal{L}_{\text {Depth }}$ is the $L_1$ loss between the rendered depth and observed depth.
% from LiDAR. This can also be replaced by the estimated depth, such as SFM \cite{schonberger2016structure} or PACKNet \cite{packnet}.
We can use LiDAR data or estimated depth by methods such as as SFM \cite{schonberger2016structure} or PACKNet \cite{packnet} to obtain the observed depth. We adopt the LiDAR data in this work as it is common in driving data.
% It is worth noting that LiDAR data is not necessary for our method.
For the convenience of scene editing, we reconstruct the static background of the scene in the world coordinate system and the object modeling the local object 3D box coordinate system.
Once we finish the training of the background and object NeRFs, they can serve as the digital driving assets for the repeated generation of novel scenes.


% in which we can not decouple a single object for the purpose of scene manipulation.
% Thus, in our framework we build the 3D models of the foreground objects and backgrounds separately. 





% we randomly select the 3D models of foreground objects and background scenes and combine them in the 3D space to create new reasonable driving situations on the road by identifying the plausible positions. Then we use volume rendering to generate new images of these scenes for the training of camera-based 3D perception tasks.

\paragraph{Analysis.}
Although the 3D models trained through this progress can be used for novel scene creation, they still struggle with several problems. First, because of the complexity of the driving scenes and the imperfect instance segmentation model, the extracted images of objects usually suffer from edge defects. 
NeRF can not model the object geometry well based on the noise mask as the background pixel may leak into the object model. 
In addition, the voxel grid representation through interpolation in voxel-based NeRF can further hinder the modeling of clear geometry near object boundaries.
% This will affect the quality of the generated images. 
Second, the range of available viewpoints for objects is very important for creating diverse driving scenes.
However, the objects in the driving data have limited views, indicating that we can only render objects within a small degree. This will restrict the diversity of the generated driving scenes. 
% Furthermore, the placement of the objects should not be arbitrary because of the physical law or other constraints of the real world. Random placement may cause many unnatural images.
In terms of this, we refine the voxel-based NeRF model to promote the object models. 
% We describe them in the following.
% To make the generated driving scenes more reasonable compared to the real world, we also adopt a placement strategy. 

\begin{figure}
  %\begin{center}
  \centering
  \includegraphics[width=1.0\linewidth]{fig/object_voxel_v0.pdf}
  \caption{\textbf{Modified NeRF} including a geometric rectified loss and a symmetric-aware training strategy. They can alleviate the effects of imperfect object extraction and increase the range of viewpoints, respectively.}
  %\end{center}
  \label{fig:module}
\end{figure} 

\subsection{Improvements of Object Models}~\label{improve}
We design a geometric rectified loss and a symmetric-aware training strategy for the object models to ensure the quality and diversity of the generated novel scenes.

\paragraph{Geometric Rectified Loss.}
To avoid the effects of imperfect object extraction, we propose a geometric rectified loss to correct the geometry of the object model. As illustrated in Figure \ref{fig:module}, we add an auxiliary task for the training of the NeRF model by the classification of the pixel as the foreground or background pixel. Because the edge defects are different across consecutive frames, the temporal inconsistency can make the model remove the edge defects.
Specifically, the probability of a rendered pixel being an object pixel can be approximated by
\begin{equation}
P(\boldsymbol{r})= 1 - \exp \left(-\int_{t_a}^{t_b} \sigma  d s\right),
\end{equation}
where $t_a$ and $t_b$ denote the entrance and exit points of the ray-object intersection respectively, and $\sigma$ is the density of sampled point in the ray. Then we implement the geometric rectified loss as
\begin{equation}
\mathcal{L}_{\text {GC}}= -\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})}log P(r) ,
\end{equation}
to decrease the voxel density near the mask edge, where $\mathcal{R}(\mathbf{P})$ is the set of rendered rays in a batch.
The final loss for training the object voxel field is defined by
\begin{equation}
\mathcal{L}_{\text{object}} = \mathcal{L}_{\text {Color }} + \mathcal{L}_{\text {Depth }} + \mathcal{L}_{\text {GC}}.
\end{equation}



\paragraph{Symmetric-aware Training Strategy.}
To enrich the viewpoints of objects, we design a symmetric-aware training strategy. Considering that objects, such as vehicles, are usually geometric symmetric in the driving scenes, we can create a symmetric object voxel field for them.
As depicted in Figure \ref{fig:module}, given a pixel $\boldsymbol{p}_r$ along the ray $\boldsymbol{r}(x, y, z)$, we can create a symmetric virtual camera that casts a ray $\boldsymbol{r}(x, -y, z)$ and targets the pixel $\boldsymbol{p}_l$, in which
$\boldsymbol{p}_r = \boldsymbol{p}_l$, supposing the symmetry of the object.
In this way, we increase the number of camera views for the object model training.
Then we can render novel views of the object rotated with a larger degree.





% \subsection{Building 3D Models}\label{3dmodel}
% The first stage is to build the 3D models of the foreground objects and backgrounds separately.
% First, we need to extract the objects from the backgrounds. For this, we use an off-the-shelf instance segmentation model to infer object masks for the entire dataset. 
% Since the object mask is not associated with specific object instance, 
% we match the object masks with 3D box in terms of IoU constraint to obtain the multi-frame masks of one object instance.
% Furthermore, we can classify the extracted mask into static object mask and moving object mask with the matched 3D box information.
% After this, we build the static background model by filtering the dynamic mask region.
% For the static or moving foreground objects, we choose the unoccluded objects for 3D modeling.

% \paragraph{Reconstruction}
% To obtain the 3D models, we use voxel-based NeRF \cite{xxx} to efficiently reconstruct the backgrounds and objects separately. Compared with MLP-based NeRF, voxel-based NeRF is much faster for reconstruction, in which a density voxel grid $\boldsymbol{V}_{\text{density}}$ and a feature voxel grid $\boldsymbol{V}_{\text{color}}$ with a shallow MLP are used to represent the geometry and appearance respectively. 
% % NeRF can reconstruct the 3D modeling of the scene given multi-view images by rendering the image pixel along the ray. 
% Given input query of point $\boldsymbol{x} $ and direction $\boldsymbol{d}$, the query density $\sigma$ and color $\boldsymbol{c}$ are obtained with the interpolation of voxel grid
% \begin{equation}
% \begin{aligned}
%  \sigma &= \text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{density}})  \\
%  \boldsymbol{c} &= \text{MLP}_{\theta}(\text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{color}}), \boldsymbol{x} , \boldsymbol{d}) 
%   \label{eq:important}
%   \end{aligned}
% \end{equation}
% The NeRF model is trained by minimizing the  loss between the rendered pixel color $\boldsymbol{C}(\boldsymbol{r})$ and observed pixel color $\hat{\boldsymbol{C}}(\boldsymbol{r})$ alon the ray $\boldsymbol{r}$ given by
% \begin{equation}
% \mathcal{L}_{\text {Color }}=\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})}\|\hat{\boldsymbol{C}}(\boldsymbol{r})-\boldsymbol{C}(\boldsymbol{r})\|_2^2,
% \end{equation}
% where $\mathcal{R}(\mathbf{P})$ is the set of rendered rays in a batch. To further improve the quality of rendered images, we also develop several techniques for building the background models and the object models described in the following.

% % We train the background and foreground object models separately using the voxel field for the convenience of scene generation. The instance segmentation model is utilized to generate the foreground object masks for each image. Then we model the static background as BVF with depth supervision by masking out the dynamic objects. We model the foreground object as OVF with proposed GCM and SAM based on the extracted object masks.


% \paragraph{Background Model.}  
% We model the static background of the scene in the world coordinate system from multiple consecutive frames. To regularize the scene geometry and accelerate the model training, we introduce depth supervision to optimize the voxel field given the LiDAR data, which is usually accompanied with autonomous data. Inspired by previous studies of MLP-based NeRF \cite{deng2022depth,rematas2022urban}, the rendered depth for voxel-based NeRF can be expressed as
% \begin{equation}
% D(\mathbf{r})=\int_{t_1}^{t_2} T(t) \sigma(\boldsymbol{r}(t)) t d t.
% \end{equation}
% where $T(t)$ is accumulated transmittance. The depth loss is defined by the rendered depth $D(\mathbf{r})$ and  depth $\hat{D}(\mathbf{r})$ from LiDAR  expressed by
% \begin{equation}
% \mathcal{L}_{\text {Depth }}=\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})} |M_{D}(\hat{D}(\boldsymbol{r})-D(\boldsymbol{r}))|,
% \end{equation}
% where $M_D \in \{0, 1\}$ is the occupancy mask for the rendered rays with $M_D(\boldsymbol{r})=1$ for the ray $\boldsymbol{r}$ with available LiDAR data. We jointly optimize the reconstruction of RGB image and depth of the scene. The final loss for the background voxel field is 
% \begin{equation}
% \mathcal{L}_{\text{Background}} = \mathcal{L}_{\text {color}} + \mathcal{L}_{\text {Depth }}.
% \end{equation}



% \paragraph{Object Model.}  
% We represent the foreground object in the local object 3D box coordinate system with inferred object mask.
% It is worth noting that the object model is built based on the extracted object pixels by a instance segmentation model. Due to imperfect mask prediction with edge defect, NeRF can not model the object geometry well and the background can leak into the object model. As illustrated in Fig \ref{fig:module}, to overcome this issue, we add a geometric rectified loss to correct the geometric shape by supervising the classification of the pixel as the foreground or background pixel.
% Specifically, the probability of a rendered pixel to be an object pixel can be approximated by
% \begin{equation}
% P(\boldsymbol{r})= 1 - \exp \left(-\int_{t_a}^{t_b} \sigma(\boldsymbol{r}(s))  d s\right),
% \end{equation}
% where $t_a$ and $t_b$ respectively denotes the entrance and exit points of the ray-object intersection. Then we implement the geometric rectified loss as
% \begin{equation}
% \mathcal{L}_{\text {GC}}= -\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})}log P(r) ,
% \end{equation}
% to decrease the voxel density near the mask edge. 
% We also add depth supervision to the object model, which is similar with the background model. The final loss for training the object voxel field is defined by
% \begin{equation}
% \mathcal{L}_{\text{object}} = \mathcal{L}_{\text {Color }} + \mathcal{L}_{\text {Depth }} + \mathcal{L}_{\text {GC}}.
% \end{equation}

% Furthermore, the range of available viewpoints is very important for the diversity of generated scenes.
% However, the views of objects are often limited in the driving scenes. To increase it, we propose a symmetric-aware sampling strategy considering that the object such as car is usually geometric symmetric in autonomous driving scene.
% As shown in Figure \ref{fig:module}, given a pixel $\boldsymbol{p}_r$ along the ray $\boldsymbol{r}(x, y, z)$, we can create a virtual camera that can cast a ray $\boldsymbol{r}(x, -y, z)$ and target the pixel $\boldsymbol{p}_l$, in which
% $\boldsymbol{p}_r = \boldsymbol{p}_l$ supposing the symmetry of the object.
% Thus, we increase the camera view for model training and also create a symmetric object voxel field.
% Then we can render novel view of the object within much larger degrees of rotation.


\subsection{Generation of Augmented Driving Scenes}\label{generation}
% After obtaining the background models and object models, we first use them to composite novel scenes and then generate new images of these scenes as data augmentation for 3D perception tasks.

\begin{figure}
  \centering
    \includegraphics[width=0.98\linewidth]{fig/valid_region.pdf}
  \caption{\textbf{Valid region} identification for the placement of vehicles, when composing a new driving scene.}
  \label{fig:validregion}
\end{figure}
As indicated in Figure \ref{fig:pipeline}, we generate a new driving scene through the combination of the object models and background models. We can place objects following the original location and orientation distributions of objects in data or change the distributions, such as making samples more balanced.
% \textcolor{red}{(bdai: following a uniform distribution or a prior distribution estimated from real data?)}
Besides, to make the placement of objects satisfy the physical law or other constraints in the real world, such as car on the road instead of the sky, we also identify the valid region of the background before the placement. Otherwise, the models of 3D perception tasks cannot learn the proper context information.  As shown in Figure \ref{fig:validregion}, we define the valid region on the bird's eye view based on the density field of the background model, which is inspired by the Lidar-Aug \cite{fang2021lidar}. To find the valid region, we first divide the 3D space into sets of pillars, and then the region can be split into valid and invalid states based on the density distribution of voxel in the corresponding pillar. 
Specifically, the valid region satisfies the low-density constraints: 
\begin{equation}
% \max(Z_p) -\min(Z_p) < \delta_h, \, \text{and} \,\,
% len(Z_p) > \delta_n,
\max(Z_p) < \delta_1,  \, \text{and} \,\, mean(Z_p) < \delta_2,
\label{eq:valid_region}
\end{equation}
where $Z_p$ is the array denoting the density of points in the pillar, and $\delta_1$ and $\delta_2$ are hyper-parameters. This represents no large object is in this region.
Furthermore, we can filter the low-density region behind the high-density region such as the wall. 
In this way, we can put the object in an appropriate position.  
% We utilize the road height information obtained from the density field to put the object at an appropriate height.  
To avoid the collision, we calculate 3D IoUs between the placed objects and existing objects to ensure consistency between the foreground objects and the backgrounds. 
% In this way, we find out the valid region to place new objects. 
Then we jointly render the merged 3D model to generate images of the new scenes. 
% Specifically, For each ray disjoints with the object model, it can be directly rendered based on the background model. For a rendered ray intersecting with the object model, the sampling points can be split into outer points and inner points in the object based on the ray-object model intersection. The outer points are rendered by the background model in the world system. For the inner points, we transform them into the corresponding local object coordinate system and render them using the object model. 
% The pixel color of the generated image is predicted by
% \begin{equation}
% \boldsymbol{C}(\boldsymbol{r})=\sum_{t=1}^{N_b+N_o} T_t \alpha_t \boldsymbol{c}_t,
% \end{equation}where $\alpha_t=1-\exp(-\sigma_t \delta_t$) with $\delta_t$ the distance between adjacent sampling points.
The generated images can be directly applied for the downstream 3D detection task to improve the performance of detectors.



% We can create scenes arbitrarily or generate scenes satisfying specific object distribution through this 3D-aware scene generation process. 




% \paragraph{Image Generation.}




% The pixel color of the generated image is predicted by
% \begin{equation}
% \boldsymbol{C}(\boldsymbol{r})=\sum_{t=1}^{N_b+N_o} T_t \alpha_t \boldsymbol{c}_t,
% \end{equation}where $\alpha_t=1-\exp(-\sigma_t \delta_t$) with $\delta_t$ the distance between adjacent sampling points.


% \subsection{Preliminaries}\label{sec:priliminary}
% The 3D scene can be represented with the NeRF model, and the neural network is utilized to map a point position $\boldsymbol{x} \in \mathbb{R}^3$  and a view direction $\boldsymbol{d} \in \mathbb{R}^3$ to the corresponding color $\boldsymbol{c} \in \mathbb{R}^3$ and volume density $\sigma$ \cite{mildenhall2020nerf}.  We apply the voxel grid to represent the scene considering the low computational cost of voxel-based NeRF \cite{sun2021direct}. The density voxel grid $\boldsymbol{V}_{\text{density}}$  and  feature voxel grid $\boldsymbol{V}_{\text{color}}$  with a shallow MLP  are adopted to represent the scene geometry and appearance, respectively. Given input queries $\boldsymbol{x} $ and $\boldsymbol{d} $,  the outputs are obtained with the interpolation
% \begin{equation}
% \begin{aligned}
%  \sigma &= \text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{density}})  \\
%  \boldsymbol{c} &= \text{MLP}_{\theta}(\text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{color}}), \boldsymbol{x} , \boldsymbol{d}) 
%   \label{eq:important}
%   \end{aligned}
% \end{equation}
% To render the image, the pixel color $\boldsymbol{C}(\boldsymbol{r})$ along the camera ray $\boldsymbol{r}(t) = \boldsymbol{r_0}+ t \boldsymbol{d}$ is approximated by the volume rendering
% \begin{equation}
% \boldsymbol{C}(\mathbf{r})=\int_{t_1}^{t_2} T(t) \sigma(\boldsymbol{r}(t)) \mathbf{c}(\boldsymbol{r}(t), \mathbf{d}) d t,
% \end{equation}
% where  $t_1$ and $t_2$ are near and far bounds for sampling points, $\boldsymbol{r_0}$ is the camera origin, and $T(t)$ is accumulated transmittance along the ray from $t_1$ to $t$ defined by
% \begin{equation}
% T(t)=\exp \left(-\int_{t_1}^t \sigma(\boldsymbol{r}(s)) d s\right).
% \label{eq:transmittance}
% \end{equation}
% The NeRF model is trained by minimizing the  loss between the rendered pixel color $\boldsymbol{C}(\boldsymbol{r})$ and observed pixel color $\hat{\boldsymbol{C}}(\boldsymbol{r})$given by
% \begin{equation}
% \mathcal{L}_{\text {Color }}=\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})}\|\hat{\boldsymbol{C}}(\boldsymbol{r})-\boldsymbol{C}(\boldsymbol{r})\|_2^2,
% \end{equation}
% where $\mathcal{R}(\mathbf{P})$ is the set of rendered rays in a batch.





\input{data/experiment.tex}



\section{Conclusion}

In this paper, we propose Drive-3DAug, the first 3D data augmentation technique for camera-based 3D perception task in autonomous driving. We represent the scene with background and foreground 3D models by NeRF and randomly combine them to generate new driving scenes. 
% To render a new image, foreground objects are randomly sampled cross scenes and placed into current scene with jittered location and orientation. 
The quality and diversity issues of generated scenes are addressed by our novel geometric rectified loss and symmetry-aware training strategies.  
% Furthermore, these models are the digital driving asset. We believe they can benefit the community of this area.
% , as well as supervising voxel fields with LiDAR data. 
% We demonstrate that our method can improve the monocular 3D object detection on popular Waymo and nuScenes datasets, achieving $1.7\%$ to $2.5\%$ performance gain under different metrics respectively. 
We demonstrate the effectiveness of our method on multiple datasets and detectors.  Furthermore, these 3D models can be regarded as the digital driving asset and benefit the community of this area.

\vspace{-2mm}
\paragraph{Discussion.}
Driving scenes are extremely complicated, including lots of object categories under different illumination and weather conditions. Currently, our method only augments limited classes of objects under good illumination conditions. It is worth to include more situations as the digital driving asset for the future work. 
% Besides, our method only changes the object with geometric transformation. More augmentation strategies, such as manipulating the background or changing the appearance of objects are also worth for further exploration.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}




\clearpage
\appendix

\input{data/sup}


\end{document}





