

\section{Preliminaries of NeRF}\label{sec:priliminary}
The 3D scene can be represented with the NeRF model, and the neural network is utilized to map a point position $\boldsymbol{x} \in \mathbb{R}^3$  and a view direction $\boldsymbol{d} \in \mathbb{R}^3$ to the corresponding color $\boldsymbol{c} \in \mathbb{R}^3$ and volume density $\sigma$ \cite{mildenhall2020nerf}.  We apply the voxel grid to represent the scene considering the low computational cost of voxel-based NeRF \cite{sun2021direct}. The density voxel grid $\boldsymbol{V}_{\text{density}}$  and  feature voxel grid $\boldsymbol{V}_{\text{color}}$  with a shallow MLP  are adopted to represent the scene geometry and appearance, respectively. Given input queries $\boldsymbol{x} $ and $\boldsymbol{d} $,  the outputs are obtained with the interpolation
\begin{equation}
\begin{aligned}
 \sigma &= \text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{density}})  \\
 \boldsymbol{c} &= \text{MLP}_{\theta}(\text{inter}(\boldsymbol{x}, \boldsymbol{V}_{\text{color}}), \boldsymbol{x} , \boldsymbol{d}) 
  \label{eq:important}
  \end{aligned}
\end{equation}
To render the image, the pixel color $\boldsymbol{C}(\boldsymbol{r})$ along the camera ray $\boldsymbol{r}(t) = \boldsymbol{r_0}+ t \boldsymbol{d}$ is approximated by the volume rendering
\begin{equation}
\boldsymbol{C}(\mathbf{r})=\int_{t_1}^{t_2} T(t) \sigma(\boldsymbol{r}(t)) \mathbf{c}(\boldsymbol{r}(t), \mathbf{d}) d t,
\end{equation}
where  $t_1$ and $t_2$ are near and far bounds for sampling points, $\boldsymbol{r_0}$ is the camera origin, and $T(t)$ is accumulated transmittance along the ray from $t_1$ to $t$ defined by
\begin{equation}
T(t)=\exp \left(-\int_{t_1}^t \sigma(\boldsymbol{r}(s)) d s\right).
\label{eq:transmittance}
\end{equation}
The NeRF model is trained by minimizing the  loss between the rendered pixel color $\boldsymbol{C}(\boldsymbol{r})$ and observed pixel color $\hat{\boldsymbol{C}}(\boldsymbol{r})$given by
\begin{equation}
\mathcal{L}_{\text {Color }}=\sum_{\boldsymbol{r} \in \mathcal{R}(\mathbf{P})}\|\hat{\boldsymbol{C}}(\boldsymbol{r})-\boldsymbol{C}(\boldsymbol{r})\|_2^2,
\end{equation}
where $\mathcal{R}(\mathbf{P})$ is the set of rendered rays in a batch.


\section{Implementation Detail of Detectors}
For FCOS3D, we utilize a ResNet-101-DCN\cite{he2016deep, dai2017deformable} as the backbone. The model is trained for 24 epochs using the SGD optimizer with an initial learning rate of 1e-4 and a momentum of 0.9. We set the weight decay to 1e-5, and the max norm of gradient clipping to 35. We also adopt a step decay learning rate scheduler with a 0.1$\times$ decrease at epoch 20 and 23, along with 1000 iterations of linear warm-up. For SMOKE, we employ a DLA-34\cite{yu2018deep} as the backbone. We use the Adam optimizer with an initial learning rate 1e-4, and the remaining settings are the same as FCOS3D. For both detectors, their backbones are initialized with ImageNet pre-trained weights. The batch size for training is set to 16.





\setlength{\tabcolsep}{8.5pt}
\begin{table}[t]
  \small
  \begin{center}
  \begin{tabular}{@{}ccccc@{}}
    \toprule
     3DAug  & DS & LET-AP &  LET-APH & LET-APL \\
    \midrule
      -     &  -           &  0.585 & 0.573 & 0.393 \\
     \checkmark  & - & 0.584 & 0.572   & 0.394 \\
    \checkmark & \checkmark &  \textbf{0.590} & \textbf{0.578} & \textbf{0.403} \\
    \bottomrule
  \end{tabular}
  \end{center}
  \caption{\textbf{Ablation Study} of Drive-3DAug for FCOS3D on Waymo validation set. 3DAug means we use DVGO \cite{sun2021direct} for data augmentation. DS means depth supervision.}
  \label{tab:ablation_depth}
\end{table}


\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.99\linewidth]{fig/depth_v1.pdf}
    \end{center}
    \caption{\textbf{Visualization of rendered depth map.}  The model with depth supervision depicts better performance.}
    \label{fig:depth}
\end{figure}



\section{Ablation Study on Depth Supervision.}
We qualitatively and quantitatively investigate the effect of depth supervision on background model training and 3D augmentation.
Table~\ref{tab:ablation_depth} shows that the 3D augmentation based on background model trained with depth supervision has better performance, with LET-AP (0.590 vs 0.585).
Figure~\ref{fig:depth} shows that NeRF can reconstruct the background with high quality given depth supervision, and the 3D background model quality can be decreased without depth information.
Thus, LET-AP, LET-APH and LET-APL on car have a slight decrease with 0.001 for 3D augmentation using background model without supervision.





\section{Visualization of Drive-3DAug}
% \tww{Aug car, person, cyclist, including RGB and depths map}
% We visualize the reconstructed depth map and new driving scene generated by Drive-3DAug in Figure~\ref{fig:depth}, indicating that 
We augment car in Figure~\ref{fig:more_vis}, indicating that
we can generate scene with high quality with Drive-3DAug.
Compared with car, pedestrian and cyclist are not rigid body and the size is small, not well applicable for NeRF modelling.
We model pedestrian and cyclist as rigid boby in the present study, which can cause the decay of object model performance.
As shown in Figure~\ref{fig:person}, although there exists flaw for augmented 
pedestrian and cyclist, we can still augment them to improve the detector performance.



\section{Cross-dataset Drive-3DAug}
We have reconstructed thousands of background and object models in Waymo and nuScenes dataset.
These models can serve as the general model assets, convenient for creating new driving scenes inside a specific dataset or cross different datasets.
As shown in Figure \ref{fig:cross}, we compose the object models from nuScenes and the background models from Waymo to create new driving scenes. This can further enlarge the diversity of the training data, and we can generate large amounts of data for the study of model generalization across different datasets.


\begin{figure*}[t]
    \begin{center}
\includegraphics[width=0.9\linewidth]{fig/more_aug.pdf}
    \end{center}
    \caption{\textbf{Visualization of the generated images by Drive-3DAug.} The yellow boxes indicate the newly added cars for the background.}
    \label{fig:more_vis}
\end{figure*}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{fig/person.pdf}
    \end{center}
    \caption{\textbf{Visualization of the generated images by Drive-3DAug.} The yellow and red boxes indicate the augmented pedestrian and cyclist, respectively.}
    \label{fig:person}
\end{figure*}



\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=0.9\linewidth]{fig/cross.pdf}
    \end{center}
    \caption{\textbf{Image Generation cross datasets.} We place the cars from nuScenes on the backgrounds of Waymo. The yellow boxes indicate the augmented cars.}
    \label{fig:cross}
\end{figure*}


