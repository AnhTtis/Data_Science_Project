\section{Experiments}

We validate the proposed Drive-3DAug method on the monocular 3D detection task, which is one of the most important 3D perception tasks in autonomous driving.

\subsection{Datasets and Metrics}\label{subsec:dataset}

\paragraph{Waymo Dataset.}
The Waymo Open Dataset \cite{sun2020scalability} is a large-scale dataset for autonomous driving that contains 798 scenes in the training dataset and 202 scenes in the validation dataset. The image resolution for the front camera is $1920 \times 1280$. Waymo uses the LET-AP \cite{hung2022let}, the average precision with longitudinal error tolerance, to evaluate detection models. Besides, Waymo also adopts the LET-APL and LET-APH metrics, which are the longitudinal affinity weighted LET-AP and the heading accuracy weighted LET-AP, respectively.
% 3D Intersection over Union (IoU)
% \begin{table}[ht]\small
%   \centering    
% \setlength{\tabcolsep}{2pt}{
%   \begin{tabular}{@{}llccc@{}}
%     \toprule
%     Dataset & Method &  Scene Num. & Model Num.&    Cost (days) \\
%     \midrule
%     Waymo~\cite{sun2020scalability}    & Drive-3DAug& 100 & 975  & 1-2   \\
%     nuScenes~\cite{caesar2020nuscenes} &   Drive-3DAug &  50 & 426 & $<1$ \\
%     Synthetic~\cite{mildenhall2020nerf} &   NeRF~\cite{mildenhall2020nerf} &  8 & 8 & $>10$ \\
    
%     \bottomrule
%   \end{tabular}
%   }
%   % The details of scene reconstruction on Waymo and nuScene datasets. 
%   \caption{\textbf{Reconstruction cost} of chose scenes are assessed on eight V100 GPUs with 16GB memory. The number of models includes both the background models and foreground models.
% %   \TWW{the last row needs to be adjusted.}
%   }
%   \label{tab:scece_recon_cost}
% \end{table}

\paragraph{nuScenes Dataset.}

The nuScenes \cite{caesar2020nuscenes} is a widely used benchmark for 3D object detection.
% the area of autonomous driving. 
It contains 700 training scenes and 150 validation scenes. The resolution of each image is 1600 $\times$ 900.  As for the metrics, nuScenes computes mAP using the center distance on the ground plane to match the predicted boxe and the ground truth. It also contains different types of true positive metrics (TP metrics). We use ATE, ASE and AOE in this paper, for measuring the errors of translation, scale and orientation, respectively.
% with 1600 $\times$ 900 pixels per image. 


% \setlength{\tabcolsep}{10pt}
% \begin{table*}[ht]
% \small
% \begin{center}
% \begin{tabular}{@{}l|ccc|cccc@{}}
%   \toprule
% %   \multirow{4}{*}{\textbf{Method}} & \multicolumn{7}{c}{\textbf{Waymo}} \\ \cmidrule{2-8}
% %   & \textbf{LET-AP}  & \textbf{LET-APH}  & \textbf{LET-APL}  & \makecell[c]{\textbf{LET-APL}\\ $[50\mathrm{m},+\infty)$} & \makecell[c]{\textbf{LET-APH}\\ $\sim45^{\circ}$} & \makecell[c]{\textbf{LET-APH}\\ $\sim90^{\circ}$} & \makecell[c]{\textbf{LET-APH}\\ $\sim135^{\circ}$} \\ 
%   \multirow{2}{*}{\textbf{Method}} 
%   & \textbf{LET-AP}  & \textbf{LET-APH}  & \textbf{LET-APL}  & \makecell[c]{\textbf{LET-APL}\\ $[50\mathrm{m},+\infty)$} & \makecell[c]{\textbf{LET-APH}\\ $\sim45^{\circ}$} & \makecell[c]{\textbf{LET-APH}\\ $\sim90^{\circ}$} & \makecell[c]{\textbf{LET-APH}\\ $\sim135^{\circ}$} \\ 
%   \midrule
%   FCOS3D            & 0.585 & 0.573 & 0.393 & 0.278 & 0.293 & 0.285 & 0.223 \\
%   + Copy-paste & 0.594 & 0.581 & 0.401 & 0.290 & 0.295 & 0.283 & 0.234 \\
%   + Drive-3DAug w/o RT         & 0.595 & 0.583 & 0.404 & 0.287 & 0.293 & 0.295 & 0.226 \\
%   + Drive-3DAug w/ RT         & \textbf{0.602} & \textbf{0.590} & \textbf{0.410} & \textbf{0.298} & \textbf{0.315} & \textbf{0.299} & 0.234 \\
%   \midrule
%   SMOKE             & 0.586 & 0.579 & 0.417 & 0.304 & 0.312 & 0.291 & 0.264 \\
%   + Copy-paste & 0.594 & 0.587 & 0.425 & 0.320 & 0.303 & 0.301 & 0.239 \\
%   + Drive-3DAug w/o RT & 0.592 & 0.584 & 0.421 & 0.318 & 0.314 & 0.298 & \textbf{0.266} \\
%   + Drive-3DAug w/ RT   & \textbf{0.598} & \textbf{0.590} & \textbf{0.426} & \textbf{0.322} & \textbf{0.333} & \textbf{0.303} & 0.255 \\
% \bottomrule
% \end{tabular}
% \caption{Monocular 3D detection results on the \textbf{Waymo} validation set. The left part shows the main metrics. LET-AP represents longitudinal error tolerant 3D average precision \cite{hung2022let}. LET-APH and LET-APL represent LET-AP penalized by heading errors and longitudinal localization errors, respectively. The right part of the table shows metrics under specific hard cases. The $[50\mathrm{m},+\infty)$ metric only calculates AP over instances with distance to ego-car greater than $50\mathrm{m}$. The $\sim h ^{\circ}$ metric only consider the instances with $\vert \mathrm{heading} \vert$ near $h$. It is observed that our \textit{Drive-3DAug w/ RT} performs consistent better than any other setting.}
% \label{tab:main_results}
% \end{center}
% \end{table*}

\setlength{\tabcolsep}{10pt}
\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{@{}l|ccc|cccc@{}}
  \toprule
  \multirow{2}{*}{\textbf{Method}} 
  & \textbf{LET-AP}  & \textbf{LET-APH}  & \textbf{LET-APL}  & \makecell[c]{\textbf{LET-APL}\\ $[50\mathrm{m},+\infty)$} & \makecell[c]{\textbf{LET-APH}\\ $\mathbin{\sim}45^{\circ}$} & \makecell[c]{\textbf{LET-APH}\\ $\mathbin{\sim}90^{\circ}$} & \makecell[c]{\textbf{LET-APH}\\ $\mathbin{\sim}135^{\circ}$} \\ 
  \midrule
  FCOS3D \cite{wang2021fcos3d}            & 0.585 & 0.573 & 0.393 & 0.278 & 0.293 & 0.285 & 0.223 \\
  + Copy-paste \cite{lian2022exploring} & 0.594 & 0.581 & 0.401 & 0.290 & 0.295 & 0.283 & \textbf{0.234} \\
  + Drive-3DAug w/o RT         & 0.595 & 0.583 & 0.404 & 0.287 & 0.293 & 0.295 & 0.226 \\
  + Drive-3DAug w/ RT         & \textbf{0.602} & \textbf{0.590} & \textbf{0.410} & \textbf{0.298} & \textbf{0.315} & \textbf{0.299} & \textbf{0.234} \\
  \midrule
  SMOKE \cite{liu2020smoke}            & 0.586 & 0.579 & 0.417 & 0.304 & 0.312 & 0.291 & 0.264 \\
  + Copy-paste \cite{lian2022exploring} & 0.594 & 0.587 & 0.425 & 0.320 & 0.303 & 0.301 & 0.239 \\
  + Drive-3DAug w/o RT & 0.592 & 0.584 & 0.421 & 0.318 & 0.314 & 0.298 & \textbf{0.266} \\
  + Drive-3DAug w/ RT   & \textbf{0.598} & \textbf{0.590} & \textbf{0.426} & \textbf{0.322} & \textbf{0.333} & \textbf{0.303} & 0.255 \\
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Monocular 3D detection results} on the Waymo validation set. The left part shows the main metrics. LET-AP represents longitudinal error tolerant 3D average precision \cite{hung2022let}. LET-APH and LET-APL represent LET-AP penalized by heading errors and longitudinal localization errors, respectively. The right part of the table shows metrics under specific hard cases. The $[50\mathrm{m},+\infty)$ metric only calculates AP over objects with distance to ego-car greater than $50\mathrm{m}$. The $\mathbin{\sim} h ^{\circ}$ metric only consider the objects with $\vert \mathrm{heading} \vert$ near. It is observed that our \textit{Drive-3DAug w/ RT} performs consistently better than any other setting.}
\label{tab:main_results}
\end{table*}



\subsection{Implementation Details}\label{subsec:implementation}
\paragraph{Building Digital Driving Asset.} 
We use the SOLO v2~\cite{wang2020solo} trained on COCO as the instance segmentation model for scene decomposition. 
% For the Waymo dataset, we model all three classes (\textit{Car}, \textit{Pedestrian}, \textit{Cyclists}),
% and we only consider \textit{Car} in the nuScenes dataset for simplicity.
We consider \textit{vehicle} (Waymo) or \textit{car} (nuScenes) as the foreground objects, since they are the most important components in driving scenes 
For 3D model reconstruction, we use the same model configuration as the DVGO~\cite{sun2021direct} with our proposed techniques. We use 30-40 consecutive frames spanning an area of about 100-200 meters as one background and train each background model with 40,000 iterations. For the background voxel grid, we set the resolution as $330^3$ with a voxel size of 0.25-0.3m. The object models are trained with 20-60 consecutive frames, and we set the voxel size as 0.25m consistent with the background voxel size. The grid point number is about 1,000.
Considering the construction cost and limitation of NeRF model for extreme illumination conditions, we select a subset of 100 sunny scenarios for each dataset. We use them for the data augmentation.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{fig/aug_method.pdf}
    \caption{\textbf{Scene generation} with different placement strategies of the \textit{vehicle}.}
    \label{fig:gen_strategy}
\end{figure}


\paragraph{Applying Data Augmentation.}
Our method generates new data through rendering the recomposed scenes of the randomly selected 3D models. We manipulate the object in 3D space by 3D location jittering and orientation jittering and place it on the valid region of arbitrary backgrounds. 
% The valid region is determined by $\delta_1=30$ and $\delta_2=15$ in Eq.~\ref{eq:valid_region}.
The location jittering is defined by the maximum translation $(T_x, T_y)$ along the x-direction and the y-direction, and the orientation jittering is given by maximum rotation angle $T_{\theta}$. We consider two data augmentation strategies to valid the effectiveness of adding rotation and translation for 3D perception. The first is $\textbf{\textit{Drive-3DAug w/o RT}}$, in which we set translation $T_x=0$, $T_y=0$, and rotation $T_{\theta}=0$ and 1-2 new objects are arbitrarily pasted into the background scene on average. The second is $\textbf{\textit{Drive-3DAug w/ RT}}$ with $T_x=20$m, $T_y=5$m, and $T_{\theta}=30^{\circ}$. 
% in which we also specify the pasted location to generate much more complex and diverse scenes except for the random location sampling. 
As for determining the valid region in the background scene, we set the pillar $Z_p$ resolution as 2m$\times$2m, $\delta_1=30$, and $\delta_2=15$ in Eq.~\ref{eq:valid_region}. 
We generate 12 new images for every background model. This progress is offline data augmentation and these images are repeatedly used by different detectors.


% Then we generate multiple augmented images with Copy-paste or Drive-3DAug for each frame. Note that, due to the lack of segmentation label corresponding to 3D instance, we use SOLO \cite{wang2020solo} to inference a high-quality instance segmentation result asbyeudo mask. For both method on the two dataset, we paste 2 vehicle instances to valid zone per frame on average. \TWW{2 vehicle? to be updated} Source instances are selected cross-view randomly, within a range of 20m to 70m from the ego car for Waymo and a range of xxm to xxm for nuScenes. For Copy-paste, no transaction or rotation is applied. For the default setting of Drive-3DAug, the transaction on x-axis and y-axis is up to 10m and 2m, and the rotation degree is up to $10^{\circ}$ on Waymo. For the Nuscenes dataset, xxx



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.99\linewidth]{fig/aug_method.pdf}
%     \caption{\textbf{Scene generation} with different placement strategies of the \textit{Car}.}
%     \label{fig:gen_strategy}
% \end{figure}

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=0.989\linewidth]{fig/heading.pdf}
    \caption{\textbf{Distribution of heading direction} for \textit{vehicle} on the Waymo training set and the distribution augmented by Drive-3DAug.}
    \label{fig:heading_distribution}
    \end{center}
\end{figure}
 % \textbf{Left)} without rotation and translation of the object. \textbf{Right)} with rotation and translation of the object.

\paragraph{Training Detectors.}
Two typical camera-based monocular 3D object detectors, FCOS3D\cite{wang2021fcos3d} and SMOKE\cite{liu2020smoke} are utilized to investigate the performance of our proposed method as they are two of the most popular and commonly used mono3D detectors. We maintain the same hyperparameters of detectors for all experiments in our study, introduced in the supplementary. We use all the scenes in the training set to train the detectors and evaluate the detectors on the entire validation set. However, we sample training data every 3 frames on Waymo during training due to limited computational resources.
Besides, although our method can be applied to any view of cameras, we only take into account images taken by the front camera on Waymo and nuScenes in this work because of the computational resource.
For the usage of the generated images, we randomly replace the image in a batch with the augmented data if it belongs to the scenes in the digital driving asset.





\subsection{Main Results}\label{subsec:results}
\paragraph{Novel Scene Generation.}
Drive-3DAug can generate diverse scenes with the ability to manipulate objects in the 3D space. 
As indicated in Figure~\ref{fig:gen_strategy}, our method can place the car at any distance or generate a very dense scene to augment the data. Moreover, previous image data augmentation methods are likely to put the car on the sky because they do not define the valid region. By contrast, our placement obeys the real world situation.
Besides the placement, our method can also alleviate the imbalance problem of vehicle orientation with orientation jittering to objects. Figure~\ref{fig:heading_distribution} shows the extremely imbalanced distribution of vehicle heading direction on the Waymo dataset, where most heading directions are at $0^{\circ}$ or $180^{\circ}$. In comparison, the distribution of vehicle heading distribution is more balanced than the original distribution, although the augmented distribution is still imbalanced. This is because of the randomly jitter for the orientation of objects.
% described on the right of Figure~\ref{fig:heading_distribution}. 
% our Drive-3DAug method can alleviate the problem w. 
% Because we randomly jitter the orientation of objects within a range for now, this distribution is still imbalanced. However, it is more balanced than the original distribution.


% We only consider the front camera and class \textit{vehicle} for all results.
\setlength{\tabcolsep}{9pt}
\begin{table}[t]
  \small
  \begin{center}
  \begin{tabular}{@{}lccccc@{}}
    \toprule
    Method & AP  & ATE  & ASE  & AOE  \\
    \midrule
    FCOS3D\cite{wang2021fcos3d}   & 0.319 & 0.739 & 0.160 & 0.096 \\
    + Copy-paste \cite{lian2022exploring}             & 0.324 & 0.721 & \textbf{0.156} & 0.123 \\
    + Drive-3DAug w/ RT              & \textbf{0.333} & \textbf{0.705} & 0.158 & \textbf{0.092} \\
    \bottomrule
  \end{tabular}
  \end{center}
  \caption{\textbf{Monocular 3D detection results} on the nuScenes validation set. ATE, ASE, and AOE are used for measuring translation, scale, and orientation errors respectively.
  }
  \label{tab:nuscenes}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/geo.pdf}
    \caption{\textbf{Visualization} of the ablative study for the geometric rectified loss. After applying such a loss, the reconstructed object models have much fewer edge defects.}
    \label{fig:ablation_geo}
\end{figure}

\paragraph{3D Object Dectection.}
Table~\ref{tab:main_results} illustrates the main results on the Waymo validation set. We compare our Drive-3DAug with the geometry-consistent copy-paste method \cite{zhang2020exploring}, which copies and pastes objects across different scenes with depth constraints and camera transformation of 3D annotations. Besides, we also apply the valid region strategy for this method with the help of LiDAR data. It also uses the selected 100 scenes for augmentation.
Since our data augmentation method is applied only to vehicles, we use the LET-AP on \textit{vehicle} to evaluate the effectiveness of our approach. Compared to \textit{baseline}, our \textit{NeRF w/ RT} approach significantly improves LET-AP by 1.5\%, LET-APH by 1.4\%, and LET-APL by 1.5\% on average across both FCOS3D and SMOKE. The consistent improvement between these detectors indicates that our approach is robust to different backbone and head architectures. It also outperforms \textit{Copy-paste} by 0.6\%, 0.6\%, and 0.5\% on LET-AP, LET-APH, and LET-APL, respectively.  Furthermore, we also compare the performance of different methods under far distances and different orientation ranges of vehicles. The right of Table~\ref{tab:main_results} shows that the detector trained with \textit{Drive-3DAug w/ RT} outperforms other methods under all the scenarios, especially for the objects with $\mathbin{\sim}45^{\circ}$ orientation, which is the fewest in the original data. This means the ability to add larger translation and rotation to placed objects is essential for the data augmentation techniques in 3D perception.




Table~\ref{tab:nuscenes} reports the experimental results on the nuScenes validation set. We can see \textit{Drive-3DAug w/ RT} also improves the performance of class \emph{car} on the nuScenes dataset. 
Additionally, it is worth noting that Drive-3DAug obtains a $1.6\%$ improvement on ATE, a $3.1\%$ improvement on AOE for the FCOS3D detector compared with the geometry-consistent copy-paste method. The result on ASE is comparable for these two methods. This is because we do not apply the scale transformation. This further proves that our method is capable of resolving the issues of previous methods with only 2D pixel movements that lack the ability to generate realistic rotated or translated objects.



% \setlength{\tabcolsep}{7.5pt}
% \begin{table}[t]
%   \small
%   \begin{center}
%   \begin{tabular}{@{}cccccc@{}}
%     \toprule
%      GRL  & SAT & DS & LET-AP &  LET-APH & LET-APL \\
%     \midrule
%      -           &  -           & - &   &  &  \\
%     -        &  -           & \checkmark & 0.590 & 0.578 & 0.403 \\
%      \checkmark & - & \checkmark & 0.596 & 0.583 & 0.407 \\
%     \checkmark & \checkmark & \checkmark & \textbf{0.602} & \textbf{0.590} & \textbf{0.410} \\
%     \bottomrule
%   \end{tabular}
%   \end{center}
%   \caption{\textbf{Ablation Study} of Drive-3DAug w R/T for FCOS3D on Waymo. GRL means the geometric rectified loss and SAT means symmetric-aware training. \tww{Problem: the third row is similar to the copy-paste, which has noisy mask.}}
%   \label{tab:ablation_nerf_model}
% \end{table}

\setlength{\tabcolsep}{8.5pt}
\begin{table}[t]
  \small
  \begin{center}
  \begin{tabular}{@{}ccccc@{}}
    \toprule
     GRL  & SAT & LET-AP &  LET-APH & LET-APL \\
    \midrule
    -        &  -           &  0.590 & 0.578 & 0.403 \\
     \checkmark & - &  0.596 & 0.583 & 0.407 \\
    \checkmark & \checkmark &  \textbf{0.602} & \textbf{0.590} & \textbf{0.410} \\
    \bottomrule
  \end{tabular}
  \end{center}
  \caption{\textbf{Ablation Study} of Drive-3DAug w R/T for FCOS3D on Waymo validation set. GRL means the geometric rectified loss and SAT means symmetric-aware training.}
  \label{tab:ablation_nerf_model}
\end{table}




% \paragraph{Drive-3DAug vs. Copy-paste.}

% % In general, the image quality generated by NeRF is inferior to the original image, causing a slight decay of detector performance. Specifically, the detector trained based on  \textit{Drive-3DAug w/o RT} may have lower performance than that on \textit{Copy-paste}.
% % Table~\ref{tab:main_results} shows that \textit{Drive-3DAug w/o RT} and \textit{Copy-paste} achieve competitive performance, indicating that the data quality generated by Drive-3DAug is high and acceptable. In addition, Drive-3DAug can generate a more diverse scene in which the objects are partly occluded by the background while this scene can not be generated by copy-paste based on a mask. Thus, our Drive-3DAug is comparative with copy-paste based on a mask in our experiments. 
% Our Drive-3DAug can degrade into the geometry-consistent copy-paste augmentation if we generate the new data without location jitter and orientation jitter. For both the data augmentation setting \textit{Drive-3DAug w/o RT} and \textit{Copy-paste}, the scene generation strategy is consistent. Table~\ref{tab:main_results} indicates that our \textit{Drive-3DAug w/o RT} is comparable with the copy-paste augmentation. \tww{This means}


% Our Drive-3DAug can generate a high-quality image on the background and foreground objects, but data generated by Drive-3DAug is still a type of simulated data. There may exist a gap between the generated images and raw images.
% \paragraph{Image quality.}
% Drive-3DAug can generate a high-quality image with low distortion in both foreground and background areas. However, we still want to explore the impact of the gap between the generated images and raw images on the training. We construct \textit{Drive-3DAug w/o RT} with the same scene generation strategy as Copy-paste, no transaction or rotation is adopted on the pasted instances. According to the results, \textit{Drive-3DAug w/o RT} obtains competitive performance compared to Copy-paste, which reveals the scene generated by Drive-3DAug does have a similar image quality compared to the image spliced with raw fore-ground and background through Copy-paste.









%\begin{figure*}
%  \centering
%\fbox{\rule{0pt}{0.1in} \rule{.9\linewidth}{0pt}}
%\caption{Pending: visualization of gt/pre to compare baseline/mask/nerf}
%\label{fig:visualize_validation}
%\end{figure*}





% \setlength{\tabcolsep}{8pt}
% \begin{table}[t]
%   \small
%   \begin{center}
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     Setting                 & LET-AP &  LET-APH & LET-APL \\
%     \midrule
%      Baseline                 & 0.258 & 0.224 & 0.155 \\
%     + Vanilla NeRF          & 0.301 & 0.272 & 0.185 \\
%     + NeRF opt.   & \textbf{0.314} & \textbf{0.285} & \textbf{0.195} \\
%     \bottomrule
%   \end{tabular}
%   \end{center}
%   \caption{The comparison of detection results based on training data rendering by different NeRF models on FCOS3D detectors.}
%   \label{tab:ablation_nerf_model}
% \end{table}

% It can make the model generate more realistic images with larger degrees of rotation. Comparison of reconstructed cars from different OVFs. The vanilla model without any optimization cannot reconstruct the cars well. GCM and SAM can improve reconstruction quality. SAM -> show a series of generation with different angles compared with the baseline.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{fig/sym.pdf}
    \caption{\textbf{Visualization} of the ablative study for the symmetric-aware training strategy. It can make the model generate more realistic images with larger degrees of rotation.}
    \label{fig:ablation_sym}
\end{figure}



% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.98\linewidth]{fig/ablation_back_nerf.pdf}
%     \caption{Visualization of rendered background depth map. The full model with depth supervision can reconstruct the scene precisely including the flat road and geometric shape of cars. }
%     \label{fig:ablation_back_nerf}
% \end{figure}


% \setlength{\tabcolsep}{9pt}
% \begin{table}[t]
%   \small
%   \begin{center}
%   \begin{tabular}{@{}lccccc@{}}
%     \toprule
%     Method & AP  & ATE  & ASE  & AOE  \\
%     \midrule
%     FCOS3D\cite{wang2021fcos3d}   & 0.176 & 0.870 & 0.182 & 0.256 \\
%     + Copy-paste             & 0.198 & 0.867 & \textbf{0.178} & 0.245 \\
%     + Drive-3DAug w/ RT              & \textbf{0.201} & \textbf{0.846} & \textbf{0.178} & \textbf{0.215} \\
%     \bottomrule
%   \end{tabular}
%   \end{center}
%   \caption{Monocular 3D detection results for pvb on the \textbf{Waymo} validation set.
%   }
%   \label{tab:pvb}
% \end{table}
% % 





\subsection{Ablative Studies}\label{subsec:analysis}

% We conduct a series of experiments on the effect of each component and also provide some results about the Pedestrian and cyclist augmentation. At last, we show some corner bases generated by our method.

% \paragraph{Ablation Study}
Table~\ref{tab:ablation_nerf_model} reports the results of our ablation experiments with the FCOS3D detector. The baseline is using DVGO \cite{sun2021direct} with depth supervision for data augmentation. The results indicate that even with vanilla voxel-based NeRF without any improvement, our proposed Drive-3DAug can improve the performance of the detector by 0.5\% on LET-AP.  After adding the geometric rectified loss and the symmetric-aware training strategy, the LET-AP is further increased by 0.6\% and 0.6\% respectively. In addition, we provide some visualizations to demonstrate the effectiveness of each component. 
% Figure \ref{fig:ablation_back_nerf} shows a comparison of the rendered background depth map produced by different models. By adding depth supervision to the training of the background model, the model learns better depth information, which further allows for more realistic translations of objects. 
Figure~\ref{fig:ablation_geo} shows that the geometric rectified loss can significantly suppress the edge defects and improve the reconstruction quality.
Figure~\ref{fig:ablation_sym} shows the results of the symmetric-aware training strategy for the novel view synthesis. We can see that adding this training strategy makes the model have larger viewpoints.



\begin{figure}[t]
  \centering
% \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}1 * 4
\includegraphics[width=0.99\linewidth]{fig/corner_case_2_2.pdf}
\caption{\textbf{Corner Cases} generated by the Drive-3DAug, where (a), (b) illustrate occlusion situations, (c) demonstrates strange vehicle heading direction, and (d) shows a vehicle appended on slope road. Cars with 3D bounding boxes are rendered by NeRF.}
\label{fig:corner_case}
\end{figure}

% the rendered objects with or without any geometric constraints. Based on the results, we can conclude that model is capable of rendering more clear boundaries for objects than the vanilla voxel-based NeRF model with the geometric correction method. Furthermore, the geometric symmetry method enables us to generate novel views of the objects, which can be used to enlarge the dataset.

% Figure \ref{fig:ablation_object_nerf} presents the rendered objects with or without any geometric constraints. Based on the results, we can conclude that model is capable of rendering more clear boundaries for objects than the vanilla voxel-based NeRF model with the geometric correction method. Furthermore, the geometric symmetry method enables us to generate novel views of the objects, which can be used to enlarge the dataset.






% \setlength{\tabcolsep}{5pt}
% \begin{table}
%   \small
%   \begin{center}
%   \begin{tabular}{@{}lcccc@{}}
%     \toprule
%     Method & \makecell[c]{LET-APL\\ $[50\mathrm{m},+\infty)$} & \makecell[c]{LET-APH\\ $\sim45^{\circ}$} & \makecell[c]{LET-APH\\ $\sim90^{\circ}$} & \makecell[c]{LET-APH\\ $\sim135^{\circ}$}  \\
%     \midrule
%     baseline    & 0.428 & 0.293	& 0.285	& 0.223 \\
%     + Drive-3DAug w/o RT     & 0.438 & 0.293	& 0.295	& 0.226 \\
%     + Drive-3DAug w/ RT     & \textbf{0.443} & \textbf{0.315} & \textbf{0.299} & \textbf{0.234} \\
%     \bottomrule
%   \end{tabular}
%   \end{center}
%   \caption{The comparison of detection results based on augmented data with different heading distributions.}
%   \label{tab:ablation_headings}
% \end{table}

% \paragraph{The Strategies of Scene Generation}

%  We evaluate three different strategies of scene generation to show xxx. For \emph{Drive-3DAug-far}, the instances is only pasted in [50m, 75m). For \emph{Drive-3DAug-dense}, the number of pasted instances is doubled. For \emph{Drive-3DAug-R/T}, the upper bound of transaction and rotation is set to 20m, 5m, and 30$^{\circ}$. From the results given in Table \ref{}, xxx. 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.98\linewidth]{aug_method.jpg}
%     \caption{Scene generation with different strategy to create the scene satisfying specified data distribution.}
%     \label{fig:heading_direction}
% \end{figure}


% \begin{table}
%   \centering
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     Setting  & LET-AP & LET-AP (far) & LET-AP (dense)   \\
%     \midrule
%     baseline   & 0.258 & 0.155 & 0.255 \\
%     Sparse     & 0.284 & 0.170 & 0.274  \\
%     Sparse+far & 0.302 & 0.174 & 0.304  \\
%     Dense+far  & 0.319 & 0.192 & 0.315  \\
%     \bottomrule
%   \end{tabular}
%   \caption{The comparison of detection result based on training data generated from different strategies of scene generation.}
%   \label{tab:ablation_scene_generation}
% \end{table}
\vspace{-2mm}
\paragraph{Reconstruction Cost.}
Table~\ref{tab:scece_recon_cost} depicts the comparison of reconstruction speed on a NVIDIA V100 GPU between previous methods and our method. Unlike the MLP-based NeRF~\cite{mildenhall2020nerf} which needs more than 20 hours of training for one background, it takes about 0.5h for the voxel-based NeRF with depth supervision. For the object model, the reconstruction time is within minutes. Considering the model size of our NeRF is rather small, we can run multiple reconstructions in parallel. 
% In our practice, we can reconstruct all the selected scenes of Waymo or nuScenes within half a day with xxx V100. 
Moreover, once these models are trained, they could be recycled for different detectors as a digital driving assets.

\setlength{\tabcolsep}{10pt}
\begin{table}[t]\small
  \begin{center}    
{
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Method & Voxel Field.  &  Depth Sup.&    Cost \\
    \midrule
    NeRF~\cite{mildenhall2020nerf}  & -            & - & $> 20$h \\
    DVGO~\cite{sun2021direct}       & $\checkmark$ & - & $0.7$h \\
    Ours                            & $\checkmark$ & $\checkmark$ & \textbf{$0.5$h} \\
    \bottomrule
  \end{tabular}
  }
  \end{center}
  \caption{\textbf{Reconstruction cost} of different methods for one background, assessed on a NVIDIA V100 GPU with 16GB memory.
  }
  \label{tab:scece_recon_cost}
\end{table}



\vspace{-2mm}
\paragraph{Corner Case Generation.}
Drive-3DAug is able to generate many photographic data for various corner cases~\cite{cornercase2021} without much effort for autonomous driving systems. As described in Figure~\ref{fig:corner_case}, we use our method to simulate several corner cases including \emph{the car occluded by the environment}, \emph{the car appearing on the road with strange positions and headings}, and \emph{the car on the slope}, which are hard to collect in the real world. This shows that 3D data augmentation can help alleviate issues of autonomous driving caused by plenty of corner cases.

% \begin{figure}[t]
%   \centering
% \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}1 * 4
% % \includegraphics[width=0.99\linewidth]{fig/corner_case_2_2.pdf}
% \caption{\textbf{Cross-dataset} generation with the car from nuScenes and the background from Waymo.}
% \label{fig:cross_data}
% \end{figure}

% \setlength{\tabcolsep}{14pt}
% \begin{table*}[t]
% \small
% \begin{center}
% \begin{tabular}{lcccccc}
% \toprule
% \multicolumn{1}{l}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{Pedestrian} & \multicolumn{3}{c}{Cyclist} \\ \cmidrule{2-7} 
% \multicolumn{1}{l}{}                        & LET-AP & LET-APH & LET-APL & LET-AP & LET-APH & LET-APL \\ \midrule
% FCOS3D                                        & 0.339    &   0.222      &    0.211     & 0.231    &    \textbf{0.185}     &    \textbf{0.146}     \\ 
% + Drive-3DAug w RT                           & \textbf{0.344}    &    \textbf{0.226}     &   \textbf{0.213}      & \textbf{0.234}    &     0.179    &    0.135     \\ \bottomrule
% \end{tabular}
% \end{center}
% \caption{Monocular 3D detection results of \textbf{pedestrian} and \textbf{cyclist} on Waymo validation set.}
% \label{tab:pb}
% \end{table*}

\setlength{\tabcolsep}{12pt}
\begin{table}[t]
\small
\begin{center}
\begin{tabular}{lcc}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{LET-AP} \\ \cmidrule{2-3}
  & Pedestrian & Cyclist \\ \midrule
FCOS3D \cite{wang2021fcos3d}                                        & 0.339         & 0.231             \\ 
+ Drive-3DAug w RT                           & \textbf{0.344}     & \textbf{0.234}        \\ \bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Monocular 3D detection results} of pedestrian and cyclist on Waymo validation set. LET-AP is the longitudinal error tolerant 3D average precision.}
\vspace{-2mm}
\label{tab:pb}
\end{table}


% \paragraph{Generation across datasets}
% Figure \ref{fig:cross_data} shows the rendering that we combine the car from nuScenes and the background from Waymo. Compared with the direct copy-paste method, our method can generate more realistic images. This can further enlarge the training data with a magnitude.

\vspace{-3mm}
\paragraph{Pedestrian and Cyclist Augmentation.}

Table \ref{tab:pb} shows the results for applying Drive-3DAug to the \textit{pedestrian} and \textit{cyclist} on Waymo. These objects are difficult for reconstruction, especially in driving scenes. Because they are not rigid objects and may change their pose dramatically in consecutive frames. However, our method can still improve the detection results for these two classes by 0.5\% and 0.3\% on LET-AP, respectively.


% choose figure: @ruoyi
% \begin{itemize}
%     \item the car occluded by the environment
%     \item the car appears on the road with strange headings
%     \item the cyclist/pedestrian occluded by the pasted car
% \end{itemize}