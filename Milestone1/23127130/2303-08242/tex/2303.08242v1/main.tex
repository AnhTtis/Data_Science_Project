% \documentclass[12pt]{article}
\documentclass[aoas]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[authoryear]{natbib}
% \RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 
%\usepackage[cmbold]{mathtime}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[ruled]{algorithm2e}
\usepackage{xcolor}
\usepackage{epstopdf}
\usepackage{amsthm}
% \usepackage[colorlinks=true, citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true, citecolor=blue,urlcolor=blue}
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{xr,xr-hyper}
\externaldocument[A-]{App}


% \usepackage[a4paper]{geometry}

\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{remark}                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{example}{Example}
\newtheorem*{fact}{Fact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{Thm}{Theorem}[section]
% \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{Con}[theorem]{Conjecture}
\newtheorem{Cor}[theorem]{Corollary}
\newtheorem{Eg}[theorem]{Example}
\newtheorem*{Rem}{Remark}
% \newtheorem{definition}[theorem]{Definition}

\newcommand{\red}{\textcolor{red}}
\newcommand{\cl}{\mathcal}
\newcommand{ \tr}{\mathrm{tr}}
 \newcommand{\ec}{\wh{\mbf{c}}}
\newcommand{\wh}{\widehat}
\newcommand{\wt}{\widetilde}
\DeclareMathOperator{\vect}{vec}
\newcommand{\mbf}{\boldsymbol}
\newcommand{\bb}{\mathbb}
\newcommand{\mrm}{\mathrm}
\newcommand{\E}{{E}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Var}{\mathrm{var}}
\newcommand{\EqD}{\overset{d}{=}}
\newcommand{\ConvP}{\overset{P}{\rightarrow}}
\newcommand{\ConvD}{\overset{d}{\rightarrow}}
\newcommand{\ConvFDD}{\overset{f.d.d.}{\longrightarrow}}

\newcommand{\qcr}{\fontfamily{qcr}\selectfont}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\endlocaldefs


%%% User-defined macros should be placed here, but keep them to a minimum.
% \usepackage[margin=1in]{geometry}


% %\pdfminorversion=4
% % NOTE: To produce blinded version, replace "0" with "1" below.
% \newcommand{\blind}{1}

% % DON'T change margins - should be 1 inch all around.
% \addtolength{\oddsidemargin}{-.5in}%
% \addtolength{\evensidemargin}{-.5in}%
% \addtolength{\textwidth}{1in}%
% \addtolength{\textheight}{-.3in}%
% \addtolength{\topmargin}{-.8in}%
% \title[Optimal Sampling Designs]{Optimal Sampling Designs for Online Estimation of Streaming Multi-dimensional Time Series}
% \author{Rui Xie}
% \address{University of Central Florida, Orlando, FL 32826 USA}

% \author{Shuyang Bai}
% \coaddress{Shuyang Bai, Department of Statistics, University of Georgia, Athens, GA 30602 USA. E-mail:{bsy9142@uga.edu}}

% % \address{University of Central Florida, Orlando, FL 32826 USA}
% % \email{bsy9142@uga.edu}

% \author[Rui Xie {\it et al.}]{Ping Ma  }
% \address{University of Georgia, Athens, GA 30602 USA}

\begin{document}

\begin{frontmatter}
\title{Optimal Sampling Designs for   Multi-dimensional Streaming Time Series with Application to Power Grid Sensor Data}
%\title{A sample article title with some additional note\thanksref{t1}}
\runtitle{Optimal Sampling Designs}
%\thankstext{T1}{A sample additional note to the title.}

\begin{aug}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Only one address is permitted per author. %%
%% Only division, organization and e-mail is %%
%% included in the address.                  %%
%% Additional information can be included in %%
%% the Acknowledgments section if necessary. %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[A]{\fnms{Rui} \snm{Xie}\ead[label=e1]{rui.xie@ucf.edu}},
\author[B]{\fnms{Shuyang} \snm{Bai}\ead[label=e2,mark]{bsy9142@uga.edu}}
\and
\author[B]{\fnms{Ping} \snm{Ma}\ead[label=e3,mark]{pingma@uga.edu}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Addresses                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\address[A]{Department of Statistics and Data Science, University of Central Florida, Orlando, FL 32826 USA,
\printead{e1}}

\address[B]{Department of Statistics, University of Georgia, Athens, GA 30602 USA,
\printead{e2,e3}}
\end{aug}

\begin{abstract}
The Internet of Things (IoT) system generates massive high-speed temporally correlated 
 streaming data and is often connected with online inference tasks under computational or energy constraints.
Online analysis of these streaming time series data often faces a trade-off between statistical efficiency and computational cost. One important approach to balance this trade-off is sampling, where only a small portion of the sample is selected for the model fitting and update.
Motivated by the demands of  dynamic relationship analysis of IoT system, 
 we study the data-dependent sample selection and online inference problem for a multi-dimensional streaming time series, aiming to provide low-cost real-time analysis of high-speed power grid electricity consumption data. 
Inspired by D-optimality criterion in design of experiments, we propose a class of online data reduction methods that achieve an optimal sampling criterion and improve the computational efficiency of the online analysis.
We show that the optimal solution amounts to a strategy that is a mixture of Bernoulli sampling and leverage score sampling. 
The leverage score sampling involves auxiliary estimations that have a computational advantage over recursive least squares updates.
Theoretical properties of the auxiliary estimations involved are also discussed. 
When applied to European power grid consumption data, the proposed leverage score based sampling methods outperform the benchmark sampling method in online estimation and prediction.
The general applicability of the sampling-assisted online estimation method is assessed via simulation studies. 
 
\end{abstract}

\begin{keyword}
\kwd{Streaming data}
\kwd{Sampling design}
\kwd{Multi-dimensional time series}
\kwd{D-optimality}
\end{keyword}

\end{frontmatter}



% \noindent%
% \keywords{D-optimality; Multi-dimensional time series; Sampling design; Streaming data.}
% \vfill

% \newpage
% \spacingset{1.5} % DON'T change the spacing!
\section{Introduction}\label{Sec:Intro}

In the era of Internet of things (IoT), the prevalence of sensor networks, wearable devices, and power grid networks   has led to   an enormous amount of  data streams being automatically collected every second, or even every millisecond.  Examples range from security monitoring in power grids
\citep{li2019online} to traffic  monitoring in smart traffic system  \citep{nellore:2016:survey}, from  health surveillance through smart wearable devices  \citep{islam:2015:internet} to soil condition sensors in precision agriculture \citep{mat:2016:iot}.
%machine   condition  monitoring in cyber-manufacturing \citep{wu:2017:fog}. 
These IoT data streams carry rich and time-sensitive information on the targeted  subjects or systems, offering an unprecedented potential for real-time monitoring, forecasting and control. 
This potential, however, has not
yet been sufficiently exploited, because  the  computing
infrastructure still lags far behind the exponential growth of data sources.
For instance, a network layer of IoT system deployed in a smart power grid usually consists of a large number of low bandwidth, low energy, low processing power nodes for communication using WiFi, 3G, 4G or power line communication technologies,  rendering sophisticated  real-time analytics
infeasible \citep{jaradat2015internet}.
The IoT sensor data streams can arrive at a very high speed,  which will accumulate a large quantity of data to be analyzed in a short period of time. For real-time tasks in IoT applications, such as prediction or dynamic information flow tracking, the inference speed may lag behind the data arriving speed, especially for complex inference tasks.   
To conquer the data overflow challenge in large-scale IoT applications, we aim to provide a  sampling solution with reliable inference performance and low computation costs. 
We use publicly available power grid electricity consumption data as a motivating example to illustrate  challenges and potential solutions. 
%whereas a centralized computational infrastructure  allowing for  sophisticated   real-time analytics  is often unavailable   \citep{bonomi:2012:fog}. 
\begin{example}
\textbf{Open Power System Data: Time series of electricity consumption}

Electricity consumption, measured by electricity loads over time in a power grid system, is a typical type of streaming data that arrives at a high speed.
In the smart IoT application, electricity consumption recorded from smart meters are capable of observing data at very high frequency, such as 25 kHz sampling frequency for the sinusoidal voltage signal~\citep{jumar2020database}.
In a power system, real-time feedback of electricity loads, which includes prediction and model fitting, is important in optimizing energy consumption patterns~\citep{marangoni2021real}.
The real-time inference helps effectively lower energy consumption by reducing energy demand and  leveling off the usage peaks.
The accurate real-time prediction will also benefit effective scheduling and decision-making in the power system~\citep{XU2021117465}.
Therefore, online analysis of the electricity load time series plays an important role in practice.
%  real-time information reduce energy consumption
% in the form of prediction or model fitting 
% 3. power load data calls for fast analysis.Real-time feedback is optimizing energy consumption patterns ~\cite{marangoni2021real} such as  effectiveness in reducing energy demand and especially in levelling off the daily peaks of.
%  real-time information reduce energy consumption
%  4. even though
% is often subject to quite restrictive terms of use, we use the public avaiable data to motivate us. 
% ~\cite{realdata}.
% \red{Challenge}

In 2020, the electricity consumption data in the United States has projected to use a total of $1,000$ million terabytes (TB) of storage~\citep{siddik2021environmental,shehabi2016united}. 
There are emerging needs for novel approaches to analyzing such massive data, especially the real-time analysis of massive data streams.
In our study, we shall work with the data streams from publicly available power system data platform \citep{realdata}, given the fact that energy data is often subject to restrictive terms of use. 
 The \cite{realdata} consists of time series of electricity consumption (load) for $37$ European countries with hourly resolution. The selected time series are recorded from 2006-01-01, 00:00:00 Coordinated Universal Time (UTC) to 2018-12-14, 23:00:00 UTC, which results in $113,544$ time points.
Electricity power consumption data from different countries are reported through different platforms in the Open Power System Data. 
We use the actual load of ENTSO-E power statistics from $19$ countries as the variables of interest. 
\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=6in]{figures/load_color-eps-converted-to.pdf}
		\caption{Load curves from 2006 to 2018 for $19$ European countries.}\label{fig:surface}
	\end{center}
\end{figure}
Figure~\ref{fig:surface} displays the dynamic evolution of the load curves from 2006 to 2018 for those $19$ countries.
\end{example}



The online analysis of the IoT systems  usually starts from  sampling~\citep{zhou2019joint} or filtering~\citep{akbar2017predictive} algorithms to optimize the data collection process from the IoT sensors, which results in massive volumes of regularly spaced multidimensional time series data streams, the latter being  a well-developed statistical subject \citep{hamilton:1994:time,lutkepohl:2005:new}.
% Data streams collected from  a sensor network, wearable devices, and a power network can be  viewed as a multidimensional time series, 
Analyzing those large-scale high-frequency data streams posts challenges in real-time inference of multidimensional time series model, where the
computational cost is in the order of $K^3n^3$ with $n$ samples of $K$-dimensional data.
Real-time calculations, including inference and prediction, are usually critical for decision making and resource optimization in IoT applications. 
% We shall now present a motivating example based on a public available electricity consumption time series.



% data fusing and data filtering technique
Methodologically speaking,    \emph{online analysis} of classical multidimensional time series is often carried out under the framework of dynamic linear model or state space model~\citep{west:1997:bayesian,petris:2009:dynamic}. With dynamic linear models, online statistical computations including estimation and forecasting can often be aided by the efficient Kalman filter recursive algorithms \citep{kalman:1960:new,kalman:1961:new}.  However, for applications in many IoT systems, the stringent computational resource often cannot afford to perform  typical dynamic linear model computations such as likelihood optimization or  Bayesian sampling fast enough to achieve real-time analysis.  In fact, even   restricting to linear systems, for which much less costly online estimation algorithms (e.g., recursive least squares) can be applied, a real-time inference utilizing the whole data stream can still be challenging \citep{gabel:2015:monitoring,berberidis:2016:online, hooi:2018:streamcast}.

For smart power gird analysis, due to the limited accessibility of the data, various sampling techniques, including simple random sampling and  Latin hypercube sampling~\citep{cai2013probabilistic},  are used to generate synthetic data points for Monte-Carlo simulation or static power system model training~\citep{balduin2022sampling}. The leverage score based sampling technique was used for monitoring cyberattacks in IoT system but not for the online inference~\citep{li2019online}.    To the best of our knowledge, the literature on sampling or data reduction method for online analysis of large-scale dynamic power grid systems is still lacking.
Such a situation calls for the development of new approaches.





Broadly speaking, the problem  described above belongs to a contemporary   research direction  that considers the trade-off between statistical efficiency and computational cost    \citep{jordan:2013:statistics}. 
One natural and important approach to balance  this trade-off  is    \emph{sampling}.  In this approach, a small  portion of the full observed data is chosen  as a
surrogate   to carry out computations of interest.  
Sampling, or more generally \emph{data reduction}, has been considered in various studies as a means to reduce computational cost. The majority of these studies aim to achieve certain numerical approximation accuracy, which has sparked the popularity of notions such as   sketching  (e.g., \cite{drineas2012fast,liberty:2013:simple,woodruff:2014:sketching,zhang:2017:randomization}) and coreset  (e.g., \cite{agarwal:2005:geometric, dasgupta:2009:sampling, feldman:2013:turning}). 





Recently in the context of big data, some statistical investigations of sampling methods with an emphasis on   \emph{statistical estimation efficiency}   have emerged \citep{ma:2015:statistical, wang:2018:optimal, wang:2018:information,  ting:2018:optimal,yu:2020:optimal,meng2020lowcon,ma2020asymptotic,10.1214/21-AOAS1462}.  The aforementioned investigations  are all carried out under the setup of independent samples and offline computation where the whole data set is available from the beginning.  For online analysis of streaming time series exhibiting temporal dependence, relevant statistical research is still largely lacking. Exceptions are the work \cite{xie:2019:online}  that considered   online estimation of a Gaussian vector autoregression (VAR) model assisted by the so-called leverage score sampling (LSS), and the work \cite{eshragh2019lsar} that applied leverage score-based sampling for the analysis of large-scale univariate time series data. Leverage score sampling proposed in \cite{xie:2019:online}  selects samples based on thresholding a leverage score of the lagged covariate in vector autoregression. However, a number of questions regarding this sampling method  were left unaddressed.  \cite{xie:2019:online} showed that LSS achieves an asymptotic efficiency superior to a naive Bernoulli sampler. However, its optimality over other possible sampling methods was not clarified. Furthermore, the sample selection rule of LSS focuses exclusively on high-leverage covariate points, which in practice might  lead to a lack of good fit over low-leverage region and sensitivity to outliers. In addition   in  \cite{xie:2019:online}, the auxiliary estimations involved in implementing LSS were not  justified.

%  By using a recently-developed algorithm of Drineas et al. (2012) to compute fast approximations to the statistical leverage scores, we also demonstrate a regime for
% large data where our shrinkage leveraging procedure is better algorithmically, in the sense
% 863
% Ma, Mahoney and Yu
% of computing an answer more quickly than the usual black-box least-squares solver, as well
% as statistically, in the sense of having smaller mean squared error than na¨ıve uniform sampling. D


The overall objective of this paper is to develop methodologies for performing online  analysis on high-speed multidimensional time series, where we apply them to provide a real-time solution for massive power grid sensor  data inference in IoT applications.
% We also establish theory of optimal sampling for online estimation of streaming multi-dimensional time series. 
On the methodology side, we propose a computationally efficient online sampling method called relaxed-LSS,  which can be applied to linear multivariate time series models including extraneous variables and enjoys more robustness compared with LSS. The relaxed-LSS and the time series model are used for online inference and prediction of the IoT power consumption data.  On the theory side, we establish the D-optimality of asymptotic estimation efficiency of LSS and more generally relaxed-LSS in a broad class of online sampling methods. We also establish consistency properties incorporating some auxiliary estimations in the sampling methods.  
 


% ies initiated in \cite{xie:2019:online}.
We organize the article as follows.
In Section \ref{Sec:linear sys}, we consider a framework that covers a large class of models subsuming the Gaussian Vector Autoregression considered in \cite{xie:2019:online}.
% , which allows for incorporating extraneous variables as well as moderate heavy-tailedness.   
In Section \ref{Sec:opt sampling},  we formulate a class of online sampling methods  within the framework of Section \ref{Sec:linear sys} and show that LSS is the optimal choice among them for asymptotic estimation efficiency. We then proceed to propose a novel  relaxed version of LSS which keeps a proportion of low-leverage covariate points. 
Section \ref{Sec:aux} addresses  the  auxiliary estimations and practical implementation involved in the sampling algorithm, including  an online estimate of an inverse covariance matrix which is performed sparsely for computational advantage.
 Section \ref{Sec:data} considers  an application to the electricity consumption time series in
Open Power System Data. 
Section \ref{Sec:simulate} includes the simulation studies of LSS and its relaxed version. 
The conclusion and future work are presented in Section \ref{con}.
The proofs of the theoretical results are collected in Supplementary Material.


\section{Model Assumptions}\label{Sec:linear sys}
By default, all vectors are column vectors.  Throughout this paper,  $\|\cdot\|$ denotes  the matrix operator norm with respect to the Euclidean norm (so it is  the Euclidean norm for   vectors).

Suppose $(\mbf{y}_t)$ is the $K$-dimensional time series of interest.   For notional convenience, we allow $t$ to vary within the whole integer set $\bb{Z}$, whereas the   starting point of $t$ will become clear in a context. 
We shall  model $(\mbf{y}_t)$ as a $\bb{R}^K$-valued    ergodic (strictly) stationary process with finite variance.    Suppose, in addition, that we have a $\bb{R}^p$-valued ergodic stationary  process $(\mbf{x}_t)$ with finite variance serving as explanatory variables for $(\mbf{y}_t)$.  We impose the following stationarity assumption,  which will help simplify the formulation of the basic results.
% and will  serve as a foundation for tackling the more involved non-stationary case in   future works.
Let $\mbf{\mu}_X=\E[\mbf{x}_t], \mbf{\mu}_Y=\E[\mbf{y}_t]$,
% \begin{equation}\label{eq:centering no}
% 	\mbf{\mu}_X=\E[\mbf{x}_t],\quad \mbf{\mu}_Y=\E[\mbf{y}_t].
% \end{equation}
we shall assume the following linear system  model for the centered time series,
\begin{equation}\label{eq:stat sys}
	\mbf{y}_t-\mbf{\mu}_Y   = B'(\mbf{x}_t-\mbf{\mu}_X)       + \mbf{e}_t,
\end{equation}
where $B$ is a $p\times K$ coefficient matrix and $B'$ stands for its transpose, $(\mbf{e}_t)$ is a $\bb{R}^K$-valued ergodic stationary noise process satisfying the martingale difference property, 
\begin{equation}\label{eq:md}
	\E[\mbf{e}_t|  \cl{F}_t]=\mbf{0},
\end{equation}
where the filtration $\cl{F}_t=\sigma(\mbf{x}_s,\ \mbf{e}_{s-1},\ s\le t)$, and the conditional expectation in \eqref{eq:md} is taken component-wise. We also assume a constant conditional  covariance matrix for the error process:
\begin{equation}\label{eq:e_t cov}
	\E [\mbf{e}_{t_1} \mbf{e}_{t_2}'|\cl{F}_t]=\Omega 1_{\{t_1=t_2\}},\quad t_1,t_2\in \bb{Z}, \ t_1,t_2>t
\end{equation}
for some non-singular covariance matrix $\Omega$.     Assume,  in addition, that $(\mbf{x}_t;\mbf{e}_t)$ is jointly stationary.
In literature~(e.g., \citet[Chapter 14]{georgebox2015}), the model \eqref{eq:stat sys} is often replaced by one with the means absorbed into an intercept term in $\mbf{x}_t$. Our formulation here singles out the estimation of the means which is computationally trivial, and facilitates the development of the optimality results.   


It is worth pointing out that the components of the explanatory variable process $(\mbf{x}_t)$ are allowed to contain lagged values of $(\mbf{y}_t)$. For instance, we may set
\begin{equation}\label{eq:VARX x_t}
	\mbf{x}_t = (\mbf{y}_{t-1}',\cdots,\mbf{y}_{t-p_1}', \mbf{v}_{t-1}',\cdots,\mbf{v}_{t-p_2}' )' \in \bb{R}^{K(p_1+p_2)},
\end{equation}
where $(\mbf{v}_t)$ is a  stationary process extraneous to $(\mbf{y}_t)$, where each $\mbf{v}_t \in \bb{R}^{K}$. The model \eqref{eq:stat sys} with the specification \eqref{eq:VARX x_t}  is often known as a VARX model (e.g., \citet[Chapter 10]{lutkepohl:2005:new}), which  becomes the well-known vector autoregression model if the extraneous variables are absent. The VARX model is more commonly expressed as
\begin{equation}\label{eq:VARX}
	\mbf{y}_t  = \sum_{i=1}^{p_1}\Phi_i \mbf{y}_{t-i}  +\sum_{j=1}^{p_2} \Psi_j \mbf{v}_{t-j} + \mbf{e}_t 
\end{equation} 
for some $K\times K$ coefficient matrices $\Phi_i$'s and $\Psi_i$'s, where  $\Phi_i$'s must satisfy appropriate constraints to allow the existence of stationary solution of \eqref{eq:VARX} (e.g.,~\citet[Section 10.1]{hamilton:1994:time}).


The linear system model \eqref{eq:stat sys} and its variants  have been applied in various IoT contexts for real-time analysis of streaming time series:   anomaly detection in streaming environmental sensor data \citep{hill:2010:anomaly};    tracking  causal interactions between brain
regions based on MEG sensor streams \citep{michalareas:2013:investigating};  design of energy-efficient operation of  low-power wireless medical sensors \citep{anagnostopoulos:2014:autoregressive};    traffic forecasting in large urban areas based on road network sensors \citep{schimbinschi:2017:topology}.

For the development of optimal online sampling theory in Section \ref{Sec:opt sampling}, we shall impose an additional assumption  on the distributional shape of the covariate $\mbf{x}_t$. Recall a $p$-dimensional elliptical (contoured) distribution $EC_p(\mbf{\mu},\Sigma,\nu)$ (cf.\ \cite{fang:1990:symmetric})  is specified by the following
three components: a vector
$\mbf{\mu}\in \bb{R}^p$ called  \emph{location}, a $p\times p$ non-negative definite matrix $\Sigma$ called \emph{scatter}, which coincides with the covariance matrix when the latter exists,    and  a probability distribution $\nu$ on $\bb{R}_+$. 
In particular, $EC_p(\mbf{\mu},\Sigma,\nu)$ 
denotes the distribution of random vector $
\mbf{\mu}+ \xi \Sigma^{1/2} \mbf{S}$,
where $\Sigma^{1/2}$ is a symmetric square root of $\Sigma$, the random variable $\xi$ follows the distribution   $\nu$ and is called the \emph{generating variate}, and $\mbf{S}$ is the uniform distribution on the $p$-dimensional unit sphere $\{\mbf{x}\in \bb{R}^p:\ \|\mbf{x}\|=1\}$ which is independent of $\xi$. 
A multivariate normal distribution  is included in this family as a special case.  
For the rest of the paper, we shall assume that for each fixed $t\in\bb{Z}$, the covariate vector 
\begin{equation}\label{eq:x EC}
	\mbf{x}_t\sim EC_p(\mbf{\mu}_X,\Sigma,\nu),
\end{equation}
where the distribution $\nu$ is  absolutely continuous and the scatter $\Sigma$ is also the positive definite covariance matrix of $\mbf{x}_t$. Hence the distribution of $\mbf{x}_t$ is absolutely continuous.



\section{Optimal Online Sampling}\label{Sec:opt sampling}

To develop the main ideas, throughout this section    we shall   also assume  for simplicity that
\begin{equation}\label{eq:centering}
	\mbf{\mu}_X=\mbf{\mu}_Y = \mbf{0}.
\end{equation}
In practice, online estimation of these means can be achieved  efficiently with a negligible computational cost compared to that of estimating $B$. See also Section \ref{Sec:aux} below for more details.

\subsection{A Class of Online Samplers}



When  a   sample stream $(\mbf{y}_t;\mbf{x}_t)_{t=1}^n$ from the whole stream   $(\mbf{y}_t;\mbf{x}_t)_{t=1}^\infty$ satisfying \eqref{eq:stat sys} is observed and $n\gg Kp$,   the least square estimator  
% \[
% \wh{B}_{n,LS}=\left( \sum_{t=1}^n  \mbf{x}_t   \mbf{x}_t'  \right)^{-1} \left( \sum_{t=1}^n \mbf{x}_t \mbf{y}_t'   \right)
% \] 
\[
\wh{B}_{n,LS}=\argmin\limits_{B}\sum_{t=1}^{n}||\mbf{y}_t - B'\mbf{x}_t ||^2
\]
may be used to estimate $B$.
In the context of online estimation, the computation of $\wh{B}_{n,LS}$ can be implemented in a recursive manner based on the Sherman–Morrison inversion formula \citep{sherman:1950:adjustment}, an algorithm often known as the \emph{recursive least squares} \citep{plackett:1950:some}. However, when  data stream arrives at an overwhelmingly fast rate with high dimension ($Kp$ is large), real-time update of $\wh{B}_{n,LS}$ can still be challenging given a limited computational capacity in the context of streaming data (see Section \ref{Sec:Intro}). 


The basic idea we shall propose is simple: instead of updating the estimation along the whole stream of data   $(\mbf{y}_t;\mbf{x}_t)_{t=1}^\infty$, we shall skip some time points $t$ and only update the estimate along a subset $I\subset \bb{Z}_+$. The problem becomes: how should $I$ be selected, or how do we select samples among $(\mbf{y}_t;\mbf{x}_t)_{t=1}^\infty$?

It is useful to note that our problem bears similarity to   the study of \emph{optimal designs}   (e.g., \cite{pukelsheim:1993:optimal,papalambros:2000:principles}). Generally speaking, an optimal design aims at achieving an optimal estimation precision with a fixed sample size (number of experimental runs). 
In optimal design for linear regression with response $\mbf{y}$ and design matrix $X$, i.e, $\mbf{y}=X\mbf{\beta}+\mbf{\epsilon}$, where random error  $\mbf{\epsilon}$ has mean zero and constant variance $\sigma^2I$, one often considers optimizing the matrix $(X' X)^{-1}$ (recall  $\Var[\wh{\mbf{\beta}}]=\sigma^{2}(X' X)^{-1}$, where $\wh{\mbf{\beta}}$ is the least square estimator) based on a certain criterion, e.g., the determinant (D-optimality). 
The optimization   is typically over a finite available set of candidate covariate points (treatment runs) with the rows of $X$ (number of runs)   fixed. In other words,  given a limited sample size, one needs to select appropriate covariate points among available ones to optimize $(X' X)^{-1}$. 

Motivated by this insight drawn from   optimal design, we propose to  base our online sample selection criterion on the covariate   $\mbf{x}_t$ as well.    
We   consider the following  class of samplers: at each time point $t$, the selection of  sample unit $(\mbf{y}_t;\mbf{x}_t)$ is determined solely by  $\mbf{x}_t\in \bb{R}^p$ up to a randomization. This is made precise   as follows. 
\begin{definition}
\label{Def:S}
	Suppose there is a measurable function  $s: \mathbb{R}^{p}\rightarrow [0,1]$, called the sampling function.
	A sampling method (or sampler) $\cl{S}(s)$ is defined  as follows: conditioning on $(\mbf{y}_t;\mbf{x}_t)_{t=1}^\infty$,   for each $t\in\bb{Z}_+ $, the  sample    $(\mbf{y}_t;\mbf{x}_t)$ is selected independently with probability $s(\mbf{x}_t)$. 
	
	The sampler $\cl{S}(s)$ can be alternatively described as follows: let $(U_t)_{t=1}^\infty$  be i.i.d.\ Uniform$(0,1)$ random variables which are independent of $(\mbf{y}_t;\mbf{x}_t)_{t=1}^\infty$.  The   selected index set $I$ is given by 
	\[
	I=\{t\in \bb{Z}_+:\ U_t\le  s(\mbf{x}_t)\}.
	\]  
\end{definition} 
The (unconditional) sampling rate of $\cl{S}(s)$   is given by
\begin{equation}\label{eq:q gen}
	q:= pr(U_t\le s(\mbf{x}_t))= \E[  s(\mbf{x}_t)]\in [0,1].
\end{equation}
When a  stream of total length $n$ passes through, the   size of selected sample is given by a random number
\[
N:=\sum_{t=1}^n 1_{\{U_t\le s(\mbf{x}_t)\}}=|I|.
\]  
By the ergodic theorem \cite[Theorem 10.6]{kallenberg:2002:foundations}, we have\begin{equation}\label{eq:N/n}
	\frac{N}{n} \rightarrow  q
\end{equation}
a.s.\ as $n\rightarrow\infty$.


As an example,
a constant sampling function $s(\mbf{x})\equiv q$ corresponds to the \emph{Bernoulli sampler}, that is, each index $t$ is  independently selected  with probability $q$ regardless of $\mbf{x}_t$.


For an \emph{ideal} stationary system, one may argue that there is no need to employ the sampling-assisted approach    for online estimation.  Indeed,   if the regression coefficients in $B$ in \eqref{eq:stat sys} stays invariant along the whole  data stream, then the older data provides exactly the same   information as the newer data about $B$.   Hence one may use the available computational capacity to update the estimate up to the best speed even if it cannot catch up with the newest received data. However, in the practice of real-time analysis of streaming time series, this approach should be avoided since it fails to reflect the latest information about the streams. In fact, the usefulness of   sampling-assisted online estimation is necessarily tied to \emph{non-stationarity}, be it for the detection of departure from stationarity, or for the predictive modeling of   non-stationary  time series.  For example, for predictive modeling, one may propose a time-varying version of \eqref{eq:stat sys} and estimate  time-varying $B$ by weighted least squares (e.g.,  \cite{zhou:2010:simultaneous,zhang:2012:inference}). Since such a system can be locally viewed as stationary, the  foundation laid in this paper will still be highly relevant in that context.
% We shall leave the study of sampling of non-stationary data to a future work. 


\subsection{D-Optimality of Leverage Score Sampler}


The next step is to formulate optimality among the class   $\{S(s)\}$.
Note that unlike a conventional setup in optimal design,    for online estimation the number of rows of the design matrix $X$ keeps increasing. So we propose to  formulate the optimization, in a sense, on an asymptotic version of  $(X^T X)^{-1}$.  
Let $\wh{B}_{n,I}$ be the least squares estimator of $B$ using only $(\mbf{y}_t;\mbf{x}_t)_{t\in I}$, namely,
\begin{equation}\label{eq:B LS}
	\wh{B}_{n,I}= \left( \sum_{t\in I}  \mbf{x}_t   \mbf{x}_t'  \right)^{-1} \left( \sum_{t\in I} \mbf{x}_t \mbf{y}_t'   \right)=\left( \sum_{t=1}^n  1_{\{U_t\le s(\mbf{x}_t)\}} \mbf{x}_t   \mbf{x}_t'  \right)^{-1} \left( \sum_{t=1}^n 1_{\{U_t\le s(\mbf{x}_t)\}} \mbf{x}_t \mbf{y}_t'   \right).
\end{equation} 
We have  the following asymptotic normality result.
\begin{theorem}\label{Thm:asymp_normality}
	Under the assumptions in Section \ref{Sec:linear sys}, suppose in addition that 
	\begin{equation}\label{eq:Gamma(s)}
		\Gamma(s)=\E\left[ s(\mbf{x}_t) \mbf{x}_t \mbf{x}_t' \right]
	\end{equation} is invertible. Then
	as    the total stream size $n\rightarrow\infty$,  
	\begin{equation}\label{eq:asymp norm lev} 
		\sqrt{N}(\vect(\wh{B}_{n,I})  - \vect(B))\sim  \sqrt{nq}(\vect(\wh{B}_{n,I})  - \vect(B))\ConvD \cl{N}(\mbf{0},  P(s)^{-1} ),
	\end{equation}
	where     $\vect(B)$ denotes  the vectorization of matrix $B$ by stacking the columns (from left to right) of $B$  into a single column, and the asymptotic precision  matrix is
	\begin{equation}\label{eq:P(s)}
		P(s)=\Omega^{-1}\otimes (q^{-1} \Gamma(s)),
	\end{equation}
	where $\otimes$ denotes the Kronecker product and $\Omega$ is the covariance matrix  defined in (\ref{eq:e_t cov}).
\end{theorem}
% The proof of Theorem \ref{Thm:asymp_normality} can be found in Appendix. 
An extension of Theorem \ref{Thm:asymp_normality} incorporating auxiliary estimates for the means and the sampling function is stated in Theorem \ref{Thm:asymp_aux} below.

It is now natural to consider the optimization of  the precision (information) matrix $P(s)$ under the constraint that the sampling rate $q=\E[s(\mbf{x}_t)]$ is fixed at a value in the interval $(0,1)$. In general,
one cannot expect to optimize $P(s)$ with respect to the Loewner order, namely, there is no    $P(s)$ which is optimal  in every   direction (e.g., \citet[Chapter 4]{pukelsheim:1993:optimal}). Instead, one may consider the optimization of a suitable scalar function of $P(s)$,  and most popularly, the determinant, which leads to the so-called D-optimality (e.g., \citet[Chapter 9]{pukelsheim:1993:optimal}).  By a property of Kronecker product,
$\det(P(s))$ is proportional to $\det(\Gamma(s))^{K}$, and hence we need to maximize $\det(\Gamma(s))$.


\begin{theorem}[D-optimality]
\label{Thm:D-opt}

	If the distribution of $\mbf{x}_t$ is elliptical as specified by \eqref{eq:x EC}, then  
	the following constrained optimization problem:
	\begin{equation}\label{eq:opt}
		\argmax_s \, \det( P(s) ) =\argmax_s \, \det( \Gamma(s) ) \quad \text{subject to}\quad \E[s(\mbf{x}_t)]=q\in (0,1],
	\end{equation}
	where the maximization is over all measurable $s:\bb{R}^p\rightarrow [0,1]$, has  solution
	\begin{equation}\label{eq:sample fun lev}
		s(\mbf{x})=1_{\{ \mbf{x}' \Sigma^{-1} \mbf{x} >r\}},
	\end{equation}
	where $r>0$ is chosen so that 
	\begin{equation}\label{eq:threshold quantile}
		pr( \mbf{x}'_t \Sigma^{-1} \mbf{x}_t >r)=q.
	\end{equation}
	This solution is almost everywhere unique with respect to the distribution of $\mbf{x}_t$.
\end{theorem}
Theorem \ref{Thm:D-opt} is a special case of Theorem \ref{Thm:D-opt relax} below, which is provided in Section \ref{Sec:relax}. 
We note that the optimality problem in Theorem \ref{Thm:D-opt} can be regarded as a special case of the general optimality problem formulated in \citet{pronzato:2006:sequential,pronzato:2020:sequential}.  Instead of obtaining an explicit solution as in \eqref{eq:sample fun lev}, the aforementioned studies focused on stochastic approximation algorithms and were in general constrained to the setup of i.i.d.\ trials. The explicit solution in Theorem \ref{Thm:D-opt}, although obtained under a more restrictive framework compared to \cite{pronzato:2006:sequential,pronzato:2020:sequential}, enables one to formulate explicit and efficient online sampling algorithms which can   be applied in the streaming time series context.

The tail quantile $r$   in \eqref{eq:threshold quantile} may not be unique if $pr( \mbf{x}'_t \Sigma^{-1} \mbf{x}_t >r)$ is not strictly decreasing with respect to $r$ (until reaching zero). One may eliminate this non-uniqueness by assuming that  the density  of $\nu$ in \eqref{eq:x EC} is positive.




%We also mention that,  
Besides  D-optimality,  one may   consider some other  optimality criteria, e.g., A-optimality which minimizes $\tr (P(s)^{-1})$, E-optimality which maximizes the minimum eigenvalue of $P(s)$, T-optimality which maximizes $\tr(P(s))$, etc (e.g., \citet[Chapter 9]{pukelsheim:1993:optimal}).  Different criteria may lead to different optimal solutions of the sampling function $s$. We have chosen  D-optimality since the   solution has  intuitively appealing interpretation as well as computational advantage.



Theorem \ref{Thm:D-opt} leads to the following  simple deterministic sampling rule: $(\mbf{y}_t;\mbf{x}_t)$ is selected if   $\mbf{x}'_t \Sigma^{-1} \mbf{x}_t$ exceeds the threshold $r$. Such a sampling rule was proposed in   \cite{xie:2019:online} for the special case of Gaussian vector autoregression  model,  and was termed \emph{leverage score sampling (LSS)} since  
\[
	\ell_t=  \mbf{x}_t' \Sigma^{-1} \mbf{x}_t ,
\]
can be interpreted as a \emph{leverage score} of the  $\mbf{x}_t$ which marks the influence of this covariate point (cf.\ \citet[Section 10.2]{seber:2012:linear}).  In  \cite{xie:2019:online} the superiority of LSS   over a Bernoulli sampler was established under the same sampling rate $q$ in terms of the Loewner order.  Theorem \ref{Thm:D-opt} above  takes a further step to show that  under the D-optimality criterion, LSS is the best choice among the $\cl{S}(s)$ class in Definition \ref{Def:S} which includes the Bernoulli sampler.


To implement LSS in practice, one needs auxiliary  estimations of the means of $\mbf{y}_t$ and $\mbf{x}_t$ (recall that we have assumed centering \eqref{eq:centering}), the calculation of leverage score $\ell_t$ as well as the $q$-quantile of $\ell_t$. These issues will be discussed in a more general context  in Section \ref{Sec:aux} below.







\subsection{Relaxed Leverage Score Sampling (Relaxed-LSS)}\label{Sec:relax}

The hard thresholding found in \eqref{eq:sample fun lev}  employed in LSS selects exclusively   high-leverage  $\mbf{x}_t$ points.  There are   two potential risks of removing low-leverage design points in a linear regression: (a) if the linear regression relationship between the response and the covariates fails to hold in the low-leverage design region, then one would not be able to detect this departure;   (b) relying on high-leverage points  may make the regression particularly susceptible to the influence of outliers. Recall that the influence of an outlier is a combined effect of  the magnitude of   the regression residual  and the leverage of the design point, as measured by Cook's distance  \citep{cook:1977:detection} for instance. 

These potential problems suggest us to relax LSS by including a reasonable fraction of low-leverage points. This motivates the following  optimization modified from that in Theorem \ref{Thm:D-opt}:
\begin{equation}\label{eq:relax LSS opt}
	\argmax_s \, \det( \Gamma(s) ) \quad \text{subject to}\quad \E[s(\mbf{x}_t)]=q\in (0,1],\quad s(\mbf{x})\ge q_0, 
\end{equation}
where $s$ is the sampling function in Definition \ref{Def:S}, and $q_0\in [0,q]$. In other words, we include the additional constraint that there is  a  base sampling rate $q_0$ regardless of the design point $\mbf{x}_t$.  
\begin{theorem}\label{Thm:D-opt relax}
	Under the same conditions as Theorem \ref{Thm:D-opt},   the optimization  problem  \eqref{eq:relax LSS opt} has   solution 
	\begin{equation}\label{eq:s tilde}
		s(\mbf{x})=q_0+(1-q_0)1_{\{ \mbf{x}' \Sigma^{-1} \mbf{x} >r\}},
	\end{equation}
	where $r$ is chosen so that
	\begin{equation}\label{eq:threshold quantile relax}
		pr( \mbf{x}'_t \Sigma^{-1} \mbf{x}_t >r)=\frac{q-q_0}{1-q_0}.
	\end{equation}
	This  solution is almost everywhere unique with respect to the distribution of $\mbf{x}_t$.
\end{theorem}
Theorem \ref{Thm:D-opt relax} includes Theorem \ref{Thm:D-opt} as a special case  when $q_0=0$. 
% The proof of Theorem \ref{Thm:D-opt relax} can be found in Appendix.
This solution has the following interpretation:  at each time point $t$, with a small probability $q_0$, the sample unit $(\mbf{y}_t;\mbf{x}_t)$ will be selected regardless of the leverage score $\ell_t$ of $\mbf{x}_t$;  with a large probability $1-q_0$,  the hard thresholding of $\ell_t$ is applied to select sample. Such a strategy is also called rejection control in Monte Carlo computing \citep{liu2004monte}.
With a fractional of small-leverage points included, one may perform model diagnostics, e.g., using regression residuals,   to assess the goodness of fit \cite[Chapter 13]{lutkepohl:2005:new} over low-leverage design region.



%\subsection{Combination with Sequential Estimation}
%
%In practice, the choice of sampling rate  $q$ in the LSS and related sampling strategies discussed above will depend on the actual computational constraint. In occasions where the collection of data is costly, it may also be desirable to control the sample size $N$ used for estimation.  For such a purpose, we  propose to consider a combination of the sampling strategy with a sequential estimation procedure. A common paradigm  in  sequential estimation is to keep updating the estimation with new data until attaining certain precision. More specifically, one monitors a proper information quantity $I_N>0$ which grows as $N$ increases, and the termination of the update occurs at a stopping time \[\tau(z)=\inf\{N\ge 1:\ I_N>z\}\] where $z>0$ is a threshold.
%
% For example, let us  consider a combination of the  Leverage Score Sampling (LSS) discussed in Section \ref{Sec:opt linear sys} with the sequential estimation in \cite{lai:1983:fixed} and \cite{galtchouk:2004:uniform}. We shall follow these studies to assume  $\Omega=\Var[\mbf{e}_t]$    as identity matrix for simplicity, whereas estimation  of $\Omega$ is needed in practice. One sets $I_N=\tr(\sum_{i=1}^N \mbf{x}_{\kappa_i}\mbf{x}_{\kappa_i}')  $, where   $\kappa_i=\min\{k\ge 1:\ \sum_{t=1}^k 1_{\{\ell_t>r\}}=i \}$ is the time of the $i$th  sample selection of LSS. Suppose $\wh{\mbf{\beta}}_N$ is the sequential least squares estimator of the regression  coefficient $\mbf{\beta}=\vect(B)$ based on $\{(y_{\kappa_i};\mbf{x}_{\kappa_i}):\ i=1,\ldots,N\}$. Let $M_k=\sum_{i=1}^k  \mbf{x}_{\kappa_i}\mbf{x}_{\kappa_i}$. Then 
%\[
%R(z):=\{\mbf{\beta}:\ (\mbf{\beta}-\wh{\mbf{\beta}}_N)'M_{\tau(z)}(\mbf{\beta}-\wh{\mbf{\beta}}_N)\le \chi_{p,1-\alpha}^2\},
%\]
%is expected to be an approximate  $(1-\alpha)\%$-confidence    region for $\mbf{\beta}$, where $\chi_{p,1-\alpha}^2$ is the $1-\alpha$ quantile for $\chi_p^2$ distribution.  Note that the size of the confidence region $R(z)$    can be quantified by the determinant or the trace of  $M_{\tau(z)}$.   Therefore for   sequentially observed streams,  one may choose the threshold $z$ or equivalently $\tau(z)$ by attaining a prefixed desirable size of $R(z)$.





\section{Auxiliary Estimates, Algorithm and Practical Implementation}\label{Sec:aux}

\subsection{Practical Implementation and Computational Complexity}

The Algorithm \ref{Alg:LSS} summarizes the relaxed-LSS-assisted online estimation procedure involving some of the  ingredients discussed below. 
\smallskip

\noindent\underline{\textbf{Incremental estimation of means}}

In practice, the means $\mbf{\mu}_Y=\E[\mbf{y}_t]$ and $\mbf{\mu}_X=\E[\mbf{x}_t]$ are often not zero and unknown.
Since the computational cost of updating  the means $\mbf{\mu}_X$ and $\mbf{\mu}_Y$ is minimal and having them estimated accurately is important for ensuring the consistency of $\wh{B}_{n,I}$,   we propose to update them  frequently, even at every step.
We suggest starting the LSS-assisted online estimation using initialized means based on a pilot stream sample, and then have them updated incrementally. 
\smallskip


\noindent\underline{\textbf{Real-time calculation of leverage score}}

An effective real-time calculation of leverage score requires less computational effort than updating the least square. In practice, the inverse covariance matrix $\Sigma^{-1}$ is unknown. 
To gain the computational advantage,   a real-time calculation of leverage score approach is used by incorporating a crude estimate of precision matrix.
A crude estimate of $\Sigma^{-1}$ will  affect only the efficiency of the leverage score sampling but not the consistency of $\wh{B}_{n,I}$.
% ,  whereas the cost of its update  is similar to that of updating the least square.  
We hence propose to use pilot estimation $\mathrm{P}_0:=\wh{\Sigma}_{n_0}^{-1}$ based on pilot data of size $n_0$ as a crude estimation of leverage score, i.e.
\begin{equation}
    \label{eq:ell_t}
    	\ell_t \approx  \mbf{x}_t' \mathrm{P}_0 \mbf{x}_t,
\end{equation}
so that we can update the leverage score with one vector-matrix multiplication, which has cost $O(p^2)$ per time point; or, alternatively, we update $\mathrm{P}_0$ sparsely in time, which has total cost $O(cp^2)$ with $c\ll n$, given $n$ as the total length of the observed stream size.
% In experiments we found that LSS with such crude estimate of $\Sigma^{-1}$ readily provides a significant gain in estimation efficiency.
\smallskip


% updated sparsely in time to gain computational advantage since overall, calculating leverage score requires less computational effort than updating the least square.  In particular, we can update the estimate of $\Sigma^{-1}$ with a desirable small update rate $u\in (0,1)$, which corresponds to i.i.d.\ Bernoulli($u$) random variables $(J_t)$ independent of everything else.
% Let $\wh{\mbf{\mu}}_{t,X}$ and $\wh{\mbf{\mu}}_{t,Y}$   be  estimators of $\mbf{\mu}_X$,  $\mbf{\mu}_Y$  based on the stream observed up to time $t$, respectively. Then we can estimate $\Sigma^{-1}$  by
% \begin{equation}\label{sigma inv est}
% \wh{\Sigma}_n^{-1}=\left( \frac{1}{M_n}\sum_{t=1}^n J_t (\mbf{x}_t-\wh{\mbf{\mu}}_{t,X}) (\mbf{x}_t-\wh{\mbf{\mu}}_{t,X})'\right)^{-1},
% \end{equation}
% where $M_n=\sum_{t=1}^n J_t$.
% Given an estimated $\Sigma^{-1}$,  the computation of $\ell_t$ can be made efficient through a Cholesky decomposition.
\noindent\underline{\textbf{Computational Complexity}}

The running time for the LSS-assisted online estimation depends on both the time to calculate the leverage scores and the time to update the model estimation using  sampled data. For an observed stream of size $n$, calculating leverage scores requires total $O(cp^2)$ time, and updating least squares estimation for sampled data requires  $O\left((qn)p^2\right)$ time with sampling rate $q\ll1$. So the total computational complexity of Algorithm \ref{Alg:LSS} is $O\left((qn+c)p^2\right)$, where $(qn+c)\ll n$.
The computational complexity of LSS or relaxed-LSS sampling methods hence is lower than the recursive least squares methods, where the latter needs to update the least squares estimates at every time point resulting in total $O(np^2)$ time. 
For the Bernoulli sampler (i.e., sampling function $s(\mbf{x})\equiv q$), the running time    is trivial. 
To gain a further computational advantage in practice, one may  use an efficient approximate computation of the leverage scores to perform relaxed-LSS algorithm (cf.\ \cite{ma:2015:statistical}).  The   approximation error analysis of the leverage scores can be found in \cite{drineas2012fast} and \cite{gittens2013revisiting}.
\smallskip

\noindent\underline{\textbf{Determination of the threshold} $r$}

Another important practical issue is the determination of the threshold $r$.   If  each $\mbf{x}_t$ is  Gaussian,  it can be shown that the sampling rate
\begin{equation}\label{eq:Q(m,r)}
	pr(\ell_t>r)=pr(\chi_p^2 >r),
\end{equation}
where $\chi_p^2$ denotes a Chi-square distribution with $p$ degrees of freedom. Hence for a predetermined sampling rate $q\in (0,1)$, one can choose $r$ based on \eqref{eq:Q(m,r)}. In reality, data often exhibit heavier-tailed fluctuation compared to Gaussian.  So one may start the sampling using a threshold $r$   determined by \eqref{eq:Q(m,r)}, and then replace it with the empirical quantile of $\ell_t$ observed thus far.
It is also possible to perform a pre-estimation of the tail quantile $r$ based on a pilot sample of $\mbf{x}_t' \Sigma^{-1} \mbf{x}_t$  (with $\Sigma^{-1}$ also pre-estimated). 

\subsection{Auxiliary Estimates}
Below we formulate a general asymptotic result extending Theorem \ref{Thm:asymp_normality} which incorporates   auxiliary estimates.   Suppose the assumptions in Section \ref{Sec:linear sys} hold, but now we do not assume \eqref{eq:centering}. 
 We shall suppose that for some $\alpha>2$,
\begin{align}\label{eq:alpha moment}
	\E\|\mbf{x}_t\|^\alpha<\infty, \quad t\in\bb{Z}_+, 
\end{align}
as well as
\begin{align}\label{eq:mu_X mean sq cons}
	\sup_{t } t^{1/2}  [\E \| \wh{\mbf{\mu}}_{t,X} -\mbf{\mu}_{X}\|^{\alpha}]^{1/\alpha} <\infty,~  \sup_{t } t^{1/2}  [\E \| \wh{\mbf{\mu}}_{t,Y} -\mbf{\mu}_{Y}\|^\alpha]^{1/\alpha} <\infty.
\end{align}       
The latter two relations are  moderate strengthening of the usual root-$n$ consistency.  If $\wh{\mbf{\mu}}_{t,X}$ and $\wh{\mbf{\mu}}_{t,Y}$ are sample means up to time $t$  and $\E \|\mbf{x}_t\|^{\alpha}<\infty$, $\E \|\mbf{y}_t\|^{\alpha}<\infty$, then \eqref{eq:mu_X mean sq cons} holds if a strong mixing condition  holds for $(\mbf{x}_t)$ and $(\mbf{y}_t)$ \citep{yokoyama:1980:moment}, or if they are short-range dependent linear processes \cite[Proposition 4.4.3]{surgailis:2012:large}.




Next, we assume that there is a 
family of sampling functions $s=s_q$ indexed by the sampling rate $q$, which satisfies
\begin{align}\label{eq:sampling fun cond aux}
	\lim_{q\rightarrow 0}\frac{q^{1-2/\alpha}\lambda_{\max}(\Gamma(s))^{1/2}}{\lambda_{\min}(\Gamma(s))}=0,
\end{align}
where $q=\E[s(\mbf{x}_t)]$ is the sampling rate and $\lambda_{\max}(\Gamma(s))$ and $\lambda_{\min}(\Gamma(s))$ stand for the largest and the smallest eigenvalue of $\Gamma(s)$ respectively. Condition \eqref{eq:sampling fun cond aux}, roughly speaking,   ensures that the magnitude of $\Gamma(s)$ decays slower than a power of $q$ as $q\rightarrow 0$. This will be verified for the sampling function corresponding to the relaxed-LSS method in Lemma \ref{Lem:aux} below. 


We   suppose that
$\wh{s}_t\in [0,1]$ is an estimate of the  plugged-in sampling function   $s(\mbf{x}_t-\mbf{\mu}_X)$ based on the data stream observed up to time $t$ such that as $t\rightarrow\infty$ ($q$ fixed),
\begin{align}\label{eq:s consistent}
	\wh{s}_t- s(\mbf{x}_t-\mbf{\mu}_X) \ConvP 0.
\end{align}
This consistency condition will also be verified in Lemma \ref{Lem:aux} below for the auxiliary estimates involved in the relaxed-LSS method. 
\begin{theorem}\label{Thm:asymp_aux} 
	Suppose that the conditions  \eqref{eq:alpha moment}, \eqref{eq:mu_X mean sq cons}, \eqref{eq:sampling fun cond aux} and \eqref{eq:s consistent} hold. Let the   estimator of $B$ based on stream up to time $n$ be as 
	\[
	\wh{B}_{n,s}=\left( \sum_{t=1}^n  \wt{\mbf{x}}_t   \wt{\mbf{x}}_t'   1_{\{U_t\le   \wh{s}_t \}} \right)^{-1} \left( \sum_{t=1}^n \wt{\mbf{x}}_t \wt{\mbf{y}}_t'    1_{\{U_t\le   \wh{s}_t \}} \right),
	\]
	where $U_t$'s are as in Definition \ref{Def:S},  and $\wt{\mbf{x}}_t=\mbf{x}_t-\wh{\mbf{\mu}}_{t,X},\quad\wt{\mbf{y}}_t=\mbf{y}_t-\wh{\mbf{\mu}}_{t,Y}.$  Then  we have the decomposition
	\begin{equation}\label{eq:asymp norm lev aux} 
		\sqrt{N}( \wh{B}_{n,s}   -  B)  =  M_n+R_n,
	\end{equation}
	where as  the total stream size $n\rightarrow\infty$,
	\begin{equation}\label{eq:asymp norm M_n}
		\vect(M_n) \ConvD \cl{N}(\mbf{0},  P(s)^{-1} )
	\end{equation}
	as $n\rightarrow\infty$ with    $P(s)$   as in \eqref{eq:P(s)} but with $\Gamma(s)$ in \eqref{eq:Gamma(s)} redefined as
	\begin{equation}\label{eq:Gamma(s) center}
		\Gamma(s)=\E\left[ s(\mbf{x}_t-\mbf{\mu}_X) (\mbf{x}_t-\mbf{\mu}_X) (\mbf{x}_t-\mbf{\mu}_X)' \right],
	\end{equation} 
	which we assume to be non-singular.
	The term $R_n$ satisfies for any $\delta>0$,
	\begin{equation}\label{eq:R_n neg}
		\lim_{q\rightarrow 0} \limsup_n pr(\|P(s)^{1/2}\vect(R_n)\|>\delta)= 0.
	\end{equation}
\end{theorem} 


The double limit in \eqref{eq:R_n neg} says that when  the sampling rate is small, as typically desired in practice, the term $R_n$ is  negligible compared to $M_n$.  Note that the same double limit with $R_n$ replaced by $M_n$ will not be zero due to \eqref{eq:asymp norm M_n}. 

% The proof of Theorem \ref{Thm:asymp_aux} can be found in Appendix. 


\begin{algorithm} 
\caption{Relaxed-LSS-Assisted Online Estimation of Stationary Linear System}\label{Alg:LSS}
			% \SetAlgoLined
	\textbf{Initialization}:\\
	Choose a sampling rate $q\in (0,1)$\;
	Choose a base sampling rate $q_0<q$\;
% 	Choose a update rate $u\in (0,1)$ for $\Sigma^{-1}$\;
	Initialize the estimates of $\mbf{\mu}_X$, $\mbf{\mu}_Y$, $\Sigma^{-1}$ and $B$ based on a   pilot sample\;	
	\smallskip	
	\textbf{Online Estimation}:\\ 
    \While{New sample $(\mbf{y}_t;\mbf{x}_t)$ at time $t$ arrives}{		
		Update     $\mbf{\mu}_X$ and $\mbf{\mu}_Y$\;		
% 		With a small probability $u$, update the estimate of $\Sigma^{-1}$ with   new centered covariate point $\mbf{x}_t-\mbf{\mu}_X$;		
		Calculate $\ell_t$ based on $\mathrm{P}_0$ in \eqref{eq:ell_t}\;
  \eIf{Bernoulli($q_0$) random number = 1}{
    Update the  estimates  of $B$ and $\Omega$  with new centered sample  $(\mbf{y}_t-\mbf{\mu}_Y;\mbf{x}_t-\mbf{\mu}_X)$\;
  }{
      \If{$\ell_t > r$  }{
      Update the  estimates  of $B$ and $\Omega$  with new centered sample  $(\mbf{y}_t-\mbf{\mu}_Y;\mbf{x}_t-\mbf{\mu}_X)$\;
      }
  }
  Update $r$ based on \eqref{r_estimate} (or use \eqref{eq:Q(m,r)} when the sample size is small). 
}	%end while
\end{algorithm}

The following lemma shows that the conditions \eqref{eq:sampling fun cond aux} and \eqref{eq:s consistent} are satisfied in the context of relaxed-LSS, and hence justify   the procedure  in Algorithm~\ref{Alg:LSS}. 
\begin{lemma}\label{Lem:aux}
	~
	\begin{enumerate}[(a)]
		\item Suppose $s(\mbf{x})=s_q(\mbf{x})=q_0+(1-q_0) 1_{\{\mbf{x}'\Sigma^{-1}\mbf{x}>r\}}$ ($q_0$ and $r$ depend on $q$) is the sampling function of the relaxed-LSS  as in Theorem \ref{Thm:D-opt relax}, where $q_0=q_0(q)$ satisfies that  for some constant  $c\in (0,1)$  
		\begin{equation}\label{eq:q_0 restr}
			q_0\le c q.
		\end{equation}
		When $\alpha\in (2,4]$,  assume in addition  that for some constant $c>0$ and $\beta\in (\alpha, 4\alpha/(4-\alpha) )$ (right boundary understood as $+\infty$ when $\alpha=4$), we have
		\begin{align}\label{eq:tail lower bound}
			\nu(x,\infty)> c x^{-\beta}
		\end{align}
		for all sufficient large $x$, where $\nu$ is as in \eqref{eq:x EC}.    Then the condition \eqref{eq:sampling fun cond aux} holds.
		\item 
		Suppose that $\wh{\Sigma}_n^{-1}$ is a consistent estimate of $\Sigma^{-1}$ based on the stream observed up to time $n$, which can be realized by
		\begin{equation}\label{sigma inv est}
\wh{\Sigma}_n^{-1}=\left( \frac{1}{M_n}\sum_{t=1}^n J_t (\mbf{x}_t-\wh{\mbf{\mu}}_{t,X}) (\mbf{x}_t-\wh{\mbf{\mu}}_{t,X})'\right)^{-1},
\end{equation}
with a desirable small update rate $u\in (0,1)$, which corresponds to i.i.d.\ Bernoulli($u$) random variables $(J_t)$ independent of everything else, where  $\wh{\mbf{\mu}}_{t,X}$ and $\wh{\mbf{\mu}}_{t,Y}$   be  estimators of $\mbf{\mu}_X$,  $\mbf{\mu}_Y$  based on the stream observed up to time $t$, respectively.

		Define the leverage score incorporating the auxiliary estimates as
		\begin{align}\label{eq:tilde ell}
			\wt{\ell}_t=\wt{\mbf{x}}_t'  \wh{\Sigma}_t^{-1}  \wt{\mbf{x}}_t.
		\end{align}
			Let $q_0$ be the base sampling rate for relaxed-LSS in Section \ref{Sec:relax}.
		Suppose that
		$\wh{r}_n$ is a consistent estimate of 
		\begin{equation}
		r(q,q_0):= \inf\left\{r\ge 0:\  pr((\mbf{x}_t-\mbf{\mu}_{X})'\Sigma^{-1}(\mbf{x}_t-\mbf{\mu}_{X})\le r) \ge  \frac{q-q_0}{1-q_0}\right\}
		\end{equation}  based on the stream observed up to time $n$, which can be realized by  
\begin{equation}\label{r_estimate}
\wh{r}_n=\inf \left\{r\ge 0: \frac{1}{n}\sum_{t=1}^n 1_{\{\wt{\ell}_t\le r\}}\ge \frac{q-q_0}{1-q_0}\right\}.
\end{equation}  Let the estimated plugged-in sample function be
		\begin{align}\label{eq:s hat t}
			\wh{s}_t = q_0+(1-q_0)1_{\{ \wt{\ell}_t> \wh{r}_t\}}.
		\end{align}
	 Then the condition \eqref{eq:s consistent} holds. 
	\end{enumerate}
\end{lemma}
% The proof of the lemma is in Appendix. 
Examining the proof reveals that   the condition \eqref{eq:q_0 restr} can be   relaxed by allowing a power of $q$. We omit such a generalization  here for simplicity. We also note that the assumption \eqref{eq:tail lower bound} is not stringent. In particular, suppose the tail probability $\nu(x,\infty)$ is regularly varying  with index $-\gamma$ (see \cite{bingham:1989:regular}, which roughly speaking,  says $\nu(x,\infty)$ behaves like $x^{-\gamma}$)  as     $x\rightarrow \infty$, $\gamma>2$. This regular variation assumption includes   common heavy-tailed distributions    such as Pareto distributions and $t$-distributions. Then   in view of Potter's bound \cite[Theorem 1.5.6]{bingham:1989:regular}, one can find $\alpha$ and $\beta$ satisfying $2<\alpha<\gamma<\beta<4\alpha/(4-\alpha)$  so that  the conditions  \eqref{eq:alpha moment} and \eqref{eq:tail lower bound} both hold.





\section{The Open Power System Data Application}\label{Sec:data}

% insert figure {fig:surface} here.

In this section, we apply our LSS and relaxed-LSS methods and benchmark Bernoulli method to the~\cite{realdata} for real-time inference on the dynamics of power grid load profiles and one-step ahead  prediction. 
We compare the proposed LSS-based methods to the benchmark sampling method and ``full sample'' estimation and demonstrate the strengths of real-time estimation and prediction of the proposed methods.
% we evaluate the performance of our LSS and relaxed-LSS methods and benchmark Bernoulli method on the real dataset from the~\cite{realdata}. 

\subsection{Open Power System Data}
Geographically, the Open Power System Data consist $37$ European countries that cover European Union and neighboring countries. The data measures the total load (in 
Terawatt hour, TWh) for a country, control area or bidding zone. The total load is a power statistic, which is defined as, roughly speaking, the total power generated or imported minus the power being consumed at power plant, stored or exported. More specifically, the data reported by ENTSO-E Transparency Platform are used due to its high efficiency in data reporting, which results in a subset of 19 countries. 
 The selected multivariate time series streams are recorded from 2006-01-01, 00:00:00 Coordinated Universal Time (UTC) to 2018-12-14, 23:00:00 UTC.
% Electricity power consumption data from different countries are reported through different platforms in the Open Power System Data. 
% We use the actual load of ENTSO-E power statistics from $19$ countries as the variables of interest. 
% Figure~\ref{fig:surface} displays the dynamic evolution of the load curves from 2006 to 2018 for those $19$ countries.
There are total $113,544$ time points been observed in the $19$-dimensional electricity load stream, which are complete without missing values.


\begin{figure}[h]
	\begin{center}
		\includegraphics[width=4.5in]{figures/Coefficient_RMSE_color-eps-converted-to.pdf}
		\vspace{-0.5cm}
		\caption{Comparison of estimation errors for each of the time points of power load data.}\label{fig:realdata_estimation}
	\end{center}
\end{figure}


\subsection{Seasonal VARX modeling and accuracy measurements}
Electricity loads exhibit strong seasonality compared to loads one day (24 hours) earlier since it was aggregated to hourly temporal resolution.  
We consider the seasonal vector autoregression model with centering:
\begin{equation}\label{eq:sVARX}
	\mbf{y}_t  = \sum_{i=1}^{p_1}\Phi_i \mbf{y}_{t-i}  +\sum_{i=1}^{p_2}\Theta_i \mbf{y}_{t-24i} + \mbf{e}_t, 
\end{equation} 
where we choose the model order as $p_1=2$, $p_2=1$ incorporating the daily seasonality. The weather, electricity prices, and clean energy generation and capacities can be added in to this seasonal VARX model as exogenous variables. However, due to the large portion and complicated missing pattern in the database, we did not include them in the real-time analysis. Our goals are to estimate the model parameters $\Phi's$ and $\Theta's$ in real time, which represent the dynamic dependence of the electricity loads, and the one-hour ahead forecasting (prediction) of electricity loads, which is crucial for effective scheduling and energy management in power grids. 

We denote the $\hat{B}_{t}$ as the real time estimation of model parameter matrix $B$ at time point $t$, and $\hat{B}_{\text{Full}}$ is the offline model parameter estimation based on the entire dataset as a substitute of the unknown population $B$. We use the relative errors to measure the parameter matrix estimation accuracy $	|| \hat{B}_{t} -\hat{B}_{\text{Full}}||_{F}/|| \hat{B}_{\text{Full}}||_{F}$.
% \begin{equation}
% $\frac{	|| \hat{B}_{t} -\hat{B}_{\text{Full}}||_{F}}{|| \hat{B}_{\text{Full}}||_{F}}.$
% \end{equation}

At time point $t$, if the sample unit $(\mbf{y}_t;\mbf{x}_t)$ is sampled, we will update the estimation $\hat{B}_{t}$ using least squares; otherwise the estimation from previous time point will be retained as the current estimation. The results are demonstrated in Figure~\ref{fig:realdata_estimation}, where the x-axis represents the time of updates.



For the pointwise prediction accuracy, we compute the one-step (one-hour) ahead relative prediction errors for each of the time points as:
$||\hat{\mbf y}_{t+1} - \mbf{y}_{t+1} ||/||\mbf{y}_{t+1} ||$.
% \begin{equation}
% $\frac{||\hat{\mbf y}_{t+1} - \mbf{y}_{t+1} ||}{||\mbf{y}_{t+1} ||}.$
% \end{equation}
We visualized the prediction errors in Figure~\ref{fig:realdata_prediction}, where x-axis represents hourly time.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Phi1_opt.png}
         % \caption{$y=x$}
         % \label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Phi2_opt.png}
         % \caption{$y=3sinx$}
         % \label{fig:three sin x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/Theta_opt.png}
         % \caption{$y=5/x$}
         % \label{fig:five over x}
     \end{subfigure}
        \caption{Visualization of the estimated coefficient matrices $\Phi_1$, $\Phi_2$ and $\Theta_1$ at time $t=113,424$ using relaxed-LSS method.}
        \label{fig:three graphs}
\end{figure}


\subsection{Results}
We compare our LSS and relaxed-LSS methods to the benchmark Bernoulli sampling method for parameter matrix estimation accuracy and prediction accuracy. 
For all three methods, we use the same pilot data of size $500$ to estimate the model order and initial values for model parameter $B$ and precision matrix $\mathrm{P}_0$.
The sampling rate is $q =0.05$ for all three methods, and the base sampling rate for relaxed-LSS is $q_0 =0.025$. The update rate for the inverse covariance matrix is $0.025$. We handle the data in a streaming fashion and take data samples and estimate the model in real time. 
% We use the pilot data to determine the model order (***if want to mention this, should say which method***) and provide the preliminary estimations. 






% Figures~\ref{fig:realdata_estimation} and~\ref{fig:realdata_prediction} display the estimation errors and prediction errors against the time for the power load data, respectively. 

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=5.0in]{figures/prediction_RMSE_color-eps-converted-to.pdf}
			\vspace{-0.5cm}
		\caption{Comparison of one-step ahead prediction errors for each of the time point of power load data.}\label{fig:realdata_prediction}
	\end{center}
		\vspace{-0.5cm}
\end{figure}

As an illustration, Figure~\ref{fig:three graphs} depicts the estimated coefficient matrices $\Phi_1$, $\Phi_2$ and $\Theta_1$ at time $t = 113,424$, which is the last update, using relaxed-LSS method. 
The cross-correlations at lag 1 (coefficient matrix $\Phi_1$) between Italy and Portugal, Austria and France, as well as Italy and Spain are higher than others, which reflect their positive interdependence in electricity consumption.  
For the cross-correlations at lag 2 (coefficient matrix $\Phi_2$), Austria and several countries are negatively correlated, and  Poland and several countries are positively correlated with magnitudes  higher than others. We also notice that Germany has higher autocorrelation at lag 1 (positively)   and lag 2 (negatively). France, Bosnia Herzegovina, Croatia and Macedonia have higher daily-seasonal  correlation than other countries. It is also worth noting that those auto and cross correlations are dynamically evolving so that the Figure~\ref{fig:three graphs} is just a snapshot of the relationships at the time point. The computational cost of online estimation of those model coefficient matrices  could be higher if other variables of the power grid system are included in the model, such as electricity prices, wind and solar power generation and capacities. Those additional variables are available at different temporal resolutions or time frames on Open Power System Data platform. The need for real-time inference under complicated models and high computing costs reinforce the necessity of data reduction methods in analyzing IoT sensor streams. 


In terms of estimation accuracy, LSS and relaxed-LSS methods behave similarly and outperform the Bernoulli sampling most of the time during updates. Overall, estimation errors for all three methods are decreasing along the update time, which suggests that the online estimates converge to the ``full sample'' estimate. However, in practice, one cannot afford to wait for the ``full sample'' due to the need for real-time monitoring in the power grid. Thus, given limited access to the data under computational constraint,  the faster it converges to the ``full sample'' estimate the better.  
Especially, given the same initial values, LSS and relaxed-LSS methods achieve better estimation accuracy at the early stage of updates than those of the Bernoulli method.   Eventually, LSS, relaxed-LSS and even Bernoulli sampling estimates are close to the ``full sample'' estimate since a large enough amount of samples are used for updating in all three sampling methods.

It is worth mentioning that there are a few sudden increases of estimation errors shortly after time at $1000$. We believe they are caused by the abnormal points observed in dimension 12 Luxembourg on 2011-03-27 at 22:00:00 UTC and in dimension 13 Macedonia on 2010-03-28 at 01:00:00 UTC. Those abnormal points were high leverage score points and thus were sampled by the LSS and relaxed-RSS methods. The corresponding model estimates deviated from the ``full sample'' estimates, which lead to sudden increases in estimation errors but were soon corrected by new data points. This phenomenon reflects the advantage of LSS-based sampling methods in capturing the influential or abnormal data points during stream monitoring. It is an important feature that LSS can be applied in the online monitoring of the dynamic dependence of the power grid system for security or online decision-making purposes. However, the phenomenon also reflects the limitation of the leverage based sampling method in lacking of robustness. The online estimation based on leverage score sampling is sensitive to the changes in underlying data generating processes or outliers. 

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=5.0in]{figures/Relaxed_Lev_dim_8_zoom_color-eps-converted-to.pdf}
			\vspace{-0.5cm}
		\caption{Residual plot  from regression fits for France, which is calculated based on the final estimate of relaxed-LSS along all historical data. A filled circle  denotes a point selected by the leverage score thresholding rule with the size of the filled circle indicating the leverage score, and a triangle point indicates a   point selected by the Bernoulli sampling rule.}\label{fig:realdata_diagnosis}
	\end{center}
	\vspace{-0.5cm}
\end{figure}

In addition to theoretically verified properties of LSS-based methods in parameter estimation, we also consider the prediction as a practical performance measurement. The accurate prediction of the electricity loads is crucial for power grid system in resource allocation and demand management. Prediction is also an important task for general IoT sensor data stream analysis due to similar reasons.
For prediction accuracy, the relaxed-LSS method consistently achieves the smallest prediction errors among the three compared methods; while Bernoulli method always delivers the largest prediction errors. The superiority of relaxed-LSS over LSS in prediction may be due to its inclusion of low-leverage covariate points. We also note the two peaks in prediction errors before and after time 30000, which are also caused by the abnormal points in Macedonia and Luxembourg.





Lastly, to examine the goodness of model fitting with relaxed-LSS sampling, we display the regression residuals from a segment of a selected sample of size $500$ from France in Figure~\ref{fig:realdata_diagnosis}, which are calculated based on the final estimate of relaxed-LSS on all historical data. In Figure~\ref{fig:realdata_diagnosis}, a filled circle denotes a point selected by the leverage score thresholding rule with the size of the filled circle indicating the leverage score, and a triangle indicates the low leverage score point. The residual plot shows that the linear relation proposed in the relaxed-LSS sampler holds equally well for both the points selected by a high leverage score and by {Bernoulli} sampling. 

\section{Simulation Studies for general applicability}\label{Sec:simulate} 
% \subsection{Simulation Set-up}
Although the proposed relaxed-LSS demonstrates better performance in inference and prediction on Open Power System Data, we also conduct simulation studies to 
evaluate the effectiveness of LSS and its relaxed version in general settings. We compare the LSS, relaxed-LSS and the Bernoulli sampling method, which corresponds to $s(\mbf{x})\equiv q$ in Definition \ref{Def:S}. We generate the multi-dimensional time series which follows the VARX model defined in~\eqref{eq:VARX} with $K=10$, $p_1=1$, and $p_2=1$. We consider the multivariate {\qcr Gaussian} and the multivariate {\qcr StudentT} distributions for noise process $({\mbf e}_t)$ and extraneous process $({\mbf v}_t)$.

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=4.5in]{figures/AR_coef_cov_matrix_color-eps-converted-to.pdf}
		\caption{Visualization of the coefficient matrices $\Phi$ and $\Psi$, and covariance matrix for the error process $\Omega$.}\label{Fig:AR_matrix}
	\end{center}
\end{figure}

\subsection{Multivariate Gaussian Distribution Case}\label{sim:Gaussian}

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=4.5in]{figures/Gaussian_RMSE_color.png}
		\caption{Comparison of estimation errors for simulated $10$ dimensional time series under {\qcr Gaussian} setting. Visualized for the first $2,500$ updates with underlying time series of length $25,000$. The average and one standard deviation error bar of the estimation errors are plotted over $1,000$ independent replicates.}\label{Fig:Gaussian}
	\end{center}
\end{figure}


We first consider the {\qcr Gaussian} case where the noise process $({\mbf e}_t)$ follows the i.i.d. multivariate Gaussian distribution $N({\mbf 0},\Omega)$ and the extraneous process $({\mbf v}_t)$ follows i.i.d. multivariate Gaussian distribution $N({\mbf 0},I_{Kp_2})$, where $I_{Kp_2} \in \bb R^{Kp_2}$ is the identity matrix.  We follow~\cite{qiu:2015:robust} to generate the coefficient matrices $\Phi$ and $\Psi$, and the covariance matrix for the error process $\Omega$, see Figure~\ref{Fig:AR_matrix} for visualization.

We generate the $10$-dimensional time series of length $n = 25,000$.
% for both {\qcr Gaussian} and {\qcr StudentT} Settings.
The sampling rate is $q =0.1$ and base sampling rate for relaxed-LSS is $q_0 =0.05$. The update rate for the inverse covariance matrix is $ 0.1$. A pilot sample of size $100$ is used to calculate initial estimations. We define the estimation error as $||\hat{B}_{\tau} -B||_{F}/||B||_{F}$,
% \begin{equation}
% 	 \frac{||\hat{B}_{\tau} -B||_{F}}{||B||_{F}},
% \end{equation}
where $\hat{B}_{\tau}$ is the $\tau$th update of the estimation of model parameter matrix $B$ (here we use $\tau$ to denote the number of updates to distinguish from the notation of time point $t$) and $||\cdot||_F$ denotes the Frobenius norm. 


% Figure~\ref{Fig:Gaussian} presents the estimation errors against number of estimate updates in {\qcr Gaussian} setting.
% The results are based on $1000$ independent replicates.

Figure~\ref{Fig:Gaussian} displays the average and one standard deviation error bar of the estimation errors at each update for each of the compared methods over $1,000$ independent replicates. 
Focusing on the first $2,500$ updates for each of the compared methods, we observe that LSS method achieve smallest estimation errors compared to relaxed-LSS and Bernoulli methods as predicted by our optimality theory. We note that the estimation errors for both LSS and relaxed-LSS are significantly smaller than those of Bernoulli method.
% we set $K=10$ and

% insert figure

%  so that the $||\Phi|| = 0.8$ and $||\Psi|| = 0.7$. Then we generate $\mbf\Sigma_y$, $\mbf\Sigma_v$ so that $||\mbf \Sigma_y|| = 2 || \Phi||$, and set $\mbf\Omega = \mbf \Sigma_y - \Phi \mbf \Sigma_y \Phi'$.

\subsection{Multivariate Student T Distribution Case}\label{sim:StudentT}

\begin{figure}[ht!]
	\begin{center}
		\includegraphics[width=4.5in]{figures/StudentT_color_RMSE.png}
		\caption{Comparison of estimation errors for simulated $10$ dimensional time series under {\qcr StudentT} setting. Visualized for the first $2,500$ updates with underlying time series of length $25,000$. The average and one standard deviation error bar of the estimation errors are plotted over $1,000$ independent replicates.}\label{Fig:StudentT}
	\end{center}
\end{figure}

We next consider the {\qcr StudentT} case.
% Similar to the {\qcr Gaussian} case, we generate the multi-dimensional time series that follows the VARX model defined in~\eqref{eq:VARX} with $K=10$, $p_1=1$, and $p_2=1$. 
The noise process $({\mbf e}_t)$ follows the i.i.d. multivariate student $t$ distribution with location $\bf 0$, degree of freedom $3$ and scale matrix $\Omega$, and the extraneous process $({\mbf v}_t)$ follows i.i.d. multivariate student $t$ distribution with location $\bf 0$, degree of freedom $3$ and scale matrix $I_{Kp_2}$.

We generate the time series following~\cite{remillard2012copula,qiu:2015:robust}.
%  following distribution for $(\mbf{y}_1',\mbf{e}_2',\ldots,\mbf{e}_t')'$:
%  multivariate student t distribution with location $\bf 0$, degree of freedom $3$ and scale matrix $ \mbf{\Sigma}$, where the extraneous variables $(\mbf{v}_1',\ldots,\mbf{v}_{t-1}')'$ are generated from a multivariate student t distribution with location $\bf 0$, scale matrix $\mbf{\Sigma}_v$ with $(i,j)$th entry $0.3\times 0.5^{|i-j|}$, and degree of freedom $3$.
% \begin{itemize}
%     % \item {\qcr Gaussian}: a multivariate Gaussian distribution $N(\mbf 0, \mbf{\Sigma})$, where the extraneous variables $(\mbf{v}_1',\ldots,\mbf{v}_{t-1}')'$ are generated from a multivariate Gaussian distribution with mean $\bf 0$ and covariance matrix $\mbf{\Sigma}_v = \text{diag}(K)$;
%     \item {\qcr StudentT}: a multivariate student t distribution with location $\bf 0$, degree of freedom $3$ and scale matrix $ \mbf{\Sigma}$, where the extraneous variables $(\mbf{v}_1',\ldots,\mbf{v}_{t-1}')'$ are generated from a multivariate student t distribution with location $\bf 0$, scale matrix $\mbf{\Sigma}_v$ with $(i,j)$th entry $0.3\times 0.5^{|i-j|}$, and degree of freedom $3$.
% \end{itemize}
Particularly, we first simulate an initial observation $\mbf{y}_1$, extraneous variables $\mbf{v}_1,\ldots,\mbf{v}_{t-1}$, and innovations $\mbf{e}_2,\ldots,\mbf{e}_t$. 
% We note that the covariance matrix of the quantity $(\mbf{y}_1',\mbf{e}_2',\ldots,\mbf{e}_t')'$ is block diagonal, i.e. $\mbf{\Sigma} = \text{diag}(\mbf{\Sigma}_y,\mbf\Omega,\ldots,  \mbf\Omega)$.
After generating  $(\mbf{y}_1',\mbf{e}_2',\ldots,\mbf{e}_t')'$ and $(\mbf{v}_1',\ldots,\mbf{v}_{t-1}')'$, one can form $(\mbf{y}_1',\mbf{y}_2',\ldots,\mbf{y}_t')'$ recursively following the iterative algorithm in \cite{remillard2012copula}. The model coefficient matrices $\Phi$ and $\Psi$, and the covariance matrix $\Omega$ are the same as the {\qcr Gaussian} case. 
% \begin{equation} \label{simu: eq1}
%     (\mbf{y}_1',\mbf{y}_2',\ldots,\mbf{y}_n')' = {\mbf G}(\mbf{y}_1',\mbf{e}_2',\ldots,\mbf{e}_n',\mbf{v}_1',\ldots,\mbf{v}_{n-1}')',
% \end{equation}
% where G {\red need to be specified.}
% \begin{equation*}
%     \mbf G := \left(\begin{array}{ccccccccc}
% \mathbf{I} & \mathbf{0} & \mathbf{0} & \cdots & \mathbf{0} & \mathbf{0} & \mathbf{0} & \cdots & \mathbf{0}\\
% {\Phi} & \mathbf{I} & \mathbf{0} & \cdots & \mathbf{0} &{\Psi} & \mathbf{0} & \cdots &  \mathbf{0} \\
% {\Phi}^{2} & {\Phi} & \mathbf{I} & \cdots & \mathbf{0} &\mathbf{0}&\Psi&\cdots&\mathbf{0}\\
% & & &\cdots & & \\
% {\Phi}^{n-1} & {\Phi}^{n-2} & {\Phi}^{n-3} & \cdots & \mathbf{I} & \mathbf{0}&\mathbf{0}&\cdots& \Psi
% \end{array}\right).
% \end{equation*}
% Specifically, we set $K=10$ and follow~\cite{han2013transition} to generate the coefficient matrices $\Phi$ and $\Psi$ so that the $||\Phi|| = 0.8$ and $||\Psi|| = 0.7$. Then we generate $\mbf\Sigma_y$, $\mbf\Sigma_v$ so that $||\mbf \Sigma_y|| = 2 || \Phi||$, and set $\mbf\Omega = \mbf \Sigma_y - \Phi \mbf \Sigma_y \Phi'$. 
We generate the multi-dimensional time series of length $t = 25,000$ and set the sampling rate $q =0.1$, the base sampling rate $q_0 =0.05$ and the update rate for the inverse covariance matrix is $ 0.1$. The pilot sample size is $100$.

Figure~\ref{Fig:StudentT} presents the average and one standard deviation error bar of estimation errors against number of estimate updates, which are based on $1,000$ independent replicates.
% \begin{equation}
%     || \hat{B}_{\tau} -B||_{F},
% \end{equation}
% where $\hat{B}_{\tau}$ is the $\tau$th update of the estimation of model parameter matrix $B$. 
% We use the Kalman filter to estimate and update the model parameter matrix $B$
% \begin{eqnarray}
%   P_{\tau} = P_{\tau-1} -  \mbf k_{\tau}  \mbf{x}_{\tau}'  P_{\tau-1}  \label{pt_local}\\
%   \hat{B}_{\tau}=  \hat{B}_{\tau-1} +  \mbf k_{\tau_j}[\mbf y_{\tau}'  - \mbf{x}_{\tau}'\hat{B}_{\tau-1} ], \label{beta_local}
% \end{eqnarray}
% where $ \mbf k_{\tau} =  \Gamma_{\tau} ^{-1}  P_{\tau-1} \mbf{x}_{\tau} $, and $\Gamma_{\tau}  = I + \mbf{x}'_{\tau} P_{\tau-1}  \mbf{x}_{\tau} $ with $P_{\tau_j}$ as the estimate of the precision matrix at time $\tau$.
% Figures~\ref{Fig:Gaussian} and \ref{Fig:StudentT} display the mean and one standard deviation error bar of the estimation errors at each update for each of the compared method over $1000$ independent replicates. 
Similar to the {\qcr Gaussian} case, the LSS method achieves the best performance as predicted by our optimality theory; the LSS and relaxed-LSS methods are comparable;   both methods are significantly better than the Bernoulli method in terms of estimation errors, where the wider difference compared to the Gaussian case is due to the fact that the $t$-distribution has a heavier tail and hence generates a larger leverage effect.



\section{Conclusion}\label{con}
We introduced a class of online sample selection (sampling) methods for large scale streaming time series with application in the online analysis of electricity power grid data. We provide a solution to online statistical inference of high speed multidimensional time series streams under computational constraint.  The proposed methods were motivated by optimal designs in design of experiments and were applied to the high temporal resolution data streams in power grid system as an example of IoT sensor network data stream analysis. 
The proposed methods were based on a relaxed version of leverage score sampling and achieved an optimality criterion. Therefore, the proposed methods enjoyed the optimality in online sampling theoretically and improved the computational efficiency of the online analysis.
The elliptical distributed synthetic data and electricity consumption real data analysis demonstrated the effectiveness of the proposed sampling methods.  
Our proposed relaxed-LSS method provides online analysis of electricity loads through data selection without loss of identification of electricity consumption patterns and flexibility.
Our work was based on the stationary linear multivariate time series models for streaming data modeling, which serves as a foundation for tackling the more involved non-stationary case. We shall leave the study of sampling of non-stationary data to future work. 

%\section*{
%Acknowledgments} We thank the Editor, Associate Editor, and two anonymous reviewers for many valuable comments and suggestions.

\begin{acks}[Acknowledgments]
We thank the Editor, Associate Editor, and two anonymous reviewers for many valuable comments and suggestions.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Funding information, if any,             %%
%% should be provided in the                %%
%% funding section.                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{funding}

This work is partially supported by NIH awards R01MD018025, R03AG069799,  NSF awards DMS-1903226, DMS-1925066 and DMS-2124493.

\end{funding}

% {\color{red}add application high-frequency load curves}.

% {\color{red}  }
% \section{BibTeX}

% We hope you've chosen to use BibTeX!\ If you have, please feel free to use the package natbib with any bibliography style you're comfortable with. The .bst file agsm has been included here for your convenience. 
% \clearpage


\bibliographystyle{chicago}
\bibliography{Bib}


% \newpage
% % \bigskip
% % \begin{center}
% % {\large\bf SUPPLEMENTARY MATERIAL}
% % \end{center}

%  \pagenumbering{arabic} 
%  \renewcommand*{\thepage}{A\arabic{page}}
% \appendix

% % \appendixone
% \section{Appendix: proofs}\label{Sec:Proofs}

% \subsection{Proof of Theorem \ref{Thm:asymp_normality}}
% \begin{proof}
% 	Let $\mbf{z}_t=1\{U_t\le s(\mbf{x}_t)\}\mbf{x}_t$.
% 	By   equation (\ref{eq:stat sys}), we have
% 	\begin{equation}\label{eq:root}
% 		\sqrt{n}(\wh{B}_{n,I} -B)= \left(\frac{1}{n}\sum_{t=1}^n  \mbf{z}_t  \mbf{z}_t'\right)^{-1} \left(\frac{1}{\sqrt{n}}\sum_{t=1}^n \mbf{z}_t \mbf{e}_t'  \right),
% 	\end{equation}
% 	which is understood as zero matrix (or any arbitrary fixed matrix) if the invertibility fails.   Using \eqref{eq:e_t cov}, we have
% 	\begin{equation}\label{eq:cov}
% 		\E[ \vect(\mbf{z}_t\mbf{e}_t')\vect(\mbf{z}_t\mbf{e}_t')']= \E[ \E(\vect(\mbf{z}_t\mbf{e}_t')\vect(\mbf{z}_t\mbf{e}_t')' |\cl{F}_t]=  \Omega\otimes \Gamma(s).
% 	\end{equation}
% 	By   the ergodic theorem \cite[Theorem 10.6]{kallenberg:2002:foundations}   applied to each entry, one has almost surely as $n\rightarrow\infty$ that
% 	\begin{equation}\label{eq:den}
% 		\frac{1}{n}\sum_{i=1}^n  \mbf{z}_t  \mbf{z}_t' \rightarrow  \Gamma(s).
% 	\end{equation}
% 	For any column vector  $\mbf{a}\in \mathbb{R}^{Kp}$, the linear combination   $\mbf{a}' \vect( \mbf{z}_t\mbf{e}_t')$ forms a stationary martingale difference in $t$ with respect to the filtration $\cl{F}_t$  in view of \eqref{eq:md} and the joint stationarity of $(\mbf{x}_t;\mbf{e}_t)$. By (\ref{eq:cov}), \eqref{eq:den} and the martingale central limit theorem \cite[Theorem 35.12]{billingsley:1995:probability}, as $n\rightarrow\infty$,
% 	\[ 
% 	\frac{1}{\sqrt{n}}\sum_{t=1}^n \mbf{a}' \vect( \mbf{z}_t\mbf{e}_t') \ConvD N(0,   \mbf{a}' \Omega\otimes \Gamma(s) \mbf{a}),
% 	\]
% 	where  the Lindeberg   condition \cite[Eq.(36.36)]{billingsley:1995:probability} follows from  \eqref{eq:e_t cov} and stationarity (see also the proof of Theorem \ref{Thm:asymp_aux} below).
% 	In view of the Cram\'er-Wold device, we have thus shown that as $n\rightarrow\infty$,
% 	\begin{equation}\label{eq:num}
% 		\frac{1}{\sqrt{n}}\sum_{i=1}^n  \vect( \mbf{z}_t\mbf{e}_t') \ConvD N(\mbf{0}, \Omega\otimes \Gamma(s)).
% 	\end{equation}
% 	Next, notice that the  invertible matrices of a fixed size form an open subset  under the product topology. Hence  $\frac{1}{n}\sum_{i=1}^n  \mbf{z}_t  \mbf{z}_t'$ is invertible with probability tending to one as $n\rightarrow\infty$.
% 	Combining \eqref{eq:N/n}, (\ref{eq:root}),  (\ref{eq:den}), (\ref{eq:num}) and Slutsky's lemma yields the conclusion.
% \end{proof}

% \subsection{Proof of Theorem \ref{Thm:D-opt relax}}
% \begin{proof}
% 	Let $\phi_\Sigma$ denote the distribution of $\mbf{x}_t\sim EC_p(\mbf{0},\Sigma,\nu)$ and let $\phi_I$ denote the distribution of $ EC_p(\mbf{0},I,\nu)$ where $I$ is the identity matrix.   By the change of variable $\mbf{z}=\Sigma^{-1/2} \mbf{x}$,
% 	\begin{equation}\label{eq:Gamma(s) simplify}
% 		\Gamma(s)=\int s(\mbf{x}) \mbf{x}  \mbf{x}' \phi_{\Sigma}(d\mbf{x}) =\Sigma^{1/2} \left(\int  s(  \Sigma^{1/2} \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}(  d\mbf{z})\right) \Sigma^{1/2}
% 	\end{equation}
% 	and 
% 	\[
% 	\E [ s(\mbf{x}_t)]=\int s( \Sigma^{1/2} \mbf{z}) \phi_I(d\mbf{z}).
% 	\]
% 	By the substitution $\wt{s}(\mbf{z})=s( \Sigma^{1/2} \mbf{z})-q_0$, in view of~\eqref{eq:Gamma(s) simplify}, the optimization problem~\eqref{eq:relax LSS opt} is equivalent to maximizing
% 	\begin{equation}\label{eq:opt det}
% 		\det\left(  \int  \wt s( \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}( d\mbf{z})\right) 
% 	\end{equation}
% 	under the constraint
% 	\begin{equation}\label{eq:rate constraint}
% 		\int \wt s( \mbf{z}) \phi_I(d\mbf{z})=q-q_0,\quad \wt s( \mbf{z})\in [0,1-q_0].
% 	\end{equation}
% 	In view of an arithmetic-geometric mean inequality for the eigenvalues, we have
% 	\begin{align}
% 		p\left(\det\left(  \int  \wt s( \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}(d\mbf{z}) \right)\right)^{1/p}&\le \tr\left(  \int  \wt s( \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}(d\mbf{z})  \right)\label{eq:art geo ineq} 
% 		=\int \wt{s}(\mbf{z}) \|\mbf{z}\|^2 \phi_I(d\mbf{z}).
% 	\end{align}
% 	Now we consider the maximization of the trace bound in \eqref{eq:art geo ineq}
% 	under the constraint  \eqref{eq:rate constraint}.
% 	We claim that this alternative problem  has   solution $\wt{s}= (1-q_0) 1_{D_r}$, where  $D_r=\{\mbf{x}\in \bb{R}^p:\ \|x\|>r\}$ and $r$ is as in \eqref{eq:threshold quantile relax}.
% 	If this is true, then by the isotropy of such $\wt{s}$ and $\phi_I$,  the matrix $\int  \wt s( \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}(d\mbf{z})$ is a nonzero multiple of the identity matrix, and hence  
% 	inequality in \eqref{eq:art geo ineq} is  an equality in this case.
	
% 	Indeed, suppose $s^*:\bb{R}^p\rightarrow [0,1-q_0]$ is another measurable function  satisfying the constraint \eqref{eq:rate constraint}. Let 
% 	\[A_1=\{\mbf{z}:\ \wt{s}(\mbf{z})>s^*(\mbf{z})\}=\{\mbf{z}:\ s^*(\mbf{z})<1-q_0,\ \|\mbf{z}\|> r\}
% 	\]and \[A_2=\{\mbf{z}:\ \wt{s}(\mbf{z})<s^*(\mbf{z})\}=\{\mbf{z}: \  s^*(\mbf{z})>0,\ \|\mbf{z}\|\le r\}.\]  Then the constraint \eqref{eq:rate constraint} implies 
% 	\begin{align}
% 		0&=\int [\wt{s}(\mbf{z})-s^*(\mbf{z}) ]   \phi_I(d\mbf{z}) 
% 		=\int_{A_1}[1-q_0-s^*(\mbf{z})]\phi_I(d\mbf{z}) -
% 		\int_{A_2} s^*(\mbf{z}) \phi_I(d\mbf{z}) .\label{eq:constraint imply}
% 	\end{align}
% 	On the other hand,
% 	\begin{align*}
% 		&\tr\left(  \int  \wt s( \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}(d\mbf{z})  \right)-\tr\left(  \int  s^*( \mbf{z}) \mbf{z}  \mbf{z}' \phi_{I}(d\mbf{z})\right) \\ =
% 		&\int [\wt{s}(\mbf z)-s^*(\mbf z)] \|\mbf{z}\|^2 \phi_I(d\mbf{z})
% 		\\=&\int_{A_1}[1-q_0-s^*(\mbf{z})]\|\mbf{z}\|^2\phi_I(d\mbf{z})-
% 		\int_{A_2} s^*(\mbf{z}) \|\mbf{z}\|^2\phi_I(d\mbf{z}) 
% 		\\\ge & r^2\left(\int_{A_1}[1-q_0-s^*(\mbf{z})] \phi_I(d\mbf{z})-
% 		\int_{A_2} s^*(\mbf{z}) \phi_I(d\mbf{z}) \right)=0,
% 	\end{align*}
% 	where in the   equality we  have used \eqref{eq:constraint imply}.  This   inequality is strict unless both $\phi_I(A_1)$ and $\phi_I(A_2)$ are zeros, which implies the uniqueness claimed in the theorem.
% \end{proof}





% \subsection{Proof of Theorem \ref{Thm:asymp_aux}}
% \begin{proof}
	
	
% 	Write
% 	\[
% 	\sqrt{n}(\wh{B}_{n,s} -B)=\left( \frac{\sum_{t=1}^n  \wt{\mbf{u}}_t   \wt{\mbf{u}}_t'}{n}    \right)^{-1}  n^{-1/2} \left( \sum_{t=1}^n \wt{\mbf{u}}_t ( \wt{\mbf{y}}_t'  -\wt{\mbf{x}}_t' B )   \right)=\wt{A}_n^{-1} \wt{D}_n,
% 	\]
% 	where 
% 	\begin{equation}\label{eq:A tilde D tilde}
% 		\wt{A}_n:=\frac{1}{n} \sum_{t=1}^n  \wt{\mbf{u}}_t   \wt{\mbf{u}}_t',\quad \wt{D}_n= n^{-1/2} \left( \sum_{t=1}^n \wt{\mbf{u}}_t ( \wt{\mbf{y}}_t'  -\wt{\mbf{x}}_t' B )   \right).
% 	\end{equation}
% 	We shall assume that the invertibility of $\wt{A}_n$ holds always without loss of generality, since below we shall show that $\wt{A}_n$ converges in probability to a non-singular matrix (see the proof of Theorem \ref{Thm:asymp_normality}).
	
% 	\noindent \emph{Step 1}.
% 	In this step we show that as $n\rightarrow\infty$,
% 	\begin{equation}\label{eq:LLN aux}
% 		\wt{A}_n:=\frac{1}{n} \sum_{t=1}^n  \wt{\mbf{u}}_t   \wt{\mbf{u}}_t'     \ConvP  \Gamma (s),
% 	\end{equation}
% 	where 
% 	\[\wt{\mbf{u}}_t = \wt{\mbf{x}}_t    1_{\{U_t\le   \wh{s}_t \}}=(\mbf{x}_t-\wh{\mbf{\mu}}_{t,X})    1_{\{U_t\le   \wh{s}_t \}}
% 	\]
% 	and $\Gamma(s)$ is as in \eqref{eq:Gamma(s) center}.
% 	Let 
% 	\[\mbf{u}_t=(\mbf{x}_t-\mbf{\mu}_X)1_{\{U_t\le   s(\mbf{x}_t-\mbf{\mu}_X) \}}\] and set 
% 	\begin{equation}
% 		A_n=\frac{1}{n} \sum_{t=1}^n  \mbf{u}_t    \mbf{u}_t',
% 	\end{equation}
% 	Since $(\mbf{u}_t  )$ is stationary and ergodic, by the ergodic theorem, we have 
% 	\[
% 	A_n\ConvP \E [\mbf{u}_1 \mbf{u}_1']= \Gamma(s)
% 	\] as $n\rightarrow\infty$. Hence it is enough to
% 	\begin{equation*} 
% 		\E \|A_n-\wt{A}_n\|\rightarrow 0
% 	\end{equation*}
% 	as $n\rightarrow\infty$, for which  by a triangular inequality it  suffices to show that as $t\rightarrow\infty$.
% 	\begin{align}\label{eq:u u tilde quad form close}
% 		\E \| \mbf{u}_t    \mbf{u}_t'     -  \wt{\mbf{u}}_t   \wt{\mbf{u}}_t'   \|\le    \E (\| \mbf{u}_t\| +\|\wt{\mbf{u}}_{t}\|)     \|\mbf{u}_t -  \wt{\mbf{u}}_t     \| \rightarrow 0
% 	\end{align}
% 	Note that 
% 	\begin{align}\label{eq:tilde u t bound}
% 		\|\wt{\mbf{u}}_t\|\le    1_{\{U_t\le \wh{s}_t\}}\left( \|\mbf{x}_t-\mbf{\mu}_X\|+\|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|\right).
% 	\end{align}
% 	Hence  by triangular and Cauchy-Schwartz inequalities,
% 	\begin{align*} 
% 		\E \| \mbf{u}_t    \mbf{u}_t'     -  \wt{\mbf{u}}_t   \wt{\mbf{u}}_t'   \|&\le   \E\left[  (2\|\mbf{x}_t-\mbf{\mu}_X\|+  \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\| )  \|\mbf{u}_t -  \wt{\mbf{u}}_t     \|\right]\notag\\&\le     \left(  2[\E \|\mbf{x}_t-\mbf{\mu}_X\|^2]^{1/2} +[\E \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|^2]^{1/2}\right) (\E \|\mbf{u}_t -  \wt{\mbf{u}}_t     \|^2)^{1/2}.
% 	\end{align*}
% 	With a triangular inequality, we have
% 	\begin{align}\label{eq:u u tilde diff moment}
% 		(\E \|\mbf{u}_t -  \wt{\mbf{u}}_t     \|^2)^{1/2}\le  [\E  \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X \|^2]^{1/2}+ [\E    \|\mbf{x}_t-\mbf{\mu}_X \|^2 |1_{\{U_t\le   \wh{s}_t\}}-1_{\{U_t\le   s(\mbf{x}_t-\mbf{\mu}_X) \}}|]^{1/2}.
% 	\end{align}
% 	%Then by H\"older's inequality, 
% 	%\begin{align}\label{eq:holder}
% 	% &\E \left[   \|\mbf{x}_t-\mbf{\mu}_X \|^2 |1_{\{U_t\le   \wh{s}_t\}}-1_{\{U_t  \le   s(\mbf{x}_t-\mbf{\mu}_X) \}}|\right] \notag\\  \le & \left[\E    \|\mbf{x}_t-\mbf{\mu}_X \|^{2+\delta} \right]^{2/(2+\delta)}   \left[\E|1_{\{U_t\le   \wh{s}_t\}}-1_{\{U_t\le   s(\mbf{x}_t-\mbf{\mu}_X) \}}|\right]^{\delta/(2+\delta)},
% 	%\end{align}
% 	%where the first factor above is a finite constant by \eqref{eq:moment delta cond} and stationarity. 
% 	The assumption \eqref{eq:mu_X mean sq cons}, since $\alpha>2$, ensures that the first term   in \eqref{eq:u u tilde diff moment} tends to zero as $t\rightarrow\infty$. To show that this is also the case for the second term, note that by independence of $U_t$ from the other random variables, the assumption \eqref{eq:s consistent} and uniform integrability from boundedness, we have
% 	\begin{align} 
% 		\E|1_{\{U_t\le   \wh{s}_t\}}-1_{\{U_t\le   s(\mbf{x}_t-\mbf{\mu}_X) \}}| 
% 		&= \E |\wh{s}_t-s(\mbf{x}_t-\mbf{\mu}_X)   | \rightarrow 0\label{eq:bound sym dff U_t}.
% 	\end{align}
% 	So  in view of stationarity of $(\mbf{x}_t)$ we have $\|\mbf{x}_t-\mbf{\mu}_X \|^2|1_{\{U_t\le   \wh{s}_t\}}-1_{\{U_t\le   s(\mbf{x}_t-\mbf{\mu}_X) \}}| \ConvP 0$ as $t\rightarrow\infty$. Since
% 	\[
% 	\|\mbf{x}_t-\mbf{\mu}_X \|^2 |1_{\{U_t\le   \wh{s}_t\}}-1_{\{U_t\le   s(\mbf{x}_t-\mbf{\mu}_X) \}}|\le  \|\mbf{x}_t-\mbf{\mu}_X \|^2,
% 	\]
% 	we conclude by  stationarity and uniform integrability that the second term in \eqref{eq:u u tilde diff moment} also tends to zero as $t\rightarrow\infty$. Combining these we see that  \eqref{eq:u u tilde quad form close} holds  and hence so does    \eqref{eq:LLN aux}.
% 	\medskip 
	
% 	\noindent \emph{Step 2}.
% 	In this step, we  establish the asymptotic normality of a modified version of $\wt{D}_n$ in \eqref{eq:A tilde D tilde}, which is
% 	\begin{equation}\label{eq:D_n}
% 		D_n=n^{-1/2} \left( \sum_{t=1}^n  \wt{\mbf{u}_t} [  (\mbf{y}_t-\mbf{\mu}_Y)'  - (\mbf{x}_t-\mbf{\mu}_X)' B ]   \right)=n^{-1/2} \left( \sum_{t=1}^n  \wt{\mbf{u}}_t \mbf{e}_t'   \right).
% 	\end{equation}
% 	Note that  by \eqref{eq:u u tilde quad form close} we have
% 	$
% 	\E[\wt{\mbf{u}}_t \wt{\mbf{u}}_t' ]  \rightarrow \Gamma(s)
% 	$ as $t\rightarrow\infty$. Hence as with \eqref{eq:cov} we have \[E[ \vect(\wt{\mbf{u}}_t\mbf{e}_t')\vect(\wt{\mbf{u}}_t\mbf{e}_t')']\rightarrow \Omega\otimes \Gamma(s)\]
% 	as $t\rightarrow\infty$.  
% 	Similarly as in the proof of Theorem \ref{Thm:asymp_normality}, we can show using  Cram\'er-Wold device, \eqref{eq:LLN aux} and martingale central limit theorem   that as $n\rightarrow\infty$,
% 	\begin{equation}\label{eq:md clt aux}
% 		\vect(D_n)=\frac{1}{\sqrt{n}}\sum_{i=1}^n  \vect( \wt{\mbf{u}}_t\mbf{e}_t') \ConvD N(\mbf{0}, \Omega\otimes \Gamma(s)).
% 	\end{equation}
% 	In particular, to verify the Lindeberg  condition  \cite[Eq.(36.36)]{billingsley:1995:probability}, with the help of \eqref{eq:tilde u t bound} it suffices to show   for any $\epsilon>0$ that
% 	\begin{align}\label{eq:lindeberg}
% 		\frac{1}{n}\sum_{t=1}^n\E  \Big[  &(\|\mbf{x}_t-\mbf{\mu}_X\|^2+\|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|^2)\|\mbf{e}_t\|^2 \times \notag  \\ &  \left( 1_{\{\|\mbf{x}_t-\mbf{\mu}_X\|\|\mbf{e}_t\|>n^{1/2}\epsilon\}}+ 1_{\{ \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|\|\mbf{e}_t\|>n^{1/2}\epsilon\}} \right)   \Big] \rightarrow 0
% 	\end{align}  
% 	as $n\rightarrow\infty$. To do so, we expand the expression above into three terms. The first term is  
% 	\[
% 	\frac{1}{n}\sum_{t=1}^n\E \left[\|\mbf{x}_t-\mbf{\mu}_X\|^2\|\|\mbf{e}_t\|^2 1_{\{  \|\mbf{x}_t-\mbf{\mu}_X\|\|\mbf{e}_t\|>n^{1/2}\epsilon\}}\right]=\E \left[\|\mbf{x}_1-\mbf{\mu}_X\|^2\|\|\mbf{e}_1\|^2 1_{\{  \|\mbf{x}_1-\mbf{\mu}_X\|\|\mbf{e}_1\|>n^{1/2}\epsilon\}}\right]
% 	\]
% 	by stationarity, which tends to zero as $n\rightarrow\infty$ by the dominated convergence theorem since by \eqref{eq:e_t cov},  
% 	\[
% 	E \left[ \|\mbf{x}_1-\mbf{\mu}_X\|^2\| \|\mbf{e}_1\|^2\right] = \E \left[ \|\mbf{x}_1-\mbf{\mu}_X\|^2 \E \left( \|\mbf{e}_1\|^2|\cl{F}_1\right)\right]=\E[\|\mbf{x}_1-\mbf{\mu}_X\|^2] \tr(\Omega)<\infty
% 	\] 
% 	The second term in the expansion of \eqref{eq:lindeberg} is
% 	\begin{align*}
% 		&\frac{1}{n}\sum_{t=1}^n\E \left[\|\mbf{x}_t-\mbf{\mu}_X\|^2\|\|\mbf{e}_t\|^2 1_{\{ \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|\|\mbf{e}_t\|>n^{1/2}\epsilon\}}\right]\\
% 		\le & \frac{1}{n}\sum_{t=1}^n\E \left[\|\mbf{x}_t-\mbf{\mu}_X\|^2\|\|\mbf{e}_t\|^2 1_{\{ \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|\|\mbf{e}_t\|>\epsilon\}}\right].
% 	\end{align*}
% 	To show this bound tends to zero as $n\rightarrow\infty$, it suffices to show that each $t$-term above tends to zero as $t\rightarrow\infty$. This follows from the fact that $\|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|\ConvP 0$ as $t\rightarrow\infty$,    stationarity and uniform integrability. The third term in the expansion  of \eqref{eq:lindeberg} is
% 	\begin{align*}
% 		&\frac{1}{n}\sum_{t=1}^n\E \left[\|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|^2\|\mbf{e}_t\|^2 \left(1_{\{  \|\mbf{x}_t-\mbf{\mu}_X\|\|\mbf{e}_t\|>n^{1/2}\epsilon\}}+1_{\{ \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|\|\mbf{e}_t\|>n^{1/2}\epsilon\}}  \right)\right]\\\le& \frac{2}{n}\sum_{t=1}^n\E\left[ \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|^2 \E \left( \|\mbf{e}_t\|^2 |\cl{F}_t\right)\right]= \frac{2\tr(\Omega)}{n}\sum_{t=1}^n\E  \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_X\|^2   \rightarrow 0
% 	\end{align*}
% 	as $n\rightarrow\infty$,
% 	where we have applied \eqref{eq:e_t cov} and \eqref{eq:mu_X mean sq cons}.  
	
% 	\medskip
% 	\noindent \emph{Step 3}.
% 	In this step we shall control the difference between $\wt{D}_n$ in \eqref{eq:A tilde D tilde}  and $D_n$ in \eqref{eq:D_n}. In particular, we shall establish for some constant $c>0$ that 
% 	\begin{equation}\label{eq:D tilde D diff}
% 		\limsup_{n\rightarrow\infty }n^{-1/2}\sum_{t=1}^n \E \|\wt{D}_t-D_t\|\le c q^{1-2/\alpha}.
% 	\end{equation} Indeed,
% 	by the triangular inequality and \eqref{eq:tilde u t bound},
% 	\begin{align*}
% 		\|\wt{D}_n-D_n\|\le &  n^{-1/2}  \sum_{t=1}^n \|\wt{\mbf{u}}_t\| \left( \|B\|     \|\mbf{x}_t- \wt{\mbf{x}}_t\|+\|\mbf{y}_t- \wt{\mbf{y}}_t\|\right) \\\le&  n^{-1/2}     \sum_{t=1}^n 1_{\{U_t\le \wh{s}_t\}} ( \|\mbf{x}_t-\mbf{\mu}_X\|+\|\wh{\mbf{ \mu}}_{t,X}-\mbf{\mu}_X\|)  (\|B\| \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_{X}\|+ \|\wh{\mbf{\mu}}_{t,Y}-\mbf{\mu}_{Y}\|  )\\
% 		=& n^{-1/2}\sum_{t=1}^n \left(E_t +F_t+G_t+H_t\right),
% 	\end{align*}
% 	where $E_t=1_{\{U_t\le \wh{s}_t\}} \|B\|  \|\mbf{x}_t-\mbf{\mu}_X\|  \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_{X}\|$, $F_t=1_{\{U_t\le \wh{s}_t\}} \|B\|  \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_{X}\|^2 $,  $G_t=1_{\{U_t\le \wh{s}_t\}} \|\mbf{x}_t-\mbf{\mu}_X\| \|\wh{\mbf{\mu}}_{t,Y}-\mbf{\mu}_{Y}\| $, $H_t= 1_{\{U_t\le \wh{s}_t\}}  \|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_{X}\| \|\wh{\mbf{\mu}}_{t,Y}-\mbf{\mu}_{Y}\|$.
% 	Note that by the generalized H\"older inequality,   and integrating out the randomness of $U_t$,
% 	\begin{align*}
% 		\E [E_t] \le \|B\|    \E[ \wh{s}_t]^{1-2/\alpha}   \E[ \|\mbf{x}_t-\mbf{\mu}_X\|^\alpha]^{1/\alpha}  \E[\|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_{X}\|^\alpha ]^{1/\alpha}.
% 	\end{align*}
% 	By the assumption \eqref{eq:s consistent},   uniform integrability and stationarity, we deduce that
% 	\[
% 	\E[  \wh{s}_t   ]\rightarrow  \E[   s(\mbf{x}_1-\mbf{\mu}_X)]=q
% 	\]
% 	as $t\rightarrow\infty$. Below  $c>0$ denotes a constant whose value may change from line to line and does not depend on $n$, $s$ or $q$. Then using   \eqref{eq:alpha moment}, \eqref{eq:mu_X mean sq cons} and the fact $\sum_{t=1}^n t^{-1/2}\le c n^{1/2}$, one can deduce that
% 	\begin{align*}
% 		\limsup_{n\rightarrow\infty }n^{-1/2}\sum_{t=1}^n \E [E_t] \le   c q^{1-2/\alpha}
% 	\end{align*}
% 	Similarly one has
% 	\begin{align*}
% 		\limsup_{n\rightarrow\infty }n^{-1/2}\sum_{t=1}^n \E [G_t] \le c q^{1-2/\alpha}. 
% 	\end{align*}
% 	On the other hand, 
% 	\[
% 	\E [F_t] \le \|B\| [\E\|\wh{\mbf{\mu}}_{t,X}-\mbf{\mu}_{X}\|^\alpha]^{2/\alpha}\le c t^{-1}
% 	\]
% 	and hence
% 	\[
% 	\limsup_{n\rightarrow\infty }n^{-1/2}\sum_{t=1}^n \E [F_t]=0.
% 	\]
% 	Similarly  
% 	\[
% 	\limsup_{n\rightarrow\infty }n^{-1/2}\sum_{t=1}^n \E [H_t]=0.
% 	\]
% 	Therefore, we conclude \eqref{eq:D tilde D diff}.
	
	
% 	\medskip
% 	\noindent \emph{Step 4}. Now we are ready to finish the proof.
% 	Let
% 	\[M_n=(N/n)^{1/2} \wt{A}_n^{-1}D_n,\] and set \[R_n=(N/n)^{1/2}\wt{A}_n^{-1} (\wt{D}_n-D_n).\] Then \eqref{eq:asymp norm lev aux} readily follows from \eqref{eq:N/n}, \eqref{eq:LLN aux}, \eqref{eq:md clt aux} and Slutsky. We are left to show \eqref{eq:R_n neg}. For a fixed $\gamma>1$, we have
% 	\begin{align*}
% 		&pr(\|(q^{-1}\Gamma(s))^{1/2}R_n\|>\delta)\\\le& pr(  q^{-1/2}\|\Gamma(s)^{1/2}\| (N/n)^{1/2} \|\wt{A}_n^{-1}\| \|\wt{D}_n-D_n\|>\delta) \\\le&  pr(\gamma\|\Gamma(s)^{-1}\|\|\Gamma(s)^{1/2}\| \|\wt{D}_n-D_n\|>\delta)+pr((N/n)^{1/2} \|\wt{A}_n^{-1}\| > \gamma q^{1/2}\|\Gamma(s)^{-1}\|)\\
% 		\le& \frac{ \gamma\|\Gamma(s)^{-1}\|\|\Gamma(s)^{1/2}\|}{\delta} \E \|\wt{D}_n-D_n\|+pr((N/n)^{1/2} \|\wt{A}_n^{-1}\| > \gamma q^{1/2}\|\Gamma(s)^{-1}\|).
% 	\end{align*}
% 	Hence combining  \eqref{eq:N/n},   \eqref{eq:LLN aux} and \eqref{eq:D tilde D diff}, we have
% 	\begin{align*}
% 		\limsup_{n\rightarrow\infty} pr(\|(q^{-1}\Gamma(s))^{1/2}R_n\|>\delta)\le\frac{c \gamma }{\delta}  \|\Gamma(s)^{-1}\|\|\Gamma(s)^{1/2}\| q^{1-2/\alpha},
% 	\end{align*}
% 	which by \eqref{eq:sampling fun cond aux} tends to zero as $q\rightarrow 0$.
% 	This implies \eqref{eq:R_n neg} in view of the identity 
% % 	\cite[Item 11.16]{seber:2008:matrix}
% \[P(s)^{1/2}\vect(R_n)=\vect(   (q^{-1}\Gamma(s))^{1/2} R_n \Omega^{-1/2})\] 
% 	and the fact that the norm of a matrix is equivalent to the norm of its vectorization.
% \end{proof}

% \subsection{Proof of Lemma \ref{Lem:aux}}
% \begin{proof}
	
% 	~\\
% 	\noindent(a)
% 	In the case $s(\mbf{x})=q_0+(1-q_0) 1_{\{\mbf{x}'\Sigma^{-1}\mbf{x}>r\}}$, in view of \eqref{eq:x EC} and \eqref{eq:Gamma(s) simplify},  we have
% 	\begin{align*}
% 		\Gamma(s)&= q_0\Sigma+ (1-q_0)  \Sigma^{1/2}\left(\int_{\|\mbf{z}\|^2>r}   \operatorname{diag}(z_1^2,\ldots,z_p^2) \phi_{I}(  d\mbf{z})\right) \Sigma^{1/2}\\&=   \left(q_0+ \frac{(1-q_0)}{p}\int_{y^2>r} y^2 \nu(dy)\right) \Sigma,
% 	\end{align*}
% 	where for the second equality above we have used the following fact due to symmetry: \[\int_{\|\mbf{z}\|^2>r}   z_1^2 \phi_{I}(  d\mbf{z})=\ldots=  \int_{\|\mbf{z}\|^2>r}   z_p^2 \phi_{I}(  d\mbf{z}).\]
% 	Therefore 
% 	\begin{align} 
% 		\frac{q^{1-2/\alpha}\lambda_{\max}(\Gamma(s)^{1/2}}{\lambda_{\min}(\Gamma(s))}=\frac{\lambda_{\max}(\Sigma)^{1/2}}{\lambda_{\min}(\Sigma)}   \left(q^{4/\alpha-2}q_0+ \frac{1-q_0}{p} q^{4/\alpha-2}\int_{y^2>r} y^2 \nu(dy)\right)^{-1/2}.\label{eq:sampling fun cond aux check}
% 	\end{align}
% 	Note that $\nu(\sqrt{r},\infty)=\frac{q-q_0}{1-q_0}$. Define $T(w)=\inf\{x\in (0,\infty):\ \nu(\sqrt{x},\infty)=  w \}$, $w\in[0,1)$ (recall $\nu$ is absolutely continuous). Then
% 	\begin{equation}\label{eq:lower bound aux}
% 		q^{4/\alpha-2}\int_{y^2>r} y^2 \nu(dy) \ge q^{4/\alpha-2}  \frac{q-q_0}{1-q_0}   r^2 \ge  c q^{4/\alpha-1} T\left(\frac{q-q_0}{1-q_0}\right)^2,
% 	\end{equation}
% 	where we have used \eqref{eq:q_0 restr}. Note that   $\lim_{q\rightarrow 0}T\left(\frac{q-q_0}{1-q_0}\right)=\lim_{w\rightarrow 0}T(w)>0$ since $\nu$ is not concentrated at the origin.  
	
	
% 	If $\alpha>4$, then the last lower bound in \eqref{eq:lower bound aux} tends to $\infty$ as $q\rightarrow 0$, and hence \eqref{eq:sampling fun cond aux check} tends to $0$.  
% 	Suppose now $\alpha\in (2, 4]$. Since $\nu(\sqrt{x},\infty)>y$ implies $T(y)>x$,  
% 	it follows from
% 	\eqref{eq:tail lower bound} that for some constant $c>0$, $T(w)> c w^{-2/\beta}$ for all sufficiently small $w$. Since also $ \frac{q-q_0}{1-q_0} \le  2q$   for all sufficiently small $q$ (recall $q_0\rightarrow 0$), we have
% 	\begin{align*}
% 		q^{4/\alpha-1} T(\frac{q-q_0}{1-q_0})^2\ge    c q^{4/\alpha-4/\beta-1} \rightarrow\infty
% 	\end{align*}
% 	as $q\rightarrow 0$ since the assumption on $\beta$ implies that $4/\alpha-4/\beta-1<0$. Therefore (a) has been proved.
	
% 	\medskip
% 	\noindent(b) 
% 	We first assume that $\wh{\Sigma}^{-1}_t$ and   $\wh{r}_t$ are consistent.
% 	With the definition in \eqref{eq:s hat t}, we have
% 	\begin{align*} 
% 		&  \E |\wh{s}_t-s(\mbf{x}_t)   |\notag\\=   &(1-q_0) \E | 1_{\{ \wt{\ell}_t> \wh{r}_t\}} -   1_{\{  \ell_t> r\}}  | 
% 		\le   pr( \ell_t> r,\wt{\ell}_t\le \wh{r}_t) + pr( \ell_t\le  r,\wt{\ell}_t> \wh{r}_t), 
% 	\end{align*}
% 	where $\wt{\ell}_t$ is as in \eqref{eq:tilde ell} and (with a little abuse of notation) \begin{equation}\label{eq:ell_t oracle}
% 	    \ell_t:=(\mbf{x}_t-\mbf{\mu}_X)' \Sigma^{-1} (\mbf{x}_t-\mbf{\mu}_X).
% 	\end{equation} With the consistency assumptions on $\wh{\mbf{\mu}}_{t,X}$, $\wh{\Sigma}^{-1}_t$, $\wh{r}_t$ and stationarity of $(\mbf{x}_t)$,  it is elementary to show  that $ (\wt{\ell}_t-\wh{r}_t )- (\ell_t-r)  \ConvP \mbf{0}$ and hence $(\ell_t-r,\wt{\ell}_t-\wh{r}_t)\ConvD (\ell_1-r,\ell_1-r)$ as $t\rightarrow\infty$ by stationarity. Using now the continuity of the distribution of $\ell_1$ (recall the distribution of $\mbf{x}_t$ is absolutely   continuous as assumed in Section \ref{Sec:linear sys}),  we deduce that 
% 	\begin{equation}\label{eq:><=} 
% 		\lim_{t\rightarrow\infty } pr( \ell_t> r,\wt{\ell}_t\le \wh{r}_t) =pr( \ell_1> r, \ell_1\le r)=0
% 	\end{equation}
% 	and 
% 	\begin{align}\label{eq:<=>}
% 		\lim_{t\rightarrow\infty } pr( \ell_t\le  r,\wt{\ell}_t> \wh{r}_t)=pr(\ell_1\le r,  \ell_1>r)=0.
% 	\end{align}
% 	Hence we have proved (b) based on the consistency assumption of $\wh{\Sigma}^{-1}_t$ and $\wh{r}_t$.
	
% It remains to show that 	$\wh{\Sigma}^{-1}_n$  in \eqref{sigma inv est} and $\wh{r}_n$ in \eqref{r_estimate} are consistent. To show the consistency of $\wh{\Sigma}^{-1}_n$, or equivalently, the consistency of $\wh{\Sigma}_n$,  we introduce its approximation $\wt{\Sigma}_n:=\frac{1}{M_n}\sum_{t=1}^n J_t (\mbf{x}_t- \mbf{\mu}_X) (\mbf{x}_t- \mbf{\mu}_X)'$. The convergence   $\wt{\Sigma}_n\ConvP \Sigma$ as $n\rightarrow\infty$ then follows from the fact $M_n/n\rightarrow u$ as $n\rightarrow\infty$ a.s.\ and the ergodic theorem. Furthermore,  
% it is not hard to  verify  based on the assumptions that $\|\wh{\Sigma}_n-\wt{\Sigma}_n\|\ConvP 0$ as $n\rightarrow\infty$. So we have established the consistency of $\wh{\Sigma}_n$.
% We now turn to the consistency of $\wh{r}_n$. Denote the empirical CDF of $\ell_t$ (cf.\ \eqref{eq:ell_t oracle}) and  $\wt{\ell}_t$ as $\wh{F}_n(r)$   and $\wt{F}_n(r)$, $r\ge 0$, respectively. We have $\E|\wt{F}_n(r)-\wh{F}_n(r)|\le \frac{1}{n}\sum_{t=1}^n   \E | 1_{\{ \wt{\ell}_t> r\}} -   1_{\{  \ell_t> r\}}| \rightarrow 0  $ as $n\rightarrow\infty$, which follows from an argument  similar to the one that leads to \eqref{eq:><=} and  \eqref{eq:<=>}.  On the other hand, the ergodic theorem entails the a.s.\ convergence $\wh{F}_n(r)\rightarrow F(r):=pr(\ell_1\le r)$, $r\ge0$, and hence $\wt{F}_n(r)\ConvP F(r)$ as $n\rightarrow\infty$. Recall that  $r(q-q_0)=\inf\{r\ge 0: \  F(r)\ge (q-q_0)/(1-q_0) \}$ and $\wh{r}_n= \inf\{r\ge 0: \  \wt{F}_n(r)\ge (q-q_0)/(1-q_0) \}$. Hence for any $\epsilon>0$ sufficiently small, 
% \begin{equation*}
% \begin{split}
% pr&\left(r(q,q_0)-\epsilon<\wh{r}_n\le r(q,q_0)+\epsilon\right)   \\
% &= pr\left(\wt{F}_n(r(q,q_0)-\epsilon)< \frac{q-q_0}{1-q_0}\le  \wt{F}_n(r(q,q_0)+\epsilon)\right)\rightarrow 1
% \end{split}
% \end{equation*}
% as $n\rightarrow\infty$.

 

% \end{proof} 


\end{document}
