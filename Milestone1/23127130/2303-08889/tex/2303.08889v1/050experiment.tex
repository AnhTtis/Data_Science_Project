\section{Social Correction Prediction}\label{sec:prediction}

In this section, we aim to answer two research questions:
\begin{itemize}
    \item RQ1: Given a misinformation tweet, can we predict whether it will be countered or not in the future?
    \item RQ2: Given a misinformation tweet that will be countered in the future, can we predict whether it will be countered with fewer or more counter-replies?
\end{itemize}

Both RQs are important to address for the combating of future misinformation. By being able to effectively predict future interactions surrounding misinformation tweets, we can better identify sets of online interactions where misinformation is being organically countered, along with those where additional countering needs to be performed. Answering RQ1 can identify sets of misinformation posts where other users may take the initiative in posting a counter-reply, while answering RQ2 can predict the intensity or magnitude of countering. 

\subsection{Dataset}
For both the research questions, we use the aforementioned dataset as we described in Section~\ref{sec:feature_analysis}. 

For RQ1, we divide the dataset into two sets of misinformation tweets: (1) misinformation tweets that have replies but none of them are counter-replies; (2) misinformation tweets that have at least one counter-reply. The sizes of these sets are 17,787 and 55,136, respectively.
For RQ2, we divide the misinformation tweets into two groups: one with a low proportion of counter-replies, and another group with a high proportion of counter-replies. Similar to the stratified setup in Section~\ref{sec:feature_analysis_strat}, we use the proportion of counter-replies as an indicator of membership for the two groups. 
The bottom 25\% of posts with respect to their countering proportion are assigned to the low countered group. On the other hand, the top 25\% posts with the highest proportion of countering replies are assigned to the highly countered group. The sizes of these sets are 14,274 and 15,224, respectively.

\subsection{Experimental setup}

Using each of these datasets, we follow similar approaches in tweet prediction tasks~\cite{micallef2020role, zahera2019fine} to address both RQ1 and RQ2.
We aim to build a binary classifier for each of RQ1 and RQ2, using the label definitions described above. For both RQs, we use the same set of features. We begin with the set of attributes listed in Table~\ref{tab:attr_list} with p < 0.001 and have non-null values for all datapoints; there are 63 such attributes (53 linguistic, 5 engagement, 5 poster). As shown in the existing tweet prediction task~\cite{micallef2020role}, the semantic information from textual embedding benefits the prediction task. Thus, we also generate the embedding vector for each tweet using RoBERTa~\cite{liu2019roberta}, which results in a 768-dimensional feature vector. Finally, we concatenate the above feature vectors to form a tweet feature vector to comprehensively represent the tweet and use it for classification.

\textbf{Classifier}: 
Following similar tweet or general text classification tasks~\cite{micallef2020role, he2021racism, he2021petgen}, 
we deploy widely-used conventional machine learning classifiers including Logistic Regression, XGBoost, and a Feed-forward Neural Network with a single hidden layer, using the feature vector as input. 
During the experiment, 10-fold cross-validation is deployed, and we report precision, recall, and F-1 score as the performance metrics. 

\subsection{Classifier Performance}

\begin{table}[!tbp]
    \centering
    \begin{tabular}{lrrr}
    \hline
    Method &  Precision &  Recall &  F-1 score \\ \hline
    Logistic Regression          &      0.801 &   0.929 &     0.860 \\
    XGBoost &      0.803 &   0.908 &     0.852 \\
    Neural Network                &      0.804 &   0.914 &     0.855 \\ \hline
    \end{tabular}
    \caption{RQ1: Classifier performance of whether tweets will get countered or not. }
    \vspace{-15pt}
    \label{tab:result_rq1}
\end{table}

In Table~\ref{tab:result_rq1}, we report the classification result for RQ1. As we can see, all three models are able to achieve good performance on the task. The logistic regression achieves the best performance in terms of precision, recall, and F-1 score; this result is also found in other similar tweet classification tasks~\cite{micallef2020role}. This high performance grants the ability to effectively predict whether a tweet will be countered or not, enabling fact-checkers and social media platforms to prioritize countering tweets identified as less likely to be countered organically.



\begin{table}[!tbp]
    \centering
    \begin{tabular}{lrrr}
    \hline    
    Method &  Precision &  Recall &  F1 score \\ \hline
    Logistic Regression         &      0.731 &   0.742 &     0.737 \\
    XGBoost             &      0.841 &   0.756 &     0.796 \\
    Neural Network                &      0.848 &   0.759 &     0.801 \\ \hline
    \end{tabular}
    \caption{RQ2: Classification performance of whether tweets will be highly countered versus that will be low countered.}
    \vspace{-15pt}
    \label{tab:result_rq2}
\end{table}

For RQ2, the classification result is shown in Table~\ref{tab:result_rq2}. As we can see, the model performance is still reasonably acceptable, but is worse compared to RQ1. 
This decrease in performance may imply that the task to identify the \textit{intensity} of countering tweets is not only more difficult, but also distinct from the task to identify \textit{whether} a tweet will be countered. In other words, the phenomenon of posting of the first counter-reply is easier to forecast than that of the posting of additional counter-replies given that at least one has already been posted.

