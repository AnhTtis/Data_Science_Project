\section{Dataset}

In this section, we describe 
the definition of the problem, as well as the corresponding dataset curation.

\subsection{Definitions}\label{sec:def}\hfill

\textbf{Misinformation}: We employ a broad definition of misinformation which includes falsehoods, inaccuracies, rumors, or misleading leaps of logic~\cite{wu2019misinformation}. Building on the existing work~\cite{hayawi2022anti}, we focus on misinformation related to the COVID-19 vaccine due to its broad impact around the world during the COVID-19 pandemic. Practically, the misinformative claims include ``the vaccine alters DNA'', ``the vaccine causes infertility'', ``the vaccine contains dangerous toxins'', and ``the vaccine contains tracking devices''; these topics are popular and widely studied by existing research works~\cite{hayawi2022anti, abbasi2022widespread}.


\textbf{Counter-reply}: Motivated by existing research works on analyzing replies that show disbelief toward misinformation~\cite{jiang2020modeling} or fact-check misinformation~\cite{pereira2022characterizing}, a direct response to a misinformation post $m$ is considered a ``countering'' reply
if it makes an attempt to explicitly or implicitly debunk or counter the misinformation tweet $m$. 
Otherwise, the reply is considered as non-countering. Practically, given a reply $r$, it is a:
\begin{itemize}
    \item \textbf{Countering reply:} Motivated by existing research works on identifying and analyzing text that is countering, debunking, disbelieving,  or disagreeing with misinformation~\cite{micallef2020role, jiang2020modeling, hossain2020covidlies}, a countering reply is a reply that explicitly or implicitly refutes the misinformation post (``this is misinformation''), points out the falsehood (``the COVID-19 vaccine does not change DNA''), insults the tweet poster (``you are born to lie''), or questions the misinformation (``Is there any reference I can check?'').
    \item \textbf{Non-countering reply:} Instead of countering, a non-countering reply supports, is in favor of, comments, repeats misinformation, etc., such as ``This is not the vaccine but the gene therapy'', ``Yes, I agree with you'', or ``It makes sense''. 
\end{itemize}
A post $m$ is considered to be countered if it receives at least one counter-reply. 
Meanwhile, given that different misinformation tweets have various numbers of replies, to have a normalized measure of the magnitude of which a misinformation tweet gets countered, we define the proportion of counter-replies to total replies, denoted as $p(m)$.



\subsection{Task Objective}
We consider the set $\mathcal{M}$ of misinformation posts about the COVID-19 vaccine. 
Each misinformation post $m \in \mathcal{M}$ has a set of $n$ replies $r = [r_1, ..., r_n]$ posted in direct response to $m$. 
Our final goal is to build a classifier $\mathcal{F}$ such that it can output a binary label $\mathcal{F}(m)$, which indicates whether the misinformation post will be countered or not, i.e., whether it will receive at least one counter-reply. 



\subsection{Dataset Curation}

\subsubsection{Misinformation Tweet Collection and Classification}

We utilize the Anti-Vax dataset from~\citet{HAYAWI202223}, 
a large-scale dataset of tweets related to the topic of COVID-19 vaccines, in order to identify misinformation tweets for our study. These tweets range eight months from December 1, 2020 to July 31, 2021, which was the relevant period covering a substantial part of the time from when the vaccines were approved by the FDA in December 2020~\cite{sharma2022covid}.
Also during this period, many uncertainties and misinformation about COVID-19 vaccines were spreading on social media~\cite{muric2021covid, sharma2022covid, HAYAWI202223}. 
The original dataset consists of approximately 15.4 million  tweets collected from the Twitter API~\cite{HAYAWI202223}, each containing at least one of the following COVID-19 vaccine relevant keywords: \{`vaccine', `pfizer', `moderna', `astrazeneca', `sputnik', `sinopharm'\}. Only original tweets were considered, i.e., retweet, reply, or quote tweets were removed. We utilized the Twitter API to retrieve the tweet text, user ID of the tweet author, datetime, conversation ID, reply settings, and tweet engagement metrics (like, retweet, quote, and reply counts). In total, we were able to retrieve 14,123,209 tweets from the original dataset while the remaining 1.3 million tweets were unavailable due to the deletion by the users or the Twitter platform.

Following the definition of misinformation in Section~\ref{sec:def} and the current approach of identifying COVID-19 vaccine related misinformation tweets~\cite{HAYAWI202223}, we first get the annotated misinformation tweets from~\citet{HAYAWI202223}, train a text classifier to determine if a tweet is misinformation or not, and classify all non-annotated tweets. Specifically, we first crawl and get 4,836 annotated misinformation and 8,596 annotated non-misinformation tweets from~\citet{HAYAWI202223}. Next, we build a text classifier using BERT~\cite{devlin2019bert}. This classifier has a promising performance in precision, recall, and F-1 scores of $0.972$, $0.979$, and $0.975$, respectively. This performance is comparable to the reported one in the original paper by ~\citet{HAYAWI202223} (i.e., the precision, recall, and F-1 scores of $0.97$, $0.98$, and $0.98$). 
The classifier has high performance as per the metrics and thus can be used for downstream classification tasks. 

Finally, we use this misinformation classifier to identify misinformation tweets in the entire dataset, resulting in 1,523,849 misinformation tweets and 12,599,360 non-misinformation tweets. Since we only focus on replies to misinformation in this work, we only use misinformation tweets for downstream analyses.



Next, we perform filtering of the dataset. Since our work focuses on categorizing misinformation by the composition of their replies, we further discard misinformation tweets that have zero replies. 
In addition, we discard tweets where the poster has limited the set of users who can reply to their tweet, to ensure that all tweets in our dataset have an equal opportunity to be replied to. This information is obtained from the Twitter API.

 

Finally, our COVID-19 vaccine misinformation tweet dataset consists of 268,990 tweets where each tweet has at least one reply. This is the final set of misinformation tweets that we use. 

\subsubsection{Counter-misinformation Reply Collection and Classification}
For each tweet in our misinformation dataset, we use the Twitter API to crawl all direct replies to the original tweet. 
In total, we collected a total of 1,991,611 replies to the 268,990 tweets. One misinformation tweet has an average of approximately 7.4 replies. 
The distribution of the reply count per tweet is shown in Figure \ref{fig:replies_per_tweet} in blue.

\textbf{Building a Counter-reply Classifier:} Since it is of high cost to manually annotate all replies, in order to identify all the counter-replies (and non-counter-replies) from this set of 1.9 million replies, we train another text-based classifier to determine if a reply counters the tweet or not. 
Here, we call this a "counter-reply classifier". 


Building on the existing works of the reply classification task~\cite{jiang2020modeling}, we first annotated replies and then built the classifier. Specifically, two students each first annotated 500 randomly-selected pairs of tweets and replies based on the textual contents into `Countering' or `Non-Countering' as per the definition provided in Section~\ref{sec:def}. 
This annotation resulted in an inter-rater agreement score of 0.7033 measured by percent agreement, resulting in 244 responses expressing countering while the remainder were non-countering. 
Then, after discussing the disagreements and creating the same annotation standard, each annotator labeled another 545 randomly selected pairs of tweets and replies. 
In total, we get 802 countering replies and 788 non-countering replies in our final annotated counter-reply dataset.


After getting the annotated replies, we utilize the Roberta-base lower-case architecture~\cite{liu2019roberta} as the classifier to which the input is the pair of tweets and replies. After the hyperparameter search across
batch size and learning rate, the classifier achieves a decent performance with a precision of 0.834, a recall of 0.819, and an F1-score of 0.822, which is sufficient for counter-reply classification on unlabeled replies.



Finally, we classify 690,047 (34.65\%) replies as counter-replies, and the remaining 1,301,564 (65.35\%) as non-counter-replies. The distribution of the counter-reply count per tweet is shown in Figure \ref{fig:replies_per_tweet} in orange. The average number of counter-replies that a misinformation tweet has is 2.57, and the average proportion of all replies of a misinformation tweet that are counter-replies is 0.271. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth, keepaspectratio]{figures/replies_per_tweet.jpg}
    \caption{Distributions of the total number of replies (blue) and number of counter-replies (orange) per misinformation tweet, each presented on a log scale.}
    \label{fig:replies_per_tweet}
\end{figure}

\subsubsection{Misinformation Poster Attribute Collection}
For each misinformation tweet, we also collect information of the user who posted the misinformation tweet, which includes date and time of account creation, number of tweets posted, account verification, follower count, and following count. In total, information for 137,929 unique users was retrieved.


Additionally, we collected all the tweets that the user posted in the 7 days leading up to them posting the misinformation tweet; we refer to these tweets as ``pre-misinformation" tweets. Only original and quote tweets were retrieved; replies and other retweets were excluded. We pull the same set of attributes as in the misinformation tweet crawling. In total, we retrieved a total of 31,450,114 ``pre-misinformation'' tweets, with an average of 116.9 ``pre-misinformation'' tweets per misinformation tweet. Note that these numbers include duplicate tweets if the user had posted two misinformation tweets within 7 days of each other.

As a final step, we identify the subset of ``pre-misinformation" tweets that are related to the topic of COVID-19 vaccines, as well as those that are also misinformative. We define a ``pre-misinformation'' tweet belonging to that subset if it contains at least one of the aforementioned six keywords that were used to collect the original Anti-Vax dataset, namely \{`vaccine', `pfizer', `moderna', `astrazeneca', `sputnik', `sinopharm'\}.
In total, 1,781,161 (5.71\%) of the ``pre-misinformation'' tweets are labeled as being about COVID-19 vaccines. We then utilize the aforementioned misinformation classifier to identify COVID-19 vaccine misinformation within this subset of ``pre- misinformation'' tweets, of which 335,458 (18.83\%) were classified as misinformative.