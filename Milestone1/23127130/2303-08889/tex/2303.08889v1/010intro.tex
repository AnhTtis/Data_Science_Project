\section{Introduction}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/exampleTweet.png}
    \caption{Examples of misinformation tweets and counter-misinformation replies. 
    }
    \vspace{-5mm}
    \label{fig:exampleTweet}
\end{figure}


Online misinformation leads to societal harm including diminishing trust in vaccines and health policies~\cite{pierri2022online,ball2020epic}, damaging the well-being of users consuming misinformation~\cite{kumar2018false,verma2022examining}, encouraging violence and harassment~\cite{arif2018acting,starbird2014rumors}, and posing a danger to democratic processes and elections~\cite{shin2017partisan,silverman2015lies,silverman2016analysis}. 
The problem has been exacerbated during the COVID-19 pandemic~\cite{micallef2020role, sharma2022covid}; particularly, COVID-19 vaccine misinformation including false claims that the vaccine causes infertility, contains microchips, and even changes one's DNA and genes has fueled vaccine hesitancy and reduced vaccine uptake~\cite{sharma2022covid}. Therefore, it is crucial to restrain the spread of online misinformation~\cite{micallef2020role, lewandowsky2012misinformation}.
In this work, we use a broad definition of misinformation which contains rumors, falsehoods, inaccuracies, decontextualized truths, or misleading leaps of logic~\cite{kumar2018false,wu2019misinformation}. 



To combat misinformation, various countermeasures have been developed~\cite{micallef2020role, miyazaki2022fake, vo2019learning}. Recent work has shown that ordinary users of online platforms play a crucial role in countering misinformation. According to the research study by~\citet{micallef2020role}, the vast majority (96\%) of online counter-misinformation responses are made by ordinary users, with the remainder being made by professionals such as fact-checkers and journalists. While fact-checking from these professionals has been widely used due to its prominent and measurable impact~\cite{micallef2020role, vo2019learning}, this process typically does not involve engaging with the actors spreading misinformation. Instead, the ordinary users' counter-misinformation efforts complement those from professional fact-checkers by directly engaging in countering conversations through making independent posts or direct replies to misinformation posts made by others~\cite{he2023reinforcement}.


Countering of misinformation messages via direct replies from ordinary users is called \textit{social correction}~\cite{bode2018see, kligler2022collective}.
One real example is shown in Figure~\ref{fig:exampleTweet}. 
Notably, social correction has been shown to be effective in curbing the spread of misinformation~\cite{colliander2019fake, wijenayake2020effect}, as well as doing so without causing increases in misperception~\cite{guess2020does,swire2020searching,wood2019elusive}. While certainly not a panacea for convincing people to reconsider potentially misinformative beliefs, they are most effective at reducing the misperceptions of those who may consume it~\cite{bode2018see,Bode2021,colliander2019fake,seo2022if}. 


However, little is known about the characteristics of misinformation tweets that attract social correction. 
Developing this understanding has several advantages: 
(1) first, it can help identify inequities in misinformation correction. For example, comparison of correction across users or communities (e.g., political ideologies) can reveal whether certain user types/communities are less likely to be self-correcting, e.g., communities where users correct misinformation when they see it. 
Identifying these disparities is the first step towards addressing them by redirecting resources towards entities that require external attention to curb misinformation; 
(2) second, if certain misinformation content is less likely to be socially corrected, targeted efforts can be directed toward countering them. 
Such instances can be escalated and prioritized for interventions by professionals or social media platforms;
(3) third, if certain misinformation content is likely to be socially corrected, then additional participants can be encouraged to provide reinforcements. 


Despite these promising benefits, characterizing and predicting social correction is non-trivial due to several challenges. 
First, existing datasets do not contain conversation-style narratives with paired misinformation posts and counter-replies.
Second, existing works (including~\citet{miyazaki2022fake}) do not analyze counter-replies to misinformation in a stratified manner where tweets with different numbers of replies are considered separately. This fine-grained analysis is necessary since comparing across or aggregating statistics across tweets that have drastically different numbers of (counter-)replies can skew the findings~\cite{jeni2013facing, higgins2008meta}.

In this work, we seek to characterize and predict counter-replies to misinformation.
The contributions can be summarized as follows:
\begin{itemize}
    \item We curate a novel large-scale dataset that contains 1,523,849 misinformation tweets and 690,047 counter-misinformation replies, along with a hand-annotated dataset of misinformation tweets and counter-replies. 
    \item We perform a stratified, fine-grained analysis of the linguistic, engagement, and poster-level characteristics of misinformation tweets that get countered versus those that do not. Our analysis reveals several features of tweets that attract social correction, such as anger and impoliteness.
    \item We create two counter-reply prediction models to identify whether a misinformation tweet will be countered or not, and if so, to what degree (i.e. low or high), based on its linguistic, engagement, and poster features. We achieve promising predictive performance with both of these models, with best F-1 scores of 0.860 and 0.801, respectively.
\end{itemize}
The code and data is accessible on \url{https://github.com/claws-lab/social-correction-twitter}.
