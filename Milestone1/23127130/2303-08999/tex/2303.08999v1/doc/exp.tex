\section{Experimental Results}
\label{sec:exp}

\begin{table*}[tb!]
    % \renewcommand\arraystretch{1.2}
    \centering
    \caption{Comparisons on multiple benchmark datasets of our model and other popular SR networks. The dictionary in our model is compressed to 10\% of original size for evaluation. Performance metrics are PSNR/SSIM. \textbf{Bold}: \textbf{best} results}
    \label{tab:performance}
    \begin{tabular}{c|c|c|c|c|c|c}
        \toprule
        Scale & Method & Set5 & Set14 & B100 & Urban100 & Manga109 \\
        \midrule
        \multirow{9}{*}{$\times$2}  &SRCNN\cite{SR-eccv14-srcnn}          &36.66/0.9542   &32.42/0.9063   &31.36/0.8879   &29.50/0.8946   &35.74/0.9661   \\
        &FSRCNN\cite{SR-eccv2016-frcnn}         &37.00/0.9558   &32.63/0.9088   &31.53/0.8920   &29.88/0.9020   &36.67/0.9694   \\
        &VDSR\cite{SR-2016cvpr-vsdr}           &37.53/0.9587   &33.03/0.9124   &31.90/0.8960   &30.76/0.9140   &37.22/0.9729   \\
        &DRRN\cite{SR-2017cvpr-drrn}           &37.74/0.9591   &33.23/0.9136   &32.05/0.8973   &31.23/0.9188   &37.92/0.9760   \\ 
        &LapSRN\cite{SR-2017-lapsrn}        &37.52/0.9590   &33.08/0.9130   &31.80/0.8950   &30.41/0.9100   &37.27/0.9740   \\
        &SRFBN-S\cite{sr-cvpr2019-SRFBNS}        &37.78/0.9597   &33.35/0.9156   &32.00/0.8970   &31.41/0.9207   &38.06/0.9757   \\
        &FALSR-A\cite{sr-icpr2021-FALSR}        &37.82/0.9595   &33.55/0.9168   &32.12/0.8987   &31.93/0.9256   &   -           \\ 
        &SRMDNF\cite{sr-cvpr2018-SRMDNF}        &37.79/0.9600   &33.32/0.9150   &32.05/0.8980   &31.33/0.9200   &   -           \\
        &Ours           &\textbf{37.98/0.9604}  &\textbf{33.59/0.9181}   &\textbf{32.19/0.8999}   &\textbf{32.09/0.9281}   &\textbf{38.60/0.9771}   \\\midrule
        \multirow{8}{*}{$\times$3}  &SRCNN\cite{SR-eccv14-srcnn}           &32.75/0.9090   &29.28/0.8209   &28.41/0.7863   &26.24/0.7989   &30.59/0.9107   \\
        &FSRCNN\cite{SR-eccv2016-frcnn}          &33.16/0.9140   &29.43/0.8242   &28.53/0.7910   &26.43/0.8080   &30.98/0.9212   \\
        &VDSR\cite{SR-2016cvpr-vsdr}           &33.66/0.9213   &29.77/0.8314   &28.82/0.7976   &27.14/0.8279   &32.01/0.9310   \\
        &DRRN\cite{SR-2017cvpr-drrn}           &34.03/0.9244   &29.96/0.8349   &28.95/0.8004   &27.53/0.8378   &32.74/0.9390   \\
        &SelNet\cite{sr-2017cvpr-selnet}         &34.27/0.9257   &30.30/0.8399   &28.97/0.8025   &   -           &   -           \\ 
        &CARN\cite{sr-eccv2018-carn}           &34.29/0.9255   &30.29/0.8407   &29.06/0.8034   &28.06/0.8493   &   -           \\  
        &SRFBN-S\cite{sr-cvpr2019-SRFBNS}        &34.20/0.9255   &30.10/0.8372   &28.96/0.8010   &27.66/0.8415   &33.02/0.9404   \\
        &Ours  &\textbf{34.35/0.9267}   &\textbf{30.33/0.8420}   &\textbf{29.11/0.8054}   &\textbf{28.12/0.8523}   &\textbf{33.48/0.9439}   \\\midrule
        \multirow{8}{*}{$\times$4}  &SRCNN\cite{SR-eccv14-srcnn}          &30.48/0.8628   &27.49/0.7503   &26.90/0.7101   &24.52/0.7221   &27.66/0.8505   \\
        &FSRCNN\cite{SR-eccv2016-frcnn}          &30.71/0.8657   &27.59/0.7535   &26.98/0.7150   &24.62/0.7280   &27.90/0.8517   \\
        &VDSR\cite{SR-2016cvpr-vsdr}            &31.35/0.8838   &28.01/0.7674   &27.29/0.7251   &25.18/0.7524   &28.83/0.8809   \\
        &DRRN\cite{SR-2017cvpr-drrn}            &31.68/0.8888   &28.21/0.7720   &27.38/0.7284   &25.44/0.7638   &29.46/0.8960   \\ 
        &LapSRN\cite{SR-2017-lapsrn}         &31.54/0.8850   &28.19/0.7720   &27.32/0.7280   &25.21/0.7560   &29.09/0.8845   \\
        &CARN\cite{sr-eccv2018-carn}            &32.13/0.8937   &28.60/0.7806   &27.58/0.7349   &26.07/0.7837   &   -           \\
        &SRFBN-S\cite{sr-cvpr2019-SRFBNS}         &31.98/0.8923   &28.45/0.7779   &27.44/0.7313   &25.71/0.7719   &29.91/0.9008   \\
        &Ours  &\textbf{32.15/0.8944}   &\textbf{28.61/0.7817}   &\textbf{27.59/0.7366}   &\textbf{26.14/0.7873}   &\textbf{30.39/0.9072}   \\\bottomrule
    \end{tabular}
\end{table*}




\subsection{Experimental Setup}

\textbf{Hardware Implementation:} 
To validate the performance of our accelerator with respect to the acceleration ratio and quality of results, 
we deploy our proposed high-performance accelerator on edge embedded GPU NVIDIA Jetson Xavier NX, in comparison with the state-of-the-art tool NVIDIA TensorRT. 
NX integrates an ARM v8.2 64-bit CPU processor and a 384-core NVIDIA Volta GPU with 48 Tensor Cores. 
To take full advantage of the AI workloads, we use 15W of power to make it deliver up to 21 TOPS to compute. 
The clock frequency of the ARM processor is 2-core 1900MHz, and 4/6 core 1400MHz. The clock frequency of the GPU processor is 1100MHz. 
We also test our accelerator on NVIDIA GeForce RTX 2080 Ti with 4352 FP32 FPUs (CUDA cores) and 544 Tensor cores for accuracy evaluation. 
The results are also compared with PyTorch. 

\textbf{Software Implementation:} 
All the designs are implemented by CUDA 11.0 and TensorRT 7.1.3. We use 32-bit floating point precision data types for evaluation. 
The training and evaluation of both original and our modified model are implementation through PyTorch based on official LAPAR code repository\cite{SR-LAPAR-repo}. 

\textbf{Dataset:} 
The proposed accelerator is evaluated on common single image super-resolution (SISR) Set5\cite{SR-bevilacqua2012low-set5}, 
Set14\cite{SR-ledig2017photo-set14}, B100\cite{SR-martin2001database-b100}, Urban100\cite{SR-ledig2017photo-Urban100}, Manga109\cite{SR-lai2017deep-mango109} dataset.


% \subsection{Analysis on Communication and Computation Bottlenecks}
% We observe that a deep super-resolution network has various kinds of layers. 
% Deep learning inference frameworks can transform networks into general computation graphs. 
% Each layer int the computation graphs has different shapes of tensors, which can cause dissimilar computation and communication behaviors. 
% We use the roofline model to implement layer by layer profiling in terms of both computation and data transfer. 
% The profiling information can be found in Table xxx. The platform we use is NVIDIA Jetson Xavier NX. 
% The peak performance is up to 21 TOPS under 15W of power. 
% The theoretical bandwidth is up to 51.2GB/s on using 8GB RAM DDR4 and we assume each interface is assigned one half of the theoretical bandwidth which is 25.6GB/s. 
% Then we plot the computational roof and bandwidth roof of the input feature map in Fig xxx for LAPAR model using 32-bit floating point data precision. 


% As input feature maps, weights of each kernel and otuput feature maps access off-chip memory simultaneously according to the dataflow in traditional deep neural network accelerator in Fig xxx,
% From the roofline model graph, the y-axis represents the attainable performance (Tops), and the x-axis denotes the operation intensity (Ops/Byte), which shows calculation per off-chip data transfer. We find that the bandwidth requirements of some layers can exceed the limitation of edge GPU devices. In spite of the rapid hardware development, computation and memory power are still constrained on edge devices . All in all, it is impractical to put all parameters in on-chip memory. As a consequence, the performance of these layers are bounded by the off-chip memory bandwidth. After analyzing the performance of each layer in super resolution model, we divide all of the layers into two categories. The first class is called memory bounded layers and the second class is called computation bounded layers. In order to improve the performance of the second class layers, we can keep their original data information in off-chip memory. 


\subsection{Performance Evaluation}


To evaluate the acceleration performance of our proposed accelerator, we compare it to the baseline designs on NVIDIA Jetson Xavier NX and RTX 2080 Ti. 
The inference time is measured with multiple input frame size and scale ratio. 
The results are in 32-bit floating point precisions. The running times are shown in \Cref{tab:runtime}.
We successfully realize SR with output of 540P quality to real-time inference. 
% Additionally, we conduct time comparison on NVIDIA Jetson Xavier NX to show significant inference speed-up on edge device. 
Compared with the widely-used PyTorch on 2080 Ti, our accelerator outperforms it by $352.27\%$. 
On average, our accelerator is faster than TensorRT by $144.49\%$ on 2080 Ti and by $156.28\%$ on Jetson Xavier NX, respectively. 
On various sizes of inputs and scales, our accelerator achieves +27.45\%$\sim$77.56\% remarkable acceleration to TensorRT. 
An exciting result is that the acceleration ratios on Jetson Xavier NX are higher than on 2080 Ti. 
This shows the outstanding performance of our accelerator while handling the complex and difficult dictionary learning algorithms on limited computation and communication resources of edge embedded GPUs. 

To demonstrate the high-quality results of our proposed accelerator without quality degradation, we compare the output images with other popular models, as shown in \Cref{tab:performance}. 
The performance metrics are PSNR (peak signal-to-noise ratio) and SSIM (structural similarity index measure), which are widely used to measure the qualities of images and videos. 
The higher values represent the better results. 
Note that our framework compresses the models with dictionary shrunk to 10\% of the original size, while all of the baselines are not compressed. 
The results show that our method is superior to all of the baselines on both of these two metrics. 

\begin{figure}[tb!]
    \centering
    \hspace{-.4in}
    \subfloat[Input Size 64$\times$64]    { \includegraphics[width=.5\linewidth]{prune-speed/64}  } \hspace{-.1in}
    \subfloat[Input Size 128$\times$128]  { \includegraphics[width=.5\linewidth]{prune-speed/128} } \\
    \hspace{-.4in}
    \subfloat[Input Size 180$\times$320]  { \includegraphics[width=.5\linewidth]{prune-speed/180} } \hspace{-.1in}
    \subfloat[Input Size 360$\times$640]  { \includegraphics[width=.5\linewidth]{prune-speed/360} }
    \caption{
        Time consumptions of the dictionary query and filtering with different compression ratios.
        Different input image sizes and scaling factors (from 2 to 4) are evaluated. 
        %The time cost continuously decreases as the dictionary size compressed to 10\%.
    }
    \label{fig:speed_up}
\end{figure}


Some ablation study results with respect to the compressions of the dictionary learning are shown in \Cref{fig:speed_up}. 
The compression ratio of 100\% represents the original dictionary without compression. 
As the compression ratios shrink, the time costs decrease continuously on all of these tests. 
When the compression ratio reaches 10\%, the dictionary query and filtering flow is accelerated by up to nearly $\times$20. 

% Despite the compromise on model size which is compressed for acceleration, our performance still realized state-of-art on multiple benchmark datasets. 
% The first section is to evaluate proposed accelerator by comparing it to baseline designs by PyTorch and the state-of-the-art tool TensorRT. 
% Detailed analysis in terms of performance improvement and resource utilization is given. 
% The second section is to use our acceleration engine to power industry super resolution model in actual production.
% We compare our 32-bit floating point results with the baseline designs and TensorRT. The runtime comparison is shown in \Cref{tab:runtime}. \\

%For performance comparison, our designs have xxx and yyy speedup in terms of runtime over the basline designs and TensorRT respectively. 
% \begin{itemize} 
% 1) The LAPAR is the model mentioned in the \Cref{sec:sr-architecture}. The input image resolution is $180 \times 320$and the output image resolution is $720 \times 1280$. Compared with the baseline designs and TensorRT tool,  our method can attain 2.6X and 1.62X speedup with one batch. Using the re-organization task mentioned in \Cref{sec:taskreo}, we set the input image resolution with $90 \times 100$ for eight batches. We can achieve 2.6X speedup and 1.6X speedup respectively.Industry-V1 and Industry-V2 are the models used in actual produciton. Compared with the Lapar, the only difference is that they have simplified branches in stage two mentioned in \Cref{sec:sr-architecture} and modules in other stages are identical. We also use the same techniques, such as task re-organization, parallel execution, pre-predict and pattern redesigns to implement on these models. \\
% 2) For Industry-V1 model, the input image resolution is $540 \times 960$ with one batch and the output image resolution is $1080 \times 1920$. In this setting, we do not use the task re-organization algorithm because it will not produce extremely-large data frames during the inference. Our design has 7.49X and 2.71X speedup in terms of runtime over the PyTorch baseline and TensorRT tool. \\
% 3) For Industry-V2 model, the input image resolution is $1080 \times 1920$ and the output image resolution is $2160 \times 3840$. In the experiment, when we input the whole image into the network. It leads to the out of memory information on our experimental platform because of the limited GPU memory (8GB) based on the PyTorch design. Therefore, we split the whole image into eight batches in order to avoid the bottleneck of memory. For performance comparison, our designs have 5.37X and 2.15X speedup in terms of eight batches over the baseline designs and TensorRT respectively. \\
% \end{itemize}

% \begin{table*}[tb!]
%     \caption{Comparisons of End-to-end Model Inference Latency and Variance}
%     \centering
%     \label{tab:finalresults}
%     \resizebox{18.0cm}{!} {
%     \begin{tabular}{c|cc|cccc|cc|cc}
%         \toprule
%         \multirow{2}{*}{Model} 
%         &\multicolumn{2}{c|} {AutoTVM}    &\multicolumn{4}{c|} {Ours}     & \multicolumn{2}{c|} {GGA} & \multicolumn{2}{c} {CHAMELEON}\\ %\cline{2-13}
%             & Search Time (s) & Latency (ms) 
%             & Search Time (s) & Ratio & Latency (ms) & Ratio
%             & Search Time  & Latency  
%             & Search Time  & Latency    
%         \\ \midrule
%         \texttt{AlexNet}         &   22005.94 & 1.3639 & 10553.15 & 2.09 x & 1.2447 & 1.0957 x &          1.3304 &          1.3304 & \textbf{- 2.46} & \textbf{-59.09} \\ 
%         \texttt{ResNet-18}       &  113728.65 & 1.8323 & 27570.85 & 3.64 x & 1.8857 & 0.9717 x &          1.7519 &          1.7519 & \textbf{- 4.39} & \textbf{-17.27} \\ 
%         \texttt{VGG-16}          &   67940.72 & 8.7448 & 37187.69 & 1.83 x & 6.7883 & 1.2882 x &          5.6183 &          5.6183 & \textbf{-13.80} & \textbf{-84.82} \\ 
%         \texttt{MobileNet-v1}    &  107031.18 & 1.4187 & 43327.06 & 2.47 x & 0.7916 & 1.7922 x &          0.7621 &          0.7621 & \textbf{-28.08} & \textbf{-92.74} \\ \midrule 
%         Average                  &   77676.62 & 3.3399 & 29659.69 & 2.62 x & 2.6775 & 1.2870 x & \textbf{2.0309} & \textbf{2.0309} & \textbf{-13.83} & \textbf{-67.74} \\ 
%         \bottomrule
%     \end{tabular}
%     }
% \end{table*}


% \begin{table*}[tb!]
%     \centering
%     \caption{Model Runtime}
%     \label{tab:runtime}
%     %\resizebox{1\linewidth}{!} {
%     \begin{tabular}{l|ccc|ccc|ccc}
%         \toprule
%         \texttt{Input Size} & \multicolumn{3}{c|}{Baselime (ms)} & \multicolumn{3}{c|}{TensorRT (ms)}  & \multicolumn{3}{c}{Ours (ms)}   \\ \midrule
%         & x2 & x3  &x4 &x2 &x3 &x4 &x2 & x3 &x4 \\
%         \texttt{64*64}        &1  &1  &1  &1   &1  &1  &-  &-  &-  \\
%         \texttt{128*128}	  &-  &-  &-   &-   &-  &-  &-  &-  &-  \\
%         \texttt{540*960}	  &-  &-  &-   &-   &-  &-  &-  &-  &-  \\

%             % \texttt{Industry V1}         &   2206.06 &   2895.15 &   4022.38 & 9216.00      &   4568.00  \\
%             % \texttt{Industry V2}     &   166.66  &   917.01  &   1206.94 & 103680.00    &   20000.00 \\
%             % \texttt{STENCIL3D}       &   369.10  &   2082.25 &   969.29  & 4608.00      &   1920.00  \\ \midrule
%             \bottomrule
%     \end{tabular}
%     %}
% \end{table*}


% \begin{table*}[tb!]
%     \centering
%     \caption{Dictionary Query time consumption (ms) with size compression ratio (\%)}
%     \label{tab:runtime}
%     %\resizebox{1\linewidth}{!} {
%     \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
%         \toprule
%         \texttt{Input size} & Scale & Original & 90\% & 80\% & 70\% & 60\% & 50\% & 40\% &30\% & 20\%  &10\% \\
%         % \texttt{Input Size} & \multicolumn{3}{c|}{Baselime (ms)} & \multicolumn{3}{c|}{TensorRT (ms)}  & \multicolumn{3}{c}{Ours (ms)}    
%         \midrule
%         \multirow{3}{*}{64*64}      &$\times$2 &0.106  &0.101  &0.087  &0.081  &0.077  &0.072  &0.047  &0.043  &0.035  &0.032  \\\cline{2-12}
%                                     &$\times$3 &0.224  &0.215  &0.172  &0.149  &0.128  &0.117  &0.096  &0.075  &0.061  &0.049  \\\cline{2-12}
%                                     &$\times$4 &0.375  &0.356  &0.288  &0.257  &0.217  &0.189  &0.160  &0.122  &0.100  &0.081  \\\cline{1-12}
%         \multirow{3}{*}{128*128}    &$\times$2 &0.375  &0.357  &0.294  &0.257  &0.217  &0.190  &0.160  &0.121  &0.099  &0.080  \\\cline{2-12}
%                                     &$\times$3 &0.866  &0.824  &0.649  &0.583  &0.495  &0.413  &0.351  &0.277  &0.197  &0.151  \\\cline{2-12}
%                                     &$\times$4 &1.451  &1.385  &1.099  &0.962  &0.794  &0.719  &0.643  &0.506  &0.343  &0.252  \\\cline{1-12}
%         \multirow{3}{*}{180*320}    &$\times$2 &1.396  &1.232  &0.983  &0.856  &0.718  &0.631  &0.523  &0.425  &0.300  &0.230  \\\cline{2-12}
%                                     &$\times$3 &2.917  &2.741  &2.187  &1.949  &1.587  &1.601  &1.136  &0.904  &0.651  &0.563  \\\cline{2-12}
%                                     &$\times$4 &5.108  &4.794  &3.907  &3.373  &2.814  &2.523  &2.036  &1.604  &1.149  &0.848  \\\cline{1-12}
%          \multirow{3}{*}{360*640}   &$\times$2 &5.112  &4.793  &3.905  &3.375  &2.815  &2.522  &2.035  &1.604  &1.150  &0.849  \\\cline{2-12}
%                                     &$\times$3 &11.205 &10.554 &8.586  &8.226  &6.232  &5.540  &4.385  &3.474  &2.441  &1.782  \\\cline{2-12}
%                                     &$\times$4 &20.200 &18.880 &15.362 &13.112 &10.947 &9.736  &7.666  &6.121  &4.266  &3.096  \\\cline{1-12}
%         % & x2 & x3  &x4 &x2 &x3 &x4 &x2 & x3 &x4 \\
%         % \texttt{64*64}        &1  &1  &1  &1   &1  &1  &-  &-  &-  \\
%         % \texttt{128*128}	  &-  &-  &-   &-   &-  &-  &-  &-  &-  \\
%         % \texttt{540*960}	  &-  &-  &-   &-   &-  &-  &-  &-  &-  \\

%             % \texttt{Industry V1}         &   2206.06 &   2895.15 &   4022.38 & 9216.00      &   4568.00  \\
%             % \texttt{Industry V2}     &   166.66  &   917.01  &   1206.94 & 103680.00    &   20000.00 \\
%             % \texttt{STENCIL3D}       &   369.10  &   2082.25 &   969.29  & 4608.00      &   1920.00  \\ \midrule
%         \bottomrule
%     \end{tabular}
%     %}
% \end{table*}


\subsection{Discussions}

The results show the outstanding performance of our high-performance SR accelerator, especially on the resource-limited edge embedded GPU NVIDIA Jetson Xavier NX. 
The difficulties are resulting from the special memory and computation patterns of the dictionary learning algorithms, which cannot be handle by the existing tools. 
Another important reason is the great memory pressures because of the large scales of the super-resolution images. 
To the best of our knowledge, our proposed accelerator is the first to achieve superior performance on SR applications on edge embedded GPUs. 

\iffalse
\begin{table*}[tb!]

    \centering
    \caption{Comparisons of Inference Times (ms) with Inference Acceleration Ratios}
    \label{tab:runtime}
    %\resizebox{1\linewidth}{!} {
    \begin{tabular}{c|c|c|c|c|cc}
        \toprule
        \texttt{Input size} & Scale & Baseline & TensorRT & Ours & Acce (baseline) & Acce (TensorRT) \\
        % \texttt{Input Size} & \multicolumn{3}{c|}{Baselime (ms)} & \multicolumn{3}{c|}{TensorRT (ms)}  & \multicolumn{3}{c}{Ours (ms)}    
        \midrule
        \multirow{3}{*}{64*64} &$\times$2&6.94 & 1.30&1.02 & $\times$680.39\% &$\times$127.45\% \\\cline{2-7}
         &$\times$3&8.26 &1.94 & 1.40& $\times$590.00\% & $\times$138.57\% \\\cline{2-7}
         &$\times$4&9.86 &2.79 &1.88 & $\times$524.46\% & $\times$148.40\% \\\cline{1-7}
        \multirow{3}{*}{128*128} &$\times$2& 8.74& 3.59& 2.66&$\times$328.57\% &$\times$134.96\% \\\cline{2-7}
         &$\times$3&13.04 &6.19 &4.16 &$\times$313.46\% &$\times$148.80\% \\\cline{2-7}
         &$\times$4& 18.07&9.71 & 6.13& $\times$294.78\%& $\times$158.40\%\\\cline{1-7}
        \multirow{3}{*}{180*320} &$\times$2&17.12 &12.40 & 9.25& $\times$185.08\% & $\times$134.05\% \\\cline{2-7}
         &$\times$3&30.83 & 21.66&14.63 & $\times$210.73\% & $\times$148.05\%\\\cline{2-7}
         &$\times$4&44.69 & 34.69& 22.12& $\times$202.03\% & $\times$156.82\% \\\cline{1-7}
         \multirow{3}{*}{360*640} &$\times$2&67.36 &50.26 &37.47 & $\times$179.77\% & $\times$134.13\% \\\cline{2-7}
         &$\times$3& 105.32&88.45 & 59.20& $\times$177.90\% & $\times$149.41\%\\\cline{2-7}
         &$\times$4& 406.93& 141.08& 91.09& $\times$540.02\% & $\times$154.88\% \\\cline{1-7}
        % & x2 & x3  &x4 &x2 &x3 &x4 &x2 & x3 &x4 \\
        % \texttt{64*64}        &1  &1  &1  &1   &1  &1  &-  &-  &-  \\
        % \texttt{128*128}	  &-  &-  &-   &-   &-  &-  &-  &-  &-  \\
        % \texttt{540*960}	  &-  &-  &-   &-   &-  &-  &-  &-  &-  \\

            % \texttt{Industry V1}         &   2206.06 &   2895.15 &   4022.38 & 9216.00      &   4568.00  \\
            % \texttt{Industry V2}     &   166.66  &   917.01  &   1206.94 & 103680.00    &   20000.00 \\
            % \texttt{STENCIL3D}       &   369.10  &   2082.25 &   969.29  & 4608.00      &   1920.00  \\ \midrule
        \bottomrule
    \end{tabular}
    %}
\end{table*}
\fi



