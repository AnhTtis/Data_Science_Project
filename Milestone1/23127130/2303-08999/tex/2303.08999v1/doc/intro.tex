\section{Introduction}
\label{sec:intro}

Super-resolution (SR), which refers to the process of recovering or generating high-resolution (HR) video frames from low-resolution (LR) frames, 
is an important class of graphical processing techniques in computer vision. 
Among the existing methods, the simplest one is to adopt basic spatially invariant nearest-neighbor, bilinear, and bicubic interpolation. 
In the past decade, with the fast developments of deep learning algorithms, 
a large variety of deep learning models has also been adopted to tackle the SR tasks, ranging from general convolution neural networks \cite{SR-2015TAMPI-CNN} 
to generative adversarial networks \cite{SR-2017CVPR-GAN, SR-2020AAAI-JSI-GAN}. 
Recently, by introducing dictionary learning methods with pixel-level local feature fusion operations \cite{SR-2020ECCV-MuCAN, SR-2020NIPS-LAPAR}, 
the qualities of the generated high-resolution images or videos are further improved, and richer details are recovered. 
Meanwhile, efficient deployments of these deep learning-based SR models have attracted more and more attention gradually. 

\begin{figure}[tb!]
    \input{figs/runtime-breakdown_2080} \\ \vspace{-.2in}
    \input{figs/runtime-2} 
    \caption{Runtime profiling results of deploying a state-of-the-art super-resolution model with PyTorch, TensorRT, and our accelerator. 
    PyTorch is installed and evaluated only on 2080Ti. Time cost breaks into 3 separate components: (1) dictionary query and filtering step (2) convolution operation; (3) data reformatting, concatenation or other operations. }
    \label{fig:time-breakdown}
\end{figure}

Lots of previous arts have been proposed to deploy different deep learning algorithms on a variety of hardware platforms 
\cite{DNNFPGA-DAC19-Hao, SPEED-2020DAC-GPUSystolic, ASIC-DAC2019-MemoryDSE}. 
The deployed models are mostly for object classification \cite{FPGA-DAC2017-Wei}, detection \cite{SPEED-2020HPCA-QuickNN}, 
neural language processing \cite{SPEED-2019FPGA-LSTM}, and \textit{etc}. 
Although a wide range of application scenarios are covered, the deep learning operators implemented by them are 
so similar to each other that no explicit technique gaps distinguish them. 
Typical operators include direct convolution, fully connected operation, pooling, softmax, and \textit{etc}. 
Due to the regularity of these operators, the commercial tools achieve state-of-the-art performance on these operators 
by using dedicatedly-optimized hardware codes. 
For example, TensorRT \cite{GPU-NVIDIA-TensorRT} outperforms other tools on NVIDIA GPUs 
and Intel MKL-DNN \cite{CPU-Intel-MKL-DNN} has the dominating performance on Intel CPUs. 

Despite the achievements of these traditional model deployments, the complexities and particularities of SR algorithms 
hinder the models from following the optimization strategies of the traditional DNN models to realize 
real-time performance (\textit{i.e.}, more than 25 frames per second), under demanding edge devices. 
Firstly, the algorithmic processing logics of the traditional models and SR models are completely opposite. 
The mainstream DNN models down-sample the inputs to learn the embedded features, 
\textit{e.g.}, VGG, GoogleNet, MobileNet, ResNet, Faster R-CNN, and \textit{etc}. 
The down-sampling characteristic eases the communication and computation pressures on features and weights. 
In contrast, SR models enlarge (up-sample) the inputs continuously to recover more details. 
Much fewer weights are shared by much more features. 
Therefore, the features instead of weights become a crucial influence factor thus making the existing memory optimization techniques powerless. 
Similar phenomenons have been discovered in \cite{SRHW-2019ICCAD-eSRCNN}. 
Secondly, the newly used SR operations, \textit{e.g.}, local pixel-shuffle and dictionary learning, exacerbate these challenges. 
The traditionally widely-used operations, \textit{e.g.}, direct convolutions and pooling, have been solved through various techniques. 
Typical techniques include loop unrolling, tiling, systolic array, and so forth 
\cite{FPGA-DAC2019-Wei, FPGA-ICCAD2019-Sun, SPEED-2020DAC-GPUSystolic}. 
In comparison, as shown in \Cref{fig:time-breakdown}, the novel operations in SR are time-consuming and require special computation re-organizations and parallelisms. 
Due to these challenges, the existing solutions are unsatisfactory, even the state-of-the-art commercial tool, \textit{e.g.}, TensorRT. 

% In this paper, we propose to implement a high-performance SR algorithm accelerator on edge GPUs. 
% To the best of our knowledge, this is the first accelerator that conducts \textit{real-time} SR processing on \textit{edge GPU}. 
% The communication and computation characteristics of layers in SR models are analyzed to find out the bottlenecks. 
% Firstly, the dictionary learning operation is analyzed in detail. 
% Then an equivalent operation of the dictionary learning is proposed which simplify the computations, reduce the communications, 
% and benefits the accelerations at the same. 
% Consequently, both of the communication and computation bottlenecks resulting from the dictionary learning operations are overcome. 
In this paper, several novel techniques are proposed to handle these challenges. % from the design level
Firstly, we propose a fast model sliming strategy for model sliming to handle the large models and dense parameters of SR models. 
Structured pruning was utilized to select and compress the SR dictionaries. 
Only the most important dictionaries are reserved so that the serial computation iterations can be accelerated remarkably without degradations of result qualities. 
% further acceleration for deployment can be realized remarkably. 
% With careful design on the model sliming, time consumption of serial computation iterations can be reduced.
% Secondly, the dictionary learning operation is analyzed in detail. 
% Then an hardware friendly operation of the dictionary learning is proposed which simplify the computations, reduce the communications, 
% and benefits the accelerations at the same time. 
% Consequently, both of the communication and computation bottlenecks resulting from the dictionary learning operations are overcome. 
Secondly, to obtain the optimal hardware implementation given the SR models, a novel constrained-based design searching algorithm is proposed. 
The GPU architecture is analyzed in detail and the resources and computational workloads are considered as the constraints to restrict the candidate of feasible hardware implementations. 
The illegal and non-optimal designs are discarded and a Bayesian optimization-based searching algorithm is proposed to find the optimal design efficiently. 
As a result, the communication latencies are hidden and the bandwidth usage is improved. 
Last but not least, the original large task is re-organized to be smaller sub-tasks and then these sub-tasks are run in parallel. 
Based on these efforts, the overall system parallelism and resource utilization are maximized aggressively 
to ameliorate the computation- and communication-bounded issues. The main contributions of this paper are listed as follows: 
\begin{itemize}
    % \item Dictionary learning algorithms on extremely large data frames are accelerated by a lot via our specifically-designed acceleration engine. 
    % \item Communication issues due to irregular memory visits are solved through re-designing communication and storage patterns. 
    % \item Task re-organization and parallel execution techniques are proposed which can greatly relieve the stress resulting from the large data frames. 
    % \item Compared with the start-of-the-art commercial tool TensorRT on edge GPU NVIDIA Jetson Xavier NX, our method achieves faster and real-time SR processing when deploying LAPAR SR model. 
    \item Dictionary learning algorithms on extremely large data frames are accelerated by a lot for the first time via our specifically designed acceleration engine. 
    \item Model Slimming for SR dictionaries and parallel execution techniques are proposed which can greatly relieve the stress resulting from the large data frames. 
    Both the computation and communication workloads are reduced. 
    \item Resources- and workloads-aware constraints dedicated for GPUs are proposed for the first time to guide the searching of optimal hardware implementations. 
    The optimal design can be achieved in a short time. 
    % \item Both of the system parallelism and communication throughput are maximized, and therefore the challenges resulting from large data frames, 
    % up-sampling operations, and the novel SR algorithms are solved efficiently in this paper. 
    \item Compared with the state-of-the-art tool TensorRT, on edge embedded GPU NVIDIA Jetson Xavier NX and server-level 2080Ti, 
    our method achieves faster and real-time SR processing. Runtime profiling results are shown in \Cref{fig:time-breakdown}. 
\end{itemize}

The remainder of the paper is organized as the following. 
\Cref{sec:pre} recaps the deep super-resolution models, dictionary learning, and the background of GPU programming. 
\Cref{sec:alg} illustrates our acceleration methods in detail. 
\Cref{sec:exp} demonstrates the experiments and results. 
Finally, we conclude this paper in \Cref{sec:conclu}.
