\begin{abstract}

% Here is a very bland Abstract just to get something down
% Though convolutional neural networks have continued to improve their overall performance and accuracy over the last decade, they have become increasingly difficult to explain their decisions. One of the most useful ways of explaining models includes using salience maps to view what regions of an image the model uses to make its prediction. However, it is not practical to manually check the salience maps given the vast amount of data, the choice of which model to use, and which trained model performs the best. In this paper, we propose a variety of metrics to access model salience and give meaningful information about the model's salience explainability. We assess our metrics using both traditional and human-guided models, and show that human-guided models have more human-explainable salience than traditional models.

% New abstract from Adam, to consider/edit:
% Adam DONE *** Dec. 31, 2022 ***
% PT EDITED/DONE *** Feb. 9, 2023 ***
The performance of convolutional neural networks has continued to improve over the last decade. At the same time, as model complexity grows, it becomes increasingly more difficult to explain model decisions. Such explanations may be of critical importance for reliable operation of human-machine pairing setups, or for model selection when the "best'" model among many equally-accurate models must be established. Saliency maps represent one popular way of explaining model decisions by highlighting image regions models deem important when making a prediction. However, examining salience maps at scale is not practical. In this paper, we propose five novel methods of leveraging model salience to explain a model behavior at scale. These methods ask: (a) what is the average entropy for a model's salience maps, (b) how does model salience change when fed out-of-set samples, (c) how closely does model salience follow geometrical transformations, (d) what is the stability of model salience across independent training runs, and (e) how does model salience react to salience-guided image degradations. To assess the proposed measures on a concrete and topical problem, we conducted a series of experiments for the task of synthetic face detection with two types of models: those trained traditionally with cross-entropy loss, and those guided by human salience when training to increase model generalizability. These two types of models are characterized by different, interpretable properties of their salience maps, which allows for the evaluation of the correctness of the proposed measures. We offer source codes for each measure along with this paper.

\end{abstract}