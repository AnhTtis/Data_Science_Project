\section{Proposed Explainability Measures}

% Model explainability in computer vision tasks commonly relies on  visual explanation, such as class activation mappings (CAMs) or gradient-weighted CAMs (Grad-CAMs). These visualization techniques highlight regions of importance, or salience, in a given model's decision-making process. 

Class activation maps (CAMs) or Gradient-Weighted CAMs (Grad-CAMs) are useful when manually checking single images of interest to the model, but fail to capture more general trends in the context a collection of samples across an entire dataset. To remedy this, the measures proposed in this paper more generally assess model salience, relieving operators of the timely task of singular image-by-image observation to understand model behavior at the dataset level. Additionally, the proposed measures allow for any type of salience to be measured and compared so long as they are presented as heatmaps; these include human annotations, eye tracking data, and various types of class activation mappings.

% % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Salience Entropy ($S_{entropy}$)}
\label{Salience Entropy}
% Adam DONE *** Dec. 31, 2022 ***

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{figures/Dataset-figure-resnet.png}
  \caption{Illustration of $S_{entropy}$ explainability measure based on {\it model's salience entropy}. {\bf Top row, from left to right:} real sample (FFHQ-sourced) and four fake samples (generated by StyleGAN, StyleGAN2, StyleGAN2-ADA, and StyleGAN3). {\bf Middle row:} corresponding salience maps (GradCAMs) for ResNet model trained with cross-entropy loss only. Model focus is indicated by a spectrum color map ranging from blue (low focus) to red (high focus). {\bf Bottom row:} salience maps for ResNet model trained in human-guided manner (CYBORG loss).}
  \label{fig:entropy_illustration}
\end{figure}

\begin{figure*}[t]
    \centering
    %\includegraphics[width=0.6\linewidth]{figures/nonsense/random-figure-resnet.pdf}
    \includegraphics[width=0.8\linewidth]{figures/saltpepper-figure-resnet.png}
    \caption{Illustration of the $S_{noise}$ explainability measure assessing model's reaction to noisy inputs. {\bf Top row:} An original input image with increasing intensities of noise, with the leftmost image the original input image with no noise, and the rightmost image an edge case when signal-to-noise ratio equals zero and the model is fed with random uniform noise. Corresponding salience maps for a ResNet model trained traditionally ({\bf middle row}) and with human guidance ({\bf bottom row}) suggest that human-guided training allows for focusing on the actual object for lower signal-to-noise ratios compared to a model trained traditionally. $S_{noise}$ delivers an assessment of this behavior aggregated over an entire dataset.}
    \label{fig:noise_illustration}
\end{figure*}

One of the most straightforward ways of statistically summarizing the model's salience is to study the entropy of generated salience maps. In terms of explainable AI, well-trained models typically have focused salience on the object it is classifying, \eg in synthetic face detection, salience maps should not be randomly-arranged, complex shapes, but should rather be focused around the face region of an image. Fig. \ref{fig:entropy_illustration} illustrates the intuition behind the proposed $S_{entropy}$ measure, in which salience maps are visualized for a few example face images. The model trained in a human-aided way shows more focused salience, which translates to lower $S_{entropy}$ scores, compared to salience maps of the model trained traditionally (without human guidance). High accuracy scores with unfocused salience maps may be indicators that the model has overfit the training data, latching onto extraneous artifacts correlated with class labels during training, but not generalizing well to unseen data.

Formally, we define $S_{entropy}$ as the normalized Shannon's entropy calculated for pixel intensities in the salience map:
%
\begin{align}
S_{entropy} = -\frac{1}{S_{\mathrm{max}}^{\hat p}(m,n)}\sum_i^I {\bf{p}}(x_i)\log_2 {\bf{p}}(x_i),
\end{align}
%
\noindent
where ${\bf{p}}(x_i)$ is the estimated probability of a salience map's $i$-th intensity, $I$ is the total number of salience intensities (\eg Grad-CAM histogram bins), and $S_{\mathrm{max}}^{\hat p}(m,n)$ is the maximum entropy for an $n\times m$ image with pixel depth $\hat p$, namely:
%
\begin{align}
\label{eqn:maxent}\nonumber
S_{\mathrm{max}}^{\hat p}(m,n) &= -\frac{mn}{2^{2\hat p}}\log_2 \left(\frac{mn}{2^{2\hat p}}\right)\times 2^{\hat p} \\
&= -\frac{1}{2^{\hat p}}nm\left(log_2\left( nm\right) - 2\hat p\right),
\end{align}
%
\noindent
where $\hat p=\min(p,\log_2(nm))$, and $p=8$ for 8-bit salience maps, or $p=\infty$ salience maps represented by real numbers. This normalization factor is a consequence of the observation that the entropy for an image of size $m\times n$ and with pixel depth $p$ is maximized when all of the probabilities ${\bf{p}}(x_i)$ are equal.

The normalization factor $S_{\mathrm{max}}^{\hat p}(m,n)$ plays a key role in getting accurate estimates of the entropy in case of low- and varying-resolution and quantized data (such as Grad-CAMs) and requires a commentary. Rather than binning the salience pixel values to determine the entropy, we consider them to represent an un-normalized probability distribution map \cite{cam_entropy2022}, and the entropy calculated is bound by the size of the input salience map, which differs and depends on the model's last convolutional layer's dimensions (\eg $7\times 7$ for DenseNet and ResNet). The model's salience maps are also often up-sampled to the original input image size (\eg $224 \times 224$) for better visualization purposes. Since this operation does not bring any new information, we calculate the entropy of the maps in their original resolution.

For the common case for arrays of float values, where the pixel depth is infinite, the ``effective'' pixel depth is $\log_2( nm))$ and the maximum entropy Eq. (\ref{eqn:maxent}) can be simplified:
%
\begin{align}
S_{\mathrm{max}}^{\infty}(m,n) &= \log_2\left(mn\right).
\end{align}
%
For example, a $7\times 7$ map can only support a pixel depth of $\log_2 7^2$ or 5.614. Therefore, the entropy from each salience map is divided by $S_{\mathrm{max}}^p(m,n) = 5.614$ for this paper's experiments, as DenseNet and ResNet models used in this work have feature map dimensions of $7\times 7$.



\begin{figure}[htb]
    \centering
    %\includegraphics[width=0.6\linewidth]{figures/nonsense/random-figure-resnet.pdf}
    \includegraphics[width=\linewidth]{figures/SaltPepperGraphic.png}
    \caption{Graphic of the $S_{noise}$ explainability measure comparing the salience map of each model with no noise compared to noisy inputs. Each line represents a model's SSIM score (y-axis) of the original salience map compared to the salience map with noise (x-axis).}
    \label{fig:noise_graphic}
\end{figure}


\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/augmentations-figure-resnet-v2.png}
  \caption{Illustration of the $S_{resilience}$ measuring how the model's salience reacts to selected geometrical transformations: shifts ({DR} = downward-right, {R} = rightward, {UR} = upward-right {D} = downward, {U} = upward, {DL} = downward-left, {L} = leftward, {UL} = upward-left), flips ({LR} = left-over-right, UD = upside-down) and rotations (90CC = 90 degrees counterclockwise, 90CW = 90 degrees clockwise).
  }

  %{\bf Top row:} ***. {\bf Middle row:} salience maps for a traditionally-trained ResNet model when fed original input image (first column) and various Euclidean transformations (remaining columns). {\bf Botom row:} same as in the middle row, except that the model was trained in a human-guided manner.
  \label{fig:resilience_illustration}
\end{figure*}


% % % % % % % % % % % % % % % % % % % % % % % % 
% NEEDS HELP: Adam re-wrote this section
% Adam DONE *** Jan. 1, 2023 ***
\subsection{Salience-Assessed Reaction To Noise ($S_{noise}$)}
\label{Salience Randomness}


It is reasonable to say that well-trained models should degrade their performance gracefully in presence of noise. Typical approaches include recording the model's accuracy or confidence scores as a function of degradation strength. What is proposed in this paper, and may complement the existing measures, is to compare the model's salience maps obtained as the noise is being gradually added to the input, with the salience map calculated for clean samples. Models more robust to a specific noise added should preserve the salience for higher amounts of image degradation. Among various image comparison measures, we propose to adopt a perception-based Structural Similarity Index (SSIM) \cite{Wang_TIP_2004}, initially developed to quantify the perception of errors between distorted and a reference images, and later widely applied as a measure of differences between images:
%
\begin{equation}
    \mbox{SSIM}(x_1,x_2) = \frac{(2\mu_1\mu_2+\epsilon_1)(2\sigma_{1,2}+\epsilon_2)}{(\mu^2_1+\mu^2_2+\epsilon_1)(\sigma^2_1+\sigma^2_2+\epsilon_2)}
    \label{eqn:ssim}
\end{equation}
%
\noindent
where $\mu_1$, $\sigma_1$, $\mu_2$, and $\sigma_2$ are mean values and standard deviations of the reference ($x_1$) and distorted ($x_2$) images, respectively, $\sigma_{1,2}$ is a covariance of reference and distorted inputs, and $\epsilon_1$ and $\epsilon_2$ prevent from division by a small denominator. Then:
%
\begin{equation}
    S_{noise} = \frac{1}{N}\sum_{i=1}^N\mbox{SSIM}(c_i,d_i)
\end{equation}
%
\noindent
where $N$ is the number of clean-degraded salience map pairs, the reference image $c$ is the salience map obtained for a clean sample, and the distorted image $d$ is the salience map obtained for the degraded image (after adding the noise). Examples of clean and degraded images with their corresponding salience maps are shown in Fig. \ref{fig:noise_illustration}. Additionally, we can measure how quickly the salience map changes with increasing levels of noise. Though this was not one of our core measures of model explainability, we highlight the potential benefits of these curves and the additional information they might bring depending on the domain, as shown in Fig. \ref{fig:noise_graphic}. 

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{figures/stability-figure-resnet.png}
  \caption{Illustration for the $S_{stability}$ measure assessing the stability of model's salience across training runs. Salience maps for 10 ResNet models trained with cross-entropy loss ({\bf top row}) and with human guidance ({\bf bottom row}) for a given input image shown in the left column.}
  % The CYBORG variation tends to be more stable, and does not show as much variation as cross-entropy.}
  \label{fig:stability_illustration}
\end{figure*}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{figures/degrad-figure-resnet.png}
    \caption{Illustration of the $S_{focus}$ measuring how the model reacts to salience-sourced image degradation. {\bf Top row:} Original image (left) with salient (middle) and non-salient (right) regions ``removed'' by blurring. Corresponding model's salience maps for a ResNet model trained traditionally ({\bf middle row}) and with human guidance ({\bf bottom row}).}
    \label{fig:degradation_illustration}
\end{figure}

% % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Salience Resilience to Geometrical Transformations ($S_{resilience}$)}
\label{Salience Augmentations}
% Adam DONE

The third way we propose to study the model's behavior is to compare the salience maps obtained for a given input image with those obtained for the same image after applying selected geometrical image transformations. The intuition behind this measure is that a model having a good ``understanding'' of the actual (rather than accidentally correlated with class labels) features will follow their geometrical placement in the input data. For this paper, we have three types of transformations: x-y translations, vertical and horizontal flips, and 90-degree rotations, as shown in Fig. \ref{fig:resilience_illustration}. But this measure is certainly not tied to any specific set of transformations, which should be selected appropriately for the task being solved by the model. For instance, in the case of synthetic face detection, trained with 
% there are not many geometry-based image transformations that can be reasonably applied given how the models were trained: on 
centrally cropped, upright face images, inputs that deviate significantly from the training data (such as homography or nonlinear transformations) could ``break'' the model. % As such, the only transformations that could be sensibly applied were left-right mirroring (labeled ``LR'') and x-y translations, both of which maintain the same general face and image structure seen during training.

Formally, we define $S_{resilience}$ for a transformation $T$ as
%
\begin{equation}
    S_{resilience,T} = \frac{1}{N}\sum_{i=1}^N\mbox{SSIM}(o_i,f_T(t_i))
\end{equation}
%
\noindent
where SSIM is defined by Eq. (\ref{eqn:ssim}), $N$ is the number of salience map pairs, the reference image $o_i$ is the salience map obtained for an original sample, the distorted image $t_i$ is the salience map obtained for the transformed image, and $f_T(\cdot)$ denotes the inverse $T$ transformation. More specifically, after each transformation is applied (upper rows in Fig. \ref{fig:resilience_illustration}), a secondary salience map $t_i$ is generated (bottom rows of Fig. \ref{fig:resilience_illustration}). The secondary salience maps $t_i$ are ``corrected'' by reversing the geometrical transformation applied to original image via $f_T(\cdot)$, which are finally compared with maps $o_i$ for original samples using the SSIM measure. 


\subsection{Salience-Based Image Degradation ($S_{focus}$)}
\label{Salience Degradation}
% Adam DONE

The fourth manner in which we measure model's salience-based properties is by comparing salience maps of original input images against images that are degraded based on model salience. In order to produce these latter images, we first generate a salience map for an original image, highlighting where on the image the model focuses during classification. We then use the salience map to blur salient and non-salient regions of the image, producing two new images, one of which has \textit{important} regions blurred out (\textit{salience removal}) and the other has \textit{unimportant} information blurred out (\textit{non-salience removal}). An important note is that we chose to blur regions of salience to remove high frequency information from the image, which allows to not introduce artificial features to the input images. This may happen if we remove information by blackening portions of the image, hence introducing strong image gradients, originally not existing in input samples.

If salient regions of the model are important in the model's decision making process, then removing them from an input image should negatively affect the model's ability to classify the images. Conversely, the removal of non-salient regions of an input image should not significantly affect the model's performance. Accordingly, this metric (in an almost self-referential way) indicates how important model salience is to the model itself. We can then evaluate the model by both observing the selected performance metric (\eg AUROC: Area Under the ROC curve) and SSIM, as defined by Eq. (\ref{eqn:ssim}), post-degradation to the baseline original images.


\subsection{Salience Stability Across Training Runs ($S_{stability}$)}
\label{Salience Stability}

The fifth way to assess model's behaviour is to compare salience maps of independently trained models with the same backbone-training strategy configuration, \ie models that have the same model architecture, loss function, training data, but the training is started with a different seed. This can be helpful to assess, quantitatively, whether a combination of training strategy (including data selection) and model architecture converges to the same salient features. Fig. \ref{fig:stability_illustration} illustrates this concept for $N=10$ independent training runs of the ResNet model, with two different training strategies, as in all previous measures (cross-entropy and human guided). It is clear from this example that human-guided model converges to similar salient features compared to the classically-trained model. Formally, we define $S_{stability}$ as
%
\begin{equation}
    S_{stability} =\frac{1}{K}\sum_{k=1}^K\frac{2}{N(N-1)}\sum_{i>j}\mbox{SSIM}(x_{k,i},x_{k,j})
\end{equation}
%
\noindent
where SSIM is defined by Eq. (\ref{eqn:ssim}), $x_{k,i}$ is the salience map corresponding to $k$-th test sample and $i$-th training run, $i,j = 1,\dots,N$, where $N$ is the number of training runs, and $k=1,\dots,K$, where $K$ is the number of test samples. Values closer to 1.0 denote high $S_{stability}$.

\begin{table*}[htb]
\centering
\caption{Values of four measures averaged across all salience maps generated for the four model architecture-training strategy configurations (rows) and for each dataset (column). Plus/minus one standard deviation of the result is given. StyleGAN is shortened to S-GAN in the table for all synthetic datasets. $S_{entropy}$ approximates the complexity of the salience maps, with lower values indicating more focused salience. $S_{noise}$ offers an aggregated assessment of a difference between clean samples (authentic and synthetic faces) and degraded samples (after adding {\bf salt-and-pepper noise}, as shown in middle column in Fig. \ref{fig:noise_illustration}). $S_{focus}$ is shown separately for both variants when the salient regions are removed (blurred) and non-salient regions (background) are removed. $S_{focus}$ values for {\bf non-salient} removals should be {\bf higher}, as information not necessary to complete the task (synthethic face detection) should not be salient for the model. Conversely, $S_{focus}$ values for {\bf salient} removals should be {\bf lower}, as these salient regions should completely disrupt the models predictive ability.}
\label{tab:results}
\begingroup
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 {\bf Measure} &{\bf Training} & {\bf Model} & {\bf Authentic samples} & {\bf Synthetic samples} & {\bf Total}\\
 &{\bf Type} & & {FFHQ} & S-GAN / S-GAN2 / S-GAN2-ADA / S-GAN3 & {\bf Average} \\ 
 \hline\hline

 %% Sentropy
\multirow{4}{*}{$S_{entropy}$} & Cross- & DenseNet   & 0.885 $\pm$ 0.123 & 0.896 $\pm$ 0.084 & 0.894 $\pm$ 0.093\\\cline{3-6}
 & entropy & ResNet & 0.884 $\pm$ 0.095 & 0.873 $\pm$ 0.074 & 0.875 $\pm$ 0.079\\
  \cline{2-6}
 & Human & DenseNet & {0.837$\pm$ 0.119} & {0.808 $\pm$ 0.108} & 0.813 $\pm$ 0.111 \\\cline{3-6}
 & salience & ResNet & 0.887 $\pm$ 0.065 & 0.852 $\pm$ 0.066 & 0.859 $\pm$ 0.068 \\
 \hline\hline

 %% Snoise
 \multirow{4}{*}{$S_{noise}$} & Cross- & DenseNet & 0.493$\pm$ 0.147 & 0.570 $\pm$0.131 & 0.555$\pm$0.139 \\\cline{3-6}
 & entropy & ResNet & 0.322$\pm$ 0.150 & 0.252 $\pm$0.133 & 0.265$\pm$0.141 \\
  \cline{2-6}
 & Human & DenseNet & 0.457$\pm$ 0.146 & 0.503 $\pm$0.131 & 0.494 $\pm$0.135 \\\cline{3-6}
 & salience & ResNet & 0.456$\pm$ 0.159 & 0.492 $\pm$ 0.139 & 0.485$\pm$ 0.145 \\
 \hline\hline

 %% Sfocus
 &\multicolumn{5}{l|}{{\bf Salience removal}}\\ \cline{2-6}
 \multirow{10}{*}{$S_{focus}$}& Cross- & DenseNet  & 0.570$\pm$0.258 & 0.484 $\pm$ 0.278 & 0.501$\pm$0.278 \\ \cline{3-6}
 & entropy & ResNet & 0.692$\pm$0.185 & 0.700 $\pm$ 0.171 & 0.698$\pm$0.174 \\
  \cline{2-6}
 & Human & DenseNet & 0.534$\pm$0.166 & 0.605$\pm$0.162 & 0.591$\pm$0.165 \\\cline{3-6}
 & salience & ResNet & 0.679$\pm$0.140 & 0.776 $\pm$ 0.106 & 0.757$\pm$0.121 \\
\cline{2-6}
 &\multicolumn{5}{l|}{{\bf Non-salience removal}}\\ \cline{2-6}
 & Cross- & DenseNet & 0.817$\pm$0.137 & 0.857$\pm$0.087 & 0.849$\pm$0.101 \\\cline{3-6}
 & entropy & ResNet & 0.706$\pm$0.142 & 0.703$\pm$0.135 & 0.703$\pm$0.137 \\
\cline{2-6}
 & Human & DenseNet & 0.625$\pm$0.190 & 0.703$\pm$0.145 & 0.687$\pm$0.159 \\\cline{3-6}
 & salience & ResNet & 0.758$\pm$0.121 & 0.786$\pm$0.102 & 0.781$\pm$0.107 \\
 \hline\hline

 %% Sstability
  \multirow{4}{*}{$S_{stability}$}&Cross- & DenseNet & 0.696 $\pm$ 0.191 & 0.760 $\pm$ 0.127 & 0.747 $\pm$ 0.144 \\\cline{3-6}
 & entropy & ResNet & 0.334 $\pm$ 0.152 & 0.406 $\pm$ 0.135 & 0.392 $\pm$ 0.142\\
  \cline{2-6}
 & Human & DenseNet & 0.884 $\pm$ 0.115 & 0.931 $\pm$ 0.061 & 0.922 $\pm$ 0.077 \\\cline{3-6}
 & salience & ResNet & 0.879 $\pm$ 0.129 & 0.941 $\pm$ 0.073 & 0.929 $\pm$ 0.091 \\
 \hline
\end{tabular}
\endgroup
\end{table*}

% % % % % % % % % % % % % % % % % % % % % % % % 





