%PT
\section{Introduction}

% Here's the problem: salience maps are great for explaining decisions but there are simply too many of them

% Given its recent success and widespread adoption, Artificial Intelligence (AI) has seen increased attention paid to human explainability. 
The explanation of AI decisions to humans, while still an emerging area, is becoming a crucial element of neural network training and evaluation.
In domains such as health, law, and finance, the ability to extract semantically meaningful information from a model is oftentimes as important as model performance. An early opinion on the applicability of AI in these domains comes from the UK House of Lords' report \cite{UK_HoL_2018}, which reads: ``We believe it is not acceptable to deploy any artificial intelligence system which could have a substantial impact on an individualâ€™s life, unless it can generate a full and satisfactory explanation for the decisions it will take. In cases such as deep neural networks, where it is not yet possible to generate thorough explanations for the decisions that are made, this may mean delaying their deployment for particular uses until alternative solutions are found.'' This sentiment has spurred the recent quest for methods that offer human-explainable reasoning behind how a model makes its final decision, while still maintaining model performance.

The most straightforward way of judging model explainability is to study \textit{model salience}. In vision-related tasks, model salience is a visualization technique that highlights where a model focuses when making a decision. In principle, this is done by examining  model output sensitivity to local changes made to the input. For black-box models, these changes can be random \cite{Fong_ICCV_2017}, while for white-box models, model gradients change with respect to a given input; this is explained in further detail in Sec. \ref{Model Salience as Explainable AI}). 
%Human salience can also be derived through voluntarily supplied annotations or gaze fixations from an eye tracker. 
These extracted {\it salience maps}, also known as {\it class activation maps}, are useful in explaining which regions of a given image are important in the decision-making process. However, given the sheer volume of image data in vision-related tasks, it is neither practical nor feasible for humans observe every salience map by hand. 
% Computers, however, can employ robust techniques for image processing, meaning they are more properly suited for this tas
% Contributions
In this work, {\bf the proposed five methods distill meaningful information from salience maps to aid humans in making informed decisions in the model selection process}. This is useful in situations where a set of models perform similarly on a given benchmark (see Fig. 1). More specifically, the proposed methods can justify preference in certain models by measuring:

\begin{itemize} 

\item[(a)] the complexity of the salience maps through their entropy; well-focused models should generate lower-entropy salience maps (Sec. \ref{Salience Entropy});

\item[(b)] the model's {\it reaction to nonsense} by observing salience for random inputs; salience maps should (on average) be less focused when the model is fed nonsense, compared to genuine inputs (Sec. \ref{Salience Randomness});

\item[(c)] how model salience changes after applying selected Euclidean transformations to model input; the salience of well-trained models should (on average) follow the same transformations (Sec. \ref{Salience Augmentations});

\item[(d)] how model salience changes when salient and non-salient regions of an input image are degraded; well-trained models should react intuitively when degradations are applied to salient and non-salient image regions; intuition suggests degradations to \textit{salient} regions should translate to large alterations of focus, while degradations to \textit{non-salient} regions should minimally impact model behavior (Sec. \ref{Salience Degradation});

\item[(e)] how model salience changes across independently trained models when sharing the same model architecture, training strategy, and training data; well-trained models should demonstrate high salience stability across multiple training runs, suggesting that -- independently of random initialization -- the models converge toward the same salient information when solving a given visual task (Sec. \ref{Salience Stability}). 

\end{itemize}

To concretize this work, the proposed measures are applied for the biometric task of synthetic face detection, in which several recent Generative Adversarial Networks (GANs) are used to generate images. To further evaluate the proposed measures, two types of models are tested: (a) those trained traditionally (with cross-entropy loss), and (b) those trained in a recently proposed human salience-guided manner, which increases the model's generalization capabilities, prevents from overfitting, and increases the stability of convergence across training seeds \cite{boyd2022human,Boyd_WACV_2023}. The juxtaposition of the measures for each model type adds to understanding their predictive capabilities.
%
% In all of these metrics, we find that human-guided models tend to be more explainable than traditionally trained methods. However, we also observe inconsistent results between model architectures, suggesting that certain models have unique explainable aspects that may favor one context over another.
%
For each method, a performance-related metric (Area Under the Curve: AUC) is reported in addition to an SSIM-based explainability-related measure. The latter aggregates salience information into standalone measures, simplifying the judgment about the model usefulness. Source codes facilitating the replicability of this work, and further use of the proposed measures, are offered along with this paper\footnote{\url{https://anonymous.4open.science/r/Explain2Me-0B19/}}.


% % \begin{enumerate}
%     \vskip1mm\noindent
%     { -- }In \textit{salience stability} (Sec. \ref{Salience Stability}), we measure how model salience changes across independently trained models that share the same model architecture, training strategy, and training data.
%     \vskip1mm\noindent{ -- }In \textit{salience preservation under Euclidean augmentations} (Sec. \ref{Salience Augmentations}), we measure how model salience changes after applying mirroring and x-y translation to an original input image.
%     \vskip1mm\noindent{ -- }In \textit{salience-based image degradation} (Sec. \ref{Salience Degradation}), we measure how model salience changes when salient and non-salient regions of an input image are degraded.
%     \vskip1mm\noindent{ -- }In \textit{salience-based reaction to nonsense} (Sec. \ref{Salience Randomness}), we measure how model salience when models are input two types of random noise: uniform random input and salt-and-pepper noise.
%     \vskip1mm\noindent{ -- }In \textit{salience entropy} (Sec. \ref{Salience Entropy}), we measure how model salience changes in regards to the complexity of the salience maps through entropy, where lower entropy-based salience maps are considered less noisy/more focused.
% % \end{enumerate}


