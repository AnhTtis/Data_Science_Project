\section{Evaluation Results}
\label{sec:results}

\begin{table*}
\centering
\caption{Values of $S_{resilience}$ averaged across all salience maps generated for the four model architecture-training strategy configurations (rows) and for each dataset (column), and averaged across genuine and synthetic face images. Meaning of geometrical transformations: {\bf shifts} ({DR} = downward-right, {R} = rightward, {UR} = upward-right {D} = downward, {U} = upward, {DL} = downward-left, {L} = leftward, {UL} = upward-left), {\bf flips} ({LR} = left-over-right, UD = upside-down) and {\bf rotations} (90CC = 90 degrees counterclockwise, 90CW = 90 degrees clockwise). All transformations are illustrated in Fig. \ref{fig:resilience_illustration}).}
\label{tab:resilience}

\begingroup
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
\small
\begin{tabular}{|c|c|c|c|c|}
\hline
 {\bf Training} & {\bf Model} & {\bf Shifts} & {\bf Flips} & {\bf 90-degree Rotations}\\
 {\bf Type} & & (DR+R+UR+D+U+DL+L+UL)/8 & (LR+TB)/2 & (90CW+90CC)/2\\ 
 \hline\hline
 Cross- & DenseNet & 0.825 & 0.690 & 0.852 \\\cline{2-5}
 entropy & ResNet & 0.609 & 0.491 & 0.814 \\
  \hline
 Human & DenseNet & 0.841 & 0.640 & 0.839 \\\cline{2-5}
 salience & ResNet & 0.716 & 0.600 & 0.851 \\
 \hline
\end{tabular}
\endgroup
\end{table*}

% Redundant section, not sure if we are saying anything new that hasn't been explained previously.
To evaluate the usefulness of the proposed measures, we calculated their values for GradCAM-based salience maps for authentic and synthetic samples, and two types of training: classical cross-entropy-based, and human-aided. Two model architectures (DenseNet and ResNet) were also used. In the case of real images, 4,000 images were sub-sampled from our selected 10,000 FFHQ images. In the case of synthetic images, 1,000 images were similarly sampled from our curated StyleGAN, StyleGAN2, StyleGAN2-ADA, and StyleGAN3 datasets. As such, we ended up with a balanced real-fake dataset with 4,000 samples in each class.


% % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Salience Entropy ($S_{entropy}$)}
% Adam DONE *** Dec. 31, 2022 ***

% Shannon's entropy effectively scores how much uncertainty is stored in a variable: in this case, in a salience heatmap. High entropy scores signal less compact salience that is spread out across the heatmap.

$S_{entropy}$ scores how much model uncertainty is stored in a variable (in our case, in a Grad-CAM salience heatmap). Table \ref{tab:results} results show that, on average, human-guided models have more compact salience maps (lower $S_{entropy}$ scores and much tighter standard deviations) when compared to cross-entropy models. In particular, the DenseNet model trained with the human-guided loss function recorded the smallest $S_{entropy}$ our of all tested models, what indicates that its salience maps tend to have less noise and be more concentrated when compared to other other models. This also resonates with the elevated consistency of Grad-CAM salience maps for human-guided models, as discussed later in the context of salience stability in Sec. \ref{Salience Stability}. 

%% StyleGAN/StyleGAN2/StyleGAN2-ADA/StyleGAN3


% % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Salience-Assessed Reaction to Reaction To Noise ($S_{noise}$)}
% Adam DONE *** Jan. 2, 2022 ***


The values of $S_{noise}$ calculated for two architectures (DenseNet and ResNet) and two types of training (cross-entropy and human-aided) are summarized in Tab. \ref{tab:results}. Values closer to 1.0 denote higher similarity between salience maps for clear and degraded (noisy) samples. This measure allows to conclude interesting properties of the models used in this example. The human-guided training regularizes both models (DenseNet and ResNet) in a way that their salience is equally robust against  added noise (``salt and pepper'' in this case). This can be concluded from similar values of $S_{noise}$ in the last two rows of the $S_{noise}$ section in Tab. \ref{tab:results}. However, training with regular cross-entropy loss produces a model with either large differences between salience maps for clean and noisy samples, or a model that is not affected by the noise.

% All models performed well with the completely random input metric, recording low AUC scores. However, we found that ResNet was slightly higher across both training types. For the salt and pepper input, the results indicate that DenseNet had higher AUC scores and was less resilient than compared to ResNet. The results are summarized in Table 5 and Table 9.

% %%%%%%%%%%% Changed, AUC Curves instead %%%%%%
\begin{figure*}[!htb]
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/DenseNet-Xent.png}
    \caption{DenseNet / Cross-entropy loss}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/DenseNet-Cyborg.png}
    \caption{DenseNet / Human-guided loss}
  \end{subfigure}

  \medskip

  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ResNet-Xent.png}
    \caption{ResNet / Cross-entropy loss}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/ResNet-Cyborg.png}
    \caption{ResNet / Human-guided loss}
  \end{subfigure}
  \caption{Areas Under ROC curves (AUROC) for all four training-architecture configurations, obtained for original samples, after salient region (object-related) removal, and after non-salient region (background) removal.}
  \label{fig:AUROC}
\end{figure*}


% % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Salience Resilience to Geometrical Transformations ($S_{resilience}$)}
% Adam DONE *** Jan. 9, 2023 ***

% \begin{table*}[htb]
% \centering
% \caption{Values of $S_{resilience}$ averaged across all salience maps generated for the four model architecture-training strategy configurations (rows) and for each dataset (column), and averaged across genuine and synthetic face images. Meaning of geometrical transformations: {\bf shifts} ({DR} = downward-right, {R} = rightward, {UR} = upward-right {D} = downward, {U} = upward, {DL} = downward-left, {L} = leftward, {UL} = upward-left), {\bf flips} ({LR} = left-over-right, UD = upside-down) and {\bf rotations} (90CC = 90 degrees counterclockwise, 90CW = 90 degrees clockwise). All transformations are illustrated in Fig. \ref{fig:resilience_illustration}).}
% \label{tab:resilience}

% \begingroup
% \setlength{\tabcolsep}{4pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1} % Default value: 1
% \small
% \begin{tabular}{|c|c|c|c|c|}
% \hline
%  {\bf Training} & {\bf Model} & {\bf Shifts} & {\bf Flips} & {\bf 90-degree Rotations}\\
%  {\bf Type} & & (DR+R+UR+D+U+DL+L+UL)/8 & (LR+TB)/2 & (90CW+90CC)/2\\ 
%  \hline\hline
%  Cross- & DenseNet & 0.825 & 0.690 & 0.852 \\\cline{2-5}
%  entropy & ResNet & 0.609 & 0.491 & 0.814 \\
%   \hline
%  Human & DenseNet & 0.841 & 0.640 & 0.839 \\\cline{2-5}
%  salience & ResNet & 0.716 & 0.600 & 0.851 \\
%  \hline
% \end{tabular}
% \endgroup
% \end{table*}

Table \ref{tab:resilience} shows values of $S_{resilience}$ for the four model architecture-training strategy configurations. To simplify this table, we present values averaged within each group of geometrical transformations (shifts, flips and rotations). It is interesting to see that one architecture (DenseNet) obtains similar $S_{resilience}$ in corresponding categories of transformations independently of the training strategy (regular cross-entropy or human saliency-based). This is not the case for another model (ResNet), which better follows the placement of salient features when the model is trained with human-guided manner. This demonstrates how $S_{resilience}$ can be utilized. Assuming that all four model architecture-training configurations demonstrate similar performance, $S_{resilience}$ suggests that DenseNet should better preserve the model's salience under selected geometrical transformations.

% We hypothesize that the reason for this similarity is the tendency of cross-entropy-trained models to have more spread out model salience than that of human-perception-trained models. 

% For salience preservation, DenseNet-Xent performs the best for all datasets in terms of mean SSIM when all transformations are considered together (Table \ref{augs1}). Interestingly, while DenseNet-Xent shows the highest mean SSIM, ResNet-Xent shows the lowest, with the two CYBORG models in the middle in terms of mean SSIM. This suggests that models trained with cross-entropy loss are more subject to variation when dealing with images that have been geometrically transformed. Evidence of this is also seen in the higher standard deviations in SSIM for Xent models (compared to CYB models) in Table \ref{augs1}. 

% Table \ref{augs2} breaks down SSIM scores (mean and standard deviation) by Euclidean transformation. DenseNet-Xent still shows higher mean SSIM than all other models for five of the nine total transformations. These five transformation include the four x-y shifts that push the face to the corner of the image (DR, UR, DL, UL).  (as discussed later in this section). 

% The findings in Table \ref{augs2} suggest that Xent models might find more similar salient regions when unseen images deviate significantly from images seen during training, whereas CYBORG models are more prone to salience preservation among images that are geometrically closer to those seen during training.

\begin{figure*}[htb]
  \begin{subfigure}[b]{1\textwidth}
      \begin{subfigure}[b]{0.45\textwidth}
          \centering
            \includegraphics[width=1\columnwidth]{figures/DenseNet.png}
          \caption{DenseNet}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.45\textwidth}
          \centering
          \includegraphics[width=1\columnwidth]{figures/ResNet.png}
          \caption{ResNet}
      \end{subfigure}
  \end{subfigure}\vskip3mm
  \caption{Graphical representation of the models (DenseNet and ResNet) and training types (Xent and CYBORG) using all of the proposed explainability measures in this paper. All measures are already normalized between 0 and 1, but in the case of $S_{focus}$, Salience removal was reversed (1 - $S_{focus}$) and averaged with Non-salience removal to reflect the appropriate polarity. $S_{entropy}$ was also reversed (1 - $S_{entropy}$) since lower values of $S_{entropy}$ reflect more focused model salience. Both DenseNet models perform similarly for $S_{noise}$ and $S_{resilience}$, but different for $S_{entropy}$, $S_{stability}$, and $S_{focus}$. On the contrary, both ResNet models were competitive $S_{entropy}$, $S_{resilience}$ and $S_{focus}$, but had large differences for $S_{stability}$ and $S_{noise}$. Our proposed measures indicate the various strengths and weaknesses of each model, allowing for more meaningful and human-explainable information to be derived than purely performance metrics.}
  \label{fig:pentagon_vis}
  \null\vskip-5mm
\end{figure*}


% % % % % % % % % % % % % % % % % % % % % % % %
\subsection{Salience-Based Image Degradation ($S_{focus}$)}
% Adam DONE

Salience-based input sample degradation can be first examined through the lens of the performance. Fig. \ref{fig:AUROC} shows ROC curves (with corresponding Areas Under the Curve value) for the four combinations of model and training strategy used in this paper. In general, we should expect a large decrease in accuracy when the salient regions are removed, and slightly lower performance when non-salient regions are removed, depending on the role the background plays in a given task. 

Interestingly, such behavior is seen for one model (DenseNet, Fig. \ref{fig:AUROC} (a) and (b)), but not for the other (ResNet, Fig. \ref{fig:AUROC} (c) and (d)). While DenseNet suffers significantly from blurring the salient regions (AUROC drops from 0.68 to 0.44 and 0.52, for cross-entropy and human-aided losses, respectively), ResNet is less impacted by the removal of salient information, and the performance of model trained with human-aided loss is on par for original and modified input samples. 

This is where $S_{focus}$ measure can offer explanation related to model's salient regions observed before and after image modifications. Tab. \ref{tab:results} quantifies how the model's salience changed for degraded samples. From that table we see that similarity between the original salience map and the new one obtained after removal of salient parts of the images, is lower on average for DenseNet that for ResNet. It means that the former model (DenseNet) focused on significantly different parts of the images after degradation, what apparently for a problem at hand was not a good strategy. Conversely, the latter model (ResNet) was more robust against salience removal, with the maximum $S_{focus}$ obtained for human-aided training. When, instead of model-wise analyses, we look at the training strategies, we see -- according to the expectations -- that values of $S_{focus}$ are higher in general (independently of salient or non-salient information removal) for models trained with the use of human perception.

% Old text:
% Table \ref{tab:salience_degradations} indicates that CYBORG models were more focused when salient regions were removed from the image (\textit{salience removal}). Conversely, Xent models often selected new regions of the image. 
% For this reason, we can see a vast difference between salient and non-salient blurring. 
% For salient blurring, DenseNet-Xent recorded the lowest average SSIM compared to the original salience map. This indicates that Xent models changed their mind after the salience blurring. On the other hand, one CYBORG model recorded a high SSIM score (0.757), indicating that it was more resistant to the blurring. 
% For the non-salient degradation, DenseNet-Xent performed the best, recording the highest average SSIM score (0.849). This indicates that the model was most resistant to the removal of low salient features. We acknowledge that this metric is dependent upon the datasets and may require more analysis.
% Table 7 and 8 summarize the results for salience and non-salience degradation.


% % % % % % % % % % % % % % % % % % % % % % % % 
\subsection{Salience Stability Across Training Runs ($S_{stability}$)}

Finally, to evaluate the usefulness of the $S_{stability}$ measure, for each architecture-training variant, ten models were independently trained. Salience maps were then generated for all images, both real and fake, rendering a collection of ten salience maps for each image (one for each trained model), as illustrated earlier in Fig. \ref{fig:stability_illustration}. 

% The mean SSIM between these 10 heatmaps was calculated for each image. Consequently, these means were averaged across all images. Qualitative results for DenseNet models (cross-entropy and CYBORG) are presented in Fig. \ref{fig:figure2}, and quantitative results can be seen for all four backbone-training strategies in Table \ref{tab:table1}. 

The quantitative results are summarized in Table. \ref{tab:results}. From the higher $S_{stability}$ values it is clear that the human-guided training ends up with models vastly outperforming the cross-entropy-trained models across real and synthetic data sets in terms of salience stability. For ten independently trained models that share the same architecture and training data, models guided by human perception during training have more similar salience maps across all training runs (for the same test samples). In contrast, cross-entropy-trained models tend to have great variation in model salience. This is further echoed by the smaller standard deviations of $S_{stability}$ for human-guided models compared to models trained classically. This aligns well with one of the goals of human-aided training, which was to make the model more agnostic to non-salient features accidentally correlated with class labels found during the stochastic training process. And shows that this measure can be also used to assess the effectiveness of the training process.

% The ResNet-CYBORG models performed most consistently on four out of five datasets, showing a mean SSIM of 0.929 across all datasets. DenseNet-CYBORG, however, recorded the highest mean SSIM across all data sets (0.936). The two Xent models performed poorly, with ResNet-Xent performing the worst, recording 0.391.

\subsection{Comparing Models Graphically}
Fig. \ref{fig:pentagon_vis} shows graphically the evaluation of the models (DenseNet and ResNet) and training types (Xent and CYBORG) using all of the proposed explainability measures in this paper. All measures are normalized between 0 and 1, and in the case of $S_{focus}$, Salience removal was reversed (1 - $S_{focus}$) and averaged with Non-salience removal to reflect the appropriate polarity. $S_{entropy}$ was also reversed (1 - $S_{entropy}$ since lower values of $S_{entropy}$ reflect more focused model salience. These graphs can be used to explore model strengths and weaknesses across the five explainability measures developed in this paper. Though the DenseNet models have the same performance (0.68), they differ in explainability measures $S_{stability}$ and $S_{focus}$. Conversely, the ResNet Xent and CYBORG models have wildly different performance (0.54 and 0.63, respectively), they achieve similar explainability for measures $S_{resilience}$ and $S_{focus}$.


%% USE THE BELOW ONE
% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/ResNet.png}
%     \caption{ResNet graphically}
%     \label{fig:degradation_illustration}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{figures/DenseNet.png}
%     \caption{DenseNet graphically.}
%     \label{fig:degradation_illustration}
% \end{figure}

