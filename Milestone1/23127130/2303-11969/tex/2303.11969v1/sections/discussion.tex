\section{Discussion}

In this paper, we broaden the utility of model salience by introducing a variety of methods to assess their role in model performance and their human explainability. The set of five introduced measures, all solely based on model's salience, includes assessment of: (a) model's salience entropy (to understand a general model's spatial selectivity of features), (b) reaction to noise (to understand the model's focus as a function of input degradation level), (c) resilience to geometrical transformations (to understand whether the spatial location of salient features follows the transformations applied to the input), (d) how the removal of important and background features impacts the new salience generated by the model, and finally (e) what is the impact of different random seeds on the shape of the salience. A core contribution of this work is that the proposed measures offer single-number quantitative assessments, which can be generated for arbitrarily-large test datasets, and thus complement usually low-at-scale qualitative assessments utilizing class activation maps.

To demonstrate how the proposed measures work in practice, we performed our metrics on two popular deep CNN architectures (DenseNet and ResNet), trained with two different strategies (using classical cross-entropy loss, and incorporating human perceptual intelligence into training). Since these four architecture-training combinations produce models behaving differently in terms of their salience, that was a convenient way to assess the usefulness of the proposed measures. It is, however, important to note that the proposed salience-based assessment ideas are not specific to these variants. Also, the types of input deformations applied in this work (\eg random noise in $S_{noise}$, or particular geometrical transformations in $S_{resilience}$, or the number of training runs in $S_{stability}$) are only illustrative examples, suitable for synthetic face detection problem. Other problems may benefit from other deformations (\eg Gaussian or salt and pepper noise can be replaced with synthetically-generated atmospheric turbulence patterns in long-range face recognition problem) and successfully apply the proposed explainability measures. To facilitate the application of this work, the source codes are offered along with this paper.

%\section*{Acknowledgments}
%Research presented in this paper was supported by the U.S. Department of Defense (Contract No. W52P1J2093009). The views and conclusions contained in this paper are those of the authors and should not be interpreted as represent- ing the official policies, either expressed or implied, of the U.S. Department of Defense or the U.S. Government. The U.S. Government is authorized to reproduce and dis- tribute reprints for Government purposes, notwithstanding any copyright notation here on.

% evaluate how human-guided models differ from tradition training types in model salience. Our work shows that human-guided models have more explainable model salience than tradition models when trained on the exact same training data. 

%% Feel free to edit at you see fit (Tim K):
%\section{Explainable AI}
%The overall goal of this work is to examine salience maps as a way to assist humans in understanding how the model is making its distinctions. Because the salience maps here are constructed, in part, by humans, the hope is that this approach offers a necessary condition for an explainable AI system in that it offers accurate and meaningful explanations within the limits that the salience maps can provide \cite{Phillips2021}. A similar technique has been used to include human input in digit identification, but this procedure creates additional measures and applies human input to improve on artificial face detection algorithms, with a goal of extending it to other tasks \cite{Gnjatovic2020}.

%It is imperative to address the groups that are most likely to benefit from this type of approach to bound expectations. This particular procedure is a interest to developers, operators affected by model decisions, as well as potential domain experts \cite{AlejandroBarredo2020}. A recognizable salience map helps ensure developers that the system is operating efficiently and there is fidelity between algorithmic goals and outputs. This in turn assists those affected by the final decisions because domain experts can step through the reasoning in the task. All of this helps establish trust in the system, despite uncertainties \cite{Broderick2021}.

%\section{Limitations}
%There are risks associated with various explainability approaches. In general, there is a danger than human operators will be more likely to follow any decision a system makes if there is any plausible explanation, regardless of accuracy of prediction or explanation \cite{Jacobs2021,Bansal2021}. The use of salience maps as explainability can appear to offer correct interpretations, but, in fact, are highly dependent on data or model construction \cite{Adebayo2018,Kindermans2017}. This can potentially increase the target surface area of attacks on the explainability of the system, but other explainability approaches are not immune to such perceived reasonable explanations \cite{Slack2020}.
