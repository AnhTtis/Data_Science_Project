\section{Evaluation Environment}
\subsection{Datasets} 
\label{Datasets}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/e2me_samples.png}
    \caption{Two examples of face images from each dataset. Columns from left to right: FFHQ, StyleGAN, StyleGAN2, StyleGAN2-ADA, and StyleGAN3.}
    \label{fig:data}
\end{figure}

Two types of datasets were used to demonstrate the efficacy of our proposed explainability metrics: authentic (real) face images pulled from the FFHQ dataset, and synthetic (fake) face images produced by the StyleGAN family of generators. Each generative model was pre-trained on the FFHQ dataset and supplied as part of the official repositories.

\paragraph{Flickr-Faces-HQ (FFHQ)}~\cite{karras2019style} consists of 70,000 high resolution face images collected by NVIDIA via web crawler. The images vary in race, age, background scene, face pose, and fashion accessories. In this work, we randomly selected 10,000 images from the full dataset to create our subset.

\paragraph{StyleGAN Generators} currently consist of four available model types: StyleGAN~\cite{karras2019style}, StyleGAN2~\cite{StyleGAN2}, StyleGAN2-ADA~\cite{Karras2020ada}, and StyleGAN3~\cite{StyleGAN3}. With each official repository, the authors at NVIDIA supply a generator model pre-trained on the FFHQ face dataset. For each model, we produced 10,000 face images with random seeds. In terms of face quality, StyleGAN2, StyleGAN2-ADA, and StyleGAN3 versions supersede the original StyleGAN; StyleGAN was susceptible to ``water bubble'' and edge-based artifacts. 
However, across all iterations of the StyleGAN technology, there exist abnormal artifacts of the synthesis process. These may include disappearing shoulders and misshapen ears (StyleGAN1 - top, bottom), glasses embedded in facial tissue and melted background people (StyleGAN2 - top, bottom), structurally impossible jewelry and unseen head wear accessories (StyleGAN2-ADA - top, bottom), and off-center teeth (StyleGAN3 - top, bottom). Examples of these synthetic artifacts can be seen in Fig.\ref{fig:data}.

\subsection{Model Architectures}

We evaluate our proposed explainability metrics using two popular backbones for deep neural networks: DenseNet121~\cite{huang2017densely} and ResNet50~\cite{ResNet}. These two backbones were chosen primarily due to the fact that they are both available with cross-entropy loss and CYBORG loss (human salience-guided).

\subsection{Training Strategies} 

For each of the two network architectures (DenseNet and ResNet), models are trained using two different loss functions during training: cross-entropy loss and CYBORG loss~\cite{Boyd_2023_WACV_CYBORG}. As proposed by Boyd \etal, CYBORG improves model generalizability by incorporating human-annotated salience into the model's loss function in order to guide the model towards human-judged regions of importance. This is done by juxtaposing the model's salience (Class Activation Mappings) and collected human annotations. The training strategy is particularly relevant in the evaluation of our metrics since we can directly compare how model salience changes across the two loss functions: human-guided (CYBORG or ``CYB'') loss and traditional (cross-entropy or ``Xent'') loss. It is important to note that the models used in this paper are not directly comparable to the original CYBORG paper because only a subset of the test set is used.

\subsection{Model Salience Estimation} 

For our experiments, we use one of the popular methods of evaluating model salience: Gradient-Weighted Class Activation Mappings (Grad-CAMs)~\cite{selvaraju2017grad}, as discussed previously in Section 2.2. %Grad-CAMs use the gradients of the last convolutional layer with respect to the input activations to produce a heatmap that highlights regions of importance to the model. This heatmap is frequently used as a visualization tool to show which portion of the input image were most salient for the model's decision. Without loss of generality, we use Grad-CAMs as our method for judging model salience throughout this work. % probably don't need to repeat explaining Grad-CAMs once more

% There are other ways of assessing model salience either through gradients or activations, but GradCAMs have proved to be the most popular in recent years and are what we select in this work. Future work includes exploring how salience changes through different salience methods.