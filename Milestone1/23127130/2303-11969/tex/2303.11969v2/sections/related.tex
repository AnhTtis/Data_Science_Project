\section{Related Work}
\subsection{Explainable Biometrics}

% TODO: reference Anil Jain's xAI-biometrics paper: \cite{Jain_TBIOM_2022}

Given the prevalence of deep learning-based models in research and practice, the ability to explain a given model's predictive behavior has seen increased emphasis~\cite{ angelov2021explainable, dovsilovic2018explainable, tjoa2020survey}. The common thread among {\it explainable AI} (XAI) research is the goal of understanding how and why models render their ultimate decision. There are currently many post hoc methods to explain models that were trained with model performance in mind rather than human explainability. Two of these methods are the well-known LIME~\cite{lime} and SHAP~\cite{lundberg2017unified} techniques. Goldstein \etal~\cite{goldstein2015peeking} and Molnar \etal~\cite{molnar2020interpretable} also offer partial dependence plots, accumulated local effect (ALE) plots, and individual conditional expectation (ICE) plots towards the common goal of model explainability. Yellowbrick~\cite{bengfort_yellowbrick_2018} and the What-If Toolkit~\cite{wexler2019if} also provide built-in statistical and visualization methods for XAI in scikit-learn and TensorFlow, respectively.

Beyond retrospective post hoc methods, XAI has been enforced proactively in the model building and training process as well as in the documentation. Fact sheets, as seen in ~\cite{arnold2019factsheets, richards2020methodology} serve as an explanatory guide to (i) developers when training models, (ii) dataset curators when releasing public data, and (iii) end users when using the models for their own use. In this way, no models are left unturned; new models can be developed and trained with an explicit emphasis on explainability, and existing models can be assessed and retrofitted to assure explainable behavior. 
% Additionally, the degree of trust or trustworthiness (which extends XAI) that humans place in AI models.

\subsection{Model Salience as Explainable AI} \label{Model Salience as Explainable AI}

\paragraph{Salience Map Methods} Model salience has been used extensively in the XAI community to help interpret why models make their decisions. There are many different ways of assessing model salience, such as through neuron activations ~\cite{score_cam, IS_cam, activation_based_cam, SS_cam}, or through the gradients~\cite{layer_cam, axiom_cam, smooth_cam, selvaraju2017grad, chattopadhay2018grad}. One of the most popular salience methods is Gradient-Weighted Class Activation Mappings (Grad-CAMs), which visualizes the gradients of the predicted class at the final convolutional layer. Grad-CAMs are a more sophisticated method of evaluating model salience since they are applicable to CNNs with fully-connected layers and various architecture designs \cite{selvaraju2017grad}. Salience maps are created at the corresponding dimensions of the convolutional feature map (7x7), and are then up-scaled to the original input size (224x224) for ease of viewing.


% moved from the introduction -- to be edited, and/or partially moved to the methods section 
Further, salience maps can vary in two main regards: (i) to the input data, and (ii) to the map construction method. Naturally, salience maps vary substantially across differing datasets. For example, salience maps generated for frontal face images for face detection will look different when compared to those generated for general object detection. This is due to the fact that model salience is explicitly derived from supplied input images, which differ between datasets. Similarly, human salience also varies in response to the input data. Humans are familiar with identifying both faces and objects in images, but the regions of interest will vary depending on which type of image is presented to a human annotator. 

In the context of constructing salience maps, there are many contributing factors, such as model architecture, how the model is trained (the loss function), which layer of the model the salience is taken from, etc. For human salience, the construction of salience differs dramatically across human annotators, especially in tasks where some are experts while others are not, such as face recognition ~\cite{noyes2017super}. Additionally, human salience can be constructed in different manners: through written annotations (text), drawn annotations, and gaze fixations on an image via eye tracker.

% salience maps for ``poorly''-trained models tend to be noisy and unfocused while ``well''-trained models will be more focused. 
% Nonetheless, there currently doesn't exist a meaningful way to measure salience given the variety of different ways it can change. Therefore, we introduce a variety of different metrics to provide more meaningful information from salience that can be used to explain models more than pure performance metrics. 

\paragraph{Salience Map Uses} Salience maps are most commonly used to ``hand check'' the model and decide if the model is generalizing appropriately~\cite{selvaraju2017grad, ramaswamy2020ablation, chattopadhay2018grad}. Human examiners can then decide whether or not the salient regions of the model follow conventional feature detection. For example, in the case for real or synthetic face detection, the model is expected to focus on regions related to the face, such as the eyes, nose and mouth. Models that focus on regions similar to humans are more easily understood by humans. If a model has high accuracy but its salience maps focus on irrelevant features, there is a chance that the model has focused on accidental information in the training data and may have trouble generalizing to unseen samples. In this case, the model may be deficient at the task at hand and likely should not be deployed. However, finding irrelevant features is often difficult for humans on certain tasks and can be entirely dataset-dependent.

Recent work has also combined salience maps generated by models and human annotators to improve model performance and explainability.
Boyd \etal features CYBORG, which uses human annotations on faces to penalize the loss function during training~\cite{Boyd_2023_WACV_CYBORG}. Not only did this model outperform standard cross-entropy models, but it even surpassed cross-entropy models trained on seven times the training data. Aswal \etal~\cite{SaliencyMapsWithoutSacrificingAccuracy} used data augmentation techniques and transformations to compare model salience to human annotations while maintaining similar levels of performance accuracy. In \cite{DeepGaze}, Lindardos \etal used model salience to help generalize models and improve overall performance of salience prediction and out-of-domain transfer learning. Human-aided salience maps have also shown to improve the generalizability of neural networks~\cite{boyd2022human}. The large-scale effort to improve salience maps, understand their decisions, and combine them with human intuition motivates the need for a rigorous assessment of these maps.


\begin{comment}
\subsection{Human-Guided ML} 

One approach to make models more human explainable is to use humans to help guide the model. There are a variety of different approaches that attempt to improve model generalizability, explainability, and overall accuracy. Recent work by Boyd \etal features CYBORG, which uses human annotations on faces to penalize the loss function during training~\cite{boyd2022human}. Not only did this model out perform standard cross-entropy models, but it even surpassed cross-entropy models trained on seven times the training data. Aswal \etal~\cite{salienceMapsWithoutSacrificingAccuracy} used data augmentation techniques and transformations to compare model salience to human annotations while maintaining similar levels of performance accuracy. In \cite{DeepGaze}, Lindardos \etal used model salience to help generalize models and improve overall performance of salience prediction and out-of-domain transfer learning. Human-aided salience maps have also shown to improve the generalizability of neural networks~\cite{HumanAidedsalienceMaps}. 

%check this last one out, not sure if it completely fits with what we're doing.

\end{comment}



% % % % % % % % % % % % % PRE TK VERSION BELOW



% \section{Related Work}
% \subsection{Explainable Biometrics}

% % TODO: reference Anil Jain's xAI-biometrics paper: \cite{Jain_TBIOM_2022}

% Given the prevalence of deep learning-based models in research and practice, the ability to explain a given model's predictive behavior has seen increased emphasis~\cite{ angelov2021explainable, dovsilovic2018explainable, tjoa2020survey}. The common thread among {\it explainable AI} (XAI) research is the goal of understanding how and why models render their ultimate decision. There are currently many post hoc methods to explain models that were trained with model performance in mind rather than human explainability. Two of these methods are the well-known LIME~\cite{lime} and SHAP~\cite{lundberg2017unified} techniques. Goldstein \etal~\cite{goldstein2015peeking} and Molnar \etal~\cite{molnar2020interpretable} also offer partial dependence plots, accumulated local effect (ALE) plots, and individual conditional expectation (ICE) plots towards the common goal of model explainability. Yellowbrick~\cite{bengfort_yellowbrick_2018} and the What-If Toolkit~\cite{wexler2019if} also provide built-in statistical and visualization methods for XAI in scikit-learn and TensorFlow, respectively.

% Beyond retrospective post hoc methods, XAI has been enforced proactively in the model building and training process as well as in the documentation. Fact sheets, as seen in ~\cite{arnold2019factsheets, richards2020methodology} serve as an explanatory guide to (i) developers when training models, (ii) dataset curators when releasing public data, and (iii) end users when using the models for their own use. In this way, no models are left unturned; new models can be developed and trained with an explicit emphasis on explainability, and existing models can be assessed and retrofitted to assure explainable behavior. 
% % Additionally, the degree of trust or trustworthiness (which extends XAI) that humans place in AI models.

% \subsection{Model Salience as Explainable AI} \label{Model Salience as Explainable AI}

% \paragraph{Salience Map Methods} Model salience has been used extensively in the XAI community to help interpret why models make their decisions. There are many different ways of assessing model salience, such as through neuron activations ~\cite{score_cam, IS_cam, activation_based_cam, SS_cam}, or through the gradients~\cite{layer_cam, axiom_cam, smooth_cam, GradCam, cam_plusplus}. One of the most popular salience methods is Gradient-Weighted Class Activation Mappings (Grad-CAMs), which visualizes the gradients of the predicted class at the final convolutional layer. Grad-CAMs are a more sophisticated method of evaluating model salience since they are applicable to CNNs with fully-connected layers and various architecture designs \cite{selvaraju2017grad}. Salience maps are created at the corresponding dimensions of the convolutional feature map (7x7), and are then up-scaled to the original input size (224x224) for ease of viewing.


% % moved from the introduction -- to be edited, and/or partially moved to the methods section 
% Further, salience maps can vary in two main regards: (i) to the input data, and (ii) to the map construction method. Naturally, salience maps vary substantially across differing datasets. For example, salience maps generated for frontal face images for face detection will look different when compared to those generated for general object detection. This is due to the fact that model salience is explicitly derived from supplied input images, which differ between datasets. Similarly, human salience also varies in response to the input data. Humans are familiar with identifying both faces and objects in images, but the regions of interest will vary depending on which type of image is presented to a human annotator. 

% In the context of constructing salience maps, there are many contributing factors, such as model architecture, how the model is trained (the loss function), which layer of the model the salience is taken from, etc. For human salience, the construction of salience differs dramatically across human annotators, especially in tasks where some are experts while others are not, such as face recognition ~\cite{noyes2017super}. Additionally, human salience can be constructed in different manners: through written annotations (text), drawn annotations, and gaze fixations on an image via eye tracker.

% % salience maps for ``poorly''-trained models tend to be noisy and unfocused while ``well''-trained models will be more focused. 
% % Nonetheless, there currently doesn't exist a meaningful way to measure salience given the variety of different ways it can change. Therefore, we introduce a variety of different metrics to provide more meaningful information from salience that can be used to explain models more than pure performance metrics. 

% \paragraph{Salience Map Uses} Salience maps are most commonly used to ``hand check'' the model and decide if the model is generalizing appropriately~\cite{GradCam, ramaswamy2020ablation, chattopadhay2018grad}. Human examiners can then decide whether or not the salient regions of the model follow conventional feature detection. For example, in the case for real or synthetic face detection, the model is expected to focus on regions related to the face, such as the eyes, nose and mouth. Models that focus on regions similar to humans are more easily understood by humans. If a model has high accuracy but its salience maps focus on irrelevant features, there is a chance that the model has focused on accidental information in the training data and may have trouble generalizing to unseen samples. In this case, the model may be deficient at the task at hand and likely should not be deployed. However, finding irrelevant features is often difficult for humans on certain tasks and can be entirely dataset-dependent.

% Recent work has also combined salience maps generated by models and human annotators to improve model performance and explainability.
% Boyd \etal features CYBORG, which uses human annotations on faces to penalize the loss function during training~\cite{boyd2022human}. Not only did this model outperform standard cross-entropy models, but it even surpassed cross-entropy models trained on seven times the training data. Aswal \etal~\cite{SaliencyMapsWithoutSacrificingAccuracy} used data augmentation techniques and transformations to compare model salience to human annotations while maintaining similar levels of performance accuracy. In \cite{DeepGaze}, Lindardos \etal used model salience to help generalize models and improve overall performance of salience prediction and out-of-domain transfer learning. Human-aided salience maps have also shown to improve the generalizability of neural networks~\cite{HumanAidedSaliencyMaps}. The large-scale effort to improve salience maps, understand their decisions, and combine them with human intuition motivates the need for a rigorous assessment of these maps.


% \begin{comment}
% \subsection{Human-Guided ML} 

% One approach to make models more human explainable is to use humans to help guide the model. There are a variety of different approaches that attempt to improve model generalizability, explainability, and overall accuracy. Recent work by Boyd \etal features CYBORG, which uses human annotations on faces to penalize the loss function during training~\cite{boyd2022human}. Not only did this model out perform standard cross-entropy models, but it even surpassed cross-entropy models trained on seven times the training data. Aswal \etal~\cite{salienceMapsWithoutSacrificingAccuracy} used data augmentation techniques and transformations to compare model salience to human annotations while maintaining similar levels of performance accuracy. In \cite{DeepGaze}, Lindardos \etal used model salience to help generalize models and improve overall performance of salience prediction and out-of-domain transfer learning. Human-aided salience maps have also shown to improve the generalizability of neural networks~\cite{HumanAidedsalienceMaps}. 

% %check this last one out, not sure if it completely fits with what we're doing.

% \end{comment}