
@inproceedings{dovsilovic2018explainable,
  title={Explainable artificial intelligence: A survey},
  author={Do{\v{s}}ilovi{\'c}, Filip Karlo and Br{\v{c}}i{\'c}, Mario and Hlupi{\'c}, Nikica},
  booktitle={2018 41st International convention on information and communication technology, electronics and microelectronics (MIPRO)},
  pages={0210--0215},
  year={2018},
  organization={IEEE}
}

@article{tjoa2020survey,
  title={A survey on explainable artificial intelligence (xai): Toward medical xai},
  author={Tjoa, Erico and Guan, Cuntai},
  journal={IEEE transactions on neural networks and learning systems},
  volume={32},
  number={11},
  pages={4793--4813},
  year={2020},
  publisher={IEEE}
}
@article{angelov2021explainable,
  title={Explainable artificial intelligence: an analytical review},
  author={Angelov, Plamen P and Soares, Eduardo A and Jiang, Richard and Arnold, Nicholas I and Atkinson, Peter M},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={11},
  number={5},
  pages={e1424},
  year={2021},
  publisher={Wiley Online Library}
}
@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{lime,
  author    = {Marco Tulio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
               Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
               13-17, 2016},
  pages     = {1135--1144},
  year      = {2016},
}
@article{goldstein2015peeking,
  title={Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation},
  author={Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal={journal of Computational and Graphical Statistics},
  volume={24},
  number={1},
  pages={44--65},
  year={2015},
  publisher={Taylor \& Francis}
}
@book{molnar2020interpretable,
  title={Interpretable machine learning},
  author={Molnar, Christoph},
  year={2020},
  publisher={Lulu. com}
}
@article{wexler2019if,
  title={The what-if tool: Interactive probing of machine learning models},
  author={Wexler, James and Pushkarna, Mahima and Bolukbasi, Tolga and Wattenberg, Martin and Vi{\'e}gas, Fernanda and Wilson, Jimbo},
  journal={IEEE transactions on visualization and computer graphics},
  volume={26},
  number={1},
  pages={56--65},
  year={2019},
  publisher={IEEE}
}
@misc{bengfort_yellowbrick_2018,
    title = {Yellowbrick},
    rights = {Apache License 2.0},
    url = {http://www.scikit-yb.org/en/latest/},
    abstract = {Yellowbrick is an open source, pure Python project that
        extends the Scikit-Learn {API} with visual analysis and
        diagnostic tools. The Yellowbrick {API} also wraps Matplotlib to
        create publication-ready figures and interactive data
        explorations while still allowing developers fine-grain control
        of figures. For users, Yellowbrick can help evaluate the
        performance, stability, and predictive value of machine learning
        models, and assist in diagnosing problems throughout the machine
        learning workflow.},
    version = {0.9.1},
    author = {Bengfort, Benjamin and Bilbro, Rebecca and Danielsen, Nathan and
        Gray, Larry and {McIntyre}, Kristen and Roman, Prema and Poh, Zijie and
        others},
    date = {2018-11-14},
    year = {2018},
    doi = {10.5281/zenodo.1206264}
}

% Qualitative Metrics

@article{wong2020much,
  title={How Much Can We Really Trust You? Towards Simple, Interpretable Trust Quantification Metrics for Deep Neural Networks},
  author={Wong, Alexander and Wang, Xiao Yu and Hryniowski, Andrew},
  journal={arXiv preprint arXiv:2009.05835},
  year={2020}
}

@article{farokhi2020modelling,
  title={Modelling and quantifying membership information leakage in machine learning},
  author={Farokhi, Farhad and Kaafar, Mohamed Ali},
  journal={arXiv preprint arXiv:2001.10648},
  year={2020}
}

@inproceedings{hitaj2017deep,
  title={Deep models under the GAN: information leakage from collaborative deep learning},
  author={Hitaj, Briland and Ateniese, Giuseppe and Perez-Cruz, Fernando},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={603--618},
  year={2017}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@article{fuchs2018dangers,
  title={The dangers of human-like bias in machine-learning algorithms},
  author={Fuchs, Daniel James},
  journal={Missouri S\&T’s Peer to Peer},
  volume={2},
  number={1},
  pages={1},
  year={2018}
}

@article{debrusk2018risk,
  title={The risk of machine-learning bias (and how to prevent it)},
  author={DeBrusk, Chris},
  journal={MIT Sloan Management Review},
  year={2018}
}

@article{krishnapriya2020issues,
  title={Issues related to face recognition accuracy varying based on race and skin tone},
  author={Krishnapriya, KS and Albiero, V{\'\i}tor and Vangara, Kushal and King, Michael C and Bowyer, Kevin W},
  journal={IEEE Transactions on Technology and Society},
  volume={1},
  number={1},
  pages={8--20},
  year={2020},
  publisher={IEEE}
}

@article{phillips2014comparison,
  title={Comparison of human and computer performance across face recognition experiments},
  author={Phillips, P Jonathon and O'toole, Alice J},
  journal={Image and Vision Computing},
  volume={32},
  number={1},
  pages={74--85},
  year={2014},
  publisher={Elsevier}
}

@article{shneiderman2020bridging,
  title={Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy Human-Centered AI systems},
  author={Shneiderman, Ben},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={10},
  number={4},
  pages={1--31},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{thomford2020implementing,
  title={Implementing artificial intelligence and digital health in resource-limited settings? Top 10 lessons we learned in congenital heart defects and cardiology},
  author={Thomford, Nicholas Ekow and Bope, Christian Domilongo and Agamah, Francis Edem and Dzobo, Kevin and Owusu Ateko, Richmond and Chimusa, Emile and Mazandu, Gaston Kuzamunu and Ntumba, Simon Badibanga and Dandara, Collet and Wonkam, Ambroise},
  journal={Omics: a journal of integrative biology},
  volume={24},
  number={5},
  pages={264--277},
  year={2020},
  publisher={Mary Ann Liebert, Inc., publishers 140 Huguenot Street, 3rd Floor New~…}
}

@article{wahl2018artificial,
  title={Artificial intelligence (AI) and global health: how can AI contribute to health in resource-poor settings?},
  author={Wahl, Brian and Cossy-Gantner, Aline and Germann, Stefan and Schwalbe, Nina R},
  journal={BMJ global health},
  volume={3},
  number={4},
  pages={e000798},
  year={2018},
  publisher={BMJ Specialist Journals}
}

@article{kok2020trust,
  title={Trust in robots: Challenges and opportunities},
  author={Kok, Bing Cai and Soh, Harold},
  journal={Current Robotics Reports},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@inproceedings{binns2018fairness,
  title={Fairness in machine learning: Lessons from political philosophy},
  author={Binns, Reuben},
  booktitle={Conference on Fairness, Accountability and Transparency},
  pages={149--159},
  year={2018},
  organization={PMLR}
}

@article{chouldechova2018frontiers,
  title={The frontiers of fairness in machine learning},
  author={Chouldechova, Alexandra and Roth, Aaron},
  journal={arXiv preprint arXiv:1810.08810},
  year={2018}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{caton2020fairness,
  title={Fairness in machine learning: A survey},
  author={Caton, Simon and Haas, Christian},
  journal={arXiv preprint arXiv:2010.04053},
  year={2020}
}

@article{gajane2017formalizing,
  title={On formalizing fairness in prediction with machine learning},
  author={Gajane, Pratik and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:1710.03184},
  year={2017}
}


@article{corabi2017superintelligent,
  title={Superintelligent AI and Skepticism},
  author={Corabi, Joseph},
  journal={Journal of Ethics and Emerging Technologies},
  volume={27},
  number={1},
  pages={4--23},
  year={2017}
}


@article{char2020identifying,
  title={Identifying ethical considerations for machine learning healthcare applications},
  author={Char, Danton S and Abr{\`a}moff, Michael D and Feudtner, Chris},
  journal={The American Journal of Bioethics},
  volume={20},
  number={11},
  pages={7--17},
  year={2020},
  publisher={Taylor \& Francis}
}


@article{hryniowski2020does,
  title={Where Does Trust Break Down? A Quantitative Trust Analysis of Deep Neural Networks via Trust Matrix and Conditional Trust Densities},
  author={Hryniowski, Andrew and Wang, Xiao Yu and Wong, Alexander},
  journal={arXiv preprint arXiv:2009.14701},
  year={2020}
}

@article{lohr2018facial,
  title={Facial recognition is accurate, if you’re a white guy},
  author={Lohr, Steve},
  journal={New York Times},
  volume={9},
  number={8},
  pages={283},
  year={2018}
}

@inproceedings{arsenovic2017facetime,
  title={FaceTime—Deep learning based face recognition attendance system},
  author={Arsenovic, Marko and Sladojevic, Srdjan and Anderla, Andras and Stefanovic, Darko},
  booktitle={2017 IEEE 15th International Symposium on Intelligent Systems and Informatics (SISY)},
  pages={000053--000058},
  year={2017},
  organization={IEEE}
}

@article{de2018algorithmic,
  title={Algorithmic decision-making based on machine learning from Big Data: Can transparency restore accountability?},
  author={De Laat, Paul B},
  journal={Philosophy \& technology},
  volume={31},
  number={4},
  pages={525--541},
  year={2018},
  publisher={Springer}
}

@inproceedings{strobel2019aspects,
  title={Aspects of transparency in machine learning},
  author={Strobel, Martin},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={2449--2451},
  year={2019}
}

@incollection{abdollahi2018transparency,
  title={Transparency in fair machine learning: the case of explainable recommender systems},
  author={Abdollahi, Behnoush and Nasraoui, Olfa},
  booktitle={Human and machine learning},
  pages={21--35},
  year={2018},
  publisher={Springer}
}

@inproceedings{zhang2018mitigating,
  title={Mitigating unwanted biases with adversarial learning},
  author={Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={335--340},
  year={2018}
}

@inproceedings{wang2020revise,
  title={REVISE: A tool for measuring and mitigating bias in visual datasets},
  author={Wang, Angelina and Narayanan, Arvind and Russakovsky, Olga},
  booktitle={European Conference on Computer Vision},
  pages={733--751},
  year={2020},
  organization={Springer}
}
@inproceedings{leavy2020mitigating,
  title={Mitigating Gender Bias in Machine Learning Data Sets},
  author={Leavy, Susan and Meaney, Gerardine and Wade, Karen and Greene, Derek},
  booktitle={International Workshop on Algorithmic Bias in Search and Recommendation},
  pages={12--26},
  year={2020},
  organization={Springer}
}


@article{vokinger2021mitigating,
  title={Mitigating bias in machine learning for medicine},
  author={Vokinger, Kerstin N and Feuerriegel, Stefan and Kesselheim, Aaron S},
  journal={Communications medicine},
  volume={1},
  number={1},
  pages={1--3},
  year={2021},
  publisher={Nature Publishing Group}
}



@inproceedings{hupont2019demogpairs,
  title={Demogpairs: Quantifying the impact of demographic imbalance in deep face recognition},
  author={Hupont, Isabelle and Fern{\'a}ndez, Carles},
  booktitle={2019 14th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2019)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}

@inproceedings{puyol2021fairness,
  title={Fairness in cardiac mr image analysis: An investigation of bias due to data imbalance in deep learning based segmentation},
  author={Puyol-Ant{\'o}n, Esther and Ruijsink, Bram and Piechnik, Stefan K and Neubauer, Stefan and Petersen, Steffen E and Razavi, Reza and King, Andrew P},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={413--423},
  year={2021},
  organization={Springer}
}


@article{leslie2020understanding,
  title={Understanding bias in facial recognition technologies},
  author={Leslie, David},
  journal={arXiv preprint arXiv:2010.07023},
  year={2020}
}

@article{garvie2016facial,
  title={Facial-recognition software might have a racial bias problem},
  author={Garvie, Clare and Frankle, Jonathan},
  journal={The Atlantic},
  volume={7},
  year={2016}
}

@article{albiero2020face,
  title={Is face recognition sexist? no, gendered hairstyles and biology are},
  author={Albiero, V{\'\i}tor and Bowyer, Kevin W},
  journal={arXiv preprint arXiv:2008.06989},
  year={2020}
}



@article{wong2020insights,
  title={Insights into Fairness through Trust: Multi-scale Trust Quantification for Financial Deep Learning},
  author={Wong, Alexander and Hryniowski, Andrew and Wang, Xiao Yu},
  journal={arXiv preprint arXiv:2011.01961},
  year={2020}
}

% Open Source Toolkits

@misc{aif360-oct-2018,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
	Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
	Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
	Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
	John Richards and Diptikalyan Saha and Prasanna Sattigeri and
	Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}

@misc{aix360-sept-2019,
    title = "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques",
    author = {Vijay Arya and Rachel K. E. Bellamy and Pin-Yu Chen and Amit Dhurandhar and Michael Hind
    and Samuel C. Hoffman and Stephanie Houde and Q. Vera Liao and Ronny Luss and Aleksandra Mojsilovi\'c
    and Sami Mourad and Pablo Pedemonte and Ramya Raghavendra and John Richards and Prasanna Sattigeri
    and Karthikeyan Shanmugam and Moninder Singh and Kush R. Varshney and Dennis Wei and Yunfeng Zhang},
    year = {2019},
    url = {https://arxiv.org/abs/1909.03012}
}

@article{art2018,
    title = {Adversarial Robustness Toolbox v1.2.0},
    author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh~Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian and Edwards, Ben},
    journal = {CoRR},
    volume = {1807.01069},
    year = {2018},
    url = {https://arxiv.org/pdf/1807.01069}
}

@misc{jacobgilpytorchcam,
  title={PyTorch library for CAM methods},
  author={Jacob Gildenblat and contributors},
  year={2021},
  publisher={GitHub},
  howpublished={\url{https://github.com/jacobgil/pytorch-grad-cam}},
}

@inproceedings{selvaraju2017grad,
  title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={618--626},
  year={2017}
}

@inproceedings{chattopadhay2018grad,
  title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
  author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  booktitle={2018 IEEE winter conference on applications of computer vision (WACV)},
  pages={839--847},
  year={2018},
  organization={IEEE}
}

@article{wang2019score,
  title={Score-cam: Improved visual explanations via score-weighted class activation mapping},
  author={Wang, Haofan and Du, Mengnan and Yang, Fan and Zhang, Zijian},
  journal={arXiv preprint arXiv:1910.01279},
  year={2019}
}

@inproceedings{ramaswamy2020ablation,
  title={Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization},
  author={Ramaswamy, Harish Guruprasad and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={983--991},
  year={2020}
}

@article{fu2020axiom,
  title={Axiom-based grad-cam: Towards accurate visualization and explanation of cnns},
  author={Fu, Ruigang and Hu, Qingyong and Dong, Xiaohu and Guo, Yulan and Gao, Yinghui and Li, Biao},
  journal={arXiv preprint arXiv:2008.02312},
  year={2020}
}

@inproceedings{muhammad2020eigen,
  title={Eigen-CAM: Class Activation Map using Principal Components},
  author={Muhammad, Mohammed Bany and Yeasin, Mohammed},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2020},
  organization={IEEE}
}

@article{jiang2021layercam,
  title={LayerCAM: Exploring Hierarchical Class Activation Maps},
  author={Jiang, Peng-Tao and Zhang, Chang-Bin and Hou, Qibin and Cheng, Ming-Ming and Wei, Yunchao},
  journal={IEEE Transactions on Image Processing},
  year={2021},
  publisher={IEEE}
}

@inproceedings{ribeiro2016should,
  title={"Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

% FactSheets

@article{arnold2019factsheets,
  title={FactSheets: Increasing trust in AI services through supplier's declarations of conformity},
  author={Arnold, Matthew and Bellamy, Rachel KE and Hind, Michael and Houde, Stephanie and Mehta, Sameep and Mojsilovi{\'c}, Aleksandra and Nair, Ravi and Ramamurthy, K Natesan and Olteanu, Alexandra and Piorkowski, David and others},
  journal={IBM Journal of Research and Development},
  volume={63},
  number={4/5},
  pages={6--1},
  year={2019},
  publisher={IBM}
}

@inproceedings{hind2020experiences,
  title={Experiences with improving the transparency of ai models and services},
  author={Hind, Michael and Houde, Stephanie and Martino, Jacquelyn and Mojsilovic, Aleksandra and Piorkowski, David and Richards, John and Varshney, Kush R},
  booktitle={Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages={1--8},
  year={2020}
}

@incollection{noyes2017super,
  title={What is a super-recogniser?},
  author={Noyes, Eilidh and Phillips, P Jonathon and O’Toole, AJ},
  booktitle={Face processing: Systems, disorders and cultural differences},
  pages={173--201},
  year={2017},
  publisher={Nova Science Publishers Inc}
}

@article{richards2020methodology,
  title={A Methodology for Creating AI FactSheets},
  author={Richards, John and Piorkowski, David and Hind, Michael and Houde, Stephanie and Mojsilovi{\'c}, Aleksandra},
  journal={arXiv preprint arXiv:2006.13796},
  year={2020}
}

@article{piorkowski2020towards,
  title={Towards evaluating and eliciting high-quality documentation for intelligent systems},
  author={Piorkowski, David and Gonz{\'a}lez, Daniel and Richards, John and Houde, Stephanie},
  journal={arXiv preprint arXiv:2011.08774},
  year={2020}
}

@article{finley2019democratization,
  title={The democratization of artificial intelligence: One library’s approach},
  author={Finley, Thomas K},
  journal={Information Technology and Libraries},
  volume={38},
  number={1},
  pages={8--13},
  year={2019}
}

@article{hosanagar2017democratization,
  title={The democratization of machine learning: What it means for tech innovation},
  author={Hosanagar, Kartik and Saxena, Apoorv},
  journal={Knowledge@ Wharton},
  year={2017}
}

@article{esteva2017dermatologist,
  title={Dermatologist-level classification of skin cancer with deep neural networks},
  author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A and Ko, Justin and Swetter, Susan M and Blau, Helen M and Thrun, Sebastian},
  journal={nature},
  volume={542},
  number={7639},
  pages={115--118},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{nickerson2014political,
  title={Political campaigns and big data},
  author={Nickerson, David W and Rogers, Todd},
  journal={Journal of Economic Perspectives},
  volume={28},
  number={2},
  pages={51--74},
  year={2014}
}

@inproceedings{chouldechova2018case,
  title={A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions},
  author={Chouldechova, Alexandra and Benavides-Prado, Diana and Fialko, Oleksandr and Vaithianathan, Rhema},
  booktitle={Conference on Fairness, Accountability and Transparency},
  pages={134--148},
  year={2018},
  organization={PMLR}
}

@inproceedings{sharma2020d,
  title={D-NetPAD: An explainable and interpretable iris presentation attack detector},
  author={Sharma, Renu and Ross, Arun},
  booktitle={2020 IEEE International Joint Conference on Biometrics (IJCB)},
  pages={1--10},
  year={2020},
  organization={IEEE}
}

@inproceedings{boyd2022human,
  title={Human-aided saliency maps improve generalization of deep learning},
  author={Boyd, Aidan and Bowyer, Kevin W and Czajka, Adam},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2735--2744},
  year={2022}
}

@InProceedings{Boyd_2023_WACV_CYBORG,
    author    = {Boyd, Aidan and Tinsley, Patrick and Bowyer, Kevin W. and Czajka, Adam},
    title     = {CYBORG: Blending Human Saliency Into the Loss Improves Deep Learning-Based Synthetic Face Detection},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {6108-6117}
}

@INPROCEEDINGS{livdet,
  author={Yambay, David and Becker, Benedict and Kohli, Naman and Yadav, Daksha and Czajka, Adam and Bowyer, Kevin W. and Schuckers, Stephanie and Singh, Richa and Vatsa, Mayank and Noore, Afzel and Gragnaniello, Diego and Sansone, Carlo and Verdoliva, Luisa and He, Lingxiao and Ru, Yiwei and Li, Haiqing and Liu, Nianfeng and Sun, Zhenan and Tan, Tieniu},
  booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)}, 
  title={LivDet iris 2017 — Iris liveness detection competition 2017}, 
  year={2017},
  volume={},
  number={},
  pages={733-741},
  doi={10.1109/BTAS.2017.8272763}
}

@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}

@inproceedings{Karras2020ada,
  title     = {Training Generative Adversarial Networks with Limited Data},
  author    = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. NeurIPS},
  year      = {2020}
}
 
@InProceedings{FFHQ,
author = {Karras, Tero and Laine, Samuli and Aila, Timo},
title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{CelebHQ,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

@inproceedings{StyleGAN2,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}

@article{StyleGAN3,
  title={Alias-free generative adversarial networks},
  author={Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={852--863},
  year={2021}
}

@article{GradCAM_Why_You_Say_That,
  title={Grad-CAM: Why did you say that?},
  author={Selvaraju, Ramprasaath R and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:1611.07450},
  year={2016}
}

@article{SaliencyMapsWithoutSacrificingAccuracy,
  title={Towards Generating Human-Centered Saliency Maps without Significantly Sacrificing Accuracy},
  author={Aswal, Vivek and Kao, Gore and Kim, Seo Young and Morrison, Katelyn},
  year={2022},
  journal={NeuroVision2022: What can computer vision learn from visual neuroscience? Workshop at CVPR 2022},
}

@inproceedings{DeepGaze,
  title={DeepGaze IIE: Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling},
  author={Linardos, Akis and K{\"u}mmerer, Matthias and Press, Ori and Bethge, Matthias},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12919--12928},
  year={2021}
}

@article{entropyMeasure,
  title={Local Shannon entropy measure with statistical tests for image randomness},
  author={Wu, Yue and Zhou, Yicong and Saveriades, George and Agaian, Sos and Noonan, Joseph P and Natarajan, Premkumar},
  journal={Information Sciences},
  volume={222},
  pages={323--342},
  year={2013},
  publisher={Elsevier}
}

% CAM Methods

@inproceedings{score_cam,
  title={Score-CAM: Score-weighted visual explanations for convolutional neural networks},
  author={Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={24--25},
  year={2020}
}

@article{layer_cam,
  title={Layercam: Exploring hierarchical class activation maps for localization},
  author={Jiang, Peng-Tao and Zhang, Chang-Bin and Hou, Qibin and Cheng, Ming-Ming and Wei, Yunchao},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={5875--5888},
  year={2021},
  publisher={IEEE}
}

@article{axiom_cam,
  title={Axiom-based grad-cam: Towards accurate visualization and explanation of cnns},
  author={Fu, Ruigang and Hu, Qingyong and Dong, Xiaohu and Guo, Yulan and Gao, Yinghui and Li, Biao},
  journal={arXiv preprint arXiv:2008.02312},
  year={2020}
}

@inproceedings{activation_based_cam,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2921--2929},
  year={2016}
}

@article{smooth_cam,
  title={Smooth grad-cam++: An enhanced inference level visualization technique for deep convolutional neural network models},
  author={Omeiza, Daniel and Speakman, Skyler and Cintas, Celia and Weldermariam, Komminist},
  journal={arXiv preprint arXiv:1908.01224},
  year={2019}
}

@article{IS_cam,
  title={IS-CAM: Integrated Score-CAM for axiomatic-based explanations},
  author={Naidu, Rakshit and Ghosh, Ankita and Maurya, Yash and Kundu, Soumya Snigdha and others},
  journal={arXiv preprint arXiv:2010.03023},
  year={2020}
}


@article{SS_cam,
  title={SS-CAM: Smoothed Score-CAM for sharper visual feature localization},
  author={Wang, Haofan and Naidu, Rakshit and Michael, Joy and Kundu, Soumya Snigdha},
  journal={arXiv preprint arXiv:2006.14255},
  year={2020}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}

@misc{UK_HoL_2018,
    title = {{AI in the UK: ready, willing and able?}},
    author = {{UK House of Lords, Select Committee on Artificial Intelligence}},
    note = {Report of Session 2017–19},
    year = {2018},
    url = {https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf}
}

@InProceedings{Fong_ICCV_2017,
  author    = {Fong, Ruth C. and Vedaldi, Andrea},
  booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Interpretable Explanations of Black Boxes by Meaningful Perturbation},
  year      = {2017},
  pages     = {3449-3457},
  doi       = {10.1109/ICCV.2017.371},
}

@ARTICLE{Jain_TBIOM_2022,
  author={Jain, Anil K. and Deb, Debayan and Engelsma, Joshua J.},
  journal={IEEE Transactions on Biometrics, Behavior, and Identity Science}, 
  title={Biometrics: Trust, But Verify}, 
  year={2022},
  volume={4},
  number={3},
  pages={303-323},
  doi={10.1109/TBIOM.2021.3115465}
}

@ARTICLE{Wang_TIP_2004,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  doi={10.1109/TIP.2003.819861}
}

@article{cam_entropy2022,
title = {Improving the Interpretability of GradCAMs in Deep Classification Networks},
journal = {Procedia Computer Science},
volume = {200},
pages = {620-628},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.260},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002691},
author = {Alfred Schöttl},
keywords = {GradCAM, Interpretability, CAM fostering},
abstract = {Deep classification networks play an important role as backbone networks in industrial AI applications. These applications are often cost or safety critical; explainability of the AI results is a highly demanded feature. We introduce CAM fostering, a method to improve the explainability of classification nets based on local layers such as convolutional or pooling layers. Several CAM interpretability measures are defined and used as additional loss terms. Even though the method requires second-order derivatives, it is demonstrated that deep nets can be trained on large datasets without frozen parameters. The training parameters can be chosen such that the accuracy degradation remains decent in favor of the CAM interpretability improvement. We conclude by comparing the results of different training parameter configurations.}
}

@inproceedings{De2020,
   abstract = {With massive computing power and data explosion as catalysts, Artificial Intelligence (AI) has finally come out of research labs to become a ground-breaking technology. Businesses are seeing its value in a wide range of applications and therefore looking for ways to make AI an integral part of their decision-making processes. However, to trust an AI model prediction or to take downstream action based on a prediction outcome, one needs to understand the reasons for the prediction. With deep neural networks increasingly becoming the algorithm of choice for models, generation of such reasons has become more challenging. Deep neural networks are highly nested non-linear models that learn patterns in the data through complex combinations of inputs. Their complex architecture makes it very difficult to decipher the exact reasons for their prediction. Due to this lack of transparency, businesses are not able to utilize this technology in many applications. To increase the adoption of deep learning models, explainability is critical in building trust in the solution and in guiding downstream actions in business applications. In this paper we aim to create human-interpretable explanations for predictions from deep learning models. We propose a hybrid of two prior approaches, integrating clustering of the network's hidden layer representation [2] with TREPAN decision tree [1], both of which uniquely deconstruct a neural network. Our aim is to visualize flow of information within the deep neural network using factors that make sense to humans, even if the underlying model uses more complex factors. This enables generation of human interpretable explanations (or, reasons codes) for each model outcome at an individual instance level. We demonstrate the new approach on credit card default prediction given by a deep feed forward neural network model. We compare and contrast this new integrated approach with three different approaches, based on the results we obtained from experimentation.},
   author = {Tanusree De and Prasenjit Giri and Ahmeduvesh Mevawala and Ramyasri Nemani and Arati Deo},
   doi = {10.1016/j.procs.2020.02.255},
   issn = {18770509},
   journal = {Procedia Computer Science},
   keywords = {Clustering,Comprehensibility,Deep Learning,Explainable AI,Fidelity,LIME,Neural Network,Reason Code,TREPAN},
   pages = {40-48},
   publisher = {Elsevier B.V.},
   title = {Explainable AI: A hybrid approach to generate human-interpretable explanation for deep learning prediction},
   volume = {168},
   year = {2020},
}

@article{Adebayo2018,
   abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
   author = {Julius Adebayo and Justin Gilmer and Michael Muelly and Ian Goodfellow and Moritz Hardt and Been Kim},
   month = {10},
   title = {Sanity Checks for Saliency Maps},
   url = {http://arxiv.org/abs/1810.03292},
   year = {2018},
}

@article{Kindermans2017,
   abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
   author = {Pieter-Jan Kindermans and Sara Hooker and Julius Adebayo and Maximilian Alber and Kristof T. Schütt and Sven Dähne and Dumitru Erhan and Been Kim},
   month = {11},
   title = {The (Un)reliability of saliency methods},
   url = {http://arxiv.org/abs/1711.00867},
   year = {2017},
}

@report{Phillips2021,
   author = {P. Jonathon Phillips and Carina A. Hahn and Peter C. Fontana and Amy N. Yates and Kristen Greene and David A. Broniatowski and Mark A. Przybocki},
   city = {Gaithersburg, MD},
   doi = {10.6028/NIST.IR.8312},
   institution = {National Institute of Standards and Technology},
   month = {9},
   title = {Four Principles of Explainable Artificial Intelligence},
   url = {https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf},
   year = {2021},
}

@article{AlejandroBarredo2020,
   abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
   author = {Alejandro Barredo Arrieta and Natalia Díaz-Rodríguez and Javier Del Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
   doi = {10.1016/j.inffus.2019.12.012},
   issn = {15662535},
   journal = {Information Fusion},
   keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
   month = {6},
   pages = {82-115},
   publisher = {Elsevier B.V.},
   title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
   volume = {58},
   year = {2020},
}

@article{Broderick2021,
   abstract = {Probabilistic machine learning increasingly informs critical decisions in medicine, economics, politics, and beyond. We need evidence to support that the resulting decisions are well-founded. To aid development of trust in these decisions, we develop a taxonomy delineating where trust in an analysis can break down: (1) in the translation of real-world goals to goals on a particular set of available training data, (2) in the translation of abstract goals on the training data to a concrete mathematical problem, (3) in the use of an algorithm to solve the stated mathematical problem, and (4) in the use of a particular code implementation of the chosen algorithm. We detail how trust can fail at each step and illustrate our taxonomy with two case studies: an analysis of the efficacy of microcredit and The Economist's predictions of the 2020 US presidential election. Finally, we describe a wide variety of methods that can be used to increase trust at each step of our taxonomy. The use of our taxonomy highlights steps where existing research work on trust tends to concentrate and also steps where establishing trust is particularly challenging.},
   author = {Tamara Broderick and Andrew Gelman and Rachael Meager and Anna L. Smith and Tian Zheng},
   month = {12},
   title = {Toward a Taxonomy of Trust for Probabilistic Machine Learning},
   url = {http://arxiv.org/abs/2112.03270},
   year = {2021},
}

@article{Gnjatovic2020,
   abstract = {This paper introduces a novel approach to human-machine collaborative learning that allows for the chronically missing human learnability in the context of supervised machine learning. The basic tenet of this approach is the refinement of a human designed software model through the iterative learning loop. Each iteration of the loop consists of two phases: (i) automatic data-driven parameter adjustment, performed by means of stochastic greedy local search, and (ii) human-driven model adjustment based on insights gained in the previous phase. The proposed approach is demonstrated through a real-life study of automatic electricity meter reading in the presence of noise. Thus, a cognitively-inspired non-connectionist approach to digit detection and recognition is introduced, which is subject to refinement through the iterative process of human-machine cooperation. The prototype system is evaluated with respect to the recognition accuracy (with the highest digit recognition accuracy of 94\%), and also discussed with respect to the storage requirements, generalizability, utilized contextual information, and efficiency.},
   author = {Milan Gnjatović and Nemanja Maček and Saša Adamović},
   doi = {10.12700/APH.17.2.2020.2.11},
   issn = {17858860},
   issue = {2},
   journal = {Acta Polytechnica Hungarica},
   keywords = {digit recognition,human-machine cooperative learning,stochastic search},
   pages = {191-210},
   title = {Putting Humans Back in the Loop: A Study in Human-Machine Cooperative Learning},
   volume = {17},
   url = {http://uni-obuda.hu/journal/Gnjatovic_Macek_Adamovic_99.pdf},
   year = {2020},
}

@inproceedings{Slack2020,
   abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
   author = {Dylan Slack and Sophie Hilgard and Emily Jia and Sameer Singh and Himabindu Lakkaraju},
   doi = {10.1145/3375627.3375830},
   isbn = {9781450371100},
   journal = {AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
   keywords = {Adversarial attacks,Bias detection,Black box explanations,Model interpretability},
   month = {2},
   pages = {180-186},
   publisher = {Association for Computing Machinery, Inc},
   title = {Fooling LIME and SHAP: Adversarial attacks on post hoc explanation methods},
   year = {2020},
}

@inproceedings{Bansal2021,
   abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI's recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?.},
   author = {Gagan Bansal and Tongshuang Wu and Joyce Zhou},
   doi = {10.1145/3411764.3445717},
   isbn = {9781450380966},
   journal = {Conference on Human Factors in Computing Systems - Proceedings},
   keywords = {Augmented intelligence,Explainable ai,Human-ai teams},
   month = {5},
   publisher = {Association for Computing Machinery},
   title = {Does the whole exceed its parts? The efect of ai explanations on complementary team performance},
   year = {2021},
}

@article{Jacobs2021,
   abstract = {Decision support systems embodying machine learning models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians’ treatment decisions will be influenced by machine learning recommendations and explanations. We used a within-subject factorial experiment to present 220 clinicians with patient vignettes, each with or without a machine-learning (ML) recommendation and one of the multiple forms of explanation. We found that interacting with ML recommendations did not significantly improve clinicians’ treatment selection accuracy, assessed as concordance with expert psychopharmacologist consensus, compared to baseline scenarios in which clinicians made treatment decisions independently. Interacting with incorrect recommendations paired with explanations that included limited but easily interpretable information did lead to a significant reduction in treatment selection accuracy compared to baseline questions. These results suggest that incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. More generally, our findings challenge the common assumption that clinicians interacting with ML tools will perform better than either clinicians or ML algorithms individually.},
   author = {Maia Jacobs and Melanie F. Pradier and Thomas H. McCoy and Roy H. Perlis and Finale Doshi-Velez and Krzysztof Z. Gajos},
   doi = {10.1038/s41398-021-01224-x},
   issn = {21583188},
   issue = {1},
   journal = {Translational Psychiatry},
   month = {6},
   pmid = {33542191},
   publisher = {Springer Nature},
   title = {How machine-learning recommendations influence clinician treatment selections: the example of the antidepressant selection},
   volume = {11},
   year = {2021},
}


