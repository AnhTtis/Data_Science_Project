\section{Methodology}
\label{sec:saliency}

Splitting a DNN into multiple possible locations is the first step for identifying alternative distributed architectures to be simulated. The positions will be ranked according to the accuracy that the system will presumably achieve with each split. Then, the ranking will be simulated to provide the configuration design meeting the QoS requirements. %The simulator workflow is as shown in Fig.~\ref{fig:Schema}.

The saliency-based splitting point search is implemented by using \gradcam{}~\cite{selvaraju2017grad}, which has been proved to pass the sanity check for saliency-based interpretability approaches~\cite{adebayo2018sanity}. Thus, ensuring that the saliency maps of \gradcam{} depend on the specific model instance taken into account.
%, while other saliency-based approaches do not. 
Specifically, the saliency computation can be applied at every layer of size $n\times m \times z$ of a DNN model. The result is a $n \times m$ class activation map. Then, the mean of the class activation map is performed, giving a single value per layer. The values obtained by all the layers compose the \textit{Cumulative Saliency} (\cs{}) curve for the trained model.

We assume that the DNN model is pre-trained. The \cs{} curve is computed on some testing set, composed of $C$ classes, each formed by $N_C$ input observations. For each $c$-th class, we proceed as follows.

For a given $i$-th layer of the neural network ($i=1,\ldots,I$), for each $j$-th input data belonging to class $c \in C$, we extract the class-discriminative activation map $L^{i}_{j,c}$. We start by computing the feature map importance coefficient $\alpha{}_{i,j}^{c}$:
\begin{equation}
\alpha{}_{i,j}^{c}=\frac{1}{z}\sum_{n}^{}\sum_{m}^{}\frac{\partial y^{c}}{\partial F_{n,m}^{i,j}}
\end{equation}
where $F^{i,j}\in R^{n \times m\times z}$ is the feature map of the convolutional layer $i$ for the input $j$, and $y^{c}$ is the output for the class $c$.

The weight $\alpha{}_{i,j}^{c}$ represents a partial linearization of the deep network downstream from $F$ and captures the value of the feature map of the $i$-th layer for a target class $c$. At this point, we perform a weighted sum between the value just calculated and the feature maps $F^{i,j}$ of the chosen layer. Finally, the \textit{ReLU} activation function is applied to reset the negative values of the gradient to zero obtaining the class activation map $L^{i}_{j,c}$ for a specific query input $j$:
\begin{equation}
L^{i}_{j,c}=ReLU\left(\sum_{k=i}^{I}\alpha{}_{k,j}^{c}F^{k,j}\right)
\end{equation}
This map focuses on the $i$-th layer, summing from the class activations of the networks back until $i$. 
We aim to obtain a single value for our class activation map $L^{i}_{j,c}$. Thus, we simply sum over the dimensions of $F^{i,j}\in R^{n \times m\times z}$, obtaining the per-input saliency values $\cs{}_{j,c}^{i}$. Ideally, computing these values for each $i$-th layer of the network gives a curve showing how the input has triggered the different layers of the network. At this point, averaging over all the inputs of all the classes provides the final CS curve, where the $i$-th point $\cs{}^i$ of the curve is a surrogate of the information conveyed through the $i$-th layer towards the decision for the correct class, for all the classes into play.

Once the \cs{} curve has been computed, the candidate split points can be identified by the layers that give local \cs{} maxima. Given these $N$ points, we will then go on to simulate each of them, and we choose the one with the best performance concerning the QoS requirements of the application.

Let $T^{i}$ be the target layer for splitting the model at index $i$ and $T^{i+1}$ the subsequent layer. We divide the network into three main blocks: \textit{i)} the head, running on the edge device, is composed of the first layers of the original DNN architecture, up to layer $T^i$; \textit{ii)} the bottleneck, an undercomplete autoencoder that learns low-dimensional latent attributes which explain the input data; \textit{iii)} and the tail, from layer $T^{i+1}$ to the very end of the network, that is executed on the server-side. The encoder part of the bottleneck is deployed to the edge device, while the decoder is executed on the server side.

In order to train the entire model $M(\cdot{})$, we first train our bottleneck. Then we fine-tune the entire network end-to-end. We create an undercomplete autoencoder which acts as a bottleneck inserted after $T^{i}$. The use of bottlenecks is standard in the SC literature~\cite{matsubara2021split}. This bottleneck should learn to replicate the input, which is the feature map in output at layer $T^{i}$. Therefore, given $\{\boldsymbol{I}_j, j=1,2,...,n\}$ as the $n$ number of training data, we train the sole bottleneck freezing the remaining network with the following loss function:
\begin{equation}
\mathcal{L}_{AE} = \frac{1}{n} \sum_{j=1}^{n} || \Phi_{T^{i}}(\boldsymbol{I}_j) - \Psi(\Phi_{T^{i}}(\boldsymbol{I}_j); \boldsymbol{W}_{AE})||^2
\end{equation}
with $\Phi_{T^{i}}$ the model layers up to the $i$-th layer $T^{i}$ (target layer), and $\Psi$ the AE part of the model with weights $\boldsymbol{W}_{AE}$.

After training the bottleneck, we perform a fine-tuning of the network with the following loss function:
\begin{equation}
\mathcal{L}_{task} = \frac{1}{n} \sum_{j=1}^{n} || \Phi_M(\boldsymbol{I}_j;\boldsymbol{W}_M) - \hat{y}_j ||^2
\end{equation}
with $\Phi_M$ the full $M$'s DNN layers, $\boldsymbol{W}_{M}$ the weights of the model $M$, and $\hat{y}_j$ the ground truth label for the input $j$.


%%%%%%%%%%%%%%%%%%%%%%%%%
\section{simulator architecture}
\label{sec:simulator}

The architecture of \mname{} is presented in Figure~\ref{fig:Schema}. To offer a large model state space, it is divided into five main layers: \textit{supervisor}, \textit{sensing}, \textit{transmitter}, \textit{netsim}, and \textit{receiver}. The network simulation is based on the SCNSL~\cite{fummi2008systemc} library, while the simulator is implemented in Python.

%Based on~\cite{clymer2009simulation}, we redefine the requirements that have to be satisfied to obtain a communication-aware simulation platform for distributed deep learning applications, on top of which we build \mname{}. In particular, these requirements are as follows:
%\begin{itemize}
%\item \emph{Behavioral representation} of the three aspects of distributed deep learning applications, \ie{}, computation, communication, and inference.
%\item \emph{Language independence}: the functional blocks of the framework should be integrated into the model independently of their implementation programming language.
%\item \emph{Modularity}: application models should be created through the composition of basic blocks,  whose order can easily be rearranged to model a variant to facilitate the exploration of several alternatives.
%\item \emph{Time accuracy}: simulated time should be independent of real-world time; the simulation of a short period of a component's life may take hours and vice versa but the tracking of simulated time should be accurate.
%\item \emph{Integration of real-world components}, such as a real computing system: the so-called ``hardware-in-the-loop'' (HIL)~\cite{bullock2004hardware}.
%\item \emph{Portability}: the ability to bring the developed components of the application into the real world with as few changes as possible to minimize effort and ensure that the behavior evaluated in the simulation does not change in the final deployment.
%\end{itemize}

The review provided in Section~\ref{sec:related} led us to aim at developing a communication-aware simulation platform for distributed deep learning applications that accurately emulates the behavior and the timing of the computation, communication, and inference performed by the system. The simulator has to be \textit{modular}, \textit{portable}, and \textit{language independent}, \ie{}, the functional blocks of the framework should be integrated into the model independently of their implementation programming language. Furthermore, it must allow the integration of real-world components, such as a real computing system: the so-called ``hardware-in-the-loop'' (HIL)~\cite{bullock2004hardware}.

%The review provided in Sec.~\ref{sec:related} led us to aim at developing a communication-aware simulation platform for distributed deep learning applications that satisfies the following requirements: \emph{behavioral representation}, \emph{modularity}, \emph{time accuracy}, and \emph{portability}. Moreover, we provide \emph{language independence}, \ie{}, the functional blocks of the framework should be integrated into the model independently of their implementation programming language, and the possibility of the integration of real-world components, such as a real computing system: the so-called ``hardware-in-the-loop'' (HIL)~\cite{bullock2004hardware}.

The \textit{supervisor} controls all the events and operations happening during the simulations. The \textit{sensing} layer is a high-level wrapper encoding the application into the architecture. The \textit{transmitter} module is responsible for the XMTR. The \textit{netsim} must guarantee that the simulation emulates the behavior of an actual communication channel: it has to execute events in the correct temporal order while taking into account the physical features of the channel, such as propagation delay, and signal loss. Finally, the \textit{receiver} is responsible for the RCVR.

Specifically, \mname{} requires the following inputs: 
\begin{enumerate}
\item \textit{Test scenario}: LC, RC, or SC (as described in Section~\ref{sec:related}).
\item \textit{Training set}: the set of data used to train the model and make it learn the hidden features/patterns in the data.
\item \textit{Trained instance of a DNN}: the network parameters, once it has been trained on a general purpose processor.
\item \textit{Test set}: the data with which the simulation is run, a proxy of the real setting the framework will work on.
\item \textit{Communication network modeling}: through some parameters which will be explained in the following.
%\item Hardware modeling, through some parameters which will be explained in the following;
\end{enumerate}
%
The communication network modeling parameters are:
\begin{enumerate}
\item \textit{Communication protocol}: the transport layer protocol that must be used, \ie{}, TCP or UDP.
\item \textit{Channel latency}: the time that each packet spends to travel from the sender to the receiver, \eg{}, $100~\mu{}s$.
\item \textit{Channel capacity}: the link available bandwidth. 
\item \textit{Interface speed}: to better model different hardware devices, even the physical speed can be set to match the desired target hardware, \eg{}, $1000~Mb/s$ to represent a Gigabit connection, $100~Mb/s$ for Fast-Ethernet, $160~Mb/s$ to represent Wi-Fi, \etc{}
\item \textit{Saboteur}: the network loss rate.
\end{enumerate}
%
%The hardware modeling parameters are:
%\begin{enumerate}[i)]
%\item \emph{Public benchmark}: to provide a realistic simulation of the computational behavior of the target device, such as~\cite{bianco2018benchmark,suzen2020benchmark}.
%\item \emph{Device}: a real HW device on which to deploy the system. 
%\end{enumerate}

The output of \mname{} is of two types: \textit{i)} the suggested configurations to simulate, ranked by the classification accuracy that the network is assumed to achieve. The engineer may then decide to simulate all or only a subset of them. \textit{ii)} The simulation results of the configurations selected in the previous step to deploy the application using the best design.
