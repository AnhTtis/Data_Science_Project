\section{Conclusions}
\label{sec:conclusions}

In this work, we have presented \mname{}, a novel framework for the design of distributed deep learning applications based on split computing. As a result, \mname{} suggests the proper configuration to match the quality of service requirements of the application and provide high performance in terms of accuracy and latency time. The experiments give an overview of specific features of \mname{} and show that the results obtained are very good, while 
%however, 
%given space constraints, 
extensive benchmarks will be the subject of future work.

%In future work, we will fine-tune the current implementation by adding functions for enabling hardware-in-the-loop simulation and exploiting further optimizations. In addition, we will investigate specific tensor reconstruction techniques to handle packet losses in UDP transmission. 
