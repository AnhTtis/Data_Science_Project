\section{Experimental Results}
\label{sec:experimental}

In this section, we show how the design of a typical real application can be carried out by \mname{}. In particular, we focus on the classification task of children's toys (such as boats, airplanes, \etc{}) passing on a conveyor belt within a real Industry 4.0 scenario: the ICE Laboratory located in Verona\footnote{\url{https://www.icelab.di.univr.it/}.}. %The tested scenarios are those described in Sec.~\ref{sec:related}, thus: local computing, remote computing, and split computing.

As DNN, we use the PyTorch implementation of the VGG16~\cite{simonyan2014very}. Other than being the workhorse of the industrial computer vision and pattern recognition~\cite{liu2021toward}, VGGs has recently been rejuvenated with diverse ``cousin'' extensions, like the RepVGG~\cite{ding2021repvgg}, a VGG-like inference-time body composed of nothing but a stack of $3\times{}3$ convolution and ReLU. RepVGG models run faster than ResNet-50 or ResNet-101~\cite{he2016deep} with higher accuracy and show a favorable accuracy-speed trade-off compared to the state-of-the-art models like EfficientNet~\cite{tan2019efficientnet}. Other than these reasons, VGG suits very well for SC: in its classic form, it exhibits a good base accuracy, a large number of parameters, is relatively deep in terms of the number of layers, and the trend of the layers in terms of complexity and output size is not regular, making the split point determination not trivial.

For the image classification task, we train our model on the CIFAR10 dataset~\cite{krizhevsky2009learning} up to 20 epochs with a learning rate of $5\times{}10^{-3}$, using Adam~\cite{kingma2014adam} as optimizer. We tested our application on images acquired in the ICE Lab. CIFAR10 has to be considered as a placeholder for bigger datasets (\ie{}, ImageNet~\cite{deng2009imagenet}); nonetheless, the focus here is to show the potentialities of the \mname{} and not beating the state-of-the-art in a specific computer vision challenge. 

In all the experiments, the split has been realized by placing an undercomplete autoencoder with a 50\% of compression rate. For the training of the encoder/decoder pair, we run up 50 epochs with a learning rate of $5\times{}10^{-4}$, always using Adam as optimizer.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Saliency-based split point search}
\begin{figure}[t]
\begin{center}
\includegraphics[width=.95\linewidth]{figures/SEI_and_accuracy_VGG16.png}
\end{center}
\caption{\textit{Cumulative Saliency} (\cs{}) as a function of the layer compared with the accuracy of the DNN split in that layer. The peaks of the \cs{} curve correspond to the points in which accuracy is preserved despite split injection. Thus, the layers in which \cs{} has a local maximum are the best candidates for splitting. (*) represents a max pooling layer.} 
\label{fig:SEI_and_accuracy_VGG16}
\vspace{-1em}
\end{figure}

The first output of \mname{} is a set of candidate split points that are worth being simulated. Figure~\ref{fig:SEI_and_accuracy_VGG16} shows the results of the saliency-based split point search. 

We can clearly appreciate how the \cs{} saliency approach identifies as candidate split points (in red) layers $5$, $9$ and $13$, corresponding to \textit{block2\_pool}, \textit{block3\_pool} and \textit{block4\_pool} (dense data), and two additional points at layers $11$ and $15$ (informative data), corresponding to \textit{block4\_conv2} and \textit{block5\_conv2}, respectively. It is worth noting how layers with the same dimensionality, \ie{}, convolutional layers belonging to the same block, do not express the same importance as shown by the \cs{} curve. In these regions of uniform dimensionality, we select layers that have higher importance, and we will show that the \cs{} values directly translate into higher classification accuracy. Figure~\ref{fig:SEI_and_accuracy_VGG16} confirms that \cs{} is a good proxy for the overall classification accuracy and thus it is worth splitting the network into the layers in which \cs{} exhibits a local maximum.

Given the results of the \cs{} computing procedure, which we remind is done without having retrained the network, then we can explore the complete behavior of the application, including its transmission aspects, just splitting into the candidate layers. In particular, due to the lack of space, in the next section, we highlight simulation results only after splitting at layers $11$ and $15$, as well as for RC.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Communication-aware split point selection}
\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/SC2.png}
\caption{Evaluation of the impact of the network on split point selection for the classification task of the images captured inside the ICE Lab. The network channel is 1 GB/s Full-Duplex with TCP protocol.}
\label{fig:SC2}
\end{figure}

Figure~\ref{fig:SC2} refers to the experiment of the SC scenario for the classification task of the images captured inside the ICE Lab. The network channel is 1 GB/s Full-Duplex with TCP protocol. The application presents a constraint on the maximum frame latency of 0.05~s (\ie{}, 20~FPS), given by the velocity of the conveyor belt. The split point is at layer $11$ and $15$, respectively. 

Figure~\ref{fig:SC2} highlights how the latency increases with the packet loss rate, due to TCP retransmission in case of packet loss. On the other hand, however, this preserves the accuracy of the application. In particular, the solid pink curve shows that with the split at layer $15$ the application requirements are always satisfied independently of the packet loss rate. Vice versa, the dotted blue curve shows that with the split at layers $11$, the 20~FPS constraint cannot be satisfied when the packet loss rate is more than 3\%. The behavior of the simulator meets expectation, \ie{}, by splitting the network at layer $11$, the amount of transmitted data is greater than the one obtained by splitting the network at layer $15$, and because of the retransmissions, the latency increases, up to violate the application constraints represented by the dashed red line.

This experiment shows the main claim of our work: given a set of possible split point solutions (see Figure~\ref{fig:SEI_and_accuracy_VGG16}), our framework through a rapid evaluation allows deciding which DNN configuration is compatible with the application requirements while considering the communication setup.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Network protocol selection}
% V1: System accuracy and overall latency with two includegraphics[]{}.
\iffalse
\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/RC1.png}
\caption{System accuracy results on the CIFAR10 test set for the classification task, contextualized in the RC scenario with the use of the TCP and UDP protocols. The network channel is 1 GB/s Full-Duplex.}
\label{fig:RC1}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/RC2.png}
\caption{Latency results on the CIFAR10 test set for the classification task, contextualized in the RC scenario with the use of the TCP and UDP protocols. The network channel is 1 GB/s Full-Duplex.}
\label{fig:RC2}
\end{figure}
\fi

% V2.
\begin{figure}[t]
\centering
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/RC1_v2.png}
%\caption*{(A)}
\label{fig:CDE_and_SEI_VGG16_notMNIST}
\end{minipage}
%
\begin{minipage}[t]{0.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/RC2_v2.png}
%\caption*{(B)}
\end{minipage}
\caption{Accuracy (left) and latency (right) results on the CIFAR10 test set for the classification task, contextualized in the RC scenario with the use of the TCP and UDP protocols. The network channel is 1 GB/s Full-Duplex.}
\label{fig:RC12}
\end{figure}

Figure~\ref{fig:RC12}-left and Figure~\ref{fig:RC12}-right show the system accuracy and the overall latency, related to the classification task in the full RC scenario, this time on the CIFAR10 test set, with the use of the TCP and UDP protocols. The network channel is 1 GB/s Full-Duplex.

Figure~\ref{fig:RC12}-left shows that, using TCP, the application accuracy does not depend on the packet loss rate. However, Figure~\ref{fig:RC12}-right shows that this feature comes at a cost: with TCP the overall latency is much greater, so it is required to make sure that this is compatible with the application requirements. UDP protocol shows a dual behavior, the latency is minimized and kept independent of the packet loss rate but the accuracy decreases in case of loss since no error checking and recovery services are provided. % Another notable discrepancy between TCP vs. UDP is that TCP provides ordered delivery of data and checks the readiness of the receiver. 

These experiments show that the proposed framework allows modeling an application's transmission details to jointly perform split point selection and transmission protocol selection.


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Neural network statistics}
\label{subsec:nnstats}
\begin{table}[t]
\centering
\caption{The neural network summary provided for the VGG16.}
\label{tab:nn_summary}
\begin{tabular}{lll}
\toprule 
\textbf{Layer} (type:depth-idx) & \textbf{Output Shape} & \textbf{Param} (\#) \\
\midrule
Sequential: 1-1 & [16, 512, 7, 7]       &   --          \\
Conv2d: 2-1     & [16, 64, 224, 224]    &   1.792       \\
ReLU: 2-2       & [16, 64, 224, 224]    &   --          \\
Conv2d: 2-3     & [16, 64, 224, 224]    &   36.928      \\
\dots{}         & \dots{}               &   \dots{}     \\
AdaptiveAvgPool2d: 1-2  & [16, 512, 7, 7]   &   --          \\
Sequential: 1-3         & [16, 1000]        &   --          \\
Linear: 2-32            & [16, 4096]        &   102.764.544 \\
ReLU: 2-33              & [16, 4096]        &   --          \\
\dots{}         & \dots{}               &   \dots{}     \\
Linear: 2-38            & [16, 1000]        &   4.097.000   \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{The neural network statistics provided for the VGG16.}
\label{tab:nn_stats}
\begin{tabular}{ll}
\toprule 
\textbf{Statistic} & \textbf{Value} \\
\midrule
Total params                    &   138.357.544 \\
Trainable params                &   138.357.544 \\
Total mult-adds (G)             &   247.74      \\
Forward/backward pass size (MB) &   1735.26     \\
Estimated Total Size (MB)       &   2298.32     \\
\bottomrule
\end{tabular}
\end{table}

%\mname{} can also provide several statistics about the DNN. The aim is to lay out information helping the user to evaluate a range of assessments such as the estimated total size  of the network, the input size, \etc{} 

%To produce the results shown in Tab.~\ref{tab:nn_summary} the model summary layer-wise and Tab.~\ref{tab:nn_stats}, in the \textit{supervisor} we implement a clean and simple interface equipped with a set of functions to view the visualization of the model, which is also helpful while debugging your network.

\mname{} can also provide several statistics about the DNN. In particular, Table~\ref{tab:nn_summary} shows the summary of each constituent DNN module. These values are particularly useful to check whether the layers are producing data correctly. Furthermore, the values may be used to guess where overfitting and computational bottlenecks are likely to occur. Table~\ref{tab:nn_stats} shows the DNN statistics. The more trainable parameters a model has, the more computing power will be needed. Specifically, these reports are particularly useful when working with a customized DNN for which statistical information is not available in the existing literature.
