
@misc{bengio_consciousness_2019,
  title = {The {Consciousness} {Prior}},
  url = {http://arxiv.org/abs/1709.08568},
  abstract = {A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.},
  urldate = {2023-03-08},
  publisher = {arXiv},
  author = {Bengio, Yoshua},
  month = dec,
  year = {2019},
  note = {arXiv:1709.08568 [cs, stat]},
}

@misc{dao_flashattention_2022,
  title = {{FlashAttention}: {Fast} and {Memory}-{Efficient} {Exact} {Attention} with {IO}-{Awareness}},
  shorttitle = {{FlashAttention}},
  url = {http://arxiv.org/abs/2205.14135},
  doi = {10.48550/arXiv.2205.14135},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3\${\textbackslash}times\$ speedup on GPT-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  urldate = {2023-03-08},
  publisher = {arXiv},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  month = jun,
  year = {2022},
  note = {arXiv:2205.14135 [cs]},
}

@misc{ainslie_etc_2020,
  title = {{ETC}: {Encoding} {Long} and {Structured} {Inputs} in {Transformers}},
  shorttitle = {{ETC}},
  url = {http://arxiv.org/abs/2004.08483},
  doi = {10.48550/arXiv.2004.08483},
  abstract = {Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.},
  urldate = {2023-03-08},
  publisher = {arXiv},
  author = {Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  month = oct,
  year = {2020},
  note = {arXiv:2004.08483 [cs, stat]},
}

@misc{beltagy_longformer_2020,
  title = {Longformer: {The} {Long}-{Document} {Transformer}},
  shorttitle = {Longformer},
  url = {http://arxiv.org/abs/2004.05150},
  doi = {10.48550/arXiv.2004.05150},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  urldate = {2023-03-08},
  publisher = {arXiv},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  month = dec,
  year = {2020},
  note = {arXiv:2004.05150 [cs]},
}

@article{hudson_compositional_2018,
  title = {Compositional {Attention} {Networks} for {Machine} {Reasoning}},
  url = {https://www.semanticscholar.org/reader/289fb3709475f5c87df8d97f129af54029d27fee},
  abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
  language = {en},
  urldate = {2023-03-07},
  journal = {ArXiv},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  year = {2018},
}

@article{mao_neuro-symbolic_2019,
  title = {The {Neuro}-{Symbolic} {Concept} {Learner}: {Interpreting} {Scenes} {Words} and {Sentences} from {Natural} {Supervision}},
  shorttitle = {The {Neuro}-{Symbolic} {Concept} {Learner}},
  url = {https://www.semanticscholar.org/reader/ec9b27d019fefadb5e97c8174ac889e831f483d7},
  abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
  language = {en},
  urldate = {2023-03-07},
  journal = {ArXiv},
  author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, J. and Wu, Jiajun},
  year = {2019},
}

@article{yi_neural-symbolic_2018,
  title = {Neural-{Symbolic} {VQA}: {Disentangling} {Reasoning} from {Vision} and {Language} {Understanding}},
  shorttitle = {Neural-{Symbolic} {VQA}},
  url = {https://www.semanticscholar.org/reader/9d15ebe3f5aaf32a9f835f88703241461324c35b},
  abstract = {An academic search engine that utilizes artificial intelligence methods to provide highly relevant results and novel tools to filter them with ease.},
  language = {en},
  urldate = {2023-03-07},
  journal = {ArXiv},
  author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, A. and Kohli, Pushmeet and Tenenbaum, J.},
  year = {2018},
}

@article{hu_learning_2017,
  title = {Learning to {Reason}: {End}-to-{End} {Module} {Networks} for {Visual} {Question} {Answering}},
  shorttitle = {Learning to {Reason}},
  url = {http://ieeexplore.ieee.org/document/8237355/},
  doi = {10.1109/ICCV.2017.93},
  abstract = {Natural language questions are inherently compositional, and many are most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer “is there an equal number of balls and boxes?” we can look for balls, look for boxes, count them, and compare the results. The recently proposed Neural Module Network (NMN) architecture [3, 2] implements this approach to question answering by parsing questions into linguistic substructures and assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, and are restricted to the module configurations proposed by these parsers rather than learning them from data. In this paper, we propose End-to-End Module Networks (N2NMNs), which learn to reason by directly predicting instance-specific network layouts without the aid of a parser. Our model learns to generate network structures (by imitating expert demonstrations) while simultaneously learning network parameters (using the downstream task loss). Experimental results on the new CLEVR dataset targeted at compositional question answering show that N2NMNs achieve an error reduction of nearly 50\% relative to state-of-theart attentional approaches, while discovering interpretable network architectures specialized for each question.},
  urldate = {2023-03-07},
  journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
  author = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  month = oct,
  year = {2017},
  note = {Conference Name: 2017 IEEE International Conference on Computer Vision (ICCV)
ISBN: 9781538610329
Place: Venice
Publisher: IEEE},
  pages = {804--813},
}

@misc{zhang_interpretable_2018,
  title = {Interpretable {Visual} {Question} {Answering} by {Visual} {Grounding} from {Attention} {Supervision} {Mining}},
  url = {http://arxiv.org/abs/1808.00265},
  abstract = {A key aspect of VQA models that are interpretable is their ability to ground their answers to relevant regions in the image. Current approaches with this capability rely on supervised learning and human annotated groundings to train attention mechanisms inside the VQA architecture. Unfortunately, obtaining human annotations specific for visual grounding is difficult and expensive. In this work, we demonstrate that we can effectively train a VQA architecture with grounding supervision that can be automatically obtained from available region descriptions and object annotations. We also show that our model trained with this mined supervision generates visual groundings that achieve a higher correlation with respect to manually-annotated groundings, meanwhile achieving state-of-the-art VQA accuracy.},
  urldate = {2023-03-07},
  publisher = {arXiv},
  author = {Zhang, Yundong and Niebles, Juan Carlos and Soto, Alvaro},
  month = aug,
  year = {2018},
  note = {arXiv:1808.00265 [cs]},
}

@misc{subramanian_obtaining_2020,
  title = {Obtaining {Faithful} {Interpretations} from {Compositional} {Neural} {Networks}},
  url = {http://arxiv.org/abs/2005.00724},
  abstract = {Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model's reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.},
  urldate = {2023-03-07},
  publisher = {arXiv},
  author = {Subramanian, Sanjay and Bogin, Ben and Gupta, Nitish and Wolfson, Tomer and Singh, Sameer and Berant, Jonathan and Gardner, Matt},
  month = sep,
  year = {2020},
  note = {arXiv:2005.00724 [cs]},
}

@misc{bahdanau_systematic_2019,
  title = {Systematic {Generalization}: {What} {Is} {Required} and {Can} {It} {Be} {Learned}?},
  shorttitle = {Systematic {Generalization}},
  url = {http://arxiv.org/abs/1811.12889},
  abstract = {Numerous models for grounded language understanding have been recently proposed, including (i) generic models that can be easily adapted to any given task and (ii) intuitively appealing modular models that require background knowledge to be instantiated. We compare both types of models in how much they lend themselves to a particular form of systematic generalization. Using a synthetic VQA test, we evaluate which models are capable of reasoning about all possible object pairs after training on only a small subset of them. Our findings show that the generalization of modular models is much more systematic and that it is highly sensitive to the module layout, i.e. to how exactly the modules are connected. We furthermore investigate if modular models that generalize well could be made more end-to-end by learning their layout and parametrization. We find that end-to-end methods from prior work often learn inappropriate layouts or parametrizations that do not facilitate systematic generalization. Our results suggest that, in addition to modularity, systematic generalization in language understanding may require explicit regularizers or priors.},
  urldate = {2023-03-07},
  publisher = {arXiv},
  author = {Bahdanau, Dzmitry and Murty, Shikhar and Noukhovitch, Michael and Nguyen, Thien Huu and de Vries, Harm and Courville, Aaron},
  month = apr,
  year = {2019},
  note = {arXiv:1811.12889 [cs]},
}

@article{cao_interpretable_2021,
  title = {Interpretable {Visual} {Question} {Answering} by {Reasoning} on {Dependency} {Trees}},
  volume = {43},
  issn = {0162-8828, 2160-9292, 1939-3539},
  url = {https://ieeexplore.ieee.org/document/8847465/},
  doi = {10.1109/TPAMI.2019.2943456},
  abstract = {Collaborative reasoning for understanding image-question pairs is a very critical but underexplored topic in interpretable visual question answering systems. Although very recent studies have attempted to use explicit compositional processes to assemble multiple subtasks embedded in questions, their models heavily rely on annotations or handcrafted rules to obtain valid reasoning processes, which leads to either heavy workloads or poor performance on compositional reasoning. In this paper, to better align image and language domains in diverse and unrestricted cases, we propose a novel neural network model that performs global reasoning on a dependency tree parsed from the question; thus, our model is called a parse-tree-guided reasoning network (PTGRN). This network consists of three collaborative modules: i) an attention module that exploits the local visual evidence of each word parsed from the question, ii) a gated residual composition module that composes the previously mined evidence, and iii) a parse-tree-guided propagation module that passes the mined evidence along the parse tree. Thus, PTGRN is capable of building an interpretable visual question answering (VQA) system that gradually derives image cues following question-driven parse-tree reasoning. Experiments on relational datasets demonstrate the superiority of PTGRN over current state-of-the-art VQA methods, and the visualization results highlight the explainable capability of our reasoning system.},
  number = {3},
  urldate = {2023-03-07},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Cao, Qingxing and Liang, Xiaodan and Li, Bailin and Lin, Liang},
  month = mar,
  year = {2021},
  pages = {887--901},
}

@inproceedings{johnson_inferring_2017,
  title = {Inferring and {Executing} {Programs} for {Visual} {Reasoning}},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Johnson_Inferring_and_Executing_ICCV_2017_paper.html},
  urldate = {2023-03-07},
  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Hoffman, Judy and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},
  year = {2017},
  pages = {2989--2998},
}

@misc{xu_show_2016,
  title = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
  shorttitle = {Show, {Attend} and {Tell}},
  url = {http://arxiv.org/abs/1502.03044},
  doi = {10.48550/arXiv.1502.03044},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  urldate = {2023-03-06},
  publisher = {arXiv},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  month = apr,
  year = {2016},
  note = {arXiv:1502.03044 [cs]},
}

@misc{chandrasekaran_explanations_2018,
  title = {Do {Explanations} make {VQA} {Models} more {Predictable} to a {Human}?},
  url = {http://arxiv.org/abs/1810.12366},
  doi = {10.48550/arXiv.1810.12366},
  abstract = {A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable 'explanations' of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model -- its responses as well as failures -- more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.},
  urldate = {2023-03-06},
  publisher = {arXiv},
  author = {Chandrasekaran, Arjun and Prabhu, Viraj and Yadav, Deshraj and Chattopadhyay, Prithvijit and Parikh, Devi},
  month = oct,
  year = {2018},
  note = {arXiv:1810.12366 [cs]},
}

@article{deng_visual_nodate,
  title = {Visual {Grounding} via {Accumulated} {Attention}},
  abstract = {Visual Grounding (VG) aims to locate the most relevant object or region in an image, based on a natural language query. The query can be a phrase, a sentence or even a multi-round dialogue. There are three main challenges in VG: 1) what is the main focus in a query; 2) how to understand an image; 3) how to locate an object. Most existing methods combine all the information curtly, which may suffer from the problem of information redundancy (i.e. ambiguous query, complicated image and a large number of objects). In this paper, we formulate these challenges as three attention problems and propose an accumulated attention (A-ATT) mechanism to reason among them jointly. Our A-ATT mechanism can circularly accumulate the attention for useful information in image, query, and objects, while the noises are ignored gradually. We evaluate the performance of A-ATT on four popular datasets (namely ReferCOCO, ReferCOCO+, ReferCOCOg, and Guesswhat?!), and the experimental results show the superiority of the proposed method in term of accuracy.},
  language = {en},
  author = {Deng, Chaorui and Wu, Qi and Wu, Qingyao and Hu, Fuyuan and Lyu, Fan and Tan, Mingkui},
}

@inproceedings{norcliffe-brown_learning_2018,
  title = {Learning {Conditioned} {Graph} {Structures} for {Interpretable} {Visual} {Question} {Answering}},
  url = {https://www.semanticscholar.org/paper/Learning-Conditioned-Graph-Structures-for-Visual-Norcliffe-Brown-Vafeias/6ac33d3dcecbed17580509a34bccdff2425f7ed8},
  abstract = {Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which allow to capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain state of the art results with 66.18\% accuracy and demonstrate the interpretability of the proposed method.},
  urldate = {2023-03-06},
  author = {Norcliffe-Brown, Will and Vafeias, Efstathios and Parisot, Sarah},
  month = jun,
  year = {2018},
}

@article{mascharka_transparency_2018,
  title = {Transparency by {Design}: {Closing} the {Gap} {Between} {Performance} and {Interpretability} in {Visual} {Reasoning}},
  shorttitle = {Transparency by {Design}},
  url = {https://ieeexplore.ieee.org/document/8578617/},
  doi = {10.1109/CVPR.2018.00519},
  abstract = {Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1\% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.},
  urldate = {2023-03-06},
  journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author = {Mascharka, David and Tran, Philip and Soklaski, Ryan and Majumdar, Arjun},
  month = jun,
  year = {2018},
  note = {Conference Name: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
ISBN: 9781538664209
Place: Salt Lake City, UT
Publisher: IEEE},
  pages = {4942--4950},
}

@misc{mao_doubly_2022,
  title = {Doubly {Right} {Object} {Recognition}: {A} {Why} {Prompt} for {Visual} {Rationales}},
  shorttitle = {Doubly {Right} {Object} {Recognition}},
  url = {http://arxiv.org/abs/2212.06202},
  doi = {10.48550/arXiv.2212.06202},
  abstract = {Many visual recognition models are evaluated only on their classification accuracy, a metric for which they obtain strong performance. In this paper, we investigate whether computer vision models can also provide correct rationales for their predictions. We propose a ``doubly right'' object recognition benchmark, where the metric requires the model to simultaneously produce both the right labels as well as the right rationales. We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions. However, by transferring the rationales from language models into visual representations through a tailored dataset, we show that we can learn a ``why prompt,'' which adapts large visual representations to produce correct rationales. Visualizations and empirical experiments show that our prompts significantly improve performance on doubly right object recognition, in addition to zero-shot transfer to unseen tasks and datasets.},
  urldate = {2023-03-06},
  publisher = {arXiv},
  author = {Mao, Chengzhi and Teotia, Revant and Sundar, Amrutha and Menon, Sachit and Yang, Junfeng and Wang, Xin and Vondrick, Carl},
  month = dec,
  year = {2022},
  note = {arXiv:2212.06202 [cs]},
}

@misc{menon_visual_2022,
  title = {Visual {Classification} via {Description} from {Large} {Language} {Models}},
  url = {http://arxiv.org/abs/2210.07183},
  doi = {10.48550/arXiv.2210.07183},
  abstract = {Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.},
  urldate = {2023-03-06},
  publisher = {arXiv},
  author = {Menon, Sachit and Vondrick, Carl},
  month = dec,
  year = {2022},
  note = {arXiv:2210.07183 [cs]},
}

@article{dijkstra_information_1972,
  title = {Information streams sharing a finite buffer},
  volume = {1},
  issn = {0020-0190},
  doi = {10.1016/0020-0190(72)90034-8},
  number = {5},
  journal = {Information Processing Letters},
  author = {Dijkstra, E.W.},
  year = {1972},
  pages = {179--180},
}

@article{cooprider_information_1974,
  title = {Information streams sharing a finite buffer: other solutions},
  volume = {3},
  issn = {0020-0190},
  shorttitle = {Information streams sharing a finite buffer},
  url = {https://www.sciencedirect.com/science/article/pii/0020019074900416},
  doi = {10.1016/0020-0190(74)90041-6},
  language = {en},
  number = {1},
  urldate = {2023-03-06},
  journal = {Information Processing Letters},
  author = {Cooprider, Lee W. and Heymans, F. and Courtois, P. J. and Parnas, David L.},
  month = jul,
  year = {1974},
  pages = {16--21},
}

@article{yi_neural-symbolic_2018-1,
  title = {Neural-{Symbolic} {VQA}: {Disentangling} {Reasoning} from {Vision} and {Language} {Understanding}},
  shorttitle = {Neural-{Symbolic} {VQA}},
  url = {https://www.semanticscholar.org/paper/Neural-Symbolic-VQA%3A-Disentangling-Reasoning-from-Yi-Wu/9d15ebe3f5aaf32a9f835f88703241461324c35b},
  abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8\% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
  urldate = {2023-03-03},
  journal = {ArXiv},
  author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, A. and Kohli, Pushmeet and Tenenbaum, J.},
  month = oct,
  year = {2018},
}

@misc{gao_pal_2023,
  title = {{PAL}: {Program}-aided {Language} {Models}},
  shorttitle = {{PAL}},
  url = {http://arxiv.org/abs/2211.10435},
  doi = {10.48550/arXiv.2211.10435},
  abstract = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
  urldate = {2023-03-01},
  publisher = {arXiv},
  author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  month = jan,
  year = {2023},
  note = {arXiv:2211.10435 [cs]},
}

@misc{fu_tell_2022,
  title = {Tell {Me} {What} {Happened}: {Unifying} {Text}-guided {Video} {Completion} via {Multimodal} {Masked} {Video} {Generation}},
  shorttitle = {Tell {Me} {What} {Happened}},
  url = {http://arxiv.org/abs/2211.12824},
  abstract = {Generating a video given the first several static frames is challenging as it anticipates reasonable future frames with temporal coherence. Besides video prediction, the ability to rewind from the last frame or infilling between the head and tail is also crucial, but they have rarely been explored for video completion. Since there could be different outcomes from the hints of just a few frames, a system that can follow natural language to perform video completion may significantly improve controllability. Inspired by this, we introduce a novel task, text-guided video completion (TVC), which requests the model to generate a video from partial frames guided by an instruction. We then propose Multimodal Masked Video Generation (MMVG) to address this TVC task. During training, MMVG discretizes the video frames into visual tokens and masks most of them to perform video completion from any time point. At inference time, a single MMVG model can address all 3 cases of TVC, including video prediction, rewind, and infilling, by applying corresponding masking conditions. We evaluate MMVG in various video scenarios, including egocentric, animation, and gaming. Extensive experimental results indicate that MMVG is effective in generating high-quality visual appearances with text guidance for TVC.},
  urldate = {2023-02-28},
  publisher = {arXiv},
  author = {Fu, Tsu-Jui and Yu, Licheng and Zhang, Ning and Fu, Cheng-Yang and Su, Jong-Chyi and Wang, William Yang and Bell, Sean},
  month = nov,
  year = {2022},
  note = {arXiv:2211.12824 [cs]
version: 1},
}

@misc{sadhu_zero-shot_2019,
  title = {Zero-{Shot} {Grounding} of {Objects} from {Natural} {Language} {Queries}},
  url = {http://arxiv.org/abs/1908.07129},
  doi = {10.48550/arXiv.1908.07129},
  abstract = {A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, "unseen" nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual "seen" settings and performs significantly better than baseline in the zero-shot setting.},
  urldate = {2023-02-23},
  publisher = {arXiv},
  author = {Sadhu, Arka and Chen, Kan and Nevatia, Ram},
  month = aug,
  year = {2019},
  note = {arXiv:1908.07129 [cs]},
}

@misc{huang_language_2022,
  title = {Language {Models} as {Zero}-{Shot} {Planners}: {Extracting} {Actionable} {Knowledge} for {Embodied} {Agents}},
  shorttitle = {Language {Models} as {Zero}-{Shot} {Planners}},
  url = {http://arxiv.org/abs/2201.07207},
  doi = {10.48550/arXiv.2201.07207},
  abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
  urldate = {2023-02-23},
  publisher = {arXiv},
  author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  month = mar,
  year = {2022},
  note = {arXiv:2201.07207 [cs]},
}

@misc{yang_improving_2022,
  title = {Improving {Visual} {Grounding} with {Visual}-{Linguistic} {Verification} and {Iterative} {Reasoning}},
  url = {http://arxiv.org/abs/2205.00272},
  doi = {10.48550/arXiv.2205.00272},
  abstract = {Visual grounding is a task to locate the target indicated by a natural language expression. Existing methods extend the generic object detection framework to this problem. They base the visual grounding on the features from pre-generated proposals or anchors, and fuse these features with the text embeddings to locate the target mentioned by the text. However, modeling the visual features from these predefined locations may fail to fully exploit the visual context and attribute information in the text query, which limits their performance. In this paper, we propose a transformer-based framework for accurate visual grounding by establishing text-conditioned discriminative features and performing multi-stage cross-modal reasoning. Specifically, we develop a visual-linguistic verification module to focus the visual features on regions relevant to the textual descriptions while suppressing the unrelated areas. A language-guided feature encoder is also devised to aggregate the visual contexts of the target object to improve the object's distinctiveness. To retrieve the target from the encoded visual features, we further propose a multi-stage cross-modal decoder to iteratively speculate on the correlations between the image and text for accurate target localization. Extensive experiments on five widely used datasets validate the efficacy of our proposed components and demonstrate state-of-the-art performance. Our code is public at https://github.com/yangli18/VLTVG.},
  urldate = {2023-02-23},
  publisher = {arXiv},
  author = {Yang, Li and Xu, Yan and Yuan, Chunfeng and Liu, Wei and Li, Bing and Hu, Weiming},
  month = jun,
  year = {2022},
  note = {arXiv:2205.00272 [cs]},
}

@misc{chen_meta_2020,
  title = {Meta {Module} {Network} for {Compositional} {Visual} {Reasoning}},
  url = {http://arxiv.org/abs/1910.03230},
  doi = {10.48550/arXiv.1910.03230},
  abstract = {Neural Module Network (NMN) exhibits strong interpretability and compositionality thanks to its handcrafted neural modules with explicit multi-hop reasoning capability. However, most NMNs suffer from two critical drawbacks: 1) scalability: customized module for specific function renders it impractical when scaling up to a larger set of functions in complex tasks; 2) generalizability: rigid pre-defined module inventory makes it difficult to generalize to unseen functions in new tasks/domains. To design a more powerful NMN architecture for practical use, we propose Meta Module Network (MMN) centered on a novel meta module, which can take in function recipes and morph into diverse instance modules dynamically. The instance modules are then woven into an execution graph for complex visual reasoning, inheriting the strong explainability and compositionality of NMN. With such a flexible instantiation mechanism, the parameters of instance modules are inherited from the central meta module, retaining the same model complexity as the function set grows, which promises better scalability. Meanwhile, as functions are encoded into the embedding space, unseen functions can be readily represented based on its structural similarity with previously observed ones, which ensures better generalizability. Experiments on GQA and CLEVR datasets validate the superiority of MMN over state-of-the-art NMN designs. Synthetic experiments on held-out unseen functions from GQA dataset also demonstrate the strong generalizability of MMN. Our code and model are released in Github https://github.com/wenhuchen/Meta-Module-Network.},
  urldate = {2023-02-22},
  publisher = {arXiv},
  author = {Chen, Wenhu and Gan, Zhe and Li, Linjie and Cheng, Yu and Wang, William and Liu, Jingjing},
  month = nov,
  year = {2020},
  note = {arXiv:1910.03230 [cs]},
}

@misc{xiao_next-qanext_2021,
  title = {{NExT}-{QA}:{Next} {Phase} of {Question}-{Answering} to {Explaining} {Temporal} {Actions}},
  shorttitle = {{NExT}-{QA}},
  url = {http://arxiv.org/abs/2105.08276},
  doi = {10.48550/arXiv.2105.08276},
  abstract = {We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial scene description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git)},
  urldate = {2023-02-22},
  publisher = {arXiv},
  author = {Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  month = may,
  year = {2021},
  note = {arXiv:2105.08276 [cs]},
}

@misc{yu_refcoco_2016,
  title = {Modeling {Context} in {Referring} {Expressions}},
  url = {http://arxiv.org/abs/1608.00272},
  doi = {10.48550/arXiv.1608.00272},
  abstract = {Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg, shows the advantages of our methods for both referring expression generation and comprehension.},
  urldate = {2023-02-22},
  publisher = {arXiv},
  author = {Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C. and Berg, Tamara L.},
  month = aug,
  year = {2016},
  note = {arXiv:1608.00272 [cs]},
}

@misc{subramanian_reclip_2022,
  title = {{ReCLIP}: {A} {Strong} {Zero}-{Shot} {Baseline} for {Referring} {Expression} {Comprehension}},
  shorttitle = {{ReCLIP}},
  url = {http://arxiv.org/abs/2204.05991},
  doi = {10.48550/arXiv.2204.05991},
  abstract = {Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP's contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29\% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP's relative improvement over supervised ReC models trained on real images is 8\%.},
  urldate = {2023-02-22},
  publisher = {arXiv},
  author = {Subramanian, Sanjay and Merrill, William and Darrell, Trevor and Gardner, Matt and Singh, Sameer and Rohrbach, Anna},
  month = may,
  year = {2022},
  note = {arXiv:2204.05991 [cs]},
}

@misc{ni_lever_2023,
  title = {{LEVER}: {Learning} to {Verify} {Language}-to-{Code} {Generation} with {Execution}},
  shorttitle = {{LEVER}},
  url = {http://arxiv.org/abs/2302.08468},
  doi = {10.48550/arXiv.2302.08468},
  abstract = {The advent of pre-trained code language models (CodeLMs) has lead to significant progress in language-to-code generation. State-of-the-art approaches in this area combine CodeLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the CodeLM is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the CodeLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base CodeLMs (4.6\% to 10.9\% with code-davinci-002) and achieves new state-of-the-art results on all of them.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Ves and Yih, Wen-tau and Wang, Sida I. and Lin, Xi Victoria},
  month = feb,
  year = {2023},
  note = {arXiv:2302.08468 [cs]},
}

@misc{sammani_nlx-gpt_2022,
  title = {{NLX}-{GPT}: {A} {Model} for {Natural} {Language} {Explanations} in {Vision} and {Vision}-{Language} {Tasks}},
  shorttitle = {{NLX}-{GPT}},
  url = {http://arxiv.org/abs/2203.05081},
  doi = {10.48550/arXiv.2203.05081},
  abstract = {Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15\${\textbackslash}times\$ faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a self-evaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Sammani, Fawaz and Mukherjee, Tanmoy and Deligiannis, Nikos},
  month = mar,
  year = {2022},
  note = {arXiv:2203.05081 [cs]},
}

@misc{merullo_linearly_2022,
  title = {Linearly {Mapping} from {Image} to {Text} {Space}},
  url = {http://arxiv.org/abs/2209.15162},
  doi = {10.48550/arXiv.2209.15162},
  abstract = {The extent to which text-only language models (LMs) learn to represent the physical, non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to ``understand'' visual inputs when the models' parameters are updated on image captioning tasks. We test a stronger hypothesis: that the conceptual representations learned by text-only models are functionally equivalent (up to a linear transformation) to those learned by models trained on vision tasks. Specifically, we show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs.{\textbackslash} elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and Pavlick, Ellie},
  month = sep,
  year = {2022},
  note = {arXiv:2209.15162 [cs]},
}

@misc{udandarao_sus-x_2022,
  title = {{SuS}-{X}: {Training}-{Free} {Name}-{Only} {Transfer} of {Vision}-{Language} {Models}},
  shorttitle = {{SuS}-{X}},
  url = {http://arxiv.org/abs/2211.16198},
  doi = {10.48550/arXiv.2211.16198},
  abstract = {Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. We further show the utility of TIP-X in the training-free few-shot setting, where we again achieve state-of-the-art results over strong training-free baselines. Code is available at https://github.com/vishaal27/SuS-X.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Udandarao, Vishaal and Gupta, Ankush and Albanie, Samuel},
  month = nov,
  year = {2022},
  note = {arXiv:2211.16198 [cs]},
}

@misc{asperti_comparing_2022,
  title = {Comparing the latent space of generative models},
  url = {http://arxiv.org/abs/2207.06812},
  doi = {10.48550/arXiv.2207.06812},
  abstract = {Different encodings of datapoints in the latent space of latent-vector generative models may result in more or less effective and disentangled characterizations of the different explanatory factors of variation behind the data. Many works have been recently devoted to the explorationof the latent space of specific models, mostly focused on the study of how features are disentangled and of how trajectories producing desired alterations of data in the visible space can be found. In this work we address the more general problem of comparing the latent spaces of different models, looking for transformations between them. We confined the investigation to the familiar and largely investigated case of generative models for the data manifold of human faces. The surprising, preliminary result reported in this article is that (provided models have not been taught or explicitly conceived to act differently) a simple linear mapping is enough to pass from a latent space to another while preserving most of the information.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Asperti, Andrea and Tonelli, Valerio},
  month = jul,
  year = {2022},
  note = {arXiv:2207.06812 [cs]},
}

@misc{koh_grounding_2023,
  title = {Grounding {Language} {Models} to {Images} for {Multimodal} {Generation}},
  url = {http://arxiv.org/abs/2301.13823},
  doi = {10.48550/arXiv.2301.13823},
  abstract = {We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
  month = jan,
  year = {2023},
  note = {arXiv:2301.13823 [cs]},
}

@misc{navon_equivariant_2023,
  title = {Equivariant {Architectures} for {Learning} in {Deep} {Weight} {Spaces}},
  url = {http://arxiv.org/abs/2301.12780},
  doi = {10.48550/arXiv.2301.12780},
  abstract = {Designing machine learning architectures for processing neural networks in their raw weight matrix form is a newly introduced research direction. Unfortunately, the unique symmetry structure of deep weight spaces makes this design very challenging. If successful, such architectures would be capable of performing a wide range of intriguing tasks, from adapting a pre-trained network to a new domain to editing objects represented as functions (INRs or NeRFs). As a first step towards this goal, we present here a novel network architecture for learning in deep weight spaces. It takes as input a concatenation of weights and biases of a pre-trained MLP and processes it using a composition of layers that are equivariant to the natural permutation symmetry of the MLP's weights: Changing the order of neurons in intermediate layers of the MLP does not affect the function it represents. We provide a full characterization of all affine equivariant and invariant layers for these symmetries and show how these layers can be implemented using three basic operations: pooling, broadcasting, and fully connected layers applied to the input in an appropriate manner. We demonstrate the effectiveness of our architecture and its advantages over natural baselines in a variety of learning tasks.},
  urldate = {2023-02-21},
  publisher = {arXiv},
  author = {Navon, Aviv and Shamsian, Aviv and Achituve, Idan and Fetaya, Ethan and Chechik, Gal and Maron, Haggai},
  month = jan,
  year = {2023},
  note = {arXiv:2301.12780 [cs]},
}

@misc{li_blip-2_2023,
  title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
  shorttitle = {{BLIP}-2},
  url = {http://arxiv.org/abs/2301.12597},
  doi = {10.48550/arXiv.2301.12597},
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  urldate = {2023-02-17},
  publisher = {arXiv},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  month = jan,
  year = {2023},
  note = {arXiv:2301.12597 [cs]},
}

@misc{chen_altclip_2022,
  title = {{AltCLIP}: {Altering} the {Language} {Encoder} in {CLIP} for {Extended} {Language} {Capabilities}},
  shorttitle = {{AltCLIP}},
  url = {http://arxiv.org/abs/2211.06679},
  doi = {10.48550/arXiv.2211.06679},
  abstract = {In this work, we present a conceptually simple and effective method to train a strong bilingual/multilingual multimodal representation model. Starting from the pre-trained multimodal representation model CLIP released by OpenAI, we altered its text encoder with a pre-trained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k-CN, COCO-CN and XTD. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding. Our models and code are available at https://github.com/FlagAI-Open/FlagAI.},
  urldate = {2023-02-15},
  publisher = {arXiv},
  author = {Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell},
  month = nov,
  year = {2022},
  note = {arXiv:2211.06679 [cs]
version: 2},
}

@misc{schick_toolformer_2023,
  title = {Toolformer: {Language} {Models} {Can} {Teach} {Themselves} to {Use} {Tools}},
  shorttitle = {Toolformer},
  url = {http://arxiv.org/abs/2302.04761},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  urldate = {2023-02-10},
  publisher = {arXiv},
  author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  month = feb,
  year = {2023},
  note = {arXiv:2302.04761 [cs]},
}

@misc{sumers_distilling_2023,
  title = {Distilling {Internet}-{Scale} {Vision}-{Language} {Models} into {Embodied} {Agents}},
  url = {http://arxiv.org/abs/2301.12507},
  doi = {10.48550/arXiv.2301.12507},
  abstract = {Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.},
  urldate = {2023-02-06},
  publisher = {arXiv},
  author = {Sumers, Theodore and Marino, Kenneth and Ahuja, Arun and Fergus, Rob and Dasgupta, Ishita},
  month = jan,
  year = {2023},
  note = {arXiv:2301.12507 [cs]},
}

@misc{buch_revisiting_2022,
  title = {Revisiting the "{Video}" in {Video}-{Language} {Understanding}},
  url = {http://arxiv.org/abs/2206.01720},
  abstract = {What makes a video task uniquely suited for videos, beyond what can be understood from a single image? Building on recent progress in self-supervised image-language models, we revisit this question in the context of video and language tasks. We propose the atemporal probe (ATP), a new model for video-language analysis which provides a stronger bound on the baseline accuracy of multimodal models constrained by image-level understanding. By applying this model to standard discriminative video and language tasks, such as video question answering and text-to-video retrieval, we characterize the limitations and potential of current video-language benchmarks. We find that understanding of event temporality is often not necessary to achieve strong or state-of-the-art performance, even compared with recent large-scale video-language models and in contexts intended to benchmark deeper video-level understanding. We also demonstrate how ATP can improve both video-language dataset and model design. We describe a technique for leveraging ATP to better disentangle dataset subsets with a higher concentration of temporally challenging data, improving benchmarking efficacy for causal and temporal understanding. Further, we show that effectively integrating ATP into full video-level temporal models can improve efficiency and state-of-the-art accuracy.},
  urldate = {2023-01-24},
  publisher = {arXiv},
  author = {Buch, Shyamal and Eyzaguirre, Cristóbal and Gaidon, Adrien and Wu, Jiajun and Fei-Fei, Li and Niebles, Juan Carlos},
  month = jun,
  year = {2022},
  note = {arXiv:2206.01720 [cs]},
  keywords = {video},
}

@misc{noauthor_sidenotes_nodate,
  title = {{SideNotes} – {Quick} {Notes} on {Screen} {Side}},
  url = {https://www.apptorium.com/sidenotes},
  abstract = {A beautiful note taking app with superpowers. It shows and hides on the side of your monitor to manage your notes distraction-free. Keep your notes organised, personalised and always at your fingertips. Markdown, tasks, pictures, colors, folders included.},
  language = {en},
  urldate = {2023-01-02},
}

@article{marino_ok-vqa_2019,
  title = {{OK}-{VQA}: {A} {Visual} {Question} {Answering} {Benchmark} {Requiring} {External} {Knowledge}},
  shorttitle = {{OK}-{VQA}},
  url = {https://arxiv.org/abs/1906.00067v2},
  doi = {10.48550/arXiv.1906.00067},
  abstract = {Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content is not sufficient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difficult, and large compared to previous knowledge-based VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain. See http://okvqa.allenai.org to download and browse the dataset.},
  language = {en},
  urldate = {2022-12-24},
  author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  month = may,
  year = {2019},
}

@inproceedings{marino_ok-vqa_2019-1,
  address = {Long Beach, CA, USA},
  title = {{OK}-{VQA}: {A} {Visual} {Question} {Answering} {Benchmark} {Requiring} {External} {Knowledge}},
  isbn = {978-1-72813-293-8},
  shorttitle = {{OK}-{VQA}},
  url = {https://ieeexplore.ieee.org/document/8953725/},
  doi = {10.1109/CVPR.2019.00331},
  abstract = {Visual Question Answering (VQA) in its ideal form lets us study reasoning in the joint space of vision and language and serves as a proxy for the AI task of scene understanding. However, most VQA benchmarks to date are focused on questions such as simple counting, visual attributes, and object detection that do not require reasoning or knowledge beyond what is in the image. In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OKVQA, where the image content is not sufﬁcient to answer the questions, encouraging methods that rely on external knowledge resources. Our new dataset includes more than 14,000 questions that require external knowledge to answer. We show that the performance of the state-of-the-art VQA models degrades drastically in this new setting. Our analysis shows that our knowledge-based VQA task is diverse, difﬁcult, and large compared to previous knowledgebased VQA datasets. We hope that this dataset enables researchers to open up new avenues for research in this domain. See http://okvqa.allenai.org to download and browse the dataset.},
  language = {en},
  urldate = {2022-12-24},
  booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  month = jun,
  year = {2019},
  pages = {3190--3199},
}

@misc{hudson_gqa_2019,
  title = {{GQA}: {A} {New} {Dataset} for {Real}-{World} {Visual} {Reasoning} and {Compositional} {Question} {Answering}},
  shorttitle = {{GQA}},
  url = {http://arxiv.org/abs/1902.09506},
  doi = {10.48550/arXiv.1902.09506},
  abstract = {We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages scene graph structures to create 22M diverse reasoning questions, all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. An extensive analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains mere 42.1\%, and strong VQA models achieve 54.1\%, human performance tops at 89.3\%, offering ample opportunity for new research to explore. We strongly hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding for images and language.},
  urldate = {2022-12-23},
  publisher = {arXiv},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  month = may,
  year = {2019},
  note = {arXiv:1902.09506 [cs]},
}

@misc{alayrac_flamingo_2022,
  title = {Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}},
  shorttitle = {Flamingo},
  url = {http://arxiv.org/abs/2204.14198},
  doi = {10.48550/arXiv.2204.14198},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  urldate = {2022-12-17},
  publisher = {arXiv},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  month = nov,
  year = {2022},
  note = {arXiv:2204.14198 [cs]},
}

@misc{dou_empirical_2022,
  title = {An {Empirical} {Study} of {Training} {End}-to-{End} {Vision}-and-{Language} {Transformers}},
  url = {http://arxiv.org/abs/2111.02387},
  doi = {10.48550/arXiv.2111.02387},
  abstract = {Vision-and-language (VL) pre-training has proven to be highly effective on various VL downstream tasks. While recent work has shown that fully transformer-based VL models can be more efficient than previous region-feature-based methods, their performance on downstream tasks often degrades significantly. In this paper, we present METER, a Multimodal End-to-end TransformER framework, through which we investigate how to design and pre-train a fully transformer-based VL model in an end-to-end manner. Specifically, we dissect the model designs along multiple dimensions: vision encoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa, DeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention), architectural design (e.g., encoder-only vs. encoder-decoder), and pre-training objectives (e.g., masked image modeling). We conduct comprehensive experiments and provide insights on how to train a performant VL transformer. METER achieves an accuracy of 77.64\% on the VQAv2 test-std set using only 4M images for pre-training, surpassing the state-of-the-art region-feature-based model by 1.04\%, and outperforming the previous best fully transformer-based model by 1.6\%. Notably, when further scaled up, our best VQA model achieves an accuracy of 80.54\%. Code and pre-trained models are released at https://github.com/zdou0830/METER.},
  urldate = {2022-12-11},
  publisher = {arXiv},
  author = {Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and Liu, Zicheng and Zeng, Michael},
  month = mar,
  year = {2022},
  note = {arXiv:2111.02387 [cs]},
}

@misc{zhang_glipv2_2022,
  title = {{GLIPv2}: {Unifying} {Localization} and {Vision}-{Language} {Understanding}},
  shorttitle = {{GLIPv2}},
  url = {http://arxiv.org/abs/2206.05836},
  doi = {10.48550/arXiv.2206.05836},
  abstract = {We present GLIPv2, a grounded VL understanding model, that serves both localization tasks (e.g., object detection, instance segmentation) and Vision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2 elegantly unifies localization pre-training and Vision-Language Pre-training (VLP) with three pre-training tasks: phrase grounding as a VL reformulation of the detection task, region-word contrastive learning as a novel region-word level contrastive learning task, and the masked language modeling. This unification not only simplifies the previous multi-stage VLP procedure but also achieves mutual benefits between localization and understanding tasks. Experimental results show that a single GLIPv2 model (all model weights are shared) achieves near SoTA performance on various localization and understanding tasks. The model also shows (1) strong zero-shot and few-shot adaption performance on open-vocabulary object detection tasks and (2) superior grounding capability on VL understanding tasks. Code will be released at https://github.com/microsoft/GLIP.},
  urldate = {2022-12-07},
  publisher = {arXiv},
  author = {Zhang, Haotian and Zhang, Pengchuan and Hu, Xiaowei and Chen, Yen-Chun and Li, Liunian Harold and Dai, Xiyang and Wang, Lijuan and Yuan, Lu and Hwang, Jenq-Neng and Gao, Jianfeng},
  month = oct,
  year = {2022},
  note = {arXiv:2206.05836 [cs]},
}

@misc{kashani_deep_2022,
  title = {Deep {Learning} {Interviews}: {Hundreds} of fully solved job interview questions from a wide range of key topics in {AI}},
  shorttitle = {Deep {Learning} {Interviews}},
  url = {http://arxiv.org/abs/2201.00650},
  abstract = {The second edition of Deep Learning Interviews is home to hundreds of fully-solved problems, from a wide range of key topics in AI. It is designed to both rehearse interview or exam specific topics and provide machine learning MSc / PhD. students, and those awaiting an interview a well-organized overview of the field. The problems it poses are tough enough to cut your teeth on and to dramatically improve your skills-but they're framed within thought-provoking questions and engaging stories. That is what makes the volume so specifically valuable to students and job seekers: it provides them with the ability to speak confidently and quickly on any relevant topic, to answer technical questions clearly and correctly, and to fully understand the purpose and meaning of interview questions and answers. Those are powerful, indispensable advantages to have when walking into the interview room. The book's contents is a large inventory of numerous topics relevant to DL job interviews and graduate level exams. That places this work at the forefront of the growing trend in science to teach a core set of practical mathematical and computational skills. It is widely accepted that the training of every computer scientist must include the fundamental theorems of ML, and AI appears in the curriculum of nearly every university. This volume is designed as an excellent reference for graduates of such programs.},
  urldate = {2022-11-30},
  publisher = {arXiv},
  author = {Kashani, Shlomo and Ivry, Amir},
  month = jan,
  year = {2022},
  note = {arXiv:2201.00650 [cs, math]},
  keywords = {interviews},
}

@misc{mrini_rethinking_2020,
  title = {Rethinking {Self}-{Attention}: {Towards} {Interpretability} in {Neural} {Parsing}},
  shorttitle = {Rethinking {Self}-{Attention}},
  url = {http://arxiv.org/abs/1911.03875},
  doi = {10.48550/arXiv.1911.03875},
  abstract = {Attention mechanisms have improved the performance of NLP tasks while allowing models to remain explainable. Self-attention is currently widely used, however interpretability is difficult due to the numerous attention distributions. Recent work has shown that model representations can benefit from label-specific information, while facilitating interpretation of predictions. We introduce the Label Attention Layer: a new form of self-attention where attention heads represent labels. We test our novel layer by running constituency and dependency parsing experiments and show our new model obtains new state-of-the-art results for both tasks on both the Penn Treebank (PTB) and Chinese Treebank. Additionally, our model requires fewer self-attention layers compared to existing work. Finally, we find that the Label Attention heads learn relations between syntactic categories and show pathways to analyze errors.},
  urldate = {2022-11-23},
  publisher = {arXiv},
  author = {Mrini, Khalil and Dernoncourt, Franck and Tran, Quan and Bui, Trung and Chang, Walter and Nakashole, Ndapa},
  month = oct,
  year = {2020},
  note = {arXiv:1911.03875 [cs]},
}

@misc{noauthor_netflix_nodate,
  title = {Netflix},
  url = {https://www.netflix.com/watch/81460598?trackId=255824129},
  urldate = {2022-11-21},
}

@misc{zhang_opt_2022,
  title = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
  shorttitle = {{OPT}},
  url = {http://arxiv.org/abs/2205.01068},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  urldate = {2022-11-16},
  publisher = {arXiv},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  month = jun,
  year = {2022},
  note = {arXiv:2205.01068 [cs]},
}

@misc{wah_caltech-ucsd_2011,
  type = {Report or {Paper}},
  title = {The {Caltech}-{UCSD} {Birds}-200-2011 {Dataset}},
  copyright = {other},
  url = {https://resolver.caltech.edu/CaltechAUTHORS:20111026-120541847},
  abstract = {CUB-200-2011 is an extended version of CUB-200 [7], a challenging dataset of 200 bird species. The extended version roughly doubles the number of images per category and adds new part localization annotations. All images are annotated with bounding boxes, part locations, and at- tribute labels. Images and annotations were filtered by mul- tiple users of Mechanical Turk. We introduce benchmarks and baseline experiments for multi-class categorization and part localization.},
  language = {en},
  urldate = {2022-11-16},
  author = {Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  month = jul,
  year = {2011},
  note = {Issue: 2010-001
Num Pages: 8
Number: 2010-001
Place: Pasadena, CA
Publisher: California Institute of Technology},
}

@inproceedings{parkhi_cats_2012,
  title = {Cats and dogs},
  doi = {10.1109/CVPR.2012.6248092},
  abstract = {We investigate the fine grained object categorization problem of determining the breed of animal from an image. To this end we introduce a new annotated dataset of pets covering 37 different breeds of cats and dogs. The visual problem is very challenging as these animals, particularly cats, are very deformable and there can be quite subtle differences between the breeds. We make a number of contributions: first, we introduce a model to classify a pet breed automatically from an image. The model combines shape, captured by a deformable part model detecting the pet face, and appearance, captured by a bag-of-words model that describes the pet fur. Fitting the model involves automatically segmenting the animal in the image. Second, we compare two classification approaches: a hierarchical one, in which a pet is first assigned to the cat or dog family and then to a breed, and a flat one, in which the breed is obtained directly. We also investigate a number of animal and image orientated spatial layouts. These models are very good: they beat all previously published results on the challenging ASIRRA test (cat vs dog discrimination). When applied to the task of discriminating the 37 different breeds of pets, the models obtain an average accuracy of about 59\%, a very encouraging result considering the difficulty of the problem.},
  booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
  month = jun,
  year = {2012},
  note = {ISSN: 1063-6919},
  pages = {3498--3505},
}

@inproceedings{bossard_food-101_2014,
  address = {Cham},
  series = {Lecture {Notes} in {Computer} {Science}},
  title = {Food-101 – {Mining} {Discriminative} {Components} with {Random} {Forests}},
  isbn = {978-3-319-10599-4},
  doi = {10.1007/978-3-319-10599-4_29},
  abstract = {In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101’000 images. With an average accuracy of 50.76\%, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88\% and 8.13\%, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods.},
  language = {en},
  booktitle = {Computer {Vision} – {ECCV} 2014},
  publisher = {Springer International Publishing},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  pages = {446--461},
}

@inproceedings{cimpoi_describing_2014,
  title = {Describing {Textures} in the {Wild}},
  url = {https://openaccess.thecvf.com/content_cvpr_2014/html/Cimpoi_Describing_Textures_in_2014_CVPR_paper.html},
  urldate = {2022-11-16},
  author = {Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  year = {2014},
  pages = {3606--3613},
}

@misc{gilpin_explaining_2019,
  title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
  shorttitle = {Explaining {Explanations}},
  url = {http://arxiv.org/abs/1806.00069},
  doi = {10.48550/arXiv.1806.00069},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  urldate = {2022-11-15},
  publisher = {arXiv},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  month = feb,
  year = {2019},
  note = {arXiv:1806.00069 [cs, stat]},
}

@misc{ilharco_patching_2022,
  title = {Patching open-vocabulary models by interpolating weights},
  url = {http://arxiv.org/abs/2208.05592},
  doi = {10.48550/arXiv.2208.05592},
  abstract = {Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch.},
  urldate = {2022-11-11},
  publisher = {arXiv},
  author = {Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  month = oct,
  year = {2022},
  note = {arXiv:2208.05592 [cs]},
}

@misc{menon_visual_2022-1,
  title = {Visual {Classification} via {Description} from {Large} {Language} {Models}},
  url = {http://arxiv.org/abs/2210.07183},
  doi = {10.48550/arXiv.2210.07183},
  abstract = {Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline.},
  urldate = {2022-11-10},
  publisher = {arXiv},
  author = {Menon, Sachit and Vondrick, Carl},
  month = oct,
  year = {2022},
  note = {arXiv:2210.07183 [cs]},
}

@misc{wei_emergent_2022,
  title = {Emergent {Abilities} of {Large} {Language} {Models}},
  url = {http://arxiv.org/abs/2206.07682},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  urldate = {2022-11-10},
  publisher = {arXiv},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  month = oct,
  year = {2022},
  note = {arXiv:2206.07682 [cs]},
}

@inproceedings{sap_social_2019,
  address = {Hong Kong, China},
  title = {Social {IQa}: {Commonsense} {Reasoning} about {Social} {Interactions}},
  shorttitle = {Social {IQa}},
  url = {https://aclanthology.org/D19-1454},
  doi = {10.18653/v1/D19-1454},
  abstract = {We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: “Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?” A: “Make sure no one else could hear”). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({\textbackslash}textgreater20\% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
  urldate = {2022-11-10},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher = {Association for Computational Linguistics},
  author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  month = nov,
  year = {2019},
  pages = {4463--4473},
}

@misc{wei_chain_2022,
  title = {Chain of {Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
  url = {http://arxiv.org/abs/2201.11903},
  doi = {10.48550/arXiv.2201.11903},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  urldate = {2022-11-10},
  publisher = {arXiv},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  month = oct,
  year = {2022},
  note = {arXiv:2201.11903 [cs]},
}

@misc{helber_eurosat_2019,
  title = {{EuroSAT}: {A} {Novel} {Dataset} and {Deep} {Learning} {Benchmark} for {Land} {Use} and {Land} {Cover} {Classification}},
  shorttitle = {{EuroSAT}},
  url = {http://arxiv.org/abs/1709.00029},
  abstract = {In this paper, we address the challenge of land use and land cover classification using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible provided in the Earth observation program Copernicus. We present a novel dataset based on Sentinel-2 satellite images covering 13 spectral bands and consisting out of 10 classes with in total 27,000 labeled and geo-referenced images. We provide benchmarks for this novel dataset with its spectral bands using state-of-the-art deep Convolutional Neural Network (CNNs). With the proposed novel dataset, we achieved an overall classification accuracy of 98.57\%. The resulting classification system opens a gate towards a number of Earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes and how it can assist in improving geographical maps. The geo-referenced dataset EuroSAT is made publicly available at https://github.com/phelber/eurosat.},
  urldate = {2022-11-08},
  publisher = {arXiv},
  author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  month = feb,
  year = {2019},
  note = {arXiv:1709.00029 [cs]},
}

@misc{tzelepis_contraclip_2022,
  title = {{ContraCLIP}: {Interpretable} {GAN} generation driven by pairs of contrasting sentences},
  shorttitle = {{ContraCLIP}},
  url = {http://arxiv.org/abs/2206.02104},
  doi = {10.48550/arXiv.2206.02104},
  abstract = {This work addresses the problem of discovering non-linear interpretable paths in the latent space of pre-trained GANs in a model-agnostic manner. In the proposed method, the discovery is driven by a set of pairs of natural language sentences with contrasting semantics, named semantic dipoles, that serve as the limits of the interpretation that we require by the trainable latent paths to encode. By using the pre-trained CLIP encoder, the sentences are projected into the vision-language space, where they serve as dipoles, and where RBF-based warping functions define a set of non-linear directional paths, one for each semantic dipole, allowing in this way traversals from one semantic pole to the other. By defining an objective that discovers paths in the latent space of GANs that generate changes along the desired paths in the vision-language embedding space, we provide an intuitive way of controlling the underlying generative factors and address some of the limitations of the state-of-the-art works, namely, that a) they are typically tailored to specific GAN architectures (i.e., StyleGAN), b) they disregard the relative position of the manipulated and the original image in the image embedding and the relative position of the image and the text embeddings, and c) they lead to abrupt image manipulations and quickly arrive at regions of low density and, thus, low image quality, providing limited control of the generative factors. We provide extensive qualitative and quantitative results that demonstrate our claims with two pre-trained GANs, and make the code and the pre-trained models publicly available at: https://github.com/chi0tzp/ContraCLIP},
  urldate = {2022-11-07},
  publisher = {arXiv},
  author = {Tzelepis, Christos and Oldfield, James and Tzimiropoulos, Georgios and Patras, Ioannis},
  month = jun,
  year = {2022},
  note = {arXiv:2206.02104 [cs]},
}

@misc{balaji_ediffi_2022,
  title = {{eDiffi}: {Text}-to-{Image} {Diffusion} {Models} with an {Ensemble} of {Expert} {Denoisers}},
  shorttitle = {{eDiffi}},
  url = {http://arxiv.org/abs/2211.01324},
  abstract = {Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiffi, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiffi's "paint-with-words" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiffi/},
  urldate = {2022-11-07},
  publisher = {arXiv},
  author = {Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and Catanzaro, Bryan and Karras, Tero and Liu, Ming-Yu},
  month = nov,
  year = {2022},
  note = {arXiv:2211.01324 [cs]},
}

@misc{hertz_prompt--prompt_2022,
  title = {Prompt-to-{Prompt} {Image} {Editing} with {Cross} {Attention} {Control}},
  url = {http://arxiv.org/abs/2208.01626},
  doi = {10.48550/arXiv.2208.01626},
  abstract = {Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.},
  urldate = {2022-11-04},
  publisher = {arXiv},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  month = aug,
  year = {2022},
  note = {arXiv:2208.01626 [cs]},
  keywords = {prompting},
}

@misc{patashnik_styleclip_2021,
  title = {{StyleCLIP}: {Text}-{Driven} {Manipulation} of {StyleGAN} {Imagery}},
  shorttitle = {{StyleCLIP}},
  url = {http://arxiv.org/abs/2103.17249},
  abstract = {Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable text-based manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGAN's style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.},
  urldate = {2022-10-21},
  publisher = {arXiv},
  author = {Patashnik, Or and Wu, Zongze and Shechtman, Eli and Cohen-Or, Daniel and Lischinski, Dani},
  month = mar,
  year = {2021},
  note = {arXiv:2103.17249 [cs]},
}

@misc{crowson_vqgan-clip_2022,
  title = {{VQGAN}-{CLIP}: {Open} {Domain} {Image} {Generation} and {Editing} with {Natural} {Language} {Guidance}},
  shorttitle = {{VQGAN}-{CLIP}},
  url = {http://arxiv.org/abs/2204.08583},
  abstract = {Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [37] to guide VQGAN [11] produces higher visual quality outputs than prior, less flexible approaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being trained for the tasks presented. Our code is available in a public repository.},
  urldate = {2022-10-21},
  publisher = {arXiv},
  author = {Crowson, Katherine and Biderman, Stella and Kornis, Daniel and Stander, Dashiell and Hallahan, Eric and Castricato, Louis and Raff, Edward},
  month = sep,
  year = {2022},
  note = {arXiv:2204.08583 [cs]},
}

@misc{ramesh_hierarchical_2022,
  title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
  url = {http://arxiv.org/abs/2204.06125},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  urldate = {2022-10-21},
  publisher = {arXiv},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  month = apr,
  year = {2022},
  note = {arXiv:2204.06125 [cs]},
}

@incollection{feris_embarrassingly_2017,
  address = {Cham},
  title = {An {Embarrassingly} {Simple} {Approach} to {Zero}-{Shot} {Learning}},
  isbn = {978-3-319-50075-1 978-3-319-50077-5},
  url = {http://link.springer.com/10.1007/978-3-319-50077-5_2},
  abstract = {Zero-shot learning consists in learning how to recognise new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalisation error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform signiﬁcantly better than the state of art on all of them, obtaining a ratio of improvement up to 17\%.},
  language = {en},
  urldate = {2022-09-29},
  booktitle = {Visual {Attributes}},
  publisher = {Springer International Publishing},
  author = {Romera-Paredes, Bernardino and Torr, Philip H. S.},
  editor = {Feris, Rogerio Schmidt and Lampert, Christoph and Parikh, Devi},
  year = {2017},
  doi = {10.1007/978-3-319-50077-5_2},
  note = {Series Title: Advances in Computer Vision and Pattern Recognition},
  pages = {11--30},
}

@misc{xian_zero-shot_2020,
  title = {Zero-{Shot} {Learning} -- {A} {Comprehensive} {Evaluation} of the {Good}, the {Bad} and the {Ugly}},
  url = {http://arxiv.org/abs/1707.00600},
  abstract = {Due to the importance of zero-shot learning, i.e. classifying images where there is a lack of labeled training data, the number of proposed approaches has recently increased steadily. We argue that it is time to take a step back and to analyze the status quo of the area. The purpose of this paper is three-fold. First, given the fact that there is no agreed upon zero-shot learning benchmark, we first define a new benchmark by unifying both the evaluation protocols and data splits of publicly available datasets used for this task. This is an important contribution as published results are often not comparable and sometimes even flawed due to, e.g. pre-training on zero-shot test classes. Moreover, we propose a new zero-shot learning dataset, the Animals with Attributes 2 (AWA2) dataset which we make publicly available both in terms of image features and the images themselves. Second, we compare and analyze a significant number of the state-of-the-art methods in depth, both in the classic zero-shot setting but also in the more realistic generalized zero-shot setting. Finally, we discuss in detail the limitations of the current status of the area which can be taken as a basis for advancing it.},
  urldate = {2022-09-29},
  publisher = {arXiv},
  author = {Xian, Yongqin and Lampert, Christoph H. and Schiele, Bernt and Akata, Zeynep},
  month = sep,
  year = {2020},
  note = {arXiv:1707.00600 [cs]},
}

@misc{noauthor_embarrassingly_nodate,
  title = {An {Embarrassingly} {Simple} {Approach} to {Zero}-{Shot} {Learning} {\textbar} {SpringerLink}},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-50077-5_2},
  urldate = {2022-09-29},
}

@misc{socher_zero-shot_2013,
  title = {Zero-{Shot} {Learning} {Through} {Cross}-{Modal} {Transfer}},
  url = {http://arxiv.org/abs/1301.3666},
  abstract = {This work introduces a model that can recognize objects in images even if no training data is available for the objects. The only necessary knowledge about the unseen categories comes from unsupervised large text corpora. In our zero-shot framework distributional information in language can be seen as spanning a semantic basis for understanding what objects look like. Most previous zero-shot learning models can only differentiate between unseen classes. In contrast, our model can both obtain state of the art performance on classes that have thousands of training images and obtain reasonable performance on unseen classes. This is achieved by first using outlier detection in the semantic space and then two separate recognition models. Furthermore, our model does not require any manually defined semantic features for either words or images.},
  urldate = {2022-09-29},
  publisher = {arXiv},
  author = {Socher, Richard and Ganjoo, Milind and Sridhar, Hamsa and Bastani, Osbert and Manning, Christopher D. and Ng, Andrew Y.},
  month = mar,
  year = {2013},
  note = {arXiv:1301.3666 [cs]},
}

@misc{xu_vgse_2022,
  title = {{VGSE}: {Visually}-{Grounded} {Semantic} {Embeddings} for {Zero}-{Shot} {Learning}},
  shorttitle = {{VGSE}},
  url = {http://arxiv.org/abs/2203.10444},
  abstract = {Human-annotated attributes serve as powerful semantic embeddings in zero-shot learning. However, their annotation process is labor-intensive and needs expert supervision. Current unsupervised semantic embeddings, i.e., word embeddings, enable knowledge transfer between classes. However, word embeddings do not always reflect visual similarities and result in inferior zero-shot performance. We propose to discover semantic embeddings containing discriminative visual properties for zero-shot learning, without requiring any human annotation. Our model visually divides a set of images from seen classes into clusters of local image regions according to their visual similarity, and further imposes their class discrimination and semantic relatedness. To associate these clusters with previously unseen classes, we use external knowledge, e.g., word embeddings and propose a novel class relation discovery module. Through quantitative and qualitative evaluation, we demonstrate that our model discovers semantic embeddings that model the visual properties of both seen and unseen classes. Furthermore, we demonstrate on three benchmarks that our visually-grounded semantic embeddings further improve performance over word embeddings across various ZSL models by a large margin.},
  urldate = {2022-09-29},
  publisher = {arXiv},
  author = {Xu, Wenjia and Xian, Yongqin and Wang, Jiuniu and Schiele, Bernt and Akata, Zeynep},
  month = mar,
  year = {2022},
  note = {arXiv:2203.10444 [cs]},
}

@inproceedings{parikh_interactively_2011,
  address = {Colorado Springs, CO, USA},
  title = {Interactively building a discriminative vocabulary of nameable attributes},
  isbn = {978-1-4577-0394-2},
  url = {http://ieeexplore.ieee.org/document/5995451/},
  doi = {10.1109/CVPR.2011.5995451},
  abstract = {Human-nameable visual attributes offer many advantages when used as mid-level features for object recognition, but existing techniques to gather relevant attributes can be inefﬁcient (costing substantial effort or expertise) and/or insufﬁcient (descriptive properties need not be discriminative). We introduce an approach to deﬁne a vocabulary of attributes that is both human understandable and discriminative. The system takes object/scene-labeled images as input, and returns as output a set of attributes elicited from human annotators that distinguish the categories of interest. To ensure a compact vocabulary and efﬁcient use of annotators’ effort, we 1) show how to actively augment the vocabulary such that new attributes resolve inter-class confusions, and 2) propose a novel “nameability” manifold that prioritizes candidate attributes by their likelihood of being associated with a nameable property. We demonstrate the approach with multiple datasets, and show its clear advantages over baselines that lack a nameability model or rely on a list of expert-provided attributes.},
  language = {en},
  urldate = {2022-09-29},
  booktitle = {{CVPR} 2011},
  publisher = {IEEE},
  author = {Parikh, Devi and Grauman, Kristen},
  month = jun,
  year = {2011},
  pages = {1681--1688},
}

@article{lampert_attribute-based_2014,
  title = {Attribute-{Based} {Classification} for {Zero}-{Shot} {Visual} {Object} {Categorization}},
  volume = {36},
  issn = {0162-8828, 2160-9292},
  url = {http://ieeexplore.ieee.org/document/6571196/},
  doi = {10.1109/TPAMI.2013.140},
  abstract = {We study the problem of object recognition for categories for which we have no training examples, a task also called zero-data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object’s color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes.},
  language = {en},
  number = {3},
  urldate = {2022-09-29},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},
  month = mar,
  year = {2014},
  pages = {453--465},
}

@misc{wikipedia_white_2022,
  title = {White wedding},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url = {https://en.wikipedia.org/w/index.php?title=White_wedding&oldid=1112110253},
  abstract = {A white wedding is a traditional formal or semi-formal wedding originating in Great Britain.
The term originates from the white colour of the wedding dress, which first became popular with Victorian era elites after Queen Victoria wore a white lace dress at her wedding. The term now also encapsulates the entire Western wedding routine, especially in the Christian religious tradition, which generally includes a church service during which the marriage begins, followed by a reception. The white wedding style was given another significant boost in 1981, when 750 million people watched Charles, Prince of Wales marry Diana Spencer in her elaborate white taffeta dress with an 8 m train. This wedding is generally considered the most influential white wedding of the 1980s.},
  language = {en},
  urldate = {2022-09-25},
  journal = {Wikipedia},
  author = {Wikipedia},
  month = sep,
  year = {2022},
  note = {Page Version ID: 1112110253},
}

@misc{wikipedia_ever_2022,
  title = {\textit{{Ever} {Given}}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url = {https://en.wikipedia.org/w/index.php?title=Ever_Given&oldid=1105011722},
  abstract = {Ever Given (simplified Chinese: 长赐轮; traditional Chinese: 長賜輪; pinyin: Cháng Cì Lún) is one of the largest container ships in the world. The ship is owned by Shoei Kisen Kaisha (a ship-owning and leasing subsidiary of the large Japanese shipbuilding company Imabari Shipbuilding), and is time chartered and operated by container transportation and shipping company Evergreen Marine, headquartered in Luzhu, Taoyuan, Taiwan. Ever Given is registered in Panama and her technical management is the responsibility of the German ship management company Bernhard Schulte Shipmanagement (BSM).On 23 March 2021, while traveling from Tanjung Pelepas in Malaysia to Rotterdam in the Netherlands under Captain Krishnan Kanthavel, the ship ran aground in the Suez Canal. She remained in place for six days before salvage crews freed her on 29 March 2021. The vessel was impounded by the Egyptian government on 13 April 2021 for refusing to pay a reported \$916 million in fees demanded by the government, including \$300 million in "loss of reputation." The compensation claim was later cut down to \$600 million. In early July 2021, the ship was released by the Egyptian authorities following an agreement on compensation.},
  language = {en},
  urldate = {2022-09-27},
  journal = {Wikipedia},
  author = {Wikipedia},
  month = aug,
  year = {2022},
  note = {Page Version ID: 1105011722},
}

@misc{wikipedia_wordle_2022,
  title = {\textit{{Wordle}}},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url = {https://en.wikipedia.org/w/index.php?title=Wordle&oldid=1112595044},
  abstract = {Wordle is a web-based word game created and developed by Welsh software engineer Josh Wardle, and owned and published by The New York Times Company since 2022. Players have six attempts to guess a five-letter word, with feedback given for each guess in the form of colored tiles indicating when letters match or occupy the correct position. The mechanics are nearly identical to the 1955 pen-and-paper game Jotto and the television game show franchise Lingo.  Wordle has a single daily solution, with all players attempting to guess the same word.
Wardle initially created the game for himself and his partner to play, eventually making it public in October 2021. The game gained a large amount of popularity in December 2021 after Wardle added the ability for players to copy their daily results as emoji squares, which were widely shared on Twitter. Many clones and variations of the game were also created, as were versions in languages besides English. The game was purchased by The New York Times Company in January 2022 for an undisclosed seven-figure sum, with plans to keep it free for all players; it was moved to the company's website in February 2022.},
  language = {en},
  urldate = {2022-09-27},
  journal = {Wikipedia},
  author = {Wikipedia},
  month = sep,
  year = {2022},
  note = {Page Version ID: 1112595044},
}

@misc{pratt_what_2022,
  title = {What does a platypus look like? {Generating} customized prompts for zero-shot image classification},
  shorttitle = {What does a platypus look like?},
  url = {http://arxiv.org/abs/2209.03320},
  doi = {10.48550/arXiv.2209.03320},
  abstract = {Open vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a \{\}") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without using explicit knowledge of the image domain and with far fewer hand-constructed sentences. To achieve this, we combine open vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that are customized for each object category. We find that this straightforward and general approach improves accuracy on a range of zero-shot image classification benchmarks, including over one percentage point gain on ImageNet. Finally, this method requires no additional training and remains completely zero-shot. Code is available at https://github.com/sarahpratt/CuPL.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Pratt, Sarah and Liu, Rosanne and Farhadi, Ali},
  month = sep,
  year = {2022},
  note = {arXiv:2209.03320 [cs]},
}

@misc{kamath_mdetr_2021,
  title = {{MDETR} -- {Modulated} {Detection} for {End}-to-{End} {Multi}-{Modal} {Understanding}},
  url = {http://arxiv.org/abs/2104.12763},
  abstract = {Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets having explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  month = oct,
  year = {2021},
  note = {arXiv:2104.12763 [cs]},
}

@misc{yuan_florence_2021,
  title = {Florence: {A} {New} {Foundation} {Model} for {Computer} {Vision}},
  shorttitle = {Florence},
  url = {http://arxiv.org/abs/2111.11432},
  doi = {10.48550/arXiv.2111.11432},
  abstract = {Automated visual understanding of our diverse and open world demands computer vision models to generalize well with minimal customization for specific tasks, similar to human vision. Computer vision foundation models, which are trained on diverse, large-scale dataset and can be adapted to a wide range of downstream tasks, are critical for this mission to solve real-world computer vision applications. While existing vision foundation models such as CLIP, ALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual representations to a cross-modal shared representation, we introduce a new computer vision foundation model, Florence, to expand the representations from coarse (scene) to fine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities (caption, depth). By incorporating universal visual-language representations from Web-scale image-text data, our Florence model can be easily adapted for various computer vision tasks, such as classification, retrieval, object detection, VQA, image caption, video retrieval and action recognition. Moreover, Florence demonstrates outstanding performance in many types of transfer learning: fully sampled fine-tuning, linear probing, few-shot transfer and zero-shot transfer for novel images and objects. All of these properties are critical for our vision foundation model to serve general purpose vision tasks. Florence achieves new state-of-the-art results in majority of 44 representative benchmarks, e.g., ImageNet-1K zero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of 97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Liu, Mengchen and Liu, Zicheng and Lu, Yumao and Shi, Yu and Wang, Lijuan and Wang, Jianfeng and Xiao, Bin and Xiao, Zhen and Yang, Jianwei and Zeng, Michael and Zhou, Luowei and Zhang, Pengchuan},
  month = nov,
  year = {2021},
  note = {arXiv:2111.11432 [cs]},
}

@misc{recht_imagenet_2019,
  title = {Do {ImageNet} {Classifiers} {Generalize} to {ImageNet}?},
  url = {http://arxiv.org/abs/1902.10811},
  doi = {10.48550/arXiv.1902.10811},
  abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3\% - 15\% on CIFAR-10 and 11\% - 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly "harder" images than those found in the original test sets.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  month = jun,
  year = {2019},
  note = {arXiv:1902.10811 [cs, stat]},
}

@misc{kumar_fine-tuning_2022,
  title = {Fine-{Tuning} can {Distort} {Pretrained} {Features} and {Underperform} {Out}-of-{Distribution}},
  url = {http://arxiv.org/abs/2202.10054},
  doi = {10.48550/arXiv.2202.10054},
  abstract = {When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \${\textbackslash}to\$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
  month = feb,
  year = {2022},
  note = {arXiv:2202.10054 [cs]},
}

@misc{zhao_vl-checklist_2022,
  title = {{VL}-{CheckList}: {Evaluating} {Pre}-trained {Vision}-{Language} {Models} with {Objects}, {Attributes} and {Relations}},
  shorttitle = {{VL}-{CheckList}},
  url = {http://arxiv.org/abs/2207.00221},
  doi = {10.48550/arXiv.2207.00221},
  abstract = {Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we introduce VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared models that were not visible from downstream task-only evaluation. Further results show promising research direction in building better VLP models. Data and Code: https://github.com/om-ai-lab/VL-CheckList},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Zhao, Tiancheng and Zhang, Tianqi and Zhu, Mingwei and Shen, Haozhan and Lee, Kyusong and Lu, Xiaopeng and Yin, Jianwei},
  month = jul,
  year = {2022},
  note = {arXiv:2207.00221 [cs]},
}

@misc{zeng_socratic_2022,
  title = {Socratic {Models}: {Composing} {Zero}-{Shot} {Multimodal} {Reasoning} with {Language}},
  shorttitle = {Socratic {Models}},
  url = {http://arxiv.org/abs/2204.00598},
  abstract = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  month = may,
  year = {2022},
  note = {arXiv:2204.00598 [cs]},
}

@misc{sammani_nlx-gpt_2022-1,
  title = {{NLX}-{GPT}: {A} {Model} for {Natural} {Language} {Explanations} in {Vision} and {Vision}-{Language} {Tasks}},
  shorttitle = {{NLX}-{GPT}},
  url = {http://arxiv.org/abs/2203.05081},
  abstract = {Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15\${\textbackslash}times\$ faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a self-evaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Sammani, Fawaz and Mukherjee, Tanmoy and Deligiannis, Nikos},
  month = mar,
  year = {2022},
  note = {arXiv:2203.05081 [cs]},
}

@misc{shen_k-lite_2022,
  title = {K-{LITE}: {Learning} {Transferable} {Visual} {Models} with {External} {Knowledge}},
  shorttitle = {K-{LITE}},
  url = {http://arxiv.org/abs/2204.09222},
  doi = {10.48550/arXiv.2204.09222},
  abstract = {Recent state-of-the-art computer vision systems are trained from natural language supervision, ranging from simple object category names to descriptive captions. This free form of supervision ensures high generality and usability of the learned visual models, based on extensive heuristics on data collection to cover as many visual concepts as possible. Alternatively, learning with external knowledge about images is a promising way which leverages a much more structured source of supervision. In this paper, we propose K-LITE (Knowledge-augmented Language-Image Training and Evaluation), a simple strategy to leverage external knowledge to build transferable visual systems: In training, it enriches entities in natural language with WordNet and Wiktionary knowledge, leading to an efficient and scalable approach to learning image representations that can understand both visual concepts and their knowledge; In evaluation, the natural language is also augmented with external knowledge and then used to reference learned visual concepts (or describe new ones) to enable zero-shot and few-shot transfer of the pre-trained models. We study the performance of K-LITE on two important computer vision problems, image classification and object detection, benchmarking on 20 and 13 different existing datasets, respectively. The proposed knowledge-augmented models show significant improvement in transfer learning performance over existing methods.},
  urldate = {2022-09-27},
  publisher = {arXiv},
  author = {Shen, Sheng and Li, Chunyuan and Hu, Xiaowei and Xie, Yujia and Yang, Jianwei and Zhang, Pengchuan and Rohrbach, Anna and Gan, Zhe and Wang, Lijuan and Yuan, Lu and Liu, Ce and Keutzer, Kurt and Darrell, Trevor and Gao, Jianfeng},
  month = apr,
  year = {2022},
  note = {arXiv:2204.09222 [cs]},
}

@misc{singh_flava_2022,
  title = {{FLAVA}: {A} {Foundational} {Language} {And} {Vision} {Alignment} {Model}},
  shorttitle = {{FLAVA}},
  url = {http://arxiv.org/abs/2112.04482},
  doi = {10.48550/arXiv.2112.04482},
  abstract = {State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal (with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising direction would be to use a single holistic universal model, as a "foundation", that targets all modalities at once -- a true vision and language foundation model should be good at vision tasks, language tasks, and cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate impressive performance on a wide range of 35 tasks spanning these target modalities.},
  urldate = {2022-09-26},
  publisher = {arXiv},
  author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  month = mar,
  year = {2022},
  note = {arXiv:2112.04482 [cs]},
}

@misc{vinyals_matching_2017,
  title = {Matching {Networks} for {One} {Shot} {Learning}},
  url = {http://arxiv.org/abs/1606.04080},
  abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
  urldate = {2022-09-23},
  publisher = {arXiv},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
  month = dec,
  year = {2017},
  note = {arXiv:1606.04080 [cs, stat]},
}

@misc{snell_prototypical_2017,
  title = {Prototypical {Networks} for {Few}-shot {Learning}},
  url = {http://arxiv.org/abs/1703.05175},
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-the-art results on the CU-Birds dataset.},
  urldate = {2022-09-23},
  publisher = {arXiv},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  month = jun,
  year = {2017},
  note = {arXiv:1703.05175 [cs, stat]},
}

@misc{chen_this_2019,
  title = {This {Looks} {Like} {That}: {Deep} {Learning} for {Interpretable} {Image} {Recognition}},
  shorttitle = {This {Looks} {Like} {That}},
  url = {http://arxiv.org/abs/1806.10574},
  doi = {10.48550/arXiv.1806.10574},
  abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
  urldate = {2022-09-23},
  publisher = {arXiv},
  author = {Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  month = dec,
  year = {2019},
  note = {arXiv:1806.10574 [cs, stat]},
}

@misc{hoffmann_this_2021,
  title = {This {Looks} {Like} {That}... {Does} it? {Shortcomings} of {Latent} {Space} {Prototype} {Interpretability} in {Deep} {Networks}},
  shorttitle = {This {Looks} {Like} {That}... {Does} it?},
  url = {http://arxiv.org/abs/2105.02968},
  doi = {10.48550/arXiv.2105.02968},
  abstract = {Deep neural networks that yield human interpretable decisions by architectural design have lately become an increasingly popular alternative to post hoc interpretation of traditional black-box models. Among these networks, the arguably most widespread approach is so-called prototype learning, where similarities to learned latent prototypes serve as the basis of classifying an unseen data point. In this work, we point to an important shortcoming of such approaches. Namely, there is a semantic gap between similarity in latent space and similarity in input space, which can corrupt interpretability. We design two experiments that exemplify this issue on the so-called ProtoPNet. Specifically, we find that this network's interpretability mechanism can be led astray by intentionally crafted or even JPEG compression artefacts, which can produce incomprehensible decisions. We argue that practitioners ought to have this shortcoming in mind when deploying prototype-based models in practice.},
  urldate = {2022-09-23},
  publisher = {arXiv},
  author = {Hoffmann, Adrian and Fanconi, Claudio and Rade, Rahul and Kohler, Jonas},
  month = jun,
  year = {2021},
  note = {arXiv:2105.02968 [cs]},
}

@inproceedings{atwood_inclusive_2020,
  address = {Cham},
  series = {The {Springer} {Series} on {Challenges} in {Machine} {Learning}},
  title = {The {Inclusive} {Images} {Competition}},
  isbn = {978-3-030-29135-8},
  doi = {10.1007/978-3-030-29135-8_6},
  abstract = {Popular large image classification datasets that are drawn from the web present Eurocentric and Americentric biases that negatively impact the generalizability of models trained on them Shreya Shankar et al. (No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017). In order to encourage the development of modeling approaches that generalize well to images drawn from locations and cultural contexts that are unseen or poorly represented at the time of training, we organized the Inclusive Images competition in association with Kaggle and the NeurIPS 2018 Competition Track Workshop. In this chapter, we describe the motivation and design of the competition, present reports from the top three competitors, and provide high-level takeaways from the competition results.},
  language = {en},
  booktitle = {The {NeurIPS} '18 {Competition}},
  publisher = {Springer International Publishing},
  author = {Atwood, James and Halpern, Yoni and Baljekar, Pallavi and Breck, Eric and Sculley, D. and Ostyakov, Pavel and Nikolenko, Sergey I. and Ivanov, Igor and Solovyev, Roman and Wang, Weimin and Skalic, Miha},
  editor = {Escalera, Sergio and Herbrich, Ralf},
  year = {2020},
  pages = {155--186},
}

@inproceedings{torralba_unbiased_2011,
  title = {Unbiased look at dataset bias},
  doi = {10.1109/CVPR.2011.5995347},
  abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
  author = {Torralba, Antonio and Efros, Alexei},
  month = jul,
  year = {2011},
  pages = {1521--1528},
}

@misc{rudin_stop_2019,
  title = {Stop {Explaining} {Black} {Box} {Machine} {Learning} {Models} for {High} {Stakes} {Decisions} and {Use} {Interpretable} {Models} {Instead}},
  url = {http://arxiv.org/abs/1811.10154},
  abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to {\textbackslash}textit\{explain\} black box models, rather than creating models that are {\textbackslash}textit\{interpretable\} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
  urldate = {2022-09-12},
  publisher = {arXiv},
  author = {Rudin, Cynthia},
  month = sep,
  year = {2019},
  note = {arXiv:1811.10154 [cs, stat]},
}

@misc{menon_pulse_2020,
  title = {{PULSE}: {Self}-{Supervised} {Photo} {Upsampling} via {Latent} {Space} {Exploration} of {Generative} {Models}},
  shorttitle = {{PULSE}},
  url = {http://arxiv.org/abs/2003.03808},
  doi = {10.48550/arXiv.2003.03808},
  abstract = {The primary aim of single-image super-resolution is to construct high-resolution (HR) images from corresponding low-resolution (LR) inputs. In previous approaches, which have generally been supervised, the training objective typically measures a pixel-wise average distance between the super-resolved (SR) and HR images. Optimizing such metrics often leads to blurring, especially in high variance (detailed) regions. We propose an alternative formulation of the super-resolution problem based on creating realistic SR images that downscale correctly. We present an algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature. It accomplishes this in an entirely self-supervised fashion and is not confined to a specific degradation operator used during training, unlike previous methods (which require supervised training on databases of LR-HR image pairs). Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the "downscaling loss," which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, we restrict the search space to guarantee realistic outputs. PULSE thereby generates super-resolved images that both are realistic and downscale correctly. We show proof of concept of our approach in the domain of face super-resolution (i.e., face hallucination). We also present a discussion of the limitations and biases of the method as currently implemented with an accompanying model card with relevant metrics. Our method outperforms state-of-the-art methods in perceptual quality at higher resolutions and scale factors than previously possible.},
  urldate = {2022-09-12},
  publisher = {arXiv},
  author = {Menon, Sachit and Damian, Alexandru and Hu, Shijia and Ravi, Nikhil and Rudin, Cynthia},
  month = jul,
  year = {2020},
  note = {arXiv:2003.03808 [cs, eess]},
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  volume = {25},
  url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  urldate = {2022-09-11},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
}

@misc{noauthor_home_nodate,
  title = {Home},
  url = {https://www.flickr.com/},
  language = {en-us},
  urldate = {2022-08-23},
  journal = {Flickr},
}

@misc{jia_visual_2022,
  title = {Visual {Prompt} {Tuning}},
  url = {http://arxiv.org/abs/2203.12119},
  doi = {10.48550/arXiv.2203.12119},
  abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.},
  urldate = {2022-08-16},
  publisher = {arXiv},
  author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  month = jul,
  year = {2022},
  note = {Number: arXiv:2203.12119
arXiv:2203.12119 [cs]},
}

@misc{noauthor_pdf_nodate,
  title = {[{PDF}] {Visual} {Prompt} {Tuning} {\textbar} {Semantic} {Scholar}},
  url = {https://www.semanticscholar.org/paper/Visual-Prompt-Tuning-Jia-Tang/adb272fbdea3631059cf88ab764bb6c2ce29f965?sort=total-citations},
  urldate = {2022-08-16},
}

@inproceedings{salman_unadversarial_2021,
  title = {Unadversarial {Examples}: {Designing} {Objects} for {Robust} {Vision}},
  shorttitle = {Unadversarial {Examples}},
  abstract = {This framework exploits the sensitivity of modern machine learning algorithms to input perturbations in order to design “robust objects,” i.e., objects that are explicitly optimized to be conﬁdently detected or classed as robust. We study a class of realistic computer vision settings wherein one can inﬂuence the design of the objects being recognized. We develop a framework that leverages this capability to signiﬁcantly improve vision models’ performance and robustness. This framework exploits the sensitivity of modern machine learning algorithms to input perturbations in order to design “robust objects,” i.e., objects that are explicitly optimized to be conﬁdently detected or classiﬁed. We demonstrate the efﬁcacy of the framework on a wide variety of vision-based tasks ranging from standard benchmarks, to (in-simulation) robotics, to real-world experiments. Our code can be found at https://git.io/unadversarial .},
  booktitle = {{NeurIPS}},
  author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Vemprala, Sai and Madry, A. and Kapoor, Ashish},
  year = {2021},
}

@misc{bahng_exploring_2022,
  title = {Exploring {Visual} {Prompts} for {Adapting} {Large}-{Scale} {Models}},
  url = {http://arxiv.org/abs/2203.17274},
  doi = {10.48550/arXiv.2203.17274},
  abstract = {We investigate the efficacy of visual prompting to adapt large-scale models in vision. Following the recent approach from prompt tuning and adversarial reprogramming, we learn a single image perturbation such that a frozen model prompted with this perturbation performs a new task. Through comprehensive experiments, we demonstrate that visual prompting is particularly effective for CLIP and robust to distribution shift, achieving performance competitive with standard linear probes. We further analyze properties of the downstream dataset, prompt design, and output transformation in regard to adaptation performance. The surprising effectiveness of visual prompting provides a new perspective on adapting pre-trained models in vision. Code is available at http://hjbahng.github.io/visual\_prompting .},
  urldate = {2022-08-16},
  publisher = {arXiv},
  author = {Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
  month = jun,
  year = {2022},
  note = {Number: arXiv:2203.17274
arXiv:2203.17274 [cs]},
}

@misc{djolonga_robustness_2021,
  title = {On {Robustness} and {Transferability} of {Convolutional} {Neural} {Networks}},
  url = {http://arxiv.org/abs/2007.08558},
  abstract = {Modern deep convolutional networks (CNNs) are often criticized for not generalizing under distributional shifts. However, several recent breakthroughs in transfer learning suggest that these networks can cope with severe distribution shifts and successfully adapt to new tasks from a few training examples. In this work we study the interplay between out-of-distribution and transfer performance of modern image classification CNNs for the first time and investigate the impact of the pre-training data size, the model scale, and the data preprocessing pipeline. We find that increasing both the training set and model sizes significantly improve the distributional shift robustness. Furthermore, we show that, perhaps surprisingly, simple changes in the preprocessing such as modifying the image resolution can significantly mitigate robustness issues in some cases. Finally, we outline the shortcomings of existing robustness evaluation datasets and introduce a synthetic dataset SI-Score we use for a systematic analysis across factors of variation common in visual data such as object size and position.},
  urldate = {2022-08-05},
  publisher = {arXiv},
  author = {Djolonga, Josip and Yung, Jessica and Tschannen, Michael and Romijnders, Rob and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Minderer, Matthias and D'Amour, Alexander and Moldovan, Dan and Gelly, Sylvain and Houlsby, Neil and Zhai, Xiaohua and Lucic, Mario},
  month = mar,
  year = {2021},
  note = {Number: arXiv:2007.08558
arXiv:2007.08558 [cs]},
  keywords = {imagenet, ood, robustness},
}

@inproceedings{tsipras_imagenet_2020,
  title = {From {ImageNet} to {Image} {Classification}: {Contextualizing} {Progress} on {Benchmarks}},
  shorttitle = {From {ImageNet} to {Image} {Classification}},
  url = {https://proceedings.mlr.press/v119/tsipras20a.html},
  abstract = {Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset—including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignment into account.},
  language = {en},
  urldate = {2022-08-04},
  booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Ilyas, Andrew and Madry, Aleksander},
  month = nov,
  year = {2020},
  note = {ISSN: 2640-3498},
  keywords = {imagenet},
  pages = {9625--9635},
}

@misc{paiss_no_2022,
  title = {No {Token} {Left} {Behind}: {Explainability}-{Aided} {Image} {Classification} and {Generation}},
  shorttitle = {No {Token} {Left} {Behind}},
  url = {http://arxiv.org/abs/2204.04908},
  abstract = {The application of zero-shot learning in computer vision has been revolutionized by the use of image-text matching models. The most notable example, CLIP, has been widely used for both zero-shot classification and guiding generative models with a text prompt. However, the zero-shot use of CLIP is unstable with respect to the phrasing of the input text, making it necessary to carefully engineer the prompts used. We find that this instability stems from a selective similarity score, which is based only on a subset of the semantically meaningful input tokens. To mitigate it, we present a novel explainability-based approach, which adds a loss term to ensure that CLIP focuses on all relevant semantic parts of the input, in addition to employing the CLIP similarity loss used in previous works. When applied to one-shot classification through prompt engineering, our method yields an improvement in the recognition rate, without additional training or fine-tuning. Additionally, we show that CLIP guidance of generative models using our method significantly improves the generated images. Finally, we demonstrate a novel use of CLIP guidance for text-based image generation with spatial conditioning on object location, by requiring the image explainability heatmap for each object to be confined to a pre-determined bounding box.},
  urldate = {2022-07-27},
  publisher = {arXiv},
  author = {Paiss, Roni and Chefer, Hila and Wolf, Lior},
  month = apr,
  year = {2022},
  note = {Number: arXiv:2204.04908
arXiv:2204.04908 [cs]},
  keywords = {explainability, gradcam},
}

@inproceedings{petroni_language_2019,
  address = {Hong Kong, China},
  title = {Language {Models} as {Knowledge} {Bases}?},
  url = {https://aclanthology.org/D19-1250},
  doi = {10.18653/v1/D19-1250},
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  urldate = {2022-07-19},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher = {Association for Computational Linguistics},
  author = {Petroni, Fabio and Rocktäschel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  month = nov,
  year = {2019},
  pages = {2463--2473},
}

@misc{yang_empirical_2021,
  title = {An {Empirical} {Study} of {GPT}-3 for {Few}-{Shot} {Knowledge}-{Based} {VQA}},
  url = {http://arxiv.org/abs/2109.05014},
  doi = {10.48550/arXiv.2109.05014},
  abstract = {Knowledge-based visual question answering (VQA) involves answering questions that require external knowledge not present in the image. Existing methods first retrieve knowledge from external resources, then reason over the selected knowledge, the input image, and question for answer prediction. However, this two-step approach could lead to mismatches that potentially limit the VQA performance. For example, the retrieved knowledge might be noisy and irrelevant to the question, and the re-embedded knowledge features during reasoning might deviate from their original meanings in the knowledge base (KB). To address this challenge, we propose PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by GPT-3's power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge. Specifically, we first convert the image into captions (or tags) that GPT-3 can understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just providing a few in-context VQA examples. We further boost performance by carefully investigating: (i) what text formats best describe the image content, and (ii) how in-context examples can be better selected and used. PICa unlocks the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa surpasses the supervised state of the art by an absolute +8.6 points on the OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent few-shot performance.},
  urldate = {2022-07-19},
  publisher = {arXiv},
  author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  month = sep,
  year = {2021},
  note = {Number: arXiv:2109.05014
arXiv:2109.05014 [cs]},
}

@inproceedings{wang_posterior_2021,
  title = {Posterior {Collapse} and {Latent} {Variable} {Non}-identifiability},
  volume = {34},
  url = {https://proceedings.neurips.cc/paper/2021/hash/2b6921f2c64dee16ba21ebf17f3c2c92-Abstract.html},
  abstract = {Variational autoencoders model high-dimensional data by positinglow-dimensional latent variables that are mapped through a flexibledistribution parametrized by a neural network. Unfortunately,variational autoencoders often suffer from posterior collapse: theposterior of the latent variables is equal to its prior, rendering thevariational autoencoder useless as a means to produce meaningfulrepresentations. Existing approaches to posterior collapse oftenattribute it to the use of neural networks or optimization issues dueto variational approximation. In this paper, we consider posteriorcollapse as a problem of latent variable non-identifiability. We provethat the posterior collapses if and only if the latent variables arenon-identifiable in the generative model. This fact implies thatposterior collapse is not a phenomenon specific to the use of flexibledistributions or approximate inference. Rather, it can occur inclassical probabilistic models even with exact inference, which wealso demonstrate. Based on these results, we propose a class oflatent-identifiable variational autoencoders, deep generative modelswhich enforce identifiability without sacrificing flexibility. Thismodel class resolves the problem of latent variablenon-identifiability by leveraging bijective Brenier maps andparameterizing them with input convex neural networks, without specialvariational inference objectives or optimization tricks. Acrosssynthetic and real datasets, latent-identifiable variationalautoencoders outperform existing methods in mitigating posteriorcollapse and providing meaningful representations of the data.},
  urldate = {2022-07-19},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author = {Wang, Yixin and Blei, David and Cunningham, John P},
  year = {2021},
  keywords = {posterior\_collapse},
  pages = {5443--5455},
}

@inproceedings{park_multimodal_2018,
  address = {Salt Lake City, UT},
  title = {Multimodal {Explanations}: {Justifying} {Decisions} and {Pointing} to the {Evidence}},
  isbn = {978-1-5386-6420-9},
  shorttitle = {Multimodal {Explanations}},
  url = {https://ieeexplore.ieee.org/document/8579013/},
  doi = {10.1109/CVPR.2018.00915},
  abstract = {Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justiﬁcations. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to deﬁne and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets deﬁne visual and textual justiﬁcations of a classiﬁcation decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justiﬁcation models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer signiﬁcant beneﬁts over unimodal approaches.},
  language = {en},
  urldate = {2022-07-01},
  booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  publisher = {IEEE},
  author = {Park, Dong Huk and Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Anna and Schiele, Bernt and Darrell, Trevor and Rohrbach, Marcus},
  month = jun,
  year = {2018},
  pages = {8779--8788},
}

@misc{vasudevan_when_2022,
  title = {When does dough become a bagel? {Analyzing} the remaining mistakes on {ImageNet}},
  shorttitle = {When does dough become a bagel?},
  url = {http://arxiv.org/abs/2205.04596},
  abstract = {Image classification accuracy on the ImageNet dataset has been a barometer for progress in computer vision over the last decade. Several recent papers have questioned the degree to which the benchmark remains useful to the community, yet innovations continue to contribute gains to performance, with today's largest models achieving 90\%+ top-1 accuracy. To help contextualize progress on ImageNet and provide a more meaningful evaluation for today's state-of-the-art models, we manually review and categorize every remaining mistake that a few top models make in order to provide insight into the long-tail of errors on one of the most benchmarked datasets in computer vision. We focus on the multi-label subset evaluation of ImageNet, where today's best models achieve upwards of 97\% top-1 accuracy. Our analysis reveals that nearly half of the supposed mistakes are not mistakes at all, and we uncover new valid multi-labels, demonstrating that, without careful review, we are significantly underestimating the performance of these models. On the other hand, we also find that today's best models still make a significant number of mistakes (40\%) that are obviously wrong to human reviewers. To calibrate future progress on ImageNet, we provide an updated multi-label evaluation set, and we curate ImageNet-Major: a 68-example "major error" slice of the obvious mistakes made by today's top models -- a slice where models should achieve near perfection, but today are far from doing so.},
  urldate = {2022-07-01},
  publisher = {arXiv},
  author = {Vasudevan, Vijay and Caine, Benjamin and Gontijo-Lopes, Raphael and Fridovich-Keil, Sara and Roelofs, Rebecca},
  month = may,
  year = {2022},
  note = {Number: arXiv:2205.04596
arXiv:2205.04596 [cs]},
}

@misc{seybold_dueling_2019,
  title = {Dueling {Decoders}: {Regularizing} {Variational} {Autoencoder} {Latent} {Spaces}},
  shorttitle = {Dueling {Decoders}},
  url = {http://arxiv.org/abs/1905.07478},
  doi = {10.48550/arXiv.1905.07478},
  abstract = {Variational autoencoders learn unsupervised data representations, but these models frequently converge to minima that fail to preserve meaningful semantic information. For example, variational autoencoders with autoregressive decoders often collapse into autodecoders, where they learn to ignore the encoder input. In this work, we demonstrate that adding an auxiliary decoder to regularize the latent space can prevent this collapse, but successful auxiliary decoding tasks are domain dependent. Auxiliary decoders can increase the amount of semantic information encoded in the latent space and visible in the reconstructions. The semantic information in the variational autoencoder's representation is only weakly correlated with its rate, distortion, or evidence lower bound. Compared to other popular strategies that modify the training objective, our regularization of the latent space generally increased the semantic information content.},
  urldate = {2022-06-17},
  publisher = {arXiv},
  author = {Seybold, Bryan and Fertig, Emily and Alemi, Alex and Fischer, Ian},
  month = may,
  year = {2019},
  note = {Number: arXiv:1905.07478
arXiv:1905.07478 [cs, stat]},
}

@misc{razavi_preventing_2019,
  title = {Preventing {Posterior} {Collapse} with delta-{VAEs}},
  url = {http://arxiv.org/abs/1901.03416},
  doi = {10.48550/arXiv.1901.03416},
  abstract = {Due to the phenomenon of "posterior collapse," current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed \${\textbackslash}delta\$-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet \$32{\textbackslash}times 32\$.},
  urldate = {2022-06-17},
  publisher = {arXiv},
  author = {Razavi, Ali and Oord, Aäron van den and Poole, Ben and Vinyals, Oriol},
  month = jan,
  year = {2019},
  note = {Number: arXiv:1901.03416
arXiv:1901.03416 [cs, stat]},
  keywords = {vae},
}

@misc{aneja_contrastive_2021,
  title = {A {Contrastive} {Learning} {Approach} for {Training} {Variational} {Autoencoder} {Priors}},
  url = {http://arxiv.org/abs/2010.02917},
  doi = {10.48550/arXiv.2010.02917},
  abstract = {Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution.},
  urldate = {2022-06-17},
  publisher = {arXiv},
  author = {Aneja, Jyoti and Schwing, Alexander and Kautz, Jan and Vahdat, Arash},
  month = nov,
  year = {2021},
  note = {Number: arXiv:2010.02917
arXiv:2010.02917 [cs, stat]},
  keywords = {contrastive, vae},
}

@techreport{xia_causal-neural_2021,
  title = {The {Causal}-{Neural} {Connection}: {Expressiveness}, {Learnability}, and {Inference}},
  shorttitle = {The {Causal}-{Neural} {Connection}},
  url = {http://arxiv.org/abs/2107.00793},
  abstract = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Specifically, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identification and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufficient and necessary to determine whether a causal effect can be learned from data (i.e., causal identifiability); it then estimates the effect whenever identifiability holds (causal estimation). Simulations corroborate the proposed approach.},
  number = {arXiv:2107.00793},
  urldate = {2022-05-13},
  institution = {arXiv},
  author = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
  month = jul,
  year = {2021},
  note = {arXiv:2107.00793 [cs]
type: article},
}

@techreport{mao_causal_2022,
  title = {Causal {Transportability} for {Visual} {Recognition}},
  url = {http://arxiv.org/abs/2204.12363},
  abstract = {Visual representations underlie object recognition tasks, but they often contain both robust and non-robust features. Our main observation is that image classifiers may perform poorly on out-of-distribution samples because spurious correlations between non-robust features and labels can be changed in a new environment. By analyzing procedures for out-of-distribution generalization with a causal graph, we show that standard classifiers fail because the association between images and labels is not transportable across settings. However, we then show that the causal effect, which severs all sources of confounding, remains invariant across domains. This motivates us to develop an algorithm to estimate the causal effect for image classification, which is transportable (i.e., invariant) across source and target environments. Without observing additional variables, we show that we can derive an estimand for the causal effect under empirical assumptions using representations in deep models as proxies. Theoretical analysis, empirical results, and visualizations show that our approach captures causal invariances and improves overall generalization.},
  number = {arXiv:2204.12363},
  urldate = {2022-05-13},
  institution = {arXiv},
  author = {Mao, Chengzhi and Xia, Kevin and Wang, James and Wang, Hao and Yang, Junfeng and Bareinboim, Elias and Vondrick, Carl},
  month = apr,
  year = {2022},
  note = {arXiv:2204.12363 [cs]
type: article},
}

@techreport{ruan_optimal_2022,
  title = {Optimal {Representations} for {Covariate} {Shift}},
  url = {http://arxiv.org/abs/2201.00057},
  abstract = {Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations. Our objectives give insights into the robustness of CLIP, and further improve CLIP's representations to achieve SOTA results on DomainBed.},
  number = {arXiv:2201.00057},
  urldate = {2022-05-13},
  institution = {arXiv},
  author = {Ruan, Yangjun and Dubois, Yann and Maddison, Chris J.},
  month = mar,
  year = {2022},
  note = {arXiv:2201.00057 [cs, math, stat]
type: article},
}

@article{gulrajani_improved_2017,
  title = {Improved {Training} of {Wasserstein} {GANs}},
  url = {http://arxiv.org/abs/1704.00028},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  urldate = {2022-05-13},
  journal = {arXiv:1704.00028 [cs, stat]},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  month = dec,
  year = {2017},
  note = {arXiv: 1704.00028},
}

@incollection{ferrari_tracking_2018,
  address = {Cham},
  title = {Tracking {Emerges} by {Colorizing} {Videos}},
  volume = {11217},
  isbn = {978-3-030-01260-1 978-3-030-01261-8},
  url = {http://link.springer.com/10.1007/978-3-030-01261-8_24},
  abstract = {We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical ﬂow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.},
  language = {en},
  urldate = {2022-05-13},
  booktitle = {Computer {Vision} – {ECCV} 2018},
  publisher = {Springer International Publishing},
  author = {Vondrick, Carl and Shrivastava, Abhinav and Fathi, Alireza and Guadarrama, Sergio and Murphy, Kevin},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  doi = {10.1007/978-3-030-01261-8_24},
  note = {Series Title: Lecture Notes in Computer Science},
  keywords = {self-supervised},
  pages = {402--419},
}

@article{noroozi_unsupervised_2017,
  title = {Unsupervised {Learning} of {Visual} {Representations} by {Solving} {Jigsaw} {Puzzles}},
  url = {http://arxiv.org/abs/1603.09246},
  abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  urldate = {2022-05-13},
  journal = {arXiv:1603.09246 [cs]},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  month = aug,
  year = {2017},
  note = {arXiv: 1603.09246},
  keywords = {self-supervised},
}

@misc{gidaris_unsupervised_2018,
  title = {Unsupervised {Representation} {Learning} by {Predicting} {Image} {Rotations}},
  url = {http://arxiv.org/abs/1803.07728},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the ﬁeld of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Speciﬁcally, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus signiﬁcantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classiﬁcation, PASCAL classiﬁcation, PASCAL segmentation, and CIFAR-10 classiﬁcation. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.},
  language = {en},
  urldate = {2022-05-13},
  publisher = {arXiv},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  month = mar,
  year = {2018},
  note = {Number: arXiv:1803.07728
arXiv:1803.07728 [cs]},
  keywords = {self-supervised},
}

@article{hoffman_cycada_2017,
  title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
  shorttitle = {{CyCADA}},
  url = {http://arxiv.org/abs/1711.03213},
  abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.},
  urldate = {2022-05-13},
  journal = {arXiv:1711.03213 [cs]},
  author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei A. and Darrell, Trevor},
  month = dec,
  year = {2017},
  note = {arXiv: 1711.03213},
  keywords = {cyclegan, uda},
}

@misc{hoffman_cycada_2017-1,
  title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
  shorttitle = {{CyCADA}},
  url = {http://arxiv.org/abs/1711.03213},
  abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difﬁcult to visualize and sometimes fail to capture pixel-level and low-level domain shifts. Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs. We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs. Our model can be applied in a variety of visual recognition and prediction settings. We show new state-of-the-art results across multiple adaptation tasks, including digit classiﬁcation and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.},
  language = {en},
  urldate = {2022-05-13},
  publisher = {arXiv},
  author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei A. and Darrell, Trevor},
  month = dec,
  year = {2017},
  note = {Number: arXiv:1711.03213
arXiv:1711.03213 [cs]},
}

@article{long_deep_2017,
  title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
  url = {http://arxiv.org/abs/1605.06636},
  abstract = {Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.},
  urldate = {2022-05-13},
  journal = {arXiv:1605.06636 [cs, stat]},
  author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
  month = aug,
  year = {2017},
  note = {arXiv: 1605.06636},
  keywords = {uda},
}

@inproceedings{li_model_2020,
  address = {Seattle, WA, USA},
  title = {Model {Adaptation}: {Unsupervised} {Domain} {Adaptation} {Without} {Source} {Data}},
  isbn = {978-1-72817-168-5},
  shorttitle = {Model {Adaptation}},
  url = {https://ieeexplore.ieee.org/document/9157645/},
  doi = {10.1109/CVPR42600.2020.00966},
  abstract = {In this paper, we investigate a challenging unsupervised domain adaptation setting — unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to bypass the dependence on the source data. Speciﬁcally, the prediction model is to be improved through generated target-style data, which provides more accurate guidance for the generator. As a result, the generator and the prediction model can collaborate with each other without source data. Furthermore, due to the lack of supervision from source data, we propose a weight constraint that encourages similarity to the source model. A clustering-based regularization is also introduced to produce more discriminative features in the target domain. Compared to conventional domain adaptation methods, our model achieves superior performance on multiple adaptation tasks with only unlabeled target data, which veriﬁes its effectiveness in this challenging setting.},
  language = {en},
  urldate = {2022-05-13},
  booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Li, Rui and Jiao, Qianfen and Cao, Wenming and Wong, Hau-San and Wu, Si},
  month = jun,
  year = {2020},
  pages = {9638--9647},
}

@article{tzeng_deep_2014,
  title = {Deep {Domain} {Confusion}: {Maximizing} for {Domain} {Invariance}},
  shorttitle = {Deep {Domain} {Confusion}},
  url = {http://arxiv.org/abs/1412.3474},
  abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.},
  urldate = {2022-05-13},
  journal = {arXiv:1412.3474 [cs]},
  author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
  month = dec,
  year = {2014},
  note = {arXiv: 1412.3474},
  keywords = {uda},
}

@article{long_learning_2015,
  title = {Learning {Transferable} {Features} with {Deep} {Adaptation} {Networks}},
  url = {http://arxiv.org/abs/1502.02791},
  abstract = {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.},
  urldate = {2022-05-13},
  journal = {arXiv:1502.02791 [cs]},
  author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael I.},
  month = may,
  year = {2015},
  note = {arXiv: 1502.02791},
  keywords = {uda},
}

@article{kim_learning_2017,
  title = {Learning to {Discover} {Cross}-{Domain} {Relations} with {Generative} {Adversarial} {Networks}},
  url = {http://arxiv.org/abs/1703.05192},
  abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available https://github.com/SKTBrain/DiscoGAN},
  urldate = {2022-05-13},
  journal = {arXiv:1703.05192 [cs]},
  author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
  month = may,
  year = {2017},
  note = {arXiv: 1703.05192},
  keywords = {cyclegan},
}

@misc{noauthor_learning_nodate,
  title = {Learning to {Discover} {Cross}-{Domain} {Relations} with {Generative} {Adversarial} {Networks}},
  url = {https://arxiv.org/abs/1703.05192},
  urldate = {2022-05-13},
}

@article{zhu_unpaired_2020,
  title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
  url = {http://arxiv.org/abs/1703.10593},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  urldate = {2022-05-13},
  journal = {arXiv:1703.10593 [cs]},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  month = aug,
  year = {2020},
  note = {arXiv: 1703.10593},
  keywords = {cycle},
}

@article{wortsman_robust_2022,
  title = {Robust fine-tuning of zero-shot models},
  url = {http://arxiv.org/abs/2109.01903},
  abstract = {Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.},
  urldate = {2022-05-13},
  journal = {arXiv:2109.01903 [cs]},
  author = {Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  month = feb,
  year = {2022},
  note = {arXiv: 2109.01903},
  keywords = {clip, soup},
}

@article{caron_unsupervised_2021,
  title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
  url = {http://arxiv.org/abs/2006.09882},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  urldate = {2022-05-13},
  journal = {arXiv:2006.09882 [cs]},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  month = jan,
  year = {2021},
  note = {arXiv: 2006.09882},
  keywords = {clustering, ot, self-supervised},
}

@inproceedings{pearl_transportability_2011,
  address = {Vancouver, BC, Canada},
  title = {Transportability of {Causal} and {Statistical} {Relations}: {A} {Formal} {Approach}},
  isbn = {978-1-4673-0005-6 978-0-7695-4409-0},
  shorttitle = {Transportability of {Causal} and {Statistical} {Relations}},
  url = {http://ieeexplore.ieee.org/document/6137426/},
  doi = {10.1109/ICDMW.2011.169},
  abstract = {We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is afﬁrmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power.},
  language = {en},
  urldate = {2022-05-11},
  booktitle = {2011 {IEEE} 11th {International} {Conference} on {Data} {Mining} {Workshops}},
  publisher = {IEEE},
  author = {Pearl, Judea and Bareinboim, Elias},
  month = dec,
  year = {2011},
  keywords = {causal, transportability},
  pages = {540--547},
}

@inproceedings{correa_statistical_2019,
  address = {Macao, China},
  title = {From {Statistical} {Transportability} to {Estimating} the {Effect} of {Stochastic} {Interventions}},
  isbn = {978-0-9992411-4-1},
  url = {https://www.ijcai.org/proceedings/2019/230},
  doi = {10.24963/ijcai.2019/230},
  abstract = {Learning systems often face a critical challenge when applied to settings that differ from those under which they were initially trained. In particular, the assumption that both the source/training and the target/deployment domains follow the same causal mechanisms and observed distributions is commonly violated. This implies that the robustness and convergence guarantees usually expected from these methods are no longer attainable. In this paper, we study these violations through causal lens using the formalism of statistical transportability [Pearl and Bareinboim, 2011] (PB, for short). We start by proving sufﬁcient and necessary graphical conditions under which a probability distribution observed in the source domain can be extrapolated to the target one, where strictly less data is available. We develop the ﬁrst sound and complete procedure for statistical transportability, which formally closes the problem introduced by PB. Further, we tackle the general challenge of identiﬁcation of stochastic interventions from observational data [Sec. 4.4, Pearl, 2000]. This problem has been solved in the context of atomic interventions using Pearl’s do-calculus, which lacks complete treatment in the stochastic case. We prove completeness of stochastic identiﬁcation by constructing a reduction of any instance of this problem to an instance of statistical transportability, closing the problem.},
  language = {en},
  urldate = {2022-05-06},
  booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  author = {Correa, Juan D. and Bareinboim, Elias},
  month = aug,
  year = {2019},
  pages = {1661--1667},
}

@article{ghosh_pitfalls_2022,
  title = {On {Pitfalls} of {Identifiability} in {Unsupervised} {Learning}. {A} {Note} on: "{Desiderata} for {Representation} {Learning}: {A} {Causal} {Perspective}"},
  shorttitle = {On {Pitfalls} of {Identifiability} in {Unsupervised} {Learning}. {A} {Note} on},
  url = {http://arxiv.org/abs/2202.06844},
  abstract = {Model identifiability is a desirable property in the context of unsupervised representation learning. In absence thereof, different models may be observationally indistinguishable while yielding representations that are nontrivially related to one another, thus making the recovery of a ground truth generative model fundamentally impossible, as often shown through suitably constructed counterexamples. In this note, we discuss one such construction, illustrating a potential failure case of an identifiability result presented in "Desiderata for Representation Learning: A Causal Perspective" by Wang \& Jordan (2021). The construction is based on the theory of nonlinear independent component analysis. We comment on implications of this and other counterexamples for identifiable representation learning.},
  urldate = {2022-05-04},
  journal = {arXiv:2202.06844 [cs, stat]},
  author = {Ghosh, Shubhangi and Gresele, Luigi and von Kügelgen, Julius and Besserve, Michel and Schölkopf, Bernhard},
  month = feb,
  year = {2022},
  note = {arXiv: 2202.06844},
  keywords = {causal, ica, identifiability},
}

@article{koh_wilds_2021,
  title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
  shorttitle = {{WILDS}},
  url = {http://arxiv.org/abs/2012.07421},
  abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
  urldate = {2022-03-25},
  journal = {arXiv:2012.07421 [cs]},
  author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton A. and Haque, Imran S. and Beery, Sara and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
  month = jul,
  year = {2021},
  note = {arXiv: 2012.07421},
  keywords = {dataset, ood},
}

@article{bandi_detection_2019,
  title = {From {Detection} of {Individual} {Metastases} to {Classification} of {Lymph} {Node} {Status} at the {Patient} {Level}: {The} {CAMELYON17} {Challenge}},
  volume = {38},
  issn = {1558-254X},
  shorttitle = {From {Detection} of {Individual} {Metastases} to {Classification} of {Lymph} {Node} {Status} at the {Patient} {Level}},
  doi = {10.1109/TMI.2018.2867350},
  abstract = {Automated detection of cancer metastases in lymph nodes has the potential to improve the assessment of prognosis for patients. To enable fair comparison between the algorithms for this purpose, we set up the CAMELYON17 challenge in conjunction with the IEEE International Symposium on Biomedical Imaging 2017 Conference in Melbourne. Over 300 participants registered on the challenge website, of which 23 teams submitted a total of 37 algorithms before the initial deadline. Participants were provided with 899 whole-slide images (WSIs) for developing their algorithms. The developed algorithms were evaluated based on the test set encompassing 100 patients and 500 WSIs. The evaluation metric used was a quadratic weighted Cohen's kappa. We discuss the algorithmic details of the 10 best pre-conference and two post-conference submissions. All these participants used convolutional neural networks in combination with pre- and postprocessing steps. Algorithms differed mostly in neural network architecture, training strategy, and pre- and postprocessing methodology. Overall, the kappa metric ranged from 0.89 to -0.13 across all submissions. The best results were obtained with pre-trained architectures such as ResNet. Confusion matrix analysis revealed that all participants struggled with reliably identifying isolated tumor cells, the smallest type of metastasis, with detection rates below 40\%. Qualitative inspection of the results of the top participants showed categories of false positives, such as nerves or contamination, which could be targeted for further optimization. Last, we show that simple combinations of the top algorithms result in higher kappa metric values than any algorithm individually, with 0.93 for the best combination.},
  language = {eng},
  number = {2},
  journal = {IEEE transactions on medical imaging},
  author = {Bandi, Peter and Geessink, Oscar and Manson, Quirine and Van Dijk, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and Ehteshami Bejnordi, Babak and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and Li, Quanzheng and Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Fukuta, Keisuke and Komura, Daisuke and Ovtcharov, Vlado and Cheng, Shenghua and Zeng, Shaoqun and Thagaard, Jeppe and Dahl, Anders B. and Lin, Huangjing and Chen, Hao and Jacobsson, Ludwig and Hedlund, Martin and Cetin, Melih and Halici, Eren and Jackson, Hunter and Chen, Richard and Both, Fabian and Franke, Jorg and Kusters-Vandevelde, Heidi and Vreuls, Willem and Bult, Peter and van Ginneken, Bram and van der Laak, Jeroen and Litjens, Geert},
  month = feb,
  year = {2019},
  pmid = {30716025},
  keywords = {dataset},
  pages = {550--560},
}

@article{schonfeld_generalized_2019,
  title = {Generalized {Zero}- and {Few}-{Shot} {Learning} via {Aligned} {Variational} {Autoencoders}},
  url = {http://arxiv.org/abs/1812.01784},
  abstract = {Many approaches in generalized zero-shot learning rely on cross-modal mapping between the image feature space and the class embedding space. As labeled images are expensive, one direction is to augment the dataset by generating either images or image features. However, the former misses fine-grained details and the latter requires learning a mapping associated with class embeddings. In this work, we take feature generation one step further and propose a model where a shared latent space of image features and class embeddings is learned by modality-specific aligned variational autoencoders. This leaves us with the required discriminative information about the image and classes in the latent features, on which we train a softmax classifier. The key to our approach is that we align the distributions learned from images and from side-information to construct latent features that contain the essential multi-modal information associated with unseen classes. We evaluate our learned latent features on several benchmark datasets, i.e. CUB, SUN, AWA1 and AWA2, and establish a new state of the art on generalized zero-shot as well as on few-shot learning. Moreover, our results on ImageNet with various zero-shot splits show that our latent features generalize well in large-scale settings.},
  urldate = {2022-03-14},
  journal = {arXiv:1812.01784 [cs]},
  author = {Schönfeld, Edgar and Ebrahimi, Sayna and Sinha, Samarth and Darrell, Trevor and Akata, Zeynep},
  month = apr,
  year = {2019},
  note = {arXiv: 1812.01784},
  keywords = {cross-modal, multimodal, vae, zero-shot},
}

@inproceedings{ye_textfusenet_2020,
  title = {{TextFuseNet}: {Scene} {Text} {Detection} with {Richer} {Fused} {Features}},
  volume = {1},
  shorttitle = {{TextFuseNet}},
  url = {https://www.ijcai.org/proceedings/2020/72},
  doi = {10.24963/ijcai.2020/72},
  abstract = {Electronic proceedings of IJCAI 2020},
  language = {en},
  urldate = {2022-03-14},
  author = {Ye, Jian and Chen, Zhe and Liu, Juhua and Du, Bo},
  month = jul,
  year = {2020},
  note = {ISSN: 1045-0823},
  keywords = {ocr},
  pages = {516--522},
}

@article{makino_generative_2022,
  title = {Generative multitask learning mitigates target-causing confounding},
  url = {http://arxiv.org/abs/2202.04136},
  abstract = {We propose a simple and scalable approach to causal representation learning for multitask learning. Our approach requires minimal modification to existing ML systems, and improves robustness to prior probability shift. The improvement comes from mitigating unobserved confounders that cause the targets, but not the input. We refer to them as target-causing confounders. These confounders induce spurious dependencies between the input and targets. This poses a problem for the conventional approach to multitask learning, due to its assumption that the targets are conditionally independent given the input. Our proposed approach takes into account the dependency between the targets in order to alleviate target-causing confounding. All that is required in addition to usual practice is to estimate the joint distribution of the targets to switch from discriminative to generative classification, and to predict all targets jointly. Our results on the Attributes of People and Taskonomy datasets reflect the conceptual improvement in robustness to prior probability shift.},
  urldate = {2022-03-10},
  journal = {arXiv:2202.04136 [cs, stat]},
  author = {Makino, Taro and Geras, Krzysztof and Cho, Kyunghyun},
  month = feb,
  year = {2022},
  note = {arXiv: 2202.04136},
  keywords = {causality, generative, multitask, ood},
}

@article{gorti_text--image--text_2018,
  title = {Text-to-{Image}-to-{Text} {Translation} using {Cycle} {Consistent} {Adversarial} {Networks}},
  url = {http://arxiv.org/abs/1808.04538},
  abstract = {Text-to-Image translation has been an active area of research in the recent past. The ability for a network to learn the meaning of a sentence and generate an accurate image that depicts the sentence shows ability of the model to think more like humans. Popular methods on text to image translation make use of Generative Adversarial Networks (GANs) to generate high quality images based on text input, but the generated images don't always reflect the meaning of the sentence given to the model as input. We address this issue by using a captioning network to caption on generated images and exploit the distance between ground truth captions and generated captions to improve the network further. We show extensive comparisons between our method and existing methods.},
  urldate = {2022-03-10},
  journal = {arXiv:1808.04538 [cs, stat]},
  author = {Gorti, Satya Krishna and Ma, Jeremy},
  month = aug,
  year = {2018},
  note = {arXiv: 1808.04538},
  keywords = {caption, gan, multimodal},
}

@article{feder_causalm_2021,
  title = {{CausaLM}: {Causal} {Model} {Explanation} {Through} {Counterfactual} {Language} {Models}},
  issn = {0891-2017, 1530-9312},
  shorttitle = {{CausaLM}},
  url = {http://arxiv.org/abs/2005.13407},
  doi = {10.1162/coli_a_00404},
  abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.},
  urldate = {2022-03-10},
  journal = {Computational Linguistics},
  author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
  month = may,
  year = {2021},
  note = {arXiv: 2005.13407},
  keywords = {counterfactual, explainability, language\_model},
  pages = {1--54},
}

@article{creager_environment_2021,
  title = {Environment {Inference} for {Invariant} {Learning}},
  url = {http://arxiv.org/abs/2010.07249},
  abstract = {Learning models that gracefully handle distribution shifts is central to research on domain generalization, robust optimization, and fairness. A promising formulation is domain-invariant learning, which identifies the key issue of learning which features are domain-specific versus domain-invariant. An important assumption in this area is that the training examples are partitioned into "domains" or "environments". Our focus is on the more common setting where such partitions are not provided. We propose EIIL, a general framework for domain-invariant learning that incorporates Environment Inference to directly infer partitions that are maximally informative for downstream Invariant Learning. We show that EIIL outperforms invariant learning methods on the CMNIST benchmark without using environment labels, and significantly outperforms ERM on worst-group performance in the Waterbirds and CivilComments datasets. Finally, we establish connections between EIIL and algorithmic fairness, which enables EIIL to improve accuracy and calibration in a fair prediction problem.},
  urldate = {2022-03-09},
  journal = {arXiv:2010.07249 [cs]},
  author = {Creager, Elliot and Jacobsen, Jörn-Henrik and Zemel, Richard},
  month = jul,
  year = {2021},
  note = {arXiv: 2010.07249},
  keywords = {invariance},
}

@article{von_kugelgen_self-supervised_2022,
  title = {Self-{Supervised} {Learning} with {Data} {Augmentations} {Provably} {Isolates} {Content} from {Style}},
  url = {http://arxiv.org/abs/2106.04619},
  abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
  urldate = {2022-03-09},
  journal = {arXiv:2106.04619 [cs, stat]},
  author = {von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  month = jan,
  year = {2022},
  note = {arXiv: 2106.04619},
  keywords = {contrastive, identifiability},
}

@article{khemakhem_variational_2020,
  title = {Variational {Autoencoders} and {Nonlinear} {ICA}: {A} {Unifying} {Framework}},
  shorttitle = {Variational {Autoencoders} and {Nonlinear} {ICA}},
  url = {http://arxiv.org/abs/1907.04809},
  abstract = {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.},
  urldate = {2022-03-09},
  journal = {arXiv:1907.04809 [cs, stat]},
  author = {Khemakhem, Ilyes and Kingma, Diederik P. and Monti, Ricardo Pio and Hyvärinen, Aapo},
  month = dec,
  year = {2020},
  note = {arXiv: 1907.04809},
  keywords = {ica, identifiability},
}

@article{mitrovic_representation_2020,
  title = {Representation {Learning} via {Invariant} {Causal} {Mechanisms}},
  url = {http://arxiv.org/abs/2010.07922},
  abstract = {Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on \$51\$ out of \$57\$ games.},
  urldate = {2022-03-09},
  journal = {arXiv:2010.07922 [cs, stat]},
  author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.07922},
  keywords = {invariance, ood},
}

@article{vahdat_score-based_2021,
  title = {Score-based {Generative} {Modeling} in {Latent} {Space}},
  url = {http://arxiv.org/abs/2106.05931},
  abstract = {Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .},
  urldate = {2022-03-09},
  journal = {arXiv:2106.05931 [cs, stat]},
  author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  month = dec,
  year = {2021},
  note = {arXiv: 2106.05931},
  keywords = {generative\_models},
}

@inproceedings{vahdat_score-based_2021-1,
  title = {Score-based {Generative} {Modeling} in {Latent} {Space}},
  url = {https://openreview.net/forum?id=P9TYG0j-wtG},
  abstract = {We present a framework for learning score-based generative models in a latent space},
  language = {en},
  urldate = {2022-03-09},
  author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  month = may,
  year = {2021},
}

@article{lu_nonlinear_2021,
  title = {Nonlinear {Invariant} {Risk} {Minimization}: {A} {Causal} {Approach}},
  shorttitle = {Nonlinear {Invariant} {Risk} {Minimization}},
  url = {http://arxiv.org/abs/2102.12353},
  abstract = {Due to spurious correlations, machine learning systems often fail to generalize to environments whose distributions differ from the ones used at training time. Prior work addressing this, either explicitly or implicitly, attempted to find a data representation that has an invariant relationship with the target. This is done by leveraging a diverse set of training environments to reduce the effect of spurious features and build an invariant predictor. However, these methods have generalization guarantees only when both data representation and classifiers come from a linear model class. We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers). It builds upon a practical and general assumption: the prior over the data representation (i.e., a set of latent variables encoding the data) given the target and the environment belongs to general exponential family distributions. Based on this, we show that it is possible to identify the data representation up to simple transformations. We also prove that all direct causes of the target can be fully discovered, which further enables us to obtain generalization guarantees in the nonlinear setting. Extensive experiments on both synthetic and real-world datasets show that our approach outperforms a variety of baseline methods. Finally, in the discussion, we further explore the aforementioned assumption and propose a more general hypothesis, called the Agnostic Hypothesis: there exist a set of hidden causal factors affecting both inputs and outcomes. The Agnostic Hypothesis can provide a unifying view of machine learning. More importantly, it can inspire a new direction to explore a general theory for identifying hidden causal factors, which is key to enabling the OOD generalization guarantees.},
  urldate = {2022-03-09},
  journal = {arXiv:2102.12353 [cs, stat]},
  author = {Lu, Chaochao and Wu, Yuhuai and Hernández-Lobato, Jośe Miguel and Schölkopf, Bernhard},
  month = oct,
  year = {2021},
  note = {arXiv: 2102.12353},
  keywords = {irm, ood},
}

@article{dong_image_2015,
  title = {Image {Super}-{Resolution} {Using} {Deep} {Convolutional} {Networks}},
  url = {http://arxiv.org/abs/1501.00092},
  abstract = {We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.},
  urldate = {2022-03-07},
  journal = {arXiv:1501.00092 [cs]},
  author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
  month = jul,
  year = {2015},
  note = {arXiv: 1501.00092},
  keywords = {superresolution},
}

@misc{noauthor_image_nodate,
  title = {Image {Super}-{Resolution} {Using} {Deep} {Convolutional} {Networks}},
  url = {https://arxiv.org/abs/1501.00092},
  urldate = {2022-03-07},
  keywords = {superresolution},
}

@inproceedings{kim_accurate_2016,
  address = {Las Vegas, NV, USA},
  title = {Accurate {Image} {Super}-{Resolution} {Using} {Very} {Deep} {Convolutional} {Networks}},
  isbn = {978-1-4673-8851-1},
  url = {http://ieeexplore.ieee.org/document/7780551/},
  doi = {10.1109/CVPR.2016.182},
  abstract = {We present a highly accurate single-image superresolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classiﬁcation [19]. We ﬁnd increasing our network depth shows a signiﬁcant improvement in accuracy. Our ﬁnal model uses 20 weight layers. By cascading small ﬁlters many times in a deep network structure, contextual information over large image regions is exploited in an efﬁcient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.},
  language = {en},
  urldate = {2022-03-07},
  booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu},
  month = jun,
  year = {2016},
  keywords = {superresolution},
  pages = {1646--1654},
}

@article{ongie_deep_2020,
  title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
  url = {http://arxiv.org/abs/2005.06001},
  abstract = {Recent work in machine learning shows that deep neural networks can be used to solve a wide variety of inverse problems arising in computational imaging. We explore the central prevailing themes of this emerging area and present a taxonomy that can be used to categorize different problems and reconstruction methods. Our taxonomy is organized along two central axes: (1) whether or not a forward model is known and to what extent it is used in training and testing, and (2) whether or not the learning is supervised or unsupervised, i.e., whether or not the training relies on access to matched ground truth image and measurement pairs. We also discuss the trade-offs associated with these different reconstruction approaches, caveats and common failure modes, plus open problems and avenues for future work.},
  urldate = {2022-03-07},
  journal = {arXiv:2005.06001 [cs, eess, stat]},
  author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
  month = may,
  year = {2020},
  note = {arXiv: 2005.06001},
  keywords = {superresolution},
}

@article{shafer_using_1983,
  title = {Using shadows in finding surface orientations},
  volume = {22},
  issn = {0734-189X},
  url = {https://www.sciencedirect.com/science/article/pii/0734189X83900993},
  doi = {10.1016/0734-189X(83)90099-3},
  abstract = {Given a line drawing from an image with shadow regions identified, the shapes of the shadows can be used to generate constraints on the orientations of the surfaces involved. This paper describes the theory which governs those constraints under orthography. A “Basic Shadow Problem” is first posed, in which there is a single light source, and a single surface casts a shadow on another (background) surface. There are six parameters to determine: the orientation (two parameters) for each surface, and the direction of the vector (two parameters) pointing at the light source. If some set of three of these are given in advance, the remaining three can then be determined geometrically. The solution method consists of identifying “illumination surfaces” consisting of illumination vectors, assigning Huffman-Clowes line labels to their edges, and applying the corresponding constraints in gradient space. The analysis is extended to shadows cast by polyhedra and curved surfaces. In both cases, the constraints provided by shadows can be analyzed in a manner analogous to the Basic shadow Problem. When the shadow falls upon a polyhedron or curved surface, similar techniques apply. The consequences of varying the position and number of light sources are also discussed. Finally, some methods are presented for combining shadow geometry with other gradient space techniques for 3D shape inference.},
  language = {en},
  number = {1},
  urldate = {2022-03-07},
  journal = {Computer Vision, Graphics, and Image Processing},
  author = {Shafer, Steven A and Kanade, Takeo},
  month = apr,
  year = {1983},
  keywords = {3D, shadows},
  pages = {145--176},
}

@article{fan_point_2016,
  title = {A {Point} {Set} {Generation} {Network} for {3D} {Object} {Reconstruction} from a {Single} {Image}},
  url = {http://arxiv.org/abs/1612.00603},
  abstract = {Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.},
  urldate = {2022-03-07},
  journal = {arXiv:1612.00603 [cs]},
  author = {Fan, Haoqiang and Su, Hao and Guibas, Leonidas},
  month = dec,
  year = {2016},
  note = {arXiv: 1612.00603},
  keywords = {3D},
}

@inproceedings{shin_pixels_2018,
  address = {Salt Lake City, UT},
  title = {Pixels, {Voxels}, and {Views}: {A} {Study} of {Shape} {Representations} for {Single} {View} {3D} {Object} {Shape} {Prediction}},
  isbn = {978-1-5386-6420-9},
  shorttitle = {Pixels, {Voxels}, and {Views}},
  url = {https://ieeexplore.ieee.org/document/8578421/},
  doi = {10.1109/CVPR.2018.00323},
  abstract = {The goal of this paper is to compare surface-based and volumetric 3D object shape representations, as well as viewer-centered and object-centered reference frames for single-view 3D shape prediction. We propose a new algorithm for predicting depth maps from multiple viewpoints, with a single depth or RGB image as input. By modifying the network and the way models are evaluated, we can directly compare the merits of voxels vs. surfaces and viewercentered vs. object-centered for familiar vs. unfamiliar objects, as predicted from RGB or depth images. Among our ﬁndings, we show that surface-based methods outperform voxel representations for objects from novel classes and produce higher resolution outputs. We also ﬁnd that using viewer-centered coordinates is advantageous for novel objects, while object-centered representations are better for more familiar objects. Interestingly, the coordinate frame signiﬁcantly affects the shape representation learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing shape representations with less dependence on category recognition.},
  language = {en},
  urldate = {2022-03-07},
  booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  publisher = {IEEE},
  author = {Shin, Daeyun and Fowlkes, Charless C. and Hoiem, Derek},
  month = jun,
  year = {2018},
  keywords = {3D, reconstruction},
  pages = {3061--3069},
}

@article{brock_generative_2016,
  title = {Generative and {Discriminative} {Voxel} {Modeling} with {Convolutional} {Neural} {Networks}},
  url = {http://arxiv.org/abs/1608.04236},
  abstract = {When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5\% relative improvement in the state of the art for object classification.},
  urldate = {2022-03-07},
  journal = {arXiv:1608.04236 [cs, stat]},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
  month = aug,
  year = {2016},
  note = {arXiv: 1608.04236},
}

@inproceedings{wang_instance_2020,
  address = {Seattle, WA, USA},
  title = {Instance {Shadow} {Detection}},
  isbn = {978-1-72817-168-5},
  url = {https://ieeexplore.ieee.org/document/9157490/},
  doi = {10.1109/CVPR42600.2020.00195},
  abstract = {Instance shadow detection is a brand new problem, aiming to ﬁnd shadow instances paired with object instances. To approach it, we ﬁrst prepare a new dataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of shadow and object instances in 1,000 photos, each with individually-labeled masks. Second, we design LISA, named after Light-guided Instance Shadow-object Association, an end-to-end framework to automatically predict the shadow and object instances, together with the shadowobject associations and light direction. Then, we pair up the predicted shadow and object instances and match them with the predicted shadow-object associations to generate the ﬁnal results. In our evaluations, we formulate a new metric named the shadow-object average precision to measure the performance of our results. Further, we conducted various experiments and demonstrate our method’s applicability to light direction estimation and photo editing.},
  language = {en},
  urldate = {2022-03-06},
  booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Wang, Tianyu and Hu, Xiaowei and Wang, Qiong and Heng, Pheng-Ann and Fu, Chi-Wing},
  month = jun,
  year = {2020},
  keywords = {3D, shadows},
  pages = {1877--1886},
}

@inproceedings{waltz_understanding_1975,
  title = {Understanding {Line} {Drawings} of {Scenes} with {Shadows}},
  abstract = {this paper, how can we recognize the identity of Figs. 2.1 and 2.2? Do we use' learning and knowledge to interpret what we see, or do we somehow automatically see the world as stable and independent bf lighting? What portions of scenes can we understand from local features alone, and what configurations require the use of 1obal hypotheses?  19 In this essay I describe a working collection of computer programs which reconstruct three-dimensional descriptions from line drawings which are obtained from scenes composed of plane-faced objects under various lighting conditions. The system identifies shadow lines and regions, groups regions which belong to the same object, and notices such relations as contact or lack of contact between the objects, support and in-front-of/behind relations between the objects as well as information about the spatial orientation of various regions, all using the description it has generated},
  booktitle = {The {Psychology} of {Computer} {Vision}},
  publisher = {McGraw-Hill},
  author = {Waltz, David},
  year = {1975},
  keywords = {3D, shadows},
  pages = {pages},
}

@inproceedings{troccoli_shadow_2004,
  title = {A {Shadow} {Based} {Method} for {Image} to {Model} {Registration}},
  doi = {10.1109/CVPR.2004.289},
  abstract = {This paper presents a novel method for 2D to 3D texture mapping using shadows as cues. This work is part of a larger set of methods that address the entire 3D modeling pipeline to create geometrically and photometrically accurate models using a variety of data sources. The focus is on building models of large outdoor, urban, historic and archaeological sites. We pose registration of 2D images with the 3D model as an optimization problem that uses knowledge of the Sun's position to estimate shadows in a scene, and use the shadows produced as a cue to refine the registration parameters. Results are presented for registration where ground truth is known and also for a large scale model consisting of 14 3D scans and 10 images on a large archaeological site in Sicily.},
  booktitle = {2004 {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshop}},
  author = {Troccoli, A.J. and Allen, P.K.},
  month = jun,
  year = {2004},
  keywords = {3D, shadows},
  pages = {169--169},
}

@article{savarese_3d_2007,
  title = {{3D} {Reconstruction} by {Shadow} {Carving}: {Theory} and {Practical} {Evaluation}},
  volume = {71},
  issn = {0920-5691},
  shorttitle = {{3D} {Reconstruction} by {Shadow} {Carving}},
  url = {https://doi.org/10.1007/s11263-006-8323-9},
  doi = {10.1007/s11263-006-8323-9},
  abstract = {Cast shadows are an informative cue to the shape of objects. They are particularly valuable for discovering object's concavities which are not available from other cues such as occluding boundaries. We propose a new method for recovering shape from shadows which we call shadow carving . Given a conservative estimate of the volume occupied by an object, it is possible to identify and carve away regions of this volume that are inconsistent with the observed pattern of shadows. We prove a theorem that guarantees that when these regions are carved away from the shape, the shape still remains conservative. Shadow carving overcomes limitations of previous studies on shape from shadows because it is robust with respect to errors in shadows detection and it allows the reconstruction of objects in the round, rather than just bas-reliefs. We propose a reconstruction system to recover shape from silhouettes and shadow carving. The silhouettes are used to reconstruct the initial conservative estimate of the object's shape and shadow carving is used to carve out the concavities. We have simulated our reconstruction system with a commercial rendering package to explore the design parameters and assess the accuracy of the reconstruction. We have also implemented our reconstruction scheme in a table-top system and present the results of scanning of several objects.},
  number = {3},
  urldate = {2022-03-04},
  journal = {International Journal of Computer Vision},
  author = {Savarese, Silvio and Andreetto, Marco and Rushmeier, Holly and Bernardini, Fausto and Perona, Pietro},
  month = mar,
  year = {2007},
  keywords = {3D, shadows},
  pages = {305--336},
}

@article{brock_generative_2016-1,
  title = {Generative and {Discriminative} {Voxel} {Modeling} with {Convolutional} {Neural} {Networks}},
  url = {http://arxiv.org/abs/1608.04236},
  abstract = {When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5\% relative improvement in the state of the art for object classification.},
  urldate = {2022-03-02},
  journal = {arXiv:1608.04236 [cs, stat]},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
  month = aug,
  year = {2016},
  note = {arXiv: 1608.04236},
  keywords = {3D},
}

@inproceedings{mescheder_occupancy_2019,
  title = {Occupancy {Networks}: {Learning} {3D} {Reconstruction} in {Function} {Space}},
  shorttitle = {Occupancy {Networks}},
  doi = {10.1109/CVPR.2019.00459},
  abstract = {With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.},
  booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  author = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  month = jun,
  year = {2019},
  note = {ISSN: 2575-7075},
  pages = {4455--4465},
}

@article{wang_desiderata_2021,
  title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
  shorttitle = {Desiderata for {Representation} {Learning}},
  url = {https://arxiv.org/abs/2109.03795v2},
  abstract = {Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, efficient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and efficiency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
  language = {en},
  urldate = {2022-03-02},
  author = {Wang, Yixin and Jordan, Michael I.},
  month = sep,
  year = {2021},
  keywords = {causal\_representations, causality},
}

@article{makar_causally_2022,
  title = {Causally motivated {Shortcut} {Removal} {Using} {Auxiliary} {Labels}},
  url = {http://arxiv.org/abs/2105.06422},
  abstract = {Shortcut learning, in which models make use of easy-to-represent but unstable associations, is a major failure mode for robust machine learning. We study a flexible, causally-motivated approach to training robust predictors by discouraging the use of specific shortcuts, focusing on a common setting where a robust predictor could achieve optimal {\textbackslash}emph\{iid\} generalization in principle, but is overshadowed by a shortcut predictor in practice. Our approach uses auxiliary labels, typically available at training time, to enforce conditional independences implied by the causal graph. We show both theoretically and empirically that causally-motivated regularization schemes (a) lead to more robust estimators that generalize well under distribution shift, and (b) have better finite sample efficiency compared to usual regularization schemes, even when no shortcut is present. Our analysis highlights important theoretical properties of training techniques commonly used in the causal inference, fairness, and disentanglement literatures. Our code is available at https://github.com/mymakar/causally\_motivated\_shortcut\_removal},
  urldate = {2022-02-27},
  journal = {arXiv:2105.06422 [cs]},
  author = {Makar, Maggie and Packer, Ben and Moldovan, Dan and Blalock, Davis and Halpern, Yoni and D'Amour, Alexander},
  month = feb,
  year = {2022},
  note = {arXiv: 2105.06422},
  keywords = {causality, ood, shortcuts},
}

@article{wang_desiderata_2021-1,
  title = {Desiderata for {Representation} {Learning}: {A} {Causal} {Perspective}},
  shorttitle = {Desiderata for {Representation} {Learning}},
  abstract = {A causal perspective on representation learning is taken, formalizing non-spuriousness and e ciency and disentanglement and using counterfactual quantities and observable consequences of causal assertions to assess the degree to which representations satisfy the desiderata of interest. Representation learning constructs low-dimensional representations to summarize essential features of high-dimensional data. This learning problem is often approached by describing various desiderata associated with learned representations; e.g., that they be non-spurious, e cient, or disentangled. It can be challenging, however, to turn these intuitive desiderata into formal criteria that can be measured and enhanced based on observed data. In this paper, we take a causal perspective on representation learning, formalizing non-spuriousness and e ciency (in supervised representation learning) and disentanglement (in unsupervised representation learning) using counterfactual quantities and observable consequences of causal assertions. This yields computable metrics that can be used to assess the degree to which representations satisfy the desiderata of interest and learn non-spurious and disentangled representations from single observational datasets.},
  journal = {ArXiv},
  author = {Wang, Yixin and Jordan, Michael I.},
  year = {2021},
  keywords = {causality, representation\_learning},
}

@inproceedings{qian_enhancing_2019,
  address = {Hong Kong, China},
  title = {Enhancing {Variational} {Autoencoders} with {Mutual} {Information} {Neural} {Estimation} for {Text} {Generation}},
  url = {https://aclanthology.org/D19-1416},
  doi = {10.18653/v1/D19-1416},
  abstract = {While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the latent variable fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the VAE. In this paper, we propose to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ neural networks for the estimation of the MI and provide a training algorithm based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in language modeling and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.},
  urldate = {2022-02-22},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher = {Association for Computational Linguistics},
  author = {Qian, Dong and Cheung, William K.},
  month = nov,
  year = {2019},
  keywords = {mutual\_information, posterior\_collapse, vae},
  pages = {4047--4057},
}

@article{von_kugelgen_self-supervised_2022-1,
  title = {Self-{Supervised} {Learning} with {Data} {Augmentations} {Provably} {Isolates} {Content} from {Style}},
  url = {http://arxiv.org/abs/2106.04619},
  abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
  urldate = {2022-02-17},
  journal = {arXiv:2106.04619 [cs, stat]},
  author = {von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
  month = jan,
  year = {2022},
  note = {arXiv: 2106.04619},
  keywords = {causality, content\_style, contrastive, data\_augmentation, self-supervised},
}

@article{chang_explaining_2019,
  title = {Explaining {Image} {Classifiers} by {Counterfactual} {Generation}},
  url = {http://arxiv.org/abs/1807.08024},
  abstract = {When an image classifier makes a prediction, which parts of the image are relevant and why? We can rephrase this question to ask: which parts of the image, if they were not seen by the classifier, would most change its decision? Producing an answer requires marginalizing over images that could have been seen but weren't. We can sample plausible image in-fills by conditioning a generative model on the rest of the image. We then optimize to find the image regions that most change the classifier's decision after in-fill. Our approach contrasts with ad-hoc in-filling approaches, such as blurring or injecting noise, which generate inputs far from the data distribution, and ignore informative relationships between different parts of the image. Our method produces more compact and relevant saliency maps, with fewer artifacts compared to previous methods.},
  urldate = {2022-02-01},
  journal = {arXiv:1807.08024 [cs]},
  author = {Chang, Chun-Hao and Creager, Elliot and Goldenberg, Anna and Duvenaud, David},
  month = feb,
  year = {2019},
  note = {arXiv: 1807.08024},
  keywords = {counterfactual, explainability, inpainting},
}

@article{chang_counterfactual-generation_nodate,
  title = {counterfactual-generation},
  language = {en},
  author = {Chang, Chun-Hao and Creager, Elliot and Goldenberg, Anna and Duvenaud, David},
  keywords = {counter},
  pages = {21},
}

@article{veitch_counterfactual_2021,
  title = {Counterfactual {Invariance} to {Spurious} {Correlations}: {Why} and {How} to {Pass} {Stress} {Tests}},
  shorttitle = {Counterfactual {Invariance} to {Spurious} {Correlations}},
  url = {http://arxiv.org/abs/2106.00545},
  abstract = {Informally, a 'spurious correlation' is the dependence of a model on some aspect of the input data that an analyst thinks shouldn't matter. In machine learning, these have a know-it-when-you-see-it character; e.g., changing the gender of a sentence's subject changes a sentiment predictor's output. To check for spurious correlations, we can 'stress test' models by perturbing irrelevant parts of input data and seeing if model predictions change. In this paper, we study stress testing using the tools of causal inference. We introduce counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn't change model predictions. We connect counterfactual invariance to out-of-domain model performance, and provide practical schemes for learning (approximately) counterfactual invariant predictors (without access to counterfactual examples). It turns out that both the means and implications of counterfactual invariance depend fundamentally on the true underlying causal structure of the data -- in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactual invariance. Similarly, counterfactual invariance implies different domain shift guarantees depending on the underlying causal structure. This theory is supported by empirical results on text classification.},
  urldate = {2022-02-01},
  journal = {arXiv:2106.00545 [cs, stat]},
  author = {Veitch, Victor and D'Amour, Alexander and Yadlowsky, Steve and Eisenstein, Jacob},
  month = nov,
  year = {2021},
  note = {arXiv: 2106.00545},
  keywords = {causality, counterfactual, invariance, ood, shortcuts},
}

@article{gentzel_case_2019,
  title = {The {Case} for {Evaluating} {Causal} {Models} {Using} {Interventional} {Measures} and {Empirical} {Data}},
  url = {http://arxiv.org/abs/1910.05387},
  abstract = {Causal inference is central to many areas of artificial intelligence, including complex reasoning, planning, knowledge-base construction, robotics, explanation, and fairness. An active community of researchers develops and enhances algorithms that learn causal models from data, and this work has produced a series of impressive technical advances. However, evaluation techniques for causal modeling algorithms have remained somewhat primitive, limiting what we can learn from experimental studies of algorithm performance, constraining the types of algorithms and model representations that researchers consider, and creating a gap between theory and practice. We argue for more frequent use of evaluation techniques that examine interventional measures rather than structural or observational measures, and that evaluate those measures on empirical data rather than synthetic data. We survey the current practice in evaluation and show that the techniques we recommend are rarely used in practice. We show that such techniques are feasible and that data sets are available to conduct such evaluations. We also show that these techniques produce substantially different results than using structural measures and synthetic data.},
  urldate = {2022-01-09},
  journal = {arXiv:1910.05387 [cs, stat]},
  author = {Gentzel, Amanda and Garant, Dan and Jensen, David},
  month = nov,
  year = {2019},
  note = {arXiv: 1910.05387},
}

@misc{noauthor_affordance_nodate,
  title = {Affordance detection of tool parts from geometric features {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
  url = {https://ieeexplore.ieee.org/document/7139369},
  urldate = {2021-12-16},
  keywords = {affordances, robotics},
}

@inproceedings{qian_enhancing_2019-1,
  address = {Hong Kong, China},
  title = {Enhancing {Variational} {Autoencoders} with {Mutual} {Information} {Neural} {Estimation} for {Text} {Generation}},
  url = {https://aclanthology.org/D19-1416},
  doi = {10.18653/v1/D19-1416},
  abstract = {While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the latent variable fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the VAE. In this paper, we propose to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ neural networks for the estimation of the MI and provide a training algorithm based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in language modeling and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.},
  urldate = {2021-12-02},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher = {Association for Computational Linguistics},
  author = {Qian, Dong and Cheung, William K.},
  month = nov,
  year = {2019},
  pages = {4047--4057},
}

@article{xu_videoclip_2021,
  title = {{VideoCLIP}: {Contrastive} {Pre}-training for {Zero}-shot {Video}-{Text} {Understanding}},
  shorttitle = {{VideoCLIP}},
  url = {http://arxiv.org/abs/2109.14084},
  abstract = {We present VideoCLIP, a contrastive approach to pre-train a unified model for zero-shot video and text understanding, without using any labels on downstream tasks. VideoCLIP trains a transformer for video and text by contrasting temporally overlapping positive video-text pairs with hard negatives from nearest neighbor retrieval. Our experiments on a diverse series of downstream tasks, including sequence-level text-video retrieval, VideoQA, token-level action localization, and action segmentation reveal state-of-the-art performance, surpassing prior work, and in some cases even outperforming supervised approaches. Code is made available at https://github.com/pytorch/fairseq/tree/main/examples/MMPT.},
  urldate = {2021-11-30},
  journal = {arXiv:2109.14084 [cs]},
  author = {Xu, Hu and Ghosh, Gargi and Huang, Po-Yao and Okhonko, Dmytro and Aghajanyan, Armen and Metze, Florian and Zettlemoyer, Luke and Feichtenhofer, Christoph},
  month = oct,
  year = {2021},
  note = {arXiv: 2109.14084},
  keywords = {clip, video, vision-language, zero-shot},
}

@misc{noauthor_bc-z_nodate,
  title = {bc-z},
  url = {https://sites.google.com/view/bc-z/home},
  abstract = {We study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such},
  language = {en},
  urldate = {2021-11-30},
  keywords = {multi-task, robotics, vision-language, zero-shot},
}

@article{stanczuk_wasserstein_2021,
  title = {Wasserstein {GANs} {Work} {Because} {They} {Fail} (to {Approximate} the {Wasserstein} {Distance})},
  url = {http://arxiv.org/abs/2103.01678},
  abstract = {Wasserstein GANs are based on the idea of minimising the Wasserstein distance between a real and a generated distribution. We provide an in-depth mathematical analysis of differences between the theoretical setup and the reality of training Wasserstein GANs. In this work, we gather both theoretical and empirical evidence that the WGAN loss is not a meaningful approximation of the Wasserstein distance. Moreover, we argue that the Wasserstein distance is not even a desirable loss function for deep generative models, and conclude that the success of Wasserstein GANs can in truth be attributed to a failure to approximate the Wasserstein distance.},
  urldate = {2021-11-30},
  journal = {arXiv:2103.01678 [cs, stat]},
  author = {Stanczuk, Jan and Etmann, Christian and Kreusser, Lisa Maria and Schönlieb, Carola-Bibiane},
  month = oct,
  year = {2021},
  note = {arXiv: 2103.01678},
  keywords = {gans, optimal\_transport, wasserstein},
}

@article{wang_simvlm_2021,
  title = {{SimVLM}: {Simple} {Visual} {Language} {Model} {Pretraining} with {Weak} {Supervision}},
  shorttitle = {{SimVLM}},
  url = {http://arxiv.org/abs/2108.10904},
  abstract = {With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74\% vqa-score), NLVR2 (+1.17\% accuracy), SNLI-VE (+1.37\% accuracy) and image captioning tasks (+10.1\% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer.},
  urldate = {2021-11-30},
  journal = {arXiv:2108.10904 [cs]},
  author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  month = aug,
  year = {2021},
  note = {arXiv: 2108.10904},
  keywords = {vision-language},
}

@article{shao_concept2robot_nodate,
  title = {{Concept2Robot}: {Learning} {Manipulation} {Concepts} from {Instructions} and {Human} {Demonstrations}},
  abstract = {We aim to endow a robot with the ability to learn manipulation concepts that link natural language instructions to motor skills. Our goal is to learn a single multi-task policy that takes as input a natural language instruction and an image of the initial scene and outputs a robot motion trajectory to achieve the speciﬁed task. This policy has to generalize over different instructions and environments. Our insight is that we can approach this problem through Learning from Demonstration by leveraging large-scale video datasets of humans performing manipulation actions. Thereby, we avoid more time-consuming processes such as teleoperation or kinesthetic teaching. We also avoid having to manually design task-speciﬁc rewards. We propose a two-stage learning process where we ﬁrst learn single-task policies through reinforcement learning. The reward is provided by scoring how well the robot visually appears to perform the task. This score is given by a video-based action classiﬁer trained on a large-scale human activity dataset. In the second stage, we train a multi-task policy through imitation learning to imitate all the single-task policies. In extensive simulation experiments, we show that the multi-task policy learns to perform a large percentage of the 78 different manipulation tasks on which it was trained. The tasks are of greater variety and complexity than previously considered robot manipulation tasks. We show that the policy generalizes over variations of the environment. We also show examples of successful generalization over novel but similar instructions.},
  language = {en},
  author = {Shao, Lin and Migimatsu, Toki and Zhang, Qiang and Yang, Karen and Bohg, Jeannette},
  keywords = {robotics},
  pages = {11},
}

@article{chefer_generic_2021,
  title = {Generic {Attention}-model {Explainability} for {Interpreting} {Bi}-{Modal} and {Encoder}-{Decoder} {Transformers}},
  url = {http://arxiv.org/abs/2103.15679},
  abstract = {Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.},
  urldate = {2021-11-24},
  journal = {arXiv:2103.15679 [cs]},
  author = {Chefer, Hila and Gur, Shir and Wolf, Lior},
  month = mar,
  year = {2021},
  note = {arXiv: 2103.15679},
  keywords = {attention, interpretability},
}

@article{selvaraju_grad-cam_2020,
  title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
  volume = {128},
  issn = {0920-5691, 1573-1405},
  shorttitle = {Grad-{CAM}},
  url = {http://arxiv.org/abs/1610.02391},
  doi = {10.1007/s11263-019-01228-7},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  number = {2},
  urldate = {2021-11-24},
  journal = {International Journal of Computer Vision},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  month = feb,
  year = {2020},
  note = {arXiv: 1610.02391},
  pages = {336--359},
}

@article{zeng_transporter_2021,
  title = {Transporter {Networks}: {Rearranging} the {Visual} {World} for {Robotic} {Manipulation}},
  shorttitle = {Transporter {Networks}},
  url = {http://arxiv.org/abs/2010.14406},
  abstract = {Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io},
  urldate = {2021-11-24},
  journal = {arXiv:2010.14406 [cs]},
  author = {Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Sindhwani, Vikas and Lee, Johnny},
  month = feb,
  year = {2021},
  note = {arXiv: 2010.14406},
  keywords = {robotics},
}

@article{wang_actionclip_2021,
  title = {{ActionCLIP}: {A} {New} {Paradigm} for {Video} {Action} {Recognition}},
  shorttitle = {{ActionCLIP}},
  url = {http://arxiv.org/abs/2109.08472},
  abstract = {The canonical approach to video action recognition dictates a neural model to do a classic and standard 1-of-N majority vote task. They are trained to predict a fixed set of predefined categories, limiting their transferable ability on new datasets with unseen concepts. In this paper, we provide a new perspective on action recognition by attaching importance to the semantic information of label texts rather than simply mapping them into numbers. Specifically, we model this task as a video-text matching problem within a multimodal learning framework, which strengthens the video representation with more semantic language supervision and enables our model to do zero-shot action recognition without any further labeled data or parameters requirements. Moreover, to handle the deficiency of label texts and make use of tremendous web data, we propose a new paradigm based on this multimodal learning framework for action recognition, which we dub "pre-train, prompt and fine-tune". This paradigm first learns powerful representations from pre-training on a large amount of web image-text or video-text data. Then it makes the action recognition task to act more like pre-training problems via prompt engineering. Finally, it end-to-end fine-tunes on target datasets to obtain strong performance. We give an instantiation of the new paradigm, ActionCLIP, which not only has superior and flexible zero-shot/few-shot transfer ability but also reaches a top performance on general action recognition task, achieving 83.8\% top-1 accuracy on Kinetics-400 with a ViT-B/16 as the backbone. Code is available at https://github.com/sallymmx/ActionCLIP.git},
  urldate = {2021-11-24},
  journal = {arXiv:2109.08472 [cs]},
  author = {Wang, Mengmeng and Xing, Jiazheng and Liu, Yong},
  month = sep,
  year = {2021},
  note = {arXiv: 2109.08472},
  keywords = {clip},
}

@article{turpin_gift_2021,
  title = {{GIFT}: {Generalizable} {Interaction}-aware {Functional} {Tool} {Affordances} without {Labels}},
  shorttitle = {{GIFT}},
  url = {http://arxiv.org/abs/2106.14973},
  abstract = {Tool use requires reasoning about the fit between an object's affordances and the demands of a task. Visual affordance learning can benefit from goal-directed interaction experience, but current techniques rely on human labels or expert demonstrations to generate this data. In this paper, we describe a method that grounds affordances in physical interactions instead, thus removing the need for human labels or expert policies. We use an efficient sampling-based method to generate successful trajectories that provide contact data, which are then used to reveal affordance representations. Our framework, GIFT, operates in two phases: first, we discover visual affordances from goal-directed interaction with a set of procedurally generated tools; second, we train a model to predict new instances of the discovered affordances on novel tools in a self-supervised fashion. In our experiments, we show that GIFT can leverage a sparse keypoint representation to predict grasp and interaction points to accommodate multiple tasks, such as hooking, reaching, and hammering. GIFT outperforms baselines on all tasks and matches a human oracle on two of three tasks using novel tools.},
  urldate = {2021-11-24},
  journal = {arXiv:2106.14973 [cs]},
  author = {Turpin, Dylan and Wang, Liquan and Tsogkas, Stavros and Dickinson, Sven and Garg, Animesh},
  month = jun,
  year = {2021},
  note = {arXiv: 2106.14973},
  keywords = {robotics},
}

@misc{noauthor_voxel51fiftyone_2021,
  title = {voxel51/fiftyone},
  url = {https://github.com/voxel51/fiftyone},
  abstract = {The open-source tool for building high-quality datasets and computer vision models},
  urldate = {2021-11-23},
  publisher = {Voxel51},
  month = nov,
  year = {2021},
  note = {original-date: 2020-04-22T13:43:28Z},
}

@misc{noauthor_opencvopencv_2021,
  title = {opencv/opencv},
  url = {https://github.com/opencv/opencv},
  abstract = {Open Source Computer Vision Library},
  urldate = {2021-11-23},
  publisher = {OpenCV},
  month = nov,
  year = {2021},
  note = {original-date: 2012-07-19T09:40:17Z},
  keywords = {software},
}

@article{kingma_adam_2017,
  title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  shorttitle = {Adam},
  url = {http://arxiv.org/abs/1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2021-11-23},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  month = jan,
  year = {2017},
  note = {arXiv: 1412.6980},
  keywords = {optimization},
}

@article{pham_combined_2021,
  title = {Combined {Scaling} for {Zero}-shot {Transfer} {Learning}},
  url = {http://arxiv.org/abs/2111.10050},
  abstract = {We present a combined scaling method called BASIC that achieves 85.7\% top-1 zero-shot accuracy on the ImageNet ILSVRC-2012 validation set, surpassing the best-published zero-shot models - CLIP and ALIGN - by 9.3\%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-\{A,R,V2,Sketch\} and ObjectNet, our model achieves 83.7\% top-1 average accuracy, only a small drop from the its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Our batch size is 65536 which is 2x more than CLIP and 4x more than ALIGN. The main challenge with scaling is the limited memory of our accelerators such as GPUs and TPUs. We hence propose a simple method of online gradient caching to overcome this limit.},
  urldate = {2021-11-23},
  journal = {arXiv:2111.10050 [cs]},
  author = {Pham, Hieu and Dai, Zihang and Ghiasi, Golnaz and Liu, Hanxiao and Yu, Adams Wei and Luong, Minh-Thang and Tan, Mingxing and Le, Quoc V.},
  month = nov,
  year = {2021},
  note = {arXiv: 2111.10050},
  keywords = {vision-language},
}

@article{abnar_quantifying_2020,
  title = {Quantifying {Attention} {Flow} in {Transformers}},
  url = {http://arxiv.org/abs/2005.00928},
  abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  urldate = {2021-11-23},
  journal = {arXiv:2005.00928 [cs]},
  author = {Abnar, Samira and Zuidema, Willem},
  month = may,
  year = {2020},
  note = {arXiv: 2005.00928},
  keywords = {attention, explainability},
}

@misc{gildenblat_explainability_2021,
  title = {Explainability for {Vision} {Transformers} (in {PyTorch})},
  copyright = {MIT},
  url = {https://github.com/jacobgil/vit-explain},
  abstract = {Explainability for Vision Transformers},
  urldate = {2021-11-23},
  author = {Gildenblat, Jacob},
  month = nov,
  year = {2021},
  note = {original-date: 2020-12-29T11:27:52Z},
  keywords = {explainability},
}

@article{russakovsky_imagenet_2015,
  title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
  url = {http://arxiv.org/abs/1409.0575},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  urldate = {2021-11-23},
  journal = {arXiv:1409.0575 [cs]},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  month = jan,
  year = {2015},
  note = {arXiv: 1409.0575},
  keywords = {dataset},
}

@techreport{packer_text_2018,
  title = {Text {Embeddings} {Contain} {Bias}. {Here}'s {Why} {That} {Matters}.},
  institution = {Google},
  author = {Packer, Ben and Mitchell, M. and Guajardo-Céspedes, Mario and Halpern, Yoni},
  year = {2018},
  keywords = {bias},
}

@inproceedings{wang_towards_2020,
  title = {Towards {Fairness} in {Visual} {Recognition}: {Effective} {Strategies} for {Bias} {Mitigation}},
  shorttitle = {Towards {Fairness} in {Visual} {Recognition}},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Towards_Fairness_in_Visual_Recognition_Effective_Strategies_for_Bias_Mitigation_CVPR_2020_paper.html},
  urldate = {2021-11-17},
  author = {Wang, Zeyu and Qinami, Klint and Karakozis, Ioannis Christos and Genova, Kyle and Nair, Prem and Hata, Kenji and Russakovsky, Olga},
  year = {2020},
  keywords = {bias},
  pages = {8919--8928},
}

@article{gal_stylegan-nada_2021,
  title = {{StyleGAN}-{NADA}: {CLIP}-{Guided} {Domain} {Adaptation} of {Image} {Generators}},
  shorttitle = {{StyleGAN}-{NADA}},
  url = {http://arxiv.org/abs/2108.00946},
  abstract = {Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.},
  urldate = {2021-11-17},
  journal = {arXiv:2108.00946 [cs]},
  author = {Gal, Rinon and Patashnik, Or and Maron, Haggai and Chechik, Gal and Cohen-Or, Daniel},
  month = aug,
  year = {2021},
  note = {arXiv: 2108.00946},
  keywords = {clip},
}

@misc{noauthor_nerdyrodentvqgan-clip_nodate,
  title = {nerdyrodent/{VQGAN}-{CLIP}: {Just} playing with getting {VQGAN}+{CLIP} running locally, rather than having to use colab.},
  url = {https://github.com/nerdyrodent/VQGAN-CLIP},
  urldate = {2021-11-17},
}

@article{rudin_interpretable_2021,
  title = {Interpretable {Machine} {Learning}: {Fundamental} {Principles} and 10 {Grand} {Challenges}},
  shorttitle = {Interpretable {Machine} {Learning}},
  url = {http://arxiv.org/abs/2103.11251},
  abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the "Rashomon set" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
  urldate = {2021-11-17},
  journal = {arXiv:2103.11251 [cs, stat]},
  author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
  month = jul,
  year = {2021},
  note = {arXiv: 2103.11251},
  keywords = {interpretable},
}

@misc{noauthor_zero-shot_nodate,
  title = {Zero-{Shot} {Text}-to-{Image} {Generation}},
  url = {https://arxiv.org/abs/2102.12092},
  urldate = {2021-11-17},
}

@article{krylov_open_2021,
  title = {Open {Images} {V5} {Text} {Annotation} and {Yet} {Another} {Mask} {Text} {Spotter}},
  url = {http://arxiv.org/abs/2106.12326},
  abstract = {A large scale human-labeled dataset plays an important role in creating high quality deep learning models. In this paper we present text annotation for Open Images V5 dataset. To our knowledge it is the largest among publicly available manually created text annotations. Having this annotation we trained a simple Mask-RCNN-based network, referred as Yet Another Mask Text Spotter (YAMTS), which achieves competitive performance or even outperforms current state-of-the-art approaches in some cases on ICDAR2013, ICDAR2015 and Total-Text datasets. Code for text spotting model available online at: https://github.com/openvinotoolkit/training\_extensions. The model can be exported to OpenVINO-format and run on Intel CPUs.},
  urldate = {2021-11-17},
  journal = {arXiv:2106.12326 [cs]},
  author = {Krylov, Ilya and Nosov, Sergei and Sovrasov, Vladislav},
  month = jun,
  year = {2021},
  note = {arXiv: 2106.12326},
}

@article{benenson_large-scale_2019,
  title = {Large-scale interactive object segmentation with human annotators},
  url = {http://arxiv.org/abs/1903.10830},
  abstract = {Manually annotating object segmentation masks is very time consuming. Interactive object segmentation methods offer a more efficient alternative where a human annotator and a machine segmentation model collaborate. In this paper we make several contributions to interactive segmentation: (1) we systematically explore in simulation the design space of deep interactive segmentation models and report new insights and caveats; (2) we execute a large-scale annotation campaign with real human annotators, producing masks for 2.5M instances on the OpenImages dataset. We plan to release this data publicly, forming the largest existing dataset for instance segmentation. Moreover, by re-annotating part of the COCO dataset, we show that we can produce instance masks 3 times faster than traditional polygon drawing tools while also providing better quality. (3) We present a technique for automatically estimating the quality of the produced masks which exploits indirect signals from the annotation process.},
  urldate = {2021-11-17},
  journal = {arXiv:1903.10830 [cs]},
  author = {Benenson, Rodrigo and Popov, Stefan and Ferrari, Vittorio},
  month = apr,
  year = {2019},
  note = {arXiv: 1903.10830},
  keywords = {dataset]},
}

@article{kuznetsova_open_2020,
  title = {The {Open} {Images} {Dataset} {V4}: {Unified} image classification, object detection, and visual relationship detection at scale},
  volume = {128},
  issn = {0920-5691, 1573-1405},
  shorttitle = {The {Open} {Images} {Dataset} {V4}},
  url = {http://arxiv.org/abs/1811.00982},
  doi = {10.1007/s11263-020-01316-z},
  abstract = {We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having unified annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.},
  number = {7},
  urldate = {2021-11-17},
  journal = {International Journal of Computer Vision},
  author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio},
  month = jul,
  year = {2020},
  note = {arXiv: 1811.00982},
  keywords = {dataset},
  pages = {1956--1981},
}

@article{scholkopf_towards_2021,
  title = {Towards {Causal} {Representation} {Learning}},
  url = {http://arxiv.org/abs/2102.11107},
  abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  urldate = {2021-11-17},
  journal = {arXiv:2102.11107 [cs]},
  author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  month = feb,
  year = {2021},
  note = {arXiv: 2102.11107},
  keywords = {causality},
}

@article{nado_uncertainty_2021,
  title = {Uncertainty {Baselines}: {Benchmarks} for {Uncertainty} \& {Robustness} in {Deep} {Learning}},
  shorttitle = {Uncertainty {Baselines}},
  url = {http://arxiv.org/abs/2106.04015},
  abstract = {High-quality estimates of uncertainty and robustness are crucial for numerous real-world applications, especially for deep learning which underlies many deployed ML systems. The ability to compare techniques for improving these estimates is therefore very important for research and practice alike. Yet, competitive comparisons of methods are often lacking due to a range of reasons, including: compute availability for extensive tuning, incorporation of sufficiently many baselines, and concrete documentation for reproducibility. In this paper we introduce Uncertainty Baselines: high-quality implementations of standard and state-of-the-art deep learning methods on a variety of tasks. As of this writing, the collection spans 19 methods across 9 tasks, each with at least 5 metrics. Each baseline is a self-contained experiment pipeline with easily reusable and extendable components. Our goal is to provide immediate starting points for experimentation with new methods or applications. Additionally we provide model checkpoints, experiment outputs as Python notebooks, and leaderboards for comparing results. Code available at https://github.com/google/uncertainty-baselines.},
  urldate = {2021-11-17},
  journal = {arXiv:2106.04015 [cs]},
  author = {Nado, Zachary and Band, Neil and Collier, Mark and Djolonga, Josip and Dusenberry, Michael W. and Farquhar, Sebastian and Filos, Angelos and Havasi, Marton and Jenatton, Rodolphe and Jerfel, Ghassen and Liu, Jeremiah and Mariet, Zelda and Nixon, Jeremy and Padhy, Shreyas and Ren, Jie and Rudner, Tim G. J. and Wen, Yeming and Wenzel, Florian and Murphy, Kevin and Sculley, D. and Lakshminarayanan, Balaji and Snoek, Jasper and Gal, Yarin and Tran, Dustin},
  month = jun,
  year = {2021},
  note = {arXiv: 2106.04015},
  keywords = {uncertainty},
}

@inproceedings{bevilacqua_recent_2021,
  address = {Montreal, Canada},
  title = {Recent {Trends} in {Word} {Sense} {Disambiguation}: {A} {Survey}},
  isbn = {978-0-9992411-9-6},
  shorttitle = {Recent {Trends} in {Word} {Sense} {Disambiguation}},
  url = {https://www.ijcai.org/proceedings/2021/593},
  doi = {10.24963/ijcai.2021/593},
  abstract = {Word Sense Disambiguation (WSD) aims at making explicit the semantics of a word in context by identifying the most suitable meaning from a predeﬁned sense inventory. Recent breakthroughs in representation learning have fueled intensive WSD research, resulting in considerable performance improvements, breaching the 80\% glass ceiling set by the inter-annotator agreement. In this survey, we provide an extensive overview of current advances in WSD, describing the state of the art in terms of i) resources for the task, i.e., sense inventories and reference datasets for training and testing, as well as ii) automatic disambiguation approaches, detailing their peculiarities, strengths and weaknesses. Finally, we highlight the current limitations of the task itself, but also point out recent trends that could help expand the scope and applicability of WSD, setting up new promising directions for the future.},
  language = {en},
  urldate = {2021-11-17},
  booktitle = {Proceedings of the {Thirtieth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  author = {Bevilacqua, Michele and Pasini, Tommaso and Raganato, Alessandro and Navigli, Roberto},
  month = aug,
  year = {2021},
  keywords = {ambiguity, bert},
  pages = {4330--4338},
}

@article{peters_deep_2018,
  title = {Deep contextualized word representations},
  url = {http://arxiv.org/abs/1802.05365},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  urldate = {2021-11-17},
  journal = {arXiv:1802.05365 [cs]},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  month = mar,
  year = {2018},
  note = {arXiv: 1802.05365},
  keywords = {bert, nlp},
}

@article{wiedemann_does_nodate,
  title = {Does {BERT} {Make} {Any} {Sense}? {Interpretable} {Word} {Sense} {Disambiguation} with {Contextualized} {Embeddings}},
  abstract = {Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al., 2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a major recent innovation in NLP. CWEs provide semantic vector representations of words depending on their respective context. Their advantage over static word embeddings has been shown for a number of tasks, such as text classiﬁcation, sequence tagging, or machine translation. Since vectors of the same word type can vary depending on the respective context, they implicitly provide a model for word sense disambiguation (WSD). We introduce a simple but effective approach to WSD using a nearest neighbor classiﬁcation on CWEs. We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets. We further show that the pre-trained BERT model is able to place polysemic words into distinct ‘sense’ regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability.},
  language = {en},
  author = {Wiedemann, Gregor and Remus, Steffen and Chawla, Avi and Biemann, Chris},
  keywords = {ambiguity, bert},
  pages = {10},
}

@inproceedings{huang_glossbert_2019,
  address = {Hong Kong, China},
  title = {{GlossBERT}: {BERT} for {Word} {Sense} {Disambiguation} with {Gloss} {Knowledge}},
  shorttitle = {{GlossBERT}},
  url = {https://aclanthology.org/D19-1355},
  doi = {10.18653/v1/D19-1355},
  abstract = {Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like WordNet, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss (sense definition) into neural networks for WSD. However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT based models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.},
  urldate = {2021-11-17},
  booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher = {Association for Computational Linguistics},
  author = {Huang, Luyao and Sun, Chi and Qiu, Xipeng and Huang, Xuanjing},
  month = nov,
  year = {2019},
  keywords = {ambiguity, bert},
  pages = {3509--3514},
}

@article{prasanna_when_2020,
  title = {When {BERT} {Plays} the {Lottery}, {All} {Tickets} {Are} {Winning}},
  doi = {10.18653/v1/2020.emnlp-main.259},
  abstract = {It is shown that the "bad" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the "good" ones, indicating that most weights in the pre-trained BERT are potentially useful. Much of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the "bad" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the "good" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the "good" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.},
  journal = {EMNLP},
  author = {Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  year = {2020},
  keywords = {bert, nlp},
}

@article{hou_dynabert_2020,
  title = {{DynaBERT}: {Dynamic} {BERT} with {Adaptive} {Width} and {Depth}},
  shorttitle = {{DynaBERT}},
  abstract = {A novel dynamic BERT model, which can run at adaptive width and depth, is proposed (abbreviated as DynaBERT), which has comparable performance as BERT (or RoBERTa), while at smaller widths and depths consistently outperforms existing BERT compression methods. The pre-trained language models like BERT and RoBERTa, though powerful in many natural language processing tasks, are both computational and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually reduce the large BERT model to a fixed smaller size, and can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can run at adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allows both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT (or RoBERTa), while at smaller widths and depths consistently outperforms existing BERT compression methods.},
  journal = {NeurIPS},
  author = {Hou, Lu and Huang, Zhiqi and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  year = {2020},
  keywords = {bert, nlp},
}

@article{vig_bertology_2021,
  title = {{BERTology} {Meets} {Biology}: {Interpreting} {Attention} in {Protein} {Language} {Models}},
  shorttitle = {{BERTology} {Meets} {Biology}},
  doi = {10.1101/2020.06.26.174417},
  abstract = {The inner workings of the Transformer is analyzed and it is shown that attention captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure. Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at https://github.com/salesforce/provis.},
  journal = {ICLR},
  author = {Vig, Jesse and Madani, Ali and Varshney, L. and Xiong, Caiming and Socher, R. and Rajani, Nazneen},
  year = {2021},
  keywords = {bert, nlp},
}

@article{chalkidis_legal-bert_2020,
  title = {{LEGAL}-{BERT}: {The} {Muppets} straight out of {Law} {School}},
  shorttitle = {{LEGAL}-{BERT}},
  doi = {10.18653/v1/2020.findings-emnlp.261},
  abstract = {This work proposes a broader hyper-parameter search space when fine-tuning for downstream tasks and releases LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications. BERT has achieved impressive performance in several NLP tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying BERT models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying BERT in specialised domains. These are: (a) use the original BERT out of the box, (b) adapt BERT by additional pre-training on domain-specific corpora, and (c) pre-train BERT from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release LEGAL-BERT, a family of BERT models intended to assist legal NLP research, computational law, and legal technology applications.},
  journal = {EMNLP 2020},
  author = {Chalkidis, Ilias and Fergadiotis, Manos and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  year = {2020},
  keywords = {bert, nlp},
}

@article{lewis_pre-training_2020,
  title = {Pre-training via {Paraphrasing}},
  abstract = {It is shown that fine-tuning gives strong performance on a range of discriminative and generative tasks in many languages, making MARGE the most generally applicable pre-training method to date. We introduce MARGE, a pre-trained sequence-to-sequence model learned with an unsupervised multi-lingual multi-document paraphrasing objective. MARGE provides an alternative to the dominant masked language modeling paradigm, where we self-supervise the reconstruction of target text by retrieving a set of related texts (in many languages) and conditioning on them to maximize the likelihood of generating the original. We show it is possible to jointly learn to do retrieval and reconstruction, given only a random initialization. The objective noisily captures aspects of paraphrase, translation, multi-document summarization, and information retrieval, allowing for strong zero-shot performance on several tasks. For example, with no additional task-specific training we achieve BLEU scores of up to 35.8 for document translation. We further show that fine-tuning gives strong performance on a range of discriminative and generative tasks in many languages, making MARGE the most generally applicable pre-training method to date.},
  journal = {NeurIPS},
  author = {Lewis, M. and Ghazvininejad, Marjan and Ghosh, Gargi and Aghajanyan, Armen and Wang, Sida and Zettlemoyer, Luke},
  year = {2020},
  keywords = {bert, nlp},
}

@article{devlin_bert_2019,
  title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url = {http://arxiv.org/abs/1810.04805},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate = {2021-11-17},
  journal = {arXiv:1810.04805 [cs]},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  month = may,
  year = {2019},
  note = {arXiv: 1810.04805},
  keywords = {bert, nlp},
}

@article{rogers_primer_2020,
  title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
  shorttitle = {A {Primer} in {BERTology}},
  url = {http://arxiv.org/abs/2002.12327},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  urldate = {2021-11-17},
  journal = {arXiv:2002.12327 [cs]},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  month = nov,
  year = {2020},
  note = {arXiv: 2002.12327},
  keywords = {analysis, bert},
}

@misc{dunteman_clip_nodate,
  title = {{CLIP} {API} by {Booste} - {Use} {OpenAI}'s newest image classifier with one line of code},
  url = {https://www.producthunt.com/posts/clip-api-by-booste},
  abstract = {Booste APIs boiling all the pains of the production ML stack into a single line of code. We're super excited to launch an API to OpenAI's powerful new no-training-needed image classifier to Python and Node. Give it a spin!},
  language = {en},
  urldate = {2021-11-17},
  journal = {Product Hunt},
  author = {Dunteman, Erik},
  keywords = {clip},
}

@misc{solawetz_how_2021,
  title = {How to {Try} {CLIP}: {OpenAI}'s {Zero}-{Shot} {Image} {Classifier}},
  shorttitle = {How to {Try} {CLIP}},
  url = {https://blog.roboflow.com/how-to-use-openai-clip/},
  abstract = {Earlier this week, OpenAI dropped a bomb on the computer vision world.},
  language = {en},
  urldate = {2021-11-17},
  journal = {Roboflow Blog},
  author = {Solawetz, Jacob and JAN 8, Brad Dwyer and Read, 2021 5 Min},
  month = jan,
  year = {2021},
  keywords = {clip},
}

@misc{gautam_build_2021,
  title = {Build your own {Search} {Engine} using {OpenAI}’s {CLIP} and {FastAPI} — {Part} 1},
  url = {https://adeshg7.medium.com/build-your-own-search-engine-using-openais-clip-and-fastapi-part-1-89995aefbcdd},
  abstract = {Okay, so this search engine will not be a full-fledged one but you’ll get a hand wtih all the capabilities of the OpenAI’s CLIP model and…},
  language = {en},
  urldate = {2021-11-17},
  journal = {Medium},
  author = {Gautam, Adesh},
  month = jun,
  year = {2021},
  keywords = {clip},
}

@article{bommasani_opportunities_2021,
  title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
  url = {http://arxiv.org/abs/2108.07258},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  urldate = {2021-11-17},
  journal = {arXiv:2108.07258 [cs]},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  month = aug,
  year = {2021},
  note = {arXiv: 2108.07258},
  keywords = {bert, clip, multimodal, pretrained},
}

@article{shridhar_cliport_2021,
  title = {{CLIPort}: {What} and {Where} {Pathways} for {Robotic} {Manipulation}},
  shorttitle = {{CLIPort}},
  url = {http://arxiv.org/abs/2109.12098},
  abstract = {How can we imbue robots with the ability to manipulate objects precisely but also to reason about them in terms of abstract concepts? Recent works in manipulation have shown that end-to-end networks can learn dexterous skills that require precise spatial reasoning, but these methods often fail to generalize to new goals or quickly learn transferable concepts across tasks. In parallel, there has been great progress in learning generalizable semantic representations for vision and language by training on large-scale internet data, however these representations lack the spatial understanding necessary for fine-grained manipulation. To this end, we propose a framework that combines the best of both worlds: a two-stream architecture with semantic and spatial pathways for vision-based manipulation. Specifically, we present CLIPort, a language-conditioned imitation-learning agent that combines the broad semantic understanding (what) of CLIP [1] with the spatial precision (where) of Transporter [2]. Our end-to-end framework is capable of solving a variety of language-specified tabletop tasks from packing unseen objects to folding cloths, all without any explicit representations of object poses, instance segmentations, memory, symbolic states, or syntactic structures. Experiments in simulated and real-world settings show that our approach is data efficient in few-shot settings and generalizes effectively to seen and unseen semantic concepts. We even learn one multi-task policy for 10 simulated and 9 real-world tasks that is better or comparable to single-task policies.},
  urldate = {2021-11-17},
  journal = {arXiv:2109.12098 [cs]},
  author = {Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  month = sep,
  year = {2021},
  note = {arXiv: 2109.12098},
  keywords = {clip, robotics, vision-language},
}

@article{fort_exploring_2021,
  title = {Exploring the {Limits} of {Out}-of-{Distribution} {Detection}},
  abstract = {It is demonstrated that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities, and a new way of using just the names of outlier classes as a sole source of information without any accompanying images is explored. Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85\% (current SOTA) to 96\% using Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66\% to 77\% using transformers and unsupervised pre-training. To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR10 can be improved to 98.7\% with just 1 image per OOD class, and 99.46\% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard vision OOD benchmark tasks.},
  journal = {ArXiv},
  author = {Fort, Stanislav and Ren, Jie and Lakshminarayanan, Balaji},
  year = {2021},
  keywords = {clip, ood\_detection},
}

@article{minderer_revisiting_2021,
  title = {Revisiting the {Calibration} of {Modern} {Neural} {Networks}},
  abstract = {It is found that the most recent models, notably those not using convolutions, are among the best calibrated, and architecture is a major determinant of calibration properties. Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.},
  journal = {ArXiv},
  author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, F. and Zhai, Xiaohua and Houlsby, N. and Tran, Dustin and Lucic, Mario},
  year = {2021},
  keywords = {calibration, clip, uncertainty},
}

@article{luo_clip4clip_2021,
  title = {{CLIP4Clip}: {An} {Empirical} {Study} of {CLIP} for {End} to {End} {Video} {Clip} {Retrieval}},
  shorttitle = {{CLIP4Clip}},
  abstract = {A CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, and LSMDC. Video-text retrieval plays an essential role in multi-modal research and has been widely used in many real-world web applications. The CLIP (Contrastive Language-Image Pretraining), an image-language pre-training model, has demonstrated the power of visual concepts learning from web collected imagetext datasets. In this paper, we propose a CLIP4Clip model to transfer the knowledge of the CLIP model to video-language retrieval in an end-to-end manner. Several questions are investigated via empirical studies: 1) Whether image feature is enough for video-text retrieval? 2) How a post-pretraining on a largescale video-text dataset based on the CLIP affect the performance? 3) What is the practical mechanism to model temporal dependency between video frames? And 4) The Hyperparameters sensitivity of the model on videotext retrieval task. Extensive experimental results present that the CLIP4Clip model transferred from the CLIP can achieve SOTA results on various video-text retrieval datasets, including MSR-VTT, MSVC, and LSMDC. We release our code at https://github. com/ArrowLuo/CLIP4Clip.},
  journal = {ArXiv},
  author = {Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
  year = {2021},
  keywords = {clip, video},
}

@inproceedings{serrano_is_2019,
  address = {Florence, Italy},
  title = {Is {Attention} {Interpretable}?},
  url = {https://aclanthology.org/P19-1282},
  doi = {10.18653/v1/P19-1282},
  abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
  urldate = {2021-11-16},
  booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
  publisher = {Association for Computational Linguistics},
  author = {Serrano, Sofia and Smith, Noah A.},
  month = jul,
  year = {2019},
  keywords = {attention, interpretability, nlp},
  pages = {2931--2951},
}

@article{brown_language_2020,
  title = {Language {Models} are {Few}-{Shot} {Learners}},
  url = {http://arxiv.org/abs/2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  urldate = {2021-11-16},
  journal = {arXiv:2005.14165 [cs]},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month = jul,
  year = {2020},
  note = {arXiv: 2005.14165},
  keywords = {gpt, language\_models, nlp, openai, prompt, zero-shot},
}

@article{mcinnes_umap_2020,
  title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
  shorttitle = {{UMAP}},
  url = {http://arxiv.org/abs/1802.03426},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  urldate = {2021-11-16},
  journal = {arXiv:1802.03426 [cs, stat]},
  author = {McInnes, Leland and Healy, John and Melville, James},
  month = sep,
  year = {2020},
  note = {arXiv: 1802.03426},
  keywords = {datavis, diffgeo, projection, umap},
}

@article{jia_scaling_2021,
  title = {Scaling {Up} {Visual} and {Vision}-{Language} {Representation} {Learning} {With} {Noisy} {Text} {Supervision}},
  url = {http://arxiv.org/abs/2102.05918},
  abstract = {Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the scale of our corpus can make up for its noise and leads to state-of-the-art representations even with such a simple learning scheme. Our visual representation achieves strong performance when transferred to classification tasks such as ImageNet and VTAB. The aligned visual and language representations enables zero-shot image classification and also set new state-of-the-art results on Flickr30K and MSCOCO image-text retrieval benchmarks, even when compared with more sophisticated cross-attention models. The representations also enable cross-modality search with complex text and text + image queries.},
  urldate = {2021-11-16},
  journal = {arXiv:2102.05918 [cs]},
  author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V. and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  month = jun,
  year = {2021},
  note = {arXiv: 2102.05918},
  keywords = {clip, vision-language},
}

@inproceedings{kalkowski_real-time_2015,
  address = {Brisbane, Australia},
  title = {Real-time {Analysis} and {Visualization} of the {YFCC100m} {Dataset}},
  isbn = {978-1-4503-3744-1},
  url = {http://dl.acm.org/citation.cfm?doid=2814815.2814820},
  doi = {10.1145/2814815.2814820},
  abstract = {With the Yahoo Flickr Creative Commons 100 Million (YFCC100m) dataset, a novel dataset was introduced to the computer vision and multimedia research community. To maximize the beneﬁt for the research community and utilize its potential, this dataset has to be made accessible by tools allowing to search for target concepts within the dataset and mechanism to browse images and videos of the dataset. Following best practice from data collections, such as ImageNet and MS COCO, this paper presents means of accessibility for the YFCC100m dataset. This includes a global analysis of the dataset and an online browser to explore and investigate subsets of the dataset in real-time. Providing statistics of the queried images and videos will enable researchers to reﬁne their query successively, such that the users desired subset of interest can be narrowed down quickly. The ﬁnal set of image and video can be downloaded as URLs from the browser for further processing.},
  language = {en},
  urldate = {2021-11-16},
  booktitle = {Proceedings of the 2015 {Workshop} on {Community}-{Organized} {Multimodal} {Mining}: {Opportunities} for {Novel} {Solutions} - {MMCommons}'15},
  publisher = {ACM Press},
  author = {Kalkowski, Sebastian and Schulze, Christian and Dengel, Andreas and Borth, Damian},
  year = {2015},
  pages = {25--30},
}

@article{thomee_yfcc100m_2016,
  title = {{YFCC100M}: {The} {New} {Data} in {Multimedia} {Research}},
  volume = {59},
  issn = {0001-0782, 1557-7317},
  shorttitle = {{YFCC100M}},
  url = {http://arxiv.org/abs/1503.01817},
  doi = {10.1145/2812802},
  abstract = {We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M), the largest public multimedia collection that has ever been released. The dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014. In this article we explain the rationale behind its creation, as well as the implications the dataset has for science, research, engineering, and development. We further present several new challenges in multimedia research that can now be expanded upon with our dataset.},
  number = {2},
  urldate = {2021-11-16},
  journal = {Communications of the ACM},
  author = {Thomee, Bart and Shamma, David A. and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  month = jan,
  year = {2016},
  note = {arXiv: 1503.01817},
  keywords = {dataset},
  pages = {64--73},
}

@misc{noauthor_clip_2021,
  title = {{CLIP}},
  copyright = {MIT},
  url = {https://github.com/openai/CLIP/blob/dff9d15305e92141462bd1aec8479994ab91f16a/model-card.md},
  abstract = {Contrastive Language-Image Pretraining},
  urldate = {2021-11-16},
  publisher = {OpenAI},
  month = nov,
  year = {2021},
  note = {original-date: 2020-12-16T11:24:42Z},
  keywords = {clip, code, github},
}

@article{klepousniotou_not_2012,
  title = {Not all ambiguous words are created equal: {An} {EEG} investigation of homonymy and polysemy},
  volume = {123},
  issn = {0093-934X},
  shorttitle = {Not all ambiguous words are created equal},
  url = {https://www.sciencedirect.com/science/article/pii/S0093934X12001125},
  doi = {10.1016/j.bandl.2012.06.007},
  abstract = {Event-related potentials (ERPs) were used to investigate the time-course of meaning activation of different types of ambiguous words. Unbalanced homonymous (“pen”), balanced homonymous (“panel”), metaphorically polysemous (“lip”), and metonymically polysemous words (“rabbit”) were used in a visual single-word priming delayed lexical decision task. The theoretical distinction between homonymy and polysemy was reflected in the N400 component. Homonymous words (balanced and unbalanced) showed effects of dominance/frequency with reduced N400 effects predominantly observed for dominant meanings. Polysemous words (metaphors and metonymies) showed effects of core meaning representation with both dominant and subordinate meanings showing reduced N400 effects. Furthermore, the division within polysemy, into metaphor and metonymy, was supported. Differences emerged in meaning activation patterns with the subordinate meanings of metaphor inducing differentially reduced N400 effects moving from left hemisphere electrode sites to right hemisphere electrode sites, potentially suggesting increased involvement of the right hemisphere in the processing of figurative meaning.},
  language = {en},
  number = {1},
  urldate = {2021-11-16},
  journal = {Brain and Language},
  author = {Klepousniotou, Ekaterini and Pike, G. Bruce and Steinhauer, Karsten and Gracco, Vincent},
  month = oct,
  year = {2012},
  keywords = {cogsci, eeg, language, neuroscience, psychology},
  pages = {11--21},
}

@article{radford_learning_2021,
  title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
  url = {http://arxiv.org/abs/2103.00020},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  urldate = {2021-11-14},
  journal = {arXiv:2103.00020 [cs]},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  month = feb,
  year = {2021},
  note = {arXiv: 2103.00020},
  keywords = {clip, multimodal, multitask, openai, vision-language, zero-shot},
}

@article{goh_multimodal_2021,
  title = {Multimodal {Neurons} in {Artificial} {Neural} {Networks}},
  volume = {6},
  issn = {2476-0757},
  url = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030},
  abstract = {We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.},
  language = {en},
  number = {3},
  urldate = {2021-11-09},
  journal = {Distill},
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  month = mar,
  year = {2021},
  keywords = {clip, multimodal, openai, vision-language},
  pages = {e30},
}

@inproceedings{saenko_unsupervised_2009,
  title = {Unsupervised {Learning} of {Visual} {Sense} {Models} for {Polysemous} {Words}},
  volume = {21},
  url = {https://proceedings.neurips.cc/paper/2008/hash/e70611883d2760c8bbafb4acb29e3446-Abstract.html},
  urldate = {2021-11-08},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author = {Saenko, Kate and Darrell, Trevor},
  year = {2009},
  keywords = {ambiguity, vision-language},
}

@article{higgins_beta-vae_2016,
  title = {beta-{VAE}: {Learning} {Basic} {Visual} {Concepts} with a {Constrained} {Variational} {Framework}},
  shorttitle = {beta-{VAE}},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  abstract = {We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner.},
  language = {en},
  urldate = {2021-09-30},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  month = nov,
  year = {2016},
  keywords = {vae},
}

@article{goh_multimodal_2021-1,
  title = {Multimodal {Neurons} in {Artificial} {Neural} {Networks}},
  volume = {6},
  issn = {2476-0757},
  url = {https://distill.pub/2021/multimodal-neurons},
  doi = {10.23915/distill.00030},
  abstract = {We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.},
  language = {en},
  number = {3},
  urldate = {2021-09-19},
  journal = {Distill},
  author = {Goh, Gabriel and †, Nick Cammarata and †, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  month = mar,
  year = {2021},
  keywords = {clip, interpretable, multimodal, vision-language},
  pages = {e30},
}

@article{zhou_learning_2021,
  title = {Learning to {Prompt} for {Vision}-{Language} {Models}},
  url = {http://arxiv.org/abs/2109.01134},
  abstract = {Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named context optimization (CoOp). The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17\% with the highest reaching over 50\%). CoOp also exhibits strong robustness to distribution shift.},
  urldate = {2021-09-03},
  journal = {arXiv:2109.01134 [cs]},
  author = {Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  month = sep,
  year = {2021},
  note = {arXiv: 2109.01134},
  keywords = {nlp, prompt, vision-language},
}

@article{liu_pre-train_2021,
  title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
  shorttitle = {Pre-train, {Prompt}, and {Predict}},
  url = {http://arxiv.org/abs/2107.13586},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  urldate = {2021-08-27},
  journal = {arXiv:2107.13586 [cs]},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  month = jul,
  year = {2021},
  note = {arXiv: 2107.13586},
  keywords = {gpt, language\_models, nlp},
}

@misc{noauthor_amazon_nodate,
  title = {Amazon scraps secret {AI} recruiting tool that showed bias against women {\textbar} {Reuters}},
  url = {https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G},
  urldate = {2021-05-28},
}

@article{ha_world_2018,
  title = {World {Models}},
  url = {http://arxiv.org/abs/1803.10122},
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  urldate = {2021-05-28},
  journal = {arXiv:1803.10122 [cs, stat]},
  author = {Ha, David and Schmidhuber, Jürgen},
  month = mar,
  year = {2018},
  note = {arXiv: 1803.10122},
  keywords = {rl, world\_models},
}

@article{oord_neural_2018,
  title = {Neural {Discrete} {Representation} {Learning}},
  url = {http://arxiv.org/abs/1711.00937},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  urldate = {2021-05-28},
  journal = {arXiv:1711.00937 [cs]},
  author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
  month = may,
  year = {2018},
  note = {arXiv: 1711.00937},
  keywords = {vae},
}

@misc{noauthor_automatic_nodate,
  title = {Automatic {Chemical} {Design} {Using} a {Data}-{Driven} {Continuous} {Representation} of {Molecules} {\textbar} {ACS} {Central} {Science}},
  url = {https://pubs.acs.org/doi/10.1021/acscentsci.7b00572},
  urldate = {2021-05-26},
  keywords = {application, drug\_discovery},
}

@article{oord_conditional_2016,
  title = {Conditional {Image} {Generation} with {PixelCNN} {Decoders}},
  url = {http://arxiv.org/abs/1606.05328},
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  urldate = {2021-05-23},
  journal = {arXiv:1606.05328 [cs]},
  author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  month = jun,
  year = {2016},
  note = {arXiv: 1606.05328},
  keywords = {autoregressive},
}

@article{he_deep_2015,
  title = {Deep {Residual} {Learning} for {Image} {Recognition}},
  url = {http://arxiv.org/abs/1512.03385},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate = {2021-05-23},
  journal = {arXiv:1512.03385 [cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = dec,
  year = {2015},
  note = {arXiv: 1512.03385},
}

@article{lake_human-level_2015,
  title = {Human-level concept learning through probabilistic program induction},
  doi = {10.1126/science.aab3050},
  abstract = {Handwritten characters drawn by a model Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.},
  journal = {Science},
  author = {Lake, B. and Salakhutdinov, R. and Tenenbaum, J.},
  year = {2015},
  keywords = {dataset},
}

@article{zhang_character-level_nodate,
  title = {Character-level {Convolutional} {Networks} for {Text} {Classiﬁcation}},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classiﬁcation. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  language = {en},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  keywords = {text},
  pages = {9},
}

@misc{facco_estimating_2017,
  title = {Estimating the intrinsic dimension of datasets by a minimal neighborhood information {\textbar} {Scientific} {Reports}},
  url = {https://www.nature.com/articles/s41598-017-11873-y},
  urldate = {2021-04-23},
  author = {Facco, Elena and d'Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
  year = {2017},
  keywords = {diff\_geo},
}

@article{huh_what_2016,
  title = {What makes {ImageNet} good for transfer learning?},
  url = {http://arxiv.org/abs/1608.08614},
  abstract = {The tremendous success of ImageNet-trained deep features on a wide range of transfer tasks begs the question: what are the properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class? To answer these and related questions, we pre-trained CNN features on various subsets of the ImageNet dataset and evaluated transfer performance on PASCAL detection, PASCAL action classification, and SUN scene classification tasks. Our overall findings suggest that most changes in the choice of pre-training data long thought to be critical do not significantly affect transfer performance.? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?},
  urldate = {2021-04-23},
  journal = {arXiv:1608.08614 [cs]},
  author = {Huh, Minyoung and Agrawal, Pulkit and Efros, Alexei A.},
  month = dec,
  year = {2016},
  note = {arXiv: 1608.08614},
  keywords = {imagenet, transfer},
}

@article{ansuini_intrinsic_2019,
  title = {Intrinsic dimension of data representations in deep neural networks},
  url = {http://arxiv.org/abs/1905.12784},
  abstract = {Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data-representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.},
  urldate = {2021-04-22},
  journal = {arXiv:1905.12784 [cs, stat]},
  author = {Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H. and Zoccolan, Davide},
  month = oct,
  year = {2019},
  note = {arXiv: 1905.12784},
  keywords = {diff\_geo},
}

@article{harvey_image_2021,
  title = {Image {Completion} via {Inference} in {Deep} {Generative} {Models}},
  url = {http://arxiv.org/abs/2102.12037},
  abstract = {We consider image completion from the perspective of amortized inference in an image generative model. We leverage recent state of the art variational auto-encoder architectures that have been shown to produce photo-realistic natural images at non-trivial resolutions. Through amortized inference in such a model we can train neural artifacts that produce diverse, realistic image completions even when the vast majority of an image is missing. We demonstrate superior sample quality and diversity compared to prior art on the CIFAR-10 and FFHQ-256 datasets. We conclude by describing and demonstrating an application that requires an in-painting model with the capabilities ours exhibits: the use of Bayesian optimal experimental design to select the most informative sequence of small field of view x-rays for chest pathology detection.},
  urldate = {2021-03-01},
  journal = {arXiv:2102.12037 [cs]},
  author = {Harvey, William and Naderiparizi, Saeid and Wood, Frank},
  month = feb,
  year = {2021},
  note = {arXiv: 2102.12037},
  keywords = {experimental\_design, inference, vae},
}

@article{zimmermann_contrastive_2021,
  title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
  url = {http://arxiv.org/abs/2102.08850},
  abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
  urldate = {2021-03-01},
  journal = {arXiv:2102.08850 [cs]},
  author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  month = feb,
  year = {2021},
  note = {arXiv: 2102.08850},
  keywords = {contrastive, probabilistic},
}

@article{esser_taming_2021,
  title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
  url = {http://arxiv.org/abs/2012.09841},
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://compvis.github.io/taming-transformers/ .},
  urldate = {2021-03-01},
  journal = {arXiv:2012.09841 [cs]},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
  month = feb,
  year = {2021},
  note = {arXiv: 2012.09841},
  keywords = {gans, transformers},
}

@article{liu_simple_2020,
  title = {Simple and {Principled} {Uncertainty} {Estimation} with {Deterministic} {Deep} {Learning} via {Distance} {Awareness}},
  url = {http://arxiv.org/abs/2006.10108},
  abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
  urldate = {2021-03-01},
  journal = {arXiv:2006.10108 [cs, stat]},
  author = {Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax-Weiss, Tania and Lakshminarayanan, Balaji},
  month = oct,
  year = {2020},
  note = {arXiv: 2006.10108},
  keywords = {gaussian\_processes, uncertainty},
}

@article{lan_perfect_2021,
  title = {Perfect density models cannot guarantee anomaly detection},
  url = {http://arxiv.org/abs/2012.03808},
  abstract = {Thanks to the tractability of their likelihood, some deep generative models show promise for seemingly straightforward but important applications like anomaly detection, uncertainty estimation, and active learning. However, the likelihood values empirically attributed to anomalies conflict with the expectations these proposed applications suggest. In this paper, we take a closer look at the behavior of distribution densities and show that these quantities carry less meaningful information than previously thought, beyond estimation issues or the curse of dimensionality. We conclude that the use of these likelihoods for out-of-distribution detection relies on strong and implicit hypotheses, and highlight the necessity of explicitly formulating these assumptions for reliable anomaly detection.},
  urldate = {2021-03-01},
  journal = {arXiv:2012.03808 [cs, stat]},
  author = {Lan, Charline Le and Dinh, Laurent},
  month = feb,
  year = {2021},
  note = {arXiv: 2012.03808},
}

@article{lan_perfect_2021-1,
  title = {Perfect density models cannot guarantee anomaly detection},
  url = {http://arxiv.org/abs/2012.03808},
  abstract = {Thanks to the tractability of their likelihood, some deep generative models show promise for seemingly straightforward but important applications like anomaly detection, uncertainty estimation, and active learning. However, the likelihood values empirically attributed to anomalies conflict with the expectations these proposed applications suggest. In this paper, we take a closer look at the behavior of distribution densities and show that these quantities carry less meaningful information than previously thought, beyond estimation issues or the curse of dimensionality. We conclude that the use of these likelihoods for out-of-distribution detection relies on strong and implicit hypotheses, and highlight the necessity of explicitly formulating these assumptions for reliable anomaly detection.},
  urldate = {2021-02-26},
  journal = {arXiv:2012.03808 [cs, stat]},
  author = {Lan, Charline Le and Dinh, Laurent},
  month = feb,
  year = {2021},
  note = {arXiv: 2012.03808},
  keywords = {flows, probabilistic},
}

@article{antoran_getting_2020,
  title = {Getting a {CLUE}: {A} {Method} for {Explaining} {Uncertainty} {Estimates}},
  shorttitle = {Getting a {CLUE}},
  url = {http://arxiv.org/abs/2006.06848},
  abstract = {Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.},
  urldate = {2021-02-03},
  journal = {arXiv:2006.06848 [cs, stat]},
  author = {Antorán, Javier and Bhatt, Umang and Adel, Tameem and Weller, Adrian and Hernández-Lobato, José Miguel},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.06848},
  keywords = {bnn, causality, uncertainty},
}

@article{tran_cauchy-schwarz_2021,
  title = {Cauchy-{Schwarz} {Regularized} {Autoencoder}},
  url = {http://arxiv.org/abs/2101.02149},
  abstract = {Recent work in unsupervised learning has focused on efficient inference and learning in latent variables models. Training these models by maximizing the evidence (marginal likelihood) is typically intractable. Thus, a common approximation is to maximize the Evidence Lower BOund (ELBO) instead. Variational autoencoders (VAE) are a powerful and widely-used class of generative models that optimize the ELBO efficiently for large datasets. However, the VAE's default Gaussian choice for the prior imposes a strong constraint on its ability to represent the true posterior, thereby degrading overall performance. A Gaussian mixture model (GMM) would be a richer prior, but cannot be handled efficiently within the VAE framework because of the intractability of the Kullback-Leibler divergence for GMMs. We challenge the adoption of the VAE framework on this specific point in favor of one with an analytical solution for Gaussian mixture prior. To perform efficient inference for GMM priors, we introduce a new constrained objective based on the Cauchy-Schwarz divergence, which can be computed analytically for GMMs. This new objective allows us to incorporate richer, multi-modal priors into the auto-encoding framework.We provide empirical studies on a range of datasets and show that our objective improves upon variational auto-encoding models in density estimation, unsupervised clustering, semi-supervised learning, and face analysis.},
  urldate = {2021-02-03},
  journal = {arXiv:2101.02149 [cs]},
  author = {Tran, Linh and Pantic, Maja and Deisenroth, Marc Peter},
  month = jan,
  year = {2021},
  note = {arXiv: 2101.02149},
  keywords = {vae},
}

@article{chalupka_causal_2017,
  title = {Causal feature learning: an overview},
  volume = {44},
  issn = {0385-7417, 1349-6964},
  shorttitle = {Causal feature learning},
  url = {http://link.springer.com/10.1007/s41237-016-0008-2},
  doi = {10.1007/s41237-016-0008-2},
  abstract = {Causal feature learning (CFL) (Chalupka et al., Proceedings of the Thirty-First Conference on Uncertainty in Artiﬁcial Intelligence. AUAI Press, Edinburgh, pp 181–190, 2015) is a causal inference framework rooted in the language of causal graphical models (Pearl J, Reasoning and inference. Cambridge University Press, Cambridge, 2009; Spirtes et al., Causation, Prediction, and Search. Massachusetts Institute of Technology, Massachusetts, 2000), and computational mechanics (Shalizi, PhD thesis, University of Wisconsin at Madison, 2001). CFL is aimed at discovering high-level causal relations from low-level data, and at reducing the experimental effort to understand confounding among the high-level variables. We ﬁrst review the scientiﬁc motivation for CFL, then present a detailed introduction to the framework, laying out the deﬁnitions and algorithmic steps. A simple example illustrates the techniques involved in the learning steps and provides visual intuition. Finally, we discuss the limitations of the current framework and list a number of open problems.},
  language = {en},
  number = {1},
  urldate = {2021-02-03},
  journal = {Behaviormetrika},
  author = {Chalupka, Krzysztof and Eberhardt, Frederick and Perona, Pietro},
  month = jan,
  year = {2017},
  pages = {137--164},
}

@inproceedings{kaushik_learning_2019,
  title = {Learning {The} {Difference} {That} {Makes} {A} {Difference} {With} {Counterfactually}-{Augmented} {Data}},
  url = {https://openreview.net/forum?id=Sklgs0NFvr},
  abstract = {Humans in the loop revise documents to accord with counterfactual labels, resulting resource helps to reduce reliance on spurious associations.},
  language = {en},
  urldate = {2021-02-03},
  author = {Kaushik, Divyansh and Hovy, Eduard and Lipton, Zachary},
  month = sep,
  year = {2019},
  keywords = {causality, counterfactual, dataset, text},
}

@article{jiang_robust_2020,
  title = {Robust {Pre}-{Training} by {Adversarial} {Contrastive} {Learning}},
  url = {http://arxiv.org/abs/2010.13337},
  abstract = {Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework, which learns representations by maximizing feature consistency under differently augmented views. This fits particularly well with the goal of adversarial robustness, as one cause of adversarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efficient and robust. We empirically evaluate the proposed Adversarial Contrastive Learning (ACL) and show it can consistently outperform existing methods. For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach by 2.99\% on robust accuracy and 2.14\% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled examples are available. Our codes and pre-trained models have been released at: https://github.com/VITA-Group/Adversarial-Contrastive-Learning.},
  urldate = {2021-01-26},
  journal = {arXiv:2010.13337 [cs]},
  author = {Jiang, Ziyu and Chen, Tianlong and Chen, Ting and Wang, Zhangyang},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.13337},
  keywords = {adversarial, contrastive},
}

@article{wang_implicit_2020,
  title = {Implicit {Semantic} {Data} {Augmentation} for {Deep} {Networks}},
  url = {http://arxiv.org/abs/1909.12220},
  abstract = {In this paper, we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping, translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds. As a consequence, translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently, we first perform an online estimate of the covariance matrix of deep features for each class, which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly, instead of augmenting the samples explicitly, we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set, leading to a highly efficient algorithm. In fact, we show that the proposed ISDA amounts to minimizing a novel robust CE loss, which adds negligible extra computational cost to a normal training procedure. Although being simple, ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet. Code for reproducing our results is available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.},
  urldate = {2021-01-26},
  journal = {arXiv:1909.12220 [cs, stat]},
  author = {Wang, Yulin and Pan, Xuran and Song, Shiji and Zhang, Hong and Wu, Cheng and Huang, Gao},
  month = apr,
  year = {2020},
  note = {arXiv: 1909.12220
version: 4},
  keywords = {data\_augmentation},
}

@article{wang_geometry_2021,
  title = {The {Geometry} of {Deep} {Generative} {Image} {Models} and its {Applications}},
  url = {http://arxiv.org/abs/2101.06006},
  abstract = {Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs.},
  urldate = {2021-01-26},
  journal = {arXiv:2101.06006 [cs, math]},
  author = {Wang, Binxu and Ponce, Carlos R.},
  month = jan,
  year = {2021},
  note = {arXiv: 2101.06006},
  keywords = {gan, geometry, steering},
}

@article{allen-zhu_towards_2020,
  title = {Towards {Understanding} {Ensemble}, {Knowledge} {Distillation} and {Self}-{Distillation} in {Deep} {Learning}},
  url = {http://arxiv.org/abs/2012.09816},
  abstract = {We formally study how Ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using Knowledge Distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the SAME architecture, trained using the SAME algorithm on the SAME data set, and they only differ by the random seeds used in the initialization. We empirically show that ensemble/knowledge distillation in deep learning works very differently from traditional learning theory, especially differently from ensemble of random feature mappings or the neural-tangent-kernel feature mappings, and is potentially out of the scope of existing theorems. Thus, to properly understand ensemble and knowledge distillation in deep learning, we develop a theory showing that when data has a structure we refer to as "multi-view", then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model by training a single model to match the output of the ensemble instead of the true label. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the "dark knowledge" is hidden in the outputs of the ensemble -- that can be used in knowledge distillation -- comparing to the true data labels. In the end, we prove that self-distillation can also be viewed as implicitly combining ensemble and knowledge distillation to improve test accuracy.},
  urldate = {2021-01-26},
  journal = {arXiv:2012.09816 [cs, math, stat]},
  author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.09816},
  keywords = {distillation, ensemble},
}

@article{holtzman_curious_2020,
  title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
  url = {http://arxiv.org/abs/1904.09751},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  urldate = {2021-01-26},
  journal = {arXiv:1904.09751 [cs]},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  month = feb,
  year = {2020},
  note = {arXiv: 1904.09751},
  keywords = {nlp, typicality},
}

@article{yang_causalvae_2020,
  title = {{CausalVAE}: {Disentangled} {Representation} {Learning} via {Neural} {Structural} {Causal} {Models}},
  shorttitle = {{CausalVAE}},
  abstract = {Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through "do-operation" to the causal factors.},
  author = {Yang, M. and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, J. and Wang, Jijie},
  year = {2020},
  keywords = {causality, vae},
}

@article{besserve_counterfactuals_2019,
  title = {Counterfactuals uncover the modular structure of deep generative models},
  url = {http://arxiv.org/abs/1812.03253},
  abstract = {Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations to uncover a modular structure of the network composed of disentangled groups of internal variables. Experiments with a variety of generative models trained on complex image datasets show the obtained modules can be used to design targeted interventions. This opens the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.},
  urldate = {2021-01-23},
  journal = {arXiv:1812.03253 [cs, stat]},
  author = {Besserve, Michel and Mehrjou, Arash and Sun, Rémy and Schölkopf, Bernhard},
  month = dec,
  year = {2019},
  note = {arXiv: 1812.03253},
  keywords = {causality, gan, steering},
}

@article{zhao_discretized_2020,
  title = {Discretized {Bottleneck} in {VAE}: {Posterior}-{Collapse}-{Free} {Sequence}-to-{Sequence} {Learning}},
  shorttitle = {Discretized {Bottleneck} in {VAE}},
  url = {http://arxiv.org/abs/2004.10603},
  abstract = {Variational autoencoders (VAEs) are important tools in end-to-end representation learning. VAEs can capture complex data distributions and have been applied extensively in many natural-language-processing (NLP) tasks. However, a common pitfall in sequence-to-sequence learning with VAEs is the posterior-collapse issue in latent space, wherein the model tends to ignore latent variables when a strong auto-regressive decoder is implemented. In this paper, we propose a principled approach to eliminate this issue by applying a discretized bottleneck in the latent space. Specifically, we impose a shared discrete latent space where each input is learned to choose a combination of shared latent atoms as its latent representation. Compared with VAEs employing continuous latent variables, our model endows more promising capability in modeling underlying semantics of discrete sequences and can thus provide more interpretative latent structures. Empirically, we demonstrate the efficiency and effectiveness of our model on a broad range of tasks, including language modeling, unaligned text style transfer, dialog response generation, and neural machine translation.},
  urldate = {2021-01-21},
  journal = {arXiv:2004.10603 [cs, stat]},
  author = {Zhao, Yang and Yu, Ping and Mahapatra, Suchismit and Su, Qinliang and Chen, Changyou},
  month = apr,
  year = {2020},
  note = {arXiv: 2004.10603},
  keywords = {discrete, posterior\_collapse, vae},
}

@article{love_topological_2021,
  title = {Topological {Deep} {Learning}},
  url = {http://arxiv.org/abs/2101.05778},
  abstract = {This work introduces the Topological CNN (TCNN), which encompasses several topologically defined convolutional methods. Manifolds with important relationships to the natural image space are used to parameterize image filters which are used as convolutional weights in a TCNN. These manifolds also parameterize slices in layers of a TCNN across which the weights are localized. We show evidence that TCNNs learn faster, on less data, with fewer learned parameters, and with greater generalizability and interpretability than conventional CNNs. We introduce and explore TCNN layers for both image and video data. We propose extensions to 3D images and 3D video.},
  urldate = {2021-01-19},
  journal = {arXiv:2101.05778 [cs]},
  author = {Love, Ephy R. and Filippenko, Benjamin and Maroulas, Vasileios and Carlsson, Gunnar},
  month = jan,
  year = {2021},
  note = {arXiv: 2101.05778},
  keywords = {tda},
}

@article{goel_model_2020,
  title = {Model {Patching}: {Closing} the {Subgroup} {Performance} {Gap} with {Data} {Augmentation}},
  shorttitle = {Model {Patching}},
  url = {http://arxiv.org/abs/2008.06775},
  abstract = {Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL's effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33\% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.},
  urldate = {2021-01-19},
  journal = {arXiv:2008.06775 [cs, stat]},
  author = {Goel, Karan and Gu, Albert and Li, Yixuan and Ré, Christopher},
  month = aug,
  year = {2020},
  note = {arXiv: 2008.06775},
  keywords = {data\_augmentation, fairness, gan, invariance},
}

@article{tian_contrastive_2020,
  title = {Contrastive {Representation} {Distillation}},
  url = {http://arxiv.org/abs/1910.10699},
  abstract = {Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: http://github.com/HobbitLong/RepDistiller.},
  urldate = {2021-01-18},
  journal = {arXiv:1910.10699 [cs, stat]},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  month = jan,
  year = {2020},
  note = {arXiv: 1910.10699},
  keywords = {contrastive, knowledge\_distillation},
}

@article{zhang_image_2020,
  title = {Image {GANs} meet {Differentiable} {Rendering} for {Inverse} {Graphics} and {Interpretable} {3D} {Neural} {Rendering}},
  url = {http://arxiv.org/abs/2010.09125},
  abstract = {Differentiable rendering has paved the way to training neural networks to perform "inverse graphics" tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice. Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D "neural renderer", complementing traditional graphics renderers.},
  urldate = {2021-01-18},
  journal = {arXiv:2010.09125 [cs]},
  author = {Zhang, Yuxuan and Chen, Wenzheng and Ling, Huan and Gao, Jun and Zhang, Yinan and Torralba, Antonio and Fidler, Sanja},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.09125},
  keywords = {gan},
}

@article{song_how_2021,
  title = {How to {Train} {Your} {Energy}-{Based} {Models}},
  url = {http://arxiv.org/abs/2101.03288},
  abstract = {Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.},
  urldate = {2021-01-18},
  journal = {arXiv:2101.03288 [cs, stat]},
  author = {Song, Yang and Kingma, Diederik P.},
  month = jan,
  year = {2021},
  note = {arXiv: 2101.03288},
  keywords = {ebm},
}

@article{ma_decoupling_2020,
  title = {Decoupling {Global} and {Local} {Representations} from/for {Image} {Generation}},
  url = {http://arxiv.org/abs/2004.11820},
  abstract = {In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting. The proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a plain log-likelihood objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.},
  urldate = {2021-01-18},
  journal = {arXiv:2004.11820 [cs, eess, stat]},
  author = {Ma, Xuezhe and Kong, Xiang and Zhang, Shanghang and Hovy, Eduard},
  month = apr,
  year = {2020},
  note = {arXiv: 2004.11820},
  keywords = {flows, vae},
}

@article{jiang_scalor_2020,
  title = {{SCALOR}: {Generative} {World} {Models} with {Scalable} {Object} {Representations}},
  shorttitle = {{SCALOR}},
  url = {http://arxiv.org/abs/1910.02384},
  abstract = {Scalability in terms of object density in a scene is a primary challenge in unsupervised sequential object-oriented representation learning. Most of the previous models have been shown to work only on scenes with a few objects. In this paper, we propose SCALOR, a probabilistic generative world model for learning SCALable Object-oriented Representation of a video. With the proposed spatially-parallel attention and proposal-rejection mechanisms, SCALOR can deal with orders of magnitude larger numbers of objects compared to the previous state-of-the-art models. Additionally, we introduce a background module that allows SCALOR to model complex dynamic backgrounds as well as many foreground objects in the scene. We demonstrate that SCALOR can deal with crowded scenes containing up to a hundred objects while jointly modeling complex dynamic backgrounds. Importantly, SCALOR is the first unsupervised object representation model shown to work for natural scenes containing several tens of moving objects.},
  urldate = {2021-01-18},
  journal = {arXiv:1910.02384 [cs, stat]},
  author = {Jiang, Jindong and Janghorbani, Sepehr and de Melo, Gerard and Ahn, Sungjin},
  month = mar,
  year = {2020},
  note = {arXiv: 1910.02384},
  keywords = {object-centric},
}

@article{xiao_vaebm_2020,
  title = {{VAEBM}: {A} {Symbiosis} between {Variational} {Autoencoders} and {Energy}-based {Models}},
  shorttitle = {{VAEBM}},
  url = {http://arxiv.org/abs/2010.00654},
  abstract = {Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256\${\textbackslash}times\$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection.},
  urldate = {2021-01-15},
  journal = {arXiv:2010.00654 [cs, stat]},
  author = {Xiao, Zhisheng and Kreis, Karsten and Kautz, Jan and Vahdat, Arash},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.00654},
  keywords = {ebm, mcmc, vae},
}

@article{snell_bayesian_2020,
  title = {Bayesian {Few}-{Shot} {Classification} with {One}-vs-{Each} {P}{\textbackslash}'olya-{Gamma} {Augmented} {Gaussian} {Processes}},
  url = {http://arxiv.org/abs/2007.10417},
  abstract = {Few-shot classification (FSC), the task of adapting a classifier to unseen classes given a small labeled dataset, is an important step on the path toward human-like machine learning. Bayesian methods are well-suited to tackling the fundamental issue of overfitting in the few-shot scenario because they allow practitioners to specify prior beliefs and update those beliefs in light of observed data. Contemporary approaches to Bayesian few-shot classification maintain a posterior distribution over model parameters, which is slow and requires storage that scales with model size. Instead, we propose a Gaussian process classifier based on a novel combination of P{\textbackslash}'olya-gamma augmentation and the one-vs-each softmax approximation that allows us to efficiently marginalize over functions rather than model parameters. We demonstrate improved accuracy and uncertainty quantification on both standard few-shot classification benchmarks and few-shot domain transfer tasks.},
  urldate = {2021-01-14},
  journal = {arXiv:2007.10417 [cs, stat]},
  author = {Snell, Jake and Zemel, Richard},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.10417},
  keywords = {few-shot, gaussian\_process, inference, softmax},
}

@article{paulus_rao-blackwellizing_2020,
  title = {Rao-{Blackwellizing} the {Straight}-{Through} {Gumbel}-{Softmax} {Gradient} {Estimator}},
  url = {http://arxiv.org/abs/2010.04838},
  abstract = {Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.},
  urldate = {2021-01-14},
  journal = {arXiv:2010.04838 [cs, stat]},
  author = {Paulus, Max B. and Maddison, Chris J. and Krause, Andreas},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.04838},
  keywords = {gumbel-softmax, variance\_reduction},
}

@inproceedings{anonymous_is_2020,
  title = {Is {Attention} {Better} {Than} {Matrix} {Decomposition}?},
  url = {https://openreview.net/forum?id=1FvkSpWosOl},
  abstract = {As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention...},
  language = {en},
  urldate = {2021-01-14},
  author = {Anonymous},
  month = sep,
  year = {2020},
  keywords = {attention, nmf},
}

@article{wang_fully_2020,
  title = {Fully {Test}-time {Adaptation} by {Entropy} {Minimization}},
  url = {http://arxiv.org/abs/2006.10726},
  abstract = {A model must adapt itself to generalize to new and different data during testing. This is the setting of fully test-time adaptation given only unlabeled test data and the model parameters. We propose test-time entropy minimization (tent) for adaptation: we optimize for model confidence as measured by the entropy of its predictions. During testing, we adapt the model features by estimating normalization statistics and optimizing channel-wise affine transformations. Tent improves robustness to corruptions for image classification on ImageNet and CIFAR-10/100 and achieves state-of-the-art error on ImageNet-C for ResNet-50. Tent demonstrates the feasibility of target-only domain adaptation for digit classification from SVHN to MNIST/MNIST-M/USPS and semantic segmentation from GTA to Cityscapes.},
  urldate = {2021-01-14},
  journal = {arXiv:2006.10726 [cs, stat]},
  author = {Wang, Dequan and Shelhamer, Evan and Liu, Shaoteng and Olshausen, Bruno and Darrell, Trevor},
  month = nov,
  year = {2020},
  note = {arXiv: 2006.10726},
  keywords = {domain\_adaptation, entropy, tta},
}

@article{child_very_2020,
  title = {Very {Deep} {VAEs} {Generalize} {Autoregressive} {Models} and {Can} {Outperform} {Them} on {Images}},
  url = {http://arxiv.org/abs/2011.10650},
  abstract = {We present a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that VAEs can actually implement autoregressive models, and other, more efficient generative models, if made sufficiently deep. Despite this, autoregressive models have traditionally outperformed VAEs. We test if insufficient depth explains the performance gap by by scaling a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. We visualize the generative process and show the VAEs learn efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  urldate = {2021-01-13},
  journal = {arXiv:2011.10650 [cs]},
  author = {Child, Rewon},
  month = nov,
  year = {2020},
  note = {arXiv: 2011.10650},
  keywords = {autoregressive, vae},
}

@inproceedings{anonymous_rethinking_2020,
  title = {Rethinking the {Role} of {Gradient}-based {Attribution} {Methods} for {Model} {Interpretability}},
  url = {https://openreview.net/forum?id=dYeAHXnpWJ4},
  abstract = {Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common...},
  language = {en},
  urldate = {2021-01-13},
  author = {Anonymous},
  month = sep,
  year = {2020},
  keywords = {ebms, gradient, interpretability},
}

@article{zhao_dataset_2020,
  title = {Dataset {Condensation} with {Gradient} {Matching}},
  url = {http://arxiv.org/abs/2006.05929},
  abstract = {As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing them and training models on them becomes more expensive. This paper proposes a training set synthesis technique for {\textbackslash}emph\{data-efficient\} learning, called {\textbackslash}emph\{Dataset Condensation\}, that learns to condense a large dataset into a small set of informative samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of a deep neural network trained on the original data and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and show that it achieves promising gains on a tight budget of memory and computations.},
  urldate = {2021-01-13},
  journal = {arXiv:2006.05929 [cs]},
  author = {Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  month = oct,
  year = {2020},
  note = {arXiv: 2006.05929},
  keywords = {distillation, gradients},
}

@article{zhang_learning_2020,
  title = {Learning {Invariant} {Representations} for {Reinforcement} {Learning} without {Reconstruction}},
  url = {http://arxiv.org/abs/2006.10742},
  abstract = {We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.},
  urldate = {2021-01-13},
  journal = {arXiv:2006.10742 [cs, stat]},
  author = {Zhang, Amy and McAllister, Rowan and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.10742},
  keywords = {contrastive, invariance, rl},
}

@article{melis_mutual_2020,
  title = {Mutual {Information} {Constraints} for {Monte}-{Carlo} {Objectives}},
  url = {http://arxiv.org/abs/2012.00708},
  abstract = {A common failure mode of density models trained as variational autoencoders is to model the data without relying on their latent variables, rendering these variables useless. Two contributing factors, the underspecification of the model and the looseness of the variational lower bound, have been studied separately in the literature. We weave these two strands of research together, specifically the tighter bounds of Monte-Carlo objectives and constraints on the mutual information between the observable and the latent variables. Estimating the mutual information as the average Kullback-Leibler divergence between the easily available variational posterior \$q(z{\textbar}x)\$ and the prior does not work with Monte-Carlo objectives because \$q(z{\textbar}x)\$ is no longer a direct approximation to the model's true posterior \$p(z{\textbar}x)\$. Hence, we construct estimators of the Kullback-Leibler divergence of the true posterior from the prior by recycling samples used in the objective, with which we train models of continuous and discrete latents at much improved rate-distortion and no posterior collapse. While alleviated, the tradeoff between modelling the data and using the latents still remains, and we urge for evaluating inference methods across a range of mutual information values.},
  urldate = {2021-01-13},
  journal = {arXiv:2012.00708 [cs, stat]},
  author = {Melis, Gábor and György, András and Blunsom, Phil},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.00708},
  keywords = {monte\_carlo, mutual\_information, posterior\_collapse, vae},
}

@article{razeghi_discriminative_2019,
  title = {Discriminative {Dimension} {Reduction} based on {Mutual} {Information}},
  url = {http://arxiv.org/abs/1912.05631},
  abstract = {The "curse of dimensionality" is a well-known problem in pattern recognition. A widely used approach to tackling the problem is a group of subspace methods, where the original features are projected onto a new space. The lower dimensional subspace is then used to approximate the original features for classification. However, most subspace methods were not originally developed for classification. We believe that direct adoption of these subspace methods for pattern classification should not be considered best practice. In this paper, we present a new information theory based algorithm for selecting subspaces, which can always result in superior performance over conventional methods. This paper makes the following main contributions: i) it improves a common practice widely used by practitioners in the field of pattern recognition, ii) it develops an information theory based technique for systematically selecting the subspaces that are discriminative and therefore are suitable for pattern recognition/classification purposes, iii) it presents extensive experimental results on a variety of computer vision and pattern recognition tasks to illustrate that the subspaces selected based on maximum mutual information criterion will always enhance performance regardless of the classification techniques used.},
  urldate = {2021-01-13},
  journal = {arXiv:1912.05631 [cs]},
  author = {Razeghi, Orod and Qiu, Guoping},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.05631},
  keywords = {dimension\_reduction, mutual\_information},
}

@inproceedings{sgouritsa_inference_2015,
  title = {Inference of {Cause} and {Effect} with {Unsupervised} {Inverse} {Regression}},
  url = {http://proceedings.mlr.press/v38/sgouritsa15.html},
  abstract = {We address the problem of causal discovery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X -{\textgreater} Y (X causes Y), th...},
  language = {en},
  urldate = {2021-01-11},
  booktitle = {Artificial {Intelligence} and {Statistics}},
  publisher = {PMLR},
  author = {Sgouritsa, Eleni and Janzing, Dominik and Hennig, Philipp and Schölkopf, Bernhard},
  month = feb,
  year = {2015},
  note = {ISSN: 1938-7228},
  keywords = {causality, semisupervised},
  pages = {847--855},
}

@article{yang_generative_nodate,
  title = {{GENERATIVE} {MAX}-{MAHALANOBIS} {CLASSIFIERS} {FOR} {IMAGE} {CLASSIFICATION}, {GENERATION} {AND} {MORE}},
  abstract = {Joint Energy-based Model (JEM) of Grathwohl et al. (2020a) shows that a standard softmax classiﬁer can be reinterpreted as an energy-based model (EBM) for the joint distribution p(x, y); the resulting model can be optimized with an energybased training to improve calibration, robustness and out-of-distribution detection, while generating samples rivaling the quality of recent GAN-based approaches. However, the softmax classiﬁer that JEM exploits is inherently discriminative and its latent feature space is not well formulated as probabilistic distributions, which may hinder its potential for image generation and incur training instability as observed in Grathwohl et al. (2020a). We hypothesize that generative classiﬁers, such as Linear Discriminant Analysis (LDA), might be more suitable hybrid models for image generation since generative classiﬁers model the data generation process explicitly. This paper therefore investigates an LDA classiﬁer for image classiﬁcation and generation. In particular, the Max-Mahalanobis Classiﬁer (MMC) Pang et al. (2020), a special case of LDA, ﬁts our goal very well since MMC formulates the latent feature space explicitly as the Max-Mahalanobis distribution Pang et al. (2018). We term our algorithm Generative MMC (GMMC), and show that it can be trained discriminatively, generatively or jointly for image classiﬁcation and generation. Extensive experiments on multiple datasets (CIFAR10, CIFAR100 and SVHN) show that GMMC achieves state-of-the-art discriminative and generative performances, while outperforming JEM in calibration, adversarial robustness and out-of-distribution detection by a signiﬁcant margin.},
  language = {en},
  author = {Yang, Xiulong and Ye, Hui and Ye, Yang and Li, Xiang and Ji, Shihao},
  pages = {19},
}

@article{kumar_maximum_2019,
  title = {Maximum {Entropy} {Generators} for {Energy}-{Based} {Models}},
  url = {http://arxiv.org/abs/1901.08508},
  abstract = {Maximum likelihood estimation of energy-based models is a challenging problem due to the intractability of the log-likelihood gradient. In this work, we propose learning both the energy function and an amortized approximate sampling mechanism using a neural generator network, which provides an efficient approximation of the log-likelihood gradient. The resulting objective requires maximizing entropy of the generated samples, which we perform using recently proposed nonparametric mutual information estimators. Finally, to stabilize the resulting adversarial game, we use a zero-centered gradient penalty derived as a necessary condition from the score matching literature. The proposed technique can generate sharp images with Inception and FID scores competitive with recent GAN techniques, does not suffer from mode collapse, and is competitive with state-of-the-art anomaly detection techniques.},
  urldate = {2021-01-09},
  journal = {arXiv:1901.08508 [cs, stat]},
  author = {Kumar, Rithesh and Ozair, Sherjil and Goyal, Anirudh and Courville, Aaron and Bengio, Yoshua},
  month = may,
  year = {2019},
  note = {arXiv: 1901.08508},
  keywords = {ebms, entropy},
}

@article{mitrovic_representation_2020,
  title = {Representation {Learning} via {Invariant} {Causal} {Mechanisms}},
  url = {http://arxiv.org/abs/2010.07922},
  abstract = {Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on \$51\$ out of \$57\$ games.},
  urldate = {2021-01-01},
  journal = {arXiv:2010.07922 [cs, stat]},
  author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.07922},
  keywords = {causality, contrastive},
}

@article{kornblith_similarity_2019,
  title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
  url = {http://arxiv.org/abs/1905.00414},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  urldate = {2021-01-01},
  journal = {arXiv:1905.00414 [cs, q-bio, stat]},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  month = jul,
  year = {2019},
  note = {arXiv: 1905.00414},
  keywords = {nn\_similarity},
}

@article{harkonen_ganspace_2020,
  title = {{GANSpace}: {Discovering} {Interpretable} {GAN} {Controls}},
  shorttitle = {{GANSpace}},
  url = {http://arxiv.org/abs/2004.02546},
  abstract = {This paper describes a simple technique to analyze Generative Adversarial Networks (GANs) and create interpretable controls for image synthesis, such as change of viewpoint, aging, lighting, and time of day. We identify important latent directions based on Principal Components Analysis (PCA) applied either in latent space or feature space. Then, we show that a large number of interpretable controls can be defined by layer-wise perturbation along the principal directions. Moreover, we show that BigGAN can be controlled with layer-wise inputs in a StyleGAN-like manner. We show results on different GANs trained on various datasets, and demonstrate good qualitative matches to edit directions found through earlier supervised approaches.},
  urldate = {2021-01-01},
  journal = {arXiv:2004.02546 [cs]},
  author = {Härkönen, Erik and Hertzmann, Aaron and Lehtinen, Jaakko and Paris, Sylvain},
  month = dec,
  year = {2020},
  note = {arXiv: 2004.02546},
  keywords = {gan, steerability},
}

@article{gao_flow_2020,
  title = {Flow {Contrastive} {Estimation} of {Energy}-{Based} {Models}},
  url = {http://arxiv.org/abs/1912.00589},
  abstract = {This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.},
  urldate = {2021-01-01},
  journal = {arXiv:1912.00589 [cs, stat]},
  author = {Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P. and Xu, Zhen and Dai, Andrew M. and Wu, Ying Nian},
  month = apr,
  year = {2020},
  note = {arXiv: 1912.00589},
  keywords = {contrastive, flow},
}

@inproceedings{lim_bijective-contrastive_2020,
  title = {Bijective-{Contrastive} {Estimation}},
  url = {https://openreview.net/forum?id=yALYfI1nPlA},
  abstract = {In this paper, we propose a new classification-based learning criterion for energy-based models, called Bijective-Contrastive Estimation (BCE). We generate a collection of contrasting distributions...},
  language = {en},
  urldate = {2021-01-01},
  author = {Lim, Jae Hyun and Huang, Chin-Wei and Courville, Aaron and Pal, Christopher},
  month = nov,
  year = {2020},
  keywords = {contrastive},
}

@inproceedings{gatopoulos_self-supervised_2020,
  title = {Self-{Supervised} {Variational} {Auto}-{Encoders}},
  url = {https://openreview.net/forum?id=T1vZvhaf7l},
  abstract = {We propose to utilize self-supervised transformations to decompose modeling a complex distribution into modeling simpler (conditional) distributions.},
  language = {en},
  urldate = {2020-12-29},
  author = {Gatopoulos, Ioannis and Tomczak, Jakub Mikolaj},
  month = nov,
  year = {2020},
  keywords = {self-supervised, vae},
}

@inproceedings{hoogeboom_argmax_2020,
  title = {Argmax {Flows}: {Learning} {Categorical} {Distributions} with {Normalizing} {Flows}},
  shorttitle = {Argmax {Flows}},
  url = {https://openreview.net/forum?id=fdsXhAy5Cp},
  abstract = {This paper introduces a new method to define and train continuous distributions such as normalizing flows directly on categorical data.},
  language = {en},
  urldate = {2020-12-29},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forré, Patrick and Welling, Max},
  month = nov,
  year = {2020},
  keywords = {categorical, normalizing\_flows},
}

@article{daxberger_expressive_2020,
  title = {Expressive yet {Tractable} {Bayesian} {Deep} {Learning} via {Subnetwork} {Inference}},
  url = {http://arxiv.org/abs/2010.14689},
  abstract = {The Bayesian paradigm has the potential to solve some of the core issues in modern deep learning, such as poor calibration, data inefficiency, and catastrophic forgetting. However, scaling Bayesian inference to the high-dimensional parameter spaces of deep neural networks requires restrictive approximations. In this paper, we propose performing inference over only a small subset of the model parameters while keeping all others as point estimates. This enables us to use expressive posterior approximations that would otherwise be intractable for the full model. In particular, we develop a practical and scalable Bayesian deep learning method that first trains a point estimate, and then infers a full covariance Gaussian posterior approximation over a subnetwork. We propose a subnetwork selection procedure which aims to optimally preserve posterior uncertainty. We empirically demonstrate the effectiveness of our approach compared to point-estimated networks and methods that use less expressive posterior approximations over the full network.},
  urldate = {2020-12-29},
  journal = {arXiv:2010.14689 [cs, stat]},
  author = {Daxberger, Erik and Nalisnick, Eric and Allingham, James Urquhart and Antorán, Javier and Hernández-Lobato, José Miguel},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.14689},
}

@article{passenheim_variational_2020,
  title = {Variational {Determinant} {Estimation} with {Spherical} {Normalizing} {Flows}},
  url = {http://arxiv.org/abs/2012.13311},
  abstract = {This paper introduces the Variational Determinant Estimator (VDE), a variational extension of the recently proposed determinant estimator discovered by arXiv:2005.06553v2. Our estimator significantly reduces the variance even for low sample sizes by combining (importance-weighted) variational inference and a family of normalizing flows which allow density estimation on hyperspheres.},
  urldate = {2020-12-29},
  journal = {arXiv:2012.13311 [cs, stat]},
  author = {Passenheim, Simon and Hoogeboom, Emiel},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.13311},
  keywords = {determinant, flows, variational\_inference},
}

@article{tomczak_efcient_nodate,
  title = {Efﬁcient {Low} {Rank} {Gaussian} {Variational} {Inference} for {Neural} {Networks}},
  abstract = {Bayesian neural networks are enjoying a renaissance driven in part by recent advances in variational inference (VI). The most common form of VI employs a fully factorized or mean-ﬁeld distribution, but this is known to suffer from several pathologies, especially as we expect posterior distributions with highly correlated parameters. Current algorithms that capture these correlations with a Gaussian approximating family are difﬁcult to scale to large models due to computational costs and high variance of gradient updates. By using a new form of the reparametrization trick, we derive a computationally efﬁcient algorithm for performing VI with a Gaussian family with a low-rank plus diagonal covariance structure. We scale to deep feed-forward and convolutional architectures. We ﬁnd that adding low-rank terms to parametrized diagonal covariance does not improve predictive performance except on small networks, but low-rank terms added to a constant diagonal covariance improves performance on small and large-scale network architectures.},
  language = {en},
  author = {Tomczak, Marcin B and Swaroop, Siddharth and Turner, Richard E},
  keywords = {rank, variational\_inference},
  pages = {13},
}

@article{voynov_big_2020,
  title = {Big {GANs} {Are} {Watching} {You}: {Towards} {Unsupervised} {Object} {Segmentation} with {Off}-the-{Shelf} {Generative} {Models}},
  shorttitle = {Big {GANs} {Are} {Watching} {You}},
  url = {http://arxiv.org/abs/2006.04988},
  abstract = {Since collecting pixel-level groundtruth data is expensive, unsupervised visual understanding problems are currently an active research topic. In particular, several recent methods based on generative models have achieved promising results for object segmentation and saliency detection. However, since generative models are known to be unstable and sensitive to hyperparameters, the training of these methods can be challenging and time-consuming. In this work, we introduce an alternative, much simpler way to exploit generative models for unsupervised object segmentation. First, we explore the latent space of the BigBiGAN -- the state-of-the-art unsupervised GAN, which parameters are publicly available. We demonstrate that object saliency masks for GAN-produced images can be obtained automatically with BigBiGAN. These masks then are used to train a discriminative segmentation model. Being very simple and easy-to-reproduce, our approach provides competitive performance on common benchmarks in the unsupervised scenario.},
  urldate = {2020-12-25},
  journal = {arXiv:2006.04988 [cs, stat]},
  author = {Voynov, Andrey and Morozov, Stanislav and Babenko, Artem},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.04988},
  keywords = {gan, latent\_space},
}

@article{voynov_unsupervised_2020,
  title = {Unsupervised {Discovery} of {Interpretable} {Directions} in the {GAN} {Latent} {Space}},
  url = {http://arxiv.org/abs/2002.03754},
  abstract = {The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection.},
  urldate = {2020-12-25},
  journal = {arXiv:2002.03754 [cs, stat]},
  author = {Voynov, Andrey and Babenko, Artem},
  month = jun,
  year = {2020},
  note = {arXiv: 2002.03754},
  keywords = {gan, latent\_space},
}

@inproceedings{betancourt_fundamental_2015,
  title = {The {Fundamental} {Incompatibility} of {Scalable} {Hamiltonian} {Monte} {Carlo} and {Naive} {Data} {Subsampling}},
  url = {http://proceedings.mlr.press/v37/betancourt15.html},
  abstract = {Leveraging the coherent exploration of Hamiltonian flow, Hamiltonian Monte Carlo produces computationally efficient Monte Carlo estimators, even with respect to complex and high-dimensional target ...},
  language = {en},
  urldate = {2020-12-24},
  booktitle = {International {Conference} on {Machine} {Learning}},
  publisher = {PMLR},
  author = {Betancourt, Michael},
  month = jun,
  year = {2015},
  note = {ISSN: 1938-7228},
  keywords = {mcmc, sgd},
  pages = {533--540},
}

@article{touvron_training_2020,
  title = {Training data-efficient image transformers \& distillation through attention},
  url = {http://arxiv.org/abs/2012.12877},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption by the larger community. In this work, with an adequate training scheme, we produce a competitive convolution-free transformer by training on Imagenet only. We train it on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. We share our code and models to accelerate community advances on this line of research. Additionally, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 84.4\% accuracy) and when transferring to other tasks.},
  urldate = {2020-12-24},
  journal = {arXiv:2012.12877 [cs]},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.12877},
  keywords = {transformers},
}

@article{gidaris_online_2020,
  title = {Online {Bag}-of-{Visual}-{Words} {Generation} for {Unsupervised} {Representation} {Learning}},
  url = {http://arxiv.org/abs/2012.11552},
  abstract = {Learning image representations without human supervision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual representations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited. With this in mind, we propose a teacher-student scheme to learn representations by training a convnet to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn representations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Extensive experiments demonstrate the interest of our BoW-based strategy which surpasses previous state-of-the-art methods (including contrastive-based ones) in several applications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 classification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of supervised pre-training. We provide the implementation code at https://github.com/valeoai/obow.},
  urldate = {2020-12-24},
  journal = {arXiv:2012.11552 [cs]},
  author = {Gidaris, Spyros and Bursuc, Andrei and Puy, Gilles and Komodakis, Nikos and Cord, Matthieu and Pérez, Patrick},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.11552},
  keywords = {bow, context, contrastive, self-supervised},
}

@article{qin_resizemix_2020,
  title = {{ResizeMix}: {Mixing} {Data} with {Preserved} {Object} {Information} and {True} {Labels}},
  shorttitle = {{ResizeMix}},
  url = {http://arxiv.org/abs/2012.11101},
  abstract = {Data augmentation is a powerful technique to increase the diversity of data, which can effectively improve the generalization ability of neural networks in image recognition tasks. Recent data mixing based augmentation strategies have achieved great success. Especially, CutMix uses a simple but effective method to improve the classifiers by randomly cropping a patch from one image and pasting it on another image. To further promote the performance of CutMix, a series of works explore to use the saliency information of the image to guide the mixing. We systematically study the importance of the saliency information for mixing data, and find that the saliency information is not so necessary for promoting the augmentation performance. Furthermore, we find that the cutting based data mixing methods carry two problems of label misallocation and object information missing, which cannot be resolved simultaneously. We propose a more effective but very easily implemented method, namely ResizeMix. We mix the data by directly resizing the source image to a small patch and paste it on another image. The obtained patch preserves more substantial object information compared with conventional cut-based methods. ResizeMix shows evident advantages over CutMix and the saliency-guided methods on both image classification and object detection tasks without additional computation cost, which even outperforms most costly search-based automatic augmentation methods.},
  urldate = {2020-12-24},
  journal = {arXiv:2012.11101 [cs]},
  author = {Qin, Jie and Fang, Jiemin and Zhang, Qian and Liu, Wenyu and Wang, Xingang and Wang, Xinggang},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.11101
version: 1},
  keywords = {data\_augmentation},
}

@article{matusch_evaluating_2020,
  title = {Evaluating {Agents} without {Rewards}},
  url = {http://arxiv.org/abs/2012.11538},
  abstract = {Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming, expensive, and error prone to human error. Competing objectives have been proposed for agents to learn without external supervision, but it has been unclear how well they reflect task rewards or human behavior. To accelerate the development of intrinsic objectives, we retrospectively compute potential objectives on pre-collected datasets of agent behavior, rather than optimizing them online, and compare them by analyzing their correlations. We study input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward. Moreover, input entropy and information gain correlate more strongly with human similarity than task reward does, suggesting the use of intrinsic objectives for designing agents that behave similarly to human players.},
  urldate = {2020-12-24},
  journal = {arXiv:2012.11538 [cs]},
  author = {Matusch, Brendon and Ba, Jimmy and Hafner, Danijar},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.11538},
  keywords = {curiosity, rl},
}

@article{gao_learning_2020,
  title = {Learning {Energy}-{Based} {Models} by {Diffusion} {Recovery} {Likelihood}},
  url = {http://arxiv.org/abs/2012.08125},
  abstract = {While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained by maximizing the recovery likelihood: the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. The recovery likelihood objective is more tractable than the marginal likelihood objective, since it only requires MCMC sampling from a relatively concentrated conditional distribution. Moreover, we show that this estimation method is theoretically consistent: it learns the correct conditional and marginal distributions at each noise level, given sufficient data. After training, synthesized images can be generated efficiently by a sampling process that initializes from a spherical Gaussian distribution and progressively samples the conditional distributions at decreasingly lower noise levels. Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.60 and inception score 8.58, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets.},
  urldate = {2020-12-24},
  journal = {arXiv:2012.08125 [cs, stat]},
  author = {Gao, Ruiqi and Song, Yang and Poole, Ben and Wu, Ying Nian and Kingma, Diederik P.},
  month = dec,
  year = {2020},
  note = {arXiv: 2012.08125},
  keywords = {ebms, likelihood},
}

@article{ozair_wasserstein_2019,
  title = {Wasserstein {Dependency} {Measure} for {Representation} {Learning}},
  url = {http://arxiv.org/abs/1903.11780},
  abstract = {Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.},
  urldate = {2020-12-20},
  journal = {arXiv:1903.11780 [cs, stat]},
  author = {Ozair, Sherjil and Lynch, Corey and Bengio, Yoshua and Oord, Aaron van den and Levine, Sergey and Sermanet, Pierre},
  month = mar,
  year = {2019},
  note = {arXiv: 1903.11780},
  keywords = {mutual\_information, optimal\_transport},
}

@article{tsai_self-supervised_2020,
  title = {Self-supervised {Learning} from a {Multi}-view {Perspective}},
  url = {http://arxiv.org/abs/2006.05576},
  abstract = {As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.},
  urldate = {2020-12-20},
  journal = {arXiv:2006.05576 [cs, stat]},
  author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  month = oct,
  year = {2020},
  note = {arXiv: 2006.05576},
  keywords = {bert, contrastive, self-supervised},
}

@article{srivastava_autoencoding_2017,
  title = {Autoencoding {Variational} {Inference} {For} {Topic} {Models}},
  url = {http://arxiv.org/abs/1703.01488},
  abstract = {Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.},
  urldate = {2020-12-20},
  journal = {arXiv:1703.01488 [stat]},
  author = {Srivastava, Akash and Sutton, Charles},
  month = mar,
  year = {2017},
  note = {arXiv: 1703.01488},
  keywords = {inference, lda, topic\_modeling, vae},
}

@article{brennan_greedy_2020,
  title = {Greedy inference with structure-exploiting lazy maps},
  url = {http://arxiv.org/abs/1906.00031},
  abstract = {We propose a framework for solving high-dimensional Bayesian inference problems using {\textbackslash}emph\{structure-exploiting\} low-dimensional transport maps or flows. These maps are confined to a low-dimensional subspace (hence, lazy), and the subspace is identified by minimizing an upper bound on the Kullback--Leibler divergence (hence, structured). Our framework provides a principled way of identifying and exploiting low-dimensional structure in an inference problem. It focuses the expressiveness of a transport map along the directions of most significant discrepancy from the posterior, and can be used to build deep compositions of lazy maps, where low-dimensional projections of the parameters are iteratively transformed to match the posterior. We prove weak convergence of the generated sequence of distributions to the posterior, and we demonstrate the benefits of the framework on challenging inference problems in machine learning and differential equations, using inverse autoregressive flows and polynomial maps as examples of the underlying density estimators.},
  urldate = {2020-12-20},
  journal = {arXiv:1906.00031 [stat]},
  author = {Brennan, Michael C. and Bigoni, Daniele and Zahm, Olivier and Spantini, Alessio and Marzouk, Youssef},
  month = nov,
  year = {2020},
  note = {arXiv: 1906.00031},
  keywords = {inference, optimal\_transport},
}

@article{vikram_loracs_2018,
  title = {The {LORACs} prior for {VAEs}: {Letting} the {Trees} {Speak} for the {Data}},
  shorttitle = {The {LORACs} prior for {VAEs}},
  url = {http://arxiv.org/abs/1810.06891},
  abstract = {In variational autoencoders, the prior on the latent codes \$z\$ is often treated as an afterthought, but the prior shapes the kind of latent representation that the model learns. If the goal is to learn a representation that is interpretable and useful, then the prior should reflect the ways in which the high-level factors that describe the data vary. The "default" prior is an isotropic normal, but if the natural factors of variation in the dataset exhibit discrete structure or are not independent, then the isotropic-normal prior will actually encourage learning representations that mask this structure. To alleviate this problem, we propose using a flexible Bayesian nonparametric hierarchical clustering prior based on the time-marginalized coalescent (TMC). To scale learning to large datasets, we develop a new inducing-point approximation and inference algorithm. We then apply the method without supervision to several datasets and examine the interpretability and practical performance of the inferred hierarchies and learned latent space.},
  urldate = {2020-12-20},
  journal = {arXiv:1810.06891 [cs, stat]},
  author = {Vikram, Sharad and Hoffman, Matthew D. and Johnson, Matthew J.},
  month = oct,
  year = {2018},
  note = {arXiv: 1810.06891},
  keywords = {hierarchical, inference, prior, vae},
}

@article{hoffman_neutra-lizing_2019,
  title = {{NeuTra}-lizing {Bad} {Geometry} in {Hamiltonian} {Monte} {Carlo} {Using} {Neural} {Transport}},
  url = {http://arxiv.org/abs/1903.03704},
  abstract = {Hamiltonian Monte Carlo is a powerful algorithm for sampling from difficult-to-normalize posterior distributions. However, when the geometry of the posterior is unfavorable, it may take many expensive evaluations of the target distribution and its gradient to converge and mix. We propose neural transport (NeuTra) HMC, a technique for learning to correct this sort of unfavorable geometry using inverse autoregressive flows (IAF), a powerful neural variational inference technique. The IAF is trained to minimize the KL divergence from an isotropic Gaussian to the warped posterior, and then HMC sampling is performed in the warped space. We evaluate NeuTra HMC on a variety of synthetic and real problems, and find that it significantly outperforms vanilla HMC both in time to reach the stationary distribution and asymptotic effective-sample-size rates.},
  urldate = {2020-12-20},
  journal = {arXiv:1903.03704 [stat]},
  author = {Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
  month = mar,
  year = {2019},
  note = {arXiv: 1903.03704},
  keywords = {inference, mcmc, optimal\_transport},
}

@incollection{marzouk_sampling_2016,
  address = {Cham},
  title = {Sampling via {Measure} {Transport}: {An} {Introduction}},
  isbn = {978-3-319-11259-6},
  shorttitle = {Sampling via {Measure} {Transport}},
  url = {https://doi.org/10.1007/978-3-319-11259-6_23-1},
  abstract = {We present the fundamentals of a measure transport approach to sampling. The idea is to construct a deterministic coupling – i.e., a transport map – between a complex “target” probability measure of interest and a simpler reference measure. Given a transport map, one can generate arbitrarily many independent and unweighted samples from the target simply by pushing forward reference samples through the map. If the map is endowed with a triangular structure, one can also easily generate samples from conditionals of the target measure. We consider two different and complementary scenarios: first, when only evaluations of the unnormalized target density are available and, second, when the target distribution is known only through a finite collection of samples. We show that in both settings, the desired transports can be characterized as the solutions of variational problems. We then address practical issues associated with the optimization-based construction of transports: choosing finite-dimensional parameterizations of the map, enforcing monotonicity, quantifying the error of approximate transports, and refining approximate transports by enriching the corresponding approximation spaces. Approximate transports can also be used to “Gaussianize” complex distributions and thus precondition conventional asymptotically exact sampling schemes. We place the measure transport approach in broader context, describing connections with other optimization-based samplers, with inference and density estimation schemes using optimal transport, and with alternative transformation-based approaches to simulation. We also sketch current work aimed at the construction of transport maps in high dimensions, exploiting essential features of the target distribution (e.g., conditional independence, low-rank structure). The approaches and algorithms presented here have direct applications to Bayesian computation and to broader problems of stochastic simulation.},
  language = {en},
  urldate = {2020-12-20},
  booktitle = {Handbook of {Uncertainty} {Quantification}},
  publisher = {Springer International Publishing},
  author = {Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  editor = {Ghanem, Roger and Higdon, David and Owhadi, Houman},
  year = {2016},
  doi = {10.1007/978-3-319-11259-6_23-1},
  keywords = {inference, optimal\_transport},
  pages = {1--41},
}

@inproceedings{barber_im_2004,
  title = {The {IM} {Algorithm} : {A} variational approach to {Information} {Maximization}},
  abstract = {The maximisation of information transmission over noisy channels is a common, albeit generally computationally diﬃcult problem. We approach the diﬃculty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA.},
  language = {en},
  booktitle = {{NeurIPS}},
  author = {Barber, David and Agakov, Felix},
  year = {2004},
  pages = {8},
}

@inproceedings{mescheder_adversarial_2017,
  title = {Adversarial {Variational} {Bayes}: {Unifying} {Variational} {Autoencoders} and {Generative} {Adversarial} {Networks}},
  shorttitle = {Adversarial {Variational} {Bayes}},
  url = {https://arxiv.org/abs/1701.04722v4},
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that
can be used to learn complex probability distributions from training data.
However, the quality of the resulting model crucially relies on the
expressiveness of the inference model. We introduce Adversarial Variational
Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily
expressive inference models. We achieve this by introducing an auxiliary
discriminative network that allows to rephrase the maximum-likelihood-problem
as a two-player game, hence establishing a principled connection between VAEs
and Generative Adversarial Networks (GANs). We show that in the nonparametric
limit our method yields an exact maximum-likelihood assignment for the
parameters of the generative model, as well as the exact posterior distribution
over the latent variables given an observation. Contrary to competing
approaches which combine VAEs with GANs, our approach has a clear theoretical
justification, retains most advantages of standard Variational Autoencoders and
is easy to implement.},
  language = {en},
  urldate = {2020-12-07},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  month = jan,
  year = {2017},
}

@inproceedings{song_multi-label_2020,
  title = {Multi-label {Contrastive} {Predictive} {Coding}},
  url = {https://arxiv.org/abs/2007.09852v2},
  abstract = {Variational mutual information (MI) estimators are widely used in
unsupervised representation learning methods such as contrastive predictive
coding (CPC). A lower bound on MI can be obtained from a multi-class
classification problem, where a critic attempts to distinguish a positive
sample drawn from the underlying joint distribution from \$(m-1)\$ negative
samples drawn from a suitable proposal distribution. Using this approach, MI
estimates are bounded above by \${\textbackslash}log m\$, and could thus severely underestimate
unless \$m\$ is very large. To overcome this limitation, we introduce a novel
estimator based on a multi-label classification problem, where the critic needs
to jointly identify multiple positive samples at the same time. We show that
using the same amount of negative samples, multi-label CPC is able to exceed
the \${\textbackslash}log m\$ bound, while still being a valid lower bound of mutual
information. We demonstrate that the proposed approach is able to lead to
better mutual information estimation, gain empirical improvements in
unsupervised representation learning, and beat a current state-of-the-art
knowledge distillation method over 10 out of 13 tasks.},
  language = {en},
  urldate = {2020-12-10},
  booktitle = {{NeurIPS}},
  author = {Song, Jiaming and Ermon, Stefano},
  month = jul,
  year = {2020},
  keywords = {contrastive, mutual\_information},
}

@inproceedings{alemi_fixing_2018,
  title = {Fixing a {Broken} {ELBO}},
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  language = {en},
  author = {Alemi, Alexander A and Poole, Ben and Fischer, Ian and Dillon, Joshua V and Saurous, Rif A and Murphy, Kevin},
  year = {2018},
  pages = {21},
}

@inproceedings{tomczak_vae_2017,
  title = {{VAE} with a {VampPrior}},
  url = {https://arxiv.org/abs/1705.07120v5},
  abstract = {Many different methods to train deep generative models have been introduced
in the past. In this paper, we propose to extend the variational auto-encoder
(VAE) framework with a new type of prior which we call "Variational Mixture of
Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture
distribution (e.g., a mixture of Gaussians) with components given by
variational posteriors conditioned on learnable pseudo-inputs. We further
extend this prior to a two layer hierarchical model and show that this
architecture with a coupled prior and posterior, learns significantly better
models. The model also avoids the usual local optima issues related to useless
latent dimensions that plague VAEs. We provide empirical studies on six
datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,
Frey Faces and Histopathology patches, and show that applying the hierarchical
VampPrior delivers state-of-the-art results on all datasets in the unsupervised
permutation invariant setting and the best results or comparable to SOTA
methods for the approach with convolutional networks.},
  language = {en},
  urldate = {2020-12-13},
  booktitle = {{AISTATS}},
  author = {Tomczak, Jakub M. and Welling, Max},
  month = may,
  year = {2017},
  keywords = {posterior\_collapse, vae},
}

@inproceedings{hoffman_elbo_2016,
  title = {{ELBO} surgery: yet another way to carve up the variational evidence lower bound},
  abstract = {We rewrite the variational evidence lower bound objective (ELBO) of variational autoencoders in a way that highlights the role of the encoded data distribution. This perspective suggests that to improve our variational bounds we should improve our priors and not just the encoder and decoder.},
  language = {en},
  booktitle = {{NeurIPS}},
  author = {Hoffman, Matthew D and Johnson, Matthew J},
  year = {2016},
  keywords = {vaes, variational\_inference},
  pages = {4},
}

@article{shu_amortized_2019,
  title = {Amortized {Inference} {Regularization}},
  url = {http://arxiv.org/abs/1805.08913},
  abstract = {The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.},
  urldate = {2020-12-12},
  journal = {arXiv:1805.08913 [cs, stat]},
  author = {Shu, Rui and Bui, Hung H. and Zhao, Shengjia and Kochenderfer, Mykel J. and Ermon, Stefano},
  month = jan,
  year = {2019},
  note = {arXiv: 1805.08913},
  keywords = {inference, vae},
}

@inproceedings{chou_capacity_1988,
  title = {The {Capacity} of the {Kanerva} {Associative} {Memory} is {Exponential}},
  url = {https://proceedings.neurips.cc/paper/1987/file/c81e728d9d4c2f636f067f89cc14862c-Paper.pdf},
  urldate = {2020-12-12},
  booktitle = {Neural {Information} {Processing} {Systems}},
  publisher = {American Institute of Physics},
  author = {Chou, Philip},
  editor = {Anderson, D.},
  year = {1988},
  pages = {184--191},
}

@article{kim_semi-amortized_2018,
  title = {Semi-{Amortized} {Variational} {Autoencoders}},
  url = {http://arxiv.org/abs/1802.02550},
  abstract = {Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.},
  urldate = {2020-12-12},
  journal = {arXiv:1802.02550 [cs, stat]},
  author = {Kim, Yoon and Wiseman, Sam and Miller, Andrew C. and Sontag, David and Rush, Alexander M.},
  month = jul,
  year = {2018},
  note = {arXiv: 1802.02550},
  keywords = {inference, vae},
}

@article{cremer_inference_2018,
  title = {Inference {Suboptimality} in {Variational} {Autoencoders}},
  url = {http://arxiv.org/abs/1801.03558},
  abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.},
  urldate = {2020-12-11},
  journal = {arXiv:1801.03558 [cs, stat]},
  author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
  month = may,
  year = {2018},
  note = {arXiv: 1801.03558},
  keywords = {vae},
}

@article{zhao_towards_2017,
  title = {Towards {Deeper} {Understanding} of {Variational} {Autoencoding} {Models}},
  url = {https://arxiv.org/abs/1702.08658v1},
  abstract = {We propose a new family of optimization criteria for variational
auto-encoding models, generalizing the standard evidence lower bound. We
provide conditions under which they recover the data distribution and learn
latent features, and formally show that common issues such as blurry samples
and uninformative latent features arise when these conditions are not met.
Based on these new insights, we propose a new sequential VAE model that can
generate sharp samples on the LSUN image dataset based on pixel-wise
reconstruction loss, and propose an optimization criterion that encourages
unsupervised learning of informative latent features.},
  language = {en},
  urldate = {2020-12-11},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  month = feb,
  year = {2017},
  keywords = {posterior\_collapse, vae},
}

@article{zhao_towards_nodate,
  title = {Towards a {Deeper} {Understanding} of {Variational} {Autoencoding} {Models}},
  abstract = {We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential VAE model that can generate sharp samples on the LSUN image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features.},
  language = {en},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  pages = {11},
}

@article{hoffman_online_2010,
  title = {Online {Learning} for {Latent} {Dirichlet} {Allocation}},
  volume = {23},
  url = {https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html},
  language = {en},
  urldate = {2020-12-10},
  journal = {Advances in Neural Information Processing Systems},
  author = {Hoffman, Matthew and Bach, Francis and Blei, David},
  year = {2010},
  pages = {856--864},
}

@article{cremer_inference_2018-1,
  title = {Inference {Suboptimality} in {Variational} {Autoencoders}},
  url = {http://arxiv.org/abs/1801.03558},
  abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.},
  urldate = {2020-12-10},
  journal = {arXiv:1801.03558 [cs, stat]},
  author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
  month = may,
  year = {2018},
  note = {arXiv: 1801.03558},
  keywords = {variational\_inference},
}

@article{tosh_contrastive_2020,
  title = {Contrastive estimation reveals topic posterior information to linear models},
  url = {http://arxiv.org/abs/2003.02234},
  abstract = {Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.},
  urldate = {2020-12-09},
  journal = {arXiv:2003.02234 [cs, stat]},
  author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
  month = mar,
  year = {2020},
  note = {arXiv: 2003.02234},
  keywords = {contrastive, inference},
}

@misc{grewal_recent_2019,
  title = {Recent trends and mutual information-based objectives in unsupervised learning},
  url = {http://karangrewal.ca/blog/mutual-information-objectives/},
  urldate = {2020-12-08},
  author = {Grewal, Karan},
  year = {2019},
  keywords = {contrastive, mutual\_information},
}

@article{hendrycks_natural_2020,
  title = {Natural {Adversarial} {Examples}},
  url = {http://arxiv.org/abs/1907.07174},
  abstract = {We introduce natural adversarial examples -- real-world, unmodified, and naturally occurring examples that cause classifier accuracy to significantly degrade. We curate 7,500 natural adversarial examples and release them in an ImageNet classifier test set that we call ImageNet-A. This dataset serves as a new way to measure classifier robustness. Like l\_p adversarial examples, ImageNet-A examples successfully transfer to unseen or black-box classifiers. For example, on ImageNet-A a DenseNet-121 obtains around 2\% accuracy, an accuracy drop of approximately 90\%. Recovering this accuracy is not simple because ImageNet-A examples exploit deep flaws in current classifiers including their over-reliance on color, texture, and background cues. We observe that popular training techniques for improving robustness have little effect, but we show that some architectural changes can enhance robustness to natural adversarial examples. Future research is required to enable robust generalization to this hard ImageNet test set.},
  urldate = {2020-12-07},
  journal = {arXiv:1907.07174 [cs, stat]},
  author = {Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  month = jan,
  year = {2020},
  note = {arXiv: 1907.07174},
  keywords = {adversarial, imagenet},
}

@article{kingma_introduction_2019,
  title = {An {Introduction} to {Variational} {Autoencoders}},
  volume = {12},
  issn = {1935-8237, 1935-8245},
  url = {http://arxiv.org/abs/1906.02691},
  doi = {10.1561/2200000056},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  number = {4},
  urldate = {2020-12-07},
  journal = {Foundations and Trends® in Machine Learning},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  note = {arXiv: 1906.02691},
  keywords = {vae},
  pages = {307--392},
}

@article{suzuki_approximating_nodate,
  title = {Approximating {Mutual} {Information} by {Maximum} {Likelihood} {Density} {Ratio} {Estimation}},
  abstract = {Mutual information is useful in various data processing tasks such as feature selection or independent component analysis. In this paper, we propose a new method of approximating mutual information based on maximum likelihood estimation of a density ratio function. Our method, called Maximum Likelihood Mutual Information (MLMI), has several attractive properties, e.g., density estimation is not involved, it is a single-shot procedure, the global optimal solution can be eﬃciently computed, and cross-validation is available for model selection. Numerical experiments show that MLMI compares favorably with existing methods.},
  language = {en},
  author = {Suzuki, Taiji and Sugiyama, Masashi and Sese, Jun and Kanamori, Takafumi},
  pages = {16},
}

@article{he_lagging_2019,
  title = {Lagging {Inference} {Networks} and {Posterior} {Collapse} in {Variational} {Autoencoders}},
  url = {http://arxiv.org/abs/1901.05534},
  abstract = {The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as "posterior collapse" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.},
  urldate = {2020-12-07},
  journal = {arXiv:1901.05534 [cs, stat]},
  author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and Berg-Kirkpatrick, Taylor},
  month = jan,
  year = {2019},
  note = {arXiv: 1901.05534},
  keywords = {posterior\_collapse, vae},
}

@article{burda_importance_2016,
  title = {Importance {Weighted} {Autoencoders}},
  url = {http://arxiv.org/abs/1509.00519},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  urldate = {2020-12-07},
  journal = {arXiv:1509.00519 [cs, stat]},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  month = nov,
  year = {2016},
  note = {arXiv: 1509.00519},
  keywords = {vae},
}

@misc{bishop_pattern_2006,
  title = {Pattern recognition and machine learning},
  url = {https://cds.cern.ch/record/998831},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  language = {en},
  urldate = {2020-12-07},
  journal = {CERN Document Server},
  author = {Bishop, Christopher M.},
  year = {2006},
  note = {ISBN: 9781493938438 9780387310732
Publisher: Springer},
}

@book{bishop_pattern_2006-1,
  address = {New York},
  series = {Information science and statistics},
  title = {Pattern recognition and machine learning},
  isbn = {978-0-387-31073-2},
  language = {en},
  publisher = {Springer},
  author = {Bishop, Christopher M.},
  year = {2006},
}

@inproceedings{mangasarian_backpropagation_1994,
  title = {Backpropagation {Convergence} {Via} {Deterministic} {Nonmonotone} {Perturbed} {Minimization}},
  volume = {6},
  url = {https://proceedings.neurips.cc/paper/1993/file/4558dbb6f6f8bb2e16d03b85bde76e2c-Paper.pdf},
  urldate = {2020-12-07},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Morgan-Kaufmann},
  author = {Mangasarian, O. L. and Solodov, M. V.},
  editor = {Cowan, J. and Tesauro, G. and Alspector, J.},
  year = {1994},
  keywords = {posterior\_collapse, vae},
  pages = {383--390},
}

@article{poole_variational_2019,
  title = {On {Variational} {Bounds} of {Mutual} {Information}},
  url = {http://arxiv.org/abs/1905.06922},
  abstract = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
  urldate = {2020-12-07},
  journal = {arXiv:1905.06922 [cs, stat]},
  author = {Poole, Ben and Ozair, Sherjil and Oord, Aaron van den and Alemi, Alexander A. and Tucker, George},
  month = may,
  year = {2019},
  note = {arXiv: 1905.06922},
  keywords = {mutual\_information},
}

@article{sugiyama_density-ratio_2012,
  title = {Density-ratio matching under the {Bregman} divergence: a unified framework of density-ratio estimation},
  volume = {64},
  issn = {0020-3157, 1572-9052},
  shorttitle = {Density-ratio matching under the {Bregman} divergence},
  url = {http://link.springer.com/10.1007/s10463-011-0343-8},
  doi = {10.1007/s10463-011-0343-8},
  abstract = {Estimation of the ratio of probability densities has attracted a great deal of attention since it can be used for addressing various statistical paradigms. A naive approach to density-ratio approximation is to ﬁrst estimate numerator and denominator densities separately and then take their ratio. However, this two-step approach does not perform well in practice, and methods for directly estimating density ratios without density estimation have been explored. In this paper, we ﬁrst give a comprehensive review of existing density-ratio estimation methods and discuss their pros and cons. Then we propose a new framework of density-ratio estimation in which a density-ratio model is ﬁtted to the true density-ratio under the Bregman divergence. Our new framework includes existing approaches as special cases, and is substantially more general. Finally, we develop a robust density-ratio estimation method under the power divergence, which is a novel instance in our framework.},
  language = {en},
  number = {5},
  urldate = {2020-12-07},
  journal = {Annals of the Institute of Statistical Mathematics},
  author = {Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
  month = oct,
  year = {2012},
  pages = {1009--1044},
}

@article{kingma_semi-supervised_2014,
  title = {Semi-{Supervised} {Learning} with {Deep} {Generative} {Models}},
  url = {http://arxiv.org/abs/1406.5298},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  urldate = {2020-12-07},
  journal = {arXiv:1406.5298 [cs, stat]},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  month = oct,
  year = {2014},
  note = {arXiv: 1406.5298},
  keywords = {semi-supervised, vae},
}

@article{kingma_semi-supervised_2014-1,
  title = {Semi-{Supervised} {Learning} with {Deep} {Generative} {Models}},
  url = {http://arxiv.org/abs/1406.5298},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  urldate = {2020-12-07},
  journal = {arXiv:1406.5298 [cs, stat]},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  month = oct,
  year = {2014},
  note = {arXiv: 1406.5298},
  keywords = {semi-supervised, vae},
}

@inproceedings{anonymous_ncp-vae_2020,
  title = {{NCP}-{VAE}: {Variational} {Autoencoders} with {Noise} {Contrastive} {Priors}},
  shorttitle = {{NCP}-{VAE}},
  url = {https://openreview.net/forum?id=c1xAGI3nYST},
  abstract = {Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially...},
  language = {en},
  urldate = {2020-12-07},
  author = {Anonymous},
  month = sep,
  year = {2020},
  keywords = {contrastive, vae},
}

@article{goodfellow_generative_2014,
  title = {Generative {Adversarial} {Networks}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2020-12-06},
  journal = {arXiv:1406.2661 [cs, stat]},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  month = jun,
  year = {2014},
  note = {arXiv: 1406.2661},
  keywords = {gan},
}

@article{srivastava_veegan_2017,
  title = {{VEEGAN}: {Reducing} {Mode} {Collapse} in {GANs} using {Implicit} {Variational} {Learning}},
  shorttitle = {{VEEGAN}},
  url = {http://arxiv.org/abs/1705.07761},
  abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
  urldate = {2020-12-06},
  journal = {arXiv:1705.07761 [stat]},
  author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
  month = nov,
  year = {2017},
  note = {arXiv: 1705.07761},
  keywords = {gan, vae},
}

@article{donahue_adversarial_2017,
  title = {Adversarial {Feature} {Learning}},
  url = {http://arxiv.org/abs/1605.09782},
  abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
  urldate = {2020-12-06},
  journal = {arXiv:1605.09782 [cs, stat]},
  author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
  month = apr,
  year = {2017},
  note = {arXiv: 1605.09782},
  keywords = {gan, vae},
}

@article{larsen_autoencoding_2016,
  title = {Autoencoding beyond pixels using a learned similarity metric},
  url = {http://arxiv.org/abs/1512.09300},
  abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
  urldate = {2020-12-06},
  journal = {arXiv:1512.09300 [cs, stat]},
  author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole},
  month = feb,
  year = {2016},
  note = {arXiv: 1512.09300},
  keywords = {gan, vae},
}

@article{tran_hierarchical_2017,
  title = {Hierarchical {Implicit} {Models} and {Likelihood}-{Free} {Variational} {Inference}},
  url = {http://arxiv.org/abs/1702.08896},
  abstract = {Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
  urldate = {2020-12-06},
  journal = {arXiv:1702.08896 [cs, stat]},
  author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
  month = nov,
  year = {2017},
  note = {arXiv: 1702.08896},
  keywords = {gans, likelihood-free},
}

@article{tran_hierarchical_2017-1,
  title = {Hierarchical {Implicit} {Models} and {Likelihood}-{Free} {Variational} {Inference}},
  url = {http://arxiv.org/abs/1702.08896},
  abstract = {Implicit probabilistic models are a ﬂexible class of models deﬁned by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we ﬁrst introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby deﬁning models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model’s ﬂexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
  language = {en},
  urldate = {2020-12-06},
  journal = {arXiv:1702.08896 [cs, stat]},
  author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
  month = nov,
  year = {2017},
  note = {arXiv: 1702.08896},
}

@article{uehara_generative_2016,
  title = {Generative {Adversarial} {Nets} from a {Density} {Ratio} {Estimation} {Perspective}},
  url = {http://arxiv.org/abs/1610.02920},
  abstract = {Generative adversarial networks (GANs) are successful deep generative models. GANs are based on a two-player minimax game. However, the objective function derived in the original motivation is changed to obtain stronger gradients when learning the generator. We propose a novel algorithm that repeats the density ratio estimation and f-divergence minimization. Our algorithm offers a new perspective toward the understanding of GANs and is able to make use of multiple viewpoints obtained in the research of density ratio estimation, e.g. what divergence is stable and relative density ratio is useful.},
  urldate = {2020-12-06},
  journal = {arXiv:1610.02920 [stat]},
  author = {Uehara, Masatoshi and Sato, Issei and Suzuki, Masahiro and Nakayama, Kotaro and Matsuo, Yutaka},
  month = nov,
  year = {2016},
  note = {arXiv: 1610.02920},
  keywords = {gan},
}

@article{zhu_eqco_2020,
  title = {{EqCo}: {Equivalent} {Rules} for {Self}-supervised {Contrastive} {Learning}},
  shorttitle = {{EqCo}},
  url = {http://arxiv.org/abs/2010.01929},
  abstract = {In this paper, we propose a method, named EqCo (Equivalent Rules for Contrastive Learning), to make self-supervised learning irrelevant to the number of negative samples in the contrastive learning framework. Inspired by the InfoMax principle, we point that the margin term in contrastive loss needs to be adaptively scaled according to the number of negative pairs in order to keep steady mutual information bound and gradient magnitude. EqCo bridges the performance gap among a wide range of negative sample sizes, so that we can use only a few negative pairs (e.g. 16 per query) to perform self-supervised contrastive training on large-scale vision datasets like ImageNet, while with almost no accuracy drop. This is quite a contrast to the widely used large batch training or memory bank mechanism in current practices. Equipped with EqCo, our simplified MoCo (SiMo) achieves comparable accuracy with MoCo v2 on ImageNet (linear evaluation protocol) while only involves 16 negative pairs per query instead of 65536, suggesting that large quantities of negative samples might not be a critical factor in contrastive learning frameworks.},
  urldate = {2020-12-06},
  journal = {arXiv:2010.01929 [cs]},
  author = {Zhu, Benjin and Huang, Junqiang and Li, Zeming and Zhang, Xiangyu and Sun, Jian},
  month = nov,
  year = {2020},
  note = {arXiv: 2010.01929},
  keywords = {contrastive},
}

@article{chen_big_2020,
  title = {Big {Self}-{Supervised} {Models} are {Strong} {Semi}-{Supervised} {Learners}},
  url = {http://arxiv.org/abs/2006.10029},
  abstract = {One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels (\${\textbackslash}le\$13 labeled images per class) using ResNet-50, a \$10{\textbackslash}times\$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.},
  urldate = {2020-12-06},
  journal = {arXiv:2006.10029 [cs, stat]},
  author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey},
  month = oct,
  year = {2020},
  note = {arXiv: 2006.10029},
  keywords = {contrastive},
}

@article{wu_unsupervised_2018,
  title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance}-level {Discrimination}},
  url = {http://arxiv.org/abs/1805.01978},
  abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
  urldate = {2020-12-06},
  journal = {arXiv:1805.01978 [cs]},
  author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
  month = may,
  year = {2018},
  note = {arXiv: 1805.01978},
  keywords = {contrastive},
}

@article{he_momentum_2020,
  title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
  url = {http://arxiv.org/abs/1911.05722},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  urldate = {2020-12-06},
  journal = {arXiv:1911.05722 [cs]},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  month = mar,
  year = {2020},
  note = {arXiv: 1911.05722},
  keywords = {contrastive},
}

@article{oord_representation_2019,
  title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
  url = {http://arxiv.org/abs/1807.03748},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  urldate = {2020-12-06},
  journal = {arXiv:1807.03748 [cs, stat]},
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  month = jan,
  year = {2019},
  note = {arXiv: 1807.03748},
  keywords = {contrastive, mutual\_information},
}

@article{tolstikhin_wasserstein_2019,
  title = {Wasserstein {Auto}-{Encoders}},
  url = {http://arxiv.org/abs/1711.01558},
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
  urldate = {2020-12-06},
  journal = {arXiv:1711.01558 [cs, stat]},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  month = dec,
  year = {2019},
  note = {arXiv: 1711.01558},
  keywords = {optimal\_transport, vae},
}

@article{liu_stein_2019,
  title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
  shorttitle = {Stein {Variational} {Gradient} {Descent}},
  url = {http://arxiv.org/abs/1608.04471},
  abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
  urldate = {2020-12-06},
  journal = {arXiv:1608.04471 [cs, stat]},
  author = {Liu, Qiang and Wang, Dilin},
  month = sep,
  year = {2019},
  note = {arXiv: 1608.04471},
  keywords = {stein},
}

@article{hjelm_learning_2019,
  title = {Learning deep representations by mutual information estimation and maximization},
  url = {http://arxiv.org/abs/1808.06670},
  abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
  urldate = {2020-12-06},
  journal = {arXiv:1808.06670 [cs, stat]},
  author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  month = feb,
  year = {2019},
  note = {arXiv: 1808.06670},
  keywords = {mutual\_information},
}

@article{phuong_mutual_2018,
  title = {The {Mutual} {Autoencoder}: {Controlling} {Information} in {Latent} {Code} {Representations}},
  shorttitle = {The {Mutual} {Autoencoder}},
  url = {https://openreview.net/forum?id=HkbmWqxCZ},
  abstract = {Variational autoencoders (VAE) learn probabilistic latent variable models by optimizing a bound on the marginal likelihood of the observed data. Beyond providing a good density model a VAE model...},
  language = {en},
  urldate = {2020-12-06},
  author = {Phuong, Mary and Welling, Max and Kushman, Nate and Tomioka, Ryota and Nowozin, Sebastian},
  month = feb,
  year = {2018},
  keywords = {mutual\_information, posterior\_collapse, vae},
}

@article{gretton_kernel_2012,
  title = {A {Kernel} {Two}-{Sample} {Test}},
  volume = {13},
  url = {http://jmlr.org/papers/v13/gretton12a.html},
  abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions.  Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).  We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic.  The MMD can be computed in quadratic time, although efficient linear time approximations are available.  Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS.  We apply our two-sample tests  to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly.  Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  number = {25},
  urldate = {2020-12-06},
  journal = {Journal of Machine Learning Research},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
  year = {2012},
  keywords = {mmd},
  pages = {723--773},
}

@article{makhzani_adversarial_2016,
  title = {Adversarial {Autoencoders}},
  url = {http://arxiv.org/abs/1511.05644},
  abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  urldate = {2020-12-06},
  journal = {arXiv:1511.05644 [cs]},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  month = may,
  year = {2016},
  note = {arXiv: 1511.05644},
  keywords = {posterior\_collapse, vae},
}

@article{rezaabad_learning_2020,
  title = {Learning {Representations} by {Maximizing} {Mutual} {Information} in {Variational} {Autoencoders}},
  url = {http://arxiv.org/abs/1912.13361},
  abstract = {Variational autoencoders (VAEs) have ushered in a new era of unsupervised learning methods for complex distributions. Although these techniques are elegant in their approach, they are typically not useful for representation learning. In this work, we propose a simple yet powerful class of VAEs that simultaneously result in meaningful learned representations. Our solution is to combine traditional VAEs with mutual information maximization, with the goal to enhance amortized inference in VAEs using Information Theoretic techniques. We call this approach InfoMax-VAE, and such an approach can significantly boost the quality of learned high-level representations. We realize this through the explicit maximization of information measures associated with the representation. Using extensive experiments on varied datasets and setups, we show that InfoMax-VAE outperforms contemporary popular approaches, including Info-VAE and \${\textbackslash}beta\$-VAE.},
  urldate = {2020-12-06},
  journal = {arXiv:1912.13361 [cs, stat]},
  author = {Rezaabad, Ali Lotfi and Vishwanath, Sriram},
  month = jan,
  year = {2020},
  note = {arXiv: 1912.13361},
  keywords = {mutual\_information, posterior\_collapse, vae},
}

@article{gulrajani_pixelvae_2016,
  title = {{PixelVAE}: {A} {Latent} {Variable} {Model} for {Natural} {Images}},
  shorttitle = {{PixelVAE}},
  url = {http://arxiv.org/abs/1611.05013},
  abstract = {Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64x64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.},
  urldate = {2020-12-06},
  journal = {arXiv:1611.05013 [cs]},
  author = {Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  month = nov,
  year = {2016},
  note = {arXiv: 1611.05013},
  keywords = {posterior\_collapse, vae},
}

@article{yang_improved_2017,
  title = {Improved {Variational} {Autoencoders} for {Text} {Modeling} using {Dilated} {Convolutions}},
  url = {http://arxiv.org/abs/1702.08139},
  abstract = {Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
  urldate = {2020-12-06},
  journal = {arXiv:1702.08139 [cs]},
  author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
  month = jun,
  year = {2017},
  note = {arXiv: 1702.08139},
  keywords = {posterior\_collapse, vae},
}

@article{chen_variational_2017,
  title = {Variational {Lossy} {Autoencoder}},
  url = {http://arxiv.org/abs/1611.02731},
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x{\textbar}z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  urldate = {2020-12-06},
  journal = {arXiv:1611.02731 [cs, stat]},
  author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  month = mar,
  year = {2017},
  note = {arXiv: 1611.02731},
  keywords = {posterior\_collapse, vae},
}

@article{bowman_generating_2016,
  title = {Generating {Sentences} from a {Continuous} {Space}},
  url = {http://arxiv.org/abs/1511.06349},
  abstract = {The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  urldate = {2020-12-06},
  journal = {arXiv:1511.06349 [cs]},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  month = may,
  year = {2016},
  note = {arXiv: 1511.06349},
  keywords = {posterior\_collapse, vae},
}

@inproceedings{suzuki_approximating_2008,
  title = {Approximating {Mutual} {Information} by {Maximum} {Likelihood} {Density} {Ratio} {Estimation}},
  url = {http://proceedings.mlr.press/v4/suzuki08a.html},
  abstract = {Mutual information is useful in various data processing tasks such as feature selection or independent component analysis. In this paper, we propose a new method of approximating mutual information...},
  language = {en},
  urldate = {2020-12-04},
  booktitle = {New {Challenges} for {Feature} {Selection} in {Data} {Mining} and {Knowledge} {Discovery}},
  publisher = {PMLR},
  author = {Suzuki, Taiji and Sugiyama, Masashi and Sese, Jun and Kanamori, Takafumi},
  month = sep,
  year = {2008},
  note = {ISSN: 1938-7228},
  keywords = {mutual\_information},
  pages = {5--20},
}

@article{dieng_avoiding_2019,
  title = {Avoiding {Latent} {Variable} {Collapse} {With} {Generative} {Skip} {Models}},
  url = {http://arxiv.org/abs/1807.04863},
  abstract = {Variational autoencoders learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as "latent variable collapse," especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior "collapses" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.},
  urldate = {2020-12-04},
  journal = {arXiv:1807.04863 [cs, stat]},
  author = {Dieng, Adji B. and Kim, Yoon and Rush, Alexander M. and Blei, David M.},
  month = jan,
  year = {2019},
  note = {arXiv: 1807.04863},
  keywords = {mutual\_information, posterior\_collapse, vae},
}

@article{lucas_dont_2019,
  title = {Don't {Blame} the {ELBO}! {A} {Linear} {VAE} {Perspective} on {Posterior} {Collapse}},
  url = {http://arxiv.org/abs/1911.02469},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational posterior distribution closely matches the prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We explain how posterior collapse may occur in pPCA due to local maxima in the log marginal likelihood. Unexpectedly, we prove that the ELBO objective for the linear VAE does not introduce additional spurious local maxima relative to log marginal likelihood. We show further that training a linear VAE with exact variational inference recovers an identifiable global maximum corresponding to the principal component directions. Empirically, we find that our linear analysis is predictive even for high-capacity, non-linear VAEs and helps explain the relationship between the observation noise, local maxima, and posterior collapse in deep Gaussian VAEs.},
  urldate = {2020-12-04},
  journal = {arXiv:1911.02469 [cs, stat]},
  author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  month = nov,
  year = {2019},
  note = {arXiv: 1911.02469},
  keywords = {elbo, posterior\_collapse, vae},
}

@article{zhao_infovae_2018,
  title = {{InfoVAE}: {Information} {Maximizing} {Variational} {Autoencoders}},
  shorttitle = {{InfoVAE}},
  url = {http://arxiv.org/abs/1706.02262},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
  urldate = {2020-12-03},
  journal = {arXiv:1706.02262 [cs, stat]},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  month = may,
  year = {2018},
  note = {arXiv: 1706.02262},
  keywords = {mutual\_information, posterior\_collapse, vae},
}

@article{ambrogioni_wasserstein_2018,
  title = {Wasserstein {Variational} {Inference}},
  url = {http://arxiv.org/abs/1805.11284},
  abstract = {This paper introduces Wasserstein variational inference, a new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. Using the Wasserstein variational inference framework, we introduce several new forms of autoencoders and test their robustness and performance against existing variational autoencoding techniques.},
  urldate = {2020-11-24},
  journal = {arXiv:1805.11284 [cs, stat]},
  author = {Ambrogioni, Luca and Güçlü, Umut and Güçlütürk, Yağmur and Hinne, Max and Maris, Eric and van Gerven, Marcel A. J.},
  month = jun,
  year = {2018},
  note = {arXiv: 1805.11284},
  keywords = {variational\_inference, wasserstein},
}

@misc{li_topic_2017,
  title = {Topic {Modeling} of {New} {York} {Times} {Articles}},
  url = {https://medium.com/swiftworld/topic-modeling-of-new-york-times-articles-11688837d32f},
  abstract = {(This article first appeared on my website)},
  language = {en},
  urldate = {2020-11-22},
  journal = {Medium},
  author = {Li, Susan},
  month = sep,
  year = {2017},
}

@article{song_understanding_2020,
  title = {Understanding the {Limitations} of {Variational} {Mutual} {Information} {Estimators}},
  url = {http://arxiv.org/abs/1910.06222},
  abstract = {Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, we develop a new estimator that focuses on variance reduction. Empirical results on standard benchmark tasks demonstrate that our proposed estimator exhibits improved bias-variance trade-offs on standard benchmark tasks.},
  urldate = {2020-11-21},
  journal = {arXiv:1910.06222 [cs, math, stat]},
  author = {Song, Jiaming and Ermon, Stefano},
  month = mar,
  year = {2020},
  note = {arXiv: 1910.06222},
  keywords = {mutual\_information},
}

@article{papamakarios_normalizing_2019,
  title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
  url = {http://arxiv.org/abs/1912.02762},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  urldate = {2020-11-21},
  journal = {arXiv:1912.02762 [cs, stat]},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.02762},
  keywords = {normalizing\_flows},
}

@article{czolbe_loss_2020,
  title = {A {Loss} {Function} for {Generative} {Neural} {Networks} {Based} on {Watson}'s {Perceptual} {Model}},
  url = {http://arxiv.org/abs/2006.15057},
  abstract = {To train Variational Autoencoders (VAEs) to generate realistic imagery requires a loss function that reflects human perception of image similarity. We propose such a loss function based on Watson's perceptual model, which computes a weighted distance in frequency space and accounts for luminance and contrast masking. We extend the model to color images, increase its robustness to translation by using the Fourier Transform, remove artifacts due to splitting the image into blocks, and make it differentiable. In experiments, VAEs trained with the new loss function generated realistic, high-quality image samples. Compared to using the Euclidean distance and the Structural Similarity Index, the images were less blurry; compared to deep neural network based losses, the new approach required less computational resources and generated images with less artifacts.},
  urldate = {2020-11-15},
  journal = {arXiv:2006.15057 [cs, eess, stat]},
  author = {Czolbe, Steffen and Krause, Oswin and Cox, Ingemar and Igel, Christian},
  month = oct,
  year = {2020},
  note = {arXiv: 2006.15057},
  keywords = {fft, perceptual\_loss, vae},
}

@article{navon_auxiliary_2020,
  title = {Auxiliary {Learning} by {Implicit} {Differentiation}},
  url = {http://arxiv.org/abs/2007.02693},
  abstract = {Training with multiple auxiliary tasks is a common practice used in deep learning for improving the performance on the main task of interest. Two main challenges arise in this multi-task learning setting: (i) Designing useful auxiliary tasks; and (ii) Combining auxiliary tasks into a single coherent loss. We propose a novel framework, AuxiLearn, that targets both challenges, based on implicit differentiation. First, when useful auxiliaries are known, we propose learning a network that combines all losses into a single coherent objective function. This network can learn non-linear interactions between auxiliary tasks. Second, when no useful auxiliary task is known, we describe how to learn a network that generates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series of tasks and domains, including image segmentation and learning with attributes. We find that AuxiLearn consistently improves accuracy compared with competing methods.},
  urldate = {2020-11-15},
  journal = {arXiv:2007.02693 [cs, stat]},
  author = {Navon, Aviv and Achituve, Idan and Maron, Haggai and Chechik, Gal and Fetaya, Ethan},
  month = oct,
  year = {2020},
  note = {arXiv: 2007.02693},
  keywords = {auxiliary, implicit\_diff, multitask},
}

@article{bengio_meta-transfer_2019,
  title = {A {Meta}-{Transfer} {Objective} for {Learning} to {Disentangle} {Causal} {Mechanisms}},
  url = {http://arxiv.org/abs/1901.10912},
  abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
  urldate = {2020-11-12},
  journal = {arXiv:1901.10912 [cs, stat]},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, Sébastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  month = feb,
  year = {2019},
  note = {arXiv: 1901.10912},
  keywords = {causality, disentanglement, representation learning},
}

@article{santoro_simple_2017,
  title = {A simple neural network module for relational reasoning},
  url = {http://arxiv.org/abs/1706.01427},
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  urldate = {2020-11-11},
  journal = {arXiv:1706.01427 [cs]},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  month = jun,
  year = {2017},
  note = {arXiv: 1706.01427},
  keywords = {clevr, object-centric, relational},
}

@article{stanic_hierarchical_2020,
  title = {Hierarchical {Relational} {Inference}},
  url = {http://arxiv.org/abs/2010.03635},
  abstract = {Common-sense physical reasoning in the real world requires learning about the interactions of objects and their dynamics. The notion of an abstract object, however, encompasses a wide variety of physical objects that differ greatly in terms of the complex behaviors they support. To address this, we propose a novel approach to physical reasoning that models objects as hierarchies of parts that may locally behave separately, but also act more globally as a single whole. Unlike prior approaches, our method learns in an unsupervised fashion directly from raw visual images to discover objects, parts, and their relations. It explicitly distinguishes multiple levels of abstraction and improves over a strong baseline at modeling synthetic and real-world videos.},
  urldate = {2020-11-11},
  journal = {arXiv:2010.03635 [cs, stat]},
  author = {Stanić, Aleksandar and van Steenkiste, Sjoerd and Schmidhuber, Jürgen},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.03635},
  keywords = {hierarhical, object-centric},
}

@article{kingma_auto-encoding_2014,
  title = {Auto-{Encoding} {Variational} {Bayes}},
  url = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efﬁcient by ﬁtting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.},
  language = {en},
  urldate = {2020-11-10},
  journal = {arXiv:1312.6114 [cs, stat]},
  author = {Kingma, Diederik P. and Welling, Max},
  month = may,
  year = {2014},
  note = {arXiv: 1312.6114},
  keywords = {vae, variational\_inference},
}

@article{siddharth_learning_2017,
  title = {Learning {Disentangled} {Representations} with {Semi}-{Supervised} {Deep} {Generative} {Models}},
  url = {http://arxiv.org/abs/1706.00400},
  abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
  urldate = {2020-11-08},
  journal = {arXiv:1706.00400 [cs, stat]},
  author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
  month = nov,
  year = {2017},
  note = {arXiv: 1706.00400},
  keywords = {disentanglement, graphical\_models, vae},
}

@article{li_shape-texture_2020,
  title = {Shape-{Texture} {Debiased} {Neural} {Network} {Training}},
  url = {http://arxiv.org/abs/2010.05981},
  abstract = {Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (e.g., an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2\%), ImageNet-A (+5.2\%), ImageNet-C (+8.3\%) and Stylized-ImageNet (+11.1\%), and on defending against FGSM adversarial attacker on ImageNet (+14.4\%). Our method also claims to be compatible to other advanced data augmentation strategies, e.g., Mixup and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.},
  urldate = {2020-11-05},
  journal = {arXiv:2010.05981 [cs]},
  author = {Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.05981},
  keywords = {shape\_bias, style\_transfer},
}

@article{tian_rethinking_2020,
  title = {Rethinking {Few}-{Shot} {Image} {Classification}: a {Good} {Embedding} {Is} {All} {You} {Need}?},
  shorttitle = {Rethinking {Few}-{Shot} {Image} {Classification}},
  url = {http://arxiv.org/abs/2003.11539},
  abstract = {The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/WangYueFt/rfs/.},
  urldate = {2020-11-04},
  journal = {arXiv:2003.11539 [cs]},
  author = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B. and Isola, Phillip},
  month = jun,
  year = {2020},
  note = {arXiv: 2003.11539},
  keywords = {fewshot, meta-learning, self-supervision},
}

@article{zamir_taskonomy_2018,
  title = {Taskonomy: {Disentangling} {Task} {Transfer} {Learning}},
  shorttitle = {Taskonomy},
  url = {http://arxiv.org/abs/1804.08328},
  abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity. We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxonomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.},
  urldate = {2020-11-04},
  journal = {arXiv:1804.08328 [cs]},
  author = {Zamir, Amir and Sax, Alexander and Shen, William and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  month = apr,
  year = {2018},
  note = {arXiv: 1804.08328},
  keywords = {task},
}

@article{friederich_scientific_2020,
  title = {Scientific intuition inspired by machine learning generated hypotheses},
  url = {http://arxiv.org/abs/2010.14236},
  abstract = {Machine learning with application to questions in the physical sciences has become a widely used tool, successfully applied to classification, regression and optimization tasks in many areas. Research focus mostly lies in improving the accuracy of the machine learning models in numerical predictions, while scientific understanding is still almost exclusively generated by human researchers analysing numerical results and drawing conclusions. In this work, we shift the focus on the insights and the knowledge obtained by the machine learning models themselves. In particular, we study how it can be extracted and used to inspire human scientists to increase their intuitions and understanding of natural systems. We apply gradient boosting in decision trees to extract human interpretable insights from big data sets from chemistry and physics. In chemistry, we not only rediscover widely know rules of thumb but also find new interesting motifs that tell us how to control solubility and energy levels of organic molecules. At the same time, in quantum physics, we gain new understanding on experiments for quantum entanglement. The ability to go beyond numerics and to enter the realm of scientific insight and hypothesis generation opens the door to use machine learning to accelerate the discovery of conceptual understanding in some of the most challenging domains of science.},
  urldate = {2020-11-04},
  journal = {arXiv:2010.14236 [physics, physics:quant-ph]},
  author = {Friederich, Pascal and Krenn, Mario and Tamblyn, Isaac and Aspuru-Guzik, Alan},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.14236},
  keywords = {experimental\_design},
}

@article{patacchiola_self-supervised_2020,
  title = {Self-{Supervised} {Relational} {Reasoning} for {Representation} {Learning}},
  url = {http://arxiv.org/abs/2006.05849},
  abstract = {In self-supervised learning, a system is tasked with achieving a surrogate objective by defining alternative targets on a set of unlabeled data. The aim is to build useful representations that can be used in downstream tasks, without costly manual annotation. In this work, we propose a novel self-supervised formulation of relational reasoning that allows a learner to bootstrap a signal from information implicit in unlabeled data. Training a relation head to discriminate how entities relate to themselves (intra-reasoning) and other entities (inter-reasoning), results in rich and descriptive representations in the underlying neural network backbone, which can be used in downstream tasks such as classification and image retrieval. We evaluate the proposed method following a rigorous experimental procedure, using standard datasets, protocols, and backbones. Self-supervised relational reasoning outperforms the best competitor in all conditions by an average 14\% in accuracy, and the most recent state-of-the-art model by 3\%. We link the effectiveness of the method to the maximization of a Bernoulli log-likelihood, which can be considered as a proxy for maximizing the mutual information, resulting in a more efficient objective with respect to the commonly used contrastive losses.},
  urldate = {2020-11-04},
  journal = {arXiv:2006.05849 [cs, stat]},
  author = {Patacchiola, Massimiliano and Storkey, Amos},
  month = oct,
  year = {2020},
  note = {arXiv: 2006.05849},
  keywords = {relational, self-supervision},
}

@article{standley_which_nodate,
  title = {Which {Tasks} {Should} {Be} {Learned} {Together} in {Multi}-task {Learning}?},
  abstract = {Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using multi-task learning. This can save computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives can compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We study task cooperation and competition in several different learning settings and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.},
  language = {en},
  author = {Standley, Trevor and Zamir, Amir R and Chen, Dawn and Guibas, Leonidas and Malik, Jitendra and Savarese, Silvio},
  keywords = {multitask},
  pages = {13},
}

@article{mukhoti_calibrating_2020,
  title = {Calibrating {Deep} {Neural} {Networks} using {Focal} {Loss}},
  url = {http://arxiv.org/abs/2002.09437},
  abstract = {Miscalibration - a mismatch between a model's confidence and its correctness - of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss [Lin et. al., 2017] allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases. Code is available at https://github.com/torrvision/focal\_calibration.},
  urldate = {2020-11-04},
  journal = {arXiv:2002.09437 [cs, stat]},
  author = {Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip H. S. and Dokania, Puneet K.},
  month = oct,
  year = {2020},
  note = {arXiv: 2002.09437},
  keywords = {focal\_loss, uncertainty},
}

@article{faghri_study_2020,
  title = {A {Study} of {Gradient} {Variance} in {Deep} {Learning}},
  url = {http://arxiv.org/abs/2007.04532},
  abstract = {The impact of gradient noise on training deep models is widely acknowledged but not well understood. In this context, we study the distribution of gradients during training. We introduce a method, Gradient Clustering, to minimize the variance of average mini-batch gradient with stratified sampling. We prove that the variance of average mini-batch gradient is minimized if the elements are sampled from a weighted clustering in the gradient space. We measure the gradient variance on common deep learning benchmarks and observe that, contrary to common assumptions, gradient variance increases during training, and smaller learning rates coincide with higher variance. In addition, we introduce normalized gradient variance as a statistic that better correlates with the speed of convergence compared to gradient variance.},
  urldate = {2020-11-03},
  journal = {arXiv:2007.04532 [cs, stat]},
  author = {Faghri, Fartash and Duvenaud, David and Fleet, David J. and Ba, Jimmy},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.04532},
  keywords = {gradients, optimization},
}

@article{kornblith_whats_2020,
  title = {What's in a {Loss} {Function} for {Image} {Classification}?},
  url = {http://arxiv.org/abs/2010.16402},
  abstract = {It is common to use the softmax cross-entropy loss to train neural networks on classification datasets where a single class label is assigned to each example. However, it has been shown that modifying softmax cross-entropy with label smoothing or regularizers such as dropout can lead to higher performance. This paper studies a variety of loss functions and output layer regularization strategies on image classification tasks. We observe meaningful differences in model predictions, accuracy, calibration, and out-of-distribution robustness for networks trained with different objectives. However, differences in hidden representations of networks trained with different objectives are restricted to the last few layers; representational similarity reveals no differences among network layers that are not close to the output. We show that all objectives that improve over vanilla softmax loss produce greater class separation in the penultimate layer of the network, which potentially accounts for improved performance on the original task, but results in features that transfer worse to other tasks.},
  urldate = {2020-11-02},
  journal = {arXiv:2010.16402 [cs]},
  author = {Kornblith, Simon and Lee, Honglak and Chen, Ting and Norouzi, Mohammad},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.16402},
}

@article{chen_gradnorm_2018,
  title = {{GradNorm}: {Gradient} {Normalization} for {Adaptive} {Loss} {Balancing} in {Deep} {Multitask} {Networks}},
  shorttitle = {{GradNorm}},
  url = {http://arxiv.org/abs/1711.02257},
  abstract = {Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter \${\textbackslash}alpha\$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.},
  urldate = {2020-10-31},
  journal = {arXiv:1711.02257 [cs]},
  author = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
  month = jun,
  year = {2018},
  note = {arXiv: 1711.02257},
  keywords = {gradients, multitask},
}

@article{robinson_contrastive_2020,
  title = {Contrastive {Learning} with {Hard} {Negative} {Samples}},
  url = {http://arxiv.org/abs/2010.04592},
  abstract = {We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use label information. In response, we develop a new class of unsupervised methods for selecting hard negative samples where the user can control the amount of hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.},
  urldate = {2020-10-29},
  journal = {arXiv:2010.04592 [cs, stat]},
  author = {Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.04592},
  keywords = {contrastive, metric\_learning, self-supervised},
}

@article{wu_sampling_2018,
  title = {Sampling {Matters} in {Deep} {Embedding} {Learning}},
  url = {http://arxiv.org/abs/1706.07567},
  abstract = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
  urldate = {2020-10-29},
  journal = {arXiv:1706.07567 [cs]},
  author = {Wu, Chao-Yuan and Manmatha, R. and Smola, Alexander J. and Krähenbühl, Philipp},
  month = jan,
  year = {2018},
  note = {arXiv: 1706.07567},
  keywords = {metric\_learning, sampling},
}

@article{yu_wavelet_2020,
  title = {Wavelet {Flow}: {Fast} {Training} of {High} {Resolution} {Normalizing} {Flows}},
  shorttitle = {Wavelet {Flow}},
  url = {http://arxiv.org/abs/2010.13821},
  abstract = {Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15x faster to train.},
  urldate = {2020-10-28},
  journal = {arXiv:2010.13821 [cs]},
  author = {Yu, Jason J. and Derpanis, Konstantinos G. and Brubaker, Marcus A.},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.13821},
  keywords = {normalizing\_flows},
}

@article{foong_expressiveness_2020,
  title = {On the {Expressiveness} of {Approximate} {Inference} in {Bayesian} {Neural} {Networks}},
  url = {http://arxiv.org/abs/1909.00719},
  abstract = {While Bayesian neural networks (BNNs) hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.},
  urldate = {2020-10-28},
  journal = {arXiv:1909.00719 [cs, stat]},
  author = {Foong, Andrew Y. K. and Burt, David R. and Li, Yingzhen and Turner, Richard E.},
  month = oct,
  year = {2020},
  note = {arXiv: 1909.00719},
  keywords = {bnns, variational\_inference},
}

@article{ranganath_black_2013,
  title = {Black {Box} {Variational} {Inference}},
  url = {http://arxiv.org/abs/1401.0118},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  urldate = {2020-10-25},
  journal = {arXiv:1401.0118 [cs, stat]},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  month = dec,
  year = {2013},
  note = {arXiv: 1401.0118},
  keywords = {variational\_inference},
}

@article{tian_understanding_2020,
  title = {Understanding {Self}-supervised {Learning} with {Dual} {Deep} {Networks}},
  url = {http://arxiv.org/abs/2010.00578},
  abstract = {We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a {\textbackslash}emph\{covariance operator\} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations, which we show leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that BYOL works due to an implicit contrastive term, acting as an approximate covariance operator. The term is formed by the inter-play between the zero-mean operation of BatchNorm and the extra predictor in the online network. Extensive ablation studies justify our theoretical findings.},
  urldate = {2020-10-03},
  journal = {arXiv:2010.00578 [cs, stat]},
  author = {Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  month = oct,
  year = {2020},
  note = {arXiv: 2010.00578},
  keywords = {self-supervised},
}

@article{mensch_online_2020,
  title = {Online {Sinkhorn}: {Optimal} {Transport} distances from sample streams},
  shorttitle = {Online {Sinkhorn}},
  url = {http://arxiv.org/abs/2003.01415},
  abstract = {Optimal Transport (OT) distances are now routinely used as loss functions in ML tasks. Yet, computing OT distances between arbitrary (i.e. not necessarily discrete) probability distributions remains an open problem. This paper introduces a new online estimator of entropy-regularized OT distances between two such arbitrary distributions. It uses streams of samples from both distributions to iteratively enrich a non-parametric representation of the transportation plan. Compared to the classic Sinkhorn algorithm, our method leverages new samples at each iteration, which enables a consistent estimation of the true regularized OT distance. We provide a theoretical analysis of the convergence of the online Sinkhorn algorithm, showing a nearly-O(1/n) asymptotic sample complexity for the iterate sequence. We validate our method on synthetic 1D to 10D data and on real 3D shape data.},
  urldate = {2020-10-02},
  journal = {arXiv:2003.01415 [math, stat]},
  author = {Mensch, Arthur and Peyré, Gabriel},
  month = jul,
  year = {2020},
  note = {arXiv: 2003.01415},
  keywords = {optimal\_transport},
}

@article{puigcerver_scalable_2020,
  title = {Scalable {Transfer} {Learning} with {Expert} {Models}},
  url = {http://arxiv.org/abs/2009.13239},
  abstract = {Transfer of pre-trained representations can improve sample efficiency and reduce computational requirements for new tasks. However, representations used for transfer are usually generic, and are not tailored to a particular distribution of downstream tasks. We explore the use of expert representations for transfer with a simple, yet effective, strategy. We train a diverse set of experts by exploiting existing label structures, and use cheap-to-compute performance proxies to select the relevant expert for each target task. This strategy scales the process of transferring to new tasks, since it does not revisit the pre-training data during transfer. Accordingly, it requires little extra compute per target task, and results in a speed-up of 2-3 orders of magnitude compared to competing approaches. Further, we provide an adapter-based architecture able to compress many experts into a single model. We evaluate our approach on two different data sources and demonstrate that it outperforms baselines on over 20 diverse vision tasks in both cases.},
  urldate = {2020-10-02},
  journal = {arXiv:2009.13239 [cs, stat]},
  author = {Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Renggli, Cedric and Pinto, André Susano and Gelly, Sylvain and Keysers, Daniel and Houlsby, Neil},
  month = sep,
  year = {2020},
  note = {arXiv: 2009.13239},
  keywords = {transfer\_learning},
}

@article{zhang_causal_2020,
  title = {Causal {Intervention} for {Weakly}-{Supervised} {Semantic} {Segmentation}},
  url = {http://arxiv.org/abs/2009.12547},
  abstract = {We present a causal inference framework to improve Weakly-Supervised Semantic Segmentation (WSSS). Specifically, we aim to generate better pixel-level pseudo-masks by using only image-level labels -- the most crucial step in WSSS. We attribute the cause of the ambiguous boundaries of pseudo-masks to the confounding context, e.g., the correct image-level classification of "horse" and "person" may be not only due to the recognition of each instance, but also their co-occurrence context, making the model inspection (e.g., CAM) hard to distinguish between the boundaries. Inspired by this, we propose a structural causal model to analyze the causalities among images, contexts, and class labels. Based on it, we develop a new method: Context Adjustment (CONTA), to remove the confounding bias in image-level classification and thus provide better pseudo-masks as ground-truth for the subsequent segmentation model. On PASCAL VOC 2012 and MS-COCO, we show that CONTA boosts various popular WSSS methods to new state-of-the-arts.},
  urldate = {2020-10-02},
  journal = {arXiv:2009.12547 [cs]},
  author = {Zhang, Dong and Zhang, Hanwang and Tang, Jinhui and Hua, Xiansheng and Sun, Qianru},
  month = sep,
  year = {2020},
  note = {arXiv: 2009.12547},
  keywords = {causality, segmentation},
}

@article{dubois_learning_2020,
  title = {Learning {Optimal} {Representations} with the {Decodable} {Information} {Bottleneck}},
  url = {http://arxiv.org/abs/2009.12789},
  abstract = {We address the question of characterizing and finding optimal representations for supervised learning. Traditionally, this question has been tackled using the Information Bottleneck, which compresses the inputs while retaining information about the targets, in a decoder-agnostic fashion. In machine learning, however, our goal is not compression but rather generalization, which is intimately linked to the predictive family or decoder of interest (e.g. linear classifier). We propose the Decodable Information Bottleneck (DIB) that considers information retention and compression from the perspective of the desired predictive family. As a result, DIB gives rise to representations that are optimal in terms of expected test performance and can be estimated with guarantees. Empirically, we show that the framework can be used to enforce a small generalization gap on downstream classifiers and to predict the generalization ability of neural networks.},
  urldate = {2020-10-02},
  journal = {arXiv:2009.12789 [cs, math, stat]},
  author = {Dubois, Yann and Kiela, Douwe and Schwab, David J. and Vedantam, Ramakrishna},
  month = sep,
  year = {2020},
  note = {arXiv: 2009.12789},
  keywords = {rep\_learning},
}

@article{hosoya_simple_2018,
  title = {A simple probabilistic deep generative model for learning generalizable disentangled representations from grouped data},
  url = {http://arxiv.org/abs/1809.02383},
  abstract = {The disentangling problem is to discover multiple complex factors of variations hidden in data. One recent approach is to take a dataset with grouping structure and separately estimate a factor common within a group (content) and a factor specific to each group member (transformation). Notably, this approach can learn to represent a continuous space of contents, which allows for generalization to data with unseen contents. In this study, we aim at cultivating this approach within probabilistic deep generative models. Motivated by technical complication in existing group-based methods, we propose a simpler probabilistic method, called group-contrastive variational autoencoders. Despite its simplicity, our approach achieves reasonable disentanglement with generalizability for three grouped datasets of 3D object images. In comparison with a previous model, although conventional qualitative evaluation shows little difference, our qualitative evaluation using few-shot classification exhibits superior performances for some datasets. We analyze the content representations from different methods and discuss their transformation-dependency and potential performance impacts.},
  urldate = {2020-09-23},
  journal = {arXiv:1809.02383 [cs, stat]},
  author = {Hosoya, Haruo},
  month = sep,
  year = {2018},
  note = {arXiv: 1809.02383},
  keywords = {disentanglement},
}

@article{suter_robustly_2019,
  title = {Robustly {Disentangled} {Causal} {Mechanisms}: {Validating} {Deep} {Representations} for {Interventional} {Robustness}},
  shorttitle = {Robustly {Disentangled} {Causal} {Mechanisms}},
  url = {http://arxiv.org/abs/1811.00007},
  abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
  urldate = {2020-09-19},
  journal = {arXiv:1811.00007 [cs, stat]},
  author = {Suter, Raphael and Miladinović, Đorđe and Schölkopf, Bernhard and Bauer, Stefan},
  month = may,
  year = {2019},
  note = {arXiv: 1811.00007},
  keywords = {causality, disentanglement},
}

@article{dadashi_primal_2020,
  title = {Primal {Wasserstein} {Imitation} {Learning}},
  url = {http://arxiv.org/abs/2006.04678},
  abstract = {Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.},
  urldate = {2020-09-18},
  journal = {arXiv:2006.04678 [cs, stat]},
  author = {Dadashi, Robert and Hussenot, Léonard and Geist, Matthieu and Pietquin, Olivier},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.04678},
  keywords = {optimal\_transport, rl},
}

@article{dash_counterfactual_2020,
  title = {Counterfactual {Generation} and {Fairness} {Evaluation} {Using} {Adversarially} {Learned} {Inference}},
  url = {http://arxiv.org/abs/2009.08270},
  abstract = {Recent studies have reported biases in machine learning image classifiers, especially against particular demographic groups. Counterfactual examples for an input---perturbations that change specific features but not others---have been shown to be useful for evaluating explainability and fairness of machine learning models. However, generating counterfactual examples for images is non-trivial due to the underlying causal structure governing the various features of an image. To be meaningful, generated perturbations need to satisfy constraints implied by the causal model. We present a method for generating counterfactuals by incorporating a known causal graph structure in a conditional variant of Adversarially Learned Inference (ALI). The proposed approach learns causal relationships between the specified attributes of an image and generates counterfactuals in accordance with these relationships. On Morpho-MNIST and CelebA datasets, the method generates counterfactuals that can change specified attributes and their causal descendants while keeping other attributes constant. As an application, we apply the generated counterfactuals from CelebA images to evaluate fairness biases in a classifier that predicts attractiveness of a face.},
  urldate = {2020-09-18},
  journal = {arXiv:2009.08270 [cs]},
  author = {Dash, Saloni and Sharma, Amit},
  month = sep,
  year = {2020},
  note = {arXiv: 2009.08270},
  keywords = {causality, counterfactual},
}

@article{zhan_adversarial_2020,
  title = {Adversarial {Image} {Composition} with {Auxiliary} {Illumination}},
  url = {http://arxiv.org/abs/2009.08255},
  abstract = {Dealing with the inconsistency between a foreground object and a background image is a challenging task in high-fidelity image composition. State-of-the-art methods strive to harmonize the composed image by adapting the style of foreground objects to be compatible with the background image, whereas the potential shadow of foreground objects within the composed image which is critical to the composition realism is largely neglected. In this paper, we propose an Adversarial Image Composition Net (AIC-Net) that achieves realistic image composition by considering potential shadows that the foreground object projects in the composed image. A novel branched generation mechanism is proposed, which disentangles the generation of shadows and the transfer of foreground styles for optimal accomplishment of the two tasks simultaneously. A differentiable spatial transformation module is designed which bridges the local harmonization and the global harmonization to achieve their joint optimization effectively. Extensive experiments on pedestrian and car composition tasks show that the proposed AIC-Net achieves superior composition performance qualitatively and quantitatively.},
  urldate = {2020-09-18},
  journal = {arXiv:2009.08255 [cs]},
  author = {Zhan, Fangneng and Lu, Shijian and Zhang, Changgong and Ma, Feiying and Xie, Xuansong},
  month = sep,
  year = {2020},
  note = {arXiv: 2009.08255},
  keywords = {illumination, multitask},
}

@article{nakkiran_distributional_2020,
  title = {Distributional {Generalization}: {A} {New} {Kind} of {Generalization}},
  shorttitle = {Distributional {Generalization}},
  url = {http://arxiv.org/abs/2009.08092},
  abstract = {We introduce a new notion of generalization-- Distributional Generalization-- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30\% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30\% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. This example is a specific instance of our much more general conjectures which apply even on distributions where the Bayes risk is zero. Our conjectures characterize the form of distributional generalization that can be expected, in terms of problem parameters (model architecture, training procedure, number of samples, data distribution). We verify the quantitative predictions of these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. These empirical observations are independently interesting, and form a more fine-grained characterization of interpolating classifiers beyond just their test error.},
  urldate = {2020-09-18},
  journal = {arXiv:2009.08092 [cs, math, stat]},
  author = {Nakkiran, Preetum and Bansal, Yamini},
  month = sep,
  year = {2020},
  note = {arXiv: 2009.08092},
  keywords = {generalization},
}

@article{nair_causal_2019,
  title = {Causal {Induction} from {Visual} {Observations} for {Goal} {Directed} {Tasks}},
  url = {http://arxiv.org/abs/1910.01751},
  abstract = {Causal reasoning has been an indispensable capability for humans and other intelligent animals to interact with the physical world. In this work, we propose to endow an artificial agent with the capability of causal reasoning for completing goal-directed tasks. We develop learning-based approaches to inducing causal knowledge in the form of directed acyclic graphs, which can be used to contextualize a learned goal-conditional policy to perform tasks in novel environments with latent causal structures. We leverage attention mechanisms in our causal induction model and goal-conditional policy, enabling us to incrementally generate the causal graph from the agent's visual observations and to selectively use the induced graph for determining actions. Our experiments show that our method effectively generalizes towards completing new tasks in novel environments with previously unseen causal structures.},
  urldate = {2020-09-09},
  journal = {arXiv:1910.01751 [cs, stat]},
  author = {Nair, Suraj and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
  month = oct,
  year = {2019},
  note = {arXiv: 1910.01751},
  keywords = {causality},
}

@inproceedings{brahmbhatt_stuffnet_2017,
  title = {{StuffNet}: {Using} ‘{Stuff}’ to {Improve} {Object} {Detection}},
  shorttitle = {{StuffNet}},
  doi = {10.1109/WACV.2017.109},
  abstract = {We propose a Convolutional Neural Network (CNN) based algorithm - StuffNet - for object detection. In addition to the standard convolutional features trained for region proposal and object detection [33], StuffNet uses convolutional features trained for segmentation of objects and 'stuff' (amorphous categories such as ground and water). Through experiments on Pascal VOC 2010, we show the importance of features learnt from stuff segmentation for improving object detection performance. StuffNet improves performance from 18.8\% mAP to 23.9\% mAP for small objects. We also devise a method to train StuffNet on datasets that do not have stuff segmentation labels. Through experiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of this method and show that StuffNet also significantly improves object detection performance on such datasets.},
  booktitle = {2017 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
  author = {Brahmbhatt, Samarth and Christensen, Henrik I. and Hays, James},
  month = mar,
  year = {2017},
  keywords = {detection, segmentation, stuff},
  pages = {934--943},
}

@article{grant_recasting_2018,
  title = {Recasting {Gradient}-{Based} {Meta}-{Learning} as {Hierarchical} {Bayes}},
  url = {http://arxiv.org/abs/1801.08930},
  abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
  urldate = {2020-09-08},
  journal = {arXiv:1801.08930 [cs]},
  author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
  month = jan,
  year = {2018},
  note = {arXiv: 1801.08930},
  keywords = {bayesian, hierarchical, meta-learning},
}

@article{sinha_small-gan_2019,
  title = {Small-{GAN}: {Speeding} {Up} {GAN} {Training} {Using} {Core}-sets},
  shorttitle = {Small-{GAN}},
  url = {http://arxiv.org/abs/1910.13540},
  abstract = {Recent work by Brock et al. (2018) suggests that Generative Adversarial Networks (GANs) benefit disproportionately from large mini-batch sizes. Unfortunately, using large batches is slow and expensive on conventional hardware. Thus, it would be nice if we could generate batches that were effectively large though actually small. In this work, we propose a method to do this, inspired by the use of Coreset-selection in active learning. When training a GAN, we draw a large batch of samples from the prior and then compress that batch using Coreset-selection. To create effectively large batches of 'real' images, we create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset-selection on those projected activations at training time. We conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows GANs to reach a new state of the art in anomaly detection.},
  urldate = {2020-09-05},
  journal = {arXiv:1910.13540 [cs, stat]},
  author = {Sinha, Samarth and Zhang, Han and Goyal, Anirudh and Bengio, Yoshua and Larochelle, Hugo and Odena, Augustus},
  month = oct,
  year = {2019},
  note = {arXiv: 1910.13540},
  keywords = {gan},
}

@article{zhao_image_2020,
  title = {Image {Augmentations} for {GAN} {Training}},
  url = {http://arxiv.org/abs/2006.02595},
  abstract = {Data augmentations have been widely studied to improve the accuracy and robustness of classifiers. However, the potential of image augmentation in improving GAN models for image synthesis has not been thoroughly investigated in previous studies. In this work, we systematically study the effectiveness of various existing augmentation techniques for GAN training in a variety of settings. We provide insights and guidelines on how to augment images for both vanilla GANs and GANs with regularizations, improving the fidelity of the generated images substantially. Surprisingly, we find that vanilla GANs attain generation quality on par with recent state-of-the-art results if we use augmentations on both real and generated images. When this GAN training is combined with other augmentation-based regularization techniques, such as contrastive loss and consistency regularization, the augmentations further improve the quality of generated images. We provide new state-of-the-art results for conditional generation on CIFAR-10 with both consistency loss and contrastive loss as additional regularizations.},
  urldate = {2020-09-05},
  journal = {arXiv:2006.02595 [cs, eess, stat]},
  author = {Zhao, Zhengli and Zhang, Zizhao and Chen, Ting and Singh, Sameer and Zhang, Han},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.02595},
  keywords = {contrastive, gan},
}

@article{jiang_fantastic_2019,
  title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
  url = {http://arxiv.org/abs/1912.02178},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  urldate = {2020-09-05},
  journal = {arXiv:1912.02178 [cs, stat]},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.02178},
  keywords = {generalization},
}

@article{menon_why_2020,
  title = {Why distillation helps: a statistical perspective},
  shorttitle = {Why distillation helps},
  url = {http://arxiv.org/abs/2005.10419},
  abstract = {Knowledge distillation is a technique for improving the performance of a simple "student" model by replacing its one-hot training labels with a distribution over labels obtained from a complex "teacher" model. While this simple approach has proven widely effective, a basic question remains unresolved: why does distillation help? In this paper, we present a statistical perspective on distillation which addresses this question, and provides a novel connection to extreme multiclass retrieval techniques. Our core observation is that the teacher seeks to estimate the underlying (Bayes) class-probability function. Building on this, we establish a fundamental bias-variance tradeoff in the student's objective: this quantifies how approximate knowledge of these class-probabilities can significantly aid learning. Finally, we show how distillation complements existing negative mining techniques for extreme multiclass retrieval, and propose a unified objective which combines these ideas.},
  urldate = {2020-09-05},
  journal = {arXiv:2005.10419 [cs, stat]},
  author = {Menon, Aditya Krishna and Rawat, Ankit Singh and Reddi, Sashank J. and Kim, Seungyeon and Kumar, Sanjiv},
  month = may,
  year = {2020},
  note = {arXiv: 2005.10419},
  keywords = {distillation},
}

@article{huang_augmented_2020,
  title = {Augmented {Normalizing} {Flows}: {Bridging} the {Gap} {Between} {Generative} {Flows} and {Latent} {Variable} {Models}},
  shorttitle = {Augmented {Normalizing} {Flows}},
  url = {http://arxiv.org/abs/2002.07101},
  abstract = {In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate state-of-the-art performance on standard benchmarks of flow-based generative modeling.},
  urldate = {2020-09-05},
  journal = {arXiv:2002.07101 [cs, stat]},
  author = {Huang, Chin-Wei and Dinh, Laurent and Courville, Aaron},
  month = feb,
  year = {2020},
  note = {arXiv: 2002.07101},
  keywords = {flows},
}

@article{feng_noise_2020,
  title = {On {Noise} {Injection} in {Generative} {Adversarial} {Networks}},
  url = {http://arxiv.org/abs/2006.05891},
  abstract = {Noise injection has been proved to be one of the key technique advances in generating high-fidelity images. Despite its successful usage in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on the geodesic normal coordinates. Guided by our theories, we find that the existing method is incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.},
  urldate = {2020-09-05},
  journal = {arXiv:2006.05891 [cs, stat]},
  author = {Feng, Ruili and Zhao, Deli and Zha, Zhengjun},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.05891},
  keywords = {gan},
}

@article{deasy_constraining_2020,
  title = {Constraining {Variational} {Inference} with {Geometric} {Jensen}-{Shannon} {Divergence}},
  url = {http://arxiv.org/abs/2006.10599},
  abstract = {We examine the problem of controlling divergences for latent space regularisation in variational autoencoders. Specifically, when aiming to reconstruct example \$x{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{m\}\$ via latent space \$z{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{n\}\$ (\$n{\textbackslash}leq m\$), while balancing this against the need for generalisable latent representations. We present a regularisation mechanism based on the skew geometric-Jensen-Shannon divergence \${\textbackslash}left({\textbackslash}textrm\{JS\}{\textasciicircum}\{{\textbackslash}textrm\{G\}\_\{{\textbackslash}alpha\}\}{\textbackslash}right)\$. We find a variation in \${\textbackslash}textrm\{JS\}{\textasciicircum}\{{\textbackslash}textrm\{G\}\_\{{\textbackslash}alpha\}\}\$, motivated by limiting cases, which leads to an intuitive interpolation between forward and reverse KL in the space of both distributions and divergences. We motivate its potential benefits for VAEs through low-dimensional examples, before presenting quantitative and qualitative results. Our experiments demonstrate that skewing our variant of \${\textbackslash}textrm\{JS\}{\textasciicircum}\{{\textbackslash}textrm\{G\}\_\{{\textbackslash}alpha\}\}\$, in the context of \${\textbackslash}textrm\{JS\}{\textasciicircum}\{{\textbackslash}textrm\{G\}\_\{{\textbackslash}alpha\}\}\$-VAEs, leads to better reconstruction and generation when compared to several baseline VAEs. Our approach is entirely unsupervised and utilises only one hyperparameter which can be easily interpreted in latent space.},
  urldate = {2020-09-05},
  journal = {arXiv:2006.10599 [cs, stat]},
  author = {Deasy, Jacob and Simidjievski, Nikola and Liò, Pietro},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.10599
version: 1},
  keywords = {variational\_inference},
}

@article{tschannen_mutual_2020,
  title = {On {Mutual} {Information} {Maximization} for {Representation} {Learning}},
  url = {http://arxiv.org/abs/1907.13625},
  abstract = {Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.},
  urldate = {2020-09-05},
  journal = {arXiv:1907.13625 [cs, stat]},
  author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K. and Gelly, Sylvain and Lucic, Mario},
  month = jan,
  year = {2020},
  note = {arXiv: 1907.13625},
  keywords = {mutual\_information, representation\_learning, self-supervised},
}

@article{nagano_wrapped_2019,
  title = {A {Wrapped} {Normal} {Distribution} on {Hyperbolic} {Space} for {Gradient}-{Based} {Learning}},
  url = {http://arxiv.org/abs/1902.02992},
  abstract = {Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called {\textbackslash}textit\{pseudo-hyperbolic Gaussian\}, a Gaussian-like distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.},
  urldate = {2020-09-05},
  journal = {arXiv:1902.02992 [cs, stat]},
  author = {Nagano, Yoshihiro and Yamaguchi, Shoichiro and Fujita, Yasuhiro and Koyama, Masanori},
  month = may,
  year = {2019},
  note = {arXiv: 1902.02992},
  keywords = {hyperbolic},
}

@article{jaderberg_spatial_2016,
  title = {Spatial {Transformer} {Networks}},
  url = {http://arxiv.org/abs/1506.02025},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  urldate = {2020-09-05},
  journal = {arXiv:1506.02025 [cs]},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  month = feb,
  year = {2016},
  note = {arXiv: 1506.02025},
}

@article{schwobel_probabilistic_2020,
  title = {Probabilistic {Spatial} {Transformers} for {Bayesian} {Data} {Augmentation}},
  url = {http://arxiv.org/abs/2004.03637},
  abstract = {High-capacity models require vast amounts of data, and data augmentation is a common remedy when this resource is limited. Standard augmentation techniques apply small hand-tuned transformations to existing data, which is a brittle process that realistically only allows for simple transformations. We propose a Bayesian interpretation of data augmentation where the transformations are modelled as latent variables to be marginalized, and show how these can be inferred variationally in an end-to-end fashion. This allows for significantly more complex transformations than manual tuning, and the marginalization implies a form of test-time data augmentation. The resulting model can be interpreted as a probabilistic extension of spatial transformer networks. Experimentally, we demonstrate improvements in accuracy and uncertainty quantification in image and time series classification tasks.},
  urldate = {2020-09-05},
  journal = {arXiv:2004.03637 [cs, stat]},
  author = {Schwöbel, Pola and Warburg, Frederik and Jørgensen, Martin and Madsen, Kristoffer H. and Hauberg, Søren},
  month = apr,
  year = {2020},
  note = {arXiv: 2004.03637},
  keywords = {bayesian, learn\_data\_aug, transformers},
}

@inproceedings{corneanu_computing_2020,
  address = {Seattle, WA, USA},
  title = {Computing the {Testing} {Error} {Without} a {Testing} {Set}},
  isbn = {978-1-72817-168-5},
  url = {https://ieeexplore.ieee.org/document/9156398/},
  doi = {10.1109/CVPR42600.2020.00275},
  language = {en},
  urldate = {2020-09-05},
  booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Corneanu, Ciprian A. and Escalera, Sergio and Martinez, Aleix M.},
  month = jun,
  year = {2020},
  keywords = {generalization, tda},
  pages = {2674--2682},
}

@inproceedings{feng_self-supervised_2019,
  address = {Long Beach, CA, USA},
  title = {Self-{Supervised} {Representation} {Learning} by {Rotation} {Feature} {Decoupling}},
  isbn = {978-1-72813-293-8},
  url = {https://ieeexplore.ieee.org/document/8953870/},
  doi = {10.1109/CVPR.2019.01061},
  abstract = {We introduce a self-supervised learning method that focuses on beneﬁcial properties of representation and their abilities in generalizing to real-world tasks. The method incorporates rotation invariance into the feature learning framework, one of many good and well-studied properties of visual representation, which is rarely appreciated or exploited by previous deep convolutional neural network based self-supervised representation learning methods. Speciﬁcally, our model learns a split representation that contains both rotation related and unrelated parts. We train neural networks by jointly predicting image rotations and discriminating individual instances. In particular, our model decouples the rotation discrimination from instance discrimination, which allows us to improve the rotation prediction by mitigating the inﬂuence of rotation label noise, as well as discriminate instances without regard to image rotations. The resulting feature has a better generalization ability for more various tasks. Experimental results show that our model outperforms current state-of-the-art methods on standard self-supervised feature learning benchmarks.},
  language = {en},
  urldate = {2020-09-05},
  booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Feng, Zeyu and Xu, Chang and Tao, Dacheng},
  month = jun,
  year = {2019},
  keywords = {disentanglement, representation\_learning, rotation, self\_supervised},
  pages = {10356--10366},
}

@article{yin_fourier_2019,
  title = {A {Fourier} {Perspective} on {Model} {Robustness} in {Computer} {Vision}},
  url = {http://arxiv.org/abs/1906.08988},
  abstract = {Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment, a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C benchmark.},
  urldate = {2020-08-27},
  journal = {arXiv:1906.08988 [cs, stat]},
  author = {Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D. and Gilmer, Justin},
  month = oct,
  year = {2019},
  note = {arXiv: 1906.08988},
  keywords = {adversarial, data\_aug, fourier, robustness},
}

@article{purushwalkam_demystifying_2020,
  title = {Demystifying {Contrastive} {Self}-{Supervised} {Learning}: {Invariances}, {Augmentations} and {Dataset} {Biases}},
  shorttitle = {Demystifying {Contrastive} {Self}-{Supervised} {Learning}},
  url = {http://arxiv.org/abs/2007.13916},
  abstract = {Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classification. Somewhat mysteriously the recent gains in performance come from training instance classification models, treating each image and it's augmented versions as samples of a single class. In this work, we first present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO and PIRL learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classification and semantic segmentation tasks.},
  urldate = {2020-08-27},
  journal = {arXiv:2007.13916 [cs]},
  author = {Purushwalkam, Senthil and Gupta, Abhinav},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.13916},
  keywords = {contrastive, invariance},
}

@article{mishkin_systematic_2017,
  title = {Systematic evaluation of {CNN} advances on the {ImageNet}},
  volume = {161},
  issn = {10773142},
  url = {http://arxiv.org/abs/1606.02228},
  doi = {10.1016/j.cviu.2017.05.007},
  abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
  urldate = {2020-08-25},
  journal = {Computer Vision and Image Understanding},
  author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
  month = aug,
  year = {2017},
  note = {arXiv: 1606.02228},
  keywords = {empirical},
  pages = {11--19},
}

@article{hospedales_meta-learning_2020,
  title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
  shorttitle = {Meta-{Learning} in {Neural} {Networks}},
  url = {http://arxiv.org/abs/2004.05439},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  urldate = {2020-08-25},
  journal = {arXiv:2004.05439 [cs, stat]},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  month = apr,
  year = {2020},
  note = {arXiv: 2004.05439},
  keywords = {meta-learning},
}

@incollection{goos_surprising_2001,
  address = {Berlin, Heidelberg},
  title = {On the {Surprising} {Behavior} of {Distance} {Metrics} in {High} {Dimensional} {Space}},
  volume = {1973},
  isbn = {978-3-540-41456-8 978-3-540-44503-6},
  url = {http://link.springer.com/10.1007/3-540-44503-X_27},
  abstract = {In recent years, the eﬀect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a eﬃciency and/or eﬀectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We speciﬁcally examine the behavior of the commonly used Lk norm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric (L1 norm) is consistently more preferable than the Euclidean distance metric (L2 norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lk norm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can signiﬁcantly improve the eﬀectiveness of standard clustering algorithms such as the k-means algorithm.},
  language = {en},
  urldate = {2020-08-22},
  booktitle = {Database {Theory} — {ICDT} 2001},
  publisher = {Springer Berlin Heidelberg},
  author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
  editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Van den Bussche, Jan and Vianu, Victor},
  year = {2001},
  doi = {10.1007/3-540-44503-X_27},
  note = {Series Title: Lecture Notes in Computer Science},
  keywords = {high-dimensional, metrics},
  pages = {420--434},
}

@article{hoffer_norm_2019,
  title = {Norm matters: efficient and accurate normalization schemes in deep networks},
  shorttitle = {Norm matters},
  url = {http://arxiv.org/abs/1803.01814},
  abstract = {Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used \$L{\textasciicircum}2\$ batch-norm, using normalization in \$L{\textasciicircum}1\$ and \$L{\textasciicircum}{\textbackslash}infty\$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.},
  urldate = {2020-08-16},
  journal = {arXiv:1803.01814 [cs, stat]},
  author = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  month = feb,
  year = {2019},
  note = {arXiv: 1803.01814},
  keywords = {normalization},
}

@article{hendrycks_many_2020,
  title = {The {Many} {Faces} of {Robustness}: {A} {Critical} {Analysis} of {Out}-of-{Distribution} {Generalization}},
  shorttitle = {The {Many} {Faces} of {Robustness}},
  url = {http://arxiv.org/abs/2006.16241},
  abstract = {We introduce three new robustness benchmarks consisting of naturally occurring distribution changes in image style, geographic location, camera operation, and more. Using our benchmarks, we take stock of previously proposed hypotheses for out-of-distribution robustness and put them to the test. We find that using larger models and synthetic data augmentation can improve robustness on real-world distribution shifts, contrary to claims in prior work. Motivated by this, we introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000x more labeled data. We find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Hence no evaluated method consistently improves robustness. We conclude that future research must study multiple distribution shifts simultaneously.},
  urldate = {2020-08-15},
  journal = {arXiv:2006.16241 [cs, stat]},
  author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
  month = aug,
  year = {2020},
  note = {arXiv: 2006.16241},
  keywords = {data\_aug, generalization, ood, robustness},
}

@article{sax_learning_2019,
  title = {Learning to {Navigate} {Using} {Mid}-{Level} {Visual} {Priors}},
  url = {http://arxiv.org/abs/1912.11121},
  abstract = {How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. navigating a complex environment)? What are the consequences of not utilizing such visual priors in learning? We study these questions by integrating a generic perceptual skill set (a distance estimator, an edge detector, etc.) within a reinforcement learning framework (see Fig. 1). This skill set ("mid-level vision") provides the policy with a more processed state of the world compared to raw images. Our large-scale study demonstrates that using mid-level vision results in policies that learn faster, generalize better, and achieve higher final performance, when compared to learning from scratch and/or using state-of-the-art visual and non-visual representation learning methods. We show that conventional computer vision objectives are particularly effective in this regard and can be conveniently integrated into reinforcement learning frameworks. Finally, we found that no single visual representation was universally useful for all downstream tasks, hence we computationally derive a task-agnostic set of representations optimized to support arbitrary downstream tasks.},
  urldate = {2020-08-14},
  journal = {arXiv:1912.11121 [cs]},
  author = {Sax, Alexander and Zhang, Jeffrey O. and Emi, Bradley and Zamir, Amir and Savarese, Silvio and Guibas, Leonidas and Malik, Jitendra},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.11121},
  keywords = {mid-level},
}

@article{wang_low-shot_2018,
  title = {Low-{Shot} {Learning} from {Imaginary} {Data}},
  url = {http://arxiv.org/abs/1801.05401},
  abstract = {Humans can quickly learn new visual concepts, perhaps because they can easily visualize or imagine what novel objects look like from different views. Incorporating this ability to hallucinate novel instances of new concepts might help machine vision systems perform better low-shot learning, i.e., learning concepts from few examples. We present a novel approach to low-shot learning that uses this idea. Our approach builds on recent progress in meta-learning ("learning to learn") by combining a meta-learner with a "hallucinator" that produces additional training examples, and optimizing both models jointly. Our hallucinator can be incorporated into a variety of meta-learners and provides significant gains: up to a 6 point boost in classification accuracy when only a single training example is available, yielding state-of-the-art performance on the challenging ImageNet low-shot classification benchmark.},
  urldate = {2020-08-13},
  journal = {arXiv:1801.05401 [cs]},
  author = {Wang, Yu-Xiong and Girshick, Ross and Hebert, Martial and Hariharan, Bharath},
  month = apr,
  year = {2018},
  note = {arXiv: 1801.05401},
  keywords = {few-shot, learn\_data\_aug, meta-learning},
}

@article{hendrycks_many_2020-1,
  title = {The {Many} {Faces} of {Robustness}: {A} {Critical} {Analysis} of {Out}-of-{Distribution} {Generalization}},
  shorttitle = {The {Many} {Faces} of {Robustness}},
  url = {http://arxiv.org/abs/2006.16241},
  abstract = {We introduce three new robustness benchmarks consisting of naturally occurring distribution changes in image style, geographic location, camera operation, and more. Using our benchmarks, we take stock of previously proposed hypotheses for out-of-distribution robustness and put them to the test. We find that using larger models and synthetic data augmentation can improve robustness on real-world distribution shifts, contrary to claims in prior work. Motivated by this, we introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000x more labeled data. We find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. We conclude that future research must study multiple distribution shifts simultaneously.},
  urldate = {2020-08-13},
  journal = {arXiv:2006.16241 [cs, stat]},
  author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.16241},
  keywords = {ood, robustness},
}

@article{detlefsen_explicit_2019,
  title = {Explicit {Disentanglement} of {Appearance} and {Perspective} in {Generative} {Models}},
  url = {http://arxiv.org/abs/1906.11881},
  abstract = {Disentangled representation learning finds compact, independent and easy-to-interpret factors of the data. Learning such has been shown to require an inductive bias, which we explicitly encode in a generative model of images. Specifically, we propose a model with two latent spaces: one that represents spatial transformations of the input data, and another that represents the transformed data. We find that the latter naturally captures the intrinsic appearance of the data. To realize the generative model, we propose a Variationally Inferred Transformational Autoencoder (VITAE) that incorporates a spatial transformer into a variational autoencoder. We show how to perform inference in the model efficiently by carefully designing the encoders and restricting the transformation class to be diffeomorphic. Empirically, our model separates the visual style from digit type on MNIST, separates shape and pose in images of human bodies and facial features from facial shape on CelebA.},
  urldate = {2020-08-13},
  journal = {arXiv:1906.11881 [cs, stat]},
  author = {Detlefsen, Nicki Skafte and Hauberg, Søren},
  month = nov,
  year = {2019},
  note = {arXiv: 1906.11881},
  keywords = {disentanglement, vae},
}

@article{bepler_explicitly_2019,
  title = {Explicitly disentangling image content from translation and rotation with spatial-{VAE}},
  url = {http://arxiv.org/abs/1909.11663},
  abstract = {Given an image dataset, we are often interested in finding data generative factors that encode semantic content independently from pose variables such as rotation and translation. However, current disentanglement approaches do not impose any specific structure on the learned latent representations. We propose a method for explicitly disentangling image rotation and translation from other unstructured latent factors in a variational autoencoder (VAE) framework. By formulating the generative model as a function of the spatial coordinate, we make the reconstruction error differentiable with respect to latent translation and rotation parameters. This formulation allows us to train a neural network to perform approximate inference on these latent variables while explicitly constraining them to only represent rotation and translation. We demonstrate that this framework, termed spatial-VAE, effectively learns latent representations that disentangle image rotation and translation from content and improves reconstruction over standard VAEs on several benchmark datasets, including applications to modeling continuous 2-D views of proteins from single particle electron microscopy and galaxies in astronomical images.},
  urldate = {2020-08-13},
  journal = {arXiv:1909.11663 [cs, q-bio]},
  author = {Bepler, Tristan and Zhong, Ellen D. and Kelley, Kotaro and Brignole, Edward and Berger, Bonnie},
  month = sep,
  year = {2019},
  note = {arXiv: 1909.11663},
  keywords = {disentanglement, vae},
}

@inproceedings{kornblith_better_2019,
  address = {Long Beach, CA, USA},
  title = {Do {Better} {ImageNet} {Models} {Transfer} {Better}?},
  isbn = {978-1-72813-293-8},
  url = {https://ieeexplore.ieee.org/document/8954384/},
  doi = {10.1109/CVPR.2019.00277},
  abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classiﬁcation networks on 12 image classiﬁcation datasets. We ﬁnd that, when networks are used as ﬁxed feature extractors or ﬁne-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we ﬁnd that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we ﬁnd that, on two small ﬁne-grained image classiﬁcation datasets, pretraining on ImageNet provides minimal beneﬁts, indicating the learned features from ImageNet do not transfer well to ﬁne-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
  language = {en},
  urldate = {2020-08-11},
  booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
  month = jun,
  year = {2019},
  keywords = {imagenet, reality\_check, transfer\_learning},
  pages = {2656--2666},
}

@article{mordatch_concept_2018,
  title = {Concept {Learning} with {Energy}-{Based} {Models}},
  url = {http://arxiv.org/abs/1811.02486},
  abstract = {Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at sites.google.com/site/energyconceptmodels},
  urldate = {2020-08-11},
  journal = {arXiv:1811.02486 [cs]},
  author = {Mordatch, Igor},
  month = nov,
  year = {2018},
  note = {arXiv: 1811.02486},
  keywords = {attention, ebms},
}

@article{ghosh_variational_2020,
  title = {From {Variational} to {Deterministic} {Autoencoders}},
  url = {http://arxiv.org/abs/1903.12436},
  abstract = {Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. {\textbackslash}footnote\{An implementation is available at: {\textbackslash}url\{https://github.com/ParthaEth/Regularized\_autoencoders-RAE-\}\}},
  urldate = {2020-08-07},
  journal = {arXiv:1903.12436 [cs, stat]},
  author = {Ghosh, Partha and Sajjadi, Mehdi S. M. and Vergari, Antonio and Black, Michael and Schölkopf, Bernhard},
  month = may,
  year = {2020},
  note = {arXiv: 1903.12436},
  keywords = {rae, vae, wae},
}

@article{wang_riemannian_2019,
  title = {Riemannian {Normalizing} {Flow} on {Variational} {Wasserstein} {Autoencoder} for {Text} {Modeling}},
  url = {http://arxiv.org/abs/1904.02399},
  abstract = {Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem, also known as the Kullback-Leibler (KL) term vanishing issue, where the posterior easily collapses to the prior, and the model will ignore latent codes in generative tasks. To address this problem, we introduce an improved Wasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow (RNF) for text modeling. The RNF transforms a latent variable into a space that respects the geometric characteristics of input space, which makes posterior impossible to collapse to the non-informative prior. The Wasserstein objective minimizes the distance between the marginal distribution and the prior directly and therefore does not force the posterior to match the prior. Empirical experiments show that our model avoids KL vanishing over a range of datasets and has better performances in tasks such as language modeling, likelihood approximation, and text generation. Through a series of experiments and analysis over latent space, we show that our model learns latent distributions that respect latent space geometry and is able to generate sentences that are more diverse.},
  urldate = {2020-08-07},
  journal = {arXiv:1904.02399 [cs]},
  author = {Wang, Prince Zizhuang and Wang, William Yang},
  month = apr,
  year = {2019},
  note = {arXiv: 1904.02399},
  keywords = {diffgeo, flows, wae},
}

@article{orhan_self-supervised_2020,
  title = {Self-supervised learning through the eyes of a child},
  url = {http://arxiv.org/abs/2007.16189},
  abstract = {Within months of birth, children have meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly defined domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of several young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic self-supervised learning objectives.},
  urldate = {2020-08-06},
  journal = {arXiv:2007.16189 [cs]},
  author = {Orhan, A. Emin and Gupta, Vaibhav V. and Lake, Brenden M.},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.16189},
  keywords = {cogsci, self-supervised},
}

@article{arvanitidis_latent_2018,
  title = {Latent {Space} {Oddity}: on the {Curvature} of {Deep} {Generative} {Models}},
  shorttitle = {Latent {Space} {Oddity}},
  url = {http://arxiv.org/abs/1710.11379},
  abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear "generator" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.},
  urldate = {2020-08-06},
  journal = {arXiv:1710.11379 [stat]},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, Søren},
  month = jan,
  year = {2018},
  note = {arXiv: 1710.11379},
  keywords = {diffgeo, gen\_models},
}

@article{peng_domain_2019,
  title = {Domain {Agnostic} {Learning} with {Disentangled} {Representations}},
  url = {http://arxiv.org/abs/1904.12347},
  abstract = {Unsupervised model transfer has the potential to greatly improve the generalizability of deep models to novel domains. Yet the current literature assumes that the separation of target data into distinct domains is known as a priori. In this paper, we propose the task of Domain-Agnostic Learning (DAL): How to transfer knowledge from a labeled source domain to unlabeled data from arbitrary target domains? To tackle this problem, we devise a novel Deep Adversarial Disentangled Autoencoder (DADA) capable of disentangling domain-specific features from class identity. We demonstrate experimentally that when the target domain labels are unknown, DADA leads to state-of-the-art performance on several image classification datasets.},
  urldate = {2020-08-04},
  journal = {arXiv:1904.12347 [cs]},
  author = {Peng, Xingchao and Huang, Zijun and Sun, Ximeng and Saenko, Kate},
  month = apr,
  year = {2019},
  note = {arXiv: 1904.12347},
  keywords = {disentanglement, domain\_adaptation, mutual\_information},
}

@article{rubenstein_latent_2018,
  title = {On the {Latent} {Space} of {Wasserstein} {Auto}-{Encoders}},
  url = {http://arxiv.org/abs/1802.03761},
  abstract = {We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders. We highlight the potential of WAEs for representation learning with promising results on a benchmark disentanglement task.},
  urldate = {2020-08-04},
  journal = {arXiv:1802.03761 [cs, stat]},
  author = {Rubenstein, Paul K. and Schoelkopf, Bernhard and Tolstikhin, Ilya},
  month = feb,
  year = {2018},
  note = {arXiv: 1802.03761},
  keywords = {disentanglement, wae},
}

@article{ganin_unsupervised_2015,
  title = {Unsupervised {Domain} {Adaptation} by {Backpropagation}},
  url = {http://arxiv.org/abs/1409.7495},
  abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
  urldate = {2020-08-04},
  journal = {arXiv:1409.7495 [cs, stat]},
  author = {Ganin, Yaroslav and Lempitsky, Victor},
  month = feb,
  year = {2015},
  note = {arXiv: 1409.7495},
  keywords = {domain\_adaptation},
}

@article{papamakarios_normalizing_2019,
  title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
  url = {http://arxiv.org/abs/1912.02762},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  urldate = {2020-08-04},
  journal = {arXiv:1912.02762 [cs, stat]},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.02762},
  keywords = {flows, gen\_models},
}

@article{rubenstein_latent_2018-1,
  title = {On the {Latent} {Space} of {Wasserstein} {Auto}-{Encoders}},
  url = {https://arxiv.org/abs/1802.03761v1},
  abstract = {We study the role of latent space dimensionality in Wasserstein auto-encoders
(WAEs). Through experimentation on synthetic and real datasets, we argue that
random encoders should be preferred over deterministic encoders. We highlight
the potential of WAEs for representation learning with promising results on a
benchmark disentanglement task.},
  language = {en},
  urldate = {2020-08-02},
  author = {Rubenstein, Paul K. and Schoelkopf, Bernhard and Tolstikhin, Ilya},
  month = feb,
  year = {2018},
  keywords = {disentanglement, gen\_models, wae},
}

@article{rubenstein_latent_nodate,
  title = {On the {Latent} {Space} of {Wasserstein} {Auto}-{Encoders}},
  abstract = {We study the role of latent space dimensionality in Wasserstein auto-encoders (WAEs). Through experimentation on synthetic and real datasets, we argue that random encoders should be preferred over deterministic encoders. We highlight the potential of WAEs for representation learning with promising results on a benchmark disentanglement task.},
  language = {en},
  author = {Rubenstein, Paul K and Schölkopf, Bernhard and Tolstikhin, Ilya},
  pages = {9},
}

@article{patrini_sinkhorn_2019,
  title = {Sinkhorn {AutoEncoders}},
  url = {http://arxiv.org/abs/1810.01118},
  abstract = {Optimal transport offers an alternative to maximum likelihood for learning generative autoencoding models. We show that minimizing the p-Wasserstein distance between the generator and the true data distribution is equivalent to the unconstrained min-min optimization of the p-Wasserstein distance between the encoder aggregated posterior and the prior in latent space, plus a reconstruction error. We also identify the role of its trade-off hyperparameter as the capacity of the generator: its Lipschitz constant. Moreover, we prove that optimizing the encoder over any class of universal approximators, such as deterministic neural networks, is enough to come arbitrarily close to the optimum. We therefore advertise this framework, which holds for any metric space and prior, as a sweet-spot of current generative autoencoding objectives. We then introduce the Sinkhorn auto-encoder (SAE), which approximates and minimizes the p-Wasserstein distance in latent space via backprogation through the Sinkhorn algorithm. SAE directly works on samples, i.e. it models the aggregated posterior as an implicit distribution, with no need for a reparameterization trick for gradients estimations. SAE is thus able to work with different metric spaces and priors with minimal adaptations. We demonstrate the flexibility of SAE on latent spaces with different geometries and priors and compare with other methods on benchmark data sets.},
  urldate = {2020-08-01},
  journal = {arXiv:1810.01118 [cs, stat]},
  author = {Patrini, Giorgio and Berg, Rianne van den and Forré, Patrick and Carioni, Marcello and Bhargav, Samarth and Welling, Max and Genewein, Tim and Nielsen, Frank},
  month = jul,
  year = {2019},
  note = {arXiv: 1810.01118},
  keywords = {autoencoder, hypersphere, optimal\_transport, vae},
}

@article{de_cao_power_2020,
  title = {The {Power} {Spherical} distribution},
  url = {http://arxiv.org/abs/2006.04437},
  abstract = {There is a growing interest in probabilistic models defined in hyper-spherical spaces, be it to accommodate observed data or latent structure. The von Mises-Fisher (vMF) distribution, often regarded as the Normal distribution on the hyper-sphere, is a standard modeling choice: it is an exponential family and thus enjoys important statistical results, for example, known Kullback-Leibler (KL) divergence from other vMF distributions. Sampling from a vMF distribution, however, requires a rejection sampling procedure which besides being slow poses difficulties in the context of stochastic backpropagation via the reparameterization trick. Moreover, this procedure is numerically unstable for certain vMFs, e.g., those with high concentration and/or in high dimensions. We propose a novel distribution, the Power Spherical distribution, which retains some of the important aspects of the vMF (e.g., support on the hyper-sphere, symmetry about its mean direction parameter, known KL from other vMF distributions) while addressing its main drawbacks (i.e., scalability and numerical stability). We demonstrate the stability of Power Spherical distributions with a numerical experiment and further apply it to a variational auto-encoder trained on MNIST. Code at: https://github.com/nicola-decao/power\_spherical},
  urldate = {2020-08-01},
  journal = {arXiv:2006.04437 [cs, stat]},
  author = {De Cao, Nicola and Aziz, Wilker},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.04437},
  keywords = {hypersphere, probability},
}

@article{kolouri_sliced-wasserstein_2018,
  title = {Sliced-{Wasserstein} {Autoencoder}: {An} {Embarrassingly} {Simple} {Generative} {Model}},
  shorttitle = {Sliced-{Wasserstein} {Autoencoder}},
  url = {http://arxiv.org/abs/1804.01947},
  abstract = {In this paper we study generative modeling via autoencoders while using the elegant geometric properties of the optimal transport (OT) problem and the Wasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE), which are generative models that enable one to shape the distribution of the latent space into any samplable probability distribution without the need for training an adversarial network or defining a closed-form for the distribution. In short, we regularize the autoencoder loss with the sliced-Wasserstein distance between the distribution of the encoded training samples and a predefined samplable distribution. We show that the proposed formulation has an efficient numerical solution that provides similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an embarrassingly simple implementation.},
  urldate = {2020-07-30},
  journal = {arXiv:1804.01947 [cs, stat]},
  author = {Kolouri, Soheil and Pope, Phillip E. and Martin, Charles E. and Rohde, Gustavo K.},
  month = jun,
  year = {2018},
  note = {arXiv: 1804.01947},
  keywords = {mmd, optimal\_transport, vae},
}

@article{antoniou_assume_2019,
  title = {Assume, {Augment} and {Learn}: {Unsupervised} {Few}-{Shot} {Meta}-{Learning} via {Random} {Labels} and {Data} {Augmentation}},
  shorttitle = {Assume, {Augment} and {Learn}},
  url = {http://arxiv.org/abs/1902.09884},
  abstract = {The field of few-shot learning has been laboriously explored in the supervised setting, where per-class labels are available. On the other hand, the unsupervised few-shot learning setting, where no labels of any kind are required, has seen little investigation. We propose a method, named Assume, Augment and Learn or AAL, for generating few-shot tasks using unlabeled data. We randomly label a random subset of images from an unlabeled dataset to generate a support set. Then by applying data augmentation on the support set's images, and reusing the support set's labels, we obtain a target set. The resulting few-shot tasks can be used to train any standard meta-learning framework. Once trained, such a model, can be directly applied on small real-labeled datasets without any changes or fine-tuning required. In our experiments, the learned models achieve good generalization performance in a variety of established few-shot learning tasks on Omniglot and Mini-Imagenet.},
  urldate = {2020-07-30},
  journal = {arXiv:1902.09884 [cs, stat]},
  author = {Antoniou, Antreas and Storkey, Amos},
  month = mar,
  year = {2019},
  note = {arXiv: 1902.09884},
  keywords = {data\_aug, meta-learning},
}

@article{jabri_unsupervised_2019,
  title = {Unsupervised {Curricula} for {Visual} {Meta}-{Reinforcement} {Learning}},
  url = {http://arxiv.org/abs/1912.04226},
  abstract = {In principle, meta-reinforcement learning algorithms leverage experience across many tasks to learn fast reinforcement learning (RL) strategies that transfer to similar tasks. However, current meta-RL approaches rely on manually-defined distributions of training tasks, and hand-crafting these task distributions can be challenging and time-consuming. Can "useful" pre-training tasks be discovered in an unsupervised manner? We develop an unsupervised algorithm for inducing an adaptive meta-training task distribution, i.e. an automatic curriculum, by modeling unsupervised interaction in a visual environment. The task distribution is scaffolded by a parametric density model of the meta-learner's trajectory distribution. We formulate unsupervised meta-RL as information maximization between a latent task variable and the meta-learner's data distribution, and describe a practical instantiation which alternates between integration of recent experience into the task distribution and meta-learning of the updated tasks. Repeating this procedure leads to iterative reorganization such that the curriculum adapts as the meta-learner's data distribution shifts. In particular, we show how discriminative clustering for visual representation can support trajectory-level task acquisition and exploration in domains with pixel observations, avoiding pitfalls of alternatives. In experiments on vision-based navigation and manipulation domains, we show that the algorithm allows for unsupervised meta-learning that transfers to downstream tasks specified by hand-crafted reward functions and serves as pre-training for more efficient supervised meta-learning of test task distributions.},
  urldate = {2020-07-30},
  journal = {arXiv:1912.04226 [cs]},
  author = {Jabri, Allan and Hsu, Kyle and Eysenbach, Ben and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.04226},
  keywords = {curriculum\_learning, meta-learning},
}

@article{chen_image_2019,
  title = {Image {Deformation} {Meta}-{Networks} for {One}-{Shot} {Learning}},
  url = {http://arxiv.org/abs/1905.11641},
  abstract = {Humans can robustly learn novel visual concepts even when images undergo various deformations and lose certain information. Mimicking the same behavior and synthesizing deformed instances of new concepts may help visual recognition systems perform better one-shot learning, i.e., learning concepts from one or few examples. Our key insight is that, while the deformed images may not be visually realistic, they still maintain critical semantic information and contribute significantly to formulating classifier decision boundaries. Inspired by the recent progress of meta-learning, we combine a meta-learner with an image deformation sub-network that produces additional training examples, and optimize both models in an end-to-end manner. The deformation sub-network learns to deform images by fusing a pair of images --- a probe image that keeps the visual content and a gallery image that diversifies the deformations. We demonstrate results on the widely used one-shot learning benchmarks (miniImageNet and ImageNet 1K Challenge datasets), which significantly outperform state-of-the-art approaches. Code is available at https://github.com/tankche1/IDeMe-Net.},
  urldate = {2020-07-30},
  journal = {arXiv:1905.11641 [cs]},
  author = {Chen, Zitian and Fu, Yanwei and Wang, Yu-Xiong and Ma, Lin and Liu, Wei and Hebert, Martial},
  month = jul,
  year = {2019},
  note = {arXiv: 1905.11641},
  keywords = {learn\_data\_aug, meta-learning, one-shot},
}

@article{zhao_latent_2020,
  title = {Latent {Variables} on {Spheres} for {Autoencoders} in {High} {Dimensions}},
  url = {http://arxiv.org/abs/1912.10233},
  abstract = {Variational Auto-Encoder (VAE) has been widely applied as a fundamental generative model in machine learning. For complex samples like imagery objects or scenes, however, VAE suffers from the dimensional dilemma between reconstruction precision that needs high-dimensional latent codes and probabilistic inference that favors a low-dimensional latent space. By virtue of high-dimensional geometry, we propose a very simple algorithm, called Spherical Auto-Encoder (SAE), completely different from existing VAEs to address the issue. SAE is in essence the vanilla autoencoder with spherical normalization on the latent space. We analyze the unique characteristics of random variables on spheres in high dimensions and argue that random variables on spheres are agnostic to various prior distributions and data modes when the dimension is sufficiently high. Therefore, SAE can harness a high-dimensional latent space to improve the inference precision of latent codes while maintain the property of stochastic sampling from priors. The experiments on sampling and inference validate our theoretical analysis and the superiority of SAE.},
  urldate = {2020-07-30},
  journal = {arXiv:1912.10233 [cs, stat]},
  author = {Zhao, Deli and Zhu, Jiapeng and Zhang, Bo},
  month = feb,
  year = {2020},
  note = {arXiv: 1912.10233},
  keywords = {hypersphere, vae},
}

@article{wong_learning_2020,
  title = {Learning perturbation sets for robust machine learning},
  url = {http://arxiv.org/abs/2007.08450},
  abstract = {Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline digit transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to learn models which have improved generalization performance and are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at https://github.com/locuslab/perturbation\_learning.},
  urldate = {2020-07-30},
  journal = {arXiv:2007.08450 [cs, stat]},
  author = {Wong, Eric and Kolter, J. Zico},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.08450},
  keywords = {adversarial, learn\_data\_aug, robustness},
}

@incollection{ma_macow_2019,
  title = {{MaCow}: {Masked} {Convolutional} {Generative} {Flow}},
  shorttitle = {{MaCow}},
  url = {http://papers.nips.cc/paper/8824-macow-masked-convolutional-generative-flow.pdf},
  urldate = {2020-07-30},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
  publisher = {Curran Associates, Inc.},
  author = {Ma, Xuezhe and Kong, Xiang and Zhang, Shanghang and Hovy, Eduard},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
  year = {2019},
  keywords = {flows},
  pages = {5893--5902},
}

@article{gemici_normalizing_2016,
  title = {Normalizing {Flows} on {Riemannian} {Manifolds}},
  url = {http://arxiv.org/abs/1611.02304},
  abstract = {We consider the problem of density estimation on Riemannian manifolds. Density estimation on manifolds has many applications in fluid-mechanics, optics and plasma physics and it appears often when dealing with angular variables (such as used in protein folding, robot limbs, gene-expression) and in general directional statistics. In spite of the multitude of algorithms available for density estimation in the Euclidean spaces \${\textbackslash}mathbf\{R\}{\textasciicircum}n\$ that scale to large n (e.g. normalizing flows, kernel methods and variational approximations), most of these methods are not immediately suitable for density estimation in more general Riemannian manifolds. We revisit techniques related to homeomorphisms from differential geometry for projecting densities to sub-manifolds and use it to generalize the idea of normalizing flows to more general Riemannian manifolds. The resulting algorithm is scalable, simple to implement and suitable for use with automatic differentiation. We demonstrate concrete examples of this method on the n-sphere \${\textbackslash}mathbf\{S\}{\textasciicircum}n\$.},
  urldate = {2020-07-30},
  journal = {arXiv:1611.02304 [cs, math, stat]},
  author = {Gemici, Mevlana C. and Rezende, Danilo and Mohamed, Shakir},
  month = nov,
  year = {2016},
  note = {arXiv: 1611.02304},
  keywords = {diffgeo, flows, hypersphere},
}

@article{genevay_learning_nodate,
  title = {Learning {Generative} {Models} with {Sinkhorn} {Divergences}},
  language = {en},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  keywords = {gen\_models, optimal\_transport},
  pages = {10},
}

@article{liu_sphereface_2018,
  title = {{SphereFace}: {Deep} {Hypersphere} {Embedding} for {Face} {Recognition}},
  shorttitle = {{SphereFace}},
  url = {http://arxiv.org/abs/1704.08063},
  abstract = {This paper addresses deep face recognition (FR) problem under open-set protocol, where ideal face features are expected to have smaller maximal intra-class distance than minimal inter-class distance under a suitably chosen metric space. However, few existing algorithms can effectively achieve this criterion. To this end, we propose the angular softmax (A-Softmax) loss that enables convolutional neural networks (CNNs) to learn angularly discriminative features. Geometrically, A-Softmax loss can be viewed as imposing discriminative constraints on a hypersphere manifold, which intrinsically matches the prior that faces also lie on a manifold. Moreover, the size of angular margin can be quantitatively adjusted by a parameter \$m\$. We further derive specific \$m\$ to approximate the ideal feature criterion. Extensive analysis and experiments on Labeled Face in the Wild (LFW), Youtube Faces (YTF) and MegaFace Challenge show the superiority of A-Softmax loss in FR tasks. The code has also been made publicly available.},
  urldate = {2020-07-30},
  journal = {arXiv:1704.08063 [cs]},
  author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
  month = jan,
  year = {2018},
  note = {arXiv: 1704.08063},
  keywords = {hypersphere},
}

@article{gorban_blessing_2018,
  title = {Blessing of dimensionality: mathematical foundations of the statistical physics of data},
  volume = {376},
  issn = {1364-503X},
  shorttitle = {Blessing of dimensionality},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5869543/},
  doi = {10.1098/rsta.2017.0237},
  abstract = {The concentrations of measure phenomena were discovered as the mathematical background to statistical mechanics at the end of the nineteenth/beginning of the twentieth century and have been explored in mathematics ever since. At the beginning of the twenty-first century, it became clear that the proper utilization of these phenomena in machine learning might transform the curse of dimensionality into the blessing of dimensionality. This paper summarizes recently discovered phenomena of measure concentration which drastically simplify some machine learning problems in high dimension, and allow us to correct legacy artificial intelligence systems. The classical concentration of measure theorems state that i.i.d. random points are concentrated in a thin layer near a surface (a sphere or equators of a sphere, an average or median-level set of energy or another Lipschitz function, etc.). The new stochastic separation theorems describe the thin structure of these thin layers: the random points are not only concentrated in a thin layer but are all linearly separable from the rest of the set, even for exponentially large random sets. The linear functionals for separation of points can be selected in the form of the linear Fisher’s discriminant. All artificial intelligence systems make errors. Non-destructive correction requires separation of the situations (samples) with errors from the samples corresponding to correct behaviour by a simple and robust classifier. The stochastic separation theorems provide us with such classifiers and determine a non-iterative (one-shot) procedure for their construction., This article is part of the theme issue ‘Hilbert’s sixth problem’.},
  number = {2118},
  urldate = {2020-07-30},
  journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
  author = {Gorban, A. N. and Tyukin, I. Y.},
  month = apr,
  year = {2018},
  pmid = {29555807},
  pmcid = {PMC5869543},
  keywords = {high\_dimensional},
}

@article{baradel_cophy_2020,
  title = {{CoPhy}: {Counterfactual} {Learning} of {Physical} {Dynamics}},
  shorttitle = {{CoPhy}},
  url = {http://arxiv.org/abs/1909.12000},
  abstract = {Understanding causes and effects in mechanical systems is an essential component of reasoning in the physical world. This work poses a new problem of counterfactual learning of object mechanics from visual input. We develop the CoPhy benchmark to assess the capacity of the state-of-the-art models for causal physical reasoning in a synthetic 3D environment and propose a model for learning the physical dynamics in a counterfactual setting. Having observed a mechanical experiment that involves, for example, a falling tower of blocks, a set of bouncing balls or colliding objects, we learn to predict how its outcome is affected by an arbitrary intervention on its initial conditions, such as displacing one of the objects in the scene. The alternative future is predicted given the altered past and a latent representation of the confounders learned by the model in an end-to-end fashion with no supervision. We compare against feedforward video prediction baselines and show how observing alternative experiences allows the network to capture latent physical properties of the environment, which results in significantly more accurate predictions at the level of super human performance.},
  urldate = {2020-07-30},
  journal = {arXiv:1909.12000 [cs]},
  author = {Baradel, Fabien and Neverova, Natalia and Mille, Julien and Mori, Greg and Wolf, Christian},
  month = apr,
  year = {2020},
  note = {arXiv: 1909.12000},
  keywords = {causality, intuitive\_physics, physics},
}

@misc{noauthor_minimization_nodate,
  title = {Minimization of {Mutual} {Information}},
  url = {http://fourier.eng.hmc.edu/e161/lectures/ica/node5.html},
  urldate = {2020-07-28},
  keywords = {jacobian},
}

@incollection{coudene_computing_2016,
  address = {London},
  title = {Computing {Entropy}},
  isbn = {978-1-4471-7285-7 978-1-4471-7287-1},
  url = {http://link.springer.com/10.1007/978-1-4471-7287-1_12},
  language = {en},
  urldate = {2020-07-28},
  booktitle = {Ergodic {Theory} and {Dynamical} {Systems}},
  publisher = {Springer London},
  author = {Coudène, Yves},
  collaborator = {Coudène, Yves},
  year = {2016},
  doi = {10.1007/978-1-4471-7287-1_12},
  note = {Series Title: Universitext},
  keywords = {jacobian},
  pages = {123--132},
}

@article{brezis_jacobian_2011,
  title = {The {Jacobian} determinant revisited},
  volume = {185},
  doi = {10.1007/s00222-010-0300-9},
  journal = {Inventiones mathematicae},
  author = {Brezis, Haim and Nguyen, Hoai-Minh},
  month = jul,
  year = {2011},
  keywords = {jacobian},
}

@article{eggers_r_nodate,
  title = {r -priors: {Towards} unbiased model comparison in {Gaussian} linear regression problems},
  language = {en},
  author = {Eggers, Hans and de Kock, Michiel},
  keywords = {hypersphere},
  pages = {40},
}

@article{ben-israel_change--variables_1999,
  title = {The {Change}-of-{Variables} {Formula} {Using} {Matrix} {Volume}},
  volume = {21},
  doi = {10.1137/S0895479895296896},
  abstract = {The matrix volume is a generalization, to rectangular matrices, of the absolute value of the determinant. In particular, the matrix volume can be used in change-of-variables formulæ, instead of the determinant (if the Jacobi matrix of the underlying transformation is rectangular). This result is applicable to integration on surfaces, illustrated here by several examples. SIAM J. Matrix Analysis 21(1999), 300‐312},
  journal = {Siam Journal on Matrix Analysis and Applications - SIAM J MATRIX ANAL APPLICAT},
  author = {Ben-Israel, Adi},
  month = jan,
  year = {1999},
  keywords = {jacobian},
}

@article{salimans_improving_2018,
  title = {Improving {GANs} {Using} {Optimal} {Transport}},
  url = {http://arxiv.org/abs/1803.05573},
  abstract = {We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.},
  urldate = {2020-07-23},
  journal = {arXiv:1803.05573 [cs, stat]},
  author = {Salimans, Tim and Zhang, Han and Radford, Alec and Metaxas, Dimitris},
  month = mar,
  year = {2018},
  note = {arXiv: 1803.05573},
  keywords = {gans, geometry, optimal\_transport},
}

@article{genevay_gan_2017,
  title = {{GAN} and {VAE} from an {Optimal} {Transport} {Point} of {View}},
  url = {http://arxiv.org/abs/1706.01807},
  abstract = {This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE).},
  urldate = {2020-07-23},
  journal = {arXiv:1706.01807 [stat]},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  month = jun,
  year = {2017},
  note = {arXiv: 1706.01807},
  keywords = {gans, gen\_models, optimal\_transport, vae},
}

@article{higgins_towards_2018,
  title = {Towards a {Definition} of {Disentangled} {Representations}},
  url = {http://arxiv.org/abs/1812.02230},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  urldate = {2020-07-22},
  journal = {arXiv:1812.02230 [cs, stat]},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  month = dec,
  year = {2018},
  note = {arXiv: 1812.02230},
  keywords = {disentanglement, invariance},
}

@article{jabri_unsupervised_2019-1,
  title = {Unsupervised {Curricula} for {Visual} {Meta}-{Reinforcement} {Learning}},
  url = {http://arxiv.org/abs/1912.04226},
  abstract = {In principle, meta-reinforcement learning algorithms leverage experience across many tasks to learn fast reinforcement learning (RL) strategies that transfer to similar tasks. However, current meta-RL approaches rely on manually-defined distributions of training tasks, and hand-crafting these task distributions can be challenging and time-consuming. Can "useful" pre-training tasks be discovered in an unsupervised manner? We develop an unsupervised algorithm for inducing an adaptive meta-training task distribution, i.e. an automatic curriculum, by modeling unsupervised interaction in a visual environment. The task distribution is scaffolded by a parametric density model of the meta-learner's trajectory distribution. We formulate unsupervised meta-RL as information maximization between a latent task variable and the meta-learner's data distribution, and describe a practical instantiation which alternates between integration of recent experience into the task distribution and meta-learning of the updated tasks. Repeating this procedure leads to iterative reorganization such that the curriculum adapts as the meta-learner's data distribution shifts. In particular, we show how discriminative clustering for visual representation can support trajectory-level task acquisition and exploration in domains with pixel observations, avoiding pitfalls of alternatives. In experiments on vision-based navigation and manipulation domains, we show that the algorithm allows for unsupervised meta-learning that transfers to downstream tasks specified by hand-crafted reward functions and serves as pre-training for more efficient supervised meta-learning of test task distributions.},
  urldate = {2020-07-22},
  journal = {arXiv:1912.04226 [cs]},
  author = {Jabri, Allan and Hsu, Kyle and Eysenbach, Ben and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.04226},
  keywords = {curriculum\_learning, meta-learning, rl, self-supervised},
}

@article{asadi_towards_2020,
  title = {Towards {Shape} {Biased} {Unsupervised} {Representation} {Learning} for {Domain} {Generalization}},
  url = {http://arxiv.org/abs/1909.08245},
  abstract = {It is known that, without awareness of the process, our brain appears to focus on the general shape of objects rather than superficial statistics of context. On the other hand, learning autonomously allows discovering invariant regularities which help generalization. In this work, we propose a learning framework to improve the shape bias property of self-supervised methods. Our method learns semantic and shape biased representations by integrating domain diversification and jigsaw puzzles. The first module enables the model to create a dynamic environment across arbitrary domains and provides a domain exploration vs. exploitation trade-off, while the second module allows the model to explore this environment autonomously. This universal framework does not require prior knowledge of the domain of interest. Extensive experiments are conducted on several domain generalization datasets, namely, PACS, Office-Home, VLCS, and Digits. We show that our framework outperforms state-of-the-art domain generalization methods by a large margin.},
  urldate = {2020-07-22},
  journal = {arXiv:1909.08245 [cs]},
  author = {Asadi, Nader and Sarfi, Amir M. and Hosseinzadeh, Mehrdad and Karimpour, Zahra and Eftekhari, Mahdi},
  month = mar,
  year = {2020},
  note = {arXiv: 1909.08245},
  keywords = {domain\_generalization, jigsaw, self-supervised, shape},
}

@article{liu_hybrid_2020,
  title = {Hybrid {Discriminative}-{Generative} {Training} via {Contrastive} {Learning}},
  url = {http://arxiv.org/abs/2007.09070},
  abstract = {Contrastive learning and supervised learning have both seen significant progress and success. However, thus far they have largely been treated as two separate objectives, brought together only by having a shared neural network. In this paper we show that through the perspective of hybrid discriminative-generative training of energy-based models we can make a direct connection between contrastive learning and supervised learning. Beyond presenting this unified view, we show our specific choice of approximation of the energy-based loss outperforms the existing practice in terms of classification accuracy of WideResNet on CIFAR-10 and CIFAR-100. It also leads to improved performance on robustness, out-of-distribution detection, and calibration.},
  urldate = {2020-07-21},
  journal = {arXiv:2007.09070 [cs, stat]},
  author = {Liu, Hao and Abbeel, Pieter},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.09070},
  keywords = {contrastive, ebms, hybrid},
}

@article{asadi_towards_2020-1,
  title = {Towards {Shape} {Biased} {Unsupervised} {Representation} {Learning} for {Domain} {Generalization}},
  url = {http://arxiv.org/abs/1909.08245},
  abstract = {It is known that, without awareness of the process, our brain appears to focus on the general shape of objects rather than superficial statistics of context. On the other hand, learning autonomously allows discovering invariant regularities which help generalization. In this work, we propose a learning framework to improve the shape bias property of self-supervised methods. Our method learns semantic and shape biased representations by integrating domain diversification and jigsaw puzzles. The first module enables the model to create a dynamic environment across arbitrary domains and provides a domain exploration vs. exploitation trade-off, while the second module allows the model to explore this environment autonomously. This universal framework does not require prior knowledge of the domain of interest. Extensive experiments are conducted on several domain generalization datasets, namely, PACS, Office-Home, VLCS, and Digits. We show that our framework outperforms state-of-the-art domain generalization methods by a large margin.},
  urldate = {2020-07-20},
  journal = {arXiv:1909.08245 [cs]},
  author = {Asadi, Nader and Sarfi, Amir M. and Hosseinzadeh, Mehrdad and Karimpour, Zahra and Eftekhari, Mahdi},
  month = mar,
  year = {2020},
  note = {arXiv: 1909.08245},
  keywords = {domain\_generalization, jigsaw, self-supervised, style\_transfer},
}

@article{nalisnick_detecting_2019,
  title = {Detecting {Out}-of-{Distribution} {Inputs} to {Deep} {Generative} {Models} {Using} {Typicality}},
  url = {http://arxiv.org/abs/1906.02994},
  abstract = {Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data (Nalisnick et al., 2019; Choi et al., 2019). We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).},
  urldate = {2020-07-16},
  journal = {arXiv:1906.02994 [cs, stat]},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  month = oct,
  year = {2019},
  note = {arXiv: 1906.02994},
  keywords = {flows, gen\_models, ood, typicality},
}

@inproceedings{ren_cross-domain_2018,
  address = {Salt Lake City, UT, USA},
  title = {Cross-{Domain} {Self}-{Supervised} {Multi}-task {Feature} {Learning} {Using} {Synthetic} {Imagery}},
  isbn = {978-1-5386-6420-9},
  url = {https://ieeexplore.ieee.org/document/8578184/},
  doi = {10.1109/CVPR.2018.00086},
  abstract = {In human learning, it is common to use multiple sources of information jointly. However, most existing feature learning approaches learn from only a single task. In this paper, we propose a novel multi-task deep network to learn generalizable high-level visual representations. Since multitask learning requires annotations for multiple properties of the same training instance, we look to synthetic images to train our network. To overcome the domain difference between real and synthetic data, we employ an unsupervised feature space domain adaptation method based on adversarial learning. Given an input synthetic RGB image, our network simultaneously predicts its surface normal, depth, and instance contour, while also minimizing the feature space domain differences between real and synthetic data. Through extensive experiments, we demonstrate that our network learns more transferable representations compared to single-task baselines. Our learned representation produces state-of-the-art transfer learning results on PASCAL VOC 2007 classiﬁcation and 2012 detection.},
  language = {en},
  urldate = {2020-07-16},
  booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  publisher = {IEEE},
  author = {Ren, Zhongzheng and Lee, Yong Jae},
  month = jun,
  year = {2018},
  keywords = {domain adaptation, multi-task, self-supervised},
  pages = {762--771},
}

@article{mahajan_domain_2020,
  title = {Domain {Generalization} using {Causal} {Matching}},
  url = {https://arxiv.org/abs/2006.07500v1},
  abstract = {Learning invariant representations has been proposed as a key technique for
addressing the domain generalization problem. However, the question of
identifying the right conditions for invariance remains unanswered. In this
work, we propose a causal interpretation of domain generalization that defines
domains as interventions under a data-generating process. Based on a general
causal model for data from multiple domains, we show that prior methods for
learning an invariant representation optimize for an incorrect objective. We
highlight an alternative condition: inputs across domains should have the same
representation if they are derived from the same base object. In practice,
knowledge about generation of data or objects is not available. Hence we
propose an iterative algorithm called MatchDG that approximates base object
similarity by using a contrastive loss formulation adapted for multiple
domains. We then match inputs that are similar under the resultant
representation to build an invariant classifier. We evaluate MatchDG on rotated
MNIST, Fashion-MNIST, and PACS datasets and find that it outperforms prior work
on out-of-domain accuracy and learns matches that have over 25{\textbackslash}\% overlap with
ground-truth object matches in MNIST and Fashion-MNIST. Code repository can be
accessed here: {\textbackslash}textit\{https://github.com/microsoft/robustdg\}},
  language = {en},
  urldate = {2020-07-16},
  author = {Mahajan, Divyat and Tople, Shruti and Sharma, Amit},
  month = jun,
  year = {2020},
  keywords = {causality, generalization, invariance, ood},
}

@article{sun_unsupervised_nodate,
  title = {{UNSUPERVISED} {DOMAIN} {ADAPTATION} {THROUGH} {SELF}-{SUPERVISION}},
  abstract = {This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously. Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classiﬁer on the source domain is shown to successfully generalize to the unlabeled target domain. The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.},
  language = {en},
  author = {Sun, Yu and Tzeng, Eric and Darrell, Trevor and Efros, Alexei A},
  keywords = {domain\_adaptation, self-supervised},
  pages = {15},
}

@incollection{liu_self-supervised_2019,
  title = {Self-{Supervised} {Generalisation} with {Meta} {Auxiliary} {Learning}},
  url = {http://papers.nips.cc/paper/8445-self-supervised-generalisation-with-meta-auxiliary-learning.pdf},
  urldate = {2020-07-16},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
  publisher = {Curran Associates, Inc.},
  author = {Liu, Shikun and Davison, Andrew and Johns, Edward},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
  year = {2019},
  keywords = {generalization, meta-learning, ood, self-supervised},
  pages = {1679--1689},
}

@article{albuquerque_improving_nodate,
  title = {Improving out-of-distribution generalization via multi-task self-supervised pretraining},
  abstract = {Self-supervised feature representations have been shown to be useful for supervised classiﬁcation, few-shot learning, and adversarial robustness. We show that features obtained using self-supervised learning are comparable to, or better than, supervised learning for domain generalization in computer vision. We introduce a new self-supervised pretext task of predicting responses to Gabor ﬁlter banks and demonstrate that multi-task learning of compatible pretext tasks improves domain generalization performance as compared to training individual tasks alone. Features learnt through self-supervision obtain better generalization to unseen domains when compared to their supervised counterpart when there is a larger domain shift between training and test distributions and even show better localization ability for objects of interest. Selfsupervised feature representations can also be combined with other domain generalization methods to further boost performance.},
  language = {en},
  author = {Albuquerque, Isabela and Naik, Nikhil and Li, Junnan and Keskar, Nitish},
  keywords = {multi-task, ood, self-supervised},
  pages = {19},
}

@article{lowe_amortized_2020,
  title = {Amortized {Causal} {Discovery}: {Learning} to {Infer} {Causal} {Graphs} from {Time}-{Series} {Data}},
  shorttitle = {Amortized {Causal} {Discovery}},
  url = {https://arxiv.org/abs/2006.10833v1},
  abstract = {Standard causal discovery methods must fit a new model whenever they
encounter samples from a new underlying causal graph. However, these samples
often share relevant information - for instance, the dynamics describing the
effects of causal relations - which is lost when following this approach. We
propose Amortized Causal Discovery, a novel framework that leverages such
shared dynamics to learn to infer causal relations from time-series data. This
enables us to train a single, amortized model that infers causal relations
across samples with different underlying causal graphs, and thus makes use of
the information that is shared. We demonstrate experimentally that this
approach, implemented as a variational model, leads to significant improvements
in causal discovery performance, and show how it can be extended to perform
well under hidden confounding.},
  language = {en},
  urldate = {2020-07-16},
  author = {Löwe, Sindy and Madras, David and Zemel, Richard and Welling, Max},
  month = jun,
  year = {2020},
  keywords = {causal\_discovery, causality},
}

@article{zhang_learning_nodate,
  title = {Learning {Structured} {Latent} {Factors} from {Dependent} {Data}:{A} {Generative} {Model} {Framework} from {Information}-{Theoretic} {Perspective}},
  abstract = {Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning. In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space. We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck (Friedman et al., 2001) to enforce it. Our model provides a principled approach to learn a set of semantically meaningful latent factors that reﬂect various types of desired structures like capturing correlation or encoding invariance, while also offering the ﬂexibility to automatically estimate the dependency structure from data. We show that our framework uniﬁes many existing generative models and can be applied to a variety of tasks, including multimodal data modeling, algorithmic fairness, and out-of-distribution generalization.},
  language = {en},
  author = {Zhang, Ruixiang and Koyama, Masanori and Ishiguro, Katsuhiko},
  keywords = {gen\_models, graphical\_models, information\_theory, variational\_inference},
  pages = {12},
}

@article{somavarapu_frustratingly_nodate,
  title = {Frustratingly {Simple} {Domain} {Generalization} via {Image} {Stylization}},
  abstract = {Convolutional Neural Networks (CNNs) show impressive performance in the standard classiﬁcation setting where training and testing data are drawn i.i.d. from a given domain. However, CNNs do not readily generalize to new domains with different statistics, a setting that is simple for humans. In this work, we address the Domain Generalization problem, where the classiﬁer must generalize to an unknown target domain. Inspired by recent works that have shown a difference in biases between CNNs and humans, we demonstrate an extremely simple yet effective method, namely correcting this bias by augmenting the dataset with stylized images. In contrast with existing stylization works, which use external data sources such as art, we further introduce a method that is entirely in-domain using no such extra sources of data. We provide a detailed analysis as to the mechanism by which the method works, verifying our claim that it changes the shape/texture bias, and demonstrate results surpassing or comparable to the state of the arts that utilize much more complex methods1.},
  language = {en},
  author = {Somavarapu, Nathan and Ma, Chih-Yao and Kira, Zsolt},
  pages = {15},
}

@article{ermolov_whitening_2020,
  title = {Whitening for {Self}-{Supervised} {Representation} {Learning}},
  url = {http://arxiv.org/abs/2007.06346},
  abstract = {Recent literature on self-supervised learning is based on the contrastive loss, where image instances which share the same semantic content ("positives") are contrasted with instances extracted from other images ("negatives"). However, in order for the learning to be effective, a lot of negatives should be compared with a positive pair. This is not only computationally demanding, but it also requires that the positive and the negative representations are kept consistent with each other over a long training period. In this paper we propose a different direction and a new loss function for self-supervised learning which is based on the whitening of the latent-space features. The whitening operation has a "scattering" effect on the batch samples, which compensates the lack of a large number of negatives, avoiding degenerate solutions where all the sample representations collapse to a single point. We empirically show that our loss accelerates self-supervised training and the learned representations are much more effective for downstream tasks than previously published work.},
  urldate = {2020-07-14},
  journal = {arXiv:2007.06346 [cs, stat]},
  author = {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.06346},
  keywords = {contrastive, self-supervised},
}

@article{gulrajani_search_2020,
  title = {In {Search} of {Lost} {Domain} {Generalization}},
  url = {http://arxiv.org/abs/2007.01434},
  abstract = {The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions -- datasets, architectures, and model selection criteria -- render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DomainBed, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DomainBed and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DomainBed, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.},
  urldate = {2020-07-11},
  journal = {arXiv:2007.01434 [cs, stat]},
  author = {Gulrajani, Ishaan and Lopez-Paz, David},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.01434},
  keywords = {data\_aug, invariance, meta-learning},
}

@article{pawlowski_deep_2020,
  title = {Deep {Structural} {Causal} {Models} for {Tractable} {Counterfactual} {Inference}},
  url = {http://arxiv.org/abs/2006.06485},
  abstract = {We formulate a general framework for building structural causal models (SCMs) with deep learning components. The proposed approach employs normalising flows and variational inference to enable tractable inference of exogenous noise variables - a crucial step for counterfactual inference that is missing from existing deep causal learning methods. Our framework is validated on a synthetic dataset built on MNIST as well as on a real-world medical dataset of brain MRI scans. Our experimental results indicate that we can successfully train deep SCMs that are capable of all three levels of Pearl's ladder of causation: association, intervention, and counterfactuals, giving rise to a powerful new approach for answering causal questions in imaging applications and beyond. The code for all our experiments is available at https://github.com/biomedia-mira/deepscm.},
  urldate = {2020-07-10},
  journal = {arXiv:2006.06485 [cs, stat]},
  author = {Pawlowski, Nick and Castro, Daniel C. and Glocker, Ben},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.06485},
  keywords = {causality, deep\_causality},
}

@article{vuorio_multimodal_2019,
  title = {Multimodal {Model}-{Agnostic} {Meta}-{Learning} via {Task}-{Aware} {Modulation}},
  url = {http://arxiv.org/abs/1910.13616},
  abstract = {Model-agnostic meta-learners aim to acquire meta-learned parameters from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. With the flexibility in the choice of models, those frameworks demonstrate appealing performance on a variety of domains such as few-shot image classification and reinforcement learning. However, one important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify the mode of tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior parameters according to the identified mode, allowing more efficient fast adaptation. We evaluate the proposed model on a diverse set of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results not only demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks but also show that training on a multimodal distribution can produce an improvement over unimodal training.},
  urldate = {2020-07-10},
  journal = {arXiv:1910.13616 [cs, stat]},
  author = {Vuorio, Risto and Sun, Shao-Hua and Hu, Hexiang and Lim, Joseph J.},
  month = oct,
  year = {2019},
  note = {arXiv: 1910.13616},
  keywords = {metalearning},
}

@article{raileanu_automatic_2020,
  title = {Automatic {Data} {Augmentation} for {Generalization} in {Deep} {Reinforcement} {Learning}},
  url = {http://arxiv.org/abs/2006.12862},
  abstract = {Deep reinforcement learning (RL) agents often fail to generalize to unseen scenarios, even when they are trained on many instances of semantically similar environments. Data augmentation has recently been shown to improve the sample efficiency and generalization of RL agents. However, different tasks tend to benefit from different kinds of data augmentation. In this paper, we compare three approaches for automatically finding an appropriate augmentation. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for certain actor-critic algorithms. We evaluate our methods on the Procgen benchmark which consists of 16 procedurally-generated environments and show that it improves test performance by {\textasciitilde}40\% relative to standard RL algorithms. Our agent outperforms other baselines specifically designed to improve generalization in RL. In addition, we show that our agent learns policies and representations that are more robust to changes in the environment that do not affect the agent, such as the background. Our implementation is available at https://github.com/rraileanu/auto-drac.},
  urldate = {2020-07-10},
  journal = {arXiv:2006.12862 [cs]},
  author = {Raileanu, Roberta and Goldstein, Max and Yarats, Denis and Kostrikov, Ilya and Fergus, Rob},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.12862},
  keywords = {data\_aug, learn\_data\_aug},
}

@article{zhou_meta-learning_2020,
  title = {Meta-{Learning} {Symmetries} by {Reparameterization}},
  url = {http://arxiv.org/abs/2007.02933},
  abstract = {Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know a-priori symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is a general approach for learning equivariances from data, without needing prior knowledge of a task's symmetries or custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably encode equivariance-inducing parameter sharing for any finite group of symmetry transformations, and we find experimentally that it can automatically learn a variety of equivariances from symmetries in data. We provide our experiment code and pre-trained models at https://github.com/AllanYangZhou/metalearning-symmetries.},
  urldate = {2020-07-09},
  journal = {arXiv:2007.02933 [cs, stat]},
  author = {Zhou, Allan and Knowles, Tom and Finn, Chelsea},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.02933},
  keywords = {inductive\_bias, metalearning},
}

@article{li_causal_2020,
  title = {Causal {Discovery} in {Physical} {Systems} from {Videos}},
  url = {http://arxiv.org/abs/2007.00631},
  abstract = {Causal discovery is at the core of human cognition. It enables us to reason about the environment and make counterfactual predictions about unseen scenarios, that can vastly differ from our previous experiences. We consider the task of causal discovery from videos in an end-to-end fashion without supervision on the ground-truth graph structure. In particular, our goal is to discover the structural dependencies among environmental and object variables: inferring the type and strength of interactions that have a causal effect on the behavior of the dynamical system. Our model consists of (a) a perception module that extracts a semantically meaningful and temporally consistent keypoint representation from images, (b) an inference module for determining the graph distribution induced by the detected keypoints, and (c) a dynamics module that can predict the future by conditioning on the inferred graph. We assume access to different configurations and environmental conditions, i.e., data from unknown interventions on the underlying system; thus, we can hope to discover the correct underlying causal graph without explicit interventions. We evaluate our method in a planar multi-body interaction environment and scenarios involving fabrics of different shapes like shirts and pants. Experiments demonstrate that our model can correctly identify the interactions from a short sequence of images and make long-term future predictions. The causal structure assumed by the model also allows it to make counterfactual predictions and extrapolate to systems of unseen interaction graphs or graphs of various sizes.},
  urldate = {2020-07-09},
  journal = {arXiv:2007.00631 [cs, stat]},
  author = {Li, Yunzhu and Torralba, Antonio and Anandkumar, Animashree and Fox, Dieter and Garg, Animesh},
  month = jul,
  year = {2020},
  note = {arXiv: 2007.00631},
  keywords = {causality, video},
}

@article{rhodes_telescoping_2020,
  title = {Telescoping {Density}-{Ratio} {Estimation}},
  url = {http://arxiv.org/abs/2006.12204},
  abstract = {Density-ratio estimation via classification is a cornerstone of unsupervised learning. It has provided the foundation for state-of-the-art methods in representation learning and generative modelling, with the number of use-cases continuing to proliferate. However, it suffers from a critical limitation: it fails to accurately estimate ratios p/q for which the two densities differ significantly. Empirically, we find this occurs whenever the KL divergence between p and q exceeds tens of nats. To resolve this limitation, we introduce a new framework, telescoping density-ratio estimation (TRE), that enables the estimation of ratios between highly dissimilar densities in high-dimensional spaces. Our experiments demonstrate that TRE can yield substantial improvements over existing single-ratio methods for mutual information estimation, representation learning and energy-based modelling.},
  urldate = {2020-07-08},
  journal = {arXiv:2006.12204 [cs, stat]},
  author = {Rhodes, Benjamin and Xu, Kai and Gutmann, Michael U.},
  month = jun,
  year = {2020},
  note = {arXiv: 2006.12204},
}

@article{besserve_counterfactuals_2019,
  title = {Counterfactuals uncover the modular structure of deep generative models},
  url = {http://arxiv.org/abs/1812.03253},
  abstract = {Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations to uncover a modular structure of the network composed of disentangled groups of internal variables. Experiments with a variety of generative models trained on complex image datasets show the obtained modules can be used to design targeted interventions. This opens the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.},
  urldate = {2020-07-02},
  journal = {arXiv:1812.03253 [cs, stat]},
  author = {Besserve, Michel and Mehrjou, Arash and Sun, Rémy and Schölkopf, Bernhard},
  month = dec,
  year = {2019},
  note = {arXiv: 1812.03253},
  keywords = {causality, gen\_models},
}

@article{ke_learning_2019,
  title = {Learning {Neural} {Causal} {Models} from {Unknown} {Interventions}},
  url = {http://arxiv.org/abs/1910.01075},
  abstract = {Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing parameters and slowly-changing meta-parameters. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions robust to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph.},
  urldate = {2020-07-02},
  journal = {arXiv:1910.01075 [cs, stat]},
  author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Pal, Chris and Bengio, Yoshua},
  month = oct,
  year = {2019},
  note = {arXiv: 1910.01075},
  keywords = {causality},
}

@inproceedings{prabhakar_temporal_2010,
  address = {San Francisco, CA, USA},
  title = {Temporal causality for the analysis of visual events},
  isbn = {978-1-4244-6984-0},
  url = {http://ieeexplore.ieee.org/document/5539871/},
  doi = {10.1109/CVPR.2010.5539871},
  abstract = {We present a novel approach to the causal temporal analysis of event data from video content. Our key observation is that the sequence of visual words produced by a space-time dictionary representation of a video sequence can be interpreted as a multivariate point-process. By using a spectral version of the pairwise test for Granger causality, we can identify patterns of interactions between words and group them into independent causal sets. We demonstrate qualitatively that this produces semanticallymeaningful groupings, and we demonstrate quantitatively that these groupings lead to improved performance in retrieving and classifying social games from unstructured videos.},
  language = {en},
  urldate = {2020-07-02},
  booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  publisher = {IEEE},
  author = {Prabhakar, Karthir and Oh, Sangmin and Wang, Ping and Abowd, Gregory D. and Rehg, James M.},
  month = jun,
  year = {2010},
  keywords = {causality, video},
  pages = {1967--1974},
}

@article{schwobel_probabilistic_2020-1,
  title = {Probabilistic {Spatial} {Transformers} for {Bayesian} {Data} {Augmentation}},
  url = {http://arxiv.org/abs/2004.03637},
  abstract = {High-capacity models require vast amounts of data, and data augmentation is a common remedy when this resource is limited. Standard augmentation techniques apply small hand-tuned transformations to existing data, which is a brittle process that realistically only allows for simple transformations. We propose a Bayesian interpretation of data augmentation where the transformations are modelled as latent variables to be marginalized, and show how these can be inferred variationally in an end-to-end fashion. This allows for significantly more complex transformations than manual tuning, and the marginalization implies a form of test-time data augmentation. The resulting model can be interpreted as a probabilistic extension of spatial transformer networks. Experimentally, we demonstrate improvements in accuracy and uncertainty quantification in image and time series classification tasks.},
  urldate = {2020-07-02},
  journal = {arXiv:2004.03637 [cs, stat]},
  author = {Schwöbel, Pola and Warburg, Frederik and Jørgensen, Martin and Madsen, Kristoffer H. and Hauberg, Søren},
  month = apr,
  year = {2020},
  note = {arXiv: 2004.03637},
}

@article{corneanu_computing_nodate,
  title = {Computing the {Testing} {Error} {Without} a {Testing} {Set}},
  language = {en},
  author = {Corneanu, Ciprian A and Escalera, Sergio and Martinez, Aleix M},
  keywords = {generalization, tda},
  pages = {9},
}

@article{kalatzis_variational_2020,
  title = {Variational {Autoencoders} with {Riemannian} {Brownian} {Motion} {Priors}},
  url = {http://arxiv.org/abs/2002.05227},
  abstract = {Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.},
  urldate = {2020-06-29},
  journal = {arXiv:2002.05227 [cs, stat]},
  author = {Kalatzis, Dimitris and Eklund, David and Arvanitidis, Georgios and Hauberg, Søren},
  month = feb,
  year = {2020},
  note = {arXiv: 2002.05227},
  keywords = {noneuclidean, riemannian, vae},
}

@inproceedings{madras_fairness_2019,
  address = {Atlanta, GA, USA},
  title = {Fairness through {Causal} {Awareness}: {Learning} {Causal} {Latent}-{Variable} {Models} for {Biased} {Data}},
  isbn = {978-1-4503-6125-5},
  shorttitle = {Fairness through {Causal} {Awareness}},
  url = {http://dl.acm.org/citation.cfm?doid=3287560.3287564},
  doi = {10.1145/3287560.3287564},
  abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.},
  language = {en},
  urldate = {2020-06-29},
  booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency} - {FAT}* '19},
  publisher = {ACM Press},
  author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  year = {2019},
  keywords = {causality, fairness, gen\_models},
  pages = {349--358},
}

@article{laskin_reinforcement_2020,
  title = {Reinforcement {Learning} with {Augmented} {Data}},
  url = {http://arxiv.org/abs/2004.14990},
  abstract = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
  urldate = {2020-06-26},
  journal = {arXiv:2004.14990 [cs, stat]},
  author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  month = jun,
  year = {2020},
  note = {arXiv: 2004.14990},
  keywords = {data\_aug, learning\_data\_aug, rl},
}

@article{pan_unsupervised_2020,
  title = {Unsupervised {Intra}-domain {Adaptation} for {Semantic} {Segmentation} through {Self}-{Supervision}},
  url = {http://arxiv.org/abs/2004.07703},
  abstract = {Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the inter-domain adaptation of the model; from this adaptation, we separate the target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard split. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-the-art approaches. The source code is available at https://github.com/feipan664/IntraDA.git.},
  urldate = {2020-06-17},
  journal = {arXiv:2004.07703 [cs]},
  author = {Pan, Fei and Shin, Inkyu and Rameau, Francois and Lee, Seokju and Kweon, In So},
  month = jun,
  year = {2020},
  note = {arXiv: 2004.07703},
  keywords = {segmentation, self-supervision, uda},
}

@article{gu_image_2020,
  title = {Image {Processing} {Using} {Multi}-{Code} {GAN} {Prior}},
  url = {http://arxiv.org/abs/1912.07116},
  abstract = {Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.},
  urldate = {2020-06-16},
  journal = {arXiv:1912.07116 [cs]},
  author = {Gu, Jinjin and Shen, Yujun and Zhou, Bolei},
  month = mar,
  year = {2020},
  note = {arXiv: 1912.07116},
  keywords = {gans, gen\_models},
}

@article{tian_what_2020,
  title = {What makes for good views for contrastive learning},
  url = {http://arxiv.org/abs/2005.10243},
  abstract = {Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we also achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification (\$73{\textbackslash}\%\$ top-1 linear readoff with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast},
  urldate = {2020-06-11},
  journal = {arXiv:2005.10243 [cs]},
  author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  month = may,
  year = {2020},
  note = {arXiv: 2005.10243},
  keywords = {contrastive, data\_aug, self-supervision},
}

@article{gowal_achieving_2020,
  title = {Achieving {Robustness} in the {Wild} via {Adversarial} {Mixing} with {Disentangled} {Representations}},
  url = {http://arxiv.org/abs/1912.03192},
  abstract = {Recent research has made the surprising finding that state-of-the-art deep learning models sometimes fail to generalize to small variations of the input. Adversarial training has been shown to be an effective approach to overcome this problem. However, its application has been limited to enforcing invariance to analytically defined transformations like \${\textbackslash}ell\_p\$-norm bounded perturbations. Such perturbations do not necessarily cover plausible real-world variations that preserve the semantics of the input (such as a change in lighting conditions). In this paper, we propose a novel approach to express and formalize robustness to these kinds of real-world transformations of the input. The two key ideas underlying our formulation are (1) leveraging disentangled representations of the input to define different factors of variations, and (2) generating new input images by adversarially composing the representations of different images. We use a StyleGAN model to demonstrate the efficacy of this framework. Specifically, we leverage the disentangled latent representations computed by a StyleGAN model to generate perturbations of an image that are similar to real-world variations (like adding make-up, or changing the skin-tone of a person) and train models to be invariant to these perturbations. Extensive experiments show that our method improves generalization and reduces the effect of spurious correlations (reducing the error rate of a "smile" detector by 21\% for example).},
  urldate = {2020-06-01},
  journal = {arXiv:1912.03192 [cs, stat]},
  author = {Gowal, Sven and Qin, Chongli and Huang, Po-Sen and Cemgil, Taylan and Dvijotham, Krishnamurthy and Mann, Timothy and Kohli, Pushmeet},
  month = mar,
  year = {2020},
  note = {arXiv: 1912.03192},
  keywords = {gans, invariance, toread},
}

@article{sanchez-martin_out--sample_2019,
  title = {Out-of-{Sample} {Testing} for {GANs}},
  url = {http://arxiv.org/abs/1901.09557},
  abstract = {We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a test set to directly measure the reconstruction quality in the original sample space (no auxiliary networks are necessary), and it also computes the (log)likelihood for the reconstructed samples in the test set. Further, EvalGAN is agnostic to the GAN algorithm and the dataset. We decided to test it on three state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets.},
  urldate = {2020-06-01},
  journal = {arXiv:1901.09557 [cs, stat]},
  author = {Sánchez-Martín, Pablo and Olmos, Pablo M. and Pérez-Cruz, Fernando},
  month = jan,
  year = {2019},
  note = {arXiv: 1901.09557},
  keywords = {gans, toread, typicality},
}

@article{kipf_contrastive_2020,
  title = {Contrastive {Learning} of {Structured} {World} {Models}},
  url = {http://arxiv.org/abs/1911.12247},
  abstract = {A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.},
  urldate = {2020-05-29},
  journal = {arXiv:1911.12247 [cs, stat]},
  author = {Kipf, Thomas and van der Pol, Elise and Welling, Max},
  month = jan,
  year = {2020},
  note = {arXiv: 1911.12247},
  keywords = {contrastive, toread},
}

@article{nalisnick_detecting_2019,
  title = {Detecting {Out}-of-{Distribution} {Inputs} to {Deep} {Generative} {Models} {Using} {Typicality}},
  url = {http://arxiv.org/abs/1906.02994},
  abstract = {Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data (Nalisnick et al., 2019; Choi et al., 2019). We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).},
  urldate = {2020-05-23},
  journal = {arXiv:1906.02994 [cs, stat]},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  month = oct,
  year = {2019},
  note = {arXiv: 1906.02994},
  keywords = {gen, typicality},
}

@article{musgrave_metric_2020,
  title = {A {Metric} {Learning} {Reality} {Check}},
  url = {http://arxiv.org/abs/2003.08505},
  abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental setup of these papers, and propose a new way to evaluate metric learning algorithms. Finally, we present experimental results that show that the improvements over time have been marginal at best.},
  urldate = {2020-05-21},
  journal = {arXiv:2003.08505 [cs]},
  author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
  month = mar,
  year = {2020},
  note = {arXiv: 2003.08505},
  keywords = {metric},
}

@article{sanchez-martin_out--sample_2019-1,
  title = {Out-of-{Sample} {Testing} for {GANs}},
  url = {http://arxiv.org/abs/1901.09557},
  abstract = {We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a test set to directly measure the reconstruction quality in the original sample space (no auxiliary networks are necessary), and it also computes the (log)likelihood for the reconstructed samples in the test set. Further, EvalGAN is agnostic to the GAN algorithm and the dataset. We decided to test it on three state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets.},
  urldate = {2020-05-20},
  journal = {arXiv:1901.09557 [cs, stat]},
  author = {Sánchez-Martín, Pablo and Olmos, Pablo M. and Pérez-Cruz, Fernando},
  month = jan,
  year = {2019},
  note = {arXiv: 1901.09557},
  keywords = {gans, gen, typicality},
}

@article{dusenberry_efficient_2020,
  title = {Efficient and {Scalable} {Bayesian} {Neural} {Nets} with {Rank}-1 {Factors}},
  url = {https://arxiv.org/abs/2005.07186v1},
  abstract = {Bayesian neural networks (BNNs) demonstrate promising success in improving
the robustness and uncertainty quantification of modern deep learning. However,
they generally struggle with underfitting at scale and parameter efficiency. On
the other hand, deep ensembles have emerged as alternatives for uncertainty
quantification that, while outperforming BNNs on certain problems, also suffer
from efficiency issues. It remains unclear how to combine the strengths of
these two approaches and remediate their common issues. To tackle this
challenge, we propose a rank-1 parameterization of BNNs, where each weight
matrix involves only a distribution on a rank-1 subspace. We also revisit the
use of mixture approximate posteriors to capture multiple modes, where unlike
typical mixtures, this approach admits a significantly smaller memory increase
(e.g., only a 0.4\% increase for a ResNet-50 mixture of size 10). We perform a
systematic empirical study on the choices of prior, variational posterior, and
methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on
CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art
performance across log-likelihood, accuracy, and calibration on the test sets
and out-of-distribution variants.},
  language = {en},
  urldate = {2020-05-20},
  author = {Dusenberry, Michael W. and Jerfel, Ghassen and Wen, Yeming and Ma, Yi-an and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
  month = may,
  year = {2020},
}

@inproceedings{li_duration--stay_2020,
  title = {Duration-of-{Stay} {Storage} {Assignment} under {Uncertainty}},
  url = {https://iclr.cc/virtual_2020/poster_Hkx7xRVYDr.html},
  abstract = {Storage assignment, the act of choosing what goods are placed in what locations in a warehouse, is a central problem of supply chain logistics. Past literature has shown that the optimal method to assign pallets is to arrange them in increasing duration of stay in the warehouse (the Duration-of-Stay, or DoS, method), but the methodology requires perfect prior knowledge of DoS for each pallet, which is unknown and uncertain under realistic conditions. Attempts to predict DoS have largely been unfruitful due to the multi-valuedness nature (every shipment contains multiple identical pallets with different DoS) and data sparsity induced by lack of matching historical conditions. In this paper, we introduce a new framework for storage assignment that provides a solution to the DoS prediction problem through a distributional reformulation and a novel neural network, ParallelNet. Through collaboration with a world-leading cold storage company, we show that the system is able to predict DoS with a MAPE of 29\%, a decrease of {\textasciitilde}30\% compared to a CNN-LSTM model, and suffers less performance decay into the future. The framework is then integrated into a first-of-its-kind Storage Assignment system, which is being deployed in warehouses across United States, with initial results showing up to 21\% in labor savings. We also release the first publicly available set of warehousing records to facilitate research into this central problem.},
  language = {en},
  urldate = {2020-05-20},
  author = {Li, Michael Lingzhi and Wolf, Elliott and Wintz, Daniel},
  month = apr,
  year = {2020},
}

@article{brock_large_2019,
  title = {Large {Scale} {GAN} {Training} for {High} {Fidelity} {Natural} {Image} {Synthesis}},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities speciﬁc to such scale. We ﬁnd that applying orthogonal regularization to the generator renders it amenable to a simple “truncation trick,” allowing ﬁne control over the trade-off between sample ﬁdelity and variety by reducing the variance of the Generator’s input. Our modiﬁcations lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fre´chet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65.},
  language = {en},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  year = {2019},
  pages = {35},
}

@article{xie_unsupervised_2019,
  title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
  url = {http://arxiv.org/abs/1904.12848},
  abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7\% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
  urldate = {2020-05-14},
  journal = {arXiv:1904.12848 [cs, stat]},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  month = sep,
  year = {2019},
  note = {arXiv: 1904.12848},
  keywords = {data\_aug, toread},
}

@article{dao_kernel_nodate,
  title = {A {Kernel} {Theory} of {Modern} {Data} {Augmentation}},
  abstract = {Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classiﬁcation. Next, we analyze more directly the effect of augmentation on kernel classiﬁers, showing that data augmentation can be approximated by ﬁrst-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workﬂows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.},
  language = {en},
  author = {Dao, Tri and Gu, Albert and Ratner, Alexander J and Smith, Virginia and Sa, Christopher De and Re, Christopher},
  keywords = {data\_aug, toread},
  pages = {10},
}

@article{hernandez-garcia_learning_2019,
  title = {Learning robust visual representations using data augmentation invariance},
  url = {http://arxiv.org/abs/1906.04547},
  abstract = {Deep convolutional neural networks trained for image object categorization have shown remarkable similarities with representations found across the primate ventral visual stream. Yet, artificial and biological networks still exhibit important differences. Here we investigate one such property: increasing invariance to identity-preserving image transformations found along the ventral stream. Despite theoretical evidence that invariance should emerge naturally from the optimization process, we present empirical evidence that the activations of convolutional neural networks trained for object categorization are not robust to identity-preserving image transformations commonly used in data augmentation. As a solution, we propose data augmentation invariance, an unsupervised learning objective which improves the robustness of the learned representations by promoting the similarity between the activations of augmented image samples. Our results show that this approach is a simple, yet effective and efficient (10 \% increase in training time) way of increasing the invariance of the models while obtaining similar categorization performance.},
  urldate = {2020-05-14},
  journal = {arXiv:1906.04547 [cs]},
  author = {Hernández-García, Alex and König, Peter and Kietzmann, Tim C.},
  month = jun,
  year = {2019},
  note = {arXiv: 1906.04547},
}

@article{chen_group-theoretic_2020,
  title = {A {Group}-{Theoretic} {Framework} for {Data} {Augmentation}},
  url = {http://arxiv.org/abs/1907.10905},
  abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
  urldate = {2020-05-10},
  journal = {arXiv:1907.10905 [cs, math, stat]},
  author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H.},
  month = feb,
  year = {2020},
  note = {arXiv: 1907.10905},
  keywords = {data\_aug, invariance},
}

@article{ho_population_2019,
  title = {Population {Based} {Augmentation}: {Efficient} {Learning} of {Augmentation} {Policy} {Schedules}},
  shorttitle = {Population {Based} {Augmentation}},
  url = {http://arxiv.org/abs/1905.05393},
  abstract = {A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46\%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.},
  urldate = {2020-05-09},
  journal = {arXiv:1905.05393 [cs, stat]},
  author = {Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
  month = may,
  year = {2019},
  note = {arXiv: 1905.05393},
  keywords = {data\_aug, toread},
}

@article{xie_unsupervised_2019-1,
  title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
  url = {http://arxiv.org/abs/1904.12848},
  abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7\% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
  urldate = {2020-05-09},
  journal = {arXiv:1904.12848 [cs, stat]},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  month = sep,
  year = {2019},
  note = {arXiv: 1904.12848},
  keywords = {data\_aug, toread},
}

@article{chen_simple_2020,
  title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
  url = {http://arxiv.org/abs/2002.05709},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  urldate = {2020-05-08},
  journal = {arXiv:2002.05709 [cs, stat]},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  month = mar,
  year = {2020},
  note = {arXiv: 2002.05709},
  keywords = {self-supervision},
}

@article{ilse_designing_2020,
  title = {Designing {Data} {Augmentation} for {Simulating} {Interventions}},
  url = {http://arxiv.org/abs/2005.01856},
  abstract = {Machine learning models trained with purely observational data and the principle of empirical risk minimization (Vapnik, 1992) can fail to generalize to unseen domains. In this paper, we focus on the case where the problem arises through spurious correlation between the observed domains and the actual task labels. We find that many domain generalization methods do not explicitly take this spurious correlation into account. Instead, especially in more application-oriented research areas like medical imaging or robotics, data augmentation techniques that are based on heuristics are used to learn domain invariant features. To bridge the gap between theory and practice, we develop a causal perspective on the problem of domain generalization. We argue that causal concepts can be used to explain the success of data augmentation by describing how they can weaken the spurious correlation between the observed domains and the task labels. We demonstrate that data augmentation can serve as a tool for simulating interventional data. Lastly, but unsurprisingly, we show that augmenting data improperly can cause a significant decrease in performance.},
  urldate = {2020-05-08},
  journal = {arXiv:2005.01856 [cs, stat]},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Forré, Patrick},
  month = may,
  year = {2020},
  note = {arXiv: 2005.01856
version: 2},
  keywords = {causality, invariance, toread},
}

@article{bengio_meta-transfer_2019,
  title = {A {Meta}-{Transfer} {Objective} for {Learning} to {Disentangle} {Causal} {Mechanisms}},
  url = {http://arxiv.org/abs/1901.10912},
  abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
  urldate = {2020-05-04},
  journal = {arXiv:1901.10912 [cs, stat]},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, Sébastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  month = feb,
  year = {2019},
  note = {arXiv: 1901.10912},
  keywords = {causality, toread},
}

@article{lopez-paz_discovering_2017,
  title = {Discovering {Causal} {Signals} in {Images}},
  doi = {10.1109/CVPR.2017.14},
  abstract = {This paper establishes the existence of observable footprints that reveal the causal dispositions of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects.},
  journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  author = {Lopez-Paz, David and Nishihara, Robert and Chintala, Soumith and Schölkopf, Bernhard and Bottou, Léon},
  year = {2017},
  keywords = {invariance},
}

@article{misra_self-supervised_2019,
  title = {Self-{Supervised} {Learning} of {Pretext}-{Invariant} {Representations}},
  url = {http://arxiv.org/abs/1912.01991},
  abstract = {The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as "pearl") that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in self-supervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.},
  urldate = {2020-05-01},
  journal = {arXiv:1912.01991 [cs]},
  author = {Misra, Ishan and van der Maaten, Laurens},
  month = dec,
  year = {2019},
  note = {arXiv: 1912.01991},
  keywords = {invariance, self-supervision},
}

@article{goyal_recurrent_2019,
  title = {Recurrent {Independent} {Mechanisms}},
  url = {http://arxiv.org/abs/1909.10893},
  abstract = {Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
  urldate = {2020-05-01},
  journal = {arXiv:1909.10893 [cs, stat]},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Schölkopf, Bernhard},
  month = sep,
  year = {2019},
  note = {arXiv: 1909.10893},
  keywords = {invariance},
}

@article{scholkopf_causality_2019,
  title = {Causality for {Machine} {Learning}},
  url = {http://arxiv.org/abs/1911.10500},
  abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
  urldate = {2020-04-30},
  journal = {arXiv:1911.10500 [cs, stat]},
  author = {Schölkopf, Bernhard},
  month = dec,
  year = {2019},
  note = {arXiv: 1911.10500},
  keywords = {invariance},
}

@incollection{denton_unsupervised_2017,
  title = {Unsupervised {Learning} of {Disentangled} {Representations} from {Video}},
  url = {http://papers.nips.cc/paper/7028-unsupervised-learning-of-disentangled-representations-from-video.pdf},
  urldate = {2020-04-30},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
  publisher = {Curran Associates, Inc.},
  author = {Denton, Emily L and Birodkar, vighnesh},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  keywords = {self-supervision},
  pages = {4414--4423},
}

@article{singh_dont_2020,
  title = {Don't {Judge} an {Object} by {Its} {Context}: {Learning} to {Overcome} {Contextual} {Bias}},
  shorttitle = {Don't {Judge} an {Object} by {Its} {Context}},
  url = {http://arxiv.org/abs/2001.03152},
  abstract = {Existing models often leverage co-occurrences between objects and their context to improve recognition accuracy. However, strongly relying on context risks a model's generalizability, especially when typical co-occurrence patterns are absent. This work focuses on addressing such contextual biases to improve the robustness of the learnt feature representations. Our goal is to accurately recognize a category in the absence of its context, without compromising on performance when it co-occurs with context. Our key idea is to decorrelate feature representations of a category from its co-occurring context. We achieve this by learning a feature subspace that explicitly represents categories occurring in the absence of context along side a joint feature subspace that represents both categories and context. Our very simple yet effective method is extensible to two multi-label tasks -- object and attribute classification. On 4 challenging datasets, we demonstrate the effectiveness of our method in reducing contextual bias.},
  urldate = {2020-04-30},
  journal = {arXiv:2001.03152 [cs]},
  author = {Singh, Krishna Kumar and Mahajan, Dhruv and Grauman, Kristen and Lee, Yong Jae and Feiszli, Matt and Ghadiyaram, Deepti},
  month = jan,
  year = {2020},
  note = {arXiv: 2001.03152},
  keywords = {invariance},
}

@article{von_kugelgen_optimal_2019,
  title = {Optimal experimental design via {Bayesian} optimization: active causal structure learning for {Gaussian} process networks},
  shorttitle = {Optimal experimental design via {Bayesian} optimization},
  url = {http://arxiv.org/abs/1910.03962},
  abstract = {We study the problem of causal discovery through targeted interventions. Starting from few observational measurements, we follow a Bayesian active learning approach to perform those experiments which, in expectation with respect to the current model, are maximally informative about the underlying causal structure. Unlike previous work, we consider the setting of continuous random variables with non-linear functional relationships, modelled with Gaussian process priors. To address the arising problem of choosing from an uncountable set of possible interventions, we propose to use Bayesian optimisation to efficiently maximise a Monte Carlo estimate of the expected information gain.},
  urldate = {2020-04-30},
  journal = {arXiv:1910.03962 [cs, stat]},
  author = {von Kügelgen, Julius and Rubenstein, Paul K. and Schölkopf, Bernhard and Weller, Adrian},
  month = oct,
  year = {2019},
  note = {arXiv: 1910.03962},
  keywords = {exp\_design},
}

@article{ahuja_invariant_2020,
  title = {Invariant {Risk} {Minimization} {Games}},
  url = {http://arxiv.org/abs/2002.04692},
  abstract = {The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of Arjovsky et al. (2019). One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in Arjovsky et al. (2019). The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks.},
  urldate = {2020-04-27},
  journal = {arXiv:2002.04692 [cs, stat]},
  author = {Ahuja, Kartik and Shanmugam, Karthikeyan and Varshney, Kush R. and Dhurandhar, Amit},
  month = mar,
  year = {2020},
  note = {ZSCC: 0000001 
arXiv: 2002.04692},
  keywords = {invariance},
}

@article{tschannen_self-supervised_2019,
  title = {Self-{Supervised} {Learning} of {Video}-{Induced} {Visual} {Invariances}},
  abstract = {We propose a general framework for self-supervised learning of transferable visual representations based on video-induced visual invariances (VIVI). We consider the implicit hierarchy present in the videos and make use of (i) frame-level invariances (e.g. stability to color and contrast perturbations), (ii) shot/clip-level invariances (e.g. robustness to changes in object orientation and lighting conditions), and (iii) video-level invariances (semantic relationships of scenes across shots/clips), to define a holistic self-supervised loss. Training models using different variants of the proposed framework on videos from the YouTube-8M data set, we obtain state-of-the-art self-supervised transfer learning results on the 19 diverse downstream tasks of the Visual Task Adaptation Benchmark (VTAB), using only 1000 labels per task. We then show how to co-train our models jointly with labeled images, outperforming an ImageNet-pretrained ResNet-50 by 0.8 points with 10x fewer labeled images, as well as the previous best supervised model by 3.7 points using the full ImageNet data set.},
  journal = {ArXiv},
  author = {Tschannen, Michael and Djolonga, Josip and Ritter, Marvin and Mahendran, Aravindh and Houlsby, Neil and Gelly, Sylvain and Lucic, Mario},
  year = {2019},
  note = {ZSCC: 0000000},
  keywords = {invariance, self-supervision},
}

@article{van_der_wilk_learning_2018,
  title = {Learning {Invariances} using the {Marginal} {Likelihood}},
  url = {http://papers.nips.cc/paper/8199-learning-invariances-using-the-marginal-likelihood.pdf},
  urldate = {2020-04-27},
  journal = {Advances in Neural Information Processing Systems 31},
  author = {van der Wilk, Mark and Bauer, Matthias and John, ST and Hensman, James},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  year = {2018},
  note = {ZSCC: NoCitationData[s0]},
  keywords = {invariance},
  pages = {9938--9948},
}

@article{arjovsky_invariant_2020,
  title = {Invariant {Risk} {Minimization}},
  url = {http://arxiv.org/abs/1907.02893},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  urldate = {2020-04-25},
  journal = {arXiv:1907.02893 [cs, stat]},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  month = mar,
  year = {2020},
  note = {ZSCC: NoCitationData[s0] 
arXiv: 1907.02893},
  keywords = {invariance},
}

@article{newell_how_2020,
  title = {How {Useful} is {Self}-{Supervised} {Pretraining} for {Visual} {Tasks}?},
  url = {http://arxiv.org/abs/2003.14323},
  abstract = {Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at {\textbackslash}href\{https://www.github.com/princeton-vl/selfstudy\}\{github.com/princeton-vl/selfstudy\}.},
  urldate = {2020-04-25},
  journal = {arXiv:2003.14323 [cs]},
  author = {Newell, Alejandro and Deng, Jia},
  month = mar,
  year = {2020},
  note = {ZSCC: 0000000 
arXiv: 2003.14323},
  keywords = {self-supervision},
}
