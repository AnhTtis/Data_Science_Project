\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage[leftcaption]{sidecap}
\usepackage{enumitem}
\usepackage{rotating}
\usepackage{dblfloatfix}
\usepackage{listings}
\usepackage{float}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    showlines=true
}

\lstset{style=mystyle}

\newcommand{\ds}[1]{{\color{cyan}[Didac says: #1]}}
\newcommand{\sxm}[1]{}
\newcommand{\cv}[1]{{\color{red}[Carl says: #1]}}


\definecolor{mygrey}{rgb}{0.7, 0.7, 0.7}
\definecolor{mygrey2}{rgb}{0.5, 0.5, 0.5}
\definecolor{mymaroon}{rgb}{0.53, 0.15, 0.34}
\definecolor{mygreen}{rgb}{0.0, 0.6, 0.0}
\definecolor{mygreen}{rgb}{0.0, 0.647, 0.32}
\definecolor{myred1}{hsb}{1, 1., 0.9}
\definecolor{myred2}{hsb}{1, 0.4, 0.9}
\newcommand{\g}[1]{\textcolor{mygrey}{#1}}

\usepackage[T1]{fontenc}


\usepackage{xcolor, soul}
\sethlcolor{white}
\newcommand{\viper}[0]{{\small\fontfamily{txtt}\selectfont \textcolor{mygreen}{\hl{ViperGPT}}}\xspace}
\newcommand{\vipernormal}[0]{{\fontfamily{txtt}\selectfont \textcolor{mygreen}{\hl{ViperGPT}}}\xspace}

\newcommand{\viperemoji}{\includegraphics[height=1\fontcharht\font`\B]{snake.pdf}}


\usepackage[scaled=.8]{beramono}  % Scale texttt. Default is .9

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{11390} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi


\linepenalty=1000
\everypar{\looseness=-1}

\begin{document}


%%%%%%%%% TITLE
\title{\vipernormal: Composing Vision and Language via Code}

\title{\vipernormal: Zero-shot Compositions for Multimodal Reasoning}

\title{\vipernormal: a Visual Language Model for Programmatic Reasoning}
\title{\vspace{-0.3cm}\viperemoji\ \vipernormal: Visual Inference via Python Execution for Reasoning with GPT}

\title{\vspace{-0.3cm}\viperemoji\ \vipernormal: Visual Inference via Python Execution for Reasoning}

\title{\vipernormal: Visual Inference via Python Execution for Reasoning}


\makeatletter
\newcommand{\printfnsymbol}[1]{%
  \textsuperscript{\@fnsymbol{#1}}%
}
\makeatother

\author{D\'idac Sur\'is\thanks{Equal contribution. Order determined via coin flip and may be listed either way.}\hspace{0.16cm}, Sachit Menon\printfnsymbol{1}, Carl Vondrick\\Columbia University\\\href{http://viper.cs.columbia.edu}{\texttt{viper.cs.columbia.edu}}
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}

% Answering visual queries involves basic visual processing and reasoning about the results. Despite this, end-to-end models remain dominant, limiting interpretability and generalization. Learning modular programs offers an alternative approach but has been limited by the difficulty of
% learning to generate programs and each perceptual module. We present \viper, a framework that leverages large language models to compose vision and language models with logic based on any textual query. We accomplish this by generating Python code and specifying available modules as context in the form of an API. This simple approach achieves state-of-the-art results across various complex visual tasks.

Answering visual queries is a complex task that requires both visual processing and reasoning.  
End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. 
Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously.
We introduce \viper, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. \viper utilizes a provided API to access the available modules, and 
composes them by generating Python code that is later executed.
This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks. 
\end{abstract}
\vspace{-0.5cm}

%%%%%%%%% BODY TEXT
\section{Introduction}

How many muffins can each kid in Figure 1 (top) eat for it to be fair? To answer this, we might 1) find the children and the muffins in the image, 2) count how many there are of each, and 3) reason that `fair' implies an even split, hence divide. People find it natural to compositionally combine individual steps together to understand the visual world. Yet, the dominant approach in the field of computer vision remains end-to-end models, which do not inherently leverage this compositional reasoning. 

Although the field has made large progress on individual tasks such as object recognition and depth estimation, end-to-end approaches to complex tasks must learn to implicitly perform all tasks within the forward pass of a neural network.
Not only does this fail to make use of the advances in fundamental vision tasks at different steps, it does not make use of the fact that computers can perform mathematical operations (\eg, division) easily without machine learning. We cannot trust neural models to generalize systematically to different numbers of muffins or children. End-to-end models also produce fundamentally uninterpretable decisions -- there is no way to audit the result of each step to diagnose failure. As models grow increasingly data and compute-hungry, this approach grows increasingly untenable. We would like to perform new tasks without additional training by recombining our existing models in new ways.

What limits us from creating such modular systems for more complex tasks?
In previous years, the pioneering works of Neural Module Networks \cite{Andreas_2016_CVPR,johnson_inferring_2017,hu_learning_2017}
attempted to decompose tasks into simpler modules. By training end-to-end with modules rearranged in different ways for different problems, the hope was that each module would learn their appropriate function and thereby become reusable. However, numerous issues made this approach difficult to extend to the real world. %\sxm{better phrasing for prev two sentences}. 
In particular, program generation relied on hand-tuned natural language parsers \cite{Andreas_2016_CVPR}, or otherwise required reinforcement learning from scratch and were thus difficult to optimize \cite{hu_learning_2017,johnson_inferring_2017}. In each case, program generation was highly domain-limited. Furthermore, learning the perceptual models jointly with the program generator made training even more difficult, often failing to produce the intended modular structure \cite{bahdanau_systematic_2019,subramanian_obtaining_2020}.

\begin{figure*}[p]
    \includegraphics[width=\linewidth]{out_of_distribution.pdf}
    \caption{\textbf{In-the-wild results.} Given a visual input and a query, \viper synthesizes a program, then executes it with the Python interpreter in order to produce the final answer. This figure shows both the generated code, and the result of intermediate variables during the execution. By composing pretrained modules, \viper obtains answers that are both correct and interpretable for open-world queries.}
    \label{fig:ood}
\end{figure*}

In this work, we present \viper\footnote{We name our method after a snake because it executes Python code.}, a framework that overcomes these bottlenecks by leveraging  code generating large language models (\eg  GPT-3 Codex \cite{chen2021evaluating}) to flexibly compose vision models based on any textual query that defines the task. It creates customized programs for each query that take images or videos as argument and return the result of the query for that image or video. We show that providing Codex an API exposing various visual capabilities (\eg \texttt{find}, \texttt{compute\_depth}), just as one might provide an engineer, is sufficient for the creation of these programs. The model's prior training on code enables it to reason about how to use these functions and implement the relevant logic. Our results demonstrate that this simple approach delivers remarkable zero-shot performance (\ie without ever training on task specific images).


Our simple approach enjoys many benefits: it is 1) \emph{interpretable}, as all the steps are explicit as code function calls with intermediate values that can be inspected; 2) \emph{logical}, as it explicitly uses built-in Python logical and mathematical operators; 
3) \emph{flexible}, as it can easily incorporate any vision or language module, only requiring the specification of the associated module be added to the API; 4) \emph{compositional}, decomposing tasks into smaller sub-tasks performed step-by-step; 5) \emph{adaptable} to advances in the field, as improvements in any of the used modules will result in a direct improvement in our approach's performance; 6) \emph{training-free}, as it does not require to re-train (or finetune) a new model for every new task; and finally, 7) \emph{general}, as it unifies all tasks into one system.

In summary, our contributions are:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item We propose a simple framework for solving complex visual queries by integrating code-generation models into vision with an API and the Python interpreter, with the benefits above.
    \item We achieve state-of-the-art zero-shot results across tasks in visual grounding, image question answering, and video question-answering, showing this interpretability \textit{aids} performance rather than hindering it.
    \item To promote research in this direction, we develop a Python library enabling rapid development for program synthesis for visual tasks, which will be open-sourced upon publication. 
\end{enumerate}

\section{Related Work}

\textbf{Modular Vision.} Our work takes inspiration from Neural Module Networks \cite{Andreas_2016_CVPR,johnson_inferring_2017}, who argue that complex vision tasks are fundamentally compositional and propose dividing them into atomic perceptual units. This visual reasoning procedure has been explored by a variety of works \cite{kimvisual,Whitehead_2021_CVPR}.
Posterior efforts have focused on explicitly reasoning about the composition by separating the reasoning from the perception, with connections to neuro-symbolic methods \cite{hu_learning_2017,johnson_inferring_2017,yi_neural-symbolic_2018}. These approaches are similar in spirit to ours, but require expensive supervision in the form of programs and end-to-end train the perception modules, which makes them not generalizable to different domains.

Due to the practical difficulty of using these methods, the field has primarily moved towards end-to-end all-in-one models \cite{alayrac2022flamingo,hu2022reveal,huang2023language,li_blip-2_2023}. Such models currently obtain state-of-the-art results, and we compare to them in Section~\ref{sec:experiments}.
Other recent works \cite{zeng2022socraticmodels,reddy2022mumuqa,wang2022language,mao_doubly_2022,menon_visual_2022,gatti2022cofar} show that large pretrained models can be used together to great effect, but hand-specify the particular way models are combined.%Recently, large language models (LLMs) have enabled new module integration approaches. 

Over the course of this project, a surge of interest in the area has resulted in a number of related manuscripts appearing on arXiv which use large language models (LLMs) for automatic module integration. 
In the natural language processing domain, they have been aimed at using external tools \cite{schick2023toolformer,parisi2022talm}, or for structured reasoning using Codex \cite{madaan2022language,wang2022code4struct,gao2022pal,chen2022program}. %The latter use Codex, like us. However, \ds{However what? They are not as flexible as ours (only one of them uses for loops, if statements, and they don't show a single result, just prompts. They also don't use external tools, and most important, they don't do any vision. Maybe we should just treat them as different just based on the fact that they are not vision papers?}
Concurrent work \cite{gupta2022visual} generates a list of pseudocode instructions and interprets them as a `visual program,' relying on in-context learning from provided examples. % uses hand-designed pseudocode examples to % examples in a hand-designed pseudocode 
Unlike them, we directly generate unrestricted Python code, which is much more flexible and enables us to demonstrate more advanced emergent abilities, such as control flow and math. Crucially, using Python allows us to leverage the strong prior knowledge Codex learns by training at scale from the Internet. Additionally, we evaluate on many established benchmarks measuring visual understanding and achieve top-performing zero-shot results.

%Our work differs from all of these in that it directly generates Python code with the full functionality of the language to perform complex visual queries given an API for various vision tasks.

\begin{figure}[t!]
    \centering
    \includegraphics{maestro-schematic.pdf}
    \caption{\textbf{Method}. \viper is a framework for solving complex visual queries programmatically.}
    \label{fig:method}
    \vspace{-0.5cm}
\end{figure}

% \ds{Other citations?:}
% problems with VQA testing \cite{luo2021just},
% Logical consistency: \cite{saha2022murmur},
% Interpretability in VQA, Visual Grounding (GradCAM-type stuff, etc)

\textbf{Interpretability.} The area of interpretability for complex queries in vision %such as grounding and visual question answering 
is extensive. Many approaches provide explanations in the form of pixel importance, \`a la GradCAM \cite{selvaraju_grad-cam_2020,zhang_interpretable_2018,deng_visual_nodate,park_multimodal_2018}, some also providing textual explanations \cite{park_multimodal_2018}. These are often post-hoc explanations rather than by construction, and do not give step-by-step reasoning including image crops and text. Hard attention in captioning \cite{xu_show_2016} aims for a similar goal regarding intermediate image crops, similarly to our \texttt{find} module, but has proven difficult to incorporate into learning algorithms. See He \etal \cite{he2021interpretable} for a complete overview.

% \textbf{Sequential processing:} Many have called for sequential reasoning
% - Hard attention comparison: `find' is like hard attention
\textbf{Pretrained models.} The perception and external knowledge modules used by \viper are GLIP~\cite{li2022grounded} for object detection, X-VLM~\cite{zeng2021multi} for text-image similarity (as it surpasses CLIP~\cite{radford2021learning} at attribute detection \cite{Bravo_2022_ovad}), MiDaS~\cite{Ranftl2022} for depth estimation, GPT-3~\cite{brown_language_2020} for external knowledge, and \mbox{BLIP-2}~\cite{li_blip-2_2023} for simple visual queries.

\section{Method}\label{sec:method}
We use notation following Johnson \etal\cite{johnson_inferring_2017}.
Given a visual input $x$ and a textual query $q$ about its contents, we first synthesize a program $z = \pi(q)$ with a program generator $\pi$ given the query. We then apply the execution engine $r = \phi(x,z)$ to execute the program $z$ on the input $x$ and produce a result $r$. Our framework is flexible, supporting image or videos as inputs $x$, questions or descriptions as queries $q$, and any type (\eg, text or image crops) as outputs $r$. 

%Unlike prior work in neural module networks, 
While prior work represents programs as graphs, like syntax trees \cite{johnson_inferring_2017} or dependency graphs \cite{cao_interpretable_2021}, we represent the class of programs $z \in \mathcal{Z}$ directly through Python code, allowing our programs to capitalize on the expressivity and capabilities afforded by modern programming languages.

\subsection{Program Generation}

Johnson \etal\cite{johnson_inferring_2017} and other work in this direction \cite{hu_learning_2017,yi_neural-symbolic_2018,hudson_compositional_2018} typically implement $\pi$ with a neural network that is trained with either supervised or reinforcement learning in order to estimate programs from queries. However, these approaches have largely been unable to scale to in-the-wild settings because either a) the supervision in the form of programs cannot be collected at scale or b) the optimization required for finding the computational graph is prohibitive.

In our approach, we instead capitalize on LLMs for code generation in order to instantiate the program generator $\pi$ that composes vision and language modules together. 
LLMs take as input a tokenized code sequence (``prompt'') and autoregressively predict subsequent tokens. We use Codex \cite{chen2021evaluating}, which has shown remarkable success on code generation tasks. Since we replace the optimization of $\pi$ with an LLM, our approach 
obviates the need for task-specific training for program generation. Using Codex as the program generator and generating code directly in Python allows us to draw on training at scale on the Internet, where Python code is abundant.

\begin{figure}[t]
    \includegraphics[width=\columnwidth]{fig_refcoco.pdf}
    \caption{\textbf{Visual grounding on RefCOCO.} 
    % \viper translates `front' to lower depth, returning the correct location.
    }
    \label{fig:refcoco}
    % \vspace{-0.5cm}
\end{figure}

% To leverage LLMs in this way, we need to define a prompt that will sample programs $z$ that compose and call these modules as needed. Our prompt consists of two components. Firstly, we provide an application programming interface (API) to the LLM as part of its input context. This API specifies Python functions corresponding to perception and knowledge modules; it provides their input and output types, as well as documentation strings to explain the purpose of the function in natural language. Secondly, we provide a few examples of API usage along with the specification in the form of query-code pairs, similarly to in-context learning \cite{suris2020learning,brown_language_2020}. Note that no visual input from any dataset is ever provided to Codex. Following the terminology in previous work Flamingo \cite{alayrac2022flamingo}, we consider this setting a zero-shot setting.

% The final input to the LLM is a sequence of code text consisting of the API specification, (optional) in-context examples, and finally the query for the sample under consideration. The expected output is a Python function definition as a string, which we then compile and run.

To leverage LLMs in this way, we need to define a prompt that will sample programs $z$ that compose and call these modules as needed. Our prompt consists of an application programming interface (API), detailed in the following section, which we provide to the LLM as part of its input context. 
The final input to the LLM is a sequence of code text consisting of the API specification followed by the query for the sample under consideration. The expected output is a Python function definition as a string, which we then compile and execute.


\begin{table}[t]
    \caption{\textbf{RefCOCO Results}. We report accuracy on the REC task and testA split. ZS=zero shot, \textcolor{mygrey2}{Sup.}=supervised.}
    \label{tab:refcoco_results}
    \centering
        \begin{tabular}{c l c c}
        \toprule
        & &\multicolumn{2}{c}{\textbf{IoU (\%)} $\uparrow$}  \\   
        & & RefCOCO & RefCOCO+\\
        \midrule
        \multirow{2}{*}{\rotatebox[origin=c]{90}{\g{Sup.}}}
        & \g{MDETR \cite{wang2022ofa}}  & \g{90.4} & \g{85.5} \\
        & \g{OFA \cite{wang2022ofa}}  & \g{94.0} & \g{91.7} \\
        \midrule
        \multirow{4}{*}{\rotatebox[origin=c]{90}{ZS}}
        & OWL-ViT \cite{minderer2022simple} & 30.3 & 29.4 \\
        & GLIP \cite{li2022grounded} & 55.0 & 52.2\\
        & ReCLIP \cite{subramanian-etal-2022-reclip} & 58.6 & 60.5 \\
        & \vipernormal (ours) & \textbf{72.0} & \textbf{67.0} \\
        \bottomrule
    \end{tabular}
        \vspace{-0.3cm}
\end{table}


\subsection{Modules and Their API}
\label{sec:api}

Our prompt, included in the Appendix~\ref{sec:appendix_api}, provides the API for different perceptual and knowledge modules, such as for object detection, depth estimation, or language model queries. From this prompt, we found that LLMs are able to induce correct programs $z$ from the query $q$.

\begin{figure*}[t]
    \includegraphics[width=\linewidth]{fig_gqa.pdf}
    \caption{\textbf{Compositional image question answering on GQA.}}
    \label{fig:gqa}
    % \vspace{-0.2cm}
\end{figure*}


The API we provide defines two global classes \texttt{ImagePatch} and \texttt{VideoSegment}, which represent an image patch and a video segment respectively. Each module is implemented as a class method, which internally calls a pretrained model to compute the result. For example, the \texttt{compute\_depth} method of \texttt{ImagePatch} returns an estimate of the median (relative) depth of the pixels in the image patch; we implement this with state-of-the-art large-scale models such as MiDaS~\cite{Ranftl2022}. We provide more details about the modules used in Section~\ref{sec:experiments}. 
% Internally, each one of these methods makes a call to the corresponding module that computes the result. In the previous example, the \texttt{depth()} method makes a call to the \texttt{DepthEstimation} module, which we implement using state-of-the-art large-scale models such as MiDaS~\cite{Ranftl2022}. 

The API specifies the input and output types for each method it defines, as well as docstrings to explain the purpose of these functions in natural language. Like most APIs, it additionally provides examples that show how to use these classes and their functions, specified in the form of query-code pairs similarly to in-context learning \cite{suris2020learning,brown_language_2020}. %One limitation of the current approach is that all of these together are limited by the context window size; while this suits the setting of our work (with a relatively small set of visual primitives, limited human-generated exemplars, etc), it may pose a limitation in circumstances where there are many modules or when human-generated programs can be collected at scale where there are many ground truth programs available. This is an active area of research in code generation \cite{etc,longformer,flashattention}, and any advances directly can be applied in \viper. Note that no visual input from any dataset is ever provided to Codex. Following the terminology in previous work Flamingo \cite{alayrac2022flamingo}, we consider this setting a zero-shot setting.

The input to Codex does not contain the full \textit{implementation} of the API. Instead, it is given the \textit{specification} for the API, including the function signatures and docstrings. Abstracting away the implementation details is beneficial for two reasons. First, LLM context windows are limited in size \cite{brown_language_2020}, making it infeasible to include the entire implementation. In addition, the abstraction makes code generation independent of changes made to the module implementation.

End-to-end perception modules are excellent when used in the right places, and \viper strongly relies on them. Analogous to dual-system models \cite{kahneman2011thinking} in cognitive science, we argue that generated programs (System 2 - analytic) should be utilized to break down tasks that require multiple steps of reasoning into simpler components, where end-to-end perception modules (System 1 - pattern recognition) are the most effective approach. By composing end-to-end modules into programs, \viper brings the System 2 capability of \textit{sequential processing} to deep learning \cite{bengio_consciousness_2019}.



\subsection{Program Execution}

At execution time, the generated program $z$ accepts an image or video as input and outputs a result $r$ corresponding to the query provided to the LLM. To execute this program, previous work (\eg, \cite{johnson_inferring_2017}) learns an execution engine $\phi$ as a neural module network, composing various modules implemented by neural networks. Their modules are responsible for not only perceptual functions such as \texttt{find}, but also logical ones such as \texttt{compare}. They learn all neural modules together simultaneously end-to-end, which fails to enable systematic generalization \cite{bahdanau_systematic_2019} and results in modules that are not \textit{faithful} to their intended tasks \cite{subramanian_obtaining_2020}, compromising the interpretability of the model.

 We provide a simple, performant alternative by using the Python interpreter in conjunction with modules implemented by large pretrained models. The Python interpreter enables logical operations while the pretrained models enable perceptual ones. Our approach guarantees faithfulness by construction.
 
 The program is run with the Python interpreter; as such, \emph{its execution is a simple Python call}. 
This means it can leverage all built-in Python functions like \texttt{sort}; control flow tools like \texttt{for} or \texttt{if/else}; and modules such as \texttt{datetime} or \texttt{math}. Notably, this does not require a custom interpreter, unlike prior approaches \cite{gupta2022visual,schick2023toolformer} % or \texttt{any}
Another advantage of a fully Pythonic implementation is compatibility with a wide range of existing tools, such as PyTorch JIT \cite{pytorch_neurips2019}. 



\begin{table}
    \caption{\textbf{GQA Results}. We report accuracy on the test-dev set.}
    \label{tab:gqa_results}
    \centering
         \resizebox{0.7\columnwidth}{!}{\begin{tabular}{c l c}
        \toprule
        & &\textbf{Accuracy (\%)} $\uparrow$ \\   
        \midrule
        \multirow{4}{*}{\rotatebox[origin=c]{90}{\g{Sup.}}}
        & \g{LGCN \cite{hu2019language}} & \g{55.8} \\
        & \g{LXMERT \cite{tan2019lxmert}} & \g{60.0} \\
        & \g{NSM \cite{hudson2019learning}}  & \g{63.0} \\
        & \g{CRF \cite{nguyen2022coarse}}  & \g{72.1} \\
        \midrule
        % & FewVLM \cite{jin2022a} & 29.3 \\
        \multirow{2}{*}{\rotatebox[origin=c]{90}{ZS}}
        & BLIP-2 \cite{li_blip-2_2023} & 44.7 \\
        & \vipernormal (ours) & \textbf{48.1} \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.5cm}
\end{table}

In our implementation, each program in a generated batch is run simultaneously with multiprocessing. 
Our producer-consumer design \cite{dijkstra_information_1972} enables efficient GPU batching, reducing the memory and computation costs. Our code is made available at \href{https://viper.cs.columbia.edu/}{\texttt{viper.cs.columbia.edu/}}.

\section{Evaluation}
\label{sec:experiments}
\viper is applicable to any tasks that query visual inputs with text. Unlike other work using large language models for vision tasks, the return values of our programs can be of arbitrary types, such as text, multiple choice selections, or image regions. We select four different evaluation settings to showcase the model's diverse capabilities in varied contexts without additional training. The tasks we consider are: 
1) visual grounding, % with the RefCOCO and RefCOCO+ datasets \cite{yu_refcoco_2016},
2) compositional image question answering, % with the GQA dataset \cite{hudson_gqa_2019},
3) external knowledge-dependent image question answering, and % with the OKVQA dataset \cite{marino_ok-vqa_2019}, and 
4) video causal and temporal reasoning.%, with the NeXT-QA dataset \cite{xiao2021next}
% \end{enumerate}

We consider these tasks to roughly build on one another, with visual grounding being a prerequisite for compositional image question answering and so on. In the following sections, we explore the capabilities \viper demonstrates in order to solve each task. 


    
\subsection{Visual Grounding}



% Given an image and a natural language query describing something that can be found in that image, the task of visual grounding aims to return the bounding box that best corresponds to the text query. It is thus closely related to text-modulated object detection, but encompasses a much broader space of queries. While object detection typically aims \sxm{change word<-} to find all bounding boxes associated with a given category, visual grounding can be as flexible as language can be. In addition to finding instances of a category, visual grounding can require reasoning about spatial relations, visual attributes, and more. We consider this task first as it serves as the first bridge between modalities: many tasks require locating complex queries past locating particular objects.

Visual grounding is the task of identifying the bounding box in an image that corresponds best to a given natural language query. 
Visual grounding tasks evaluate reasoning about spatial relationships and visual attributes.
We consider this task first as it serves as the first bridge between text and vision: many tasks require locating complex queries past locating particular objects.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{fig_okvqa.pdf}
    \caption{\textbf{Programmatic chain-of-thought with external knowledge for OK-VQA.}}
    \label{fig:okvqa}\vspace{-1em}
\end{figure}


We provide \viper with the  API for the following modules (pretrained models in parentheses). \texttt{\textbf{find}}~({\footnotesize GLIP \cite{li2022grounded}}) takes as input an image and a short noun phrase (\eg ``car'' or ``golden retriever''), and returns a list of image patches containing the noun phrase. \texttt{\textbf{exists}} ({\footnotesize GLIP \cite{li2022grounded}}) takes as input an image and a short noun phrase and returns a boolean indicating whether an instance of that noun phrase is present in the image. Similarly, \texttt{\textbf{verify\_property}} ({\footnotesize X-VLM~\cite{zeng2021multi}}) takes as input an image, a noun phase representing an object, and an attribute representing a property of that object; it returns a boolean indicating whether the property is present in the image. \texttt{\textbf{best\_image\_match}} ({\footnotesize X-VLM~\cite{zeng2021multi}}) takes as input a list of image patches and a short noun phrase, and returns the image patch that best matches the noun phrase. Symmetric to this operation, \texttt{\textbf{best\_text\_match}} takes as input a list of noun phrases and one image, and returns the noun phrase that best matches the image. (This module is not necessary for visual grounding, but rather for tasks with text outputs; we describe it here for simplicity.) They are implemented using an image-text similarity model as in CLIP~\cite{radford2021learning}. Finally, \texttt{\textbf{compute\_depth}} ({\footnotesize MiDaS~\cite{Ranftl2022}}) computes the median depth of the image patch. 
We also define the function \texttt{\textbf{distance}}, which computes the pixel-distance between two patches, using only built-in Python tools.

For evaluation, we use the RefCOCO and RefCOCO+ datasets. The former allows for spatial relations while the latter does not, thereby providing different insights into \viper's capabilities. We compare \viper against end-to-end methods, and outperform other zero-shot methods on both datasets (see Table~\ref{tab:refcoco_results}). We show examples\footnote{Examples in the paper have been cosmetically cleaned by removing comments and error handling, but the logic is unchanged.} in Figure~\ref{fig:refcoco}.
See Appendix~\ref{sec:appendix_models} for more details about the experimental setup.

\begin{figure*}[t]
    \includegraphics[width=\linewidth]{fig_nextqa.pdf}
    \caption{\textbf{Temporal reasoning on NeXT-QA.}}
    \label{fig:nextqa}
\end{figure*}


\subsection{Compositional Image Question Answering}

We also evaluate \viper on image question answering. We focus on compositional question answering, which requires decomposing complex questions into simpler tasks. 
We use the GQA dataset \cite{hudson_gqa_2019}, which was created to measure performance on complex compositional questions. 
Consider Figure \ref{fig:gqa} for example questions as well as our provided reasoning. Even if a question \textit{can} be answered end-to-end, it is both more interpretable and more human-aligned to provide intermediate reasoning rather than requiring the model to compress all steps into one forward pass; as our final result is constructed directly from the intermediate values, they provide a fully faithful interpretation of how the model came to its answer.

For GQA, we incorporate the module \texttt{\textbf{simple\_query}} ({\footnotesize BLIP-2~\cite{li2022grounded}}), which handles basic queries that are not further decomposable, such as ``What animal is this?'' We also add the aforementioned \texttt{\textbf{best\_text\_match}}. This leads us to the best accuracy on GQA among zero-shot models (Table~\ref{fig:gqa}).


\begin{table}
\caption{\textbf{OK-VQA Results}.}
\label{tab:okvqa_results}
\centering
    \begin{tabular}{c l c}
    \toprule
    &&\textbf{Accuracy (\%)} $\uparrow$ \\   
    \midrule
    \multirow{5}{*}{\rotatebox[origin=c]{90}{\g{Sup.}}}
    % \g{\textbf{VLC-BERT} \cite{}} & \g{$43.1$} \\
    & \g{TRiG \cite{gao2022transform}} & \g{50.5} \\
    & \g{KAT \cite{gui-etal-2022-kat}} & \g{54.4} \\
    & \g{RA-VQA \cite{lin-byrne-2022-retrieval}} & \g{54.5} \\
    & \g{REVIVE \cite{lin2022revive}}  & \g{58.0} \\
    & \g{PromptCap \cite{hu2022promptcap}}  & \g{58.8} \\
    \midrule
    \multirow{5}{*}{\rotatebox[origin=c]{90}{ZS}}
    & PNP-VQA \cite{tiong-etal-2022-plug} & 35.9 \\
    & PICa \cite{yang2022empirical} & 43.3 \\ %-Base %(16-shot) 
    & BLIP-2 \cite{li_blip-2_2023} & 45.9 \\
    % & \textbf{PICa} (16-shot) \cite{yang2022empirical} & 48.0 \\
    & Flamingo \cite{alayrac2022flamingo} & 50.6 \\ % (zero-shot)
    % & \textbf{Flamingo (32-shot)} \cite{alayrac2022flamingo} & 57.8 \\
    & \vipernormal (ours) & \textbf{51.9} \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{External Knowledge-dependent Image Question Answering}

Many questions about images can only be answered correctly by integrating outside knowledge about the world. By equipping \viper with a module to query external knowledge bases in natural language, it can combine knowledge with visual reasoning to handle such questions. We add a new module \texttt{\textbf{llm\_query}} ({\footnotesize GPT-3~\cite{brown_language_2020}}), which exploits text models as unstructured knowledge bases. We find that the combination of step-by-step reasoning from Codex along with external knowledge queried from \mbox{GPT-3}'s text model achieves impressive performance in this setting.

We evaluate on the OK-VQA dataset \cite{marino_ok-vqa_2019}, which is designed to evaluate models' ability to answer questions about images that require knowledge that cannot be found in the image. Items in this dataset often require more than one step of reasoning to produce a correct answer. For example, in Figure \ref{fig:okvqa}, one must first perceive from the image that ``this toy'' is a ``bear,'' then use external knowledge to answer what bears do in the winter. End-to-end models must directly produce an answer, and therefore may pick words that are more directly related to the image than the question intended. In this case, the best available end-to-end model guesses ``ski,'' presumably as that is a common winter activity (though, not for bears). \viper, on the other hand, can employ a form of chain-of-thought reasoning \cite{wei_chain_2022} to break down the question as previously described, first determining the type of toy using perception modules and then using the perceived information in conjunction with an external knowledge module to produce the correct response.

\viper outperforms all zero-shot methods, and when compared to models using publicly available resources, it surpasses the best previous model by $6\%$, a wide margin for this dataset (see Table~\ref{tab:okvqa_results}).

\subsection{Video Causal/Temporal Reasoning}

We also evaluate how \viper extends to videos and queries that require causal and temporal reasoning. To explore this, we use the NExT-QA dataset, designed to evaluate video models ability to perform this type of reasoning. 
We evaluate using the NExT-QA multiple choice version.

We provide an additional module \texttt{\textbf{select\_answer}} ({\footnotesize \mbox{GPT-3}~\cite{brown_language_2020}}), which, given textual information about a scene and a list of possible answers, returns the answer that best fits the information. Other than that, the only additional content given in the API is the definition of the class \texttt{VideoSegment}, that contains the video bytestream as well as the start and end timestamps of the video segment that it represents. It also defines an iterator over the frames, which returns an \texttt{ImagePatch} object representing every frame.

\begin{table}
\caption{\textbf{NExT-QA Results}. Our method gets overall state-of-the-art results (including \emph{supervised} models) on the hard split. ``T'' and ``C'' stand for ``temporal'' and ``causal'' questions, respectively.}
\label{tab:nextqa_results}
\centering
    \resizebox{\columnwidth}{!}{\begin{tabular}{c l c c c}
    \toprule
    & &\multicolumn{3}{c}{\textbf{Accuracy (\%)} $\uparrow$}  \\   
    \cmidrule(lr){3-5}
    & & Hard Split - T & Hard Split - C & Full Set\\   
    \midrule
    \multirow{3}{*}{\rotatebox[origin=c]{90}{\g{Sup.}}}
    & \g{ATP \cite{buch2022revisiting}} & \g{45.3} & \g{43.3} & \g{54.3}\\
    & \g{VGT \cite{xiao2022video}}  & \g{-}  & \g{-}& \g{56.9}\\
    & \g{HiTeA \cite{ye2022hitea}}  & \g{48.6} & \g{47.8}& \g{63.1} \\
    \midrule
    \multirow{1}{*}{\rotatebox[origin=c]{90}{ZS}}
    & \vipernormal (ours) & \textbf{49.8} & \textbf{56.4}& 60.0\\
    \bottomrule
\end{tabular}}
\end{table}

We find that despite only being provided with perception modules for images, \viper displays emergent causal and temporal reasoning when applied to videos provided as an ordered list of images. In particular, we observe it generates programs that apply perception to determine which frames are relevant for a given query, then reasons about the information extracted from these frames along with associated frame numbers to produce a final answer. 

Despite seeing no video data whatsoever, \viper achieves accuracy results on par with the best \textit{supervised} model (see Table~\ref{tab:nextqa_results}), and even surpassing it on the NeXT-QA hard split \cite{buch2022revisiting}, both for temporal and causal queries. Of course, the framework of \viper also allows for incorporation of video models, which we expect would further improve the performance well beyond this threshold. 


Computational ability presents even more of an obstacle for video understanding than for images. It is infeasible to fit every frame of a moderately-sized video into GPU memory on even the best hardware. \viper may provide a way forward for video understanding that overcomes the limitations of systems that need to perform computation on a whole video simultaneously. See examples in Figure~\ref{fig:nextqa}.

\begin{SCfigure}
  \centering
  \includegraphics[width=0.6\columnwidth]{refcoco_ablation.pdf}
    \label{fig:intervention}
    \caption{\textbf{Intervention.} We analyze the importance of various \textcolor{myred1}{vision modules} and \textcolor{myred2}{Python functions} in the generated programs as measured by the drop in mIoU when they are made nonfunctional.
  }
\end{SCfigure}

\section{Exploring New Capabilities}

In this section, we showcase various interesting capabilities enabled by use of \viper.

\subsection{Queries Beyond Benchmarks}

We believe that the evident strength of this approach may not be adequately explored by existing benchmarks, which are designed for end-to-end models. In Figure \ref{fig:ood}, we show examples of interesting queries that are interesting in the real world but would not show up in existing benchmarks. We do not add any new API specifications other than the ones already used in the benchmarks. See the Appendix~\ref{sec:appendix_api} for more details.

These examples show that the modules we included are general and cover a wide range of tasks. In settings where new capabilities are required, the framework is general and permits the addition of any modules, like \texttt{ocr}, \texttt{surface\_normal\_estimation}, \texttt{segmentation}, etc.


\subsection{Interventional Explainability}
\looseness=-1
Our programmatic approach enables automatic diagnosis of which modules are responsible for prediction errors, potentially informing which types of models to improve and where to collect more data.  Evaluating the intermediate output of each module is impractical due to the lack of ground truth labels, and naively comparing accuracy
between programs that use a certain module and those that do not could be confounded \eg by the difficulty of the problem.  We can instead perform \textit{interventions} to better understand a module's performance. For each module, we can define a default value that provides no information, and substitute the underlying model for this default output. For instance, \texttt{find} could always return the full input image. We can then consider how much performance drops if evaluating the same code for the examples that use that module. If the intervention has a minimal impact on performance, the module is likely not useful.


\begin{figure}[t]
    \includegraphics[width=\linewidth]{fig_context.pdf}
    \caption{\textbf{Contextual programs}. \viper readily incorporates additional context into the logic of the generated programs.}
    \label{fig:context}
    \vspace{-0.3cm}
\end{figure}


We show an example of this analysis in Figure~\ref{fig:intervention} for visual grounding on RefCOCO, where we observe a similar level of importance for perception modules and Python operations. Both are tightly integrated in our approach.

\subsection{Conditioning on Additional Information}

We found \viper readily admits program generation based on additional knowledge. This context can be provided as a comment prior to the code generation. Such context can be critical to correctly responding to a wide range of queries. In Figure \ref{fig:context} we show one such example. The correct side of the road varies by country, so the initial query cannot be answered. Provided with the context of where the photo was taken, the model produces different logic for each case, adjusted based on the relevant prior knowledge.

\vspace{-0.1cm}
\section{Conclusions}
\vspace{-0.1cm}

We present \viper, a framework for programmatic composition of specialized vision, language, math, and logic functions for complex visual queries. \viper is capable of connecting individual advances in vision and language; it enables them to show capabilities beyond what any individual model can do on its own. As the models implementing these functions continue to improve,  we expect \viper's results will also continue to improve in tandem.


{\small\textbf{Acknowledgements:} This research is based on work partially supported by the DARPA MCS program under Federal Agreement No.\ N660011924032 and the NSF CAREER Award \#2046910. DS is supported by the Microsoft PhD Fellowship and SM is supported by the NSF GRFP.}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib,references}
}



\appendix
% \input{appendix}
\input{appendix_singlecolumn}
\end{document}
