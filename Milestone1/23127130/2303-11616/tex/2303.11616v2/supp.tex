\newpage
\onecolumn 

\vspace{0.5in}
\begin{center}
 \rule{6.875in}{0.7pt}\\ % 4.0
 {\Large\bf Supplementary Material for\\ `` HRDFuse: Monocular 360$^\circ$ Depth Estimation by Collaboratively Learning Holistic-with-Regional Depth Distributions ''}
 \rule{6.875in}{0.7pt}
\end{center}
\appendix

\section{Abstract}
\label{ab:ab}
Due to the lack of space in the main paper, we provide more details of the proposed method and experimental results in the supplementary material. Sec.~\ref{ab:tp} adds more details of tangent projection. And in Sec.~\ref{ab:fs}, we further discuss the necessity of the SFA module. Sec.~\ref{ab:cddc} provides the detailed calculation progress of the Collaborative Depth Distribution Classification (CDDC) module. Sec.~\ref{ab:loss} introduces our loss function, and Sec.~\ref{ab:data} presents a detailed description of the used benchmark datasets and metrics. In the Sec.~\ref{ab:com_res}  and Sec.~\ref{ab:vis_res} , we show additional comparison results and visual results about experiments. Furthermore, we discuss the rationality of SFA module in the Sec.~\ref{ab:sfa} and show some comparison results on real data in the Sec.~\ref{ab:vis_real}.

\section{More Details of Tangent Projection}
\label{ab:tp}

We start by introducing an example of the tangent projection (TP)~\cite{Ai2022DeepLF}. As shown in Fig.~\ref{fig:suptp}, $P_s$ is a point on the sphere surface, $O$ is the center of the sphere, $P_c$ is the center of the tangent plane, and $P_t$ is the intersection point of the tangent plane and the extension line of $\overrightarrow{OP_s}$. As both $P_s$ and $P_c$ are on the sphere surface, we represent their spherical coordinates as $(\theta_s,\phi_s)$ and $(\theta_c,\phi_c)$, respectively. Then, we can obtain the planar coordinate $(u_t,v_t)$ of the point $P_t$ on the tangent plane as follows:
\begin{equation}
\small
\begin{split}
&    u_t=\frac{\cos(\phi_s)\sin(\theta_s-\theta_c)}{\cos(c)}, \\
&    v_t=\frac{\cos(\phi_c)\sin(\phi_s)-\sin(\phi_c)\cos(\phi_s)\cos(\theta_s-\theta_c)}{\cos(c)}, \\
&   \cos(c)=\sin(\phi_c)\sin(\phi_s)+\cos(\phi_c)\cos(\phi_s)\cos(\theta_s-\theta_c).
\end{split}
\label{eq.1}
\end{equation}
And the inverse transformations are:
\begin{equation}
\small
\begin{split}
& \theta_s = \theta_c + \tan^{-1}(\frac{u_t\sin(\sigma)}{\gamma\cos(\phi_c)\cos(\sigma)-v_t\sin(\phi_c)\sin(\sigma)}), \\
& \phi_s=\sin^{-1}(\cos(\sigma)\sin(\phi_c)+\frac{1}{\gamma}v_t\sin(\sigma)\cos(\phi_c)),
\end{split}
\label{eq.2}
\end{equation}

\noindent where $\gamma=\sqrt{u_{t}^{2}+v_{t}^{2}}$ and $\sigma=\tan^{-1}\gamma$. With Eq.\ref{eq.1} and Eq.~\ref{eq.2}, we can convert the points on the sphere and pixels in TP patches to each other. In addition, we can convert the spherical points into pixels in the ERP image with $(u_e,v_e)=(\frac{\theta_s*w}{2\pi},\frac{\phi_s*h}{\pi})$, where $w$ and $h$ are the width and height of the ERP image, respectively. Therefore, given the spherical coordinate of a TP patch center, we can achieve the mapping between the pixels in the ERP images and those in the corresponding TP patches. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{tp.png}
    \vspace{-5pt}
    \caption{An example of TP and ERP. Two TP patches are projected from two different areas (red area and yellow area).}
    \label{fig:suptp}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{tp_number.png}
    \vspace{-5pt}
    \caption{(a) An ERP image; (b) TP patches with the patch number $N=10$, which are sampled at three latitudes; (c) TP patches with $N=18$, which are sampled at four latitudes; (d) TP patches with $N=26$, which are sampled at five latitudes; (e) TP patches with $N=46$, which are sampled at six latitudes}
    \vspace{-10pt}
    \label{fig:tp_number}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{tp_patchsize.png}
    \vspace{-5pt}
    \caption{TP patches with different patch sizes.}
    \vspace{-5pt}
    \label{fig:tp_size}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{patchfov.png}
    \vspace{-5pt}
    \caption{TP patches with different patch FoVs.}
    \vspace{-5pt}
    \label{fig:tp_fov}
\end{figure*}

% For the number of TP patches which are projected from a 360$^\circ$ spherical image, it depends on the sampling latitudes (the range of latitude is from -90$^\circ$ to 90$^\circ$) and the sampling number at each latitude. 
The number of TP patches projected from a 360$^\circ$ spherical image depends on the sampling latitudes (the range of latitude is from -90$^\circ$ to 90$^\circ$) and the sampling number at each latitude.
For instance, in Omnifusion~\cite{Li2022OmniFusion3M}, TP patches are sampled from four latitudes: -67.5$^\circ$, -22.5$^\circ$, 22.5$^\circ$, 67.5$^\circ$, with 3, 6, 6, 3 patches on each latitude, respectively (see Fig~\ref{fig:tp_number}c). Besides, for one more case, as shown in  Fig~\ref{fig:tp_number}d, the sampled latitudes can be set: -72.2$^\circ$, -36.1$^\circ$, 0$^\circ$, 36.1$^\circ$, 72.2$^\circ$, while the sampled patch numbers are 3, 6, 8, 6, 3, respectively. From Fig~\ref{fig:tp_number}, we can see that with the patch number increased, the area of the overlapping regions increased correspondingly. As shown in Table.~\ref{fig:tp_number}, too few patches can not provide sufficient regional structural information, while too many patches lead to the redundancy of regional information. As a result, we chose to use a relatively small patch number of 18.

We fix the patch FoV to 80$^\circ$ and compare TP patches with different patch sizes of 32$\times$32, 64$\times$64, 128$\times$128, and 256$\times$256 in Fig.~\ref{fig:tp_size}, and it demonstrates that different patch sizes do not affect the content in each TP patch, but a large patch size does produce TP patches with more details. However, as shown in Table.~3 of the main paper, too large patch size will increase computational costs and the redundancy of regional structural information (the amount of pixels in the overlapping regions), which may further influence the prediction from holistic contextual information and decrease the overall performance. As a result, we chose to use a relatively large patch size of 128$\times$128.

For the patch FoV, we fix the patch size to 128$\times$128, and change the patch FoVs to obtain a set of TP patches, as shown in Fig.~\ref{fig:tp_fov}. Compared with the complete view of Fig.~\ref{fig:tp_number}a, too small FoV causes the loss of the scene information, while too large FoV causes the redundancy of information in the overlapping areas. As a result, we chose to use patch FoV $80^{\circ}$.

\section{More Discussion of Feature Similarity}
\label{ab:fs}
Let us consider two pixels in two different TP patches but corresponding to the same pixel in the ERP image (See Fig.~\ref{abl:fig1}). Using a look-up table, we can easily search the locations in two different TP patches $\mathtt{TP}_1$ and $\mathtt{TP}_2$ \textit{w.r.t.} the pixel $A$. However, when re-projecting the pixel values in $\mathtt{TP}_1$ and $\mathtt{TP}_2$ to $A$ of ERP, we have to know which one is better (as done by OmniFusion), \textcolor{blue}{rendering the look-up table inapplicable for TP-to-ERP projection}. We will clarify this accordingly.
% Appreciate for your affirmation of the quality and writing of our paper.
\begin{figure*}[h]
\vspace{-10pt}
    \centering
    \includegraphics[width=0.7\linewidth]{ab_fig1.pdf}
    \vspace{-10pt}
    \caption{The relationships between pixels in ERP and TP patches.}
    \label{abl:fig1}
\vspace{-15pt}
\end{figure*}

\section{More Details of Collaborative Depth Distribution Classification (CDDC)}
\label{ab:cddc}

In this section, we introduce the calculation process of the collaborative depth distribution classification (CDDC) module in detail.

First, given an ERP image with the size of $H_e\times W_e\times3$, we follow the gnomonic projection to obtain $N$ TP patches with the size of $H_t \times W_t \times 3$. Through the feature extractors, we can obtain the ERP feature map $f^E$ with the size $H_e/2 \times W_e/2 \times C_e$ and TP feature maps $\left\{f_n^T\right\}, n= 1, \dots, N$ with the size of $H_t/2 \times W_t/2 \times C_t \times N$, as the inputs of CDDC module. Then we summarize the detailed layer-by-layer network configurations in Table.~\ref{network}. Especially, we introduce network configurations in four parts: holistic depth distribution classification, holistic depth prediction, regional depth distribution classification, and regional depth prediction. 
\begin{table*}[t]
\centering
\scriptsize
\renewcommand\arraystretch{1.2}
\begin{tabular}{c|c|ccc|c|c|c}
\hline
\multicolumn{8}{c}{Collaborative Depth Distribution Classification (CDDC)} \\
\hline
  Input& InpRes & Kernel & Stride & Ch I/O  & Opt. & OutRes & Output  \\
\hline
\multicolumn{8}{c}{Holistic Depth Distribution Classification} \\
\hline
$F^{\mathrm{ERP}}$ & $H_e/2 \times W_e/2 \times C_e$ & 8 & 8 & $C_e/C_1$ & Flatten & $(\frac{H_e * W_e}{256}) \times C_1$ & $Tk^{H}_{in}$ \\
\hline
$Tk^{H}_{in}$ &  $(\frac{H_e * W_e}{256}) \times C_1$ &  - & -& $C_1/C_1$ & Transformer Encoder & $(\frac{H_e * W_e}{256}) \times C_1$  & $Tk^{H}_{out}$\\
\hline
$Tk^{H}_{out}[0]$   & $1 \times C_1$ & - & - & $C_1/B$ & Eq.~\ref{eq:sup1}, Eq.~\ref{eq:sup2} & $1 \times B$ & $\mathbf{c}^H$\\
\hline
\multicolumn{8}{c}{Holistic Range Attention Map} \\
\hline
$F^{\mathrm{ERP}}$& $H_e/2 \times W_e/2 \times C_e$ & 3 & 1 & $C_e/C_1$ &  & $H_e/2 \times W_e/2 \times C_1$ & $F^{H}$\\
\hline
$F^{H}\& $  &$H_e/2 \times W_e/2 \times C_1 \& $&- & -& - & \multirow{2}*{$\odot$}  & \multirow{2}*{$H_e/2 \times W_e/2 \times C_2$} & \multirow{2}*{$R^H$}\\
$Tk^{H}_{out}[1:C_2+1] (T^H)$  &$C_2\times C_1$&- & -& - & & & \\

\hline
$\mathcal{R}^H$& $H_e/2 \times W_e/2 \times C_2$  & - & - & - & Up-sample & $H_e \times W_e \times C_2$ & $\mathcal{R}^{H^{'}}$\\
\hline
$\mathcal{R}^{H^{'}}$& $H_e \times W_e \times C_2$  & 1 & 1 & $C_2/B$ & Softmax & $H_e \times W_e \times B$ & $P^H$\\
\hline
\multicolumn{8}{c}{Holistic Depth Prediction} \\
\hline
$\mathbf{c}^H \& P^H  $  & $1\times B\& H_e \times W_e \times B $ & - & - & $B/1$ & Eq.~\ref{eq:sup3} & $H_e \times W_e \times 1$ & $D^H$\\
\hline
\hline
\multicolumn{8}{c}{Regional Depth Distribution Classification} \\
\hline
$\left\{F_n^{\mathrm{TP}}\right\}$& $\frac{H_t}{2} \times \frac{ W_t}{2} \times C_t \times N$  & 4 & 4 & $C_t/C_1$ & Flatten &  $(\frac{H_t*W_t}{64}) \times C_1 \times N $ & $Tk^{R}_{in}$\\
\hline
$Tk^{R}_{in}$ & $(\frac{H_t* W_t}{64}) \times C_1 \times N $  & -& - & $C_1/C_1$ & Transformer Encoder  & $(\frac{H_t*W_t}{64}) \times C_1 \times N $ & $Tk^{R}_{out}$ \\
\hline
$Tk^{R}_{out}$[0]   &   $1 \times C_1 \times N$     &    -    &-& $C_1/B$ & Similar to Eq.~\ref{eq:sup1}, Eq.~\ref{eq:sup2}& $1 \times B \times N$ & $\mathbf{c}^R$\\
\hline
$\mathbf{c}^R \& M$    &   $1 \times B \times N \& \frac{H_e}{2}\times \frac{W_e}{2} \times N$  & - &-& - & Eq.~\ref{eq:sup4} & $\frac{H_e}{2}\times \frac{W_e}{2}\times B$ & $M_c$\\
\hline
\multicolumn{8}{c}{Regional Range Attention Map} \\
\hline
$Tk^{R}_{out}[1:C_2+1]$  &   $C_2 \times C_1 \times N$     &    -    &-& - & Mean & $C_1 \times N$ & $T^R$\\
\hline
$T^R \& M$   &   $C_1 \times N\& \frac{H_e}{2}\times \frac{W_e}{2} \times N$     &    -    &-& - & Similar to Eq.~\ref{eq:sup4} &  $\frac{H_e}{2}\times \frac{W_e}{2}\times C_1$ & $M_{key}$\\
\hline
$M_{key}\& T^H$   &   $\frac{H_e}{2}\times \frac{W_e}{2}\times C_1\& C_2 \times C_1$     &    -    &-& - &  $\odot$ &  $\frac{H_e}{2}\times \frac{W_e}{2}\times C_2$ & $R^R$\\
\hline
$R^R$&   $\frac{H_e}{2}\times \frac{W_e}{2}\times C_2$     &    -    &-& - &  Up-sample &  $H_e \times W_e\times C_2$ & $R^{R^{'}}$ \\
\hline
$R^{R^{'}}$&   $H_e \times W_e\times C_2$    &    1    &1& $C_2/B$ & Softmax &  $H_e \times W_e\times B$ & $P^R$ \\
\hline
\multicolumn{8}{c}{Regional Depth prediction} \\
\hline
$M_c$&  $\frac{H_e}{2}\times \frac{W_e}{2}\times B$   & -&-& - & Up-sample &  $H_e \times W_e\times B$ & $M_c^{'} $ \\
\hline
$M_c^{'} \& P^R $&   $H_e \times W_e \times B \& H_e \times W_e \times B $     &    -    &-& $B/1$ &  Similar to Eq.~\ref{eq:sup4} &  $H_e \times W_e\times1$ & $D^R$ \\
\hline
\end{tabular}
\caption{Network summary of the CDDC module ($\odot$ denotes the dot-production).}
\label{network}
\end{table*}

In the holistic depth distribution classification, given the output of the transformer encoder, embedding tokens $Tk^{H}_{out}$, we select the first token $Tk^{H}_{out}[0]$ to calculate the bin center vector $\mathbf{c}^H$ as
\begin{equation}
    \mathbf{c}^H_i = D_{min} +  (\mathbf{w}^H_i/2+\sum_{j=1}^{i-1}\mathbf{w}^H_j),
    \label{eq:sup1}
\end{equation}
\begin{equation}
    \mathbf{w}^H_i= (D_{max}-D_{min})\frac{(\mathrm{mlp}(Tk^{H}_{out}[0]))_i+ \epsilon}{\sum_{j=1}^{B}{(\mathrm{mlp}(Tk^{H}_{out}[0]))_j + \epsilon}},
    \label{eq:sup2}
\end{equation}
where $i,j=1,\dots,B$, $\mathbf{w}^H$ is the bin widths of the holistic distribution histogram,  $\mathrm{mlp}$ denotes a multi-layer perceptron (MLP) head with a ReLU activation, $(D_{min}, D_{max})$ is the depth range of the dataset, $B$ denotes the number of depth distribution bins,  and $\epsilon$ is a small constant to ensure that each value of $\mathbf{w}^H$ is positive. 
For the holistic depth prediction, the bin centers $\mathbf{c}^H$ are linearly blended with a probability score map $P^H$ to predict the depth value at each pixel $(i,j)$:
\begin{equation}
    D^{H}(i,j) = \sum_{b=1}^{B} P^{H}(i,j)_b \cdot \mathbf{c}^H_b.
\label{eq:sup3}
\end{equation}

For the regional depth distribution classification, as illustrated in the Table.~\ref{network}, we collect regional depth bin center vectors from the collection of TP feature map $\left\{F^\mathrm{TP}_n\right\}$ and concatenate the center vectors to obtain the tensor $\mathbf{c}^R$ with the size of $ B\times N$. Moreover, with the spatial guidance of index map $M$, we can obtain an ERP format bin center map $M_{c}$ based on $\mathbf{c}^R$ as follows:
\begin{equation}
    M_{c}(i,j) = \sum_{n=1}^{N} M(i,j)_{n} \cdot \mathbf{c}^R_n
\label{eq:sup4}
\end{equation}

where $(i,j)$ is the pixel coordinate, and $n$ is the patch index. The bin center map $M_c$ represents the depth distribution of each pixel with the regional structural information. Meanwhile, we concatenate the collection of selected tokens and reduce the first dimension of the concatenation with the average operation, to obtain the tensor $T^R$. Then we combine $T^R$ with the spatial locations of index map $M$ to obtain a feature map $M_{key}$. Moreover, we introduce the embedding vectors $T^H$ of the ERP branch. With $M_{key}$ as the ``keymap" and $T^H$ as the ``queries", we can predict the probability score map $P^R$ and further output the ERP format regional depth map $D^R$.

\begin{figure*}[h]
\centering
\includegraphics[width=0.7\linewidth]{cddc_sup.png}
\caption{Overview of the CDDC module with two steps.}
    \label{ab:fig:cddc}
\end{figure*}
\section{More Details of Loss Functions}
\label{ab:loss}
As introduced in the main paper, our loss consists of two terms: the pixel-wise depth loss and the holistic distribution loss. For the pixel-wise depth loss, following existing works~\cite{Li2022OmniFusion3M, Jiang2021UniFuseUF}, we adopt BerHu loss~\cite{IroLaina2016DeeperDP} for pixel-wise depth supervision, which is formulated as

\begin{align}
\small
&\mathcal{L}_{depth}=\sum_{i\in P}\mathcal{B}(D^i- D^i_{GT}),\\
    &\mathcal{B}(x)=\left\{\begin{aligned}	
		&\left | x\right |, \left | x\right |\leq c\\
		&\frac{x^2+c^2}{2c}, \left | x\right | > c\\
	\end{aligned}\right.
\end{align}

\noindent where $D_{GT}$ is the ERP format ground truth, $P$ indicates pixels which are valid in the ground truth depth map. c is a threshold hyper-parameter and set to 0.2 empirically~\cite{Li2022OmniFusion3M,Jiang2021UniFuseUF}.

Furthermore, following~\cite{Bhat2021AdaBinsDE}, we employ the bi-directional Chamfer Loss~\cite{HaoqiangFan2016APS} to encourage the holistic depth bin centers $\mathbf{c}^H(b)$ to be consistent with the distribution of all depth values (X) in the ground truth map as:
\begin{equation}
\small
  \mathcal{L}_{H_{bin}}= \textit{Cha}(X, \mathbf{c}^H(b))
\end{equation}
\begin{equation}
\small
  \textit{Cha}(X_1,X_2)=\sum_{x \in X_1} \min _{y \in X_{2}}\|x-y\|_{2}^{2}+\sum_{y \in X_{2}} \min _{x \in X_{1}}\|x-y\|_{2}^{2}
\end{equation}
Finally, the total loss is the summation of both two terms:
\begin{equation}
\small
  \mathcal{L}_{total}=\mathcal{L}_{depth} + \lambda \mathcal{L}_{H_{bin}}.
\end{equation}
For the balance weight $\lambda$, we follow~\cite{Bhat2021AdaBinsDE} and set $\lambda=0.1$ for all our experiments.

\section{More Details of Datasets and Metrics}
\label{ab:data}
We conduct experiments on three benchmark datasets: Stanford2D3D~\cite{Armeni2017Joint2D}, Matterport3D~\cite{Chang2017Matterport3DLF} and 3D60 dataset~\cite{Zioulis2018OmniDepthDD}. Note that Stanford2D3D dataset and Matterport3D dataset are real-world datasets, while 3D60 dataset is composed of two synthetic datasets (SunCG~\cite{ShuranSong2016SemanticSC} and SceneNet~\cite{AnkurHanda2016SceneNetAA}) and two real-world datasets (Stanford2D3D and Matterport3D). 
Stanford2D3D contains 1413 panoramic samples and we split it into 1,000 samples for training, 40 samples for validation and 373 samples for testing. Matterport3D is the largest real-world dataset for indoor panorama scenes containing 10,800 panoramas and we follow the official split to split it into 33875 samples for training, 800 samples for validation, and 1298 samples for testing. As the largest 360$^\circ$ depth estimation dataset, 3D60 totally contains 35973 panoramic samples where 33875 of them are used for training, 800 samples for validation, and 1298 samples for testing.  During training and testing, we resize the resolution of the panorama and depth map in the former two datasets into 512 $\times$ 1024. For 3D60, we set the input size into $256 \times 512$.

\section{Additional Comparison Results}
\label{ab:com_res}
As shown in the open source code of
% \href{https://github.com/zhijieshen-bjtu/PanoFormer/blob/main/PanoFormer/stanford2d3d.py}{https://github.com/zhijieshen-bjtu/PanoFormer/blob/main/PanoFormer/stanford2d3d.py},
PanoFormer~\cite{Shen2022PanoFormerPT}, the authors applied the masking strategy for the Stanford2D3D dataset:
\begin{lstlisting}[language=Python]
mask = torch.ones([512, 1024])
mask[0:int(512*0.15), :] = 0
mask[512-int(512*0.15):512, :] = 0
\end{lstlisting}

Therefore, we apply the same masking strategy
for the Stanford2D3D dataset and compare with PanoFormer in Table.~\ref{tab:sup_stan_comparison}. With the masking strategy, our HRDFuse outperforms PanoFormer~\cite{Shen2022PanoFormerPT} by a significant margin, \eg, 5.8$\%$ (Abs Rel), 11.3$\%$ (Sq Rel), 5.3$\%$ (RMSE). Furthermore, we compare our method with the PanoFormer on 3D60 dataset in Table.~\ref{tab:sup_3d60_comparison}. Note that PanoFormer did not provide the pre-trained models on the 3D60  dataset, we re-train the PanoFormer for 60 epochs with the official hyper-parameters and same experiment setting as OmniFusion and UniFuse. Our HRDFuse outperforms PanoFormer by a large margin.

\begin{table*}[h]
    \centering
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    Datasets&Method& Patch size$/$FoV &Abs Rel $\downarrow$& Sq Rel $\downarrow$&RMSE $\downarrow$ &RMSE(log) $\downarrow$ &$\delta_1$ $\uparrow$ & $\delta_2$ $\uparrow$ & $\delta_3$ $\uparrow$\\
    \midrule
    \multirow{6}*{Stanford2D3D} &PanoFormer*~\cite{Shen2022PanoFormerPT}&$-/-$& 0.1131 &0.0723 & 0.3557& 0.2454& 0.8808& 0.9623& 0.9855 \\
   &PanoFormer$^{\dag}$~\cite{Shen2022PanoFormerPT}&$-/-$& 0.0721 & 0.0506 & 0.3187& 0.1949& 0.9260& 0.9766& 0.9922 \\

    \cmidrule{2-10}
    &HRDFuse*,Ours &$128\times128$ $/$ 80$^\circ$&0.0984&0.0530&0.3452&0.1465&0.8941&0.9778&0.9923\\
    &HRDFuse$^\dag$,Ours &$128\times128$ $/$ 80$^\circ$&0.0730&0.0469& 0.3265&0.1311&0.9213&0.9807&0.9934\\
    &HRDFuse*,Ours &$256\times256$ $/$ 80$^\circ$&0.0935&0.0508&0.3106&0.1422&0.9140&0.9798&0.9927\\
    &HRDFuse$^\dag$,Ours &$256\times256$ $/$ 80$^\circ$&\textbf{\textred{0.0679}}&\textbf{\textred{0.0449}}&\textbf{\textred{0.3017}}&\textbf{\textred{0.1271}}&\textbf{\textred{0.9327}}&\textbf{\textred{0.9826}}&\textbf{\textred{0.9935}}\\
    \bottomrule
    \end{tabular}}
    \vspace{-8pt}
    \caption{Quantitative comparison with the SOTA methods. $*$ represents that the model is re-trained following the official setting. $\dag$ represents that the model is evaluated with the masking strategy in PanoFormer~\cite{Shen2022PanoFormerPT}.
    }
    \vspace{-10pt}
    \label{tab:sup_stan_comparison}
\end{table*}
\begin{table*}[h]
    \centering
    \resizebox{\textwidth}{!}{ 
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    Datasets&Method& Patch size$/$FoV &Abs Rel $\downarrow$& Sq Rel $\downarrow$&RMSE $\downarrow$ &RMSE(log) $\downarrow$ &$\delta_1$ $\uparrow$ & $\delta_2$ $\uparrow$ & $\delta_3$ $\uparrow$\\
    \midrule
    \multirow{10}*{3D60} & FCRN~\cite{Laina2016DeeperDP}&$-/-$&0.0699&0.2833&-&-&0.9532&0.9905&0.9966\\
     &RectNet~\cite{Zioulis2018OmniDepthDD}&$-/-$&0.0702&0.0297&0.2911&0.1017&0.9574&0.9933&0.9979\\
     &Mapped Convolution~\cite{Eder2019MappedC}&$-/-$&0.0965& 0.0371 &0.2966& 0.1413& 0.9068& 0.9854& 0.9967\\
    &BiFuse with fusion~\cite{Wang2020BiFuseM3}&$-/-$&0.0615& -& 0.2440& - &0.9699 &0.9927 &0.9969\\
   &UniFuse with fusion~\cite{Jiang2021UniFuseUF}&$-/-$&0.0466& - &0.1968 &- &0.9835 &0.9965& 0.9987\\
   &ODE-CNN~\cite{Cheng2020OmnidirectionalDE}&$-/-$&0.0467& 0.0124& 0.1728& 0.0793& 0.9814 &0.9967 &0.9989\\
    &OmniFusion (1-iter)~\cite{Li2022OmniFusion3M}&$128\times128$ $/$ 80$^\circ$&0.0469& 0.0127 &0.1880& 0.0792& 0.9827& 0.9963 &0.9988\\
    &OmniFusion (2-iter)~\cite{Li2022OmniFusion3M}&$128\times128$ $/$ 80$^\circ$&0.0430 & 0.0114 & 0.1808 & 0.0735 & 0.9859 & 0.9969 & 0.9989\\
    &PanoFormer*~\cite{Shen2022PanoFormerPT}&$-/-$& 0.0442 & 0.0124& 0.1691& 0.0676& 0.9861& 0.9966& 0.9987\\
     \cmidrule{2-10}
    &HRDFuse,Ours &$128\times128$ $/$ 80$^\circ$&0.0363&0.0103&0.1565&0.0594&0.9888&\textbf{\textred{0.9974}}&0.9990\\
    &HRDFuse,Ours &$256\times256$ $/$ 80$^\circ$&\textbf{\textred{0.0358}}&\textbf{\textred{0.0100}}&\textbf{\textred{0.1555}}&\textbf{\textred{0.0592}}&\textbf{\textred{0.9894}}&0.9973&\textbf{\textred{0.9990}}\\
    \bottomrule
    \end{tabular}}
    \vspace{-8pt}
    \caption{Quantitative comparison with the SOTA methods. $*$ represents that the model is re-trained following the official setting. 
    }
    \vspace{-10pt}
    \label{tab:sup_3d60_comparison}
\end{table*}

\begin{figure}[h]
\centering 
\begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supp_stan.jpg}
        \caption{Visual comparisons on Stanford2D3D and Matterport3D.}
        \label{fig:supp_stan}
    \end{subfigure}%
    
~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=\textwidth]{supp_3d60.jpg}
        \caption{Visual comparisons on 3D60 dataset.}
        \label{fig:supp_3d60}
    \end{subfigure} 
 \label{fig:a}        
\caption{ More visual comparison results. }     
\label{fig}     
\end{figure}

\section{Additional Visual Results}
\label{ab:vis_res}

\noindent\textbf{More visual comparisons on Stanford2D3D and Matterport3D.} In Fig.~\ref{fig:supp_stan}, we perform qualitative comparisons with the SOTA methods, UniFuse.~\cite{Jiang2021UniFuseUF} and OmniFusion~\cite{Li2022OmniFusion3M}, on the Stanford2D3D dataset and Matterport3D dataset, whose samples are from real-world scenes. From the visual results, we confirm that our HRDFuse predicts the depth maps which are more precise and contain more structural details than other methods.

\noindent\textbf{More visual comparisons on 3D60.} In Fig.~\ref{fig:supp_3d60}, we perform qualitative comparisons with the SOTA methods, UniFuse.~\cite{Jiang2021UniFuseUF} and OmniFusion~\cite{Li2022OmniFusion3M}, on the 3D60 dataset, which contains both real-world and synthetic samples. From the visual results, we further confirm the superiority of our HRDFuse.

\section{Discussion on the rationality of SFA module}
\label{ab:sfa}

As shown in the Fig~.\ref{fig:suppindex1}, for a scene with simple structure, our SFA module make the index map centralized to several representative TP patches with higher frequency of index, \eg, 4, 6, 10 (Fig.~\ref{fig:suppindex1}(c)). Combined with the Fig.~\ref{fig:suppindex1}(d), we can observe that the feature alignment based on the feature similarity in the SFA module tends to employ the most representative regional depth distributions to avoid the redundant usage. Meanwhile, facing the special depth values, SFA module will introduce the corresponding the regional depth distributions to predict them (\eg, with index 11, 14). Especially, with the scene structure becoming
more complex, the more TP patches are needed to describe the holistic depth information, as shown in the Fig.~\ref{fig:suppindex2}. The frequency of TP index is more balanced.
\begin{figure*}[h]
    \centering
\includegraphics[width=1\textwidth]{index1.pdf}
    \caption{The visualization of (a) ERP depth ground truth, (b) TP index map (colored according to the attached color card), (c) TP index frequency and (d) TP depth patches with the corresponding index numbers from a scene with simple structure.}
    \label{fig:suppindex1}
\end{figure*}
\begin{figure*}[h]
    \centering
\includegraphics[width=1\textwidth]{index2.pdf}
    \caption{The visualization of (a) ERP depth ground truth, (b) TP index map (colored according to the attached color card), (c) TP index frequency and (d) TP depth patches with the corresponding index numbers from a scene with complex structure.}
    \label{fig:suppindex2}
\end{figure*}
\clearpage
\section{Visual comparisons and discussion on real data.} 
\label{ab:vis_real}
To better compare the generation capability of our HRDFuse and other SoTA methods, we capture the two real images which records the indoor scene (considering the limited max depth value, we ignore the outdoor scene) and directly use the models trained on Matterport3D training dataset to predict their depth maps. As shown in the Fig.~\ref{fig:suppreal}, we can observe that our HRDFuse predicts more precise depth maps for the captured scenes. By contrast, the results of PanoFormer~\cite{Shen2022PanoFormerPT} tend to be blurry and over-smooth on unseen scenes.

\begin{figure*}[h]
    \centering
\includegraphics[width=1\textwidth]{supp_real.pdf}
    \caption{Visual comparisons on real data (captured by Ricoh Theta Z1).}
    \label{fig:suppreal}
\end{figure*}

\clearpage
% Use \bibliography{yourbibfile} instead or the References section will not appear in your paper

