
\section{Methodology}

This section introduces our visual analytics workflow.\footnote{The related source code is available at \url{https://github.com/hylu1994/Network-CV}}
We first describe design considerations of the workflow, which are identified based on our literature survey in \autoref{sec:related_work} and targeted analysis on multivariate networks (for the detailed definition of multivariate networks, refer to the introduction by Kerren et al.~\cite{kerren2014multivariate}).
We then provide an overview of the workflow, followed by the details of each step. 

\subsection{Design Considerations}

The following five design considerations (DCs) are identified to support the analysis task for understanding the relations of interest and to fill the analytical gap that is not covered by existing methods.

\newcommand{\Flexibility}{DC1: Flexibility}
\newcommand{\Expressivity}{DC2: Expressivity}
\newcommand{\Interpretability}{DC3: Interpretability}
\newcommand{\Steerability}{DC4: Tunability}
\newcommand{\Extensibility}{DC5: Extensibility}

\ifarxiv
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/workflow.pdf}
    \caption{\CaptionWorkflow{}}
    \label{fig:workflow}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ui.pdf}
    \caption{\CaptionUI{}}
    \label{fig:ui}
\end{figure*}
\fi

\textbf{\Flexibility.} Reviewing multivariate networks requires analyses from multiple aspects, such as the relationships among structural information as well as semantic information (e.g., network node's age and gender). 
Also, based on the analysis purpose and available datasets, analysts often want to investigate networks from different levels, such as node, link, and network levels.
The workflow should provide the flexibility to conduct analyses from multiple aspects at various levels.
    
\textbf{\Expressivity.} Our work's main objective is to effectively support the stated analysis---examining the relationships among the target (e.g., the addiction level to social media) and other related information (e.g., gender, age, and the number of friends). 
The workflow should be able to extract features that are expressive for this analysis task (i.e., containing important information for the task) rather than providing a general summary of the network information. 

\textbf{\Interpretability.} To gain insights through the analyses, the interpretation of results is essential. 
Because expressive features such as those extracted by NNs are often difficult to explain only with each individual attribute's influence, the workflow should provide more advanced interpretation support that considers the combined influence from multiple attributes.

\textbf{\Steerability.}
Conflict often exists between the expressiveness and interpretability of features (i.e., more expressive features are often more difficult to interpret).
Also, the requirement for the interpretation's preciseness can differ by the analysis (i.e., more complicated but precise interpretation vs less precise but simple interpretation). 
The workflow should allow the analysts to control the balance of the expressiveness and interpretability and interactively examine the results based on their interests. 
    
\textbf{\Extensibility.} While we provide a concretized method for each step of the workflow, a different set of methods may be more suitable according to the characteristics of future datasets and analysis goals.
The workflow should be extensible to incorporate new or different methods.

\ifarxiv
\else
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/workflow.pdf}
    \caption{\CaptionWorkflow{}}
    \label{fig:workflow}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ui.pdf}
    \caption{\CaptionUI{}}
    \label{fig:ui}
\end{figure*}
\fi

\subsection{Workflow Overview}

\autoref{fig:workflow} shows our workflow for reviewing the relations embedded in multivariate networks.
It consists of six steps that are performed first with a script for machine learning (Steps 1--4) and then with our user interface (UI) designed for visual analysis (Steps 5--6), as shown in \autoref{fig:ui}.\footnote{The demonstration of the UI can be found at \url{https://github.com/hylu1994/Network-CV}
}
As we describe in \autoref{sec:nrl}, we extract structural information as a set of network measures (e.g., degree and betweenness centralities) for each element (i.e., node, link, or network); thus, in the following, we use a term, attribute, to indicate both extracted structural (e.g., degree) and semantic information (e.g., age).

\textbf{Step 1.} 
The workflow begins with the selection of input attributes and one output/target attribute. 
For example, to know how students' grades in a school class are related to their surrounding conditions, an analyst may choose their attending class size, mental health status (e.g., depression level), and centrality in the social media connections (e.g., degree) as inputs and their grade as an output.

\textbf{Step 2.} 
NRL is performed based on the selected inputs and output. 
A network representation is generated by an NN trained to predict the output from the input values. 
The representation, thus, can be specific to relations of the analyst's interest. 

\textbf{Step 3.} 
This step compresses the dimensionality of the network representation from many dimensions into one while preserving the prediction quality as much as possible. 
This makes the remaining steps simpler to complete and the related results easier to interpret.

\textbf{Step 4.} 
This step evaluates and ranks each input attribute's contribution to the construction of the 1D compressed representation. 
The obtained ranks can be considered as the recommendation levels of the inclusion of the corresponding attributes for the composite variable construction in Step 5. 

\textbf{Step 5.}
Finally, based on the recommendation levels and interests, the analyst manually selects a small set of attributes (e.g., 2--5 attributes) and runs a composite variable construction algorithm. 
The optimized composite variable (e.g., the $y$-axis in \autoref{fig:ui}-b2) maximally resembles the 1D compressed representation (e.g., the $x$-axis in \autoref{fig:ui}-b2). 
This composite variable provides an intuitive explanation of how the selected small set of attributes is related to the output/target attribute. 

\textbf{Step 6.}
The UI in \autoref{fig:ui} visualizes the information related to each of the previous steps as well as the detailed structural and semantic information of the multivariate network. 
By interactively reviewing these pieces of information, the analyst can gain analytical insights or adjust the settings used for the previous steps based on their analysis interests (the backward arrows in \autoref{fig:workflow}).
The analyst can also refer to the UI at each step (the forward arrows in \autoref{fig:workflow}).

\subsection{Datasets}
\label{sec:datasets}

In the rest of this section, our explanations refer to a real-world dataset, Dataset I, described below as a concrete analysis target.
We also analyze Datasets I and II in our case studies (\autoref{sec:case_studies}).


\textbf{Datasets: College students and adults’ network resources.} 
We analyze multivariate networks derived from the Facebook usage and survey data of two representative cohorts, senior college students and sampled adults in Taiwan.
For \textbf{Dataset\,I (senior college students)}~\cite{chang2019social}, the survey is on topics related to their school life, such as their personalities, moods, and grades. 
\textbf{Dataset\,II (Taiwanese adults)}~\cite{lee2022indirect} uses Taiwan Social Change Survey, which includes important network-related topics, such as core discussion networks and network resources, as well as personal attributes, such as their backgrounds, personalities, mood, and social lives.
With Facebook usage data, we represent each respondent as a network node and create a link based on the interactions on Facebook.
We construct an undirected link if a ``like'', ``comment'', or ``tagged'' was made between respondents on a post. 
We can consider such links as ``friendships'' in social media and 1-hop neighbors of each node as ``friends''. 
We further use the answers for the survey as each node's attributes.


Below, from Dataset I, we identify and review college students' attributes that are highly related to their \texttt{scorelevel}, the past three years' mean score levels reported on a scale of four from the top to bottom. 
While the resultant network consists of 1886 nodes, 64156 links, and over 200 survey-based attributes for each node, we picked 14 survey attributes related to this analysis. 


\subsection{Learning Representations}
\label{sec:repr_learning}

We explain Steps 1--3, where we learn representations from multivariate networks. 
To provide a concise explanation, here we describe each process for the case producing a latent vector for each node (i.e., node feature learning).
However, our learning processes are generic enough to be adjusted to learn a latent vector for each link or network (\Flexibility).


\subsubsection{Network Representation Learning}
\label{sec:nrl}

To allow analysts to select attributes related to the structural information in Step 1, similar to existing works~\cite{vandenelzen2016reducing,fujiwara2020visual}, we first precompute a set of network measures. 
For node feature learning, such measures can be  degree, eigenvector, betweenness centralities, and many others~\cite{newman2018networks}.

Each of these centralities informs the importance of each node in a network from different aspects.
Degree centralities, or node degrees (i.e., the number of links connected to each node), measure  local importance of nodes in the network.
Eigenvetor centralities (i.e., the eigenvector corresponding to the greatest eigenvalue of the network's adjacency matrix) are more suitable to show global importance of nodes as eigenvector centralities consider the transitive importance of links (e.g., a link connecting to a node with 100 links is more important than one with 1 link).
Betweenness centralities (the number of shortest paths that pass through each node) show the importance of each node as a ``bridge'' connecting other nodes.
For more details, refer to the introduction to network theory by Newman~\cite{newman2018networks}.

Furthermore, to capture more detailed-level structural information, we precompute advanced measures related to the statistics of each node's neighbors, such as the mean, variance, and maximum of $k$-hop neighbors' degrees. 
Such neighbor statistics can be also computed for semantic attributes (e.g., the mean age of neighbors).
The neighbor statistics have been shown to be useful for various analytical tasks~\cite{rossi2018deep,fujiwara2020visual}.
For the precomputation, we specifically use DeepGL~\cite{rossi2018deep} to achieve fast computation while avoiding producing many redundant measures. 

DeepGL summarizes 1-hop neighbors' centrality/attribute values (called \textit{base features}), using \textit{relational functions}.
For relational functions, we can specify multiple neighbor types (e.g., in-, out-, and total-neighbors) and aggregation functions (e.g., sum, mean, variance, and maximum). 
After obtaining the neighbor statistics for all nodes, DeepGL prunes redundant statistics that show similar value distributions to others. 
The strength of pruning can be controlled with a hyperparameter of DeepGL.
Moreover, DeepGL can repeatedly apply the relational functions and pruning process to summarize farther neighbors' features. 
For more details, refer to the work by Rossi et al.~\cite{rossi2018deep}.
For Dataset I, as base features, we select 3 fundamental network centralities, degree, eigenvector, and betweenness, and the 14 survey attributes; then, we generate 1-hop total-neighbor statistics of the 17 attributes, resulting in 85 attributes in total after the pruning process.

After selecting input and output attributes in Step 1, we first apply the Z-score normalization to the attributes.
We then train an NN to predict the output from input values, and extract representations from an NN layer (\Expressivity). 
While we could predict an output value directly by designing the NN for a regression task, we categorize output values and perform binary classification of two value ends (e.g., top and bottom 25\% of attribute values). 
Note that the classification is performed only on these ends, and thus unrelated instances are not involved in training.
If we perform regression, instead of this process, the NN tries to fit all instances, resulting in a representation that might be mainly useful to predict values in the middle range (e.g., when fitting to the two ends is harder than the middle range).
While we assume this focus on two ends is reasonable for many analyses, this setting can be easily changed based on the analysis interests.  
This design employing binary classification also allows a comparison of two categorical groups (e.g., students who major in engineering vs. business). 

By default, we use a multi-layer perceptron (MLP) consisting of five fully-connected layers (one input, three hidden, and one output layers) and take the last hidden layer's activations as the learned representation.
We employ the leaky rectified linear unit as an activation function to obtain expressive representations by utilizing its nonlinearity. 
On the other hand, with the consideration of the balance between expressivity and interpretability, we keep the numbers of hidden layers and nodes small relative to the size of the input data.
However, we should note that, for this step, it is acceptable to cause overfitting to some extent.
As described in \autoref{sec:repr_simplification}, we simplify this step's high-dimensional network representation with linear transformation, which can mitigate overfitting when it occurs.
With consideration for the next step, this NN-based learning step should rather focus on achieving an accurate alignment of the defined classes in the learned representation.
When we have such an alignment, the NN's classification accuracy should be high, which can be also checked in the UI, as shown in \autoref{fig:ui}-e.

While the five-layer MLP is our default NN architecture, the analyst can easily replace it with more complex architecture (\Extensibility). 
For example, when the analyst needs to capture nonlinear neighbor relations in a network, they can incorporate a GNN into the NN architecture.
Also, as stated, the above process can be adjusted to learn latent vectors of links or networks. 
This can be achieved by replacing the precomputation of network measures, accordingly.
For example, instead of the centralities, for network feature learning, we can extract the mean degree, the number of nodes, network diameter, etc.
Then, we can apply the same procedure employing an NN.

\subsubsection{Representation Simplification}
\label{sec:repr_simplification}

We simplify the high-dimensional network representation into a 1D representation for the ensuing interpretation steps (\Interpretability).
Because the NN is trained for binary classification, this simplification is similar to the process NNs usually perform for the output layer: transforming the last hidden layer's activations of multiple nodes into a single node at the output layer.
Thus, we expect that even though the simplified representation is 1D, it can preserve sufficient information for the prediction (\Expressivity).

A major difference from conventional NNs is that we employ linear transformation, instead of nonlinear transformation.
For conventional NNs, a nonlinear activation function such as a sigmoid or softmax function is usually employed to encourage producing a value of 0 or 1 for classification.
Consequently, the corresponding 1D representations would consist of many values close to 0 or 1.
This close-to-discrete distribution makes it difficult not only to perform the optimization for the composite variable construction but also to observe patterns from visualizations shown in \autoref{fig:ui}-b. 

One way to obtain a 1D representation with linear transformation is using a linear activation function between the last hidden and output layers, as in NNs for regression tasks.
However, as stated, we want to avoid directly performing regression in the NN. 
Instead, as a post-hoc process, we apply a DR method, specifically LDA, to the high-dimensional network representation.
With the class information, LDA produces a low-dimensional representation that places instances in different classes as far as possible. 
To do so, LDA minimizes data variance within each class while maximizing the separation of each class’s centroid.
The benefit of this post-hoc approach is that LDA can efficiently generate a 1D representation that is the global optimum solution for class separation.

However, similar to NNs, LDA can easily cause overfitting when the number of instances is small relative to the number of input features~\cite{guo2007regularized}. 
To solve this issue, we use an enhanced LDA, called regularized LDA, that has a regularization mechanism to avoid overfitting~\cite{guo2007regularized}.
The use of regularized LDA brings an additional benefit for the post-hoc approach: the regularization strength can be controlled outside of the NN training process. 
While the L1 and L2 regularizations can be used for NNs to avoid overfitting, the hyperparameter adjustment for these often requires many trials and errors, and becomes time-consuming by retraining NNs many times.
Applying regularization inside of LDA does not require retraining of NNs and LDA is computationally efficient; thus, we can adjust the regularization strength more conveniently (\Steerability).

The quality of the 1D representation can be investigated with a swarm-plot-like visualization, as shown in \autoref{fig:ui}-b1.
This visualization shows an instance as a dot and a 1D representation value as an $x$-coordinate.
Similar to the swarm plot~\cite{swarmplot}, $y$-direction is used to pile up dots while involving positional jitters to mitigate overplotting within a limited vertical space.
The resultant area height roughly shows the frequency/density around the corresponding $x$-coordinate as similar to a histogram.
We select this instance/point-based visualization to keep it consistent with other visualizations using $x$-coordinates to represent the 1D representation values (e.g., \autoref{fig:ui}-b2).
As shown in the fan-shape colormap located at \autoref{fig:ui}-e, the color of each dot shows the density and the ratio of each class's density at the corresponding position, which we explain in \autoref{sec:twoclass-scatterplot}.
When the 1D representation has good quality, as seen in \autoref{fig:ui}-b1, two classes (\texttt{Class\,0}: red, \texttt{Class\,1}: blue) should have small or no overlaps (note: overlapped parts have purple colors). 
When the quality is not satisfactory, the analyst can update the NN architecture and/or the selection of input and output attributes.

Before we chose the above architecture first applying an MLP and then LDA, we experimented with two alternative designs. 
As the first alternative design, we directly applied LDA  to obtain the 1D representation.
The result after directly applying LDA to the same dataset as \autoref{fig:ui}-b is shown in \autoref{fig:alternatives}-a. 
As this design does not utilize the strengths of NNs, the two classes are more mixed together when compared with \autoref{fig:ui}-b.
Second, we only used an MLP, in which a softmax activation function is used to output the 1D representation (unlike our approach using LDA for this step).
The result shown in \autoref{fig:alternatives}-b exhibits the aforementioned issue of the close-to-discrete distributions, although the two classes are well separated.
These two results show that our final design is better for providing a good balance of expressivity and interpretability.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/alternatives.pdf}
    \caption{The swarm-plot-like visualizations of the 1D representations generated by the designs using (a) only LDA and (b) only an MLP.}
    \label{fig:alternatives}
\end{figure}

\subsection{Understanding Representations}
\label{sec:repr_interpretation}

We describe Steps 4 and 5, which are mainly for understanding the representation obtained through Steps 1--3.


\subsubsection{Attribute Contribution Measurement}
\label{sec:attrib_contrib_shap}

In Step 4, we extract each input attribute's contribution to the 1D representation which can be considered as a latent space that disentangles the output attribute by referring to the input attributes.
We utilize the attribute contribution to understand each attribute's relationship to the output attribute---the interpretation from a single attribute level---as well as to recommend attributes for the composite variable construction---the interpretation considering the combinational attribute influences (\Interpretability). 

To measure the attribute contributions, we use the SHAP method~\cite{lundberg2017shap}.
By employing cooperative game theory, for each instance, the SHAP method calculates a measure, called the SHAP value, that shows an attribute's contribution to a given target value (refer to \cite{lundberg2017shap} for details). 
In our case, we apply the SHAP method to a transferred model that combines the MLP trained in Step 2 and LDA trained in Step 3.
Then, the SHAP value indicates how much having a corresponding attribute value contributes to moving an instance toward a positive direction of the 1D representation.
For example, when instance A is age 25 and its SHAP value for the age attribute is 0.01, by having this age, instance A is placed at a more positive side of the 1D representation by the degree of 0.01.

\begin{figure*}[t]
    \centering
    \ifarxiv
      \includegraphics[width=0.85\linewidth]{figures/shap.pdf}
    \else
      \includegraphics[width=\linewidth]{figures/shap.pdf}
    \fi
    \caption{\CaptionSHAP{}}
    \label{fig:shap}
\end{figure*}

As shown in \autoref{fig:ui}-a, we visualize SHAP values with a set of scatterplots.
Each row corresponding to one attribute shows a scatterplot where the $x$- and $y$-axes show the SHAP and attribute values, respectively.
Each dot represents one instance and its color informs the density and ratio of classes, as in \autoref{fig:ui}-b1.
From these plots, for example, we can observe that larger \texttt{open} (openness) has a negative impact on the 1D representation reflecting \texttt{scorelevel}. 
On the other hand, larger \texttt{consci} (conscientiousness) has a positive impact.
Moreover, \texttt{netaddict} (internet addiction) has clearly different patterns between \texttt{Class\,0} (red, low \texttt{scorelevel}) and \texttt{Class\,1} (blue, high \texttt{scorelevel}): larger \texttt{netaddict} tends to have a more negative impact for \texttt{Class\,0}. 
As shown in \autoref{fig:shap}, we also tested different visual designs.
\autoref{fig:shap}-a is based on the default plot in the SHAP Python package~\cite{shap_library}, where the swarm-plot-like visualization is used, as in \autoref{fig:ui}-b1. 
Using $x$-coordinates and colors of dots, this visualization informs SHAP and attribute values.
From this visualization, we can roughly understand the relationships between the values (e.g., larger \texttt{open} tends to have a more negative SHAP value); however, it is infeasible to know the different patterns between the two classes.
Even when jointly using a visualization that color-encodes class labels (see \autoref{fig:shap}-b), we still cannot clearly see such different patterns. 
When compared with these, our final design (\autoref{fig:shap}-c) can  clearly depict the relationships between SHAP and attribute values as well as the different patterns seen in the two classes.


Because each instance has one SHAP value for each attribute, as a global measure of each attribute's contribution, we use the mean absolute value of all instances' SHAP values.
Then, we sort and display attributes in \autoref{fig:ui}-a based on their contributions. 
The analyst can refer to these ordered attributes when selecting attributes for the composite variable construction.

\subsubsection{Composite Variable Construction}
\label{sec:comp_var_construction}

The composite variable is commonly used when using a single attribute is not sufficient to understand a target phenomenon ({\Interpretability} and \Expressivity). 
For example, in baseball, on-base plus slugging (OPS)---the sum of two different attributes of the player's performance, on-base percentage (OBP) and slugging percentage (SLG)---is used to analyze each player's contributions to team runs or winnings.
This is because OPS is a simple composite variable but extremely correlates to team runs~\cite{albert2010sabermetrics}. 

We describe the optimization algorithm designed to construct a composite variable from user-selected attributes.
First, we need to define the objective function for the optimization. 
To explain the 1D representation with the selected attributes as much as possible, we want to obtain the maximum value for a certain ``measure of dependence''~\cite{reshef2016measuring} (i.e., a measure indicating how closely two variables are related). 
We use either Pearson's or Spearman's correlation coefficient as a measure of dependence.
Also, we construct a composite variable with a linear combination of attributes for a simple, intuitive interpretation as well as an efficient optimization to allow interactive construction.
A linear combination that achieves the highest Pearson's correlation coefficient can be computed with multivariate linear regression. 
As Spearman's involves a non-differentiable operation to rank the 1D representation values, we cannot perform the optimization with simple regression or gradient-based solvers.
Thus, we employ a gradient-free solver, specifically COBYLA (or constrained optimization by linear approximations)~\cite{powell1998direct} while using the optimized result for Pearson's as an initial solution. 
Although the optimization for Pearson's is much more efficient, Spearman's could be more effective in some cases.
This is because the 1D representation is constructed from the non-linearly transformed attributes by the NN, and the 1D representation may have a nonlinear correlation with the original input attributes.

With the view in \autoref{fig:ui}-a, the analyst can select attributes and a measure of dependence, and generate a composite variable by clicking ``Construct New Composite Variable'' located at the bottom.
The selected attributes are highlighted with a gray background.
Then, as shown in \autoref{fig:ui}-b2, the relationships between the 1D representation ($x$-axis) and composite variable ($y$-axis) are visualized with the two-class density scatterplots described in \autoref{sec:twoclass-scatterplot}. 
The UI allows the analyst to generate multiple composite variables to compare their expressivity and interpretability (\Steerability). 
A newly constructed composite variable's visualization is appended to a list of scatterplots in the scrollable view.
The analyst can also discard a scatterplot by clicking a ``$\smash{\times}$'' mark at the top right.  

In essence, through this composite variable construction process, the analyst can utilize their domain knowledge to review the simplified representation.
From the attributes ranked by SHAP values, the analyst can judge attributes they should consider.  
Then, with the help of the above optimization algorithm, the analyst can interactively examine and fine-tune the composite variables to find useful insights.

For Dataset I analyzed in \autoref{fig:ui}, Spearman's correlation coefficient between each of the top-5 contributed attributes and the 1D representation is \texttt{open}:\,-0.147, \texttt{consci}:\,0.237, \texttt{netaddict}:\,0.134 \texttt{min\_happy} (the minimum of 1-hop neighbors' happiness levels):\,0.093, and \texttt{extra} (extraversion):\,0.026.
As shown in \autoref{fig:ui}-b2, using all the five attributes, we can generate a composite variable that has a better correlation coefficient, 0.300.
This is a moderate correlation based on Dancey and Reidy's categorization~\cite{dancey2017statistics}, which we follow in the rest of the paper when describing correlation strengths.
As all attributes are normalized, each attribute's weight in the composite variable shows the contribution level to the composite variable (\texttt{open}:\,-0.4, \texttt{consci}:\,+0.7, \texttt{netaddict}:\,-0.3, \texttt{min\_happy}:\,+0.3, \texttt{extra}:\,+0.3).
We can see that \texttt{consci} contributes most, as expected from its higher Spearman's correlation (0.237) than the others. 
On the other hand, while \texttt{extra} has an extremely small correlation (0.026) as a single attribute, a relatively large weight is assigned (+0.3).
In fact, when excluding \texttt{extra}, the correlation coefficient decreases from 0.300 to 0.289. 
By checking how the inclusion of \texttt{extra} improves on the other attribute's correlation, we observe that \texttt{open} has much larger improvement (from 0.147 to 0.170) than the others (e.g., \texttt{consci} has no improvement).
From this, we can expect that by including \texttt{open} and \texttt{extra} with different signs (-0.4 and 0.3), the composite variable captures the subtle difference between these two personalities, which is more related to \texttt{scorelevel} than either of \texttt{open} or \texttt{extra}. 
We should emphasize that the above insights cannot be derived if only investigating a single attribute relationship to the 1D representation. 
Also, this example demonstrates the reason for the use of the SHAP method for the attribute recommendation, instead of simply relying on the correlation coefficients.

We should note that while we want to make a composite variable correlated to the 1D representation to some extent (e.g., Spearman's $\smash{\geq 0.2}$, weakly correlated), we do not have to see highly correlated results (e.g., Spearman's $\smash{\geq 0.7}$, strongly correlated).
If an extremely strong correlation can be found with a linear combination of a few attributes, the use of NNs becomes unnecessary as we can expect linear learning methods such as LDA are sufficient for the prediction. 
Rather, here we want to take a further step from the single-attribute-based interpretation using the SHAP method or others~\cite{lundberg2017shap,kwon2018clustervision,neto2021multivariate} by providing the interpretation method considering the combinational influence of attributes. 
Such an influence can be analyzed via the signed weights in a composite variable and the interactive adjustment of the composite variable, as demonstrated above. 
And, the correlation coefficient can be used as a reference to know how much of the 1D representation is explained with the composite variable.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/scatterplots.pdf}
    \caption{\CaptionScatterplots{}}
    \label{fig:scatterplots}
\end{figure*}

\subsubsection{Two-class Density Scatterplot}
\label{sec:twoclass-scatterplot}

From a scatterplot showing the relationships between the 1D representation and the composite variable, we want to find (1) patterns related to binary classes; (2) trends including correlations; (3) other supplemental patterns, such as noises, outliers, and clusters.
For example, we may want to check the quality of the class separation to refine the settings for Steps 1--3, update the attribute selection in Step 5 based on the correlation pattern, or find subgroups for further analyses (e.g., only a subset of instances might have a clear trend).

Based on the survey on scatterplot designs~\cite{sarikaya2018scatterplots}, we should employ a visual encoding that involves the aggregation of instances for the identification of trends (especially, correlations) while we should show individual instances for the other tasks (e.g., finding outliers).
However, providing an encoding that satisfies these requirements is not straightforward.
For example, in \autoref{fig:scatterplots}-a, the class information is encoded with colors of dots. 
This encoding easily suffers from overplotting when analyzing a large dataset (e.g., 1000 instances). 
In \autoref{fig:scatterplots}-a, it is difficult to find a clear trend hidden by noises. 
The density scatterplot is effective to find trends, as shown in \autoref{fig:scatterplots}-b, where the correlated pattern can be seen along the diagonal direction.
However, from the density scatterplot, we cannot grasp where each class's instances are located, and we do not know, for example, whether or not the dense area mostly consists of a single class.

To solve the above issues, as shown \autoref{fig:scatterplots}-d, we develop a \textit{two-class density scatterplot}, using a bivariate colormap.
As shown in its legend, the density and the ratio of each class's density are encoded with color lightness and hue, respectively.
From this plot, we can clearly see the correlated pattern as well as confirm that the two dense areas at the bottom left (more red colors) and top right (more blue colors) mainly consist of a single class.
From the purple color area annotated with the arrow in \autoref{fig:scatterplots}-d, we can also observe that a part of the dense area contains a considerable amount of both classes' instances.
We should note that a bivariate colormap has been already utilized to simultaneously show two different measures over scatterplots (e.g., \cite{lespinats2011checkviz}); however, we introduce the computation of the ratio of each class's density and select color encoding suitable for showing class and density information with scatterplots.

Similar to an ordinal density scatterplot, we first estimate the 2D probability density function (PDF) from the instances' coordinates. 
This can be done by applying a Gaussian kernel density estimation to all instances. 
Let $\ProbDensFuncAll$ denote the 2D PDF estimated with all instances.
Then, the density at a 2D coordinate, $\Point$, can be computed with $\ProbDensFuncAll(\Point)$.
Similarly, let $\ProbDensFuncClassZero$ and $\ProbDensFuncClassOne$ denote 2D PDFs estimated with \texttt{Class\,0}'s instances and \texttt{Class\,1}'s instances, respectively. 
Also, let $\nInstsClassZero$ and $\nInstsClassOne$ be the numbers of instances in \texttt{Class\,0} and \texttt{Class\,1}.
Then, a function that estimates the ratio of \texttt{Class\,0}'s density at $\Point$ can be written with: $\DensRatioFuncClassZero(\Point) = \nInstsClassZero \ProbDensFuncClassZero(\Point) / (\nInstsClassZero \ProbDensFuncClassZero(\Point) + \nInstsClassOne \ProbDensFuncClassOne(\Point))$.
When assigning the same total density for each class is more appropriate for an analysis, instead, we can use  $\DensRatioFuncClassZero(\Point) = \ProbDensFuncClassZero(\Point) / ( \ProbDensFuncClassZero(\Point) + \ProbDensFuncClassOne(\Point))$.

Now, we encode the density, $\ProbDensFuncAll(\Point)$, and the ratio of \texttt{Class\,0}'s density, $\DensRatioFuncClassZero(\Point)$, with a bivariate colormap.
We select color lightness to encode the density because of its natural metaphor: The more accumulation of dots, the denser color.
We then employ hue to represent the class ratio.
While we tested ordinary HSL and HSV color spaces for this encoding and a similar encoding using saturation instead of lightness, a subtle density difference was difficult to recognize from the generated colors. 
Thus, we utilize existing carefully designed sequential colormaps, specifically the red and blue sequential colormaps in Matplotlib.\footnote{\href{https://matplotlib.org/stable/tutorials/colors/colormaps.html}{https://matplotlib.org/stable/tutorials/colors/colormaps.html}}
We can obtain a pair of colors corresponding to the density from these two colormaps and then generate an interpolated color from the pair based on $\DensRatioFuncClassZero(\Point)$.
To show this 2D color space as a legend, we use a polar coordinate, as shown at the bottom right of \autoref{fig:scatterplots}.
This is to clearly indicate the difference between the meanings of two measures (density and ratio) as well as to inform that the class ratio difference is less emphasized in low-density areas.
As shown in \autoref{fig:scatterplots}-c, we also tested existing bivariate colormaps such as one introduced in \cite{lespinats2011checkviz}.
However, from \autoref{fig:scatterplots}-c, it is difficult to recognize, for example, \texttt{Class\,1}'s high-density area and the density difference between the high-density areas of \texttt{Class\,0} and \texttt{Class\,1}.

While we develop the two-class density scatterplot for the views in \autoref{fig:ui}-b, we employ the same design for \autoref{fig:ui}-a. 
As presented in \autoref{sec:attrib_contrib_shap}, the two-class density scatterplot can clearly show patterns of two classes even in the limited plotting space. 


\subsection{Interactive Visual Interface}

All the visualizations in the UI are fully linked and share the same or similar color encodings (e.g., red represents \texttt{Class\,0}). 
The user can perform lasso selection in \autoref{fig:ui}-a, b, c, and the selected instances from \texttt{Classes\,0} and \texttt{1} are highlighted in yellow (e.g., \autoref{fig:case2}-c).
With this linking, the analyst can investigate specific patterns, such as outliers and subgroups.
In \autoref{fig:ui}-c and d, we provide visualizations of the dataset's structural and semantic information to supplement the interpretation of the results.

\textbf{Structural information.} 
We use \autoref{fig:ui}-c to show network layouts as the structural information of the data. 

As the precomputed network measures (e.g., node degree) used for NRL (see \autoref{sec:nrl}) are computed based on links among all instances, to locate patterns related to the structural information, we need to review the entire network, instead of the subnetwork only consisting of instances in the two ends (i.e., \texttt{Classes\,0} and \texttt{1}).
Thus, only in this view, we visualize all instances including even those not belonging to the two ends.
We lay out all instances by scalable force-directed placement (SFDP)~\cite{hu2005efficient} and use red, blue, and gray colors to represent \texttt{Class\,0}, \texttt{Class\,1}, and other instances, respectively.
When performing lasso selection in this view, only \texttt{Class\,0} and \texttt{Class\,1} instances inside of the lasso are selected to consistently link with the other views.

\textbf{Attribute information.} 
We visualize the distribution of each attribute as a double-bar histogram. 
By default, as shown in \autoref{fig:ui}-d, the UI shows the distribution for each class.
When the lasso selection is performed, we show the distributions of selected and non-selected instances (e.g., \autoref{fig:case1}-b1).
As we have limited space, we order histograms based on the two groups' distribution differences measured by the Kolmogorov--Smirnov (KS) statistic.

\textbf{Implementation.}
The UI is developed as a web application. 
For the back end, we use Python to perform Step 2 with DeepGL and MLPs, Step 3 with regularized LDA, Step 4 with the SHAP method, Step 5 with multivariate regression and the COBYLA, and Step 6 with the two-class density scatterplot generation, SFDP, and KS statistic.
For these algorithms, we utilize various libraries and packages, such as deepgl~\cite{fujiwara2022network}, PyTorch~\cite{pytorch2019paszke} (for MLPs), ulca~\cite{fujiwara2021interactive} (for regularized LDA), SHAP~\cite{shap_library}, Scikit-learn~\cite{pedregosa2011scikit} (for multivariate regression), NumPy/SciPy~\cite{virtanen2020scipy} (for the COBYA, KS statistic, and Gaussian kernel density estimation), graph-tool~\cite{graphtool2014peixoto} (for SFDP), seaborn~\cite{waskom2021seaborn} (for the swarm-like visualization), and ColorAide~\cite{coloraide} (for color generation).
The front-end UI is implemented with a combination of HTML5, JavaScript, and D3~\cite{bostock2011d3}. 
We use WebSocket to communicate between the front- and back-end modules.
