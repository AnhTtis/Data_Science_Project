\section{Related Work}
\label{sec:related_work}

\noindent
Our work is closely related to two topics in the visualization field: NRL and the interpretation of learned representations. 
For the broader discussion on visualizations for analyzing networks and interpreting ML results, refer to existing surveys~\cite{kerren2014multivariate,mcgee2019state,beck2017taxonomy,chatzimparmpas2020state}.

\subsection{Learning Network Representation}

\noindent
NRL aims to generate a set of low-dimensional vectors (also called representation) that captures certain important characteristics of networks, nodes, or links~\cite{zhang2018network}. 
The representation is usually learned for downstream tasks, such as node classification and link prediction. 
Various NRL methods are developed, including node2vec~\cite{grover2016node2vec}, graph convolutional networks~\cite{kipf2016semi}, graph neural networks (GNNs) with the self-attention~\cite{ying2021transformers}, to name but a few~\cite{zhang2018network}. 

The researchers have been utilizing NRL for visualization to interactively examine complex network datasets.
For example, Freire et al.~\cite{freire2010manynets} represented one network by a set of network statistics (e.g., degree distribution) to compare many networks in a tabular interface. 
Gove~\cite{gove2019gragnostics} suggested several network-level features (e.g., density) that are easier to interpret and faster to compute for interactive visualization.
Other researchers utilized the occurrences of graphlets~\cite{prvzulj2007biological} (small, connected, non-isomorphic subgraph patterns) to identify visually similar networks~\cite{von2009visual,harrigan2012egonav,kwon2017would}.
When node correspondence exists among networks, another common approach is directly applying DR methods to networks' adjacency matrices to capture the similarities of networks~\cite{bach2016time,fujiwara2017visual}.
Van den Elzen et al.~\cite{vandenelzen2016reducing} took a similar DR approach while further incorporating network statistics. 
Martins et al.~\cite{martins2012multidimensional,martins2017mvn} used DR to lay out nodes by their structural and semantic similarities.

Similar to ours, recently, a few works employed NN-based NRL.
Fujiwara et al.~\cite{fujiwara2022network} introduced contrastive NRL (cNRL) by integrating a variant of GNNs and contrastive learning~\cite{zou2013contrastive}.
cNRL extracts a representation of two networks to highlight salient characteristics in one network relative to another.
Utilizing linear DR, they further designed an interpretable cNRL method and enhanced it with interactive visualizations~\cite{fujiwara2020visual}. 
Song et al.~\cite{song2022interactive} used GNNs to support interactive subgraph pattern search, where GNNs are used to covert each network in a comparable, fixed-length latent vector. 

Unlike the above approaches, we use NN-based NRL to obtain representations that are specifically for uncovering the associations of interest in multivariate networks.
Also, we address the interpretation of network representations with composite variable construction, which is easier to understand when compared with the approaches referring to complex coefficients in linear DR results~\cite{fujiwara2020visual,fujiwara2022network}.

\subsection{Interpreting Representations}

\noindent
Although the interpretation support for NRL is still sparsely studied (e.g., \cite{fujiwara2020visual}), a variety of interpretation methods are developed to explain high-dimensional data representations that are extracted by DR methods or ML models~\cite{molnar2020interpretable, huang2023va, la2023state, choo2018visual, hohman2018visual}. 
Existing interpretation methods can be mainly categorized into two approaches: (1) identifying essential information to specific patterns found in representations (i.e., post-hoc explanation approach) and (2) constructing simple, interpretable representations during a learning phase (i.e., explainability-by-design approach~\cite{hamon2020robustness}).


\subsubsection{Post-Hoc Approach}

\noindent
Within visual analytics methods, researchers have identified influential attributes on the cluster formation in DR results from statistical charts (e.g., boxplots of attributes for each cluster)~\cite{kwon2018clustervision,neto2021multivariate,vanozenoodt2022outoftheplane}.
As univariate statistics are often insufficient to capture the cluster characteristics, researchers further considered influences from multiple attributes~\cite{fujiwara2020supporting,joia2015uncovering,turkay2012representative,zhou2016dimension,zang2022evnet}.
For example, Joia et al.~\cite{joia2015uncovering} applied PCA to each cluster to examine multivariate influences.

Another common visual analytics strategy is utilizing measures that inform how strongly the change of an input attribute value influences an ML inference result (e.g., how many more bikes are rented if the temperature increases one degree)~\cite{krause2016interacting, wexler2019if, apley2020visualizing, li2020multimodel, angelini2023visual}. 
Such measures include partial dependence (PD) and SHAP values~\cite{lundberg2017shap}.
For example, Krause et al.~\cite{krause2016interacting} developed a system to interactively investigate PD. 
Their system only supports analysis of PD that is obtained by changing each attribute's values individually---interpretation only from a single attribute level. 
Angelini et al.~\cite{angelini2023visual} introduced a visual analytics framework that can even deal with PD derived from a simultaneous change of multiple attributes.

Similar to Angelini et al.'s work~\cite{angelini2023visual}, composite variable construction in our workflow also allows a multi-attribute level interpretation.
However, PD or other perturbation-based measures (e.g., SHAP values) merely output a value for each perturbed input.
Consequently, it is not trivial to induce numerical relationships between input attributes and ML inference results. 
In contrast, our workflow's composite variable provides a linear function that summarizes the associations among multiple attributes.
This ability helps us perform the interpretation both efficiently and intuitively.


\subsubsection{Explainability-By-Design Approach} 
When compared with the post-hoc approach, the explainability-by-design approach is taken by a smaller number of visual analytics works.
Knittle et al.~\cite{knittel2020visual} used NNs consisting of one hidden layer with a small number of NN nodes to extract nonlinear representations that relate input attributes to a target output attribute.
This simple NN allows analysts to identify 
a small number of representations that show clear associations between the input and target attributes. 
Gleicher~\cite{gleicher2013explainers} produced simple composite variables that are to classify a user-selected attribute.
To craft such composite variables, Gleicher performed a support-vector machine-based exhaustive search for the selection of composing variables while considering a balance between their simplicity and expressiveness.

In terms of using NNs to extract the input-output relationships, the work by Knittle et al.~\cite{knittel2020visual} is closely related to ours. 
However, their interpretation of the obtained representations is based only on univariate value distributions, which is insufficient when NNs capture complex input-output relationships.
Similar to Gleicher's work~\cite{gleicher2013explainers}, our work crafts simple composite variables, but we do not involve the computationally expensive exhaustive search. 
Instead, we rank variables based on their contributions to the NNs' predictions and involve analysts' knowledge to select attributes of interest.