\documentclass[12pt]{article}
\usepackage{latexsym,amsfonts,amssymb}
\setlength{\parindent}{16pt} \setlength{\parskip}{8pt}
\setlength{\baselineskip}{8pt plus 2pt minus 1pt}
\setlength{\textheight}{230 mm} \setlength{\textwidth}{174 mm}
\oddsidemargin=-0.4cm
%\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\hoffset 0cm \voffset -2.1cm

\usepackage{csvsimple}
\usepackage{verbatim}
\usepackage{bm}
\usepackage[dvips]{color}
\usepackage{colordvi,multicol}
\usepackage{amsmath}
 \usepackage{graphicx}
 \usepackage[english]{babel}
\usepackage{caption}
\captionsetup[figure]{font=scriptsize}
\usepackage{tabu}
\usepackage{float}
\usepackage[pagewise, left, displaymath, mathlines]{lineno}
\usepackage{dsfont}
\usepackage[shortlabels]{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algcompatible}
\usepackage{titlesec}
\usepackage{mathtools}

\setcounter{secnumdepth}{4}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
%\linenumbers
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\arraystretch}{1.5}
\newcommand{\starm}{{}_{\overset{M}{\star}}}
\newcommand{\xms}{X_{t-}^\star}
\newcommand{\yms}{Y_{t-}^\star}
\newcommand{\zms}{Z_{t-}^\star}
\newcommand{\xd}{X_{t}^\dagger}
\newcommand{\yd}{Y_{t}^\dagger}
\newcommand{\zd}{Z_{t}^\dagger}


\newcommand{\xmsd}{X_{t_{k-1}-}^\star}
\newcommand{\ymsd}{Y_{t_{k-1}-}^\star}
\newcommand{\zmsd}{Z_{t_{k-1}-}^\star}

\newcommand{\xdd}{X_{t_{k-1}}^\dagger}
\newcommand{\ydd}{Y_{t_{k-1}}^\dagger}
\newcommand{\zdd}{Z_{t_{k-1}}^\dagger}

\newcommand{\ce}{C^\epsilon_t(1)}
\newcommand{\co}{C^0_t(1)}
\newcommand{\ci}{C^\epsilon_t(i)}
\newcommand{\coi}{C^0_t(i)}
\newcommand{\ces}{C^\epsilon_s(1)}
\newcommand{\cs}{C^0_s(1)}
\newcommand{\cm}{C^\epsilon_{s-}(1)}
\newcommand{\com}{C^0_{s-}(1)}
\newcommand{\dint}{\displaystyle\int}
\def \cal{\mathcal}
\newcommand{\comm}[1]{}
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exa}[thm]{Example}
\newtheorem{ass}[thm]{Assumption}
%\newfam\msbfam
%\font\tenmsb=msbm10 \textfont\msbfam=\tenmsb \font\sevenmsb=msbm7
%\scriptfont\msbfam=\sevenmsb \font\fivemsb=msbm5
%\scriptscriptfont\msbfam=\fivemsb
%\newfam\msbfam
%\font\tenmsb=msbm10 \textfont\msbfam=\tenmsb \font\sevenmsb=msbm7
%\scriptfont\msbfam=\sevenmsb \font\fivemsb=msbm5
%\scriptscriptfont\msbfam=\fivemsb




\date{}

\begin{document}
\title{\bf Parameter estimation of stochastic SIR model driven by small L\'{e}vy noises with time-dependent periodic transmission}
 \author{Terry Easlick and Wei Sun\thanks{Corresponding author.}\\ \\
  {\small Department of Mathematics and Statistics}\\
    {\small Concordia University, Canada}\\ \\
{\small  terry.easlick@concordia.ca,\ \ \ \ wei.sun@concordia.ca}}





\maketitle

\begin{abstract}
\noindent We investigate the parameter estimation and forecasting of two forms of
the stochastic SIR model driven by small L\'{e}vy noises. We accomplish this by utilizing least squares estimators.
We include simulation studies of both forms using the method of projected gradient descent to aid
in the parameter estimation.
\end{abstract}

\noindent  {\it MSC:} 62M05; 92D30; 62F12

\noindent  {\it Keywords:} Unified stochastic SIR model, parameter estimation, least squares method,
 time-dependency, periodic transmission, L\'evy noise.

\section{Introduction}

Interest in epidemiological models has increased in the previous decade especially with random noises added to the models. The classical Susceptible-Infected-Recovered (SIR) model, which was introduced by Kermack and McKendrick \cite{ker} nearly a century ago is defined as:
\begin{eqnarray*}
	\begin{cases}
		\frac{dX_t}{dt} =  - \beta X_tY_t, \\
		\frac{dY_t}{dt} = \left( \beta X_t - \gamma\right)Y_t,\\
		\frac{dZ_t}{dt} = \gamma Y_t,
	\end{cases}
	\end{eqnarray*}
	where $\beta$ is the transmission rate and $\gamma$ the recovery rate. Additionally, demographics may be introduced to include birth rate $\Lambda$ and mortality rate $\mu$ as:
	\begin{eqnarray*}
	\begin{cases}
		\frac{dX_t}{dt} = \Lambda - \mu X_t - \beta X_tY_t, \\
		\frac{dY_t}{dt} = \left[ \beta X_t - (\mu+\gamma )\right]Y_t,\\
		\frac{dZ_t}{dt} = \gamma Y_t - \mu Z_t.
	\end{cases}
	\end{eqnarray*}
These deterministic models have been put into various
stochastic frameworks that make the situation more realistic (cf. e.g., \cite{T}, \cite{G}, \cite{JJS}, \cite{Bao}, \cite{Chen1}, \cite{Zhang0},  \cite{Zhou1}, \cite{Zhou2}, \cite{EL}, \cite{Liu}, \cite{Pri} and  \cite{EL2}).
More recently, the unified stochastic SIR (USSIR) model was introduced by the authors in \cite{eas}. The USSIR model is defined as
	\begin{eqnarray}\label{1March}
		\begin{cases}
		\begin{split}
		&dX_t =\ b_1(t,X_t,Y_t,Z_t)dt + \sum\limits_{j=1}^r \sigma_{1j}(t,X_t,Y_t,Z_t)dB_t^{(j)}\\
			&\indent\indent + \int_{\left\{\lvert u \rvert \leq 1\right\}}\hspace{-.3cm}H_1(t,X_{t-},Y_{t-},Z_{t-},u)\tilde{N}(dt,du)+ \int_{\left\{\lvert u \rvert > 1\right\}}\hspace{-.3cm}G_1(t,X_{t-},Y_{t-},Z_{t-},u)N(dt,du),
		\end{split}\\
		\begin{split}
		&dY_t =\ b_2(t,X_t,Y_t,Z_t)dt + \sum\limits_{j=1}^r\sigma_{2j}(t,X_t,Y_t,Z_t)dB_t^{(j)}\\
			&\indent\indent+ \int_{\left\{\lvert u \rvert \leq 1\right\}}\hspace{-.3cm}H_2(t,X_{t-},Y_{t-},Z_{t-},u)\tilde{N}(dt,du)+  \int_{\left\{\lvert u \rvert > 1\right\}}\hspace{-.3cm}G_2(t,X_{t-},Y_{t-},Z_{t-},u)N(dt,du),
		\end{split}\\
		\begin{split}
		&dZ_t =\ b_3(t,X_t,Y_t,Z_t)dt + \sum\limits_{j=1}^r\sigma_{3j}(t,X_t,Y_t,Z_t)dB_t^{(j)} \\
			&\indent\indent+\int_{\left\{\lvert u \rvert \leq 1\right\}}\hspace{-.3cm}H_3(t,X_{t-},Y_{t-},Z_{t-},u)\tilde{N}(dt,du) + \int_{\left\{\lvert u \rvert > 1\right\}}\hspace{-.3cm}G_3(t,X_{t-},Y_{t-},Z_{t-},u)N(dt,du).
		\end{split}\\
		\end{cases}
		\end{eqnarray}
	Hereafter, $\mathbb{R}_+$ denotes the set of all positive real numbers, $(B_t)_{t\ge 0}=(B^{(1)}_t,\dots,B^{(r)}_t)_{t \ge 0}$ is a standard
	$r$-dimensional Brownian motion, $N$ is a Poisson random measure on $\mathbb{R}_+\times (\mathbb{R}^l\backslash\{0\})$ with intensity
	measure $\mu$ satisfying $\int_{\mathbb{R}^l\backslash\{0\}}(1\wedge|u|^2)\mu(du)<\infty$ and $\tilde{N}(dt,du) = N(dt,du) - \mu(du)dt$,
	$(B_t)_{t \ge 0}$ and $N$ are independent, $b_i, \sigma_{ij}:[0,\infty) \times \mathbb{R}^3_+\mapsto\mathbb{R}$,
	$H_i,G_i:  [0,\infty) \times \mathbb{R}^3_+\times (\mathbb{R}^l\backslash\{0\}) \mapsto\mathbb{R}$, $i=1,2,3,\ j=1,2,\ldots,n$, are measurable functions.
		

The goal of this paper is to accomplish parameter estimation of a periodic transmission function present in a stochastic SIR model, e.g., the USSIR model (\ref{1March}). It is a natural question to ask if transmission of a
disease is periodic. Understanding this periodicity greatly aids in ability to predict possible outcomes. %We consider
%the case when the period is unknown since there is no reason to have this knowledge a priori.
A potential application is in the study of periodicity in
the COVID-19 pandemic, as well as future pandemics. The stochastic model chosen has driving noise of the L\'{e}vy variety subjected
to a small coefficient $\varepsilon$.  In our study, we focus not only on estimating unknown parameters and forecasting future
behaviour but also the asymptotics of our estimators.
Given the complexity of the model, we use optimization techniques to iteratively solve for approximations to the estimators.

In recent times, the study of parameter estimation in stochastic epidemiological
models has been gaining momentum, but there is still much work that needs to be completed. Notable works are available in the literature, see e.g.,  \cite{Feng}, \cite{Contact}, \cite{XLiu}, \cite{Mum1}, \cite{Pan}, \cite{Zhang}, \cite{Green},
\cite{Li}, \cite{Mum}, \cite{Jagan}, \cite{Wacker}, \cite{Alen}, \cite{Chen}, \cite{Sim}, \cite{Gir}, \cite{Paul},  \cite{Bodhi} and \cite{Kroger}. For our studies,
the focus is primarily the periodicity of transmission of disease.
It is worth noting that we assume no prior knowledge on the explicit form of the random noise. Additionally,
the periodicity we seek to estimate is not assumed to be tied to specific time periods such as days, weeks or months;
rather, whatever the observed time period may be is what determines the time scale in which the periodicity occurs.

In addition to papers on parameter estimation of epidemiological models, there is a growing number
of works which consider the more general setting of parameter estimation for stochastic differential equations (SDEs) with small L\'{e}vy
noises regardless of specific applications (cf. e.g., \cite{KA}, \cite{ms}, \cite{Uch}, \cite{Xu}, \cite{gl},
 \cite{Shi}, \cite{Sun}, \cite{DJ}, \cite{Long} and \cite{KO}).
The methodology we use in the paper is directly inspired by the results of  Long et al. in \cite{Sun} and \cite{Long}. More specifically,
we use the least-squares method, similarly to what has been completed in \cite{Sun} and \cite{Long}, to estimate a true parameter $\theta_0$ of a discretely observed stochastic
process. We introduce a contrast function from which we are able to derive the least-squares estimators (LSEs) and obtain results on the consistency
and limiting distributions of the estimators.

Consider a stochastic process $(S^{\varepsilon}_t)_{t\ge 0}$ which satisfies the SDE:
\begin{eqnarray}\label{MarchE11}
	dS^{\varepsilon}_t&=&b(t,S^{\varepsilon}_t,\theta)dt\nonumber\\
	&&+\varepsilon\left\{\sigma(t,S^{\varepsilon}_t)dB_t
	+{\int_{\{|u|\le1\}}}H(t,S^{\varepsilon}_{t-},u)\widetilde{N}
	(dt,du)+{\int_{\{|u|> 1\}}}G(t,S^{\varepsilon}_{t-},u)N
	(dt,du)\right\},\nonumber\\
	&&
	\end{eqnarray}
	where $0<\varepsilon<1$, $t\in[0,1]$, $S^{\varepsilon}_0=s\in \mathbb{R}^d$, $\theta\in \overline{\Theta}$, the closure of an open convex bounded subset $\Theta$ of $\mathbb{R}^p$,  $b(\cdot,\cdot,\cdot):[0,\infty)\times\mathbb{R}^d\times\Theta\to \mathbb{R}^d$,
	$\sigma(\cdot,\cdot):[0,\infty)\times \mathbb{R}^d \to \mathbb{R}^{d\times r}$,
	$H(\cdot,\cdot,\cdot)$, $G(\cdot,\cdot,\cdot):[0,\infty)\times \mathbb{R}^d \times  (\mathbb{R}^l\backslash\{0\}) \to \mathbb{R}^d$ are Borel measurable functions. Suppose
$(S^{\varepsilon}_t)_{t\ge 0}$ is observed at regularly
spaced time points $\{t_k = \frac{k}{n}$, $k = 1, 2,\dots, n\}$. Define the contrast function
  $$
  \Psi_{n,\varepsilon}(\theta)=\sum_{k=1}^n\varepsilon^{-2}nP_k^*(\theta)P_k(\theta),
  $$
  where
  $$
  P_k(\theta)= S^{\varepsilon}_{t_k} - S^{\varepsilon}_{t_{k-1}}
  -\frac{1}{n}b(t_{k-1},S^{\varepsilon}_{t_{k-1}}, \theta).
  $$
  Let $\hat{\theta}_{n,\varepsilon}$ be a minimum contrast estimator, i.e., a random variable satisfying
  $$
  \hat{\theta}_{n,\varepsilon} := {\rm arg\, min}_{\theta\in\Theta}\Psi_{n,\varepsilon}(\theta).
  $$


As we will see for the USSIR model, finding a closed form for the LSE $\hat{\theta}_{n,\varepsilon}$ can be quite difficult, thus
we look for suitable approximations
$\hat{\theta}_{n,\varepsilon}^*$ of $\hat{\theta}_{n,\varepsilon}$. In the vernacular for optimization, we may refer to our contrast function $\Psi_{n,\varepsilon}(\theta)$ as an objective function. Given an objective function that one
wishes to minimize for estimation purposes a commonly utilized
method is gradient descent (GD) (cf. \cite[Section 8.1]{beck}  for more information). More explicitly, given a convex function  $f$ without
constraints such that one wishes to solve
$$ \arg\min_x f(x), $$
the  method of GD minimizes by making use of the update rule
$$
	x^{(k+1)} = x^{(k)} - \eta \nabla f(x^{(k)})
$$
for some initial value $x^{(0)}$ and learning rate $\eta > 0$.
With the presence of constraints, GD is still applicable in the form of projected gradient descent (PGD)  (cf. \cite[Section 10.2]{beck}). That is, given a convex function
$f$ and a constraint set $\mathcal{C}$, one would use a similar update rule which can be stated as
$$
 y^{(k+1)} = x^{(k)}  - \eta \nabla f(x^{(k)}) ;\ \  x^{(k+1)} = \arg\min_{x\in\mathcal{C}}|y^{(k+1)}-x|.
$$

The remainder of this paper is organized as follows.  Section 2 gives the theoretical results on LSEs for time-dependent SDEs driven by small L\'{e}vy noises with applications to the USSIR model (\ref{1March}). Sections 3 and 4 cover simulation studies of SIR models for population proportions and population numbers, respectively. Section 5 is a closing discussion.

\section{Least-squares estimators for time-dependent SDEs driven by small L\'{e}vy noises}\setcounter{equation}{0}
\subsection{General time-dependent SDEs driven by small L\'{e}vy noises}

In this subsection, we investigate LSEs  for discretely observed stochastic
processes driven by small L\'evy noises. The results presented here generalize the results of Long et al. in \cite{Sun} and \cite{Long}. Our contributions are the generalization to
a time-dependent jump-diffusion model, which is more general to include different (singular and non-singular) coefficient functions for the Brownian motion, small jump and large jump portions.  Also noteworthy, is we use a contrast function which is different from that in \cite{Long}; however,
we also provide a result which uses  the same contrast function as that in \cite{Long} when the coefficient functions are equal and non-singular.

The primary focus of this paper is the parameter estimation for a stochastic SIR model; hence, proofs for the following results are omitted. We wish to note
that the proofs required to establish our results are obtained without essential difficulties and  follow the same pattern as that of \cite{Sun}
and \cite{Long}.

Suppose that $(S^{\varepsilon}_t)_{t\ge 0}$  satisfies the SDE (\ref{MarchE11}).
Consider an underlying deterministic (ordinary) differential equation denoted as
$$
		dS^0_t = b(t,S_t^0,\theta_0),\ t \in [0,1],\ S^0_0 = s,
$$
where $\theta_0$ is the true value of the drift parameter.
%Our estimation efforts are primarily concerned with estimating a periodic drift with parameter
%the parameter $\theta = (\theta_j : 1\leq j\leq p)$ whilst the noise decreases as
%$\varepsilon \to 0$ and number $n$ of observations increases.
We take the following assumptions which are modifications of those given in \cite{Long}.

\noindent {\bf({A}1)}
For all $\varepsilon > 0$, the SDE (\ref{MarchE11}) admits a unique strong solution $S^{\varepsilon}$ (cf. \cite[Theorem 2.2]{GS} for concrete sufficient conditions).

\noindent {\bf({A}2)}
There exist $K>0$ and $\eta,\xi\in {\cal B}_+(\mathbb{R}^l )$ such that for any $t\in[0,1]$, $x,y\in \mathbb{R}^d$, $u\in \mathbb{R}^l\backslash\{0\}$ and $\theta\in \overline{\Theta}$,
 \begin{eqnarray*}
 	&&|b(t,x,\theta) - b(t,y,\theta)| \le K|x-y|,\ \ \ \ \int_{|v|\le 1}\eta^2(v)\mu(dv)<\infty,\\
&& | b(t,x,\theta)|+|\sigma(t,x)|+\frac{1_{\{|u|\le 1\}}|H(t,x,u)|}{\eta(u)}+\frac{1_{\{|u|> 1\}}|G(t,x,u)|}{\xi(u)} \le K(1+|x|).
 \end{eqnarray*}

\noindent {\bf({A}3)}
$b(\cdot,\cdot,\cdot)\in C_{\uparrow}^{1,1,3}([0,1]\times\mathbb{R}^d \times \Theta ; \mathbb{R}^d)$, i.e., $b$ is continuously differentiable and thrice continuously
 differentiable in $t,x$ and $\theta$, respectively, and there exist constants $C,\lambda>0$ such that
$$
	\sup_{t \in [0,1]} \sup_{\theta \in \Theta} \lvert \partial^{\nu_3}_\theta\partial^{\nu_2}_x \partial^{\nu_1}_t b(t, x, \theta) \rvert
	\leq C(1+|x|)^{\lambda},
$$
for any multi-indices $\nu_1,\nu_2,\nu_3$ satisfying
$$0 \leq \nu_1\le1,\ \ 0\le\sum_{i = 1}^d\nu_2^{(i)} \leq 1,\ \ 0 \leq \sum_{j=1}^p \nu_3^{(j)} \leq 3.$$

\noindent {\bf({A}4)}  $\theta \neq \theta_0 \Leftrightarrow \exists t \in [0,1]$ such that  $b(t,S_t^0,\theta) \neq b(t,S_t^0,\theta_0)$.


\noindent {\bf({A}5)} $\sigma(\cdot,\cdot)$ is continuous on $[0,1]\times \mathbb{R}^d$ and $H(\cdot,\cdot,u)$, $G(\cdot,\cdot,u)$ are continuous on $[0,1]\times \mathbb{R}^d$ for any $u\in \mathbb{R}^l\backslash\{0\}$.


\noindent {\bf({A}6)}
  $\varepsilon=\varepsilon_n\rightarrow 0$ and $n\varepsilon \rightarrow \infty$ as $n \rightarrow\infty$.

We adopt the notation $o_p(1)$ which is short for a sequence of
random vectors that converges to zero in probability, and  the notation $\xrightarrow{P_{\theta_0}}$ which is short for convergence in probability under $P_{\theta_0}$. By virtue of \cite[Theorem 5.7]{van}, similar to \cite[Theorem 2.1]{Sun, Long}, we can prove the following result on consistency of the LSEs.

  \begin{thm}\label{thm1} Let $\hat{\theta}^*_{n,\varepsilon}$ be any sequence of estimators with $\Psi_{n,\varepsilon}(\hat{\theta}^*_{n,\varepsilon})\le \Psi_{n,\varepsilon}(\theta_0)+o_p(1)$. Then, under conditions (A1)-(A4), we have
	\begin{equation*}
		\hat{\theta}^*_{n,\varepsilon} \xrightarrow{P_{\theta_0}} \theta_0 \text{ as } \varepsilon \to 0
		\text{ and } n \to \infty.
	\end{equation*}
	\end{thm}
	
	Define the matrix $I(\theta)=(I^{ij} (\theta) )_{1\le i,j\le p}$ by
	$$
	I^{ij}(\theta)=\int_0^1(\partial_{\theta_i}b)^*(r,S^0_r,\theta)\partial_{\theta_j}b(r,S^0_r,\theta)dr.
	$$
Similar to \cite[Theorem 2.2]{Sun, Long}, we can prove the following result on rate of convergence of the LSEs.	

	\begin{thm}\label{thm2} Assume that conditions (A1)-(A6) hold and $I(\theta_0)$ is positive definite. Then,
	\begin{eqnarray*}
	\varepsilon^{-1}(\hat{\theta}_{n,\varepsilon} -\theta_0)
	&\xrightarrow{P_{\theta_0}}&I^{-1}(\theta_0)\left(\int_0^1(\partial_{\theta_i}b)^*(r,S^0_r,\theta)\Bigg\{\sigma(r,S^0_r)dB_r\right.\\
	&&\left.
	\ \ \ \ +{\int_{\{|u|\le1\}}}H(r,S^0_r,u)\widetilde{N}
	(dr,du)+{\int_{\{|u|> 1\}}}G(r,S^0_r,u)N
	(dr,du)\Bigg\}\right)^*_{1\le i\le p}
	\end{eqnarray*}
	$\text{ as } \varepsilon \to 0
		\text{ and } n \to \infty$.
	\end{thm}
	
\begin{rem}\label{rem1} Consider the following SDE:
\begin{eqnarray*}
	dS^{\varepsilon}_t=b(t,S^{\varepsilon}_t,\theta)dt+\varepsilon\sigma(t,S^{\varepsilon}_t)\left\{dB_t
	+{\int_{\{|u|\le1\}}}u\widetilde{N}
	(dt,du)+{\int_{\{|u|> 1\}}}uN
	(dt,du)\right\}.
	\end{eqnarray*}
In the event the diffusion matrix $\sigma\sigma^*$ is invertible, we may use the following contrast function from Long et al. \cite{Long}:

\begin{eqnarray}\label{March7a}
  \Psi_{n,\varepsilon}(\theta)=\bigg(\sum_{k=1}^n\varepsilon^{-2}nP_k^*(\theta)\Lambda_{k-1}^{-1}P_k(\theta)\bigg)1_{\{D>0\}},
\end{eqnarray}
  where
  $$
  P_k(\theta)= S^{\varepsilon}_{t_k} - S^{\varepsilon}_{t_{k-1}}
  -\frac{1}{n}b(t_{k-1},S^{\varepsilon}_{t_{k-1}}, \theta),\ \  \Lambda_{k-1} = [\sigma\sigma^*](t_{k-1},S_{t_{k-1}}),\ \ D = \inf_{k=0,\ldots,n-1}\det \Lambda_k.
$$
Define the matrix $I(\theta)=(I^{ij} (\theta) )_{1\le i,j\le p}$ by
\begin{eqnarray}\label{March13a}
	I^{ij}(\theta)=\int_0^1(\partial_{\theta_i}b)^*(r,S^0_r,\theta)[\sigma\sigma^*]^{-1}(r,S^0_r)\partial_{\theta_j}b(r,S^0_r,\theta)dr.
\end{eqnarray}
We make the following additional assumption.

\noindent {\bf({A}7)}
There exists an open convex subset $\mathcal{U}\subset \mathbb{R}^d$ such that
	$X_t^0 \in \mathcal{U}$ for all $t \in [0,1]$, $\sigma$ is smooth on $[0,1]\times \mathcal{U}$, and $\sigma\sigma^*$ is invertible on $[0,1]\times\mathcal{U}$.

 Following the arguments of \cite[Theorems 2.1 and 2.2]{Sun, Long}, we can prove the following result.

\begin{cor} Assume that conditions ({A}1)-({A}7) hold and $I(\theta_0)$ defined by (\ref{March13a}) is positive definite. Then, the assertions of
	Theorems \ref{thm1} and \ref{thm2} hold for the LSE derived from the contrast function (\ref{March7a}).
\end{cor}

\end{rem}

\subsection{Application to USSIR model with periodic transmission}
Setting $d=3$, we use equation (\ref{MarchE11}) to write the USSIR model as

\begin{eqnarray}\label{eqw}
\begin{bmatrix}	
dX_t^\varepsilon \\
dY_t^\varepsilon \\
dZ_t^\varepsilon \\
\end{bmatrix}
&=&
\begin{bmatrix}
	b_1(t,X_t^\varepsilon,Y_t^\varepsilon,Z_t^\varepsilon, \theta) \\
	b_2(t,X_t^\varepsilon,Y_t^\varepsilon,Z_t^\varepsilon,\theta) \\
	b_3(t,X_t^\varepsilon,Y_t^\varepsilon,Z_t^\varepsilon,\theta)
\end{bmatrix} dt +\varepsilon\bigg\{\sigma(t,X_t^\varepsilon,Y_t^\varepsilon,Z_t^\varepsilon)dB_t \\ &&
\ \ \ \ +{\int_{\{|u|\le1\}}}H(t,X^{\varepsilon}_{t-},Y^{\varepsilon}_{t-},Z^{\varepsilon}_{t-},u)\widetilde{N}
(dt,du)\nonumber\\ &&\
 \ \ \ +{\int_{\{|u|> 1\}}}G(t,X^{\varepsilon}_{t-},Y^{\varepsilon}_{t-},Z^{\varepsilon}_{t-},u)N
(dt,du)\bigg\},\nonumber	
\end{eqnarray}
such that all previously stated assumptions hold.
Above in equation (\ref{eqw}), the drift function $b=(b_1,b_2,b_3)$ is given in more general terms but in the subsequent sections explicit instances will be
given. Moreover, at the centre of our focus in this paper is the presence of a periodic transmission function. Since
any well-behaved periodic function may be approximated using a Fourier series, we consider the drift function
to contain a periodic transmission function of the form
\begin{equation}\label{beta}
	\beta(t) = \alpha_0 + \sum_{k=1}^K \alpha_{1,k}\cos\left(\frac{2\pi kt}{\theta}\right)  + \alpha_{2,k}\sin\left(\frac{2\pi kt}{\theta}\right),
\end{equation}
where $\theta,\alpha_0,\alpha_{1,k},\alpha_{2,k}>0,1\leq k\leq K$. This leads to the  contrast function being denoted
by $\Psi_{n,\varepsilon}(\theta,\alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K)$ so that our parameters are represented by the vector
$(\theta,\alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K)$ $\in \mathbb{R}_+^{2K+2}$. Moreover, assume that $\theta \in [0,1]$, a natural assumption to make
given the estimation happens from time $t=0$ to $t=1$. The contrast function $\Psi_{n,\varepsilon}(\theta,\alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K)$ is not globally convex; however, for a fixed $\theta$, it becomes convex in the remaining parameters. Hence, we introduce the following algorithm:

\begin{algorithm}\label{ssgd}
	\caption{\ \ Linear-Search Gradient-Descent (LS-GD)}%\label{alg:cap}
	{\bf For} {$i$ in $1$ to $M$,} \\
	\phantom{Step}Fix a test value of $\theta$: $\theta_i \in \left( \frac{i-1}{M}, \frac{i}{M}  \right)$;\\
	\phantom{Step}Set the initial value $(\alpha_0^{(0)},(\alpha_{1,k}^{(0)},\alpha_{2,k}^{(0)})_{k=1}^K)$;\\
	\phantom{Step}Run Gradient Descent on the function $\Psi_{n,\varepsilon}(\theta_i, \alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K)$ with update rule\\
	\phantom{Step}$(\alpha_0^{(l)},(\alpha_{1,k}^{(l+1)},\alpha_{2,k}^{(l+1)})_{k=1}^K) =
	(\alpha_0^{(l)},(\alpha_{1,k}^{(l)},\alpha_{2,k}^{(l)})_{k=1}^K) - \eta \nabla_{\{\alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K\}}\Psi_{n,\varepsilon}(\theta_i, \alpha_0^{(l)},(\alpha_{1,k}^{(l)},\alpha_{2,k}^{(l)})_{k=1}^K )$;\\
	\phantom{Step}Store $\Psi^*_i := \arg\hspace{-2em}\min\limits_{\{\alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K\}}\hspace{-1em} \Psi_{n,\varepsilon}(\theta_i, \alpha_0,(\alpha_{1,k},\alpha_{2,k})_{k=1}^K )$.\\
	{\bf End}\\
	Return $ \min_{i \in \{1,\ldots, M\}}\Psi^*_i$.
\end{algorithm}

\noindent The returned value $\min_{i \in \{1,\ldots, M\}}\Psi^*_i$ will be the approximation $(\theta^*,\alpha_0^*,(\alpha_{1,k}^*,\alpha_{2,k}^*)_{k=1}^K)$ of the LSE
$(\hat{\theta},\hat{\alpha}_0,(\hat{\alpha}_{1,k},\hat{\alpha}_{2,k})_{k=1}^K)$ we seek.
\begin{rem}
	The above algorithm can be modified for PGD, which we do indeed use as will be seen in
	the simulation studies.
\end{rem}

For the remainder, we take $K=1$, that is we
consider the case when $\beta(t) = \alpha_0 + \alpha_{1}\cos(2\pi t/\theta)  + \alpha_{2}\sin(2\pi t/\theta)$.
Choosing $K>1$ does not contribute any more insights. Additionally, choosing $K>1$ would be more computationally expensive to evaluate in our simulation studies.

Recall that our primary goal is estimation of the unknown period $\theta$ and the coefficients $(\alpha_0,\alpha_1,\alpha_2)$ of the periodic transmission function
$\beta(t)$. The presence of noise makes for a more realistic model; moreover, we make no assumptions about the exact forms of the
noise coefficient functions $\sigma, H,G$.

%\begin{rem}
%	It is not difficult to verify that the drift function $b = (b_1,b_2,b_3)$ will satisfy assumption $%[{\bf({A}4)}]$ will satisfy assumption given it will be
%	sufficent to verify this assumption for $\beta(t)$ given in (\ref{beta}).
%\end{rem}


\section{Simulation study of SIR model for population proportions}\setcounter{equation}{0}
For this study, the unknown parameters are represented by the vector $\left( \theta,\alpha_0,\alpha_1,\alpha_2 \right) \in \mathbb{R}^4_+$
and our objective function is denoted $\Psi_{n,\varepsilon}\left( \theta,\alpha_0,\alpha_1,\alpha_2 \right)$ where
$n$ is the number of observations we have and $\varepsilon \in (0,1)$.
For a fixed $\theta$, recall the convexity of $\Psi_{n,\varepsilon}\left( \theta,\alpha_0,\alpha_1,\alpha_2 \right)$ with respect to  $(\alpha_0,\alpha_1,\alpha_2)$
one only needs to check the Hessian matrix is positive semi-definite.

For our simulation and numerical estimation results we make use of the Julia programming language. This language is well suited for
this application as it was created with the purpose of being a high-level programming language which is scientific computing centric.
We use two well maintained Julia packages; namely,  {\it DifferentialEquations.jl} and {\it Optim.jl } (cf. \cite{optim} and \cite{DiffEq}).
More of which is discussed below; however,  we want to note the package {\it Optim.jl } handles the projection to
the constraint set for us; namely, as we are not computing these estimates by hand we are able to
reasonably circumvent difficulty presented by projecting to the constraint set. In short, we pass
the constraint set $\mathcal{C}$ as an argument to the suitable method from {\it Optim.jl}.

Using the Julia programming language, and utilizing the package {\it DifferentialEquations.jl}, we are able to generate synthetic data for our testing purposes -- we generate the data such that we have $100$ observations.
Namely, an Euler-Maruyama discretization of the USSIR model is used from which we take $100$ observations to utilize in the estimation of
the unknown parameters. We next construct the contrast function and approximate the LSE by use of the LS-GD algorithm.
We accomplish the task of implementing the LS-GD algorithm with the aid of the Julia package {\it Optim.jl}.
More explicitly, the learning rate $\eta$, present in the update rule in (Algorithm 1), is chosen by a Hager-Zhang line-search
algorithm. For a more in-depth exploration, we refer the reader to the {\it Optim.jl } documentation in \cite{optim} and
also to the paper by Hager and Zhang \cite{hager}.
%In order to up the efficiency of the above algorithm we
%pass not only $\Psi_{n,\varepsilon}(\hat{\theta_i}, \alpha_0,\alpha_1,\alpha_2)$ but
%also $\nabla \Psi_{n,\varepsilon}(\hat{\theta_i}, \alpha_0,\alpha_1,\alpha_2)$ with respect to
%to $\alpha_0,\alpha_1,\alpha_2$.

Noteworthy, is that the contrast function $\Psi_{n,\varepsilon}$ is non-linear in the unknown parameters.
This is clear by the definition of $\beta(t)$. Hence,
there is inter-play, or dependence of optimal estimators between $\alpha_i$ and $\theta$
for $i =1,2$. Moreover, this leads to the interesting results that we are able to
estimate two or more unknown parameters which are present in a dependent setting
of a multiplicative form.

\begin{rem}
	Through some heuristics we arrived at the initial value
	$(\alpha_0^{(0)},\alpha_1^{(0)},\alpha_2^{(0)}) \newline = (0.5,0.31,0.21)$; which is
	used in all implementations of the LS-GD algorithm for the purposes of this paper. There is nothing
	particularly significant about this value hence we have no reason to believe it had any impact on our findings.

	For choosing the test values of $\theta_i$, we chose them such that $\theta_i \sim \text{Uniform}(\frac{i-1}{M}, \frac{i}{M})$.
	As the value $M$ increases, it does not make a difference if we individually choose the value
	or allow it to be randomly assigned within each subinterval.
\end{rem}

\subsection{Model for population proportions}
Consider the following model for population proportions:
\begin{equation}\label{ex2}
\begin{bmatrix} dX_t \\ dY_t \\ dZ_t \end{bmatrix} = \begin{bmatrix}  - \beta(t)X_tY_t \\ \beta(t)X_tY_t  - \gamma Y_t \\ \gamma Y_t \end{bmatrix} dt\, +\,
	 \varepsilon\! \begin{bmatrix}-\sigma X_{t-}Y_{t-}Z_{t-}   \\ 2\sigma X_{t-}Y_{t-}Z_{t-}   \\-\sigma X_{t-}Y_{t-}Z_{t-} \end{bmatrix} dL_t, \vspace{1cm}
\end{equation}
where
$$
L_t = B_t + \dint_0^t\dint_{\{|u|>  0.1\}} uN(ds,du),
$$
 and as before,
$$\beta(t) = \alpha_0 + \alpha_1\cos\left(\frac{2\pi t}{\theta}\right)  + \alpha_2\sin\left(\frac{2\pi t}{\theta}\right) ,\ \ \ \ \theta \in [0,1],\alpha_0,\alpha_1,\alpha_2 > 0,$$
$\gamma, \sigma>0$ are constants, and $N = N_1 + N_2 $ such that $N_1$, $N_2$ are independent Poisson random measures with respective intensity
measures $\mu_1$ and $\mu_2 $:
\begin{equation}\label{j1}
	\begin{cases}
		N_1 \sim \frac{2}{3}\lambda dt\mu_1(du) \text{ with } \mu_1 = \delta(\{-0.1,0.1,0.0\}), \\
		N_2 \sim \frac{1}{3}\lambda dt\mu_2(du) \text{ with } \mu_2 = \delta(\{0.0,-0.1,0.1\}),
	\end{cases}
\end{equation}
where $\lambda \sim \text{Uniform}(\{1,2,3,4\})$.

By  \cite[Theorem 2.1]{eas}, the SDE (\ref{ex2}) has a unique strong solution taking values
in the set $\Delta = \{(x,y,z) \in \mathbb{R}^3_+ : x+y+z =1\}$. It is easy to verify that all assumptions of Theorems 2.1 and 2.2 hold. We use the following contrast function:

$$
	\Psi_{\varepsilon,n}(\theta,\alpha_0,\alpha_1,\alpha_2) =100\varepsilon^{-2} \left(\sum_{k=1}^{100} P_k^*(\theta,\alpha_0,\alpha_1,\alpha_2)P_k(\theta,\alpha_0,\alpha_1,\alpha_2) \right),
$$
where
$$
P_k(\theta,\alpha_0,\alpha_1,\alpha_2) = \begin{bmatrix}X_{t_k}  - X_{t_{k-1}} - \frac{1}{100} \left[- \beta({t_{k-1}} )X_{t_{k-1}}Y_{t_{k-1}} \right]\\ Y_{t_k}  - Y_{t_{k-1}} - \frac{1}{100}\left[ \beta({t_{k-1}} )X_{t_{k-1}}Y_{t_{k-1}} - \gamma Y_{t_{k-1}} \right] \\  Z_{t_k}  - Z_{t_{k-1}}  - \frac{1}{100}\gamma Y_{t_{k-1}}  \end{bmatrix}.
$$.


\subsection{Parameter estimation}
We generate the synthetic data $10000$ times and randomly chose $1000$ such data sets to perform estimation upon.
Set $\gamma = 0.07142$, $\sigma = 0.5$ and $(X_0,Y_0,Z_0) = (0.84,0.07,0.11)$.
For the unknown parameters, the true values for use in generating the synthetic data are randomly decided
upon as follows
\begin{equation}\label{true}
\theta \sim \text{Uniform}(0,1),\ \  \alpha_0 \sim \text{Uniform}(0.1,0.8),\ \  \alpha_1,\alpha_2 \sim \text{Uniform}\left(0,\frac{\alpha_0}{\sqrt{2}}\right).
\end{equation}
It is noteworthy that although the distributions of $\alpha_1,\alpha_2$ are dependent on
the distribution of $\alpha_0$, their specific values are not. The reasoning for this dependency is we require $\beta(t) \ge 0$.
Also note that for each generation of synthetic data, the jump parameter $\lambda$ is randomly chosen as described in (\ref{j1}).

The subsequent plots and table contain our findings and the metric we use to test our methodology is

\[
MAE = \sum_{i=1}^{1000} \frac{\lvert y_i - \hat{y_i}\rvert }{1000},
\]
where $y_i $ is the true value and $\hat{y_i}$ is the estimated value for $\theta,\alpha_0,\alpha_1,\alpha_2$.

\begin{table}[h!]
	\centering
	\begin{tabular}{c | c | c | c}
	 \hline
	$\varepsilon $& Parameter & MAE \\ [0.5ex]
	\hline\hline
	$ 0.3$ & & & \\ [0.5ex]
	 \hline\hline
	  &$\theta$ & 0.0980807 \\
	&$ \alpha_0$ & 0.021481 \\
	&$\alpha_1$ & 0.055916\\
	&$\alpha_2$ &  0.058691\\ [1ex]
	\hline\hline
	$ 0.1$ & & & \\ [0.5ex]
	 \hline\hline
	  &$\theta$ & 0.0416584\\
	&$ \alpha_0$ &  0.009259 \\
	&$\alpha_1$ & 0.034566\\
	&$\alpha_2$ & 0.0368507 \\ [1ex]
	\hline\hline
	 $ 0.01$ & & & \\ [0.5ex]
	  \hline\hline
	   &$\theta$ & 0.021081\\
	&$ \alpha_0$ & 0.005788 \\
	 &$\alpha_1$ & 0.033258\\
	 &$\alpha_2$ &  0.0333328 \\ [1ex]
		\hline\hline
	 $ 0.001$ & & & \\ [0.5ex]
	  \hline\hline
	 & $\theta$ & 0.0205791\\
	&$ \alpha_0$ & 0.005601\\
	 &$\alpha_1$ & 0.03208\\
	 &$\alpha_2$ & 0.03183\\ [1ex]
	 \hline
	\end{tabular}
	\caption{Metrics of estimated parameters for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$.  }
	\label{table:1}
	\end{table}
	\newpage
\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{03_res_sing_200_plot.png}
	\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.3$. }
	\end{center}
	\end{figure}
\begin{figure}[H]\label{fig1}
		\begin{center}
		\includegraphics[width=.8\textwidth]{01_res_sing_200_plot.png}
		\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.1$. }
		\end{center}
\end{figure}

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{001_res_sing_200_plot.png}
	\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.01$. }
	\end{center}
\end{figure}

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{0001_res_sing_200_plot.png}
	\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.001$. }
	\end{center}
\end{figure}

\subsection{Forecasting}

We now take the results from the previous section to complete a prediction simulation study
of model (\ref{ex2}). We make use of a similar methodology and carry out our study as follows.
\begin{enumerate}
\item We generate a true parameter vector $(\theta, \alpha_0, \alpha_1, \alpha_2)$ and use this to
create synthetic data for training.	
\item We train the model using $100$ observations on the time interval $[0,1]$ by
using the LS-GD algorithm with $500$ test values of the period $\theta$.
\item Once training is complete, we simulate model (\ref{ex2}) on the time interval
$[0,3]$ with both the true parameter vector and the trained parameter vector. 	
\end{enumerate}
We generate the initial conditions as
$$
(X_0,Y_0,Z_0) \sim \bigg(\text{Uniform}(0.4,1),\,\frac{1-X_0}{n},\,\frac{(1-X_0)(n-1)}{n}\bigg),
$$
where $n \sim \text{Uniform}\{2,3,4,5,6,7\}$. The reason for this is the SIR model has the
requirement that
$$X_t + Y_t + Z_t =1,\ \ \ \  t\ge 0 .$$

We use a component-wise form
of mean-squared error as the metric of the trials. We run $1000$ prediction trials, and for each trial we have $100$ observations/samples. Hence for each trial $j$, $j \in
\{1,\ldots,1000\}$, we
compute the following sum.
$$
	T_j = \frac{1}{100}\sum\limits_{k=1}^{100}
\begin{bmatrix}
(X_{t_k} - \hat{X}_{t_k})^2 ,& (Y_{t_k} - \hat{Y}_{t_k})^2
,& (Z_{t_k} - \hat{Z}_{t_k})^2
\end{bmatrix}.
$$
Then, we compute
$
	 \frac{1}{1000}\sum_{j=1}^{1000}T_j$.
This metric gives us the ability to check each component error individually while providing insight into the error of the trials of the whole model.

\begin{rem}
	There is nothing particularly special about our choice to randomly generate $X_0$ then
	use its value to assign values to $Y_0$ and $Z_0$. Additionally, nothing special
	about the assignment of the factors $1/n$ and $(n-1)/n$ --
merely a convenience for computation.
\end{rem}


The following table includes the true values against the trained (estimated) values for the proportional model.
\begin{table}[h!]
	\centering
	\begin{tabular}{|c | c | c | c|}
	\hline
	$(\theta, \alpha_0, \alpha_1, \alpha_2)$ & $(0.26836304, 0.15114833,0.0621514,0.096762)$ \\
	\hline\hline
	$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.3 $ & $(0.27069755,0.1707693, 0.11016354, 0.082015210)$
	\\
	\hline\hline
	$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.1 $ & $(0.261749799, 0.1717002782, 0.025364225 ,  0.1017260206)$ \\
	\hline\hline
	$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.01 $ & $(0.2668796604,0.146115022,0.04899088,0.0966312804)$ \\
	\hline\hline
	$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.001 $ & $(0.26743667816,0.151314344,0.058359562,0.0989769703)$ \\	[.5ex]
	\hline
	\end{tabular}
	\caption{True paramter values versus trained parameter values for proportional model.}
	\label{table:1}
	\end{table}

The subsequent figures give simulation results using the true values and the trained values
for the parameters. The true parameter observations are marked by a $+$ while the trained parameter
observations are marked by a dot.

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_03_sl_s.png}
	\caption{Simulation of model (\ref{ex2}) for  $\varepsilon = 0.3$. }
	\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_01_sl_s.png}
	\caption{Simulation of model (\ref{ex2}) for  $\varepsilon = 0.1$. }	\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_001_sl_s.png}
	\caption{Simulation of model (\ref{ex2}) for  $\varepsilon = 0.01$. }	\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_0001_sl_s.png}
	\caption{Simulation of model (\ref{ex2}) for  $\varepsilon = 0.001$. }	\end{center}
\end{figure}	

\begin{table}[h!]
	\centering
	\begin{tabular}{|c | c | c | c|}
	\hline	
	$\varepsilon$ &  Component-wise MSE: $(\hat{X_t},\hat{Y_t},\hat{Z_t})$ \\
	\hline
	\hline	
	$0.3$ &   $(\begin{matrix} 0.000024732 &   0.00006754 & 0.00001291\end{matrix})$ \\
	\hline	
	\hline	
	$0.1$ &   $(\begin{matrix}0.000009734 & 0.00001537 & 0.000002157\end{matrix})$ \\
	\hline
	\hline	
	$0.01$ &   $(\begin{matrix} 0.000002564 &  0.000003539 & 0.0000006851\end{matrix})$ \\
	\hline
	\hline	
	$0.001$ &   $(\begin{matrix}  0.0000001432 &  0.00000003593 & 0.00000007774 \end{matrix})$ \\
	\hline
	\end{tabular}
	\caption{True parameter values versus trained parameter values for model (\ref{ex2}). }
	\label{table:1}
	\end{table}


\section{Simulation study of  SIR model for population numbers}\setcounter{equation}{0}

\subsection{Model for population numbers}

In this section, we consider the following model for population
numbers:
\begin{equation}\label{ex1}
\begin{bmatrix} dX_t \\ dY_t \\ dZ_t \end{bmatrix} = \begin{bmatrix} \Lambda - \mu X_t - \beta(t)X_tY_t \\ \beta(t)X_tY_t - (\mu + \gamma)Y_t \\ \gamma Y_t - \mu Z_t\end{bmatrix} dt\, +\,
	 \varepsilon\! \begin{bmatrix}\sigma X_{t-}Y_{t-}Z_{t-} & 0 & 0 \\ 0 &\sigma X_{t-}Y_{t-}Z_{t-} & 0 \\ 0 & 0 &\sigma X_{t-}Y_{t-}Z_{t-} \end{bmatrix} \begin{bmatrix} dL^{(1)}_t \\ dL^{(2)}_t \\ dL^{(3)}_t \end{bmatrix},\\
\end{equation}
where
$$
\begin{bmatrix} L^{(1)}_t \\ L^{(2)}_t \\ L^{(3)}_t \end{bmatrix} = \begin{bmatrix} B^{(1)}_t + \dint_0^t\dint_{\{|u|>  0.1\}}uN^{(1)}(ds,du)\\ B^{(2)}_t + \dint_0^t\dint_{\{|u|>  0.1\}} uN^{(2)}(ds,du)\\ B^{(3)}_t + \dint_0^t\dint_{\{|u|>  0.1\}} uN^{(3)}(ds,du) \end{bmatrix},
$$
$$\beta(t) = \alpha_0 + \alpha_1\cos\left(\frac{2\pi t}{\theta}\right)  + \alpha_2\sin\left(\frac{2\pi t}{\theta}\right) ,\ \ \ \ \theta \in [0,1],\alpha_0,\alpha_1,\alpha_2 > 0,$$
$\Lambda,\mu,\gamma, \sigma>0$ are constants, and $N = N_1 + N_2$ as given in (\ref{j1}).

By  \cite[Theorem 3.1]{eas},  the SDE (\ref{ex1}) has a unique strong solution
taking values in $\mathbb{R}^3_+$.
%Note that (\ref{ex1}) as stated is a special case of the model given by Long et al. in \cite{Long}.
 We utilize the  contrast function (\ref{March7a}), which is similar to that given in \cite{Long} and takes the explicit form:
$$
	\Psi_{\varepsilon,n}(\theta,\alpha_0,\alpha_1,\alpha_2) =100\varepsilon^{-2}
	\left(\sum_{k=1}^{100} P_{t_k}^*(\theta,\alpha_0,\alpha_1,\alpha_2)\Lambda^{-1}_{k-1}P_{t_k}(\theta,\alpha_0,\alpha_1,\alpha_2) \right),
$$
where
$$
P_{t_k}(\theta,\alpha_0,\alpha_1,\alpha_2) = \begin{bmatrix}X_{t_k}  - X_{t_{k-1}} - \frac{1}{100} \left[ \Lambda - \mu X_{t_{k-1}} - \beta({t_{k-1}} )X_{t_{k-1}}Y_{t_{k-1}} \right]\\ Y_{t_k}  - Y_{t_{k-1}} - \frac{1}{100}\left[ \beta({t_{k-1}} )X_{t_{k-1}}Y_{t_{k-1}} - (\mu + \gamma)Y_{t_{k-1}} \right] \\  Z_{t_k}  - Z_{t_{k-1}}  - \frac{1}{100}\left(\gamma Y_{t_{k-1}}  - \mu Z_{t_{k-1}}  \right) \end{bmatrix},
$$
and
$$
\Lambda_{k-1} = \begin{bmatrix}[\sigma X_{t_{k-1}} Y_{t_{k-1}} Z_{t_{k-1}} ]^2 & 0 & 0 \\ 0 &[\sigma X_{t_{k-1}} Y_{t_{k-1}} Z_{t_{k-1}}]^2  & 0 \\ 0 & 0 & [\sigma X_{t_{k-1}} Y_{t_{k-1}} Z_{t_{k-1}}]^2  \end{bmatrix}.
$$

\subsection{Parameter estimation}
We generate the synthetic data $10000$ times and randomly chose $1000$ such data sets to perform estimation upon.
Set $\Lambda = 0.018,\mu = 0.00042$, $\gamma = 0.07142$, $\sigma = 0.5$ and $(X_0,Y_0,Z_0) = (2.3,0.19,0.25)$ (taken to be in millions). Again, for this study we keep the jumps simple; namely, there is no compensated jump portion. We also make use of the same metric MAE to test our results and the
true values for the synthetic data are generated as given in (\ref{true}).



\begin{table}[h!]
\centering
\begin{tabular}{c | c | c | c}
 \hline
$\varepsilon $& Parameter & MAE \\ [0.5ex]
   \hline\hline
   $ 0.3$ & & & \\ [0.5ex]
   \hline\hline
  &$\theta$ & 0.0944492 \\
 &$ \alpha_0$ & 0.023988  \\
  &$\alpha_1$ &  0.0563110 \\
  &$\alpha_2$ & 0.058243  \\ [1ex]
   \hline\hline
  $ 0.1$ & & & \\ [0.5ex]
   \hline\hline
  & $\theta$ & 0.03130246 \\
 &$ \alpha_0$ &  0.00960035 \\
  &$\alpha_1$ & 0.0365961 \\
  &$\alpha_2$ & 0.0369104 \\ [1ex]
	\hline\hline
 $ 0.01$ & & & \\ [0.5ex]
  \hline\hline
   &$\theta$ & 0.01966768\\
&$ \alpha_0$ & 0.0057932 \\
 &$\alpha_1$ &0.0283396\\
 &$\alpha_2$ & 0.0291711 \\ [1ex]
    \hline\hline
 $ 0.001$ & & & \\ [0.5ex]
  \hline\hline
 & $\theta$ & 0.01657112 \\
&$ \alpha_0$ & 0.0056  \\
 &$\alpha_1$ & 0.02781 \\
 &$\alpha_2$ &0.0294392 \\ [1ex]
 \hline
\end{tabular}
\caption{Metrics of estimated parameters where $1000$ estimations are completed given $100$ observations and we used $200$ test values of $\hat{\theta}$.  }
\label{table:1}
\end{table}
\newpage

\begin{figure}[H]\label{fig1}
\begin{center}
\includegraphics[width=.8\textwidth]{03_res_plot_200.png}
\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.3$. }
\end{center}
\end{figure}

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{01_res_plot_200.png}
	\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.1$. }
	\end{center}
	\end{figure}

\begin{figure}[H]\label{fig1}
		\begin{center}
		\includegraphics[width=.8\textwidth]{001_res_plot_200.png}
		\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.01$. }
		\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
\begin{center}
\includegraphics[width=.8\textwidth]{0001_res_plot_200.png}
\caption{Estimates against true values  for $1000$ estimations of $100$ observations using $200$ values of $\hat{\theta}$ while  $\varepsilon = 0.001$. }
\end{center}
\end{figure}

\subsection{Forecasting}
In a similar fashion to the previous section, we perform a predictive simulation study. The primary difference being the initial conditions
are randomized such that $$(X_0, Y_0, Z_0) \sim
(\text{Uniform}(1,4),\, \text{Uniform}(0.1,1),\, \text{Uniform}(0.1,1)).$$
Otherwise, the methodology is the same as before including using the component-wise MSE. The following table includes the randomly-generated
true parameter values against those we obtained in the estimation. The subsequent figures below display the simulation of model (\ref{ex1}) using the true parameters and the
trained parameters values. The true parameter observations are marked by a $+$ while the trained parameter
observations are marked by a dot.

\begin{table}[h!]
\centering
\begin{tabular}{|c | c | c | c|}
\hline
$(\theta, \alpha_0, \alpha_1, \alpha_2)$ & $(0.26836304, 0.15114833,0.0621514,0.096762)$ \\
\hline\hline
$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.3 $ & $(0.2759104,0.15106501,0.09268711, 0.0418802)$
\\
\hline\hline
$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.1 $ & $(0.268413479, 0.14653753, 0.058398519, 0.116306536)$ \\
\hline\hline
$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.01 $ & $(0.26945644,0.1511415,0.0667543397,0.09347593)$ \\
\hline\hline
$(\hat{\theta}, \hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2)$, $\varepsilon = 0.001 $ & $(0.26857489,0.151209307,0.06245031,0.0967899)$ \\	[.5ex]
\hline
\end{tabular}
\caption{True parameter values versus trained parameter values for model (\ref{ex1}).  }
\label{table:1}
\end{table}



\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_03_sl.png}
	\caption{Simulation of model (\ref{ex1}) for  $\varepsilon = 0.3$. }
	\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_01_sl.png}
	\caption{Simulation of model (\ref{ex1}) for  $\varepsilon = 0.1$. }
	\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_001_sl.png}
	\caption{Simulation of model (\ref{ex1}) for  $\varepsilon = 0.01$. }
	\end{center}
\end{figure}	

\begin{figure}[H]\label{fig1}
	\begin{center}
	\includegraphics[width=.8\textwidth]{new_0001_sl.png}
	\caption{Simulation of model (\ref{ex1}) for  $\varepsilon = 0.001$. }
	\end{center}
\end{figure}	




\begin{table}[h!]
	\centering
	\begin{tabular}{|c | c | c | c|}
	\hline	
	$\varepsilon$ &  Component-wise MSE: $(\hat{X_t},\hat{Y_t},\hat{Z_t})$\\
	\hline
	\hline	
	$0.3$ &   $(\begin{matrix} 0.119862755 &  0.4229034 & 0.1641841226\end{matrix})$ \\
	\hline	
	\hline	
	$0.1$ &   $(\begin{matrix}0.01354954 & 0.0286335 & 0.01803421\end{matrix})$ \\
	\hline
	\hline	
	$0.01$ &   $(\begin{matrix}0.00032701 &  0.0003369 &   0.00015043 \end{matrix})$ \\
	\hline
	\hline	
	$0.001$ &   $(\begin{matrix} 0.000221016 & 0.000165453 & 0.0000109 \end{matrix})$ \\
	\hline
	\end{tabular}
	\caption{True parameter values versus trained parameter values for model (\ref{ex1}) .  }
	\label{table:1}
	\end{table}


\begin{rem}
		Mean-squared error is scale-dependent; hence, comparing the metrics we have
		for the two model versions is of no valuable use. The former model only takes values in the interval $[0,1]$
		whereas the latter model can take values larger than $1$.
\end{rem}	


\section{Conclusion}\setcounter{equation}{0}

The novelty of this paper is:
\begin{itemize}
\item Estimating the unknown period of a time-dependent periodic transmission function as well as its unknown coefficients.
\item The introduction and use of a Linear-Search Gradient-Descent algorithm to iteratively solve for suitable approximations to the LSEs.
\item Parameter estimation results for when the noise coefficient (diffusion matrix) is a non-square or non-invertible matrix.
\item Extending asymptotic results for consistency and limiting distributions given by Long et al. in \cite{Long} to include SDEs driven by general L\'{e}vy noises with time-dependent coefficients.
\end{itemize}

Our aim was to study the ability of estimating parameters governing a periodic transmission for a stochastic SIR model.
The model comes in one of two forms, population proportions or population numbers, both of which we have given simulation studies
on which we are able to effectively estimate the period of a periodic transmission function.

The theoretical results from Section 2 give reasoning that these estimations efforts hold in general. Moreover, that the asymptotics of the estimators can be ascertained -- namely, consistency and
limiting distributions. Additionally, these results aid in the generalization of existing efforts in the literature on parameter estimation
for SDEs driven by L\'{e}vy noises with small coefficient $\varepsilon$.

We introduced the LS-GD algorithm; which was a useful tool in our simulation studies since calculating the LSEs by use of algebraic manipulation
may not be a feasible task.
Moreover, it provided a way to estimate parameters when there was some multiplicative interaction between them (e.g., recall how $\beta(t)$ was defined).

The work here lays a foundation for further work on parameter estimation of the USSIR model. In future work, we are interested in applying these results
to real world data (e.g., COVID-19).
\section*{Acknowledgements}
This work was partially supported by the Natural Sciences and Engineering Research Council of
Canada (No. 4394-2018). We wish to thank Calcul Qu\'{e}bec a regional partner of Digital Research Alliance of Canada for providing
high-performance computing
resources to accomplish the work presented here.






\begin{thebibliography}{10}



\bibitem{Alen}
M. Alenezi, F. Al-Anzi and H. Alabdulrazzaq. Building a sensible SIR estimation model for COVID-19 outspread in Kuwait. Alexandria Eng. J. 60, 3161-3175 (2021).

\bibitem{Bao}  J. Bao and C. Yuan.  Stochastic population dynamics driven by L\'evy noise. J. Math. Anal. Appl. 391, 363-375 (2012).

\bibitem{beck}
A. Beck. First-Order Methods in Optimization. Siam (2017).
	
\bibitem{Bodhi}
A. Bodini, S. Pasquali, A. Pievatolo and F. Ruggeri.  Underdetection in a stochastic SIR model for the analysis of the COVID-19 Italian epidemic. Stoch. Environ. Res. Risk Ass. 36, 137-155 (2022).

\bibitem{Chen1} G. Chen, T. Li and C. Liu. Lyapunov exponent of a stochastic SIRS model. Comp. Rend. Math. 351, 33-35 (2013).

\bibitem{Chen}
X. Chen, J. Li, C. Xiao and P. Yang.  Numerical solution and parameter estimation for uncertain SIR model with application to COVID-19. Fuzzy Optim. Decis. Mak. 20, 189-208 (2021).
	
\bibitem{Contact}
 S. Cl\'{e}men\c con , V.C. Tran and H. de Arazoza.  A stochastic SIR model with contact-tracing: large population limits and statistical inference. J. Biol. Dyn. 2, 392-414 (2008).

	
\bibitem{DJ}
	S.M.  Djouadi, V. Maroulas, X. Pan and J. Xiong. On least-squares estimation for partially observed jump-diffusion processes. ACC (2016).

\bibitem{eas}
T. Easlick and  W. Sun. A unified stochastic SIR model driven by L\'evy noise with time-dependency. arXiv:2201.03406v2 (2022).

\bibitem{Sim}
Z. El Kharrazi and S. Saoud.  Simulation of COVID-19 epidemic spread using stochastic differential equations with jump diffusion for SIR model. ICOA (2021).

\bibitem{EL}
	A. El Koufi, J. Adnani, A. Bennar and N. Yousfi. Analysis of a stochastic SIR model with vaccination and nonlinear incidence rate. Int. J. Diff. Equ. 9275051 (2019).

\bibitem{EL2}
A. El Koufi, J. Adnani, A. Bennar and N. Yousfi. Dynamics of a stochastic SIR epidemic model driven by L\'evy jumps with saturated incidence rate and saturated treatment function. Stoch. Anal. Appl. (2021).

\bibitem{Feng}
Z. Feng, D. Xu and H. Zhao. Epidemiological models with non-exponentially distributed disease stages and applications to disease control. Bull. Math. Biol. 69, 1511-1536 (2007).
	
\bibitem{Gir}
P. Girardi and C. Gaetan.  An SEIR model with time-varying coefficients for analyzing the SARS-CoV-2 epidemic. Risk Anal. (2021).

\bibitem{gl}
A. Gloter and M. S{\o}rensen. Estimation for stochastic differential equations with a small diffusion coefficient. Stoch.
Proc. Appl. 119, 679-699 (2009)

 \bibitem{G} A. Gray, D. Greenhalgh, L. Hu, X. Mao and J. Pan. A stochastic differential equation SIS epidemic model. SIAM J. Appl. Math. 71, 876-902 (2011).

\bibitem{Green}
 S. Greenhalgh and T. Day. Time-varying and state-dependent recovery rates in epidemiological models. Infect. Dis. Mod. 2, 419-430 (2017).

\bibitem{GS} X.X. Guo and W. Sun. Periodic solutions of stochastic differential equations driven by L\'evy
 noises. J. Nonl. Sci. 31:32  (2021).
	
\bibitem{hager}
W. Hager and H. Zhang. Algorithm 851: CG-DESCENT, a conjugate gradient method with guaranteed descent. ACM Trans. Math. Soft. 32, 113-137 (2006).

\bibitem{Jagan}
M. Jagan, M. deJonge, O. Krylova and D. Earn.  Fast estimation of time-varying infectious disease transmission rates. PLOS Comp/ Biol. (2020).

 \bibitem{JJS} C. Ji, D. Jiang and N. Shi. Multigroup SIR epidemic model with stochastic permutation. Physica A. 390, 1747-1762 (2011).

\bibitem{KA}
	R.A. Kasonga. The consistency of a non-linear least squares estimator from diffusion processes. Stoch. Proc. Appl. 30, 263-275 (1988)
	
\bibitem{ker}
W. Kermack and A. McKendrick. Contributions to the mathematical theory of epidemics.
Proc. R. Soc. Lond. Ser. A. 115, 700-721 (1927).

\bibitem{KO}
	M. Kobayashi and Y. Shimizu. Least squares estimators based on the Adams method for stochastic differential equations with small L\'{e}vy noise. 	arXiv:2201.06787 (2022).

\bibitem{Kroger}
M. Kr\"{o}ger and R. Schlickeiser. SIR-solution for slowly time-dependent ratio between recovery and infection rates. Physics (MDPI) 4 (2022).

\bibitem{Li}
C. Li, Y. Pei, M. Zhu and Y. Deng. Parameter estimation on a stochastic SIR
model with media coverage. Disc. Dyn. Nature Soc. 3187807 (2018).

\bibitem{XLiu}
X. Liu and P. Stechlinski. Infectious disease models with time-varying parameters and general nonlinear incidence rate. Appl. Math. Model. 1974-1994, (2012).

\bibitem {Liu} Y. Liu, Y. Zhang and Q-Y. Wang.  A stochastic SIR epidemic model with L\'evy jump and media coverage. Adv. Diff. Equ. 70  (2020).

\bibitem{Pri} N. Privault and L. Wang. Stochastic SIR L\'evy jump model with heavy-tailed increments. J. Nonlin Sci. 31:15 (2021).

\bibitem{Sun}
H. Long, Y. Shimizu and W. Sun. Least squares estimators for discretely observed stochastic processes
	driven by small L\'evy noises. J. Mul.. Anal. 116, 422-439 (2012).

\bibitem{Long}
	H. Long, C. Ma and Y. Shimizu. Least squares estimators for stochastic differential equations driven by small L\'{e}vy noises. Stoch. Proc. Appl. 127, 1475-1495 (2016)
	
 	
\bibitem{optim}
P.K. Mogensen and A.N. Riseth. Optim: A mathematical optimization package for Julia. J. Open Sour. Soft. 3, 615 (2018).

\bibitem{Mum1}
A. Mummert. Studying the recovery procedure for the time-dependent transmission rate(s) in epidemic models. J. Math. Biol. 67, 483-507 (2013).	

\bibitem{Mum}
A. Mummert and O. Otunuga. Parameter identification for a stochastic SEIRS epidemic model: case studyiInfluenza. J. Math. Biol. 79, 705-729 (2019).

	
\bibitem{Pan}
J. Pan, A. Gray, D. Greenhalgh and X. Mao. Parameter estimation for the stochastic SIS epidemic model. Stat. Inf. Stoch. Proc. 17, 75-98 (2014).

 \bibitem{Paul}
 S. Paul and E. Lorin.  Estimation of COVID-19 recovery and decease periods in Canada using delay model. Sci. Rep. 11, 23763 (2021).

\bibitem{DiffEq}
C. Rackauckas and Q. Nie. DifferentialEquations.jl -- a performant and feature-rich ecosystem for solving differential equations in julia. J. Open Res. Soft. 5, 15 (2017).
	
\bibitem{Shi}
Y. Shimizu. Quadratic type contrast functions for discretely observed non-ergodic diffusion processes. Osaka Univ., Math. Sc. Res. Rep. No. 09-04 (2010).


\bibitem{ms}
M. S{\o}rensen and M. Uchida. Small diffusion asymptotics for discretely sampled stochastic differential equations.
Bernoulli. 9, 1051-1069  (2003)

\bibitem{T} E. Tornatore, S. Buccellato and P. Vetro. Stability of a stochastic SIR system. Physica A. 354, 111-126 (2005).

\bibitem{Uch}
 M. Uchida. Estimation for discretely observed small diffusions based on approximate martingale estimating
functions. Scand. J. Stat. 31, 553-566 (2004)

\bibitem{van} A.W. van der Vaart. Asymptotic Statistics. Cambridge University Press (1998).

\bibitem{Wacker}
B. Wacker and J. Schluter. Time-continuous and time-discrete SIR models revisited: theory and applications. Adv. Differ. Eq. 556 (2020).

\bibitem{Xu}
	P. Xu and S. Shimada. Least squares parameter estimation in multiplicative noise model. Comm. Stat. Sim. Comp. 29, 83-96 (2007).

\bibitem{Zhang0} X. Zhang and K. Wang. Stochastic SIR model with jumps. Appl. Math. Lett. 26, 867-874 (2013).

\bibitem{Zhang}
X. Zhang, Z. Zhang, J. Tong and M. Dong. Ergodicity of stochastic smoking model and parameter estimation. Adv. Diff. Equ. 274 (2016).

\bibitem{Zhou1} Y. Zhou, S. Yuan and D. Zhao. Threshold behavior of a stochastic SIS model with L\'{e}vy jumps. Appl. Math. Comp. 275, 255-267 (2016).

\bibitem{Zhou2} Y. Zhou and W. Zhang. Threshold of a stochastic SIR epidemic model with L\'{e}vy jumps. Physica A. 446, 204-216 (2016).

\end{thebibliography}





\end{document}




