% !TEX root =  main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 


\section{Expert Study via Multi-Dimensional Rubric
}\label{sec:surveystudy}


\looseness-1In this section, we evaluate \algmultihop~w.r.t. the desired features specified in the objective, i.e., \ind, \interp, and \conceal~(see Section~\ref{sec:notation.setup}). In particular, we seek to compare \algmultihop~with its variants resulting from different design choices in Section~\ref{sec:model}. To this end, we conduct an expert study via a multi-dimensional rubric.


% \vspace{1mm}
\looseness-1\textbf{Variants of \algmultihop~algorithm.} We compare the performance of \algmultihop{} with the following variants: \algsame, \alghintpolicystruct, and \algmincode{}. \algsame~and \alghintpolicystruct~differ from \algmultihop~only in the \hintpolicy{} routine used in Stage 1  of Fig.~\ref{fig:pipeline.abstract} when generating $S^\text{quiz}$. In particular, Stage 1 of \algsame~always returns the sketch of the solution code, i.e., $S^\text{quiz} := S^{\text{in,}\star}$; Stage 1 of \alghintpolicystruct~returns a sketch directly from the $1$-hop neighborhood of $S^\text{in,stu}$, i.e., $S^\text{quiz} \in \mathcal{N}_\sketchspace(S^\text{in,stu},1)$. The third baseline, \algmincode, differs from \algmultihop~only in Stage 2(i) of Fig.~\ref{fig:pipeline.abstract} when generating $C^\text{quiz}$ from $S^\text{quiz}$. In particular, Stage 2(i) of \algmincode~generates $C^\text{quiz}$ as a direct reduction of the solution code w.r.t. the sketch obtained in Stage 1, i.e., $C^\text{quiz} \in \reducedcodes(\solutioncode~|~S^\text{quiz})$.


% \vspace{1mm}
\looseness-1\textbf{Simulated student attempts.} For this expert evaluation, we simulated unsuccessful student attempts as seen in block-based programming domains~\cite{DBLP:conf/lats/PiechSHG15}. In particular, for each reference task, we manually created four student attempts as follows: (a) Stu-A: $C^\text{in,stu}$ uses only action blocks, i.e., (\DSLMove, \DSLTurnLeft, \DSLTurnRight, \DSLPickMarker, \DSLPutMarker); (b) Stu-B: $C^\text{in,stu}$ uses a subset of programming constructs in \solutioncode; (c) Stu-C: $C^\text{in,stu}$ is structurally the same as \solutioncode, i.e., $S^\text{in,stu}= S^{\text{in,}\star}$; (d) Stu-D: $C^\text{in,stu}$ has a structure more complex than \solutioncode. These four types of attempts exhaustively cover all the scenarios that an algorithm might encounter when deployed (see Section~\ref{sec:userstudy}).


%%%%%%%%%% figure for Karel-task-6 here
\input{./fig/fig-karel-example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \vspace{1mm}
\looseness-1\textbf{Multi-dimensional evaluation rubric.} Inspired by the evaluation rubric in \cite{DBLP:conf/aied/PriceZB17,DBLP:conf/edm/ZhiMDLPB19},  we assess pop quizzes on a multi-dimensional rubric with three attributes, each rated on a three-point Likert scale (with higher scores being better). More concretely, we have: (i) \ind~attribute measuring the degree of individualization of the pop quiz to the current student attempt ($3$: high; $2$: medium; $1$: low); (ii) \interp~attribute measuring how easy the pop quiz is to comprehend/solve ($3$: easy; $2$: might confuse the student sometimes; $1$: either incorrect or is very difficult to solve.); (iii) \conceal~attribute measuring the extent to which the pop quiz conceals the solution code ($3$: sufficiently conceals; $2$: reveals the solution to some extent; $1$: reveals the solution to a large extent). \overall{} denotes the sum of scores across three attributes for a pop quiz.


% \vspace{1mm}
\looseness-1\textbf{Expert study setup.} We picked three tasks spanning different types of constructs and complexity: T-1, T-4, and T-5 from Fig.~\ref{fig:experiments.analysis}. Thus, in total we evaluated $48$ scenarios: $4$ algorithm variants $\times$ $4$ student types $\times$ $3$ tasks (see Figs.~\ref{fig:intro} and~\ref{fig:karel.illustration} as example scenarios). Two researchers, with experience in block-based programming, evaluated each of the $48$ scenarios independently.
%
The evaluation was done through a web survey where a scenario was introduced at random, and assessed based on the rubric.



%%%%%%%%%%% input results table
\input{./fig/results_table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \vspace{1mm}
\looseness-1\textbf{Expert study results.} First, we validate the expert ratings using the quadratic-weighted Cohen's kappa inter-agreement reliability value~\cite{DBLP:conf/aied/PriceZB17} for each attribute: $0.62$ (\ind), $0.69$ (\interp), $0.79$ (\conceal), and $0.7$ (\overall). The values indicate \textit{substantial agreement} between the raters. The average ratings are presented in Fig.~\ref{fig:surveystudy} and \algmultihop~has the highest \overall~score. We analyze these ratings per attribute based on the Kruskal-Wallis significance test~\cite{macfarland2016kruskal}; the results discussed next are statistically significant with $p < 0.01$. On the~\ind~attribute, \algsame{} performs significantly worse because it does not account for the student attempt (see Section~\ref{sec:model.stage1}). On the ~\interp~attribute, \alghintpolicystruct~performs significantly worse because there are instances where no valid code reduction of \solutioncode~w.r.t. $S^\text{quiz}$ is found (see Footnote~\ref{footnote:sec3}, Section~\ref{sec:model.stage2}). 
%
Finally, on the \conceal~attribute, \algmincode~performs significantly worse because it obtains $C^\text{quiz}$ via a direct reduction of \solutioncode{} without any mutation (see Section~\ref{sec:model.stage2}). 
