% !TEX root =  main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
\setcounter{footnote}{0}
\looseness-1The emergence of block-based visual programming platforms has made coding more interactive and appealing for novice students. Block-based programming uses ``code blocks'' that reduce the burden of syntax and focuses on key programming concepts. Led by the success of languages like \emph{Scratch}~\cite{DBLP:journals/cacm/ResnickMMREBMRSSK09}, initiatives like \emph{Hour of Code} by Code.org~\cite{hourofcode}, and online courses like \emph{Intro to Programming with Karel} by CodeHS.com \cite{intro_to_karel_codehs,pattis1981karel}, block-based programming has become integral to introductory CS education. 
% Considering \emph{Hour of Code} alone, over $1$ billion hours of coding activity has been performed so far by over $50$ million unique students worldwide~\cite{codeorg}.


%%%%%%%%%%%%%%%%%%%%%%%%% Overview Figure
\input{./fig/fig-overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\looseness-1Programming tasks on these platforms are conceptual and open-ended, requiring multi-step deductive reasoning to solve, thereby making them challenging for students. To effectively support a struggling student to solve a particular task, it is important to provide feedback on their attempts. However, on platforms that have millions of students, it is infeasible for human tutors to provide feedback. Hence, there is a critical need for automated feedback generation systems to provide personalized support to  students~\cite{adaptivefeedbackmarwan20,DBLP:journals/corr/abs-2102-05741}. Existing work in the domain has explored various methods of personalized feedback generation within a task, such as providing next-step hints in the form of next code blocks to use in a student attempt~\cite{DBLP:conf/its/RiversK14,DBLP:conf/lats/PiechSHG15,DBLP:journals/aiedu/RiversK17,DBLP:conf/kbse/ZimmermanR15,DBLP:conf/edm/PriceZB17}, providing adaptive worked examples~\cite{DBLP:conf/aied/PriceZB17,DBLP:conf/edm/ZhiMDLPB19,DBLP:conf/sigcse/PriceDL17}, and providing data-driven analysis of a student's misconceptions~\cite{DBLP:conf/sigcse/CherenkovaZP14,DBLP:conf/iwpc/WieseRKA19,DBLP:conf/icse/WieseRF19,DBLP:conf/aaai/WuMGP19,DBLP:journals/corr/abs-1708-06564,DBLP:conf/edm/EfremovGS20,DBLP:conf/iclr/BunelHDSK18}. 

\looseness-1In this paper, we investigate an alternate method of personalized feedback generation that guides a student towards a task's solution while involving inquiry-driven and problem-solving aspects~\cite{DBLP:conf/sigcse/CordovaCGW21}. In particular, we introduce a scaffolding framework based on pop quizzes that contain new programming tasks presented as multi-choice questions.\footnote{We refer to these multi-choice questions as ``pop quizzes'' as the framework could present these quizzes whenever a student needs help~\cite{DBLP:conf/sigcse/Cicirello09}.}
%\footnote{We refer to these multi-choice questions as ``pop quizzes'' as the framework could be designed to present these quizzes to a student unannounced~\cite{DBLP:conf/sigcse/Cicirello09}.} 
Our framework is inspired by prior studies that showed the efficacy of multi-choice questions in helping novice students learn to code \cite{DBLP:conf/chi/PriceWSM20,DBLP:journals/ijet/ZhangLZH20,DBLP:conf/sigcse/Grover21,DBLP:conf/iticse/SoltanpoorTD18,DBLP:conf/ecai2/EneS19}. The framework is designed to be invoked as follows: Given a task and a student's current unsuccessful attempt, the framework can help the student by presenting a pop quiz intended to resolve their misconception. For the scaffolding to be effective, we center the design of the new programming task for a pop quiz around three features: \ind, \interp, and \conceal; see details in Fig.~\ref{fig:intro} and Section~\ref{sec:notation.setup}.
% (a) \ind~(i.e., individualized to the student's current attempt), (b) \interp~(i.e., easy to comprehend and solve), and (c) \conceal~(i.e., do not reveal the solution code); see details in Fig.~\ref{fig:intro} and Section~\ref{sec:notation.setup}. 
However, hand-crafting these new quizzes is time-consuming and potentially error-prone when required for a large number of tasks and different student attempts.
To this end, we seek to \emph{automatically} generate these pop quizzes by synthesizing new programming tasks. 


% \vspace{-1mm}
\subsection{Key Challenges and Our Contributions}\label{sec:intro.contributions}
% \vspace{-1mm}

\looseness-1There are several challenges in synthesizing new visual programming tasks with the above mentioned features, including the following:
(i) current techniques for synthesizing visual programming tasks do not adapt to student attempts~\cite{DBLP:conf/nips/AhmedCEFGRS20}; (ii) the mapping from the space of visual tasks to their solution codes is highly discontinuous as shown in \cite{DBLP:conf/nips/AhmedCEFGRS20}, and hence task mutation based techniques are ineffective~\cite{DBLP:conf/aaai/SinghGR12,DBLP:conf/ijcai/PolozovOSZGP15}; (iii) the space of possible tasks and their solutions is potentially unbounded, and hence techniques that rely on exhaustive enumeration are intractable~\cite{DBLP:conf/aaai/SinghGR12,DBLP:conf/ijcai/AhmedGK13,DBLP:conf/aaai/AlvinGMM14}.

\looseness-1In this work, we develop a novel algorithm, \algmultihop, that synthesizes pop quizzes with the desirable features of our scaffolding framework.
Our algorithm overcomes the above-mentioned challenges by using techniques of symbolic execution, search algorithms, and graph-based code representations. Our key contributions are: \textbf{(I)} We present a modular and extensible algorithm for generating pop quizzes that operates in three stages (see Sections~\ref{sec:notation}~and~\ref{sec:model});\footnote{Implementation of the algorithm is publicly available at\\ \text{\quad \ \ \  }\url{https://github.com/machine-teaching-group/aied2022\_pquizsyn\_code}} \textbf{(II)} We show that our approach can generate hundreds of pop quizzes for different types of student attempts on reference tasks from real-world programming platforms (see Section \ref{sec:evaluation}); \textbf{(III)} We assess the quality of our algorithm through expert ratings using a multi-dimensional evaluation rubric (see Section~\ref{sec:surveystudy}); \textbf{(IV)} We have built an online platform with our framework and demonstrate the utility of pop quiz based feedback through an initial user study (see Section~\ref{sec:userstudy}).\footnote{Online platform is publicly available at \url{https://www.teaching-blocks-hints.cc/}\label{footnote:webapp}}
%
%Link to our implementation of \algmultihop{}
%Github: 

% \vspace{-1mm}
\subsection{Additional Related Work}
% \vspace{-1mm}
\looseness-1\textbf{Feedback via modelling programming concepts.} Apart from the above-mentioned methods such as next-step hints, there has been extensive work on feedback generation via modelling programming concepts. Here, several techniques have been proposed, including: (a) detecting challenging concepts by analyzing student attempts~\cite{DBLP:conf/sigcse/CherenkovaZP14,DBLP:conf/iwpc/WieseRKA19,DBLP:conf/icse/WieseRF19}; (b) discovering student misconceptions using task-specific rubrics and neural program embeddings~\cite{DBLP:conf/aaai/WuMGP19}; (c) defining concepts through knowledge components~\cite{DBLP:conf/icer/RiversHK16,DBLP:conf/sigcse/CrichtonSH21,DBLP:conf/edm/AkramAMWMNBL20}.
%

% \vspace{1mm}
\looseness-1\textbf{Evaluation of feedback methods.} An important aspect to consider when developing feedback generation methods is their evaluation criteria. Most next-step feedback generation methods are evaluated based on expert annotations or automated procedures ~\cite{DBLP:conf/lats/PiechSHG15,DBLP:journals/corr/abs-1708-06564,DBLP:journals/aiedu/PriceDZPLCB19}. In contrast, example-driven feedback techniques are typically evaluated using a multi-dimensional rubric~\cite{DBLP:conf/aied/PriceZB17,DBLP:conf/edm/ZhiMDLPB19}. In our work, we evaluate the scaffolding framework through expert ratings using a rubric, as well as an initial user study. 
%