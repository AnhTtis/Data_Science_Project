%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% GENERAL DISCUSSION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results and Discussion}
\label{sec:results} 

\begin{table*}
%\small
\centering
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
Generation & & & & & & & \\ Method & R-1 & R-2 & R-L & FA\%  & FM\% & FO\% & MRR \\
% \hline
% Target Queries&1&1&1\\
\hline
Verbatim Claim & 0.343 & 0.294 & 0.314 & 15.1 & 17.2 & 18.2 & 0.114 \\
Noun Phrases & 0.455 & 0.368 & 0.396 & 19.7 & 20.5 & 22 & 0.145 \\
Named Entities & \textit{0.577} & \textit{0.423} & \textit{0.484} & \textit{30.0} & \textit{32.3} & \textit{33.8} & \textit{0.217} \\
\hline
zero-shot &	0.533 &	0.388 &	0.438 & 25.1 & 26.2 &	29 &	0.181 \\
one-shot &	0.546 &	0.339 &	0.418 & 19.0 & 20.5 &	23.1 &	0.15 \\
two-shot &	0.582 &	0.382 &	0.46 & 25.6 & 26.4 &	28.2 &	0.191 \\
three-shot &	0.593 &	0.402 &	0.474 & 30.3 & 31.5 &	32.8 &	0.225 \\
fine-tuned &\textbf{0.646} & \textbf{0.472} & \textbf{0.56} & \textbf{41.0} & \textbf{41.8} & \textbf{42.6} & \textbf{0.321} \\
\hline
\end{tabular}
\caption{Performance metrics for each query generation method and rule-based baselines. For each generation method, the best-performing template is shown.} 
\label{tab:ev-coll-overall-best}
\end{table*}

Table~\ref{tab:ev-coll-overall-best} shows the results for the rule-based baselines, as well as the top performing prompt for the zero-shot, few-shot and fine-tuning methods, measured by the FM\%. 
The effect of prompt selection on results is discussed in Appendix~\ref{sec:appendix}.  An analysis of the results shows that FA\%, FM\% and FO\% are not very far apart. On average, the difference between the three metrics is ~4\% across all experiments. In other words, if the target result was found at least once in the results, it's very likely to come up most of the time. However, the fact that the metrics diverge confirms that web search results are non-deterministic. They can also change in a short time span: all Bing searches for each query were done immediately one after another, yet still frequently produced different results. 

We also observed that web pages seem to drop out of results quite quickly. Recall that at the time of dataset creation, the target queries were validated to confirm they returned the target URLs. Only those queries that returned the target URL the majority of the time were included in the dataset. Later, during our experiments, we tested using the target queries for collection. As shown in Table~\ref{tab:target-queries-results-websearch}, only 93.6\% of the target queries returned the target URL in the results, while 95.1\% returned it at least once. This means in the time between dataset creation and running these experiments (approximately two months), the target URL dropped completely from the results for almost 5\% of the samples. This constitutes an upper bound on how well any system can perform.

\begin{table}
%\small
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
Generation & & & & \\
Method & FA\% & FM\% & FO\% & MRR \\
\hline
Target Queries&92.1&93.6&95.1&0.697\\
\hline
\end{tabular}
\caption{Web search evaluation results using target queries.} 
\label{tab:target-queries-results-websearch}
\end{table}

Since we evaluate the generated queries in relation to textual similarity with the target queries, as well as the quality of the search results, we assess the correlation between these two evaluation methods. The results are shown in Table~\ref{tab:metrics-correlation} and confirm that similarity to the target query is a very good indicator of the potential quality of search results. The Rouge-2 metric has a very high correlation with the FA\%, FO\% and FM\% metrics, while Rouge-L shows the highest correlation with the MRR score. 

\begin{table}
%\small
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
& FA\% & FM\% & FO\% & MRR \\
\hline
Rouge-1  & 0.842 & 0.831 & 0.817 & 0.848 \\
Rouge-2  & 0.976 & \textbf{0.98} & \textbf{0.984} & 0.972 \\
Rouge-L  & \textbf{0.977} & 0.972 & 0.966 & \textbf{0.979} \\
\hline
\end{tabular}
\caption{Correlation between similarity-based and search-based metrics.} 
\label{tab:metrics-correlation}
\end{table}

However, a low similarity score does not necessarily mean that the query will not return the target URL. While analysing the results, we found many cases where queries with low Rouge scores still produced the desired search results. In the majority of these cases, the target URL was a Wikipedia page, which meant that the inclusion of a single named entity in the search query was likely enough for the search engine to produce the related page. Conversely, we also found generated queries with very high similarity to the target queries (based on the Rouge-2 metric), that nevertheless failed to return the target URL.% These are harder to characterise. 

Returning to the results in  Table~\ref{tab:ev-coll-overall-best}, we note that the simple named entities baseline approximates the target queries most closely of our rule-based baselines. This suggests that a high proportion of human-created queries include named entities. 

Of the in-context generation methods, three-shot learning achieves competitive performance with the NE baseline, but it fails to surpass it. None of the other in-context methods come close to the baseline performance. Meanwhile, fine-tuning outperforms the named entity baseline. This suggests that while named entities are an important element of the target queries, there are other factors that go into making an effective query. For example, these could be selecting the correct named entity when several are present in the text, and including other words which are not named entities. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% COMBINING METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To investigate the differences in performance between the various generation methods, we consider the specific samples that each of the top-two methods are able to solve correctly. Table~\ref{tab:ner-ft-twoway} shows the overlap between the correct samples for the NE baseline and the fine-tuning method. While the NE baseline is outperformed by the fine-tuning method, there is a significant proportion of samples (30 out of a possible 227) that NE gets correct and fine-tuning does not. This points to the fact that different methods have different strengths, and the optimal way to employ them would be to combine those strengths. 

\begin{table}
% \small
\centering
\begin{tabular}{|l|c|c|c|}
\hline
& NE Matches & NE Fails & Total \\
\hline
Fine-tuning	& & & \\
Matches & 96  &  67  & 163 \\
\hline
Fine-tuning	& & & \\
Fails	& 30  & 197  & 227  \\
\hline
Total &	126	 & 264  & 390 \\
\hline
\end{tabular}
\caption{Overlap of samples for which the NE baseline and the fine-tuning generation method produce the target URL in the search results.} 
\label{tab:ner-ft-twoway}
\end{table}

\begin{table*}
%\small
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
Generation Method & FA\% &  FM\% & FO\% & MRR \\
\hline
NE + Fine-tuning & 45.1 & 46.9 & 47.9 & 0.332 \\% The two best methods
NE + Three-shot + Fine-tuning & 44.9 & 46.7 & 47.9 & 0.345 \\ % The three best methods
All Rule-based (NE + NP + Verbatim) & 35.4 & 37.9 & 41.3 & 0.227 \\ % All rule-based approaches
All Rule-based + Fine-tuning & 47.9 & 49.2 & 50.5 & 0.331 \\ % All rule-based approaches + finetuning
All Rule-based + Three-shot + Fine-tuning & 47.4 & 49.7 & 51.3 & 0.356 \\ % All rule-based approaches + best in-context + finetuning
\hline
\end{tabular}
\caption{Web search evaluation results combining different methods using the Borda count.} 
\label{tab:combined-results-websearch-borda}
\end{table*}


In order to explore the potential for an ensembled system further, we use a modified Borda count method to combine the rankings of each of the query generation methods, while prioritising the most individually effective. We produce a combined top 10 ranking of search results, and score this using the same metrics as for individual methods. We use this approach to combine the top-two performing query generation methods (NE and Fine-tuning) and the top-three methods (NE, Fine-tuning and Three-shot). We also experiment with combining all rule-based baseline methods. As can be seen in Table~\ref{tab:combined-results-websearch-borda}, an ensemble of different methods can outperform any of the single methods being combined.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ERROR ANALYSIS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Error Analysis}

Even when combining the top-performing generation methods, the performance metrics still show much room for improvement.
In order to identify the best opportunities for improvement, we analysed a sample of the fine-tuning model output that failed to produce the target URL, and classify them. A detailed breakdown is shown in Table~\ref{tab:ft-error-analysis} in Appendix~\ref{sec:appendix}. The most common error mode is for the model to fail to include key terms in the query, which change the nature of the information being sought. The second most common corresponds to cases where the target query included terms or context external to the claim sentence. This could potentially be solved by including text from the article title, or previous sentences, in the model input. The third most common cause of error is the model focusing on the wrong named entity when multiple are present in the claim text. With the same frequency, we also found cases of hallucination, where the output of the model did not take into account the text input, and instead, it produced text from what appeared to be training samples. The next failure mode is where the model simply reproduced the claim text verbatim up to its token limit. Finally, we found a small percentage of cases where, while the query was different from the target, it appeared to cover the key terms necessary to get the correct results. In these cases, it may be due to a change in the website structure, or simply due to the inherent variation in web search engine results. 
