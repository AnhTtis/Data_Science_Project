\section{Experiments}
\label{sec:experiments}
In this section, we describe the specific implementations of each approach used in our experiments.

\subsection{Rule-based Query Creation Baselines}

 We use the Spacy library to extract Named Entity Mentions as well as the parse tree of the text in a claim.  Table \ref{tab:querygenexamples} in Appendix~\ref{sec:appendix} shows examples of the application of the different rule-based methods.   

\subsection{Conditional Text Generation}
\label{sec:experiments_prompts}

We use the T5 language model \citep{raffel_exploring_2020}, specifically, the language generation adapted model (t5-lm-adapt) introduced in \citet{lester-etal-2021-power}. This version is trained for an additional 100K steps on the LM task, in order to overcome training limitations in the original T5 model which struggles to use uncorrupted text. We use the large checkpoint of the model which has 800 million parameters.  
We use the implementation provided by the Huggingface Transformers python library\footnote{https://github.com/huggingface/transformers}. We set the following parameters for text generation: we use beam search with 10 beams, restrict the model from repeating bigrams, and set generation early stopping to true. Since the generated queries should not be very long, we set the maximum number of new tokens to 16. 

During our experiments, we noticed that the T5 model has a tendency to reproduce parts of the prompt in the generated text. We observed this specifically in the few-shot and zero-shot learning experiments. To account for this, we add a post-processing step to the generation: if the prompt text (either from the prefix or suffix) is included verbatim in the model output, it is removed. Partial inclusions are not removed from the output.

As found in \citet{petroni-etal-2019-language,brown_language_2020, schick-schutze-2021-just}, the choice of prompt to use has a significant effect on model performance.  Out of the tasks included in the original T5 paper, we consider summarisation to be the closest to the query extraction task, and thus experiment with the ``summarize'' prompt here.  We also experiment with prompts that describe the task the models needs to perform in different ways: \textit{Generate Search Query}, \textit{Fact-check the following sentence}, \textit{Verify the following sentence} and \textit{Summarize the following sentence} prompt. We also consider shorter prompts, which only use one or two tokens to indicate the action required from the model: \textit{Search query}, \textit{Fact-check}, \textit{Verify} and \textit{Summarize}. We experiment with using the shorter prompts both as prefix and suffix to the model input.  The prompts used in our experiments are detailed in Appendix\ref{sec:appendix}.

\subsubsection{Zero-shot learning}
With this method, the prompt selection is the only thing guiding the output produced by the model. 

\subsection{Few-shot learning}
\label{sec:experiments_fewshot}
We evaluate the few-shot learning method using 4-fold cross-validation. The split is done based on the source article of the claims, meaning claims originating on the same article only appear in one of the sets, training or testing.  See Table~\ref{tab:ec-dataset-fold-sizes} in Appendix \ref{sec:appendix} for the number of records in the train and test set for each of the folds. 
At each fold, the model is evaluated on 25\% of the data, and the remaining 75\% can be used as in-context examples for the text generation model. In order to select the optimal in-context examples, we first run a one-shot learning evaluation, using each individual sample in the training set as examples, and evaluating the generated queries against the rest of the training set using the Levenshtein distance metric. The best 3 performing examples then undergo the complete evaluation, in the one, two and three-shot settings.

\subsection{Conventional Fine-tuning}

We again use 4-fold cross-validation.  Models are fine-tuned on 75\% of the claims and evaluating on the remaining 25\% each time. This split is done as before based on the source article.  For each fold, ~15\% of the training data is selected as a validation set.  The validation set is used to evaluate the model performance at the end of each fine-tuning epoch. The model is evaluated based on the Levenshtein similarity ratio to the target search queries of the validation set. At the end of the fine-tuning process, the best performing checkpoint of the model is saved. Each model is fine-tuned for 10 epochs on a single GeForce RTX 3090 GPU. Following \citet{raffel_exploring_2020}, we use the Adafactor optimiser, with a learning rate of 1e-3 and max input sequence length of 512. We use the same generation parameters as for zero-shot and few-shot learning.