\section{Appendix}
\label{sec:appendix}

Table~\ref{tab:ev-coll-prompt-templates} Shows the different templates used for prompting in the zero-shot and few-shot generation methods. The same templates were applied during fine-tuning.
The scores reported in Table~\ref{tab:combined-results-websearch-borda} were achieved when using template-03 for zero-shot learning, template-06 for one-shot learning, template-05 for two and three-shot learning, and no-prompt for fine-tuning. In general, prompts with longer task descriptions were more useful for zero-shot learning. Meanwhile, short suffix prompts were produced the best results for few-shot learning. When using shorter prompts, having them as a suffix to the input produced better results, while short prefix prompts produced poor results, especially in the few-shot setting. For fine-tuning, the choice of prompt had very little effect, while the best performance was achieved when using no prompts at all.

\begin{table*}
%\small
\centering
\begin{tabular}{|l|l|p{2cm}|p{2cm}|p{4cm}|}
\hline
Format & Template ID&Prefix&Suffix&Zero-shot example\\
\hline
no-prompt & no-prompt & - & - & Gerry Ford, 63, founded the business in 1997. \\
\hline

Long-explanation & template-01 & Generate search query: & Search query: &  \textit{Generate search query:} Gerry Ford, 63, founded the business in 1997. \textit{Search query:} \\
 &template-02 & Fact-check the following sentence: & Fact-check: &  \textit{Fact-check the following sentence:} Gerry Ford, 63, founded the business in 1997. \textit{Fact-check:}  \\
 &template-03 & Verify the following sentence: & Verify: &  \textit{Verify the following sentence:} Gerry Ford, 63, founded the business in 1997. \textit{Verify:}  \\
 &template-04 & Summarize the following sentence: & Summarize: &  \textit{Summarize the following sentence:} Gerry Ford, 63, founded the business in 1997. \textit{Summarize:}  \\
\hline
Short-suffix & template-05 & - & Search query: &  Gerry Ford, 63, founded the business in 1997. \textit{Search query:}  \\
& template-06 & - & Fact-Check:  &  Gerry Ford, 63, founded the business in 1997. \textit{Fact-Check:}  \\
 & template-07 & - & Verify: &  Gerry Ford, 63, founded the business in 1997. \textit{Verify:}  \\
 & template-08 & - & Summarize: &  Gerry Ford, 63, founded the business in 1997. \textit{Summarize:}  \\
\hline
Short-prefix & template-09 & Search query: & - &  \textit{Search query:} Gerry Ford, 63, founded the business in 1997. \\
& template-10 & Fact-Check: & - &  \textit{Fact-Check:} Gerry Ford, 63, founded the business in 1997. \\
& template-11 & Verify: & - &  \textit{Verify:} Gerry Ford, 63, founded the business in 1997. \\
& template-12 & Summarize: & - &  \textit{Summarize:} Gerry Ford, 63, founded the business in 1997. \\

\hline
\end{tabular}
\caption{Prompt templates used for zero-shot, few-shot and fine-tuning methods.}
\label{tab:ev-coll-prompt-templates}
\end{table*}

Each method's sensitivity to prompt choice was highly varied. The few-shot setting showed to be the most sensitive, with experiments using no prompts or short-prefix type prompts resulting in very low performance. This effect was not as noticeable for zero-shot learning, while in fine-tuning, the prompt choice had a negligible effect. Table~\ref{tab:prompt-choice-var} shows the average performance for each method across all prompt templates, as well as the standard error. We also show the same metrics for few-shot while removing the results using no prompts or short-prefix style prompts. 

\begin{table*}
%\small
\centering
\begin{tabular}{|l|c|c|}
\hline
Method & R-2 & FM\% \\
\hline
zero-shot & 0.340 (0.008) & 19.7 (1.1) \\
fine-tuning & 0.454 (0.004)	& 38.4 (0.5) \\
\hline
one-shot & 0.263 (0.014)	& 12.1 (1.7) \\
two-shot & 0.285 (0.026)	& 16.2 (2.9) \\
three-shot & 0.297 (0.027)	& 18.8 (3.2) \\
\hline
one-shot* & 0.299 (0.009)	& 16.5 (1.1) \\
two-shot* & 0.359 (0.004)	& 24.4 (0.8) \\
three-shot* & 0.372 (0.005)	& 27.9 (0.7) \\
\hline
\end{tabular}
\caption{Variability of results by prompt choice. Numbers are the average of metrics across experiments using different prompt templates. Numbers in parenthesis are standard error. Methods marked with an asterisk exclude experiments using no prompt or short-prefix prompts.}
\label{tab:prompt-choice-var}
\end{table*}



Table~\ref{tab:querygenexamples} shows the output of applying the rule-based baselines to the example sentence: \textit{Netanyahu didn't become Israel's longest-serving prime minister by mistake.} The fact under verification is whether Netanyahu was indeed the longest-serving prime minister of Israel.

\begin{table*}
%\small
\centering
\begin{tabular}{|p{4cm}|p{8cm}|}
\hline
Generation Method&Query\\
\hline
Verbatim&Netanyahu didn't become Israel's longest-serving prime minister by mistake.\\
\hline
Named Entities &Netanyahu, Israel\\
\hline
Noun Phrases& Netanyahu,Israel's longest-serving prime minister, mistake \\ 

\hline
\end{tabular}
\caption{Output of different rule-based query generation baselines.} 
\label{tab:querygenexamples}
\end{table*}

Table~\ref{tab:ec-dataset-fold-sizes} shows the size of training, dev and test sets derived using 4-fold cross-validation. The split was done based on the source article of the claims.

\begin{table*}
%\small
\centering
\begin{tabular}{|l|c|c|c|}
\hline
Fold & Train & Dev & Test \\
\hline
K0 & 245 & 45	& 100 \\
K1 & 246	& 48	& 96 \\
K2 & 243	& 48	& 99 \\
K3 & 248	& 47	& 95 \\
\hline
\end{tabular}
\caption{Number of samples in fine-tuning data splits.}
\label{tab:ec-dataset-fold-sizes}
\end{table*}

Table~\ref{tab:ft-error-analysis} contains the results and examples from the error analysis of a sample of fine-tuned model predictions. 

\begin{table*}
%\small
\centering
\begin{tabular}{|p{4cm}|p{6cm}|c|}
\hline
Reason  & Example & Percentage \\
\hline
Query missing key information/term & Claim Text: Meanwhile, Hostelling Scotland — the YHA’s counterpart north of the border, turning 90 this year — began a phased reopening of its 60-plus hostels two days ago.  	& 30\% \\
Query required context outside claim& Claim Text: The opening ceremony, in the National Stadium in Tokyo, is set for July 23.	&	26.7\% \\
& Target Query: tokyo olympics opening ceremony & \\
& Generated Query: National Stadium in Tokyo & \\
& Target Query: hosteling scotland history & \\
& Generated Query: hostelling scotland & \\
Extracted wrong named entity & Claim Text: Priti Patel is considering withdrawing the UK from part of the European Social Charter that gives citizens of 26 countries a £55 discount on application fees for most worker visas.	&	16.7\% \\
& Target Query: european social charter fee exemption & \\
& Generated Query: Priti Patel & \\
Hallucination/Memorised from training data&	Claim Text: For the ride-hailing giant, that judgment in February could be the beginning of a wider reckoning that lands it with a £2 billion-plus VAT bill.&	16.7\% \\
& Target Query: uber february £2billion & \\
& Generated Query: mup scotland & \\
Recreated claim text& Claim Text: Colorado and Oregon have followed suit with similar lotteries and on Thursday California announced an even larger draw, with ten prizes worth \$1.5 million.	&	6.7\% \\
& Target Query: california vaccine lottery amount & \\
& Generated Query: Colorado and Oregon have followed suit with similar lotteries & \\
Query looks good & Claim Text: Chris James, founder of the impact investing firm Engine No. 1, scored a stunning victory over the biggest beast in the Big Oil jungle last week: Exxon Mobil.	&	3.3\% \\
& Target Query: chris james engine no 1 & \\
& Generated Query: Chris James founder of the impact investing firm Engine No. & \\
\hline
\end{tabular}
\caption{Fine-tuned model output error analysis.} 
\label{tab:ft-error-analysis}
\end{table*}