\section{Background and Related Work}
\label{sec:related}
In this section, we review the literature on evidence collection for fact-checking (Section \ref{sec:related_evidence}) and prompting large language models / in-context learning (Section \ref{sec:related_prompting}).

\subsection{Evidence Collection in Fact-Checking Works}
\label{sec:related_evidence}

Relatively little research has been done in fact-checking using the entirety of the web as the source, as is done in this work. In \citet{popat-etal-2016-credibility}, the authors perform a credibility assessment of claims collected from the Snopes\footnote{https://www.snopes.com} fact-checking website, and from Wikipedia, classifying claims as \textit{true} or \textit{fake}. To collect evidence articles from the web, the authors use the full claim text on the Google search engine and scrape the first three pages of results from each. The authors use a reliability score for evidence pages based on the website's AlexaRank and PageRank~\citep{page1999pagerank}, as well as a series of lexicons to create a representation of the linguistic style of these articles. They found that the source reliability score had little effect on the outcomes.

\citet{karadzhov_fully_2017} presents an approach using web search as a source of evidence for fact-checking, using both Bing and Google as search engines. Search queries are constructed using a combination of rules: adding the top-ranked verbs, nouns and adjectives using TF-IDF, as well as adding all named entities present in the claim. Queries are limited to 5-10 tokens. They evaluate their approach in the rumour detection dataset published by \citet{ma-etal-2016-detecting}. They report finding no practical difference in performance from using Bing or Google. They also found almost no difference when using the full text of the page as evidence versus only the snippet delivered by the search engine.

The FEVER task \citep{thorne-etal-2018-fever} consists of the classification of claims into Supported, Refuted and Not Enough Info, based on evidence contained in Wikipedia. Systems are evaluated not only on their ability to produce the correct classification but also on the retrieval of the correct evidence. Participants in FEVER have used a variety of approaches to evidence retrieval, sometimes including the use of web search or similar search APIs. For example, \citet{hanselowski-etal-2018-ukp} extract Noun Phrases from the claim text and use them as search queries on the Wikipedia API, which finds matching articles using the Wikipedia search engine. This approach resulted in the highest evidence recall of all participants in the first edition of the shared task and was re-used by participants in later editions. In FAKTA~\citep{nadeem-etal-2019-fakta}, in addition to using the 2017 Wikipedia dump for evidence, the authors use the Google Custom Search API to search specific websites for evidence, classified by their reliability by the Media Bias/Fact Check website\footnote{https://mediabiasfactcheck.com}. Queries are created by only considering the verbs, nouns, adjectives, and named entities present in a claim.

In \citet{fan-etal-2020-generating}, the authors introduce a dataset geared towards the generation of fact-checking briefs, intended to present information that can assist fact-checkers. This includes QA briefs, which extract specific questions and answers based on the claim. To generate these, the authors train a text-to-text model, BART~\citep{lewis-etal-2020-bart}, to automatically generate questions based on the claim of the text. They then train a QA model to generate answers to these questions, based on the question text and the source document used to originally fact-check the source claims. Our work differs from this in that the claim text is used to generate search queries that are then applied directly to a search engine, in order to find evidence documents from the web. 

\subsection{Prompting and In-context Learning}
\label{sec:related_prompting}

  A recent paradigm shift in the usage of large language models (LLMs) has been referred to as the “pre-train, prompt, and predict” approach \citep{liu_pre-train_2021}.  The intuition behind it is to modify the task at hand to approximate more closely the model pre-training task. Since LLMs are pre-trained with language modelling tasks, these modifications have consisted of changing the input, so it more closely resembles natural language. Depending on the task at hand this has been done through the use of templates, for example, to turn classification tasks into cloze tasks \citep{petroni-etal-2019-language}. It can also be done through the use of prompts \citep{brown_language_2020} which are short textual explanations of the task added to the model input. The line of research focused on identifying effective prompts is known as prompt engineering \citep{liu_pre-train_2021}. In fact, the main motivation for the T5 model \citep{raffel_exploring_2020} was the reformulation of traditional NLP tasks into text-to-text (text as input, and text as output) tasks, so that other tasks can be attempted using a prompting approach.

An extension to this approach is to also include solved examples of the task the model needs to perform. This is known as in-context learning \citep{brown_language_2020}, also referred to as few-shot learning. Using in-context examples has been shown to produce results that are competitive with the fine-tuning approach in tasks such as  translation, question-answering, and cloze tasks. Although not able to surpass fine-tuning in performance, in-context learning has the advantage of requiring smaller amounts of data and computing resources, as the model is only performing inference.

The selection of examples to add to the context is one of the key elements of this method, as it can have a large effect on the model's performance\citep{lu-etal-2022-fantastically}. These examples should reflect the task that the model is expected to perform. Examples can be randomly sampled from a training dataset \citep{brown_language_2020}, or a smaller held-out development dataset. However, in a truly limited data setting, this may not be a viable approach. An alternative could be to sample examples from closely-related tasks. However, \citet{perez_true_2021} have argued against such use of out-of-domain data. 
