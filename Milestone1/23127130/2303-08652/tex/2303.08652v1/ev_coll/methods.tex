\section{Methodology}
\label{sec:methods}
In this section, we first define the query generation task \ref{sec:methods_query}.  We then outline rule-based \ref{sec:methods_rules} and conditional text generation \ref{sec:methods_cond} approaches to solving it and the evaluation methods \ref{sec:methods_eval}  we have employed.      
\subsection{The Query Generation task}
\label{sec:methods_query}
The query generation task is defined as follows: each sample is composed of a text claim \(c\), a human-created query \(q\)  and a target url \(url\), which contains the evidence required to validate \(c\). Given the text of the claim \(c\), generate a search query \(q'\) that, when executed on a search engine, will produce \(url \) within the first page of results.  We evaluate the quality of \(q'\) by its similarity to \(q\) , as well as by executing it on the Bing search engine and looking for \(url\) in the results. 

\subsection{Rule-based Query Generation}
\label{sec:methods_rules}
Rule-based query generation approaches take into account the syntactic features of the tokens in claim \(c\) to decide whether they should be included in query \(q\).  We consider 3 approaches described below:

\noindent\textbf{Verbatim}: This method uses the full text of the claim as query. The search engine will likely find close paraphrases of the text in \(c\). 

\noindent\textbf{Named Entities}: All named entities identified in the sentence are concatenated together. This approach is expected to find results where the entities are mentioned together, and which may be stating similar relations between them, but may not necessarily agree with the claims being made in the source sentence.

\noindent\textbf{Noun Phrases}: All noun phrases identified in the sentence are concatenated together. This is similar to the above; however, as the definition of a noun phrase is broader than that of a Named Entity, it incorporates additional content from the claim.


\subsection{Conditional Text Generation}
\label{sec:methods_cond}
We model the query generation task as conditional text generation. That is, a text generation model is input with the full text of the claim \(c\) together with any additional contextual information, and outputs the query \(q'\). We experiment with three conditional text generation approaches:

\noindent\textbf{Zero-shot learning}: 
A prompt template is used to add a prefix to the claim \(c\) before it is input to the text generation model.   We experiment with different prompts, described in Section \ref{sec:experiments_prompts}.

\noindent\textbf{Few-shot learning}: In few-shot learning, the model is shown a small number of examples before attempting the task.  It is done by pre-pending these examples directly before the main task input. 

\noindent\textbf{Conventional Fine-tuning}: Finally, we also experiment with conventional fine-tuning. In this setting, the T5 model is fine-tuned receiving the claim sentence \(c\) as input and human-created query \(q\) as target. Due to the limited size of the dataset, we use 4-fold cross-validation. For each fold, the model is fine-tuned on ~75\% of the data and tested on the remaining 25\%. We average the evaluation metrics over the 4 test sets. 


\subsection{Evaluation}
\label{sec:methods_eval}
We evaluate the different query generation methods in two ways: (1) on the generated queries' similarity to the target queries; and (2) on the generated queries' ability to return the target URLs. 

\subsubsection*{Query Similarity Evaluation}

Our reasoning for using this evaluation is that a model that produces the same query as the human annotator is a high-quality model. It further stands to reason that an identical query has a higher probability of producing the same search results than a different one. This is true even when considering the search result inconsistencies described before.

\noindent\textbf{Rouge}: We use Rouge \citep{lin-2004-rouge} to evaluate the automatically generated queries' textual similarity to the target queries. This metric is widely used to evaluate model performance in tasks such as summarisation and translation. We use Google Research's python implementation of Rouge\footnote{https://github.com/google-research/google-research/tree/master/rouge}. Since both the target and the automatically generated queries are quite short, we instead use character-based Rouge, ie. we replace the standard tokeniser with simply splitting sentences into characters. We report the Rouge-1, Rouge-2 and Rouge-L metrics for all experiments.

We also considered using the Levenshtein or edit distance \citep{levenshtein1966binary} between queries.  Whilst, theoretically, character-based Rouge scores should be less affected by the re-ordering of words in queries, we found in practice that Levenshtein distance scores were highly correlated with the Rouge-L scores, so we do not include them here. However, Levenshtein distance is used to select the optimal samples to be used as in-context examples in few-shot learning (see Section \ref{sec:experiments_fewshot}).

\subsubsection*{Search Result Evaluation}

We execute the queries via the Bing search API\footnote{https://www.microsoft.com/en-us/bing/apis/bing-web-search-api} and collect the top 10 results, corresponding to the first page in a browser. We evaluate whether the target URL appears in the search results. 

There are a number of complications to this evaluation. The first is the consistency of the search results: during our experiments, we found that using the same query does not guarantee receiving the same results from the search engine. In order to account for this variation, we execute each query 3 times and collect the three different ranked lists of search results.
Second, this evaluation comes with a financial cost from using the search provider's APIs. This cost can quickly add up when evaluating different methods and hyperparameter configurations, both for fine-tuning and generation. This is compounded by the repeated executions required in order to improve the consistency of the results. To account for this, we use query similarity evaluation when tuning model hyperparameters, and only use search result evaluation on the final models.  We use the following metrics for search result evaluation:

\noindent\textbf{Found All Percentage (FA\%)}: The proportion of test samples where the target URL was present in the search results for all three executions.

\noindent\textbf{Found Majority Percentage (FM\%)}: The proportion of test samples where the target URL was present in the search results for the majority of executions, i.e. at least two.

\noindent\textbf{Found Once Percentage (FO\%)}: The proportion of samples where the target URL was found in the results for at least one of the executions.

\noindent\textbf{MRR}: Mean Reciprocal Rank (MRR@K) is a metric frequently used for evaluation in information retrieval tasks \citep{voorhees-tice-2000-trec}. This metric takes into consideration the highest-ranked relevant answer in the retrieved documents.  We set \(K=10\), corresponding to the first page of results of a search engine.  Each list of search results corresponding to an executed search query is considered separately and contributes to the overall MRR score.
