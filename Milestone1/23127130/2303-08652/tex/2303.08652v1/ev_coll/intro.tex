\section{Introduction}
\label{sec:intro}
The need for fact-checking, i.e., to check the veracity of statements that are being presented as facts, has become an integral part of everyday life in our digital age.  Fact-checking may be carried out by the presenters of the facts, e.g., as part of an internal sub-editing process at a news organisation, or externally by the receivers of those facts.  In either case, the collection of evidence is a key step in the fact-checking process. Factual information is required to make informed decisions on the veracity of claims.  Evidence can come from a variety of sources, many of which make their information freely available on the Internet.

Search engines have become the default way for people to interact with the internet. Whilst shadowing sub-editors at a major UK newspaper performing their fact-checking duties, we found that a web search was often the first step for locating verifying evidence.  Then, sub-editors would either select pages from the search results to read or, if no relevant pages were found, they would adjust their query, making additions or other modifications, and search again until a suitable evidence page was found. 

We set out to replicate this behaviour by developing an evidence collection module for an automated system. This module takes care of generating search queries based on sentences where claims have been identified, and collecting the text from these pages. Selecting the relevant snippets and assessing the veracity of claims is beyond the scope of this paper. However, coming up with the right search queries is a non-trivial task. Long sentences may have more than one potential fact being stated. It can be difficult to identify the actual fact being verified by the user. Thus the query generation technique used is a key component of the evidence collection module. 

In this work, we analyse a number of techniques for conditional text generation and compare their performance to a number of rule-based generation baselines. We evaluate the automatically generated queries with respect to their similarity to human-crafted search queries, both in terms of textual similarity and in terms of the search results produced when executed on a search engine. 

Our contributions are threefold.  First, we introduce an evidence collection dataset specific to the fact-checking application for supporting sub-editing.  Second, we demonstrate how conditional text generation techniques can be used effectively to create useful search engines for the collection of evidence both alone and in combination with rule-based approaches.  Third, we demonstrate that similarity to human-created search queries is a useful proxy for determining whether an automatically generated query will be effective in retrieving the same evidence.  However, there are non-trivial cases when seemingly different search queries can result in the same evidence being collected. 

This paper is organised as follows: Section~\ref{sec:related} discusses related work in evidence retrieval for fact-checking and prompting/in-context learning. Section~\ref{sec:dataset} introduces the dataset we annotated for the query generation task. Section~\ref{sec:methods} explains the methods we use for conditional generation and our evaluation approach. Section~\ref{sec:experiments} describes the experiments. Section~\ref{sec:results} presents our results, and section~\ref{sec:conclusion} closes with conclusions and future work.