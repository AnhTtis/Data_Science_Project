{
    "arxiv_id": "2303.08652",
    "paper_title": "Automated Query Generation for Evidence Collection from Web Search Engines",
    "authors": [
        "Nestor Prieto-Chavana",
        "Julie Weeds",
        "David Weir"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CL",
        "cs.IR"
    ],
    "abstract": "It is widely accepted that so-called facts can be checked by searching for information on the Internet. This process requires a fact-checker to formulate a search query based on the fact and to present it to a search engine. Then, relevant and believable passages need to be identified in the search results before a decision is made. This process is carried out by sub-editors at many news and media organisations on a daily basis. Here, we ask the question as to whether it is possible to automate the first step, that of query generation. Can we automatically formulate search queries based on factual statements which are similar to those formulated by human experts? Here, we consider similarity both in terms of textual similarity and with respect to relevant documents being returned by a search engine. First, we introduce a moderate-sized evidence collection dataset which includes 390 factual statements together with associated human-generated search queries and search results. Then, we investigate generating queries using a number of rule-based and automatic text generation methods based on pre-trained large language models (LLMs). We show that these methods have different merits and propose a hybrid approach which has superior performance in practice.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08652v1"
    ],
    "publication_venue": null
}