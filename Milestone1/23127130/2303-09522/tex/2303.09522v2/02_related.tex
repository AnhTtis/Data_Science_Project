\section{Related works}

\subsection{Extended 
Spaces in Generative Models}

Exploring neural sub-spaces in generative models has been extensively explored, most notably in
StyleGAN~\cite{Karras2020ada,karras2019style}.
The extended textual conditioning $\PromptP$ is reminiscent of StyleGAN's extended latent space \cite{abdal2019image2stylegan,abdal2020image2stylegan++}, also commonly referredÂ to as $\mathcal{W+}$. 
Similar to $\mathcal{W+}$, $\mathcal{P+}$ is significantly more expressive, where instead of a single code shared by all layers, there is one per layer. However, while $\mathcal{W+}$ is an extended latent space, here the extended space relates to the textual conditions used by the network. It should be noted, though, that while $\mathcal{W+}$ is expressive, the extended code is less editable \cite{tov2021designing}. In contrast, $\mathcal{P+}$ remains practically as editable as $\mathcal{P}$. In addition, other sub-spaces lay within deeper and more disentangled layers \cite{wu2021stylespace} have been explored and exploited in various editing and synthesis applications \cite{bermano2022state}.

% where the original latent space of the model has been expanded to have a per layer code \cite{abdal2019image2stylegan,abdal2020image2stylegan++}, which enables to improve the expressivity of the model and its reconstruction capability when applying GAN inversion~\cite{xia2022gan}. 


In the case of text-to-image diffusion models, the denoising U-net, which is the core model of most of the text-to-image diffusion models, is usually conditioned by text prompts via a set of cross-attention layers \cite{ramesh2022hierarchical, rombach2021highresolution, saharia2022photorealistic}. 
In many neural architectures, different layers are responsible for different abstraction levels \cite{bau2020units, karras2019style, voynovrpgan, zeiler2014visualizing}. It is natural to anticipate that the diffusion denoising U-Net backbone operates in a similar manner, with different textual descriptions and attributes proving beneficial at different layers.

\subsection{Text-Driven Editing}

There has been a significant advancement recently in generating images based on textual inputs through Text-to-Image models~\cite{chang2023muse, ramesh2022hierarchical,rombach2021highresolution, saharia2022photorealistic}, where most of them exploit the powerful architecture of diffusion models \cite{ho2020denoising,rombach2021highresolution,sohl2015deep,song2020denoising,song2019generative}.

In particular, recent works have attempted to adapt text-guided diffusion models to the fundamental problem of single-image editing, aiming to exploit their rich and diverse semantic knowledge of this generative prior.

In a pioneering attempt, Meng et al.~\cite{meng2021sdedit} add noise to the input image and then perform a denoising process from a predefined step. Yet, they struggle to accurately preserve the input image details, which were preserved by a user provided mask in other works~\cite{avrahami2022blended, avrahami2022blendedlatent, nichol2021glide}. DiffEdit \cite{couairon2022diffedit} employs DDIM inversion for image editing, but to prevent any resulting distortion, it generates a mask automatically that allows background preservation.

Text-only editing approaches split into approach that supports to global editing  \cite{crowson2022vqgan, kim2021diffusionclip, kwon2021clipstyler, patashnik2021styleclip}, and local editing ~\cite{bar2022text2live,wang2022imagen}. Prompt-to-prompt~\cite{hertz2022prompt} introduces an intuitive editing technique that enables manipulation of local or global details by injecting internal cross-attention maps.
%\cite{mokady2022null} proposed an approach to invert real images into the latent space of the diffusion model, such that prompt-to-prompt can be applied to real images.
To allow prompt-to-prompt to be applied to real images, Null-Text Inversion \cite{mokady2022null} is proposed as means to invert real images into the latent space of the diffusion model.
Imagic \cite{Kawar2022ImagicTR} and UniTune~\cite{valevski2022unitune} have demonstrated impressive text-driven editing capabilities, but require the costly fine-tuning of the model. The InstructPix2Pix~\cite{brooks2022instructpix2pix}, Plug-and-Play~\cite{pnpDiffusion2022}, and Parmar et al.~\cite{parmar2023zero} allow users to input an instruction or target prompt and manipulate real images accordingly, to achieve the desired edits.

% The maps injecting enables the regeneration of an image, while modifying it through prompt editing only.
 
% Imagic \cite{Kawar2022ImagicTR} and UniTune\cite{valevski2022unitune} have demonstrated impressive text-driven editing capabilities. Yet, they both require the costly fine-tuning of the model. Moreover, Imagic requires a new tuning for each editing, while UniTune involves parametric search for each image. 

\subsection{Personalization}
Synthesizing particular concepts or subjects which are not widespread in the training data is a challenging task. 
This requires an \textit{inversion} process that given input images would enable regenerating the depicted object using a text-guided diffusion model. Inversion has been studied extensively for GANs \cite{bermano2022state, creswell2018inverting, lipton2017precise, xia2021gan, yeh2017semantic, zhu2016generative}, ranging from latent-based optimization \cite{abdal2019image2stylegan,abdal2020image2stylegan++} and encoders \cite{richardson2020encoding,tov2021designing} to feature space encoders \cite{Wang2021HighFidelityGI} and fine-tuning of the model \cite{alaluf2021hyperstyle, roich2021pivotal, nitzan2022mystyle}. 
%

The notion of personalization of text-to-image models has been shown to be a powerful technique. 
Personalization of models \cite{kumari2022customdiffusion, ruiz2022dreambooth} in general or of text tokens \cite{gal2022image} has quickly been adapted for various applications \cite{kawar2022imagic,lin2022magic3d}. In addition to their high computational cost, current methods face a clear-trade-off between learning tokens that accurately capture concepts vs. avoidance of overfitting. This can result in learned tokens that are overly tuned to the input images, thus limiting their ability to generalize to new contexts or generate novel variations of the concept. 

Similar to TI, our approach does not require any fine-tuning or modification of the weights, thus, reduces the risk of overfitting and degrading the editability capabilites. In contrast, our inversion process into $\PromptP$ is both faster and more precise, thanks to the greater number of tokens that improve reconstruction capabilities without sacrificing editability. 

% \dcc{Do we want to cite Rinon Encoder?}
% \kac{we have it in the conclusions..}
% \dcc{I know... the question is whether it would be expected then to be mentioned here...}

% Motivated by this, Gal et al.~\cite{gal2022image} present a textual inversion scheme for diffusion models that enables regenerating a user-provided concept out of $3-5$ images. Concurrently, Ruiz et al.\cite{ruiz2022dreambooth} tackled the same task with DreamBooth model fine-tuning. 

% personalizaized models has quickly been adapted for various applications 
% \cite{kawar2022imagic,lin2022magic3d}.







