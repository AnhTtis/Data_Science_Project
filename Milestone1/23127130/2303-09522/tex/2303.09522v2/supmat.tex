% \renewcommand\thefigure{\arabic{figure}}

\newpage
{\centering \Large{Supplementary}}

\subsection{Regularization}

When applying style mixing, we discovered that optimizing XTI with an additional density regularization loss term (Equation 2) improves the mixing capability while maintaining the overall quality of the inversion. To achieve this, we use an extra loss term $-\lambda \cdot \sum_{i=1}^l \log p_{\mathcal{E}}(e_i)$, where $\lambda = 0.002$ serves as a small regularization scale. This loss term encourages the newly added look-up table embedding to be even more regular, causing the optimized token distribution from Figure \ref{fig:embed_log_density} to shift closer to the original token distribution. We contend that this is particularly advantageous for the mixture application because in this scenario, generation is conditioned by two different XTI tokens, making it crucial to have them interact naturally, i.e., with the two tokens lying closer to the natural language manifold.

However, applying this regularization term to the original TI for subject recontextualization leads to a degradation in subject similarity of the inversion. Even with a small $\lambda$ scaling factor, this regularization enforces significant simplification in the inversion process, leaving the reconstructed token with limited freedom and expressivity. Meanwhile, although the drop in quality for XTI is minimal when the regularization is added, we do not use it by default for the recontextualization task for XTI because it would increase its complexity (another hyperparameter) and convergence rate.

% \newpage

\begin{figure*}
    \centering
    \includegraphics[width=0.875\textwidth]{figures/scatter_outer.pdf}
    \caption{\textbf{Style mixing examples}. \textit{Top row}: Style source subjects, \textit{First column}: Geometry source subjects. The geometry subject's tokens are passed to the three layers in the range \texttt{(16,~'down',~1) - (16,~'up',~0)}, while all the rest are conditioned on the appearance subject's token.}
    \label{fig:mix_inner}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=0.875\textwidth]{figures/scatter_inner.pdf}
    \caption{\textbf{Style mixing with more appearance layers}. \textit{Top row}: Style source subjects, \textit{First column}: Geometry source subjects. Here the geometry subject's tokens are passed only to two layers \texttt{(8,~'down',~0)} and \texttt{(16,~'up',~0)}. Thus this emphasizes the appearance subject's token more, resulting in a more dominant appearance compared to the previous setup in Figure~\ref{fig:mix_inner}.}
    \label{fig:mix_inner_2}
\end{figure*}

% \newpage



\section{Further Results and Details}

Figures \ref{fig:mix_inner} and \ref{fig:mix_inner_2} provide more examples of geometry and style mixing. In Figure \ref{fig:mix_inner} objects prompts are passed to a wider range of layers compared to Figure \ref{fig:mix_inner_2}, enforcing higher source geometry alignment.

Figure \ref{fig:samples_all} provides more uncurated examples generated with Textual Inversion and Extended Textual Inversion.

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/samples_all_crop.pdf}
    \caption{Samples generated with Textual Inversion (TI) and the proposed Extended Textual Inversion (XTI). XTI has better text alignment while providing more accurate subject reconstruction.}
    \label{fig:samples_all}
\end{figure*}

As for inference we use the PNDM scheduler \cite{pndm} with 50 denoising steps and a classifier-free guidance \cite{ho2021classifier} scale of 7.5. 

We implemented DreamBooth with a learning rate of 5e-6, a batch size of 4, and conducted 400 finetuning steps per-subject, using Stable Diffusion. Our optimization focused solely on the U-net weights and did not involve prior preservation. 

\subsection{Coarse/Fine Layers Split}
We provide the details of the layer subsets we used in the shape-style mixing experiments in Figures \ref{fig:attn_frac} and \ref{fig:mix_bars}. We name the cross-attention layers of Stable Diffusion U-net as follows, in the order they appear in the U-net: \\
\texttt{(64,~'down',~0), (64,~'down',~1), (32,~'down',~0), (32,~'down',~1), (16,~'down',~0), (16,~'down',~1), (8,~'down',~0), (16,~'up',~0), (16,~'up',~1), (16,~'up', 2), (32,~'up',~0), (32,~'up',~1), (32,~'up', 2), (64,~'up',~0), (64,~'up',~1), (64,~'up', 2)}. \\
\\
The first number represents the spatial resolution, \texttt{'down' / 'up'} represents whether the layer is on the downward (contracting) or upward (expansive) part of the U-net, and the third number indicates the index among the cross-attention layers of the same resolution and direction.

In Figure \ref{fig:mix_bars} we use the following growing sequence of cross-attention layer subsets: \\
0: Empty set \\
1: Layer \texttt{(8,~'down', 0)} only \\
2: \texttt{(16,~'down',~1)~-~(8,~'down',~0)} \\
3: \texttt{(16,~'down',~1)~-~(16,~'up',~0)} \\
4: \texttt{(16,~'down',~0)~-~(16,~'up',~0)} \\
5: \texttt{(16,~'down',~0)~-~(16,~'up',~1)} \\
6: \texttt{(16,~'down',~0)~-~(16,~'up',~2)} \\
7: \texttt{(64,~'down',~0)~-~(64,~'up',~2)} \\
The ranges listed above are inclusive.
\\

In Figures 2 and 11 in the main text we condition the layers \texttt{(8,~'down',~0), (16,~'up',~0)} on the target shape textual embeddings, and the other layers on the target style textual embeddings. In Figure 3 in the main text we provide the target shape textual embeddings to layers \texttt{(16,~'down',~1) - (16,~'up',~0)}.

\subsection{Text Prompts}

\subsubsection{Cross-Attention Analysis (Fig. 5)}

We used the following lists of objects and appearances for generating the \texttt{"appearance object"} and \texttt{"object, appearance"} prompts in Figure 5:

Objects (50): \texttt{"dog", "cat", "tree", "chair", "book", "phone", "car", "bike", "lamp", "table", "flower", "desk", "computer", "pen", "pencil", "lamp", "television", "picture", "mirror", "shoe", "boot", "sandals", "house", "building", "street", "park", "river", "ocean", "lake", "mountain", "chair", "couch", "armchair", "bookcase", "rug", "lampshade", "fan", "conditioner", "heater", "door", "window", "bed", "pillow", "blanket", "curtains", "kitchen", "refrigerator", "stove", "oven", "microwave"}

Appearances (20): \texttt{"fuzzy", "shiny", "bright", "fluffy", "sparkly", "dull", "smooth", "rough", "jagged", "striped", "painting", "retro", "vintage", "modern", "bohemian", "industrial", "rustic", "classic", "contemporary", "futuristic"}
\\

\subsubsection{Image Attributes Analysis (Fig. \ref{fig:mix_bars})}
For Figure \ref{fig:mix_bars}, we used the following object, color and style words to generate the prompts:

Objects (13): \texttt{"chair", "dog", "book", "elephant", "guitar", "pillow", "rabbit", "umbrella", "yacht", "house", "cube", "sphere", 'car'}

Colors (11): \texttt{"black", "blue", "brown", "gray", "green", "orange", "pink", "purple", "red", "white", "yellow"}

Style descriptions (7): \texttt{"watercolor", "oil painting", "vector art", "pop art style", "3D rendering", "impressionism picture", "graffiti"}

\subsubsection{Text Similarity Metric Prompts}

\label{sec:metric_prompts}
For Text Similarity evaluation we use the following 14 prompts:

\texttt{"A photograph of <token>", "A photo of <token> in the jungles", "A photo of <token> on a beach", "Aquarelle painting of <token>", "Oil painting of <token>", "Marc Chagall painting of <token>", "Sketch drawing of <token>", "Night photograph of <token>", "Professional studio photograph of <token>", "3d rendering of <token>", "Fantasy CG art painting of <token>", "A statue of <token>", "A photograph of two <token> on a table", "App icon of <token>"}. 

Here $\texttt{<token>}$ represents the placeholder for XTI inversion tokens to be replaced with the corresponding textual description in Table~\ref{table:detailed_captions}.

\begin{table*}[ht]
\centering
\begin{tabular}{ll}
\vspace{5pt}
\textbf{Original Dataset} & \textbf{Text Description} \\
\texttt{elephant} & \texttt{a statue of an elephant} \\
\texttt{cat\_statue} & \texttt{a statue of a cat} \\
\texttt{colorful\_teapot} & \texttt{a colorful teapot} \\
\texttt{clock} & \texttt{an alarm clock} \\
\texttt{mug\_skulls} & \texttt{a cup with a mummy} \\
\texttt{physics\_mug} & \texttt{a black cup with math equations} \\
\texttt{red\_teapot} & \texttt{a red teapot} \\
\texttt{round\_bird} & \texttt{a round bird sculpture} \\
\texttt{thin\_bird} & \texttt{a sculpture of a thin bird} \\
\texttt{barn} & \texttt{an old wooden barn} \\
\texttt{cat} & \texttt{a kitten} \\
\texttt{dog} & \texttt{a grey dog} \\
\texttt{teddybear} & \texttt{a teddy bear} \\
\texttt{tortoise\_plushy} & \texttt{a tortoise plush} \\
\texttt{wooden\_pot} & \texttt{an artistic wooden pot} \\
\end{tabular}
\vspace{10pt}
\caption{Detailed text descriptions for each dataset. The first 9 correspond to the datasets provided in \cite{gal2022image}, and the remaining 6 correspond to the datasets provided in \cite{kumari2022customdiffusion}.}
\label{table:detailed_captions}
\end{table*}


% \texttt{"elephant": "a statue of an elephant", "cat\_statue": "a statue of a cat", "colorful\_teapot": "a colorful teapot", "clock": "an alarm clock", "mug\_skulls": "a cup with a mummy", "physics\_mug": "a black cup with math equations", "red\_teapot": "a red teapot", "round\_bird": "a round bird sculpture", "thin\_bird": "a sculpture of a thin bird", "barn": "an old wooden barn", "cat": "a kitten", "dog": "a grey dog", "teddybear": "a teddy bear", "tortoise\_plushy": "a tortoise plush", "wooden\_pot": "an artistic wooden pot",}

% eval details


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/task_samples.jpg}
    \caption{Human labeling interface. On the \textit{left} we depict a sample task to evaluate subject similarity, and on the \textit{right} the task to evaluate text similarity. The comparing methods raws are always shuffled. Both methods use the same random seed.}
    \label{fig:my_label}
\end{figure*}