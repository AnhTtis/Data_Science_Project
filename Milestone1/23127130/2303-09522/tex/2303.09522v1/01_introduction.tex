\section{Introduction}

Neural generative models have advanced the field of image synthesis, allowing us to create incredibly expressive
%\dcc{not necessarily realistic, how about expressive ?} 
and diverse images. 
Yet, recent breakthroughs in text-to-image models based on large language-image models have taken this field to new heights and stunned us with their ability to generate images from textual descriptions, providing a powerful tool for creative expression, visualization, and design.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/unet_default.pdf}
    \includegraphics[width=0.49\columnwidth]{figures/unet_pp.pdf}
    \caption{{\bf $\mathbfcal{P}$ vs. $\mathbfcal{P}+$}. Standard textual conditioning, where a single text embedding is injected to the network (left), vs. our proposed extended conditioning, where different embeddings are injected into different layers of the U-net (right).}
    \label{fig:unet}
    \vspace{-10pt}
\end{figure}

\begin{figure}[b]
    \centering
    \includegraphics[width=\columnwidth]{figures/teaser_mix_v2.pdf}
    \caption{
    %\textbf{Appearance transfer.} 
    {\bf Shape-Style Mixing in XTI. }The extended textual space allows mixing concepts learned from two separate extended textual inversions (XTIs). The inversion of the kitten (right) is injected to the coarse inner layers of the U-net, affecting the shape of the generated image, and the inversion of the cup (left) is injected to the outer layers, affecting the style and appearance.  
    }
    \label{fig:teaser_mix}
\end{figure}



$\PromptP$ 
These text-to-image diffusion models use the encoded text as conditioning. We can refer to the conditioning space defined by the tokens embedding space of the language model as $\Prompt$ space. 
In other words, $\Prompt$ is the \textit{textual-conditioning space}, where during synthesis, an instance $p \in \Prompt$ (after passing through a text encoder) is injected to all attention layers of a U-net, as illustrated in Figure \ref{fig:unet} (left).
In this paper, we introduce the \textit{Extended Textual Conditioning} space. This space, referred to as $\PromptP$ space, consists of $n$ textual conditions $\{p_1, p_2, ... p_n\}$, where each $p_i$ is injected to the corresponding layer $i$ in the U-net (see Figure \ref{fig:unet} (right)). $\mathcal{P}+$ space is more expressive, disentangled and thus provides better control on the synthesized image. As will be analyzed in this paper, different layers have varying degrees of control over the attributes of the synthesized image. In particular, the coarse layers primarily affect the structure of the image, while the fine layers predominantly influence its appearance.

% This is illustrated in Figure \ref{fig:swap}, where two prompts that correspond to two textual conditioning $p_1$ and $p_2$, are used during inference, one is applied to the \textit{coarse} bottleneck layers, while the other is applied to the \textit{fine} higher-resolution outer layers. As demonstrated in this example, and as will be analyzed in the paper, different layers have varying degrees of control over the attributes of the synthesized image. In particular, the coarse layers primarily affect the structure of the image, while the fine layers predominantly influence its appearance.


The introduction of $\mathcal{P+}$ textual conditioning space opens the door to a particularly exciting advancement in the domain personalization of text-to-image models~\cite{gal2022image,ruiz2022dreambooth}, where the model learns to represent a specific concept described in a few input images as a dedicated token. This learned token can then be employed in a text prompt to produce diverse and novel images related to the concept provided by the user. This technique of learning tokens is referred to as Textual Inversion (TI) \cite{gal2022image}.

In our work, we introduce Extended Textual Inversion (XTI), where we invert the input images into a set of token embeddings, one per layer, namely, inversion into $\PromptP$.
% Namely, once an element $\{p_1, \dots, p_n\}$ of $\PromptP$ is passed to the model, first, we perform the sentences tokenization and each of the tokens is replaced with the correspondent embedding of the look-up table. The $\PromptP$ inversion introduces $n$ new tokens and the new associated look-up table elements.
% More formally, $\PromptP$ is formed by the set of $n$ instances of the original $\mathcal{P}$ spaces. Once the original textual inversion performed the inversion to tokens embeddings that formed the $\mathcal{P}$ embeddings space, we invert 
%, with one token assigned to each level of the Unet-based diffusion model. 
Our findings reveal that the expanded inversion process in $\PromptP$ is not only faster than TI, but also more expressive and precise, owing to the increased number of tokens that provide superior reconstruction capabilities. Remarkably, the improved reconstruction does not compromise editability, as demonstrated by our results. 
% Furthermore, we show that the extended inversion method mitigates the risk of overfitting and does not involve tradeoff between reconstruction and editability. \cqc{Remove this part?}

Furthermore, we leverage the distinctive characteristics of $\PromptP$ to advance the state-of-the-art in object-appearance mixing through text-to-image generation. Specifically, we employ the insertion of inverted tokens of diverse subjects into the different layers to capitalize on the inherent shape-style disentanglement exhibited by these layers. This approach enables us to achieve previously unattainable results as shown in Figure~\ref{fig:teaser_mix}.

We conduct extensive experiments and evaluation to demonstrate the effectiveness of the new space, analyzing its properties and showcasing its power for personalizing text-to-image models, object-style mixing, and more. Project page: \url{https://prompt-plus.github.io}