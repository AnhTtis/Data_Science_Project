\section{Experiments and Evaluation}
\label{sec:exp}

% \kac{Provide a high level overview here:} 
In this section, we conduct an in-depth analysis of the various properties exhibited by the U-net cross-attention layers, and investigate how these characteristics are distributed across the layers. This analysis constitutes a motivation for the effectiveness of our $\PromptP$ space. We then present a comprehensive evaluation of our proposed XTI approach for the personalization task, encompassing quantitative, qualitative, and user study analysis. For more details about the user study setting please refer to the supplementary material.

% First, we expand on our earlier experiment with \texttt{"green cube"} and thoroughly analyze the roles of coarse and fine cross-attention layers in synthesizing the final image. Then, we evaluate our XTI approach against the Textual Inversion (TI) baseline~\cite{gal2022image}.

In all of our experiments we use the Stable Diffusion 1.4 model~\cite{Rombach_2022_CVPR}. It is built on top of CLIP~\cite{radford2021learning}, whose token embedding is represented by a vector with $768$ entries, such that $\Prompt\subseteq\mathbb{R}^{768}$. Stable Diffusion a latent diffusion model whose denoising U-net operates on an autoencoded image latent space. The U-net has four spatial resolution levels - 8x8, 16x16, 32x32, and 64x64. The 16, 32, and 64 resolution levels each have two cross-attention layers on the downward (contracting) path and three cross-attention layers on the upward (expansive) path. Resolution 8 has only 1 cross-attention layer. Thus there are a total of 16 cross-attention layers and 16 conditional token embeddings that comprise our $\PromptP\subseteq\mathbb{R}^{768 \times 16}$ space.

\subsection{$\PromptP$ Analysis}

% \cqc{Moved this to before setup since that's explaining xti and ti hyperparameters}}

% In this section, we examine how the different U-net cross attention layers affect different image properties. It is for this reason that $\PromptP$ is a more precise space that provides more control over image generation.

\paragraph{Cross-Attention Analysis}

We first analyze how the distribution of the cross attention varies across layers. We create a list of 50 objects and 20 appearance adjectives (10 style descriptors and 10 texture descriptors, see supplementary material list). From these lists, we create 2000 ($=50 \times 20 \times 2$) prompts following the patterns \texttt{"appearance object"} and \texttt{"object, appearance"}, and generate 8 images for each prompt using different seeds. We store the cross-attention values for each layer for only the object or appearance token(s), then average over the batch, spatial dimensions, and timesteps to get a ratio of attention on the object token(s) to attention on the appearance token(s). Figure \ref{fig:attn_frac} reports the corresponding ratios. The coarse layers (8, 16) attend proportionally more to the object token and fine layers (32, 64) attend more to the appearance token. This experiment gives us the intuition that coarse layers are more responsible for object shape and structure compared to the fine layers. 

% how cross attention weights for the object and appearance token(s) vary by layer. We generated prompts with a combination of different objects and appearances (eg. \texttt{"fuzzy dog"}, \texttt{"vintage bookcase"}) and found that coarse layers attend relatively more to the object tokens and fine layers attend more to the appearance tokens. Detailed results are in Fig. \ref{fig:attn_frac}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/attn_frac_per_dim_dir.pdf}
    \caption{{\bf Object-appearance attention ratio.} Mean ratio of attention features of the object token(s) and appearance token(s), per cross-attention layer. 
    % Error bars show standard error. The inner coarse layers (8, 16) attend proportionally more to the object token than the fine layers (32, 64). Refer to text for more details. \kac{explain in one sentce what we learn from this figure.}
    % \kac{reduce the aspect ratio of the figure.} \dcc{what is CI? anyway.. it is not clear. I suggest adding then what it implies: "These numbers indicate that the different behavior of the coarse inner layers and fine outer layers". and probably : see more details in the text or supp}
    % Note the middle coarse layers (8, 16) attend proportionally more to the object token than the appearance token compared to the inner/outer fine layers (32, 64). Experiment details in supplementary.
    }
    \label{fig:attn_frac}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/mix_scheme_imgs.pdf}
    \caption{Visualization of mixed conditioning of the U-net cross-attention layers. The rows represent two different starting seeds and the columns represent eight growing subsets of layers, from coarse to fine. We start by conditioning all layers on \texttt{"Blue car, impressionism"} in the left column. As we move right, we gradually condition more layers on \texttt{"Red house, graffiti"}, starting with the innermost coarse layers and then the outer fine layers. Note that \textit{shape} changes (\texttt{"house"}) take place once we condition the coarse layers, but \textit{appearance} (\texttt{"red"}) changes only take place after we condition the fine layers.}
    \label{fig:mix_imgs}
    \vspace{-5pt}
\end{figure*}

\paragraph{Attributes Distribution}
\cq{We further analyze how different cross-attention layers impact different image attributes (shape, color, etc.). To do so, we make use of the CLIP similarity metric \cite{radford2021learning} to quantify the contribution of each layer.}
% extend the earlier \texttt{"green cube"} experiment in Fig. \ref{fig:swap} and show that this intuition can be concretely shown by the CLIP similarity metric \cite{radford2021learning}.

First, we divide the $16$ cross-attention layers into $8$ subsets, starting from the the empty set, followed by the middle coarse layer and growing outwards to include the outer fine layers, and finally the full set (see Figure \ref{fig:mix_bars} for a visual explanation and the supplementary for the detailed list).

Next, we take three lists of \texttt{object}, \texttt{color} and \texttt{style} words and randomly generate prompts with the format \texttt{"color object, style"}. For example, \texttt{"green bicycle, oil painting"} or \texttt{"red house, vector art"}. We then randomly sample $64$ pairs of these prompts. For every pair, we condition the aforementioned subset of layers on one prompt, and condition the complement set on the other prompt. We then generate 8 images with fixed seeds for each prompt-pair and subset. 

 Next, we measure the similarity of the output image to each \texttt{object}, \texttt{color} and \texttt{style} attribute with CLIP similarity. This measures the relative contribution of either conditioning prompt.

Figure \ref{fig:mix_imgs} demonstrates this process for a single prompt pair and two image seeds. We start on the left column with all layers conditioned on the first prompt \texttt{"Blue car, impressionism"}. As we move from left to right, we condition more layers from coarse to fine with the other prompt \texttt{"Red house, graffiti"}. Note that even though we already condition some layers on \texttt{"Red house, graffiti"} in the middle column, the house only starts to appear red towards the end when the fine layers are also conditioned on the same prompt.


\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/prompts_merge_full.pdf}
    \caption{
    Relative CLIP similarities for \texttt{object}, \texttt{color} and \texttt{style} attributes, by subset of U-net layers. {\color{orange}{Orange}} represents the similarity to the first prompt, and {\color{blue}{blue}} represents similarity to the second. As we move from left to right, we gradually grow the subset of layers conditioned with the second prompt from coarse to fine.
    }
    \label{fig:mix_bars}
    % \vspace{-10pt}
\end{figure}

The results averaged over images and prompt pairs are shown in Figure \ref{fig:mix_bars}. We see that at either extreme, the CLIP similarities are dominated by either prompt (represented as orange or blue). However, like the example in Figure \ref{fig:mix_imgs}, different prompt attributes demonstrate different behaviors in between.
We can see that it is sufficient to condition only the coarse layers for \texttt{object}, while \texttt{color} requires that we condition the full set of layers. \texttt{style} lies somewhere in-between.
Thus, coarse layers determine the \texttt{object} shape and structure of the image, and the fine layers determine the \texttt{color} appearance of the image.  \texttt{style} is a more ambiguous descriptor that involves both shape and texture appearance, so every layer has some contribution towards it. 

\subsection{XTI Evaluation}

Next, we evaluate our proposed XTI and compare our results to the original Textual Inversion (TI)~\cite{gal2022image}.
% and to DreamBooth~\cite{ruiz2022dreambooth}
We use a combined dataset of the TI dataset of 9 concepts, and the dataset from \cite{kumari2022customdiffusion} with 6 concepts. For both datasets, each concept has 4-6 original images.

\cq{We focus on TI as a baseline because it is a model-preserving inversion approach that does not fine tune the model weights. These fine-tuning approaches like DreamBooth~\cite{ruiz2022dreambooth} and Custom Diffusion~\cite{kumari2022customdiffusion} explicitly embed the concept within the model's output domain and thus have excellent reconstruction. However, they have several disadvantages. Firstly, they risk destroying the model's existing prior (catastrophic forgetting). Secondly, they have several orders of magnitude more parameters. Recent work with Low-Rank Adaptation (LoRA)~\cite{lora} reduces the number of fine-tuned parameters to a fraction, but this is still about $\sim100$x more than XTI. Lastly, they are difficult to scale to multiple concepts since the fine-tuned parameters for each concept have to be merged.}
% Optimization in TI and XTI are done independently for each concept. \cqc{Not sure whether to include this last point}}
% Nevertheless, we show DreamBooth as an alternative baseline for quantitative metrics.

% \ka{In our inversion evaluation we avoid comparing to model-destructive approaches that modify the weights (fine-tuning) of the model for each concept / subject~\cite{ruiz2022dreambooth,kumari2022customdiffusion}.} 
% \kac{elaborate about the obvious advantages of such models, and that we want to focus on inversion.}

\subsubsection{Setup}

% \subsection{Coarse vs Fine U-net Layers}
% As explained earlier, the coarse U-net layers (8, 16) model the image content and structure at a high level of abstraction, while the fine layers (32, 64) model details like style and texture. As a corollary, we show that coarse layers attend more to \textit{structure} tokens while fine layers attend more to \textit{appearance} tokens.

% We chose a list of 50 objects and 20 appearance adjectives (10 style descriptors and 10 texture descriptors) (see supplementary). From these lists, we created a total of 2000 ($50 * 20 * 2$) prompts in the form of "\textit{adjective} \textit{object}" and "\textit{object}, \textit{adjective}" and generated 8 images for each prompt. We stored the cross-attention values for each layer for only the object or appearance token(s), then averaged over the token(s) and batch, spatial and timestep dimensions to get a ratio of attention on the object token(s) to attention on the appearance token(s). Results are in Fig. \ref{fig:attn_frac}. As expected, coarse layers attend proportionally more to the object token(s) and fine layers attend more to the appearance token(s).

We followed the batch size of 8 and performed 5000 optimization steps for Textual Inversion, consistent with the original paper. However, we opted to use a reduced learning rate of 0.005 without scaling for optimization, as opposed to the Latent Diffusion Model from \cite{Rombach_2022_CVPR} used in the original paper. In our experiments, Stable Diffusion with this learning rate worked better. For our proposed XTI, we used the same hyperparameters as for Textual Inversion, except for the number of optimization steps which we reduced to 500, {\bf resulting in significantly faster convergence}. Both Textual Inversion and XTI shared all other hyperparameters, including the placeholder training prompts. On 2$\times$Nvidia A100 GPUs, the whole optimization takes $\sim$15 minutes for XTI compared to $\sim$80 minutes for TI.

% For DreamBooth, we use a learning rate of 5e-6, batch size of 4, and $400$ steps, taken from the Hugging Face implementation \cite{dreambooth_training}. We only optimize the U-net weights and do not use prior preservation.

% On 2 A100 GPUs, the whole optimization takes $\sim$15 minutes for XTI, compared to  $\sim$80 minutes for TI and $\sim$10 minutes for DreamBooth. XTI is slightly slower than DreamBooth from having to backpropagate through the text encoder despite having roughly equal number of training steps. 

\subsubsection{Quantitative Evaluation}

 Following \cite{gal2022image}, to evaluate the editability quality of the inversions, we use the average cosine similarity between CLIP embeddings of the generated images and the prompts used to generate the images (\textit{Text Similarity}). To measure the distortion of the generated images from the original concept (\textit{Subject Similarity}), we use the average pairwise cosine similarity between ViT-S/16 DINO~\cite{DINO} embeddings of the generated images and the original dataset images. Compared to CLIP which is trained with supervised class labels, \cite{ruiz2022dreambooth} argued that DINO embeddings better capture differences between images of the same class due to its self-supervised training.
 All the methods reported in Figure \ref{fig:scatter} are evaluated over 15 subjects from \cite{gal2022image} and \cite{kumari2022customdiffusion}, each generated with 14 different prompts templates that place the concept in novel context (e.g. \texttt"A photograph of \{\} in the jungles", see Section \ref{sec:metric_prompts} in the supplementary for details). For each test concept and prompt we generated 32 images, making a total of $15 \times 14 \times 32 = 6720$ images. We fix the generation seed across different methods.

In Figure \ref{fig:scatter} we report the evaluation of the proposed Extended Textual Inversion (XTI). Among Textual Inversion \cite{gal2022image}, as for comparison we also include DreamBooth~\cite{ruiz2022dreambooth} which is not a model-preserving method. Notably, XTI outperforms TI at both subject and text similarity despite using 10x fewer training steps.
We also report TI using 500 optimization steps, which is the number of steps we use for XTI. This improves the Text Similarity because fewer number of optimization steps prevents the optimized token embedding from being out of distribution. However, it degrades reconstruction as measured by Subject Similarity. 

We also report the inversion in a data-hungry setup, where the subject is represented with only a single image. Notably, even in this extreme setting the proposed XTI performs better than multi-image TI in terms of subject similarity (see Section \ref{sec:1img} for details).

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/similarity_plot.pdf}
    \caption{{\bf Comparison of Textual Similarity and Subject Similariy. } Textual Inversion (TI)~\cite{gal2022image}, Extended Textual Inversion (XTI), DreamBooth~\cite{ruiz2022dreambooth}. We also evaluate the metrics for both multi-image and single-image inversion setups. For the latter, a subject is represented by a single image. The ``Reference" label corresponds to images containing the subject images themselves, while the "Textual description" label used the given text description but replaced the explicit subject's description (e.g. "a colorful teapot"). The standard error is visualized in the bars.}
    \label{fig:scatter}
    % \vspace{-10pt}
\end{figure}


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/samples.pdf}
    \caption{{\bf Textual Inversion (TI) vs. Extended Textual Inversion (XTI).}  \textit{Column 1:} 
    % \dc{These are no the inversions but the reconstructions! right?}
    Original concepts. \textit{Column 2:} TI results. \textit{Column 3:} XTI results. It can be seen that XTI exhibits superior subject and prompt fidelity, as corroborated by the results of our user study.}
    \label{fig:xti_samples}
\end{figure*}

\subsubsection{Human Evaluation}

Figure \ref{fig:xti_samples} shows a visual comparison of our XTI approach with the original TI. Our method demonstrates less distortion to the original concept \textit{and} to the target prompt.

To assess the efficacy of our proposed method from a human perspective, we conducted a user study. The study, summarized in Table \ref{table:qualitative_results}, asked participants to evaluate both Textual Inversion (TI) and Extended Textual Inversion (XTI) based on their fidelity to the original subject and the given prompt. The results show a clear preference for XTI for both subject and text fidelity.

\begin{table}[ht]
    \centering
    \begin{tabular}{lrr}
        \textbf{Method} & \textbf{Subject Fidelity} & \textbf{Text Fidelity} \\ \hline
        Textual Inversion & 24\% & 27\% \\ 
        XTI (Ours) & \textbf{76\%} & \textbf{73\%} \\ 
        ~ & ~ & ~ \\ 
    \end{tabular}
    \caption{User study preferences for subject and text fidelity for TI and XTI. See supplementary material for more details.}
    \label{table:qualitative_results}
\end{table}

\subsection{Single Image Inversion}
\label{sec:1img}

The Extended Textual Inversion also appears to be very effective in a data-hungry setup, when a target subject is represented with a single image. As for single image training for all the runs we reduce learning rate to $0.001$ to better prevent overfitting. Figure \ref{fig:single_image} provides visual comparison of TI and XTI inversions in the this single image setting. We omit single-image DreamBooth results from Figure~\ref{fig:single_image} and \ref{fig:scatter} due to its comparatively poor performance, namely Text Similarity of 0.25 and Subject Similarity of 0.40. In particular, we found DreamBooth in this single-image setting to be prone to overfitting and difficult to optimize.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/samples_1img.pdf}
    \caption{\textbf{Single Image Textual Inversion (TI) vs Single Image Extended Textual Inversion (XTI)}. \textit{Column 1}: Original concepts. \textit{Column 2}: TI results. \textit{Column 3}: XTI results. It can be seen that XTI exhibits superior subject and prompt fidelity and produce pleasable results even when trained on a single image.}
    \label{fig:single_image}
\end{figure*}


\subsection{Embedding Density} \label{sec:kde}

As the textual embeddings inverted with XTI have better editability properties compared to the original TI, this suggests that these tokens are better aligned with the original tokenizer look-up table embedding, \ka{which represents the manifold of natural language embedding}. To quantify this intuition, we evaluate the density of the newly-optimized tokens with respect to the original ``natural" tokens look-up table embeddings. We perform kernel-based density estimation (KDE) in the look-up table tokens embeddings space \cq{independently for each dimension}. Let us define $\mathcal{E}$ to be the set of all original tokens look-up table embeddings, before adding the extra optimized token(s). Assuming that $\mathcal{E}$ is sampled from some continuous distribution, one can define the approximation of its density function at a point $x$ as:

\begin{equation}
\log p_{\mathcal{E}}(x) \approx \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} K(x - e), \label{eq:kde}    
\end{equation}
where $K$ is the Gaussian kernel density function \cite{parzen1962estimation, rosenblatt1956remarks}. As for the embeddings optimized with original TI, this quantity always appears to be significantly smaller compared to the densities at the original embeddings $\mathcal{E}$. Figure \ref{fig:embed_log_density} illustrates the original tokens density distribution, and the textual inversion tokens densities. This demonstrates that the proposed approach provides embeddings that are closer to the original distribution, enabling a more natural reconstruction and better editability.

% \cqc{Do we explain why here? It is because of fewer training steps, more expressive space ...?}
% operates and reconstruct a subject in a more natural way, that preserves the desired editability.

% \subsection{Inversion Regularization}
% \kac{Add motivation - TI token grabs high attention, and it overrides other words in the sentence / ``composabiltiy". Then we propose a solution that doesn't work in TI but works in XTI. Provide a good explanation about why it doesn't work with TI - in TI it's editable but it comes on the reconstruction quality, unlike XTI.}
% We found out that in the original textual inversion algorithm, the objective always leads the optimized token embedding to be out of the original tokenizer token embeddings. In particular, this commonly leads to quality degradation, once an image is synthesized with the optimized inversion token in a complex sentence. It is common that the optimized token takes too much attention and as the result, the synthesized image lacks some of details provided in a prompt (see Fig.\ref{fig:TODO}). For further exploration of this effect, we perform the kernel-based density estimation (KDE) in the look-up table tokens embeddings space. Let us define $\mathcal{E}$ to be the set of all original tokens look-up table embeddings, before adding the extra optimized token. In assumption that $\mathcal{E}$ is sampled from some continuous distribution, one can define the approximation of its density function at a point $x$ as:

% \begin{equation}
% \log p_{\mathcal{E}}(x) \approx \frac{1}{|\mathcal{E}|} \sum_{e \in \mathcal{E}} K(x - e), \label{eq:kde}    
% \end{equation}
% where $K$ is the Gaussian kernel density function (see e.g., \cite{TODO}). As for the embeddings optimized with the original textual inversion algorithm, this quantity always appears to be significantly smaller compared to the densities at the original embeddings $\mathcal{E}$. Fig. \ref{fig:TODO} illustrates the original tokens density distribution, and the textual inversion tokens densities. Once being optimized with the density $\log p_\mathcal{E}$ maximization penalty, original textual inversion notably suffers from the inversion quality degradation. In particular, even with a small scale this regularization induces significant inversion simplification, as shown in Fig.\ref{TODO}. Surprisingly, the opposite happens for the proposed XTI: in that case the regularization notably improves the alignment of images synthesized with the inverted tokens with complex textual descriptions, while not harming the inversion quality. This regularization enforces the added tokens embeddings to stay on the "natural" tokens embeddings manifold. Thus, our final XTI objective takes the form
% \begin{equation}
% \mathcal{L}_{\mathrm{XTI}} = \mathcal{L}_\mathrm{TI} - \lambda \cdot \sum_{i=1}^l \log p_{\mathcal{E}}(e_i)
% \end{equation}
% where $\lambda$ is the regularization strength.



\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/embed_log_density.pdf}
    \caption{Estimated log-density of the original look-up table token embeddings ({\color{gray}{gray}}), embeddings optimized with textual inversion ({\color{blue}{blue}}), and embeddings optimized with XTI ({\color{orange}{orange}}). Our method demonstrates a more regular representation which is closer to the manifold of natural words.}
    \label{fig:embed_log_density}
    % \vspace{-10pt}
\end{figure}


\section{Style Mixing Application}

% \subsection{Style Mixing}

As we showed earlier, different layers of the denoising U-net are responsible for different aspects of a synthesized image. This allows us to combine the \textit{shape} of one inverted concept with the \textit{appearance} of another inverted concept. We call this Style Mixing. 

Let us consider two independent XTI inversions of two different concepts. We can combine the inversions by passing tokens from different subject to different layers, as illustrated in Figure \ref{fig:teaser_mix}. This mixed conditioning produces an image with a coarse geometry from the first concept, and appearance from the second concept. Formally, we are given two extended prompts: $\{\texttt{p}_n, \dots, \texttt{p}_n\}$, and $\{\texttt{q}_1, \dots, \texttt{q}_n\}$. We form a new extended prompt $\{\texttt{p}_1, \dots, \texttt{p}_k, \texttt{q}_{k+1}, \dots, \texttt{q}_K, \texttt{p}_{K+1}, \dots, \texttt{q}_n\}$ with the separators $1 \leq k < K \leq n$. 
% we use one prompt at the coarse cross-attention layers that more responsible for structure, and another prompt on the fine layers that are more responsible for style. This mixed conditioning produces an image with a coarse geometry aligned with the first prompt, and appearance aligned with the second prompt. 

% We further note that our aforementioned Embedding Density metric (see Section \ref{sec:kde}) can be added as regularization to improve quality of inversions for Style Mixing. By adding the negative estimated embedding density as a small extra loss, we improve mixing capability. More details are provided as supplementary material.

Our observations indicate that the optimization of XTI with an additional density regularization loss term indicated in \ref{eq:kde} enhances its ability to mix objects and styles, without compromising the quality of the inversion output. More details are provided as supplementary material.

% We further note that if we add our additional embedding density (\ref{eq:kde}) as a regularization loss term, Style Mixing is improved while preserving inversion quality. More details are provided as supplementary material.

% As for the Style Mixing application, we noticed that once the XTI is optimized with additional density regularization loss term \ref{eq:kde}, this also improves the mixing capability, while preserving the overall inversion quality. Thus, in this particular case we add the extra loss term $-\lambda \cdot \sum_{i=1}^l \log p_{\mathcal{E}}(e_i)$ where $\lambda = 0.002$ is a small regularization scale. This loss encourages the added look-up table embeddings to be even more regular, shifting the XTI-distribution from \ref{fig:embed_log_density} closer to the original tokens distribution. We argue, that this is helpful for the mixture application as in this case the generation is conditioned to two tokens and it's particularly important to make them interact as natural as possible. Meanwhile, once optimized with this regularization term, the original textual inversion suffers from the inversion quality degradation. In particular, even with a small scale $\lambda$ this regularization induces significant inversion simplification.

Figure \ref{fig:style_mix} demonstrates combining the \texttt{"skull mug"} and \texttt{"cat statue"} concepts from \cite{gal2022image}. Different rows of the plot correspond to different blending ranges $k, K$. From top to bottom, we gradually expand it from the middle coarse layer to all the cross-attention layers. This range $(k, K)$ gives the control over the amount of details we want to bring from one inversion to another. By varying $k$ and $K$, we can adjust the contributions of the second subject appearance to the first. 
% At the top row on Figure \ref{fig:style_mix} we also demonstrate appearance transfer, based on the textual description of the prompts, generated with the original Textual Inversion: we combine two independently optimized tokens in a single sentence which enforces geometry of the first token in the appearance of the second. 

Figure \ref{fig:mix_samples} shows a variety of examples generated with this method. Both shape and appearance are inherited remarkably well. More illustrations are provided in supplementary.

Figure \ref{fig:mix_baselines} provides a qualitative comparison between our XTI-based style mixing and baselines of TI~\cite{gal2022image} and DreamBooth~\cite{ruiz2022dreambooth}. The results demonstrate that our approach outperforms the baselines significantly, both in terms of preserving the sources' fidelity and disentangling the attributes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/style_mix.pdf}
    \caption{\textbf{Style Mixing in $\PromptP$}. Rows are generated by varying the degree of mixing by adjusting the proportion of layers conditioned on either of the two $\PromptP$ inversions.
    }
    \label{fig:style_mix}
    % \vspace{20pt}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{figures/mix_samples_v2.pdf}
    \caption{\textbf{More Style Mixing examples}. \textit{Top row}: Shape source concepts. \textit{Left column}: Appearance source concepts.}
    \label{fig:mix_samples}
    % \vspace{-10pt}
\end{figure}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/mix_baseline.pdf}
    \caption{\textbf{Style Mixing comparison.} We compare against Textual Inversion \cite{gal2022image} and Dreambooth \cite{ruiz2022dreambooth} baselines. For TI we independently invert target subject and target appearance, and generate the images with the sentence "\texttt{\textbf{<object>} that looks like \textbf{<appearance>}}". Prompt variations did not make any remarkable improvements. The style source concept was inverted with style prompts (see \cite{gal2022image} for details). DreamBooth was trained with a pair of subjects. Our proposed Extended Textual Inversion clearly outperforms both baselines.}
    \label{fig:mix_baselines}
\end{figure*}