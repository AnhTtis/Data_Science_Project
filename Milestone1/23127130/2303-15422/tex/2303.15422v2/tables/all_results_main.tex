\setlength{\tabcolsep}{3.5pt}
\begin{table*}[]
 \centering
 % \resizebox{\columnwidth}{!}{%
 \resizebox{\linewidth}{!}{
 \begin{tabular}{c | c | c | c c c | c c | c c | c c }
 \hline
 \multirow{2}{*}{\textbf{ID}} & \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{\#KP}} & \multicolumn{3}{c|}{\textbf{Saliency}} & \multicolumn{2}{c|}{\textbf{Faithfulness}} & \multicolumn{2}{c|}{\textbf{Diversity}} & \multicolumn{2}{c}{\textbf{Utility}} \\
& & & $SemP \uparrow$ & $SemR \uparrow$ & $SemF1 \uparrow$ & $U \uparrow$ & $G \uparrow$& $dup \downarrow$ &$emb\_sim \downarrow$ & $RR@5 \uparrow$ & $Recall@5 \uparrow$ \\
 \hline
 \multicolumn{8}{l}{\textbf{\textit{KP20k}}} \\
 \hdashline
% \textbf{M0}  & Human References          & 5.3  & n/a & n/a & n/a & 0.697 & 3.86 & \textbf{0.073}$^\dagger$& 0.133 & 0.824                   & 0.722                   \\
\textbf{M1}  & TF-IDF                    & 10.0 & 0.431                   & 0.524                   & 0.463                   & 0.696 & 4.52 & 0.396 & 0.161 & 0.849                   & 0.758                   \\
\textbf{M2}  & TextRank                  & 10.0 & 0.432                   & 0.518                   & 0.460                   & 0.737 & 4.57 & 0.391 & 0.180 & 0.840                   & 0.748                   \\
\textbf{M3}  & MultipartiteRank          & 10.0 & 0.364                   & 0.548                   & 0.429                   & 0.712 & 4.35 & 0.148 & \underline{0.092} & 0.839                   & 0.742                   \\
\textbf{M4}  & Kea                       & 10.0 & 0.452                   & 0.533                   & 0.479                   & 0.715 & 4.51 & 0.445 & 0.175 & \textbf{0.855 }                  & \underline{0.765 }                  \\
\textbf{M5}  & BERT+CRF                  & 3.6  & \textbf{0.675}$^\dagger$                   & 0.447                   & 0.515                   & \textbf{0.801}$^\dagger$ & 4.63 & 0.137 & 0.399 & 0.832                   & 0.737                   \\
\textbf{M6}  & HyperMatch                & 10.0 & 0.505                   & \textbf{0.631}$^\dagger$                   & 0.549                   & 0.772 & 4.14 & 0.297 & 0.233 & \underline{0.854}                   & \textbf{0.767}$^\dagger$                   \\
\textbf{M7}  & CatSeq                    & 7.2  & 0.596                   & 0.531                   & 0.548                   & 0.774 & 4.51 & 0.370 & 0.310 & 0.838                   & 0.745                   \\
\textbf{M8}  & CatSeqTG+2RF1             & 7.9  & 0.550                   & 0.579                   & 0.553                   & 0.734 & 3.98 & 0.355 & 0.251 & 0.846                   & 0.754                   \\
\textbf{M9}  & ExHiRD-h                  & 5.5  & 0.587                   & 0.559                   & 0.562                   & 0.781 & 4.50 & 0.214 & 0.195 & n/a & n/a \\
\textbf{M10} & SEG-Net                   & 11.0 & 0.560                   & 0.596                   & 0.565                   & 0.757 & 4.50 & 0.260 & 0.177 & 0.853                   & 0.761                   \\
\textbf{M11} & Transformer               & 8.2  & 0.558                   & 0.593                   & 0.562                   & 0.747 & 4.37 & 0.289 & 0.239 & 0.835                   & 0.739                   \\
\textbf{M12} & SetTrans                  & 7.7  & 0.570                   & \underline{0.623}                   & \underline{0.583}                   & 0.766 & 4.38 & 0.308 & 0.203 & \textbf{0.855}                   & \underline{0.765}                   \\
\textbf{M13} & SciBERT-G                 & 6.1  & 0.587                   & 0.590                   & 0.577                   & 0.762 & 4.40 & 0.152 & 0.177 & 0.835                   & 0.739                   \\
\textbf{M14} & BART-large                & 6.6  & 0.567                   & 0.607                   & 0.574                   & 0.759 & 4.25 & 0.144 & 0.168 & 0.834                   & 0.738                   \\
\textbf{M15} & KeyBART                   & 6.0  & 0.580                   & 0.599                   & 0.577                   & 0.765 & 4.32 & 0.130 & 0.163 & 0.841                   & 0.741                   \\
\textbf{M16} & SciBART-large+OAGKX       & 6.1  & \underline{0.601}                   & \underline{0.623}                   & \textbf{0.601}$^\dagger$                   & 0.766 & 4.44 & 0.129 & 0.158 & 0.836                   & 0.743                   \\
\textbf{M17} & text-davinci-003 (0-shot) & 9.5  & 0.433                   & 0.616                   & 0.498                   & 0.777 & \underline{4.72} & 0.143 & 0.110 & 0.850                   & 0.758                   \\
\textbf{M18} & text-davinci-003 (5-shot) & 6.5  & 0.503                   & 0.599                   & 0.535                   & \underline{0.790} & \textbf{4.74} & \underline{0.084} & 0.113 & 0.849                   & 0.761                   \\
\textbf{M19} & Amazon Comprehend         & 10.0 & 0.224                   & 0.324                   & 0.259                   & 0.693 & 3.91 & 0.436 & 0.103 & 0.789                   & 0.683                   \\
\textbf{M20} & Azure Cognitive Services  & 10.0 & 0.387                   & 0.536                   & 0.440                   & 0.778 & 4.53 & 0.198 & \textbf{0.068}$^\dagger$ & 0.834                   & 0.736 \\
 \hline
 \multicolumn{8}{l}{\textbf{\textit{KPTimes}}} \\
 \hdashline
% \textbf{M0}  & Human References          & 5.0                     & n/a & n/a & n/a & 0.701                   & 3.48                    & 0.069                   & 0.203                   & 0.324                   & 0.230                   \\
\textbf{M1}  & TF-IDF                    & 10.0                    & 0.403                   & 0.557                   & 0.461                   & 0.658                   & 3.98                    & 0.175                   & 0.163                   & 0.342                   & 0.253                   \\
\textbf{M2}  & TextRank                  & 10.0                    & 0.457                   & 0.532                   & 0.483                   & 0.667                   & 4.03                    & 0.371                   & 0.246                   & \textbf{0.365}$^\dagger$                   & \textbf{0.269}$^\dagger$                   \\
\textbf{M3}  & MultipartiteRank          & 10.0                    & 0.410                   & 0.575                   & 0.472                   & 0.670                   & 4.05                    & 0.123                   & 0.150                   & 0.342                   & 0.252                   \\
\textbf{M4}  & Kea                       & 10.0                    & 0.427                   & 0.575                   & 0.484                   & 0.699                   & \underline{4.07}                    & 0.219                   & 0.175                   & \underline{0.357}                   & \underline{0.266}                   \\
\textbf{M5}  & BERT+CRF                  & 2.3                     & 0.741                   & 0.459                   & 0.550                   & \textbf{0.769}$^\dagger$                   & 3.90                    & 0.072                   & 0.556                   & 0.315                   & 0.225                   \\
\textbf{M6}  & HyperMatch                & 10.0                    & 0.493                   & 0.655                   & 0.554                   & 0.703                   & 3.68                    & 0.124                   & 0.271                   & 0.341                   & 0.250                   \\
\textbf{M7}  & CatSeq                    & 5.9                     & 0.723                   & 0.714                   & 0.708                   & 0.683                   & 3.34                    & 0.191                   & 0.257                   & 0.312                   & 0.246                   \\
\textbf{M8}  & CatSeqTG+2RF1             & n/a & n/a & n/a & n/a & n/a & n/a & n/a & n/a & n/a & n/a \\
\textbf{M9}  & ExHiRD-h                  & 5.8                     & 0.725                   & 0.713                   & 0.709                   & 0.680                   & 3.30                    & 0.149                   & 0.238                   & 0.310                   & 0.241                   \\
\textbf{M10} & SEG-Net                   & n/a & n/a & n/a & n/a & n/a & n/a & n/a & n/a & n/a & n/a \\
\textbf{M11} & Transformer               & 5.7                     & 0.720                   & 0.736                   & 0.717                   & 0.658                   & 3.10                    & 0.133                   & 0.231                   & 0.316                   & 0.227                   \\
\textbf{M12} & SetTrans                  & 8.4                     & 0.662                   & \underline{0.801}                   & 0.716                   & 0.645                   & 3.00                    & 0.210                   & 0.232                   & 0.324                   & 0.232                   \\
\textbf{M13} & SciBERT-G                 & 4.5                     & \textbf{0.784}                   & 0.737                   & 0.749                   & 0.716                   & 3.51                    & \underline{0.064}                   & 0.208                   & 0.323                   & 0.231                   \\
\textbf{M14} & BART-large                & 5.4                     & 0.768                   & 0.796                   & \textbf{0.770}$^\dagger$                   & 0.706                   & 3.43                    & 0.074                   & 0.203                   & 0.326                   & 0.231                   \\
\textbf{M15} & KeyBART                   & 5.9                     & 0.751                   & \textbf{0.807}$^\dagger$                   & \underline{0.766}                   & 0.696                   & 3.35                    & 0.086                   & 0.206                   & 0.325                   & 0.229                   \\
\textbf{M16} & SciBART-large+OAGKX       & 4.8                     & \underline{0.782}                   & 0.766                   & 0.763                   & 0.718                   & 3.52                    & 0.069                   & 0.203                   & 0.323                   & 0.232                   \\
\textbf{M17} & text-davinci-003 (0-shot) & 14.1                    & 0.383                   & 0.620                   & 0.467                   & 0.690                   & 3.98                    & 0.082                   & \underline{0.137}                   & 0.352                   & 0.258                   \\
\textbf{M18} & text-davinci-003 (5-shot) & 6.7                     & 0.549                   & 0.643                   & 0.582                   & 0.725                   & \textbf{4.28}$^\dagger$                   & \textbf{0.044}$^\dagger$                   & 0.188                   & 0.345                   & 0.253                   \\
\textbf{M19} & Amazon Comprehend         & 10.0                    & 0.258                   & 0.347                   & 0.292                   & 0.617                   & 3.17                    & 0.291                   & 0.203                   & 0.298                   & 0.215                   \\
\textbf{M20} & Azure Cognitive Services  & 10.0                    & 0.312                   & 0.425                   & 0.355                   & \underline{0.728}                   & 3.71                    & 0.140                   & \textbf{0.133}$^\dagger$                   & 0.326                   & 0.232                 \\
 \hline
 \end{tabular}
 }
 % \vspace{-2mm}
 \caption{Result for all aspects for human references and five model groups. \#KP = Number of keyphrases, $U$ = UniEval, $G$ = ChatGPT, $dup$ = $dup\_token\_ratio$. Due to cost constraints, the scores reported in $G$ columns are measured with the first 500 documents. We use $\uparrow$ for the higher the better and $\downarrow$ for the reverse. The best entry is boldfaced and the second best is underlined. $^\dagger$ indicates statistically significantly better than the second best with $p < 0.01$ via a paired t-test.}
 \label{tab:all-results-main}
 % \vspace{-2mm}
\end{table*}