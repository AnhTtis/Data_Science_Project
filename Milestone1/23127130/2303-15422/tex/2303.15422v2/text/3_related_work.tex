\section{Related Work}
% This section situates the paper in the KPE, KPG, and the keyphrase evaluation literature. 

In \cref{related-work-keyphrase-systems}, we introduce related works in KPE and KPG, among which we select 20 representative systems to evaluate in \cref{re-eval}. Then, \cref{related-work-keyphrase-evaluation} introduces the advances in evaluating keyphrase systems. % Finally, \cref{aspects-scope} motivates the evaluation aspects considered by this paper.

\subsection{Keyphrase Systems}
\label{related-work-keyphrase-systems}

\paragraph{Keyphrase extraction systems}
Traditionally, KPE is done in an unsupervised manner where noun phrase candidates are ranked with heuristics \citep{hulth-2003-improved,mihalcea-tarau-2004-textrank}. Other supervised methods use feature-based ranking \citep{witten1999kea}, sequence labeling \citep{zhang-etal-2016-keyphrase}, and task-specific objectives with pre-trained language models \citep{song-etal-2021-importance,song-etal-2022-hyperbolic}. We evaluate 6 KPE methods: [\textbf{M1}] TF-IDF \citep{tf-idf}, [\textbf{M2}] TextRank \citep{mihalcea-tarau-2004-textrank}, [\textbf{M3}] MultipartiteRank \citep{boudin-2018-unsupervised}, [\textbf{M4}] Kea \citep{witten1999kea}, [\textbf{M5}] BERT+CRF \citep{2212.10233}, and [\textbf{M6}] HyperMatch \citep{song-etal-2022-hyperbolic}.


\paragraph{Keyphrase generation systems}
KPG models are usually trained with three types of supervised objectives: \textit{One2One} -- generating one keyphrase given $\mathcal{X}$ \citep{meng-etal-2017-deep}; \textit{One2Seq} -- generating a sequence of keyphrases given $\mathcal{X}$ \citep{yuan-etal-2020-one}; or \textit{One2Set} -- generating a set of keyphrases given $\mathcal{X}$ \citep{ye-etal-2021-one2set}. Various approaches have been developed, including incorporating linguistic constraints \citep{zhao-zhang-2019-incorporating}, semi-supervised learning \citep{ye-wang-2018-semi}, hierarchical modeling of phrases and words \citep{chen-etal-2020-exclusive}, reinforcement learning \citep{chan-etal-2019-neural}, unifying KPE and KPG \citep{chen-etal-2019-integrated,ahmad-etal-2021-select}, and using pre-trained language models \citep{kulkarni-etal-2022-learning}. We study 10 models: [\textbf{M7}] CatSeq \citep{yuan-etal-2020-one}, [\textbf{M8}] CatSeqTG+2RF1 \citep{chan-etal-2019-neural}, [\textbf{M9}] ExHiRD-h \citep{chen-etal-2020-exclusive}, [\textbf{M10}] SEG-Net \citep{ahmad-etal-2021-select}, [\textbf{M11}] Transformer \citep{ye-etal-2021-one2set}, [\textbf{M12}] SetTrans \citep{ye-etal-2021-one2set}, [\textbf{M13}] SciBERT-G \citep{2212.10233}, [\textbf{M14}] BART-large \citep{2212.10233}, [\textbf{M15}] KeyBART \citep{kulkarni-etal-2022-learning}, and [\textbf{M16}] SciBART-large fine-tuned on OAGKX \citep{2212.10233}.

\paragraph{Large language models and APIs}
Recent work has shown the ability of large language models to efficiently learn new tasks via in-context learning \citep{brown2020language}. For KPG, we benchmark GPT-3.5 \citep{ouyang2022training}, denoted as [\textbf{M17}] zero-shot prompting GPT-3.5 and [\textbf{M18}] prompting GPT-3.5 with five random examples from the train set\footnote{We use OpenAI's \texttt{text-davinci-003} via API.}. Inspired by \citet{ribeiro-etal-2020-beyond}, we also study two commercial keyphrase extraction APIs: [\textbf{M19}] the \href{https://docs.aws.amazon.com/comprehend/latest/dg/how-key-phrases.html}{Amazon Comprehend API} and [\textbf{M20}] the \href{https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/key-phrase-extraction/quickstart?pivots=programming-language-python}{Azure Cognitive Services API}. 

\paragraph{Datasets} We use two popular keyphrase datasets: KP20k \citep{meng-etal-2017-deep} with over 500k Computer Science papers from online digital libraries and KPTimes \citep{gallina-etal-2019-kptimes} with over 250k news documents collected from New York Times. KP20k's keyphrase annotations are obtained from the metadata of the papers, while KPTimes's keyphrases are assigned by expert editors. We evaluate on KP20k's 20k test set and on KPTimes' 10k in-distribution test set. 

\subsection{Keyphrase Evaluation}
\label{related-work-keyphrase-evaluation}

% \wade{Compress this section.}

% This section introduces previous advances in the evaluation of keyphrase systems. 

\paragraph{Reference-based evaluation} The major metrics for evaluating KPE and KPG systems are precision, recall, and F1 based on exact-match with human-specified references, computed after stemming the predictions and the references \citep{mihalcea-tarau-2004-textrank, meng-etal-2017-deep, yuan-etal-2020-one}. This method indiscriminately penalizes unmatched predictions even though many of them have synonyms and child or parent concepts in the reference. 

Later works attempt to solve the problem by \textit{relaxing the matching criterion}. \citet{zesch-gurevych-2009-approximate} propose to use R-precision with approximate matching, which tolerates predictions that are substrings of some reference and vice versa. \citet{kim-etal-2010-evaluating} employ n-gram matching metrics such as BLEU \citep{papineni-etal-2002-bleu} and Rouge \citep{lin-2004-rouge}. \citet{chan-etal-2019-neural} expand the labels with name variations. \citet{luo-etal-2021-keyphrase-generation} propose a fine-grained score that combines token-level matching, edit distance, and duplication penalty. \citet{koto-etal-2022-lipkey} and \citet{glazkova2022applying} use the semantic-based BertScore \citep{bert-score}. % However, the token-level matching formulation requires concatenating phrases in $\mathcal{P}$ and $\mathcal{Y}$ to form the hypothesis and the reference string, which fails to capture the semantics of individual phrases.

Meanwhile, ranking-based metrics such as Mean Reciprocal Rank, mean Averaged Precision, and Normalized Discounted Cumulative Gain are introduced to evaluate the ranking strategy in KPE systems \citep{florescu-caragea-2017-positionrank, boudin-2018-unsupervised, kim-etal-2021-structure}. These metrics also use exact matching to references during their evaluation. 

\paragraph{Reference-free evaluation} A less-adopted approach is directly evaluating $\mathcal{P}$ without $\mathcal{Y}$. Early studies use small-scaled human evaluation \citep{barker2000using, matsuo2004keyword} and report descriptive statistics such as the number of generated keyphrases and their average lengths. Later work evaluates keyphrases' utility in downstream applications such as retrieval \citep{Bracewell2005MultilingualSD, boudin-gallina-2021-redefining} or summarization \citep{litvak-last-2008-graph}. \citet{bahuleyan-el-asri-2020-diverse} conduct reference-free evaluation of the diversity of predicted keyphrases. 

\paragraph{Meta-evaluation} Evaluating whether metrics reflect human preferences is referred to as meta-evaluation. For keyphrase metrics, meta-evaluation studies have been of a limited scope. \citet{kim-etal-2010-evaluating} compared between BLEU, NIST, METEOR, Rouge, and R-precision, and concluded that R-precision has the highest Spearman correlation with human judgments. \citet{bougouin2016termith} annotated a meta-evaluation corpus with 400 documents in French, evaluating 3 keyphrase models in terms of  "appropriateness" and "silence", corresponding to "precision" and "1 - recall" in a broader sense.


% \subsection{What aspects should be evaluated?}
\subsection{Discussion}

Although a range of metrics have been proposed with strong motivations, several challenges still remain, which this paper aims to address:
\begin{itemize}
    \item Despite the claimed improvements over exact matching, existing reference-based metrics have not been thoroughly compared to exact matching via meta-evaluation.
    \item Existing metrics cannot sufficiently capture \textit{phrase semantics} for keyphrase evaluation, leading to suboptimal phrase matching metrics such as those for saliency and diversity.
    \item There lacks a \textit{systematic view} on keyphrase evaluation: what aspects are essential? What are the desiderata for evaluating them?
    \item Implementation-wise, there lacks a \textit{reliable and extensible evaluation framework}. As a result, a comprehensive comparison on different models' relative strengths and weaknesses has not been established.
\end{itemize}


To address these issues, we propose \textsc{KPEval}, a holistic evaluation framework for four crucial aspects of keyphrase systems (\cref{section-framework}). We carefully design  semantic-based metrics to operationalize these aspects, and conduct rigorous meta-evaluation studies to confirm their effectiveness (\cref{ref-based-eval}, \cref{ref-free-eval}). Finally, we demonstrate the usability of the proposed framework by re-evaluating 20 keyphrase systems and discuss the novel insights (\cref{re-eval}).
