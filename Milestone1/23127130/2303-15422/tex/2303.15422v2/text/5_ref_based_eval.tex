\section{Improving Reference-based Evaluation with Semantic Matching}
\label{ref-based-eval}

To begin with, we introduce a semantic matching formulation for reference-based evaluation of saliency, the most extensively investigated aspect. Meta-evaluation studies demonstrate the advantage of our method over previous metrics. %Finally, we conduct a human study to explain why the proposed framework is useful and necessary. \diwu{link to sections}

\subsection{Saliency: Metric Design}
\label{saliency-metric-design}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: semantically similar phrases should be credited; matching should be at phrase-level.}}
}
\vspace{0.3pt}

Although numerous metrics have been adopted for reference-based evaluation of saliency, surprisingly, all of them have limitations in either the \textit{matching granularity} or the \textit{similarity measure}. For instance, F1@5 \citep{meng-etal-2017-deep} and F1@M \citep{yuan-etal-2020-one} conduct phrase-level matching, but fails to evaluate semantically similarity. On the other hand, BertScore \citep{bert-score} calculated with concatenated predictions and references captures semantic similarity, but its token-level matching obscures the semantics of individual phrases. Recognizing these limitations, we propose \textit{phrase-level semantic matching} and define semantic precision ($SemP$), recall ($SemR$), and F1 ($SemF1$) as follows:
\begin{equation}
\resizebox{0.9\hsize}{!}{$SemP(\mathcal{P},\mathcal{Y})=\frac{\sum_{p\in\mathcal{P}}\max_{y\in\mathcal{Y}}(\mathbbm{1}(sim(p,y)>\alpha)\cdot sim(p,y))}{|\mathcal{P}|}$}\nonumber,
\end{equation}
\begin{equation}
\resizebox{0.9\hsize}{!}{$SemR(\mathcal{P},\mathcal{Y})=\frac{\sum_{y\in\mathcal{Y}}\max_{p\in\mathcal{P}}(\mathbbm{1}(sim(p,y)>\alpha)\cdot sim(p,y))}{|\mathcal{Y}|}$}\nonumber,
\end{equation}
\begin{equation}
\resizebox{0.75\hsize}{!}{$SemF1(\mathcal{P},\mathcal{Y})=\frac{2\cdot SemP(\mathcal{P},\mathcal{Y})\cdot SemR(\mathcal{P},\mathcal{Y})}{SemP(\mathcal{P},\mathcal{Y}) + SemR(\mathcal{P},\mathcal{Y})}$}\nonumber,
\end{equation}
where $sim$ defines the similarity between the representation of two phrases and $\alpha$ is a hyperparameter\footnote{$\alpha$ is included in the formulation to allow application-specific adjustments. We use $\alpha=0$ throughout the paper.} to account for the noise in $sim$. 


To define $sim$, the key ingredient is the phrase representation. While alternatives such as hyperbolic embedding \citep{song-etal-2022-hyperbolic} exist, we use dense embedding and cosine similarity to utilize available pre-trained models:
\begin{equation}
\resizebox{0.8\hsize}{!}{$sim(p,q)=cos\_sim(h_p,h_q)=\frac{h_p^Th_q}{||h_p||\cdot||h_q||}$}\nonumber,
\end{equation}
% \begin{equation}
% \resizebox{0.75\hsize}{!}{$union(\mathcal{P})=max\_pooling(h_{p_1}, ..., h_{p_m})$}\nonumber,
% \end{equation}
where $h_p$ is the representation of phrase $p$ obtained by mean-pooling the embedding model's last hidden states across all tokens in the phrase. To obtain a high quality phrase-level embedding, we adapt a model from \citet{reimers-gurevych-2019-sentence}\footnote{We fine-tune \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}.} using a keyphrase-aware contrastive learning objective based on SimCSE \citep{gao-etal-2021-simcse}. Given a batch of $B$ phrases, the training loss is expressed as:
\begin{equation}
\resizebox{0.85\hsize}{!}{$\mathcal{L}_{simcse}=\frac{1}{B}\sum_{i=1}^B -\log\frac{e^{sim(h_i, h_i')/\tau}}{\sum_{j=1}^Be^{sim(h_i,h_j')/\tau}}$}\nonumber,
\end{equation}
where $h_i$ and $h_i'$ are the representations of phrase $i$ obtained using two separate forward passes. The goal of this contrastive fine-tuning stage is to discourage the clustering of unrelated phrases in the representation space while maintaining a high similarity between semantically related phrases. 

We fine-tune the model on $\mathcal{L}_{simcse}$ using 1.04 million keyphrases from the training set of KP20k \citep{meng-etal-2017-deep}, KPTimes \citep{gallina-etal-2019-kptimes}, StackEx \citep{yuan-etal-2020-one}, and OpenKP \citep{xiong-etal-2019-open}, covering a wide range of domains including science, news, forum, and web documents. We use the AdamW optimizer with maximum sequence length 12, batch size 512, and learning rate 1e-6, and fine-tune for 1 epoch. The training is done on a single Nvidia GeForce RTX 2080 Ti GPU card and roughly takes 30 minutes.

\subsection{Meta-evaluation}
\label{ref-based-eval-meta-eval}
\label{meta-evaluation}
We conduct a series of rigorous meta-evaluation to validate the performance of $SemF1$. Our annotations will be released to facilitate future studies.

\subsubsection{Human Evaluation Setup}
We use 50 documents from the test sets of KP20k and KPTimes each for human evaluation. For each document, we obtain predictions from a diverse set of KPE and KPG models (M3, M7, M12, M15/M16, and M18)\footnote{We use M16 for KP20k and M15 for KPTimes to represent in-domain pre-trained language models.}. Then, we ask three annotators to rate on Likert-scale the \textit{semantic similarity} between each predicted keyphrase and the closest phrase in the references, and vice versa. Figure \ref{scoring-interface} presents the annotation interface. % We note that this annotation methodology requires densely scoring each individual keyphrase in the predictions and references and thus scoring in total 100 documents will yield a large number of annotations.

We conducted the experiments on Amazon Mechanical Turk (MTurk) and designed a qualification task to train and hire annotators. We require the annotators to be United States or United Kingdom citizens and have finished more than 1000 HITs with 97\% pass rate. In the qualification task, the annotators are presented with the definition of semantic similarity with examples, and then asked to annotate three documents. 46 annotators that have average error $\le1.5$ per document are selected. A total of 1500 document-level annotations with 13401 phrase-level evaluations are collected from the qualified annotators, costing approximately \$1400. We adjust the unit pay to ensure \$15 hourly pay. 

The annotations are averaged to compute (1) \textit{phrase-level} scores for matching a single phrase to a set of phrases ($p_i\rightarrow\mathcal{Y}$ and $y_i\rightarrow\mathcal{P}$) and (2) \textit{instance-level} precision, recall, and F1 scores. The inter-annotator agreement is presented in Table \ref{tab:iaa}. The annotators reach around 0.75 Krippendorff's alpha for both the datasets and the matching directions, indicating a high agreement. 

\input{tables/iaa}

\begin{figure}[]
\includegraphics[width=\columnwidth]{figures/main_corr_kp20k_kptimes_f1_only.pdf}
% \vspace{-8mm}
\caption{A comparison between the 95\% confidence intervals for the Kendall Tau between human and automatic metrics on KP20k and KPTimes.}
% \vspace{-2mm}
\label{instance-level-tau-intervals}
\end{figure}

\subsubsection{Instance-level Meta-evaluation}
Using the \textit{instance-level} F1 score annotations, we compare $SemF1$ with seven widely used reference-based keyphrase metrics: 

\begin{compactenum}
    \item Exact Matching $F1@M$ \citep{yuan-etal-2020-one}.
    \item $F1@M$ with Substring Matching. We conclude a match between two phrases if either one is a substring of the other. This corresponds to the INCLUDES and PARTOF strategy in \citet{zesch-gurevych-2009-approximate}.
    \item R-precision \citep{zesch-gurevych-2009-approximate}.
    \item FG \citep{luo-etal-2021-keyphrase-generation}.
    \item Rouge-L $F1$ \citep{lin-2004-rouge}.
    \item BertScore $F Score$ \citep{bert-score}. We use the roberta-large model at the 17th layer\footnote{Recommended by the official implementation at \url{https://github.com/Tiiiger/bert_score}}. We concatenate all the phrases in $\mathcal{P}$ with commas to form a single prediction string, and do the same for $\mathcal{Y}$ to form the reference string. 
    % \item BartScore $F Score$ \citep{bart-score}. We use the bart-large without summarization training. We perform the same preprocessing as BertScore. 
\end{compactenum}

We apply Porter Stemmer \citep{Porter1980AnAF} on $\mathcal{P}$ and  $\mathcal{Y}$ before calculating Exact Matching, Substring Matching, and R-precision.

Following \citet{deutsch-etal-2021-statistical}, we report the 95\% confidence interval of Kendall's Tau via input-level bootstrap resampling with 1000 samples. Figure \ref{instance-level-tau-intervals} presents the results. Surprisingly, among the lexical-based metrics, R-precision and FG do not have higher correlation with human compared to exact matching. Substring matching consistently outperforms exact matching, but only by a small amount. This result indicates that although exact matching produces a number of false negatives, approximate semantic matching by lexical matching schemes brings too many false positives and does not provide much overall performance improvement. For semantic-based metrics, we find that BertScore's performance is highly \textit{domain-dependent}: it achieves the second-best correlation with humans on KPTimes while scores poorly on KP20k. By contrast, semantic matching $SemF1$ greatly outperforms other metrics on both datasets with a much higher mean score and a smaller variation, as indicated by the small interval span. 

\input{tables/qualitative_study_matching}

To demonstrate the advantage of semantic matching, we present a simple example in Figure \ref{qualitative-study-matching}. In this example, the model (\textbf{M18}) predicts two phrases exactly matching to some reference phrases and two phrases semantically similar to two phrases in the reference. Correspondingly, human annotators assign partial credits to both of the "near-miss" phrases. However, for these phrases, exact matching fails by giving a score 0 and substring matching fails by giving full credit. As the reference and the prediction contain many similar tokens, BertScore is also near 1.0. By contrast, semantic matching's scoring is similar to human annotators. 

\paragraph{Generality} We remark that although the embedding model for $SemF1$ is pre-trained on a wide range of domains, generalizing $SemF1$ is still an important research question. Specifically, $SemF1$ performs better in the more general news domain (Figure \ref{instance-level-tau-intervals}). This could be caused by the inherent difficulty of specifying the similarity between two scientific concepts. In addition, although our experiments focus on keyphrases in English, $SemF1$ is by design generalizable to other languages by using multilingual text embeddings. % To demonstrate this point, we perform ablation studies in the next section and show that $SemF1$ with off-the-shelf English sentence embeddings outperforms exact matching, despite performing worse than $SemF1$ with the proposed embedding (\cref{saliency-metric-design}).



\subsubsection{Phrase-level Meta-evaluation}

Having established $SemF1$'s strong reference-based evaluation ability, we further use the fine-grained \textit{phrase-level} annotations of matching a single phrase to a set of references, and evaluate whether the proposed training method learns better embedding than off-the-shelf embeddings for semantic matching. We use the annotations on both directions of matching (i.e., $p_i\rightarrow\mathcal{Y}$ and $y_i\rightarrow\mathcal{P}$).

Table \ref{tab:meta-eval-embedding} presents the phrase-level pearson correlation ($r$), spearman correlation ($\rho$), and Kendall's Tau ($\tau$) of exact matching $F1@M$, semantic matching $F1@M$, as well as $SemF1$ with different embedding models: Phrase-BERT \citep{wang-etal-2021-phrase}, SpanBERT \citep{joshi-etal-2020-spanbert}, SimCSE \citep{gao-etal-2021-simcse}, the sentence BERT before the proposed contrastive fine-tuning (SBERT, \citet{reimers-gurevych-2019-sentence}), and the proposed embedding model. Overall, we find a similar pattern compared to the document-level correlations. Semantic matching significantly outperforms exact matching and substring matching, with a difference of 0.1 absolute points in Kendall's Tau and more than 0.15 absolute points in Pearson correlation and Spearman correlation. Among all the embedding models, our model achieves the highest correlation with human when used for semantic matching. 

% \diwu{need to reconsider this paragraph} To further investigate the intrinstic properties of different embeddings, we leverage \citet{chan-etal-2019-neural}'s augmented KP20k test set where keyphrases are linked to their \textit{name variations}. Specifically, we investigate two properties of the embedding spaces inspired by \citet{wang2020understanding}: (1) \textit{alignment}, closeness between two similar phrases, measured as the average cosine similarity between all name variation pairs; (2) \textit{uniformity}, whether unrelated phrases are uniformly distributed, measured with the same similarity calculated with 50000 random keyphrase pairs. As shown in Table \ref{tab:meta-eval-embedding-nv}, the proposed model has the best ability to distinguish between unrelated phrases, as indicated by the highest difference ($\Delta$) between alignment and uniformity. We note that clearly distinguishing similar and unrelated keyphrases (i.e., having a higher "resolution") is more important than having a high alignment value, as the scores can be post-processed and re-scaled to fit the needs of downstream uses. \diwu{This paragraph seems a bit dry right now. Try to think about more concrete insights.}


\input{tables/meta_eval_matching_embedding}


\subsection{Why is semantic matching desired?}
\label{label-variation}

A major motivation for semantic matching is that models may express valid predictions in many ways. But at the same time, \textit{do humans also write valid references with large variations}? We investigate this hypothesis with a pilot study of model-in-the-loop keyphrase annotation. 

% \subsubsection{Human Study Setup}
\paragraph{Setup}
We use 100 documents each from the test sets of KP20k and KPTimes and combine each document's reference keyphrases with the predictions from four systems: M5, M7, M12, and M15 (KPTimes only) / M16 (KP20k only). Three MTurk workers are presented with the document and the phrases re-ordered alphabetically. They are then asked to write keyphrases that best capture the salient information. We emphasize that they may select from the provided phrases or write new keyphrases. Figure \ref{labeling-interface} presents the annotation interface. We use the same recruiting criteria and collect 3226 keyphrase annotations, which approximately cost \$700.

% We require the annotators to be United States or United Kingdom citizens and have finished more than 1000 HITs with 97\% pass rate or have obtained the â€œmaster" rating on MTurk. The per-item price is adjusted to ensure a \$15 hourly pay.

% \subsubsection{Results}

\begin{figure}[]
\centering
\includegraphics[width=0.49\textwidth]{figures/selection_preference.pdf}
\caption{Annotators' selection distribution for each of the keyphrase sources. The reported counts are averaged across three annotators. Annotators do not prefer selecting keyphrases the belong to the original labels. }
\vspace{-1mm}
\label{selection-preference}
\end{figure}
% Are "ground-truth" keyphrases especially appealing to annotators?

\paragraph{Keyphrase annotations exhibit lexical variations.} Figure \ref{selection-preference} presents the distribution of phrase selected by the annotators from each source. The reported counts are averaged over three annotators. Surprisingly, we find that the keyphrases in the original labels are not preferred over the outputs from other models. First, nearly 70\% keyphrases in the original labels are not selected. Second, the annotators select several keyphrases from each model's outputs that do not appear in the label set. For KP20k, the annotators even select more such keyphrases compared to the phrases from the labels. This suggests that \textit{label variations} can be common in keyphrase annotations, even if candidate keyphrases are given as a guidance. 

However, are the observed label variations caused by annotators writing entirely different concepts? Our further analyses show that the average character-level edit distance between the selected phrases and the closest phrase in label keyphrases is 11.0 for KP20k and 7.0 for KPTimes, much smaller than the metric for phrases that are not selected (17.5 for KP20k and 14.6 for KPTimes). This result implies that humans agree on similar high-level concepts as the keyphrases, but often use lexically different phrasings to express them.

% \paragraph{Do annotators select semantically similar keyphrases?} Are the observed label variations caused by annotators similar concepts with different lexical forms? We compute the $SemP$ of all the phrases selected and not selected by the annotators with respect to original labels. The results suggest an affirmative answer: we find that the former group reach much higher $SemP$ (0.514 for KP20k and 0.576 for KPTimes) than the latter group (0.476 for KP20k and 0.514 for KPTimes). 

\paragraph{Semantic matching is more robust to label variations.} A desired property of keyphrase metrics is outputting consistent scores with labels that share the same semantics. Using the 200 annotated documents, we compare $F1@M$ and $SemF1$ on the outputs from \textbf{M17} with four sets of labels: the original labels and three sets of newly annotated labels\footnote{We choose \textbf{M17} as many supervised models' outputs largely overlap with those included in the annotation process.}. As shown in Table \ref{tab:metric-stability}, the output of $SemF1$ is more stable using different label sets, indicating a higher robustness and reliability compared to $F1@M$. 


\input{tables/metric_stability}

