\clearpage
\appendix
\twocolumn[{%
 \centering
 \Large\bf Supplementary Material: Appendices \\ [20pt]
}]


\section{A survey of evaluation methods used in recent keyphrase papers}
\label{kp-paper-eval-survey}
We survey 64 papers published from 2017 to 2022 in major conferences for natural language processing (e.g., *ACL and EMNLP), artificial intelligence (e.g., AAAI), and information retrieval (e.g., SIGIR). 31 papers design new keyphrase generation methods, 28 introduce new keyphrase extraction methods, and 5 conduct empirical studies. 

We manually check each paper's experiment sections and note down whether the reported evaluation metrics fall into any of the six major categories: (1) precision, recall, and F1 based on exact-matching; (2) diversity metrics, such as duplication ratio; (3) ranking-based metrics such as mAP, $\alpha$-NDCG, and MRR; (4) approximate versions of exact match and n-gram matching; (5) retrieval-based utility metrics; (6) human evaluation.

The survey results are presented in Figure \ref{eval-metric-choice}. The majority of papers report exact matching precision, recall, and F1. Only one thirds of all papers consider alternative evaluation metrics. Human evaluation is only conducted in one paper surveyed \citep{bennani-smires-etal-2018-simple}. Overall, the survey suggests that exact matching is still the major method for assessing the performance of newly proposed keyphrase systems, and there has  been limited progress in designing and adopting better metrics despite the suboptimal grading scheme. 


\section{Detailed information of the considered keyphrase systems}
\label{model-description}
\label{implementation-details-unsup-sup}
\label{implementation-details-blackbox}

\input{tables/model_descriptions}

We summarize the properties of all the considered keyphrase systems in Table \ref{tab:model-descriptions}. We benchmark 7 keyphrase extraction models and 11 keyphrase generation models. Among the keyphrase generation models, 5 models are trained from scratch, 4 models are fine-tuned from pre-trained language models, and 2 are prompting GPT-3.5 either zero-shot or with with examples.

We obtain the output from the original author for the following systems: M6, M7 and M8 for KP20k; M5, M13, M14, and M16 for both KP20k and KPTimes. For the other unsupervised and supervised methods, we reproduce the results on our own. For M1, M2, M3, and M4, we obtain the model outputs using the \href{https://github.com/boudinfl/pke}{pke} library. For M6, M7, M8, M9, and M10, M11, and M12, we use the original implementations provided by the authors. For M15, we use the \href{https://github.com/uclanlp/DeepKPG}{DeepKPG} toolkit. 

For the commercial APIs from Amazon (M19) and Azure (M20), we implement the API call following the instructions. We referred to \href{https://docs.aws.amazon.com/comprehend/latest/dg/how-key-phrases.html}{this page} for M19 and \href{https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/key-phrase-extraction/quickstart?pivots=programming-language-python}{this page} for M20. We obtained the results on 3/5/2023 for M19 and 3/11/2023 for M20.

For GPT-3.5, we use the instruction-tuned \texttt{text-davinci-003} model. We always start the prompt with the following task definition: 
\begin{lstlisting}[]
Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases.
\end{lstlisting}
In the zero-shot setting, we then specify the document title and body in two separate lines, and start a new line with
\begin{lstlisting}[]
Keyphrases (separated by comma):
\end{lstlisting}
to finish the prompt. In the 5-shot setting, we randomly sample 5 examples from the train set for each test document, and add their title, body, and keyphrases in the same format to the prompt before the document tested. 

For all the systems, we truncate the input to 512 tokens. We apply a credit on newly registered accounts for M17 and M18. For GPT-3.5, we spent around \$1500 to obtain all the results.


\begin{figure}[]
\centering
\includegraphics[width=\columnwidth]{figures/eval_metric_choice_seaborn.pdf}
\caption{Distribution of the choice of evaluation metrics for 64 keyphrase system papers in top NLP and IR conferences from 2017 to 2022.}
\label{eval-metric-choice}
\end{figure}

% \section{Phrase Embedding Training Setup}
% \label{phrase-embed-training}
% We collect all the keyphrases from the training datasets of KP20k \citep{meng-etal-2017-deep}, KPTimes \citep{gallina-etal-2019-kptimes}, StackEx \citep{yuan-etal-2020-one}, and OpenKP \citep{xiong-etal-2019-open}. These datasets cover the domain of news, science, online forum, and web documents. After lower casing and deduplication, 1,037,309 phrases are left. 

% Our training implementation is based on the \href{https://github.com/UKPLab/sentence-transformers}{sentence-transformers} library. The model is initialized from \href{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}{sentence-transformers/all-mpnet-base-v2}. We use the AdamW optimizer with maximum sequence length 12, batch size 512, and learning rate 1e-6, and fine-tune the model for 1 epoch. Training is done on a single Nvidia GeForce RTX 2080 Ti and roughly takes 30 minutes.

% \begin{figure*}[h!]
% \includegraphics[width=0.98\textwidth]{figures/main_corr_kptimes_style1.pdf}
% \caption{A comparison between the 95\% confidence intervals for the Kendall Tau between human and automatic metrics on KPTimes. We use input-level bootstrap resampling following \citet{deutsch-etal-2021-statistical}. }
% \label{main-corr-kptimes}
% \end{figure*}


\section{Faithfulness: implementation details}
\label{nat-faith-details}

\subsection{UniEval}

UniEval \citep{zhong-etal-2022-towards} evaluates in a boolean QA fashion: the score is the probability of the model generating "Yes" given a prompt normalized by the probability of the model generating either "Yes" or "No". The model we use is pre-trained on a range of tasks including natural language inference, question answering, and linguistically related tasks, and then fine-tuned on evaluating the coherence, consistency, fluency, and relevance of summaries \citep{zhong-etal-2022-towards}\footnote{We use the model hosted at \url{https://huggingface.co/MingZhong/unieval-sum}.}. We leverage its expertise in consistency to evaluate the faithfulness of a single keyphrase. We convert the evaluation tasks into sentence-level inputs to better align with the model's knowledge.
To evaluate the faithfulness of $p_i$ with respect to the input $\mathcal{X}$, we use the following prompt:
\begin{lstlisting}[]
question: Is this claim consistent with the document? </s> summary: the document discusses about p_i. </s> document: X
\end{lstlisting}

%  We use the following prompt for assessing the naturalness of $p_i$:
% \begin{lstlisting}[]
% question: Is this a natural utterance? </s> utterance: This article discusses p_i.
% \end{lstlisting}

\subsection{ChatGPT}
We prompt ChatGPT (\texttt{gpt-3.5-turbo-0301}) to directly score each aspect on a Likert scale. We use temperture 0 for reproducibility. The prompt contains the aspect definition, several example phrases paired with human-assigned scores, the article (only for faithfulness), and the phrase to score. The prompt is presented in Figure \ref{chatgpt-faithfulness-prompt}.

\input{tables/chatgpt_faithfulness_prompt}
% \begin{lstlisting}[]
% Given a keyphrase with the corresponding document, rate its faithfulness on a scale from 1 to 5 (1 for not faithful at all and 5 for perfectly faithful). If a keyphrase is irrelevant, i.e., not actually discussed by the document, then it should receive a low faithfulness score. Abbreviations may be treated as faithful phrases as long as they are valid. [digit] symbols may also be treated as faithful. Note that phrases appear as a subtring of the text can be an unfaithful keyphrase, and that phrases absent from the text can be perfectly faithful. You may use partial scores (decimals). Only output score. Do not output anything else.

% Document: security personalization for internet and web services . [sep] the growth of the internet has been accompanied by the growth of internet services ( e.g. , e commerce , e health ) . this proliferation of services and the increasing attacks on them by malicious individuals have highlighted the need for service security . the security requirements of an internet or pleb service may be specified in a security policy . the provider of the service is then responsible. , for implementing the security measures contained in the policy .

% Keyphrase: security personalization
% Faithfulness: 5
% Keyphrase: cyberattacks
% Faithfulness: 5
% Keyphrase: internet
% Faithfulness: 4
% Keyphrase: security model
% Faithfulness: 3
% Keyphrase: customer service
% Faithfulness: 2
% Keyphrase: public policy
% Faithfulness: 2
% Keyphrase: infrastructure
% Faithfulness: 2

% Document: X
% Keyphrase: p_i
% Faithfulness:
% \end{lstlisting}

% \section{KPTimes meta-evaluation results}
% \label{kptimes-meta-evaluation}
% We present the meta-evaluation results on KPTimes in Figure \ref{main-corr-kptimes}. Overall, we observe a similar pattern compared to the results on KP20k. 

% \section{All evaluation results}
% \label{all-evaluation}
% We present all the evaluation results for each system on KP20k and KPTimes in Table \ref{tab:human-eval-results}, Table \ref{tab:all-results-ref-based}, Table \ref{tab:all-results-ref-free-kp20k}, and Table \ref{tab:all-results-ref-free-kptimes}. Table \ref{tab:human-eval-results} contains the scores human study in section \ref{meta-evaluation}, and Table \ref{tab:all-results-ref-based}, \ref{tab:all-results-ref-free-kp20k}, and \ref{tab:all-results-ref-free-kptimes} contain the results on automatic metrics for the six dimensions. We group the results by reference-free and reference-based metrics.

% \section{Implementation details of the benchmarked keyphrase systems}


\section{Utility: ad-hoc query annotation}
\label{utility-annotation}

For KP20k, we prompt GPT-4 to generate citation-like queries in the style of \citep{boudin-gallina-2021-redefining}. For KPTimes, we instruct the model to generate short phrases that serve as queries. The prompts are presented in Figure \ref{adhoc-query-prompt}. For both datasets, we sample three outputs with the temperature set to 0.9.
% \begin{lstlisting}[]
% For each paper, write a short citation text that summarizes some idea reflected in the abstract without copying anything here. Use a fake paper id like [3] or [5] to refer to the paper. Do not present in a summary format. Instead, write as if you are citing the paper in another paper.

% Title: How Should I Explain? A Comparison of Different Explanation Types for Recommender Systems
% Abstract: Recommender systems help users locate possible items of interest more quickly by filtering and ranking them in a personalized way. Some of these systems provide the end user not only with such a personalized item list but also with an explanation which describes why a specific item is recommended and why the system supposes that the user will like it. Besides helping the user understand the output and rationale of the system, the provision of such explanations can also improve the general acceptance, perceived quality, or effectiveness of the system. In recent years, the question of how to automatically generate and present system-side explanations has attracted increased interest in research. Today some basic explanation facilities are already incorporated in e-commerce Web sites such as Amazon.com. In this work, we continue this line of recent research and address the question of how explanations can be communicated to the user in a more effective way. In particular, we present the results of a user study in which users of a recommender system were provided with different types of explanation. We experimented with 10 different explanation types and measured their effects in different dimensions. The explanation types used in the study include both known visualizations from the literature as well as two novel interfaces based on tag clouds. Our study reveals that the content-based tag cloud explanations are particularly helpful to increase the user-perceived level of transparency and to increase user satisfaction even though they demand higher cognitive effort from the user. Based on these insights and observations, we derive a set of possible guidelines for designing or selecting suitable explanations for recommender systems.
% Citation: The ability for an artificially intelligent system to explain recommendations has been shown to be an important factor for user acceptance and satisfaction [13]. 

% ... two examples omitted ...

% Title: [document_title]
% Abstract: [document_abstract]
% Citation: 
% \end{lstlisting}



% \begin{lstlisting}[]

% \end{lstlisting}


\input{tables/adhoc_query_prompt}

% \section{Further analysis}
% \diwu{Dump findings here. Select some to include.}
% \subsection{Is ChatGPT capable of performing human-level annotation?}
% We further investigate whether ChatGPT can be used as an alternative to crowd-sourced annotations for the naturalness and faithfulness of keyphrase predictions. Specifically, we (1) calculate the score distributions of annotators and ChatGPT's predictions and (2) randomly replace an annotator with ChatGPT and compute again the inter-annotator agreement. 

% \paragraph{Score distribution} Figure \ref{human-vs-chatgpt-score-dist} presents the results on KP20k. For naturalness, ChatGPT's scores follows a unimodal distribution centered at 4, similar to human distributions. For faithfulness, ChatGPT agrees with human by predicting much more 4's and 5's than lower scores, but predicts more 1's and 5's compared to humans. Overall, we observe ChatGPT can predict scores similar to general annotators, while has a slightly higher preference to predict extreme scores. 

% \begin{figure}[]
% \centering
% \includegraphics[width=0.5\textwidth]{figures/human_vs_chatgpt_score_dist.pdf}
% \caption{Distribution of the naturalness and faithfulness ratings (for KP20k keyphrases) written by the annotators vs. predicted by ChatGPT. }
% % \vspace{-2mm}
% \label{human-vs-chatgpt-score-dist}
% \end{figure}

% \paragraph{IAA after replacement}
% Table \ref{tab:iaa-after-chatgpt-replacement} presents the inter-annotator agreement (IAA) after replacing one of the top contributing annotators with ChatGPT. We define top contributing as contributing over 40 phrase-level scoring entries. There are 9 annotators for KP20k and 8 annotators for KPTimes satisfying the criterion. We observe that this replacement does not significantly affect the inter-annotator agreement, indicating ChatGPT is able to produce naturalness and faithfulness scorings at the level of a general crowd-source worker.

% \setlength{\tabcolsep}{3.5pt}
% \begin{table}[]
%     \centering
%     % \resizebox{\columnwidth}{!}{%
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{l | c c | c c }
%     \hline
%     & \multicolumn{2}{c|}{\textit{Naturalness}} & \multicolumn{2}{c}{\textit{Faithfulness}} \\
%     & original & w/ replacement & original & w/ replacement \\
%     \hline
%     KP20k & $0.218$ & $0.217\pm0.019$ & $0.370$ & $0.390\pm0.021$ \\
%     KPTimes & $0.333$ & $0.327\pm0.023$ & $0.546$ & $0.522\pm0.023$ \\
%     \hline
%     \end{tabular}
%     }
%     % \vspace{-2mm}
%     \caption{Inter-annotator agreement measured via the interval Krippendorff's alpha. "w/ replacement" = the averaged inter-annotator agreement after randomly replacing one of the top-contributing annotators with ChatGPT. We report the standard deviation after the $\pm$ sign. }
%     \label{tab:iaa-after-chatgpt-replacement}
%     % \vspace{-3mm}
% \end{table}


% \subsection{Are absent keyphrases truly absent?}
% Previous literature (e.g., \citet{meng-etal-2017-deep}) often separately discuss present keyphrases and absent keyphrases for evaluation. With semantic-based phrase representations, we investigate how many of the ground-truth and predictions are truly "absent". In this case study, we use SetTrans \citep{ye-etal-2021-one2set} and the KP20k/KPTimes test set.

% First, we evaluate the "absentness" of ground-truth absent keyphrases written by human. For each absent keyphrase, we report its semantic matching score against the set of all noun phrases in the document (defined by the cosine similarity of the embeddings predicted by the proposed embedding model). We find the averaged score 0.649 for KP20k and 0.567 for KPTimes. This score is close to those of name variations (Table \ref{tab:meta-eval-embedding}), indicating that many of the absent keyphrases can be considered as named variations of some noun phrase in the document (i.e., the "near misses" case in previous literature). 

% Next, for all the absent keyphrase predictions made by SetTrans, we calculate the semantic matching score in a similar manner. The resulting score is 0.476 for KP20k and 0.677 for KPTimes. This result suggests that SetTrans mostly repeats mentioned concepts in its predictions for KPTimes, but is able to bring in new (possibly more high-level) concepts in its predictions for KP20k. 

% \subsection{What phrases are unnatural/unfaithful?}
% We manually inspect the phrases labelled with a low naturalness or faithfulness score ($<$ 3.0) and assign categories to the possible underlying reason. For naturalness, the categories are (1) gibberish or spelling mistakes, (2) wrong phrase boundaries, (3) rare concepts, (4) rare named entities, and (5) annotation noise. For faithfulness, we have (1) unnatural phrase, (2) high-level irrelevant phrases, and (3) too specific unsupported concepts. 

% The results are presented in Figure \ref{low-score-reasons}. Interestingly, we find that a lot of annotators rate a low naturalness score for rare named entities in KPTimes. For both datasets, the annotator error rates for naturalness is low. For faithfulness, we find that the major reason for low scores is too general phrases appearing unrelated to the main text.

% \begin{figure}[]
% \centering
% \includegraphics[width=\columnwidth]{figures/low_score_reasons.pdf}
% \caption{A categorization of reasons for labelled low naturalness and faithfulness score.}
% \label{low-score-reasons}
% \end{figure}

% \subsection{Metric models' bias}
% \citet{liu2023geval} showed the propensity of model-based evaluators to favor the text generated by pre-trained language models (LMs). Therefore, we investigate whether UniEval and ChatGPT prefer pre-trained LMs. For each of the naturalness and faithfulness annotations, we compare the human score (averaged across three annotators) and the score predicted by ChatGPT or UniEval (re-scaled to the Likert scale) and group into three categories: (1) the human score is higher by more than $\delta$, (2) the model score is higher by more than $\delta$, or (3) neither. We use $\delta=1.0$.

% Table \ref{tab:metric-bias} presents all the results. For naturalness, we find both models favoring human on KP20k and favoring GPT3.5 on KPTimes. For faithfulness, we do not find UniEval or ChatGPT to significantly favor either human or models. Overall, we conclude that for evaluating keyphrases, model-based scorers do not exhibit a consistent trend of favoring either human or model outputs. 

% \input{tables/metric_bias}

% % \subsection{Score distribution}
% % Check how human score and automatic metrics are distributed. 

% \subsection{Inter-metric correlation}
% Using the outputs on the 20 studied models, we measure the inter-metric Pearson correlation for the metrics proposed in the framework. We observe that metrics measuring different aspects do not correlate with each other except for the reference-based metrics, which have a high correlation among themselves. This result indicates that the different aspects are indeed measured in distinct manner. 

% \begin{figure*}[]
% \centering
% \includegraphics[width=\linewidth]{figures/metric_correlations.pdf}
% \caption{Inter-metric Pearson correlation measured on the studied models.}
% \label{metric-correlatins}
% \end{figure*}


% \input{tables/human_eval_results}
% \input{tables/all_results_ref_based}
% \input{tables/all_results_ref_free}



% \section{Details for the meta-evaluation study}
% \label{annotation-details-study-2}
% For this task, we add a qualification stage: workers are required to take a test on whether they understand the definition of keyphrases and whether they are able to follow the instructions on determining semantic similarity. For the qualification study, we recruit senior annotators from US or UK with more than 5000 HITs approved and more than 96\% approval rate. 150 annotators attempted the qualification test, among which 46 qualified annotators work for the evaluation study. 

%  Note that for this task, we make the concept of "keyphrases" transparent to the annotators because their main job is to evaluate semantic similarity. We also provide a number of example pairs to train the annotators on evaluating semantic similarity with partial credits. To determine the payment for each HIT, we first find several NLP researcher volunteers to perform a trial run and use their time as an estimate. Through this study, we determine that about 2 minutes per model is required to finish the task. Therefore, in the final version, crowd-source annotators are asked to annotate all the model outputs for a single document per HIT, which is labeled as \$3.0. In total, we spent around \$1200 for this experiment.

% We follow a similar protocol to obtain the naturalness and faithfulness annotations for KP20k and KPTimes. The annotation costed around \$1000.

% We conducted the experiments on Amazon Mechanical Turk (MTurk) and designed a qualification task to train and hire annotators. We require the annotators to be United States or United Kingdom citizens and have finished more than 1000 HITs with 97\% pass rate. In the qualification task, the annotators are presented with the definition of semantic similarity with examples, and then asked to annotate the main task for three documents. 46 annotators that have average error $\le1.5$ are selected. In the main annotation stage, one of the authors attempted the task and recorded the time. The price is then adjusted to ensure a \$15 hourly pay. A total of 1500 document-level annotations with 13401 phrase-level decisions are collected from the qualified annotators, costing approximately \$1400. We present an example in the instructions in Figure \ref{scoring-instructions} and the annotation interface in Figure \ref{scoring-interface}. 


% \section{Details for the label variation study}
% \label{annotation-details-study-1}
% % For this task, we only recruit "MTurk Master" annotators from US or UK with more than 100 HITs approved and more than 97\% approval rate. We present an annotation interface in Figure \ref{labeling-interface}. To determine the payment for each HIT, we first find several NLP researcher volunteers to perform a trial run and use their time as an estimate. Through this study, we determine that about 5 minutes per document is required to finish the task. Therefore, in the final version, crowd-source annotators are asked to annotate five document per HIT, which is labeled as \$5.0. In total, we spent around \$375 for this experiment.
% We use 100 documents each from the test sets of KP20k and KPTimes and combine each document's reference keyphrases with the predictions from four systems: M5, M7, M12, and M15 (KPTimes only) / M16 (KP20k only). This setting simulates a realistic annotation setting where humans write keyphrases with some assistance from automatic algorithms. Three MTurk workers are presented with the document and the phrases re-ordered alphabetically. They are then asked to directly write keyphrases that best capture the salient information. We emphasize that they may select from the provided phrases or write new keyphrases. Figure \ref{labeling-interface} presents the annotation interface. We require the annotators to be United States or United Kingdom citizens and have finished more than 1000 HITs with 97\% pass rate or have obtained the â€œmaster" rating on MTurk. The per-item price is adjusted to ensure a \$15 hourly pay. In total, we collect 3226 keyphrase annotations for 200 documents, which approximately cost \$700.



\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.9\textwidth]{figures/scoring_instructions.png}}
\caption{An example of the annotation instructions for the keyphrase evaluation study in \cref{meta-evaluation}.}
\label{scoring-instructions}
\end{figure*}


\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.9\textwidth]{figures/scoring_interface.png}}
\caption{An example of the annotation interface for the keyphrase evaluation study in \cref{meta-evaluation}.}
\label{scoring-interface}
\end{figure*}


\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.9\textwidth]{figures/labeling_interface.png}}
\caption{An example of the annotation interface for the keyphrase annotation study in \cref{label-variation}.}
\label{labeling-interface}
\end{figure*}
