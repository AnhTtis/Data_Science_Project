\section{Reference-free Evaluation}
\label{ref-free-eval}

For a range of text generation tasks, it is generally agreed that the optimal output may vary based on the considered aspect \citep{wen-etal-2015-semantically,mehri-eskenazi-2020-usr,fabbri-etal-2021-summeval}. As such, reference-based evaluation may not faithfully reflect the evaluation goals. For evaluating KPE and KPG systems, \textsc{KPEval} identifies three important evaluation aspects where reference-based evaluation fails to handle and introduces corresponding reference-free evaluation procedures.


\subsection{Faithfulness}
\label{nat-faith-methods}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: keyphrases should always be grounded in the document.}}
}
\vspace{0.3pt}

In real-world applications, it is crucial that keyphrase systems' outputs only reflect concepts that are discussed in the document. Determining whether a keyphrase is faithful is non-trivial and often beyond checking whether it is a present keyphrase or not. For instance, an absent keyphrase could be faithful by being synonyms or abstract versions of concepts in the document. On the other hand, a present keyphrase could still be unfaithful by misrepresenting the phrase boundaries. For instance, if the document discusses "NP-hard problem", then the keyphrase "hard problem" would be considered unfaithful. To evaluate faithfulness, we introduce a novel formulation that leverages models pre-trained to align with human preferences. We consider the following two setup:

\paragraph{UniEval} Introduced by \citet{zhong-etal-2022-towards}, UniEval scores text generation in a boolean QA fashion: the score is the normalized probability of the model generating "Yes". We leverage a summarization evaluation checkpoint's capability in consistency evaluation to evaluate the faithfulness of a keyphrase prediction. We convert the evaluation into single-sentence summaries to better align with the model's original usage scenario.

\paragraph{ChatGPT} We also consider prompting ChatGPT with a form-filling formulation\footnote{We use the \texttt{gpt-3.5-turbo-0301} model.}. Specifically, in the prompt we provide the definition of faithfulness, several examples, the article, and the phrase to score. Then, the model is required to generate the scoring for the asked aspect.

We present further implementation details of UniEval and ChatGPT in appendix section \ref{nat-faith-details}.

\subsection{Diversity}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: reward more semantically distinct concepts and penalize repetitions.}}
}
\vspace{0.3pt}

Generating unique concepts with minimal repetition is a desirable property of keyphrase systems. To measure the duplication in $\mathcal{P}$, \textsc{KPEval} includes one lexical and one semantic metric based on \citet{bahuleyan-el-asri-2020-diverse}. The lexical metric $dup\_token\_ratio$ is the percentage of duplicate tokens after stemming. The semantic metric $dup\_emb\_sim$ is the average of pairwise consine similarity, using the same phrase embedding model we train for saliency:
\begin{equation}
\resizebox{0.8\hsize}{!}{$emb\_sim(\mathcal{P})=\frac{\sum_{i=1}^m\sum_{j=1}^m\mathbbm{1}(i\ne j)sim(p_i,p_j)}{m(m-1)}$}\nonumber.
\end{equation}
We note that by design, we do not penalize a model that over-generates uninformative keyphrases, as intuitively this model has a high diversity. Judging the informativeness of the keyphrases is instead delegated to the metrics for saliency\footnote{As a result, metrics such as the orthogonal regularization term used by CatSeqD \citep{yuan-etal-2020-one} are not suitable for our purposes, as the term naturally increases with $|\mathcal{P}|$.}.


\subsection{Utility}
\label{utility-def}

\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: reward predictions that enable effective and efficient retrieval of the document.}}
}
\vspace{0.3pt}

Information Retrieval (IR) is an important downstream application for keyphrases \citep{10.1145/312624.312671, 10.1145/1141753.1141800, kim-etal-2013-applying}. To directly evaluate whether $\mathcal{M}$ can generate useful keyphrases for IR-related downstream tasks, we propose to measure the effectiveness of $\mathcal{P}$ in facilitating \textit{ad-hoc retrieval} of $\mathcal{X}$ from a set of in-domain documents \citep{boudin-gallina-2021-redefining}. 

Concretely, we assume that an in-domain corpus $\mathcal{C}$ is available, where each document is paired with human-annotated keyphrases. To evaluate $\mathcal{P}$, we combine $\mathcal{C}$ with ($\mathcal{X}$, $\mathcal{P}$) and index each document with (keyphrases, document title). Then, we retrieve from this combined pool using a set of queries $\mathcal{Q}=\{q_1,q_2,...,q_{|Q|}\}$ specifically written for retrieving $\mathcal{X}$. The retrieval effectiveness is measured with Recall at $k$ ($Recall@k$) and Reciprocal Rank at $k$ ($RR@k$), averaged across all queries. We use GPT-4 \citep{OpenAI2023GPT4TR} to annotate three ad-hoc queries per document on 1000 documents each from KP20k and KPTimes. BM25 \citep{10.1007/978-1-4471-2099-5_24} is used as the retriever. Appendix \ref{utility-annotation} presents further details.


% \subsection{Meta-evaluation}
% 