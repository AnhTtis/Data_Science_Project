
\section{Fine-grained re-evaluation of keyphrase extraction and generation systems}
\label{re-eval}

\input{tables/all_results_main}

In this section, we conduct a thorough re-evaluation of 20 keyphrase systems with \textsc{KPEval}. The implementation details are presented in appendix \ref{implementation-details-blackbox}. Table \ref{tab:all-results-main} presents the metric results on all aspects.

% We also evaluate human-written keyphrase labels with reference-free metrics. 

\subsection{Reference-based evaluation}
We first note that semantic matching alleviates the problem of performance underestimation caused by exact matching, with the best model achieving around 0.6 $SemF1$ on KP20k and 0.8 on KPTimes. The performance of different models is also clearly distinguished compared to BertScore reported in \citet{koto-etal-2022-lipkey} and \citet{glazkova2022applying}. For all the reference-based metrics, we observe that the KPG-PLM family is generally the best. GPT-3.5 achieves competitive coverage in both the 0-shot and 5-shot settings. With five examples, the saliency significantly increases, although the gap between GPT-3.5 and supervised models may still be large in well-defined distributions such as KPTimes. For the Amazon and Azure APIs, we find that they do not outperform zero-shot prompting GPT-3.5 in the reference-based setting.

\subsection{Reference-free evaluation}

\paragraph{Faithfulness} For faithfulness, GPT-3.5 (5-shot setting) leads most KPE and KPG models (with the exception of BERT+CRF) by a large margin. The results suggest that the model already has a strong ability to extract well-grounded concepts without the need of examples. In addition, we find supervised models have much higher faithfulness scores than unsupervised models when UniEval is used for evaluation. This preference is not significant for ChatGPT. In terms of domain, the predictions for KPTimes are more open-ended and thus may have lower faithfulness scores. By comparison, as the scientific terminologies have fewer variations and present in the document, models often have higher faithfulness scores on KP20k.  

\paragraph{Diversity} For diversity, we find that $dup\_token\_ratio$ and $embed\_sim$ generally show similar trends: the former prefers humans and GPT-3.5; the latter prefers GPT-3.5 and keyphrase APIs. Compared to KPE models and non-PLM KPG models, PLM-based KPG models have higher diversity. After a manual inspection, we find that the major reason is that PLM-based KPG generate much fewer repetitions compared to the non-PLM models even if greedy decoding is used for both. In addition, some KPE models' ranking heuristics rank similar phrases together, causing a high duplication. From this perspective, methods that explicitly model topical diversity (such as M3) have a great advantage.

\paragraph{Utility} The last two columns of Table \ref{tab:all-results-main} show the utility of the keyphrase predictions for ad-hoc retrieval. We report the scores with $k=5$. For both datasets, Kea, HyperMatch, and methods based on GPT-3.5 exhibit higher utility than others. Unsupervised methods only have high utility scores on KPTimes, and SetTrans only has high utility scores on KP20k. Notably, our evaluation reveals that \textit{high $SemF1$ does not translate to good utility}, and that \textit{training the models on a certain human-labelled distribution may harm the utility}. On the other hand, GPT-3.5 has an outstanding ability to pick useful keyphrases and rank them properly, outperforming API-based methods. 

\subsection{Discussion}
We highlight the key findings of this section:
\begin{itemize}
    \item \textit{No one-winning-for-all model}. SetTrans and PLMs generally outperform other systems for reference-based evaluation, while GPT-3.5 excels in reference-free evaluation. In terms of utility for indexing, unsupervised models are not necessarily worse than supervised ones.
    \item \textit{Do not over-trust human references}. Human references are often not directly optimized for a high utility or a high diversity. As a direct result, scoring high in reference-based evaluation does not indicate a high utility.
    \item In the zero-shot setting, GPT-3.5 already has an outstanding ability to select useful keyphrases. \textit{Few-shot prompting can improve saliency and faithfulness, but not utility. }
    \item In general, \textit{KPE methods and keyphrase extraction APIs do not outperform GPT-3.5}. However, we do find that unsupervised models exhibit strong utility performance on KP20k. They would be a sensible choice if the inference cost is a concern.
\end{itemize}


