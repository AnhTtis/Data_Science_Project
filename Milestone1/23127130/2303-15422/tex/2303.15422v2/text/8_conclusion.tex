\section{Conclusion}
We introduce \textsc{KPEval}, a fine-grained 
evaluation framework for keyphrase extraction and generation systems. \textsc{KPEval} conducts semantic-based evaluation on saliency, faithfulness, diversity, and utility. We show the advantage of our reference-based method via rigorous human evaluation, and exhibit the usability of \textsc{KPEval} through a large-scale evaluation. Our work marks the first step towards improving the evaluation of keyphrase systems in the era of pre-trained language models and large language models. 

We hope \textsc{KPEval} can inspire future works for adopting more accurate evaluation metrics and further advancing the evaluation methodology. Specifically, expanding the evaluation to more domains and multilingual keyphrase systems is an important research direction. Future work may also research on defining utility metrics based on the unique needs of the emerging multi-modal applications. 


\section*{Acknowledgments}
The research is supported in part by Taboola, NSF CCF-2200274, and an Amazon AWS credit award. We thank the Taboola team for helpful discussion. We thank Amita Kamath, Wasi Ahmad, as well as other members of the UCLA-NLP group for providing their valuable feedback. We also thank Jingnong Qu, Xiaoxian Shen and Xueer Li for their help in a meta-evaluation pilot study. 
