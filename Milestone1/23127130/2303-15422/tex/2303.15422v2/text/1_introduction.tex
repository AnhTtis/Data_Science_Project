
\begin{figure*}[]
\includegraphics[width=\textwidth]{figures/kpeval_main_figure.pdf}
\caption{An illustration of the proposed framework. We consider four desired properties of keyphrase systems and design semantic-based metrics to faithfully evaluate them.}
\label{main-framework}
% \vspace{-2mm}
\end{figure*}

\section{Introduction}
Building systems to automatically predict keyphrases of a document is a long lasting research problem in the Information Retrieval (IR) and the Natural Language Processing (NLP) communities \citep{witten1999kea, hulth-2003-improved, meng-etal-2017-deep}. With the availability of large annotated keyphrase datasets, pre-trained text representations, and deep learning-based methods, numerous keyphrase systems have been proposed, claiming significant performance improvements compared to previous baselines (\citet{meng-etal-2017-deep,boudin-2018-unsupervised,chan-etal-2019-neural, ye-etal-2021-one2set}, \textit{i.a.}). 

However, these methods largely rely on exact matching stemmed predictions to human references to assess the performance. According to a survey of 64 recent papers on keyphrase extraction (KPE) and keyphrase generation (KPG) (appendix \ref{kp-paper-eval-survey}), exact matching is adopted by 63/64 papers and only 24/64 papers consider other metrics. Despite its popularity, exact matching is known to be insufficient: it fails to credit synonyms and parent/child concepts that are spelled differently from the references \citep{zesch-gurevych-2009-approximate}. Later works propose to relax the matching criteria \citep{kim-etal-2010-evaluating, zesch-gurevych-2009-approximate, luo-etal-2021-keyphrase-generation}, evaluate with name variations \citep{chan-etal-2019-neural}, or use semantic representations for matching \citep{jarmasz-and-barriere-2004, koto-etal-2022-lipkey,glazkova2022applying}. This line of work still fails to capture \textit{phrase-level semantics} well, resulting in a weak correlation with human judgments (\cref{meta-evaluation}). Moreover, the exclusive focus on \textit{reference-based} evaluation ignores properties such as diversity \citep{wang-etal-2016-extracting, bahuleyan-el-asri-2020-diverse} or retrieval effectiveness \citep{boudin-gallina-2021-redefining} which are highly relevant to real-world applications of keyphrase systems.

Aiming to address current evaluation methods' limitations and reflect the needs in real applications, we propose \textsc{KPEval}, a fine-grained semantic-based evaluation framework considering four aspects: \textbf{saliency}, \textbf{faithfulness}, \textbf{diversity}, and \textbf{utility}. To accurately evaluate each aspect, we design semantic-based metrics such as semantic matching and model-based scoring (Figure \ref{main-framework}, details in \cref{section-framework}). To build high-quality phrase representations for the evaluation, we leverage unsupervised contrastive learning with large-scale keyphrase data. 

We conduct extensive human studies to compare \textsc{KPEval}'s reference-based metrics with human judgments. Results show that our semantic matching strategy has the state-of-the-art performance in evaluating saliency, outperforming exact matching and BertScore by more than 0.15 absolute points in Kendall's Tau (\cref{meta-evaluation}). Further analyses show that humans use various lexical forms in keyphrase annotations, and that semantic matching exhibits superior stability under label variations (\cref{label-variation}). 

Using \textsc{KPEval}, we benchmark 20 keyphrase extraction and generation models and APIs on KP20k \citep{meng-etal-2017-deep} and KPTimes \citep{gallina-etal-2019-kptimes}. Notably, we find that (1) supervised models excel in saliency, while prompting GPT-3.5 \citep{ouyang2022training} performs well in the reference-free aspects; (2) reference-based scores are not a good indicator of retrieval utility; (3) prompting GPT-3.5 with examples improves saliency and faithfulness, but not the other aspects; (4) most keyphrase extraction methods and APIs cannot outperform GPT-3.5. Our implementation will be released at \url{https://github.com/uclanlp/KPEval} when the paper is formally published.

