
\section{Preliminaries}
 In this section, we outline the keyphrase systems and datasets we will study in this paper and discuss their related work. We select 18 models from three main categories, as summarized in Table \ref{tab:model-descriptions} in appendix. Note that our goal is to include strong systems of diverse types rather than exhaustively benchmarking all available systems. 

\subsection{Keyphrase Systems}
\paragraph{Keyphrase extraction systems}
Traditionally, keyphrase extraction is done in an unsupervised manner where noun phrase candidates are ranked with heuristics \citep{hulth-2003-improved,mihalcea-tarau-2004-textrank,10.5555/1620163.1620205,bougouin-etal-2013-topicrank,boudin-2018-unsupervised, liang-etal-2021-unsupervised, ding-luo-2021-attentionrank,zhang-etal-2022-mderank}. Other supervised methods use feature-based ranking \citep{witten1999kea}, sequence labeling \citep{zhang-etal-2016-keyphrase,luan-etal-2017-scientific,arxiv.1910.08840}, and task-specific objectives with pre-trained language models \citep{song-etal-2021-importance,song-etal-2022-hyperbolic}. We evaluate five keyphrase extraction methods: [\textbf{M1}] TF-IDF \citep{tf-idf}, [\textbf{M2}] TextRank \citep{mihalcea-tarau-2004-textrank}, [\textbf{M3}] MultipartiteRank \citep{boudin-2018-unsupervised}, [\textbf{M4}] Kea \citep{witten1999kea}, and [\textbf{M5}] BERT+CRF \citep{arxiv.1910.08840,2212.10233}.

\paragraph{Keyphrase generation systems}
The keyphrase generation task, introduced by \citet{meng-etal-2017-deep}, requires a model to generate keyphrases that may not present in the input document. Models are usually trained using three types of supervised objectives: \textit{One2One} -- generating one keyphrase give a document \citep{meng-etal-2017-deep}; \textit{One2Seq} -- generating a sequence of keyphrases given a document \citep{yuan-etal-2020-one}; or \textit{One2Set} -- generating a set of keyphrases given a document \citep{ye-etal-2021-one2set}. Various approaches have been developed, including incorporating linguistic constraints \citep{zhao-zhang-2019-incorporating}, exploiting semi-supervised learning signals from titles \citep{ye-wang-2018-semi}, hierarchical modeling of phrases and words \citep{chen-etal-2020-exclusive}, incorporating reinforcement learning \citep{chan-etal-2019-neural, luo-etal-2021-keyphrase-generation} or GANs \citep{swaminathan-etal-2020-preliminary}, unifying keyphrase extraction with generation \citep{chen-etal-2019-integrated,ahmad-etal-2021-select}, utilizing pre-trained language models \citep{2201.05302, kulkarni-etal-2022-learning, gao-etal-2022-retrieval, 2212.10233, wu2022fast}, and unsupervised keyphrase generation \citep{shen2021unsupervised}. 

We select nine keyphrase generation methods to evaluate: [\textbf{M6}] CatSeq \citep{yuan-etal-2020-one}, [\textbf{M7}] CatSeqTG+2RF1 \citep{chan-etal-2019-neural}, [\textbf{M8}] ExHiRD-h \citep{chen-etal-2020-exclusive}, [\textbf{M9}] Transformer \citep{ye-etal-2021-one2set}, [\textbf{M10}] SetTrans \citep{ye-etal-2021-one2set}, [\textbf{M11}] SciBERT-G \citep{2212.10233}, [\textbf{M12}] BART-large \citep{2212.10233}, [\textbf{M13}] KeyBART \citep{kulkarni-etal-2022-learning}, and [\textbf{M14}] SciBART-large fine-tuned on OAGKX \citep{2212.10233}. 

\paragraph{Large language models and APIs}
Recently, the emergent ability of pre-trained large language models for few-shot in-context learning has gained much attention \citep{brown2020language}. For keyphrase generation, we benchmark GPT-3.5, which is trained on instruction following with human preferences \citep{ouyang2022training}. We discuss our prompting strategy in detail in appendix section \ref{implementation-details-blackbox}. Inspired by \citet{ribeiro-etal-2020-beyond}, we also include two keyphrase extraction APIs from Amazon and Microsoft. We denote them as: [\textbf{M15}] zero-shot prompting \texttt{text-davinci-003}, [\textbf{M16}] five-shot prompting \texttt{text-davinci-003}, [\textbf{M17}] the Amazon Comprehend API, and [\textbf{M18}] the Azure Cognitive Services API. 

\subsection{Evaluation Setup}
\paragraph{Datasets}
We use two widely-used keyphrase datasets: KP20k \citep{meng-etal-2017-deep} with over 500k computer science papers from online digital libraries and KPTimes \citep{gallina-etal-2019-kptimes} with over 250k news documents collected from New York Times. KP20k's keyphrase annotations are obtained from the metadata of the papers, while KPTimes's keyphrases are assigned by expert editors. We report performance on KP20k's 20k test set and on KPTimes' 10k in-distribution test set.

\paragraph{Implementation details}
We provide implementation details of the evaluated keyphrase systems in appendix \ref{implementation-details-unsup-sup}. We solicit the results from the original authors and carefully re-implement them if the original results are not available. To ensure a fair comparison, we only consider the top 10 predictions from M1-M4 and M17-M18.
