\clearpage
\appendix
\twocolumn[{%
 \centering
 \Large\bf Supplementary Material: Appendices \\ [20pt]
}]


\section{A survey of evaluation methods used in recent keyphrase papers}
\label{kp-paper-eval-survey}
We survey 64 papers published from 2017 to 2022 in major conferences for natural language processing (e.g., *ACL and EMNLP), artificial intelligence (e.g., AAAI), and information retrieval (e.g., SIGIR). 31 papers design new keyphrase generation methods, 28 introduce new keyphrase extraction methods, and 5 conduct empirical studies. 

We manually check each paper's experiment sections and note down whether the reported evaluation metrics fall into any of the six major categories: (1) precision, recall, and F1 based on exact-matching; (2) diversity metrics, such as duplication ratio; (3) ranking-based metrics such as mAP, $\alpha$-NDCG, and MRR; (4) approximate versions of exact match and n-gram matching; (5) retrieval-based utility metrics; (6) human evaluation.

The survey results are presented in Table \ref{eval-metric-choice}. The majority of papers report exact matching precision, recall, and F1. Only one thirds of all papers consider alternative evaluation metrics. Human evaluation is only conducted in one paper surveyed \citep{bennani-smires-etal-2018-simple}. Overall, the survey suggests that exact matching is still the major method for assessing the performance of newly proposed keyphrase systems, and there has  been limited progress in designing and adopting better metrics despite the suboptimal grading scheme. 

\begin{figure*}[]
\centering
\includegraphics[width=0.95\textwidth]{figures/eval_metric_choice.pdf}
\caption{Distribution of the choice of evaluation metrics for keyphrase system papers in top NLP and IR conferences from 2017 to 2022. We survey 64 papers in total.}
\label{eval-metric-choice}
\end{figure*}

\section{Detailed information of the considered keyphrase systems}
\label{model-description}
\input{tables/model_descriptions}
We summarize the properties of all the considered keyphrase systems in Table \ref{tab:model-descriptions}. We benchmark 7 keyphrase extraction models and 11 keyphrase generation models. Among the keyphrase generation models, 5 models are trained from scratch, 4 models are fine-tuned from pre-trained language models, and 2 are prompting GPT-3.5 either zero-shot or with with examples.

\section{Phrase Embedding Training Setup}
\label{phrase-embed-training}
We collect all the keyphrases from the training datasets of KP20k \citep{meng-etal-2017-deep}, KPTimes \citep{gallina-etal-2019-kptimes}, StackEx \citep{yuan-etal-2020-one}, and OpenKP \citep{xiong-etal-2019-open}. These datasets cover the domain of news, science, online forum, and web documents. After lower casing and deduplication, 1,037,309 phrases are left. 

Our training implementation is based on the \href{https://github.com/UKPLab/sentence-transformers}{sentence-transformers} library. The model is initialized from \href{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}{sentence-transformers/all-mpnet-base-v2}. We use the AdamW optimizer with maximum sequence length 12, batch size 512, and learning rate 1e-6, and fine-tune the model for 1 epoch. Training is done on a single Nvidia GeForce RTX 2080 Ti and roughly takes 30 minutes.

\begin{figure*}[h!]
\includegraphics[width=0.98\textwidth]{figures/main_corr_kptimes_style1.pdf}
\caption{A comparison between the 95\% confidence intervals for the Kendall Tau between human and automatic metrics on KPTimes. We use input-level bootstrap resampling following \citet{deutsch-etal-2021-statistical}. }
\label{main-corr-kptimes}
\end{figure*}

\section{KPTimes meta-evaluation results}
\label{kptimes-meta-evaluation}
We present the meta-evaluation results on KPTimes in Figure \ref{main-corr-kptimes}. Overall, we observe a similar pattern compared to the results on KP20k. 

\section{All evaluation results}
\label{all-evaluation}
We present all the evaluation results for each system on KP20k and KPTimes in Table \ref{tab:human-eval-results}, Table \ref{tab:all-results-ref-based}, Table \ref{tab:all-results-ref-free-kp20k}, and Table \ref{tab:all-results-ref-free-kptimes}. Table \ref{tab:human-eval-results} contains the scores human study in section \ref{meta-evaluation}, and Table \ref{tab:all-results-ref-based}, \ref{tab:all-results-ref-free-kp20k}, and \ref{tab:all-results-ref-free-kptimes} contain the results on automatic metrics for the six dimensions. We group the results by reference-free and reference-based metrics.

\section{Implementation details of the benchmarked keyphrase systems}
\label{implementation-details-unsup-sup}
\label{implementation-details-blackbox}
We obtain the output from the original author for the following systems: M7 and M8 for KP20k; M5, M11, M12, and M14 for both KP20k and KPTimes. 

For the other unsupervised and supervised methods, we obtain the results on our own. For M1, M2, M3, and M4, we obtain the results using the \href{https://github.com/boudinfl/pke}{pke} library. For M6, M7, M8, M9, and M10, we use the original authors' implementations. For M13, we use the \href{https://github.com/uclanlp/DeepKPG}{DeepKPG} toolkit. 

For the commercial APIs from Amazon (M17) and Azure (M18), we implement the API call following the instructions. We referred to \href{https://docs.aws.amazon.com/comprehend/latest/dg/how-key-phrases.html}{this page} for M17 and \href{https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/key-phrase-extraction/quickstart?pivots=programming-language-python}{this page} for M18. We obtained the results on 3/5/2023 for M17 and 3/11/2023 for M18.

For GPT-3.5, we use the instruction-tuned \texttt{text-davinci-003} model. We always start the prompt with the following task definition: 
\begin{lstlisting}[]
Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases.
\end{lstlisting}
In the zero-shot setting, we then specify the document title and body in two separate lines, and start a new line with
\begin{lstlisting}[]
Keyphrases (separated by comma):
\end{lstlisting}
to finish the prompt. In the 5-shot setting, we randomly sample 5 examples from the train set for each test document, and add their title, body, and keyphrases in the same format to the prompt before the document tested. 

For all the systems, we truncate the body to 512 tokens. We apply a credit on newly registered accounts for M17 and M18. For GPT-3.5, we spent around \$1500 to obtain all the results.

\input{tables/human_eval_results}
\input{tables/all_results_ref_based}
\input{tables/all_results_ref_free}

\section{Details for the label variation study}
\label{annotation-details-study-1}
For this task, we only recruit "MTurk Master" annotators from US or UK with more than 100 HITs approved and more than 97\% approval rate. We present an annotation interface in Figure \ref{labeling-interface}. To determine the payment for each HIT, we first find several NLP researcher volunteers to perform a trial run and use their time as an estimate. Through this study, we determine that about 5 minutes per document is required to finish the task. Therefore, in the final version, crowd-source annotators are asked to annotate five document per HIT, which is labeled as \$5.0. In total, we spent around \$375 for this experiment.


\section{Details for the meta-evaluation study}
\label{annotation-details-study-2}
For this task, we add a qualification stage: workers are required to take a test on whether they understand the definition of keyphrases and whether they are able to follow the instructions on determining semantic similarity. For the qualification study, we recruit senior annotators from US or UK with more than 5000 HITs approved and more than 96\% approval rate. 150 annotators attempted the qualification test, among which 46 qualified annotators work for the evaluation study. 

We present an example in the instructions in Figure \ref{scoring-instructions} and the annotation interface in Figure \ref{scoring-interface}. Note that for this task, we make the concept of "keyphrases" transparent to the annotators because their main job is to evaluate semantic similarity. We also provide a number of example pairs to train the annotators on evaluating semantic similarity with partial credits. To determine the payment for each HIT, we first find several NLP researcher volunteers to perform a trial run and use their time as an estimate. Through this study, we determine that about 2 minutes per model is required to finish the task. Therefore, in the final version, crowd-source annotators are asked to annotate all the model outputs for a single document per HIT, which is labeled as \$3.0. In total, we spent around \$1200 for this experiment.

\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.9\textwidth]{figures/labeling_interface.png}}
\caption{An example of the annotation interface for the keyphrase annotation study in Section \ref{label-variation}.}
\label{labeling-interface}
\end{figure*}


\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.9\textwidth]{figures/scoring_instructions.png}}
\caption{An example of the annotation instructions for the keyphrase evaluation study in Section \ref{evaluation_human_study}.}
\label{scoring-instructions}
\end{figure*}


\begin{figure*}[h!]
\centering
\fbox{\includegraphics[width=0.9\textwidth]{figures/scoring_interface.png}}
\caption{An example of the annotation interface for the keyphrase evaluation study in Section \ref{evaluation_human_study}.}
\label{scoring-interface}
\end{figure*}