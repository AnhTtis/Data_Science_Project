\input{tables/grouped_results_all_dims}
% \input{tables/winners}

\section{Fine-grained re-evaluation of keyphrase extraction and generation systems}
\label{evaluation_human_study}

Using the proposed framework, we conduct a thorough re-evaluation of 18 keyphrase systems, grouped into keyphrase extraction (\textbf{KPE}, M1-M5), keyphrase generation with neural networks trained from scratch (\textbf{KPG-NN}, M6-M10), keyphrase generation with pre-trained language models (\textbf{KPG-PLM}, M11-M14), prompting \textbf{GPT-3.5} (M15-M16), and calling keyphrase extraction \textbf{API}s (M17-M18). Table \ref{tab:grouped-results-all-dims} presents the metric results on all dimensions. We also evaluate human-written keyphrase labels with reference-free metrics. The per-model performance is recorded in appendix \ref{all-evaluation}. % Table \ref{tab:winners} shows the highest performing model in all evaluated dimensions. 

\subsection{Reference-based evaluation}
From Table 4, we first observe that the performance underestimation problem of exact matching is alleviated by semantic matching, with the best model achieving around 0.6 $SemF1$ on KP20k and 0.8 on KPTimes. The difference between KPE and KPG models is also clearly distinguished compared to BertScore reported in \citet{koto-etal-2022-lipkey} and \citet{glazkova2022applying}. For all the reference-based metrics, we observe that the KPG-PLM family is consistently the best. However, comparing the best performing models in both families (Table \ref{tab:all-results-ref-based}), we find that they achieve the same level of coverage, while KPG-PLM models are better at generating phrases with higher saliency. 

For GPT-3.5, we observe that it achieves a competitive coverage in both 0-shot and 5-shot setting. The zero-shot setting achieves a low saliency as the model is not shown any demonstrations. With five examples, the saliency significantly increases. For the Amazon and Azure APIs, we find that they cannot outperform zero-shot prompting GPT-3.5 in the reference-based setting (appendix \ref{all-evaluation}).


\subsection{Reference-free evaluation}
\paragraph{Naturalness} We find that models trained on the sequence generation objective have the highest naturalness. By contrast, KPE models overall exhibit poor naturalness. Pre-trained language models such as BERT and BART do not significant outperform models trained from scratch on KP20k or KPTimes. Noteably, GPT-3.5 exhibits the best performance on KPTimes while performs worse than KPG-NN models on KP20k, suggesting that the model is more prone to generating unnatural phrases in the specialized science domain.

\paragraph{Faithfulness} In terms of faithfulness, GPT-3.5 leads the other models by a large margin. We hypothesize that the GPT-3.5 model has obtained a strong ability of extracting the concepts from the input with minimal paraphrasing. Surprisingly, KPE models outperform KPG-PLM/NN on KPTimes, but not on KP20k. One explanation is that KPE models' prediction may group words that do not belong to the same phrase, which may likely be deemed unfaithful in the scientific domain.

In addition, human references do not obtain a high score, which can be caused by humans writing more abstract absent keyphrases. This suggests that the metric model can be further improved for judging the faithfulness of concepts absent from the input. We also note that UniEval may inherently prefer similar pre-trained language models like GPT-3.5 and is not suitable as a single gold standard \citep{deutsch-etal-2022-limitations}.

\paragraph{Diversity} For diversity, we find that $dup\_token\_ratio$ and $embed\_sim$ do not always agree. While the former prefers humans and GPT-3.5, the latter prefers GPT-3.5 and keyphrase APIs. In addition, KPG-PLM models have much higher diversity compared to KPG-NN and KPE models. After a manual inspection, we find that the major reason is that KPE-PLM models generate much less duplications compared to KPG-NN even if greedy decoding is used for both. In addition, some KPE models use ranking heuristics that rank similar phrases together, causing a high duplication. From this perspective, methods that explicitly model topical diversity (such as M3) has a great advantage. 

\paragraph{Utility} The last two columns of Table \ref{tab:grouped-results-all-dims} show the performance of the keyphrase predictions for downstream document retrieval. For both datasets, we use the training and the testing corpus as $C$ and report the scores for $k=5$. It is notable that the \textbf{utility evaluation does not agree with reference-based evaluation}, i.e., scoring high against human references does not guarantee good downstream performance. For both retrieval effectiveness ($RR$) and efficiency ($Spare$), we observe that GPT-3.5 leads the other types of models by a large margin, indicating its an outstanding ability to pick useful keyphrases and rank them properly. Human-written keyphrases have the lowest $RR$, possibly due to the low number of keyphrases per example. This issue is mitigated with $Spare_5$, which always focuses on the top five keyphrases. 

\subsection{Discussion}
In the previous two subsections, we have benchmarked a range of keyphrase systems using the proposed framework. What insights can these results provide for NLP practitioners?

\paragraph{No one-winning-for-all model. Think about what you need.}
As shown earlier, different models have different assumptions and capabilities. Therefore, we define fine-grained goals and select proper tools for evaluating them. If a "human-like behavior" as defined by a large dataset is desired, then KPG-PLM models are the best choice. However, if the goal is to have a system that can generate natural, faithful, and diverse keyphrases for humans and robust to inputs from multiple domains, then prompting GPT-3.5 with a few examples should be preferred. If the keyphrases are merely used for IR tasks and inference cost is a concern, then a keyphrase extraction model such as M3 or M18 would be the best choice (Table \ref{tab:all-results-ref-free-kp20k} and \ref{tab:all-results-ref-free-kptimes}). 

\paragraph{Do not overtrust human references.} Our results suggest that judging the models based on their similarity to human references is insufficient for evaluating keyphrase systems. Particularly, we have shown that human references themselves are suboptimal in terms of diversity and utility. 
 

\paragraph{Nuances in model selection.} In general, model selection decisions are affected by both the metric and the benchmarking dataset. Our framework produces different model ranking than previous work such as \citet{ye-etal-2021-one2set} and \citet{2212.10233}. We thus warn that even for the same dimension, different metrics (e.g., F1 calculated by different matching strategies) have different connotations and one should not assumption a metric's ranking be identical to another. In addition, different model rankings can be concluded for highly specialized domains (such as KP20k) compared to general domains (such as KPTimes). 


% As a side note, for a more specialized domain (KP20k), we observe a higher retrieval performance (0.1 to 0.2 absolute points higher) than a more general domain (KPTimes), as scientific papers are more likely to be uniquely identified by specific keyphrases. 


% For instance, \citet{2212.10233} report a better performance of M14 compared to M10 in terms of F1@M, while the two have a similar $SemF1$. \citet{ye-etal-2021-one2set} report that M7 is more superior than M8, while we observe the reverse for $SemF1$. It shows that our metrics have different assumptions compared to $F1@k$ and their correlation should not be assumed in practice.

