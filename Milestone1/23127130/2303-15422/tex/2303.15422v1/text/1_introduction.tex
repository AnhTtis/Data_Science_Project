
\begin{figure*}[]
\includegraphics[width=\textwidth]{figures/kpeval_main_figure.pdf}
\caption{An illustration of the proposed framework. We consider six desired properties of keyphrase systems and design semantic-based metrics to faithfully evaluate them.}
\label{main-framework}
\vspace{-2mm}
\end{figure*}

\section{Introduction}
Building systems to automatically predict keyphrases of a document is a long lasting research problem in the Information Retrieval (IR) and the Natural Language Processing (NLP) communities \citep{witten1999kea, hulth-2003-improved, meng-etal-2017-deep}. With the availability of large annotated keyphrase datasets, pre-trained text representations, and deep learning-based methods, numerous keyphrase systems have been proposed, claiming significant performance improvements (\citet{meng-etal-2017-deep,boudin-2018-unsupervised,chan-etal-2019-neural}, \textit{i.a.}). 

However, their performance assessment is largely based on performing exact matching between stemmed predictions and human references. This approach is known to be insufficient, as it fails to credit synonyms and parent/child concepts that are spelled differently from the references \citep{zesch-gurevych-2009-approximate}. Despite this, there has been limited progress in adopting better metrics. Our literature survey of 64 papers between 2017 and 2022 (appendix \ref{kp-paper-eval-survey}) suggests that exact matching is still the primary evaluation method (used by 63/64 papers) and only 24/64 papers calculate other metrics to complement exact matching. In addition, these studies mainly focus on \textit{reference-based} evaluation, while ignoring \textit{reference-free} measures such as diversity \citep{wang-etal-2016-extracting, bahuleyan-el-asri-2020-diverse} or retrieval effectiveness \citep{boudin-gallina-2021-redefining} which might be more relevant for real-world applications of keyphrase systems. 

To address the drawbacks of exact matching, a few approaches have been proposed. One branch of solution uses n-gram matching \citep{kim-etal-2010-evaluating}, approximate matching \citep{zesch-gurevych-2009-approximate, luo-etal-2021-keyphrase-generation}, or evaluating with name variations \citep{chan-etal-2019-neural}. By design, these methods still struggle to recognize synonyms of keyphrases. Another approach relies on the semantic representation for matching \citep{jarmasz-and-barriere-2004}, including metrics based on pre-trained language models such as BertScore \citep{koto-etal-2022-lipkey,glazkova2022applying}. However, BertScore is a token-level matching score that may not adequately capture the semantics of individual keyphrases. In fact, we find that it's correlation with human judgments is even weaker than exact matching.

To better understand the strengths of different keyphrase systems, in this paper, we propose a novel \textit{fine-grained} \textit{semantic-based} keyphrase evaluation framework that addresses the limitations of existing methods. This framework considers six dimensions: \textbf{\textit{naturalness}}, \textbf{\textit{faithfulness}}, \textbf{\textit{saliency}}, \textbf{\textit{\textit{coverage}}}, \textbf{\textit{diversity}}, and \textbf{\textit{utility}}. For each dimension, we operationalize its measurement with advanced semantic-based metrics. To build strong phrase representations for evaluation, we design a novel keyphrase-aware contrastive learning scheme.

Through comprehensive human studies in the reference-based evaluation setting, we find that (1) label variations are prevalent in keyphrase annotation and exact matching struggles to handle them, and that (2) the proposed semantic matching metric outperforms exact matching by more than 0.15 absolute points in Kendall's Tau in terms of the correlation with human in evaluating saliency and coverage. More crucially, when used to match a phrase against a set of references, exact match has a much lower agreement with human compared to semantic matching, suggesting the latter's superior explainability. We also confirm that our phrase embedding model significantly outperforms a range of publicly available models in assigning high similarity to synonym pairs and low similarity to unrelated phrases, which is crucial for evaluating keyphrases.

Using this framework, we benchmark 18 unsupervised, supervised, and black-box keyphrase extraction and generation systems on two datasets, KP20k \citep{meng-etal-2017-deep} and KPTimes \citep{gallina-etal-2019-kptimes}. Notably, we find that (1) different types of models excel in different dimensions, with pre-trained models achieving the best in most dimensions; (2) the utility in downstream applications does not always agree with reference-based metrics; and (3) GPT-3.5 \citep{ouyang2022training} exhibits near state-of-the-art performance in a range of reference-free metrics. We hope our study can provide a novel perspective for rethinking the goal, design, and evaluation of keyphrase systems. Our implementation is available upon request and will be released when the paper is formally published.
