\section{A Fine-grained Evaluation Framework}
This section defines the keyphrase evaluation problem and proposes a multi-dimensional evaluation framework. We then introduce the metrics to evaluate each of the dimensions, including (1) semantic-based matching for saliency and coverage, (2) lexical and semantic overlap for diversity, (3) retrieval-based evaluation for utility, and (4) model-based evaluation for naturalness and faithfulness.

\subsection{Problem Formulation}
The \textit{keyphrase evaluation} problem for a single document can be formulated with a 4-element tuple $(\mathcal{X}, \mathcal{Y}, \mathcal{P}, \mathcal{C})$. $\mathcal{X}$ is the input document. $\mathcal{Y}=\{y_1,...,y_n\}$ is a list of $n$ reference phrases written by human. $\mathcal{P}=\{p_1,...,p_m\}$ is a list of $m$ predictions made by a model $\mathcal{M}$ on $\mathcal{X}$. To focus on semantic-based evaluation, we do not differentiate between keyphrases that appear in $\mathcal{X}$ ("present keyphrases") and those that do not ("absent keyphrases").  As the definition of keyphrases is domain-dependent, we introduce $\mathcal{C}$, a set of corpus documents, to represent the domain.

A \textit{keyphrase metric} is a function $f$ whose value $f(\mathcal{X}, \mathcal{Y}, \mathcal{P}, \mathcal{C})\in\mathbb{R}$ reflects a certain quality of $\mathcal{M}$. $f$ is a phrase-level metric if it is calculated as the average of metric values calculated on each $p_i\in\mathcal{P}$. $f$ is reference-free if its calculation is independent of $\mathcal{Y}$, or reference-based otherwise. In practice, the final score for $\mathcal{M}$ is often the averaged score over a set of testing documents. 

\subsection{How to define a good keyphrase system?}
Deciding the evaluation goals is crucial for building evaluation systems. We argue that the following six unique properties should be considered when evaluating keyphrase exaction or generation systems:

\begin{compactenum}
    \item \textbf{\textit{Naturalness}}: $p_i$ is a natural and grammatically correct phrase.
    \item \textbf{\textit{Faithfulness}}: all keyphrases in $\mathcal{P}$ are covered in $\mathcal{X}$, i.e., free of hallucination.
    \item \textbf{\textit{Saliency}}: $p_i$ carries salient information associated with $\mathcal{X}$. As the saliency varies with the domain, we use $\mathcal{Y}$ as a proxy for the set of salient information associated with $\mathcal{X}$.
    \item \textbf{\textit{Coverage}}: how much salient information in $\mathcal{X}$ is covered by $\mathcal{P}$.
    \item \textbf{\textit{Diversity}}: the degree to which $\mathcal{P}$ contains diverse concepts with minimal repetition.
    \item \textbf{\textit{Utility}}: the extent to which $\mathcal{P}$ facilitates downstream applications, such as retrieval.
\end{compactenum}

Table \ref{tab:kp-sys-property-assumptions} outlines the assumptions of these properties. Naturalness, faithfulness, and saliency are targeted at evaluating individual phrases, while coverage, diversity, and utility evaluate the entire set $\mathcal{P}$. Faithfulness and utility require $\mathcal{X}$, while saliency and coverage require $\mathcal{Y}$. To gauge the utility, $\mathcal{C}$ is required for specifying the domain. Next, we discuss how we operationalize these dimensions.


\subsection{Saliency and Coverage}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: semantically similar phrases should be credited; matching should be at phrase level.}}
}
\vspace{0.3pt}

Precision and recall with exact matching can capture saliency and coverage, while they struggle at dealing with semantic similarity. Conversely, BertScore \citep{bert-score} calculated with concatenated predictions and references captures semantic similarity, but its token-wise matching with contextual representations built in one encoding pass prevents evaluating the semantic similarity among individual phrases. To enjoy the advantage of both, we introduce \textit{phrase-level semantic-based} matching and define semantic precision ($SemP$), recall ($SemR$), and F1 ($SemF1$) as follows:
\begin{equation}
\resizebox{0.9\hsize}{!}{$SemP(\mathcal{P},\mathcal{Y})=\frac{\sum_{p\in\mathcal{P}}\max_{y\in\mathcal{Y}}(\mathbbm{1}(sim(p,y)>\alpha)\cdot sim(p,y))}{|\mathcal{P}|}$}\nonumber
\end{equation}
\begin{equation}
\resizebox{0.9\hsize}{!}{$SemR(\mathcal{P},\mathcal{Y})=\frac{\sum_{y\in\mathcal{Y}}\max_{p\in\mathcal{P}}(\mathbbm{1}(sim(p,y)>\alpha)\cdot sim(p,y))}{|\mathcal{Y}|}$}\nonumber
\end{equation}
\begin{equation}
\resizebox{0.75\hsize}{!}{$SemF1(\mathcal{P},\mathcal{Y})=\frac{2\cdot SemP(\mathcal{P},\mathcal{Y})\cdot SemR(\mathcal{P},\mathcal{Y})}{SemP(\mathcal{P},\mathcal{Y}) + SemR(\mathcal{P},\mathcal{Y})}$}\nonumber
\end{equation}
where $sim$ is a function defining the similarity between the representation of two phrases and $\alpha$ is a hyperparameter to filter out the noise in $sim$\footnote{We use 
$\alpha=0$ throughout the paper. $\alpha$ is included in the formulation to satisfy application-specific needs.}. To quantify whether $\mathcal{P}$ covers the overall semantics of $\mathcal{Y}$, we also introduce an overall semantic coverage index $SemCov$, defined as 
\begin{equation}
\resizebox{0.9\hsize}{!}{$SemCov(\mathcal{P}, \mathcal{Y})=sim(union(\mathcal{P}), union(\mathcal{Y}))$}\nonumber
\end{equation}
where $union$ is a function that computes a representation for a set of phrases.

\input{tables/dimension_assumptions}

To define the metrics, the key ingredient is the $sim$ function, which encodes two phrases and computes their similarity. While alternatives for the representation space exist, such as hyperbolic embedding \citep{song-etal-2022-hyperbolic}, we choose dense embedding and cosine similarity to take advantage of available models pre-trained with large corpus:
\begin{equation}
\resizebox{0.85\hsize}{!}{$sim(p,q)=cos\_sim(h_p,h_q)=\frac{h_p^Th_q}{||h_p||\cdot||h_q||}$}\nonumber,
\end{equation}
\begin{equation}
\resizebox{0.8\hsize}{!}{$union(\mathcal{P})=max\_pooling(h_{p_1}, ..., h_{p_m})$}\nonumber,
\end{equation}
where $h_p$ and $h_q$ are the representations for phrase $p$ and $q$ 
obtained by average pooling the embedding model's hidden state of the last layer. For $union$, we use max pooling to preserve the salient dimensions from each phrase. For the embedding model, we fine-tune a pre-trained sentence-level paraphrase model from \citet{reimers-gurevych-2019-sentence}\footnote{We download and fine-tune the model from \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}.} using a keyphrase-aware contrastive learning objective based on SimCSE \citep{gao-etal-2021-simcse}. Given a batch consisting of $B$ phrases, the training loss $\mathcal{L}_{simcse}$ is expressed as
\begin{equation}
\resizebox{0.9\hsize}{!}{$\mathcal{L}_{simcse}=\frac{1}{B}\sum_{i=1}^B -\log\frac{e^{sim(h_i, h_i')/\tau}}{\sum_{j=1}^Be^{sim(h_i,h_j')/\tau}}$}\nonumber,
\end{equation}
where $h_i$ and $h_i'$ are the representations of phrase $i$ obtained using two separate forward passes. The goal of this contrastive fine-tuning stage is to discourage the clustering of unrelated phrases in the representation space while maintaining a high alignment between semantically similar phrases. We fine-tune the model on $\mathcal{L}_{simcse}$ using 1.04 million keyphrases from the training set of KP20k \citep{meng-etal-2017-deep}, KPTimes \citep{gallina-etal-2019-kptimes}, StackEx \citep{yuan-etal-2020-one}, and OpenKP \citep{xiong-etal-2019-open}. The detailed training setup and hyperparameters are presented in the appendix \ref{phrase-embed-training}.

\subsection{Diversity}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: reward more semantically distinct concepts and penalize self-repetitions.}}
}
\vspace{0.3pt}

Generating unique concepts with minimal repetition is a desirable property of keyphrase systems. We modify one lexical and one semantic diversity metric used in  \citet{bahuleyan-el-asri-2020-diverse}. For the lexical metric, we use $dup\_token\_ratio$, the percentage of duplicate tokens after stemming. For the semantic metric, we calculate $emb\_sim$, the average of pairwise similarity, using the same phrase embedding model for saliency and coverage. 
\begin{equation}
\resizebox{0.85\hsize}{!}{$emb\_sim(\mathcal{P})=\frac{\sum_{i=1}^M\sum_{j=1}^M\mathbbm{1}(i\ne j)sim(p_i,p_j)}{M(M-1)}$}\nonumber
\end{equation}
Different from \citet{bahuleyan-el-asri-2020-diverse}, diversity evaluation is reference-free in our approach. While penalizing self-repetitions, our metrics do not penalize generating excessive new concepts\footnote{As a result, metrics such as the orthogonal regularization term used by CatSeqD \citep{yuan-etal-2020-one} are not suitable for our purposes, as the term naturally increases with $|\mathcal{P}|$.}.

\subsection{Utility}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: reward predictions that enable effective and efficient retrieval of the document.}}
}
\vspace{0.3pt}

Information Retrieval (IR) is an important downstream application for keyphrases \citep{10.1145/312624.312671, 10.1145/1141753.1141800, kim-etal-2013-applying}. To directly evaluate whether $\mathcal{M}$ can generate useful keyphrases for IR-related downstream tasks, we propose two metrics to measure (1) retrieval effectiveness and (2) retrieval efficiency of $\mathcal{P}$. 

Different from the query expansion setup in \citet{boudin-gallina-2021-redefining}, we use a setup similar to \citet{wu-etal-2022-representation} where the keyphrases are used as queries to retrieve $\mathcal{X}$ from $\mathcal{C}$. We measure the retrieval \textit{effectiveness} using the Reciprocal Rank at $k$ ($RR@k$), which is the reciprocal of $\mathcal{X}$'s rank if $\mathcal{X}$ can be retrieved in top $k$ documents using $\mathcal{P}$ as the query and 0 otherwise. $RR@k$ rewards high retrieval effectiveness, but does not require important phrases to be ranked high in $\mathcal{P}$. As a result, a sub-optimal model may achieve high $RR@k$ inefficiently by generating a lot of moderately salient keyphrases. To detect this behavior, we introduce a novel \textit{efficiency} metric $Spare_{base}@k$, defined as 
\begin{equation}
\resizebox{0.83\hsize}{!}{$Spare_{base}@k(\mathcal{X},\mathcal{P},\mathcal{C})=1-\frac{\min(base, j)}{base}$}\nonumber,
\end{equation}
where $j$ is the minimum index such that querying with $\{p_1, â€¦, p_j\}$ allows $\mathcal{X}$ to be retrieved in top $k$ documents. $base$ is the maximum number of phrases to consider. Intuitively, $\mathcal{P}$ with a high $Spare$ is able to retrieve $\mathcal{X}$ with fewer top-ranked phrases, thus having a higher retrieval efficiency.

We compute the scores by averaging results obtained from three retrieval systems: (1) BM25 \citep{10.1007/978-1-4471-2099-5_24}, (2) a single-document encoder for dense similarity search\footnote{We use \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}}, and (3) re-ranking (2)'s results using a cross-encoder\footnote{We use \url{https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2}}. 

% (1) BM25 \citep{10.1007/978-1-4471-2099-5_24}, (2) dense similarity search single-document encoder\footnote{We use \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}}, and (3) reranking the results from (2) with a cross-encoder\footnote{We use \url{https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2}}. 

\subsection{Naturalness and Faithfulness}
\fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: ill-formed phrases and hallucinations should be penalized.}}
}
\vspace{0.3pt}

% Traditional automatic evaluation of keyphrase systems overlooked naturalness and faithfulness. 

With the advance of neural methods that may produce more non-natural phrases or factual hallucinations, it is increasingly crucial to measure the naturalness and faithfulness of the predictions. To evaluate these two dimensions, we introduce a novel formulation that leverages metric models that are pre-trained to align with human preferences. Specifically, we use UniEval \citep{zhong-etal-2022-towards} that evaluates in a boolean QA fashion: the score is the probability of the model generating "Yes" given a prompt normalized by the probability of the model generating either "Yes" or "No". 

The model we use is pre-trained on a range of tasks including natural language inference, question answering, and linguistically related tasks, and then fine-tuned on evaluating the coherence, consistency, fluency, and relevance of summaries \citep{zhong-etal-2022-towards}\footnote{We use the model hosted at \url{https://huggingface.co/MingZhong/unieval-sum}.}. We leverage its expertise in fluency and consistency to evaluate naturalness and faithfulness of keyphrases. We convert the evaluation tasks into sentence-level inputs to better align with the model's knowledge. We use the following prompt for assessing the naturalness of $p_i$:
\begin{lstlisting}[]
question: Is this a natural utterance? </s> utterance: This is an article about p_i.
\end{lstlisting}
To evaluate the faithfulness of $p_i$ with respect to the input $\mathcal{X}$, we use the following prompt:
\begin{lstlisting}[]
question: Is this claim consistent with the document? </s> summary: the concept p_i is mentioned or described in the document. </s> document: X
\end{lstlisting}

% To summarize, we have introduced a comprehensive evaluation framework that evaluates six important properties of keyphrase systems. Its advantages include 1) performing fine-grained and semantic-based evaluation, 2) not relying on human supervision, and 3) highly interpretable for saliency, coverage, diversity, and utility. The rest of the paper presents human studies to support the proposed evaluation strategies and discusses empirical results of re-evaluating keyphrase systems.

In summary, our framework evaluates six key properties of keyphrase systems, offering fine-grained semantic evaluation with high interpretability. Next, we present human studies supporting our evaluation strategies and discuss empirical results from re-evaluating keyphrase systems.