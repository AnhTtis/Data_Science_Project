\section{Related work}
This section introduces previous advances in the evaluation of keyphrase systems. 

\paragraph{Reference-free evaluation} Early keyphrase systems adopt human evaluation to avoid the need for references \citep{barker2000using, matsuo2004keyword}. Another branch of work evaluates keyphrases by their utility in downstream applications such as retrieval \citep{Bracewell2005MultilingualSD, boudin-gallina-2021-redefining} or summarization \citep{litvak-last-2008-graph}. Other reference-free metrics include diversity \citep{bahuleyan-el-asri-2020-diverse} and descriptive statistics such as the number of keyphrase generated and their average lengths.
%  However, the studies often observe only moderate inter-annotator agreement and argued for the use of automatic metrics whenever human references are available. 

\paragraph{Reference-based evaluation} Traditionally, keyphrase extraction and keyphrase generation systems are evaluated with precision, recall, and F1 based on exact-match computed with human-specified references \citep{mihalcea-tarau-2004-textrank, meng-etal-2017-deep, yuan-etal-2020-one}. The metric uniformly penalizes unmatched predictions even though many of them are synonyms and child/parent concepts of some phrase in the reference. Later works attempt to solve the by relaxing the matching scheme. \citep{zesch-gurevych-2009-approximate} propose to use R-precision with approximate matching, which tolerates predictions that are substrings of some reference and vice versa. \citet{kim-etal-2010-evaluating} propose to use summarization metrics such as BLEU \citep{papineni-etal-2002-bleu} and Rouge \citep{lin-2004-rouge} that use n-gram matching. \citet{chan-etal-2019-neural} use outside knowledge sources to resolve name variations before evaluation. \citet{luo-etal-2021-keyphrase-generation} propose a heuristic fine-grained score that combines token-level matching, edit distance, and duplication penalty. \citet{koto-etal-2022-lipkey} and \citet{glazkova2022applying} apply the semantic-based token-level matching score BertScore \citep{bert-score} in their evaluation. In addition, a number of previous work apply ranking-based metrics such as Mean Reciprocal Rank (MRR), mean Averaged Precision (mAP), and Normalized Discounted Cumulative Gain (NDCG) \citep{florescu-caragea-2017-positionrank, boudin-2018-unsupervised, kim-etal-2021-structure}. These metrics can better evaluate the ranking strategy in ranking-based keyphrase extraction systems.

\paragraph{Meta-evaluation} There has been only limited studies conducted on evaluating automatic keyphrase metrics against human preferences. \citet{kim-etal-2010-evaluating} compared between BLEU, NIST, METEOR, Rouge, and R-precision, and concluded that R-precision has the highest Spearman correlation with human judgments. \citet{bougouin2016termith} annotated a meta-evaluation corpus with 400 documents in French, evaluating 3 keyphrase models in terms of  "appropriateness" and "silence", corresponding to "precision" and "1 - recall" in a broader sense. In Section \ref{meta-evaluation}, we present a modern meta-evaluation study of reference-based metrics, matching strategies, and phrase embeddings used for keyphrase evaluation.
  