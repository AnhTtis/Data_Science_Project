\section{Label variation challenges keyphrase evaluation: an annotation study}
\label{label-variation}

We conduct a human study that simulates an annotation setup to investigate three questions: (1) Is the definition of "keyphrases" agreed by annotators? (2) How much does keyphrase annotations vary? (3) How does this affect keyphrase evaluation?

We select the first 50 test documents from KP20k and KPTimes. Each document's labels are combined with keyphrases predicted by four systems:  M5 (BERT+CRF), M6 (CatSeq), M10 (SetTrans), and M13/M14 (in-domain BART with task-adaptive pre-training). Then, three annotators are presented with the title, body, and these seed phrases re-ordered alphabetically. They are then asked to write keyphrases that capture the most salient information. We explicitly clarify that they may select from the given phrases or write new ones. We use Amazon Mechanical Turk for this study and appendix \ref{annotation-details-study-1} presents further details.

\paragraph{Humans reach agreement on their inherent definition of keyphrases.} On average, two annotators agree on the same phrase in 30\% keyphrases written for KP20k and 39\% for KPTimes. On the unigram-level, the agreement rises to 64\% for KP20k and 62\% for KPTimes. In addition, the annotators exhibit a decent pairwise Kendall's Tau (0.47 for KP20k and 0.56 for KPTimes) and Spearman Correlation (0.52 for KP20k and 0.59 for KPTimes) for ranking the five sources by selection counts. These results support the feasibility of evaluating keyphrase systems through human studies.

\begin{figure}[]
\centering
\includegraphics[width=0.5\textwidth]{figures/annotation_preference.pdf}
\caption{Distribution of the sources of keyphrases written by the annotators. }
% \vspace{-2mm}
\label{annotation-preference}
\end{figure}
\paragraph{Humans use varying lexical forms to express the same key information.} Figure \ref{annotation-preference} and \ref{selection-preference} suggest that humans do not unanimously implicitly prefer selecting phrases coming from the original labels. The preference is implicit because the sources are transparent to the annotators. For the supervised models (M6, M10, and M13/14), annotators also select a number of phrases from the predictions that do not match with any label. The average character-level edit distance between these selected phrases and the closest phrase in label keyphrases is 11.0 for KP20k and 7.0 for KPTimes, much smaller than the metric for phrases that are not selected (17.5 for KP20k and 14.6 for KPTimes). This result implies that humans agree on similar high-level concepts but often use slightly different phrasings.

\begin{figure}[]
\centering
\includegraphics[width=0.49\textwidth]{figures/selection_preference.pdf}
\caption{Annotators' implicit selection preferences of keyphrase sources.}
% \vspace{-2mm}
\label{selection-preference}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=0.49\textwidth]{figures/human_vs_machine_scores.pdf}
\caption{Humans score poorly for exact matching F1 while ourperforms models in semantic coverage.}
% \vspace{-2mm}
\label{human-vs-machine-scores}
\end{figure}

\paragraph{This variation necessitates using metrics beyond exact matching.} Figure \ref{human-vs-machine-scores} presents the scores of the human annotations and the models' outputs calculated with the references provided by the dataset. We observe that F1@M based on exact matching fails to credit human-written keyphrases when they are not identical to the references, whereas semantic coverage suggests that the annotations have a high semantic overlap with the reference. This result further motivates the need for fine-grained and semantic-based evaluation.

% \paragraph{Keyphrase annotation is strongly affected by domain knowledge.} We find that annotators are more likely to write new keyphrases in KPTimes compared to KP20k (Figure \ref{annotation-preference}), and they write around 50\% more keyphrases with larger variation. This is possibly due to the annotators being more familiar with the news domain and thus more comfortable with creatively writing new keyphrases instead of copying from the candidates. This result highlights the importance of considering annotators' familiarity with the domain.