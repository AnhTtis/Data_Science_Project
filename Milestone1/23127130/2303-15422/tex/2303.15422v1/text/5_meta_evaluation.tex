
\section{Matching with label variation: a meta-evaluation study}
\label{meta-evaluation}

After confirming the label variation in keyphrase annotations, we conduct a rigorous meta-evaluation study of reference-based evaluation strategies for saliency and coverage. Specifically, we compare the precision, recall, and F1 scores of reference-based metrics against the human-labeled counterparts. In addition, we compare exact matching, substring matching, BertScore precision, and semantic matching with different embeddings for matching a phrase to a set of reference phrases. 

\subsection{Experiment setup}
We conduct a reference-based human evaluation of five representative models (M3, M6, M10, M13/M14, and M16) on KP20k and KPTimes. We select the first 50 documents from each dataset and ask three annotators to rate the semantic similarity between each predicted phrase and the closest phrase in the references, as well as between each reference phrase and its closest predicted phrase. The experiments are conducted on Amazon Mechanical Turk. Details about the protocol and pricing are provided in appendix \ref{annotation-details-study-2}.

We aggregate human annotations to \textbf{compute document-level precision, recall, and F1 scores}, and \textbf{semantic similarity decisions} for matching a single phrase to a set of phrases. We collect 1500 document-level annotations with 13401 phrase-level decisions. The interval Krippendorff's alpha is 0.735 for KP20k and 0.788 for KPTimes for matching predictions to sets of references, and 0.750 for KP20k and 0.741 for KPTimes for matching reference phrases to sets of predictions.

\input{tables/qualitative_study_matching}

\begin{figure*}[h!]
\includegraphics[width=0.98\textwidth]{figures/main_corr_kp20k_style1.pdf}
\caption{A comparison between the 95\% confidence intervals for the Kendall Tau between human and automatic metrics on KP20k. Exact Matching achieves the best correlation for Recall, and the proposed Semantic Matching is the best for precision and F1. We use input-level bootstrap resampling following \citet{deutsch-etal-2021-statistical}. }
% \vspace{-2mm}
\label{main-corr-kp20k}
\end{figure*}

\subsection{Phrase-level semantic-based matching aligns well with human preference}
Use the document-level annotations, we evaluate seven reference-based metrics: Exact Matching, Substring Matching, R-precision \citep{zesch-gurevych-2009-approximate}, Rouge-L \citep{lin-2004-rouge}, BertScore \citep{bert-score}, BartScore \citep{bart-score}, and the proposed Semantic Matching. Substring Matching is implemented as concluding a match between two phrases if any one is the substring of the other. Note that we apply Porter Stemmer \citep{Porter1980AnAF} before calculating Exact Matching, Substring Matching, and R-precision. 

We use input-level bootstrap resampling with 1000 samples to calculate the 95\% confidence interval of Kendall's Tau for KP20k, following \citet{deutsch-etal-2021-statistical}, and report the results in Figure \ref{main-corr-kp20k}. Note that the corresponding metric's precision, recall, and F1 are used to calculate the correlation in the Precision, Recall, and F1 subplots, except for R-precision. Surprisingly, we find that exact matching outperforms Rouge-L, BertScore, and BartScore, making it a strong baseline. The proposed semantic matching achieves state-of-the-art correlation with human in Precision and F1. Similar patterns are observed on KPTimes (appendix \ref{kptimes-meta-evaluation}).

Next, we use the single matching decisions to compare between different matching strategies. Table \ref{tab:meta-eval-matching} presents the pearson correlation ($r$), spearman correlation ($\rho$), and Kendall's Tau ($\tau$). Interestingly, we find that exact matching's high correlation with human in the aggregated score setting may be \textit{for a wrong reason}: phrase-level correlations suggest that the exact matching significantly underperforms semantic matching, with a difference of 10 absolute points in Kendall's Tau and more than 15 absolute points in Pearson correlation and Spearman correlation. BertScore precision achieves the lowest correlation in this setting, suggesting that it's not suitable for judging keyphrases.

\input{tables/meta_eval_matching_strategy}

\input{tables/meta_eval_matching_embedding}

Figure \ref{qualitative-study-matching} presents a qualitative analysis. Although a number of predictions are semantically related to the references, exact matching gives a score 0. On the other hand, substring matching and BertScore are overly permissive. The proposed semantic matching's output is closest to human evaluation and has a reasonable breakdown at the phrase level. $SemCov$ gives a reasonable estimation of the overall semantic coverage of the prediction. 

\subsection{Benchmarking embeddings for matching}
Next, we investigate the embedding choice for semantic matching. Using the same set of single-phrase matching decisions, we compare Phrase-BERT \citep{wang-etal-2021-phrase}, SpanBERT \citep{joshi-etal-2020-spanbert}, SimCSE \citep{gao-etal-2021-simcse}, sentence BERT (SBERT) \citep{reimers-gurevych-2019-sentence}, and our phrase embedding model. In addition, using the augmented KP20k test set in \citet{chan-etal-2019-neural} where keyphrases are linked to their name variations (KP20k-NV), we compute the alignment-like and uniformity-like properties of the embedding spaces \citep{wang2020understanding}. For alignment, we report the average cosine similarity between all name variation pairs. For uniformity, we calculate the metric with 50000 random keyphrase pairs. 

Table \ref{tab:meta-eval-embedding} reports the results for comparing phrase embeddings. Our model achieves the highest correlation with human when used for semantic matching. Its embedding also has the best ability to distinguish unrelated phrases, as shown by the highest difference between alignment and uniformity. The alignment results also sheds light on the semantics of our similarity score: name variation pairs are assigned a score of 0.58 on average. 
