% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{meng2019does,
  title={Does order matter? an empirical study on generating multiple keyphrases as a sequence},
  author={Meng, Rui and Yuan, Xingdi and Wang, Tong and Brusilovsky, Peter and Trischler, Adam and He, Daqing},
  journal={arXiv preprint arXiv:1909.03590},
  year={2019}
}

@article{Tang2017QALinkET,
  title={QALink: Enriching Text Documents with Relevant Q\&A Site Contents},
  author={Yixuan Tang and Weilong Huang and Qi Liu and Anthony K. H. Tung and Xiaoli Wang and Jisong Yang and Beibei Zhang},
  journal={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  url={https://leuchine.github.io/papers/cikm17.pdf},
  year={2017}
}

@incollection{zellers2019neuralfakenews,
title = {Defending Against Neural Fake News},
author = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {9054--9065},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9106-defending-against-neural-fake-news.pdf}
}

@inproceedings{wang2020understanding,
  title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  pages={9929--9939},
  year={2020},
  organization={PMLR}
}

@inproceedings{wu2022fast,
  title={Fast and constrained absent keyphrase generation by prompt-based learning},
  author={Wu, Huanqin and Ma, Baijiaxin and Liu, Wei and Chen, Tao and Nie, Dan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11495--11503},
  year={2022}
}

@inproceedings{witten1999kea,
  title={KEA: Practical automatic keyphrase extraction},
  author={Witten, Ian H and Paynter, Gordon W and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig G},
  booktitle={Proceedings of the fourth ACM conference on Digital libraries},
  pages={254--255},
  year={1999}
}

@article{2212.10233,
  doi = {10.48550/ARXIV.2212.10233},
  url = {https://arxiv.org/abs/2212.10233},
  author = {Wu, Di and Ahmad, Wasi Uddin and Chang, Kai-Wei},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{10.1145/312624.312671,
author = {Jones, Steve and Staveley, Mark S.},
title = {Phrasier: A System for Interactive Document Retrieval Using Keyphrases},
year = {1999},
isbn = {1581130961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/312624.312671},
doi = {10.1145/312624.312671},
booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {160–167},
numpages = {8},
keywords = {query interface, interactive retrieval interface, evaluation, keyphrase-based retrieval},
location = {Berkeley, California, USA},
series = {SIGIR '99}
}

@article{10.5555/1039791.1039794,
author = {Zhang, Yongzheng and Zincir-Heywood, Nur and Milios, Evangelos},
title = {World Wide Web Site Summarization},
year = {2004},
issue_date = {January 2004},
publisher = {IOS Press},
address = {NLD},
volume = {2},
number = {1},
issn = {1570-1263},
abstract = {Summaries of Web sites help Web users get an idea of the site contents without having to spend time browsing the sites. Currently, manually constructed summaries of Web sites by volunteer experts are available, such as the DMOZ Open Directory Project. This research is directed towards automating the Web site summarization task. To achieve this objective, an approach which applies machine learning and natural language processing techniques is developed to summarize a Web site automatically. The information content of the automatically generated summaries is compared, via a formal evaluation process involving human subjects, to DMOZ summaries, home page browsing and time-limited site browsing, for a number of academic and commercial Web sites. Statistical evaluation of the scores of the answers to a list of questions about the sites demonstrates that the automatically generated summaries convey the same information to the reader as DMOZ summaries do, and more information than the two browsing options.},
journal = {Web Intelli. and Agent Sys.},
month = {jan},
pages = {39–53},
numpages = {15},
url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.620&rep=rep1&type=pdf}
}

@inproceedings{10.3115/1220175.1220243,
author = {Hulth, Anette and Megyesi, Be\'{a}ta B.},
title = {A Study on Automatically Extracted Keywords in Text Categorization},
year = {2006},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1220175.1220243},
doi = {10.3115/1220175.1220243},
abstract = {This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization. In summary we show that a higher performance --- as measured by micro-averaged F-measure on a standard text categorization collection --- is achieved when the full-text representation is combined with the automatically extracted keywords. The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords. We also present results for experiments in which the keywords are the only input to the categorizer, either represented as unigrams or intact. Of these two experiments, the unigrams have the best performance, although neither performs as well as headlines only.},
booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics},
pages = {537–544},
numpages = {8},
location = {Sydney, Australia},
series = {ACL-44}
}

@InProceedings{zhang2020pegasus,
  title = 	 {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author =       {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11328--11339},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhang20ae.html},
  abstract = 	 {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.}
}


@inproceedings{Hammouda2005,
author = {Hammouda, Khaled and Matute, Diego and Kamel, Mohamed S.},
year = {2005},
month = {07},
pages = {265-274},
title = {CorePhrase: Keyphrase Extraction for Document Clustering},
booktitle = {International workshop on machine learning and data mining in pattern recognition} ,
isbn = {978-3-540-26923-6},
doi = {10.1007/11510888_26}
}

@inproceedings{10.1145/1367497.1367723,
author = {Wu, Xiaoyuan and Bolivar, Alvaro},
title = {Keyword Extraction for Contextual Advertisement},
year = {2008},
isbn = {9781605580852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1367497.1367723},
doi = {10.1145/1367497.1367723},
abstract = {As the largest online marketplace, eBay strives to promote its inventory throughout the Web via different types of online advertisement. Contextually relevant links to eBay assets on third party sites is one example of such advertisement avenues. Keyword extraction is the task at the core of any contextual advertisement system. In this paper, we explore a machine learning approach to this problem. The proposed solution uses linear and logistic regression models learnt from human labeled data, combined with document, text and eBay specific features. In addition, we propose a solution to identify the prevalent category of eBay items in order to solve the problem of keyword ambiguity.},
booktitle = {Proceedings of the 17th International Conference on World Wide Web},
pages = {1195–1196},
numpages = {2},
keywords = {contextual advertisement, keyword extraction},
location = {Beijing, China},
series = {WWW '08}
}

@inproceedings{10.1145/1871437.1871754,
author = {Dave, Kushal S. and Varma, Vasudeva},
title = {Pattern Based Keyword Extraction for Contextual Advertising},
year = {2010},
isbn = {9781450300995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1871437.1871754},
doi = {10.1145/1871437.1871754},
abstract = {Contextual Advertising (CA) refers to the placement of ads that are contextually related to the web page content. The science of CA deals with the task of finding advertising keywords from web pages. We present a different candidate selection method to extract advertising keywords from a web page. This method makes use of Part-of-Speech (POS) patterns that restrict the number of potential candidates a classifier has to handle. It fetches words/phrases that belong to the selected set of POS patterns. We design four systems based on chunking method and the features they use. These systems are trained on a naive Bayes classifier with a set of web pages annotated with 'advertising' keywords. The systems can then find advertising keywords from previously unseen web pages. Empirical evaluation shows that systems using the proposed chunking method perform better than the systems using N-Gram based chunking. All improvements in the systems are found statistically significant at a 99% confidence interval.},
booktitle = {Proceedings of the 19th ACM International Conference on Information and Knowledge Management},
pages = {1885–1888},
numpages = {4},
keywords = {contextual advertising, keyword extraction},
location = {Toronto, ON, Canada},
series = {CIKM '10}
}

@inproceedings{10.5555/1620163.1620205,
author = {Wan, Xiaojun and Xiao, Jianguo},
title = {Single Document Keyphrase Extraction Using Neighborhood Knowledge},
year = {2008},
isbn = {9781577353683},
publisher = {AAAI Press},
abstract = {Existing methods for single document keyphrase extraction usually make use of only the information contained in the specified document. This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results demonstrate the good effectiveness and robustness of our proposed approach.},
booktitle = {Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2},
pages = {855–860},
numpages = {6},
location = {Chicago, Illinois},
url = {https://www.aaai.org/Papers/AAAI/2008/AAAI08-136.pdf},
series = {AAAI'08}
}

@InProceedings{boudin:2016:COLINGDEMO,
  author    = {Boudin, Florian},
  title     = {pke: an open source python-based keyphrase extraction toolkit},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations},
  month     = {December},
  year      = {2016},
  address   = {Osaka, Japan},
  pages     = {69--73},
  url       = {http://aclweb.org/anthology/C16-2015}
}

@misc{1910.09700,
  doi = {10.48550/ARXIV.1910.09700},
  
  url = {https://arxiv.org/abs/1910.09700},
  
  author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  
  keywords = {Computers and Society (cs.CY), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Quantifying the Carbon Emissions of Machine Learning},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@inproceedings{Chen2019TitleGuidedEF,
  title={Title-Guided Encoding for Keyphrase Generation},
  author={Wang Chen and Yifan Gao and Jiani Zhang and Irwin King and Michael R. Lyu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/4587/4465},
  year={2019}, 
  month={Jul.}, 
  pages={6268-6275},
  volume={33}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{DBLP:journals/corr/abs-1901-08746,
  author    = {Jinhyuk Lee and
               Wonjin Yoon and
               Sungdong Kim and
               Donghyeon Kim and
               Sunkyu Kim and
               Chan Ho So and
               Jaewoo Kang},
  title     = {BioBERT: a pre-trained biomedical language representation model for
               biomedical text mining},
  journal   = {CoRR},
  volume    = {abs/1901.08746},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.08746},
  eprinttype = {arXiv},
  eprint    = {1901.08746},
  timestamp = {Sat, 23 Jan 2021 01:13:47 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-08746.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{10.3115/1119355.1119383,
author = {Hulth, Anette},
title = {Improved Automatic Keyword Extraction given More Linguistic Knowledge},
year = {2003},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1119355.1119383},
doi = {10.3115/1119355.1119383},
abstract = {In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed. The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on statistics (such as term frequency and n-grams), a better result is obtained as measured by keywords previously assigned by professional indexers. In more detail, extracting NP-chunks gives a better precision than n-grams, and by adding the PoS tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.},
booktitle = {Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing},
pages = {216–223},
numpages = {8},
series = {EMNLP '03}
}

@inproceedings{Krapivin2009LargeDF,
  title={Large Dataset for Keyphrases Extraction},
  author={Mikalai Krapivin and Aliaksandr Autaeu and Maurizio Marchese},
  year={2009}, 
  url={http://eprints.biblio.unitn.it/1671/1/disi09055-krapivin-autayeu-marchese.pdf},
  note={Technical report, University of Trento}
}

@InProceedings{10.1007/978-3-540-77094-7_41,
author="Nguyen, Thuy Dung
and Kan, Min-Yen",
editor="Goh, Dion Hoe-Lian
and Cao, Tru Hoang
and S{\o}lvberg, Ingeborg Torvik
and Rasmussen, Edie",
title="Keyphrase Extraction in Scientific Publications",
booktitle="Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="317--326",
url={https://www.comp.nus.edu.sg/~kanmy/papers/icadl2007.pdf},
abstract="We present a keyphrase extraction algorithm for scientific publications. Different from previous work, we introduce features that capture the positions of phrases in document with respect to logical sections found in scientific discourse. We also introduce features that capture salient morphological phenomena found in scientific keyphrases, such as whether a candidate keyphrase is an acronyms or uses specific terminologically productive suffixes. We have implemented these features on top of a baseline feature set used by Kea [1]. In our evaluation using a corpus of 120 scientific publications multiply annotated for keyphrases, our system significantly outperformed Kea at the p{\thinspace}<{\thinspace}.05 level. As we know of no other existing multiply annotated keyphrase document collections, we have also made our evaluation corpus publicly available. We hope that this contribution will spur future comparative research.",
isbn="978-3-540-77094-7"
}

@article{liu2020keyphrase,
  author={Liu, Rui and Lin, Zheng and Wang, Weiping},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Addressing Extraction and Generation Separately: Keyphrase Prediction With Pre-Trained Language Models}, 
  year={2021},
  volume={29},
  number={},
  pages={3180-3191},
  doi={10.1109/TASLP.2021.3120587}}

@article{sahrawat2019keyphrase,
      title={Keyphrase Extraction from Scholarly Articles as Sequence Labeling using Contextualized Embeddings}, 
      author={Dhruva Sahrawat and Debanjan Mahata and Mayank Kulkarni and Haimin Zhang and Rakesh Gosangi and Amanda Stent and Agniv Sharma and Yaman Kumar and Rajiv Ratn Shah and Roger Zimmermann},
      year={2019},
      eprint={1910.08840},
      archivePrefix={arXiv},
      journal={ArXiv},
      primaryClass={cs.CL}
}

@misc{guu2020realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{shen2021unsupervised, 
title={Unsupervised Deep Keyphrase Generation}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/21381}, DOI={10.1609/aaai.v36i10.21381}, 
abstractNote={Keyphrase generation aims to summarize long documents with a collection of salient phrases. Deep neural models have demonstrated remarkable success in this task, with the capability of predicting keyphrases that are even absent from a document. However, such abstractiveness is acquired at the expense of a substantial amount of annotated data. In this paper, we present a novel method for keyphrase generation, AutoKeyGen, without the supervision of any annotated doc-keyphrase pairs. Motivated by the observation that an absent keyphrase in a document may appear in other places, in whole or in part, we construct a phrase bank by pooling all phrases extracted from a corpus. With this phrase bank, we assign phrase candidates to new documents by a simple partial matching algorithm, and then we rank these candidates by their relevance to the document from both lexical and semantic perspectives. Moreover, we bootstrap a deep generative model using these top-ranked pseudo keyphrases to produce more absent candidates. Extensive experiments demonstrate that AutoKeyGen outperforms all unsupervised baselines and can even beat a strong supervised method in certain cases.}, 
number={10}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Shen, Xianjie and Wang, Yinghan and Meng, Rui and Shang, Jingbo}, year={2022}, 
month={Jun.}, 
pages={11303-11311} }

@inproceedings{textrank2004,
author = {Hulth and Anette},
title={TextRank: Bringing Order into Texts}, 
year = {2004},
publisher = {Association for Computational Linguistics},
url = {https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf},
abstract = {In this paper, we introduce TextRank – a graph-based
ranking model for text processing, and show how this
model can be successfully used in natural language
applications. In particular, we propose two innovative unsupervised methods for keyword and sentence
extraction, and show that the results obtained compare favorably with previously published results on
established benchmarks.},
booktitle = {Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing},
pages = {404-411},
series = {EMNLP '04}
}

@ARTICLE{8954611,  author={Sun, Yi and Qiu, Hangping and Zheng, Yu and Wang, Zhongwei and Zhang, Chaoran},  journal={IEEE Access},   title={SIFRank: A New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model},   year={2020},  volume={8},  number={},  pages={10896-10906},  doi={10.1109/ACCESS.2020.2965087}}

@ARTICLE{tf-idf,  author={Jones, Karen Sparck},  journal={Journal of Documentation},   title={A statistical interpretation of term specificity and its application in retrieval},   year={1972},  volume={28},  number={1},  pages={11-21},  doi={10.1108/eb026526}}

@article{kulkarni2021learning,
  title={Learning Rich Representation of Keyphrases from Text},
  author={Mayank Kulkarni and Debanjan Mahata and Ravneet Arora and Rajarshi Bhowmik},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.08547},
  url = {https://arxiv.org/pdf/2112.08547.pdf}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  doi="10.48550/arXiv.1910.10683",
}

@InProceedings{10.1007/978-1-4471-2099-5_24,
author="Robertson, S. E.
and Walker, S.",
editor="Croft, Bruce W.
and van Rijsbergen, C. J.",
title="Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval",
booktitle="SIGIR '94",
year="1994",
publisher="Springer London",
address="London",
pages="232--241",
abstract="The 2-Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.",
isbn="978-1-4471-2099-5",
url="https://dl.acm.org/doi/pdf/10.5555/188490.188561",
}

@article{Porter1980AnAF,
  title={An algorithm for suffix stripping},
  author={Martin F. Porter},
  journal={Program},
  year={1980},
  volume={40},
  pages={211-218},
  doi = "10.1108/00330330610681286",
}

@inproceedings{xorqa,
    title   = {{XOR} {QA}: Cross-lingual Open-Retrieval Question Answering},
    author  = {Akari Asai and Jungo Kasai and Jonathan H. Clark and Kenton Lee and Eunsol Choi and Hannaneh Hajishirzi},
    booktitle={NAACL-HLT},
    year    = {2021}
}

@inproceedings{wan2008single,
  title={Single document keyphrase extraction using neighborhood knowledge.},
  author={Wan, Xiaojun and Xiao, Jianguo},
  booktitle={AAAI},
  volume={8},
  pages={855--860},
  year={2008}
}

@misc{arxiv.1910.08840,
  doi = {10.48550/ARXIV.1910.08840},
  
  url = {https://arxiv.org/abs/1910.08840},
  
  author = {Sahrawat, Dhruva and Mahata, Debanjan and Kulkarni, Mayank and Zhang, Haimin and Gosangi, Rakesh and Stent, Amanda and Sharma, Agniv and Kumar, Yaman and Shah, Rajiv Ratn and Zimmermann, Roger},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Keyphrase Extraction from Scholarly Articles as Sequence Labeling using Contextualized Embeddings},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{bougouin2016termith,
  title={Termith-eval: a french standard-based resource for keyphrase extraction evaluation},
  author={Bougouin, Adrien and Barreaux, Sabine and Romary, Laurent and Boudin, Florian and Daille, B{\'e}atrice},
  booktitle={LREC-Language Resources and Evaluation Conference},
  year={2016}
}

@inproceedings{barker2000using,
  title={Using noun phrase heads to extract document keyphrases},
  author={Barker, Ken and Cornacchia, Nadia},
  booktitle={Advances in Artificial Intelligence: 13th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, AI 2000 Mont{\'e}al, Quebec, Canada, May 14--17, 2000 Proceedings 13},
  pages={40--52},
  year={2000},
  organization={Springer}
}

@article{matsuo2004keyword,
  title={Keyword extraction from a single document using word co-occurrence statistical information},
  author={Matsuo, Yutaka and Ishizuka, Mitsuru},
  journal={International Journal on Artificial Intelligence Tools},
  volume={13},
  number={01},
  pages={157--169},
  year={2004},
  publisher={World Scientific}
}

@article{Bracewell2005MultilingualSD,
  title={Multilingual single document keyword extraction for information retrieval},
  author={D. Bracewell and F. Ren and S. Kuriowa},
  journal={2005 International Conference on Natural Language Processing and Knowledge Engineering},
  year={2005},
  pages={517-522}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@INPROCEEDINGS{9481005,
  author={Dascalu, Cristian and Trăuşan-Matu, Ştefan},
  booktitle={2021 23rd International Conference on Control Systems and Computer Science (CSCS)}, 
  title={Experiments with Contextualized Word Embeddings for Keyphrase Extraction}, 
  year={2021},
  volume={},
  number={},
  pages={447-452},
  doi={10.1109/CSCS52396.2021.00079}}
  
@ARTICLE{arxiv.2004.10462,
  doi={10.1109/TASLP.2021.3120587},
  author = {Liu, Rui and Lin, Zheng and Wang, Weiping},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Keyphrase Prediction With Pre-trained Language Model},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year = {2021},
  volume={29},
  pages={3180-3191},
}

@INPROCEEDINGS{arxiv.2203.08118,
  doi = {10.48550/ARXIV.2203.08118},
  
  url = {https://arxiv.org/abs/2203.08118},
  
  author = {Wu, Di and Ahmad, Wasi Uddin and Dev, Sunipa and Chang, Kai-Wei},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Representation Learning for Resource-Constrained Keyphrase Generation},
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
  year = {2022},
}

@INPROCEEDINGS{arxiv.1905.03197,
    author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
    title = {Unified Language Model Pre-Training for Natural Language Understanding and Generation},
    year = {2019},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
    articleno = {1170},
    numpages = {13}
}

@article{Lee_2019,
	doi = {10.1093/bioinformatics/btz682},
  
	url = {https://doi.org/10.1093%2Fbioinformatics%2Fbtz682},
  
	year = 2019,
	month = {sep},
  
	publisher = {Oxford University Press ({OUP})},
  
	author = {Jinhyuk Lee and Wonjin Yoon and Sungdong Kim and Donghyeon Kim and Sunkyu Kim and Chan Ho So and Jaewoo Kang},
  
	editor = {Jonathan Wren},
  
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
  
	journal = {Bioinformatics}
}

@article{Gu_2022,
	doi = {10.1145/3458754},
  
	url = {https://doi.org/10.1145%2F3458754},
  
	year = 2022,
	month = {jan},
  
	publisher = {Association for Computing Machinery ({ACM})},
  
	volume = {3},
  
	number = {1},
  
	pages = {1--23},
  
	author = {Yu Gu and Robert Tinn and Hao Cheng and Michael Lucas and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},
  
	title = {Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing},
  
	journal = {{ACM} Transactions on Computing for Healthcare}
}

@misc{2010.09885,
  doi = {10.48550/ARXIV.2010.09885},
  
  url = {https://arxiv.org/abs/2010.09885},
  
  author = {Chithrananda, Seyone and Grand, Gabriel and Ramsundar, Bharath},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Chemical Physics (physics.chem-ph), Biomolecules (q-bio.BM), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, FOS: Biological sciences, FOS: Biological sciences, I.2.7; I.2.1; J.2; J.3},
  
  title = {ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{2006.08097,
    doi = {10.5555/3491440.3492062},
    url = {https://dl.acm.org/doi/abs/10.5555/3491440.3492062},
    author = {Liu, Zhuang and Huang, Degen and Huang, Kaiyu and Li, Zhuang and Zhao, Jun},
    title = {FinBERT: A Pre-Trained Financial Language Representation Model for Financial Text Mining},
    year = {2021},
    isbn = {9780999241165},
    booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
    articleno = {622},
    numpages = {7},
    location = {Yokohama, Yokohama, Japan},
    series = {IJCAI'20}
}

@INPROCEEDINGS{9443960,
  author={Liu, Rui and Lin, Zheng and Fu, Peng and Wang, Weiping},
  booktitle={2020 IEEE Intl Conf on Parallel \& Distributed Processing with Applications, Big Data \& Cloud Computing, Sustainable Computing \& Communications, Social Computing \& Networking (ISPA/BDCloud/SocialCom/SustainCom)}, 
  title={Reinforced Keyphrase Generation with BERT-based Sentence Scorer}, 
  year={2020},
  pages={1-8},
  doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom51426.2020.00027}
 }
 
@misc{2201.05302,
  doi = {10.48550/ARXIV.2201.05302},
  
  url = {https://arxiv.org/abs/2201.05302},
  
  author = {Chowdhury, Md Faisal Mahbub and Rossiello, Gaetano and Glass, Michael and Mihindukulasooriya, Nandana and Gliozzo, Alfio},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Applying a Generic Sequence-to-Sequence Model for Simple and Effective Keyphrase Generation},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{2109.15290,
  doi = {10.48550/ARXIV.2109.15290},
  
  url = {https://arxiv.org/abs/2109.15290},
  
  author = {Gupta, Tanishq and Zaki, Mohd and Krishnan, N. M. Anoop and {Mausam}
},
  
  keywords = {Computation and Language (cs.CL), Materials Science (cond-mat.mtrl-sci), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@inproceedings{10.1145/1141753.1141800,
author = {Song, Min and Song, Il Yeol and Allen, Robert B. and Obradovic, Zoran},
title = {Keyphrase Extraction-Based Query Expansion in Digital Libraries},
year = {2006},
isbn = {1595933549},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1141753.1141800},
doi = {10.1145/1141753.1141800},
abstract = {In pseudo-relevance feedback, the two key factors affecting the retrieval performance most are the source from which expansion terms are generated and the method of ranking those expansion terms. In this paper, we present a novel unsupervised query expansion technique that utilizes keyphrases and POS phrase categorization. The keyphrases are extracted from the retrieved documents and weighted with an algorithm based on information gain and co-occurrence of phrases. The selected keyphrases are translated into Disjunctive Normal Form (DNF) based on the POS phrase categorization technique for better query refomulation. Furthermore, we study whether ontologies such as WordNet and MeSH improve the retrieval performance in conjunction with the keyphrases. We test our techniques on TREC 5, 6, and 7 as well as a MEDLINE collection. The experimental results show that the use of keyphrases with POS phrase categorization produces the best average precision.},
booktitle = {Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries},
pages = {202–209},
numpages = {8},
keywords = {keyphrase extraction, POS, query expansion, WordNet, information gain},
location = {Chapel Hill, NC, USA},
series = {JCDL '06}
}

@misc{arxiv.1907.11692,
  doi = {10.48550/ARXIV.1907.11692},
  
  url = {https://arxiv.org/abs/1907.11692},
  
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{10.5555/645530.655813,
author = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
title = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
year = {2001},
isbn = {1558607781},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
pages = {282–289},
numpages = {8},
series = {ICML '01}
}

@misc{arxiv.2002.12804,
  doi = {10.48550/ARXIV.2002.12804},
  
  url = {https://arxiv.org/abs/2002.12804},
  
  author = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Piao, Songhao and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{arxiv.1908.08962,
  doi = {10.48550/ARXIV.1908.08962},
  
  url = {https://arxiv.org/abs/1908.08962},
  
  author = {Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Campos2018YAKECA,
  title={Yake! collection-independent automatic keyword extractor},
  author={Campos, Ricardo and Mangaravite, V{\'\i}tor and Pasquali, Arian and Jorge, Al{\'\i}pio M{\'a}rio and Nunes, C{\'e}lia and Jatowt, Adam},
  booktitle={European Conference on Information Retrieval},
  pages={806--810},
  year={2018},
  organization={Springer}
}

@inproceedings{Kea1999,
author = {Witten, Ian and Paynter, Gordon and Frank, Eibe and Gutwin, Carl and Nevill-Manning, Craig},
year = {1999},
month = {08},
pages = {254-255},
booktitle={Proceedings of the fourth ACM conference on Digital libraries},
doi = {10.1145/313238.313437}
}

@misc{sportsbert,
  author = {Srinivasan,  Prithvishankar and Mashetty, Santosh},
  title = {SportsBERT},
  howpublished = "\url{https://huggingface.co/microsoft/SportsBERT}"
}

@misc{phan2021scifive,
      title={SciFive: a text-to-text transformer model for biomedical literature}, 
      author={Long N. Phan and James T. Anibal and Hieu Tran and Shaurya Chanana and Erol Bahadroglu and Alec Peltekian and Grégoire Altan-Bonnet},
      year={2021},
      eprint={2106.03598},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{johnson2016mimic,
  title={MIMIC-III, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{gupta2022matscibert,
  title={MatSciBERT: A materials domain language model for text mining and information extraction},
  author={Gupta, Tanishq and Zaki, Mohd and Krishnan, NM and others},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={1--11},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{houbre2022large,
  title={A Large-Scale Dataset for Biomedical Keyphrase Generation},
  author={Houbre, Mael and Boudin, Florian and Daille, Beatrice},
  journal={arXiv preprint arXiv:2211.12124},
  year={2022}
}

@article{singh2020open4business,
  title={Open4Business (O4B): An Open Access Dataset for Summarizing Business Documents},
  author={Singh, Amanpreet and Balasubramanian, Niranjan},
  journal={arXiv preprint arXiv:2011.07636},
  year={2020}
}

@article{ju2020CovidDialog,
  title={CovidDialog: Medical Dialogue Datasets about COVID-19},
  author={Ju, Zeqian and Chakravorty, Subrato and He, Xuehai and Chen, Shu and Yang, Xingyi and Xie, Pengtao},
  journal={ https://github.com/UCSD-AI4H/COVID-Dialogue}, 
  year={2020}
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  year={2020},
  booktitle={International Conference on Learning Representations},
  url={https://openreview.net/pdf?id=SkeHuCVFDr}
}

@inproceedings{bart-score,
 author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27263--27277},
 publisher = {Curran Associates, Inc.},
 title = {BARTScore: Evaluating Generated Text as Text Generation},
 url = {https://proceedings.neurips.cc/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{sinha2015an,
author = {Sinha, Arnab and Shen, Zhihong and Song, Yang and Ma, Hao and Eide, Darrin and Hsu, Bo-June Paul and Wang, Kuansan},
title = {An Overview of Microsoft Academic Service (MAS) and Applications},
booktitle = {International World Wide Web Conferences},
year = {2015},
month = {May},
abstract = {In this paper we describe a new release of a Web scale entity graph that serves as the backbone of Microsoft Academic Service (MAS), a major production effort with a broadened scope to the namesake vertical search engine that has been publicly available since 2008 as a research prototype. At the core of MAS is a heterogeneous entity graph comprised of six types of entities that model the scholarly activities: field of study, author, institution, paper, venue, and event. In addition to obtaining these entities from the publisher feeds as in the previous effort, we in this version include data mining results from the Web index and an in-house knowledge base from Bing, a major commercial search engine. As a result of the Bing integration, the new MAS graph sees significant increase in size, with fresh information streaming in automatically following their discoveries by the search engine. In addition, the rich entity relations included in the knowledge base provide additional signals to disambiguate and enrich the entities within and beyond the academic domain. The number of papers indexed by MAS, for instance, has grown from low tens of millions to 83 million while maintaining an above 95% accuracy based on test data sets derived from academic activities at Microsoft Research. Based on the data set, we demonstrate two scenarios in this work: a knowledge driven, highly interactive dialog that seamlessly combines reactive search and proactive suggestion experience, and a proactive heterogeneous entity recommendation.},
publisher = {Microsoft},
url = {https://www.microsoft.com/en-us/research/publication/overview-microsoft-academic-service-mas-applications/},
}

@article{Li2016BioCreativeVC,
  title={BioCreative V CDR task corpus: a resource for chemical disease relation extraction},
  author={Jiao Li and Yueping Sun and Robin J. Johnson and Daniela Sciaky and Chih-Hsuan Wei and Robert Leaman and Allan Peter Davis and Carolyn J. Mattingly and Thomas C. Wiegers and Zhiyong Lu},
  journal={Database: The Journal of Biological Databases and Curation},
  year={2016},
  volume={2016}
}

@article{Dogan2014NCBIDC,
  title={NCBI disease corpus: A resource for disease name recognition and concept normalization},
  author={Rezarta Islamaj Dogan and Robert Leaman and Zhiyong Lu},
  journal={Journal of biomedical informatics},
  year={2014},
  volume={47},
  pages={
          1-10
        }
}

@article{huang2022batterybert,
  title={BatteryBERT: A Pretrained Language Model for Battery Database Enhancement},
  author={Huang, Shu and Cole, Jacqueline M},
  journal={Journal of Chemical Information and Modeling},
  year={2022},
  publisher={ACS Publications}
}

@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

@article{huang2019clinicalbert,
  title={Clinicalbert: Modeling clinical notes and predicting hospital readmission},
  author={Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  journal={arXiv preprint arXiv:1904.05342},
  year={2019}
}

@article{TREWARTHA2022100488,
title = {Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science},
journal = {Patterns},
volume = {3},
number = {4},
pages = {100488},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100488},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000733},
author = {Amalie Trewartha and Nicholas Walker and Haoyan Huo and Sanghoon Lee and Kevin Cruse and John Dagdelen and Alexander Dunn and Kristin A. Persson and Gerbrand Ceder and Anubhav Jain},
keywords = {NLP, NER, BERT, transformer, language model, pre-train, materials science, solid-state, doping, gold nanoparticles  07.00.00, 81.00.00},
abstract = {Summary
A bottleneck in efficiently connecting new materials discoveries to established literature has arisen due to an increase in publications. This problem may be addressed by using named entity recognition (NER) to extract structured summary-level data from unstructured materials science text. We compare the performance of four NER models on three materials science datasets. The four models include a bidirectional long short-term memory (BiLSTM) and three transformer models (BERT, SciBERT, and MatBERT) with increasing degrees of domain-specific materials science pre-training. MatBERT improves over the other two BERTBASE-based models by 1%∼12%, implying that domain-specific pre-training provides measurable advantages. Despite relative architectural simplicity, the BiLSTM model consistently outperforms BERT, perhaps due to its domain-specific pre-trained word embeddings. Furthermore, MatBERT and SciBERT models outperform the original BERT model to a greater extent in the small data limit. MatBERT’s higher-quality predictions should accelerate the extraction of structured data from materials science literature.}
}

@article{biogpt,
    author = {Luo, Renqian and Sun, Liai and Xia, Yingce and Qin, Tao and Zhang, Sheng and Poon, Hoifung and Liu, Tie-Yan},
    title = "{BioGPT: generative pre-trained transformer for biomedical text generation and mining}",
    journal = {Briefings in Bioinformatics},
    volume = {23},
    number = {6},
    year = {2022},
    month = {09},
    abstract = "{Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98\\%, 38.42\\% and 40.76\\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2\\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.}",
    issn = {1477-4054},
    doi = {10.1093/bib/bbac409},
    url = {https://doi.org/10.1093/bib/bbac409},
    note = {bbac409},
    eprint = {https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf},
}

@article{taylor2022galactica,
  title={Galactica: A large language model for science},
  author={Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  journal={arXiv preprint arXiv:2211.09085},
  year={2022}
}

@inproceedings{jarmasz-and-barriere-2004,
author = {Mario Jarmasz and Caroline Barri{\`{e}}re},
title = {Keyphrase Extraction: Enhancing List},
year = {2004},
booktitle = {Proceedings of the 2nd Conference on Computational Linguistics in the North-East (CLiNE 2004)},
url = {https://arxiv.org/pdf/1204.0255.pdf},
location = {Montréal, Canada},
}

@article{glazkova2022applying,
  title={Applying transformer-based text summarization for keyphrase generation},
  author={Glazkova, Anna and Morozov, Dmitry},
  journal={arXiv preprint arXiv:2209.03791},
  year={2022}
}