\section{\textsc{KPEval}: Reference-Based Evaluation with Semantic Matching}
\label{ref-based-eval}

To begin with, we focus on reference agreement, the most extensively investigated aspect. Recognizing the limitations of previous approaches, we introduce a semantic matching formulation and conduct meta-evaluation to confirm its effectiveness.

\subsection{Reference Agreement: Metric Design}
\label{saliency-metric-design}
% \fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
% \fcolorbox{purple!20}{purple!5}{\parbox{0.47\textwidth}{
\fcolorbox{teal!30}{green!10}{\parbox{0.465\textwidth}{
% \fcolorbox{orange!30}{orange!10}{\parbox{0.47\textwidth}{
\textit{\textbf{Desiderata}: a prediction should be credited if it is semantically similar to a human-written keyphrase; matching should be at phrase-level.}}
}
\vspace{0.3pt}

Despite the prevalent use of existing reference-based metrics, their designs harbor intrinsic limitations. On one hand, $F1@5$ \citep{meng-etal-2017-deep} and $F1@M$ \citep{yuan-etal-2020-one} fail to credit many semantically correct predictions. On the other hand, BertScore with concatenated predictions and references \citep{koto-etal-2022-lipkey} reflects semantic similarity, but its token-level matching strategy obscures the semantics of individual keyphrases. Recognizing these limitations, we propose a \textit{phrase-level semantic matching} strategy in \textsc{KPEval} and define semantic precision ($SemP$), recall ($SemR$), and F1 ($SemF1$) as follows\footnote{Out metric should be distinguished from \citet{bansal-etal-2022-sem}. We keep the name choice as the tasks are different.}:
\begin{equation}
\resizebox{0.8\hsize}{!}{$SemP(\mathcal{P},\mathcal{Y})=\frac{1}{|\mathcal{P}|}\sum_{p\in\mathcal{P}}\max_{y\in\mathcal{Y}}sim(p,y)$}\nonumber,
\end{equation}
\begin{equation}
\resizebox{0.8\hsize}{!}{$SemR(\mathcal{P},\mathcal{Y})=\frac{1}{|\mathcal{Y}|}\sum_{y\in\mathcal{Y}}\max_{p\in\mathcal{P}}sim(p,y)$}\nonumber,
\end{equation}
\begin{equation}
\resizebox{0.75\hsize}{!}{$SemF1(\mathcal{P},\mathcal{Y})=\frac{2\cdot SemP(\mathcal{P},\mathcal{Y})\cdot SemR(\mathcal{P},\mathcal{Y})}{SemP(\mathcal{P},\mathcal{Y}) + SemR(\mathcal{P},\mathcal{Y})}$}\nonumber,
\end{equation}
where $sim$ is the similarity between the representation of two phrases. To enable the use of any existing dense embedding model, in this paper, we operationalize $sim$ with the cosine similarity:
\begin{equation}
\resizebox{0.8\hsize}{!}{$sim(p,q)=cos\_sim(h_p,h_q)=\frac{h_p^Th_q}{||h_p||\cdot||h_q||}$}\nonumber,
\end{equation}
% \begin{equation}
% \resizebox{0.75\hsize}{!}{$union(\mathcal{P})=max\_pooling(h_{p_1}, ..., h_{p_m})$}\nonumber,
% \end{equation}
where $h_p$ is the representation of phrase $p$ obtained by aggregating the representation of all tokens in the phrase. To obtain a high quality embedding that captures phrase-level semantics well, we fine-tune a paraphrase model from \citet{reimers-gurevych-2019-sentence}\footnote{We use the checkpoint at \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}.} using unsupervised SimCSE \citep{gao-etal-2021-simcse} on 1.04 million keyphrases from the training sets of KP20k, KPTimes, StackEx \citep{yuan-etal-2020-one}, and OpenKP \citep{xiong-etal-2019-open}. The data covers a wide range of domains including science, news, forum, and web documents. At inference time, a single phrase $p$ is provided as the input to the model, and the last hidden states are mean-pooled to obtain $h_p$. We further document the training details of this model in Appendix \ref{phrase-embedding-details}.

\subsection{Meta-Evaluation Setup}
\label{meta-evaluation-setup}
We conduct rigorous meta-evaluation to compare $SemF1$ with existing metrics. We sample 50 documents from the test sets of KP20k and KPTimes each. For each document, we obtain predictions from five representative models:  MultipartiteRank, CatSeq, SetTrans, in-domain BART models from \citet{wu-etal-2023-rethinking}, as well as five-shot prompting GPT-3.5\footnote{We use SciBART+OAGKX for KP20k and KeyBART for KPTimes to represent in-domain BART models. Detail regarding the evaluated models are provided in Appendix \ref{kp-system-impl}.}. This variety encompasses both KPE and KPG model, and includes unsupervised, supervised, and few-shot prompting methods. Then, three crowd-source annotators are asked to rate on Likert-scale the \textit{semantic similarity} between (1) each prediction keyphrase $p_i$ and the most semantically similar keyphrase in $\mathcal{Y}$, and (2) each reference keyphrase $y_i$ and the most semantically similar keyphrase in $\mathcal{P}$. We report the details of the annotator recruitment process, the annotation instructions, and the interface in Appendix \ref{human-eval-setup}. 

A total of 1500 document-level annotations with 13401 phrase-level evaluations are collected. As presented in Table \ref{tab:iaa}, we observe 0.75 Krippendorff's alpha for both datasets and both matching directions, indicating a high inter-annotator agreement. The annotations are aggregated to obtain (1) \textit{phrase-level} scores for matching a single phrase to a set of phrases ($p_i\rightarrow\mathcal{Y}$ and $y_i\rightarrow\mathcal{P}$) and (2) \textit{document-level} precision, recall, and F1 scores, calculated after normalizing the scores to a 0-1 scale. 

\subsection{Meta-Evaluation Results}
\label{ref-based-eval-meta-eval}
\label{meta-evaluation}

\input{tables/iaa}


Using the \textit{document-level} F1 score annotations, we compare $SemF1$ with six baseline metrics: 

\begin{compactenum}
    \item Exact Matching $F1@M$ \citep{yuan-etal-2020-one}.
    \item $F1@M$ with Substring Matching. We conclude a match between two phrases if either one is a substring of the other. This corresponds to the INCLUDES and PARTOF strategy in \citet{zesch-gurevych-2009-approximate}.
    \item R-precision \citep{zesch-gurevych-2009-approximate}.
    \item \textit{FG} \citep{luo-etal-2021-keyphrase-generation}.
    \item Rouge-L $F1$ \citep{lin-2004-rouge}.
    \item BertScore $F Score$ \citep{bert-score}\footnote{We use the RoBERTa-large model and the representation at the 17\textsuperscript{th} layer, as recommended by the official implementation at \url{https://github.com/Tiiiger/bert_score}.}. We concatenate all the phrases in $\mathcal{P}$ with commas to form a single prediction string, and do the same for $\mathcal{Y}$ to form the reference string\footnote{We find that BertScore is insensitive to the order of labels and predictions. Details are discussed in Appendix \ref{bertscore-order-sensitivity}.}. 
    % \item BartScore $F Score$ \citep{bart-score}. We use the bart-large without summarization training. We perform the same preprocessing as BertScore. 
\end{compactenum}

We apply Porter Stemmer \citep{Porter1980AnAF} on $\mathcal{P}$ and  $\mathcal{Y}$ before calculating baseline 1, 2, and 3.

\begin{figure}
\includegraphics[width=\columnwidth]{figures/main_corr_kp20k_kptimes_f1_only.pdf}
\vspace{-8mm}
\caption{The 95\% confidence intervals for the Kendall's Tau between human and automatic metrics on KP20k and KPTimes. $SemF1$ exhibits a higher correlation with humans and smaller intervals.}
\vspace{-2mm}
\label{document-level-tau-intervals}
\end{figure}

In Figure \ref{document-level-tau-intervals}, we report the 95\% confidence interval of Kendall's Tau via input-level bootstrap resampling with 1000 samples, following \citet{deutsch-etal-2021-statistical}. Surprisingly, although exact matching produces many false negatives, \textit{existing proposals to relax exact matching do not provide much overall performance gains} either: while substring matching consistently outperforms exact matching by a small amount, R-precision and FG have a lower correlation with human compared to exact matching. BertScore's performance is highly domain-dependent: it achieves the second-best performance on KPTimes while performs poorly on KP20k. By contrast, $SemF1$ greatly outperforms other metrics on both datasets, with a much higher mean score and a smaller variation. 

\input{tables/meta_eval_matching_embedding}

Is the observed high performance of $SemF1$ consistent with any embedding model? Our ablation studies suggest a negative answer. We evaluate with fine-grained \textit{phrase-level annotations} for both directions of matching (i.e., $p_i\rightarrow\mathcal{Y}$ and $y_i\rightarrow\mathcal{P}$). Table \ref{tab:meta-eval-embedding} presents the Pearson correlation ($r$), Spearman correlation ($\rho$), and Kendall's Tau ($\tau$) of exact matching and semantic matching with various embedding models: FastText \citep{joulin2016fasttext}\footnote{We use the \texttt{crawl-300d-2M.vec} model.}, Phrase-BERT \citep{wang-etal-2021-phrase}, SpanBERT \citep{joshi-etal-2020-spanbert}, SimCSE \citep{gao-etal-2021-simcse}\footnote{We use the \texttt{simcse-roberta-large} models distributed by the original authors on huggingface.}, our phrase embedding, and the model before the proposed fine-tuning. Despite being a strong strategy by design, semantic matching fails to outperform exact matching in Kendall's Tau with SpanBERT embedding. With the proposed model, semantic matching outperforms exact matching by 0.1 absolute points in Kendall's Tau and more than 0.15 absolute points in Pearson and Spearman correlation. It is worth-noting that although the base SBERT model already achieves strong performance, our phrase-level contrastive fine-tuning provides further performance gains.

\paragraph{Remark} We have confirmed that the semantic matching strategy better accommodates semantically correct \textit{predictions}. Additionally, our preliminary study indicates that human references often exhibit lexical variability. When faced with such variability, $SemF1$ demonstrates lower variance than $F1@M$ (detailed in Appendix \ref{label-variation}). 
