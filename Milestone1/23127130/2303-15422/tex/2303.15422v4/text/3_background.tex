\section{Background}
This section formulates the keyphrase prediction and evaluation tasks and outlines the scope of study. 

\subsection{Keyphrase Prediction}

We denote an instance of keyphrase prediction as a tuple \textbf{$(\mathcal{X},\mathcal{Y})$}, where $\mathcal{X}$ represents an input document and $\mathcal{Y}=\{y_1,...,y_n\}$ is a set of $n$ reference keyphrases provided by humans. Each $y_i$ is categorized as a \textit{present keyphrase} if it corresponds to contiguous word sequences in $\mathcal{X}$ after stemming, or an \textit{absent keyphrase} if it does not. Keyphrase generation (KPG) assumes $\mathcal{Y}$ to include both present and absent keyphrases, whereas keyphrase extraction (KPE) only allows present keyphrases in $\mathcal{Y}$.

\subsection{Keyphrase Evaluation}
\label{problem-formulation}
The \textit{keyphrase evaluation} process can be viewed as mapping a 4-element tuple $(\mathcal{X}, \mathcal{Y}, \mathcal{P}, \mathcal{C})$ to a real number via a function $f$. $\mathcal{P}=\{p_1,...,p_m\}$ is a \textit{set} of $m$ predictions made by a model $\mathcal{M}$ on $\mathcal{X}$. Different from the commonly followed works \cite{meng-etal-2017-deep,yuan-etal-2020-one}, we do not distinguish between present and absent keyphrases. This enables matching a predicted keyphrase to any semantically relevant reference, and vice versa.


$\mathcal{C}$ is a corpus that represents the domain of interest, which is an important factor in assessing keyphrase quality. For example, "sports games" may be informative in a general news domain but less so in the specialized domain of basketball news\footnote{Check \citet{tomokiyo-hurst-2003-language} for more examples.}. In this paper, $\mathcal{C}$ will play a crucial role in evaluating the utility of keyphrases for facilitating ad-hoc in-domain document retrieval (\cref{utility-def}).
% within a domain-specific corpus 

\subsection{Evaluation Scope}

\paragraph{Models} This paper covers 21 representative, strong, and diverse keyphrase prediction models spanning three categories: (1) KPE models, (2) KPG models, and (3) large language models (LLMs) and APIs. We aim to include highly cited (up to February 2024) models such as MultipartiteRank
(\citet{boudin-2018-unsupervised}, 219 citations), CatSeq (\citet{yuan-etal-2020-one}, 92 citations), and SetTrans (\citet{ye-etal-2021-one2set}, 65 citations). We provide introductions and implementation details in Appendix \ref{kp-system-impl}. 

\paragraph{Datasets} We test on two datasets throughout the paper: (1) KP20k \citep{meng-etal-2017-deep} that features 20k Computer Science papers with keyphrases extracted from the paper metadata and (2) KPTimes \citep{gallina-etal-2019-kptimes} that provides 10k news documents paired with keyphrases assigned by expert editors. The two datasets are selected due to their large training sets (500k for KP20k and 250k for KPTimes) and their wide usage in keyphrase research. As such, the models' performance is easier for the community to relate to and the reproduction correctness can be verified more easily. % We evaluate on KP20k's 20k test set and on KPTimes' 10k test set. 
