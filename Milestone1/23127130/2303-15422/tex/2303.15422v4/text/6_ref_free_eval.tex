\section{\textsc{KPEval}: Reference-Free Evaluation}
\label{ref-free-eval}

For a range of text generation tasks, the optimal output is often highly aspect-specific \citep{wen-etal-2015-semantically,mehri-eskenazi-2020-usr,fabbri-etal-2021-summeval}. As such, reference-based evaluation is incomplete as it does not always align with the evaluation goals. To address this gap, \textsc{KPEval} introduces three novel evaluation aspects, along with corresponding reference-free metrics, aimed at aligning closer with real-world application requirements.

\subsection{Faithfulness}
\label{nat-faith-methods}
% \fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\fcolorbox{teal!30}{green!10}{\parbox{0.465\textwidth}{
\textit{\textbf{Desiderata}: keyphrase predictions should always be grounded in the document.}}
}
\vspace{0.3pt}

In practical scenarios, it is vital for keyphrase systems to refrain from producing concepts not covered in the document, which we term as unfaithful keyphrases. Determining whether a keyphrase is faithful is non-trivial: an absent keyphrase could be faithful by being synonyms or parent/child concepts of the concepts in the document, while a present keyphrase could be deemed unfaithful if it has a wrong boundary. For instance, the keyphrase "hard problem" is unfaithful to a document discussing "NP-hard problem". This example also illustrates the inadequacy of reference-based evaluation, as "hard problem" may achieve a high score when matched against "NP-hard problem". 

\input{tables/model_faithfulness}

\paragraph{Are existing keyphrase models faithful?} We conduct a human evaluation of the same set of five models in \cref{meta-evaluation-setup} on 100 documents from KP20k and KPTimes each. For each (document, keyphrase prediction) pair, three annotators are asked to make a binary judgement between faithful and unfaithful (details in Appendix \ref{human-eval-setup}). Table \ref{tab:iaa} presents the inter-annotator agreement. We find a moderate agreement for present keyphrases and a lower agreement for absent keyphrases. We aggregate the present keyphrase annotations by majority voting. For the absent keyphrases, two of the authors manually resolve the instances where the crowd-source annotators do not agree. Table \ref{tab:model-faithfulness} presents the faithfulness scores for the evaluated models. Surprisingly, M3's outputs are not as faithful as the neural KPG models, supporting the hypothesis that extractive models can suffer from the boundary mistakes that harm their faithfulness. In addition, models make more unfaithful predictions in KP20k compared to KPTimes, indicating the possible difficulty of accurately generating concepts grounded in scientific papers compared to news documents. 

\paragraph{Automatic Faithfulness Evaluation} Using the human annotations, we evaluate three automatic metrics for judging a keyphrase's faithfulness:
\begin{compactenum}
    \item The precision metric of \textbf{BertScore}($\mathcal{X}$, $p_i$). We use the RoBERTa-large model as in \cref{meta-evaluation}.
    % $Pr(p_i|\mathcal{X})$
    \item The faithfulness metric of \textbf{BartScore} \citep{bart-score}. $p_i$ is embedded into \textcolor{codegreen}{"\textit{In summary, this is a document about }$p_i$"} for calculating its probability given $\mathcal{X}$. BART-large trained on CNN-DM \citep{see-etal-2017-get} is used.
    \item The consistency metric of \textbf{UniEval} \citep{zhong-etal-2022-towards}, which scores text generation as boolean QA. We embed $\mathcal{X}$ and $p_i$ with a template for summarization evaluation: \textcolor{codegreen}{"\textit{question: Is this claim consistent with the document? </s> summary:} \textit{the document discusses about} $p_i$\textit{. </s> document:} $\mathcal{X}$"}. Then, we use the UniEval model for summarization evaluation provided by the original authors to obtain a score expressed as the probability of the model generating "Yes" normalized by the probability for "Yes" and "No".  

\end{compactenum}

All of these metrics output a real number score. To compare their performance, we report their AUROC in Table \ref{tab:meta-eval-faithfulness}. On both datasets, UniEval outperforms BertScore and BartScore, achieving the highest agreement with human raters. Currently, \textsc{KPEval} adopts UniEval as the default faithfulness metric. We encourage future work to continue developing stronger metrics for this aspect.

\input{tables/meta_eval_faithfulness}

\subsection{Diversity}
\label{diversity-def}
% \fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\fcolorbox{teal!30}{green!10}{\parbox{0.465\textwidth}{
\textit{\textbf{Desiderata}: reward more semantically distinct concepts and penalize repetitions.}}
}
\vspace{0.3pt}

Generating keyphrases with minimal repetition is a desirable property of keyphrase applications. To assess the diversity of $\mathcal{P}$, \textsc{KPEval} includes one lexical and one semantic metric based on \citet{bahuleyan-el-asri-2020-diverse}. The lexical metric $dup\_token\_ratio$ is the percentage of duplicate tokens after stemming. The semantic metric $dup\_emb\_sim$ is the average of pairwise cosine similarity, using the phrase embedding in \cref{saliency-metric-design}:
\begin{equation}
\resizebox{0.8\hsize}{!}{$emb\_sim(\mathcal{P})=\frac{\sum_{i=1}^m\sum_{j=1}^m\mathbbm{1}(i\ne j)sim(p_i,p_j)}{m(m-1)}$}\nonumber.
\end{equation}
We note that by design, we do not penalize over-generating uninformative keyphrases, as it intuitively implies a high diversity\footnote{As a result, metrics such as the orthogonal regularization term used by CatSeqD \citep{yuan-etal-2020-one} are not suitable for our purposes, as the term naturally increases with $|\mathcal{P}|$.}. Judging the quality of the keyphrases is instead delegated to the metrics for reference agreement and faithfulness.

\subsection{Utility}
\label{utility-def}

% \fcolorbox{pink}{pink}{\parbox{0.47\textwidth}{
\fcolorbox{teal!30}{green!10}{\parbox{0.465\textwidth}{
\textit{\textbf{Desiderata}: reward predictions that facilitate effective ad-hoc retrieval of the document.}}
}
\vspace{0.3pt}

Information Retrieval (IR) is an important downstream application for keyphrases \citep{10.1145/312624.312671, 10.1145/1141753.1141800, kim-etal-2013-applying}. To directly evaluate whether $\mathcal{M}$ can generate useful keyphrases for IR-related tasks, \textsc{KPEval} tests $\mathcal{P}$ on facilitating \textit{ad-hoc retrieval} of $\mathcal{X}$ from an in-domain corpus $\mathcal{C}$ \citep{boudin-gallina-2021-redefining}. 

Concretely, we leverage an in-domain corpus $\mathcal{C}$ that has documents and human-annotated keyphrases\footnote{We use the respective training sets as $\mathcal{C}$ for KP20k and KPTimes. In practice, one can run any keyphrase prediction method if human-written keyphrases are not available for $\mathcal{C}$.}. We first index $\mathcal{C}$'s documents into the form \textcolor{codegreen}{\textit{(title, keyphrases)} $\rightarrow$ \textit{document}}. To evaluate $\mathcal{P}$, we add a single entry \textcolor{codegreen}{\textit{($\mathcal{X}$'s title, $\mathcal{P}$)} $\rightarrow \mathcal{X}$} to the aforementioned database. Then, a set of queries $\mathcal{Q}=\{q_1,q_2,...,q_{|Q|}\}$ specifically written for $\mathcal{X}$ are used to attempt to retrieve $\mathcal{X}$ from this pool. The utility of $\mathcal{P}$ is measured by two metrics for retrieval effectiveness: Recall at $k$ ($Recall@k$) and Reciprocal Rank at $k$ ($RR@k$), averaged across all queries in $\mathcal{Q}$. To simulate the queries written by real users, we use GPT-4 \citep{OpenAI2023GPT4TR} to annotate three ad-hoc queries based on each document. For KP20k, the queries are in the style of in-text citations similar to \citet{boudin-gallina-2021-redefining}. For KPTimes, we generate short phrases that mimic queries on web search engines. We present the prompting details in Appendix \ref{adhoc-query-construction}. For metric calculation, we consider BM25 \citep{10.1007/978-1-4471-2099-5_24} and a dense embedding model\footnote{We use \texttt{cross-encoder/ms-marco-MiniLM-L-6-v2} model via huggingface.} as the retriever and report the averaged scores.
