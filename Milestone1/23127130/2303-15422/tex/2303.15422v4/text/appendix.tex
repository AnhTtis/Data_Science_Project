\clearpage
\appendix
\twocolumn[{%
 \centering
 \Large\bf Supplementary Material: Appendices \\ [20pt]
}]

\section{Literature survey: evaluation methods used in recent keyphrase papers}
\label{kp-paper-eval-survey}

 We survey all the papers published from 2017 to 2023 in major conferences for AI, NLP, and IR (ACL, NAACL, EMNLP, AAAI, and SIGIR) about keyphrase extraction or keyphrase generation. We choose year 2017 as it marks the start of deep keyphrase generation methods \citep{meng-etal-2017-deep}. We manually check each paper's experiment sections and note down which of the six major categories do the reported evaluation metrics belong to: (1) precision, recall, and F1 based on exact-matching; (2) diversity metrics, such as duplication ratio; (3) ranking-based metrics such as mAP, $\alpha$-NDCG, and MRR; (4) approximate versions of exact matching such as n-gram matching; (5) retrieval-based utility metrics; (6) human evaluation. We make sure each of metrics used in the surveyed papers can fall under one and only one category under this ontology.

The survey results are presented in Figure \ref{eval-metric-choice}. Overall, despite its limitation, exact matching has been \textit{de facto} the method for assessing the performance of newly proposed keyphrase systems, and there has been limited progress in adopting alternative metrics. The majority of papers report exact matching precision, recall, and F1. Two thirds of all papers use exact matching as the only metric, including 10 out of 11 papers published in 2023. Human evaluation is only conducted in one paper surveyed \citep{bennani-smires-etal-2018-simple}. 

\section{Keyphrase Systems: Implementation Details and Full Evaluation Results}
\label{kp-system-impl}
\label{full-eval-results}

In this section, we describe in detail the considered keyphrase systems as well as how we obtain their outputs for evaluation. 

\subsection{Keyphrase Systems}

We consider three types of keyphrase systems: keyphrase extraction models, keyphrase generation models, and APIs including large language models. 

\paragraph{Keyphrase Extraction Systems}
KPE has traditionally been approached through unsupervised methods, where noun phrase candidates are ranked using heuristics \citep{hulth-2003-improved,mihalcea-tarau-2004-textrank}. Supervised approaches include feature-based ranking \citep{witten1999kea}, sequence labeling \citep{zhang-etal-2016-keyphrase}, and the use of pre-trained language models (PLMs) for task-specific objectives \citep{song-etal-2021-importance,song-etal-2022-hyperbolic}. We consider the following nine KPE models:

\begin{figure}[]
\centering
\includegraphics[width=\columnwidth]{figures/eval_metric_choice_seaborn.pdf}
% \vspace{-8mm}
\caption{Distribution over 75 papers of the adopted evaluation metrics: (1) F1 score based on exact-matching; (2) diversity metrics such as duplication ratio; (3) ranking-based metrics such as mAP, $\alpha$-NDCG, and MRR; (4) approximate versions of exact matching; (5) retrieval-based utility metrics; (6) human evaluation.}
\label{eval-metric-choice}
% \vspace{-5mm}
\end{figure}

\begin{compactenum}
    \item [\textbf{M1}] \textbf{TF-IDF} \citep{tf-idf} selects the phrases containing words with highest TF-IDF weight.
    \item [\textbf{M2}] \textbf{TextRank} \citep{mihalcea-tarau-2004-textrank} runs PageRank \citep{brin1998anatomy} on an undirected word cooccurrence graph. 
    \item [\textbf{M3}] \textbf{RAKE} \citep{rose2010automatic} is an efficient single-document unsupervised KPE algorithm that uses the cooccurrence graph to score keyphrase candidates.
    \item [\textbf{M4}] \textbf{MultipartiteRank} \citep{boudin-2018-unsupervised} represents the document as a multipartite graph to encode topical diversity and improve intra-topic keyphrase selection preferences.
    \item [\textbf{M5}] \textbf{YAKE!} \citep{YAKE} is an unsupervised KPE method relying on local features such as term co-occurrence and frequencies.
    \item [\textbf{M6}] \textbf{Kea} \citep{witten1999kea} builds a supervised keyphrase classifier using statistical features including TF-IDF and position information.
    \item [\textbf{M7}] \textbf{BERT+CRF} \citep{2212.10233} fine-tunes a pre-trained BERT \citep{devlin-etal-2019-bert} on sequence labeling with conditional random fields \citep{10.5555/645530.655813}. 
    \item [\textbf{M8}] \textbf{HyperMatch} \citep{song-etal-2022-hyperbolic} trains a supervised model to rank phrase-document relevance in a hyperbolic space.
    \item [\textbf{M9}] \textbf{PromptRank} \citep{kong-etal-2023-promptrank} ranks the phrases by their probability given a prompt prefix using a sequence-to-sequence pre-trained language models.
\end{compactenum}

\paragraph{Keyphrase Generation Systems}
KPG models are often trained with various supervised objectives, including \textit{One2One}, \textit{One2Seq}, and \textit{One2Set} \citep{meng-etal-2017-deep, yuan-etal-2020-one, ye-etal-2021-one2set}. A range of strategies have been proposed, including hierarchical modeling of phrases and words \citep{chen-etal-2020-exclusive}, reinforcement learning \citep{chan-etal-2019-neural}, unifying KPE and KPG \citep{chen-etal-2019-integrated,ahmad-etal-2021-select}, and using PLMs \citep{kulkarni-etal-2022-learning}. We consider ten KPG models: 

\begin{compactenum}
    \item [\textbf{M10}] \textbf{CatSeq} \citep{yuan-etal-2020-one} is an RNN trained with copy mechanism \citep{gu-etal-2016-incorporating} on generating keyphrases as a sequence.
    \item [\textbf{M11}] \textbf{CatSeqTG-2RF1} \citep{chan-etal-2019-neural} introduces an RL-based approach using recall and F1 as the rewards.
    \item [\textbf{M12}] \textbf{ExHiRD-h} \citep{chen-etal-2020-exclusive} extends CatSeq with a hierarchical decoding and an exclusion mechanism to avoid duplications.
    \item [\textbf{M13}] \textbf{SEG-Net} \citep{ahmad-etal-2021-select} unifies keyphrase extraction and keyphrase generation training and introduces layer-wise coverage attention mechanism
    \item [\textbf{M14}] \textbf{Transformer} \citep{ye-etal-2021-one2set} is a Transformer model \citep{vaswani2017attention} trained with copy mechanism on generating keyphrases as a sequence. 
    \item [\textbf{M15}] \textbf{SetTrans} \citep{ye-etal-2021-one2set} generates keyphrases in parallel based on control codes trained via a k-step target assignment process.
    \item [\textbf{M16}] \textbf{SciBERT-G} \citep{2212.10233} fine-tunes SciBERT \citep{beltagy-etal-2019-scibert} for seq2seq keyphrase generation using a prefix-LM objective \citep{arxiv.1905.03197}.
    \item [\textbf{M17}] \textbf{BART-large} \citep{2212.10233} is a BART model \citep{lewis-etal-2020-bart} fine-tuned on generating keyphrases as a sequence. 
    \item [\textbf{M18}] \textbf{KeyBART} \citep{kulkarni-etal-2022-learning} is a BART model adapted to scientific keyphrase generation before fine-tuning.
    \item [\textbf{M19}]\textbf{SciBART-large+OAGKX} \citep{2212.10233} is pre-trained on scientific corpus and scientific keyphrase generation before fine-tuning.
\end{compactenum}

\paragraph{Large language models and APIs.} Recent advancements highlight the capability of large language models (LLMs) to perform in-context learning \citep{brown2020language}. We explore GPT-3.5 \citep{ouyang2022training} for KPG in the zero-shot and few-shot prompting setting\footnote{We use OpenAI's \texttt{text-davinci-003} via API.}. We also assess two commercial keyphrase extraction APIs. 

\begin{compactenum}
    \item [\textbf{M20}] \textbf{Zero-shot prompting GPT-3.5.}
    \item [\textbf{M21}] \textbf{Five-shot prompting GPT-3.5.}
    \item [\textbf{M22}] \href{https://docs.aws.amazon.com/comprehend/latest/dg/how-key-phrases.html}{\textbf{Amazon Comprehend API}}
    \item [\textbf{M23}] \href{https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/key-phrase-extraction/quickstart?pivots=programming-language-python}{\textbf{Azure Cognitive Services API}}
\end{compactenum}

\subsection{Implementation Details}

We obtain the output from the original authors for M8, M10, M11 for KP20k, and M7, M16, M17, M18 for both KP20k and KPTimes. For the other KPE and KPG models, we reproduce the results on our own. For M1, M2, M4, M6, we obtain the outputs using the \href{https://github.com/boudinfl/pke}{pke} library. For M3, M5, M8, M10, M11, M12 (KPTimes only) and M9, M13, M14, M15, we use the original implementations provided by the authors. For M18, we use the \href{https://github.com/uclanlp/DeepKPG}{DeepKPG} toolkit. For the commercial APIs, we implement the API call following the instructions. We obtained results on 3/5/2023 for M20 and 3/11/2023 for M21. Following existing KPE literature, we consider the top 10 predictions from M1, 2, 3, 4, 5, 6, 8, 9, 22, and 23. For all the systems, we truncate the input to 512 tokens. We perform hyperparameter tuning on the validation sets and ensure that the models match the performance reported by original paper or existing works such as \citet{2212.10233}. For M11 and M13 on KPTimes, we failed to obtain reasonable performance and thus choose to omit the results.

For GPT-3.5, we always start the prompt with a task definition: \textit{\textcolor{codegreen}{"Keyphrases are the phrases that summarize the most important and salient information in a document. Given a document's title and body, generate the keyphrases."}}

In the zero-shot setting, we provide the title and body in two separate lines, and start a new line with \textit{\textcolor{codegreen}{"Keyphrases (separated by comma):"}}. In the 5-shot setting, we randomly sample 5 examples from the train set for each test document, and provide their title, body, and keyphrases in the same format in the prompt before the document tested. 


% \subsection{Complete Evaluation Results}

% We present the full evaluation results of the 21 models on KP20k and KPTimes in Table \ref{tab:all-results-main}. Due to budget constraints, we only sample 1000 documents per dataset for utility evaluation. For the other aspects, the complete test sets are used.

% \input{tables/all_results_main}

\section{\textsc{KPEval} Metrics: Implementation Details and Further Analyses}
\subsection{Phrase Embedding Training Details}
\label{phrase-embedding-details}

We fine-tune the paraphrase model provided by \citet{reimers-gurevych-2019-sentence} distributed at \url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}. Unsupervised SimCSE \citep{gao-etal-2021-simcse} is used as the training loss. Specifically, given a batch of $B$ phrases, the loss can be expressed as:
\begin{equation}
\resizebox{0.85\hsize}{!}{$\mathcal{L}_{simcse}=\frac{1}{B}\sum_{i=1}^B -\log\frac{e^{sim(h_i, h_i')/\tau}}{\sum_{j=1}^Be^{sim(h_i,h_j')/\tau}}$}\nonumber,
\end{equation}
where $h_i$ and $h_i'$ are the representations of phrase $i$ obtained using two separate forward passes with dropout enabled. This objective discourages the clustering of unrelated phrases in the representation space and retains a high similarity between semantically related phrase pairs. $\tau$ is a scaling factor which we empirically set to $0.05$.

We fine-tune the model on $\mathcal{L}_{simcse}$ using 1.04 million keyphrases from the training set of KP20k, KPTimes, StackEx \citep{yuan-etal-2020-one}, and OpenKP \citep{xiong-etal-2019-open}, covering a wide range of domains including science, news, forum, and web documents. We use the AdamW optimizer with maximum sequence length 12, batch size 512, dropout 0.1, and learning rate 1e-6 to fine-tune for 1 epoch. The hyperparameters are determined using a grid search on the following search space: batch size \{128, 512, 1024, 2048\}, learning rate \{1e-6, 5e-6, 1e-5, 5e-5\}. We randomly hold out 0.5\% from the training data for validation and model selection. The final training takes 30 minutes on a single Nvidia GeForce RTX 2080 Ti GPU.

\paragraph{Remark on embedding quality} In Table \ref{tab:alignment-uniformity}, we provide an additional study on the trained embedding. Specifically, following \citet{gao-etal-2021-simcse, wang2020understanding}, we evaluate alignment, the average similarity between keyphrases of similar meanings, and uniformity, the average similarity between unrelated keyphrase pairs. For alignment, we utilize the name-variation pairs constructed by \citet{chan-etal-2019-neural}. We find that our model achieves the best uniformity, which means that it assigns close to 0 similarity for unrelated pairs. For phrases with similar meanings, it achieves 0.58 alignment, which is also close to human perceptions. Finally, the separation between uniformity and alignment is also the largest for our embedding model.

\input{tables/alignment_uniformity}

\subsection{Ad-hoc Query Construction for Utility}
\label{adhoc-query-construction}

We use GPT-4 \citep{OpenAI2023GPT4TR} to annotate three ad-hoc queries per document from KP20k and KPTimes test sets. For both datasets, we sample with temperature set to 0.9 to balance quality and diversity. Due to budget constraints, we sample 1000 documents per dataset to construct the evaluation set. The prompts are presented in Figure \ref{adhoc-query-prompt}. 

\subsection{Inter-metric Correlations}
\label{inter-metric-correlation}

Using the document-level evaluation scores of the 21 keyphrase systems, we calculate the pair-wise Kendall's Tau for all the metrics in \textsc{KPEval}. The results are shown in Figure \ref{kpeval-metric-corr}. Overall, we find that only the metrics for the same dimension show a moderate or strong correlation with each other, and the metrics for different aspects hardly correlate. This results suggest that \textsc{KPEval}'s aspects measure distinct abilities and that optimizing a single metric does not automatically transfer to a superior performance on the other aspects. 

\begin{figure*}[]
% \vspace{-5mm}
\includegraphics[width=\textwidth]{figures/metric_correlations.pdf}
% \vspace{-6mm}
\caption{Correlation between \textsc{KPEval}'s metrics, measured by Kendall's Tau. The diversity scores are negated to provide a more intuitive view. We find that only the metrics for the same dimension correlate with each other. }
\label{kpeval-metric-corr}
% \vspace{-3mm}
\end{figure*}

\subsection{Order Sensitivity of BertScore}
\label{bertscore-order-sensitivity}

\input{tables/bertscore_sensitivity}

As BertScore is evaluated on two sequences instead of two sets of phrases, previous works concatenate all the predicted phrases as a single string and evaluate against all the references concatenated together \citep{koto-etal-2022-lipkey, glazkova2022applying}. However, it is unclear how to order the prediction and reference keyphrases within these two strings and whether BertScore's performance is sensitive to this ordering or not. We conduct phrase-level meta-evaluation of BertScore with phrase-level permutation applied to the matching target. Specifically, we shuffle the labels and the references before calculating meta-evaluation metrics. We repeat this process for 100 times and report the mean and standard deviation of human correlation in Table \ref{tab:bertscore-stability}. Overall, we find that the metric-human correlation of BertScore is relatively insensitive to permutations: when given reference phrases or prediction phrases concatenated in different orders, BertScore maintains a similar evaluation quality. One notable pattern is that when the references and the predictions are not permuted, BertScore obtains slightly higher performance. We hypothesize that in this case many phrases may present in the same order in the reference and the prediction, making the exactly matched instances easier to distinguish.

\subsection{Variation in Keyphrase Annotations Motivates Semantic Matching}
\label{label-variation}

A major motivation for semantic matching is that valid predictions vary in many ways. But at the same time, \textit{do human references also exhibit lexical variations}? We investigate with a pilot study of model-in-the-loop keyphrase annotation. 

% \subsubsection{Human Study Setup}
\paragraph{Setup}
We sample 100 documents each from the test sets of KP20k and KPTimes and combine each document's $\mathcal{Y}$ with $\mathcal{P}$ from four systems: M8, M10, M15, and M18 (KPTimes only)/M19 (KP20k only). Three MTurk annotators are presented with the document and the phrases re-ordered alphabetically. They are then asked to write keyphrases that best capture the salient information. We state that they may select from the provided keyphrases or write new keyphrases. Figure \ref{labeling-interface} presents the annotation interface. We use the same set of annotators in \cref{human-eval-setup} and collect 3226 keyphrase annotations, which approximately cost \$700.


% \subsubsection{Results}

\begin{figure}[]
\centering
\vspace{-3mm}
\includegraphics[width=0.49\textwidth]{figures/selection_preference.pdf}
\vspace{-8mm}
\caption{Annotators' selection distribution for each of the keyphrase sources. The reported counts are averaged across three annotators. Annotators do not prefer selecting keyphrases belonging to the original labels.}
\vspace{-4mm}
\label{selection-preference}
\end{figure}

\paragraph{Keyphrase annotations exhibit lexical variations.} Figure \ref{selection-preference} presents the distribution of phrase selected by the annotators from each source. The reported counts are averaged over three annotators. Surprisingly, we find that the keyphrases in the original labels are not preferred over the outputs from other models. First, nearly 70\% keyphrases in the original labels are not selected. Second, the annotators select several keyphrases from each model's outputs that do not appear in the label set. For KP20k, the annotators even select more such keyphrases compared to the phrases from the labels. This suggests that \textit{label variations} can be common in keyphrase annotations, even if candidate keyphrases are given as a guidance. 

Are the observed label variations caused by annotators writing entirely different concepts? We find that the average character-level edit distance between the selected phrases and the closest phrase in label keyphrases is 11.0 for KP20k and 7.0 for KPTimes, much smaller than the metric for phrases that are not selected (17.5 for KP20k and 14.6 for KPTimes). In other words, keyphrases written by different humans are lexically similar to the original references, with slightly different phrasings.


\paragraph{Semantic matching is more robust to label variations.} A desired property of keyphrase metrics is outputting consistent scores with similar sets of labels. Using the 200 annotated documents, we compare $F1@M$ and $SemF1$ on the outputs from \textbf{M20} with four sets of labels\footnote{We choose \textbf{M20} as many supervised models' outputs largely overlap with those included in the annotation process.}: the original labels and three sets of newly annotated labels. As shown in Table \ref{tab:metric-stability}, the output of $SemF1$ is more stable using different label sets, indicating a higher robustness and reliability compared to $F1@M$. 

\input{tables/metric_stability}


\section{Annotation for Meta-Evaluation}
\label{human-eval-setup}

\paragraph{Annotator Hiring} For all the annotation experiments, we use Amazon Mechanical Turk (MTurk) and designed a qualification task to hire and train annotators. We require the annotators to be located in United States or United Kingdom and have finished more than 1000 HITs with 97\% pass rate. In the qualification task, the annotators are presented with the definition of semantic matching with examples, and then asked to annotate three documents. 46 annotators that have average $\le1.5$ wrong annotations per document are selected. We ensure that the purpose of the main tasks and how we use the annotations are clearly explained in the qualification task to the potential annotators. 

\paragraph{Cost} For reference agreement, a total of 1500 document-level annotations with 13401 phrase-level evaluations are collected from the qualified annotators, costing approximately \$1400. For faithfulness, we collect 6450 phrase-level annotations for KP20k and 6486 annotations for KPTimes, costing approximately \$800. We adjust the unit pay to ensure \$15 hourly pay. 

\paragraph{Interface} The annotation instructions and interface for reference agreement are presented in Figure \ref{scoring-instructions} and Figure \ref{scoring-interface}. The interface for faithfulness is presented in Figure \ref{labeling-interface-faithfulness}.

\section{Qualitative Study}
\label{qualitative-study}

\subsection{$SemF1$ vs. existing metrics}
\label{qual-study-semantic-matching}

In Figure \ref{qualitative-result-matching}, we present a qualitative example where the model (\textbf{M21}) predicts two keyphrases exactly matching to some reference keyphrases and two keyphrases semantically similar to the reference. Correspondingly, human annotators assign partial credits to both of the "near-misses". However, for these phrases, exact matching gives a score 0 and substring matching gives full credit. As the reference and the prediction contain many similar tokens, BertScore is also near 1.0. Semantic matching's scoring is the most similar to humans. 

\subsection{Rethinking model comparisons}
\label{qual-study-model-comparisons}

In Figure \ref{example-zeroshot-outputs-kp20k}, we compare ExHiRD-h and CatSeqTG-2RF1 on two instances from KP20k. When evaluated with exact matching, CatSeqTG-2RF1 is preferred. However, since a lot of correct keyphrases are not recognized by exact matching, the irrelevant concepts and duplications are under-penalized by $F1@M$. By contrast, these issues are identified by the metrics from \textsc{KPEval} dedicated to faithfulness and diversity. With semantic-based evaluation, \textsc{KPEval} suggests that ExHiRD-h outperforms CatSeqTG-2RF1 on these two instances.

\input{tables/qualitative_study_matching}

\input{tables/qualitative_study_exhird}

\input{tables/adhoc_query_prompt}

\begin{figure*}[]
\centering
\fbox{\includegraphics[width=0.9\linewidth]{figures/scoring_instructions.png}}
\caption{An example of the annotation instructions for the keyphrase evaluation study in \cref{meta-evaluation}.}
\label{scoring-instructions}
\end{figure*}

\begin{figure*}[]
\centering
\fbox{\includegraphics[width=0.9\linewidth]{figures/scoring_interface.png}}
\caption{An example of the annotation interface for the keyphrase evaluation study in \cref{meta-evaluation}.}
\label{scoring-interface}
\end{figure*}

\begin{figure*}[]
\centering
\fbox{\includegraphics[width=0.9\linewidth]{figures/labeling_interface.png}}
\caption{An example of the annotation interface for the keyphrase annotation study in Appendix \ref{label-variation}.}
\label{labeling-interface}
\end{figure*}

\begin{figure*}[]
\centering
\includegraphics[width=0.95\linewidth]{figures/scoring_interface_faithfulness.png}
\caption{An example of the annotation interface for the faithfulness study in \cref{nat-faith-methods}.}
\label{labeling-interface-faithfulness}
\end{figure*}