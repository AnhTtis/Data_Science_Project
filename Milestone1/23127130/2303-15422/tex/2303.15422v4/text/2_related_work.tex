\section{Related Work}
In this section, we review the relevant literature on evaluating keyphrase systems.

\paragraph{Reference-based evaluation} The major metrics for evaluating keyphrase systems are precision, recall, and F1 based on exact-match between the stemmed predictions and references \citep{mihalcea-tarau-2004-textrank, meng-etal-2017-deep, yuan-etal-2020-one}. This method indiscriminately penalizes unmatched predictions, including synonyms or parent/child concepts of the reference. Later works attempt to improve the metric by \textit{relaxing the matching criterion}. \citet{zesch-gurevych-2009-approximate} propose to use R-precision with approximate matching, tolerating a prediction to be a substring of a reference and vice versa. \citet{kim-etal-2010-evaluating} employ n-gram matching metrics such as BLEU \citep{papineni-etal-2002-bleu} and Rouge \citep{lin-2004-rouge}. \citet{chan-etal-2019-neural} expand the references with name variations. \citet{luo-etal-2021-keyphrase-generation} propose a fine-grained score that combines token-level matching, edit distance, and duplication penalty. \citet{koto-etal-2022-lipkey} and \citet{glazkova2022applying} use the semantic-based BertScore \citep{bert-score} with predictions and references concatenated into two strings. 

Meanwhile, ranking-based metrics such as Mean Reciprocal Rank, mean Averaged Precision, and Normalized Discounted Cumulative Gain are introduced to evaluate the ranking provided by keyphrase extraction models \citep{florescu-caragea-2017-positionrank, boudin-2018-unsupervised, kim-etal-2021-structure}. These metrics also compute exact matching to the references during their evaluation. 

\paragraph{Reference-free evaluation} Directly evaluating keyphrase predictions without references is less common. Early studies conduct human evaluation \citep{barker2000using, matsuo2004keyword}. Later work evaluates the predictions' utility in applications such as retrieval \citep{Bracewell2005MultilingualSD, boudin-gallina-2021-redefining} or summarization \citep{litvak-last-2008-graph}. \citet{bahuleyan-el-asri-2020-diverse} conduct reference-free evaluation of the predictions' diversity. 

\paragraph{Meta-evaluation} 
Meta-evaluation studies that compare keyphrase metrics with human evaluations have been limited in scope, with a focus on reference-based evaluation. \citet{kim-etal-2010-evaluating} compare five lexical matching metrics and concluded that R-precision has the highest Spearman correlation with human judgments. \citet{bougouin2016termith} annotated a meta-evaluation corpus with 400 documents in French, evaluating 3 keyphrase models on  "appropriateness" and "silence", approximately corresponding to precision and false negative rate. 

\paragraph{Discussion} Building upon existing literature, this work systematically rethinks the goals of keyphrase evaluation and advances the evaluation methodology. We introduce \textsc{KPEval}, a holistic evaluation framework encompassing four key aspects (\cref{section-framework}). \textsc{KPEval} incorporates semantic-based metrics validated via rigorous meta-evaluation (\cref{meta-evaluation} and \cref{nat-faith-methods}). Finally, we conduct a large-scale evaluation of 21 keyphrase systems and offer novel insights into existing model comparisons and LLMs (\cref{re-eval}).
