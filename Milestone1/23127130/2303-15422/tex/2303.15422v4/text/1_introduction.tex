\section{Introduction}

\begin{figure*}[t!]
\vspace{-5mm}
\includegraphics[width=\textwidth]{figures/kpeval_main_figure.pdf}
\vspace{-6mm}
\caption{An illustration of the proposed keyphrase evaluation framework. \textsc{KPEval} evaluates keyphrase systems on four crucial properties and incorporates semantic-based metrics for more accurate assessment.}
\label{main-framework}
\vspace{-3mm}
\end{figure*}

Building automated keyphrase prediction systems has been a long-lasting research interest of Information Retrieval (IR) and Natural Language Processing (NLP) \citep{witten1999kea, hulth-2003-improved, meng-etal-2017-deep}. While a large number of keyphrase prediction systems have been proposed, the majority of them are assessed using a simplistic method: comparing the stemmed predictions with human references for exact matches. An extensive review of 76 recent keyphrase extraction and generation papers published in major conferences reveals a predominant reliance on exact matching, with 75 of 76 papers employing it and 51 treating it as the sole evaluation criterion (Appendix \ref{kp-paper-eval-survey}).

This over-reliance brings two major concerns. First, it has been established that the \textit{evaluation accuracy} of exact matching is inadequate \citep{zesch-gurevych-2009-approximate}. Although a number of heuristics are proposed to relax the matching criteria \citep{zesch-gurevych-2009-approximate, kim-etal-2010-evaluating, luo-etal-2021-keyphrase-generation, koto-etal-2022-lipkey} or enrich the label set \citep{chan-etal-2019-neural}, they still struggle to accurately capture \textit{phrase semantics} and have not been validated by systematic meta-evaluation. Second, solely relying on reference-based evaluation is an \textit{incomplete} strategy that overlooks critical aspects such as diversity of the predicted keyphrases \citep{bahuleyan-el-asri-2020-diverse} or their utility in practical applications of keyphrase systems such as indexing for IR applications \citep{boudin-gallina-2021-redefining}.

In this paper, we undertake a systematic approach to advance keyphrase evaluation. For reference-based evaluation, we propose a phrase-level semantic matching metric with a high quality embedding trained on large-scale keyphrase data. Based on the human evaluation corpus annotated on KP20k \citep{meng-etal-2017-deep} and KPTimes \citep{gallina-etal-2019-kptimes}, the meta-evaluation on five keyphrase systems shows that our metric significantly outperforms existing metrics by more than 0.15 absolute points in Kendall's Tau. By contrast, many proposed improvements to exact matching surprisingly fail to improve its human agreement  (\cref{meta-evaluation}). Further analyses reveal that the proposed metric exhibits enhanced stability under the label variations commonly present in the keyphrase annotations.

Next, we move beyond reference agreement and holistically consider the desiderata for evaluating keyphrase systems. Three crucial aspects are introduced: (1) \textit{faithfulness}, whether the predictions are grounded to the document (\cref{nat-faith-methods}); (2) \textit{diversity}, whether the predictions represent distinct concepts (\cref{diversity-def}); and (3) \textit{utility} for downstream IR applications (\cref{utility-def}). To accurately assess each aspect, we propose semantically-oriented metric designs including embedding similarity, model-based consistency evaluation, and dense retrieval. 

Together, these aspects and metrics form \textsc{KPEval}, a \textit{fine-grained semantic-based} keyphrase evaluation framework (Figure \ref{main-framework}). In \cref{re-eval}, we employ \textsc{KPEval} to evaluate 23 keyphrase systems, producing intriguing insights: 

\begin{compactenum}
    \item \textsc{KPEval} uncovers \textit{blind-spots} in established model comparisons, such as the actual superiority of ExHiRD-h \citep{chen-etal-2020-exclusive} in many aspects as well as a common difficulty to outperform a baseline in all the aspects.
    \item We find that \textit{large language models} (LLMs), particularly GPT-3.5 \citep{ouyang2022training}, exhibit remarkable performance compared to current state-of-the-art keyphrase generation and extraction models. Our results challenge existing conclusions and lead to a reconsideration of using LLMs as keyphrase systems. 
    \item Finally, \textsc{KPEval}'s four aspects test distinct abilities at which different models excel, suggesting the importance of aligning evaluation with the diverse needs of real applications.  
\end{compactenum}


In summary, \textsc{KPEval} establishes a new standard for keyphrase evaluation by advancing reference-based evaluation accuracy and aligning model development with application values via holistic reference-free evaluation. To facilitate future studies, the implementation is released as a toolkit along with the meta-evaluation annotations at \url{https://github.com/uclanlp/KPEval}.

