\section{Conclusion}
We introduce \textsc{KPEval}, a fine-grained 
evaluation framework that conducts semantic-based evaluation on reference agreement, faithfulness, diversity, and utility of keyphrase systems. We show the advantage of our metrics via rigorous human evaluation, and exhibit the usability of \textsc{KPEval} through a large-scale evaluation of keyphrase systems including LLM-based methods and keyphrase APIs. Our framework marks the first step towards systematically evaluating keyphrase systems in the era of LLMs. We hope \textsc{KPEval} can motivate future works to adopt more accurate evaluation metrics and further advance the evaluation methodology. Future studies might also explore the development of utility metrics tailored to the specific requirements of applications in niche domains.

\section*{Limitations}
While our study sheds light on enhancing the keyphrase evaluation methodology, several limitations exist for \textsc{KPEval}. 

\begin{enumerate}
    \item \textbf{Multilingual Evaluation.} We encourage future work to extend the evaluations in this paper to multilingual setting. By design, the aspect and metric formulations in \textsc{KPEval} are language-agnostic. For instance, $SemF1$ can be implemented with multilingual embeddings. Such embeddings need not to keyphrase-specific. For instance, Table \ref{tab:meta-eval-embedding} suggests that off-the-shelf embeddings cam already have reasonable performance. 
    \item \textbf{Alternative Scoring Schemes.} \textsc{KPEval}'s evaluation and meta-evaluation strategies always target at producing fine-grained numeric scores. This is different from tasks like machine translation where direct assessment scores are annotated \citep{graham-etal-2013-continuous} or LLM competitions that report Elo scores \citep{ChatArena}. Exploring whether these schemes may provide better evaluation quality is an important question for future work on keyphrase evaluation. 
    \item \textbf{LLM-Based Evaluation.} Recent works have shown the viability of using LLMs for human-aligning aspect-specific evaluation \citep{liu-etal-2023-g}. By comprehensively establishing the possible evaluation aspects and curating meta-evaluation data for reference agreement and faithfulness, our work sets up the necessary preparations for evaluating LLM-based metrics. We encourage future work to formally define and investigate the performance of keyphrase evaluation metrics based on LLMs.
\end{enumerate}


