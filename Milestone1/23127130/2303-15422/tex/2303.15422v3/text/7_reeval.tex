\section{Fine-grained benchmarking of keyphrase systems}
\label{re-eval}

Finally, we benchmark 21 keyphrase systems with \textsc{KPEval}. The implementation details and full evaluation results are presented in Appendix \ref{kp-system-impl}. This section presents the insights on two questions:
\begin{compactenum}
    \item Do our finding align with the conclusions drawn from previous model comparisons?
    \item How do large language models compare with existing keyphrase prediction methods?
\end{compactenum}

\input{tables/settrans_reeval}

\begin{figure*}[t!]
\vspace{-2mm}
\includegraphics[width=\linewidth]{figures/radar_chart.pdf}
\vspace{-6mm}
\caption{A comparison between GPT-3.5 and strong supervised (left) and unsupervised (right) keyphrase extraction and keyphrase generation methods. GPT-3.5 achieves strong performance on both datasets and most dimensions. We use $RR@5$ to represent utility and $1-dup\_token\_ratio$ to represent diversity. }
\label{fig-radar-chart}
\vspace{-6mm}
\end{figure*}

\paragraph{Uncovering blind-spots of established results} We revisit a set of models compared in \citet{ye-etal-2021-one2set}: CatSeq, CatSeqTG-2RF1, ExHiRD-h, and SetTrans. As shown in Table \ref{tab:settrans-reeval}, a nuanced pattern emerges when evaluating beyond $F1@M$, the main metric reported in the original paper. $SemF1$ is consistent with $F1@M$ in recognizing SetTrans as the best model for reference agreement. However, SetTrans does not outperform all the three baselines in reference-free evaluation. Specifically, the best faithfulness and diversity scores are achieved by ExHiRD-h, and the difference between SetTrans and CatSeqTG-2RF1 in utility is insignificant. Moreover, contradicting with $F1@M$, \textsc{KPEval}'s metrics show a superiority of ExHiRD-h over CatSeqTG-2RF1 for reference agreement, faithfulness, and diversity. We provide several supporting examples in Appendix \ref{qualitative-study}. By revealing these blind-spots in previous results, \textsc{KPEval} enables a holistic view in model comparison and a stricter criterion in establishing the state-of-the-art.

\paragraph{LLM vs traditional keyphrase models} With the ascent of LLMs as foundational elements in NLP, their efficacy in keyphrase prediction warrants examination. Prior research reported significant performance gaps when evaluating LLMs with exact matching $F1@M$ \citep{song2023chatgpt, martinez2023chatgpt}. With \textsc{KPEval}, we conduct a more comprehensive investigation. In Figure \ref{fig-radar-chart}, we compare GPT-3.5 with state-of-the-art KPE and KPG methods. For supervised methods, performance of five-shot prompting GPT-3.5 is comparable or better than HyperMatch along every dimension, and comparable to SciBART+OAGKX in diversity, utility, and faithfulness. In addition, zero-shot prompting outperforms the unsupervised TextRank, PromptRank, and the Azure API in diversity while being competitive across other dimensions. These results suggest that the potential of LLMs for keyphrase prediction may be underappreciated under traditional evaluation paradigms.

\paragraph{Discussion} What have we learned in this re-evaluation? First, the refined evaluation facilitated by \textsc{KPEval} challenges some of the existing model comparisons and emphasizes the difficulty of outperforming baselines across all aspects. In fact, these aspects test unique abilities, exhibiting weak cross-aspect correlations (Appendix \ref{inter-metric-correlation}) and distinct preferences for keyphrase systems (Table \ref{tab:settrans-reeval}, \ref{tab:all-results-main}). Our findings advocate for a tailored approach to metric weighting, allowing users to customize evaluations based on their evaluation desiderata. Finally, our results reveal strong performance of GPT-3.5, encouraging future work to further understand and improve LLMs for keyphrase prediction.

