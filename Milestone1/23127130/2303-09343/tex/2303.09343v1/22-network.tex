Nonlinear elasticity problems are generally solved using a Newton method, which yields very accurate displacement fields at a high computational cost. In this paper, we give a boost to the direct solution procedure by using a pre-trained neural network to compute displacements from forces. This results in much faster estimates, while the quality of solutions depends on the network training.

Artificial neural networks are composed of elements named artificial neurons grouped into multiple layers. A layer applies a transformation on its input data and passes it to the associated activation layer. The result of this operation is then passed to the next layer in the architecture.
Activation functions play an important role in the learning process of neural networks. Their role is to apply a nonlinear transformation to the output of the associated layers thus greatly improving the representation capacity of the network.

While a wide variety of architectures are possible we will use the one proposed by \citet{odot2022}.
It consists of a fully-connected feed-forward neural network with 2 hidden layers
(see \Cref{fig:FCNN}).

 \begin{figure}[ht]
        \centering
        \includegraphics[width=.6\linewidth]{FCNN.png}
        \caption{The proposed architecture is composed of 4 fully connected layers of size the number of degrees of freedom with a PReLU activation function. The input is the nodal forces and the output is the respective nodal displacements.}
        \label[figure]{fig:FCNN}
\end{figure}

The connection between two adjacent layers can be expressed as follows
\begin{equation}\label{eq:forward-propagation}
    \zv_i = \sigma_i(\Wv_i\zv_{i-1} + \bv_{i})\text{ for }1\leqslant i \leqslant n+1,
\end{equation}
where $n$ is the total number of layers, $\sigma(.)$ denotes the element wise activation function, $\zv_0$ and $\zv_{n+1}$ denotes the input and output tensors respectively, $\Wv_i$ and $\bv_i$ are the trainable weight matrices and biases in the $i^{th}$ layer.

In our case the activation functions $\sigma(.)$ are PReLU \cite{he2015}, which provides a learnable parameter $a$, allowing us to adaptively consider both positive and negative inputs.
From now on, we denote the forward pass operation in the network by
\begin{equation}
    \uv_\gv = \Nv(\gv).
\end{equation}
