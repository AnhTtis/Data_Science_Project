We now give a closer look at the procedure to evaluate $\Phi$ and its derivatives.
We use an adjoint method, where the only variable controlled by the optimization solver is $\gv$.
As $J$ only operates on displacement fields,  the physical model plays the role of an intermediary
between these two protagonists.
The adjoint method is well suited to the network-based configuration, as the network can be used
as a black box.

In a standard adjoint procedure, a displacement is computed from a force distribution by solving
\eqref{eq:hyperelastic} using a Newton method, and it is then used to evaluate $\Phi(\gv)$. The Newton method is the algorithm of choice when dealing with non-linear materials, it iteratively solves the hyper-elastic problem producing accurate solutions. This method is also known for easily diverging when the load is reaching a certain limit that depends on the problem. To compute the deformation, one requires the application of multiple substeps of load which highly increases the computation times.
The backward chain requires solving an adjoint problem to evaluate the objective gradient, namely
\begin{equation}\label{eq:adjoint-system}
    \nabla \Phi(\gv) = \pv_\gv
    \quad\text{where}\quad
    \nabla \Fv(\uv_\gv)\T\pv_\gv = \nabla J(\uv_\gv).
\end{equation}
In \eqref{eq:adjoint-system}, the adjoint state $\pv_\gv$ is solution to a linear system involving the
hyperelasticity Jacobian matrix $\nabla \Fv(\uv_\gv)$.
When the network is used, however, the whole pipeline is much more straightforward, as the network
forward pass is only composed of direct operations.
The network-based forward and backward chains read
\begin{equation}
    \Phi(\gv) = J \circ \Nv(\gv)
    \quad\text{and}\quad
    \nabla \Phi(\gv) = \pv_\gv = \left[\nabla\Nv(\gv)\right]\T \nabla J(\uv_\gv),
\end{equation}
respectively.
On a precautionnary basis, let us take a brief look at the (linear) adjoint operator $\nabla \Nv(\gv)\T$.
When $\nabla \Nv(\gv)\T$ is applied, the information propagates backward in the network, following
the same wires as the forward pass.
The displacement gradient $\nabla J(\uv_\gv)$ is fed to the output tensor $\sv_{n+1}$ and the adjoint state is read at the network entry $\sv_0$.
In between, the relation between two layers is the adjoint operation to \eqref{eq:forward-propagation}.
It reads
\begin{equation}\label{eq:backward-propagation}
    \sv_{i-1} = \Wv_i\T\, \nabla\sigma_i(\Wv_i\zv_{i-1} + \bv_{i})\,\sv_i \quad \text{ for } \quad 1\leqslant i \leqslant n+1,
\end{equation}
where $\nabla\sigma_i(\Wv_i\zv_{i-1} + \bv_{i})$ is a diagonal matrix saved during the forward pass.

The network-based adjoint procedure is summarized in \Cref{alg:adjoint}, keeping in mind the backward chain is handled automatically.
Given a nodal force vector $\gv$, evaluating $\Phi(\gv)$ and $\nabla \Phi(\gv)$ requires one forward pass
and one backward pass in the network.
Then, \eqref{eq:opt-problem} may be solved iteratively using a standard gradient-based optimization
algorithm.
Because both network passes consist only of direct operations, the optimization solver is less likely to fail for accuracy reasons, compared to a $\Phi$ evaluation based on an iterative method.
\vspace{3ex}

\begin{algorithm}[H]
    \SetAlgoLined
    \KwData{Current iterate $\gv$}
    Perform the forward pass $\uv_\gv = \Nv(\gv)$\\
    Evaluate $J(\uv_gv)$ and $\nabla J(\uv_\gv)$\\
    Perform the backward pass $\pv_\gv = \left[\nabla\Nv(\gv)\right]\T \nabla J(\uv_\gv) $ \\
    \KwResult{$\nabla \Phi(\gv) = \pv_\gv$}
    \label[algorithm]{alg:adjoint}
    \caption{Network-based adjoint method to evaluate $\Phi$.}
\end{algorithm}

%\textcolor{red}{Justifier que l'adjoint = backward}
