% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {$L_1$}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
  url={https://dl.acm.org/doi/abs/10.1145/1273496.1273501}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK},
    url={https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005},
	url={https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf}
}

@article{ct1965,
  title={An algorithm for the machine calculation of complex {F}ourier series},
  author={Cooley, James W. and Tukey, John W.},
  journal={Mathematics of Computation},
  volume={19},
  number={90},
  pages={297--301},
  year={1965},
  url={https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf}
}



@inproceedings{hannan2020manymodalqa,
  title={Manymodalqa: Modality disambiguation and qa over diverse inputs},
  author={Hannan, Darryl and Jain, Akshay and Bansal, Mohit},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7879--7886},
  year={2020}
}

@article{chen2020open,
  title={Open question answering over tables and text},
  author={Chen, Wenhu and Chang, Ming-Wei and Schlinger, Eva and Wang, William and Cohen, William W},
  journal={arXiv preprint arXiv:2010.10439},
  year={2020}
}

@article{zhu2021tat,
  title={TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance},
  author={Zhu, Fengbin and Lei, Wenqiang and Huang, Youcheng and Wang, Chao and Zhang, Shuo and Lv, Jiancheng and Feng, Fuli and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2105.07624},
  year={2021}
}

@article{aly2021feverous,
  title={Feverous: Fact extraction and verification over unstructured and structured information},
  author={Aly, Rami and Guo, Zhijiang and Schlichtkrull, Michael and Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Cocarascu, Oana and Mittal, Arpit},
  journal={arXiv preprint arXiv:2106.05707},
  year={2021}
}

@InProceedings{jin2022surveytable,
author="Jin, Nengzheng
and Siebert, Joanna
and Li, Dongfang
and Chen, Qingcai",
editor="Sun, Maosong
and Qi, Guilin
and Liu, Kang
and Ren, Jiadong
and Xu, Bin
and Feng, Yansong
and Liu, Yongbin
and Chen, Yubo",
title="A Survey on Table Question Answering: Recent Advances",
booktitle="Knowledge Graph and Semantic Computing: Knowledge Graph Empowers the Digital Economy",
year="2022",
publisher="Springer Nature Singapore",
address="Singapore",
pages="174--186",
abstract="Table Question Answering (Table QA) refers to providing precise answers from tables to answer a user's question. In recent years, there have been a lot of works on table QA, but there is a lack of comprehensive surveys on this research topic. Hence, we aim to provide an overview of available datasets and representative methods in table QA. We classify existing methods for table QA into five categories according to their techniques, which include semantic-parsing-based, generative, extractive, matching-based, and retriever-reader-based methods. Moreover, because table QA is still a challenging task for existing methods, we also identify and outline several key challenges and discuss the potential future directions of table QA.",
isbn="978-981-19-7596-7"
}

@article{chen2020kgpt,
  title={Kgpt: Knowledge-grounded pre-training for data-to-text generation},
  author={Chen, Wenhu and Su, Yu and Yan, Xifeng and Wang, William Yang},
  journal={arXiv preprint arXiv:2010.02307},
  year={2020}
}

@article{nan2022fetaqa,
  title={FeTaQA: free-form table question answering},
  author={Nan, Linyong and Hsieh, Chiachun and Mao, Ziming and Lin, Xi Victoria and Verma, Neha and Zhang, Rui and Kry{\'s}ci{\'n}ski, Wojciech and Schoelkopf, Hailey and Kong, Riley and Tang, Xiangru and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={35--49},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{heinzerling2020language,
  title={Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries},
  author={Heinzerling, Benjamin and Inui, Kentaro},
  journal={arXiv preprint arXiv:2008.09036},
  year={2020}
}

@article{yang2022tableformer,
  title={Tableformer: Robust transformer modeling for table-text encoding},
  author={Yang, Jingfeng and Gupta, Aditya and Upadhyay, Shyam and He, Luheng and Goel, Rahul and Paul, Shachi},
  journal={arXiv preprint arXiv:2203.00274},
  year={2022}
}
@article{eisenschlos2021mate,
  title={MATE: multi-view attention for table transformer efficiency},
  author={Eisenschlos, Julian Martin and Gor, Maharshi and M{\"u}ller, Thomas and Cohen, William W},
  journal={arXiv preprint arXiv:2109.04312},
  year={2021}
}

@article{herzig2021open,
  title={Open domain question answering over tables via dense retrieval},
  author={Herzig, Jonathan and M{\"u}ller, Thomas and Krichene, Syrine and Eisenschlos, Julian Martin},
  journal={arXiv preprint arXiv:2103.12011},
  year={2021}
}

@article{huang2022mixed,
  title={Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA},
  author={Huang, Junjie and Zhong, Wanjun and Liu, Qian and Gong, Ming and Jiang, Daxin and Duan, Nan},
  journal={arXiv preprint arXiv:2210.05197},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{shi2023replug,
  title={REPLUG: Retrieval-Augmented Black-Box Language Models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@article{yu2022generate,
  title={Generate rather than retrieve: Large language models are strong context generators},
  author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
  journal={arXiv preprint arXiv:2209.10063},
  year={2022}
}

@article{bonifacio2022inpars,
  title={Inpars: Data augmentation for information retrieval using large language models},
  author={Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  journal={arXiv preprint arXiv:2202.05144},
  year={2022}
}

@inproceedings{cho2018adversarial,
  title={Adversarial tableqa: Attention supervision for question answering on tables},
  author={Cho, Minseok and Amplayo, Reinald Kim and Hwang, Seung-won and Park, Jonghyuck},
  booktitle={Asian Conference on Machine Learning},
  pages={391--406},
  year={2018},
  organization={PMLR}
}

@inproceedings{chenopen,
  title={Open Question Answering over Tables and Text},
  author={Chen, Wenhu and Chang, Ming-Wei and Schlinger, Eva and Wang, William Yang and Cohen, William W},
  booktitle={International Conference on Learning Representations}
}

@inproceedings{lewis2020retrieval,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9459--9474},
 publisher = {Curran Associates, Inc.},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 url = {https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{colon2021combining,
  title={Combining pre-trained language models and structured knowledge},
  author={Colon-Hernandez, Pedro and Havasi, Catherine and Alonso, Jason and Huggins, Matthew and Breazeal, Cynthia},
  journal={arXiv preprint arXiv:2101.12294},
  year={2021}
}

@article{richardson2023commonsense,
  title={Commonsense Reasoning for Conversational AI: A Survey of the State of the Art},
  author={Richardson, Christopher and Heck, Larry},
  journal={arXiv preprint arXiv:2302.07926},
  year={2023}
}

@article{jeronymo2023inpars,
  title={InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval},
  author={Jeronymo, Vitor and Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Zavrel, Jakub and Nogueira, Rodrigo},
  journal={arXiv preprint arXiv:2301.01820},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{zhang2022contrastive,
  title={Contrastive learning of medical visual representations from paired images and text},
  author={Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D and Langlotz, Curtis P},
  booktitle={Machine Learning for Healthcare Conference},
  pages={2--25},
  year={2022},
  organization={PMLR}
}

@inproceedings{trotman2014improvements,
author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
title = {Improvements to BM25 and Language Models Examined},
year = {2014},
isbn = {9781450330008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2682862.2682863},
doi = {10.1145/2682862.2682863},
abstract = {Recent work on search engine ranking functions report improvements on BM25 and Language Models with Dirichlet Smoothing. In this investigation 9 recent ranking functions (BM25, BM25+, BM25T, BM25-adpt, BM25L, TF1°δ°p\texttimes{}ID, LM-DS, LM-PYP, and LM-PYP-TFIDF) are compared by training on the INEX 2009 Wikipedia collection and testing on INEX 2010 and 9 TREC collections. We find that once trained (using particle swarm optimization) there is very little difference in performance between these functions, that relevance feedback is effective, that stemming is effective, and that it remains unclear which function is best over-all.},
booktitle = {Proceedings of the 2014 Australasian Document Computing Symposium},
pages = {58–65},
numpages = {8},
keywords = {Relevance Ranking, Procrastination, Document Retrieval},
location = {Melbourne, VIC, Australia},
series = {ADCS '14}
}

@inproceedings{zhang2019dialogpt,
author = {Zhang, Yizhe and Sun, Siqi and Galley, Michel and Chen, Yen-Chun and Brockett, Chris and Gao, Xiang and Gao, Jianfeng and Liu, JJ (Jingjing) and Dolan, Bill},
title = {DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
booktitle = {arXiv:1911.00536},
year = {2019},
month = {November},
abstract = {We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.

Github link: https://github.com/microsoft/dialogpt},
url = {https://www.microsoft.com/en-us/research/publication/dialogpt-large-scale-generative-pre-training-for-conversational-response-generation/},
}

@article{dinan2018wizard,
  title={Wizard of wikipedia: Knowledge-powered conversational agents},
  author={Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
  journal={arXiv preprint arXiv:1811.01241},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{heck2020zero,
  title={Zero-Shot Visual Slot Filling as Question Answering},
  author={Heck, Larry and Heck, Simon},
  journal={arXiv preprint arXiv:2011.12340},
  year={2020}
}

@article{heck2013multimodal,
  title={Multimodal conversational search and browse},
  author={Heck, Larry and Hakkani-T{\"u}r, Dilek and Chinthakunta, Madhu and Tur, Gokhan and Iyer, Rukmini and Parthasacarthy, Partha and Stifelman, Lisa and Shriberg, Elizabeth and Fidler, Ashley},
  year={2013}
}

@article{huang2015leveraging,
  title={Leveraging deep neural networks and knowledge graphs for entity disambiguation},
  author={Huang, Hongzhao and Heck, Larry and Ji, Heng},
  journal={arXiv preprint arXiv:1504.07678},
  year={2015}
}

@inproceedings{hakkani-tr2014probabilistic,
author = {Hakkani-Tür, Dilek and Celikyilmaz, Asli and Heck, Larry and Tur, Gokhan and Zweig, Geoff},
title = {Probabilistic Enrichment of Knowledge Graph Entities for Relation Detection in Conversational Understanding},
booktitle = {Proceedings of Interspeech},
year = {2014},
month = {September},
abstract = {Knowledge encoded in semantic graphs such as Freebase has been shown to benefit semantic parsing and interpretation of natural language user utterances. In this paper, we propose new methods to assign weights to semantic graphs that reflect common usage types of the entities and their relations. Such statistical information can improve the disambiguation of entities in natural language utterances. Weights for entity types can be derived from the populated knowledge in the semantic graph, based on the frequency of occurrence of each type. They can also be learned from the usage frequencies in real world natural language text, such as related Wikipedia documents or user queries posed to a search engine. We compare the proposed methods with the unweighted version of the semantic knowledge graph for the relation detection task and show that all weighting methods result in better performance in comparison to using the unweighted version.},
url = {https://www.microsoft.com/en-us/research/publication/probabilistic-enrichment-of-knowledge-graph-entities-for-relation-detection-in-conversational-understanding/},
edition = {Proceedings of Interspeech},
}

@INPROCEEDINGS{jia2017learning,
  author={Jia, Robin and Heck, Larry and Hakkani-Tür, Dilek and Nikolov, Georgi},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Learning concepts through conversations in spoken dialogue systems}, 
  year={2017},
  volume={},
  number={},
  pages={5725-5729},
  doi={10.1109/ICASSP.2017.7953253}}
