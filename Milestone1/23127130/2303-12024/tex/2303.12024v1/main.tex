% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumerate}
% \usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{textcomp}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}

% \usepackage{natbib}
\usepackage{booktabs}
\usepackage{multirow,bigstrut}
\usepackage{tabu}
\usepackage{multirow, multicol}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{placeins}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{multirow, makecell}
\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{cTBL: Augmenting Large Language Models for Conversational Tables}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Anirudh S Sundar, Larry Heck \\
  AI Virtual Assistant (AVA) Lab \\ The Georgia Institute of Technology\\
  \texttt{ \{asundar34,larryheck\}@gatech.edu} \\}

\begin{document}
\maketitle
\begin{abstract}

An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (\textsc{cTBL}), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. \textsc{cTBL} uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5\% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the \textsc{HyrbiDialogue} dataset. Additionally, \textsc{cTBL} performs tabular knowledge retrieval using encoder and decoder models, resulting in up to 46\% relative improvement in ROUGE scores and better human evaluation for response generation on \textsc{HyrbiDialogue}. 

\end{abstract}

\section{Introduction}
Equipping conversational AI with multimodal capabilities expands the scope of conversations humans have with such systems. An ongoing challenge in multimodal conversational AI is the development of systems that generate conversationally coherent responses grounded in textual and non-textual modalities \cite{sundar-heck-2022-multimodal}. 

Though large language models store real-world information in their parameters \cite{roberts-etal-2020-much, heinzerling-inui-2021-language}, augmenting them with conversation-dependent extrinsic knowledge to generate accurate responses is an open research problem. While humans can process tables by looking up contextual information based on rows and columns, large language models struggle to retrieve relevant information in the presence of conversationally plausible distractors. To address this problem, this paper introduces Conversational Tables (\textsc{cTBL}), a three-step encoder-decoder architecture to augment large language models with tabular information for conversations.

% To tackle this problem, we present Table Informed Dialogue Agent (\textsc{\textsc{cTBL}}), a three-step approach to retrieve and generate dialogue responses using information available in conversation history and external tabular sources. 

In the first step, \textsc{cTBL} uses a dual-encoder Transformer-based  \cite{vaswani2017attention} Dense Table Retriever (DTR) to retrieve the correct table from the entire corpus based on the user's query. The second step consists of a fine-tuned dual-encoder Transformer to track the system state and rank cells in the retrieved table based on their relevance to the conversation. Finally, \textsc{cTBL} prompts GPT-3.5 with the ranked cells to generate a natural language response. Figure \ref{fig:system_overview} provides an overview of the system architecture. 

While prior work separates knowledge retrieval and response generation, this paper shows that large language models can jointly perform knowledge retrieval and response generation over small knowledge sources. The knowledge retrieval in \textsc{cTBL} is split into two stages. The first stage ranks knowledge sources conditioned on an input query, existing dialogue context, and a retrieved table. The second stage utilizes large language models to filter out irrelevant information from these ranked sources and generate the correct response. 

\textsc{HybriDialogue} \cite{nakamura-etal-2022-hybridialogue}, a dataset of conversations grounded on structured and unstructured knowledge from tables and text, introduces the task of responding to a message on a given turn by utilizing information from external knowledge and prior dialogue turns.  

\textsc{cTBL} builds on the baseline \textsc{HybriDialogue} model for table retrieval, system state tracking, and response generation. Resulting experiments demonstrate up to 5\% relative improvement in table retrieval and up to 46\% improvement in ROUGE scores on generated responses. Additionally, human evaluation shows that \textsc{cTBL} is preferred over previous approaches. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{Images/cTBL.drawio.png}
    \caption{\textsc{cTBL} System Architecture Overview. Dense Table Retrieval identifies the table most relevant to the initial query. The retrieved table is provided to the state tracker for follow-up queries. State tracking ranks the cells in the table based on their ability to answer a follow-up query. Response Generation utilizes a Transformer Decoder provided with the ranked cell information and the follow-up query to convert tabular data into a natural language response and continue the conversation.}
    \label{fig:system_overview}
\end{figure*}


Our contributions are as follows:

 \begin{enumerate}
    \item We introduce Conversational Tables (\textsc{cTBL}), a Transformer-based model addressing the problem of response generation grounded on tabular knowledge 
     \item We experimentally demonstrate that Dense Table Retrieval using neural models fine-tuned with a summary of tabular information outperforms keyword-matching-based sparse techniques for table retrieval.
     \item We show that augmenting state-of-the-art large language model decoders using knowledge sources ranked by encoder language models yields better results on automatic (ROUGE) and human (Coherence, Fluency, and Informativeness) evaluation for knowledge-grounded response generation while limiting the number of API calls to these models. 
 \end{enumerate}

% Outline of the paper $\hdots$
In this paper, we introduce the \textsc{cTBL} system and demonstrate its application to the \textsc{HybriDialogue} dataset. In Section \ref{sec:related_work}, we discuss prior work in the fields of Table Question Answering and Knowledge Grounded Response Generation. Section \ref{sec:Method} details the various components of the \textsc{cTBL} system architecture as outlined in Figure \ref{fig:system_overview}. In Section \ref{sec:Experiments}, we evaluate \textsc{cTBL} to previous approaches for conversations over tables and provide experimental results from automatic and human evaluation. Finally, Section \ref{sec:Conclusion} concludes the paper and discusses future work.  



\section{Related Work}
\label{sec:related_work}
\subsection{Table Question Answering}
A well-studied precursor to conversations over tables is Table Question Answering. In \textsc{WikiTableQuestions}, \citet{pasupat-liang-2015-compositional} transform HTML tables into a knowledge graph and retrieve the correct answer by converting natural language questions into graph queries. FRETS \cite{jauhar-etal-2016-tables} uses a log-linear model conditioned on alignment scores between cells in tables and individual QA pairs in the training set. \textsc{NeOp} \cite{cho2018adversarial} uses a multi-layer sequential network with attention supervision to answer queries conditioned on tables. \textsc{ManyModalQA} \cite{hannan2020manymodalqa} uses a modality selection network along with pre-trained state-of-the-art text-based QA, Table-based QA, and Image-based QA models to jointly answer questions over text, tables, and images. \textsc{Hybrider} \cite{chen-etal-2020-hybridqa} performs multi-hop QA over tables by using keyword-matching for cell linking followed by BERT \cite{devlin-etal-2019-bert} for reasoning.   OTT-QA \cite{chen2020open} uses a fusion retriever to identify relevant tables and text and a cross-block reader based on a long-range Sparse Attention Transformer \cite{ainslie-etal-2020-etc} to choose the correct answer. Visual Slot \cite{heck2020zero} performs multi-task fine-tuning of Transformer encoders by modeling slot filling as question answering over tabular and visual information.     \textsc{TaPaS} \cite{herzig-etal-2020-tapas} and TaBERT \cite{yin-etal-2020-tabert} extend BERT \cite{devlin-etal-2019-bert} for table question answering by pre-training a masked language model over text-table pairs. More recent work building off of the Transformer architecture for Table Question Answering include \cite{ eisenschlos2021mate,li-etal-2021-dual, herzig-etal-2021-open, zayats-etal-2021-representations, zhao-etal-2022-multihiertt,huang2022mixed, yang2022tableformer}. \citet{jin2022surveytable} provides a detailed survey of advances in table question answering. 

\subsection{Knowledge Grounded Response Generation}
Early work concerning grounding responses generated by language models in real-world knowledge were inspired by solutions to improve priors for open-domain dialogue \cite{heck2013multimodal, hakkani-tr2014probabilistic, huang2015leveraging, jia2017learning}.  More recently,  knowledge grounded response generation has been applied to mitigate the hallucination problem \cite{maynez-etal-2020-faithfulness, shuster-etal-2021-retrieval-augmentation} in Large Language Models (LLMs). RAG \cite{lewis2020retrieval} fine-tunes LLMs using Dense Passage Retrieval \cite{karpukhin-etal-2020-dense} over a Wikipedia dump to ground responses for Open Domain Question Answering. KGPT \cite{chen2020kgpt} and SKILL \cite{moiseev-etal-2022-skill} pre-trains a Transformer encoder \cite{vaswani2017attention} with English Wikidump for Natural Language Generation. Fusion-in-Decoder \cite{izacard-grave-2021-leveraging} fine-tunes decoder models based on evidence obtained using Dense Passage Retrieval.   Recent work has also seen a dual-stage approach where LLMs generate knowledge sources based on prompts \cite{yu2022generate, bonifacio2022inpars, jeronymo2023inpars}. Closest to our work, Wizard of Wikipedia \cite{dinan2018wizard} jointly optimizes an encoder-decoder Transformer to produce dialogue responses conditioned on retrieved knowledge and dialogue context but does not extend their approach to the multiple modalities. \textsc{RePlug} \cite{shi2023replug} ensembles output responses generated by prompting large language models with inputs from a dense retriever in a zero-shot setting. However, this requires multiple API calls to state-of-the-art LLMs.  A survey of knowledge fusion in LLMs is available in \citet{colon2021combining} and \citet{richardson2023commonsense}.

In contrast to prior work that focuses on either Table Question Answering or Knowledge Grounded Response Generation, \textsc{cTBL} solves the problem of response generation grounded on tabular knowledge. Furthermore, while \textsc{cTBL} is fine-tuned for table retrieval and filtering out incorrect references, it leverages the power of large pre-trained language models for dialogue generation. Additionally, filtering out incorrect references by fine-tuning open-source table and knowledge retrievers results in \textsc{cTBL} limiting the number of API calls to LLMs.  

% LLMs store knowledge in their weights \cite{ heinzerling2020language}
% External knowledge \cite{bi-etal-2019-incorporating}

% Dialogue \cite{reddy-etal-2019-coqa}

 


\section{Method}
\label{sec:Method}
The challenge of conversations grounded in tabular information consists of three tasks - table retrieval, system state tracking, and response generation. Table retrieval involves identifying the correct table in the dataset given a natural language query. System state tracking requires ranking the cells in the table to answer a follow-up query. Finally, response generation is concerned with converting the ranked cells into a natural language response. 

\subsection{Retrieval}
The first step in responding to queries related to tabular information requires selecting the correct table from a large corpus. \textsc{cTBL} formulates table retrieval as document retrieval, where the goal is to assign a score to each table based on its relevance to the natural language query. Following \citet{karpukhin-etal-2020-dense}, \textsc{cTBL} consists of a dual-encoder-based Dense Table Retrieval (DTR) model. The DTR model pre-computes a vectorized embedding of all tables in the corpus. Given a question at inference, the retrieved table is closest to the question in the embedded space, indicated by the upper-left portion of Figure \ref{fig:system_overview}. 

The DTR model consists of a table encoder and a question encoder, initialized from RoBERTa-base \cite{liu2019roberta}. The input to the table encoder contains the table's title, the section text from the Wikipedia page on which the table occurs, and the text that appears in the article's introduction on Wikipedia as shown in Figure \ref{fig:WNBA}.  The input to the question encoder is the current query from the first turn of conversation. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{Images/WNBA_Finals_crop.png}
    \caption{The input to the DTR text-encoder includes the title of the page, the introduction to the article, and the section title and introduction paragraph associated with the table}
    \label{fig:WNBA}
\end{figure}

Averaging the last hidden state at the table and question encoder produces 768-dimensional embeddings of the table information and the query. 

The DTR model is fine-tuned using a contrastive prediction task to maximize the similarity between embeddings of a query $q$ and the correct table to retrieve $p$ while minimizing the similarity to incorrect tables $\{n_i\}_{i=1}^N$. Following \citet{karpukhin-etal-2020-dense}, normalized embedding vectors are used to optimize the objective in Equation \ref{eq:table_ret}:
\begin{equation}
\begin{split}
    \min L(q,p,n_1,\hdots, n_N) =\\ \min \bigg{(} -\log \frac{e^{q^Tp}}{e^{q^Tp} + \sum_{i=1}^Ne^{q^Tn_i}} \bigg{)}
    \label{eq:table_ret}
\end{split}
\end{equation}

Given a batch $B$ of $d$-dimensional query embeddings $Q$ and table embeddings $P$, the DTR model computes the similarity $\mathbf{Q}\mathbf{P}^T ( \in \mathbb{R}^{B \times B} )$ between every query and table in the batch. This allows sampling negatives as the tables from other query-table pairs and creates $B^2$ training samples in each batch (B positive pairs along the diagonal and $B^2-B$ negatives). 

\subsection{Coarse System State Tracking}
The next task, system state tracking, involves ranking the cells in the retrieved table by their relevance to a follow-up query. \textsc{cTBL} splits system state tracking into two sub-tasks - coarse and fine system state tracking. Coarse system state tracking involves ranking the cells in the table, while fine system state tracking identifies the fine-grained information in the most relevant cell to answer the query. 

\textsc{cTBL} uses a RoBERTa-base dual-encoder architecture for coarse system state tracking. The table encoder embeds all cells and the associated hyperlinked information while the question encoder embeds the dialogue history (consisting of queries and responses from previous turns) and the current query. 

The question and table encoder is optimized using the triplet loss configuration to rank the cells by their relevance to the follow-up query, indicated by the section on the upper-right of Figure \ref{fig:system_overview}. The objective function aims to minimize the distance between the anchor $a$ and the positive $p$, while pushing the negative $n$ further away from the anchor up to a margin $m$ (Equation \ref{eq:triplet_loss}).  

\begin{equation}
\begin{split}
    \min L(a,p, &n) =  \\ \min ( \max\{d(a_i,p_i) - d(a_i, & n_i) + m, 0 \} ) 
    \label{eq:triplet_loss}
\end{split}
\end{equation}

\begin{equation}
    d(x,y) = ||x - y||_p
\end{equation}

The anchor consists of the entire dialogue history (queries and responses on previous turns) concatenated with the current query, and the positive is the correct cell. We use other cells from the same table that do not answer the query as negatives. The distance function $d(\cdot)$ is the 2-norm. 

\subsection{Fine System State Tracking and Response Generation}
In contrast to coarse system state tracking, fine system state tracking involves identifying the correct cell and the exact phrase that answers the query from a ranked subset. The extracted phrase is converted into a natural language response that is conversationally coherent. 

\textsc{cTBL} uses GPT-3.5 \cite{brown2020language} to perform fine system state tracking and response generation jointly. GPT-3.5 is prompted to generate a natural language response to the follow-up query conditioned on cells of the table ranked by their relevance to a query as obtained from the coarse state tracker. The bottom-right portion of Figure \ref{fig:system_overview} outlines this process. 

\begin{comment}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/WNBA_Finals_edit.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\end{comment}

\section{Experiments}
\label{sec:Experiments}

\subsection{\textsc{HybriDialogue}}
The \textsc{HybriDialogue} dataset \cite{nakamura-etal-2022-hybridialogue} consists of 4800 natural language conversations grounded in text and tabular information from Wikipedia. Crowd workers decompose multi-hop questions from the OTT-QA dataset \cite{chen2020open} into natural questions and conversational responses about tabular data. Dialogues in the dataset average 4-5 conversation turns, with a total of 21,070 turns in the dataset. Example conversations are available in Table \ref{tab:Response_comparison}. 

\begin{comment}    
\begin{table}[t]
    \centering
    \begin{tabular}{c|c}
         \hline 
         Dataset Statistics &  \\
         \hline 
         # Training conversations & 4359 \\
         # Development conversations & 242 \\ 
         # Testing conversations & 243 \\
         Turns per conversation & 4.34  \\ 
         \hline 
    \end{tabular}
    \caption{A summary of the \textsc{HybriDialogue}dataset \cite{nakamura-etal-2022-hybridialogue}}
    \label{tab:hybridialogue_summary}
\end{table}
\end{comment}
\subsection{Table Retrieval}

The first conversation turn requires selecting the correct table based on the input query. The Dense Table Retriever is fine-tuned for 20 epochs using Adam \cite{kingma2014adam} with a learning rate of 1e-6 and a linear learning schedule with five warmup steps. The loss function is a modification of the contrastive loss implementation from ConVIRT \cite{zhang2022contrastive}, with image embeddings replaced by table embeddings. The baseline table retriever implementation is the  BM25Okapi Retriever \cite{trotman2014improvements} from \href{https://github.com/dorianbrown/rank_bm25}{rank-bm25}. Results from Table \ref{tab:retrieval} indicate that \textsc{cTBL}-DTR achieves better Mean Reciprocal Rank (MRR), Top-1 Accuracy, and Top-3 Accuracy than BM25 on \textsc{HybriDialogue}.   
\begin{table}[t]
    \centering
    % \begin{tabular}{c|c|c|c}
    \begin{tabularx}{\columnwidth} { 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X | }
    \hline
          &  MRR $@$10 & Top 1 Acc & Top 3 Acc  \\
          \hline 
         BM25 & 0.800 & 0.740 & 0.855 \\
         \textsc{cTBL} - 
         DTR & {\textbf{0.846}} & {\textbf{0.777}} & {\textbf{0.901}}\\
        \hline 
    \end{tabularx}
    \caption{BM25 vs \textsc{cTBL}-DTR for retrieval on first turn of conversation, results on \textsc{HybriDialogue} testing dataset. \textsc{cTBL}-DTR obtains up to 5\% relative improvement over sparse table retrieval}
    \label{tab:retrieval}
\end{table}


\subsection{Coarse State Tracking}
Coarse state tracking ranks cells from a table based on their relevance to queries from later conversation turns. As before, the dual-encoder coarse state tracker of \textsc{cTBL} consists of RoBERTa-base fine-tuned using Adam with a learning rate of 1e-6 and a linear learning schedule with five warmup steps. In contrast to Table Retrieval, the state tracker uses the triplet margin loss with a margin of 1.0 instead of contrastive loss. Fine-tuning RoBERTa-base beats SentenceBERT \cite{reimers-gurevych-2019-sentence} and nearly achieves the same MRR $@10$ as TaPas \cite{herzig-etal-2020-tapas} fine-tuned on the SQA dataset \cite{iyyer-etal-2017-search} and \textsc{HybriDialogue}, as indicated by results in Table \ref{tab:state_tracking}. 
In addition, Table \ref{tab:dst_topk} reports the Top-1, Top-3, and Top-10 accuracy of \textsc{cTBL}. The correct cell is retrieved in approximately 56\% of conversations at inference and ranks among the Top-3 and Top-10 retrievals about 78\% and 93\% of the time, respectively. 

\begin{table}[h]
    \centering
    % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|c}
          & MRR$@$10 \\
    \hline 
         SentenceBERT \tiny{\cite{reimers-gurevych-2019-sentence}} & 0.603  \\
         TaPas \tiny{\cite{herzig-etal-2020-tapas}}  & \textbf{0.689} \\
         \textsc{cTBL} - RoBERTa-base & 0.683 \\
    \hline 
    \end{tabular}
    % }
    \caption{System state tracking results on \textsc{HybriDialogue}. \textsc{cTBL} achieves nearly the same MRR$@$ 10 as TaPaS, without additional table pretraining on SQA \cite{iyyer-etal-2017-search}}
    \label{tab:state_tracking}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
    \hline 
         Top-1 & 0.559 \\
         Top-3 & 0.778 \\ 
         Top-10 & 0.925 \\
    \hline 
    \end{tabular}
    \caption{Top-k accuracy for \textsc{cTBL} on coarse system state tracking. \textsc{cTBL} ranks the correct cell as the top reference in 56\% of follow-up queries on \textsc{HybriDialogue}.}
    \label{tab:dst_topk}
\end{table}

\subsection{Fine State Tracking and Response Generation}
\textsc{cTBL} uses GPT-3.5 (text-davinci-003) with the existing dialogue context, the current query, and the retrieved references from coarse state tracking to obtain a natural language response. Since fine-tuning the best available version of the model is cost prohibitive, we opt to prompt GPT-3.5 to generate responses instead. 

\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|c|c|c}
         \hline 
         & \textsc{cTBL} NoK & \textsc{cTBL} Top-1 & \textsc{cTBL} Top-3 &\textsc{HybriDialogue} \\
         \hline 
         ROUGE-1 Precision & 0.487 & 0.603 & \textbf{0.642} & 0.438\\ 
         ROUGE-2 Precision & 0.229 & 0.304 &\textbf{0.322} & 0.212 \\   
         ROUGE-L Precision & 0.422 & 0.517 & \textbf{0.548} & 0.375\\
         \hline
    \end{tabular}
    \caption{Automatic Evaluation Metrics - \textsc{cTBL} No Knowledge (NoK), Top-1 Knowledge, Top-3 Knowledge, and DialoGPT}
    \label{tab:Automatic eval}
\end{table*}

Motivated by the results from Table \ref{tab:dst_topk}, the fine state tracker of \textsc{cTBL} is evaluated in two different configurations by prompting GPT-3.5 augmented with the Top-1 and Top-3 knowledge references retrieved by the coarse state tracker (\textsc{cTBL} Top-1 and \textsc{cTBL} Top-3). Due to limits on token length associated with the OpenAI API, stopwords are removed from the knowledge provided in the prompt. Since LLMs store factual information in their weights \cite{roberts-etal-2020-much, heinzerling-inui-2021-language}, the baseline is few-shot prompting (using two examples) with no knowledge sources (\textsc{cTBL}-NoK). In addition, \textsc{cTBL} is measured against fine-tuned DialoGPT-medium \cite{zhang2019dialogpt} with Top-1 knowledge for a better comparison with prior work \cite{nakamura-etal-2022-hybridialogue}. 

Table \ref{tab:Automatic eval} reports ROUGE-1, ROUGE-2, and ROUGE-L precision \cite{lin-2004-rouge} for all evaluated models. Results indicate that using additional knowledge improves n-gram overlap with the ground truth reference, indicated by the fact that \textsc{cTBL} Top-1 scores higher than the baseline no-knowledge model. Furthermore, \textsc{cTBL} Top-3 achieves the best performance on all automatic metrics, indicating the advantages of splitting knowledge retrieval into coarse and fine state tracking. All three configurations of \textsc{cTBL} outperform \textsc{HybriDialogue}. 

\begin{comment}
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
         Method & SacreBLEU  \\
         DialoGPT & 21.63 \\
         \textbf{Ours - Fine tuned GODEL} & \textbf{24.66 } 
    \end{tabular}
    \caption{Dialogue Generation Results}
    \label{tab:dialogue_generation}
\end{table}
\end{comment}



\subsection{Human Evaluation}
We also perform human evaluation to better understand the performance of \textsc{cTBL} using metrics presented in \citet{nakamura-etal-2022-hybridialogue} - Coherence, Fluency, and Informativeness.

Coherence and Fluency are measured using crowd workers from Amazon Mechanical Turk (AMT), while measuring Informativeness is divided among the paper's authors. Crowd workers evaluate the Coherence and Fluency of responses from DialoGPT and GPT-3.5 with the top-3 knowledge references for 50\% of the testing data. 

Borrowing from \citet{nakamura-etal-2022-hybridialogue}, the conversationally coherent response is defined as one that best continues the conversation given prior context. Crowdworkers choose the response that is better at answering the last question of the context and continuing dialogue flow. The absence of grammatical and spelling errors and the appropriate use of parts of speech defines a response's fluency. To ensure response quality, we recruit Turkers with the Masters qualification on AMT and from English-speaking countries (USA, Canada, Australia, New Zealand, or Great Britain). The median time to complete a task is 30 seconds, and workers are paid \$0.05 per task. Two crowd workers evaluate each response. A model's response is considered more coherent or fluent only if there is an agreement between both evaluations. Based on the results from Table \ref{tab:coherence_fluency}, the response generated by \textsc{cTBL} Top-3 is more coherent than fine-tuned DialoGPT from \textsc{HybriDialogue} in 84.2\% of cases. It is more fluent 82.7\% of the time. 

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
     \hline 
     & \textsc{cTBL} Top-3 vs \textsc{HybriDialogue} \\
     \hline
     Coherence & 0.842 \\ 
     Fluency & 0.827 \\
     \hline 
    \end{tabular}
    \caption{Coherence and Fluency - \textsc{cTBL} Top-3 is more conversationally coherent than the best performing \textsc{HybriDialogue} system 84.2\% of the time and is more fluent 82.7\% of the time  }
    \label{tab:coherence_fluency}
\end{table}

To evaluate informativeness, we identify whether generated responses are semantically identical to the ground truth response. Two annotators evaluate each response, and a model is considered more informative only if there is inter-annotator agreement. The lack of demonstrative examples in the prompting process causes responses generated by \textsc{cTBL} Top-1 and Top-3 to be longer than the ground truth response. Therefore, the knowledge-augmented \textsc{cTBL} responses are said to be informative if all the information provided in the ground truth response is captured in the model response, even if \textsc{cTBL} includes additional information. 
% As a result, we evaluate whether all the information provided in the ground truth response is captured in the GPT-3.5 response, even if GPT-3.5 includes additional information. 
The results in Table \ref{tab:GT_same} indicate that \textsc{cTBL} Top-3 contains the same information as the ground truth response 54\% of the time, higher than \textsc{cTBL} Top-1 at 48.8\%, demonstrating the advantages of splitting retrieval into coarse and fine state tracking. Based on these results, we hypothesize that the attention mechanism in decoder models allows for additional knowledge retrieval. \textsc{cTBL} NoK produces the correct response 26.4\% of the time. This shows that \textsc{HybriDialogue} consists of questions and answers based on general world knowledge stored in the weights of LLMs. Fine-tuning DialoGPT-medium performs close to \textsc{cTBL} NoK, indicating that fine-tuning a smaller parameter model (345M) yields similar results to prompting a much larger model (175B). 

Table \ref{tab:Response_comparison} compares responses generated by the different configurations of \textsc{cTBL} on \textsc{HybriDialogue}. The Context represents the dialogue history, with the last question on the last line representing the follow-up query to be answered. The green text denotes the ground truth response. Knowledge K1, K2, and K3 represent cells of the table retrieved during state tracking. \textsc{cTBL} NoK generates a response based on the context alone, \textsc{cTBL} Top-1 generates a response conditioned on K1, and \textsc{cTBL} Top-3 generates a response conditioned on K1, K2, and K3. In the first two examples, when K1 cannot answer the question, \textsc{cTBL} NoK and \textsc{cTBL} Top-1 hallucinate a response. 
In contrast, \textsc{cTBL} Top-3 can generate the correct response since  K2 or K3 contains necessary knowledge, representing the advantages of augmenting response generation with additional relevant knowledge. The third example illustrates a failure case where the state tracker module correctly retrieves three rows of the table corresponding to oil and gas industries. Still, the response generation module fails to utilize this information when converting the retrieved state into a response. In general, \textsc{cTBL} struggles with counting, which we hope to address in future work. 

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
    \hline 
          & Informativeness \\
          \hline 
         \textsc{HybriDialogue} & 0.215 \\
         \textsc{cTBL} - NoK& 0.264 \\
         \textsc{cTBL} Top-1 &  0.488 \\
         \textbf{\textsc{cTBL} Top-3} & \textbf{0.540} \\
    \hline 
    \end{tabular}
    \caption{Human Evaluation Metrics - Fraction of cases where model response is semantically identical to ground truth response. Using more knowledge sources results in responses that are more informative}
    \label{tab:GT_same}
\end{table}


\begin{table*}[h]
    \centering 
    \begin{tabular}{p{4.5cm}|p{4.5cm}|p{4.5cm}}
         Context & Knowledge & Response   \\
         \hline
          \multirow{3}{4.6cm}{
          Q1: Can you give me a list of transfer in to the D.C. United season? R1: Here is a list of transfer in to the D.C. United season.  Q2: Who was born on July 7, 1989? R2: Casey Townsend was born July 7,1989.  Q3: What number does Casey Townsend wear? \textcolor{OliveGreen}{R3:Casey Townsend wore the number 16.}} & K1: [CELL] Fee/Notes is 2014 MLS SuperDraft & \textsc{cTBL} NoK: Casey Townsend wears the number 31 for D.C. United. \\
          & K2: [CELL] No is 16 & \textsc{cTBL} Top-1: Casey Townsend was drafted in the 2014 MLS SuperDraft and wears the number 9 for D.C. United.   \\
          & K3: [CELL] Date is January
25 , 2013 & \textsc{cTBL} Top-3: Casey Townsend wears number 16, and he was drafted by D.C. United in the 2014 MLS SuperDraft on January 25, 2013 \\
\hline 
% \multirow{3}{4.5cm}{Q1: Can you tell me more about Netherland video games? R1:Netherland video games market make up 3.95\% of the entire European market.  Q2: Can you give me a list of game developers form the Netherlands? R2: Here is a list of game developers form the Netherlands. Q3:  What company developed a ship simulator? R3:VSTEP developed the ship simulator. Q4: When was VSTEP founded? \textcolor{OliveGreen}{A4: VSTEP was founded in 2002.} } & - & VSTEP was founded in 2004. \\
%& {\small [PARAGRAPH] Ship Simulator is a ship simulator , a type of vehicle simulation computer game which simulates maneuvering various ships in different environments , although without the effects of wind and current . It was developed by Dutch company VSTEP and released by the former company Lighthouse Interactive , which closed down in 2009.}  & {\small VSTEP was founded in 2006 and they developed the Ship Simulator game. The game is a type of vehicle simulation computer game that simulates maneuvering various ships in different environments, although without the effects of wind and current.} \\
% & {\small [PARAGRAPH] Ship Simulator is a ship simulator , a type of vehicle simulation computer game which simulates maneuvering various ships in different environments , $\hdots$ [CELL] Founded is 2002 [ROW] Company is VSTEP ; Location is Rotterdam ; Founded is 2002 ; Type is serious games ;  VSTEP: Ship Simulator is a ship simulator , a type of vehicle simulation computer game which simulates maneuvering various ships in different environments , although without the effects of wind and current. It was developed by Dutch company VSTEP and released by the former company Lighthouse Interactive , which closed down in 2009} & {\small VSTEP is a Dutch company founded in 2002 that developed the Ship Simulator, a vehicle simulation computer game that simulates maneuvering various ships in different environments, although without the effects of wind and current. The company was originally released by former company Lighthouse Interactive, which closed in 2009.}
\multirow{3}{4.6cm}{
          Q1: Can you show me a list of teams from that season.
          R1: Here's a list of teams and the stadiums they play in.
          Q2: Thanks. Which stadium has the lowest capacity?
          R2: That would be Við Margáir, which has a capacity of 1,000. 
          Q3: Which club plays there? 
          \textcolor{OliveGreen}{R3:EB/Streymur plays there.}} & K1: [CELL] Stadium is Við Margáir & \textsc{cTBL} NoK: B68 Toftir plays their home matches at Við Margáir stadium. \\
          & K2: [ROW] Team is GÍ Gøta,  City is Norðragøta, 	Stadium is Sarpugerði, Capacity is 2000 & \textsc{cTBL} Top-1: Við Margáir is the home stadium of B36 Tórshavn.   \\
          &  K3: [CELL] Team is EB/Streymur & \textsc{cTBL} Top-3: GÍ Gøta plays at the Serpugerdi Stadium which has a capacity of 2,000. EB/Streymur plays at the Við Margáir stadium which has the lowest capacity of 1,000. \\
\hline 

\multirow{3}{4.6cm}{
Q1: Tell me about publicly traded companies? R1: Publicly traded companies having the greatest market capitalization. This list is primarily based on the Financial Times Global 500. 
Q2: How many Publicly traded companies in 2002? 
R2: There are 10 Publicly traded companies in 2002. 
Q3: How many Oil and gas industry? 
\textcolor{OliveGreen}{R3: There are 3 Oil and Gas Industry}} & K1: [ROW] Rank is 3 ; Name is Exxon Mobil ; Headquarters is United States ; Primary industry is Oil and gas ; USD is 241,037 & \textsc{cTBL} NoK: There are about 60 publicly traded companies in the oil and gas industry, with BP being one of the largest in the UK. \\ 
& K2: [ROW] Rank is 8 ; Name is Royal Dutch Shell ; Headquarters is The Netherlands ; Primary industry is Oil and gas ; USD is 149,034 ; & \textsc{cTBL} Top-1: In 2002, there were 10 publicly traded companies, with BP from the UK being one of them. \\ 
& K3:  [PARAGRAPH] BP plc ( formerly The British Petroleum Company plc and BP Amoco plc ) is a multinational oil and gas company headquartered in London , England & \textsc{cTBL} Top-3: There were 10 publicly traded companies in 2002. \\
    \end{tabular}
    \caption{Analysis of responses by \textsc{cTBL} NoK, Top-1, and Top-3. Ground Truth response is in \textcolor{OliveGreen}{Green} }
    \label{tab:Response_comparison}
\end{table*}

\section{Conclusion}
\label{sec:Conclusion}
In this paper we introduce Conversational Tables (\textsc{cTBL}), a system addressing multi-turn dialogue grounded in tabular information. \textsc{cTBL} splits tabular dialogue into three tasks, table retrieval, system state tracking, and response generation. \textsc{cTBL}'s dense table retrieval system yields up to 5\% relative improvement in Top-1 Accuracy and Mean Reciprocal Rank $@$ 10 over keyword-matching based techniques on the \textsc{HybriDialogue} dataset. Additionally, \textsc{cTBL} performs system state tracking in a two-step process shared between encoder and decoder models. This approach results in natural language responses with up to 46\% relative improvement in ROUGE scores and is preferred by human annotators to prior work. 

\begin{comment}
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
         & Preference Fraction \\
         No-Knowledge &  0.64\\
         Top-1 Knowledge & 0.49 \\
         Top-3 Knowledge & 0.55 \\
    \end{tabular}
    \caption{Fraction of the cases where the response from the model is in the set of preferred responses. Crowdworkers can prefer responses from multiple models }
    \label{tab:Mturk_Eval}
\end{table}
\end{comment}


% Entries for the entire Anthology, followed by custom entries

\clearpage
 \bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is a section in the appendix.

\end{document}
