% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{ACL2023}
\usepackage[]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{enumerate}
% \usepackage{algorithmic}
\usepackage{graphicx}
% \usepackage{textcomp}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}

% \usepackage{natbib}
\usepackage{booktabs}
\usepackage{multirow,bigstrut}
\usepackage{tabu}
\usepackage{multirow, multicol}
\usepackage{adjustbox}
\usepackage{float}
\usepackage{placeins}
\usepackage{caption}
\usepackage{verbatim}
\usepackage{multirow, makecell}
\usepackage{tabularx}
    \newcolumntype{L}{>{\raggedright\arraybackslash}X}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{cTBLS: Augmenting Large Language Models with Conversational Tables}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Anirudh S Sundar, Larry Heck \\
  AI Virtual Assistant (AVA) Lab \\ The Georgia Institute of Technology\\
  \texttt{ \{asundar34,larryheck\}@gatech.edu} \\}

\begin{document}
\maketitle
\begin{abstract}

Optimizing accuracy and performance while eliminating hallucinations of open-domain conversational large language models (LLMs) is an open research challenge. A particularly promising direction is to augment and ground LLMs with information from structured sources. This paper introduces Conversational 
Tables (\mbox{cTBLS}), a three-step architecture to retrieve and generate dialogue responses grounded on retrieved tabular information. \mbox{cTBLS} uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 125\% relative improvement over the retriever in the previous state-of-the-art system on the \textsc{HyrbiDialogue} dataset. \mbox{cTBLS} then uses a shared process between encoder and decoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking combined with a GPT-3.5 LLM response generator to yield a 2x relative improvement in ROUGE scores. Finally, human evaluators prefer cTBLs +80\% of the time (coherency, fluency) and judge informativeness to be 4x better than the previous state-of-the-art. 

\end{abstract}
%\vspace*{.1in}
\section{Introduction}
Equipping conversational AI with multimodal capabilities broadens the range of dialogues that humans have with such systems. A persisting challenge in multimodal conversational AI is the development of systems that produce conversationally coherent responses grounded in textual and non-textual modalities \cite{sundar-heck-2022-multimodal}.

It is well-established that large language models (LLMs) possess real-world knowledge stored within their parameters, as demonstrated by recent research \cite{roberts-etal-2020-much, heinzerling-inui-2021-language}. Nevertheless, the incorporation of conversation-specific extrinsic knowledge into these models to yield precise responses remains an active area of investigation.  While humans can easily retrieve contextual information from tables by examining rows and columns, LLMs often struggle to identify relevant information amidst conversational distractions. 

\textsc{HybriDialogue} \cite{nakamura-etal-2022-hybridialogue}, a dataset of conversations grounded on structured and unstructured knowledge from tables and text, introduces the task of responding to messages by utilizing information from external knowledge and prior dialogue turns. The authors also  present an approach and experimental results on \textsc{HybriDialogue} that represents the current state-of-the-art (SoTA).

This paper proposes an extension to the SoTA approach of \textsc{HybriDialogue} in the form of Conversational Tables (\mbox{cTBLS}) \footnote{Our code will be available at \url{https://github.com/avalab-gt/cTBLS}}, a novel three-step encoder-decoder architecture designed to augment LLMs with tabular data in conversational settings.
% To tackle this problem, we present Table Informed Dialogue Agent (\textsc{\mbox{cTBLS}}), a three-step approach to retrieve and generate dialogue responses using information available in conversation history and external tabular sources. 
In the first step, \mbox{cTBLS} uses a dual-encoder Transformer-based  \cite{vaswani2017attention} Dense Table Retriever (DTR) to retrieve the correct table from the entire corpus based on the user's query. The second step employs a fine-tuned dual-encoder Transformer to track system state and rank cells in the retrieved table according to their relevance to the conversation. Finally, \mbox{cTBLS} utilizes GPT-3.5 to generate a natural language response by prompting it with the ranked cells. 
%Figure \ref{fig:system_overview} provides an overview of the proposed architecture. 

While previous research separated knowledge retrieval and response generation between encoder and decoder models, this paper demonstrates that LLM decoders can perform these tasks jointly when prompted with knowledge sources ranked by language model encoders. Furthermore, by pre-training the Dense Table Retriever to perform retrieval over a corpus of tables, \mbox{cTBLS} can be extended to new knowledge sources without re-training, by appending additional knowledge to the corpus.
%\mbox{cTBLS} employs a Transformer encoder to rank knowledge sources conditioned on the input query, existing dialogue context, and retrieved table. Following the ranking process, \mbox{cTBLS} uses a Transformer decoder to generate a response. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Images/cTBL_Ani_hires.png}
    \caption{\mbox{cTBLS} for conversations on \textsc{HybriDialogue}. Dense Table Retrieval identifies the table most relevant to the initial query. The retrieved table is provided to the state tracker for follow-up queries. State Tracking ranks cells in the table based on their ability to answer a follow-up query. Response Generation utilizes a LLM Decoder provided with the ranked cell information and the follow-up query to convert tabular data into a natural language response and continue the conversation. Details on individual components are provided in Section \ref{sec:Method}.}
    \label{fig:system_overview}
\end{figure*}

Compared to the previous SoTA, experiments on \mbox{cTBLS} show up to 125\% relative improvement in table retrieval and a 2x relative improvement in ROUGE scores. In addition, human evaluators prefer cTBLs +80\% of the time (coherency, fluency) and judge informativeness to be 4x better than the previous SoTA. 


% \bigbreak 
% \newpage




Our contributions are as follows:

 \begin{enumerate}
    \item The introduction of Conversational Tables (\mbox{cTBLS}), a  novel three-step encoder-decoder architecture designed to augment LLMs with tabular data in conversational settings.
     \item Experimental results demonstrating that Dense Table Retrieval, which utilizes neural models fine-tuned with a summary of tabular information, outperforms sparse techniques based on keyword matching for table retrieval.
     \item The presentation of evidence that augmenting state-of-the-art LLM decoders using knowledge sources ranked by encoder language models leads to better results on automatic (ROUGE-Precision) and human (Coherence, Fluency, and Informativeness) evaluation for knowledge-grounded response generation while limiting the number of API calls to these models. 
 \end{enumerate}

% Outline of the paper $\hdots$
This paper presents the \mbox{cTBLS} system and demonstrates its application to the \textsc{HybriDialogue} dataset. In Section \ref{sec:related_work}, we review the existing literature in the fields of Table Question Answering and Knowledge Grounded Response Generation. Section \ref{sec:Method} describes the various components of \mbox{cTBLS} as presented in Figure \ref{fig:system_overview}. In Section~\ref{sec:Experiments}, we evaluate the performance of \mbox{cTBLS} against previous methods for conversations over tables and report experimental results from automatic and human evaluations. Finally, Section \ref{sec:Conclusion} concludes the paper and outlines potential directions for future research.


\section{Related Work}
\label{sec:related_work}
\subsection{Table Question Answering}
Table Question Answering is a well-researched precursor to conversations over tables.  In \textsc{WikiTableQuestions}, \citet{pasupat-liang-2015-compositional} transform HTML tables into a knowledge graph and retrieve the correct answer by converting natural language questions into graph queries. FRETS \cite{jauhar-etal-2016-tables} uses a log-linear model conditioned on alignment scores between cells in tables and individual QA pairs in the training set. \citet{cho2018adversarial} introduce \textsc{NeOp}, a multi-layer sequential network with attention supervision to answer queries conditioned on tables. \citet{hannan2020manymodalqa} propose \textsc{ManyModalQA}, which uses a modality selection network and pre-trained text-based QA, Table-based QA, and Image-based QA models to jointly answer questions over text, tables, and images. \citet{chen-etal-2020-hybridqa} present \textsc{Hybrider}, which performs multi-hop QA over tables using keyword-matching for cell linking followed by BERT \cite{devlin-etal-2019-bert} for reasoning. \citet{chen2020open} propose OTT-QA, which uses a fusion retriever to identify relevant tables and text and a cross-block reader based on a long-range Sparse Attention Transformer \cite{ainslie-etal-2020-etc} to choose the correct answer. \citet{heck2020zero} perform multi-task fine-tuning of Transformer encoders by modeling slot filling as question answering over tabular and visual information in Visual Slot. \citet{herzig-etal-2020-tapas} and \citet{yin-etal-2020-tabert} extend BERT for Table Question Answering by pre-training a masked language model over text-table pairs in \textsc{TaPaS} and TaBERT, respectively. Recent work building off the Transformer architecture for Table Question Answering includes \cite{eisenschlos2021mate, li-etal-2021-dual, herzig-etal-2021-open, zayats-etal-2021-representations, zhao-etal-2022-multihiertt, huang2022mixed, yang2022tableformer,chen2022large}. \citet{jin2022surveytable} provide a comprehensive survey of advancements in Table Question Answering.

\subsection{Knowledge Grounded Response Generation}
Early work related to grounding responses generated by language models in real-world knowledge was motivated by the need to improve prior information for open-domain dialogue \cite{heck2013multimodal, Tur2014eyegaze, hakkani-tr2014probabilistic, huang2015leveraging, jia2017learning}.  More recently,  knowledge grounded response generation has been applied to mitigate the hallucination problem \cite{maynez-etal-2020-faithfulness, shuster-etal-2021-retrieval-augmentation} in LLMs. RAG \cite{lewis2020retrieval} fine-tunes LLMs using Dense Passage Retrieval \cite{karpukhin-etal-2020-dense} over a Wikipedia dump to ground responses for Open Domain Question Answering. KGPT \cite{chen2020kgpt} and SKILL \cite{moiseev-etal-2022-skill} pre-train a Transformer encoder \cite{vaswani2017attention} with English Wikidump for Natural Language Generation. Fusion-in-Decoder \cite{izacard-grave-2021-leveraging}  fine-tunes decoder models using evidence acquired through Dense Passage Retrieval.   

Recent research also includes a dual-stage approach where LLMs generate knowledge sources based on prompts \cite{yu2022generate, bonifacio2022inpars, jeronymo2023inpars}. Closest to our work, Wizard of Wikipedia \cite{dinan2018wizard} jointly optimizes an encoder-decoder Transformer to produce dialogue responses conditioned on retrieved knowledge and dialogue context but does not extend their approach to the multiple modalities. \textsc{RePlug} \cite{shi2023replug} ensembles output responses generated by prompting large language models with inputs from a dense retriever in a zero-shot setting. However, this requires multiple API calls to state-of-the-art LLMs. \textsc{LLM-Augmenter} \cite{peng2023check} incorporates external knowledge in LLM responses by matching keywords in dialogue state to candidate knowledge sources obtained through web-search.  A survey of knowledge fusion in LLMs is available in \citet{colon2021combining} and \citet{richardson2023commonsense}.

In contrast to prior research that focuses on either Table Question Answering or Knowledge Grounded Response Generation, our work, \mbox{cTBLS}, addresses the challenge of generating responses grounded on tabular knowledge. Moreover, while \mbox{cTBLS} is fine-tuned to retrieve tables and filter out incorrect references, it leverages the power of SoTA pre-trained LLMs for response generation. Furthermore, by fine-tuning open-source table and knowledge retrievers to remove inaccurate references, \mbox{cTBLS} reduces the number of API calls to the SoTA LLMs.  

% LLMs store knowledge in their weights \cite{ heinzerling2020language}
% External knowledge \cite{bi-etal-2019-incorporating}

% Dialogue \cite{reddy-etal-2019-coqa}

 


\section{Method}
\label{sec:Method}
The challenge of developing conversational systems grounded in tabular information consists of three tasks, namely table retrieval, system state tracking, and response generation. Table retrieval requires identifying the most relevant table in the dataset based on a given natural language query. System state tracking is responsible for ranking the cells in the table, enabling the system to provide responses to follow-up queries about the table. Finally, response generation involves converting the ranked cells into a natural language response.

\subsection{Table Retrieval}
\label{subsec:tab_ret}
% The first step in responding to queries related to tabular information requires selecting the correct table from a large corpus. 
Table retrieval is a prerequisite to answering queries when the exact table to converse over is unspecified. The objective is to identify the correct table from a vast corpus. 
\mbox{cTBLS} proposes formulating table retrieval as document retrieval by assigning a relevance score to each table based on its relevance to the natural language query.  Inspired by \citet{karpukhin-etal-2020-dense} and \citet{huang2013learning}, \mbox{cTBLS} uses a dual-encoder-based Dense Table Retrieval (DTR) model. The DTR model pre-computes a vectorized embedding of all tables in the corpus. Given a query at inference, the retrieved table is closest to the query in the embedded space, indicated by the upper-left portion of Figure \ref{fig:system_overview}. 

The DTR model consists of a table encoder and a question encoder, initialized from RoBERTa-base \cite{liu2019roberta}. The input to the table encoder comprises the table's title and, if available, textual information associated with the table. 
Figure \ref{fig:WNBA} presents an example of table-associated text in the context of Wikipedia, where introductions from the page and section provide additional grounding. The input to the question encoder is the current query to be answered. 
Taking the average over the sequence of the last hidden state at the table and question encoder results in 768-dimensional embeddings of the table information and the query. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{Images/WNBA_Finals_2.png}
    \caption{An example of table-associated text in the context of Wikipedia, where the input to the DTR text-encoder includes the page title, the introduction to the article, the section title, and the introduction paragraph.}
    \label{fig:WNBA}
\end{figure}

The DTR model is optimized through a contrastive prediction task, which aims to maximize the similarity between embeddings of a given query~$q$ and the table to be retrieved $\tau$ while minimizing the similarity to other incorrect tables $\tau_{{n_i}}$ for $i=1,\ldots,N$. As per \cite{karpukhin-etal-2020-dense}, normalized embedding vectors are utilized to optimize the objective in Equation \ref{eq:table_ret}:
\begin{equation}
\small{
\arg\min\limits_
{
\tau\;\;\;\;\;\;\;\;
} 
\left(
-\log 
\frac{
%e^{{q'}\tau}
e^{{q}\cdot \tau}
}
{
% e^{{q'}\tau}
e^{{q}\cdot \tau}
\;+\;
% \sum_{i=1}^N e^{{q'}\tau_{n_i}}
\sum_{i=1}^N e^{{q}\cdot \tau_{n_i}}
}
\right)
 \label{eq:table_ret}
 }
\end{equation}
%\begin{equation}
%\begin{split}
%    % \min L(q,p,n_1,\hdots, n_N) %=\\ \min \bigg{(} -\log %\frac{e^{q^Tp}}{e^{q^Tp} + %\sum_{i=1}^Ne^{q^Tn_i}} \bigg{)}
%    \max (\text{sim}(q,t)) \cup \min (\text{sim}(q,\{n_i\}))  =\\ \min \bigg{(} -\log \frac{e^{q^Tt}}{e^{q^Tt} + \sum_{i=1}^Ne^{q^Tn_i}} \bigg{)}
%    \label{eq:table_ret}
%\end{split}
%\end{equation}

Given a batch $B$ of $d$-dimensional query embeddings $\mathbf{Q}$ and table embeddings $\mathbf{T}$, the DTR model computes the similarity $\mathbf{Q}\mathbf{T}^T ( \in \mathbb{R}^{B \times B} )$ between every query and table in the batch. This similarity computation enables the sampling of negatives from other query-table pairs, resulting in $B^2$ training samples in each batch, consisting of $B$ positive pairs along the diagonal and $B^2-B$ negatives.

\subsection{Coarse System State Tracking}
Given a table, system state tracking involves ranking cells in the table by their relevance to conversational queries. In contrast to quesiton-answering, conversational queries require leveraging information from external modalities in conjunction with prior dialogue turns to generate coherent responses \cite{sundar-heck-2022-multimodal}. \mbox{cTBLS} addresses system state tracking through two sub-tasks - coarse and fine system state tracking. Coarse system state tracking ranks cells in the table, while fine system state tracking identifies fine-grained information in the most relevant cell to answer the query. 

\mbox{cTBLS} uses a RoBERTa-base dual-encoder architecture for coarse system state tracking. The cell encoder embeds all cells and associated hyperlinked information, and the question encoder generates embeddings for the dialogue history ($\mathbf{D}_\mathbf{h}$) that includes the current turn's query as well as previous queries and responses.

To rank cells based on their relevance to the follow-up query, as illustrated in the upper-right section of Figure \ref{fig:system_overview}, the question and cell encoders are optimized using a triplet loss configuration. This optimization aims to minimize the distance between the anchor $\mathbf{D}_\mathbf{h}$ and the positive cell $c$, while pushing the negative cell $\overline{c}$ further away from $\mathbf{D}_\mathbf{h}$ by a margin $m$ (Equation~\ref{eq:triplet_loss}).
\begin{equation}
\small{
\arg\min\limits_
{
c_i\;\;\;\;\;\;\;\;\;
} 
(\max
\{
d({\bf D_h},c) - d({\bf D_h},\overline{c}) + m,0
\})
\label{eq:triplet_loss}
}
\end{equation}
%\begin{equation}
%\begin{split}
%    % \min L(a,c, &n) =  \\ \min ( %\max\{d(a_i,p_i) - d(a_i, & n_i) + %m, 0 \} ) 
%    \min ( \max\{d(\mathbf{D}_\mathbf{h},c) - d(\mathbf{D}_\mathbf{h},  n) + m, 0 \} ) 
%    \label{eq:triplet_loss}
%\end{split}
%\end{equation}
\begin{equation}
\small{
    d(x,y) = ||x - y||_2
    }
\end{equation}

For our approach, we utilize an anchor-positive-negative triplet consisting of the complete dialogue history (including queries and responses from previous turns) concatenated with the current query as the anchor, the correct cell as the positive, and other cells from the same table that are not relevant to the query as negatives. We measure the distance between the anchor and the positive and between the anchor and the negatives using the 2-norm distance function $d(\cdot)$.

\subsection{Fine System State Tracking and Response Generation}
In contrast to coarse system state tracking, fine system state tracking involves identifying the exact phrase that answers the query from a ranked subset. The extracted phrase is converted into a natural language response that is coherent within the context of the conversation.

\mbox{cTBLS} employs GPT-3.5 \cite{brown2020language} to perform fine system state tracking and response generation jointly. GPT-3.5 is prompted to generate a natural language response to a follow-up query conditioned on cells of the table ranked by their relevance to the query as obtained from the coarse state tracker. The prompt includes the dialogue history, ranked knowledge sources, and the query to be answered. The bottom-right section of Figure~\ref{fig:system_overview} outlines this process. 

\begin{comment}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/WNBA_Finals_edit.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}
\end{comment}

\section{Experiments}
\label{sec:Experiments}

\subsection{\textsc{HybriDialogue}}
The \textsc{HybriDialogue} dataset \cite{nakamura-etal-2022-hybridialogue} comprises 4800 natural language conversations grounded in text and tabular information from Wikipedia. Crowdsourced workers break down multi-hop questions from the OTT-QA dataset \cite{chen2020open} into natural questions and conversational responses related to tabular data. On average, dialogues in the dataset consist of 4-5 conversation turns, with a total of 21,070 turns available in the dataset. Examples of conversations can be found in Figures \ref{fig:dial_1_example} and \ref{fig:dial_2_example}.

\begin{comment}    
\begin{table}[t]
    \centering
    \begin{tabular}{c|c}
         \hline 
         Dataset Statistics &  \\
         \hline 
         # Training conversations & 4359 \\
         # Development conversations & 242 \\ 
         # Testing conversations & 243 \\
         Turns per conversation & 4.34  \\ 
         \hline 
    \end{tabular}
    \caption{A summary of the \textsc{HybriDialogue}dataset \cite{nakamura-etal-2022-hybridialogue}}
    \label{tab:hybridialogue_summary}
\end{table}
\end{comment}
\subsection{Table Retrieval}

The first conversation turn of \textsc{HybriDialogue} requires selecting the correct table based on the input query for which we use the Dense Table Retriever outlined in Section~\ref{subsec:tab_ret}.  The Dense Table Retriever is fine-tuned for 20 epochs using Adam \cite{kingma2014adam} with a learning rate of 1e-6 and a linear learning schedule with five warmup steps. The loss function is a modification of the contrastive loss implementation from ConVIRT \cite{zhang2022contrastive}, with image embeddings replaced by table embeddings. The table retriever used in the \textsc{HybriDialogue} paper \cite{nakamura-etal-2022-hybridialogue} was the BM25Okapi Retriever \cite{trotman2014improvements} from \href{https://github.com/dorianbrown/rank_bm25}{rank-bm25}. According to the results presented in Table \ref{tab:retrieval}, \mbox{cTBLS}-DTR outperforms BM25 in terms of Mean Reciprocal Rank (MRR), Top-1 Accuracy, and Top-3 Accuracy on \textsc{HybriDialogue}.
\begin{table}[t]
    \centering
    % \begin{tabular}{c|c|c|c}
    \begin{tabularx}{\columnwidth} { 
   >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X  }
    \toprule 
          &  MRR $@$10 & Top 1 Acc & Top 3 Acc  \\
          \hline 
         {\small BM25} & 0.491 & 0.345 &  0.460 \\
         {\small cTBLS-}{\tiny DTR} & {\textbf{0.846}} & {\textbf{0.777}} & {\textbf{0.901}}\\
        \bottomrule 
    \end{tabularx}
    \caption{BM25 vs \mbox{cTBLS}-DTR for retrieval on first turn of conversation, results on \textsc{HybriDialogue} testing dataset. \mbox{cTBLS}-DTR obtains up to 125\% relative improvement over sparse table retrieval}
    \label{tab:retrieval}
\end{table}


\subsection{Coarse State Tracking}
Coarse state tracking ranks cells from a table based on their relevance to a query. As before, the dual-encoder coarse state tracker of \mbox{cTBLS} consists of RoBERTa-base fine-tuned using Adam with a learning rate of 1e-6 and a linear learning schedule with five warmup steps. In contrast to table retrieval, the state tracker uses triplet margin loss with a margin of 1.0~(Equation \ref{eq:triplet_loss}) instead of contrastive loss~(Equation \ref{eq:table_ret}). The results, as demonstrated in Table~\ref{tab:state_tracking}, show that fine-tuning RoBERTa-base solely on \textsc{HybriDialogue} surpasses the performance of SentenceBERT \cite{reimers-gurevych-2019-sentence}. Furthermore, it nearly attains the same MRR~$@10$ as TaPas \cite{herzig-etal-2020-tapas}, even without additional table pre-training on the SQA dataset \cite{iyyer-etal-2017-search}.
% In addition, Table \ref{tab:dst_topk} reports the Top-1, Top-3, and Top-10 accuracy of \mbox{cTBLS}. The correct cell is retrieved in approximately 56\% of conversations at inference and ranks among the Top-3 and Top-10 retrievals about 78\% and 93\% of the time, respectively. 

\begin{table}[t]
    \centering
    % \resizebox{\columnwidth}{!}{%
    \begin{tabular}{c|c}
    \toprule  
          & MRR$@$10 \\
    \hline 
         SentenceBERT \tiny{\cite{reimers-gurevych-2019-sentence}} & 0.603  \\
         TaPas \tiny{\cite{herzig-etal-2020-tapas}}  & \textbf{0.689} \\
         \mbox{cTBLS} - RoBERTa-base & 0.683 \\
    \bottomrule 
    \end{tabular}
    % }
    \caption{System state tracking results on \textsc{HybriDialogue}. \mbox{cTBLS} achieves nearly the same Mean Reciprocal Rank (MRR) $@$ 10 as TaPaS, without additional table pre-training on SQA \cite{iyyer-etal-2017-search}}
    \label{tab:state_tracking}
\end{table}

\begin{table}[b]
    \centering
    \begin{tabular}{c|c|c|c}
    \toprule 
             & Top-1 & Top-3 & Top-10 \\
             \hline 
         \small{\mbox{cTBLS} - RoBERTa-base}  & 0.559 & 0.778 & 0.925 \\
    \bottomrule  
    \end{tabular}
    \caption{Top-k accuracy for \mbox{cTBLS} on coarse system state tracking. \mbox{cTBLS} ranks the correct cell as the top reference in 56\% of follow-up queries on \textsc{HybriDialogue}. The correct cell is ranked in the Top-3 and Top-10 retrievals in approximately 78\% and 93\% of conversations, respectively.}
    \label{tab:dst_topk}
\end{table}

\subsection{Fine State Tracking and Response Generation}
\mbox{cTBLS} uses GPT-3.5 (text-davinci-003) with the existing dialogue context, the current query, and the retrieved references from coarse state tracking to obtain a natural language response. Since fine-tuning the best available version of the model is cost prohibitive, we opt to prompt GPT-3.5 to generate responses instead. 

\begin{comment}
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|c|c|c}
         \hline 
         & \mbox{cTBLS} NoK & \mbox{cTBLS} Top-1 & \mbox{cTBLS} Top-3 &\textsc{HybriDialogue} \\
         \hline 
         ROUGE-1 Precision & 0.487 & 0.603 & \textbf{0.642} & 0.438\\ 
         ROUGE-2 Precision & 0.229 & 0.304 &\textbf{0.322} & 0.212 \\   
         ROUGE-L Precision & 0.422 & 0.517 & \textbf{0.548} & 0.375\\
         \hline
    \end{tabular}
    \caption{Automatic Evaluation Metrics - \mbox{cTBLS} No Knowledge (NoK), Top-1 Knowledge, Top-3 Knowledge, and DialoGPT}
    \label{tab:Automatic eval}
\end{table*}
\end{comment}


\begin{table*}[t]
    \centering 
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule 
         Model & TR & KR & RG &  ROUGE-1 & ROUGE-2 & ROUGE-L \\
         \hline 
         - & BM25 & Top-1 & DialoGPT & 0.207 & 0.042 & 0.181 \\
         - & BM25 & Top-3 & DialoGPT & 0.212 & 0.045 & 0.186 \\
         - & BM25 & Top-1 & GPT3.5 & 0.428 & 0.207 & 0.369 \\ 
         - & BM25 & Top-3 & GPT3.5 & 0.475 & 0.242 & 0.413 \\ 
         \midrule 
         - & DTR & Top-1 & DialoGPT & 0.222 & 0.051 & 0.195 \\
         - & DTR & Top-3 & DialoGPT & 0.226 & 0.059 & 0.199 \\
         - & DTR & Top-1 & GPT3.5 &  0.494 & 0.255 & 0.424 \\ 
         - & DTR & Top-3 & GPT3.5
         &  0.560 & 0.295 & 0.479 \\ 
         \midrule 
         \textsc{HybriDialogue} & Gold & Top-1 & DialoGPT & 0.438 & 0.212 & 0.375 \\
         \mbox{cTBLS} NoK & Gold & - & GPT3.5 & 0.487 & 0.229 & 0.422 \\ 
         \mbox{cTBLS} Top-1 & Gold & Top-1 & GPT3.5 & 0.603 & 0.304 & 0.517 \\ 
         \mbox{cTBLS} Top-3 & Gold & Top-3 & GPT3.5 & \textbf{0.642} & \textbf{0.322} & \textbf{0.548} \\ 
         % \multicolumn{3}{c|}{\textsc{HybriDialogue}} & 0.438 & 0.212 & 0.375 \\
         % \multicolumn{3}{c|}{\mbox{cTBLS} NoK} & 0.487 & 0.229 & 0.422 \\
         % \multicolumn{3}{c|}{\mbox{cTBLS} Top-1} & 0.603 & 0.304 & 0.517 \\ 
        % \multicolumn{3}{c|}{\mbox{cTBLS} Top-3}& \textbf{0.642} & \textbf{0.322} & \textbf{0.548} \\
    \bottomrule
    \end{tabular}
        \caption{Ablation study on automatic evaluation metrics ROUGE-1, ROUGE-2, and ROUGE-L Precision.    Using Dense Table Retrieval (DTR) improves results over BM25 across Top-1 and Top-3 knowledge for DialoGPT and GPT3.5. 
        Furthermore, using Top-3 knowledge sources results in better results than using only Top-1 knowledge sources for DialoGPT and GPT3.5 using both table retrieval methods.  
        \mbox{cTBLS} No Knowledge (NoK), Top-1 Knowledge, Top-3 Knowledge, and \textsc{HybriDialogue} use ground truth table retrieval. \mbox{cTBLS} exhibits a 2x relative improvement in ROUGE Precision over \textsc{HybriDialogue}. 
        \mbox{TR: Table Retrieval}, \mbox{KR: Knowledge Retrieval}, \mbox{RG: Response Generation}}
    \label{tab:Automatic eval}
\end{table*}

The results presented in Table \ref{tab:dst_topk} demonstrate that the coarse state tracker successfully retrieves the correct cell in approximately 56\% of conversations during inference. Furthermore, it achieves Top-3 and Top-10 retrievals in approximately 78\% and 93\% of conversations, respectively.
Motivated by these results, the fine state tracker of \mbox{cTBLS} is evaluated in two different configurations by prompting GPT-3.5 augmented with the \mbox{Top-1} and Top-3 knowledge references (\mbox{cTBLS} Top-1 and \mbox{cTBLS Top-3}). Due to limits on token length associated with the OpenAI API, we remove stopwords from the knowledge provided in the prompt and do not experiment with Top-10 knowledge augmentation. 

Since LLMs store factual information in their weights \cite{roberts-etal-2020-much, heinzerling-inui-2021-language}, we compare to few-shot prompting (using two examples) with no knowledge sources (\mbox{cTBLS}-NoK).  Furthermore, to enable a meaningful comparison with existing research \cite{nakamura-etal-2022-hybridialogue}, we measure \mbox{cTBLS} against the system proposed by \textsc{HybriDialogue} that utilizes a fine-tuned DialoGPT-medium \cite{zhang2019dialogpt} model augmented with Top-1 knowledge.

Table \ref{tab:Automatic eval} presents ROUGE-1, ROUGE-2, and ROUGE-L precision \cite{lin-2004-rouge} for all models assessed. The results demonstrate that superior downstream performance can be achieved through improvements in table retrieval. Specifically, when keeping the number of knowledge sources constant, we observe an improvement in ROUGE precision scores when transitioning from BM25 to DTR, and from DTR to gold table retrieval. The inclusion of additional knowledge sources leads to an improved n-gram overlap with the ground truth reference, as evidenced by the Top-3 knowledge augmented models outperforming their Top-1 counterparts utilizing the same table retriever, and \mbox{cTBLS Top-1} outperforming the baseline model \mbox{cTBLS} NoK. Moreover, \mbox{cTBLS} Top-3 achieves the best performance across all automatic metrics, suggesting the benefits of splitting knowledge retrieval into coarse and fine state tracking, and utilizing additional knowledge sources. Finally, all three configurations of \mbox{cTBLS} demonstrate superior performance to \textsc{HybriDialogue}.

\begin{comment}
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
         Method & SacreBLEU  \\
         DialoGPT & 21.63 \\
         \textbf{Ours - Fine tuned GODEL} & \textbf{24.66 } 
    \end{tabular}
    \caption{Dialogue Generation Results}
    \label{tab:dialogue_generation}
\end{table}
\end{comment}



\subsection{Human Evaluation}
To gain a deeper understanding of \mbox{cTBLS}, we conducted human evaluation using the metrics outlined by \citet{nakamura-etal-2022-hybridialogue}, namely Coherence, Fluency, and Informativeness. For the evaluation of these metrics, we enlisted crowd workers from Amazon Mechanical Turk (AMT) to assess 50\% of the test data. The evaluation process involved a comparison between the responses generated by \textsc{HybriDialogue} and \mbox{cTBLS} Top-3.

In accordance with the methodology delineated in \citet{nakamura-etal-2022-hybridialogue}, Coherence was defined as the degree to which a response continued the conversation in a logically coherent manner based on prior context. Fluency, conversely, was determined by evaluating absence of grammatical and spelling errors, and appropriate use of parts of speech.

To ensure the quality of the evaluated responses, we engaged crowd workers possessing a Masters qualification on AMT and originating from English-speaking countries (USA, Canada, Australia, New Zealand, or Great Britain). Each task required approximately 30 seconds to complete, and workers were remunerated at a rate of \$0.05 per task. Moreover, to minimize bias and guarantee the dependability of the evaluations, we assigned two crowd workers to assess each response, with a response deemed more coherent or fluent only if both evaluations concurred.

The results presented in Table \ref{tab:coherence_fluency} reveal that the responses generated by \mbox{cTBLS} Top-3 were more coherent than those produced by \textsc{HybriDialogue} in 84.2\% of cases and exhibited greater fluency 82.7\% of the time, suggesting that improvements in table retrieval, knowledge retrieval, and response generation lead to better downstream performance.   

\begin{table}[t]
    \centering
    \begin{tabular}{c|c}
     \toprule
     & \mbox{cTBLS} Top-3 vs \textsc{HybriDialogue} \\
     \hline
     Coherence & 0.842 \\ 
     Fluency & 0.827 \\
     \bottomrule
    \end{tabular}
    \caption{Coherence and Fluency - \mbox{cTBLS} Top-3 is more conversationally coherent than the best performing \textsc{HybriDialogue} system 84.2\% of the time and is more fluent 82.7\% of the time.}
    \label{tab:coherence_fluency}
\end{table}

Informativeness represents the accuracy of machine-generated responses when compared to the ground-truth \cite{nakamura-etal-2022-hybridialogue} and serves as a measure of hallucination in LLMs. Hallucinated responses tend to be less informative, deviating significantly from the ground-truth. 

To evaluate informativeness, crowd workers determined whether generated responses were semantically equivalent to the ground truth response. Each response was assessed by two Turkers, and a response was deemed more informative only if there was inter-annotator agreement. The absence of illustrative examples in the prompting process resulted in responses generated by \mbox{cTBLS} Top-1 and \mbox{cTBLS Top-3} being longer than the ground truth response. Consequently, the knowledge-augmented \mbox{cTBLS} responses were considered informative if all the information provided in the ground truth was encapsulated in the model response, even if \mbox{cTBLS} included supplementary information.

The data in Table \ref{tab:GT_same} indicate that \mbox{cTBLS} Top-3 encompasses the same information as the ground truth response 50\% of the time, a higher rate than \mbox{cTBLS} Top-1 at 45.6\%, exemplifying the benefits of partitioning retrieval into coarse and fine state tracking and augmenting with additional knowledge. Based on these findings, we hypothesize that the attention mechanism in decoder models facilitates additional knowledge retrieval. \mbox{cTBLS NoK} generates the correct response 30.6\% of the time, suggesting that \textsc{HybriDialogue} comprises questions and answers predicated on general world knowledge embedded in the weights of LLMs. Responses produced by \textsc{HybriDialogue} are informative in merely 12.4\% of instances.

\begin{table}[t]
    \centering
    \begin{tabular}{c|c}
    \toprule 
          & Informativeness \\
          \hline 
         \textsc{HybriDialogue} & 0.124 \\
         \mbox{cTBLS} - NoK& 0.306 \\
         \mbox{cTBLS} Top-1 &  0.456 \\
         \textbf{\mbox{cTBLS} Top-3} & \textbf{0.500} \\
    \bottomrule
    \end{tabular}
    \caption{Human Evaluation Metrics - Fraction of cases where model response is semantically equivalent to ground truth response. Using more knowledge sources results in responses that are more informative, helping reduce hallucination.}
    \label{tab:GT_same}
\end{table}


\begin{comment}
    

\begin{table*}[h!]
    \centering 
    \begin{tabular}{p{4.5cm}|p{4.5cm}|p{4.5cm}}
         Context & Knowledge & Response   \\
         \hline
          \multirow{3}{4.6cm}{
          Q1: Can you give me a list of transfer in to the D.C. United season? R1: Here is a list of transfer in to the D.C. United season.  Q2: Who was born on July 7, 1989? R2: Casey Townsend was born July 7,1989.  Q3: What number does Casey Townsend wear? \textcolor{OliveGreen}{R3:Casey Townsend wore the number 16.}} & K1: [CELL] Fee/Notes is 2014 MLS SuperDraft & \mbox{cTBLS} NoK: Casey Townsend wears the number 31 for D.C. United. \\
          & K2: [CELL] No is 16 & \mbox{cTBLS} Top-1: Casey Townsend was drafted in the 2014 MLS SuperDraft and wears the number 9 for D.C. United.   \\
          & K3: [CELL] Date is January
25 , 2013 & \mbox{cTBLS} Top-3: Casey Townsend wears number 16, and he was drafted by D.C. United in the 2014 MLS SuperDraft on January 25, 2013 \\
\hline 
% \multirow{3}{4.5cm}{Q1: Can you tell me more about Netherland video games? R1:Netherland video games market make up 3.95\% of the entire European market.  Q2: Can you give me a list of game developers form the Netherlands? R2: Here is a list of game developers form the Netherlands. Q3:  What company developed a ship simulator? R3:VSTEP developed the ship simulator. Q4: When was VSTEP founded? \textcolor{OliveGreen}{A4: VSTEP was founded in 2002.} } & - & VSTEP was founded in 2004. \\
%& {\small [PARAGRAPH] Ship Simulator is a ship simulator , a type of vehicle simulation computer game which simulates maneuvering various ships in different environments , although without the effects of wind and current . It was developed by Dutch company VSTEP and released by the former company Lighthouse Interactive , which closed down in 2009.}  & {\small VSTEP was founded in 2006 and they developed the Ship Simulator game. The game is a type of vehicle simulation computer game that simulates maneuvering various ships in different environments, although without the effects of wind and current.} \\
% & {\small [PARAGRAPH] Ship Simulator is a ship simulator , a type of vehicle simulation computer game which simulates maneuvering various ships in different environments , $\hdots$ [CELL] Founded is 2002 [ROW] Company is VSTEP ; Location is Rotterdam ; Founded is 2002 ; Type is serious games ;  VSTEP: Ship Simulator is a ship simulator , a type of vehicle simulation computer game which simulates maneuvering various ships in different environments , although without the effects of wind and current. It was developed by Dutch company VSTEP and released by the former company Lighthouse Interactive , which closed down in 2009} & {\small VSTEP is a Dutch company founded in 2002 that developed the Ship Simulator, a vehicle simulation computer game that simulates maneuvering various ships in different environments, although without the effects of wind and current. The company was originally released by former company Lighthouse Interactive, which closed in 2009.}
\multirow{3}{4.6cm}{
          Q1: Can you show me a list of teams from that season.
          R1: Here's a list of teams and the stadiums they play in.
          Q2: Thanks. Which stadium has the lowest capacity?
          R2: That would be Við Margáir, which has a capacity of 1,000. 
          Q3: Which club plays there? 
          \textcolor{OliveGreen}{R3:EB/Streymur plays there.}} & K1: [CELL] Stadium is Við Margáir & \mbox{cTBLS} NoK: B68 Toftir plays their home matches at Við Margáir stadium. \\
          & K2: [ROW] Team is GÍ Gøta,  City is Norðragøta, 	Stadium is Sarpugerði, Capacity is 2000 & \mbox{cTBLS} Top-1: Við Margáir is the home stadium of B36 Tórshavn.   \\
          &  K3: [CELL] Team is EB/Streymur & \mbox{cTBLS} Top-3: GÍ Gøta plays at the Serpugerdi Stadium which has a capacity of 2,000. EB/Streymur plays at the Við Margáir stadium which has the lowest capacity of 1,000. \\
\hline 

\multirow{3}{4.6cm}{
Q1: Tell me about publicly traded companies? R1: Publicly traded companies having the greatest market capitalization. This list is primarily based on the Financial Times Global 500. 
Q2: How many Publicly traded companies in 2002? 
R2: There are 10 Publicly traded companies in 2002. 
Q3: How many Oil and gas industry? 
\textcolor{OliveGreen}{R3: There are 3 Oil and Gas Industry}} & K1: [ROW] Rank is 3 ; Name is Exxon Mobil ; Headquarters is United States ; Primary industry is Oil and gas ; USD is 241,037 & \mbox{cTBLS} NoK: There are about 60 publicly traded companies in the oil and gas industry, with BP being one of the largest in the UK. \\ 
& K2: [ROW] Rank is 8 ; Name is Royal Dutch Shell ; Headquarters is The Netherlands ; Primary industry is Oil and gas ; USD is 149,034 ; & \mbox{cTBLS} Top-1: In 2002, there were 10 publicly traded companies, with BP from the UK being one of them. \\ 
& K3:  [PARAGRAPH] BP plc ( formerly The British Petroleum Company plc and BP Amoco plc ) is a multinational oil and gas company headquartered in London , England & \mbox{cTBLS} Top-3: There were 10 publicly traded companies in 2002. \\
\hline 
    \end{tabular}
    \caption{Analysis of responses by \mbox{cTBLS} NoK, Top-1, and Top-3. \mbox{cTBLS} NoK generates responses conditioned on the Context alone, \mbox{cTBLS} Top-1 generates responses conditioned on the context and K1, and \mbox{cTBLS} Top-3 generates responses conditioned on the Context, K1, K2, and K3. In \textcolor{OliveGreen}{green} is the ground truth response from \textsc{HybriDialogue}.}
    \label{tab:Response_comparison}
\end{table*}
\end{comment}


Figure \ref{fig:dial_1_example} presents a comparison of responses generated by various configurations of \mbox{cTBLS} on the \textsc{HybriDialogue} dataset. The entire dialogue history constitutes the context and is depicted as an exchange between the user (in blue) and the system (in yellow). The final question box represents the follow-up query to be addressed, while the last answer chat box indicates the ground truth response. Knowledge K1, K2, and K3 correspond to cells of the table retrieved during state tracking, based on which responses are produced. \mbox{cTBLS} NoK generates a response solely relying on the context, \mbox{cTBLS} Top-1 formulates a response conditioned on K1, and \mbox{cTBLS} Top-3 devises a response based on K1, K2, and K3.



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Faroe_Islands.png}
    \caption{Generated responses vs Ground Truth on \textsc{HybriDialogue} test set. Questions are in \textcolor{MidnightBlue}{blue} and responses in \textcolor{Dandelion}{yellow}. K1, K2, and K3 represent the Top 3 knowledge sources ranked by relevance to the query "Which team plays there?". \mbox{cTBLS} Top-3 is able to leverage K3 to generate the correct response while \mbox{cTBLS} NoK hallucinates a response and \mbox{cTBLS} Top-1 generates an incorrect response based on K1. Table obtained from Wikipedia \href{https://en.wikipedia.org/wiki/2007_Faroe_Islands_Premier_League}{available here}}
    \label{fig:dial_1_example}
\end{figure*}


\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{Images/Publicly_Traded_Companies.png}
    \caption{Generated responses vs Ground Truth on \textsc{HybriDialogue} test set. Despite selecting the rows of the table corresponding to Oil and gas industries, \mbox{cTBLS} NoK, Top-1, and Top-3 struggle with counting and hallucinate a response. Table obtained from Wikipedia \href{https://en.wikipedia.org/wiki/List_of_public_corporations_by_market_capitalization}{available here}}
    \label{fig:dial_2_example}
\end{figure*}

\mbox{cTBLS} NoK creates a hallucinated response, answering with the random Faroese club B68 Toftir. Similarly, \mbox{cTBLS} Top-1 hallucinates a response, opting for B36 Tórshavn, as K1 refers to the stadium Viò Margáir rather than the correct club's name. In contrast, \mbox{cTBLS} Top-3 produces the accurate response, EB/Streymur, since K3 contains the necessary information. This example demonstrates the benefits of augmenting response generation with additional pertinent knowledge, which aids in mitigating the hallucination problem \cite{maynez-etal-2020-faithfulness}.


\section{Conclusion}
\label{sec:Conclusion}
In this paper, we introduce Conversational Tables (\mbox{cTBLS}), a system designed to address multi-turn dialogues that are grounded in tabular data. \mbox{cTBLS} separates tabular dialogue into three distinct tasks, specifically table retrieval, system state tracking, and response generation. The dense table retrieval system of \mbox{cTBLS} yields an enhancement of up to 125\% relative to keyword-matching based techniques on the \textsc{HybriDialogue} dataset, with regard to Top-1 Accuracy and Mean Reciprocal Rank~$@$ 10. Furthermore, \mbox{cTBLS} conducts system state tracking utilizing a two-step process shared between encoder and decoder models. This methodology results in natural language responses exhibiting a 2x relative improvement in ROUGE scores. Human evaluators favor \mbox{cTBLS}  +80\% of the time (coherency and fluency) and judge informativeness to be 4x better than the previous state-of-the-art.



\section{Limitations}
Although \mbox{cTBLS} enhances LLMs with tabular knowledge to generate grounded responses, certain limitations remain to be addressed.

Firstly, the efficacy of \mbox{cTBLS} is constrained by the total number of knowledge sources employed during the augmentation process. Token length restrictions in the OpenAI API limit the knowledge augmentation to the top three cells of the table. Another limitation is the incapacity of \mbox{cTBLS} to handle queries pertaining to the entire table. Figure~\ref{fig:dial_2_example} demonstrates one such instance in which the state tracker module accurately retrieves three rows of the table corresponding to oil and gas industries, yet the response generation module fails to utilize this information when transforming the retrieved state into a response. Generally, \mbox{cTBLS} encounters difficulties with counting, comparing the values of cells, and other mathematical operations, an issue we aim to address in future research.

%  \newpage 
\begin{comment}
\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
         & Preference Fraction \\
         No-Knowledge &  0.64\\
         Top-1 Knowledge & 0.49 \\
         Top-3 Knowledge & 0.55 \\
    \end{tabular}
    \caption{Fraction of the cases where the response from the model is in the set of preferred responses. Crowdworkers can prefer responses from multiple models }
    \label{tab:Mturk_Eval}
\end{table}
\end{comment}

\section{Acknowledgements}
We would like to thank Christopher Richardson, Benjamin Z Reichman, Atishay Jain, and Srikar Bhumireddy for their contributions. We would also like to thank the review committee for their feedback. 
This work was supported by NSF IIS-2112633 and by CoCoSys, one of seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.

% Entries for the entire Anthology, followed by custom entries

 \bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

%appendix
\begin{comment}
\clearpage 
\section{Appendix}
\label{sec:appendix}

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{Images/Casey_Townsend.png}
    \caption{Caption}
    \label{fig:my_label}
\end{figure*}
\end{comment}

% This is a section in the appendix.

\end{document}
