\section{Experiment}
\label{sec:exp}

\input{tables/dataset}
\input{tables/sota}


\subsection{Datasets}
We evaluate our model on a variety of datasets for four tasks: forgery detection, shadow detection, defocus blur detection, and camouflaged object detection. A summary of the basic information of these datasets is illustrated in Table~\ref{tab:dataset}.


\paragraph{Forgery Detection.} CASIA~\cite{dong2013casia} is a large dataset for forgery detection, which is composed of 5,123 training and 921 testing spliced and copy-moved images. 
IMD20~\cite{novozamsky2020imd2020} is a real-life forgery image dataset that consists of 2, 010 samples for testing.
We follow the protocol of previous works~\cite{liu2022pscc, hao2021transforensics,wang2022objectformer} to conduct the training and evaluation at the resolution of $256 \times 256$. We use pixel-level Area Under the Receiver Operating Characteristic Curve~(AUC) and $F_{1}$ score to evaluate the performance. 

\paragraph{Shadow Detection.} SBU~\cite{sbu} is the largest annotated shadow dataset which contains 4,089 training and 638 testing samples, respectively.
ISTD~\cite{wang2018stacked} contains triple samples for shadow detection and removal, we only use the shadowed image and shadow mask to train our method.
Following \cite{mtmt,zhu2018bidirectional,zhu2021mitigating}, we train and test both datasets with the size of $400 \times 400$. 
As for the evaluation metrics, We report the balance error rate~(BER).

\paragraph{Defocus Blur Detection.} 
Following previous work~\cite{zhao2018defocus, cun2020defocus}, we train the defocus blur detection model in the CUHK dataset~\cite{shi2014discriminative}, which contains a total of 704 partial defocus samples. We train the network on the 604 images split from the CUHK dataset and test in DUT~\cite{zhao2018defocus} and the rest of the CUHK dataset. 
The images are resized into $320 \times 320$, following~\cite{cun2020defocus}. We report performances with commonly used metrics: F-measure~($F_{\beta}$) and mean absolute error~(MAE). 



\paragraph{Camouflaged Object Detection.} 
COD10K~\cite{fan2020camouflaged} is the largest dataset for camouflaged object detection, which contains 3,040 training and 2,026 testing samples. CHAMELEON~\cite{skurowski2018animal} includes 76 images collected from the Internet for testing. CAMO~\cite{le2019anabranch} provides diverse images with naturally camouflaged objects and artificially camouflaged objects. Following~\cite{fan2020camouflaged,mei2021camouflaged}, we train on the combined dataset and test on the three datasets. We employ commonly used metrics: S-measure~($S_{m}$), mean E-measure~($E_\phi$), weighted F-measure~($F_\beta^w$), and MAE for evaluation. 




\subsection{Implementation Details}
All the experiments are performed on a single NVIDIA Titan V GPU with 12G memory. AdamW~\cite{adam} optimizer is used for all the experiments. The initial learning rate is set to $2e^{-4}$ for defocus blur detection and camouflaged object detection, and $5e^{-4}$ for others. Cosine decay is applied to the learning rate. The models are trained for 20 epochs for the SBU~\cite{sbu} dataset and camouflaged combined dataset~\cite{fan2020camouflaged,skurowski2018animal}, and 50 epochs for others. Random horizontal flipping is applied during training for data augmentation. The mini-batch is equal to 4. Binary cross-entropy (BCE) loss is used for defocus blur detection and forgery detection, balanced BCE loss is used for shadow detection, and BCE loss and IOU loss are used for camouflaged object detection. All the experiments are conducted with SegFormer-B4~\cite{xie2021segformer} pre-trained on the ImageNet-1k~\cite{imagenet} dataset.
 

\input{figs/sota}
\input{tables/ablation}

\subsection{Main Results}
\label{sec:main_results}


\paragraph{Comparison with the task-specific methods.}
EVP performs well when compared with task-specific methods. We report the comparison of our methods and other task-specific methods in Table~\ref{tab:sota_defocus}, Table~\ref{tab:sota_shadow}, Table~\ref{tab:sota_forgery}, and Table~\ref{tab:sota_cod}. Thanks to our stronger backbone and prompting strategy, EVP achieves the best performance in 5 datasets across 4 different tasks. However, compared with other well-designed domain-specific methods, EVP only introduces a small number of tunable parameters with the frozen backbone and obtains non-trivial performance. We also show some visual comparisons with other methods for each task individually in Figure~\ref{fig:sota_result}. We can see the proposed method predicts more accurate masks compared to other approaches.



\paragraph{Comparison with the efficient tuning methods.}
We evaluate our method with full finetuning and only tuning the decoder, which are the widely-used strategies for down-streaming task adaption. And similar methods from image classification, \ie, VPT~\cite{vpt} and AdaptFormer~\cite{chen2022adaptformer}. 
The number of prompt tokens is set to 10 for VPT and the middle dimension of AdaptMLP is set to 2 for a fair comparison in terms of the tunable parameters.
It can be seen from Table~\ref{tab:sota_finetune} that when only tuning the decoder, the performance drops largely. Compared with similar methods, introducing extra learnable tokens~\cite{vpt} or MLPs in Transformer block~\cite{chen2022adaptformer} also benefits the performance. We introduce a hyper-parameter~($r$) which is used to control the number of parameters of the Adaptor as described in equation~\ref{eqn:fpe}. We first compare EVP~($r$=16) with similar parameters as other methods. From the table, our method achieves much better performance. We also report EVP~($r$=4), with more parameters, the performance can be further improved and outperforms full-tuning on 3 of 4 datasets. 


\input{figs/ablation_arch}
\subsection{Ablation Study}
\label{sec:ablation_study}
We conduct the ablation to show the effectiveness of each component. The experiments are performed with the scaling factor $r=4$ except specified.

\paragraph{Architecture Design.}
To verify the effectiveness of the proposed visual prompting architecture, we modify it into different variants. As shown in Table~\ref{tab:arch} and Figure~\ref{fig:ablation_arch}, sharing $\mathtt{MLP^i_{tune}}$ in different Adaptors only saves a small number of parameters~(0.55M \textit{v.s.} 0.34M) but leads to a significant performance drop. It cannot obtain consistent performance improvement when using different $\mathtt{MLP_{up}}$ in different Adaptors, moreover introducing a large number of parameters~(0.55M \textit{v.s.} 1.39M). On the other hand, the performance will drop when we remove $F_{pe}$ or $F_{hfc}$, which  means that they are both effective visual prompts.


\paragraph{Tuning Stage.}
We try to answer the question: which stage contributes mostly to prompting tuning? Thus, we show the variants of our tuning method by changing the tunable stages in the SegFormer backbone. SegFormer contains 4 stages for multi-scale feature extraction. We mark the Stage$_{x}$ where the tunable prompting is added in Stage $x$. Table~\ref{tab:tuning_stage} shows that better performance can be obtained via the tunable stages increasing. Besides, the maximum improvement occurs in Stage$_{1,2}$ to Stage$_{1,2,3}$. Note that the number of transformer blocks of each stage in SegFormer-B4 is 3, 8, 27, and 3, respectively. Thus, the effect of EVP is positively correlated to the number of the prompted transformer blocks.


\paragraph{Scale Factor $r$~(equation \ref{eqn:fpe}).}
We introduce $r$ in Sec~\ref{sec:explicit_visual_prompting} of the main paper to control the number of learnable parameters. A larger $r$ will use fewer parameters for tuning. As shown in Table~\ref{tab:model_size}, the performance improves on several tasks when $r$ decreases from 64 to 4; when $r$ continues to decrease to 2 or 1, it can not gain better performance consistently even if the model becomes larger. It indicates that $r=4$ is a reasonable choice to make a trade-off between the performance and model size. 



\paragraph{EVP in Plain ViT.}
We experiment on SETR~\cite{zheng2021rethinking} to confirm the generalizability of EVP. SETR employs plain ViT as the backbone and a progressive upsampling ConvNet as the decoder, while SegFormer has a hierarchical backbone with 4 stages. Therefore, the only distinction between the experiments using SegFormer is that all modifications are limited to the single stage in plain ViT. The experiments are conducted with ViT-Base~\cite{dosovitskiy2020image} pretrained on the ImageNet-21k~\cite{imagenet} dataset. The number of prompt tokens is set to 10 for VPT, the middle dimension of AdaptMLP is set to 4 for AdaptFormer, and $r$ is set to 32 for our EVP. As shown in Table~\ref{tab:setr}, EVP also outperforms other tuning methods when using plain ViT as the backbone.

