\appendix
\label{sec:appendix}
\renewcommand\thetable{\Alph{section}\arabic{table}}   
\renewcommand\thefigure{\Alph{section}\arabic{figure}}  
% \section{Appendix}

 
\section{Implementation Details}
\label{sec:implementation_details}
We give more implementation details in the main paper of the "comparison with the task-specific methods" and "comparison with the efficient tuning methods". 

\paragraph{Basic Setting.} Our method contains a backbone for feature extraction and a decoder for segmentation prediction. We initialize the weight of the backbone via ImageNet classification pre-training, and the weight of the decoder is randomly initialized. Below, we give the details of each variant.


\paragraph{Full-tuning.} We follow the basic setting above, and then, fine-tune all the parameters of the encoder and decoder.

\paragraph{Only Decoder.} We follow the basic setting above, and then, fine-tune the parameters in the decoder only.

\paragraph{VPT~\cite{vpt}.} We first initialize the model following the basic setting. Then, we concatenate the prompt embeddings in each transformer block of the backbone only. Notice that, their prompt embeddings are implicitly shared across the whole dataset. We follow their  original paper and optimize the parameters in the prompt embeddings and the decoder. 

\paragraph{AdaptFormer~\cite{chen2022adaptformer}.} We first initialize the model following the basic setting above. Then, the AdaptMLP is added to each transformer block of the backbone for feature adaptation. We fine-tune the parameters in the decoder and the newly introduced AdaptMLP. 

\paragraph{EVP~(Ours).} We also initialize the weight following the basic setting. Then, we add the explicit prompting as described in the main paper of Figure~\ref{fig:arch}. 

\paragraph{Metric.}
AUC calculates the area of the ROC curve. ROC curve is a function of true positive rate~($\frac{tp}{tp + fn}$) in terms of false positive rate~($\frac{fp}{fp + tn}$), where $tp$, $tn$, $fp$, $fn$ represent the number of pixels which are classified as true positive, true negative, false positive, and false negative, respectively. $F_{1}$ score is defined as $F_{1} = \frac{2 \times precision \times recall}{precision + recall}$, where $precision = \frac{tp}{tp + fp}$ and $recall = \frac{tp}{tp + fn}$. 
The balance error rate~(BER) $ = \left(1-\frac{1}{2}\left(\frac{tp}{tp+fn}+\frac{tn}{tn+fp}\right)\right) \times 100$.
F-measure is calculated as $F_{\beta} = \frac{\left(1+\beta^2\right) \times precision \times recall}{\beta^2 \times precision + recall}$, where $\beta^2=0.3$. MAE computes pixel-wise average distance.
Weighted F-measure ($F_\beta^w$) weighting the quantities TP, TN, FP, and FN according to the errors at their location and their neighborhood information:
$F_\beta^w = \frac{\left(1+\beta^2\right) \times precision^w \times recall^w}{\beta^2 \times precision^w + recall^w}$.
E-measure~($E_\phi$) jointly considers image statistics and local pixel matching:
$E_\phi=\frac{1}{W \times H} \sum_{i=1}^W \sum_{j=1}^H \phi_S(i, j)$,
where $\phi_S$ is the alignment matrix depending on the similarity of the prediction and ground truth.


\paragraph{Training Data.}
Note that most forgery detection methods~(ManTraNet~\cite{wu2019mantra}, SPAN~\cite{hu2020span}, PSCCNet~\cite{liu2022pscc}, and ObjectFormer~\cite{wang2022objectformer} in Table~\ref{tab:sota_forgery}) and one shadow detection method~(MTMT~\cite{mtmt} in Table~\ref{tab:sota_shadow}) use extra training data to get better performance. We only use the training data from the standard datasets and obtain SOTA performance.



\section{More Results}
\label{sec:more_results}
We provide more experimental results in addition to the main paper.


\subsection{High-Frequency Prompting}
Our method gets the knowledge from the explicit content of the image itself, hence we also discuss other similar explicit clues of images as the prompts. Specifically, we choose the common-used Gaussian filter, the noise-filter~\cite{fridrich2012rich}, the all-zero image, and the original image as experiments. From Table~\ref{tab:filter}, we find the Gaussian filter shows a better performance in defocus blur since 
it is also a kind of blur. Also, the noise filter~\cite{fridrich2012rich} from forgery detection also boosts the performance. Interestingly, we find that simply replacing the original image with an all-zero image also boosts the performance, since it can also be considered as a kind of implicitly learned embeddings across the full dataset as in VPT~\cite{vpt}. Differently, the high-frequency components of the image achieve consistent performance improvement to other methods on these several benchmarks.


\subsection{HFC \textit{v.s.} LFC} We conduct the ablation study on choosing of high-frequency features or the low-frequency features in Table~\ref{tab:hfc}. From the table, using the low-frequency components as the prompting just show some trivial improvement on these datasets. Differently, the high-frequency components are more general solutions and show a much better performance in shadow detection, forgery detection, and camouflaged detection. Similar to the Gaussian filter as we discussed above, the LFC is also a kind of blur, which makes the advantage of LFC in the defocus blur detection.

\subsection{Mask Ratio $\tau$} We further evaluate the hyper-parameter mask ratio~$\tau$ introduced in Section~\ref{Preliminary}. From Table~\ref{tab:hfc}, when we mask out 25\% of the central pixels in the spectrum, it achieves consistently better performance in all the tasks. We also find that the performance may drop when the increasing of mask ratio~(all 0 images), especially in shadow detection, forgery detection, and camouflaged object detection. 


\input{tables/supp}

% \section{Visualization}
\section{Additional Visual Results}
\label{sec:visulation}

We give more visual results of EVP and other task-specific methods on the four tasks in Figure~\ref{fig:supp_forgery},~\ref{fig:supp_shadow},~\ref{fig:supp_defocus}, and~\ref{fig:supp_cod} as supplementary to the visual results in the main paper.

\input{figs/supp_fig}
