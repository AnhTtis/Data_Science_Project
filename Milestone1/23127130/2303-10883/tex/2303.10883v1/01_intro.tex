
\section{Introduction}
\label{sec:intro}
\input{figs/teaser}

Advances in image editing and manipulation algorithms have made it easy to create photo-realistic but fake pictures~\cite{huh2018fighting,rombach2022high,kawar2022imagic}. Detecting such manipulated regions becomes an important problem due to its potential negative impact related to surveillance and crime~\cite{huh2018fighting}. Low-level structures are known to be beneficial to tampered region detection, \ie, resizing and copy-pasting will destroy the JPEG compression levels between the temper region and the host image~\cite{luo2010jpeg,huang2010detecting,popescu2005exposing}, the noise level of the tempered region and the background is also different~\cite{zhou2018learning, wu2019mantra}.
Interesting, to segment the blurred pixels~\cite{shi2014discriminative}, shadowed regions~\cite{panagopoulos2011illumination}, and concealed objects~\cite{fan2020camouflaged}, low-level clues also play important roles. 
These detection tasks are shown to be beneficial to numerous computer vision tasks, including auto-refocus~\cite{bae2007defocus}, image retargeting~\cite{karaali2016image}, object tracking~\cite{mikic2000moving}, \etc.

% previous works and problems
Although all these tasks belong to low-level structure segmentation, they are typically addressed by domain-specific solutions with carefully designed network architectures~\cite{zhou2018learning,cun2020defocus,zhu2021mitigating}. Moreover, the lack of large-scale datasets is often considered a major factor, which limits the performances~\cite{huh2018fighting}.


In this work, we propose a solution to address the four tasks in a unified fashion. We take inspiration from recent advances of \textit{prompting}~\cite{vpt,chen2022adaptformer,bar2022visual}, which is a concept that initially emerged in natural language processing (NLP)~\cite{brown2020language}. The basic idea is to efficiently adapt a frozen large foundation model to many downstream tasks with the minimum extra trainable parameters. As the foundation model has already been trained on a large-scale dataset, prompting often leads to better model generalization on the downstream tasks~\cite{brown2020language}, especially in the case of the limited annotated data. Prompting also significantly saves the storage of models since it only needs to save a shared basic model and task-aware promptings.


Our main insight is to tune the task-specific knowledge \textit{only} from the features of each individual image itself because the pre-trained base model contains sufficient knowledge for semantic understanding. This is also inspired by the effectiveness of hand-crafted image features, such as SIFT~\cite{huang2008detection}, JPEG noise~\cite{luo2010jpeg}, resampling artifacts~\cite{popescu2005exposing} in these tasks~\cite{liu2011detection,luo2010jpeg,huang2008detection,huang2010detecting,popescu2005exposing,zhou2018learning}.

Based on this observation, we propose \textit{explicit visual prompting~(EVP)}, where the tuning performance can be hugely improved via the re-modulation of image features. Specifically, we consider two kinds of features for our task. 
The first is the features from the frozen patch embedding, which is critical since we need to shift the distribution of the original model.
Another is high-frequency components of the input image since the pre-trained visual recognition model is learned to be invariant to these features via data augmentation. 
As shown in Figure~\ref{fig:teaser}, we take a model pre-trained on a large-scale dataset and freeze its parameters. Then, to adapt to each task, we tune the embedded features and learn an extra embedding for high-frequency components of each individual image. 


In terms of experiments, we validate our approach on nine datasets of four tasks: forgery detection, shadow detection, defocus blur detection as well as camouflaged object detection. Our simple and unified network achieves very competitive performance with the whole model fine-tuning and outperforms task-specific solutions without modification.

In summary, our main contributions are as follows:
\begin{itemize}
    \item We design a unified approach that produces state-of-the-art performances for a number of tasks, including forgery detection, defocus blur detection, shadow detection, and camouflaged object detection. 
    
    \item We propose explicit visual prompting (EVP), which takes the features from the frozen patch embedding and the input's high-frequency components as prompting. It is demonstrated to be effective across different tasks and outperforms other parameter-efficient tuning methods. 
    
    \item Our method greatly simplifies the low-level structure segmentation models as well as achieves comparable performance with well-designed SOTA methods.
    
\end{itemize}