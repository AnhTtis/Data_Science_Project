\begin{table*}[!t]
\centering
% \resizebox{\textwidth}{!}
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{Method}& Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) &$F_{\beta}\uparrow$ & MAE $\downarrow$      & BER $\downarrow$    & $F_1\uparrow$ & AUC $\uparrow$  & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$  & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$   \\ \hline
Decoder~(No prompting) & 3.15 & .891 & .080 & 4.36  & .396 & .722  & .783 & .827 & .671 & .088 \\ 
Ours w/o $F_{pe}$ & 3.61 & .924 & .049 & 1.68  & .540 & .833  & .840 & .887 & .759 & .065 \\ 
Ours w/o $F_{hfc}$ & 3.58 & .926  & .046  & 1.61 & .619   & .846 & .844 & .893 & .773 & .063 \\ 
Ours w/ Shared $\mathtt{MLP^i_{tune}}$ & 3.49 & .928   & .048    & 1.77 & .619   & .860 & .837 & .889 & .763 & .064 \\
Ours w/ Unshared $\mathtt{MLP_{up}}$ & 4.54 & .927   & \textbf{.045}    & \textbf{1.33} & \textbf{.647}   & \textbf{.875}  & .844 & .893 & .774 & .060 \\ 
Ours & 3.70 & \textbf{.928}  & \textbf{.045}  & 1.35 & .636   & .862 & \textbf{.846} & \textbf{.895} & \textbf{.777} & \textbf{.059} \\ \bottomrule
\end{tabular}}
\caption{Ablation on the architecture designs described in Figure~\ref{fig:arch}. We conduct evaluations on four datasets for four different tasks. The proposed prompting strategy (Decoder + $F_{hfc}$ + $F_{pe}$ + Adaptor) performs more effectively.}
\label{tab:arch}
\end{table*}



\begin{table*}[!t]
\centering
% \resizebox{\textwidth}{!}
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
Tuning & Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      Stage & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) &$F_{\beta}\uparrow$ & MAE $\downarrow$      & BER $\downarrow$    & $F_1\uparrow$ & AUC $\uparrow$  & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$  & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$   \\ \hline
Stage$_{1}$ & 3.16 & .895 & .072 & 3.64  & .408 & .725  & .793 & .834 & .681 & .088 \\ 
Stage$_{1,2}$ & 3.18 & .917 & .058 & 2.45  & .457 & .765  & .806 & .853 & .706 & .081 \\ 
Stage$_{1,2,3}$ & 3.43 & .927   & .047  & 1.46 & .627 & .858 & .841 & .888 & .768 & .062 \\
Stage$_{1,2,3,4}$ & 3.70 & \bf.928  & \bf.045  & \bf1.35 & \bf.636   & \bf.862 & \bf.846 & \bf.895 & \bf.777 & \bf.059 \\ 
\bottomrule
\end{tabular}}
\caption{Ablation on the tuning stages in SegFormer. We conduct evaluations on four datasets for four different tasks. The performance of EVP becomes better as the tuning stages increase.}
\label{tab:tuning_stage}
\end{table*}



\begin{table*}[!t]
\centering
% \resizebox{\textwidth}{!}
{
\begin{tabular}{c||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{$r$}& Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) &$F_{\beta}\uparrow$ & MAE $\downarrow$      & BER $\downarrow$    & $F_1\uparrow$ & AUC $\uparrow$  & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$  & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$   \\ \hline
64 & 3.17 & .910 & .055 & 2.09  & .547 & .830  & .829 & .875 & .743 & .070 \\ 
32 & 3.18 & .919 & .054 & 1.84  & .574 & .844  & .832 & .877 & .749 & .067 \\ 
16  & 3.22 & .924  & .051  & 1.67 & .602 & .857 & .838 & .888 & .761 & .065 \\
8  & 3.34 & .923  & .049  & 1.46 & .619 & .856 & .841 & .890 & .767 & .062 \\
4 & 3.70 & .928  & .045   & 1.35 & .636   & \bf.862 & \bf.846 & .895 & .777 & \bf.059 \\
2 & 4.95 & .929  & .042   & \bf1.31 & \bf.642   & .859 & .842 & \bf.896 & .776 & .059 \\ 
1 & 9.56 & \bf.931  & \bf.040   & 1.48 & .621   & .847 & .843 & .894 & \bf.778 & .059 \\ \bottomrule
\end{tabular}}
\caption{Ablation on the parameter scale factor $r$. We conduct evaluations on four datasets for four different tasks. EVP gets the balance between the number of tunable parameters and performances when $r=4$.}
\label{tab:model_size}
\end{table*}


% ------SETR-------
\begin{table*}[t]
\centering
% \resizebox{\textwidth}{!}
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{Method} & Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) &$F_{\beta}\uparrow$ & MAE $\downarrow$      & BER $\downarrow$    & $F_1\uparrow$ & AUC $\uparrow$  & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$  & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$   \\ \hline
Full-tuning & 98.98 & \bf.862 & \bf.077 & 4.39  & .290 & .650  & .593 & \bf.677 & .382 & .157 \\ 
Only Decoder & 13.00 & .836 & .097 & 4.77  & \color{orange}{.318} & \color{orange}{.662}  & \color{orange}{.615} & .659 & \color{orange}{.385} & .162 \\ 
VPT~\cite{vpt}  & 13.09 & .843  & .092  & 4.56 & \color{orange}{.315} & \color{orange}{.666} & \color{orange}{.615} & .660 & \color{orange}{.387} & .161 \\
AdaptFormer~\cite{chen2022adaptformer} & 13.08 & .845   & .092  & 4.60 & \color{orange}{.319} & \color{orange}{.662} & \color{orange}{.614} & .662 & \color{orange}{.387} & .161 \\
EVP & 13.06 & .850  & 087   & \color{orange}{\bf4.36} & \color{orange}{\bf.324}   & \color{orange}{\bf.675} & \color{orange}{\bf.622} & .674 & \color{orange}{\bf.402} & \color{orange}{\bf.156} \\ \bottomrule
\end{tabular}}
\caption{Comparison with other tuning methods with SETR~\cite{zheng2021rethinking} on four different tasks. We conduct an evaluation on four datasets for four different tasks.
The best performance is shown as \textbf{bold}. 
The prompt-tuning method which achieves better performance than full-tuning is marked as \color{orange}{orange}.}
\label{tab:setr}
\end{table*}
