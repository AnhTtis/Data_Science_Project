
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}

\begin{table*}[t]
	\centering
	\begin{minipage}{0.3\linewidth}
		\centering
        \tablestyle{1pt}{1.05}
        \begin{tabular}{l||cc|cc}
            \toprule
             \multirow{2}{*}{Method} &  \multicolumn{2}{c|}{DUT~\cite{zhao2018defocus}}   & \multicolumn{2}{c}{CUHK~\cite{shi2014discriminative}}  \\  \cline{2-5}
            & $F_{\beta}\uparrow$ & MAE$\downarrow$   & $F_{\beta}\uparrow$ & MAE$\downarrow$    \\ \hline            DeFusionNet~\cite{tang2019defusionnet}  &  .823    & .118  & .818 & .117 \\
            BTBNet~\cite{zhao2019btbnet}  &  .827    & .138  & .889 & .082 \\
            CENet~\cite{zhao2019cenet}    & .817    & .135  & .906 & .059 \\
            DAD~\cite{zhao2021self}     & .794    & .153  & .884    & .079  \\
            EFENet~\cite{zhao2021defocus} & .854    & .094  & .914   & .053  \\ \hline
            Ours & \textbf{.890} & \textbf{.068} & \textbf{.928} & \textbf{.045}  \\ \bottomrule
        \end{tabular} 
        % \vspace{-1em}
        \caption{Comparison with state-of-the-art approaches on defocus blur detection.}
    \label{tab:sota_defocus}
	\end{minipage}\quad
    \vspace{.1em}
    % \hspace{4em}
	\begin{minipage}{0.3\linewidth}
		\centering
        \tablestyle{1pt}{1.05}
        \begin{tabular}{l||c|c}
            \toprule
             \multirow{2}{*}{Method}  & ISTD~\cite{wang2018stacked} &  SBU~\cite{sbu}  \\   \cline{2-3}
            & BER$\downarrow$ & BER$\downarrow$    \\ \hline
            BDRAR~\cite{zhu2018bidirectional}  & 2.69   & 3.89   \\
            DSC~\cite{hu2018direction}          & 3.42      & 5.59   \\
            DSD~\cite{zheng2019distraction}                 & 2.17 & 3.45   \\
            MTMT~\cite{mtmt}   & 1.72 & 3.15 \\
            FDRNet~\cite{zhu2021mitigating} & 1.55 & \textbf{3.04} \\ \hline
            Ours & \textbf{1.35} & 4.31 \\ \bottomrule
        \end{tabular}
        % \vspace{1em}
        \caption{Comparison with state-of-the-art approaches on shadow detection.}
    \label{tab:sota_shadow}
	\end{minipage}\quad
    \vspace{.1em}
    % \hspace{-1em}
	\begin{minipage}{0.3\linewidth}
		\centering
        \tablestyle{1pt}{1.05}
        \begin{tabular}{l||cc|cc}
            \toprule
             \multirow{2}{*}{Method} & \multicolumn{2}{c|}{IMD20~\cite{novozamsky2020imd2020}}   & \multicolumn{2}{c}{CAISA~\cite{dong2013casia}}  \\ \cline{2-5}
            & F1$\uparrow$ & AUC$\uparrow$   & F1$\uparrow$ & AUC$\uparrow$      \\ \hline
            ManTra~\cite{wu2019mantra}    & -         & .748    &  -   & .817       \\
            SPAN~\cite{hu2020span}        &    -    & .750         & .382       & .838       \\
            PSCCNet~\cite{liu2022pscc}        &   -  & .806         & .554       & .875       \\
            TransForensics~\cite{hao2021transforensics} & -   & \textbf{.848}    & .627    & .837       \\ 
            ObjectFormer~\cite{wang2022objectformer}    & -   & .821     & .579     & \textbf{.882}       \\\hline
            Ours  & \textbf{.443} & .807 & \textbf{.636} & .862  \\ \bottomrule
        \end{tabular}
        % \vspace{1em}
        \caption{Comparison with state-of-the-art approaches on forgery detection.}
    \label{tab:sota_forgery}
	\end{minipage}
    \vspace{.1em}
    % \hspace{8em}
    \vspace{1em}
    \begin{minipage}{1.0\linewidth}
		\centering
        % \footnotesize
		% \tablestyle{1pt}{1.05}
        \begin{tabular}{l||cccc|cccc|cccc}
            \toprule
            \multirow{2}{*}{Method}&  \multicolumn{4}{c|}{CHAMELEON~\cite{skurowski2018animal}} & \multicolumn{4}{c|}{CAMO~\cite{le2019anabranch}} & \multicolumn{4}{c}{COD10K~\cite{fan2020camouflaged}}    \\ \cline{2-13}
            & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$ & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$  & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$ & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$ & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$ & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$\\ \hline
            SINet~\cite{fan2020camouflaged} & .869 & .891 & .740 & .044 & .751 & .771 & .606 & .100 & .771 & .806 & .551 & .051 \\
            RankNet~\cite{lv2021simultaneously}  & .846 & .913 & .767 & .045 & .712 & .791 & .583 & .104 & .767 & .861 & .611 & .045 \\
            JCOD~\cite{li2021uncertainty}  & .870 & .924 & - & .039 & .792 & .839 & - & .082 & .800 & .872 & - & .041 \\
            PFNet~\cite{mei2021camouflaged} & .882 & \textbf{.942} & .810 & .033 & .782 & .852 & .695 & .085 & .800 & .868 & .660 & .040  \\
            FBNet~\cite{jiaying2022frequency} & \textbf{.888} & .939 & \textbf{.828} & \textbf{.032} & .783 & .839 & .702 & .081 & .809 & .889 & .684 & .035 \\
            \hline
            Ours & .871 & .917 & .795 & .036 & \textbf{.846} & \textbf{.895} & \textbf{.777} & \textbf{.059} & \textbf{.843} & \textbf{.907} & \textbf{.742} & \textbf{.029} \\
            \bottomrule
        \end{tabular}
        % \vspace{-1em}
        \caption{Comparison with state-of-the-art approaches on camouflaged object detection.}
    \label{tab:sota_cod}
	\end{minipage}\quad
% \vspace{-15pt}
% \vspace{-.5em}
% \caption{Comparison with state-of-the-art approaches on four different tasks. The best performance among all methods is shown as \textbf{bloded}.
% }
\end{table*}




\begin{table*}[t]
\centering
% \resizebox{\textwidth}{!}
{
\begin{tabular}{l||c|cc|c|cc|cccc}
\toprule
\multirow{3}{*}{Method}& Trainable & \multicolumn{2}{c|}{\textbf{Defocus Blur}} & \textbf{Shadow} & \multicolumn{2}{c|}{\textbf{Forgery }} & \multicolumn{4}{c}{\textbf{Camouflaged}}\\
      & Param.&  \multicolumn{2}{c|}{CUHK~\cite{shi2014discriminative}} & ISTD~\cite{wang2018stacked}& \multicolumn{2}{c|}{CASIA~\cite{dong2013casia}} & \multicolumn{4}{c}{CAMO~\cite{le2019anabranch}}\\ 
      & (M) &$F_{\beta}\uparrow$ & MAE $\downarrow$      & BER $\downarrow$    & $F_1\uparrow$ & AUC $\uparrow$  & $S_\alpha \uparrow$ & $E_\phi$ $\uparrow$  & $F_\beta^w$ $\uparrow$ & MAE $\downarrow$   \\ \hline
Full-tuning & 64.00 & \textbf{.935} & \textbf{.039} & 2.42  & .465 & .754  & .837 & .887 & \textbf{.778} & .060 \\  
Only Decoder & 3.15 & .891 & .080 & 4.36  & .396 & .722  & .783 & .827 & .671 & .088 \\
VPT-Deep~\cite{vpt} & 3.27 & .913   & .058    & \color{orange}{1.73} & \color{orange}{.588}   & \color{orange}{.847} & .833 & .884 & .751 & .068 \\
AdaptFormer~\cite{chen2022adaptformer} & 3.21 & .912   & .057    & \color{orange}{1.85} & \color{orange}{.602}   & \color{orange}{.855}  & .830 & .877 & .750 & .068 \\
Ours~(r=16) & 3.22 & .924 & .051 & \color{orange}{1.67}  & \color{orange}{.602} & \color{orange}{.857}  & \color{orange}{.838} & \color{orange}{.888} & .761 & .065 \\ 
Ours~(r=4) & 3.70 & .928  & .045  & \color{orange}{\textbf{1.35}} & \color{orange}{\textbf{.636}}   & \color{orange}{\textbf{.862}} & \color{orange}{\textbf{.846}} & \color{orange}{\textbf{.895}} & .777 & \color{orange}{\textbf{.059}} \\ \bottomrule
\end{tabular}}
\caption{
Comparison with state-of-the-art efficient tuning approaches. We conduct evaluations on four datasets for four different tasks.
The efficient tuning method which achieves better performance than full-tuning is marked as {\color{orange}{orange}}.
The best performance among all methods is shown as \textbf{blod}.
}
\label{tab:sota_finetune}
\end{table*}


