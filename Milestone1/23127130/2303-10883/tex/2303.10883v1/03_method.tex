

\section{Method}
\label{sec:method}
In this section, we propose Explicit Visual Prompting~(EVP) for adapting recent Vision Transformers~(SegFormer~\cite{xie2021segformer} as the example) pre-trained on ImageNet~\cite{deng2009imagenet} to low-level structure segmentations. EVP keeps the backbone frozen and only contains a small number of tunable parameters to learn task-specific knowledge from the features of frozen image embeddings and high-frequency components. Below, we first present SegFormer\cite{xie2021segformer} and the extraction of high-frequency components in Section~\ref{Preliminary}, then the architecture design in Section~\ref{sec:explicit_visual_prompting}. 



\subsection{Preliminaries}
\label{Preliminary}

\paragraph{SegFormer~\cite{xie2021segformer}.} SegFormer is a hierarchical transformer-based structure with a much simpler decoder for semantic segmentation. Similar to traditional CNN backbone~\cite{resnet}, SegFormer captures multi-stale features via several stages. Differently, each stage is built via the feature embedding layers\footnote{SegFormer has a different definition of patch embedding in ViT~\cite{dosovitskiy2020image}. It uses the overlapped patch embedding to extract the denser features and will merge the embedding to a smaller spatial size at the beginning of each stage.} and vision transformer blocks~\cite{vaswani2017attention, dosovitskiy2020image}. As for the decoder, it leverages the multi-scale features from the encoder and MLP layers for decoding to the specific classes. Notice that, the proposed prompt strategy is not limited to SegFormer and can be easily adapted to other network structures, \eg, ViT~\cite{dosovitskiy2020image} and Swin~\cite{liu2021swin}.

\input{figs/archs}

\paragraph{High-frequency Components (HFC).} As shown in Figure~\ref{fig:hfc}, for an image $I$ of dimension $H \times W$, we can decompose it into low-frequency components $I_l$ (LFC) and high-frequency components $I_h$ (HFC), \textit{i.e.} $I = \{I_l, I_h\}$. Denoting $\mathtt{fft}$ and $\mathtt{ifft}$ as the Fast Fourier Transform and its inverse respectively, we use $z$ to represent the frequency component of $I$. Therefore we have $z = \mathtt{fft}(I)$ and $I = \mathtt{ifft}(z)$. We shift low frequency coefficients to the center $(\frac{H}{2}, \frac{W}{2})$. To obtain HFC, a binary mask $\mathbf{M}_h \in \{0, 1\}^{H \times W}$ is generated and applied on $z$ depending on a mask ratio $\tau$: 

\begin{equation}
\mathbf{M}^{i,j}_h(\tau) = \begin{cases} 
	0, & \frac{4|(i - \frac{H}{2})(j - \frac{W}{2})|}{HW} \le \tau\\ 
	1, & \text{otherwise} 
\end{cases}
\end{equation}

$\tau$ indicates the surface ratio of the masked regions. HFC can be computed:
\begin{align}
	I_{hfc} =  \mathtt{ifft}(z \mathbf{M}_h(\tau))
\end{align}

Similarly, a binary mask $\mathbf{M}_l \in \{0, 1\}^{H \times W}$ can be properly defined to compute LFC: 
\begin{equation}
	\mathbf{M}^{i,j}_l(\tau) = \begin{cases} 
		0, & \frac{HW - 4|(i - \frac{H}{2})(j - \frac{W}{2})|}{HW} \le \tau\\ 
		1, & \text{otherwise} 
	\end{cases}
\end{equation}
and LFC can be calculated as:
\begin{align}
	I_{lfc} =  \mathtt{ifft}(z \mathbf{M}_l(\tau))
\end{align}

Note that for RGB images, we compute the above process on every channel of pixels independently.
 
 \input{figs/arch_hier}

 
\subsection{Explicit Visual Prompting}
\label{sec:explicit_visual_prompting}

In this section, we present the proposed Explicit Visual Prompting (EVP). Our key insight is to learn explicit prompts from image embeddings and high-frequency components.
We learn the former to shift the distribution from the pre-train dataset to the target dataset. 
And the main motivation to learn the latter is that the pre-trained model is learned to be invariant to these features through data augmentation.
Note that this is different from VPT~\cite{vpt}, which learns implicit prompts. Our approach is illustrated in Figure~\ref{fig:arch}, which is composed of three basic modules: patch embedding tune, high-frequency components tune as well as Adaptor.

\paragraph{Patch embedding tune.} This module aims at tuning pre-trained patch embedding. In pre-trained SegFormer~\cite{xie2021segformer}, a patch $I^p$ is projected to a $C_{seg}$-dimension feature. We freeze this projection and add a tunable linear layer $\mathtt{L_{pe}}$ to project the original embedding into a $c$-dimension feature $F_{pe} \in \mathbb{R}^c$. 

\begin{align}
	F_{pe} =  \mathtt{L_{pe}}(I^p) \text{, with } c = \frac{C_{seg}}{r} \label{eqn:fpe}
\end{align}
% \begin{align}
	% F_{pe} =  \mathtt{L_{pe}}(I^p)  \label{eqn:fpe}
% \end{align}
where we introduce the scale factor $r$ to control the tunable parameters.



\paragraph{High-frequency components tune.} For the high frequency components $I_{hfc}$, we learn an overlapped patch embedding similar to SegFormer~\cite{xie2021segformer}. Formally,
% $I^p_{hfc}$ 
$I_{hfc}$ is divided into small patches with the same patch size as SegFormer~\cite{xie2021segformer}. Denoting patch $I^p_{hfc} \in \mathbb{R}^C$ and $C = h \times w \times 3$, we learn a linear layer $\mathtt{L_{hfc}}$ to project the patch into a $c$-dimension feature $F_{hfc} \in \mathbb{R}^c$. 

% \begin{align}
% 	F_{hfc} =  \mathtt{L_{hfc}}(I^p_{hfc}) \text{, with } c = \frac{C_{seg}}{r}
% \end{align}
\begin{align}
	F_{hfc} =  \mathtt{L_{hfc}}(I^p_{hfc})
\end{align}


\paragraph{Adaptor.} The goal of Adaptor is to efficiently and effectively perform adaptation in all the layers by considering features from the image embeddings and high-frequency components.
For the $i$-th Adaptor, we take $F_{pe}$ and $F_{hfc}$ as input and obtain the prompting $P^i$: 
\begin{align}
 	% P^i & =  \mathtt{L_{share}({GELU(\mathtt{L^i}(F_{hfc}+ F_{pe}))})} 
   	P^i & =  \mathtt{MLP_{up}({GELU(\mathtt{MLP^i_{tune}}(F_{pe}+F_{hfc}))})} 
\end{align}
where $\mathtt{GELU}$ is GELU~\cite{hendrycks2016gaussian} activation. $\mathtt{MLP^i_{tune}}$ is a linear layer for producing different prompts in each Adaptor. $\mathtt{MLP_{up}}$ is an up-projection layer shared across all the Adaptors for matching the dimension of transformer features. $P^i$ is the output prompting that attaches to each transformer layer. 
