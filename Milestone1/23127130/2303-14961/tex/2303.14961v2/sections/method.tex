\section{Certified Robust OOD Detection}\label{sec:method}

This section explains how using local Lipschitz continuity, achieved through smoothing the classifier with Gaussian noise, can guarantee the detection of OOD samples within a $\ell_2$-sphere around the input. 


\textbf{Preliminaries.} 
To determine how well a classifier distinguishes between ID and OOD samples, it is common to threshold the confidence level and to calculate the area under the receiver operating characteristic curve (AUROC or AUC).
Formally, let us consider a function\footnote{e.g. the Maximum Softmax Probability~\cite{msp}, or the Energy function~\cite{energy}.} $h\in\R^d \to \R$, the AUC is defined as:
\begin{equation*}
    \text{AUC}_h (\set{D}_{in}, \set{D}_{out}) = \E_{
    \begin{subarray}{l} x \sim \set{D}_{in}, \\ 
    z \sim \set{D}_{out}\end{subarray}} 
    \left[\mathbbm{1}_{h(x) > h(z)}\right],
\end{equation*}
where $\set{D}_{in}, \set{D}_{out}$ are ID and OOD data sets, respectively, and $\mathbbm{1}$ returns 1 if the argument is true and 0 otherwise.
A number of prior works~\cite{ccu, good, atom, berrada2021make, prood} also investigated the worst-case AUC (WCAUC), which is defined as the lowest AUC attainable when every OOD sample is perturbed so that the highest level of confidence is achieved within a specific threat model.
Specifically, the WCAUC is defined as:
\begin{equation*}
    \text{WCAUC}_h (\set{D}_{in}, \set{D}_{out}) = \E_{
    \begin{subarray}{l} x \sim \set{D}_{in}, \\ 
    z \sim \set{D}_{out}\end{subarray}}
    \left[\mathbbm{1}_{h(x) > \underset{\norm{\tilde{z}-z}_p \leq \epsilon}{\max} h(\tilde{z}) } \right].
\end{equation*}

Due to the intractable nature of the maximization problem, we can compute upper or lower bounds only, i.e. $\underline{h}(z) \leq \max_{\norm{\tilde{z}-z}_p \leq \epsilon} h(\tilde{z}) \leq \bar{h}(z)$.
The lower bound $\underline{h}(z)$ is typically calculated using projected gradient methods~\citep{pgd, apgd} and named Adversarial AUC (AAUC) (upper bound of WCAUC).
In the context of $\ell_\infty$-norm, the upper bound $\bar{h}(z)$, called Guaranteed AUC (GAUC) (lower bound of WCAUC), is computed using IBP in \citet{good} and \citet{prood}.

Here, we propose a method for computing the upper bound of any classifier without the need for special training or modifications.
Thus, the main theorem for an $\ell_2$-norm robustly certified upper bound is stated.

\begin{restatable}{theorem}{upperbound}\label{th:upper_bound}
Let $F:\sR^d \to \prob{(\set{Y})}$ be any soft classifier and $G$ be its associated smooth classifier as defined in ~\autoref{eq:smooth_classifier}, with $\sigma > 0$.
If $p = \max_{y\in\set{Y}} G(x)_y > 1/2$, then, we have that:
\begin{equation}
    \max_{y\in \set{Y}} G(x+\delta)_y \leq \sqrt{\frac{2}{\pi}} \Phi^{-1}(p) + p,
\end{equation}
for every $\norm{\delta}_2 < \sigma \Phi^{-1}(p)$.
\end{restatable}
%
Proof is given in~\autoref{app:theorem_upper_bound}.
In other words, if the smooth classifier assigns the most likely class more than half the time, it is locally Lipschitz continuous in $x$, and its maximum prediction is bounded within a radius smaller than $R = \sigma \Phi^{-1}(p)$.

\textbf{Discussion.}
While this theorem provides some advantages, it is important to note a couple of its limitations.
One of the main limitations is that the upper bound of the smooth classifier $G$ only applies to $G$ and not to the original classifier $F$. 
As a result, the guarantee only applies to $G$, and its robustness at a given input point $x$ cannot be precisely evaluated or certified. 
To overcome this, Monte Carlo algorithms can be used to approximate these evaluations with high probability~\cite{randomized_smoothing}.

Another limitation is that the guarantees provided by this theorem are only probabilistic in practice. 
Therefore, a hypothesis test~\cite{hypothesis_test} should be used to avoid making predictions with low confidence. 
As with randomized smoothing~\cite{randomized_smoothing}, a large number of samples must be generated in order to achieve high levels of confidence in the certification radius. 
However, generating these samples can be computationally expensive for complex models.

Despite these limitations, the theorem provides a novel way of calculating the upper bound of any classifier, without the need for special training or modification.
Additionally, we provide a tighter certificate compared to previous approaches \cite{good, prood}, as they used IBP.
This can be useful for evaluating the certified robustness of a broader category of standard OOD detection methods as well as larger models, where IBP bounds explode in size and make them unusable~\cite{hurt_training}.
