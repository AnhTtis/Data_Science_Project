\section{Background}\label{sec:background}

We define a \textit{hard} classifier as a function $f : \sR^d \to \set{Y}$ which maps input samples $x \in \sR^d$ to output $y \in \set{Y}$, where $\set{Y} = \{1, \dots, K\}$ is the discrete set of $K$ classes. 
Additionally, we introduce a \textit{soft} version $F : \sR^d \to \prob(\set{Y})$ of $f$, where $\prob(\set{Y})$ is the set of probability distributions over $\set{Y}$.
Thus, a soft classifier assigns each data point a distribution over classes, rather than just assigning it to a class. 
This distribution indicates the likelihood that each class will occur.
It is possible to convert any soft classifier $F$ into a hard classifier $f$ by mapping $f(x) = \argmax_{y\in\set{Y}} F(x)_y$.
Additionally, we define as $\set{N}(0, 1)$ the standard Gaussian distribution, as $\Phi(x)$ the Gaussian CDF and as $\Phi^{-1}(x)$ its inverse (or quantile).

\textbf{Robustness Certificates.}
Even though an adversarially-trained network is resilient to attacks created during training, it can still be susceptible to unseen new attacks.
To overcome this problem, certified defenses formally guarantee the stability of the prediction in a neighbourhood of the input. 
In other words, a neural network $f$ is certifiably robust for the input $x \in \mathbb{R}^d$, if the prediction for all perturbed versions $\tilde{x}$ remains unchanged such that $\norm{\tilde{x}-x}_p \leq \epsilon$, where $\norm{\cdot}_p$ is the $\ell_p$-norm around $x$ of size $\epsilon > 0$.
% \textbf{Virtual Outliers Synthesis (VOS)} \cite{vos}.
% The underlying assumption of this method is that the feature representation of input instances forms a class-conditional multivariate Gaussian distribution:
% \begin{equation}
%     \prob( h(\vec x) | y = c) = \set{N}(\symvec{\mu}_c, \vec \Sigma), \quad \forall c\in\set{Y}, 
% \end{equation}
% where $\symvec{\mu}_c$ is the Gaussian mean, $\vec{\Sigma}$ is the tied covariance matrix, and $h(\vec{x}) \in \mathbb{R}^{m}$ is the latent representation of an input $\vec{x}\in\mathbb{R}^n$, with $m \ll n$.
% \citet{vos} propose to sample the virtual outliers from the feature representation space, using the previous 

\textbf{Randomized Smoothing.} 
This robustness verification method \cite{randomized_smoothing, randomized_smoothing_2} computes the $\ell_2$-norm certificates around an input sample $x$ by counting which class is most likely to be returned when $x$ is perturbed by isotropic Gaussian noise. 
Formally, given a \textit{soft} classifier $F$, randomized smoothing considers a \textit{smooth} version of $F$ defined as:
%
\begin{equation}\label{eq:smooth_classifier}
    G(x) \eqdef \E_{\delta \sim \set{N}(0, \sigma^2I)}\left[F(x + \delta)\right],
\end{equation}
%
where $\sigma > 0$ represents the standard deviation.
As previously, we define the hard version of $G(x)$ as $g(x) =\argmax_{y\in\set{Y}} G(x)_y$.
Contrary to other formal verification methods, randomized smoothing does not make any assumptions regarding the model's properties, allowing certification to be scaled to larger and more complex networks.
\citet{randomized_smoothing} demonstrated that $G$ is robust to perturbations of radius $R$, where the radius $R$ is defined as the difference in probabilities between the most likely class and the second most likely class.
A more general interpretation is given by ~\citet{yang2020randomized}.
%
\begin{restatable}{lemma}{randomizedsmoothing}[\citet{yang2020randomized}]\label{th:randomized_smoothing}
    Given a smoothed classifier $G$ defined as in \autoref{eq:smooth_classifier}, such that $G(x) = (G(x)_1, \dots, G(x)_K)$ is a vector of probabilities that $G$ assigns to each class $1, \dots, K$. 
    Suppose $G$ predicts class $c$ on input $x$, and the probability is $p = max_{y\in \set{Y}} G(x)_y > 1/2$, then $G$ continues to predict class $c$ when $x$ is perturbed by any $\delta$ with:
    \begin{equation*}
        \norm{\delta}_2 < \sigma \Phi^{-1}(p).
    \end{equation*}
\end{restatable}
%
% Proof is given in \autoref{app:theorem_randomized_smoothing}.
One should consider $p$ as the probability that the smoothed classifier will assign to the predicted class rather than any other.
As a consequence, if $p > 1/2$, it will continue to do so even if the input is perturbed by Gaussian noise of magnitude smaller than the radius $R = \sigma \Phi^{-1}(p)$.
We observe that as $p$ approaches 1, the radius becomes infinite.
This makes sense as it only occurs when the original classifier always classify as $c$.


\textbf{$L$-Lipschitz Continuity.}
Previous works have attempted to convert the robustness analysis problem into a local or global Lipschitz constant estimation problem~\citep{weng2018evaluating, virmaux2018lipschitz, fazlyab2019efficient}.
However, instead of trying to make neural networks Lipschitz continuous, which is difficult while maintaining high accuracy, \citet{salman} show that randomized smoothing can postprocess the network to make it locally Lipschitz continuous.
So, let us recall the definition of a $L$-Lipschitz continuous function. 
\begin{definition}[$L$-Lipschitz continuous]
    A function $h:\mathbb{R}^d \to \mathbb{R}$ is $L$-Lipschitz continuous in a norm $\norm{\cdot}$ if there exists a constant $L$ such that:
    \begin{equation*}
        \forall \tilde{x}, x \in \R^d, \quad |h(\tilde{x}) - h(x)| \leq L \norm{\tilde{x} - x}.
    \end{equation*}
\end{definition}
%

If $h$ is differentiable, then $h$ is $L$-Lipschitz continuous if $\norm{\nabla h(x)}_* \leq L$ for all $x$, where $\norm{\cdot}_*$ is the dual norm to $\norm{\cdot}$.
The connection between randomized smoothing and Lipschitz continuity is provided in the following lemma, which offers an analytical form of the gradient of a smooth function.

\begin{restatable}{lemma}{lemmastein}[\citet{stein1981estimation}]\label{lemma:stein}
    Let $\sigma>0$, let $h:\R^d \to \R$ be measurable, and let $H(x) = \E_{\delta \sim \set{N}(0, \sigma^2 I)}[h(x+\delta)]$. Then $H$ is differentiable, and moreover:
    \begin{equation*}
        \nabla H(x) = \frac{1}{\sigma^2}\E_{\delta \sim \set{N}(0, \sigma^2 I)} \left[ \delta \cdot h(x + \delta) \right].
    \end{equation*}
\end{restatable}
%Proof is given in \autoref{app:lemma_stein}.
The smoothed function $H$ is also known as the \textit{Weierstrass transform} of $h$, and a classical property of the
Weierstrass transform is its induced smoothness.

\textbf{Diffusion Denoised Smoothing.} 
A number of recent developments in denoising diffusion probabilistic models have led to state-of-the-art results in image generation~\citep{sohl2015deep, ho2020denoising, nichol2021improved}.
In a nutshell, forward diffusion involves adding Gaussian noise to an image until it produces an isotropic Gaussian distribution with a large variance. 
Denoising diffusion probabilistic models work by learning how to reverse this process.
In formal terms, given an input sample $x \in \R^{d}$, a diffusion model selects a predetermined \textit{timestep} $t\in \mathbb{N}^+$  and samples a noisy image $x_t$ as follows:
%
\begin{equation}\label{eq:diffusion}
    x_t \eqdef \sqrt{\alpha_t} \cdot x + \sqrt{1 - \alpha_t} \cdot \set{N}(0, I),    
\end{equation}
%
where the amount of noise to be added to the image is determined by a constant called $\alpha_t$ derived from $t$.

As \citet{salman2020denoised} suggested, denoising Gaussian pertubed images leads to out-of-the-box certified robustness for plain models.
Following this trend~\citet{dds} make use of a diffusion model as one-shot denoiser achiving state-of-the-art performances.
The minor proposed adjustment held in the estimation of $t$, computed such that $\frac{1-\alpha_t}{\alpha_t} = \sigma^2$.
Additionally, the perturbed version $\tilde{x} = x + \delta$ is scaled by $\sqrt{\alpha_t}$, to match the noise model of \autoref{eq:diffusion}.