\section{Introduction}\label{intro}

Although recent advances in Machine Learning (ML) demonstrate its validity in a wide range of applications, its use in safety-critical conditions remains challenging.
Since the appearance of unexpected low robustness to natural~\citep{hendrycks2018benchmarking, engstrom2019exploring} and adversarial~\citep{biggio2013evasion, SzegedyZSBEGF13} perturbations to the input data, several types of defenses have been proposed along the years.
Two main branches of defenses exist: \textit{empirical}~\cite{fgsm, pgd, madry2018towards} and \textit{certified}~\citep{katz2017reluplex, zhang2018efficient, pmlr-v80-wong18a, randomized_smoothing}, which aim at \textit{improving} or \textit{assuring} the robustness of the prediction in the vicinity of the input, respectively.
For the sake of clarity, in this paper we refer to \textit{empirical} robustness as \textit{adversarial} robustness.

\textit{Certified} defenses might give the inaccurate impression that robustness makes ML systems ready for deployment in safety-critical applications.
Unfortunately, the certified robustness of a model does not necessarily mean that its prediction is always accurate~\cite{franco2023understanding}.
Further issues lie also beyond robustness, because deep-learning-based models have numerous problems, including the lack of guarantees for Out-Of-Distribution (OOD) data, the lack of fairness, or the lack of explainability~\cite{paleyes2022challenges}.

In the field of OOD detection, several studies emphasize the importance of ensuring low confidence in the presence of OOD data~\citep{acet, good, prood}.
These methods incorporate both certified defenses and standard OOD detection techniques~\cite{msp, oe, vos, logitnorm}.
Similar to In-Distribution (ID), robustness for OOD detection can be separated into two types: \textit{adversarial} and \textit{certified}.
Adversarial attacks heuristically maximizes the confidence for OOD samples within a $\ell_p$-norm around the input.
If the model can maintain low confidence under these attacks, then it is adversarially robust.
In contrast, certified robustness provides a formal statement of an upper bound for the maximum confidence within the same $\ell_p$-norm.


It is widely recognized that increasing robustness often comes at the expense of reduced accuracy~\cite{hurt_training}.
This applies to robust OOD detection as well.
Recently, a new approach was suggested to improve robustness in the $\ell_\infty$-norm by adding a certified binary discriminator, while maintaining high top-1 accuracy on ID data~\citep{prood}.
However, this method does not provide adequate adversarial and certified robustness for ID data and its results for certified robustness on OOD samples are not yet usable in practice, suggesting the need for new approaches.
%Even though the method has been successful in these efforts, it lacks adversarial and certified robustness for ID data. 
%In addition, results for certified robustness on OOD samples are not yet usable in practice, suggesting the need for new approaches.

\begin{table*}[htb] 
\vspace{-0.5em}
    \centering
    \caption{Comparison between this work and previous methods in terms of ID and OOD robustness properties. In this case, the \checkmark indicates that property was provided in the work. While (\checkmark) indicates that the property is actually lower than expected.}
    \label{tab:contribution}
    \begin{adjustbox}{width=0.95\textwidth,center}
        \begin{tabular}{lcccccccc}
            \toprule
             \multirow{3}{*}{Methods}   &\multicolumn{3}{c}{In-Distribution (ID) Accuracy} &\multicolumn{5}{c}{Out-Of-Distribution (OOD) Detection} \\
             \cmidrule(lr){2-4} \cmidrule(lr){5-9}
             &Clean &Adversarial  &Certified &Clean &Adversarial &\multicolumn{2}{c}{Certified} &Asymptotic \\
             & &$\ell_\infty$ &$\ell_2$ & &$\ell_\infty$ &$\ell_\infty$ &$\ell_2$ &underconfidence \\
            \midrule
            - \textbf{Standard} \\
            OE~\cite{oe}        &\checkmark & & &\checkmark     &   &   &  &  \\
            VOS~\cite{vos}    &\checkmark & & &\checkmark & & & & \\
            LogitNorm~\cite{logitnorm}    &\checkmark & & &\checkmark & & & & \\
            - \textbf{Adversarial} \\
            ACET~\cite{acet} &(\checkmark) &\checkmark & &\checkmark &(\checkmark) & & &   \\
            ATOM~\cite{atom} &(\checkmark) & & &\checkmark     &(\checkmark) & & &  \\
            - \textbf{Guaranteed} \\
            GOOD~\cite{good} &           & & &               &\checkmark  &\checkmark  &  &\checkmark    \\
            ProoD~\cite{prood}  &\checkmark & & &\checkmark &\checkmark &\checkmark & &\checkmark  \\
            DISTRO (Our)        &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
\vspace{-0.5em}
\end{table*}


In this study, we propose a novel technique for certifying OOD detection within the $\ell_2$-norm of the input sample, without requiring the use of binary discriminators or specific training.
This enables us to establish a guaranteed upper bound on the classifier's confidence within a defined region surrounding the input.
Unlike before, certified robust OOD detection can now be computed for standard OOD detection approaches.
Additionally, we enhance and build upon the method of \citet{prood}. 
We incorporate a diffusion denoiser~\cite{nichol2021improved, dds}, which recovers the perturbed images and returns high quality denoised inputs.
This leads to better levels of both adversarial and certified robustness for ID and OOD data.

In summary, our contributions are:
\begin{itemize}
    \item A novel technique to robustly certify the confidence of any classifier within an $\ell_2$-norm on OOD data. 
    We utilize the local Lipshitz continuity obtained through smoothing the classifier. 
    This yields a certified upper bound on the confidence.
    This method can be applied to any architecture and does not require additional components, even though it has higher computational costs compared to previous approaches.
    \item A method named DISTRO: \textbf{DI}ffusion denoised \textbf{S}moo\textbf{T}hing for \textbf{R}obust \textbf{O}OD detection.
    This method incorporates a diffusion denoiser model to improve the detection of adversarial and certified OOD samples, while providing high adversarial and certified accuracy for ID data.
\end{itemize}