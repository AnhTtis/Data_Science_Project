\section{Related Work}\label{sec:related}

This work and previous methods are compared in \autoref{tab:contribution}.


% \textbf{Randomized Smoothing.}
% One of the most advanced certification techiniques is \textit{randomized smoothing}~\citep{randomized_smoothing, randomized_smoothing_2}, which adds isotropic Gaussian noise to the input data before making predictions. 
% By taking a majority vote over the outputs of the classifier on the noisy input versions, randomized smoothing can certify the robustness under the $\ell_2$-norm.
% The training of certified adversarially robust deep learning models requires specialized techniques designed explicitly to obtain a provably robust classification \cite{randomized_smoothing}. 
% These models, however, are extremely difficult to train to high accuracy and significantly degrade clean accuracy.
% Recently, \citet{dds} have proposed using diffusion models as a way to denoise the images after the addition of Gaussian noise. 
% This has been shown to achieve state-of-the-art certified robustness with plain classifiers.

\textbf{Standard OOD Detection.}
Standard OOD detection is the process of identifying inputs that do not conform to the training distribution and therefore the model should abstain from making a prediction. 
\citet{msp} introduced the Maximum Softmax Probability (MSP) as a baseline method for OOD detection. 
Assuming that ID samples should have a higher softmax probability (or confidence in the prediction) than OOD samples, a threshold is determined to distinguish between ID and OOD samples.
Outlier Exposure (OE)~\cite{oe} is a popular training technique.
The basic idea is to train a model to distinguish between ID and OOD samples by exposing it to a large number of OOD samples during training. 
Afterwards, the model can be evaluated on new, unseen OOD samples.
Two recent approaches are VOS~\cite{vos} and LogitNorm~\cite{logitnorm}. 
Virtual Outlier Synthesis (VOS) is an original framework for detecting OOD by adaptively synthesizing virtual outliers during training.
In contrast, LogitNorm assumes that the logit (pre-softmax) vector's norm increases during training, leading to overconfident predictions. 
Therefore, it normalizes the logit vector by the $\ell_2$-norm to reduce this increase.
Additionally, thresholding the normalized logit vector can be used as a simple and effective test to detect OOD samples.


\textbf{Adversarial OOD Detection.}
Other lines of research, such as \citet{acet, ccu, atom}, focus on providing low confidence for OOD data when perturbed with adversarial noise.
The authors of ~\citet{acet} show that ReLU networks can have arbitrarily high confidence for data that is \textit{far enough} from the training distribution. 
They also propose an adversarial training method to enforce low confidence on OOD data, but at the cost of decreased ID accuracy.
ATOM~\cite{atom} addresses this issue by using outlier mining techniques to automatically select a diverse set of OOD samples from a large pool of potential OOD samples.


\textbf{Guaranteed OOD Detection.}
Recent studies like \citet{good, prood} bring forth $\ell_\infty$-norm certified robustness for OOD data with a simple but effective method: Interval Bound Propagation (IBP)~\citep{ibp, ibp2}.
First, \citet{good} proposed a training approach to derive a provable upper bound on the maximum confidence of the network in an $\ell_\infty$-norm of $\epsilon$ around a given sample using IBP.
Although this method leads to classifiers with pointwise certified robustness even for near-OOD samples, IBP can produce loose bounds that result in a drop in network accuracy~\citep{hurt_training}.
Secondly, to overcome this problem, ProoD~\citep{prood} combines a certified binary discriminator, to provide adversarial and certified robustness on OOD data, with an OE model which preserves high clean accuracy.
Although they achieve state-of-the-art performance in different OOD metrics and test distributions, the results are not yet useful in practice as most are still below 60\%.
Other downsides of this approach are: (i) it is not robust to adversarial ID attacks and has low certified ID accuracy, (ii) it does require external datasets, which are not only hard to obtain and filter, but also the generalization from a single OOD dataset to the entire OOD space is often not given, and (iii) training with IBP hurts the performance of larger discriminator models, making the method less prone to scale.