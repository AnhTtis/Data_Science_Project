\section{Certified Robust OOD Detection}\label{sec:method}

This section explains how using local Lipschitz continuity, achieved through smoothing the classifier with Gaussian noise, can guarantee the detection of OOD samples within a $\ell_2$-sphere around the input. 


\textbf{Preliminaries.} 
To determine how well a classifier distinguishes between ID and OOD samples, it is common to threshold the confidence level and to calculate the area under the receiver operating characteristic curve (AUROC or AUC).
Formally, let us consider a function\footnote{e.g. the Maximum Softmax Probability~\cite{msp}, or the Energy function~\cite{energy}.} $h\in\R^d \to \R$, the AUC is defined as:
\begin{equation*}
    \text{AUC}_h (\set{D}_{in}, \set{D}_{out}) = \E_{
    \begin{subarray}{l} x \sim \set{D}_{in}, \\ 
    z \sim \set{D}_{out}\end{subarray}} 
    \left[\mathbbm{1}_{h(x) > h(z)}\right],
\end{equation*}
where $\set{D}_{in}, \set{D}_{out}$ are ID and OOD data sets, respectively, and $\mathbbm{1}$ returns 1 if the argument is true and 0 otherwise.
A number of prior works~\cite{ccu, good, atom, prood} also investigated the worst-case AUC (WCAUC), which is defined as the lowest AUC attainable when every OOD sample is perturbed so that the highest level of confidence is achieved within a specific threat model.
Specifically, the WCAUC is defined as:
\begin{equation*}
    \text{WCAUC}_h (\set{D}_{in}, \set{D}_{out}) = \E_{
    \begin{subarray}{l} x \sim \set{D}_{in}, \\ 
    z \sim \set{D}_{out}\end{subarray}}
    \left[\mathbbm{1}_{h(x) > \underset{\norm{\tilde{z}-z}_p \leq \epsilon}{\max} h(\tilde{z}) } \right].
\end{equation*}

Due to the intractable nature of the maximization problem, we can compute upper or lower bounds only, i.e. $\underline{h}(z) \leq \max_{\norm{\tilde{z}-z}_p \leq \epsilon} h(\tilde{z}) \leq \bar{h}(z)$.
The lower bound $\underline{h}(z)$ is typically calculated using projected gradient methods~\citep{pgd, apgd} and named Adversarial AUC (AAUC) (upper bound of WCAUC).
In the context of $\ell_\infty$-norm, the upper bound $\bar{h}(z)$, called Guaranteed AUC (GAUC) (lower bound of WCAUC), is computed using IBP in \citet{good} and \citet{prood}.

Here, we propose a method for computing the upper bound of any classifier without the need for special training or modifications.
Thus, the main theorem for an $\ell_2$-norm robustly certified upper bound is stated.

\begin{restatable}{theorem}{upperbound}\label{th:upper_bound}
Let $F:\sR^d \to \prob{(\set{Y})}$ be any soft classifier and $G$ be its associated smooth classifier as defined in ~\autoref{eq:smooth_classifier}, with $\sigma > 0$.
If $p = \max_{y\in\set{Y}} G(x)_y > 1/2$, then, we have that:
\begin{equation}
    \max_{y\in \set{Y}} G(x+\delta)_y \leq \sqrt{\frac{2}{\pi}} \Phi^{-1}(p) + p,
\end{equation}
for every $\norm{\delta}_2 < \sigma \Phi^{-1}(p)$.
\end{restatable}
%
\begin{proof}
As a prerequisite to proving the theorem, we need to know the analytic form of the gradient of a smoothed function given in \autoref{lemma:stein}.
Let us consider the soft classifier $F(x):\R^d \to \prob(\set{Y})$, and its smooth version $G(x) = \E_{\delta \sim \set{N}(0, \sigma^2 I)} [F(x + \delta)]$, with $\sigma > 0$.
Since $F$ its a measurable function, we consider the \textit{Weierstrauss} transform of $F$ (which coincide with the \textit{smooth} version of $F$):
\begin{equation*}
    \E_{\delta \sim \set{N}(0, \sigma^2I)}\left[F(x+\delta)\right] = \left(F * \set{N}(0, \sigma^2I)\right)(x),
\end{equation*}
where $*$ denotes the convolution operator.
Thus, $G(x)$ is differentiable and from \autoref{lemma:stein} we have:
%
\begin{equation*}
    \nabla G(x) = \frac{1}{\sigma^2}\E_{\delta \sim \set{N}(0, \sigma^2 I)} \left[ \delta \cdot h(x + \delta) \right].
\end{equation*}
%
Since $F:\R^d \to [0, 1]$ and $\ell_2$ is self-dual, it is sufficient to show that the gradients of $G$ are bounded in $\ell_2$. 
From \autoref{lemma:stein}, for any unit vector $v\in \R^d$ we have that $|\langle v, \nabla G(x) \rangle|$ is equal to:
\begin{equation*}
    \begin{aligned}
    &\left\lvert \frac{1}{(2\pi \sigma^2)^{\nicefrac{d}{2}}} \int_{\sR^{d}} F(t) \left\langle v, \frac{t - x}{\sigma^2} \right\rangle e^{\left(-\frac{1}{2\sigma^2} \norm{x-t}_2^2 \right)} dt \right\rvert, \\
    &\leq \frac{1}{(2\pi \sigma^2)^{\nicefrac{d}{2}}} \int_{\sR^{d}} \left\lvert \left\langle v, \frac{t - x}{\sigma^2} \right\rangle \right\rvert e^{\left(-\frac{1}{2\sigma^2} \norm{x-t}_2^2 \right)} dt,  \\
    \end{aligned}
\end{equation*}
where we make use of the triangle inequality and know that $F$ is bounded by 1.
Given that projections of Gaussians are Gaussians and from the classical integration of the Gaussian density, we obtain:
\begin{equation*}
    \begin{aligned}
        &\frac{1}{(2\pi \sigma^2)^{\nicefrac{d}{2}}} \int_{\sR^{d}} \left\lvert \left\langle v, \frac{t - x}{\sigma^2} \right\rangle \right\rvert e^{\left(-\frac{1}{2\sigma^2} \norm{x-t}_2^2 \right)} dt, \\
        =& \frac{1}{\sigma^2}\E_{Z\sim \set{N}(0, \sigma^2)} \left[\lvert Z \rvert\right] = \sqrt{\frac{2}{\pi\sigma^2}},
    \end{aligned}
\end{equation*}
where we consider the supremum over all unit vectors $v$.
Since, we know that $G(x)$ is $\sqrt{\frac{2}{\pi\sigma^2}}$-Lipschitz in $\ell_2$, it is possible to use the Lipschitz constant to bound the difference between $G(x + \delta)$ and $G(x)$ for any value of $\delta$, with $\norm{\delta}_2 < \sigma \Phi^{-1}(p)$, where $p = \max_{y\in\set{Y}} G(x)_y$.
Formally:
%
\begin{equation*}
    |G(x + \delta)| \leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2 + |G(x)|,
\end{equation*}
%
where we make use of the reverse triangle inequality.
Since $G(x):\sR^d \to [0, 1]$, we can assume $|G(x)|=G(x)$, and moreover:
\begin{equation*}
    \max_{y \in \set{Y}} G(x + \delta)_y \leq \sqrt{\frac{2}{\pi}}\Phi^{-1}(p) + \max_{y \in \set{Y}} G(x)_y.
\end{equation*}

\end{proof}
In other words, if the smooth classifier assigns the most likely class more than half the time, it is locally Lipschitz continuous in $x$, and its maximum prediction is bounded within a radius smaller than $R = \sqrt{\frac{2}{\pi}} \Phi^{-1}(p)$.

\subsubsection*{Discussion}

While this theorem provides some advantages, it is important to note a couple of its limitations.
One of the main limitations is that the upper bound of the smooth classifier $G$ only applies to $G$ and not to the original classifier $F$. 
As a result, the guarantee only applies to $G$, and its robustness at a given input point $x$ cannot be precisely evaluated or certified. 
To overcome this, Monte Carlo algorithms can be used to approximate these evaluations with high probability~\cite{randomized_smoothing}.

Another limitation is that the guarantees provided by this theorem are only probabilistic in practice. 
Therefore, a hypothesis test~\cite{hypothesis_test} should be used to avoid making predictions with low confidence. 
As with randomized smoothing~\cite{randomized_smoothing}, a large number of samples must be generated in order to achieve high levels of confidence in the certification radius. 
However, generating these samples can be computationally expensive for complex models.

Despite these limitations, the theorem provides a novel way of calculating the upper bound of any classifier, without the need for special training or modification.
Additionally, we provide a tighter certificate compared to previous approaches \cite{good, prood}, as they used IBP.
This can be useful for evaluating the certified robustness of a broader category of standard OOD detection methods as well as larger models, where IBP bounds explode in size and make them unusable~\cite{hurt_training}.
