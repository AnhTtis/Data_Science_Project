\section{Introduction \& Related Work}\label{intro}

Although recent advances in Machine Learning (ML) demonstrate its validity in a wide range of applications, its use in safety-critical conditions remains challenging.
Since the appearance of unexpected low robustness to natural~\citep{hendrycks2018benchmarking} and adversarial~\citep{pgd} perturbations to the input data, several types of defenses have been proposed along the years.
Two main branches of defenses exist: \textit{empirical}~\cite{madry2018towards} and \textit{certified}~\citep{randomized_smoothing}, which aim at \textit{improving} or \textit{assuring} the robustness of the prediction in the vicinity of the input, respectively.
\textit{Certified} defenses might give the inaccurate impression that robustness makes ML systems ready for deployment in safety-critical applications.
Unfortunately, further issues lie also beyond robustness, including the lack of guarantees for Out-Of-Distribution (OOD) data, the lack of fairness, or the lack of explainability~\cite{paleyes2022challenges}.


\textbf{OOD Detection.}
With Maximum Softmax Probability (MSP)~\cite{msp} as a baseline method, OOD detection aims to identify inputs that fall outside the scope of the training distribution.
Outlier Exposure (OE)~\cite{oe} trains models to differentiate between in-distribution (ID) and out-of-distribution (OOD) samples. 
Recent approaches include Virtual Outlier Synthesis (VOS)~\cite{vos} and LogitNorm~\cite{logitnorm}. 
VOS adaptively synthesizes virtual outliers, while LogitNorm normalizes the logit vector to reduce overconfidence, using thresholding for OOD detection.

\textbf{Adversarial OOD Detection.}
Other lines of research~\cite{acet, ccu, atom}, focus on providing low confidence for OOD data when perturbed with adversarial noise.
\citet{acet} show that ReLU networks can have arbitrarily high confidence for data that is \textit{far enough} from the training distribution. 
Additionally, they propose ACET~\cite{acet}, an adversarial training method to enforce low confidence on OOD data, but at the cost of decreased ID accuracy.
ATOM~\cite{atom} addresses this issue by using outlier mining techniques to automatically select a diverse set of OOD samples from a large pool of potential OOD samples.


\textbf{Guaranteed OOD Detection.}
Recent studies like \citet{good, prood} bring forth $\ell_\infty$-norm certified robustness for OOD data with a simple but effective method: Interval Bound Propagation (IBP)~\citep{ibp}.
GOOD~\cite{good} proposes a training approach using IBP, but it produce loose bounds, impacting accuracy. 
While ProoD~\cite{prood} combines a certified discriminator and OE model, achieving state-of-the-art performance but with practical limitations: low certified accuracy, reliance on external datasets, and reduced scalability due to IBP's impact on larger models.


\begin{table*}[htb] 
\vspace{-0.5em}
    \centering
    \caption{Comparison between this work and previous methods in terms of ID and OOD robustness properties. In this case, the \checkmark indicates that property was provided in the work. While (\checkmark) indicates that the property is actually lower than expected.}
    \label{tab:contribution}
    \begin{adjustbox}{width=0.8\textwidth,center}
        \begin{tabular}{lcccccccc}
            \toprule
             \multirow{3}{*}{Methods}   &\multicolumn{3}{c}{In-Distribution (ID) Accuracy} &\multicolumn{5}{c}{Out-Of-Distribution (OOD) Detection} \\
             \cmidrule(lr){2-4} \cmidrule(lr){5-9}
             &Clean &Adversarial  &Certified &Clean &Adversarial &\multicolumn{2}{c}{Certified} &Asymptotic \\
             & &$\ell_\infty$ &$\ell_2$ & &$\ell_\infty$ &$\ell_\infty$ &$\ell_2$ &underconfidence \\
            \midrule
            - \textbf{Standard} \\
            OE~\cite{oe}        &\checkmark & & &\checkmark     &   &   &  &  \\
            VOS~\cite{vos}    &\checkmark & & &\checkmark & & & & \\
            LogitNorm~\cite{logitnorm}    &\checkmark & & &\checkmark & & & & \\
            - \textbf{Adversarial} \\
            ACET~\cite{acet} &(\checkmark) &\checkmark & &\checkmark &(\checkmark) & & &   \\
            ATOM~\cite{atom} &(\checkmark) & & &\checkmark     &(\checkmark) & & &  \\
            - \textbf{Guaranteed} \\
            GOOD~\cite{good} &           & & &               &\checkmark  &\checkmark  &  &\checkmark    \\
            ProoD~\cite{prood}  &\checkmark & & &\checkmark &\checkmark &\checkmark & &\checkmark  \\
            DISTRO (Our)        &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark &\checkmark \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
% \vspace{-1em}
\end{table*}


In this study, we propose a novel technique for certifying OOD detection within the $\ell_2$-norm of the input sample, without requiring the use of binary discriminators or specific training.
This enables us to establish a guaranteed upper bound on the classifier's confidence within a defined region surrounding the input.
Unlike before, certified robust OOD detection can now be computed for standard OOD detection approaches.
Additionally, we incorporate a diffusion denoiser~\cite{nichol2021improved, dds}, which recovers the perturbed images and returns high quality denoised inputs.
This leads to better levels of both adversarial and certified robustness for ID and OOD data. This work and previous methods are compared in \autoref{tab:contribution}.

In summary, our contributions are:
\begin{itemize}
    \item A novel technique to robustly certify the confidence of any classifier within an $\ell_2$-norm on OOD data.
    This technique can be applied to any architecture and does not require additional components, even though it has higher computational costs compared to previous approaches.
    \item A method named DISTRO: \textbf{DI}ffusion denoised \textbf{S}moo\textbf{T}hing for \textbf{R}obust \textbf{O}OD detection.
    This method incorporates a diffusion denoiser model to improve the detection of adversarial and certified OOD samples, while providing high adversarial and certified accuracy for ID data.
\end{itemize}

% Code at \url{https://github.com/FraunhoferIKS/distro}.