
\subsection{Evaluation Metrics}


To discriminate between ID and OOD samples, we use the confidence of the classifier, i.e. MSP~\cite{msp}. 
Traditionally, the following metrics are used to evaluate the OOD detection performance: 
(i) false positive rate (FPR95) of OODs when ID samples have a 95\% true positive rate;
(ii) the area under the receiver operating characteristic curve (AUROC or AUC); and
(iii) the area under the precision-call curve (AUPR).
In order to determine robustness, we compare adversarial (AAUC, AAUPR, AFPR) and guaranteed (GAUC, GAUPR, GFPR) versions of the previous metrics.
For the adversarial metrics, we use the settings in \citet{prood} to ensure a fair comparison.

\begin{figure}[htb]
% \vspace{-1em}
    \centering
    \includegraphics[width=0.45\textwidth]{graphics/average_distro.pdf}    
    \vspace{-1em}
    \caption{Kernel density estimation (bandwidth = 1) of the distribution of certified smooth ($\sigma = 0.12$) scores for DISTRO on ID (\texttt{CIFAR10}) and OOD (all other datasets) samples.}
    \label{fig:distribution_of_certified_scores}
    % \vspace{-1em}
\end{figure}

\textbf{Guaranteed.}
The guaranteed metrics (GAUC, GAUPR and GFPR) are computed for $\ell_2$ and $\ell_\infty$ norms robustness certificates.
Similarly to \citet{prood}, the $\ell_\infty$-norm is obtained with IBP only on OOD data.
On the other hand, the $\ell_2$-norm is computed with \autoref{th:upper_bound} on both ID and OOD data.
Similarly to \autoref{sec:id-results}, we sampled 10'000 Gaussian data points around the input with a deviation $\sigma = 0.12$. 
Since, the certified bound is only probabilistic in practice, we ran a binomial proportion confidence test~\cite{binomial_test} with failure probability of $0.001$.
We have assigned a score of 0 to all samples that fail to be certified, i.e. with $p \leq 1/2$. 
The Lipschitz continuity does not hold in the case of non-certified samples, therefore we are unable to bound the score.
To ensure a fair comparison, we decided to compute the $\ell_2$-norm GAUC on both ID and OOD datasets.

In \autoref{fig:distribution_of_certified_scores}, we plot the normalized frequency of occurrences of the certified upper bound ($\sqrt{2/\pi} \cdot \Phi^{-1}(p) + p$) for ID versus OOD data of DISTRO.
We observe that OOD data tend to peak close to zero, while ID data are spread out with larger values. 
This suggests that a large radius is more likely to be associated with ID data versus OOD samples.
As a result, robustly certifying the detection of OOD samples becomes more feasible.


\subsection{Out-Of-Distribution Results}

\begin{table*}[htb]
\vspace{-0.5em}
    \centering
    \caption{\textbf{Robust OOD detection.} We consider the following metrics: clean top-1 accuracy on \texttt{CIFAR10/100} test sets, clean AUC, guaranteed (GAUC), adversarial AUC (AAUC), clean AUPR, guaranteed AUPR (GAUPR), adversarial AUPR (AAUPR), clean FPR\@95\% (FPR), guaranteed FPR\@95\% (GFPR) and adversarial FPR\@95\% (AFPR). 
    Averaging was performed on a variety of OOD datasets. We consider MSP~\cite{msp} for all methods and metrics (with temperature $T=1$). The guaranteed $\ell_2$-norm is computed for $\sigma = 0.12$ for all $R>0$, while the adversarial and guaranteed $\ell_\infty$-norm are computed for $\epsilon = 0.01$. The grayed-out models have an accuracy drop greater than $3\%$ relative to the model with the highest accuracy. \textbf{Bold} numbers are superior results.}
    \label{tab:ood_average}
    \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{lr|rrrr|rrrr|rrrr}
    \toprule
        ID: CIFAR10 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
        - \textbf{Standard} \\
        Plain$^*$   &95.01  &94.56  &48.86  &0.00 &24.52  &99.42     &60.05   &0.00    &82.30  &35.72  &100.0  &100.0     &96.72 \\
        OE$^*$      &\textbf{95.53} &\textbf{98.78}  &46.88 &0.00 &37.91  &\textbf{99.87}  &63.08 &0.00 &84.49  &\textbf{4.71}  &100.0   &100.0     &70.26 \\
        VOS$^\dag$ &94.62  &90.82 &30.13 &0.00 &20.62 &99.15 &41.62 &0.00 &81.80 &61.66 &94.10 &100.0 &100.0 \\
        LogitNorm$^\ddag$ &94.48  &96.71 &40.73 &0.00 &39.76 &99.64 &49.31 &0.00 &86.47 &13.95 &100.0 &100.0 &91.10 \\
        - \textbf{Adversarial} \\
        \gray{ACET$^*$} &\gray{91.48} &\gray{97.24} &\gray{60.21} &\gray{0.00} &\gray{93.01} &\gray{99.68} &\gray{76.22} &\gray{0.00} &\gray{99.16} &\gray{13.82} &\gray{95.65} &\gray{100.0} &\gray{32.15} \\
        \gray{ATOM$^*$} &\gray{92.33} &\gray{98.82} &\gray{97.15} &\gray{0.00} &\gray{44.65} &\gray{99.86} &\gray{95.51} &\gray{0.00} &\gray{85.74} &\gray{4.14}  &\gray{5.04}  &\gray{100.0} &\gray{62.65} \\
        \multicolumn{1}{l}{- \textbf{Guaranteed}} \\
        \gray{GOOD$^*_{80}$}   &\gray{90.13} &\gray{93.12} &\gray{36.45} &\gray{57.52} &\gray{78.11} &\gray{99.22}  &\gray{52.31} &\gray{89.54} &\gray{95.19} &\gray{30.00}  &\gray{100.0} &\gray{72.45} &\gray{47.55} \\
        ProoD$^*\Delta=3$ &95.46 &98.72 &52.36 &\textbf{59.56} &64.22 &\textbf{99.87}  &66.53 &\textbf{93.89} &94.52 &5.49  &100.0  &100.0 &86.49 \\
        DISTRO (our)     &95.47 &98.72 &\textbf{88.97} &59.53 &\textbf{83.24} &\textbf{99.87}  &\textbf{92.75}  &\textbf{93.89} &\textbf{97.32} &5.29 &\textbf{67.86} &100.0 &\textbf{34.56} \\
        \midrule
        ID: CIFAR100 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
        - \textbf{Standard} \\
        Plain$^*$   &\textbf{77.38} &81.60 &30.63 &0.00 &16.98 &97.84 &45.10 &0.00 &81.27 &82.52 &100.0 &100.0 &100.0 \\
        OE$^*$      &77.28 &90.41 &39.87 &0.00 &22.79 &98.90 &49.46 &0.00 &81.96 &47.49 &100.0 &100.0 &87.74 \\
        - \textbf{Adversarial} \\
        ACET$^*$ &74.47 &90.27 &36.36 &0.00 &27.68 &98.84 &43.50 &0.00 &82.60 &44.11 &\textbf{90.41} &100.0 &74.99 \\
        \gray{ATOM}$^*$ &\gray{71.73} &\gray{91.72} &\gray{84.38} &\gray{0.00} &\gray{31.52} &\gray{98.88} &\gray{79.95} &\gray{0.00} &\gray{83.36}      &\gray{30.81} &\gray{30.09}  &\gray{100.0} &\gray{73.69} \\
        - \textbf{Guaranteed} \\
        ProoD$^* \Delta=1$   &76.79 &\textbf{90.90} &42.83 &\textbf{37.67} &43.81 &\textbf{98.91} &50.90 &\textbf{89.66} &90.46 &42.12 &100.0 &100.0 &97.11 \\
        DISTRO (our) &76.78 &90.89 &\textbf{59.39} &37.53  &\textbf{62.77} &98.90 &\textbf{69.41} &89.63 &\textbf{93.59}  &\textbf{40.94} &100.0  &100.0 &\textbf{58.58} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \footnotesize{$*$ Pre-trained models from \citet{prood}, $\dagger$ Pre-trained from \citet{vos}, $\ddag$ Pre-trained from \citet{logitnorm}.
    }
  \vspace{-1em}
\end{table*}


Here, we describe the results shown in \autoref{tab:ood_average}.
As previously, we grayed-out models with an accuracy drop greater than $3\%$ with respect to the model with highest accuracy.
The objective of this choice is to prioritize clean ID accuracy over all other metrics.
A comparison of the remaining metrics is then made on an equal basis.
Despite this, there is no direct comparison between the GAUC of $\ell_2$ and $\ell_\infty$ norms.
This is primarily due to the fact that the guaranteed upper bound of $\ell_\infty$ is computed only for OOD data, whereas $\ell_2$ is computed for both (ID \& OOD).
Additionally, we choose any radius $R>0$ for $\ell_2$, while for $\ell_\infty$, $\epsilon$ is fixed to $0.01$\footnote{This problem can be addressed by considering $R \geq \sqrt{d}\cdot \epsilon$.}.

We observe that the performances of LogitNorm and VOS on clean AUC, AUPR and FPR are suboptimal.
The reason for this is that we are evaluating MSP~\cite{msp} instead of the suggested normalization~\cite{logitnorm} and energy~\cite{vos} functions for LogitNorm and VOS, respectively.
To ensure a fair comparison we decided to standardize the output function across all models.
On \texttt{CIFAR100}, only the most effective methods of \texttt{CIFAR10} have been tested.

\textbf{Outcomes.}
In light of these considerations, we note that OE achieved the highest clean AUC, AUPR, and FPR.
In case of AAUC, ACET shows the best results for \texttt{CIFAR10}.
While ATOM achieves close to optimal performance for the guaranteed $\ell_2$-norm AUC, AUPR and FPR.
Both methods are trained adversarially on outliers, which makes them more robust on OOD data, but at the expense of a reduced clean accuracy.


\begin{wraptable}{l}{4.2cm}
    % \vspace{-1em}
    \centering
    \caption{\small Overall average between the metrics of \autoref{tab:ood_average} for \texttt{CIFAR10/100} (\texttt{C-10}, \texttt{C-100}).}
    \label{tab:ood_overall}
    % \vspace{0.3em}
    \begin{adjustbox}{width=0.25\textwidth,center}
    \begin{tabular}{lrr}
    \toprule
    Method      &\multicolumn{2}{c}{Average} \\
                &\texttt{C-10} &\texttt{C-100} \\
    \midrule
     Plain              &44.02 &34.48 \\
     OE                 &50.12 &40.42 \\
     VOS                &38.60 &- \\
     LogitNorm          &46.31 &- \\
     ACET               &59.64 &41.86 \\
     ATOM               &64.79 &54.38 \\
     GOOD$_{80}$        &64.74 &- \\
     ProoD $\Delta=3$   &64.09 &52.51 \\
     DISTRO (our)       &\textbf{77.08} &\textbf{59.95} \\
     \bottomrule
    \end{tabular}
    \end{adjustbox}
    % \vspace{-1em}
\end{wraptable}


Similarly to the ID results, DISTRO demonstrates the potential benefits of diffusion models to augment the model robustness in terms of $\ell_2$-norm guaranteed and adversarial AUCs.
Although there is a slight decrease in $\ell_\infty$-norm GAUC, GAUPR and GFPR, which could likely be suppressed by fine tuning the classifier in conjunction with the denoiser.
In \autoref{tab:ood_overall}, we average all the metrics of \autoref{tab:ood_average} for \texttt{CIFAR10} (including clean ID accuracy).
Surprisingly, ATOM shows similar results as ProoD and GOOD. 
This can be related to the high certification radius obtained for GAUC of $\ell_2$-norm. 
% We report each individual dataset's results in \autoref{app:experiments}.


% In \autoref{app:standardized},  an additional comparison on a similar network architecture for all methods is presented.

\subsubsection{Similar Model Capacity}\label{app:standardized}

Here, we outline the configurations and results of \autoref{tab:ood_average_standard}. 
Each technique is evaluated using the same architecture, acknowledging that the results from \autoref{tab:ood_average} do not depend just on the performance of the method, but also on the robustness of the model and the specific OOD dataset utilized.
Therefore we retrain all presented methods using a ResNet18~\cite{resnet} architecture for \texttt{CIFAR10} and \texttt{CIFAR100} respectively. 
For methods that require an additional OOD dataset for training, such as OE~\cite{oe}, ACET~\cite{acet}, ATOM~\cite{atom}, ProoD~\cite{prood} and DISTRO, we use the same subset of \texttt{OpenImages}~\cite{openimages} containing 50'000 images.
Furthermore, we consider an input normalization of $0.5$ across all dimensions for both mean and standard deviation.
In addition, we attempt to be as minimally intrusive as possible when it comes to the default training procedure.


For Plain, OE and LogitNorm we run the implementation\footnote{\href{https://github.com/Jingkang50/OpenOOD}{https://github.com/Jingkang50/OpenOOD}} from \citet{yang2022openood} and leave the hyperparameters unchanged.
Similarly for ACET and ATOM, we only change the model architecture and normalization and run both  implementations from ATOM\footnote{\href{https://github.com/jfc43/informative-outlier-mining}{https://github.com/jfc43/informative-outlier-mining}}.
Lastly, we train ProoD\footnote{\href{https://github.com/AlexMeinke/Provable-OOD-Detection}{https://github.com/AlexMeinke/Provable-OOD-Detection}} from \citet{prood} using their training configuration files, where the discriminator is trained for 1000 epochs and the bias shift ($\Delta$) is 3/1 for \texttt{CIFAR10/100}, respectively.


% All images are normalized with mean and standard deviation set to $0.5$ across all channels. 
% For Plain, Outlier Exposure and LogitNorm we use RandomHorizontalFlip and RandomCrop as augmentation techniques. 
% We optimize these models using Stochastic Gradient Descent for 100 epochs, with an initial learning rate of $0.1$, annealing to $10^{-6}$ over the course of training, momentum of $0.9$ and weight\_decay of $0.0005$. 
% The training batch size is $128$ for all methods.
% In general for ATOM and ACET, we try to follow the hyperparameters from the Github implementation. 
% For ACET and ATOM the training is mostly similar, but we use SGD with weight decay of $0.0001$ and a the initial learning rate of $0.1$ is multiplied by $0.1$ at epoch 50, 75 and 90. 
% The PGD attack in ACET is conducted using $\eps=2/255$ for 5 epochs using a step size of 2. 
% To select the outliers in ATOM, we use a quantile size of $1/8$.
% For ProoD, we follow their original implementation and just change add the normalization of $0.5$ for both mean and standard deviation.

\begin{table*}[htb]
\vspace{-0.5em}
    \centering
    \caption{\textbf{Robust OOD detection with ResNet18.} We consider the following metrics: clean top-1 accuracy on \texttt{CIFAR10/100} test sets, clean AUC, guaranteed (GAUC), adversarial AUC (AAUC), clean AUPR, guaranteed AUPR (GAUPR), adversarial AUPR (AAUPR), clean FPR\@95\% (FPR), guaranteed FPR\@95\% (GFPR) and adversarial FPR\@95\% (AFPR). 
    Averaging was performed on a variety of OOD datasets. We consider MSP~\cite{msp} for all methods and metrics (with temperature $T=1$). The guaranteed $\ell_2$-norm is computed for $\sigma = 0.12$ for all $R>0$, while the adversarial and guaranteed $\ell_\infty$-norm are computed for $\epsilon = 0.01$. The grayed-out models have an accuracy drop greater than $3\%$ relative to the model with the highest accuracy. \textbf{Bold} numbers are superior results.
    } 
    \label{tab:ood_average_standard}
    \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{lr|rrrr|rrrr|rrrr}
    \toprule
        ID: CIFAR10 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
 		Plain &94.32 &92.28 &35.81 &0.00 &23.71 &99.00 &46.83 &0.00 &82.00 &40.21 &93.56 &100.0 &98.88 \\
		LogitNorm &94.71 &95.58 &34.19 &0.00 &35.00 &99.54 &49.63 &0.00 &85.14 &33.06 &95.12 &100.0 &92.20\\
		OE &92.41 &97.35 &50.56 &0.00 &37.95 &99.71 &62.25 &0.00 &85.51 &13.44 &100.0 &100.0 &74.91\\
		ACET &93.66 &\textbf{97.86} &37.45 &0.00 &65.21 &\textbf{99.75} &50.26 &0.00 &91.99 &8.94 &100.0 &100.0 &\textbf{50.29} \\
		\gray{ATOM} &\gray{91.90} &\gray{98.12} &\gray{97.98} &\gray{97.63} &\gray{62.79} &\gray{99.78} &\gray{98.16} &\gray{99.78} &\gray{91.49} &\gray{8.7} &\gray{9.42} &\gray{0.00} &\gray{51.56} \\
		ProoD &\textbf{95.20} &96.91 &44.95 &\textbf{63.44} &64.61 &99.63 &60.27 &\textbf{94.37} &94.42 &\textbf{16.03} &100.0 &\textbf{91.90} &78.22 \\
		DISTRO (our) &\textbf{95.20} &96.80 &\textbf{86.63} &59.86 &\textbf{71.70} &99.62 &\textbf{90.80} &93.78 &\textbf{95.72} &16.55 &\textbf{66.88} &99.96 &67.59\\
        \midrule
        ID: CIFAR100 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
		Plain &77.54 &84.50 &38.11 &0.00 &24.17 &98.16 &44.96 &0.00 &82.32 &67.61 &100.0 &100.0 &98.04 \\
		LogitNorm &76.25 &84.06 &40.93 &0.00 &47.64 &98.04 &46.80 &0.00 &87.25 &73.70 &100.0 &100.0 &87.98\\
		OE &75.84 &88.96 &38.90 &0.00 &17.90 &\textbf{98.72} &48.82 &0.00 &81.43 &49.61 &100.0 &100.0 &99.41\\
		\gray{ACET} &\gray{73.71} &\gray{95.65} &\gray{42.03} &\gray{0.00} &\gray{52.49} &\gray{99.44} &\gray{48.54} &\gray{0.00} &\gray{89.23} &\gray{13.96} &\gray{100.0} &\gray{100.0} &\gray{60.39} \\
		ProoD &\textbf{77.77} &\textbf{89.47} &40.72 &\textbf{37.68} &49.16 &98.66 &49.97 &\textbf{89.66} &91.08 &\textbf{40.44} &100.0 &100.0 &84.15 \\
		DISTRO (our) &77.73 &88.90 &\textbf{55.57} &29.71 &\textbf{51.89} &98.60 &\textbf{67.62} &87.44 &\textbf{91.71} &43.24 &100.0 &100.0 &\textbf{79.34} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
  \vspace{-1em}
\end{table*}



\subsubsection*{Discussion}

It is evident that the $\ell_2$-norm GAUC (and GAUPR) diverge from zero when standard OOD detection models are considered. 
This illustrates the potential of the $\ell_2$-norm to provide certified OOD detection for any method and architecture.
Consequently, it facilitates the experimental evaluation of new robust OOD detection algorithms (both adversarial and certified).

As a side note, the one-shot denoiser appears to improve robustness certification metrics while not compromising clean metrics, such as AUC. 
In some cases, it also appears to be slightly better, even though the denoising process should produce images that are as similar as possible to those considered during training.
This is because a single shot of denoising does not compromise the OOD sample or generate an allucinated one.
Additionally, one-shot denoising introduces so little variance that in this benchmark, the results were similar across multiple runs.