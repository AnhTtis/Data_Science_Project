\section{Experiments}\label{sec:experiments}

In this section, DISTRO is evaluated for a variety of robust ID and OOD tests and is compared to previous approaches.
As baseline, we consider the pre-trained models\footnote{\href{https://github.com/AlexMeinke/Provable-OOD-Detection}{https://github.com/AlexMeinke/Provable-OOD-Detection}} from \citet{prood}.
The normal trained (\textbf{Plain}) and outlier exposure (\textbf{OE})~\cite{oe} models share the same ResNet18~\cite{resnet} architecture and hyperparameters as \textbf{ProoD}~\cite{prood}.
\textbf{GOOD}~\cite{good} uses a 'XL' convolutional neural network.
Additionally, we evaluate the pretrained DenseNet101~\cite{densenet} models for \textbf{ATOM}~\cite{atom} and \textbf{ACET}~\cite{acet}; and the standard OOD detection methods: \textbf{VOS}\footnote{\href{https://github.com/deeplearning-wisc/vos}{https://github.com/deeplearning-wisc/vos}}~\cite{vos} and \textbf{LogitNorm}\footnote{\href{https://github.com/hongxin001/logitnorm_ood}{https://github.com/hongxin001/logitnorm\_ood}}~\cite{logitnorm} with the pretrained WideResNet40~\cite{wideresnet} models provided in the respective works.
We consider \textbf{DDS}~\cite{dds} with a pre-trained diffusion model\footnote{\href{https://github.com/openai/improved-diffusion}{https://github.com/openai/improved-diffusion}} from \citet{nichol2021improved} in front of the OE classifier.
With \textbf{DISTRO}, we incorporate the same pre-trained diffusion model of DDS before the main classifier of ProoD, and maintain its discriminator.
The diffusion models have been used with the settings described in \citet{dds}.
In the context of $\ell_\infty$, we set $\sigma = \sqrt{d} \cdot \epsilon$.

We evaluate all methods on the standard datasets \texttt{CIFAR10/100}~\cite{cifar} as ID.
For the OOD detection evaluation we consider the following set of datasets: 
\texttt{CIFAR100/10}, \texttt{SVHN}~\cite{svhn}, LSUN~\cite{lsun} cropped (\texttt{LSUN\_CR}) and resized (\texttt{LSUN\_RS}),  TinyImageNet~\cite{tiny} cropped (\texttt{TinyImageNet\_CR}), \texttt{Textures}~\citep{textures} and synthetic (\texttt{Gaussian} and \texttt{Uniform}) noise distributions.
We use a random but fixed subset of 1000 images for all datasets considered as a test for OOD.
For ID, we consider the entire dataset.
We run all our experiments on a single NVIDIA A100. 

\subsection{In-Distribution Results}\label{sec:id-results}

Here, we compare clean, adversarial, and certified accuracy for ID samples.
Adversarial accuracy is evaluated with AutoAttack~\citep{apgd} for $\ell_\infty$-norm attacks of budget $\epsilon \in \{\nicefrac{2}{255}, \nicefrac{8}{255}\}$.
We ran the standard version of AutoAttack without additional hyper-parameters. 
Certified accuracy is evaluated for $\ell_2$-norm robustness of deviation $\sigma \in \{0.12, 0.25\}$.
To this end, random smoothing is performed on 10'000 Gaussian distributed samples around the input with a failure probability of $0.001$.
All $R>0$ are considered for the certified accuracy.
In the context of DISTRO and DDS we run 100 evaluation of the entire test set of \texttt{CIFAR10} to estimate the clean accuracy and report the average.
Further, we ran AutoAttack in both \textit{rand} and \textit{standard} modes, and considered the lowest results for DISTRO and DDS.


\begin{table}[htb]
\vspace{-0.5em}
    \centering
    \caption{\textbf{ID Accuracy}: Results of clean, adversarial and certified accuracy (\%) on the \texttt{CIFAR10} test set.
    The grayed-out models have an accuracy drop greater than $3\%$ relative to the model with the highest accuracy.}
    \label{tab:in-distribution}
    \begin{adjustbox}{width=0.5\textwidth,center}
        \begin{tabular}{llccccc}
            \toprule
            \multirow{2}{*}{Method} &\multirow{2}{*}{Clean} &\multicolumn{2}{c}{Adversarial ($\ell_\infty$)} &\multicolumn{2}{c}{Certified ($\ell_2$)} \\
            & &$\epsilon = \nicefrac{2}{255}$ &$\epsilon = \nicefrac{8}{255}$ &$\sigma=0.12$ &$\sigma = 0.25$ \\
            \midrule
            Plain$^*$       &95.01  &2.16   &0.00   &28.14  &14.17 \\
            OE$^*$          &95.53 &1.97   &0.00   &31.48  &10.88 \\
            VOS$^\dag$      &94.62  &2.24   &0.00   &13.13   &10.02       \\
            LogitNorm$^\ddag$  &94.48  &2.65   &0.00   &12.53  &10.25 \\
            \gray{ATOM$^*$}    &\gray{92.33}  &\gray{0.00}   &\gray{0.00}   &\gray{0.00}   &\gray{0.00}  \\
            \gray{ACET$^*$}    &\gray{91.49}  &\gray{69.01}  &\gray{6.04}   &\gray{57.13}  &\gray{12.48} \\
            \gray{GOOD$^*_{80}$} &\gray{90.13}  &\gray{11.65}  &\gray{0.23}   &\gray{17.33}  &\gray{10.31} \\
            ProoD$^*$ $\Delta=3$  &95.46  &2.69   &0.00   &33.92  &13.50 \\
            DDS                   &\textbf{95.55} &72.97 &24.09 &82.26 &64.58 \\
            DISTRO (our)          &95.47  &\textbf{73.34} &\textbf{27.14}  &\textbf{82.77}   &\textbf{65.63} \\
            \bottomrule
        \end{tabular}
    \end{adjustbox}
    \scriptsize{$*$ Pre-trained models from \citet{prood}, $\dagger$ Pre-trained from \citet{vos}, \\ $\ddag$ Pre-trained from \citet{logitnorm}.
    }
\vspace{-2em}
\end{table}

In \autoref{tab:in-distribution}, we show the results.
As expected, Plain and OE are not robust to adversarial attacks.
This applies to ProoD as well, since OE is its primary classifier.
Similarly, standard OOD detection methods, as LogitNorm and VOS, show poor robustness for ID data.
GOOD demonstrates better results than ProoD for adversarial attacks and worse in terms of certified accuracy.
Suprisingly, ACET reveals strong adversarial and certified accuracy despite of its reduced clean accuracy.
Meanwhile, ATOM results in zero for all tests since any slight perturbation of the input triggers the last neuron used for OOD detection.

\subsubsection*{Discussion}

It is clear that diffusion models can enhance adversarial and certified robustness while maintaining high clean accuracy.
As diffusion introduces variance into gradient estimators, standard attacks become much less effective.
Nevertheless, robustness accuracy of diffusion models varies over different runs for the same input, so it should be defined differently from deterministic accuracy, e.g. as expectation.
Luckily, one-shot diffusion introduces such a tiny variance that throughout a few of runs, our results were similar.