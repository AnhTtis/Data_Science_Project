
\subsection{Evaluation Metrics}


To discriminate between ID and OOD samples, we use the confidence of the classifier, i.e. MSP~\cite{msp}. 
Traditionally, the following metrics are used to evaluate the OOD detection performance: 
(i) false positive rate (FPR95) of OODs when ID samples have a 95\% true positive rate;
(ii) the area under the receiver operating characteristic curve (AUROC or AUC); and
(iii) the area under the precision-call curve (AUPR).
In order to determine robustness, we compare adversarial (AAUC, AAUPR, AFPR) and guaranteed (GAUC, GAUPR, GFPR) versions of the previous metrics.
In \autoref{app:adversarial}, we describe the settings for AAUC, AAUPR and AFPR.

\textbf{Guaranteed.}
The guaranteed metrics (GAUC, GAUPR and GFPR) are computed for $\ell_2$ and $\ell_\infty$ norms robustness certificates.
Similarly to \citet{prood}, the $\ell_\infty$-norm is obtained with IBP\footnote{For standard OOD detection methods, such as Plain, OE, VOS and LogitNorm, we run IBP-CROWN with \texttt{auto\_LiRPA}~\cite{auto_lirpa}.} only on OOD data.
On the other hand, the $\ell_2$-norm is computed with \autoref{th:upper_bound} on both ID and OOD data.
Similarly to \autoref{sec:id-results}, we sampled 10'000 Gaussian data points around the input with a deviation $\sigma = 0.12$. 
Since, the certified bound is only probabilistic in practice, we ran a binomial proportion confidence test~\cite{binomial_test} with failure probability of $0.001$.
We have assigned a score of 0 to all samples that fail to be certified, i.e. with $p \leq 1/2$. 
The Lipschitz continuity does not hold in the case of non-certified samples, therefore we are unable to bound the score.
To ensure a fair comparison, we decided to compute the $\ell_2$-norm GAUC on both ID and OOD datasets.

\begin{figure}[htb]
\vspace{-1em}
    \centering
    \includegraphics[width=0.45\textwidth]{graphics/average_distro.pdf}    
    % \vspace{-1em}
    \caption{Kernel density estimation (bandwidth = 1) of the distribution of certified smooth ($\sigma = 0.12$) scores for DISTRO on ID (\texttt{CIFAR10}) and OOD (all other datasets) samples.}
    \label{fig:distribution_of_certified_scores}
    \vspace{-1em}
\end{figure}

In \autoref{fig:distribution_of_certified_scores}, we plot the normalized frequency of occurrences of the certified upper bound ($\sqrt{2/\pi} \cdot \Phi^{-1}(p) + p$) for ID versus OOD data of DISTRO.
We observe that OOD data tend to peak close to zero, while ID data are spread out with larger values. 
This suggests that a large radius is more likely to be associated with ID data versus OOD samples.
As a result, robustly certifying the detection of OOD samples becomes more feasible.


\subsection{Out-Of-Distribution Results}

\begin{table*}[htb]
\vspace{-0.5em}
    \centering
    \caption{\textbf{Robust OOD detection.} We consider the following metrics: clean top-1 accuracy on \texttt{CIFAR10/100} test sets, clean AUC, guaranteed (GAUC), adversarial AUC (AAUC), clean AUPR, guaranteed AUPR (GAUPR), adverarial AUPR (AAUPR), clean FPR\@95\% (FPR), guaranteed FPR\@95\% (GFPR) and adversarial FPR\@95\% (AFPR). 
    Averaging was performed on a variety of OOD datasets. We consider MSP~\cite{msp} for all methods and metrics (with temperature $T=1$). The guaranteed $\ell_2$-norm is computed for $\sigma = 0.12$ for all $R>0$, while the adversarial and guaranteed $\ell_\infty$-norm are computed for $\epsilon = 0.01$. The grayed-out models have an accuracy drop greater than $3\%$ relative to the model with the highest accuracy. \textbf{Bold} numbers are superior results.}
    \label{tab:ood_average}
    \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{lr|rrrr|rrrr|rrrr}
    \toprule
        ID: CIFAR10 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
        - \textbf{Standard} \\
        Plain$^*$   &95.01  &94.56  &48.86  &0.00 &24.52  &99.42     &60.05   &0.00    &82.30  &35.72  &100.0  &100.0     &96.72 \\
        OE$^*$      &\textbf{95.53} &\textbf{98.78}  &46.88 &0.00 &37.91  &\textbf{99.87}  &63.08 &0.00 &84.49  &\textbf{4.71}  &100.0   &100.0     &70.26 \\
        VOS$^\dag$ &94.62  &90.82 &30.13 &0.00 &20.62 &99.15 &41.62 &0.00 &81.80 &61.66 &94.10 &100.0 &100.0 \\
        LogitNorm$^\ddag$ &94.48  &96.71 &40.73 &0.00 &39.76 &99.64 &49.31 &0.00 &86.47 &13.95 &100.0 &100.0 &91.10 \\
        - \textbf{Adversarial} \\
        \gray{ACET$^*$} &\gray{91.48} &\gray{97.24} &\gray{60.21} &\gray{0.00} &\gray{93.01} &\gray{99.68} &\gray{76.22} &\gray{0.00} &\gray{99.16} &\gray{13.82} &\gray{95.65} &\gray{100.0} &\gray{32.15} \\
        \gray{ATOM$^*$} &\gray{92.33} &\gray{98.82} &\gray{97.15} &\gray{0.00} &\gray{44.65} &\gray{99.86} &\gray{95.51} &\gray{0.00} &\gray{85.74} &\gray{4.14}  &\gray{5.04}  &\gray{100.0} &\gray{62.65} \\
        \multicolumn{1}{l}{- \textbf{Guaranteed}} \\
        \gray{GOOD$^*_{80}$}   &\gray{90.13} &\gray{93.12} &\gray{36.45} &\gray{57.52} &\gray{78.11} &\gray{99.22}  &\gray{52.31} &\gray{89.54} &\gray{95.19} &\gray{30.00}  &\gray{100.0} &\gray{72.45} &\gray{47.55} \\
        ProoD$^*\Delta=3$ &95.46 &98.72 &52.36 &\textbf{59.56} &64.22 &\textbf{99.87}  &66.53 &\textbf{93.89} &94.52 &5.49  &100.0  &100.0 &86.49 \\
        DISTRO (our)     &95.47 &98.72 &\textbf{88.97} &59.53 &\textbf{83.24} &\textbf{99.87}  &\textbf{92.75}  &\textbf{93.89} &\textbf{97.32} &5.29 &\textbf{67.86} &100.0 &\textbf{34.56} \\
        \midrule
        ID: CIFAR100 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
        - \textbf{Standard} \\
        Plain$^*$   &\textbf{77.38} &81.60 &30.63 &0.00 &16.98 &97.84 &45.10 &0.00 &81.27 &82.52 &100.0 &100.0 &100.0 \\
        OE$^*$      &77.28 &90.41 &39.87 &0.00 &22.79 &98.90 &49.46 &0.00 &81.96 &47.49 &100.0 &100.0 &87.74 \\
        - \textbf{Adversarial} \\
        ACET$^*$ &74.47 &90.27 &36.36 &0.00 &27.68 &98.84 &43.50 &0.00 &82.60 &44.11 &\textbf{90.41} &100.0 &\textbf{74.99} \\
        \gray{ATOM}$^*$ &\gray{71.73} &\gray{91.72} &\gray{84.38} &\gray{0.00} &\gray{31.52} &\gray{98.88} &\gray{79.95} &\gray{0.00} &\gray{83.36}      &\gray{30.81} &\gray{30.09}  &\gray{100.0} &\gray{73.69} \\
        - \textbf{Guaranteed} \\
        ProoD$^* \Delta=1$   &76.79 &\textbf{90.90} &42.83 &\textbf{37.67} &43.81 &\textbf{98.91} &50.90 &\textbf{89.66} &90.46 &42.12 &100.0 &100.0 &97.11 \\
        DISTRO (our) &76.78 &90.89 &\textbf{59.39} &37.53  &\textbf{62.77} &98.90 &\textbf{69.41} &89.63 &\textbf{93.59}  &\textbf{40.94} &100.0  &100.0 &58.58 \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \footnotesize{$*$ Pre-trained models from \citet{prood}, $\dagger$ Pre-trained from \citet{vos}, $\ddag$ Pre-trained from \citet{logitnorm}.
    }
  \vspace{-1em}
\end{table*}


Here, we describe the results shown in \autoref{tab:ood_average}.
As previously, we grayed-out models with an accuracy drop greater than $3\%$ with respect to the model with highest accuracy.
The objective of this choice is to prioritize clean ID accuracy over all other metrics.
A comparison of the remaining metrics is then made on an equal basis.
Despite this, there is no direct comparison between the GAUC of $\ell_2$ and $\ell_\infty$ norms.
This is primarily due to the fact that the guaranteed upper bound of $\ell_\infty$ is computed only for OOD data, whereas $\ell_2$ is computed for both (ID \& OOD).
Additionally, we choose any radius $R>0$ for $\ell_2$, while for $\ell_\infty$, $\epsilon$ is fixed to $0.01$\footnote{This problem can be addressed by considering $R \geq \sqrt{d}\cdot \epsilon$.}.

We observe that the performances of LogitNorm and VOS on clean AUC, AUPR and FPR are suboptimal.
The reason for this is that we are evaluating MSP~\cite{msp} instead of the suggested normalization~\cite{logitnorm} and energy~\cite{vos} functions for LogitNorm and VOS, respectively.
To ensure a fair comparison we decided to standardize the output function across all models.
On \texttt{CIFAR100}, only the most effective methods of \texttt{CIFAR10} have been tested.

\textbf{Outcomes.}
In light of these considerations, we note that OE achieved the highest clean AUC, AUPR, and FPR.
In case of AAUC, ACET shows the best results for \texttt{CIFAR10}.
While ATOM achieves close to optimal performance for the guaranteed $\ell_2$-norm AUC, AUPR and FPR.
Both methods are trained adversarially on outliers, which makes them more robust on OOD data, but at the expense of a reduced clean accuracy.


\begin{wraptable}{l}{4.2cm}
    \vspace{-1em}
    \centering
    \caption{\small Overall average between the metrics of \autoref{tab:ood_average} for \texttt{CIFAR10/100} (\texttt{C-10}, \texttt{C-100}).}
    \label{tab:ood_overall}
    % \vspace{0.3em}
    \begin{adjustbox}{width=0.25\textwidth,center}
    \begin{tabular}{lrr}
    \toprule
    Method      &\multicolumn{2}{c}{Average} \\
                &\texttt{C-10} &\texttt{C-100} \\
    \midrule
     Plain              &44.02 &34.48 \\
     OE                 &50.12 &40.42 \\
     VOS                &38.60 &- \\
     LogitNorm          &46.31 &- \\
     ACET               &59.64 &41.86 \\
     ATOM               &64.79 &54.38 \\
     GOOD$_{80}$        &64.74 &- \\
     ProoD $\Delta=3$   &64.09 &52.51 \\
     DISTRO (our)       &\textbf{77.08} &\textbf{59.95} \\
     \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{-1em}
\end{wraptable}


Similarly to the ID results, DISTRO demonstrates the potential benefits of diffusion models to augment the model robustness in terms of $\ell_2$-norm guaranteed and adversarial AUCs.
Although there is a slight decrease in $\ell_\infty$-norm GAUC, GAUPR and GFPR, which could likely be suppressed by fine tuning the classifier in conjunction with the denoiser.

In \autoref{tab:ood_overall}, we average all the metrics of \autoref{tab:ood_average} for \texttt{CIFAR10} (including clean ID accuracy).
Surprisingly, ATOM shows similar results as ProoD and GOOD. 
This can be related to the high certification radius obtained for GAUC of $\ell_2$-norm. 
We report each individual dataset's results in \autoref{app:experiments}.


\textbf{Discussion.}
It is evident that the $\ell_2$-norm GAUC (and GAUPR) diverge from zero when standard OOD detection models are considered. 
This illustrates the potential of the $\ell_2$-norm to provide certified OOD detection for any method and architecture.
Consequently, it facilitates the experimental evaluation of new robust OOD detection algorithms (both adversarial and certified).

As a side note, the one-shot denoiser appears to improve robustness certification metrics while not compromising clean metrics, such as AUC. 
In some cases, it also appears to be slightly better, even though the denoising process should produce images that are as similar as possible to those considered during training.
This is because a single shot of denoising does not compromise the OOD sample or generate an allucinated one.
Additionally, one-shot denoising introduces so little variance that in this benchmark, the results were similar across multiple runs.

In \autoref{app:standardized},  an additional comparison on a similar network architecture for all methods is presented.