% \section{Mathematical Proofs}\label{app:math}

%\subsection{Proof of \autoref{th:randomized_smoothing}}\label{app:theorem_randomized_smoothing}

%\randomizedsmoothing*

% \begin{proof}
% W.l.o.g. we consider $\sigma = 1$ (by linearity the proof holds for any $\sigma > 0$).
% For any unit vector $v$ let $\gamma_v$ be a standard Gaussian random variable, such that $\left\langle v, \delta \right\rangle \in \R$ with $\delta \sim \set{N}(0, I)$.
% For $p \in [0, 1]$, let us define the random variable:
% \begin{equation*}
%     \gamma_v^{(p)} = \left\{\begin{array}{ll}
%         \set{N}(0, 1)|_{[\tau, \infty)} &\text{with prob. } p,  \\
%         0 &\text{with prob. } 1 - p,
%     \end{array}\right.
% \end{equation*}
% %
% where $\tau \eqdef \Phi^{-1}(1 - p)$ and $\set{N}(0, 1)|_{[\tau, \infty)}$ is a standard Gaussian $z$ conditioned to be $z \geq \tau$.
% Thus, for any unit vector $v$:
% \begin{equation*}
%     \E [\gamma_v^{(p)}] = \E_{z \sim \set{N}(0, 1)} z \mathbbm{1}(z \geq \tau) = \left. \frac{1}{\sqrt{2 \pi}}e^{-\nicefrac{z^2}{2}}\right|^\tau_\infty
%     = \Phi^\prime (\tau) = \Phi^\prime \left(\Phi^{-1}(1 - p)\right).
% \end{equation*}
% Finally, by considering the differential method proposed in \citet{yang2020randomized} (Theorem F.6) and setting $1 - G(x)_c = 1 - p$, we obtain the provable robust radius:

% \begin{equation*}
%     \norm{\delta}_2 < \int_{1 - p}^{1/2} \frac{1}{\E [\gamma_v^{(p)}]}dp = \int_{1 - p}^{1/2} \frac{1}{\Phi^\prime \left(\Phi^{-1}(1 - p)\right)}dp = \int_0^{\Phi^{-1}(p)} d\tau = \Phi^{-1}(p),
% \end{equation*}
% %
% as desired.
% \end{proof}

% \subsection{Proof of \autoref{lemma:stein}}\label{app:lemma_stein}

% \lemmastein*

% \begin{proof}
% Let us consider the \textit{Weierstrauss} transform of $h$ (which coincide with the \textit{smooth} version of $h$):
% \begin{equation*}
%     H(x) = \left(h * \set{N}(0, \sigma^2I)\right)(x) = \E_{\delta \sim \set{N}(0, \sigma^2I)}\left[h(x+\delta)\right],
% \end{equation*}
% where $*$ denotes the convolution operator.
% To make this demonstration as readable as possible, we skip the analysis that proves $H$ can be differentiated.
% Based on this assumption, we have:
% \begin{equation*}
% \begin{aligned}
%     \nabla H(x) &= \nabla\left( \frac{1}{(2\pi \sigma^2)^{d/2}} \int_{\R^d} h(t) \exp \left( -\frac{1}{2\sigma^2}\norm{x - t}_2^2 \right) dt\right) \\
%     &= \frac{1}{(2\pi \sigma^2)^{d/2}} \int_{\R^d} h(t) \nabla \left(\exp\left( -\frac{1}{2\sigma^2}\norm{x - t}_2^2 \right) \right) dt \\
%     &= \frac{1}{(2\pi \sigma^2)^{d/2}} \int_{\R^d} h(t) \frac{t-x}{\sigma^2}\exp\left( -\frac{1}{2\sigma^2}\norm{x - t}_2^2 \right) dt, \\
% \end{aligned}
% \end{equation*}
% where a change in variables and the definition of expectation result in the final equality.

% \end{proof}


\section{Proof of \autoref{th:upper_bound}}\label{app:theorem_upper_bound}

\upperbound*

\begin{proof}
As a prerequisite to proving the theorem, we need to know the analytic form of the gradient of a smoothed function given in \autoref{lemma:stein}.
Let us consider the soft classifier $F(x):\R^d \to \prob(\set{Y})$, and a measurable function $h(x) \eqdef F(x)_y$, where $y\in\set{Y}$.
We define the smooth version of $h$ as $H(x) = \E_{\delta \sim \set{N}(0, \sigma^2 I)} [h(x + \delta)]$, with $\sigma > 0$.
Additionally, we consider the \textit{Weierstrauss} transform of $h$ (which coincide with the \textit{smooth} version of $h$):
\begin{equation*}
    H(x) = \left(h * \set{N}(0, \sigma^2I)\right)(x) = \E_{\delta \sim \set{N}(0, \sigma^2I)}\left[h(x+\delta)\right],
\end{equation*}
where $*$ denotes the convolution operator.
Thus, $H(x)$ is differentiable and:
%
\begin{equation*}
    \nabla H(x) = \frac{1}{\sigma^2}\E_{\delta \sim \set{N}(0, \sigma^2 I)} \left[ \delta \cdot h(x + \delta) \right].
\end{equation*}
%
Since $h:\R^d \to [0, 1]$ and $\ell_2$ is self-dual, it is sufficient to show that the gradients of $H$ are bounded in $\ell_2$. 
From \autoref{lemma:stein}, for any unit vector $v\in \R^d$ we have:
\begin{equation*}
    \begin{aligned}
    |\langle v, \nabla H(x) \rangle| &= \left\lvert \frac{1}{(2\pi \sigma^2)^{\nicefrac{d}{2}}} \int_{\sR^{d}} h(t) \left\langle v, \frac{t - x}{\sigma^2} \right\rangle \exp{\left(-\frac{1}{2\sigma^2} \norm{x-t}_2^2 \right)} dt \right\rvert, \\
    &\leq \frac{1}{(2\pi \sigma^2)^{\nicefrac{d}{2}}} \int_{\sR^{d}} \left\lvert \left\langle v, \frac{t - x}{\sigma^2} \right\rangle \right\rvert \exp{\left(-\frac{1}{2\sigma^2} \norm{x-t}_2^2 \right)} dt,  \\
    \end{aligned}
\end{equation*}
where we make use of the triangle inequality and know that $h$ is bounded by 1.
Given that projections of Gaussians are Gaussians and from the classical integration of the Gaussian density, we obtain:
\begin{equation*}
        |\langle v, \nabla H(x) \rangle| \leq \frac{1}{(2\pi \sigma^2)^{\nicefrac{d}{2}}} \int_{\sR^{d}} \left\lvert \left\langle v, \frac{t - x}{\sigma^2} \right\rangle \right\rvert \exp{\left(-\frac{1}{2\sigma^2} \norm{x-t}_2^2 \right)} dt = \frac{1}{\sigma^2}\E_{Z\sim \set{N}(0, \sigma^2)} \left[Z\right] = \sqrt{\frac{2}{\pi\sigma^2}},
\end{equation*}
where we consider the supremum over all unit vectors $v$.
Since, we know that $H(x)$ is $\sqrt{\frac{2}{\pi\sigma^2}}$-Lipschitz in $\ell_2$, it is possible to use the Lipschitz continuity of $H$ to establish some useful properties.
First, it is worth noting that the Lipschitz continuity of $H$ does not directly imply that is concave or convex in $x$. 
However, it does imply that $H$ is \textit{well-behaved} in the sense that it does not vary too rapidly with $x$.
Given this, it is possible to use the Lipschitz constant to bound the difference between $H(x + \delta)$ and $H(x)$ for any value of $\delta$, with $\norm{\delta}_2 < \sigma \Phi^{-1}(x)$.
Formally:
%
\begin{equation*}
    \begin{aligned}
    |H(x + \delta)| - |H(x)| \leq |H(x + \delta) - H(x)| &\leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2, \\
    |H(x + \delta)| &\leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2 + |H(x)|, \\
    \end{aligned}
\end{equation*}
%
where we make use of the reverse triangle inequality.
This means that the difference between $H(x + \delta)$ and $H(x)$ is bounded by a multiple of $\norm{\delta}_2$.
Since $H(x):\sR^d \to [0, 1]$, we can assume $|H(x)|=H(x)$, and moreover:
\begin{equation*}
    \begin{aligned}
    % \max_{\norm{\delta}_2 \leq R} F(x+\delta) \leq \max_{\norm{\delta}_2 \leq \epsilon} L\norm{\delta}_2 + F(x) \leq L\epsilon + F(x)
    \E_{\delta \sim \set{N}(0, \sigma^2 I)} [h(x + 2\delta)] &\leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2 + \E_{\delta \sim \set{N}(0, \sigma^2 I)} [h(x + \delta)], \\
    \E_{\delta \sim \set{N}(0, \sigma^2 I)} [F(x + 2\delta)_y] &\leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2 + \E_{\delta \sim \set{N}(0, \sigma^2 I)} [F(x + \delta)_y], \\
    G(x + 2\delta)_y &\leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2 + G(x + \delta)_y, \\
    \max_{y \in \set{Y}} G(z + \delta)_y &\leq \sqrt{\frac{2}{\pi\sigma^2}} \norm{\delta}_2 + \max_{y \in \set{Y}} G(z)_y, \\
    \end{aligned}
\end{equation*}

where in the last step we make a change of variables $z=x+\delta$.
Finally, we can insert the certification radius:
\begin{equation*}
    \max_{y \in \set{Y}} G(z + \delta)_y \leq \sqrt{\frac{2}{\pi}}\Phi^{-1}(z) + \max_{y \in \set{Y}} G(z)_y.
\end{equation*}

\end{proof}\textbf{}