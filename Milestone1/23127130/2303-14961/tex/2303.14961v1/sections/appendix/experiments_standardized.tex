\section{Experimental Results on Similar Model Capacity}\label{app:standardized}

In this section we compare each method on a similar model architecture, since the results in \autoref{tab:contribution} are not only dependent on the method performance, but also on the capacity of the model and OOD dataset used.
Therefore we retrain all presented methods using a ResNet18 architecture for \texttt{CIFAR10} and \texttt{CIFAR100} respectively. 
For methods that require an additional OOD dataset for training, such as OE \cite{oe}, ACET \cite{acet}, ATOM \cite{atom}, ProoD \cite{prood} and DISTRO, we use the same subset of \texttt{OpenImages}~\cite{openimages} containing 50'000 images.
Furthermore, we consider an input normalization of $0.5$ across all dimensions for both mean and standard deviation.
In addition, we attempt to be as minimally intrusive as possible when it comes to the default training procedure.


For Plain, OE and LogitNorm we run the implementation\footnote{\href{https://github.com/Jingkang50/OpenOOD}{https://github.com/Jingkang50/OpenOOD}} from \citet{yang2022openood} and leave the hyperparameters unchanged.
Similarly for ACET and ATOM, we only change the model architecture and normalization and run both  implementations from ATOM\footnote{\href{https://github.com/jfc43/informative-outlier-mining}{https://github.com/jfc43/informative-outlier-mining}}.
Lastly, we train ProoD\footnote{\href{https://github.com/AlexMeinke/Provable-OOD-Detection}{https://github.com/AlexMeinke/Provable-OOD-Detection}} from \citet{prood} using their training configuration files, where the discriminator is trained for 1000 epochs and the bias shift ($\Delta$) is 3/1 for \texttt{CIFAR10/100}, respectively.


% All images are normalized with mean and standard deviation set to $0.5$ across all channels. 
% For Plain, Outlier Exposure and LogitNorm we use RandomHorizontalFlip and RandomCrop as augmentation techniques. 
% We optimize these models using Stochastic Gradient Descent for 100 epochs, with an initial learning rate of $0.1$, annealing to $10^{-6}$ over the course of training, momentum of $0.9$ and weight\_decay of $0.0005$. 
% The training batch size is $128$ for all methods.
% In general for ATOM and ACET, we try to follow the hyperparameters from the Github implementation. 
% For ACET and ATOM the training is mostly similar, but we use SGD with weight decay of $0.0001$ and a the initial learning rate of $0.1$ is multiplied by $0.1$ at epoch 50, 75 and 90. 
% The PGD attack in ACET is conducted using $\eps=2/255$ for 5 epochs using a step size of 2. 
% To select the outliers in ATOM, we use a quantile size of $1/8$.
% For ProoD, we follow their original implementation and just change add the normalization of $0.5$ for both mean and standard deviation.

\begin{table*}[htb]
\vspace{-0.5em}
    \centering
    \caption{\textbf{Robust OOD detection with ResNet18.} The guaranteed $\ell_2$-norm is computed for $\sigma = 0.12$, while the adversarial and guaranteed $\ell_\infty$-norm are computed for $\epsilon = 0.01$. The grayed-out models have an accuracy drop of $>3\%$ relative to the model with the highest accuracy. \textbf{Bold} numbers are superior results.
    } 
    \label{tab:ood_average_standard}
    \begin{adjustbox}{width=\textwidth,center}
    \begin{tabular}{lr|rrrr|rrrr|rrrr}
    \toprule
        ID: CIFAR10 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
 		Plain &94.32 &92.28 &35.81 &0.00 &23.71 &99.00 &46.83 &0.00 &82.00 &40.21 &93.56 &100.0 &98.88 \\
		LogitNorm &94.71 &95.58 &34.19 &0.00 &35.00 &99.54 &49.63 &0.00 &85.14 &33.06 &95.12 &100.0 &92.20\\
		OE &92.41 &97.35 &50.56 &0.00 &37.95 &99.71 &62.25 &0.00 &85.51 &13.44 &100.0 &100.0 &74.91\\
		ACET &93.66 &\textbf{97.86} &37.45 &0.00 &65.21 &\textbf{99.75} &50.26 &0.00 &91.99 &8.94 &100.0 &100.0 &\textbf{50.29} \\
		\gray{ATOM} &\gray{91.90} &\gray{98.12} &\gray{97.98} &\gray{97.63} &\gray{62.79} &\gray{99.78} &\gray{98.16} &\gray{99.78} &\gray{91.49} &\gray{8.7} &\gray{9.42} &\gray{0.00} &\gray{51.56} \\
		ProoD &\textbf{95.20} &96.91 &44.95 &\textbf{63.44} &64.61 &99.63 &60.27 &\textbf{94.37} &94.42 &\textbf{16.03} &100.0 &\textbf{91.90} &78.22 \\
		DISTRO (our) &\textbf{95.20} &96.80 &\textbf{86.63} &59.86 &\textbf{71.70} &99.62 &\textbf{90.80} &93.78 &\textbf{95.72} &16.55 &\textbf{66.88} &99.96 &67.59\\
        \midrule
        ID: CIFAR100 &Acc. &AUC$\uparrow$ &\multicolumn{2}{c}{GAUC$\uparrow$} &AAUC$\uparrow$ &AUPR$\uparrow$ &\multicolumn{2}{c}{GAUPR$\uparrow$} &AAUPR$\uparrow$ &FPR$\downarrow$ &\multicolumn{2}{c}{GFPR$\downarrow$} &AFPR$\downarrow$ \\
        &   &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c|}{$\ell_\infty$} &   &\multicolumn{1}{c}{$\ell_2$} &\multicolumn{1}{c}{$\ell_\infty$} &\multicolumn{1}{c}{$\ell_\infty$} \\
        \midrule
		Plain &77.54 &84.50 &38.11 &0.00 &24.17 &98.16 &44.96 &0.00 &82.32 &67.61 &100.0 &100.0 &98.04 \\
		LogitNorm &76.25 &84.06 &40.93 &0.00 &47.64 &98.04 &46.80 &0.00 &87.25 &73.70 &100.0 &100.0 &87.98\\
		OE &75.84 &88.96 &38.90 &0.00 &17.90 &\textbf{98.72} &48.82 &0.00 &81.43 &49.61 &100.0 &100.0 &99.41\\
		\gray{ACET} &\gray{73.71} &\gray{95.65} &\gray{42.03} &\gray{0.00} &\gray{52.49} &\gray{99.44} &\gray{48.54} &\gray{0.00} &\gray{89.23} &\gray{13.96} &\gray{100.0} &\gray{100.0} &\gray{60.39} \\
		ProoD &\textbf{77.77} &\textbf{89.47} &40.72 &\textbf{37.68} &49.16 &98.66 &49.97 &\textbf{89.66} &91.08 &\textbf{40.44} &100.0 &100.0 &84.15 \\
		DISTRO (our) &77.73 &88.90 &\textbf{55.57} &29.71 &\textbf{51.89} &98.60 &\textbf{67.62} &87.44 &\textbf{91.71} &43.24 &100.0 &100.0 &\textbf{79.34} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
  \vspace{-1em}
\end{table*}