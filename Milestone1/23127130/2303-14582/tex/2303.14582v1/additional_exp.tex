\section{Experiment Details}\label{sec_exp_detail}

We describe details that were left out of the paper's main text.
First, we describe the additional experimental setup and the implementation specifics.
Second, we present results to further validate the sample complexity of task modeling.
Third, we provide the experimental results that are omitted from Section \ref{sec:exp}, including the results for fairness measures and ablation studies.

\begin{figure}[!htbp]
    \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_chemprot.pdf}\vspace{-0.1in}
    \caption{Chemprot}
  \end{subfigure}
 \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_cdr.pdf}\vspace{-0.1in}
    \caption{CDR}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_trec.pdf}\vspace{-0.1in}
    \caption{TREC}
  \end{subfigure}\\
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_semeval.pdf}\vspace{-0.1in}
    \caption{Semeval}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_cola.pdf}\vspace{-0.1in}
    \caption{CoLA}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_rte.pdf}\vspace{-0.1in}
    \caption{RTE}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_copa.pdf}\vspace{-0.1in}
    \caption{COPA}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_wsc.pdf}\vspace{-0.1in}
    \caption{WSC}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_HI.pdf}\vspace{-0.1in}
    \caption{HI}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_LA.pdf}\vspace{-0.1in}
    \caption{LA}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_MN.pdf}\vspace{-0.1in}
    \caption{MN}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_NM.pdf}\vspace{-0.1in}
    \caption{NM}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_KS.pdf}\vspace{-0.1in}
    \caption{KS}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_NJ.pdf}\vspace{-0.1in}
    \caption{NJ}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_NV.pdf}\vspace{-0.1in}
    \caption{NV}
  \end{subfigure}
  \begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/err_income_2018_SC.pdf}\vspace{-0.1in}
    \caption{SC}
  \end{subfigure}
   \begin{subfigure}[b]{0.33\textwidth}
     \centering
     \includegraphics[width=0.7\textwidth]{figures/err_income_2018_RI.pdf}
     \vspace{-0.1in}\caption{Target task: RI}
   \end{subfigure}
  \caption{The MSE of linear surrogate models converges close to the variance of MTL performances.
    The red line shows the variance of $f$, measured across five random seeds.
    We observe that the Spearman correlation coefficient between the predictions and the true performances is \textbf{0.8} on average.
  \textbf{(a-d)} Weak supervision tasks. 
  \textbf{(e-h)} NLP tasks.
  \textbf{(i-p)} Multi-group learning tasks.}
  \label{fig_convergence}  
\end{figure}


\subsection{Implementation Details}\label{add_exp_setup}

For evaluating multitask learning with natural language processing tasks, we collect twenty-five tasks from several benchmarks, including GLUE, SuperGLUE, TweetEval, and ANLI. 
Due to the computation constraint, we did not include the tasks with a training set size larger than 100k. 
The collection spans numerous categories of tasks, including sentence classification, natural language inference, and question answering. 
Table \ref{tab_text_statistics} shows the statistics of the twenty-five tasks.

\begin{table*}[h!]
\centering
\caption{Dataset description and statistics of twenty-five text datasets.}\label{tab_text_statistics}
{\footnotesize
\begin{tabular}{@{}lccccc@{}}
\toprule
Task & Benchmark & Train. Set & Dev. Set  & Task Category & Metrics \\ \midrule
CoLA  & GLUE & 8.5k & 1k & Grammar acceptability & Matthews corr. \\
MRPC  & GLUE & 3.7k & 1.7k & Sentence Paraphrase & Acc./F1 \\
RTE   & GLUE & 2.5k & 3k & Natural language inference & Acc.\\
SST-2 & GLUE & 67k & 1.8k & Sentence classification & Acc. \\
STS-B & GLUE & 7k & 1.4k & Sentence similarity & Pearson/Spearman corr. \\
WNLI  & GLUE & 634 & 146 & Natural language inference & Acc. \\
BoolQ   & SuperGLUE & 9.4k & 3.3k & Question answering & Acc. \\
CB      & SuperGLUE & 250 & 57 & Natural language inference & Acc./F1 \\
COPA    & SuperGLUE & 400 & 100 & Question answering & Acc. \\
MultiRC & SuperGLUE & 5.1k & 953 & Question answering & F$1_a$/EM \\
WiC     & SuperGLUE & 6k  & 638 & Word sense disambiguation & Acc. \\
WSC     & SuperGLUE & 554 & 104 & Coreference resolution & Acc. \\
Emoji       & TweetEval & 45k & 5k & Sentence classification & Macro-averaged F1\\
Emotion     & TweetEval & 3.2k & 374 & Sentence classification & Macro-averaged F1\\
Hate        & TweetEval & 9k & 1k & Sentence classification & Macro-averaged F1\\
Irony       & TweetEval & 2.9k & 955 & Sentence classification & F$1^{(i)}$\\
Offensive   & TweetEval & 12k & 1.3k & Sentence classification & Macro-averaged F1\\
Sentiment   & TweetEval & 45k & 2k & Sentence classification & Macro-averaged Recall\\
Stance (Abortion)   & TweetEval & 587 & 66 & Sentence classification & Avg. of F$1^{(a)}$ and F$1^{(f)}$ \\
Stance (Atheism)    & TweetEval & 461 & 52 & Sentence classification & Avg. of F$1^{(a)}$ and F$1^{(f)}$ \\
Stance (Climate)    & TweetEval & 355 & 40 & Sentence classification & Avg. of F$1^{(a)}$ and F$1^{(f)}$\\
Stance (Feminism)   & TweetEval & 597 & 67 & Sentence classification & Avg. of F$1^{(a)}$ and F$1^{(f)}$ \\
Stance (H. Clinton) & TweetEval & 620 & 69 & Sentence classification & Avg. of F$1^{(a)}$ and F$1^{(f)}$\\
ANLI (A1) & ANLI & 1.7k & 1k & Natural language inference & Acc. \\
ANLI (A2) & ANLI & 4.5k & 1k & Natural language inference & Acc. \\
\bottomrule
\end{tabular}
}
\end{table*}



\bigskip
We run the baselines using the open-sourced implementations from the respective publications. 
We describe the hyperparameters for baselines as follows. 

For higher-order approximation and task affinity grouping, we compute the task affinity scores between source and target tasks. Then, we select $m$ tasks with the largest task affinity scores as source tasks for each target task. $m$ is searched between 0 and the number of total tasks.

For gradient decomposition, we search the number of decomposition basis and auxiliary task gradient direction parameters, following the search space in \citet{dery2021auxiliary}.

For weighted training, we search the task weight learning rate in $[10^{-2}, 10^2]$. 
The hyper-parameters are tuned on the validation dataset by grid search. For each target task, we search 10 times over the hyper-parameter space. We use the same number of trials in tuning hyper-parameters for baselines.


\subsection{Omitted Results from Section \ref{sec_exp_res}}\label{sec_omitted_results}

\textbf{Complete results for NLP tasks.} In Table \ref{tab_acc_res_text_tasks}, we report the complete experimental results for applying our approach to NLP tasks, as reported in Section \ref{sec_exp_res}.

\begin{table*}[h!]
\centering
\caption{Accuracy/Correlation scores for five NLP tasks on the development set using surrogate modeling followed by thresholding (ours), as compared with STL and MTL methods.}\label{tab_acc_res_text_tasks}
\begin{small}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Dataset    & CoLA & RTE & CB & COPA & WSC \\ 
Metrics    & Matthews Corr. & Accuracy & Accuracy & Accuracy & Accuracy \\\midrule
Train      & 8500 & 2500 & 250 & 400 & 554  \\
Validation & 1000 & 3000 & 57 & 100 & 104 \\ 
\midrule %
STL       & 59.38$\pm$0.70 & 67.94$\pm$0.74 & 70.36$\pm$1.82
 & 64.00$\pm$2.19 & 60.00$\pm$2.76 \\
Naive MTL & 57.11$\pm$0.81 & 69.31$\pm$0.97 & 71.78$\pm$1.39
 & 66.00$\pm$2.02 & 58.20$\pm$1.98 \\ %
HOA       & 60.09$\pm$0.75 & 69.03$\pm$2.03 & 80.71$\pm$2.62
 & 67.20$\pm$2.56 & 61.35$\pm$3.12 \\ %
\midrule %
\textbf{Alg. \ref{alg:task_modeling} (Ours)}        & \textbf{60.43$\pm$0.79} & \textbf{70.83$\pm$1.97} & \textbf{83.57$\pm$2.43} & \textbf{69.20$\pm$3.71} & \textbf{65.38$\pm$2.31} \\\bottomrule %
\end{tabular}
\end{small}
\end{table*}

\bigskip





\textbf{Optimizing fairness-related metrics.}
We show that task modeling is applicable to various performance metrics for capturing task affinity.

Besides the average performance and worst-group performance discussed in Section \ref{sec_exp_res}, we consider two fairness measures: demographic parity and equality of opportunity \cite{ding2021retiring}.
\begin{itemize}
    \item The demographic parity measure is defined as:
\[ \Big|{\Pr\big[\hat y = 1 \mid g = \text{black}\big] - \Pr\big[\hat y = 1 \mid g = \text{white}\big]}\Big|, \]
which measures the difference in the positive rates between white and African American demographic groups.

    \item The equality of opportunity measure is defined as:
\[ \Big|{\Pr\big[\hat y = 1 \mid y = 1, g = \text{black}\big] -  \Pr\big[\hat y = 1 \mid y = 1, g = \text{white}\big]}\Big|, \]
which measures the difference in the true positive rates between the two groups.
\end{itemize}

We consider the binary classification tasks with multiple subpopulation groups.
Table \ref{tab:fairness_measure} shows the comparative results. 
First, similar to the worst-group accuracy results, we find that multitask approaches (including ours and previous methods) decrease the violation of both fairness measures compared to ERM, suggesting the benefit of combining related datasets. 
Second, our approach consistently reduces both fairness measure violations more by \textbf{1.26\%} and \textbf{2.31\%} on average than previous multitask learning approaches, respectively.



\begin{table}[h!]
\centering
\caption{Violation of two fairness-related measures (demographic parity and equality of opportunity) on six multi-group learning tasks with tabular features, averaged over ten random seeds. \textbf{Lower is better.}}\label{tab:fairness_measure}
\begin{footnotesize}
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
\makecell[l]{Demographic parity} & HI & KS & LA & NJ & NV & SC  \\ \midrule
STL & 12.95$\pm$1.76 & 4.09$\pm$1.15 & 26.30$\pm$1.21 & 26.06$\pm$0.53 & 12.62$\pm$1.99 & 22.51$\pm$0.47  \\
Naive MTL  & 8.25$\pm$1.31 & 4.06$\pm$1.17 & 21.24$\pm$0.66 & 27.73$\pm$0.94 & 13.35$\pm$0.51 & 18.83$\pm$0.80   \\
HOA  & 8.63$\pm$2.95 & 6.15$\pm$3.00 & 22.83$\pm$0.53 & 26.14$\pm$0.29 & 13.15$\pm$0.64 & 19.39$\pm$1.05 \\
TAG  & 8.93$\pm$2.35 & 3.97$\pm$0.61 & 20.72$\pm$0.86 & 25.21$\pm$0.68 & 12.24$\pm$0.82 & 18.77$\pm$0.85 \\
TAWT  & 18.12$\pm$1.80 & 4.84$\pm$0.71 & 25.77$\pm$0.94 & 25.66$\pm$0.38 & 12.40$\pm$0.74 & 23.16$\pm$0.42   \\
\midrule
\textbf{Alg. \ref{alg:task_modeling} (Ours)}  & \textbf{7.63$\pm$2.12} & \textbf{1.06$\pm$0.62} & \textbf{17.25$\pm$1.13} & \textbf{24.96$\pm$0.63} & \textbf{11.34$\pm$1.31} & \textbf{17.66$\pm$0.80}   \\\midrule \midrule
\makecell[l]{Equality of opportunity} & HI & KS & LA & NJ & NV & SC \\ \midrule
STL            & 9.86$\pm$1.29 & 1.43$\pm$3.62 & 29.64$\pm$3.24 & 22.43$\pm$1.02 & 13.61$\pm$3.67 & 29.93$\pm$0.77   \\
Naive MTL            & 3.86$\pm$0.84 & 2.03$\pm$2.11 & 21.26$\pm$1.35 & 24.43$\pm$1.49 & 12.14$\pm$2.21 & 21.22$\pm$1.75  \\
HOA  &3.55$\pm$2.85 & 4.34$\pm$3.18 & 22.88$\pm$1.72 & 22.98$\pm$1.18 & 12.92$\pm$2.23 & 23.31$\pm$1.77 \\
TAG  & 4.27$\pm$0.25 & 1.18$\pm$0.97 & 20.66$\pm$1.43 & 21.89$\pm$0.69 & 11.66$\pm$1.58 & 19.89$\pm$1.10  \\
TAWT  & 4.21$\pm$2.25 & 1.40$\pm$2.14 & 30.38$\pm$2.17 & 23.26$\pm$0.30 & 11.77$\pm$1.01 & 30.86$\pm$0.84  \\
\midrule
\textbf{Alg. \ref{alg:task_modeling} (Ours)}  & \textbf{0.24$\pm$1.32} & \textbf{0.21$\pm$1.34} & \textbf{14.14$\pm$2.32} & \textbf{21.48$\pm$0.90} & \textbf{9.65$\pm$3.49} & \textbf{18.54$\pm$1.61}   \\\bottomrule
\end{tabular}
\end{footnotesize}
\end{table}

\subsection{Ablation Studies for Constructing Surrogate Models}\label{sec_ablate}

\noindent\textbf{Subset size:} 
Recall that we collect training results by sampling $n$ subsets from a uniform distribution over subsets of a constant size.
We evaluate the MSE of task models by varying $\alpha \in \set{2, 5, 10, 20}$.
To control the computation budget the same, we scale the number of subsets $n$ according to $\alpha$. We train $n = 800, 400, 200, 100$ models with $\alpha = 2, 5, 10, 20$, respectively.
We observe similar convergence results as in Figure \ref{fig_varying_alpha}.
Among them, $\alpha = 5$ yields a highest Spearman's correlation of $0.89$ between $f(\cdot)$ and $g(\cdot)$. 


\begin{figure*}[h!]
  \begin{subfigure}[b]{0.245\textwidth}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/err_income_2018_HI_alpha_5.pdf}%
    \caption{$\alpha = 2$}
  \end{subfigure}\hfill%
  \begin{subfigure}[b]{0.245\textwidth}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/err_income_2018_HI.pdf}%
    \caption{$\alpha = 5$}    
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.245\textwidth}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/err_income_2018_HI_alpha_20.pdf}%
    \caption{$\alpha = 10$}    
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.245\textwidth}
    \centering
    \includegraphics[width=0.98\textwidth]{figures/err_income_2018_HI_alpha_40.pdf}%
    \caption{$\alpha = 20$}    
  \end{subfigure}
    \caption{Fitting surrogate models using different $\alpha$ evaluated on a fixed target task.}\label{fig_varying_alpha}  
\end{figure*}




\medskip

\noindent\textbf{Loss function:} We consider three choices of prediction losses, including zero-one accuracy, cross-entropy loss, and classification margin.
We observe that the classification margin is more effective than the other two metrics. %
The Spearman's correlation of using the margin is $0.86$ on average over two tasks (HI and LA).
In contrast, the Spearman's correlations of using the loss and accuracy are $0.61$ and $0.34$, respectively.
Besides, we compare the task selection using the three metrics in Table \ref{tab_ablation_choices_of_f}. We find that using the margin outperforms the other two by 0.37\% on average over the six target tasks in terms of worst-group accuracy.

\begin{table*}[h!]
\centering
\caption{Choosing different loss functions $\ell$ for six target tasks in the multi-group learning setting.}\label{tab_ablation_choices_of_f}
\begin{footnotesize}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
  & HI & KS & LA & NJ & NV & SC \\ \midrule
$f$ uses zero-one accuracy  & 75.16$\pm$0.70 & 76.39$\pm$1.09 & 75.15$\pm$0.43 & 77.40$\pm$0.49 & 74.34$\pm$1.81 & 77.29$\pm$0.19  \\
$f$ uses cross-entropy loss      & 75.33$\pm$0.80 & 75.82$\pm$0.60 & 74.19$\pm$1.37 & 77.51$\pm$0.35 & 74.55$\pm$1.60 & 77.21$\pm$0.27  \\
$f$ uses classification margin    & {75.47$\pm$0.73} & {76.96$\pm$0.69} & {75.62$\pm$0.11} 
 & {78.17$\pm$0.36}  & {75.21$\pm$0.52} & {77.62$\pm$0.34} \\
 \bottomrule
\end{tabular}
\end{footnotesize}
\end{table*}


\medskip
\noindent\textbf{Number of sampled subsets:} Lastly, we show that task selection remains stable under different values of $n$. 
We measure the effect on two tasks (HI and LA) by comparing the 10 tasks with the smallest coefficients estimated from $n = 100, 200, 400$ subsets. 
We observe that using $100$ subsets identifies 7/10 source tasks compared with $n = 400$. Increasing $n$ to $200$ further identifies 9/10 source tasks compared with $n=400$.





