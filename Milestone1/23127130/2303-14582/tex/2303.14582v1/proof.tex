\section{Complete Proofs}\label{sec_proof}


\subsection{Proof of Lemma \ref{lemma_vec}}\label{proof_lemma_1st}

In the first part of the proof, we prove the convergence from $\hat\theta$ to $\bar\theta$ by dealing with the randomness of $S_1, S_2, \dots, S_n$.
Recall that $\cU$ is the uniform distribution over subsets of $\set{1,2,\dots,k}$ with size $\alpha$.
Let $\abs{\cU} = \binom{k}{\alpha}$ denote the number of subsets from $\cU$.

\begin{proof}[Proof of Lemma \ref{lemma_vec}]
    Recall the definitions of $\hat\theta$ and $\bar\theta$ from Section \ref{sec_convergence_guarantees}:
    \begin{align*}
        \hat\theta = \left( \frac{\cI_n^{\top} \cI_n} {n} \right)^{-1} \frac{v}{n}
        \quad \text{ and } \quad
        \bar\theta = \left( \frac{\pmb{\cI}^{\top} \pmb{\cI}}{\abs{\cU}} \right)^{-1} \frac{ \pmb{\cI}^{\top} \pmb{f} } {\abs{\cU}},
    \end{align*}
    where $\abs{\cU}$ denotes the size of distribution $\cU$'s support set.
    We will use the triangle inequality to separate the error between $\hat\theta$ and $\bar\theta$ into two parts:
    \begin{align}
        \bignorm{\hat\theta - \bar\theta}
        = & \bignorm{ \left( \left( \frac {\cI^{\top}_n \cI_n} {n}  \right)^{-1} - \left( \frac {\pmb{\cI}^{\top} \pmb{\cI}} {\abs{\cU}}  \right)^{-1} \right) \frac{v}{n} + \left(\frac{\pmb{\cI}^{\top} \pmb{\cI}}{\abs{\cU}} \right)^{-1} \Big(\frac {v} n - \frac {\pmb{\cI}^{\top} {\pmb{f}}} {\abs{\cU}}\Big)} \nonumber \\
        \le & \bignorm{\left(\frac{{\cI}^{\top}_n {\cI}_n} {n} \right)^{-1} - \left(\frac{\pmb{\cI}^{\top} \pmb{\cI}} {\abs{\cU}} \right)^{-1}}_2 \cdot \bignorm{\frac {v} n} \label{eq_dev_err1} \\
            &+ \bignorm{\left( \frac{\pmb{\cI}^{\top} \pmb{\cI}} {\abs{\cU}}  \right)^{-1}}_2 \cdot \bignorm{\frac{v} n - \frac {\pmb{\cI}^{\top} {\pmb{f}}} {\abs{\cU}}}, \label{eq_dev_err2}
    \end{align}
    where $\norm{\cdot}_2$ denotes the spectral norm (or the largest singular value) of a matrix.
    We compare  $\frac{v}{n}$ and $\frac{\pmb{\cI}^{\top}{\pmb{f}}}{\abs{\cU}}$.
    Recall that both vectors have $k$ coordinates, each corresponding to one task.
    For any task $i = 1, \dots, k$, let $\cE_i$ denote the difference between the $i$-th coordinate of $\frac{v}{n}$ and $\frac{\pmb{\cI}^{\top} {\pmb{f}}}{\abs{\cU}}$:
    \begin{align}
        \cE_i = \frac{1}{n} \sum_{1 \le j \le n:~i \in S_j} f(S_j) - \frac{1}{\abs{\cU}} \sum_{T \in \cU:~i \in T } f(T).
        \label{eq_Ei}
    \end{align}
    Notice that the sampling of $S_1, S_2, \dots, S_n$ is independent of the randomness in $f$.
    Therefore, we have that the expectation of $\cE_i$ is zero:
    \[ \ex{\cE_i} = 0, \text{ for any $i = 1,2,\dots, k$}. \]
    Next, we apply Chebyshev's inequality to analyze the deviation of $\cE_i$ from its expectation.
    We consider the variance of $\cE_i$, which is equal to the expectation of $\cE_i^2$ since the mean of $\cE_i$ is zero:
    {\begin{align}
        \ex{\cE_i^2}
        &= \ex{\left(\frac 1 n \sum_{1 \le j\le n:~ i\in S_j} f(S_j) - \frac 1 {\abs{\cU}} \sum_{T\in \cU:~ i \in T} f(T) \right)^2} \nonumber \\
        &= \ex{\frac 1 {n^2} \left(\sum_{1\le j\le n: i\in S_j} f(S_j)\right)^2
        - \frac 2 {n  \abs{\cU}} \sum_{1\le j \le n:~ i\in S_j} f(S_j) \sum_{T\in \cU:~ i\in T} f(T)
        + \frac 1 {\abs{\cU}^2} \Big(\sum_{T\in \cU:~ i\in T} f(T) \Big)^2 } \label{eq_concen_stab}
    \end{align}}%
    Notice that for any $T\in \cU$ such that $i \in T$, the probability that $T$ is sampled in the training dataset of size $n$ is equal to
    \[ \frac{\binom{\abs{\cU} - 1}{n - 1}}{\binom{\abs{\cU}}{n}} = \frac{n}{\abs{\cU}}. \]
    For any two subsets $T \neq T'$ that are both from $\cU$ such that $i \in T$ and $i \in T'$, the probability that $T$ and $T'$ are both sampled in the training set (of size $n$) is equal to
    \[ \frac{\binom{\abs{\cU} - 1}{n-1}  }{\binom{\abs{\cU}}{n} } \cdot \frac{\binom{\abs{\cU} - 1}{n-1}  }{\binom{\abs{\cU}}{n} }
    = \frac{n^2}{\abs{\cU}^2 }. \]
    Thus, by taking the expectation over the randomness of the sampled subsets in equation \eqref{eq_concen_stab} conditional on $f$, we can cancel out the cross terms for every pair of two tasks $i \neq i'$, leaving only the squared terms as:
    \begin{align*}
        \ex{\cE_i^2} = \ex{\left(\frac 1 {n^2} \frac {n} {\abs{\cU}} - \frac{2}{n\abs{\cU}} \frac {n}{\abs{\cU}} + \frac {1} {\abs{\cU}^2}\right) \sum_{T\in\cU:~ i\in T} \big(f(T) \big)^2}
        \le \frac {C^2} {n} \cdot \frac{\bigabs{T\in\cU:~i\in T}} {\abs{\cU}} \le \frac{C^2}{n},
    \end{align*}
    since the value of  $f$ is bounded from above by an absolute constant $C$.
    Therefore,
    \begin{align*}
        \ex{\sum_{i=1}^k \cE_i^2} \le \frac{C^2 k} {n}.
    \end{align*}
    By Markov's inequality, for any $a > 0$,
    \begin{align*}
        \Pr\left[ {\sum_{i=1}^k \cE_i^2} \ge {\frac {a^2 k } { n}} \right] \le \frac {C^2}{a^2}.
    \end{align*}
    Therefore, with probability at least $1 - \delta$, for any $\delta > 0$, conditional on the randomness of $f$, we have that
    \begin{align}
        \bignorm{\frac {v} n - \frac {\pmb{\cI}^{\top}{\pmb{f}}}{\abs{\cU}}} \le C  \sqrt{\frac{ k} {\delta n }}. \label{eq_err_vec}
    \end{align}
    Next, we use random matrix concentration results to analyze the difference between the indicator matrix of the sampled subsets and the indicator matrix of all subsets in $\cU$.
    Denote by \[ E = \frac {\cI_n^{\top} \cI_n} {n} - \frac{ \pmb{\cI}^{\top} \pmb{\cI}} {\abs{\cU}}
    ~~\text{ and }~~ A = \frac{\pmb{\cI}^{\top} \pmb{\cI}} {\abs{\cU}}. \]
    By the Sherman-Morrison formula calculating matrix inversions, we get
    \begin{align}
         \bignorms{\Big( \frac{\cI_n^{\top} \cI_n}{n}  \Big)^{-1} - \Big( \frac{\pmb{\cI}^{\top} \pmb{\cI} }{\abs{\cU}}  \Big)^{-1}}
        =& \bignorms{(E + A)^{-1} - A^{-1}} \nonumber \\
        =& \bignorms{A^{-1} \Big( A E^{-1} + \id_{k\times k}\Big)^{-1}} \nonumber \\
        =& \bignorms{A^{-1} E \Big( A + E \Big)^{-1}} \nonumber \\
        \le& \big(\lambda_{\min}(A) \big)^{-1} \cdot \norm{E}_{2} \cdot \big(\lambda_{\min}(A + E) \big)^{-1} \nonumber \\
        \le& \frac {\norm{E}_2} {\lambda_{\min}(A) (\lambda_{\min}(A) - \norm{E}_2)}. \label{eq_err_1}
    \end{align}
    We now use the matrix Bernstein inequality (cf. Theorem 6.1.1 in Tropp (2015)) to deal with the spectral norm of $E$.
    Let
    \[ X_i =  {\mathbbm{1}_{S_i} \mathbbm{1}_{S_i}^{\top}} - \frac { \pmb{\cI}^{\top} \pmb{\cI}} {\abs{\cU}}, \text{ for any }~ i = 1, \dots, n. \]
    In expectation over $\cU$, we know that $\ex{X_i} = 0$, for any $i = 1,\dots, n$.
    Additionally, $\norm{X_i}_2 \le 2\alpha$, since it is a linear combination of indicator vectors with $\alpha$ entries of ones in each indicator vector.
    Therefore, for all $t \ge 0$,
    \begin{align*}
        \Pr\left[ \norm{E}_2 \ge t \right]
        = \Pr\left[ \bignorm{\sum_{i=1}^n X_i}_2 \ge nt \right]
        \le 2k \cdot \exp\left( - \frac {(nt)^2 / 2} {(2\alpha)^2 n + (2\alpha) nt / 3}\right).
    \end{align*}
    With some standard calculations,
    this implies that for any $\delta \ge 0$, with probability at least $1 - \delta$,
    \begin{align}
        \norm{E}_2 \le \frac {4\alpha \cdot \log\big({2k} {\delta}^{-1}\big)} {\sqrt n}. \label{eq_err_E}
    \end{align}
    By applying equation \eqref{eq_err_vec} into equation \eqref{eq_dev_err1} and equation \eqref{eq_err_E} into equation \eqref{eq_dev_err2}, we have shown that with probability at least $1 - 2\delta$, for any $\delta \ge 0$,
    \begin{align}
        \bignorm{\hat\theta - \bar\theta}
        \le \bignorm{\frac {v} n}_{2} \cdot \frac{4\alpha  \cdot \log\Big( {2k} {\delta}^{-1}\Big)} {\sqrt n}
        + \frac 1 {\big(\lambda_{\min}(A) \big)^2 \big(\lambda_{\min}(A) - \norm{E}_2\big)} \cdot C  \sqrt{\frac { k} {\delta n}}.
        \label{eq_err_2}
    \end{align}
    Lastly, we examine the norm of $\frac {v} n$. Let $z_i$ be the number of subsets $S_j$ among $1 \le j\le n$ such that $i \in S_j$, for any $i = 1,\dots, n$.
    Recall that the value of $f$ is bounded from above by an absolute constant $C$. Thus, based on the definition of $v$ from equation \eqref{eq_theta_j}, we have:
    \begin{align}
        \bignorm{\frac {v} n } \le \frac 1 n \sqrt{C^2  \sum_{i=1}^k z_i^2}
        \le \frac {C} n \left(\sum_{i=1}^k z_i \right) = C \alpha, \label{eq_vn}
    \end{align}
    since the size of each subset is strictly equal to $\alpha$.
    
    Regarding the minimum eigenvalue of $A$, notice that the diagonal entry of $\frac { \pmb{\cI}^{\top} \pmb{\cI} } {\abs{\cU}}$ is equal to
    $\binom{k - 1} {\alpha - 1}$.
    The off-diagonal entries of this matrix are equal to $\binom{k - 2}{\alpha - 2}$.
    Thus, based on standard algebra, one can prove that
    \begin{align}
        \lambda_{\min}(A) \ge 1 - \frac {\binom{k-2}{\alpha  - 2}} {\binom{k-1} {\alpha  -1}} = 1 - \frac{\alpha  - 1} {k-1}
        \ge 1 - \frac {\alpha} k. \label{eq_lambda_min_A}
    \end{align}
    Applying equations \eqref{eq_vn} and \eqref{eq_lambda_min_A} back into equation \eqref{eq_err_2}, we conclude that with probability at least $1 - 2\delta$, $\hat\theta$ the estimation error between $\hat\theta$ and $\bar\theta$ grows at a rate of $\sqrt {\frac k n}$ as follows:
    \begin{align*}
        \bignorm{\hat\theta - \bar\theta}
        \le 4C\alpha^2 \log({2k}{\delta}^{-1}) \cdot \sqrt{\frac k n} + \Big(1 - \frac {\alpha} k\Big)^{-3} C \alpha  \cdot \sqrt{\frac k {\delta n}}.
    \end{align*}
    Thus, we have proved that equation \eqref{eq_lem1} holds, and the proof is complete.
\end{proof}

\subsection{Proof of Lemma \ref{lemma_rad}}

In the second part, we prove the convergence from $\bar\theta$ to $\theta^{\star}$ by dealing with the randomness of $f$.

\begin{proof}[Proof of Lemma \ref{lemma_rad}]
    Based on the definitions of $\bar\theta$ and $\theta^{\star}$, their difference can be written as follows:
    \begin{align}
        \bignorm{\bar{\theta} - {\theta}^{\star}}
        =& \bignorm{ \Big( {\pmb{\cI}^{\top} \pmb{\cI}} \Big)^{-1} {\pmb{\cI}^{\top}\big(\pmb{f} - \ex{\pmb{f}}\big)} } \label{eq_rad_err_2} \\
        \le& \bignorm{ \Big( \frac { {\pmb{\cI}^{\top} \pmb{\cI}} } {\abs{\cU}} \Big)^{-1} \frac{\pmb{\cI}^{\top}}{\sqrt {\abs{\cU}}} }_2 \cdot {\bignorm{ \frac{\pmb{f} - \ex{\pmb{f}}} {\sqrt{\abs{\cU}}} }}  \nonumber \\
        =& \sqrt{\Bigg(\frac{\pmb{\cI}^{\top} \pmb{\cI} }{\abs{\cU}}\Bigg)^{-1}} \cdot \frac{\bignorm{ {\pmb{f} - \ex{\pmb{f}}} }} {\sqrt {\abs{\cU}}} \label{eq_rad_err_1} \nonumber \\
        \le& \Big(1- \frac{\alpha} k\Big)^{-\frac 1 2} \cdot \frac{\bignorm{\pmb f - \ex{\pmb f}}} {\sqrt{\abs{\cU}}}. \tag{by equation \eqref{eq_lambda_min_A}}
    \end{align}
    For each subset $T \in \cU$, recall that $f(T)$ is the MTL outcome of combining the datasets of all tasks of $T$ with the main target task.
    We will apply a Rademacher complexity-based generalization bound to analyze the generalization error $f(T) - \ex{{f}(T)}$.
    Recall the Rademacher complexity of $\cF$ with $m$ samples from $\cD_t$ is defined in equation \eqref{eq_rad_def}.
    By \citet[Theorem 5]{bartlett2002rademacher}, with probability at least $1 - \delta$, we can get:
    \begin{align}
        f(T) &\le \ex{f(T)} + \frac{\cR_{m}(\cF)} {2}  + \sqrt{\frac {\log\big(1 / \delta\big)} {2 m}}. \label{eq_rad}
    \end{align}
    Similarly, one can get the result for the other directions of the error estimate.
    With a union bound over all subsets $T \in \cU$, with probability at least $1- \delta$, we get:
    \begin{align}
        {f}(T) \le \ex{f(T)} + \frac {\cR_{m}(\cF)} 2 + \sqrt{\frac{\alpha  \log \big(\frac k {\delta}\big)} {2m}}, \text{ for all } T \in \cU, \label{eq_union_bound}
    \end{align}
    since
    \[ \log\left(\frac {\binom{k}{\alpha } }{ \delta}\right) \le \alpha  \log \left(\frac k {\delta}\right). \]
    Let $z = \sqrt{\alpha  \log\big(k\delta^{-1}\big) / (2m)}$.
    Applying equation \eqref{eq_union_bound} back into equation \eqref{eq_rad_err_2}, we have shown
    \begin{align*}
        \bignorm{\bar{\theta} - {\theta}^{\star}}
        &\le \Big(1 - \frac{\alpha}{k}\Big)^{-\frac 1 2} \sqrt{\frac {1} {\abs{\cU}} \sum_{T\in\cU} \left(\frac {\cR_{m}(\cF)} 2  + z\right)^2} \\
        &= \Big(1 - \frac {\alpha} k\Big)^{-\frac 1 2} \left(\frac {\cR_{m}(\cF)} 2 + z\right).
    \end{align*}
    Thus, based on the condition that $\alpha \le k /2$, the proof of equation \eqref{eq_lem2} is complete.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm_converge}.]
Notice that equation \eqref{eq_converge_theta} follows by combining equation \eqref{eq_lem1} from Lemma \ref{lemma_vec} and equation \eqref{eq_lem2} from Lemma \ref{lemma_rad}, together with the condition that $\alpha \le 1/2$.
Thus, the proof of the theorem is finished.
\end{proof}

\textbf{Remark.} Our result depends on the Rademacher complexity of the function class. This complexity measure can be vacuous on real data for deep neural networks.
It would be interesting to incorporate data-dependent generalization bounds in the proof (e.g., \citet{li2021improved,ju2022robust,ju2022generalization}).

\subsection{Convergence of the Empirical Risk}\label{sec_proof_conv}

Based on the results from Lemma \ref{lemma_vec} and Lemma \ref{lemma_rad}, we can also prove the convergence of the loss values.
This is stated precisely in the following result.

\begin{corollary}[of Theorem \ref{thm_converge}]
    In the setting of Theorem \ref{thm_converge}, we have that
    \begin{align}
     \cL(\theta^{\star}) - \hat{\cL}_n\big(\hat\theta\big) 
    \lesssim\,  C \alpha  \cdot\cR_{m}(\cF)  
     + C\alpha^{1.5}\sqrt{\frac { { \log(\delta^{-1}k )}} { m}} 
    + C^2 \alpha^{3.5} \log\Big({}{}\frac{k}{\delta}\Big) \sqrt{\frac { k} { n}} +  C^2 \alpha^{2.5}\sqrt{\frac { {{}k}} {\delta n}}. \label{eq_converge_mse}
    \end{align}
\end{corollary}

\begin{proof}
    To analyze the generalization error of $\hat\theta$, based on equation \eqref{eq_gS}, we can expand the loss term as
    \begin{align}
        \hat{\cL}_n(\hat\theta) &= \frac 1 n\bignorm{\cI_n \hat\theta - \hat f}^2 \nonumber \\
        &= \frac 1 n\bignorm{\cI_n \hat\theta - \exarg{\hat f}{\hat f} + \exarg{\hat f}{\hat f} - \hat f}^2 \nonumber \\
        &= \frac 1 n\bignorm{\cI_n\hat\theta - \exarg{\hat f}{\hat f}}^2 + \frac 2 n\inner{\cI_n \hat{\theta}_n - \exarg{\hat f}{\hat f}}{\exarg{\hat f}{\hat f} - \hat f}
        + \frac 1 n \bignorm{\exarg{\hat f}{\hat f} - \hat f}^2. \label{eq_thm_proof_1}
    \end{align}
    Based on Lemma \ref{lemma_vec}, the distance between $\hat\theta$ and $\theta^{\star}$ is at the order of $\textup{O}(n^{-1/2})$ with high probability.
    We will use this result to deal with the first term in equation \eqref{eq_thm_proof_1} as follows:
    \begin{align}
        & \frac 1 n \bignorm{\cI_n \hat{\theta}_n - \exarg{\hat f}{\hat f}}^2 - \frac 1 n \bignorm{\cI_n \theta^{\star} - \exarg{\hat f}{\hat f}}^2 \label{eq_thm_proof_2} \\
        =& \bigabs{ \frac 1 n\inner{ \cI_n^{\top} \cI_n}{\hat\theta (\hat\theta)^{\top} - \theta^{\star} (\theta^{\star})^{\top}} - \frac 2 n \inner{\exarg{\hat f}{\hat f}} {\hat\theta - \theta^{\star}}} \nonumber \\
        \le& \bignorm{\frac 1 n \cI_n^{\top} \cI_n}_2 \cdot \bignormFro{\theta^{\star} (\theta^{\star})^{\top} - \hat\theta (\hat\theta)^{\top}}
        + \frac 2 n \bignorm{ {\exarg{\hat f}{\hat f}}} \cdot \bignorm{\theta^{\star} - \hat\theta} \tag{by triangle inequality} \\
        \le& \alpha  \bignormFro{\theta^{\star} (\theta^{\star})^{\top} - \hat\theta (\hat\theta)^{\top}}
        + 2C \alpha  \cdot e_1, \nonumber
    \end{align}
    where $e_1$ denotes the right hand side of equation \eqref{eq_converge_theta} and $\bignormFro{X}$ denotes the Frobenius norm of a matrix.
    In the last step, the first part uses the fact that $\cI_n^{\top} \cI_n / n$ is the average of $n$ rank one matrix, each with spectral norm $\alpha$ since they have exactly $\alpha$ ones.
    The second part uses an argument similar to equation \eqref{eq_vn} and the result of equation \eqref{eq_converge_theta}.
    Next,
    \begin{align*}
        \bignormFro{\theta^{\star} (\theta^{\star})^{\top} - \hat{\theta}_n (\hat{\theta}_n)^{\top}}
        &= \bignormFro{\theta^{\star} (\theta^{\star} - \hat{\theta}_n)^{\top} + (\theta^{\star} - \hat{\theta}_n) (\hat{\theta}_n)^{\top}} \\
        &\le \bignormFro{\theta^{\star} (\theta^{\star} - \hat{\theta}_n)^{\top}}
        + \bignormFro{(\theta^{\star} - \hat{\theta}_n) (\hat\theta)^{\top}} \tag{by triangle inequality} \\
        &\le \Big(\bignorm{\theta^{\star}} + \bignorm{\hat\theta}\Big) e_1. \tag{by equation \eqref{eq_converge_theta}}
    \end{align*}
    We show that the norm of $\theta^{\star}$ and $\hat{\theta}_n$ are both bounded by a constant factor times $\sqrt k$.
    To see this,
    \begin{align*}
        \bignorm{\theta^{\star}}
        &= \bignorm{({\pmb{\cI}}^{\top} \pmb{\cI})^{-1} {\pmb{\cI}}^{\top} \ex{\pmb f}} \\
        &\le \bignorm{\Big(\frac { \pmb{\cI}^{\top} \pmb{\cI} } {\abs{\cU}}\Big)^{-1}}_2 \cdot \bignorm{\frac {\pmb{\cI}^{\top} \ex{\pmb f}} {\abs{\cU}}} \\
        &\le \Big(1 - \frac {\alpha}k\Big)^{-1} \cdot C \sqrt{\alpha} \tag{by equation \eqref{eq_lambda_min_A} and the condition that $f$ is bounded by $C$}
    \end{align*}
    Notice that the spectral norm of the difference between $\pmb{\cI}^{\top} \pmb{\cI} / \abs{\cU}$ and $\cI_n^{\top} \cI_n / n$ has been analyzed in equation \eqref{eq_err_E}.
    Thus, with similar steps as above, we can show that
    \begin{align*}
        \bignorm{\hat\theta}
        \le \left(\Big(1 - \frac {\alpha} k\Big)^{-1} + \frac {4\alpha  \log\big( {2k} {\delta}^{-1}\big)} {\sqrt n}\right) C\sqrt k.
    \end{align*}
    To wrap up our analysis above, we have shown that equation \eqref{eq_thm_proof_2} is at most
    \begin{align*}
        e_3 = \alpha  \left(2(1-\alpha/k)^{-1} + \frac{4\alpha \log\big( {2k} {\delta}^{-1}\big)} {\sqrt n}\right) C \sqrt{\alpha} \cdot e_1 + 2C \alpha  \cdot e_1. \label{eq_thm_proof_3}
    \end{align*}
    Next, we consider the second term in equation \eqref{eq_thm_proof_1}.
    Let $e_2 = \frac {\cR_{m}(\cF)} 2 + \sqrt{\frac{\alpha  \log ( k/ {\delta})} {2m}}$ be the deviation error indicated in equation \eqref{eq_union_bound}.
    Thus, every entry of $\hat f - \ex{\hat f}$ is at most $e_2$.
    Besides, each entry of $\cI_n \hat{\theta}_n - \ex{\hat f}$ is less than
    \[ \sqrt{\alpha} \norm{\hat{\theta}_n} + C, \]
    because $\bignorms{\cI_n} \le \sqrt{\alpha}$ and $f$ is bounded from above by $C$.
    Thus, the second term in equation \eqref{eq_thm_proof_1} is less than
    \begin{align*}
        e_4 = e_2 \left(\sqrt{\alpha} \cdot \Big(\Big(1 - \frac{\alpha}k \Big)^{-1} + \frac {4\alpha \log\big({2k} {\delta}^{-1}\big)} {\sqrt n}\Big) C \sqrt{\alpha} + C \right).
    \end{align*}
    For the population loss $\cL(\theta^{\star})$, notice that
    \begin{align}
        \cL(\theta^{\star}) &= \exarg{\pmb f}{\frac 1 {\abs{\cU}}\bignorm{\pmb{\cI} \theta^{\star} - \pmb{f}}^2} \nonumber \\
        &= \exarg{\pmb f}{\frac 1 {\abs{\cU}}\bignorm{\pmb{\cI}\theta^{\star} - \exarg{\pmb f}{\pmb f} + \exarg{\pmb f}{\pmb f} - \pmb f}^2} \nonumber \\
        &= \frac 1 {\abs{\cU}} \bignorm{\pmb{\cI}\theta^{\star} - \exarg{\pmb f}{\pmb f}}^2 + \frac 1 {\abs{\cU}} \left(\exarg{\pmb f}{\bignorm{\pmb f - \exarg{\pmb f}{\pmb f}}^2} \right) \label{eq_concen_err_1}
    \end{align}
    We know that each entry of $\pmb{\cI}{\theta^{\star}} -\ex{\pmb f}$ is at most
    $(1-\alpha/k)^{-1} \sqrt{\alpha} + C$.
    Thus, by Hoeffding's inequality, with probability at least $1 - \delta$, we have
    \begin{align}
        \bigabs{ \frac 1 n \bignorm{\cI_n \theta^{\star} - \exarg{\hat f}{\hat f}} - \frac 1 {\abs{\cU}} \bignorm{\pmb \cI \theta^{\star} - \exarg{\pmb f}{\pmb f}}}
        \le \Big((1 - \alpha/k)^{-1} \sqrt{\alpha} + C\Big)\sqrt{\frac {\log\big(\delta^{-1}\big)} {n}}. \label{eq_hoeff}
    \end{align}
    Lastly, we consider the third term in equation \eqref{eq_thm_proof_1}, compared with the second term in equation \eqref{eq_concen_err_1}.
    For every $T\in\cU$, let $e_T = f(T) - \ex{f(T)}$.
    By equation \eqref{eq_union_bound}, we know that $e_T$ is of order $O(m^{-1/2})$, for every $T\in\cU$.
    Therefore
    \begin{align}
        \bigabs{\frac 1 n \sum_{i=1}^n e_{S_i}^2}
        \le \Big(\frac {\cR_m(\cF)} 2 + \sqrt{\frac{\alpha \log(k/\delta)} {2m}}\Big)^2,
    \end{align}
    which is of order $O(m^{-1})$. Similarly, the same holds for the variance of $\pmb f$ in the second term of equation \eqref{eq_concen_err_1}.
    
    Comparing equations \eqref{eq_hoeff} and \eqref{eq_thm_proof_1}, we have shown that
    \begin{align*}
        \cL(\theta^{\star}) - \hat{\cL}_n(\hat\theta)
        &\le \left((1 - \alpha/k)^{-1} \sqrt{\alpha} + C + C^2\right) \sqrt{\frac {\log(\delta^{-1})} n} + C \cdot e_2 + e_3 + e_4 \\
        &\lesssim (C + C \alpha) \left(\cR_{m}(\cF) + \frac {\sqrt{\alpha \log(k \delta^{-1})}} {\sqrt {m}}  \right) + \frac {C^2 \alpha^{7/2} \log\big(2k\delta^{-1}\big) + 8 C^2 \alpha^{5/2} \delta^{-1/2} \sqrt k} {\sqrt n}.
    \end{align*}
    The above follows by incorporating the definitions of the error terms $e_2, e_3, e_4$.
    Thus, we have proved that equation \eqref{eq_converge_mse} holds.
    The proof is now finished.
\end{proof}



\subsection{Proof of Theorem \ref{thm_analysis}}\label{sec_proof_select}

Recall that $\cI_n\in\set{0,1}^{n\times k}$ is the indicator matrix corresponding to the task indices from the training dataset.
Given a set of tasks $S$ with size $\alpha$, denote their feature matrices and label vectors as $(X_{1}, Y_1)$, $(X_{2}, Y_{2})$, \dots, $(X_{\alpha}, Y_{\alpha})$.
With hard parameter sharing \cite{yang2020analysis}, we minimize
\begin{align}
    \ell(B) = \sum_{i = 1}^{\alpha} \bignorm{X_{i} B - Y_{i}}^2. \label{eq_hps}
\end{align}
The minimizer of $\ell(B)$, denoted as $\hat B$, is equal to the following
\begin{align*}
        \hat{B} = \left(\sum_{i=1}^{\alpha} X_{i}^{\top} X_{i}\right)^{-1}  \left(\sum_{i=1}^{\alpha} X_{i}^{\top} Y_{i}\right).
\end{align*}
For isotropic covariates, by matrix concentration results, the loss of using $B$ on the validation set of the target task is equal to
\begin{align*}
    f(S) = \bignorm{\hat B - \beta^{(t)}}^2 + \bigo{\sqrt{\frac p m}}.
\end{align*}

First, we state the proof of Lemma \ref{lemma_gap} from Section \ref{sec_select}.

\begin{proof}[Proof of Lemma \ref{lemma_gap}]
    We have that $Y_{i}= X_{i} \beta^{(i)} + \epsilon^{(i)}$, where $\epsilon^{(i)}$ is a random vector whose entries are sampled independently with mean $0$ and variance $\sigma^2$.
    We have
    \begin{align}
        f(S) = \bignorm{\left(\sum_{i=1}^{\alpha} X_{i}^{\top} X_{i}\right)^{-1} \sum_{i=1}^{\alpha} X_{i}^{\top} \epsilon^{(i)}}^2.
    \end{align}
    For a task $i$, we know that its coefficient is equal to the $i$-th entry of
    \[ \Big(\frac {\cI_n^{\top} \cI_n} n  \Big)^{-1} \frac {\cI_n^{\top} \hat f} n. \]
    Let $Z = \cI_n^{\top}\cI_n / n$.
    By equation \eqref{eq_cov}, for any $i \neq j$, we observe that
    \begin{align*}
			\bigabs{\frac {\hat\theta_i - \hat\theta_j} n - \frac{k}{\alpha} \cdot \frac {v_i - v_j} n}
			&= \bigabs{(e_i - e_j)^{\top} \big(Z^{-1} - \ex{Z}^{-1}\big) \frac {v} n} \\
			&\le \norm{e_i - e_j} \cdot \bignorm{Z^{-1} - \ex{Z}^{-1}}_2 \cdot \bignorm{\frac {v} n} \\
			&\le 2C \alpha   \cdot \bignorm{Z^{-1} - \ex{Z}^{-1}}_2 \tag{by equation \eqref{eq_vn}} \\
			&\le \frac {4\alpha  \log\big( {2k} {\delta}^{-1}\big)} {\sqrt n} \frac 2 {(1- \alpha/k)^2}. \tag{by equations \eqref{eq_err_1}, \eqref{eq_err_E}, \eqref{eq_lambda_min_A}}
	\end{align*}
	The last step follows by applying equations \eqref{eq_err_E} and \eqref{eq_lambda_min_A} into equation \eqref{eq_err_1}.
	Thus, we have finished the proof of equation \eqref{eq_lem_gap}.
\end{proof}


Second, we show that provided $n$, and $d$ are sufficiently large, a separation exists in the coefficients of $v$ between good and bad tasks.

\begin{proof}[Proof of Theorem \ref{thm_analysis}]
    We calculate $v_i / n$ for all $i = 1,\dots,k$ and compare its value between a good task and a bad task.
    We first compare their expectations over the randomly sampled subsets.
    By equation \eqref{eq_err_vec}, we get
    \begin{align*}
        &\bigabs{\frac{v_i} n - \frac 1 {\abs{\cU}} \sum_{T\in\cU:~ i \in T} f(T)}
        \le \frac{C k \delta^{-1/2}} {\sqrt n}, \text{ and }\\
        &\bigabs{\frac{v_j} n - \frac 1 {\abs{\cU}} \sum_{T\in\cU:~ j \in T} f(T)}
        \le \frac{C k \delta^{-1/2}} {\sqrt n}.
    \end{align*}
    Therefore, by applying the triangle inequality with the above two results, we get
    \begin{align}
        \bigabs{\frac{v_i - v_j} n
        - \frac{\sum_{T\in\cU: i \in T} f(T) - \sum_{T\in\cU: j \in T} f(T)} {\abs{\cU}}}
        \le \frac {2C k \delta^{-1/2}} {\sqrt n}. \label{eq_cov1}
    \end{align}
    To deal with equation \eqref{eq_cov1}, we apply a union bound over the sample covariance matrix of every subset $T$ in $\cU$ to show that they are close to their expectation.
    By Gaussian covariance estimation results (e.g., \citet[equation (6.12)]{wainwright2019high}), for a fixed $T\in\cU$ such that $T = \set{i_1, i_2, \dots, i_{\alpha}}$, we get
    \begin{align}
        \bigabs{\frac 1 {\alpha d} \sum_{j\in T} X_{j}^{\top} X_{j} - \id_{p\times p}}
        \le 2 \sqrt{\frac p {\alpha d} } + 2\epsilon + \left( \sqrt{\frac p {\alpha d} } + \epsilon \right)^2, \label{eq_task_prob}
    \end{align}
    with probability at least $1 - 2\exp\big(- \frac 1 2{\alpha d \epsilon^2} \big)$.
    With a union bound over all $T\in\cU$, we have that the above holds with probability at least $1- \delta$ for all $T\in \cU$, for $\epsilon$ that is equal to
    \[ \epsilon = \sqrt {\frac {2\alpha k \log(2k  \delta^{-1}) }  {\alpha d}}. \]
    
    Let $\varepsilon_1$ denote the error term from equation \eqref{eq_task_prob}, by inserting the value of $\epsilon$:
    \[ \varepsilon_1 = 2 \sqrt{\frac p {\alpha d}} + 2\sqrt{\frac {2\alpha \log(2k \delta^{-1})} {\alpha d}} + \left( \sqrt{\frac p {\alpha d} }+ \epsilon\right)^2. \]
    Let \[ u_T = \frac 1 {\alpha d} \sum_{j\in T} X_{j}^{\top} \epsilon^{(j)}, \text{ for any $T \in \cU$}. \]
    One can verify that
    \begin{align*}
        \bigabs{f(T) - \bignorm{u_T}^2}
        \le \big({(1 - \varepsilon_1)^{-2} - 1}\big) \bignorm{u_T}^2
        \le 3 \varepsilon_1 \bignorm{u_T}^2.
    \end{align*}
    Notice that
    \begin{align*}
        \ex{\norm{u_T}^2} = \ex{\frac 1 {(\alpha d)^2} \bigtr{\sum_{j\in T} X_j^{\top} \varepsilon^{(j)} (\varepsilon^{(j)})^{\top} X_j}}.
    \end{align*}
    If $j$ is a good task, then the expectation over $\varepsilon^{(j)}$ is equal to $a^2\id$ by the assumption of Theorem \ref{thm_analysis}.
    If $j$ is a bad task, on the other hand, then the expectation over $\varepsilon^{(t)}$ is equal to $b^2\id$.
    
    Let $s(T)$ denote the number of good tasks in $T$, for any $T \subseteq \set{1, 2, \dots, k}$. Thus,
    \begin{align}
        \ex{\norm{u_T}^2} = \frac {p \Big(a^2 s(T) + b^2 \big(\alpha - s(T)\big) \Big)} {\alpha^2 d}. \label{eq_sT}
    \end{align}
    
    To argue about the deviation error of $\norm{u_T}^2$, we use the following two estimates (see, e.g., \citet{vershynin2011spectral}), which holds with high probability:
    \begin{align*}
        &\bigabs{\big(\varepsilon^{(j)}\big)^{\top} X_j X_j^{\top} \varepsilon^{(j)} - \ex{\big(\varepsilon^{(j)}\big)^{\top} X_j X_j^{\top} \varepsilon^{(j)}} }\lesssim p \sqrt d a^2, \text{ for any } j = 1, \dots, k; \\
        &\bigabs{\big(\varepsilon^{(i)}\big)^{\top} X_i X_j^{\top} \varepsilon^{(j)}} \lesssim p \sqrt d a^2, \text{ for any } 1\le i < j \le k.
    \end{align*}
    Therefore, we get that for any $T \in \cU$,
    \begin{align}
        \bigabs{\bignorm{u_T}^2 - \ex{\bignorm{u_T}^2}}
        \le \frac {p \sqrt d a^2} {d^2}. \label{eq_uT}
    \end{align}
    
    To finish the proof, consider a good task $i$ versus a bad task $j$.
    We need the gap in the expectation term between the good/bad tasks to dominate the standard deviation from the error terms.
    The gap in the expectations is based on equation \eqref{eq_sT}.
    The standard deviation terms are upper bounded by the sum of equations \eqref{eq_cov1} and \eqref{eq_uT}.
    
    Thus, provided that
    \begin{align}
        (1 - 3\varepsilon_1) \frac {p (a^2 - b^2)} {\alpha^2 d}
        \ge (1 + 3\varepsilon_1) \frac {p \sqrt d a^2} {d^2} + \frac {2 C k \delta^{-1/2}} {\sqrt n}, \label{eq_condition}
    \end{align}
    there must exist a threshold separating all the good tasks from the bad ones.
    We can verify that condition \eqref{eq_condition} is satisfied when
    \begin{align*}
        n &\gtrsim C^2 \cdot k^2 \cdot \frac{1}{(a^2 - b^2)^2}, ~~\text{ and}\\
        d &\gtrsim \Big(\frac {a^2} {a^2 - b^2}\Big)^2  k^4 + k \log\Big(\frac {2k} {\delta}\Big) + p.
    \end{align*}
    
    To apply Algorithm \ref{alg:task_modeling}, we set the threshold $\gamma$ as $k / \alpha$ times any value between the left-hand and right-hand side of equation \eqref{eq_condition} (recall that $k / \alpha$ is inherited from Lemma \ref{lemma_gap}).
    Thus, when $n$ and $d$ satisfy the condition above, combined with Lemma \ref{lemma_gap}, with high probability, for any $i$ such that $\hat\theta_i < \gamma$, $i$ must be a good task.
    When $\hat\theta_i > \gamma$, $i$ much be a bad task.
    Thus, we have finished the proof.
\end{proof}