\section{Predicting Multitask Learning Performances Using Surrogate Models}\label{sec_approach}

This section describes the design and analysis of surrogate models for multitask learning.
We begin by defining the problem setup.
Then, we describe the construction of surrogate models and the estimation of the relevance scores.
Lastly, we analyze the sample complexity of the construction procedure.
As described in the introduction, our approach involves two steps.
This section talks about the first step of our approach.
The second step will be presented in the next section.

\subsection{Preliminaries}

\textbf{Problem Setup.}
Let $t = 0$ denote the main target task of interest.
Suppose the task's features and labels are drawn from an unknown distribution, denoted as $\cD_t$.
Let $\cX$ denote the feature space. Let the set of all possible labels be denoted as $\cY$.
We are given a dataset, which includes a list of examples drawn independently from $\cD_t$.
Besides, we are also given $k$ datasets from related source tasks,
which are all supported on $\cX \times \cY$.

A naive approach to optimize MTL is combining all the datasets and evaluating the trained model on the target task.
However, this might result in worse performance than single-task learning.
Thus, it is crucial to identify if a source task would help or hurt.
The most thorough solution for addressing this question is by enumerating all possible combinations of source tasks, leading to a total of $2^k$ combinations.
For each combination of source tasks, train a multitask model using the selected source tasks and the main task.
While this procedure optimizes the performance of MTL, it is too slow for large $k$.

How can we optimize the performance of MTL efficiently?
Relatedly, given a set of source tasks, can we predict their transfer effects upon the target task efficiently? 
Below, we define two common transfer effects.

\textbf{Positive vs. Negative Transfer.}
Consider any multitask learning algorithm, denoted as $\cA$, which trains a joint model given any set of tasks.
For any subset $S \subseteq \set{1,2,\dots,k}$, we say that $S$ provides a negative transfer to $t$ if the performance of $\cA(S \cup \set{t})$ is worse than $\cA(\set{t})$ (e.g., in terms of higher loss values).
Likewise, we say that $S$ provides a positive transfer to $t$ if the performance of $\cA(S \cup \set{t})$ is better than $\cA(\set{t})$.
We aim to design a scalable method to predict such positive and negative transfer effects.

It is worth highlighting that both types of transfers are often observed in practice.
To give an example, we consider a binary classification dataset that involves a total of $51$ tasks.
We pick one of them as the target task, use the rest as source tasks, and consider the case where $\abs{S} = 1$.
This leads to training $50$ models for each target task, one for every combination of one source task and the target task.
The results are shown in Figure \ref{fig_source_tasks}, which provides illustrations for four different target tasks.
The $y$-axis corresponds to the accuracy difference between the MTL and STL results.
We consistently find a mix of positive and negative transfers for all four target tasks.

\revision{\textbf{Surrogate Models.} A recent paper by \citet{ilyas2022datamodels} designs a linear regression method to predict the predictions of deep neural networks trained on a subset of training data. A surprising finding from the paper of \citet{ilyas2022datamodels} is that linear regression models provide a good fit on a number of popular benchmark datasets such as CIFAR. This finding is later studied in a follow-up paper using harmonic analysis \cite{saunshi2022understanding}.
Both papers focus on single-task supervised learning.
In this paper, we aim to apply the idea of surrogate modeling to multi-task learning.
We will elaborate more on related work in Section \ref{sec_related}.}


\begin{figure*}[t!]
  \begin{minipage}[b]{0.24\textwidth}
    \centering
    \includegraphics[width=0.975\textwidth]{figures/source_tasks_income_2018_HI.pdf}
  \end{minipage}\hfill%
  \begin{minipage}[b]{0.24\textwidth}
    \centering
    \includegraphics[width=0.975\textwidth]{figures/source_tasks_income_2018_LA.pdf}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.24\textwidth}
    \centering
    \includegraphics[width=0.975\textwidth]{figures/source_tasks_income_2018_NV.pdf}
  \end{minipage}\hfill
  \begin{minipage}[b]{0.24\textwidth}
    \centering
    \includegraphics[width=0.975\textwidth]{figures/source_tasks_income_2018_SC.pdf}
  \end{minipage}
  \caption{Illustration of mixed outcomes in multitask learning: In each figure, we train $50$ multitask models based on one fixed target task and one source task from $50$ source tasks.
  \textbf{$\boldsymbol{x}$ axis}: Each bar represents one source task from $50$ source tasks. 
  \textbf{$\boldsymbol{y}$ axis}: Test accuracy of MTL with one source task minus the test accuracy of STL with the target task alone.
  For further description of the experiment setup, see Section \ref{sec:exp_setup}.}
  \label{fig_source_tasks}
\end{figure*}





\subsection{Constructing the Linear Surrogate Model} %

In the first step of our approach, we aim to build an approximation of the multitask learning performances.
We first specify the definition of the MTL performance of a single subset of source tasks.
Given any subset $S$, let $\phi$ be an encoder that is shared by the source tasks and also the target task.
For any input features $x\in\cX$, the encoder $\phi$ maps $x$ into a feature vector.
For every source task in $S$ and the target task, there is a separate prediction layer for each of them.
Let $\psi_{0}, \psi_1, \psi_2, \dots, \psi_{k}$ denote the prediction layers, which map the feature vectors to the output.

We train an MTL model by fitting the parameters of $\phi$ and $\psi_i$, for $i \in S \cup\set{0}$.
We minimize the average loss over the combined training data along with the target task.
Let $\phi^{(S)}$ and $\psi^{(S)}_i$, for $i \in S \cup \set{0}$, be the trained model.
We evaluate its loss on the target task's validation dataset.
Let $\widetilde\cD_t = \set{(x_1, y_1), \dots, (x_m, y_m)}$ denote a set of $m$ independent samples from $\cD_t$, which is used as a validation set for the target task.
Let $\ell$ be a non-negative loss such as the cross-entropy loss.
We define \emph{multitask learning performances} as:
\begin{align}
    f(S) = \frac 1 m \sum_{i=1}^m \ell\Big(\psi_0^{(S)}\big(\phi^{(S)}(x_i)\big), y_i\Big), \text{ for any $S\subseteq\set{1,2,\dots,k}$}. \label{eq_ft}
\end{align}%
Our main idea is to construct a surrogate model, parametrized by a relevance score $\theta_i$ for each source task $i$.
We use a linear specification inspired by recent work of \citet{ilyas2022datamodels}:\footnote{We use this specification for scalability consideration. Note that it is possible to consider more complex specifications, such as adding quadratic variables $\theta_{1,2}, \theta_{1,3} \dots,\theta_{k-1,k}$. The construction procedure and the analysis is conceptually the same. However, the sample complexity for fitting these quadratic variables is $\textup{O}(k^2)$, rendering it infeasible for large $k$, e.g., $k \ge 100$.}
\begin{align}
    g_{\theta}(S) = \sum_{i \in S} \theta_i, \text{ where $\theta_i$ is the $i$-th entry of $\theta$, for any $i = 1,2,\dots, k$}. \label{eq_gS}
\end{align}%
The procedure for estimating $\theta$ is as follows.
First, sample $n$ subsets of source tasks from $\set{1, 2,\dots, k}$, denoted as $S_1, S_2, \dots, S_n$.
\revision{We sample each subset from the uniform distribution over subsets with a fixed size of $\alpha$; We will justify this choice later in Section \ref{sec_analyze_thresholding}.
For instance, to capture the transfer from five source tasks to the target task, we can set $\alpha = 5$.}
Then, compute the value of $f(S_i)$ by training one MTL model for every $i = 1, 2, \dots, n$.
Lastly, minimize the mean squared error (MSE) between $g_{\theta}(S_i)$ and $f(S_i)$, averaged over all $i$:
\begin{align}\label{eq_fit_task_model}
    \hat\cL_n(\theta) = \frac 1 n \sum_{i=1}^n \Big(g_{\theta}(S_i) - f(S_i)\Big)^2.
\end{align}
Let $\hat\theta$ denote the minimizer of the above MSE. For brevity, we refer to $\hat\theta$ as the task model.
\revision{After estimating $\hat\theta$, for an unseen subset of source tasks $S$, we predict its MTL performance as $g_{\hat\theta}(S) = \sum_{i \in S} \hat\theta_i$.}

\subsection{Sample Complexity for Estimating the Linear Model}\label{sec_convergence_guarantees}

Next, we provide a theoretical analysis of the estimation of $\hat\theta$.
We show that given $n = \textup{O}\big(k\log^2(k)\big)$, we can estimate $\hat\theta$ accurately.
To be precise, %
let $\cU$ denote the uniform distribution over all subsets of size $\alpha$ drawn from $\set{1,2,\dots,k}$.
Let $T$ denote an unseen subset drawn from $\cU$.
The population risk for a given $\theta$ is defined as the expected MSE between $f(T)$ and $g_{\theta}(T)$: %
{\begin{align}
    \cL(\theta) = \mathop{\mathbb{E}}_{f}\exarg{T}{{\big( f(T) - g_{\theta}(T) \big)^2}}. \label{eq_tm_popu}
\end{align}}%
Let the minimizer of the above risk be denoted as $\theta^{\star}$.
We prove that $\hat\theta$ converges to $\theta^{\star}$ using Rademacher complexity-based arguments.
Let the function class of $\psi_t$ and $\phi$ be denoted as $\cH$.
Let the loss function class be
$\cF = \{\ell(\psi_t(\phi(x)), y) \ |\  \forall\, \psi_t, \phi \text{ from $\cH$} \}.$
\revision{Recall that $\tilde\cD_t = \set{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)}$ refers to the dataset used to evaluate the value of $f(T)$, and its size is equal to $m$.}
Let $\sigma_1, \sigma_2, \dots, \sigma_{m}$ be $m$ independent Rademacher random variables, collectively as $\sigma_{1:m}$.
The Rademacher complexity of $\cF$ over $\widetilde\cD_t$ is
defined as %
\begin{align}
    \cR_{m}(\cF) = \mathop{\mathbb{E}}_{\widetilde\cD_t}\exarg{\sigma_{1:m} }{\sup_{h\in\cF} \frac 1 {m} \sum_{i = 1}^m \sigma_i \cdot h\big(x_i, y_i\big)}, \label{eq_rad_def}
\end{align}
\revision{where the expectation is taken over the randomness of the empirically-drawn dataset $\tilde \cD_t$ and the Rademacher random variables $\sigma_{1:m}$.}
We follow the convention of big-O notations for stating the result.
Given two functions $h(n)$ and $h'(n)$, we use $h(n) = \textup{O}(h'(n))$ or $h(n) \lesssim h'(n)$   to indicate that $h(n) \le C \cdot h'(n)$ for some fixed constant $C$ when $n$ is large enough.
Our result is stated formally below.

\medskip
\begin{theorem}\label{thm_converge}
    Suppose the functions in $\cF$ are all bounded from above by a fixed constant $C > 0$.
    Suppose $\alpha$ is less than $k/2$.
    Let $n$ be the number of sampled subsets and $m$ be the size of the set used to evaluate $f$.
    With probability at least $0.99$, $\hat{\theta}_n$ converges to $\theta^{\star}$ as $n, m$ are both large enough:
    {\begin{align}
        \bignorm{\hat\theta - \theta^{\star}} \lesssim\,  
        C \sqrt{ \frac {\big(k \log^2\big({k}\big) \big) \alpha^4} { n}} 
        + \sqrt{\frac {{\big(\log({k}{})\big) \alpha}} { m}} + \cR_{m}(\cF), \label{eq_converge_theta}
        \end{align}}%
    where $\norm{\cdot}$ denotes the Euclidean norm of a vector.
\end{theorem}%
Based on the above result, it is clear from equation \eqref{eq_converge_theta} that provided with $\textup{O}(k\log^2(k))$ random samples, the first error term relating to $n$ shrinks to a negligible value  (one may think of $\alpha$ as a fixed constant such as $5$ or $10$).
There are two error terms decreasing with $m$, the size of $\widetilde \cD_t$ used to evaluate $f$.
The Rademacher complexity $\cR_m(\cF)$ is known to be of order $\textup{O}(m^{-1/2})$ when $\cH$ represents a family of neural networks \cite{bartlett2017spectrally}.
These two error terms are due to the variance of $f$ since it is measured on a finite set.
\revision{Lastly, we note that the probability value of $0.99$ in the above theorem statement can be adjusted to other values. In the proofs, we state the result more generally for any probability value $1 - \delta$, where $\delta >0$; See the statements of Lemma \ref{lemma_vec} and Lemma \ref{lemma_rad} below for details.}




\medskip
\textbf{Proof Overview.}
We introduce a few notations to examine $g_{\theta}(S)$ and the covariance of $\cU$.
Let $\cI_n \in \set{0,1}^{n \times k}$ be a zero-one matrix; For any $i = 1, 2,\dots, n$, the $i$-th row is $\mathbbm{1}_{S_i}$, the characteristic vector of $S_i$.
Let $\hat f$ be a vector in which $\hat f_i = f(S_i)$, for any $i = 1, 2,\dots, n$.
The $\hat\theta$ that minimizes equation \eqref{eq_fit_task_model} is equal to
\begin{align}
    \hat\theta = \left( {\cI_n^{\top} \cI_n}  \right)^{-1} \cI_n^{\top} \hat f. \label{eq_theta_hat}
\end{align}
Let $v = \cI_n^{\top} \hat f$ and let $v_i$ be the $i$-th entry of $v$, for $i = 1,\dots,k$.
Based on the definition of $\cI_n$, we observe that
\begin{align}
     v_i = \sum_{1\le j \le n:\, i \in S_j} f(S_j), \text{ for any } 1\le i\le k. \label{eq_theta_j}
\end{align}
Next, let $\pmb{\cI} \in \set{0,1}^{\abs{\cU} \times k}$ be a zero-one matrix, where $\abs{\cU}$ is the number of subsets in $\cU$.
Each row of $\pmb{\cI}$ corresponds to the characteristic vector of a subset.
Let $\pmb{f}$ be a vector such that each entry of this vector corresponds to the MTL performances (cf. equation \eqref{eq_ft}) of a subset in distribution $\cU$.
The population risk minimizer $\theta^{\star}$ for reducing $\cL(\theta)$ in equation \eqref{eq_tm_popu} is equal to
\[ \theta^{\star} = \left( \pmb{\cI}^{\top} \pmb{\cI}  \right)^{-1}  {\pmb{\cI}^{\top} \ex{\pmb{f}} }. \]
Our proof involves two steps.
First, we deal with the error due to the randomness of the random subsets.
Let \[ \bar\theta = \left({\pmb{\cI}^{\top} \pmb{\cI}} {}\right)^{-1} {\pmb{\cI}^{\top} \pmb{f}} {}. \]
We state the following result, which shows that $\hat\theta$ converges to $\bar\theta$ as $n$ increases.

\medskip
\begin{lemma}\label{lemma_vec}
    In the setting of Theorem \ref{thm_converge}, conditional on $f(T)$ for any subset $T\in\cU$, with probability $1 - 2\delta$ over the randomness of $S_1, S_2, \dots, S_n$, for any $\delta \ge 0$,
    the Euclidean distance between $\hat\theta$ and $\bar\theta$ satisfies:
    \begin{align}
        \bignorm{\hat\theta - \bar\theta}
        \le 4C  \sqrt{\frac {\big(k \log^2( 2{}{k}\delta^{-1})\big) \alpha^4} n} + 8C \sqrt{\frac {k \alpha^2} {\delta n}}. \label{eq_lem1}
    \end{align}
\end{lemma}

The proof of the above result relies on a novel union bound taken over all subsets in $\cS$. %
Crucially, there are at most $2^k$ subsets.
By taking the logarithm of $2^k$ after the union bound, we get a factor of $k$ as shown in equation \eqref{eq_lem1}.
Second, we prove the convergence from $\bar\theta$ to $\theta^{\star}$, as $m$ increases.

\medskip
\begin{lemma}\label{lemma_rad}
    In the setting of Theorem \ref{thm_converge}, for any $\delta > 0$, with probability at least $1 - \delta$ over the randomness of $S_1, S_2, \dots, S_n$ and $f(S_1), f(S_2), \dots, f(S_n)$, the Euclidean distance between $\bar{\theta}$ and ${\theta}^{\star}$ satisfies:
    \begin{align}
        \bignorm{\bar{\theta} - \theta^{\star}}
        \le \frac{\cR_{m}(\cF)} {\sqrt 2} + \sqrt{\frac{\big(\log\big(  {\delta^{-1} k}\big)\big) \alpha} {4m}}. \label{eq_lem2}
    \end{align}
\end{lemma}

Combining Lemma \ref{lemma_vec} and Lemma \ref{lemma_rad}  together, we have thus proved that equation \eqref{eq_converge_theta} holds.
The proof of the above two results can be found in Appendix \ref{sec_proof}.
This result justifies using a linear specification, as we can scale up the sample complexity.

\medskip
\begin{remark} \normalfont \revision{The proof of Theorem \ref{thm_converge} uses the design of the $\alpha$-sized subsets. In particular, the covariates of these $\alpha$-sized subsets are zero-one vectors, with the ones being drawn randomly.
We show that the population covariance of all the $\alpha$-sized subsets is an identity matrix plus a rank-one matrix. See equation \eqref{eq_cov} in Section \ref{sec_theory} for the derivation. This implies that the inverse of the covariance matrix is an identity matrix plus a rank-one matrix, which is crucial for our subset selection procedure described next.}
\end{remark}



\section{Subset Selection for Multitask Learning}\label{sec_theory}

We now describe the second step of our approach.
Recall that this step performs subset selection on all the source tasks.
Towards this end, we will optimize the target task's performance based on the approximations provided by the surrogate model.
We observe that the best subset predicted by the surrogate model corresponds to placing a threshold on the source tasks' relevance scores.
Then, we will analyze this algorithm in a simple setting where the tasks are separated into two groups. The first group is more similar (measured by Euclidean distances) to the target task than the second group.
We prove that our algorithm is guaranteed to find the first group of source tasks.

\subsection{Selecting Source Tasks by Thresholding Relevance Scores}\label{sec_select}

Provided with the surrogate model, we can use its predicted MTL performances as a proxy to optimize the target task's prediction performance.
We consider subset selection by minimizing the function value of $g_{\hat\theta}(S)$ over subset $S$.
Due to the linear specification of $g_{\hat\theta}$, this is equivalent to selecting source tasks with a small $\hat\theta_i$.
Thus, we select a source task $i$ if $\hat\theta_i$ is below the desired threshold $\gamma$, which can be determined via cross-validation.
Then, we train a model by combining the selected source tasks with the target task.
The complete procedure is shown below.
We will rigorously justify the existence of a threshold afterward.

\begin{algorithm}[h!]
	\caption{\textbf{Subset Selection for Multi-Task Learning}}\label{alg:task_modeling}
	\begin{small}
		\textbf{Input}: $k$ source tasks; Training and validation datasets of the target task.\\
		\textbf{Require:} Size of each subset $\alpha$; Number of sampled subsets $n$; MTL algorithm $f$; Task selection threshold $\gamma$. \\
		\textbf{Output}: Trained model $\phi^{(S^{\star})}, \psi_t^{(S^{\star})}$.
		\begin{algorithmic}[1] %
			\STATE For $i = 1, \dots, n$, sample a random subset $S_i$ from $\set{1,2,\dots,k}$ with size $\alpha$; evaluate $f(S_i)$ following equation \eqref{eq_ft}.
			\STATE Estimate the relevance scores $\hat\theta$ following equation \eqref{eq_fit_task_model}.
			\STATE Select source tasks based on their relevance scores: $S^{\star} = \big\{i: \hat\theta_i < \gamma ~\mid \forall\, i = 1, 2, \dots, k\big\}$.
            \STATE Train a model by combining $S^{\star}$ and $t$; denote the trained model as $\phi^{(S^{\star})}$, and $\psi_i^{(S^{\star})}$ for all $i \in S^{\star} \cup \set{t}$.
		\end{algorithmic}
	\end{small}
\end{algorithm}


\subsection{Analysis of the Algorithm}\label{sec_analyze_thresholding}

Next, we present an analysis of our algorithm in a simple setting where the dataset labels are created following a linear relationship.
For the simplicity of the analysis, we also assume that the input features for each task are drawn from an isotropic Gaussian distribution with $p$ dimensions.
For each task $i$ from $0$ to $k$, let $\beta^{(i)} \in \real^p$ denote the unknown linear model parameters for task $i$.
Given a data point from task $i$ with feature vector $x$, its label is generated as $y = x^{\top} \beta^{(i)} + \epsilon$, where $\epsilon$ is a random variable with mean $0$ and variance $\sigma^2$.

Suppose there are two groups of tasks depending on their distances to $\beta^{(t)}$, given by $a, b$ such that $b > a > 0$.
For every $i = 1, \dots, k$, task $i$ is called
    a \emph{good} task if $\norm{\beta^{(i)} - \beta^{(t)}} \le a$; %
    On the other hand, $i$ is a \emph{bad} task if $\norm{\beta^{(i)} - \beta^{(t)}} \ge b$. %
We show that there exists a threshold that separates the good tasks from the bad tasks under our setting, stated formally as follows.
Given the existence of this threshold, we could then find it in practice via cross-validation.

\smallskip
\begin{theorem}\label{thm_analysis}
    In the setting described above, suppose $f(\cdot)$ is bounded from above by a fixed constant $C > 0$.
    Suppose there are $d \gtrsim k \log k + p + {a^4 k^4}(a^2 - b^2)^{-2}$ data samples from every source task and the target task.
    Suppose $n \gtrsim {C^2} k^2 {(a^2 - b^2)^{-2}}$ and $m\gtrsim p\log p$.
    With probability at least $0.99$, there exists a threshold $\gamma$ such that the following holds: %
    \begin{itemize}[leftmargin=20pt,topsep=0.0pt,itemsep=3.0pt,partopsep=0pt,parsep=0pt]
        \item For any $i = {1,2,\dots,k}$, if task $i$ is a good task, then $\hat\theta_i < \gamma$.
        \item Otherwise, if task $i$ is a bad task, then $\hat\theta_i > \gamma$.
     \end{itemize}
\end{theorem}

\medskip
\textbf{Proof Overview.}
The intuition behind the above result is that $\hat\theta_i$ averages the MTL performances of all subsets involving $i$.
If $i$ is a good task, the average performance will be lower, leading to a lower relevance score.
Moreover, there exists a threshold that separates the relevance scores of good tasks and bad tasks.
We give a toy example to illustrate why $\hat\theta$ can preserve the Euclidean distance gaps from $\beta$.
Our experiments later also confirm the existence of such a separation (cf. Figure \ref{fig_correct_labels}).

\medskip
\begin{example}[A one-dimensional example]
Consider a one-dimensional case where $p = 1$ and every $\beta$ is a real value. 
Let $\beta^{(t)} = 0$.
Let $0 < \beta^{(i)} < a$ if $i$ is a good task.
Let $b < \beta^{(i)}$ if $i$ is a bad task.
\end{example}
\begin{itemize}[leftmargin=20pt, topsep=0.0pt,itemsep=3.0pt,partopsep=0pt,parsep=0pt]
    \item Our first observation is that $\hat\theta_i$ is proportional to $v_i$ (cf. equation \eqref{eq_theta_j}), as shown in Lemma \ref{lemma_gap}.

    \item Our second observation is that $v_i$ is proportional to $\beta^{(i)}$.  
    This is because $v_i$ is the sum of $f(S)$ among all $S \in \cU$ involving $i$, and $f(S)$ is the average of $\beta^{(j)}$ among $j \in S$.
    Thus, $v_i$ is the average of all $\beta$'s from the $n$ random subsets, while $\beta^{(i)}$ has a larger weight in $v_i$ than the other $\beta$'s because $i$ is always in $S$.
\end{itemize}
Taken together, we conclude that the relevance scores can preserve the relative values of $\beta$ in this example.

We now generalize the intuition from the one-dimensional case, beginning with the first observation.
We show that the relevance scores preserve the distance gap of every pair of tasks from $\beta$.

\medskip
\begin{lemma}\label{lemma_gap}
    In the setting of Theorem \ref{thm_analysis}, with probability $1 - \delta$, for any $\delta > 0$, the following holds:
    \begin{align}
        \bigabs{ \frac 1 n \Big(\hat\theta_i - \hat\theta_j\Big) - \frac{k} {\alpha n}   \Big(v_i - v_j\Big) } \lesssim \frac{\log(\delta^{-1} k)} {\sqrt n}, \text{ for any } 1\le i< j\le k. \label{eq_lem_gap}
    \end{align}
\end{lemma}
The above result analyzes the covariance of $\cI_n$, which is proportional to identity plus a constant shift. %
Let $\id_{k\times k}$ be a $k$ by $k$ identity matrix and $e\in\real^k$ be a vector whose entries are all equal to one.
By the definition of $\cI_n$ and Woodbury matrix identity, we have
\begin{align}
    \ex{\frac{\cI_n^{\top} \cI_n }{n}} = \frac {\alpha } {k} \id_{k\times k} + \frac {\alpha (\alpha - 1)} {k(k-1)} e e^{\top} 
    ~~\Rightarrow~~ \ex{\frac{\cI_n^{\top}\cI_n}{n}}^{-1} = \frac {k} {\alpha}\left( \id_{k\times k} - \frac{\alpha - 1}{k\alpha - 1} ee^{\top} \right). \label{eq_cov}
\end{align}
Crucially, if we multiply $v$ on the right-hand side of equation \eqref{eq_cov}, then we will get $\frac{k}{\alpha}(v - \frac{\alpha-1}{k\alpha-1}{ (e^{\top} v)} e)$.
Recall that $e$ is all one's vector, which has the same entry in every coordinate after rescaling.
Then, recall $\hat\theta$ from equation \eqref{eq_fit_task_model}. By matrix concentration inequalities, the spectral norm (denoted as $\norms{\cdot}$ for a matrix) of the deviation from $\frac{\cI_n^{\top}\cI_n}{n}$ to its expectation satisfies
\begin{align}
    \bignorms{\frac{\cI_n^{\top}\cI_n}{n} - \ex{\frac{\cI_n^{\top}\cI_n}{n}}} \lesssim \frac{\alpha \log(k\delta^{-1})}{\sqrt n}. \label{eq_dev}
\end{align}
See equation \eqref{eq_err_E}, Appendix \ref{proof_lemma_1st} for the proof.
Thus, combining equations \eqref{eq_cov} and \eqref{eq_dev}, we claim that $\hat\theta$ is equal to $\alpha^{-1} k v$ minus a shared term for every task, modulo the deviation error of order $\textup{O}(n^{-1/2})$.
By subtracting $\hat\theta_i - \frac k {\alpha} v_i$ and $\hat\theta_j - \frac k {\alpha} v_j$, we can cancel out the shared term, leading to equation \eqref{eq_lem_gap}.

Next, we formalize the second observation from the one-dimensional case. %
Based on equation \eqref{eq_theta_j}, $v_i$ is a sum of $f(S)$ for all subsets $S$ such that $i \in S$.
We then show that $f(S)$ is the sum of $\beta^{(j)}$ for all $j \in S$, based on the pooling structure of our MTL model. %
Thus, the Euclidean distance between $\beta^{(i)}$ and $\beta^{(t)}$ will also reflect in $v_i$.
For complete proof of Theorem \ref{thm_analysis} (and Lemma \ref{lemma_gap}), see Appendix \ref{sec_proof_select}.
This result substantiates our intuition that $\hat\theta_i$ provides the relevance score of each source task $i$ to the target task while accounting for the presence of other source tasks.

\smallskip
\begin{remark}
    \normalfont
    \revision{After identifying the related tasks from all the source tasks, we can then combine them together with the target task for multi-task learning.
    We can show that provided the distance between their $\beta$-coefficients is small enough (i.e., $a$ is small enough), then multi-task learning will be better than single-task learning. The details are omitted.}
\end{remark}