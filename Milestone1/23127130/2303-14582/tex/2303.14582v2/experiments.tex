\section{Experiments}\label{sec:exp}

We apply our approach to three settings.
The first setting is about applying weak supervision to unlabeled data, and we apply our algorithm to select labeling functions for combining the weak labels of the labeling functions.
The second setting involves language prediction tasks from NLP benchmarks.
Again, we use our algorithm to select source tasks to improve the performance of target tasks.
The third setting involves learning from multiple groups of heterogeneous subpopulations, where the goal is to train a model with robust performance across all groups.
We cast this multi-group learning problem into an MTL framework and apply our algorithm to select a subset of groups to improve the robustness of target tasks.
For all these settings, we show that surrogate models can predict negative transfers accurately and fit MTL performances well;
Moreover, our approach provides consistent benefits over various optimization methods for multi-task learning. {The code repository for reproducing our experiments can be found at \url{https://github.com/VirtuosoResearch/Task-Modeling}.}

\subsection{Experimental Setup}\label{sec:exp_setup}

\textbf{Datasets.}
%We consider three types of MTL datasets.
First, we apply our approach to several text classification tasks from a weak supervision dataset \cite{zhang2021wrench}. Each dataset uses several labeling functions to create labels for every unlabeled example. The labels generated by different labeling functions may conflict with each other. 
We view each labeling function as a source task. The goal is to predict an unlabeled set of examples which is viewed as the target task.
A validation dataset that includes the correct labels is available for cross-validation.
We include the dataset statistics in Table \ref{tab_acc_res_weakspervision}.

Second, we consider MTL with natural language processing tasks.
We collect twenty-five datasets across a broad range of tasks, spanning sentiment classification, natural language inference, question answering, etc., from GLUE, SuperGLUE, TweetEval, and ANLI.
We view one task as the target and the rest as source tasks.
The goal is to select a subset of source tasks for the best MTL performance. 
We provide the statistics of the twenty-five tasks in Table \ref{tab_text_statistics}, Appendix \ref{add_exp_setup}.

Third, we consider multi-group learning settings where a dataset involves multiple subpopulation groups. We consider income prediction tasks based on US census data \cite{ding2021retiring}. 
The goal is to predict whether an individual's income is above \$50,000 using ten features, including the individual's education level, age, sex, etc.
There are 51 states in this dataset; we view each state as one task.
For prediction, we use one state as the target task and the remaining fifty as source tasks.
We use the racial group of each individual to split a state population into nine subpopulation groups. 
We evaluate the robustness of a model by the worst-group accuracy. This metric measures the accuracy of the worst-performing group among all groups.
%Additionally, we consider two fairness-related measures, including demographic parity and equality of opportunity (cf. Appendix \ref{sec_omitted_results}).
We use six states as the target task. See Table \ref{tab:acc_res_worst_acc} for dataset statistics.

\noindent\textbf{Implementation.}
We use a standard approach for conducting MTL, i.e., hard parameter sharing.
For text classification, we use BERT-Base as the encoder.
For tabular features, we use a fully-connected layer with a hidden size of $32$.
The surrogate modeling procedure requires three parameters: the size of a subset, the number of samples, and the loss function.
We select the size between $3, 5, 10$, and $15$.
We select the number of samples from a range between $50, 200, 400$, and $800$, depending on $k$.
%Then, we describe the number of subsets $n$ for each dataset. 
%For multitask weak supervision datasets, since the number of source tasks (c.f. Table \ref{tab_acc_res_weaalphaupervision}) varies among datasets, we obtain $n = \{50, 200, 200, 400, 800\}$ results with $|S| = \{3, 5, 5, 10, 15\}$ from Youtube, Chemprot, CDR, TREC, Semeval datasets, respectively.
%For NLP tasks, we obtain $n=200$ results on $|S| = 5$ source tasks.
%For the income prediction tasks, we obtain $n=400$ results on $|S| = 5$ source tasks.
We also collect a holdout set of size $100$ for constructing the surrogate model.
For classification tasks, we set the loss function as the negative classification margin, i.e., the difference between the correct-class probability and the highest incorrect-class probability. 
%For regression tasks, we set the loss function as MSE (on the holdout set). 
%First, we consider datasets from a multitask weak supervision benchmark \cite{zhang2021wrench}, which include labeling functions that generate weak labels for an unlabeled corpus of data.
%Each labeling function works as an extra source task for the main task. 
%The number of labeling functions ranges between 10 and 164.
%Second, we consider NLP datasets from several existing benchmarks including GLUE. Each dataset has 24 source tasks.
%Third, we consider multi-group learning settings. Each tabular dataset has 50 source tasks. 
%For weak supervision and NLP datasets, We use BERT-Base as the encoder $\phi$. For tabular datasets, we use a fully-connected layer as the encoder. 
%More details of our implementation can be found in Section \ref{sec:exp_setup}. 
%We choose the sizes of the subsets in $\cS$ between 1 and 20 via cross-validation.
After estimating the surrogate model $g$ from equation \eqref{eq_fit_task_model}, we use $g(S)$ as the predicted multitask loss for an unseen subset $S$.
We compare $g(S)$ with the STL performance of task $t$ to determine whether the transfer from $S$ to $t$ is positive or negative.
We measure the $F_1$-score for the minority class (between the positive and negative classes) on the holdout set.  %between positive and negative transfers, since we observe that the two types of transfers are imbalanced.
% We use BERT-Base as the encoder $\phi$ for the text classification tasks.
% It is clear that once we identify the minority class, we can then identify the other class.
%Besides tasks with noisy supervision sources, we also consider tabular and text datasets from different input distributions and observe similar results.




%\vspace{-0.1in}
\subsection{Results for Predicting Negative Transfers in Multitask Learning}\label{sec_identify}

We validate that our fitted models can accurately identify positive vs. negative transfers from source tasks.
Then, we show that these models can be constructed efficiently by reporting the runtime.

\textbf{Results.} We test the accuracy of using surrogate models to predict positive vs. negative transfers. We first evaluate the four examples shown in Figure \ref{fig_source_tasks}.
We set the size of $\alpha$ as $5$ and $n$ as $400$.
Using the model to compare the MTL performances with STL performances, we can correctly predict the transfers with an $F_1$-score of \textbf{0.82}, averaged over the four target tasks.
Second, we conduct the same tests for weak supervision and NLP tasks. 
Similarly, task models can predict positive vs. negative transfers with an average $F_1$-score of \textbf{0.8} for ten different target tasks.

Furthermore, we compare these results with two baselines that compute first-order task affinity scores or higher-order approximations by averaging first-order affinity scores.
Our approach yields much more accurate predictions across different subset sizes of $\alpha$, ranging from $5$ up to $20$.
Figure \ref{transfer_prediction_res} provides the illustration for one target task, which is conducted on the US Census dataset, along with fifty source tasks.
%\footnote{The F1 score for each dataset is listed as: 
%Youtube (0.73), TREC (0.83), CDR (0.72), Chemprot (0.83), Semeval (0.78),  
%CB (0.82), CoLA (0.78), COPA (0.74), RTE (0.84), and WSC (0.74).}
%HI (0.83), KS (0.84), LA (0.84), NJ (0.87), NV (0.77), SC (0.88).

Lastly, we measure Spearman's correlation between predicted and true performances. We observe an average coefficient of \textbf{0.8} across 16 target tasks. See Appendix \ref{sec_exp_detail} for the details.

\begin{figure}[h!]
    \centering
  \begin{subfigure}[b]{0.98\textwidth}
    \includegraphics[width=0.99\textwidth]{./figures/prediction_corr_n_acc.pdf}
  \end{subfigure}
  \caption{
  \textbf{Left}: Our approach can consistently predict positive/negative transfers from up to $20$ source tasks to the target task.
  \textbf{Right}: Convergence of surrogate models as $n$ increases up to $400$, leading to an $F_1$-score of $0.8$ for predicting positive/negative transfers from up to $20$ source tasks to one target task.}\label{transfer_prediction_res}
\end{figure}



\begin{wrapfigure}[12]{r}{0.25\textwidth}
    \centering
    \vspace{-0.0in}
    \includegraphics[width=0.23\textwidth]{figures/plot_running_time.pdf}
    \vspace{-0.2in}
    \caption{\revision{We show that the runtime of our approaches scales linearly with $k$, the number of source tasks.}}\label{fig_runtime}
\end{wrapfigure}
\textbf{Computational cost.}
Next, we report the runtime cost collected on an NVIDIA Titan RTX card.
First, we show that the running time of our procedure scales linearly with $k$, the number of source tasks.
Recall that our approach requires training $n$ models, one for each random subset.
Section \ref{sec_convergence_guarantees} shows that the sample complexity for learning task models is linear in the number of source tasks. 
In practice, we find that collecting $n \le 8k$ samples suffice for fitting the model.
We provide empirical evidence to support this result.
We plot the convergence of task modeling on sixteen target tasks from three datasets described in Section \ref{sec:exp_setup}.
We measure the MSE between task model predictions and empirical training results on the holdout set of size 100, following the experimental setup described in Section \ref{sec_identify}.
Figure \ref{fig_convergence}, which can be found in Appendix \ref{sec_exp_detail}, shows the results.
Moreover, the results hold for 16 target tasks.

Thus, we conclude that linear surrogate models can be accurately fitted with less than $8k$ samples, and the fitted model can accurately predict the performances of unsampled subsets.
In Figure \ref{fig_runtime}, we plot the number of GPU training hours as a function of $k$. 
The results confirm the linear scaling behavior of our approach.

Our approach is also comparable with the baseline approaches.
Among them, the most related ones compute first-order affinity scores and conduct a branch-and-bound search algorithm over the task space, which has exponential complexity in $k$ \cite{standley2020tasks,fifty2021efficiently}.
In our experience, with more than 20 tasks, these methods take more than 200 hours.
Our approach requires, at most, 145 hours.
This is consistent with our theoretical predictions in Section \ref{sec_approach}. 
Later in Section \ref{sec_speedup}, we elaborate on two simple techniques to accelerate surrogate model training in practice.



% Then, we compare the runtime of our approach against the baselines. 
% Compared with computing first-order affinity scores, our approach has the same complexity in $k$.
% For example, ours takes 1.24 GPU hours in one target task, while computing first-order affinity scores by TAG takes 0.87 GPU hours.

% On the other hand, naively computing $f$ for all possible subsets requires training $\binom{k}{\alpha}$ models.
% We manage to reduce this down to $O((k\log^2(k))\alpha^4)$ by sampling.
% Taken together, we conclude that our approach scales efficiently to many tasks.

%We use a fully-connected layer as the encoder and collect results on 400 task subsets of size 5. Constructing task models on tabular datasets takes 46.3 hours. 
%For the text datasets, there are 24 source tasks where the average training size is 11740. 
%We use BERT-Base as the encoder and collect results on 200 task subsets of size 5. Constructing task models on text datasets takes 121.4 hours. 
%We report the GPU hours of collecting training results for each target task in Appendix \ref{sec_omitted_results}.
%We report the GPU hours of constructing task models for each of the eleven target tasks in Table \ref{tab_acc_res_weaalphaupervision} and \ref{tab:acc_res_worst_acc}. Datasets (GPU hours) are listed in the following: 
%Youtube (4.0), TREC (37.0), CDR (55.4), Chemprot (68.2), Semeval (85.9),  
%CB (121.4), CoLA (145.5), COPA (138.8), RTE (123.6), WSC (133.3),
%HI (42.4), KS (44.0), LA (49.9), NJ (47.6), NV (43.7), SC (50.2). 
% Across all eleven cases, constructing task models until convergence takes at most 85.9 GPU hours, evaluated on an Nvidia Titan RTX instance.

%We also notice that the required $n$ decreases as $|S|$ increases, as shown in Figure \ref{fig_varying_alpha} of Section \ref{sec_ablate}.
%As a result, our approach takes the same amount of time for different sizes of $S$ up to $20$, which is less than 52 GPU hours.
%By contrast, the runtime of the exhaustive search increases exponentially as the size of $S$ grows.

% Recall that our approach requires sampling $n = O(k\alpha^4\log^2 k)$ in theory.


\subsection{Results for Improving Multitask Learning Performance}\label{sec_exp_res}

Next, we apply our approach to MTL on weak supervision and NLP tasks.
We compare our approach with the following baselines. 
First, we consider training by naively combining all source and target tasks. 
Second, we consider bilevel optimization methods, including TAWT \cite{chen2021weighted} and Auto-$\lambda$ \cite{liu2022auto}, and MTL optimization methods, including HOA \cite{standley2020tasks}, TAG \cite{fifty2021efficiently}.  
The latter two methods use a branch-and-bound algorithm that does not scale to over 20 tasks in one dataset. To allow for a comparison with them, we apply the thresholding procedure to their first-order task affinity scores to select source tasks. 
\revision{To set the threshold $\gamma$ in our algorithm, we use grid search from $-0.5$ to $0.5$ at an interval of $0.1$.
We choose this range because it covers the values of most coefficients in our experiments.}

\smallskip
\textbf{Multitask weak supervision.} First, we apply our algorithm to five weak supervision datasets, which involve text classification from multiple weak labels.
We select a subset of labeling functions so that using their weak labels to train an end model best improves performance on the target task.
We also compare against methods that use a label model to aggregate the weak labels and then train an end model on the aggregated label.
These include taking a majority vote on the weak labels, applying probabilistic modeling to combine the noisy labels \cite{ratner2016data}, and MeTaL \cite{ratner2019training}.

Next, we compare the experimental results in Table \ref{tab_acc_res_weakspervision}.
Compared with naively MTL, which trains all tasks together, our algorithm improves the test performance by \textbf{6.4\%} on average.
Compared with MTL optimization and weak supervision methods, our algorithm outperforms their results by up to \textbf{3.6\%} absolute and \textbf{2.3\%} on average. %This result demonstrates the benefit of modeling higher-order task transfers.

\smallskip
\textbf{Illustrating the separation between selected and not selected source tasks.} Lastly, we examine the labeling functions selected by our approach.
Recall that our procedure places a threshold over the learned coefficients to separate related and unrelated source tasks.
Here, we use the number of correct and incorrect labels as a proxy of relatedness between a labeling function and the target task.
\revision{Figure \ref{fig_correct_labels} shows the results, measured on two datasets, namely Chemprot and TREC.
Each dot represents one source task.
We observe a clear separation between selected and excluded source tasks when we compare the correct/incorrect labels in each task.}
This shows that our algorithm selects more accurate labeling functions.

\begin{table*}[t!]
\centering
%\vspace{-0.1in}
\caption{Accuracy/F1-score from surrogate modeling followed by task selection (ours), as compared with MTL methods and weak supervision methods that use a label model to aggregate the weak labels.}\label{tab_acc_res_weakspervision}
%\vspace{-0.1in}
\begin{footnotesize}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Dataset (Metrics)    & Youtube (Acc.) & TREC (Acc.) & CDR (F1) & Chemprot (Acc.) & Semeval (Acc.) \\ \midrule
Training & 1,586 & 4,965 & 8,430 & 12,861 & 1,749  \\
Validation & 120 & 500 & 920 & 1,607 & 178 \\
Test & 250 & 500  & 4,673 & 1,607 & 600 \\
\# source tasks & 10 & 68 & 33 & 26 & 164  \\
\midrule % & Avg. Rank  
Naive MTL & 94.72$\pm$0.85 & 64.10$\pm$0.50	& 58.20$\pm$0.55	& 53.43$\pm$0.53	& 89.00$\pm$1.06 \\ % & 7.8 
HOA     & 94.93$\pm$1.80 & 74.67$\pm$4.66	& 59.76$\pm$0.97	& 45.57$\pm$0.41	& 89.94$\pm$4.42 \\ % & 6.2 
%Gradient similarity    & 95.33$\pm$0.68 & 78.25$\pm$3.71	& 59.21$\pm$0.80	& 53.67$\pm$1.89 & 89.89$\pm$2.17 \\ % & 4.0 
TAG    & 95.20$\pm$0.65 & 77.50$\pm$3.62	& 59.31$\pm$0.15	& 53.67$\pm$2.74	& 89.06$\pm$1.47 \\ % & 4.2 
TAWT       & 94.53$\pm$1.05 & 72.40$\pm$2.36	& 59.85$\pm$0.30	& 53.76$\pm$2.96 & 86.83$\pm$1.78 \\ % & 5.0 
\revision{Auto-$\lambda$} & \revision{95.80$\pm$0.85} & \revision{73.70$\pm$0.67} & \revision{59.07$\pm$0.05} & \revision{52.50$\pm$1.28} & \revision{87.91$\pm$0.66} \\
Majority voting                          & 95.36$\pm$1.71 & 66.56$\pm$2.31 & 58.89$\pm$0.50 & 57.32$\pm$0.98 & 85.03$\pm$0.83 \\ % & 4.6 
Probabilistic modeling  & 93.84$\pm$1.61 & 68.64$\pm$3.57 & 58.48$\pm$0.73 & 57.00$\pm$1.20 & 83.93$\pm$0.83 \\ % & 6.6 
MeTaL  & 92.32$\pm$1.44 & 58.28$\pm$1.95 & 58.48$\pm$0.90 & 56.17$\pm$0.66 & 71.74$\pm$0.57 \\ % & 8.4 
%Gradient decomposition  & 95.28$\pm$0.16 & 65.80$\pm$1.81	& 58.81$\pm$0.36 & 54.76$\pm$0.67 & 78.57$\pm$0.13 \\ 
\midrule % & 7.0 
\textbf{Alg. \ref{alg:task_modeling} (Ours)}                     & \textbf{97.47$\pm$0.82} & \textbf{81.80$\pm$1.14}	& \textbf{61.22$\pm$0.39}	& \textbf{57.54$\pm$0.55}	& \textbf{93.50$\pm$0.24}  \\\bottomrule % & \textbf{1.0} 
\end{tabular}
\end{footnotesize}
%\vspace{-0.1in}
\end{table*}

\begin{figure}[!t]%[15]{r}{0.51\textwidth}
    %\vspace{-0.20in}
    \centering
  %    \begin{subfigure}[b]{0.24\textwidth}
  %   \centering
  %   \includegraphics[width=0.90\textwidth]{figures/corr_correct_labels_cdr.pdf}
  % \end{subfigure}\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/corr_correct_labels_chemprot.pdf}
    \caption{Illustration for LFs in the Chemprot dataset}
  \end{subfigure}\hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/corr_correct_labels_trec.pdf}
    \caption{Illustration for LFs in the TREC dataset}
  \end{subfigure}
      \caption{\revision{We find that the selected and not-selected source tasks are separated by the number of correct labels provided by each source task versus the number of incorrect labels of each source task. Each dot represents the number of correct/incorrect labels for one labeling function.}}\label{fig_correct_labels}  
      %\vspace{-0.1in}
  % \hfill
  % \begin{subfigure}[b]{0.24\textwidth}
  %   \centering
  %   \includegraphics[width=0.90\textwidth]{figures/corr_correct_labels_semeval.pdf}
  % \end{subfigure}
\end{figure}



\bigskip
\textbf{NLP tasks.}
Next, we test our approach for NLP tasks.
We collect 25 datasets from GLUE, SuperGLUE, TweetEval, and ANLI. See Table \ref{tab_text_statistics}, Appendix \ref{sec_omitted_results} for a complete list.
We evaluate our approach by first selecting source tasks and then applying MTL. We test on five  target tasks: CoLA, RTE, CB, COPA, and WSC. For each task, we use the rest 24 tasks as  source tasks.
%Due to space limits, we defer the results to Table \ref{tab_acc_res_text_tasks}, Appendix \ref{sec_omitted_results}.

We first compare our approach with STL and naive MTL.
We observe that naive MTL can perform worse than STL, e.g., on CoLA and WSC datasets. 
By contrast, our approach always outperforms STL (by \textbf{5.5\%}) and naive MTL (by \textbf{5.4\%}), on average.
We then compare our approach with TAG and HOA. Our approach shows an average improvement of \textbf{2.2\%} and is especially effective for tasks with a small training set.

\subsection{Results for Improving Robustness in Multi-group Learning}\label{sec_fair}

%Gradient decomposition  & 73.20$\pm$0.57 & 72.24$\pm$1.19 & 73.51$\pm$0.66	& 76.38$\pm$0.69 & 73.71$\pm$0.84 & 76.77$\pm$0.66 \\  % & 8.0 

We apply our approach to multi-group learning settings where the input distribution contains a heterogeneous mixture of subpopulations.
The objective of these problems is to learn a model that performs robustly for all groups.
In particular, we apply our approach to three performance metrics: worst-group accuracy, democratic disparity, and equality of opportunity.
%We evaluate these metrics on binary classification tasks with multiple subpopulations.
%Our algorithm selects source tasks to improve the target task's robustness performance.
We also compare against STL methods, including group distributional robust optimization (GroupDRO, \citet{sagawa2019distributionally}) and supervised contrastive learning (correct-n-contrast, \citet{zhang2022correct}).
Table \ref{tab:acc_res_worst_acc} presents the comparison.

Compared with STL, including GroupDRO and correct-n-contrast,  task modeling improves the worst-group accuracy by \textbf{1.17\%} on average.
Compared with existing MTL optimization methods, our approach shows a gain of up to \textbf{1.9\%} absolute accuracy.
Measured by two fairness metrics, namely democratic disparity and equality of opportunity, our algorithm also outperforms the baselines (see Appendix \ref{sec_omitted_results} for details).

\begin{table*}[t!]
\centering
\caption{Worst-group accuracies using MTL with source tasks selected by our algorithm, as compared to STL, MTL optimization methods, and exhaustive search over combinations of up to two source tasks.}\label{tab:acc_res_worst_acc}
%\vspace{-0.1in}
\begin{footnotesize}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
Dataset & HI & KS & LA & NJ & NV & SC  \\ \midrule
Training & 4,638 & 9,484 & 12,400 & 28,668 & 8,884 & 14,927 \\
Validation & 1,546 & 3,161 & 4,133 & 9,556 & 2,961 & 4,976 \\
Test & 1,547 & 3,162 & 4,134 & 9,557 & 2,962 & 4,976 \\
Smallest group size & 67 & 75 & 58 & 52 & 61 & 203 \\ 
\midrule % & Avg. Rank
GroupDRO  & 74.56$\pm$0.58 & 75.50$\pm$0.59 & 74.90$\pm$0.38 & 76.95$\pm$0.20 & 73.06$\pm$0.66 & 75.56$\pm$1.36 \\ % & 5.5
Correct-n-contrast & 74.37$\pm$0.27 & 75.52$\pm$1.19 & 74.25$\pm$0.15 & 77.60$\pm$0.10 & 73.22$\pm$0.40 & 76.23$\pm$0.98 \\ % & 5.0 
%Empirical risk minimization & 74.46$\pm$0.48 & 73.73$\pm$1.19 & 72.39$\pm$1.96 & 76.34$\pm$0.64 & 72.89$\pm$1.42 & 75.20$\pm$1.07  \\ % & 9.7
%Importance weighting \cite{byrd2019effect} & 74.53$\pm$0.81 & 72.84$\pm$1.74 & 74.82$\pm$0.94 & 76.43$\pm$0.50 & 71.25$\pm$1.73 & 75.30$\pm$0.05 \\ % & 7.8 
Naive MTL & 73.63$\pm$0.46 & 75.22$\pm$0.73 & 73.24$\pm$1.01 & 77.28$\pm$0.25 & 73.22$\pm$1.12 & 76.23$\pm$0.49  \\ % & 8.0
HOA  & 74.67$\pm$0.32 & 75.22$\pm$1.48 & 73.69$\pm$0.86 & 77.49$\pm$0.25 & 73.88$\pm$0.66 & 76.80$\pm$0.65 \\ % & 3.6 
%Gradient similarity     & 74.53$\pm$0.52 & 75.22$\pm$2.02 & 73.66$\pm$1.22 & 77.44$\pm$0.38 & 74.38$\pm$0.91 & 77.03$\pm$0.54 \\ % & 3.8 
TAG    & 74.48$\pm$0.41 & 75.97$\pm$1.18 & 73.24$\pm$1.01 & 77.41$\pm$0.48 & 74.05$\pm$0.84 & 76.41$\pm$0.50 \\ % & 5.1
TAWT       & 73.53$\pm$0.44 & 75.14$\pm$1.39 & 73.51$\pm$1.38	& 76.47$\pm$1.31 & 72.89$\pm$0.81 & 76.59$\pm$0.97 \\ % & 7.8
Exhaustive search ($\alpha \le 2$)  & 75.10$\pm$0.37 & \textbf{77.03$\pm$0.76} & 73.60$\pm$1.02 & 77.40$\pm$0.24 & 73.21$\pm$1.10 & 77.16$\pm$0.21 \\
\midrule
\textbf{Alg. \ref{alg:task_modeling} (Ours)} & \textbf{75.47$\pm$0.73} & {76.96$\pm$0.69} & \textbf{75.62$\pm$0.11} 
 & \textbf{78.17$\pm$0.36}  & \textbf{75.21$\pm$0.52} & \textbf{77.62$\pm$0.34} \\\bottomrule % & \textbf{1.0}
\end{tabular}
\end{footnotesize}
%\vspace{-0.1in}
\end{table*}

\subsection{Techniques to Accelerate Surrogate Model Training}\label{sec_speedup}

\revision{
Lastly, we show that we can further reduce the computational cost of our approach by applying two techniques. 
We aim to achieve comparable results to the ones shown in Table \ref{tab_acc_res_weakspervision}, but we will speed up the computation of $f(S_1), f(S_2), \dots, f(S_n)$ using the following two simple techniques: 
\begin{itemize}
\item First, we can reduce the size of the training set for computing $f$ by downsampling the training data from each task by a fixed proportion. 
\item Second, we can reduce the number of iterations for training each MTL model by early stopping the training procedure.
% This is a common technique in hyperparameter optimization. 
\end{itemize}
}

To illustrate the benefit of these two techniques, we apply them to two weak supervision datasets. The results are shown in Table \ref{table_speed_up}. 
We find that by downsampling \textbf{40\%} of the training data and early stopping at \textbf{20\%} of the training epochs, we can achieve comparable performance to fully training MTL models.
In particular, the accuracy difference is within 0.5\% for both datasets. 
However, we manage to reduce the training time for computing $f(S_1), f(S_2), \dots, f(S_n)$ by \textbf{12$\times$} times.

We also report the running time for all the baselines on these two datasets. 
We notice that the running time of our approach is comparable to MTL optimization methods after adding early stopping and downsampling to reduce the training time.
Our approach is slightly slower than weak supervision methods that directly aggregate the weak labels while achieving 5\% better performance on average.
Overall, our approach is comparable to the baseline optimization methods regarding efficiency.


\subsection{Ablation Studies}\label{sec_validate}

\textbf{Benefit of modeling higher-order transfers.} 
We validate the benefit of modeling higher-order task transfers over approaches that only precompute first-order or second-order task affinities.
First, compared with approaches that compute first-order task affinities, our approach improves the accuracy by \textbf{3.0\%}, as is clear from Tables \ref{tab_acc_res_weakspervision} and \ref{tab:acc_res_worst_acc}.
Second, we precompute the MTL performance for every combination of two source tasks.
We run an exhaustive search over $k(k-1)/2$ combinations to find the best combination for MTL.
We test on six target tasks with $k = 50$, which requires training $1,225$ MTL models with two source tasks and one target task each time.
Our selection procedure consistently outperforms the best two-task subsets by \textbf{1.21\%} absolute accuracy.
This is shown in the last two lines in Table \ref{tab:acc_res_worst_acc}.

\smallskip
\noindent\textbf{Sensitivity of model parameters.}
We highlight three parameters that require careful tuning: the subset size $\alpha$, the number of samples $n$, and the loss function $\ell$.
{We vary $\alpha$ for each dataset between $\{3, 5, 10, 15\}$ via cross-validation, on a holdout set of $100$ subsets. We pick $n$ in $\{50, 200, 400, 800\}$ according to the number of tasks $k$.
Besides, we find that choosing $\ell$ as the classification margin function performs the best in practice.}

\revision{The threshold $\gamma$ is usually set as $0.3$ or $0.4$ for weak supervision datasets, which selects most of the source tasks on average except the highly noisy labels. For instance, on the Semeval dataset with 164 source tasks, our approach selected 160, while $\alpha$ is 15.
For the NLP and multi-group learning tasks, $\gamma$ is usually set as $-0.5$. This usually selects 3 or 4 source tasks, while $\alpha$ is 5. Thus, there are only a few helpful source tasks for a particular target task.}
%We report the task selection threshold for all target tasks in Appendix \ref{sec_omitted_results}.

Lastly, the selected tasks remain the same when using multiple random seeds to train the surrogate model.
For details, see Appendix \ref{sec_ablate}.

\begin{table}[t!]
\centering
\caption{\revision{Speeding up our approach by training models on sampled subsets of tasks with 20\% training epochs (early stopping) and 40\% training data (downsampling). With these two speed-up techniques, we can speed up the computation of $f(S_1), f(S_2), \dots, f(S_n)$, while achieving comparable performance compared to fully computing these scores.}}
\label{table_speed_up}
\begin{small}
\begin{tabular}{@{}lcc@{}}
\toprule
Dataset (Metrics)  & CDR (Hours / F1) & Chemprot (Hours / Acc.) \\
\midrule % & Avg. Rank  
Naive MTL & 1.99 / 58.20$\pm$0.55	& 1.89 / 53.43$\pm$0.53 \\
Majority voting & 2.00 / 58.89$\pm$0.50 & 1.91 / 57.32$\pm$0.98 \\
Probabilistic modeling   & 2.00 / 58.48$\pm$0.73 & 1.91 / 57.00$\pm$1.20  \\ 
MetaL  & 2.00 / 58.48$\pm$0.90 & 1.91 / 56.17$\pm$0.66 \\
TAWT & 2.30 / 59.85$\pm$0.30 & 2.02 / 53.76$\pm$2.96 \\
Auto-$\lambda$ & 3.46 / {59.07$\pm$0.05} & 3.31 / {52.50$\pm$1.28}\\
% HOA & $>$ 200 / 59.76$\pm$0.97	& $>$ 200 / 45.57$\pm$0.41 \\
% TAG & $>$ 200 / 59.31$\pm$0.15	& $>$ 200 /  53.67$\pm$2.74  \\
\midrule
Alg. \ref{alg:task_modeling} w/o early stopping and downsampling &              {38.34 / 61.22$\pm$0.39}	& {31.14 / 57.54$\pm$0.55}	  \\
Alg. \ref{alg:task_modeling} w/ early stopping and downsampling &    
2.89 / 60.77$\pm$0.05 & 3.76 / 57.06$\pm$0.84 \\
\bottomrule
\end{tabular}
\end{small}
\end{table}