\section{Related Work}\label{sec_related}

There is a vast body of work on multi-task learning from various fields.
A recurring theme for multitask learning research is inspired by a desire to imitate human intelligence as we continue to learn new information and extrapolate the learned information to new tasks and domains \cite{thrun1998learning}.
In the early literature, many studies focus on MTL with linear and kernel-based models.
A common approach is to set up separate parameters for each task while adding explicit regularization to the combined parameters \cite{evgeniou2004regularized,argyriou2007spectral,argyriou2008convex}.
For linear models, this approach can be related to low-rank matrix approximation \cite{AZ05}.
Inspired by the development of deep learning, recent works focus on MTL with deep neural networks \cite{yang2016deep}.
More broadly, see several recent surveys \cite{zhang2021survey,jiang2022transferability} for more comprehensive references.
Within this vast literature, the contribution of our work is in the identification of negative transfers and the design of subset selection methods.
Below, we discuss several relevant topics in detail.

\smallskip
\noindent\textbf{Understanding Black-box Predictions.}
\revision{
Surrogate modeling is a classic
technique for studying black-box functions \cite{sacks1989design,ong2003evolutionary}, which we use as a proxy to study task relatedness.
Our approach builds on the recent work of datamodels \cite{ilyas2022datamodels}.
However, there are two major differences between our work and their work.
First, we apply the idea of surrogate models to multitask learning, whereas their work focuses on the single-task supervised learning setting.
Second, besides empirical demonstrations, we have also conducted a theoretical analysis of our approach to multi-task learning. %
Our findings reinforce the result of \citet{ilyas2022datamodels} that the performances of deep neural networks can be extrapolated efficiently and accurately.
Recent work has sought to explain why datamodels can perform well using harmonic analysis \cite{saunshi2022understanding}.
It would be interesting to see if their techniques can be used to explain the empirical findings of our work in the context of MTL.
More broadly, there is a line of work on developing techniques to understand the influence of data in black-box models through influence functions. See  \citet{koh2017understanding,yeh2018representer} for further references.
}

\smallskip
\noindent\textbf{Formal Notions of Task-relatedness.} There is a rich discussion about formulating notions of task-relatedness in the literature \cite{ben2003exploiting}.
\citet{ben2010theory} introduces a discrepancy notion called $\cH$-divergence, which leads to a generalization bound for minimizing the empirical risk of naive MTL.
Transfer exponents are another measure of discrepancy between two distributions \cite{hanneke2019value}.
Geometric distance measures for linear data models have also been considered in few-shot learning \cite{du2020few} and meta-learning \cite{kong2020meta,saunshi2021representation}.

Note that none of these task-relatedness measures can be measured on deep neural networks due to the complexity of these models. One heuristic solution is to measure the cosine similarity between the gradients of each task's loss functions during training \cite{yu2020gradient,dery2021auxiliary,chen2021weighted}.
Another solution is to measure the similarity of the predicted probabilities between tasks \cite{nguyen2020leep}.
This leads to a noisy estimate of task-relatedness, which is best for capturing first-order transfers.
\citet{standley2020tasks} combines domain knowledge from visual intelligence to build a task relation taxonomy for 26 tasks.
Compared with their approach, our approach is more generic, applies to MTL settings with little to no domain knowledge, and efficiently captures higher-order transfer in a principled framework.
Rather than defining an explicit relatedness measure, our work uses surrogate models to measure task-relatedness.
This perspective circumvents the design of explicit task-relatedness measures for deep neural networks but is still useful for predicting transfers and for optimizing the performance of MTL.




\smallskip
\textbf{Optimization Methods for Multi-Task Learning.} An empirical motivation for this paper stems from recent work using weak supervision for training deep models \cite{ratner2016data}.
We build on a multi-task weak supervision approach \cite{ratner2019training} while adding new capability to deal with conflicts between labeling functions in the end model. 
\revision{This problem has also been studied in the rich literature about learning from noisy labels \cite{liu2015classification}.
For example, \citet{xia2019anchor} and \citet{xia2020part} propose to estimate transition matrices for multi-class prediction and use statistically-consistent weighting to integrate multiple noisy labels.
Complementary to these works, we fit a surrogate model to approximate multitask learning performances and use the surrogate model to predict the performance of unseen task combinations.}


\revision{
Our approach selects source tasks for learning a target task, which has been studied in several recent works using optimization methods \cite{autosem,chen2021weighted}. 
Recent work \cite{liu2022auto} optimizes a weighted combination of per-task loss functions and jointly updates task-specific weights by the gradients of per-task losses during training. 
By contrast, our approach focuses on subset selection. Besides, our approach can separate tasks with more noisy labels when source tasks have disparate labeling precision.
Our setting is also related to the task grouping problem \cite{kumar2012learning}, which aims to assign tasks into several groups, with each group of tasks learned in a separate MTL model. Unlike this problem, we select a subset of source tasks for a particular target task. 
}

\revision{There are also works that apply low-rank tensor factorization to the parameters of multiple linear regression tasks \cite{wimalawarne2014multitask}. Along this line of research, several recent works apply low-rank regularization methods with a block-diagonal structure on the model parameters \cite{nie2018calibrated,yang2020task}.
\citet{yang2016deep} revisit the idea of tensor factorization in the context of deep neural networks. 
\citet{liu2016algorithm} provide generalization bounds for multi-task learning under a low-rank structural condition on all the tasks.
Their results shed light on when MTL would be better than STL.
}

Lastly, we note that task relations are characteristically different between different benchmarks due to the nature of the data.
This paper focuses on developing a methodology for predicting MTL performances using rigorous theoretical and empirical arguments. 
Our extensive experiments demonstrate the usefulness of the methodology.
It would be interesting to apply our methodology to large-scale benchmarks beyond what we have studied \cite{zamir2018taskonomy,aribandi2021ext5}.
Besides, it would be interesting to see if our approach can be applied to other related settings such as federated learning \cite{wang2020federated} and multitask reinforcement learning \cite{wang2022thompson}, where the problem of identifying negative transfers also arises.
\revision{Lastly, although our work focuses on subset selection for multitask learning at the task level, it would be interesting to see if similar approaches could be applied at the feature level.}














\section{Conclusion}\label{sec_conclude}

This paper studied how to efficiently predict negative transfers from multiple source tasks to one target task. 
The main contribution is the design and analysis of surrogate models for predicting multi-task learning performances.
Both theoretical and empirical results show that our approach is efficient, accurate, and advances over prior optimization methods for multi-task learning.

Our work opens up many interesting questions for future work.
Although we demonstrated the empirical strength of linear models for MTL, a rigorous explanation is lacking; Can recent analytic tools for understanding datamodels \cite{saunshi2022understanding} be used to gain further insight?
Can more advanced sampling techniques, such as adaptive sampling, help speed up the training of surrogate models, which might enable the training of more powerful models?
\revision{Lastly, our experiments show that the validation set size of the target task does not need to be very large for the approach to perform well. This is currently not explained by our Rademacher complexity-based bound. It is possible that with a tighter generalization analysis via data-dependent bounds, one might get a result that captures few-shot learning scenarios.}
This would be an interesting question for future work.
In a follow-up paper \cite{li2023boosting}, we apply ideas from this paper to multitask learning on graph-structured data.
More broadly, understanding task relationships in multitask learning is a complex and challenging research question. We hope our work inspires more principled studies in this direction.


\section*{Acknowledgment}

Thanks to Andrew Ilyas, Simon Du, Shuxiao Chen, Nikunj Saunshi, Chicheng Zhang, and David Bau for helpful discussions at various stages of this work.
Thanks to the anonymous referees and the action editor for providing constructive feedback on our work.
D. L. acknowledges financial support from a seed grant and the startup fund from the Khoury College of Computer Sciences, Northeastern University.
