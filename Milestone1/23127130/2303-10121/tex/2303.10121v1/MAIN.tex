%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}




\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[WWW '23 Companion]{Companion Proceedings of the ACM Web Conference 2023}{April 30-May 4, 2023}{Austin, TX, USA}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion), April 30-May 4, 2023, Austin, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3543873.3587571}
\acmISBN{978-1-4503-9419-2/23/04}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

% CUSTOM PACKAGES
\usepackage{subfig}
\usepackage{multirow}
\usepackage{booktabs}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Retrieving false claims on Twitter during the Russia-Ukraine conflict}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Valerio La Gatta}
\authornote{V. La Gatta and C. Wei contributed equally to this work.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Information Sciences Institute, University of Southern California}
  \country{Los Angeles, California, USA}
}
\affiliation{%
  \institution{University of Naples Federico II}
  \city{Naples}
  \country{Italy}
}

\email{valerio.lagatta@unina.it}

\author{Chiyu Wei}
\authornotemark[1]
\affiliation{%
  \institution{Information Sciences Institute \\ University of Southern California}
  \city{Los Angeles}
  \state{California}
  \country{USA}}
\email{chiyuwei@isi.edu}
\author{Luca Luceri}
\affiliation{%
  \institution{Information Sciences Institute \\ University of Southern California}
  \city{Los Angeles}
  \state{California}
  \country{USA}}
\email{lluceri@isi.edu}

\author{Francesco Pierri}
\affiliation{%
  \institution{Polititecnico di Milano}
  \country{Milan, Italy}
  }
\affiliation{%
  \institution{Information Sciences Institute, University of Southern California}
  \country{Los Angeles, California, USA}
  }
\email{francesco.pierri@polimi.it}

\author{Emilio Ferrara}
\affiliation{%
  \institution{Information Sciences Institute \\ University of Southern California}
  \city{Los Angeles}
  \state{California}
  \country{USA}}
\email{ferrarae@isi.edu}




%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{La Gatta et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

%After the beginning of the war between Russia and Ukraine, false and unverified information was shared on social media platforms to mislead people perception about this geo-political event. In addition, 

Nowadays, false and unverified information on social media sway individuals' perceptions during major geo-political events and threaten the quality of the whole digital information ecosystem. Since the Russian invasion of Ukraine, several fact-checking organizations have been actively involved in verifying stories related to the conflict that circulated online. In this paper, we leverage a public repository of fact-checked claims to build a methodological framework for automatically identifying false and unsubstantiated claims spreading on Twitter in February 2022. Our framework consists of two sequential models: First, the \emph{claim detection} model identifies whether tweets incorporate a (false) claim among those considered in our collection. Then, the \emph{claim retrieval} model matches the tweets with fact-checked information by ranking verified claims according to their relevance with the input tweet. Both models are based on pre-trained language models and fine-tuned to perform a text classification task and an information retrieval task, respectively. In particular, to validate the effectiveness of our methodology, we consider 83 verified false claims that spread on Twitter during the first week of the invasion, and manually annotate 5,872 tweets according to the claim(s) they report. Our experiments show that our proposed methodology outperforms standard baselines for both \emph{claim detection} and \emph{claim retrieval}. Overall, our results highlight how social media providers could effectively leverage semi-automated approaches to identify, track, and eventually moderate false information that spreads on their platforms. 
%and also in more challenging settings where the models should generalize to unseen claims. 

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
% <concept>
% <concept_id>10002951.10003227.10003241</concept_id>
% <concept_desc>Information systems~Decision support systems</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Information systems~Decision support systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{claim detection, claim retrieval, fact-checking, Twitter}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
On 24th February 2022, Russia started its still ongoing invasion of Ukraine, causing unprecedented backlashes for the rest of the world.\footnote{\url{https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine}} Soon afterward, concerns were raised about the presence of Russian disinformation campaigns on online social media, aimed at repurposing  the invasion as a ``special operation'' against alleged Nazis in Ukraine, or attempting to blame NATO's expansion for causing the invasion \cite{hanley2022special,park2022voynaslov,pierri2022propaganda,hanley2022happenstance,pierri2022does}. Russian interference with other countries' democratic processes is anything but new, as extensively reported in the past, especially in the context of the 2016 U.S. Presidential election \cite{badawy2018analyzing,luceri2020detecting}.
During major geo-political events, manual fact-checking represents the leading strategy to debunk false information through domain experts' analyses, crowd-sourcing approaches \cite{10.1145/3511808.3557279}, and semi-automatic systems assessing news truthfulness \cite{DBLP:journals/corr/abs-2108-11896}. However, given the impossibility of manually checking every piece of information circulating online \cite{pierri2019false}, social media providers still struggle with 
keeping track of and moderating false information that spreads online \cite{pierri2023one,nogara2022disinformation}. 

Given these premises, in this work, we aim to build a methodological framework to automatically identify false and unsubstantiated claims -- verified by news agencies and fact-checking organizations (e.g., Politifact, Snopes) -- that were shared on Twitter at the dawn of the Russian invasion of Ukraine. To this end, we collect 83 false claims that were verified in the first weeks of the invasion and annotate 5,872 original tweets based on the claim(s) they discuss. Then, we deploy an automatic pipeline that comprises two models: (i) the \emph{claim detection} model identifies whether an input tweet contains a (false) claim among the 83 present in our collection; and, assuming the input tweet reports a claim, (ii) the \emph{claim retrieval} model ranks those 83 claims according to their relevance with the input tweet. For these models, we leverage modern transformer-based architectures and adopt transfer learning on the annotated dataset to optimize the performance on both tasks. 

Previous work to design automatic tools for fact-checking follows two main directions. On the one hand, research extensively focuses on the problem of assessing the truthfulness of pieces of information \cite{DBLP:journals/corr/abs-2103-07769}, 
also supporting the veracity prediction with factual evidence \cite{samarinas-etal-2021-improving,Chen_2022}. On the other hand, relatively less research is devoted to the \emph{claim detection} problem, which can be formulated as a ranking task where several input sentences are ranked by check-worthiness \cite{DBLP:journals/corr/abs-2109-12987,10.1145/3297722}, or as a binary classification task to predict whether an input sentence constitutes a claim \cite{DBLP:journals/corr/abs-2101-11891,10.1145/3412869,10.1145/2806416.2806652}. In this paper, we consider the latter setting as we aim to identify false claims and we are less interested in prioritizing tweets that require fact-checking. 

Once verified that the input tweet reports a (false) claim, we aim to retrieve the most relevant verified claim(s) in our collection. \citet{shaar-etal-2020-known} have recently proposed and defined  \emph{claim retrieval} as the task of ranking a corpus of verified documents according to their relevance to an input text. Recently, competitors at CheckThat!2021 (\cite{DBLP:journals/corr/abs-2109-12987}) have shown that fine-tuning transformer-based models lead to promising performance improvements with respect to standard information retrieval approaches (e.g., BM25 \cite{10.1561/1500000019}). Accordingly, \citet{10.1007/978-3-030-99736-6_25} obtain similar results under multilingual COVID-19 claims. Conversely, considering a political debate scenario, \citet{DBLP:journals/corr/abs-2104-07423} evaluate the impact of modeling the claim's global and local contexts on the ranking performance. Overall, the above contributions compete on benchmark datasets and assume that input sentences contain a claim. Instead, we define a more challenging scenario where \emph{claim retrieval} is combined with \emph{claim detection} in a pipeline, whose goal is to understand whether a generic tweet reports a false claim related to the specific topic of the Russian invasion of Ukraine.  


\paragraph{\textbf{Contributions of this work}}
To summarize, our contributions are as follows:

\begin{itemize}
    \item We collect 83 false claims that spread on Twitter during the first weeks of the invasion and we manually annotate 5,872 original tweets to determine whether they discuss a (false) claim and, if so, which claim(s) they discuss. 
    \item We develop an automatic pipeline to detect and retrieve tweets discussing any of the 83 false claims in our collection. In particular, our models are based on modern transformer-based architectures and perform, in order, \emph{claim detection} and \emph{claim retrieval}. 
    \item We show the effectiveness of the proposed approach in retrieving the claim(s) that are referenced in the input tweet. In addition, we also observe how our models generalize to new claims that were unseen during the training process.
\end{itemize}


% related work
%Existing literature on claim retrieval and related tasks includes identifying check-worthy claims (claim detection), finding the evidence supporting or refuting a claim (evidence retrieval) and detecting claim truthfulness (claim verification) \cite{DBLP:journals/corr/abs-2103-07769,thorne-etal-2018-fever}. The task of detecting previously fact-checked information can be formulated as an information-retrieval problem where a corpus of verified documents is ranked according to their relevance with an input claim \cite{shaar-etal-2020-known}. Based on this, results from the CheckThat!2021 challenge \cite{DBLP:journals/corr/abs-2109-12987} show that fine tuning state-of-the-art transformers into a re-ranking stage provides a considerable performance improvement compared to standard algorithms (e.g. BM25 \cite{10.1561/1500000019}). In addition, MTM \cite{sheng-etal-2021-article} improves transformer's re-ranking performance identifying key sentences and common pattern templates inside the documents corpus. Accordingly, \citep{10.1007/978-3-030-99736-6_25} obtained similar results under multilingual, i.e. English and Arabic, Covid-19 claims. Conversely, considering a debate scenario, \cite{DBLP:journals/corr/abs-2104-07423} evaluates the impact of modelling the claim's global and local contexts on the (re-)ranking performance. On the other hand, MAN \cite{vo2020facts} defines its own neural network-based architecture to perform claim retrieval with multimodal data, i.e. the texts and the images of the claim and of the verified information. 

% problem formulation e contribution

\section{Methodology}

In this section, we describe the data collected for the analysis and the methodology employed to annotate tweets and their corresponding claims. Then, we present our methodological framework by formally defining the \emph{claim detection} and the \emph{claim retrieval} tasks and corresponding models.
% that we address in this work.
 
\subsection{Data Collection}

\begin{table*}
    \centering

    \caption{Examples of some tweet-claim pairs annotated in the dataset}
    \label{tab:examples}
    
    \scalebox{1.}{
    \begin{tabular}{cll}\toprule
    
  \textbf{No.} & \textbf{Claim} & \textbf{Tweet}  \\
 \midrule


\multirow{3}{*}{\begin{tabular}[l]{r@{}} 1 \end{tabular}} 
& Russian President Vladimir Putin threatened & Putin has warned India that don't try to interfere in  \\
& India against getting involved in the Ukraine & their matter, otherwise be ready to face the consequences \\
& crisis. & \\
 \midrule

%\multirow{3}{*}{\begin{tabular}[l]{r@{}} 1 \end{tabular}} 
%&  Ukrainian President Volodymyr Zelensky joined & Ukrainian President Volodymyr Zelenskyy vowed that  \\
%&  military in fighting against Russia. &  his military will keep fighting back and he ordered \\
%&  & a full mobilization.  \\ \midrule

\multirow{3}{*}{\begin{tabular}[l]{r@{}} 2 \end{tabular}} 
& The President Of Ukraine, Volodymyr Zelenskyy, & Volodymyr Zelenskyy the president of Ukraine has \\
&  Is On The Ground With His Fellow Troops  &  decided to stay behind and fight among his people \\
&   &  against the Russian army send to kyiv $[...]$ \\ \midrule


\multirow{3}{*}{\begin{tabular}[l]{r@{}} 3 \end{tabular}} 
& The Russian armed forces are not striking at the  & It is clear that the Russian army does not want to harm  \\
&  cities of Ukraine; they are not threatening the  & civilians, its strikes were directed only at military \\
&  civilian population. & targets, $[...]$ life seems almost normal in Kiev.  \\ \midrule
\multirow{3}{*}{\begin{tabular}[l]{r@{}} 4 \end{tabular}} 
& The Russian armed forces are not striking at the  & Russian forces continue strikes in multiple cities $[...]$.  \\
&  cities of Ukraine; they are not threatening the  & This is premeditated mass murder and must be responded  \\
&  civilian population. & to as such. \\ \bottomrule
    \end{tabular}}


\end{table*}

% We define a claim in accordance with \citet{konstantinovskiy2021toward} as ``an assertion about the world that is checkable''. In particular, since we are interested in detecting false claims regarding the Ukraine-Russia war, we directly target information that has been already verified and considered false by fact-checking news outlets. 
%In particular, since we are interested in detecting false claims regarding the Ukraine-Russia war, we further specialize the above-mentioned definition to consider assertions that are not only ``checkable'', but also false or unverified. Concretely, to identify false and unsubstantiated claims that circulated online during the same period, 

To identify false and unsubstantiated claims that circulated online during the same period, we rely on the Russia-Ukraine ConflictMisinfo Dashboard\footnote{\url{www.shorturl.at/puN37}}, which provides a collection of true and false claims and rumors verified by fact-checking outlets such as \emph{USA Today} and \emph{Snopes}. Specifically, we collect $83$ English false claims that were verified in the period 22nd February - March 1st. 

In addition, we leverage an existing dataset\footnote{Reference omitted for blind review.} of tweets matching over 30 conflict-related keywords in English, Russian and Ukrainian language, which were identified through a snowball sampling approach, collected via Twitter's Filter v1.1 Streaming  API\footnote{\url{https://developer.twitter.com/en/docs/twitter-api/v1}}. In particular, we focus on English-language tweets shared during the initial weeks of the invasion, i.e., from February 22nd, 2022 to March 8th, 2022. Note that we consider the tweets posted until one week after the above-mentioned false claims were verified (March 1st) so as to capture their propagation on Twitter. Overall, in this observation period, the collected dataset contains more than 2M English tweets with original content, i.e., original tweets, replies, and quotes. We purposely exclude retweets as their textual content is exactly the same. 

\subsection{Data Annotation}
\label{sec:dataset}
Given the collected fact-checked information, our goal is to find tweets reporting such verified claims.
% according to their relevance with respect to the collected false claims. 
However, as manually annotating the whole corpus of tweets in our dataset is not a suitable solution, we deploy a machine learning-based annotation strategy to maximize the likelihood of finding tweets related to one of the claims under analysis. %The output of this procedure is a set of tweet-claim  pairs which can be used within a supervised machine learning pipeline. 

Specifically, we first use the RoBERTa transformer \cite{DBLP:journals/corr/abs-1907-11692} to extract the vector embeddings of both claims and tweets, and then we compute the cosine similarity between each claim and tweet in our data, retaining the top-100 most similar tweets for each claim.  Consequently, we end up with 8,300 unique tweet-claim pairs that were inspected via a \emph{manual} labeling process. It is worth noting that the choice of the RoBERTa transformer depends on the higher similarity scores provided by this model with respect to other transformers (\emph{ms-marco-MiniLM-L-4-v2} and \emph{quora-roberta-base}), which allows us to maximize the chance of finding a matching pair.  

%the more diverse similarity scores given for all tweet-claim pairs with respect to other cross-encoders, i.e., \emph{ms-marco-MiniLM-L-4-v2} and \emph{quora-roberta-base}.

Next, we annotate the above-mentioned tweet-claim pairs considering a strict definition of relevance, i.e., the tweet discusses the claim if it explicitly mentions the same entities and events reported in the fact-checked information. Table \ref{tab:examples} shows some examples of false claims and matched tweets, which highlight how the matched tweet can discuss a claim without expressing any stance (examples no. 1 and 2) or can support or refute the claim (examples no. 3 and 4, respectively). Finally, it is worth noting that the number of unique tweets is different from the number of pairs because a tweet can be related to multiple claims. In particular, we find 5,872 unique tweets in the ranking of the 83 claims. 

% The annotation procedure was carried out by two master's students in computer science who were familiar with the topic. Each one was instructed to annotate all tweet-claim pairs according to a binary setting, i.e. whether the tweet was related or not to the claim. Concretely, we adopted a strict definition of relevance, i.e. the tweet is related to the claim if they explicitly mention the same entities and event. Please refer to Table \ref{tab:examples} for some examples. 

% On the first round, the annotators obtained a fair agreement (the Cohen Kappa $\kappa=0.291$). All pairs with disagreement were analyzed during a second round of labeling where the annotators reached an agreement. 

Our manual annotation results in 2,359 (out of 5,872 -- $40.2\%$) tweets associated with at least one claim. Figure \ref{fig:dist} shows the distribution of the number of tweets with respect to the number of claims: most of the tweets are related to less than five claims and only 13 tweets discuss more than 10 claims. Overall, each claim has at least one matching tweet and the most matched claim has 100 related tweets. Conversely, we find 3,513 (out of 5,872 -- $59.8\%$) tweets that do not report any claim.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{imgs/dist.pdf}
  \caption{Distribution of the number of tweets with respect to the number of claims }
  \label{fig:dist}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{imgs/model_design.png}
  \caption{Our proposed methodological framework: the \emph{claim detection} model detects whether the input tweet reports a fact-checked (false) claim. If a claim is detected, the \emph{claim retrieval} model retrieves the most relevant claims (within the corpus of ``Verified Claims'') related to the tweet. }
  \label{fig:model_design}
\end{figure}

\subsection{Our Framework}

Figure \ref{fig:model_design} describes the two-step pipeline proposed in our methodological framework. The first step is characterized by a \emph{claim detection} model, which verifies whether an input tweet discusses a (false) claim included in a predetermined set of already verified claims. Next, if the input tweet actually discusses a false claim in the \emph{Verified Claims} corpus, a \emph{claim retrieval} model ranks the claims in the corpus according to their relevance with the input tweet. It is worth noting that, even if we target false claims, our methodology does not depend on the claim's truthfulness and can be adopted to retrieve tweets matching any claim associated with a specific topic. 

%Our framework, depicted in Figure \ref{fig:model_design}, is a two-stages pipeline: first, it verifies whether an input tweet contains a (false) claim; second, if the tweet contains a claim, it ranks a set of predetermined claims according to their relevance with the input. It is worth to note that our methodology does not depend on the claim truthfulness but can support its estimation since the retrieved claims can be used as evidences for the verification task.

%Concretely, the stages in our pipeline refer to the first two steps in the automatic fact-checking process, i.e., claim detection and claim retrieval, respectively. Next, we formalize the two tasks as a classification and ranking problems, respectively. 

\subsubsection{Claim Detection}
\label{sec:cd}

%We define a claim in accordance with \citet{konstantinovskiy2021toward} as ``an assertion about the world that is checkable''. In addition, since we are interested in detecting false claims regarding the Ukraine-Russia war, we further specialize the above-mentioned definition to consider assertions that are not only ``checkable'', but also false or unverified. 

Overall, we frame the claim detection problem as a binary classification task: given an input tweet $t$, the objective is to establish whether $t$ contains a false claim among the ones collected in the corpus of \emph{Verified Claims}. Therefore, we leverage BERT-based language models \cite{DBLP:journals/corr/abs-1810-04805}, which have proven their effectiveness in several text classification tasks \cite{DBLP:journals/corr/abs-2106-04554} due to the extensive knowledge they acquire during the pre-training process. In particular, we fine-tune the \emph{BERT-base-cased} with the Huggingface library\footnote{https://huggingface.co} for five epochs using Adam optimizer, categorical cross-entropy as a loss function, and 20 tweets as batch size.

In addition, assuming a realistic scenario where only a minority of tweets report specific factual events related to the war \cite{DBLP:journals/corr/abs-2008-08854} (i.e., 2.3k out of 5.9k tweets identified by our annotation procedure), we evaluate our model by performing random oversampling of the minority class (i.e., tweets with a specific claim). In other words, we randomly replicate tweets containing a claim so as to have the same number of tweets belonging to the positive and negative classes. 

\subsubsection{Claim Retrieval}
Following the cascade architecture of our framework (see Fig. \ref{fig:model_design}), we 
% Assuming the previous stage of the pipeline has found that the input tweet contains a false claim, 
are now interested in finding the most relevant claims associated with the input tweets that report a claim, as determined with the \emph{claim detection} model.\footnote{Note that if the claim detection model assesses the input tweet to be unrelated to any claim, the input tweet is automatically discarded and not considered in the claim retrieval model.}
% understanding to which false claim it is related to. 
% In other words, we want to know whether the input text should be processed as a brand-new claim or it can be associated with another one that has already been verified. 
Formally, we leverage the information retrieval formulation related to the task of identifying fact-checked claims \cite{shaar-etal-2020-known}: given an input tweet $t$ containing a claim, rank a set of verified (false) claims $\{c_1, c_2, \cdots, c_n\}$ based on their relevance to $t$. 
 
In particular, we consider the tweet-claim pairs as input of the system and use the similarity score to rank the verified claims. However, performing the supervised training process, we need to build the negative class by finding instances of unrelated tweet-claim pairs. This problem is common when addressing information retrieval tasks \cite{10.1145/3486250}, and several (negative) sampling strategies have been evaluated across different ranking tasks \cite{li-etal-2019-sampling,DBLP:journals/corr/abs-2007-00808}. Among the others, we find that random negative sampling yields good results in our use scenario, i.e., for each tweet we randomly select 10 unrelated claims.

Next, as for the claim detection model, we leverage a transformer-based architecture. Indeed, when optimized for ranking tasks, pre-trained language models have proven their effectiveness in a variety of information retrieval tasks, including semantic search \cite{DBLP:journals/corr/abs-1908-02451} and semantic textual similarity \cite{DBLP:journals/corr/abs-1908-10084}. Specifically, we finetune the \emph{stsb-roberta-base} cross-encoder within the SBERT library\footnote{https://www.sbert.net}. 

We train the model for three epochs using categorical cross-entropy loss and 16 tweet-claim pairs as batch size. 

%As a result, there will be 10 ``negative'' pairs for each tweet. 

%In particular, we considered  the 2,359 tweets related to at least one of the 83 claims from the Russia-Ukraine ConflictMisinfo Dashboard, and build up the dataset from the corresponding tweet-claim pair. We ended up with a sample of XXX \emph{positive} tweet-claim pairs. Next, to perform any supervised training process, we need to select \emph{negative} pairs, i.e., pairs where the tweet and the claim do not match. This problem is common when addressing information retrieval tasks \cite{10.1145/3486250}, and several negative sampling strategies have been evaluated across different ranking tasks \cite{li-etal-2019-sampling,DBLP:journals/corr/abs-2007-00808}. We found that random negative sampling yielded to good results in our use case, thus, we randomly selected 10 negative claims to pair each tweet. As a result, we ended up with XXX*11 pairs which, in turn, were split in training (60\%), validation (20\%) and testing sets. Next, we fine tuned two BERT-based models from the the SBERT library, i.e. the \emph{stsb-roberta-base} cross-encoder and the \emph{mpnet-base} bi-encoder. While the former has better ranking performance, the latter is more efficient and scales better to higher number of tweet-claim pairs. However, we only compare these two models on their effectiveness because we did not face any computing time issue because of the limited size of our dataset.





\section{Experiments}
This section presents the experiments we have performed to evaluate our framework. All experiments are conducted on a machine equipped with CPU Intel Xeon-4116, RAM 32 GB, and one NVIDIA A100. 

\subsection{Evaluation Data and Metrics}
In this section, we describe both the data sets and metrics used to evaluate our methodological framework. We distinguish these for the claim detection and claim retrieval models, given the different prediction goals of the two tasks.
For the claim detection task, we consider the annotated tweets described beforehand. Our dataset includes 2,359 (out of 5,872 -- 40,2\%) tweets reporting at least one (false) claim and 3,513  (out of 5,872 -- 59,8\%) tweets that do not incorporate any claim. Given the classification task, we measure the performance of our models considering binary classification metrics such as precision, recall, f-score, and accuracy. 

For the claim retrieval task, we consider the 2,359 tweets related to the 83 claims gathered from the Russia-Ukraine ConflictMisinfo Dashboard. We build up a dataset of 40,007 
tweet-claim pairs as previously discussed, consisting of 3,637 \emph{positive} tweet-claim pairs and 36,370 \emph{negative} tweet-claim pairs.
% . Overall, after having applied random negative sampling, we obtained  
Consistently with previous work \cite{vo2020facts,shaar-etal-2020-known}, we evaluate the performance of the claim retrieval model with the HitRatio@K, i.e., whether the correct claim (i.e., the claim matching the tweet) is among the top-k results of the ranking. It is worth noting that, since most of the tweets match only one claim, HitRatio@K is almost equal to Recall@K \cite{vo2020facts}. On the one hand, from the end-user's perspective, the HitRatio on lower values of $k$ (e.g. $k \in \{1,3,5\}$) might be indicative of the system utility in easing manual fact-checkers works, i.e., experts would spot in real-time if the top-ranked results are relevant to the input claim. On the other hand, the HitRatio on higher values of $k$ (e.g. $k \in \{10,20, 50\}$) should be considered in offline settings and/or in an automated fact-checking pipeline where results should be further processed as evidence for the veracity prediction. 

\subsection{Experimental Protocol}

\begin{table}
    \centering

    \caption{Claim detection: performance with and without random oversampling}
\label{tab:oversampling}

    \scalebox{1.}{
    \begin{tabular}{r|cc}\toprule
    
  \textbf{Metric} & \textbf{No Oversampling} & \textbf{With Oversampling}  \\ \midrule

Precision & 79.90\% & \textbf{81.39\%} \\
Recall    & 80.19\% & \textbf{80.24\%} \\
F1-score & 79.35\% & \textbf{80.57\%} \\
Accuracy  & 80.11\% & \textbf{81.59\%} \\ \bottomrule
\end{tabular}}


\end{table}


\begin{table*}
    \centering

\caption{Claim detection: performance comparison per class, and their 95\% confidence interval, between TF-IDF baseline and our approach (bold indicates best on average, $^*$ indicates statistical significance ($p<0.01$)}
\label{tab:per_class}

    \scalebox{1.}{
    \begin{tabular}{r|cc|cc}\toprule
    
  \textbf{Metric} & \multicolumn{2}{c}{Claim (Positive class)} & \multicolumn{2}{c}{No Claim (Negative class)}  \\  \cmidrule{2-5}

  & TF-IDF & Ours & TF-IDF & Ours \\ \midrule

Precision  & $67.88\% \pm 1.57\% $ & $\mathbf{79.75\%  \pm 3.05\%}^*$  & $80.97\% \pm 2.92\% $ & $\mathbf{83.02\%   \pm 1.87\%}$    \\
Recall     & $73.01\% \pm 4.99\% $ & $\mathbf{73.31\%  \pm 2.09\%}$  & $76.84\% \pm 0.81\% $ & $\mathbf{87.17\%   \pm 3.85\%}^*$    \\
F1-score   & $70.33\% \pm 3.05\% $ & $\mathbf{76.17\%  \pm 1.71\%}^*$  & $78.84\% \pm 1.40\% $ & $\mathbf{84.97\%   \pm 1.39\%}^*$  \\ \bottomrule

\end{tabular}}


\end{table*}

 For each task, we perform a 5-fold cross-validation to evaluate the performance of our models. In particular, we consider two evaluation settings:

 \begin{itemize}
     \item Leave Tweet Out (LTO) Assessment: for both the claim detection and retrieval tasks, we ensure that the tweets in the training, validation, and testing sets do not overlap.  
     \item Leave Claim Out (LCO) Assessment: for the claim detection task, we ensure that the tweets in the training, validation, and testing sets match different claims. For the claim retrieval task, we ensure that the claims within the tweet-claim pairs in the training, validation, and testing sets do not overlap.
 \end{itemize}

The LTO assessment refers to the classical evaluation of supervised machine learning approaches and is consistent with experiments from previous works \cite{shaar-etal-2020-known,vo2020facts,DBLP:journals/corr/abs-2109-12987}. The LCO assessment focuses on the generalization of the models to unseen claims and is close to a real-world scenario. Indeed, when deployed "in the wild", both claim detection and retrieval should be performed on tweets and claims that the models have not seen during the training process. 

When evaluating the performance of the claim detection task, we compare our BERT-based model to a TF-IDF baseline \cite{10.1145/3412869}, also assessing the effectiveness of the oversampling strategy detailed in Section \ref{sec:cd}. For the claim retrieval task, we compare the ranking performance of our BERT-based cross-encoder with respect to Sentence-BERT \cite{DBLP:journals/corr/abs-1908-10084}. %It is worth noting that we do not evaluate their computational efficiency because of the limited size of our dataset, given that there are only 83 claims to rank and we never faced any computing time issue.
%\subsubsection{Experimental Setup and Training Details}

%We leveraged modern state-of-the-art transformers \cite{DBLP:journals/corr/abs-2106-04554} and fine tuned these models on the dataset annotated with tweets and (false) claims  that spread on Twitter during the Ukraine-Russian war. All experiments have been conducted on a machine equipped with CPU Intel Xeon-4116, RAM 32 GB and one NVIDIA A100. 

%For the claim detection task, we considered a binary classification setting: the 2,359 tweets related to at least one (false) claim are considered as the positive class, while the 3,513 tweets which do not have any related claim are the negative class. Next, we finetuned the pre-trained \emph{BERT-base-cased} within the SBERT library\footnote{} and, after having optimized some hyperparameters (e.g. number of epochs,  batch size, learning rate), we evaluated the performance on the testing set. Finally, since the dataset is not balanced, i.e., there are more tweets with the negative class (59,8\%) than with the positive class (40,2\%), we assessed whether dealing with data imbalance could improve the performance. In particular, we performed random oversampling of the minority class (tweets with no claim) on the training set and compared the results with and without oversampling on the testing set. Overall, we measured the performance of the system considering standard classification metrics, i.e. precision, recall, f-score and accuracy. 

%For the claim retrieval task, we considered the tweet-claim pairs as input of the system. In particular, we considered  the 2,359 tweets related to at least one of the 83 claims from the Russia-Ukraine ConflictMisinfo Dashboard, and build up the dataset from the corresponding tweet-claim pair. We ended up with a sample of XXX \emph{positive} tweet-claim pairs. Next, to perform any supervised training process, we need to select \emph{negative} pairs, i.e., pairs where the tweet and the claim do not match. This problem is common when addressing information retrieval tasks \cite{10.1145/3486250}, and several negative sampling strategies have been evaluated across different ranking tasks \cite{li-etal-2019-sampling,DBLP:journals/corr/abs-2007-00808}. We found that random negative sampling yielded to good results in our use case, thus, we randomly selected 10 negative claims to pair each tweet. As a result, we ended up with XXX*11 pairs which, in turn, were split in training (60\%), validation (20\%) and testing sets. Next, we fine tuned two BERT-based models from the the SBERT library, i.e. the \emph{stsb-roberta-base} cross-encoder and the \emph{mpnet-base} bi-encoder. While the former has better ranking performance, the latter is more efficient and scales better to higher number of tweet-claim pairs. However, we only compare these two models on their effectiveness because we did not face any computing time issue because of the limited size of our dataset. Consistently with previous works \cite{vo2020facts,shaar-etal-2020-known}, we measured performance with the HitRatio@K, i.e. whether there is a true positive claim among the top-k results. It is worth to note that, since most of the tweets match only one claim, HitRatio@K is almost equal to Recall@K \cite{vo2020facts}. From the application perspective, the HitRatio on lower values of $k$ (e.g. $k \in \{1,3,5\}$) might be indicative of the system utility in easing manual fact-checkers works, i.e. experts would spot in real time if the top ranked results are relevant to the input claim. On the other hand, the HitRatio on higher values of $k$ (e.g. $k \in \{10,20, 50\}$) should be considered in offline settings and/or in an automated fact-checking pipeline where results should be further processed as evidences for the veracity prediction. 


\subsection{Results}


\subsubsection{Claim Detection}

Table \ref{tab:oversampling} shows classification performance with and without the random oversampling strategy. In particular, we can notice that oversampling the minority class improves the predictive power of the claim detection model for every classification metric. 

Table \ref{tab:per_class} shows the performance marginalized for each class of the \emph{claim detection} task. In particular, our BERT-based model outperforms the TF-IDF baseline for both classes and all metrics. Overall, we can observe that performance is better on the negative class, i.e., the system has higher precision and recall in the detection of tweets that do not report any claim. We do not report the accuracy metric as it is equivalent to the recall computed on the single class. 

Finally, Table \ref{tab:lto_lco} shows the aggregated performance under the LTO and LCO evaluation settings. We can observe that LCO evaluation is more challenging since the model has to deal with claims that it has never seen during the training process. 

\begin{table}
    \centering

    \caption{Claim detection: LTO and LCO assessment}
\label{tab:lto_lco}

    \scalebox{1.}{
    \begin{tabular}{r|cc} 
    & \multicolumn{2}{c}{Settings} \\ 
    
    \toprule
    
  \textbf{Metric} & \textbf{LTO} & \textbf{LCO}  \\ \midrule

Precision & $81.39\% \pm 1.14\% $ & $78.07\% \pm 1.48\% $ \\
Recall    & $80.24\% \pm 1.36\% $ & $77.58\% \pm 1.67\% $ \\
F1-score  & $80.57\% \pm 1.38\% $ & $77.63\% \pm 1.60\% $ \\
Accuracy  & $81.59\% \pm 1.42\% $ & $78.03\% \pm 1.56\% $ \\ \bottomrule
\end{tabular}}

\end{table}


\subsubsection{Claim Retrieval}
Table \ref{tab:retrieval} shows the ranking performance of our claim retrieval model in comparison with sentence-BERT \cite{DBLP:journals/corr/abs-1908-10084}. In both evaluation settings (i.e., LTO and LCO), our model outperforms the baseline, and the performance gap is even larger when considering the top positions in the ranking ($k \in \{1,3\}$). This result highlights that our system can actually ease fact-checkers' work because it can spot tweets that discuss false claims that were already verified. In addition, we can notice again that the performance under LTO settings is slightly better than the ones under LCO settings, even if the gap is smaller with respect to what we found in the claim detection task. 

\begin{table*}
    \centering

    \caption{Claim retrieval: performance comparison, and their 95\% confidence interval, between the sentence-BERT baseline and our approach (bold indicates best on average, $^*$ indicates statistical significance ($p<0.01$)}

    \label{tab:retrieval}
    
\resizebox{\textwidth}{!}{  
    \begin{tabular}{rcccccc}\toprule
    
Setting & Model  &  \multicolumn{5}{c}{HitRatio{$@k$}} \\
 & &  $k=1$ & $k=3$ & $k=5$ & $k=10$& $k=20$  \\ \midrule

\multirow{2}{*}{\begin{tabular}[c]{c}LTO\end{tabular}} &
Sentence-BERT   & $85.25\% \pm 2.07\% $ &  $94.87\% \pm 1.69\% $ & $97.24\% \pm 0.96\% $ & $98.77\% \pm 0.43\%1 $ & $99.27\% \pm 0.39\% $   \\ 
& Ours & $\mathbf{86.05\% \pm 0.95\% }$ &  $\mathbf{96.35\% \pm 0.71\% }^*$ & $\mathbf{98.04\% \pm 0.57\% }^*$ & $\mathbf{99.27\% \pm 0.36\%} $ & $\mathbf{99.78\% \pm 0.11\%}^* $   \\ \midrule

\multirow{2}{*}{\begin{tabular}[c]{c}LCO\end{tabular}} &
Sentence-BERT   & $77.60\% \pm 0.1196 $ &  $95.68\% \pm 6.74\% $ & $98.01\% \pm 3.66\% $ & $99.63\% \pm 0.66\% $ & $99.78\% \pm 0.00\% $   \\ 
& Ours & $\mathbf{82.25\% \pm 10.81\%}^* $ &  $\mathbf{96.42\% \pm 2.59\%} $ & $\mathbf{98.26\% \pm 1.45\%} $ & $\mathbf{99.88\% \pm 0.02\%} $ & $\mathbf{99.96\% \pm 0.00\%} $  \\ \bottomrule

    \end{tabular}
    }

\end{table*}



\subsubsection{Evaluation in the wild}

Finally, we apply our claim detection and retrieval models on the whole Twitter dataset to assess how our methodology performs in a real scenario.  In this case, we do not have any annotation for the LTO and LCO settings, and thus we cannot perform a formal quantitative analysis. However, we randomly sample 100 (resp. 100) tweets assessed as reporting a claim (resp. not reporting any claim) by the claim detection model. Then, we manually verify whether the models make the correct prediction, i.e., whether the tweet discusses a false claim and, if so, to which false claim it was related.

%200 tweets from the dataset and manually verified whether the models make the correct prediction, i.e., whether the tweet discusses a false claim and, if so, to which false claim it was related to. In particular, we annotate 152 tweets which are not related to any claim and 48 tweets which discuss one of the 83 false claims considered through this study. For the latter tweets, we also annotate the claim(s) that each tweet is related to. 

Out of the 100 tweets that are predicted by the \emph{claim detection} model as not related to any (false) claim, we find 12 misclassified instances, i.e., tweets that are actually related to a false claim. This result confirms that the \emph{claim detection} model has a high precision ($>80\%$) on the negative class (see Table \ref{tab:per_class} for further details). 

On the contrary, out of the 100 tweets that are predicted by the \emph{claim detection} model as discussing a (false) claim, we find 64 false positives, i.e., tweets do not discuss any false claim but generally report information about the Ukraine-Russia war. This result confirms the performance differences between the positive and negative classes (see Table \ref{tab:per_class}) of the \emph{claim detection} model, and probably depends on data imbalance, i.e., the number of tweets with no claim is higher than the number of tweets reporting a claim. 

%This result confirms the worse performance achieved on the positive class by the classification model, and probably depends on the least number of tweets discussing a claim within the training dataset. 

In addition, we apply the \emph{claim retrieval} model for the 36 tweets that were actually related to some false claims. In particular, we retrieve the top-3 relevant false claims and find that the correct matching claim was retrieved 34 times (out of 36). 

%As a result, the overall pipeline has been effective for 71\% of the tweets (34 out of 48) discussing false claims. In particular, 12 errors depends on the claim detection model which has not recognized a tweet related to a false claim, and 2 errors depends on the claim retrieval model which has not retrieved the correct claim within the corpus of verified claims. 

Overall, these results confirm the promising performance of the \emph{claim retrieval} model but highlight the limitations of the \emph{claim detection} model, which overestimates the number of tweets discussing false claims. 


\section{Conclusions and Future Work}

% In this paper, in the context of the war between Russia and Ukraine, w
We presented a methodological framework to detect over 80 false and unsubstantiated claims that were shared on Twitter during the first week of the conflict. Our framework first performs \emph{claim detection} to identify whether an input tweet contains a (false) claim or not. Then, assuming the input tweet reports a claim, the system performs \emph{claim retrieval} to rank a set of already-verified false claims according to their relevance with the input tweet. When fine-tuning modern BERT-based models, our methodology achieves promising performance to automate both tasks. Indeed, our models show good generalization capabilities, i.e., they reach good performance even when the claims that need to be detected were never seen during the training process. 

Despite the promising performance, we highlight some limitations of our approach. First, we considered a limited set of false claims because of the short observation window, i.e., the first week of the invasion. However, when focusing on a longer time period, the number of verified claims increases as well as what was considered false at the beginning of the conflict could become true afterward (e.g., NATO's members providing Ukraine with military weapons). Second, when collecting tweets discussing false claims, we did not consider their stance and implicitly assumed 
they support or divulge the claims \cite{youtubetweetpair}. However, we actually find some tweets that reported (false) claims debating their veracity.  


There is a number of avenues to explore in future research. First, we aim to extend our analysis to multimodal data, i.e., images and videos that were shared within the tweets could help our system to improve its classification and ranking performance. Second, we plan to apply our methodology ``into the wild'' to investigate the diffusion of the most shared (false) claims on Twitter and unveil the communities that support or are most susceptible to false narratives. Third, we will deploy our framework within a fact-checking pipeline and assess the extent to which such a system could improve manual fact-checking by allowing debunkers to focus on brand-new claims and ignoring similar claims that have been already verified.  

\section{Acknowledgments}
Work supported in part by
DARPA (contract \#HR001121C0169) and PRIN grant HOPE (FP6, Italian Ministry of Education).
% Darpa?
% Valerio?
%Work supported in part by PRIN grant HOPE (FP6, Italian Ministry of Education)

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base.bib}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
