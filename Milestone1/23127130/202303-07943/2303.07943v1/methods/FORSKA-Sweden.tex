\documentclass[../main.tex]{subfiles}

\begin{document}

\textit{H. Håkansson, A. Sjöberg, M. C. Toribio, M. Önnheim, M. Olberg, E. Gustavsson, M. Lindqvist, M. Jirstrand, J. Conway}\newline

\noindent The FORSKA-Sweden team performed source detection using a U-Net \citep{Ronneberger2015U-Net:Segmentation} CNN with a ResNet  \citep{He2016DeepRecognition} encoder. A training set was generated from the lower 80\% of the development cube, split along the x-axis, by applying a binary mask to all pixels within range of a source defined by a cylinder using source properties (major axis, minor axis, line width) from the truth catalogue. Batches of 128 cubes of size $32\times32\times32$ pixels were sampled from the training area. Half of these cubes contained pixels assigned to a source in the target mask, which caused galaxy pixels to be over-represented in a training batch compared to the full development cube. This over-representation made training more efficient. The remaining 20\% of the development cube was used for frequent validation and tuning of model hyperparameters. 

We used the soft Dice loss as the objective function \citep{Milletari2016V-Net:Segmentation,Khvedchenya_Eugene_2019_PyTorch_Toolbelt}. The initial weights of the model, pretrained from ImageNet, were provided by the {\sc PyTorch}-based {\sc Segmentation Models} package \citep{Yakubovskiy:2019}. Each 2D $k \times k$-filter of the pretrained model was converted to a 3D filter with a procedure similar to \cite{Yang2021ReinventingImages}. We aligned two dimensions to the spatial plane, and repeated the same 2D filter for $k$ frequencies, which resulted in a $k \times k \times k$ filter. The Adam optimizer \citep{Kingma2014Adam:Optimization} with an initial learning rate of $10^{-3}$ was used for training the model. The trained CNN was applied to the raw Challenge data cube to produce a binary segmentation mask assigning each pixel either to a galaxy or not (Fig.~\ref{fig:forska}). 

The {\it merging} and {\it mask dilation} modules from {\sc SoFiA 1.3.2} \citep{Serra2015SoFiA:Data} were employed to post-process the mask and extract coherent segments into a list of separated sources. The last step of the pipeline was to compute the characterisation properties for each extracted source. Some source properties were estimated in the aforementioned {\sc SoFiA} modules, while others had to be computed outside in our code. The most recent weights obtained from CNN training and a fixed set of hyperparameters from the post-processing step were used to compute a score intended to mimic the scoring of the Challenge. The best model from training was then used as a basis for hyperparameter tuning, again using the mimicked scoring.







\begin{figure}
    \centering
    \includegraphics[width=1.05\linewidth]{images/segmentation.png}
    \caption{Cross-section images of input data, target and prediction with velocity and one positional dimension for one of the sources in the cube by team FORSKA-Sweden. The position axis is aligned with the major axis of the source.}
    \label{fig:forska}
\end{figure}



\end{document}