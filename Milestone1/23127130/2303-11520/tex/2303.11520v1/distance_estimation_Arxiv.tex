\documentclass[10pt,twocolumn,letterpaper]{article} 

\usepackage{avss}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{times}
\usepackage{epsfig}
\usepackage{xcolor}
\usepackage{float}
\usepackage{bm}
\usepackage{subcaption}

% Comments and revisions
\newif\ifcomments
% \commentstrue
\commentsfalse

\def\todo#1{{\color{red} [TODO: #1]}}
\ifcomments
    \usepackage{ulem}
    \def\mc#1{{\color{blue} [\textbf{MC:} #1]}}
    \def\mcrep#1#2{{ \sout{#1}}{\color{cyan}\ #2}}
    \def\jlk#1{{\color{red} [\textbf{JLK:} #1]}}
    \def\jlkrep#1#2{{\sout{#1}}{\color{orange}\ #2}}
    \def\picomment#1{{\color{magenta} [\textbf{PI:} #1]}}
    \def\pirevised#1#2{{\color{red}\sout{#1}}{\color[rgb]{1.0,0.4,0.0}\ #2}}
    \def\zl#1{{\color{green} [\textbf{ZL:} #1]}}
    \def\zlrep#1#2{{\sout{#1}}{\color{pink}\ #2}}
\else 
    \def\mc#1{}
    \def\mcrep#1#2{#2}
    \def\jlk#1{}
    \def\jlkrep#1#2{#2}
    \def\picomment#1{}
    \def\pirevised#1#2{#2}
    \def\zledit#1{}
    \def\zlrep#1#2{#2}
\fi

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\avssfinalcopy % *** Uncomment this line for the final submission

\def\avssPaperID{14} % *** Enter the AVSS Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifavssfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
%\title{Social Distance Violation Detection using Fisheye Cameras}
%\title{Estimation of Distance Between People from a Single Overhead Fisheye Camera with Application to Social-Distancing Oversight}

\title{Estimating Distances Between People using a Single Overhead Fisheye Camera with Application to Social-Distancing Oversight}
      
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{Zhangchi Lu, Mertcan Cokbas, Prakash Ishwar, Janusz Konrad\thanks{This work was supported by ARPA-E (agreement DE-AR0000944) and by Boston University Undergraduate Research Opportunities Program.
% 978-1-6654-6382-9/22/\$31.00 ©2022 IEEE
}\\
Department of Electrical and Computer Engineering, Boston University\\
8 Saint Mary's Street, Boston, MA 02215\\
{\tt\small [zhchlu, mcokbas, pi, jkonrad]@bu.edu}
}

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
  Unobtrusive monitoring of distances between people indoors is a useful tool in the fight against pandemics.  A natural resource to accomplish this are surveillance cameras. Unlike previous distance estimation methods, we use a single, overhead, fisheye camera with wide area coverage and propose two approaches. One method leverages a geometric model of the fisheye lens, whereas the other method uses a neural network to predict the 3D-world distance from people-locations in a fisheye image. To evaluate our algorithms, we collected a first-of-its-kind dataset using single fisheye camera, that comprises a wide range of distances between people (1--58 ft) and will be made publicly available. The algorithms achieve 1--2 ft distance error and over 95\% accuracy in detecting social-distance violations.
   %Estimating the distance between people has gained more significance especially with the recent pandemic introducing terms like ``social distancing" into our daily lives. To address this problem, we propose two methods. Unlike previous distance estimation methods, we use a single omnidirectional camera to cover a large space while remaining cost-efficient. Our first method uses the geometric properties of the fisheye lens to map the pixel locations to 3-D world coordinates to estimate the distance between people, whereas, the second method maps two pixel locations to a 3-D world distance using a neural network. For evaluating our algorithms, we collected an inter-people distance dataset with a fisheye camera, which is the first of its kind and will be publicly available. Both algorithms achieve excellent accuracy in detecting the social distance violations.
   \vglue -0.5cm
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The general problem of depth/distance estimation in 3D world has been studied in computer vision from its beginnings. However, the narrower problem of estimating the distance between people has gained attention only recently. In particular, the COVID pandemic has sparked interest in inconspicuous monitoring of social-distance violations (e.g., less than 6ft) \cite{IPM_distance, track_and_estimate, CNN_distance, YOLO_3_distance,social_deep, Pose_estimation, KORTE_dataset}. 
%One sensing modality to accomplish this goal are 
A natural, cost-effective resource that can be leveraged to accomplish this goal are the
%
surveillance cameras widely deployed in commercial, office and academic buildings.
%

%Social distancing is the practice of people staying at least 6 feet apart from each other. A ``social distance violation" occurs when people are less than 6 feet apart. Currently, building managements use human effort to detect ``social distance violations". However, automating this process would save resources for building managements. In this paper, we will introduce two distance estimation methods that use cameras to monitor the building.

%Estimating the distance between objects/people from images has been a fundamental problem in computer vision. It gained even more popularity as the recent pandemic introduced the term ``social distance" into our daily lives \cite{IPM_distance, track_and_estimate, CNN_distance, YOLO_3_distance,social_deep}. Social distancing is the practice of people staying at least 6 feet apart from each other. A ``social distance violation" occurs when people are less than 6 feet apart. Currently, building managements use human effort to detect ``social distance violations". However, automating this process would save resources for building managements. In this paper, we will introduce two distance estimation methods that use cameras to monitor the building.

Recent methods developed for the estimation of 3D distance have typically used 2 cameras (stereo) equipped with either rectilinear \cite{stero_camera, distort_compensate} or fisheye \cite{equirect_images, disparity_offset} lenses. Stereo-based methods, however, require careful camera calibration (both intrinsic and extrinsic parameters) and are very sensitive to misalignments between cameras (translation and rotation) after calibration. Although methods have been proposed using single rectilinear-lens camera \cite{track_and_estimate, CNN_distance,social_deep, KORTE_dataset, Pose_estimation}, that do not suffer from stereo calibration and misalignment shortcomings, usually one such camera can cover only a fragment of a large space. While multiple cameras can be deployed, this increases the cost and complexity of the system.

%The classical approach to estimating distance between objects using cameras leverages stereo \cite{stero_camera, distort_compensate, equirect_images, disparity_offset} and requires careful camera calibration (both intrinsic and extrinsic parameters). \jlk{Depth/distance from stereo is a classical old problem and we are citing only recent literature. I think we need to limit this to people which is a newer application.}  {\color{red}Recently, methods have been proposed using single rectilinear-lens camera \cite{track_and_estimate, CNN_distance,social_deep} that require no such calibration.} \jlk{Learning based?} \mc{Actually, the single camera methods also use geometric properties of the cameras. They assume that they know the parameters. So, I think we should not put this sentence into the paper.}

%Our focus is on large indoor spaces ($>$1,000ft$^2$) whose full coverage would require a number of rectilinear-lens cameras with associated costs and difficulties. Instead, we use a single overhead fisheye camera with a 185$^\circ$-wide lens. However, such cameras introduce geometric distortions that must be accounted for when estimating distances in 3D space.

%\picomment{Last para discussed 2-camera stereo and 1-camera rectilinear. What about 1-camera fisheye? Is ours the very first?}
In this paper, we focus on estimating the distance between people indoors using a \textit{single} overhead {\it fisheye} camera with 360$^\circ\times$180$^\circ$ field of view. Such a camera can effectively cover a room up to 2,000ft$^2$ greatly reducing deployment costs compared to multiple rectilinear-lens cameras. However, fisheye cameras introduce geometric distortions so methods developed for rectilinear-lens cameras are not directly applicable; the geometric distortions must be accounted for when estimating distances in 3D space.

%s, such as large classrooms, cafeterias, open offices. (72 $\times$ 28 feet) room by using a single camera. We believe using a single camera will reduce the cost of the deployment of our system. Thus, the stereo-based distance estimation methods do not apply to our case. To capture the whole room with a single camera, we used a fisheye-lens camera with 360$^\circ\times$180$^\circ$ FOV. Fisheye cameras have wide FOV in expense of fisheye lens distortion. Due to these distortions, we were not able to use the distance estimation methods that use single rectilinear lens camera.

We propose two methods to estimate the distance between people using a single fisheye camera. The first method leverages a fisheye-camera model and its calibration methodology developed by Bone \textit{et al.}~ \cite{geo_paper} to inverse-project location of a person from fisheye image to 3D world. This inverse projection suffers from scale (depth) ambiguity that we address by using a human-height constraint. Knowing the 3D-world coordinates of two people we can easily compute the distance between them. Unlike the first method based on camera geometry, the second method uses the Multi-Layer Perceptron (MLP) and is data-driven.
%We discuss the training data and strategy to estimate distances by using a MLP.
In order to train the MLP, we collected training data using a large chess mat. For testing both methods, we collected another dataset with people placed in various locations of a 72$\times$28-foot room. The dataset includes over 300 pairs of people with over 70 different distances between them. %While at some locations a person's body is fully visible from the camera, at many locations it is partially occluded by tables and/or chairs.
Unlike other inter-people distance-estimation datasets, our dataset, due to the large field of view, includes a wide range of distances between people (from 1 ft to 58 ft). We call this dataset {\it Distance Estimation between People from Overhead Fisheye cameras} (DEPOF). 

The main contributions of this work are: 

%We propose two methods to estimate distance between people using a single fisheye camera. The first method that we propose is an extension to an existing model proposed by Bone \textit{et al.}~ \cite{geo_paper}. By using this geometric model that is developed for fisheye cameras, we map the pixel locations of people to their 3D-world coordinates. Then, we use these 3D-world coordinates to estimate the distance between people. The second method we propose is based on a MLP. We discuss the training data and strategy to estimate distances by using a MLP. Moreover, we collected training and calibration data for the proposed methods. For testing, we collected and labelled over 250 point pairs. Unlike other distance estimation datasets, due to wide FOV fisheye cameras, in our dataset, we have indoor point pairs which are 700 inches away from each other. The main contributions of this work are: 

% \begin{enumerate}

% \item \textbf{Development of two methods for distance estimation between people using a \textit{single overhead fisheye camera}.} To the best of our knowledge no such method has been developed to date.
% %\picomment{Has nobody done this using 1 fisheye or has nobody done this using 1 overhead fisheye?}
% %\jlk{I do not think so. The paper I sent yesterday only applies a single overhead fisheye camera and RAPiD to detect social distancing violations but does not provide any algorithm or numerical results.}
% %We propose two pipelines, a geometry-based approach and a deep learning-based approach.

% \item \textbf{A fisheye-camera dataset for evaluation of inter-people distance-estimation methods.} This is the first dataset of its kind that we will make publicly available, including calibration and training data.
% %for the proposed algorithms. Unlike existing distance estimation datasets, our dataset is collected with a single fisheye camera.

% \end{enumerate}
\begin{enumerate}

\item \textbf{We propose two approaches
for distance estimation between people using a \textit{single overhead fisheye camera}.} To the best of our knowledge no such approach has been developed to date.

\item \textbf{We created a fisheye-camera dataset for the evaluation of inter-people distance-estimation methods.} This is the first dataset of its kind that 
%includes calibration and training data, and 
is publicly available at \href{https://vip.bu.edu/depof}{vip.bu.edu/depof}

\end{enumerate}


\section{Related Work}

In the last two years, spurred by the COVID pandemic, a number of methods have been developed to estimate distances between people indoors. Most of these methods comprise two key steps: the detection of people in an image, and estimation of the 3D-world distance between people.

In order to detect people/objects, some methods \cite{YOLO_3_distance,geo_flat} rely on YOLO, other methods \cite{CNN_distance,track_and_estimate} use Faster R-CNN and still other methods \cite{IPM_distance} use GMM-based foreground detection. However, this is not the focus of this paper; we assume that bounding boxes around people are available.

%Researchers tried out different object detection algorithms. In \cite{YOLO_3_distance,geo_flat} YOLO, in \cite{CNN_distance,track_and_estimate} FRCNN and in \cite{IPM_distance} GMM-based foreground detection is used for object/people detection. 

%As for the estimation of distance between detected people, two types of approaches have emerged: stereo-based \cite{stero_camera, distort_compensate, equirect_images, disparity_offset} and single-camera \cite{track_and_estimate, CNN_distance, KORTE_dataset, Pose_estimation, social_deep}. Approaches requiring the use of two (or more) cameras suffer from increased deployment costs and the need for careful camera calibration. They are also sensitive to post-calibration misalignments. On the other hand, a single camera typically covers a relatively small area, especially when equipped with a rectilinear lens.

To estimate the distance between detected people, a number of approaches have emerged that use a single camera with rectilinear lens. Some approaches rely on typical dimensions of various body parts (e.g., shoulder width) \cite{Pose_estimation, KORTE_dataset}, while others perform a careful camera calibration \cite{track_and_estimate, social_deep, CNN_distance} to infer inter-person distances. Also, stereo-based methods (two cameras) have been recently proposed to estimate the distance to a person/object \cite{stero_camera, distort_compensate}, but they require very precise camera calibration and are sensitive to post-calibration misalignments.

%\jlk{What are the weaknesses of these methods? Do they all assume that feet are visible so no occlusions? This could be another distinguishing feature of our method.}  \mc{Some of them do deal with occlusion. They do body pose estimation, so even if the feet are not visible they estimate where would it be. One weakness of stereo-based methods that authors pointed out is that, their performance drop when people are far from the cameras.}

%The main downside of the stereo-based approaches is that they require multiple cameras to operate, which increases the cost of deployment of these systems. On the other hand, problem with the proposed single-camera methods is that, all these methods are developed for rectilinear-lens cameras which do not have wide enough FOV to cover a large room (e.g. 72 × 28 feet classroom in our case).

%Since our focus is on large indoor spaces ($>$1,000ft$^2$), in order to reduce deployment costs we opt for a single overhead fisheye camera instead of multiple rectilinear-lens cameras. However, fisheye cameras introduce geometric distortions that must be accounted for when estimating distances in 3D space; methods developed for rectilinear-lens cameras cannot be directly applied.

%To date, fisheye cameras have been used for distance estimation in stereo configuration \cite{equirect_images, disparity_offset}. Also, recently fisheye stereo was proposed for person re-identification based on location rather than appearance \cite{geo_paper}. To accomplish this, the authors developed an interesting calibration method to determine both intrinsic and extrinsic camera parameters. We leverage this study to calibrate our single fisheye camera and we use a geometric model developed therein.

%\jlkrep{Since rectilinear-lens cameras have a fairly narrow field of view,}{}
Very recently, a single overhead fisheye camera was proposed to \textit{detect} social distance violations in buses (which is coarser goal than distance estimation), but no quantitative results were published \cite{Tsik2022}.
%Front-facing stereo-fisheye cameras were proposed for distance estimation in autonomous navigation \cite{equirect_images, disparity_offset}. Recently overhead fisheye-stereo was proposed for person re-identification indoors based on location rather than appearance \cite{geo_paper}.
Fisheye-stereo is often used in front-facing configuration for distance estimation in autonomous navigation \cite{equirect_images, disparity_offset}, but recently it was proposed in overhead configuration for person re-identification indoors based on location rather than appearance \cite{geo_paper}.
To accomplish this, the authors developed a novel calibration method to determine both intrinsic and extrinsic fisheye-camera parameters. We leverage this study to calibrate our {\it single} fisheye camera and we use a geometric model developed therein.

%One study that we utilized for 3D location estimation of objects under fisheye camera is the ``Inverse Forward Mapping”  discussed by Bone \textit{et al.}~\cite{geo_paper}. This work proposes to calibrate cameras with stereo-based approach (which is a one-time process)\zlrep{, h}{. H}owever, for the ``inverse forward mapping", where it maps a 2D pixel location to 3D-world coordinates, it utilizes only single camera. Given the height of an object, this model could provide the 3D-world coordinate of the object. In this work we set the height of an object as an adjustable parameter, and accommodate the geometric model as one of our approaches in estimating distances of objects under one fisheye camera and evaluate social distance violation based on the estimated distances.

%Compared to traditional cameras, fisheye cameras have a much wider field of view, thus, a single camera can monitor a large room. However, such cameras introduce geometric distortions that must be accounted for when estimating distances between people in 3D space. Currently most of the studies on distance estimation using fisheye cameras are utilizing stereo \cite{equirect_images, disparity_offset}. One study that we utilized for 3D location estimation of objects under fisheye camera is the ``Inverse Forward Mapping”  discussed by Bone \textit{et al.}~\cite{geo_paper}. This work proposes to calibrate cameras with stereo-based approach (which is a one-time process)\zlrep{, h}{. H}owever, for the ``inverse forward mapping", where it maps a 2D pixel location to 3D-world coordinates, it utilizes only single camera. Given the height of an object, this model could provide the 3D-world coordinate of the object. In this work we set the height of an object as an adjustable parameter, and accommodate the geometric model as one of our approaches in estimating distances of objects under one fisheye camera and evaluate social distance violation based on the estimated distances.

In terms of benchmark datasets for estimating distances between people, Epfl-Mpv-VSD,  Epfl-Wildtrack-VSD, OxTown-VSD \cite{Pose_estimation} and KORTE \cite{KORTE_dataset} are prime examples. Out of them only Epfl-Mpv-VSD and KORTE include some indoor scenes. More importantly, however, all of them have been collected with rectilinear-lens cameras, and are not useful for our study. In fact, the first three datasets have been derived by Aghaei \textit{et al.}~\cite{Pose_estimation} from Epfl-Mpv\cite{epfl_mpv}, Epfl-Wildtrack \cite{epfl_wildtrack} and OxTown \cite{Oxtown} datasets that are designed for people detection and tracking. In contrast, our datasets have been specifically designed for the estimation of distances between people in an indoor, large-space setting under a variety of occlusion scenarios.

%In other words, when Epfl-Mpv\cite{epfl_mpv}, Epfl-Wildtrack \cite{epfl_wildtrack} and OxTown \cite{Oxtown} were published their main use was not for distance estimation, however, these datasets provide the 3D world coordinates of each person, thus they have been exploited as distance estimation dataset in \cite{Pose_estimation}.

%In fact, Epfl-Mpv-VSD, Epfl-Wildtrack-VSD and OxTown-VSD are derived by Aghaei \textit{et al.}~\cite{Pose_estimation} by using Epfl-Mpv\cite{epfl_mpv}, Epfl-Wildtrack \cite{epfl_wildtrack} and OxTown \cite{Oxtown}. In other words, when Epfl-Mpv\cite{epfl_mpv}, Epfl-Wildtrack \cite{epfl_wildtrack} and OxTown \cite{Oxtown} were published their main use was not for distance estimation, however, these datasets provide the 3D world coordinates of each person, thus they have been exploited as distance estimation dataset in \cite{Pose_estimation}.

%\jlk{I do not think it is important how these datasets were obtained.}
%\mc{I tried to point out that our dataset is specifically designed for social distancing, however, those datasets are actually meant for people detection/tracking. But I'm fine with cutting it off.}


\section{Methodology} \label{sec:methodology}

%To detect social distance violations, first, we estimate the distance between people using an overhead fisheye camera. Then, we check whether the distance between people is less than 6 feet (which is the universal social distance) or not. For estimating the distance in between two people, we take the center pixel of these two people as input and refer to these two people as person A and person B. The center of a person is defined as the center of the bounding box that tightly surrounds the person in the image. The pixel coordinates of the centers for person A and person B are denoted $\bm{x}_A, \bm{x}_B \in \mathbb{Z}^2$, respectively. In this section, we introduce two approaches to estimate the distance between person A and person B, given $\bm{x}_A$ and $\bm{x}_B$. These approaches are namely, Geometry-based Approach and Deep Learning-based Approach. 

We focus on large indoor spaces monitored by a single, overhead, fisheye camera. An example of an image captured in this scenario is shown in Fig.~\ref{fig:classroom_view}. We propose two methods to measure the distance between two people visible in such an image. One method uses a geometric model of a previously calibrated camera while the other makes no assumptions about the camera and is data-driven. Although these methods are well-known, we apply them in a unique way to address the distance estimation problem using a {\it single} fisheye camera.

In this work, we are not concerned with the {\it detection} of people; this can be accomplished by any recent method developed for overhead fisheye cameras \cite{Tamura_et_al, Li_et_al, RAPiD}. Therefore, we assume that tight bounding boxes around people are given. Furthermore, we assume that the center of a bounding box defines the location of the detected person.

Let $\bm{x}_A, \bm{x}_B \in \mathbb{Z}^2$ be the pixel coordinates of bounding-box centers for person $A$ and person $B$, respectively. Given a pair $(\bm{x}_A,\bm{x}_B)$, the task is to estimate the 3D-world distance between people captured by the respective bounding boxes. Below, we describe two methods to accomplish this.

\begin{figure}[!t]
  \centering
  \vglue 0cm
  \includegraphics[width=0.45\textwidth]{images/height_adaptation_sketch.png}
  \caption{Field of view from an Axis M3057-PLVE camera mounted on the ceiling of a 72$\times$28 ft$^2$ classroom and illustration of height adjustment (see Section~\ref{ssec:heightadj} for details).}
  \label{fig:classroom_view}
  \vglue -0.5cm
\end{figure}

\subsection{Geometry-based method}

%In this approach, we adapted the ``Inverse Forward Mapping" proposed by Bone \textit{et al.}~ \cite{geo_paper} to estimate the distance between person A and person B. In Bone \textit{et al.}~ \cite{geo_paper}, inverse forward mapping, $G: \mathbb{Z}^2  \mapsto \mathbb{R}^3 $ , is used for mapping a 2D pixel location to a 3D-world coordinate. It is defined as follows,

In this approach, to estimate the 3D-world distance between two people we adopt the {\it unified spherical model} (USM) proposed by Gayer and Danilidis \cite{Geyer01} for fisheye cameras and a calibration methodology to find this model's parameters developed by Bone \textit{et al.} \cite{geo_paper}. This model, derived in \cite{geo_paper}, enables computation of an inverse mapping from image coordinates to 3D space as described next.

%For a fisheye-image point at $\bm{x}$, its corresponding 3D-world coordinates $\bm{P}=[P_x,P_y,P_z]^T\in \mathbb{R}^3$ \zlrep{}{with respect to fisheye camera}can be computed \mcrep{ as follows,}{. $P_z$ is defined as follows (see Fig. \ref{fig:p_z_illustration})

Consider the scenario in Fig.~\ref{fig:p_z_illustration} where the center of the 3D-world coordinate system is at the optical center of a fisheye camera mounted overhead at height $B$ above the floor and a person of height $H$ stands on the floor. Let a 3D-world point $\bm{P}=[P_x,P_y,P_z]^T\in \mathbb{R}^3$ be located on this person's body at half-height and let $\bm{P}$ 
%\pirevised{be projected onto a fisheye image at 2D location $\bm{x}$.}{}
appear at 2D coordinates $\bm{x}$ in the fisheye image.

%\pirevised{The inverse mapping derived by Bone {\it et al.} \cite{geo_paper} is defined as follows:}{}
Bone {\it et al.} \cite{geo_paper} showed that the 3D-world coordinates $\bm{P}$ can be recovered from $\bm{x}$ with knowledge of $P_z$ and a 5-vector of USM parameters ${\bm{\omega}}$ via a non-linear function $G$:
%
\begin{equation} \label{eqn:G}
  \bm{P} = G(\bm{x},P_z; {\bm{\omega}}),
\end{equation}
%
%\pirevised{where ${\bm{\omega}}$ is a 5-vector of USM parameters characterizing the non-linear function $G$.}{}
%
In order to estimate ${\bm{\omega}}$, an automatic calibration method using a moving LED light was developed in \cite{geo_paper}.
In addition to $\bm{\omega}$, the value of $P_z$ is needed since this is a 2D-to-3D mapping. However, based on Fig.~\ref{fig:p_z_illustration} we see that $P_z=B-H/2$.
%
\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.45\textwidth]{images/p_z_illustration.png}
  \caption{Illustration of the relationship between $P_z$ and person's height $H$. \label{fig:p_z_illustration}}
\end{figure}

%\jlk{I think we need a figure here to illustrate $P_z$, for example one-half of Fig.~1 from Bone et al. with $\bm{P}$ shown in the middle of a person standing under the camera and $P_x,P_y,P_z$ clearly shown. We could use $H$ for a person's height and $B$=100in for camera-installation height. Then, $P_z=B-H/2$. The small $h$ would continue to be BB height.} \mc{I prepared the illustration and referred to in this section.}

In practice, we can only get a pixel-quantized estimate $\widehat{\bm{x}}$ of $\bm{x}$ from which we can compute an estimate $\widehat{\bm{P}}$ of $\bm{P}$ using (\ref{eqn:G}). Let $\widehat{\bm{P}}_A$ and $\widehat{\bm{P}}_B$ denote the estimated 3D-world coordinates of person $A$ and person $B$, respectively, based on the centers of their bounding boxes $\widehat{\bm{x}}_A$ and $\widehat{\bm{x}}_B$. Then, we can estimate the 3D-world Euclidean distance $\widehat{d}_{AB}$ between them via:
%
%\pirevised{Therefore, if we know a person's height (more on this later), we can use mapping (\ref{eqn:G}) to find 3D-world coordinates $\bm{P}_A$ and $\bm{P}_B$ for person $A$ and person $B$ whose bounding-box centers are at $\bm{x}_A$ and $\bm{x}_B$, respectively. We can estimate the distance between person $A$ and person $B$ by computing the Euclidean distance between 3D points $\bm{P}_A$ and $\bm{P}_B$:}{}
%By using inverse forward mapping, we map pixel coordinates of person A and person B, $\bm{x}_A$ and $\bm{x}_B$, to their 3D-world coordinates $\bm{P}_A$ and $\bm{P}_B$. We estimate the distance between person A and person B, by computing the Euclidean distance between $\bm{P}_A$ and $\bm{P}_B$,
%
\begin{equation} \label{eqn:geo_based_final_estimation}
  \widehat{d}_{AB} = ||\widehat{\bm{P}}_A-\widehat{\bm{P}}_B||_2.
\end{equation}
%\picomment{Slight issue with notation and terminology: in Eq.(2) we use the exact 3D coordinates. Then why is it called an estimate? I think since $\bm{x}_A$ and $\bm{x}_B$ are estimates (actually they should be $\widehat{\bm{x}}_A, \widehat{\bm{x}}_B$), the 3D coordinates derived via Eq.(1) are only estimates of the true 3D coordinates and should be called $\widehat{\bm{P}}_A, \widehat{\bm{P}}_B$. But we are conflating the true and estimated 3D coordinates in Eq.(2) and in the text before it.}

%\subsection{Deep Learning-based Approach}
\subsection{Neural-network approach}

In this approach, we train a neural network to estimate the distance between person $A$ and person $B$. Since the distance between two points in a fisheye image is invariant to rotation, we pre-process locations $\bm{x}_A$ and $\bm{x}_B$ before feeding them into the network. First, we convert $\bm{x}_A$ and $\bm{x}_B$ to polar coordinates: $\bm{x}_A \rightarrow(r_A,\theta_A)$ and $\bm{x}_B\rightarrow (r_B,\theta_B)$, where $r_\cdot$ denotes radius and $\theta_\cdot$ denotes angle. Then, we compute the angle between normalized locations as follows:
%
\begin{equation}
  \theta := (\theta_A - \theta_B) \textrm{ mod } \pi.
\end{equation}
%
Note that by its definition, $0\le\theta\le\pi$. Finally, we form a feature vector associated with locations $\bm{x}_A$ and $\bm{x}_B$ as follows: $\bm{V} = [r_{A},r_{B},\theta]^T$. 
%
We chose a regression Multi-Layer Perceptron (MLP) to estimate the 3D-world distance between people (in lieu of a CNN) since the input vector is a 3-vector with no required ordering of coordinates for which convolution would be beneficial. 
%
%\picomment{Reasoning is unclear to me: the phrasing suggest there is something special about 3-vectors which makes the MLP choice natural. Why? Also, selection of MLP in lieu of what other method?}
%
We collected a training set of images, where for each vector $\bm{V}$ we know the ground-truth distance $d_{AB}$, and trained the MLP, $F: \mathbb{R}^3  \mapsto \mathbb{R} $, as a regression model that performs the following mapping:
%
\begin{equation} \label{eqn:F}
  \widehat{d}_{AB} = F(\bm{V}).
\end{equation}
%
In training, we used the mean squared-error (MSE) loss:
%
\begin{equation} \label{eqn:loss_function}
  \mathcal{L} = \frac{1}{M} \sum_{i=1}^{M} || \widehat{d}_{AB_{i}} - d_{AB_{i}} ||^2,
\end{equation}
%
where $M$ is the batch size.


%\subsection{Height Adaptation Trick for Inference}
\subsection{Person's height adjustment}
\label{ssec:heightadj}

%Geometry-based approach can be tuned for a specific person height through $P_z$. However, deep learning-based model itself has no such adaptability for different heights. In our case, deep learning-based approach is trained with point pairs at table height (32.5 inches), more information on training data can be found in section \ref{sec:training data info}. However, people's centers are mostly above the table height. To compensate for this height mismatch between training data and testing data, we propose a height adaptation trick for inference.

While the geometry-based approach can be tuned for specific height of a person through $P_z$ (\ref{eqn:G}), the neural-network approach would require %multiple networks, each tuned to a different height of a person. 
%
%This would require multiple 
a training dataset having a large number of annotated examples at multiple heights.
%an impractical proposition. 
%
%\picomment{Difficulty stated in last 2 sentences seems artificial: Why can't we train a single MLP with height as a fourth input?}
%
Since this is extremely labor intensive, we train the MLP at a single height of 32.5 inches (details in Section~\ref{sec:training data info}) which corresponds to one-half of $H =$ 65 in, an average person's height. Recall that we assume a person's location in the image is the center-point of the person's bounding box.
%(see second paragraph of Section~\ref{sec:methodology}). 
%
%\picomment{I cannot find where the relationship between the bounding box center and the person's location has been explicitly stated previously.}\mc{In the second and third paragraph of methodology section we introduce this relationship.}
%
Therefore, for a standing, fully-visible 65-inch person this center-point matches the 32.5-inch training height well. However, there would be a mismatch for people of other heights or when a person is partially occluded, for example by a table. In the latter case, the detected bounding box would be above the table and so would be the bounding-box center. To compensate for this height mismatch between the training and testing data, we %\pirevised{propose an adjustment}{}
propose a test-time adjustment in the MLP approach.

This height adjustment can be thought of as lowering the center-point of a person in terms of pixel coordinates. An illustration of this idea is shown in Fig.~\ref{fig:classroom_view}
%Each person's bounding box is represented by a vector $[w, h, c_x, c_y, \beta]$,
%\zl{the vector in our provided dataset is sequenced as $[c_x, c_y, w, h, \beta]$, do we need to maintain consistency in sequence between dataset and paper?}
%\jlk{No, the paper does not have to use the same notation as the code. This is fine.}
%where $w$ is the width, $h$ is the height, $c_x$ is the $x$-coordinate of the center, $c_y$ is the $y$-coordinate of the center and $\beta$ is the angle of the bounding box. 
%
%\picomment{There seems to be an unstated, but implicit, assumption that the people-detection algorithms for overhead fisheye cameras will produce bounding boxes with major axes that are purely radial. The RAPID algorithm, for example, does not guarantee this, right?}
%
%In Fig.~\ref{fig:classroom_view},
where the red point represents the center of the red bounding box and $h$ its height. In the process of height adjustment during test time, we move the {\it actual} center (red point) of the bounding box along the box's axis pointing to the center of the image (white-dashed line) to produce an {\it adjusted} center (green point).
%\jlk{Is the movement along the dashed line or along the box's longer axis? In this case they coincide.}
%\zl{It is along the box's longer axis. Here we did not mention what white dashed line actually is. I agree that we should put this point out}
This displacement is defined as $\alpha \times \frac{h}{2}$ and we consider a range of values for $\alpha$ (see Fig.~\ref{fig:height_adaptation_plot}).
%where $\alpha_{min} < \alpha < \alpha_{max}, \alpha \in \mathbb{R}$.
%\zl{do we need to specify what $\alpha_{min}$ and $\alpha_{max}$ is?}
%\jlk{I think we should specify these values in the captions of the plots.}
A value of $\alpha > 0$ corresponds to moving the bounding-box center towards the image center, i.e., we reduce the height of a detected person.
%
%\picomment{The values of $\alpha_{max}$ and $\alpha_{min}$ are not stated here. But is it explained later how to set them? Need to check.}
%
\begin{figure*}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{images/training_data_with_mid_v2.png}
  \leftline{\small\hskip 0.85cm (a) Layout of chessboard mats  \hskip 1.0cm (b) Chessboard mat at position \#4\hskip 1.5cm (c) Chessboard mat at position \#13}
  \caption{Illustration of chessboard-mat layout used for training the MLP model. 
  %
%  \jlk{Since the location \#18 is directly under camera 1 and the chessboard in the image from subfigure (b) is roughly centered vertically, the orange square \#18 should be roughly aligned with the black square \#4, rather than being vertically between squares \#4 and \#5. I would also check camera 2 (location \#17) and camera 3 (location \#19) and revise the figure.}
  %
%  \zl{from camera 2 and camera 3 view, it seems that location \#17 is right aligned in the middle of \#6 and \#7, and \#19 is right between \#2 and \#3. Also from camera 3, \#3 is still lower than \#18, so \#18 should be lower, but not lower than \#4} 
  %
%  \mc{I updated the image}
  %
  \label{fig:training_data}}
\end{figure*}


\section{Datasets} \label{sec:dataset info}

We introduce a unique dataset, {\it Distance Estimation between People from Overhead Fisheye cameras} (DEPOF) \footnote{\href{https:\\vip.bu.edu/depof}{vip.bu.edu/depof}} which was collected with Axis M3057-PLVE cameras at 2,048$\times$2,048-pixel resolution.

\subsection{Training dataset} \label{sec:training data info}

%In our training data for the deep learning-based approach, we have the ground truth distance between person A and person B, $d_{AB}$, for each $\bm{x}_A,\bm{x}_B$ pair. To collect such data, we utilized a 9ft $\times$ 9ft chessboard mat. We moved the chessboard mat at table height (32.5 inches) by following the pattern that is shown in Figure \ref{fig:training_data}. From chessboards \#1 to \#8, there is no vertical gap and they are all horizontally aligned. Same applies to chessboards \#9 to \#16. The chessboards in columns from \#1 to \#8 and \#9 to \#16 are vertically aligned. In other words, all the chessboards that are in black color in the layout are aligned horizontally and vertically with respect to each other. However, chessboards \#17 to \#19 (i.e. orange chessboard) are not aligned with any other chessboards. So, for chessboards \#17 to \#19 we can only compute distance within a chessboard.

In order to train the MLP, we need ground-truth distance data. We placed a 9 ft $\times$ 9 ft chessboard mat on classroom tables of equal height (32.5 inches) in 19 different locations as shown in Fig.~\ref{fig:training_data}. The mat at location \#2 was carefully placed with sides parallel to those of the mat at location \#1 and forming a contiguous extension of mat \#1 (as if two mats were placed abutting each other). Similarly, the mat at location \#3 was placed contiguously with respect to the mat at location \#2 and so on until location \#8. The same procedure was repeated for locations \#9-\#16 with the mat at location \#9 carefully aligned with the mat at location \#1 and the distance between them carefully measured (121.5 inches). 
%
In order to provide ground-truth data in the center of camera's field of view, the mat was also placed directly under each of 3 cameras (locations \#17, \#18, \#19 shown in orange) with no alignment to mats at other locations.
%
\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.35\textwidth]{images/Real_people_position_v1.png}
  \caption{Spatial layout of locations in testing datasets. \label{fig:test_layout}}
  \vglue -0.4cm
\end{figure}

All the black/white corners of chessboard images were annotated, resulting in numerous $(\bm{x}_A,\bm{x}_B)$ pairs. Since we know that each square is of size 12.5 inches and the neighboring chessboards are abutting and aligned, %\pirevised{computing}{}
we could accurately compute the 3D distances between physical-mat points corresponding to $\bm{x}_A$ and $\bm{x}_B$ 
%\pirevised{was trivial}{}. 
The overall process can be thought of as creating a virtual grid with 12.5-inch spacing placed 32.5 inches above the floor of the classroom.

%The length of a single square on the chessboard is 12.5 inches and the gap between the  columns \#1 to \#8 and \#9 to \#16 is 121.5 inches.

%\zl{At the same time, due to low resolution problem, the rows on chessboard mat 8 and 16 are not clear enough to annotate every row. And thus, the rows on chessboard mat 8 and 16 are annotated 2 row by 2 row, which means the gaps between rows on these 2 mats are 2*12.5 = 25 inches}\mc{This is not a very important detail and it might confusing for the reader. So, let's skip it for now. We can mention all these in the website when we publish the paper. I will leave this discussion here, for Professors to take a look.}


\subsection{Testing datasets}

In order to test both of the proposed approaches, we collected a dataset with people in a 72 ft $\times$ 28 ft classroom. First, we marked locations on the floor where individuals would stand  (Fig.~\ref{fig:test_layout}). We measured distances between all locations marked by a letter (green disk) 
%
which gives us ${10\choose 2} = 45$ distances which turned out to be distinct.
%
%\pirevised{In terms of}{}
For locations marked by a number (yellow squares), we measured the distances along the dashed lines (20 distinct distances).
%
Using this spatial layout, we collected and annotated two sets of data. 

\begin{itemize}\itemsep 0em
\item{\bf Fixed-height dataset:} One person of height $H =$ 70.08 inches moves from one marked location to another and an image is captured at each location. This allows us to evaluate our algorithms on people of the same {\it known} height.

%This allows us to measure the distance between same-height people by using images captured at different times. We use this dataset to evaluate our algorithms when a person's height is known. 

\item{\bf Varying-height dataset:} Several people of different heights stand at different locations in various permutations to capture multiple heights at each location. We use this dataset to evaluate sensitivity of our algorithms to a person's height variations.
\end{itemize}

In addition to the 65 distances ($45+20$), we performed $8$ additional measurements for the fixed-height dataset and $2$ additional measurements for the varying-height dataset.

Depending on
%
%\pirevised{location of a person}{}
their location with respect to the camera, 
%\pirevised{this person}{}
a person may be fully visible or partially occluded (e.g., by a table or chair). 
%\pirevised{We group}{}
In order to understand the impact of occlusions on distance estimation, we grouped all the pairs in both testing datasets into 4 categories as follows:
%
%\begin{itemize}
%  \item{\bf No Occlusion-No Occlusion (N-N):} Both people in the pair are not occluded in the frame.
%  \item{\bf No Occlusion - Occlusion (N-O):} One person is occluded and the other person is not occluded.
%  \item{\bf Occlusion - Occlusion (O-O):} Both people in the pair are occluded in the frame.
%  \item{\bf All:} All pairs, regardless of their occlusion status.
%\end{itemize}
%
\begin{itemize}\itemsep 0em
  \item Visible-Visible (V-V): Both people are fully visible.
  \item Visible-Occluded (V-O): One person is visible while the other one is partially occluded.
  \item Occluded-Occluded (O-O): Both people are partially occluded.
  \item All: All pairs, regardless of the occlusion status.
\end{itemize}

Table~\ref{tab:Dataset_stats} shows various statistics for both datasets: the number of pairs in each category, the number of distances measured and their range as well as the number of pairs with distance in three ranges: 0ft--6ft, 6ft--12ft and $>$12ft.

%\picomment{Why is number of all pairs equal to number distances in fixed-height dataset but not in the varying-height dataset? This is not clear to me.} \mc{In varying-height dataset, some distances are captured multiple times with different people pairs. In fixed-height dataset, every distance is captured once because we are just using a single person.}

%Fixed-height dataset covers 73 different distances and Varing-height dataset covers 67 different distances. The furthest distance measured is from letter "A" to letter "J" which is 701.96 inches. The shortest distance we measured is from G to 11 which is 11.63 inches.
% \mc{Zhangchi, for varying height dataset, did you manually label the people or did you use RAPiD?} \zl{I used RAPiD}

\begin{table}[htt]
\centering
%\caption{Number of pairs with respect to the occlusion status. N-N: No Occlusion-No Occlusion, N-O: No Occlusion - Occlusion, O-O: Occlusion - Occlusion, All: All \label{tab:Dataset_stats}}
\caption{Statistics of the testing datasets. \label{tab:Dataset_stats}}
%\small
\begin{tabular}{|c|c|c|}
\hline
 & Fixed- & Varying-\\
 & height & height\\
 & dataset & dataset\\
%\hline
%Number of V-V pairs & 35 & \zlrep{116}{100} \\
%\hline
%Number of V-O pairs & 32 & \zlrep{121}{126}\\
%\hline
%Number of O-O pairs & 6 & \zlrep{19}{30}\\
\hline
Number of V-V pairs & 35 & 100 \\
%\hline
Number of V-O pairs & 32 & 126\\
%\hline
Number of O-O pairs & 6 & 30\\
%\hline
Number of All pairs & 73 & 256 \\
\hline
Number of distances & 73 & 67\\
\hline
Smallest distance (G to 11) & \multicolumn{2}{c|}{11.63 inches}\\
\hline
Largest distance (A to J) & \multicolumn{2}{c|}{701.96 inches}\\
\hline
Number of pairs: 0 ft to 6 ft & 25 & 45\\
Number of pairs: 6 ft to 12 ft & 15 & 73\\
Number of pairs: above 12 ft & 33 & 138\\
\hline
\end{tabular}
\end{table}

In both datasets, we found bounding boxes of people using a state-of-the-art people-detection algorithm for overhead fisheye images \cite{RAPiD} from which we computed their locations (bounding-box centers). To measure the real-world distances between people, we used a laser tape measure.

%For annotating the people in the images, we used a people detection algorithm that is designed for fisheye cameras\cite{RAPiD}. The output of the people detection algorithm gave us bounding boxes for each person. For our application, we extracted the centers of these bounding boxes. To measure the distance in between people in real-world, we used a electronic measuring device that sent out a laser beam from one person to another.


% \begin{table*}[t]
% \centering
% \begin{tabular}{|c|cccc|cccc|}

% \hline
% %&
% %\multicolumn{8}{c|}{Fixed Height} \\
% %\hline
% &
% \multicolumn{4}{c|}{MAE} &
% \multicolumn{4}{c|}{Relative MAE} \\
% \hline
% &  V-V & V-O & O-O & All & V-V & V-O & O-O & All \\
% \hline
% Geometry-based & 12.56 & 36.93 & 41.83 & 26.41 & 0.16 & 0.40 & 0.24 & 0.27\\
% \hline
% Neural network & 19.88 & 48.37 & 60.67 & 37.14 & 0.23  & 0.47 & 0.28 & 0.34\\
% \hline
% \end{tabular}
% \caption{Inter-people distance estimation results for the fixed-height dataset (see the text for abbreviations).}
% \label{tab:Fixed Height Distance_Estimation_Result_table}
% \end{table*}

% \begin{table*}[t]
% \centering
% \begin{tabular}{|c|cccc|cccc|}

% \hline
% %&
% %\multicolumn{8}{c|}{Varying Height} \\
% %\hline
% &
% \multicolumn{4}{c|}{MAE} &
% \multicolumn{4}{c|}{Relative MAE} \\
% \hline
% &  V-V & V-O & O-O & All & V-V & V-O & O-O & All \\
% \hline
% Geometry-based & 31.73 & 48.75 & 59.02 & 41.80 & 0.27 & 0.38 & 0.25 & 0.32 \\
% \hline
% Neural network & 35.32 & 57.11 & 71.95 & 48.34 & 0.29 & 0.42 & 0.27 & 0.35\\
% \hline
% \end{tabular}
% \caption{Inter-people distance estimation results for the varying-height dataset (see the text for abbreviations).}
% %\caption{Results for distance estimation on varying height dataset. N-N: No Occlusion-No Occlusion, N-O: No Occlusion - Occlusion, O-O: Occlusion - Occlusion, All: All}
% \label{tab:Varying Height Distance_Estimation_Result_table}
% \end{table*}


\section{Experimental Results} \label{sec:Results}

In this section, we evaluate both of the proposed methods. First, we describe the experimental setup. Then, we compare the methods in terms of distance estimation accuracy and evaluate the impact of person's height adjustment on performance. Finally, we assess the ability of both methods to detect social-distancing violations.

%In section \ref{sec:Distance Estimation Evaluation} we show the results on how accurate the proposed approaches estimated the distance between people. In \ref{sec: Evaluation of Social Distancing Violation Detection}, we report the performance of the two approaches on social distance violation detection.

%In section \ref{sec:Distance Estimation Evaluation} and \ref{sec: Evaluation of Social Distancing Violation Detection}, we did not use height adaptation trick for inference. In other words, we used a fixed height for $P_z$ and used the deep learning-based model with no height adaptation. We demonstrate the impact of height adaptation trick in section \ref{sec: Impact of height adaptation trick}.


\subsection{Experimental setup}

In the  geometry-based approach, to learn parameters ${\bm{\omega}}$ of the inverse mapping $G$ (\ref{eqn:G}) we used the method described by Bone \textit{et al.}~\cite{geo_paper}. This method requires the use of 2 fisheye cameras, but is largely automatic and has to be applied only once for a given camera type (model and manufacturer). 
%
In the experiments, we used one camera at a time (3 cameras are installed in the test classroom -- locations \#17-\#19 in Fig.~\ref{fig:training_data}) and report the results only for the center camera due to space constraints. Results for other cameras are similar.

%
%\picomment{Last sentence is confusing: if we are reporting results only for the center camera, then why even state ``...we used only one camera at a time''? What is the need for the non-center cameras? This point is not clear to me.}

%We used $H=\text{35.04}$in, the exact height of the person who volunteered for the fixed-height dataset acquisition.

%For the results in Table \ref{tab:Fixed Height Distance_Estimation_Result_table} and Table \ref{tab:Varying Height Distance_Estimation_Result_table}, in the geometry-based algorithm, we used $P_z$=35.04 inches, which is equal to the  half of the height of the person that volunteered for the fixed height dataset.

In the neural-network approach, we used an MLP with 4 hidden layers and 100 nodes per layer. In training, we used MSE loss (\ref{eqn:loss_function}) and Adam optimizer with 0.001 learning rate.

\subsection{Distance estimation evaluation} \label{sec:Distance Estimation Evaluation}

In Tables \ref{tab:Fixed Height Distance_Estimation_Result_table} and \ref{tab:Varying Height Distance_Estimation_Result_table}, we compare the performance of both methods when estimating the distance between people on the fixed-height and varying-height datasets, respectively. We report the mean absolute error (MAE) between the estimated and ground-truth distances:
%
\begin{equation} \label{eqn:MAE}
  \textrm{MAE} = \frac{1}{N} \sum_{i=1}^{N}|\widehat{d}_{AB_{i}} - d_{AB_{i}}|
\end{equation}
%
where $N$ is the number of pairs in the dataset while $\widehat{d}_{AB_i}$ and $d_{AB_i}$ are the estimated and ground-truth distances for the $i$-th pair $AB$, respectively.
% %
% \begin{equation} \label{eqn:relative_MAE}
%   \textrm{Relative MAE} = \frac{1}{N} \sum_{i=1}^{N} \frac{|\widehat{d}_{AB_{i}} - d_{AB_{i}}|}{d_{AB_{i}}}
% \end{equation}

It is clear from Table~\ref{tab:Fixed Height Distance_Estimation_Result_table} that the geometry-based approach using $H/2 =$ 35.04 inches (to compute $P_z$) consistently outperforms the same approach using $H/2 =$ 32.5 inches, which, in turn, significantly outperforms the neural-network approach trained on chess mats placed at the height of 32.5 inches. While it is not surprising that knowing a test-person's height of $H =$ 70.08 inches improves geometry-based method's accuracy, it is interesting that even assuming $H/2 =$ 32.5 inches the geometry-based approach significantly outperforms the MLP optimized for a fixed height of 32.5 inches during training.

%It is clear from Table~\ref{tab:Fixed Height Distance_Estimation_Result_table} that the geometry-based approach using $H/2 =$ 35.04 in (to compute $P_z$) consistently outperforms the neural-network approach trained on chess mats placed at the height of 32.5 in with a significant margin. This was to be expected since in the fixed-height dataset a single person of height $H$=70.08in appears at all locations. The geometry-based approach leverages this information, whereas the neural-network approach has no such option as it was optimized for a fixed height of 32.5in during training. However, for $H/2$=32.5in the geometry-based approach outperforms the neural-network approach by a \mcrep{much}{} smaller margin \mcrep{}{compared to $H/2$=35.04in}.

%\begin{table}[!htb]
%\caption{Mean-absolute distance error between two people for the fixed-height dataset (see the text for abbreviations).}
%\label{tab:Fixed Height Distance_Estimation_Result_table}
%\centering
%\begin{tabular}{|c|cccc|}
%\hline
%&
%\multicolumn{8}{c|}{Fixed Height} \\
%\hline
%&
%\multicolumn{4}{c|}{MAE [in]} \\
%\hline
%&  V-V & V-O & O-O & All \\
%\hline
%Geometry-based & 9.85 & 31.69 & 32.30 & 21.27 \\
%($H/2$=35.04in) & & & &\\
%\hline
%Geometry-based & 12.20 & 39.90 & 42.64 & 26.84 \\
%($H/2$=32.5in) & & & &\\
%\hline
%Neural network & \zlrep{19.88}{17.72} & \zlrep{48.37}{48.84} & \zlrep{60.67}{56.58} & \zlrep{37.14}{34.56}\\
%(trained on 32.5in) & & & &\\
%\hline
%\end{tabular}
%\end{table}

\begin{table}[!htb]
\caption{Mean-absolute distance error between two people for the fixed-height dataset.}
\label{tab:Fixed Height Distance_Estimation_Result_table}
\centering
\begin{tabular}{|c|cccc|}
\hline
%&
%\multicolumn{8}{c|}{Fixed Height} \\
%\hline
&
\multicolumn{4}{c|}{MAE [in]} \\
\hline
&  V-V & V-O & O-O & All \\
\hline
Geometry-based & 9.85 & 31.69 & 32.30 & 21.27 \\
($H/2 =$ 35.04 in) & & & &\\
\hline
Geometry-based & 12.20 & 39.90 & 42.64 & 26.84 \\
($H/2 =$ 32.5 in) & & & &\\
\hline
Neural network & 17.72 & 48.84 & 56.58 & 34.56\\
(trained on 32.5 in) & & & &\\
\hline
\end{tabular}
\end{table}

%\jlkrep{}{When tested on the varying-height dataset (Table~\ref{tab:Varying Height Distance_Estimation_Result_table}), the geometry-based approach using $H/2 = $ 35.04 in outperforms the neural-network approach for ``All'' pairs by a margin of 12.81 in, a similar margin to the one in the fixed-height dataset. However, for $H/2 = $ 32.5 in the geometry-based approach outperforms the neural-network approach by 4.45 in only.}

%\jlk{Mertcan and Zhangchi, in the commented-out text above (not visible in PDF) the stated error differences do not agree with Table 3. For example, for 32.5in MLP does not outperform geometric. Are the values in Table 3 correct?}
%\mc{Zhangchi, I have asked you earlier that please make sure the text agrees with the tables. Please take a look at it, as soon as you have a chance. Since, there has been so many updates to the table, I think you would be the best person to make this change.}\zl{Sorry that I just realize that the original illustration is "outperformed by", not "outperforms". I have changed "is outperformed by" above to "outperforms". I checked the numbers and values I commented previously and they are correct.}
%\jlk{Where is the 15.32in from? In my math: 45.43in - 32.62in = 12.81in.}

Similar performance trends can be observed in Table~\ref{tab:Varying Height Distance_Estimation_Result_table} for the varying-height dataset but with larger distance-error values than in Table~\ref{tab:Fixed Height Distance_Estimation_Result_table}. This is due to the fact that in the varying-height dataset people have different heights, so a selected parameter $H$ in the geometry-based algorithm or a training height in the neural-network algorithm cannot match all people's heights at the same time. 
%Had the people heights been known, the geometry-based algorithm could have used them directly unlike the neural-network algorithm which would have to be retrained for each height.
%\pirevised{}{require a large annotated dataset with multiple heights.}
%
%\picomment{Not if you train a single MLP with height as the fourth input and a single training dataset with varying, but known, heights.}

%\begin{table}[!htb]
%\caption{Mean-absolute distance error between two people for the varying-height dataset (see the text for abbreviations).}
%\label{tab:Varying Height Distance_Estimation_Result_table}
%\centering
%\begin{tabular}{|c|cccc|}
%\hline
%&
%\multicolumn{8}{c|}{Varying Height} \\
%\hline
%&
%\multicolumn{4}{c|}{MAE [in]} \\
%\hline
%&  V-V & V-O & O-O & All \\
%\hline
%Geometry-based & \zlrep{24.27}{14.70} & \zlrep{38.63}{41.37} & %\zlrep{45.29}{55.55} & 32.62\\
%($H/2$=35.04in) & & & &\\
%\hline
%Geometry-based & \zlrep{30.97}{20.18} & \zlrep{48.02}{51.14} & %\zlrep{57.18}{67.61} & 40.98\\
%($H/2$=32.5in) & & & &\\
%\hline
%Neural network & \zlrep{35.32}{24.64} & \zlrep{57.11}{55.88} & %\zlrep{71.95}{70.87} & \zlrep{48.34}{45.43}\\
%(trained on 32.5in) & & & &\\
%\hline
%\end{tabular}
%\end{table}

\begin{table}[!htb]
\caption{Mean-absolute distance error between two people for the varying-height dataset.}
\label{tab:Varying Height Distance_Estimation_Result_table}
\centering
\begin{tabular}{|c|cccc|}
\hline
&
%\multicolumn{8}{c|}{Varying Height} \\
%\hline
%&
\multicolumn{4}{c|}{MAE [in]} \\
\hline
&  V-V & V-O & O-O & All \\
\hline
Geometry-based & 14.70 & 41.37 & 55.55 & 32.62\\
($H/2 = $ 35.04 in) & & & &\\
\hline
Geometry-based & 20.18 & 51.14 & 67.61 & 40.98\\
($H/2 = $ 32.5 in) & & & &\\
\hline
Neural network & 24.64 & 55.88 & 70.87 & 45.43\\
(trained on 32.5 in) & & & &\\
\hline
\end{tabular}
\end{table}

%This variability of heights hurts the geometry-based approach more than it does the neural-network approach. Still, the geometry-based approach outperforms the neural-network approach because the training-data height of 32.5in is not matched well to the height of the people's centers.

Note that for two fully-visible people of the same and {\it known} height (Table~\ref{tab:Fixed Height Distance_Estimation_Result_table}), the geometry-based algorithm has an average distance error of less than 10 inches. This error grows to about 21 inches for all pairs (visible and occluded). For people of different and {\it unknown} heights (Table~\ref{tab:Varying Height Distance_Estimation_Result_table}), the average error for pairs of fully-visible individuals (for $H/2 =$ 35.04 inches) is slightly above 1 ft and for all pairs it is less than 3 ft. While these might seem to be fairly large distance errors, one has to note that the distances between people are as large as 58.5 ft (702 inches).
%
%\picomment{Yes, but then a reviewer would like to know the errors for different ranges of ground-truth distances. That would provide a more complete picture about where the large errors are coming from. If they are mainly from well-separated people then the point made in the last sentence is fine. Otherwise not.}

%\mcrep{}{For a fair comparison, we performed the testing for both algorithms on a CPU. Our choice of CPU was Intel(R) Xeon(R) CPU E5-2680 v4@2.40GHz. In terms of runtime, both algorithms take around 300$\mu s$ to map the pixel locations to a 3D world distance. Thus, both algorithms can be used for real-time applications. }

%\jlkrep{}{In terms of computational complexity, on an Intel(R) Xeon(R) CPU E5-2680 v4@2.40GHz both algorithms take around 0.3 ms to map two pixel locations to a 3D world distance. Clearly, both algorithms can support real-time applications.}

%\jlk{I do not understand the next paragraph from Zhangchi. Why are the times different than what Mertcan wrote above? Do we need this discussion?} \mc{My numbers are incorrect. Zhangchi was supposed to comment out my paragraph and then add the next paragraph. I think we should get rid of the paragraph above. The only thing that we should keep from the paragraph above is the hardware information}

In terms of the computational complexity, on an Intel(R) Xeon(R) CPU E5-2680 v4@2.40GHz both algorithms can easily support real time operation although the geometry-based algorithm is significantly faster. 
%\pirevised{Let's assume there are 100 locations in an image for which to compute 3D-world distances}{}
For example, suppose 3D-world distances are to be computed between all pairs of 100 image locations. The geometry-based algorithm can first map all pixel coordinates to 3D world coordinates (\ref{eqn:G}) and then compute the Euclidean distance for all ${100\choose 2}= $ 4,950 pairs. This, on average, takes 4 $\mu s$. The neural-network algorithm has to apply the MLP to all 4,950 pairs separately taking on average 949 $\mu s$.
%\jlk{Zhangchi: Are these times for calculating distances of all 4,950 pairs or for one pair only?}
%\zl{These are after picking 100 unique random points from chess mat dataset and then calculating all ${100\choose 2}= $ 4,950 pairs in these 100 points}
%\jlk{I am not sure how useful this paragraph is. If we are pushed for space we can shorten or remove it, I think.}
%
%\picomment{I agree with Prof. Konrad.}

%\zlrep{}{In terms of computational complexity, on an Intel(R) Xeon(R) CPU E5-2680 v4@2.40GHz, when choosing pairs from 100 random locations in MLP training dataset, the Geometry-based algorithm takes around 4.0 $\mu s$, while Neural network algorithm takes around 949$\mu s$. The Geometry-based algorithm out performs Neural network algorithm in runtime by a large margin, as for N pixel locations, Geometry-based algorithm could first conduct N 3D-world mapping, and then calculating Euclidean distance for each pair, while Neural network algorithm must conduct neural network model on all ${N\choose 2}$ pairs.}

%\zl{ Previous paragraph is commented out}

%\jlk{Should we give statistics like a histogram?} \mc{I think histograms would be useful, but would occupy some space. I would rather use that space for reporting some brief social distancing results. Not extensive results, but some brief results (e.g., just CCR and F1-Score for the best alpha values), so that we can put social distancing into the title, which would strengthen the paper in my opinion. }

%In a 2000 ft$^2$ room, where people are getting apart as much as 700 inches, without adjusting to the height of the people, both approaches achieve MAE less than 50 inches. In the next section, we will demonstrate the effect of adjusting the heights on the performance of our algorithms.

% One common trend in the performance of both algorithms on both datasets is that the relative MAE is the highest for V-O pairs, as seen in Tables~\ref{tab:Fixed Height Distance_Estimation_Result_table} and \ref{tab:Varying Height Distance_Estimation_Result_table}. More specifically, while the relative MAE for V-O pairs is in 0.38-0.47 range, it is in 0.16-0.29 range for V-V pairs and 0.24-0.28 range for O-O pairs. At the first glance this might seem counter-intuitive since one might expect that it should be the highest for O-O pairs. However, for a person partially-occluded by a table or chair (typical occlusions in our dataset) the detected bounding box has its center further away radially from image center than it would have been had the person been fully visible. \mcrep{}{An example of this can be seen in Fig. \ref{fig:center_shift_observation}} This radial shift suggests the person is further away from the camera than the person really is. \jlk{I think we need an illustration here, for example on a carefully selected frame. A picture is worth a thousand words :)}\mc{I prepared a figure and referenced it in this section.}  In an O-O pair, this radial shift of the bounding-box center occurs in the same direction for both people only modestly affecting the pixel distance between them and, therefore, modestly affecting the real-world distance. In a V-O pair, however, the radial shift applies only to the partially-occluded person thus more significantly affecting the pixel distance between people and leading to a larger real-world distance error.
% A similar trend can be observed for MAE between V-O and V-V pairs, but not between V-O and O-O pairs. This is due to the fact that O-O pairs appear at larger distances from the camera compared to V-O pairs (at larger distances the real-world distance errors are larger).

%\subsection{Evaluation of Social Distance Violation Detection}
% \subsection{Application to social distancing oversight}
% \label{sec: Evaluation of Social Distancing Violation Detection}

% %We formulate the social distance violation detection as a binary classification problem. People being closer to each other less than 6 feet is considered as ``positive" and people being more than 6 feet apart is considered as ``negative''. In Table \ref{tab:Social Distance Violation Results} we show the performance of both algorithms on social distance violation detection for "All" pairs.

% We apply the proposed approaches to the problem of visual detection of scenarios where a social-distancing constraint (typically, 6ft) is being violated. More formally, we treat the social-distance violation detection as a binary classification problem; people closer to each other than 6ft are considered to be a ``positive'' case (violation takes place) while people more than 6ft apart are considered to be a ``negative'' case (no violation). In Table \ref{tab:Social Distance Violation Results}, we show the performance of both algorithms applied in this context to ``All'' pairs category. We use the Correct Classification Rate (CCR), Precision, Recall and F1-Score as the performance measures.

% On the fixed-height dataset, the geometry-based approach outperformed the neural-network approach by 1.37 percentage-points in CCR. The 100\% Precision for both approaches means that none of them has  triggered a ``false-positive'', i.e., classified a distance of more than 6ft as less than 6ft. In terms of Recall, the geometry-based approach outperformed the neural-network approach by 4 percentage-points, that is it resulted in fewer false negatives (i.e., a distance of less than 6ft classified as more than 6ft). Consequently, the geometry-based approach resulted in a higher F1-Score since it is a harmonic mean of Precision and Recall.
% %geometry-based algorithm caused 7 false-negatives and deep learning-based algorithm caused 8 false negatives

% The results on the fixed-height dataset are not surprising since the geometric algorithm leverages person's height as side information whereas the neural-network approach is optimized for a 65-inch person. However, the performance of both algorithms is identical on the varying-height dataset. In this case, people of different heights appear at different locations, and neither approach uses precise height information. Overall the CCR of about a 90\% seems very good for a 2,000ft$^2$ space served by a single camera. The 100\% precision makes the algorithm non-intrusive in the sense that a false social-distance violation never occurs. However, the 40\% Recall shows that both methods produce lots of misses when people of different heights are present. \jlk{The Recall is very low and so is F1-Score. Reviewers will not like it.}

%As it has been discussed in Section \ref{sec:Distance Estimation Evaluation}, the performance of the two algorithms are much similar to each other on varying height distance compared to fixed height dataset. The main reason for this is that, geometry-based algorithm can be tuned for a specific height, on the other hand, this does not apply to deep-learning based approach. Thus, geometry-based approach outperforms deep learning-based approach when the height is fixed throughout the dataset. However, when there are people with different heights in the dataset, two algorithms perform identical in terms of social distance violation detection.

%\begin{table*}[t]
% \centering
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% &
% \multicolumn{2}{c|}{Fixed-height dataset} &
% \multicolumn{2}{c|}{Varying-height dataset} \\
% \hline
% &
% Geometry-based & Neural network & Geometry-based & Neural network \\
% \hline
% CCR &  90.41 & 89.04  & 89.45 & 89.45  \\
% \hline
% Precision &  100.00 & 100.00 & 100.00 & 100.00  \\
% \hline
% Recall &  72.00 & 68.00 & 40.00 & 40.00  \\
% \hline
% F1-Score & 83.72  & 80.95 & 57.14 & 57.14  \\
% \hline
% \end{tabular}
% \vglue 0.2cm
% \caption{Social-distance violation detection results. All results are reported in percentages (\%).}
% \label{tab:Social Distance Violation Results}
% \end{table*}


\subsection{Impact of a person's height adjustment}
\label{sec: Impact of height adaptation trick}

As we discussed in Section~\ref{ssec:heightadj}, the centers of the detected bounding boxes may not reflect the true height of a person due to occlusions. In this context, we proposed a method to adjust a bounding-box center location during testing to compensate for the occlusion effect.

Here, we evaluate the impact of this height adjustment on each method's performance. For a fair comparison, we use $H/2 =$ 32.5 inches (table height) in the geometry-based method. Recall that the neural-network approach was trained on chess mats placed at this height.

The value of MAE as a function of height-adjustment parameter $\alpha$ is shown in Figs.~\ref{fig:height_adaptation_plot}(a) and \ref{fig:height_adaptation_plot}(b)
%\ref{fig:MLP_fixed_after_lowering} and \ref{fig:Geo_fixed_after_lowering} 
for the fixed-height dataset. When the true bounding-box centers are used ($\alpha=0$), the MAE for the neural-network approach and V-V pairs (blue line) is close to 20 inches. However, when the centers are lowered by about 10\% ($\alpha \approx$ 0.10), the MAE for V-V pairs drops to about 9 in. 
%
Similar trends can be observed in Figs.~\ref{fig:height_adaptation_plot}(c) and \ref{fig:height_adaptation_plot}(d)
%\ref{fig:MLP_varying_after_lowering} and \ref{fig:Geo_varying_after_lowering} 
for the varying-height dataset. If the true bounding-box centers are used, the MAE for V-V pairs is above 20 inches for the neural-network approach. However, when the centers are lowered by about 15\% ($\alpha \approx$ 0.15), the MAE for V-V pairs drops to around 12 inches. Analogous trends can be seen for other types of pairs and for all pairs, as well as for the geometry-based approach.

\begin{figure}[!t] %\label{fig:heightadj}
    \centering
    % \vglue -0.4cm
    \begin{subfigure}[h]{0.41\textwidth}  
      \centering
      \includegraphics[width=0.95\textwidth]{images/Cam1_myself_MLP_MAE.png}
      % \vglue -0.2cm
      \caption{Neural-network algorithm, fixed-height dataset}
      %\label{fig:MLP_fixed_after_lowering}
    \end{subfigure}
    \begin{subfigure}[h]{0.41\textwidth}  
      \centering
      % \vglue -0.2cm
      \includegraphics[width=0.95\textwidth]{images/Cam1_myself_geometric_table_height_MAE.png}
      % \vglue -0.2cm
      \caption{Geometry-based algorithm, fixed-height dataset}
      %\label{fig:Geo_fixed_after_lowering}
    \end{subfigure}
    \begin{subfigure}[h]{0.41\textwidth}  
      \centering
      % \vglue -0.2cm
      \includegraphics[width=0.95\textwidth]{images/Cam1_everyone_MLP_MAE.png}
      % \vglue -0.2cm
      \caption{Neural-network algorithm, varying-height dataset}
      %\label{fig:MLP_varying_after_lowering}
    \end{subfigure}
    \begin{subfigure}[h]{0.41\textwidth}  
      \centering
      % \vglue -0.2cm
      \includegraphics[width=0.95\textwidth]{images/Cam1_everyone_geometric_table_height_MAE.png}
      % \vglue -0.2cm
      \caption{Geometry-based algorithm, varying-height dataset}
      %\label{fig:Geo_varying_after_lowering}
    \end{subfigure}
    \quad
     %add desired spacing between images, e. g. ~, \quad, \qquad etc.
      %(or a blank line to force the subfigure onto a new line)
    \caption{MAE for height adjustments: $-0.1\leq\alpha<1.0$.\label{fig:height_adaptation_plot}}
    % \vglue -0.6cm
\end{figure}

In Tables~\ref{tab:Fixed Height After Height Adaptation Distance_Estimation_Result_table} and \ref{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table}, we show the lowest MAE values for each pair type along with the corresponding value of $\alpha$. The two methods perform quite similarly (except for O-O pairs in the fixed-height dataset on which the geometry-based method performs better). For example, for the fixed-height dataset in Table \ref{tab:Fixed Height After Height Adaptation Distance_Estimation_Result_table} MAE for the best $\alpha$ for V-V pairs drops to about 9 inches for both algorithms compared to 12-18 inches seen in Table~\ref{tab:Fixed Height Distance_Estimation_Result_table}. Overall, in both datasets, with the right choice of $\alpha$, the MAE is well below 2 ft, which can be argued to be a reasonable result considering that the inter-people distances in our dataset are up to 58.5 ft. 
%\picomment{See my comment in the second-last para of Sec 5.2.}

Looking at Fig.~\ref{fig:height_adaptation_plot} and Tables~\ref{tab:Fixed Height After Height Adaptation Distance_Estimation_Result_table}-\ref{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table}, one notes that MAE is minimized for much smaller values of $\alpha$ for V-V pairs ($\alpha =$ 0.08-0.17) than for O-O pairs ($\alpha =$ 0.32-0.51). 
%
%\pirevised{The reason for this is that in the testing dataset the majority of occlusions happen in the lower half of people's bodies.}{}
This is because the majority of occlusions happen in the lower half of people's bodies in the testing dataset.
%
When a person is blocked in the bottom half, the bounding-box center radially shifts away from the image center. An example of this can be seen in Fig. \ref{fig:classroom_view}, where the person delineated by the yellow bounding box would have been delineated by the blue bounding box had there been no occlusion. Due to the occlusion, however, the bounding-box center shifts from the blue point to the yellow point. Therefore, the O-O pairs need to be compensated more than the V-V pairs, i.e., a higher value of $\alpha$ is needed.

\begin{table}[!htb]
\caption{Best mean-absolute distance error between two people for both methods using height adaptation for the fixed-height dataset. Under each MAE value, the corresponding optimum ``$\alpha$'' value used is written in parentheses. 
%\jlk{I have put $(\alpha)$ under MAE [in]. I think we do not need this explanation.}
%\mc{If we run out of space, we can remove the explanation in the caption. Otherwise, I think it might help the reader.}
}
\label{tab:Fixed Height After Height Adaptation Distance_Estimation_Result_table}
\centering
\begin{tabular}{|c|cccc|}
\hline
%&
%\multicolumn{8}{c|}{Fixed Height} \\
%\hline
&
\multicolumn{4}{c|}{MAE [in]}\\
&
\multicolumn{4}{c|}{($\alpha$)}\\
\hline
&  V-V & V-O & O-O & All\\
\hline
Geometry-based & 9.36 & 21.07 & 8.31 & 18.10 \\
($H/2 = $ 32.5in) & (0.08) & (0.28) & (0.32) & (0.18)\\
\hline
Neural network & 8.79 & 22.20 & 11.24 & 19.27 \\
(Trained on 32.5 in) & (0.12) & (0.33) & (0.42) & (0.26)\\
\hline
\end{tabular}
\end{table}
%Geometry-based & 8.24 & 20.12 & 6.48 & 16.77 \\
%($H/2$=35.04in) & (0.08) & (0.28) & (0.32) & (0.18)\\

\begin{table}[!htb]
\caption{Best mean-absolute distance error between two people for both methods using height adaptation for the varying-height dataset. Under each MAE value, the corresponding optimum ``$\alpha$'' value used is written in parentheses.}
\label{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table}
\centering
\begin{tabular}{|c|cccc|}
%\hline
%&
%\multicolumn{8}{c|}{Fixed Height} \\
%\hline
%&
%\multicolumn{4}{c|}{MAE [in]}\\
%&
%\multicolumn{4}{c|}{($\alpha$)}\\
%\hline
%&  V-V & V-O & O-O & All\\
%\hline
%Geometry-based & \zlrep{21.59}{12.76} & \zlrep{18.70}{20.49} & %\zlrep{4.94}{18.66} & 21.48 \\
%($H/2$=32.5in) & \zlrep{(0.15)}{(0.12)} & \zlrep{(0.41)}{(0.41)} & %\zlrep{(0.38)}{(0.48)} & (0.38)\\
%\hline
%Neural network & \zlrep{19.25}{11.62} & \zlrep{20.27}{21.30} & %\zlrep{4.21}{18.60} & 21.47 \\
%(Trained on 32.5in) & \zlrep{(0.18)}{(0.17)} & %\zlrep{(0.45)}{(0.45)} & \zlrep{(0.47)}{(0.51)} & (0.41)\\
%\hline
\hline
&
\multicolumn{4}{c|}{MAE [in]}\\
&
\multicolumn{4}{c|}{($\alpha$)}\\
\hline
&  V-V & V-O & O-O & All\\
\hline
Geometry-based & 12.76 & 20.49 & 18.66 & 21.48 \\
($H/2 = $ 32.5in) & (0.12) & (0.41) & (0.48) & (0.38)\\
\hline
Neural network & 11.62 & 21.30 & 18.60 & 21.47 \\
(Trained on 32.5 in) & (0.17) & (0.45) & (0.51) & (0.41)\\
\hline
\end{tabular}
\end{table}
%Geometry-based & 22.03 & 19.58 & 7.41 & 22.36 \\
%($H/2$=35.04in) & (0.15) & (0.41) & (0.37) & (0.38)\\

%When both algorithms use the actual center of a bounding box as a person's half-height, the geometry-based algorithm outperforms the neural-network approach (Table~\ref{tab:Fixed Height Distance_Estimation_Result_table} and Table~\ref{tab:Varying Height Distance_Estimation_Result_table}).

%due to \mcrep{ its adaptability to a person's height}{ the fact that $P_z$ was fixed according to the person's height in the fixed-height dataset. In these set of experiments, we set $P_z$ according to the table height, which is the height that the neural-network approach is trained on}. \jlk{But $H$ was fixed not adaptive. The reason that it worked better was that the selected $H$ was closer to true people's heights, whereas 32.5in of table height was not. We must be careful. In this section, in the geometry based algorithm we could have adjusted $H$ instead of the BB center. The advantage of center adjustment is that it applies to both algorithms.} \mc{I see your point. I revised to make it clear.}

%However, for a suitably selected value of $\alpha$, the two algorithms perform similarly as can be seen in Tables~\ref{tab:Fixed Height After Height Adaptation Distance_Estimation_Result_table} and \ref{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table}, and much better than in Tables~\ref{tab:Fixed Height Distance_Estimation_Result_table} and \ref{tab:Varying Height Distance_Estimation_Result_table}.

%\jlk{What this seems to show is that geometry-based algorithm performs better only since we know H. Even if they both perform similarly, the huge advantage of the GB algorithm is that it can be used without re-training especially valuable if somehow it could use the true height of a person. This could be the main message, but we would need results for GB for H=32.5in in Section 5.3.} \mc{Zhangchi put these results into the tables.}

%It can be seen from Figures~\ref{fig:MLP_fixed_after_lowering} and \ref{fig:Geo_fixed_after_lowering}, that with $\alpha \approx 0.15$ both algorithms perform very similar to each other \mcrep{}{on V-V pairs} and much better compared to the results in Table~\ref{tab:Fixed Height Distance_Estimation_Result_table}. \jlk{For which pairs? This is difficult to see from the plots.} \mc{I agree it was not clear, so, I tried clear it up. Maybe for this statement, we should point the reader to Table 4 and 5 instead of Figure 6.}

% \begin{figure}[!htb]
%   \centering
%   \includegraphics[width=0.5\textwidth]{images/center_shift_illustration_v1.png}
%   \caption{Example of bounding-box center shifting up due to occlusion.} \label{fig:center_shift_observation}
% \end{figure}

%\jlk{I think this argument was already made in Section 3.3} \mc{I agree that this observation is already stated in Section 3.3. But I think it is nice to confirm this observation with concrete results in this section. Also, in my opinion, the effect of occlusion on the choice of alpha is made clear here. In other words, in section 3.3, we don't say that for more severe occlusion we need a higher alpha.}

%\jlk{I find it interesting that the 4 curves in each of the plots intersect (have about the same MAE) at $\alpha\approx 0.3$.}

%\picomment{That is indeed a curious phenomenon. But I am not able to think of a satisfying explanation for it.}

% As it can be seen in Figure \ref{fig:CCR_after_lowering}, the height adjustment also improves the performance of social-distance violation detection. While for $\alpha=0$ (no adjustment), the CCR for on "All" pairs was 89.04\% (Table~\ref{tab:Social Distance Violation Results}), after after height adjustment with $\alpha$ = 0.7 the CCR reaches 96.87\%.

% \jlk{In Fig.~4, we need to change the legend from N to V. More importantly, it is difficult to quantify the minimum (MAE and relative MAE) and maximum (CCR). How about providing minimum/maximum values in the legend, for example "(minimum: CCR=..., $\alpha$= ...)"} \zl{by the way, for finding the minimum and its corresponding $\alpha$, should we include all sets? Due to their complex relationship of N-O sets it greatly changed the argmin of $\alpha$ (for example, N-O sets almost keep decreasing during the entire shift, thus putting the argmin of $\alpha$ much higher). But if we separate them, then it seems to be too much on this part}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%In order to further improve performance of the neural-network approach, calibration for a range of chess-mat heights would be needed -- a time-consuming, labor-intensive process. 
%
%\picomment{Is the dataset for camera calibration for the geometry-based method not time consuming and labor-intensive? Is collecting multiple height chess-mat datasets an order of magnitude more intensive than for a single height? There could be other ideas which could ease the labor that we have not considered.}
%
%The  geometry-based method, however, uses height information as a parameter during inference. Therefore, 
%For the results reported in Tables~\ref{tab:Fixed Height After Height Adaptation Distance_Estimation_Result_table} and \ref{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table} 
%
 % \vglue -0.2cm
In results reported thus far, the same value of $\alpha$ was used for both people in every pair. In the V-O and `All' categories, however, it could be advantageous to use different values of $\alpha$ for the visible and occluded person.
%
%Performance could be further improved if we could tune the height-adjustment parameter $\alpha$ specifically for each person. 
To verify this hypothesis, we %\pirevised{have}{} 
applied $\alpha = 0.1$ to all visible bounding boxes and $\alpha = 0.5$ to all occluded bounding boxes in the fixed-height dataset. This $\alpha$ adjustment per person 
%\pirevised{resulted in}{}
gave an MAE of 12.80 inches (down from 18.10 inches) for the geometry-based algorithm and 11.06 inches (down from 19.27 inches) for the neural-network approach. The corresponding MAE values for the varying-height dataset were: 13.97 inches (down from 21.48 inches) and 13.14 inches (down from 21.47 inches). Clearly, an automatic detection of body occlusions and a suitable adjustment of parameter $\alpha$ can further improve the distance estimation accuracy. 
%\pirevised{This is a future direction for research.}{}
This could be a fruitful direction for future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Application in the context of social distancing}
\label{sec: Evaluation of Social Distancing Violation Detection}

One very practical application of the proposed methods is to detect situations when social-distancing recommendations (typically 6 ft) are being violated. This problem can be cast as binary classification: two people closer to each other than 6 ft are considered to be a ``positive'' case (violation takes place) whereas two people more than 6 ft apart are considered to be a ``negative'' case (no violation). To measure performance, we use Correct Classification Rate (CCR) and F1-score. Table~\ref{tab:Social Distance Result table} shows their values for both algorithms applied in this context to ``All'' pairs.  We report results for $\alpha = $ 0.5 which gives the lowest MAE for pairs with occlusions on the varying-height dataset (Table~\ref{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table}).
%\picomment{If that's the stated logic for the varying-height dataset, to be consistent shouldn't we choose a different value when reporting results for the fixed-height dataset? A reviewer may consider this to be inconsistent. The full picture would be caputured by showing CCR and F1 curves as a function of alpha (similar in spirit to Fig. 5).}

%We apply the proposed approaches to the problem of visual detection of scenarios where a social-distancing constraint (typically, 6ft) is being violated. More formally, we treat the social-distance violation detection as a binary classification problem; people closer to each other than 6ft are considered to be a ``positive'' case (violation takes place) while people more than 6ft apart are considered to be a ``negative'' case (no violation). In Table \ref{tab:Social Distance Result table}, we show the performance of both algorithms at $\alpha$ = 0.5 applied in this context to ``All'' pairs. We use the Correct Classification Rate (CCR) and F1-Score as the performance measures.

%\mcrep{Both methods perform very well on the fixed-height dataset achieving CCR values in excess of 94\% and F1-scores over 91\%. Both approaches perform identically for $H/2$=32.5in. On the varying-height dataset, both algorithms perform similarly in terms of CCR achieving values around 94\%. In terms of F1-score, the neural-network approach outperforms the geometry-based algorithm by 1.64\% points when $H/2$=35.04in and by 3.31\% points when $H/2$=32.5in. Overall, both methods are able to detect social distance violation with a high accuracy.}

%The methods perform identically on the fixed-height dataset, achieving CCR value close to 96\% and F1-score close to 92\%. On the varying-height dataset, the neural-network approach slightly outperforms the geometry-based algorithm in terms of CCR (by 0.78\% points) and more significantly (by 3.31\% points) in terms of F1-score.
On the fixed-height dataset, the neural-network approach slightly outperforms the geometry-based algorithm: by 1.37\% points in terms of CCR and by 0.63\% points in terms of F1-score. The methods perform identically on the varying-height dataset, achieving CCR value close to 95\% and F1-score close to 80\%. These results suggest that despite the presence of people of different heights both approaches achieve high enough CCR and F1-score values to be potentially useful in practice for the detection of social-distance violations in the wild. 
%\picomment{But only if we know the correct value of alpha to use and the test dataset comes from conditions very similar to the training dataset.}


%%%%
%\zl{If we check the plot of F1 and CCR under geometric model with $H/2$ = 35.04in, it seems that the curve of F1 and CCR of $H/2$ = 35.04in is already dropping down at alpha = 0.4. On the contrary the curve of F1 and CCR of $H/2$ = 32.5in changes slower, since it needs more $\alpha$ to let the image center reach the table height than my height. Should we include this in explaining why the neural-network approach outperforms the geometry-based algorithm by 1.64\% when $H/2$=35.04in and by 3.31\% when $H/2$=32.5in?}
%
%\mc{In fact, I thought of a similar explanation while I was writing this section. But then, it will raise the question of, why did we choose $\alpha$=0.5. I chose $\alpha$=0.5, because the results were good at this value of $\alpha$, but clearly we can't say it like that in the paper. For now, I have swept it under the carpet, by not mentioning the reason for choosing $\alpha$=0.5} 
%
%\picomment{I think Zhangchi's explanation is a re-statement of the empirical observations and not an explanation based on something we can physically relate to. I therefore suggest leaving it alone. But I think we should try to provide some justification for choosing  $\alpha=0.5$. Ideally we should have produced the CCR/F1 scores using the best values for each dataset and method from Tables 4, 5. A possible explanation we could add is the following: ``The results of Table~\ref{tab:Social Distance Result table} are for $\alpha = 0.5$ which is close to the optimum values of $\alpha$ for many pairs with occlusions in Table~\ref{tab:Varying Height After Height Adaptation Distance_Estimation_Result_table}''.  } 
%
%\zl{I rethink of the $\alpha$ we should choose. Actually maybe we should try $\alpha = 0.3$ for this part. 4 curves in each of the plots in Figure 5 intersect (have about the same MAE) at $\alpha\approx 0.3$. Also according to the plots, F1 and CCR at all situations seem to be reaching its peak.}
%
%\picomment{In that case, please prepare a table with $\alpha = 0.4$. We can discuss and finalize if we should use $\alpha = 0.3$ or $0.5$ today evening.} 
%
%\mc{Zhangchi prepared the same table for $\alpha$=0.3 and $\alpha$=0.4, they are commented out right now. While I was thinking about these issues, I realized one thing, in Table 4 and Table 5, we do not report the results for H/2=35.04. Perhaps, we should not also include the results for H/2=35.04 in Table 6. Or if we want to keep the results for H/2=35.04 in Table 6, then maybe we should add the results for H/2=35.04 to Table 4 and 5 (Zhangchi computed these numbers too and added as a comment in Table 4 and 5). In any case, we have all the numbers and it will just require a small change to modify it either way. In my opinion, removing H/2=35.04in results from Table 6 would be the best, because that would also help us to save some space which we need at the moment. I did not want to mess up the current flow, before discussing this with you.}
%




\begin{table}[!ht]
\caption{Social-distance violation detection results ($\alpha$=0.5).}
\label{tab:Social Distance Result table}
\vspace{-2ex}
\centering
\begin{tabular}{|c|cc|cc|}
\hline
%&
%\multicolumn{8}{c|}{Fixed Height} \\
%\hline
&
\multicolumn{2}{c|}{Fixed-height} &
\multicolumn{2}{c|}{Varying-height}\\
&
\multicolumn{2}{c|}{dataset} &
\multicolumn{2}{c|}{dataset}\\
\hline
&  CCR & F1 & CCR & F1 \\
&  [\%] & [\%] & [\%] & [\%]\\
\hline

% Geometry-based&  94.52 & 91.14 & 94.14 & 78.05\\
% ($H/2$=35.04in) & & & &\\
% \hline
Geometry-based &  94.52 &  91.14 & 94.53 & 79.69\\
($H/2$=32.5in) & & & &\\
\hline
Neural network &  95.89 & 91.77 & 94.53 & 79.69\\
(Trained on 32.5in) & & & &\\
\hline
\end{tabular}
%Geometry-based 35.04in & 97.14 & 90.62 & 100.00 & 94.52 & 98.55 & 78.98 & 100.00 & 91.14\\
\vspace{-2ex}
\end{table}

% \begin{table}[!ht]
% \caption{Social-distance violation detection results for $\alpha$=0.4.}
% \label{tab:Social Distance Result table_1}
% \centering
% \begin{tabular}{|c|cc|cc|}
% \hline
% %&
% %\multicolumn{8}{c|}{Fixed Height} \\
% %\hline
% &
% \multicolumn{2}{c|}{Fixed-height} &
% \multicolumn{2}{c|}{Varying-height}\\
% &
% \multicolumn{2}{c|}{dataset} &
% \multicolumn{2}{c|}{dataset}\\
% \hline
% &  CCR & F1-score & CCR & F1-score \\
% &  [\%] & [\%] & [\%] & [\%]\\
% \hline

% Geometry-based&  95.89 & 91.77 & 93.75 & 76.38\\
% ($H/2$=35.04in) & & & &\\
% \hline
% Geometry-based &  94.52 &  88.95 & 93.75 & 76.38\\
% ($H/2$=32.5in) & & & &\\
% \hline
% Neural network &  94.52 & 88.95 & 93.75 & 76.38\\
% (Trained on 32.5in) & & & &\\
% \hline
% \end{tabular}
% \end{table}


% \begin{table}[!ht]
% \caption{Social-distance violation detection results for $\alpha$=0.3.}
% \label{tab:Social Distance Result table_2}
% \centering
% \begin{tabular}{|c|cc|cc|}
% \hline
% %&
% %\multicolumn{8}{c|}{Fixed Height} \\
% %\hline
% &
% \multicolumn{2}{c|}{Fixed-height} &
% \multicolumn{2}{c|}{Varying-height}\\
% &
% \multicolumn{2}{c|}{dataset} &
% \multicolumn{2}{c|}{dataset}\\
% \hline
% &  CCR & F1-score & CCR & F1-score \\
% &  [\%] & [\%] & [\%] & [\%]\\
% \hline

% Geometry-based&  94.52 & 88.95 & 93.75 & 76.38\\
% ($H/2$=35.04in) & & & &\\
% \hline
% Geometry-based &  93.15 &  86.07 & 93.75 & 76.38\\
% ($H/2$=32.5in) & & & &\\
% \hline
% Neural network &  93.15 & 86.07 & 93.75 & 76.38\\
% (Trained on 32.5in) & & & &\\
% \hline
% \end{tabular}
% \end{table}


% \begin{table*}[!ht]
% \centering
% \begin{tabular}{|c|cccc|cccc|}

% \hline
% %&
% %\multicolumn{8}{c|}{Fixed Height} \\
% %\hline
% &
% \multicolumn{4}{c|}{CCR} &
% \multicolumn{4}{c|}{F1-score} \\
% \hline
% &  V-V & V-O & O-O & All & V-V & V-O & O-O & All \\
% \hline
% Geometry-based 32.5in & 100.00 & 90.62 & 100.00 & 95.89 & 100.00 & 78.98 & 100.00 & 91.77\\
% \hline
% Geometry-based 35.04in & 97.14 & 90.62 & 100.00 & 94.52 & 98.55 & 78.98 & 100.00 & 91.14\\
% \hline
% Neural network & 100.00 & 90.62 & 100.00 & 95.89 & 100.00  & 78.98 & 100.00 &91.77\\
% \hline
% \end{tabular}
% %Geometry-based 35.04in & 97.14 & 90.62 & 100.00 & 94.52 & 98.55 & 78.98 & 100.00 & 91.14\\
% \caption{Social distance violation detection results for the fixed-height dataset (For $\alpha$=0.5).}
% \label{tab:Fixed Height Social Distance Result table}
% \end{table*}

% \begin{table*}[!ht]
% \centering
% \begin{tabular}{|c|cccc|cccc|}

% \hline
% %&
% %\multicolumn{8}{c|}{Fixed Height} \\
% %\hline
% &
% \multicolumn{4}{c|}{CCR} &
% \multicolumn{4}{c|}{F1-score} \\
% \hline
% &  V-V & V-O & O-O & All & V-V & V-O & O-O & All \\
% \hline
% Geometry-based 32.5 in & 98.27 & 88.42 & 100.00 & 93.75 & 94.66 & 40.56 & 100.00 & 76.38\\
% \hline
% Geometry-based 35.04in & 98.27 & 89.25 & 100.00 & 94.14 & 94.66 & 46.65 & 100.00 & 78.05\\
% \hline
% Neural network & 98.27 & 90.08 & 100.00 & 94.53 & 94.66  & 52.29 & 100.00 & 79.69\\
% \hline
% \end{tabular}
% %Geometry-based 35.04in & 98.27 & 89.25 & 100.00 & 94.14 & 94.66 & 46.65 & 100.00 & 78.05\\
% \caption{Social distance violation detection results for the varying-height dataset (For $\alpha$=0.5).}
% \label{tab:Varying Height Social Distance Result table}
% \end{table*}



\section{Concluding Remarks}

We developed two methods (the first of their kind) for estimating the distance between people in indoor scenarios based on a single image from a \textit{single} overhead fisheye camera. Demonstrating the ability to accurately measure the distance between people from a single overhead fisheye camera (with its wide FOV) has practical utility since it can decrease the number of cameras (and cost) needed to monitor a given area. A novel methodological contribution of our work is the use of a height-adjustment test-time pre-processing operation which makes the distance estimates resilient to height variation of individuals as well as body occlusions. We demonstrated that both methods can achieve errors on the order of 10-20in
%in a large space of more than 2,000ft$^2$
for suitable choices of height-adjustment tuning parameters. We also showed that both of our methods can predict social distance violation with a high F1-score and accuracy. 
% \vspace{-2ex}

%An alternative is to estimate heights of people from a fisheye image and use this information in the geometry-based algorithm for improved 3D-world distance estimation between people. 

%In order to further improve performance of the neural-network approach, calibration data for a range of chess-mat heights would be needed. This, however, would be a time-consuming, labor-intensive process. On the other hand, while the  geometry-based method requires camera calibration this process needs to be completed only once for a specific camera model (Axis M3057-PLVE in our case). Once calibrated, the camera can be mounted overhead in a new space and only its installation height $B$ (Fig.~\ref{fig:p_z_illustration}) needs to be measured. The method can predict distances with high accuracy when the heights of individuals are known; otherwise errors are higher. We showed that this error can be reduced by height-adjustment pre-processing. \zlrep{}{One of the future directions in height adjustment researching is to apply different height-adjustment on bounding boxes based on its occlusion status. For example, when applying $\alpha = 0.1$ on visible bounding boxes and $\alpha = 0.5$ on occluded bounding boxes, the MAE of all fixed-height dataset is 12.80in based on Geometry-based algorithm, and is 11.06in based on Neural network approach. At the same circumstance, the MAE of all varying-height dataset is 13.97in based on Geometry-based algorithm, and is 13.14in based on Neural network approach.} An alternative is to estimate heights of people from a fisheye image and use this information in the geometry-based algorithm for improved 3D-world distance estimation between people. This is a future direction for our research.

%Our geometry-based method requires knowledge of camera calibration parameters and can predict distances with high accuracy when the heights of individuals are similar and known. However when heights of individuals differ from the nominal, the errors are higher. 
%
%Our neural network method was trained using ground-truth distance measurements of chess-board patterns collected at a fixed height. A single position of our chess-board pattern provides us with distances between a large number of pairs of corner points. A comparable number of positions would be quite challenging to collect and annotate with real people. However, our chess-board training has no height variation and this can negatively impact performance in a diverse population with people of various heights. This could be addressed by collecting additional samples with height variation and use them to fine tune the network trained on the fixed-height chess-board dataset via transfer learning techniques. 


% \picomment{These bullets were inserted as placeholders by Mertcan at my request. They will have to be removed at some point.}
% \begin{itemize}

%     \item{Our proposed methods use single fisheye camera to measure distance between people. That makes the cost of deployment cheaper, cover a larger are due to wide FOV.}

%     \item{Geometry based approach is easier to adapt for different height compared to neural network based approach. However, during calibration geometry-based approach requires multiple cameras, on the other hand, neural network based approach requires a single camera for training.}
    
%     \item{To improve the neural network based approach, training data can be collected in multiple heights, then, the height can be added as to the input vectors. This will allow height adaptability for the neural network based approach. }
    
% \end{itemize}



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

%%%%%%%%%%%%%%%%%%%%%%%% RELATED WORK RESEARCH

% \cite{stero_camera} Measure distance between objects under stero vision system. Problem is that it always requires 2-camera involved even in testing. (not like Josh's geometric model that only require one camera in testing)

% \cite{greyscale_level} Measure distance of object to camera using greyscale level and scale of baseline across the object. Problem is that it could only be used for traditional flat camera on the ground in flat indoor environment, and measuring distance from object to camera.

% \cite{distort_compensate} Measure distance of object to camera using distortion compensation algorithm. Problem is it requires 2-camera involved in prediction and is mainly used for measuring distance of object to camera.

% \cite{equirect_images} Propose equirectangular projection for stero fisheye camera distance prediction to improve detection range and accuracy. Problem is that it always requires 2-camera involved even in testing.

% \cite{disparity_offset} Improve the accuracy of a fisheye stereo camera by correcting images using a disparity offset map. Problem is it requires 2-camera involved in prediction and is mainly used for measuring distance of object to camera.

% \cite{CNN_distance} Use FRCNN model to first project Centroids in top perspective and then identify social distance violation. Problem is that it seems not involving height property of objects and is using traditional camera.

% \cite{IPM_distance} Use IPM model to predict location of objects and then predict distances between objects in camera images. Problem is that it is using traditional camera.

% \cite{geo_flat} Use chess mat boards to calibrate geometric model(no height involved) and then predict distance between objects in camera images. Problem is that it seems not involving height property of objects (setting height = 0) and is using traditional camera.

% \cite{YOLO_3_distance} Use YOLO3 to first detect people with bounding box, then take the pixel of mid point of bottom line of bounding box as input, project it into real world and then estimate distance. Problem is that it is using traditional camera and also could only be used in open space where there is no occlusion.

% \cite{track_and_estimate} After using R-CNN to detect and track people, estimate the real distance of each person from the camera in metres. Then estimate distance between people using both the absolute difference in distance of people to camera and the absolute difference in the horizontal coordinates of their centroids. Problem is that camera must be placed so the same plane of people.


\end{document}
