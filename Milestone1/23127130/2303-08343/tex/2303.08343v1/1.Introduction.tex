\section{Introduction}
\label{sec:introduction}
\vspace{-3pt}




Automatic speech recognition (ASR) is an essential component in a growing number of spoken language interfaces on mobile devices. Recently, long running applications such as transcribed recording, live captioning, and generalized keyword spotting are emerging, and are even more challenging on edge devices due to the limited resources. Always-on ambient speech recognition, the most ambitious use scenario, leverages advances in both deep learning and embedded neural processing hardware to enable always-running ASR on low power edge devices.

To achieve efficient always-on recognition with edge devices, instead of running a standard ASR model, alternative approaches are usually considered, such as recognizing a single keyword~\cite{alvarez2019end} or just a small set of intents~\cite{ray2021listen}. 
However, this is not possible for keywordless interactions and also requires newly trained models each time new intents are made available. As such, in this work, we aim to focus on the extendability achieved by a generalized speech recognition model.
However, recent advances in machine learning come at the expense of ever increasing model sizes (i.e., $100$M parameters and higher). These models are not tractable for always-running on neural accelerators such as edge TPUs~\cite{antonini2019resource}, which are limited to fewer than $6$M parameters due to hardware memory constraints. In fact, to achieve inference using such large model sizes, the model must be split into smaller chunks which are then continuously transferred from memory to TPU, leading to poor energy usage and poor latency for ambient speech recognition tasks.


In this paper, we look for methods to reduce the size of Conformer-based~\cite{conformer} speech recognition models to achieve always-on ambient speech recognition, which can efficiently leverage specialized hardware such as edge TPUs. To do this, we propose model weight reuse at different levels within our Conformer architecture such as: (i) repeating full conformer layers, (ii) sharing specific modules across conformer layers, (iii) sharing specific sub-components within each conformer module, and (iv) sharing low-rank sub-weights after low-rank decomposition. 
Unlike other model compression techniques like low-bit quantization~\cite{hubara2016binarized} and sparsity~\cite{han2015learning} which assume the use of specialized hardware features which we will discuss in \Cref{sec:related_works}, both sharing and low rank architectures can be achieved with existing neural accelerators.
By sharing weights across layers, we can increase the number of virtual transformations applied to our input data without increasing the physical size of the model weights in memory. 
Increasing the number of virtual transformations in our model allows for 
more complex
transformations on our model input which emulates the transforms typically found by increasing the number of layers in a model.


