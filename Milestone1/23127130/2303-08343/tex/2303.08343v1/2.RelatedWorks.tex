\section{Related Works}
\label{sec:related_works}
\vspace{-3pt}



Performing machine learning model inference on-board low power edge devices has recently achieved greater attention for tasks such as device-free wireless sensing~\cite{hernandez2022wifi}, computer vision~\cite{xu2022ultra}, and numerous other tasks~\cite{banbury2020benchmarking}. 
At the core of edge model inference is achieving model compression for use on low power and low resourced devices.

Model compression has commonly been achieved through a number of methods such as 
sparsity pruning~\cite{han2015learning,wu2021dynamic,ding2021audio}, low-bit quantization~\cite{zhou2018adaptive, novac2021quantization, ding22c_interspeech}, %
knowledge distillation~\cite{DBLP:conf/interspeech/CeruttiPBF19, yang2020model}, and low-rank matrix factorization~\cite{DBLP:journals/corr/abs-2207-00112,yu2017compressing}.
These techniques can typically be applied regardless of the model architecture which allows them to be generalized to different tasks. However, some methods assume access to specific hardware features that may not be available on edge devices.
Model sparsity techniques offers the ability to prune weights until an exact model size is achieved. However, without structured sparsity~\cite{wen2016learning}, the resulting model requires irregular memory access and without hardware support, memory usage and computation become inefficient.
Quantization is typically applied to reduce model weights from $32$-bit floating point values down to $8$-bit integer values, and is also applied to lower quantization levels (i.e., $1$-bit, $2$-bit, or $4$-bit~\cite{hubara2016binarized, ding22c_interspeech}) and even mixed-precision quantization~\cite{schaefer2022edge}.
However, computations on low-bit quantization level models are not available on typical real-world hardware.
On the other hand, techniques like knowledge distillation and low-rank decomposition are computed off-device and thus performing inference on these compressed models 
is identical to non-compressed models.






