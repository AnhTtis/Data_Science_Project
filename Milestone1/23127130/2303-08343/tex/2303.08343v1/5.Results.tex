\vspace{-5pt}
\section{Results}
\label{sec:results}
\vspace{-3pt}







\subsection{Repeat Full Layers}
We begin our evaluations by reviewing the results of sharing full conformer layers. When sharing conformer layers, we repeat the conformer transformations in order over multiple repetitions. Suppose we have $N$ conformer blocks repeated $R$ times, we define the number of physical conformer layers as $N$ and the number of virtual conformer layers as $N \times R$. By increasing $R$, we can achieve an increase in the number of transformations applied to our input data with the expectation that increasing the number of transformations will allow an improvement in the model quality. In
\Cref{tab:share_conformer_blocks}, we begin by reviewing the model quality with a single physical conformer layer repeated different numbers of times. We observe that even with one physical conformer block (SL0-SL2), the WER decreases as the number of repetitions is increased (i.e., an increase in the number of virtual layers). With just $3$ repetitions of the single conformer layer (SL2), our model is able to decrease the WER by $-2.30$ and $-1.82$ for dev and test respectively. Even so, the WER rates are still large, so while we demonstrated that increasing the number of virtual layers can improve the model quality, the quality is still good enough. To improve this, we increase to $4$ physical conformer layers (SL3-SL6) which also brings our model size closer towards our goal of $5$M parameters. We find that repeating the conformer block transformations three times (SL5) reduces the WER by $-0.93$ and $-0.85$ for dev and test respectively compared to just one iteration of each conformer block layer. However, we find that further increasing to four repetitions per conformer block (SL6) begins to degrade our model quality. Thus, there is a limit to the number of times conformer blocks should be repeated.



\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \cline{2-6}
        \multicolumn{1}{c|}{} & \multirow{2}{*}{\thead{Non-Shared\\Modules}} & \multirow{2}{*}{\thead{Model\\Dim.}} & \multicolumn{2}{c|}{\thead{WER}} & \multirow{2}{*}{\thead{Model\\Size}} \\
    \cline{4-5}
        \multicolumn{1}{c|}{} & & & \thead{dev} & \thead{test} &  \\
\cline{2-6}\noalign{\vspace{2.5pt}}
    \hline
    SM0 & F.F. Start & 96 & 3.56 & 3.64 & 4.93M \\
    \hline
    SM1 & Attention & 128 & 3.19 & 3.48 & 4.99M \\
    \hline
    SM2 & Convolution & 136 & 3.13 & 3.35 & 5.03M \\
    \hline
    SM3 & F.F. End & 96 & 3.64 & 3.88 & 4.93M \\
    \hline
    \multirow{2}{*}{SM4} & \multirow{2}{*}{\makecell[c]{Attention\\+ Convolution}} & \multirow{2}{*}{120} & \multirow{2}{*}{3.22} & \multirow{2}{*}{3.36} & \multirow{2}{*}{5.03M} \\
    & & & & & \\
    \hline
    \end{tabular}
    \caption{Sharing conformer layers (i.e., $4$ Physical, $12$ Virtual), while unsharing specific modules.}
    \label{tab:shared_modules}
\vspace{-10pt}
\end{table}

\vspace{-3pt}
\subsection{Sharing Conformer Modules}
\vspace{-2pt}
Next, we dig into the structure of the conformer blocks to identify the major modules which we can either enable or disable sharing. 
In \Cref{tab:shared_modules}, we use the shared conformer block model with $4$ physical conformer layers repeated $3$ times (SL5) as our base model and then select certain conformer modules to disable sharing (i.e., unshare). By unsharing individual modules, the model size increases, and thus, we must reduce the internal model dimension to compensate. Unsharing the convolution layer (SM2) offers the lowest WER rates at $3.13$ and $3.35$ for dev and test respectively. However, it is interesting to observe that unsharing the feed forward start (SM0) and feed forward end (SM3) modules significantly increases the WER rates. We can attribute this to the fact that both feed forward modules are so large in size, and thus, by not sharing these modules, we must greatly reduce the size of the model weight dimensionality hyperparameter to compensate.








\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
\cline{2-6}
    \multicolumn{1}{c|}{} &\multicolumn{2}{c|}{\thead{Non-Shared Sub-Components}} & \multicolumn{2}{c|}{\thead{WER}} & \multirow{2}{*}{\thead{Model\\Size}} \\
\cline{2-5}
    \multicolumn{1}{c|}{} & \thead{Module} & \thead{Sub-Component} & \thead{dev} & \thead{test} &  \\
\cline{2-6}\noalign{\vspace{2.5pt}}
\hline
SC0 & F.F. Start & Linear (1) & 3.10 & 3.23 & 6.02M \\
\hline
SC1 & F.F. Start & Linear (2) & 3.16 & 3.20 & 6.02M \\
\hline
\hline
SC2 & Attention & Query & 3.24 & 3.37 & 5.01M \\
\hline
SC3 & Attention & Value & 3.23 & 3.40 & 5.01M \\
\hline
SC4 & Attention & Key & 3.09 & 3.29 & 5.01M \\
\hline
\hline
SC5 & Conv. & Pre-Conv. & 5.49 & 5.81 & 5.17M \\
\hline
SC6 & Conv. & Conv. & 3.02 & 3.16 & 5.35M \\
\hline
SC7 & Conv. & Post-Conv. & 3.37 & 3.71 & 5.01M \\
\hline
\hline
SC8 & F.F. End & Linear (1) & 2.99 & 3.18 & 6.02M \\
\hline
SC9 & F.F. End & Linear (2) & 3.08 & 3.30 & 6.02M \\
\hline
\hline
SC10 & All & Misc. Small & 2.95 & 3.28 & 5.36M \\
\hline
    \end{tabular}
    \caption{Effect of allowing certain conformer sub-components to be shared or not shared.}
    \label{tab:unshare_sub_weights}
\vspace{-3pt}
\end{table}


\vspace{-3pt}
\subsection{Sharing Sub-Components}
\vspace{-2pt}
To further our understanding of how sharing of different components affects quality, we next look at disabling sharing (i.e., unsharing) for specific sub-components within our model. Again, we leverage the best model from \Cref{tab:share_conformer_blocks} where we have $4$ physical layers repeated three times giving a total of $12$ virtual transformations (SL5) which achieved a WER of $3.20$ for a model of size $4.84$M. In \Cref{tab:unshare_sub_weights}, we unshare single weight variables at a time for each module. We see that unsharing these sub-components still keeps our model size close to $5$M parameters except in the case of the linear sub-components in both feed forward start and end modules (SC0, SC1, SC8, and SC9). In addition to unsharing the individual module sub-components which were shown in Fig.~\ref{fig:conformer_block_structure}, a number of other significantly smaller weights are also found within each module. These weights are small enough that they do not have a large impact on the overall model size (i.e., only an increase of $0.52$M parameters). Thus, we also evaluate unsharing these miscellaneous small weights as well (SC10). We can see that unsharing the convolution sub-components within the convolution module allows for the lowest WER (SC6) while unsharing the other sub-components in the conformer layer each result in an increase in the WER. For the attention module, unsharing both query (SC2) and value (SC3) results in similar WER to the original model, yet unsharing key (SC4) does see a decrease in WER, thus implying that the attention key sub-components contains important information for our model. 



\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
\cline{2-7}
    \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\thead{\# Conformer Layers}} & \multirow{2}{*}{\thead{Rank (k)}} & \multicolumn{2}{c|}{\thead{WER}} & \multirow{2}{*}{\thead{Model\\Size}} \\
\cline{2-3}\cline{5-6}
    \multicolumn{1}{c|}{} & \thead{Physical} & \thead{Virtual} & & \thead{dev} & \thead{test} & \\
\cline{2-7}\noalign{\vspace{2.5pt}}
\hline
LR0 & 4 & 4 & N/A & 4.13 & 4.30 & 4.84M \\
\hline
LR1 & 8 & 8 & 50 & 3.46 & 3.69 & 5.04M \\
\hline
LR2 & 12 & 12 & 20 & 3.70 & 3.75 & 4.98M \\
\hline
LR3 & 16 & 16 & 6 & 3.81 & 4.05 & 5.00M \\
\hline
\hline
LRS0 & 8 & 16 & 50 & 3.14 & 3.36 & 5.04M \\
\hline
LRS1 & 8 & 24 & 50 & 2.99 & 3.23 & 5.04M \\
\hline
LRS2 & 8 & 32 & 50 & 2.88 & 3.25 & 5.04M \\
\hline
LRS3 & 8 & 40 & 50 & 2.84 & 2.98 & 5.04M \\
\hline
    \end{tabular}
    \caption{After applying low-rank architecture for feed forward modules. With and without sharing layers.}
    \label{tab:low_rank}
\vspace{-5pt}
\end{table}

\vspace{-3pt}
\subsection{Low-Rank (and Sharing)}
\vspace{-2pt}
Next we look towards low-rank architecture in \Cref{tab:low_rank}. By reducing the $k$, we can subsequently achieve an increase in the number of physical layers. As we can see, with $k=50$ (LR1), we are able to increase from $8$ physical layers compared to only $4$ when a low-rank architecture is not applied (LR0). We find that $k=50$ also decreases the WER of the model by $-0.67$ and $-0.61$ for dev and test respectively. However, while we expect increasing the number of physical conformer layers should improve the quality, we find that $k$ directly counteracts these WER improvements and thus while $k=20$ (LR2) and $k=6$ (LR3) achieve lower WER compared to the non low-rank architecture, they both perform worse than $k=50$ (LR1). Continuing with $k=50$ and the number of physical layers at $8$, we also apply our layer sharing technique\footnote{Preliminary results show only marginal improvements in combining low-rank and sub-component sharing due to the large search space.} to increase the number of virtual layers from $16$ (LRS0) up until $40$ (LRS3) by repeating each conformer layer. With this, we find WERs as low as $2.84$ and $2.98$ on dev and test are achievable at our bounds of $5$M parameters when repeating the $8$ physical conformer layers $5$ times each (LRS3).

\vspace{-3pt}
\subsection{Overview}
\vspace{-2pt}
Our overall best results for the evaluated methods are shown in \Cref{tab:overview}.
Each of our evaluated models was created based on an initial $14$M parameter model (B0) described in~\cite{conformer}.
We compare these models to a handcrafted  $5$M parameter model (B1) which was created by manually reducing hyperparameters (e.g., number of conformer blocks). 
The lowest overall WER was achieved by LRS3 because 
low-rank decomposition reduces the size of each physical conformer layer, thus allowing for a greater number of physical conformer layers while sharing layer weights through repeating offers an even greater number of virtual conformer layer transformations without increasing model size.
While reducing model size does increase WER compared to larger models, our goal in this work is to create a model which fits completely within TPU memory, thus offering low-power, always-on ASR. With this, we can handle most ASR tasks, while defer to a larger model only when necessary.


\begin{table}[]
    \centering
    \begin{tabular}{|l|c|c|c|}
    \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{\thead{Model}}} & \multicolumn{2}{c|}{\thead{WER}} & \multirow{2}{*}{\thead{Model\\Size}} \\
    \cline{2-3}
     & \thead{dev} & \thead{test} &  \\
    \hline
    \hline
    Conformer (S) \cite{conformer} (B0) & 2.18 & 2.53 & 14M \\
    \hline
    \hline
    Handcrafted (B1) & 3.53 (-0.00) & 3.72 (-0.00) & 4.9M \\
    \hline
    \hline
    Share Layers (SL5) & 3.20 (-0.33) & 3.45 (-0.27) & 4.84M \\
    \hline
    Share Modules (SM2) & 3.13 (-0.40) & 3.35 (-0.37) & 5.09M \\
    \hline
    Shared Sub-C. (SC10) & 2.95 (-0.58) & 3.28 (-0.44) & 5.36M \\
    \hline
    Low-Rank (LR2) & 3.46 (-0.07) & 3.69 (-0.03) & 5.04M \\
    \hline
    L.R. Share (LRS3) & 2.84 (-0.69) & 2.98 (-0.74) & 5.04M \\
    \hline
    \end{tabular}
    \caption{Overall best results for the evaluated compression methods.}
    \label{tab:overview}
\vspace{-10pt}
\end{table}


