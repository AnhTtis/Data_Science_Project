\vspace{-3pt}
\section{Experiment Design}

\vspace{-3pt}
\subsection{Dataset}

We evaluate on the LibriSpeech datasets~\cite{panayotov2015librispeech} which consists of $960$ hours of training data (i.e., train-clean and train-other)
and we evaluate on dev-clean and test-clean.
The spoken-word input data is structured as $80$ log Mel-filterbank energy features with a window size of $25$ms and a $10$ms stride. The output is modelled using a word-piece model (WPM)
embedding with a dimensionality of $1,024$. 

\vspace{-3pt}
\subsection{Model Architecture}


We begin our evaluations with a conformer architecture (B0) with $14$M parameters and $16$ conformer block layers consisting of $0.7$M parameters each. 
This baseline architecture achieves a word-error rate (WER) of $2.18$ on dev-clean and $2.53$ on test-clean, however, our goal is to reduce this model down to approximately $5$M parameters (a reduction of approximately $-65\%$), thus this model is not applicable for always-on ambient ASR using low-power edge TPU devices.
Notice, we begin with a $14$M parameter model rather than a larger $100$M$+$ parameter model since it is a common size for ``small'' models in the literature~\cite{conformer, han2020contextnet}.
We also design a handcrafted $5$M parameter version of this model (B1) as a baseline with $8$ conformer block layers with a size of $0.5$M parameters each. This $5$M baseline model achieves a WER of $3.53$ on dev-clean
and $3.72$ on test-clean.