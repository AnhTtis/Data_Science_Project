\vspace{-6pt}
\section{Methods}
\label{sec:methods}
\vspace{-3pt}

\subsection{Conformer Model}

\vspace{-3pt}
For our ambient ASR task, we leverage the conformer model architecture~\cite{conformer}, an extension to the transformer model architecture~\cite{vaswani2017attention}. For the intents of this work, we will focus on reducing the size of the conformer encoder since we find that it takes up greater than $90\%$ of the overall model size. The size of the encoder is primarily a result of the $N$ conformer blocks, thus we will also focus on ways to reduce the size of the encoder by both reducing the size of the individual conformer blocks as well as reducing the need for large values of $N$.

We define the $i$-th conformer block $\mathbb{C}_{(i)}$ in our model as $\mathbb{C}_{(i)}\big(F_\text{start}^{(i)}, A^{(i)}, C^{(i)}, F_\text{end}^{(i)}\big)$ where $F_\text{start}$, $A$, $C$, $F_\text{end}$ are the parameters for the feed forward start, attention, convolution and, feed forward end modules respectively within the $i$-th conformer block $\mathbb{C}_{(i)}$. 
Fig.~\ref{fig:conformer_block_structure} illustrates the largest size sub-components for each of the modules. It is important to recognize these largest sub-components when reducing the size of our model because these are the weight matrices which we should focus on compressing. Notice, that while the architecture of our conformer contains many unique features, %
the size of the conformer blocks is primarily the result of several linear layers.
Thus, if we can apply a compression technique to simple linear layers, then they 
can be
similarly applied throughout the entire conformer model. 

    
    




\vspace{-3pt}
\subsection{Repeat Full Layers}
\vspace{-2pt}

Suppose we have a conformer model with $N$ conformer blocks $\mathbb{C}_{(i)}$ where $i \in \{1, \dots, N\}$, then we can repeat each $i$-th layer $R[i]$ times as suggested in~\cite{dabre2019recurrent} by sharing the $i$-th layer's parameters. Our conformer transformation will then be described as a series of $n$-fold iterative functions defined as:

\begin{equation}
    f^{\circ n} = \underbrace{f \circ f \circ \dots \circ f}_{n},
\end{equation}
where $f$ is some transformation function, and $n$ is the number of times that the function is composed over itself. As such, our conformer transformation could be described as:
\vspace{-5pt}
\begin{equation}
    \underbrace{\mathbb{C}_{(N)} \circ \dots \circ \mathbb{C}_{(N)}}_{R[N]} \circ  \dots \circ
    \underbrace{\mathbb{C}_{(2)} \circ \dots \circ \mathbb{C}_{(2)}}_{R[2]} \circ
    \underbrace{\mathbb{C}_{(1)} \circ \dots \circ \mathbb{C}_{(1)}}_{R[1]},
\end{equation}
\vspace{-5pt}
or 
\begin{equation}
    \mathbb{C}_{(N)}^{\circ R[N]} \circ 
    \dots \circ 
    \mathbb{C}_{(2)}^{\circ R[2]} \circ 
    \mathbb{C}_{(1)}^{\circ R[1]}.
\end{equation}
By repeating layers, we retain a set number of physical conformer layers ($N$) while increasing the number of virtual conformer layers or the number of transformations ($R \times N$). By performing $n$-fold iterative conformer transformations, we expect that we can transform our input with more complex transformations without directly increasing our cost from a model size perspective. 

\vspace{-3pt}
\subsection{Sub-component Customization}
\vspace{-2pt}
While we expect that the higher complexity transformations offered by repeating layers will be of benefit to our model architecture, there is still a clear intuition that a model with $R \times N$ virtual layers will likely not be able to perform better than a model with $R \times N$ physical layers due to the increased number of distinct parameters available in the model. Thus, we may wish to allow for layer repeating but with some slight customization per layer. 
By allowing customization, we can retain our ability to reduce model size through sharing, but we also allow conformer blocks to perform unique transformations instead of strictly iterative function composition.

Our first effort towards this is to only share certain modules within our conformer blocks. 
Sharing specific modules such as feed forward or attention modules was initially reviewed in~\cite{lan2019albert} towards a reduced size BERT model. We define sharing indices $\mathcal{I}_{FS}$, $\mathcal{I}_{A}$, $\mathcal{I}_{C}$, $\mathcal{I}_{FE}$ which correspond to our feed forward start, attention, convolution, and feed forward end modules respectively\footnote{Other modules are ignored due to the large relative size of these modules.}. Each of the described sharing indices $\mathcal{I}_{x}$, are subject to the following constraints: 
$|\mathcal{I}_{x}| = N$, 
$min(\mathcal{I}_{x}) = 1$, 
$max(\mathcal{I}_{x}) \leq N$. 
With these sharing indices, we can now define our $i$-th conformer block as: 
\vspace{-3pt}
\begin{equation}
\mathbb{C}_{(i)}\bigg(
F_{start}^{\big(\mathcal{I}_{FS}^{(i)}\big)}, 
A^{\big(\mathcal{I}_{A}^{(i)}\big)}, 
C^{\big(\mathcal{I}_{C}^{(i)}\big)}, 
F_{end}^{\big(\mathcal{I}_{FE}^{(i)}\big)}
\bigg).
\vspace{-5pt}
\end{equation}



\begin{figure}
    \centering

\begin{minipage}{\linewidth}
\setlength{\DTbaselineskip}{9pt}
\scriptsize
\dirtree{%
.1 \textbf{Encoder (12.2M)}.
.2 \textbf{Conformer Block ($\times$16)} \dotfill 100.0\%.
.3 \text{FF Start} \dotfill 39.0\%.
.4 \textit{Linear (1)} \dotfill 19.5\%.
.4 \textit{Linear (2)} \dotfill 19.5\%\vspace{1mm}.
.3 \text{Attention} \dotfill 14.0\%.
.4 \textit{Key} \dotfill 2.8\%.
.4 \textit{Value} \dotfill 2.8\%.
.4 \textit{Query} \dotfill 2.8\%.
.4 \textit{Post} \dotfill 2.8\%.
.4 \textit{Pos Query} \dotfill 2.8\%\vspace{1mm}.
.3 \text{Convolution} \dotfill 8.0\%.
.4 \textit{Pre-Conv} \dotfill 5.2\%.
.4 \textit{Conv} \dotfill 0.2\%.
.4 \textit{Post-Conv} \dotfill 2.6\%\vspace{1mm}.
.3 \text{FF End} \dotfill 39.0\%.
.4 \textit{Linear (1)} \dotfill 19.5\%.
.4 \textit{Linear (2)} \dotfill 19.5\%.
}
\dirtree{%
.1 \textbf{Decoder (1.8M)}.
}
\end{minipage}
    
    \caption{High level composition of our base $14$M parameter Conformer model (B0). We focus on reducing the size of the $16$ conformer blocks within the encoder portion through sharing 
    (i) full conformer blocks, (ii) modules, and (iii) sub-components.
    }
    \label{fig:conformer_block_structure}
\vspace{-15pt}
\end{figure}



Digging deeper into the structure of our conformer modules, even smaller sub-components can be found. In Fig.~\ref{fig:conformer_block_structure} we see that the largest sub-components are primarily linear layers. Beyond these sub-components, there are also some much smaller modules which have miniscule effect on decreasing the model size, yet may allow for better customization of our shared conformer layers. As such, we suggested that these smaller components can be excluded from our model sharing system, thus allowing improved model performance. Furthermore, we expect that certain sub-components may hold great importance in providing improved model performance and thus should not be shared even though they do
contribute to increasing model size.

\subsection{Low-Rank Factorization}

As illustrated in Fig.~\ref{fig:conformer_block_structure}, the largest sub-components within our conformer are normal linear layers composed of a weight matrix $M$ and bias $b$. The complexity of the conformer comes from the specific architecture of the model rather than the complexity of the specific sub-components. Since the weight matrix is much larger in size than the bias, we will focus on reducing the size of $M$.
Supposing that $M \in \mathbb{R}^{m \times n}$, we can apply low-rank decomposition~\cite{DBLP:journals/corr/abs-2207-00112} to reduce $M$ into three distinct sub-matrices: $U \in \mathbb{R}^{m \times k}$, $V \in \mathbb{R}^{n \times k}$, and $\Sigma \in \mathbb{R}^{k \times k}$ a diagonal matrix which can be found through:
\vspace{-3pt}
\begin{equation}
\min_{U,\Sigma,V} ||M - U \Sigma V^T||,  %
\vspace{-3pt}
\end{equation}
using singular value decomposition (SVD). 
Given $k \ll \text{min}(m, n)$, 
the number of parameters for any $M$ in our model can be reduced.



\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
    \cline{2-6}
        \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{\thead{\# Conformer Layers}} & \multicolumn{2}{c|}{\thead{WER}} & \multirow{2}{*}{\thead{Model\\Size}} \\
    \cline{2-5}
        \multicolumn{1}{c|}{} & \thead{Physical} & \thead{Virtual} & \thead{dev} & \thead{test} &  \\
\cline{2-6}\noalign{\vspace{2.5pt}}
    \hline
        SL0 & 1 & 1 & 10.69 & 10.53 & 2.55M \\
    \hline
        SL1 & 1 & 2 & 9.15 & 9.18 & 2.55M \\
    \hline
        SL2 & 1 & 3 & 8.39 & 8.71 & 2.55M \\
    \hline 
    \hline
        SL3 & 4 & 4 & 4.13 & 4.30 & 4.84M \\
    \hline
        SL4 & 4 & 8 & 3.50 & 3.76 & 4.84M \\
    \hline
        SL5 & 4 & 12 & 3.20 & 3.45 & 4.84M \\
    \hline
        SL6 & 4 & 16 & 3.31 & 3.67 & 4.84M \\
    \hline 
    \end{tabular}
    \caption{Sharing full layers by passing layer output into itself.}
    \label{tab:share_conformer_blocks}
\vspace{-10pt}
\end{table}


While reducing $k$ allows us to reduce the number of parameters for a given matrix, it also greatly increases the reconstruction error. To account for this, typically model fine-tuning is performed after decomposition to account for loss in model performance. In our case, we instead begin with a low-rank structure when training from scratch which allows us to forgo the need for training followed by fine tuning and also allows us to ignore the use of singular value decomposition. As such, for simplicity, $\Sigma$ can be combined with $U$ or $V$ in our low-rank reconstruction structure as suggested in~\cite{xue2013restructuring}. Thus, we can restate our reconstruction structure as $M \sim U V^T$.




