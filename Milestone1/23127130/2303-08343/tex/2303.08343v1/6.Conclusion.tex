\vspace{-3pt}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-2pt}

In this work, we propose to reduce the size of Conformer-based models through parameter weight reuse at four levels: (i) repeating conformer block layer transformations, (ii) sharing specific conformer modules, (iii) sharing or not sharing sub-components per conformer module, and (iv) sharing low-rank decomposed sub-weights. 
By sharing model weight across layers, we find that we can increase the number of virtual transformations of our input data without further increasing the size of our model and thus we can retain our model in-memory for always-on ambient ASR leveraging low-power and low-resource neural accelerators such as edge TPU hardware.
Through our evaluations, we find that sharing model weights and applying a low-rank Conformer architecture (LRS3) offers the greatest performance for our $5$M parameter models, achieving a WER of $2.84$ and $2.98$ for LibriSpeech dev-clean and test-clean respectively. 

