\section{ML-enhanced Data Spaces}


Semantic interoperability in data spaces is a complex issue that involves multiple aspects, as illustrated in Figure~\ref{fig:aspects}. While machine learning has the potential to improve each of these aspects, traditional approaches have primarily utilized machine learning techniques in isolation, rather than within the broader context of data spaces. It is vital to consider the full spectrum of semantic interoperability aspects and integrate machine learning in a comprehensive and holistic manner within the data space environment. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/aspects.pdf}
    \caption{Semantic interoperability aspects in data spaces that machine learning can enhance}
    \label{fig:aspects}
\end{figure}




Figure~\ref{fig:overview} presents an overview of the ML-enhanced data space in the International Data Spaces environment, showcasing six key aspects of data management among three stakeholders, including data providers and consumers and service providers. These aspects are:
\begin{itemize}[leftmargin=*]
\item  \textbf{Automatic Metadata Extraction (\circled{1})}: A machine learning model can automatically extract essential attribute values from the data if metadata is not already available, helping data providers to prepare their data for exchange and consumption without the need for manual metadata preparation.

\item \textbf{Ontology and Vocabulary Alignment (\circled{2})}: The vocabulary of the data space is aligned with the vocabulary of the data provider, enabling data consumers to understand the data being exchanged. This eliminates the need for members in the data space to adopt the same internal vocabulary, which can often be a challenging task.

\item \textbf{FAIRness Evaluation (\circled{3})}: The FAIRness level of the data is assessed based on provided or extracted metadata, allowing the data provider to improve the FAIRness of their data and allowing the data consumer to understand the ease of use of the data.

\item \textbf{Data Quality Assessment \& Enhancement (\circled{4})}: The quality of the data is evaluated and improved if possible, based on the format of the data. Machine learning can be used to evaluate and enhance structured and tabular data, however, it's important to recognize that the quality metrics may vary depending on the format of the data. For example, it might be challenging to assess the quality of unstructured data (e.g., a corpus of documents).

%not all formats support quality assessment.

\item \textbf{Privacy Preserving (\circled{5})}: ML-based anonymization and masking techniques can be applied to data that contains private, sensitive, or personal information to make it shareable. Sensitive data can be automatically detected or provided by the data provider, allowing data providers to share their data without any privacy concerns.

\item \textbf{Compatibility Improvement (\circled{6})}: The data is transformed into a readable format for the data consumer. In cases where data is being merged with the consumer's data, the consumer will communicate the structure and format, enabling the data to be transformed accordingly. This allows the consumer to make use of the received data without having to put in additional effort to read and understand it.
\end{itemize}


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/overview.pdf}
    \caption{An overview of an ML-enhanced Data Space with three members. (1) Automatic Metadata Extraction, (2) Ontology and Vocabulary Alignment, (3) FAIRness Evaluation, (4) Data Quality Assessment \& Enhancement, (5) Privacy Preserving, (6) Compatibility Improvement.}
    \label{fig:overview}
\end{figure*}



In the following, we discuss each of these aspects: 


\subsection{Automatic Metadata Extraction}
Metadata plays a vital role in data exchange as it enables data consumers to understand the data and determine if it meets their needs. However, many data providers may be hesitant to provide the necessary metadata due to a lack of capacity or knowledge to prepare it for their resources. This can be a significant obstacle in data exchange, as it limits the ability of consumers to access and utilize the data they need.

To overcome this challenge, machine learning can be leveraged to (semi) automatically extract metadata from resources. Machine learning algorithms can be trained on a dataset of resources and their corresponding metadata, allowing them to learn the patterns and relationships between the data and the metadata. These algorithms can then be applied to new resources to extract the relevant metadata. This approach has the advantage of being able to handle complex and nuanced relationships between the data and the metadata. It can also be easily updated and adapted as the data and its needs evolve. However, it is important to note that a typical challenge in data spaces is that the resources have different, heterogeneous formats.




%Machine learning can also assist in the creation of metadata for resources by using natural language processing to extract information from the resource's title or description to automatically generate the metadata. The metadata properties can vary based on the resource being exchanged and it's necessary to use different ML models for different sources.


Different resources being exchanged in data spaces can have varying metadata properties, and it may be necessary to utilize different machine learning (ML) models for different resources and metadata attributes. For instance, in the case of document corpora, Natural Language Processing (NLP) techniques can be employed to extract titles and descriptions. Specifically, automatic metadata extraction techniques such as those in~\cite{boukhers2022vision, tkaczyk2017new} can be utilized to extract metadata from each document, such as \emph{Publication Date}, \emph{Author}, \emph{Language}, etc. This metadata can then be used to derive the metadata for the entire collection, such as \emph{Publication Range}, \emph{Authors}, \emph{Languages}, etc.



\subsection{Ontology and Vocabulary Alignment}
\label{sec:onto}

The International Data Spaces Reference Architecture\footnote{\url{https://internationaldataspaces.org/use/reference-architecture/}} highlights the importance of common vocabularies for effective data exchange within a data space. However, in practice, data providers may have their own unique vocabularies, making it difficult to align them with the vocabulary used in the data space. This can be due to the cost and effort involved in mapping their existing vocabularies to the data space vocabulary, or due to the fact that a data provider may participate in multiple data spaces with different vocabularies.

To tackle these challenges, machine learning algorithms can be utilized to support automatic mapping between the local vocabulary of a data provider and the vocabulary used in the data space. This allows for seamless and interoperable data exchange, without requiring data providers to adopt a new vocabulary.

Machine learning-based methods for ontology alignment~\cite{nezhadi2011ontology} and ontology matching~\cite{doan2004ontology} can be applied to automatically map concepts and terms from one ontology or vocabulary to another. These algorithms use techniques such as semantic similarity measures~\cite{sousa2022supervised}, graph-based methods~\cite{shenoy2013secured}, and deep learning models~\cite{khoudja2018ontology, iyer2020veealign, bento2020ontology} to identify correspondences between concepts in different ontologies or vocabularies. The goal is to produce a mapping that enables data exchange between systems using different ontologies or vocabularies while preserving the meaning of the data.



\subsection{FAIRness Evaluation}

%Recently, the FAIR principles (i.e. \textbf{F}indable, \textbf{A}ccessible, \textbf{I}nteroperable and \textbf{R}esuable) are highly regarded in data exchange as compliance with them increases the likelihood of resources being reused. Therefore, evaluating the FAIRness of a resource can assess its fitness for use. For instance, even though the data might be suitable for a particular use case, the accompanying license might not be appropriate. Identifying this in advance can save time and resources, as the cost of negotiating for data exchange or waiting for access to data that does not meet all the requirements (including FAIRness, licensing, and access conditions) can take several months. Thus, evaluating the FAIRness level of a resource beforehand can help reduce the time and effort spent on obtaining data that may not be suitable for the intended use.


The FAIR (i.e., \textbf{F}indable, \textbf{A}ccessible, \textbf{I}nteroperable and \textbf{R}eusable) principles are becoming increasingly important in data exchange and sharing. These principles aim to ensure that data resources are easily discoverable, accessible, can be easily integrated with other data sources, and can be reused for multiple purposes. Compliance with these principles makes it more likely that data will be used and reused, as it increases the overall quality and usability of the resource.


Evaluating the FAIRness of a resource is a crucial step in determining its fitness for use, as it helps to identify any potential barriers to reuse. This can include issues such as licensing restrictions, data access conditions, and data interoperability issues. Conducting this evaluation in advance can save valuable time and resources, as it helps to avoid the need for costly negotiations or lengthy wait times for access to data that may not be suitable for the intended use.

As discussed in Section~\ref{sec:onto}, the use of shared vocabularies, such as ontologies, is important for increasing the findability and interoperability of resources. However, only using mapping techniques (see Section~\ref{sec:onto}) may not be enough, as internal ontologies that describe the metadata may not be represented using common classes. To address this issue, machine learning techniques, such as BERTmap~\cite{he2022bertmap}, can be used to assess the level of compatibility between the provider's ontology and the data space's ontology. Additionally, rule-based and semantic web technologies can be used to evaluate the structure of the metadata, further increasing the overall FAIRness of the resource.


\subsection{Data Quality Assessment \& Enhancement}
Data quality is a crucial concern for data consumers, as it impacts the trustworthiness and usefulness of the data. Unfortunately, metadata alone cannot provide any indication of the quality of the data. To ensure the quality of data, various dimensions must be considered, including accuracy, completeness, correctness, validity, integrity, and uniqueness. The importance of each dimension may vary depending on the intended use of the data and the needs of the data consumer.


Accuracy refers to how closely the data reflects the real-world phenomenon it represents. Completeness refers to the extent to which all necessary data is present. Correctness pertains to the degree to which the data adheres to established rules, such as those related to syntax, semantics, or data constraints. Validity refers to the degree to which the data follows the predefined format, structure, and domain. Integrity is the degree to which the data is protected against unauthorized changes. Lastly, uniqueness refers to the degree to which each data item is distinct and identifiable.

To ensure data quality, data providers must take steps to assess and improve the quality of their data. This can include implementing data validation and quality checks, using techniques like data profiling and data cleaning, and implementing data governance policies and procedures. Data consumers should also take steps to assess the quality of the data they receive, such as evaluating the data's source and provenance, performing data quality checks, and monitoring the data for anomalies.


Machine learning algorithms can play a crucial role in ensuring the quality and accuracy of data. One way they achieve this is by comparing the data to other sources to validate its accuracy. Additionally, machine learning algorithms can be trained to identify patterns and anomalies in the data~\cite{agrawal2015survey, pang2021deep}, helping to flag any potential inaccuracies or errors.

Another benefit of using machine learning algorithms is the ability to complete missing data. By analyzing patterns and relationships in the data, machine learning models can make predictions about missing values and fill them in~\cite{thomas2021systematic, raja2020missing, hasan2021missing}. This is especially useful in cases where it would be time-consuming or challenging to manually fill in missing data.

Furthermore, machine learning techniques can also be applied to identify and remove duplicates in data, improving the overall uniqueness and consistency of the data~\cite{park2022deepsketch,tarun2021scheme, christen2019towards}.


\subsection{Privacy Preserving}

Private and sensitive data, such as personal information, medical records, and financial data, is often subject to strict regulations and guidelines for protection and access.
% These regulations.
In order for different systems to exchange and use private data, they must be able to accurately interpret and understand the meaning and context of the data, and ensure that it is being used in compliance with applicable laws and regulations. ensuring semantic interoperability for private data requires a combination of technical solutions, such as secure data exchange protocols and data anonymization techniques, and strict governance and compliance mechanisms.

To achieve this, data providers can use machine learning techniques to automatically detect private and sensitive data in their systems~\cite{ray2021sensitive, ahmed2021automated} and take appropriate actions to mask~\cite{torra2022privacy} or anonymize~\cite{majeed2020anonymization} the data. This can help protect individuals' privacy while enabling data sharing and interoperability. For example, techniques such as data de-identification, data masking, and differential privacy can be used to remove identifying information from data while preserving its usefulness for analysis. 



\subsection{Compatibility Improvement}
Also, when the same vocabulary and ontology are used by the data provider and consumer, resources are not semantically interoperable if they are not compatible with the consumer system of their resource to be integrated with. To overcome the incompatibility of resources in data exchange, solutions include data mapping and data transformation. Machine learning techniques have shown great performance in these tasks. 


Resources are not semantically interoperable when they cannot be understood or used by the systems that need to access them. This can occur when the resources have different data formats or structures, making it difficult for systems to integrate and make use of the information.

To overcome the incompatibility of resources in data exchange, solutions include data mapping~\cite{li2018mfecnn} and data transformation~\cite{sajid2019predictive}. Data mapping is the process of aligning the data elements from one resource to the corresponding elements in another resource. Data transformation is the process of converting data from one format or structure to another. Both of these solutions can help to make resources compatible and enable data exchange. Machine learning can also be used to convert data from one format to another, such as natural language text to structured data~\cite{verma2020unstructured}. 







\iffalse

Figure~\ref{fig:overview} illustrates an overview of an ML-enhances data space with three members and service providers. The ML-enhanced approach to data space management has six aspects, as outlined below:

Automatic Metadata Extraction (\circled{1}): If the metadata is not already created, the machine learning technique can extract the most important attributes from the data.

Ontology and Vocabulary Alignment (\circled{2}): The data provider's vocabulary is mapped to the vocabulary of the data space, to ensure that the data consumers can understand the received data.

FAIRness Evaluation (\circled{3}): The FAIRness level of the data is assessed based on the provided or extracted metadata.

Data Quality Assessment & Enhancement (\circled{4}): Quality assessment and improvement of data are possible for data with a format that allows it. Structured and tabular data can be evaluated and enhanced using machine learning techniques, but it is important to note that not all data formats support quality assessment.

Privacy-Preserving (\circled{5}): If the data contains private, sensitive, or personal information, anonymization and masking techniques are applied to make the data suitable for sharing. The sensitive data can be automatically detected or provided by the data provider.

Compatibility Improvement (\circled{6}): The data is transformed into a readable format for the data consumer. In cases where the data is to be merged with the data consumer's data, the latter communicates its structure and format to the data provider, allowing the data to be transformed accordingly.





First, if the metadata is not already created, it can be automatically extracted from the data (see \circled{1}). The capability of the machine learning technique can differ according to the type of data but in general, the values of the most important attributes can be extracted. Second, the vocabulary of the data provider can be mapped to the vocabulary of the data space to ensure that the data consumers can understand the received data (see \circled{2}). Third, the FAIRness level is assessed given the provided or extracted metadata (see \circled{3}). Fourth, The quality of the data is assessed and improved if the format of the data allows it (see \circled{4}). Specifically, The quality of structured and tabular data can theoretically be assessed and improved using machine learning techniques. Fifth, if the data contains private, sensitive or personal data, anonymization and masking techniques can be used to make the data ready to be shared (see \circled{5}). The sensitive data can be either detected automatically or provided by the data provider. Finally, the data is transformed in such a way that it can be readable by the data consumer. In case the data is planned to be merged with the data consumer's data, the latter communicates its structure and format with the data provider so that the data is transformed accordingly. 

\fi