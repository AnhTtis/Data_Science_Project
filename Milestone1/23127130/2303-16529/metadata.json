{
    "arxiv_id": "2303.16529",
    "paper_title": "Importance Sampling for Stochastic Gradient Descent in Deep Neural Networks",
    "authors": [
        "Thibault Lahire"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG"
    ],
    "abstract": "Stochastic gradient descent samples uniformly the training set to build an unbiased gradient estimate with a limited number of samples. However, at a given step of the training process, some data are more helpful than others to continue learning. Importance sampling for training deep neural networks has been widely studied to propose sampling schemes yielding better performance than the uniform sampling scheme. After recalling the theory of importance sampling for deep learning, this paper reviews the challenges inherent to this research area. In particular, we propose a metric allowing the assessment of the quality of a given sampling scheme; and we study the interplay between the sampling scheme and the optimizer used.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16529v1"
    ],
    "publication_venue": "17 pages, 3 figures"
}