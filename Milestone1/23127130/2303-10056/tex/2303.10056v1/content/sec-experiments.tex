\input{   figs/fig-multilingual.tex}

\section{Experiments}
\vspace{-2mm}
\label{sec:experiments}
\subsection{Experiments Setup}
\label{sec:experiments-setup}
\vspace{-1mm}

\input{   figs/fig-pie-user-study.tex}

Upon the shoulders of giants, this paper applies the Latent-Diffusion Model (LDM) and Stable Diffusion Model (SDM)~\cite{rombach2022high} as the baselines with a similar model but different text encoders and scalabilities. LDM uses a Bert-like~\cite{devlin2018bert} model as the text encoder that is jointly updated with the image decoder based on DDPM loss, while SDM uses a frozen pre-trained CLIP text encoder  during training. The LDM is designed for the images in 256 $\times$ 256 and SDM targets 512 $\times$ 512 images. We implement the multilingual T2I and sound-to-image generation upon the SDM. 



\textbf{Implementation.} The GlueNet is implemented as the Fig.~\ref{fig:method} with an encoder $M$ and a decoder $N$ in symmetrical architecture consisting $\sim$51M parameters. For simplicity in denotation, we refer GlueNet encoder as GlueNet in the following experiments, whose body net consists of 5 residual modules as default. We take the AdamW~\cite{loshchilov2017decoupled} as the optimizer based on PyTorch~\cite{NEURIPS2019_9015}. The learning rate is assigned as 1$\times 10^{-4}$ for GlueNet training and 1$\times 10^{-6}$ for LDM-T5-GlueNet finetuning. The GlueNet is trained on Nvidia-A100, requiring 0.5 to 5 GPU days for different uses. 




\textbf{Condition Encoders.} We apply the T5-3B~\cite{raffel2020exploring} to upgrade the text encoder of LDM Bert. T5-3B has 6x more parameters than LDM Bert and is trained by an enormous text corpus beyond the image captions. Furthermore, to enable multilingual T2I generation, we choose the XLM-Roberta-L~\cite{devlin2018bert} as the new condition encoder. In sound-to-image generation, we take the AudioCLIP~\cite{guzhov2022audioclip} audio encoder.



\textbf{Datasets.}
We have collected the text data for GlueNet training. In monolingual T2I, only English texts are required, and we extract 18M captions from Laion-400M as training corpus, and more data will be more helpful. Multilingual GlueNets require parallel texts in different languages. These data are collected by Wikimarix~\cite{schwenk2019wikimatrix} and \href{https://pypi.org/project/googletrans/}{Googletrans} with $\sim$2M sentences for each. To train GlueNet for sound-to-image generation, we use the Urbansound8k dataset~\cite{salamon2014dataset}\footnote{https://urbansounddataset.weebly.com/urbansound8k.html}, which consists of more than 8,000 audio samples collected from city environments in ten different classes, such as dog barking, children playing, car horn, siren, etc.




 \vspace{-1mm}
 
\subsection{Monolingual Text-to-Image Generation}
 \vspace{-1mm}



Monolingual T2I generation focuses on the comparison with LDM given the English text prompts. We have decomposed our pipeline into two stages: alignment and finetuning. GlueNet helps to align the new language model whose potential can be better exploited by further finetuning of image text pairs. This section verifies the benefits of our vanilla and finetuned models in both quantitative and qualitative evaluations.  \textbf{T5+GlueNet} refers to the LDM model with the stacking of T5 and GlueNet as the text encoder where the LDM image decoder is unchanged. \textbf{T5+FT} represents finetuning of the LDM image decoder with DDPM loss based on text conditions from a fixed T5. \textbf{T5+GlueNet+FT} is the final version, including both GlueNet and joint finetuning with the image decoder. The finetuning takes $\sim$ 100 GPU days of 116M image-text pairs filtered from Laion-400M by BLIP score~\cite{li2022blip}. Unet finetuning is optional and not needed for the remaining experiments in Sec.~\ref{sec:exp-multilingual} and Sec.~\ref{sec:exp-sound2img}.

\input{   tabels/tab-fid.tex}


 \vspace{-1mm}
\subsubsection{Quantitative Evaluation}

 \vspace{-2mm}

Tab.~\ref{tab:fid} reported the zero-shot FID score~\cite{heusel2017gans} on COCO dataset~\cite{lin2014microsoft} based on the pyorch-fid~\cite{Seitzer2020FID} package. Our re-implementation score of LDM is slightly higher than the paper's one~\cite{rombach2022high}, which may because of different implementations. The table shows the minor drop of T5+GlueNet from LDM since our current GlueNet model still leaves some incorrect alignments, which could be fixed by either joint finetuning or collecting more text corpus to train GlueNet. With the $\sim$100 GPU-days finetuning of a subset of the Laion dataset (\ie, 116M image-text pairs), the overall performance has been considerably improved, even outperforming the LDM by certain margins. 

 \vspace{-1mm}

\input{   figs/fig-hybrid.tex}

\input{   figs/fig-db-wn-demo.tex}



We have also conducted a user study to compare the text controllability in Fig.~\ref{fig:user-study} by Amazon Turk~\cite{doi:10.1177/1745691610393980}. There are $~\sim$1K prompts collected from Drawbench~\cite{saharia2022photorealistic} and Winoground~\cite{thrush2022winoground}, which comprehensively cover various scenarios and numerous difficult cases. We report the percentages of users' votes for the three methods in Fig.~\ref{fig:user-study}. 
In the user study of controllability, ours outperformed the LDM by 3\%, which is a non-trivial improvement. Moreover, the \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binomtest.html}{p-value} is computed as 0.04 with three repeats of user study that is statistically significant according to the criteria of $<$0.05.

 \vspace{-2mm}

\subsubsection{Visual Comparison}
 \vspace{-2mm}
 
In Fig.~\ref{fig:db_wn_demo}, we have exhibited some generated images of different methods. By comparing LDM and T5+GlueNet, we can conclude that the proposed GlueNet helps to achieve comparable high-quality images even replacing conditions. Moreover, the T5+GlueNet+FT shows stronger controllability in many cases compared with LDM, such as ``a cube made of brick'', whereas the LDM's results fail to display the concept of ``cube''.

\subsection{Multilingual Text-to-Image Generation}
\label{sec:exp-multilingual}
Our proposed framework is very general and generic to many language models, including the multilingual backbones. We choose the SDM as our base model with the pre-trained {clip-vit-large-patch14}\footnote{https://huggingface.co/openai/clip-vit-large-patch14} as the text encoder to be upgraded by the multilingual one, \ie, {XLM-Roberta-L}. The GlueNet helps to align such two backbones with the parallel corpus as training data. Thus, the old SDM image decoder can successfully understand the text embeddings of other languages encoded by GlueNet and XLM-Roberta-L without any additional parameters updating except GlueNet.

\subsubsection{Comparison}
\input{   tabels/tab-sound2img.tex}


\input{   figs/fig-alignment-analysis.tex}
\input{   tabels/tab-multilingual.tex}

\input{   figs/fig_sound_results}



\input{   figs/fig-user-study-sound2img}

Fig.~\ref{fig:multi} shows the example of multilingual generation results with the prompt ``afternoon garden oil painting painted by impressionists'' in many languages. Most of the results reveal the content of ``afternoon garden''. The ``impressionists'' is a challenging notion that rarely appears in our training data. Thus, more high-quality parallel data will be greatly helpful. Compared with those finetuned over multilingual image text pairs, our solution is far more efficient and cheaper. For quantitative evaluation,  we have compared it with a popular multilingual translation model M2M100-418M~\cite{fan2021beyond} (418M \#params) trained by 7.5B parallel sentences. In contrast, our GlueNet uses far fewer training data (2M sentences for each pair). The results are listed in Tab.~\ref{tabs:multilingual} where we reported the Multilingual-CLIP score calculated over the 100 randomly selected multilingual image-caption pairs from Crossmodal dataset~\cite{thapliyal2022crossmodal}.The re-weighted version achieved competitive results of translation-based models under a significantly lower training cost.



\subsubsection{Hybrid Multilingual Generation}



The hybrid multilingual generation is more challenging that both the original SDM and third-party translation toolbox cannot address. Our proposed GlueNet has surprising capability in this problem. According to the results in Fig.~\ref{fig:hybrid}, the multilingual backbone bonded with GlueNet gives the precise guide to the SDM decoder even with the prompts in three different languages and randomly mixed. 


\subsection{Sound-to-Image}
\label{sec:exp-sound2img}



Beyond the text signals, the proposed GlueNet also achieves sound-to-image generation by aligning the CLIP text encoder with AudioCLIP~\cite{guzhov2022audioclip} audio encoders. This is a challenging task due to the modalities gap and the mismatch of token lengths (77 for CLIP and 1 for AudioCLIP). To address these challenges, we apply a reweighted objective with a high focus on the top and informative signals, as described in Sec.~\ref{sec:cross-modal}.
The input for GlueNet is the pairing data $<sound, label>$ encoded by AudioCLIP-audio-encoder (ESResNeXt~\cite{guzhov2021esresne}) and the CLIP-text-encoder respectively. Then, GlueNet is trained to align the sound embedding with the text embedding according to the objectives described in Sec.~\ref{sec:method}. To evaluate the effectiveness of GlueNet, we conducted both quantitative and visual comparisons over the testing set of Urbansound8k. Our baseline for comparison is the deployment of AudioCLIP audio and text models to retrieve the labels with audio input. The retrieved labels are then fed into the Stable Diffusion to generate results. The CLIP score in Tab.~\ref{tabs:sound2img} strongly reveals the superiority of our GlueNet over the baseline, particularly over mixed sounds, which are more challenging than single-source sounds and can be difficult for humans to distinguish sometimes.
We also introduce cross-modal signals fusion as another contribution of this paper. Our analysis of the sound-text fusion in Tab.\ref{tabs:sound2img} and Fig.~\ref{fig:sound2img_results} demonstrate that our proposed GlueNet achieves the best results.



\vspace{-2mm}
\subsection{Alignment Analysis}
\vspace{-2mm}
To understand how GlueNet maps the cross-model features, we have visualized its trajectory with the generative results in different iterations in Fig.~\ref{fig:alignment}. In the beginning, the old image decoder could not understand the new text embeddings at all. Then, certain patterns such as ``pencil drawing'' appear after 300 iterations of training with a significant decrease in losses. Such a gap can finally be sealed at the end where both $\mathcal{L}_{mse}$  and $\mathcal{L}_{rec}$ converged to small values.
