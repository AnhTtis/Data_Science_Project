\appendix
\onecolumn


\begin{center}
\Large
\textbf{Appendix}
\end{center}


\section{Details of GlueNet}
The key contribution of this paper is GlueNet, which addresses both cross-model alignment and feature protection. In this section, we will provide more details about GlueNet, including its implementation and configuration, an analysis of its sizes, and other ablation studies.

\subsection{Architecture and Configuration}
In Section 4.2 of the main paper, we introduced the basic architecture of GlueNet. Different sizes of GlueNet are available, and their configurations are detailed in Table~\ref{tab:configutations}. The GlueNet with three residual modules is the smaller variant, with 34M parameters in its encoder, while the GlueNet-5RMs has 51M parameters. However, larger models tend to have slower speed and higher computation costs. Moreover, there could be slight difference in the architecture for different encoders alignment. To maintain inference speed and efficiency during the finetuning stage, the size of GlueNet, when working as an injected module, should not exceed that of the image decoder and text encoder. Based on our empirical observations, assigning RM as five produces satisfactory results. GlueNet-3RMs, on the other hand, appears weak in representation learning. Besides the MLP-mixer, a self-attention-based model may also be used to implement GlueNet. In the future, we plan to explore more suitable architectures for GlueNet.


\begin{table}[h]
\caption{Model configurations for our GlueNet. We introduce two configurations of GlueNet-3RMs and GlueNet-5RMs. [LN] represent that layer-normalization is optional in the Tail Net. DIM-OUT and TOKEN-OUT are 77 and 1024 for Stable Diffusion v1. DIM-IN and TOKEN-IN depend on the target encoder to replace. $N$ is assigned as 1 if TOKEN-IN is equal to TOKEN-OUT. $N$ is larger than 1 (2 or 3) if TOKEN-IN is not equal to TOKEN-OUT.  }
\vspace{0.1cm}
\centering
\label{tab:configutations}
{\footnotesize
\resizebox{.8\textwidth}{!}{% <------ Don't forget this %
\begin{tabular}{c|c|c|c|c}
\Xhline{3\arrayrulewidth}
 Stage&Dimensions & Block & GlueNet-3RMs & GlueNet-5RMs  \\
 % Stage 1
 \hline
\makecell{ \\ \\ Head Net} & TOKEN-IN $\rightarrow$ TOKEN-OUT & 
\makecell{Token\\MLP} & $\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right] \times N $&  
$\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right] \times N$

\\
%
%
\cline{2-5}
 \rule{0pt}{5ex} & DIM-IN $\rightarrow$DIM-OUT & \makecell{Sequence\\MLP} 
 &  $\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right] \times N$
 &  $\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right] \times N$
 \\

%
% % Stage 2
\hline
\makecell{ \\ \\ Body Net} & TOKEN-OUT $\rightarrow$ TOKEN-OUT  & 
\makecell{Token\\MLP} & $\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right] \times 3$&  
$\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right] \times 5$
\\
%
%
\cline{2-5}
 \rule{0pt}{5ex} &DIM-OUT $\rightarrow$DIM-OUT & \makecell{Sequence\\MLP} 
 &  $\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right]\times 3$
 &  $\left[\makecell{\mathrm{Linear, LN, \sigma} \\ \mathrm{Linear, LN, \sigma} \\\mathrm{Linear, LN}} \right]\times 5$
 \\
%
%  % Stage 3
\hline
\makecell{ \\ \\ Tail Net} & TOKEN-OUT $\rightarrow$ TOKEN-OUT & 
\makecell{Token\\MLP} & $\left[\makecell{\mathrm{Linear,[LN], \sigma} \\ \mathrm{Linear, [LN], \sigma} \\\mathrm{Linear, [LN]}} \right]$&  
$\left[\makecell{\mathrm{Linear, [LN], \sigma} \\ \mathrm{Linear, [LN], \sigma} \\\mathrm{Linear, [LN]}} \right]$
\\
%
%
\cline{2-5}
 \rule{0pt}{5ex} &DIM-OUT $\rightarrow$DIM-OUT & \makecell{Sequence\\MLP} 
 &  $\left[\makecell{\mathrm{Linear, [LN], \sigma} \\ \mathrm{Linear, [LN], \sigma} \\\mathrm{Linear, [LN]}} \right]$
 &  $\left[\makecell{\mathrm{Linear, [LN], \sigma} \\ \mathrm{Linear, [LN], \sigma} \\\mathrm{Linear, [LN]}} \right]$
 \\
%

\Xhline{3\arrayrulewidth}
\end{tabular}
}
}
\end{table}

\section{Analysis of Text Encoder Replacement}
\subsection{Analysis of GlueNet Sizes}

Figure~\ref{fig:rm_sup} presents a visual comparison of example prompts across three different methods. The original LDM model with the checkpoint~\footnote{https://github.com/CompVis/latent-diffusion} shared in its repository is referred to as LDM Ori. Our models, GlueNet-3RMs and GlueNet-5RMs, contain three or five residual modules within their body nets, respectively. The text encoder is replaced with T5-3B, while the image decoder is the same as LDM Ori. Both GlueNet models are trained on the same text corpus, consisting of 18 million English sentences. The figure shows that GlueNet-3RMs struggles with some complex prompts, such as "a virus monster is playing guitar, oil on canvas" and "there is a penguin with a dog head standing," where GlueNet-5RMs performs better. We can, therefore, conclude that deep GlueNet is necessary for precise alignment. GlueNet-5RMs is the default configuration mostly.


\subsection{Ablation of Losses}
In order to analyze the impacts of different losses, we present their t-SNE map visualizations in Figure~\ref{fig:tsne_sup}. It is evident from the figure that using just the MSE loss is insufficient to align the features of the two models accurately. The reconstruction loss is crucial to maintain discrimination and avoid overfitting. In contrast, the adversarial loss does not seem to provide significant improvement in the figure. Through empirical study, we found that the adversarial loss only yields limited gains.


\begin{figure}[t]
\centering
\includegraphics[height=0.3\linewidth,width=0.8\linewidth]{  figs/Tsne-three-sup.pdf}
\caption{t-SNE~\cite{van2008visualizing} cross-model feature map visualization. }\label{fig:tsne_sup}
\end{figure}

We conducted a quantitative ablation study on a randomly selected subset of 5,000 images from COCO dataset. For alignment, we used T5-3B as the text encoder and aligned it with LDM Unet using our GlueNet. The finetuning of Unet is denoted by FT (Unet finetuning is only needed here). We report FID and CLIP scores in the following table for comparing image quality and image-text alignment. The testing data is inferred by DDIM with 200 steps, and the image size is 256 $\times$ 256. For a comprehensive analysis, the experiments were performed using three classifier-free guidances with $s$=1.5, 5, and 7.5, according to Eq. (\ref{eq:sampling}). Table ~\ref{tabs:ablation_loss} summarizes the results of our ablation study.

The first row of the table represents the direct combination of T5 and LDM, which yields nonsensical results due to severe misalignment. The second row reports results of T5+LDM trained from scratch. The finetuning of Unet in T5+LDM took 100 GPU days, whereas GlueNet training required only 5 GPU days. However, even with ten times the cost, its results were inferior to those of GlueNets'. By comparing the third, fourth, fifth, and sixth rows, we can easily conclude the superiority of the full-version model.


\input{  tabels/tab-ablation-ldm.tex}


\subsection{ {Text Encoders Analysis}}

Our proposed framework is compatible with a wide range of text encoders. In this subsection, we experimented with several language models as the text encoder, including Bert-base (110M parameters), Roberta-Large (355M parameters), Clip-text (123M parameters), T5-large (770M parameters), and T5-3B (2.8B parameters). These pre-trained text encoders were aligned with the Latent Diffusion Model (LDM) using our proposed model without requiring any finetuning of the LDM UNet. These plug-and-play models were evaluated on a subset of COCO consisting of 5,000 randomly selected samples. The FID and CLIP scores are reported in the table below for image quality and image-text alignment comparison. The GlueNets were trained on 18 million sentences sampled from the captions of Laion-400M. The testing data were inferred using DDIM with 200 steps, and the image size was set to 256Ã—256. To provide a comprehensive analysis, we conducted experiments on three classifier-free guidance settings with $s$=1.5, 5 and 7.5. The results are summarized in Table~\ref{tabs:ablation_textmodels}. As shown in the table, we observe that both FID and CLIP scores increase with the use of larger text models, which is consistent with the findings reported in Imagen~\cite{saharia2022photorealistic}. However, CLIP-text does not perform well in our experiments due to a larger domain gap with text-only models like Bert.


\input{  tabels/tab-ablation-textmodels.tex}

\subsection{ {Variant Token Lengths}}

GlueNet demonstrates strong ability in handling text of different lengths. The token number is fixed at 77 for both LDM and SDM. Our proposed GlueNet can handle variable length text encoders without any finetuning of the Unet model. To verify this, we conducted an experimental study, and the results are reported in Table~\ref{tabs:ablation_tokenlength}. In this experiment, we used Roberta-L and its tokenizers to encode text with maximum tokens of 77, 128, and 256, respectively. The guidance setting was 5, and other experimental settings were consistent with those in Table~\ref{tabs:ablation_textmodels}.

\input{  tabels/tab-tokenlength.tex}

\subsection{ {Data Sizes}}

Conditional generation is a challenging task, and the alignment between the text encoder and image Unet remains an open question. We have found empirically that replacing the text encoder can yield comparable results to the source model, but it requires significant effort to surpass it. The bottleneck may lie in the Unet architecture. In this section, we provide precise computations costs for different training set sizes in Table~\ref{tabs:gluenet_cost_data}. The benchmark was performed on COCO-5K using T5-3B to replace the LDM text encoder with GlueNet, consistent with previous experiments. As shown in the table, we observe that performance increases with both the size of the training set and the computation cost.

\input{  tabels/tab-ablation-textdata.tex}


\section{Monolingual (English) Text-to-Image Generation}
To demonstrate the generality of our proposed framework, we also replaced the CLIP text encoder of Stable Diffusion (v1-4) with T5-Large. As shown in Figure~\ref{fig:sdm_sup}, our model exhibits precise controllability and excellent visual quality compared to the standard Stable Diffusion model. However, it falls short of outperforming the original Stable Diffusion model due to minor mismatches.



\begin{figure}[t]
\centering
\includegraphics[height=0.8\linewidth,width=0.7\linewidth]{  figs/RMs-sup.pdf}
\caption{Monolingual generation (T5 + GlueNet + LDMUnet) of example prompts in 256 $\times$ 256 with guidance weight 7.5 and DDIM steps 200.  Both T5 and LDMUnet are pre-trained ones. We only train GlueNet with different model sizes to fulfill.}\label{fig:rm_sup}
\end{figure}



\begin{figure}[t]
\centering
\includegraphics[height=0.9\linewidth,width=0.8\linewidth]{  figs/Sdm-ours-sup.pdf}
\caption{Monolingual generation of example prompts in 512 $\times$ 512 with guidance weight 7.5 and 50 PLMS~\cite{liu2022pseudo} sampling steps.  }\label{fig:sdm_sup}
\end{figure}

\clearpage

\section{Multilingual Text-to-Image Generation}
The additional results in Figures~\ref{fig:multi_sup_fr}, ~\ref{fig:multi_sup_es},~\ref{fig:multi_sup_zh},~\ref{fig:multi_sup_ja} and~\ref{fig:multi_sup_it} help to verify the multilingual text-to-image generation capabilities of our proposed framework. Each language is associated with a dedicated GlueNet. To build an automated pipeline, the model should be able to select the appropriate GlueNet based on the detected language.



\section{Sound-to-Image Generation}
We provide additional visual results in Figures~\ref{fig:sound_to_image_1} and ~\ref{fig:sound_to_image_2}, comparing the vanilla GlueNet (without the re-weight objective) with the Adaptive GluNet (with the re-weight objective) as described in Section \ref{sec:cross-modal} of the main paper. These figures clearly show that the Adaptive GluNet improves the stability and accuracy of the generated images compared to the vanilla GlueNet, thus demonstrating the effectiveness of the objective re-weighting technique proposed in Section \ref{sec:cross-modal}.

\section{Multimodal-to-Image Generation}
According to Section \ref{sec:cross-modal} of the main paper, the fusion of multi-modal condition features from different encoders can be achieved through non-parametric operations, such as concatenating the top K signals and averaging the rest (excluding the last K). More multi-modal generation results are presented in Figure~\ref{fig:sound_text_to_image}. In our experiment, we used a text encoder (CLIPText) to extract the text embedding (with the input of "in painting style by Picasso"). We also inputted audio data to the AudioCLIP model, which was appended with a GlueNet to map it into an embedding. Then, we applied the proposed feature fusion operator, merging both text and sound embeddings, which enabled the stable diffusion model to generate reasonable results.


We have conducted an ablation study to determine the optimal value of K for fusing multi-modal condition features using the non-parametric operations described in Section \ref{sec:cross-modal} of the main paper. The results, which are presented in Figure~\ref{fig:singal_k_analysis}, show that when $K<=6$, the audio signals dominate the generation process. As $K$ increases, text signals gradually begin to appear and eventually dominate the conditioning when $K=10$. Therefore, to strike a good balance between the two cross-modal signals, we recommend selecting an appropriate value of $K$ based on empirical observations.


\begin{figure}[t]
\centering
\includegraphics[height=0.625\linewidth,width=\linewidth]{  figs/appendix/signal-fusion-k.pdf}
\caption{Analysis of top $K$ selected signals for multimodal (sound and text) feature fusion. This operation is  non-parametric (also training-free) which only needs concatenating top  $K$ token signals and averaging the rest. }\label{fig:singal_k_analysis}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[height=0.9\linewidth,width=0.8\linewidth]{  figs/appendix/sound2img-1.pdf}
\caption{Sound-to-image generation (Part 1/2) with vanilla and adaptive GlueNets.   }\label{fig:sound_to_image_1}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[height=0.9\linewidth,width=0.8\linewidth]{  figs/appendix/sound2img-2.pdf}
\caption{Sound-to-image generation (Part 2/2) with vanilla and adaptive GlueNets.   }\label{fig:sound_to_image_2}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[height=0.9\linewidth,width=0.75\linewidth]{  figs/appendix/sound-text-mix.pdf}
\caption{Multimodal-to-image generation (sound-text-mix) results. The condition encoders are ClipText and AudioCLIP+GlueNet. The two-modality features are fused to Stable Diffusion Unet to generate such right-side result.  }\label{fig:sound_text_to_image}
\end{figure}




\begin{figure}[t]
\centering
\includegraphics[height=0.78\linewidth,width=0.9\linewidth]{  figs/appendix/french.pdf}
\caption{Multilingual generation results in 512 $\times$ 512  of XLM-Roberta + GlueNet + Stable Diffusion Unet (v1-5) with the French captions. The three results are generated with different random noises.   }\label{fig:multi_sup_fr}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[height=0.78\linewidth,width=0.9\linewidth]{  figs/appendix/spanish.pdf}
\caption{Multilingual generation results in 512 $\times$ 512  of XLM-Roberta + GlueNet + Stable Diffusion Unet (v1-5) with the Spanish captions. The three results are generated with different random noises.   }\label{fig:multi_sup_es}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[height=0.78\linewidth,width=0.9\linewidth]{  figs/appendix/chinese.pdf}
\caption{Multilingual generation results in 512 $\times$ 512  of XLM-Roberta + GlueNet + Stable Diffusion Unet (v1-5) with the Chinese captions. The three results are generated with different random noises.   }\label{fig:multi_sup_zh}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[height=0.78\linewidth,width=0.9\linewidth]{  figs/appendix/japanese.pdf}
\caption{Multilingual generation results in 512 $\times$ 512  of XLM-Roberta + GlueNet + Stable Diffusion Unet (v1-5) with the Japanese captions. The three results are generated with different random noises.   }\label{fig:multi_sup_ja}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[height=0.78\linewidth,width=0.9\linewidth]{  figs/appendix/italian.pdf}
\caption{Multilingual generation results in 512 $\times$ 512  of XLM-Roberta + GlueNet + Stable Diffusion Unet (v1-5) with the Italian captions. The three results are generated with different random noises.   }\label{fig:multi_sup_it}
\end{figure}