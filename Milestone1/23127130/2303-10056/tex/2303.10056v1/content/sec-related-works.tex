\section{Related Work of Text-to-Image Models}
Generative Adversarial Nets (GANs) \cite{Goodfellow2014GAN} is one of the major frameworks in text-to-image generation. The method in \cite{reed2016generative} was an early stage approach to use a GAN to train a text-to-image generation network. Since then, other GAN based methods, \eg,   Stack-GAN \cite{han2017stackgan}, Attn-GAN \cite{xu2018Attngan} and SD-GAN \cite{Yin2019SDGAN}, have obtained promising results. In addition, recent works showed more improvements on the generation quality. DM-GAN \cite{zhu2019dmgan} improved text-to-image performance by introducing a dynamic memory component. DF-GAN \cite{tao2022dfgan} designed a fusion module to fuse text and image features. LAFITE \cite{zhou2021lafite} took advantage of CLIP's model to construct pseudo image-text pairs, and proposed a GAN model to learn text-image pairs.

Auto-regressive transformer is another major framework in text-to-image generation. DALL-E \cite{ramesh2021zero} and CogView \cite{ding2021cogview} adopted an autoregressive transformer \cite{Vaswani2017Transformer} to train the correspondences between visual tokens and text tokens. Parti \cite{yu2022scaling} used a powerful visual encoder, ViT-VQGAN \cite{Yu2022VitVQGAN}, to improve results upon DALL-E. The method in \cite{gafni2022make} is similar to CogView and DALL-E, while introducing additional controlling elements to improve the tokenization process. N\"{U}WA \cite{wu2022nuwa} and NUWA-infinity \cite{wu2022nuwainfinity} trained autoregressive visual synthesis model to support both text-to-image and text-to-video generation tasks.

Concurrently, diffusion models \cite{sohl2015deep,song2019generative,song2020score} became a main research focus. In such methods, noise is added to an image, and a score network is trained to denoise and recover the input image. GLIDE \cite{nichol2021glide} performed guided inference with and without a classifier network to generate high-fidelity images. DALL-E 2 \cite{ramesh2022hierarchical} and Imagen \cite{saharia2022photorealistic} set new state-of-the-art results in the text-to-image generation. In DALL-E 2, a prior to produce CLIP image embeddings conditioned on text was learned, and a diffusion model was used to decode the image embeddings to an image. In Imagen, a large-scale frozen text encoder T5-XXL \cite{raffel2020exploring} was adopted to generate embeddings, and a cascade of conditional diffusion models was used to map these embeddings to images of increasing resolutions. Latent diffusion model and stable diffusion model \cite{rombach2022high} are state-of-the-art methods that apply the diffusion model on the latent embedding space as in \cite{sinha2021d2c,vahdat2021score}. These methods are computationally friendly while achieving impressive results. To incorporate more conditions for controllable content generation, \cite{zhang2023adding,mou2023t2i,brooks2022instructpix2pix} introduced additional parameters with new data to inject external knowledge within the current pre-trained model. \cite{chen2022altclip} tried to alter the language model for more capacities. However, all these models are designed to adapt text and image (or similar 2d grid) conditions without the attempt for sequential signals such as audio.
