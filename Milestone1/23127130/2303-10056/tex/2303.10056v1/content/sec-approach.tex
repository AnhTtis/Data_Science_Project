\section{Preliminary}
\vspace{-1mm}
\subsection{Latent Diffusion Model}
The Latent Diffusion Model (LDM)~\cite{rombach2022high} and its extension, \ie, Stable Diffusion\footnote{https://github.com/CompVis/stable-diffusion}, which are variants of Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising} family, are selected as our baseline models due to their excellent balance in efficiency and visual quality. In general, LDM is composed of two stages. Firstly, there is an auto-encoder (AE) pre-trained by enormous images with the regularization in either KL-divergence or vector quantization~\cite{van2017neural,agustsson2017soft}. An encoder network, \ie, $E$, is applied for latent feature extraction as $z=E(x)$, which can be mapped back to image space by a decoder network. The input images $x$ and reconstructed images $\hat{x}$ are almost identical $x \approx \hat{x}$. 

Secondly, LDM trains a diffusion model in the latent space~\cite{sinha2021d2c,vahdat2021score}. It follows the standard DDPM~\cite{ho2020denoising} with denoising loss and uses U-net~\cite{ronneberger2015u} as the image decoder as in ~\cite{song2019generative}. To enable generative controllability, LDM has applied multiple conditioning signals ($y$) such as text, mask, or layout, encoded aside and injected into the U-net, with the help of cross-attention layers. This can be formulated as:
\begin{equation}
    \mathcal{L}_{LDM} := \mathbb{E}_{z\sim E(x), \varepsilon \sim N(0,1), t, y} \left [ \left \| \varepsilon - \varepsilon_{\theta}(z_t,t,c_{\phi}(y)) \right \| \right ],
\end{equation}
where $t$ represents the time step, $z_t$ is the noise corrupted latent tensor at time step $t$, and $z_0 = E(x)$. $\varepsilon$ is the unscaled Gaussian noise, $c_{\phi}$ is the conditioning network parameterized by $\phi$ and $\varepsilon_{\theta}$ is the Unet-like denoising network (\aka, image decoder). The parameters of both conditioning and denoising networks, \ie, ${\theta, \phi}$, are trained by the LDM loss. During inference,  clean images can be generated via  classifier-free guidance~\cite{ho2022classifier} as:
\begin{equation}
\hat\varepsilon_{\theta}(z_t|y) = \varepsilon_{\theta}(z_t) + s \cdot (\varepsilon_{\theta}(z_t, c_{\phi}(y)) - \varepsilon_{\theta}(z_t)),
\label{eq:sampling}
\end{equation}
where $s$ is the guidance weight to balance text controllability and image fidelity. 

The basic T2I LDM model is trained on Laion-400M~\cite{schuhmann2021laion} dataset. The Stable Diffusion Model (SDM) was trained with more epochs and training data, \ie, Laion2B-en and Laion-high-resolution~\cite{schuhmann2022laion}. 

\input{  figs/fig-method.tex}

\subsection{Condition Encoder}
LDM uses  Bert-like~\cite{devlin2018bert} encoder as the text encoder jointly trained with the image decoder using the DDPM loss. SDM uses a pre-trained CLIP~\cite{radford2021learning} text encoder frozen during training. It has been found that increasing the size of the text encoder is very helpful for performance~\cite{saharia2022photorealistic}. In order to upgrade existing models, our goal is to be able to efficienlty plug in more advanced language condition encoders such as T5-3B~\cite{raffel2020exploring}, AudioClip~\cite{guzhov2022audioclip}, or XLM-Roberta~\cite{devlin2018bert}.



\section{GlueNet}
\label{sec:method}
\vspace{-1mm}



The text encoder is one of the key components of T2I models, as generation requires precise and fine-grained text embedding for guidance. The overall performance can be greatly boosted by increasing the size of the text encoder~\cite{saharia2022photorealistic}, however, upgrading an existing text model to a more powerful one is challenging. 
Because existing T2I models are not modular, 
directly replacing (or augmenting with) another condition encoder does not work. The key technical challenge is the mis-alignment between the new condition encoder and the old image decoder. Moreover, joint finetuning also falls short due to catastrophic forgetting occuring when updating well trained parameters. 

Considering these two different condition encoders as source and target domains, we apply a neural network to learn to align. As shown in Fig.~\ref{fig:setting}, this paper has presented a general framework called GlueGen.  Within the framework, GlueNet works as an additional module to bridge the new language model and the old image decoder.


\subsection{Objectives}


\red{Given sets of parallel corpus $\mathcal{X}^s = \{x_{ij}^s|i=1,...,M^s, j=1,...,N^s\}$  and $\mathcal{X}^t = \{x_{ij}^t|i=1,...,M^t, j=1,...,N^t\}$ with $M$ as length of tokens and $N$ as total quantity,  the source model ${F}^{s}$ and a target encoder ${F}^{t}$ have mapped the raw data into embeddings denoted as $\mathcal{S} = \{s_{ij}|i=1,...,M_s, j=1,...,N\}$ where $s_{ij} = F^{s}(x_{ij}^s) \in \mathbb{R}^{d_{s}}$ and  $\mathcal{T} = \{t_{ij}|i=1,..,M_t, j=1,...,N\}$ where $t_{ij} = F^{t}(x_{ij}^t) \in \mathbb{R}^{d_{t}}$. Due to the different models, there is severe distribution mismatch between the source and target features $p(s) \neq p(t)$.}
The  GlueNet is an autoencoder whose encoder $M(\cdot, \Theta_{M})$ learns to map the new features from a source language model, \red{\ie, $s \in \mathcal{S}$,} to align with the current target encoder, \red{\ie, $t \in \mathcal{T}$, where $p(M(s, \Theta_{M})) \approx p(t)$}. \red{Therefore,} this translation (\ie, ${F}^{s} \overset{\tiny{\Theta_{M}} }{\longrightarrow}
 {F}^{t}$ ) enables the image generator to understand the new  features coming from the new condition model without finetuning. To achieve this aim, we consider it as a domain adaptation~\cite{ganin2016domain,qin2019pointdan,long2018conditional} problem and apply both element-wise and distribution-wise alignment losses, which includes the minimization of the mean square error (MSE) loss, \ie, $\mathcal{L}_{mse}$, and the adversarial loss, \ie, $\mathcal{L}_{adv}$, measured over a discriminator network, \ie, $D(\cdot, \Theta_D)$:
\begin{align}
\mathcal{L}_{mse}( \Theta_M) & := \mathbb{E}_{x_t \sim X_t, x_s  \sim X_s}[||{F}^{t}(x_t)- \notag \\  & M(F^{s}(x_s), \Theta_M)||_{2}^{2}], \label{eq:loss-mse} \\
\mathcal{L}_{adv}( \Theta_D, \Theta_M) & :=  \mathbb{E}_{x_t  \sim X_t}[\log{D({F}^{t}(x_t), \Theta_D)}] \notag \\ + & \mathbb{E}_{x_s  \sim X_s}[\log{1-D(M({F}^{s}(x_s), \Theta_M), \Theta_D)}], \label{eq:loss-adv}
\end{align}
where $X_s$ and $X_t$ denote the source and target inputs respectively and they can be the same prompts in English (\eg, LDM $\rightarrow$ T5-3B~\cite{raffel2020exploring}) or in bilingual but parallel content for multilingual T2I or audio-label pairs.

Some insightful investigations in transfer learning reveal that the cross-domain alignment will inevitably decrease feature discrimination~\cite{pmlr-v97-chen19i,cui2020towards}. To project the rich semantics necessary for model upgrading, we further apply a decoder network, \ie,  $N(\cdot, \Theta_N)$, for feature reconstruction:
\begin{align}
    \mathcal{L}_{rec}&(\Theta_M, \Theta_N)  :=  \notag \\ &\mathbb{E}_{x_s  \sim X_s} [||x_s- N(M(F^{s}(x_s), \Theta_M), \Theta_N)||_{2}^{2} ]. \label{eq:loss-rec}
\end{align}

 GlueNet is trained in an end-to-end manner and the details of which  can be referred to in Appendix and Sec.~\ref{sec:experiments}. \cite{ma2022principles} introduced that a universal learning engine should seek a compressive closed-loop transcription with good property in structure-preserving. Our cross-model mapping also requires a similar functionality.  GlueNet desires stability after a loop whose input and output should keep almost equivalent, \ie, $x \approx \hat{x} \approx \hat{\hat{x}} \approx ... $, due to the constraint of reconstruction loss. Therefore, inputting the reconstructed data $\hat{x}$ to  GlueNet is expected to be consistent with the previous loop. This is helpful for accelerating  GlueNet training for T5-to-LDM adaptation.




\subsection{Model Architecture}
\label{sec:arch}
The input data to alignment can be represented as the sequence of tokens: $\mathrm{X}_{i,*} = \{x_{i,1}, x_{i,2}, ...,x_{i,S}\}$ where $\mathrm{X}_{i,*} \in \mathcal{X}$ and ${ x_{i,j} \in \mathbb{R}^{C}}$, with sequence length $S$ and token dim $C$. Inspired by the methods of image super-resolution~\cite{lim2017enhanced,ahn2018fast} to densely regress target data under different input-output dimensions, the translator network stacks three sub-nets in sequence, as seen in Fig.~\ref{fig:method} (c). The head net is only used for simple feature transformation. The body net has many residual modules for fine-grained representation learning. The tail network is appended and is mainly employed for dimension conversion to match the target tensors. Every residual module is implemented based on the MLP-based token mixer~\cite{tolstikhin2021mlp,ma2021rethinking} with high efficiency and strong ability in representation learning. Within the MLP-mixer block, it consists of a Token Mixer and a Sequence Mixer for orthometric purposes. The Token Mixer learns the representation of each token with shared MLP, and the Sequence Mixer is designed for learning the same channel in sequential tokens:
\begin{align}
 \mathrm{U}_{\cdot,i} = & \mathrm{X}_{\cdot,i} + \mathrm{W}_{2}\sigma(\mathrm{W}_{1} * \mathrm{LN}_{\gamma_1, \beta_1}(\mathrm{X})_{\cdot,i}), i=1,...,C \\
  \mathrm{X}_{j,\cdot} = & \mathrm{X}_{j,\cdot} + \mathrm{W}_{4}\sigma(\mathrm{W}_{3} * \mathrm{LN}_{\gamma_2, \beta_2}(\mathrm{U})_{j,\cdot}),  j=1,...,S
\end{align}
where $\mathrm{X}_{\cdot,\cdot}$ is the feature, $\mathrm{LN}_{\gamma, \beta}$ denotes layer normalization parameterized by $\gamma$ and $\beta$, and $\sigma$ indicates the non-linear operator such as GELU~\cite{hendrycks2016gaussian}. The $\mathrm{U}_{\cdot,i}$ is the outputs of Seq MLP and $\mathrm{W}$ represents the weight matrix within the MLP mixer. 


\subsection{Cross-modality Alignment}
\label{sec:cross-modal}
\input{  figs/fig-clip-singnals}
GlueNet provides a way to align the cross-modal representations between the text encoder, CLIPText~\cite{radford2021learning}, with the AudioClip~\cite{guzhov2022audioclip} model that maps sound signals into embedding spaces. This alignment is useful for extending the functionality of diffusion models like Stable Diffusion, which only takes text signals as input. However, achieving this alignment is challenging because the length of the tokens in each model is different. CLIPText's embeddings require 77 tokens in a sequence, while AudioClip maps sound signals to a single token with a sequence length of 1. Using the standard GlueNet directly fails in this case.

During our exploration, we found that condition information is not uniformly distributed across all 77 tokens of CLIPText outputs. The top K tokens contain the majority of the information. We computed an average dissimilarity map over the CLIPText tokens, visualized in Fig.~\ref{fig:signal-clip} (a), and observed a prominent gap around the eighth or ninth token, with the later ones being highly similar. We also found that corrupting the last token embedding with some random noise would not degrade the generative results. Therefore, it can be concluded that the contribution of different tokens decreases progressively and could be estimated by the feature distance with the last token, $w_{\cdot,j} = dist(s_{\cdot,j}, s_{\cdot,M})$, as shown in Fig.~\ref{fig:signal-clip} (b). We take the l2 distance as $dist(\cdot, \cdot)$ and use this value to reweight the GlueNet objective over different tokens for the sound-to-image generation.

Cross-modal signals fusion is another contribution. To do this, we select the top K tokens of each modality output from GlueNet. Then, the average of the remaining later tokens (excluding last K tokens) of two modality features is concatenated with the top ones. This is a non-parametric operation without any training.  K is empirically chosen from 4 to 8 (or higher). For more details, please refer to Appendix. 

\vspace{-3mm}

