\section{Introduction}

Text-to-image (T2I) generative models have made great progress in the last few years thanks to algorithmic advances and the availability of large-scale paired training datasets
~\cite{ramesh2022hierarchical,yu2022scaling,rombach2022high,schuhmann2021laion,schuhmann2022laion}. Diffusion-based T2I generative models in particular 
have achieved remarkable results in terms of image quality
~\cite{ho2022classifier,nichol2021glide,balaji2022ediffi,saharia2022photorealistic,mou2023t2i,zhang2023adding}. 
Despite these strong results, controllable generation for these methods is still challenging: generated images are often not faithful to the captions, compositional capabilities are lacking, and prompt engineering is often required to achieve the desired results~\cite{dall-e-prompt-book}. Moreover, most large-scale models have only been trained on English text captions, greatly limiting their use across the world.

\input{    figs/fig-demo-results}

Recent research has emphasized the crucial role of text encoders in improving Text-to-Image (T2I) models' performance, and their ability to comprehend and represent text is considered a bottleneck for image generation~\cite{saharia2022photorealistic, croitoru2022diffusion}. However, the current T2I models' text encoders are often trained on short image captions, which limits their performance on complex prompts and challenges their quality of feature extraction~\cite{rombach2022high}. Furthermore, T2I models' capacities are limited to generating images from text, and they cannot incorporate multimodal conditions such as sound and audio easily. Nevertheless, replacing the text encoder in existing T2I models is challenging since the text encoder and image generator's representation spaces are tightly coupled~\cite{rombach2022high, ramesh2022hierarchical}. This severe domain gap between the new conditions and the existing model impedes the image generation's final performance, and training the entire T2I model from scratch, with higher quality image-caption pairs, would be prohibitively expensive~\cite{edwards_2022}.\footnote{The cost of training a Stable Diffusion model is around 600K USD.}


As seen in Fig.~\ref{fig:demo-resuls}, we propose GlueNet to address the challenge of efficiently replacing or upgrading the text encoder in existing diffusion-based T2I models. With GlueNet, off-the-shelf pre-trained language models and multimodal encoders can be easily aligned with image encoders of T2I models, greatly enlarging their functionalities at a low cost. Importantly, this can be achieved without requiring retraining from scratch or even finetuning, maintaining the representation alignment between the text and image encoders. The proposed method follows an encoder-decoder structure. The encoder of GlueNet first aligns the representation space of the new condition encoder with that of the T2I model's image generator, minimizing both element-wise and global-wise discrepancy. Then, the decoder of GlueNet maps the aligned condition representations back to the original representation space of the new condition encoder by minimizing the reconstruction loss, preserving rich semantics captured by the pre-trained model during alignment training. Align existing models would inevitably decrease feature discriminability~\cite{pmlr-v97-chen19i,cui2020towards}, which makes the feature decoder necessary.
The entire training of GlueNet requires only a parallel corpus with the same content but different modalities or languages. At inference time, only the encoder of GlueNet is applied on top of the new condition encoder for representation alignment.



To verify the effectiveness of the proposed framework, we conducted three major experiments ranging from single- and multi-modal encoders. Firstly, we upgraded the existing text encoders of the Latent Diffusion Model~\cite{rombach2022high} using a stronger language model, T5-3B~\cite{raffel2020exploring}. Our model showed competitive improvements in FID score and user study ranking compared to the baselines but it still required finetuning for the overall performance boost. Secondly, we aligned a multilingual language model, XLM-Roberta-L~\cite{conneau2019unsupervised}, using our approach, enabling multilingual text-to-image generation. It achieved competitive results of translation-based models under a significantly lower training cost. Finally, we demonstrated GlueNet's capability to bring new functionalities beyond text signals into existing T2I models. 
The alignment of the AudioClip~\cite{guzhov2022audioclip} encoder enables sound-to-image generation without requiring any parameter finetuning of the image generator. This new capability allows the existing Stable Diffusion model to generate high-quality images that correspond to sound signals such as dogs barking and street music. This new capability goes beyond the traditional T2I generation and opens up new possibilities for creating multimedia content towards X-to-Image (X2I) generation.

\input{    figs/fig-intro-modules.tex}

Our contributions can be summarized as follows:

\begin{itemize}
\item To the best of our knowledge, this is the first work to consider the problem of efficiently aligning a pre-trained audio model with a pre-trained T2I diffusion model for sound-to-image generation.

\item  Extensive experiments on text-to-image generation benchmarks demonstrate the superiority of our model over the baseline LDM method on both image quality and language controllability.

\item Our framework also enables text-to-image generation beyond English prompts without the need of multilingual image-text pairs for retraining.

\end{itemize}