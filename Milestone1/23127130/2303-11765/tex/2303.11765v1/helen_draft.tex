% \subsection{Perception Systems \& Human Demonstrations}
% The perception system uses  RGBD data from a Microsoft Azure kinect camera and the RGB data generated by GelSight v1.5 optical tactile sensors.

% \subsubsection{Vision System}
% The RGBD data from the Kinect is used to create the corresponding point cloud, which we parse to detect the various types of fixtures -  including the start position, vertical pegs, channels/slots and the connector on the taskboard as well as their positions and orientations. Specifically, we use methods like Agglomerative clustering to determine the locations of the fixtures, and PCA to determine type and orientation.  \todo{helen}

% The human demonstration consists of an RGBD image of the taskboard with the cable routed and fully assembled into the goal configuration. The manually assembled cable is then detected from the demonstration data. We move along the cable from the previously identified start position towards the connector, thereby assigning an order to each of the fixtures. Thus we estimate the goal cable configuration, taskboard configuration and order of traversal from a single human demonstration.

%todo: talk about how we do human demonstration

% -----
% \begin{figure}[h]
%     \centering
%     \includegraphics[width=\linewidth]{figures/visionpipeline_examples.png}
%     \caption{{\bf Output Examples of Vision Pipeline for State Estimation.} Here we show the recovered 3D bounding boxes of the fixtures given the 2 RGB-D images of the peg board, one with human demonstration and one without. We use color filtering to determine the fixtures' position and type, Principal Component Analysis (PCA) to determine the orientation and shape, and Coherent Point Drift (CPD) to determine the equation of the cable, which then allows us to determine the fixture order and fine-tune on the orientation. The coordinates in the figure are with respect to the base of the robot, and for the purposes of our task, we round the orientation angle to the nearest 90 degrees.\wenzhao{the bounding box can be thicker and more salient.} \helen{yeah i'll do this in post after we finalize on a figure here}}
%     \label{fig:vision_pipe}
% \end{figure}


% \subsubsection{Vision Pipeline for Board State Estimation}
\subsubsection{Visual perception}
\label{sec:visual_perception}
The visual perception module requires an assembled task board from human demonstration, and infers the goal configuration of the cable.
Given a point cloud from the Kinect camera, we aim to recover the type, position, orientation and ordering of each fixture on the task board. We also track the demonstration cable in the image to parse the order of the fixtures along it. The task parsing section in Fig.~\ref{fig:pipeline} shows an example input-output pair of the visual perception module.

\textbf{Fixture pose estimation:} We first use a point cloud of the peg board with the fixtures but without the cable. We locate all fixtures via simple color filtering and clustering. We apply Principal Component Analysis (PCA) to estimate the type (using the shape, or oblongness) and orientation of each detected fixture. The ``start" and ``connector" fixtures needs their orientation ambiguity to be resolved, so we use the cable heading direction to infer their orientations. Specifically, we take the point from the cable leading up to the given fixture and the current point of the cable the fixture is on to calculate the directional vector.


% \textbf{Fixture Pose Estimation:},\wenzhao{Simplify further if possible} we use a point cloud of the peg board with the fixtures, with no cable. We get the locations of all of the fixtures by using simple color filtering and clustering. We use Principal Component Analysis (PCA) to estimate the shape (oblongness) and orientation of each of these detected fixtures. To ensure that one of the axis vectors is always perpendicular to the pegboard, we first fit a best-fit plane to the point cloud to recover a  parametric equation corresponding to the peg board. We then project the points of each fixture onto the taskboard plane. If one of the principal components is significantly larger than the other, then we deduce that the fixture is rectangular. We use the relative orientation of the largest principal components of the fixtures and the taskboard to recover the orientation However, since the "start" and "connector" fixtures require direction for the general orientation, we use the cable information to retrieve their orientations; we take the point from the cable leading up to the given fixture and the current point of the cable the fixture is on and calculate the vector. 

\textbf{Cable state estimation:} We track the cable in the demonstration board to get the correct order of the routing task. To segment the points corresponding to the white cable, we take all of the white-colored points from the point cloud from the second RGBD image defining the goal cable configuration. The resulting cable point cloud has occlusions created by the fixtures, so we use Reeb Graph \cite{schulman2013tracking} to construct a set of cable nodes. This gives an initialization for Coherent Point Drift (CPD) \cite{myronenko2010point} to complete the cable. With the completed sequence of cable nodes, the order of the fixtures are readily determined by tracing the cable. This cable node sequence also disambiguates the orientation of the ``start" and ``connector" fixtures, as described in the paragraph above. 

Using this visual perception pipeline, the type, location, orientation and order of the fixtures are estimated, which defines the routing and assembly task. This task description then serves as the input for the motion primitives introduced in Section~\ref{sec:primtives}.


% Steps:
% - clustering and color filtering to get the fixtures and cable
%     - this resulting cable has occlusions from the fixtures
%     - only get points that correspond to above the peg board plane because there is a lot of noise surrounding the peg board, 
%         - like table, etc
% - PCA to identify the type of fixtures and get orientation of the slot fixtures
% - use Reeb and CPD to complete the cable and get the cable equation
% - use the result from CPD to get the order of fixtures & orientation of the start and end fixtures

% Things to put:
% - generalizes well (maybe in experiments, accuracy etc)
% - does not need to assume locations and types of fixtures are given




% We use color filtering and clustering to get the positions of the fixtures. It is assumed that the "start" and "slot" fixtures are red, and the "peg" and "conn" fixtures are yellow. 




