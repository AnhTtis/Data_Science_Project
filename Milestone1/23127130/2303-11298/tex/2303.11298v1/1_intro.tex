\section{Introduction}
\label{sec:intro}

Humans tend to overestimate their abilities, a cognitive bias known as Dunning-Kruger
effect~\cite{Kruger1999UnskilledAndUnaware}. Unfortunately, so do deep neural
networks. Despite impressive performance on a wide range of tasks, deep learning models tend to be overconfident---that is, they 
predict with high-confidence
even when they are wrong~\cite{GuoICML17OnCalibrationOfModernNNs}. This effect is even more severe under domain shifts, where models tend to underperform in general~\cite{ovadia2019can, RechtICLR19DuImageNetClassifiersGeneralizeToImageNet, HendrycksICLR19BenchmarkingNNRobustnessCommonCorruptionsPerturbations}.

\input{splash_fig}
While these vulnerabilities affect deep models in general, they are often studied for classification models and are comparably less explored for semantic segmentation, a fundamental task in computer vision that is key to many
% semantic image image segmentation but when
% it comes to specific tasks, they have been mostly studied for simple case studies such as classification.
%
% In this work, we put segmentation image segmentation models under the lens, studying them both in terms of \textit{robustness} and \textit{reliability} under domain shifts. 
% Semantic image segmentation is a fundamental task in computer vision,
% leveraged in 
critical applications such as autonomous driving and
AI-assisted medical imaging. In those applications, domain shifts are more the rule than the exception (\eg, changes in weather for
a self-driving car or differences across patients for a medical imaging system).
Therefore, brittle performance and overconfidence under domain
shifts are two important and challenging problems 
% \rict{that undermine the \textit{reliability}
% ~\cite{TranX22PlexReliability} 
% of machine learning systems and we need} to address \rict{them} 
to address
for a
safe deployment of 
% such
artificial intelligence
systems in the real world.

With that in mind, we argue that a \textit{reliable} model should 
% \textit{1)} 
\textit{i)}
be robust to domain shifts and 
% \textit{2)} 
\textit{ii)}
provide good uncertainty estimates. The core goal of this study is providing an answer to the following, crucial question:
\textbf{are state-of-the-art semantic segmentation models improving in terms of robustness \textit{and} uncertainty estimation?}

To shed light on this, we evaluate a large body of segmentation models, assessing their in-domain (\id) \vs out-of-domain (\ood) prediction quality (\textbf{robustness}) together with their calibration, misclassification detection and \ood detection (\textbf{uncertainty estimation}).

We argue that a study of this kind is crucial to understand whether research on semantic segmentation is moving in the right direction.
%
Following the rise of transformer architectures in computer vision~\cite{DosovitskiyICLR21AnImageIsWorth16x16WordsTransformersAtScale, touvron2021training, carion2020end,LiuICCV21SwinTransformerHierarchicalViTShiftedWindows},
several studies have compared recent self-attention and CNN-based \textit{classification} models in terms of robustness \cite{BhojanapalliICCV21UnderstandingRobustnessTransformersImageClass, naseer2021intriguing, bai2021transformers, paul2022vision, mao2022towards, LiuCVPR22AConvNet4The2020s} and predictive uncertainty \cite{minderer2021revisiting, pinto2022impartial}. Yet, when it comes to \textit{semantic segmentation}, prior studies \cite{XieNIPS21SegFormerSemSegmTransformers, zhou2022understanding} only focused on robustness, using synthetic corruptions as domain shifts (\eg, blur, noise) \cite{KamannCVPR20BenchmarkingRobustnessSemSegmModels}. In contrast, we 
% study 
consider
natural, realistic domain shifts and study segmentation models both in terms of robustness and uncertainty, leveraging datasets captured in different conditions---see \cref{figure:splash} (bottom). 

% Although improvements in classification and segmentation often go hand in hand, we argue a thorough study focusing on segmentation models is warranted since different trends may arise. 
Task-specific studies are important, since task-specific architectures and learning algorithms may carry different behaviors and some observations made for classification might not hold true when switching to segmentation.
For instance, contrary to Minderer~\etal \cite{minderer2021revisiting}, we observe that improvements in calibration are far behind those in robustness, see \cref{figure:splash} (top). Furthermore, previous analyses only
consider simple 
% temperature scaling 
calibration approaches~\cite{GuoICML17OnCalibrationOfModernNNs}
% to improve 
while assessing
model reliability;
% meanwhile, 
in contrast,
we make a step forward and
explore content-dependent calibration
strategies~\cite{gong2021confidence, ding2021local}, which show promise to improve reliability out of domain.

% This allows us individuating important research directions to improve
% reliability of segmentation models when deployed out of domain.
% By clarifying 
Our analysis allows us individuating
in which directions 
% are we 
we are
improving and in which 
% do we lag behind.
we are lagging behind.
% our study can
% To the best of our knowledge, this
This is the first work to systematically study robustness
and uncertainty under domain shift for a large suite of segmentation models and
we believe it
% this will
can
help 
% both 
practitioners and researchers 
% building on the state of the art of 
working on
semantic segmentation. 
% Our main observations are:
We summarize our main observations in the following.


\myparintro{i) Remarkable improvements in robustness, but poor in calibration} Under domain shifts, recent segmentation models perform significantly better (in terms of mIoU)---with larger improvements for stronger shifts. Yet, \ood calibration error increases dramatically for all models.

\noindent
\textit{\textbf{ii) Content-dependent calibration~\cite{ding2021local} can improve \ood calibration}}, especially under strong domain shifts, where models are poorly calibrated.
% These are even more effective when they have access to \ood images during calibration. 
%\todo{Mention proof?}

\myparintro{iii) Misclassification detection shows different 
% behavior 
model ranking
in and out of domain} 
% \rict{Perhaps surprisingly, } 
When tested in domain, recent models 
% actually 
underperform the ResNet baseline. As the domain shift increases, 
% more 
recent models take the lead.

\myparintro{iv) \ood detection is inversely correlated 
% to 
with
performance} 
% When it comes to \ood detection, 
% In this task, 
Indeed,
a small ResNet-18 backbone performs best.

\myparintro{v) Content-dependent calibration~\cite{ding2021local} can improve \ood detection and misclassification out of domain} We observe a significant increase in misclassification detection under strong domain shifts after improving calibration. We also observe improvements for \ood detection, albeit milder.


% \todo{Make fig/table caption more self-consistent by adding title/bold sentences}

% \todo{use ood/id instead of \ood/in-domain throughout text}

\input{tables/meta_studies_small}

