% \todo{Uniform: "domain" to "domain"}

\section{Are modern segmentors more \textit{reliable}?}

% The first axis of this study is the robustness of segmentation models
% to natural domain shifts. To assess this, we take a large body of CS-trained
% segmentation models and test them on IDD and ACDC.
% %
% The second axis is assessing uncertainty in segmentation models, especially out of domain. In particular, we focus on three different uncertainty estimation tasks: calibration, misclassification and \ood detection.
In the following, we present the main findings of our study on the reliability of semantic segmentation models. 
% under natural distribution shifts.

% \subsection{Are modern segmentors more robust?}
\subsection{Robustness}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{Figures/miou_vs_ece.pdf}
\caption{
% Expected calibration error $(\downarrow)$  \vs mIoU error $(\downarrow)$  
\textbf{Expected mIoU error $(\downarrow)$ \vs calibration error $(\downarrow)$}
before and after \ts for different model families (top and bottom, respectively). All models trained and calibrated on CS. Markersize proportional to number of parameters. Notice how \ts yields marginal \textit{relative} gains in \ood settings (IDD and ACDC).
}
\label{figure:miou_vs_ece}
% \vspace{-5pt}
\end{figure}

Since the domain shifts we consider
are natural and not synthetically induced, there is no straightforward way to evaluate 
their strength. To this end, we establish an 
ordering for the severity of the shift
based on performance degradation of the ResNet baselines (DLV3+R101 and UPNetR101), which results in
% :
CS $<$ IDD $<$ ACDC.
% Our 
This aligns with a
qualitative evaluation 
% based on visual inspection 
of 
% the images 
the different datasets
% also 
% aligns with 
% suggests this ordering
(see \cref{figure:splash}, bottom, for a few samples). 
In \cref{figure:splash} (top left) we present the mIoU error (\ie, $100 - \textrm{mIoU}$) for several models evaluated on the three datasets. 
% To illustrate the changes in performance from CS, 
To highlight the loss in performance as we increase the shift,
we normalize 
all
errors 
% by
\wrt
the best performing CS model (ConvNeXt).
The trend is clear: the larger the domain shift, the larger the improvement brought by more recent segmentation models.
% From the error trend as we increase the domain shift, it is clear that recent segmentation models have better performance 
% % under natural 
% % % covariate 
% % domain
% shifts, moreover, the gap grows larger with the strength of the shift. 

% A more detailed result is showed 
We expand on this
in \cref{figure:miou_vs_ece} (top), where we plot the mIoU error (on the x-axis) \vs the calibration error for all 
% models. 
the models belonging to the different families (Sec.~\ref{sec:arch}).
The size of the markers is proportional to the number of parameters. Similarly to \cref{figure:splash}, we observe that the gap in mIoU between ResNet baselines and recent models grows larger as we increase the domain shift (\cf marker position on the x-axis).
% Even more remarkable,
Interestingly,
two of the models that perform best under ACDC's 
strong
% \rict{hard}
shift (SETR and Segmenter) are not significantly better than the ResNet baseline on the training domain (CS). This
% would indicate 
indicates
that, in our setting, 
% stronger \id performance does not directly link to better robustness.
only assessing \id performance can hide the real value of 
newly crafted models and, hence, it is important to evaluate architectures
out of domain in order to fully grasp their potential.
% Perhaps this is due to the fact that CS is not complex enough compared to large datasets used for pre-training recent classification models.
%
% Moreover, between transformers and CNNs, 

While there is no 
% a 
single model family that performs significantly better in all datasets,
% All in all, our observation is
we can reach the clear conclusion
that \textit{all recent models are significantly more robust than well established baselines under natural shifts}.



\subsection{Calibration error}

\myparagraph{Off-the-shelf calibration} In \cref{figure:splash} (top-right) we present the ECE for different models as we increase the domain shift. Similarly to the mIoU error, ECE is normalized by the 
% best calibrated model on CS, this way the increase in relative error in ECE and mIoU is comparable. 
best CS model (in terms of calibration).
Interestingly, 
% the remarkable improvement of recent models in 
% robustness is not followed when looking at calibration error.
despite the remarkable improvements in terms of robustness, recent models are not significantly better calibrated.
% \rict{when looking at OOD calibration, we do not see any substantial
% improvements by using more recent models---the same that yield huge robustness
% boosts.}
%
When moving from CS to ACDC, the \textit{relative} mIoU error increases by a factor $\sim$2$\times$ for recent models \vs $\sim$3$\times$ for ResNet baselines; yet, in terms of \textit{relative} calibration error, all models increase by a $\sim$40$\times$ factor. This clearly highlights the need for further advances in model calibration.

In \cref{figure:miou_vs_ece} (top) we show the ECE \vs mIoU error for all models and datasets. 
% We do not observe a single model family that performs best in terms of ECE \vs mIoU trade-off. 
When it comes to 
% ECE \vs mIoU 
calibration \vs robustness
trade-off, there is no 
% a
clear winner among the model families we consider. 
Moreover, 
we do not observe a
% there is not a 
clear trend between mIoU and ECE in any 
domain.
% of the datasets.

\myparagraph{Calibration with \ts} In \cref{figure:miou_vs_ece} (bottom) we present the same results after applying \ts~\cite{GuoICML17OnCalibrationOfModernNNs}, 
tuned on CS.
% After TS, w
Comparing top and bottom,
we observe an overall improvement in calibration for all networks. In particular, SegFormer models (\textcolor{sb_violet}{$\Diamondblack$}), which had the largest ECE on CS and IDD, seem to benefit the most from \ts. Nevertheless, even after this improvment, \ood calibration error (IDD, ACDC) remains significantly larger than the \id one (CS) for all models.
%
Regarding ECE \vs mIoU out of domain, a mild trend emerges after \ts : 
%
For transformer models, better-calibrated models are also the most robust (before \ts, SegFormer (\textcolor{sb_violet}{$\Diamondblack$}) did not follow this trend). On the other hand, for ResNet baselines (\textcolor{sb_orange}{$\times$},\textcolor{sb_blue}{$\medbullet$}) the better-calibrated models are generally the most brittle.
%
% PREV VERSION BELOW
% For ResNet baselines (\textcolor{sb_orange}{$\times$},\textcolor{sb_blue}{$\medbullet$}), best calibrated models seem to have the largest mIoU error. On the other hand, for transformer models, the better-performing are usually better calibrated.
%
Regarding ConvNeXt (\textcolor{sb_green}{$\blacksquare$}), although it is one of the 
% better 
best
performing models, it seems to be worse calibrated than other models with similar mIoU. Overall, \textit{recent models are not significantly better calibrated than ResNet baselines neither before nor after \ts}. 

\subsection{Can we improve out-of-domain calibration?}
% error?}
\label{sec:improving_calibration}
% In domain, all models seem to be reasonably well calibrated, 
Calibration error on samples from the training domain in not alarming,
especially after TS. Nonetheless, the sharp increase in ECE out of domain is concerning for many applications, especially since recent segmentation models do not show a clear improvement in this direction. This renders methods that seek to improve calibration out of domain all the more important, but yet this is a rather underexplored research area. 
As discussed in \cref{sec:related_work}, 
to the best of our knowledge, only Gong~\etal~\cite{gong2021confidence} tackle \ood calibration \textit{without} additional information about the test domain. They suggest clustering the calibration set into multiple ``domains'' based on the image features extracted by the network. A different temperature per cluster is then selected and test-time predictions are scaled according to the cluster assigned to the images. This \textit{adaptive} \ts method was originally devised for classification, but we extend it to the segmentation task by scaling all the logits of a given image with the same temperature. Regarding the number of clusters, we find 16 to be a reasonable number (see \cref{sec:ablation_num_clusters} for this 
analysis).
% ablation).

% \subsubsection{Adaptive temperature via clustering} 
% In order to take into account multiple domains at test time, Gong \etal suggest clustering the calibration set into multiple ``domains'' based on the features of the images extracted by the network. The pre-trained model is then calibrated independently on the images of each cluster via \ts and a different temperature per cluster is selected. At test time, each image is assigned to a cluster and the output logits are scaled with the temperature of the corresponding cluster. Although this method was originally devised for classification, we extend it by simply scaling all logits of a given image by the same temperature. Regarding the number of clusters, we find 16 to be a reasonable number, see \cref{sec:ablation_num_clusters} for a detailed ablation.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Figures/clustering_ablation_new_colors.pdf}
\caption{\textbf{ECE $(\downarrow)$ after clustering \ts} for a selection of models (best mIoU on CS per family). If the calibration samples are representative of the test domains \textit{Clust All}
% dsets) 
% clustering 
can indeed improve ECE, however, without access to \ood samples (\textit{Clust CS} and \textit{Clust CS aug.}) benefits of clustering are limited. Oracle baselines (\textit{O}) always use calibration images from the test domain.
}
\label{figure:cluster_ablation}
% \vspace{-5pt}
\end{figure}

\myparagraph{Clustering on different calibration sets} \label{sec:clustering}
In \cref{figure:cluster_ablation} we present the results of calibrating with clusters computed on different calibration sets. Since our training dataset is CS, we assume that only CS images are available for calibration. As a naive alternative to obtain more diverse clusters, we introduce a \textit{CS aug.} dataset where some of the CS calibration images are randomly augmented with different transformations (\eg, color scaling, changes in brightness, contrast, etc). 
The rationale is that more diverse clusters might generalize better to new domains.
% \rict{The rationale is that randomizing the training domain is beneficial while learning arbitrary tasks; we want to assess whether it may also help TS.}

To assess how beneficial \ood samples can be during calibration, we also add another calibration set which contains images 
% of 
from
all the datasets (CS, IDD and ACDC) mixed together, we will refer to it as \textit{All}.
This serves as a sort of upper bound, since our main goal is still assessing robustness in unseen domains.
Furthermore, we introduce two more oracle baselines (\textit{O}), which use calibration images from the test domain. For instance, when evaluating on IDD, the oracle calibration set will consist of \textit{only} IDD while \textit{All} will contain images of IDD, ACDC and CS mixed. One oracle baseline uses clustering, while the other uses vanilla \ts (\textit{Clust O} and \textit{TS O}, respectively).

As expected, 
calibrating on all datasets (\textit{Clust All}) significantly improves ECE, with comparable performance to oracles in most settings. In contrast, without access to \ood samples (\textit{Clust CS}), calibrating with the method by Gong~\etal yields rather limited improvements. Moreover, increasing cluster diversity via data augmentation (\textit{Clust CS aug.}) is not always beneficial.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Figures/num_clusters_test_val.pdf}
\caption{\textbf{ECE $(\downarrow)$ \vs number of clusters for oracles} 
(calibration images 
extracted 
from the test domain). 
Even when evaluating on the calibration set, where there can be no overfitting of the temperatures,
% \rict{Despite calibration on test set,}
ECE does not decrease monotonically 
% with 
as we increase
the number of clusters (one cluster is equivalent to vanilla TS).
}
\label{figure:val_set_clustering}
% \vspace{-5pt}
\end{figure}

To gain more intuition, we visualize the cluster assignments (see \cref{sec:visualization_cluster_samples}). When using all datasets for calibration, test-time images are qualitatively close to the assigned clusters; yet, with \textit{Clust CS} or \textit{Clust CS aug.}, \ood images do not blend well with the calibration images of their corresponding clusters. We argue that one implicit assumption for clustering to work well is that test-time images are close to one of the clusters (domains) in the calibration set; therefore, under strong domain shifts, it is unlikely to bring much improvement.\looseness=-1

On the bright side, if representative images from the deployment domains are available, clustering could be applied to allow a single model to be calibrated on multiple domains. Of course, with \ood annotated samples, additional fine-tuning or adaptation techniques could be applied, but this is out of the scope of this study, since our focus is \textit{off-the-shelf} model robustness, without adaptation.\looseness=-1



\myparagraph{Clustering does not improve ECE in domain} Comparing oracles in~\cref{figure:cluster_ablation}, we can observe that clustering does not improve significanly over \ts. In some settings, it is even worse. One possible explanation would be that this is due to an overfitting of the temperature parameters to the particular calibration clusters. However, in \cref{figure:val_set_clustering} we observe that even when evaluating the ECE in the calibration set, the error does not monotonically decrease with the number of clusters. Although somewhat surprising, this is in fact possible since decreasing the ECE for several disjoint subsets of images (clusters) independently does not guarantee that the ECE on the union set will decrease. We provide a formal theorem in \cref{sec:proof_subset_ECE}, in support of this claim. Note that we are not asserting that clustering will not improve ECE \textit{in general} (we empirically observed it can, if provided with a representative calibration set) but rather that it is not guaranteed to do so.

To sum up, \textit{clustering the calibration set does not bring significant improvements unless representative images of the test domain are present in the calibration set}. Moreover, \textit{it is not better than \ts for \id calibration.}\looseness=-1

\begin{figure}
\centering
\includegraphics[width=\linewidth]{Figures/LTS_ablation_new_colors.pdf}
\caption{\textbf{ECE $(\downarrow)$ after local \ts (LTS)} for a selection of models (best mIoU on CS per family). Even without access to \ood samples (\textit{LTS CS}), LTS calibration improves ECE out of domain, especially under strong domain shifts (ACDC). With access to \ood samples (\textit{LTS All}), ECE out of domains improves further, albeit it degrades in domain.
}
\label{figure:LTS_ablation}
% \vspace{-5pt}
\end{figure}

\subsubsection{Adaptive temperature via calibration network} 
% Although, to the best of our knowledge, \cite{gong2021confidence} is the only work tackling zero-shot \ood calibration, the key to improving calibration \ood resides in adjusting the temperature depending on the input. In that regard,
The partial failure of the clustering approach motivates us to investigate other methods that adjust the temperature adaptively \wrt the input, since we 
% consider this to be key to improve 
can expect this to help improving
\ood calibration. In 
% that 
this
regard, Ding~\etal~\cite{ding2021local} suggest 
% learning 
training
a small
calibration network that predicts the 
temperature values
% temperatures 
as a function of both the
input image and the segmentation model logits. This Local Temperature Scaling
(LTS) method is specific to segmentation so the output is not a single
temperature per image but a ``temperature map'' with 
% one temperature per pixel. 
pixel-level temperature values.
% Even though it was not designed 
Despite not being designed
for \ood conditions,
our intuition is that a network providing sample-dependent temperatures can be 
% substantially 
beneficial under domain shifts.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{Figures/calib_methods_comparison_new_colors.pdf}
% \includegraphics[width=\linewidth]{Figures/calib_methods_comparison_all_models.pdf}
\caption{\textbf{Comparison of calibration methods.} ECE $(\downarrow)$ after calibration for a selection of models (best mIoU on CS per family). All models are calibrated on CS calibration set. LTS is markedly the best calibration method out of domain with remarkable improvements under strong domain shifts.
}
\label{figure:calib_methods_comparison}
% \vspace{-5pt}
\end{figure}

\myparagraph{LTS using different calibration sets} \label{sec:LTS} 
As in \cref{sec:clustering}, we test LTS on multiple calibration sets (\textit{CS}, \textit{CS aug.}, \textit{All}), which in this case are used to learn the calibration network. 
Also here, we compare against oracles, 
% which calibrate with images from the test domain.
for which the calibration network is learned using images from the test domains.
Results are shown in \cref{figure:LTS_ablation}. Interestingly, we find that LTS using only CS images for calibration (\textit{LTS CS}) leads to 
a noticeable improvement in \ood ECE.
% particularly in ACDC where the shift is stronger: here, 
In particular, when testing on ACDC---where the 
domain shift is stronger---\textit{LTS CS} outperforms even \ts with access to images on the test domain (\textit{TS O}) for some models. Also 
% here, 
in these experiments,
introducing naive data augmentations on CS (\textit{LTS CS aug.}) does not yield substantial improvements.

When using all the datasets for calibration (\textit{LTS All}), \ood results improve even further; yet, there is a noticeable increase in calibration error on CS. Unlike clustering, where the temperature was optimized independently for each cluster, LTS trains the calibration network using all the samples at once and samples with large calibration error (like ACDC or IDD) 
% will 
may
dominate the loss.
% term.
We hypothesize that further improvements in the architecture and training schedule of the calibration network can lead to even better performance and are promising directions 
% of 
for
future work. 
% \todo{@Ric expand more on domain generalization}.

% Regarding
Focusing on
the oracle baselines, LTS outperforms \ts on IDD and
% especially 
ACDC, but \ts outperforms LTS on CS (\cf \textit{LTS O} and \textit{TS O}). This may be due to the fact that CS is a very 
% uniform 
homogeneous
dataset if compared to the other two, hence, it 
can be reasonable 
% is reasonable to assume
that a simpler method may perform best.



\myparagraph{Comparing all calibration methods} \label{sec:comparison_calib_methods}
In \cref{figure:calib_methods_comparison} we compare 
% vanilla \ts, its clustering version and LTS using only CS as the calibration set. 
\textit{TS CS}, \textit{Clust CS} and \textit{LTS CS} (all calibrated on CS).
LTS is markedly the best calibration method out of domain, especially under stronger domain shifts. In domain, \ts works best, but it does not bring significant improvements out of domain. Since LTS predicts the temperature parameter at the pixel level, this motivates an ablation of clustering where we predict different temperatures per image; yet this does not improve results (see details in \cref{sec:per_class_clustering}). Additionally, we perform an ablation of LTS using only the image or the logits for calibration. Image information seems to be more important for \ood calibration, while the logits are more important in the \id setup (see \cref{sec:LTS_image_logits_ablation}).

\begin{figure}[t]
\centering
\begin{subfigure}[t]{\linewidth}
\includegraphics[width=\linewidth]{Figures/misclassification_all_models_no_legend.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{Figures/OOD_all_models.pdf}
\end{subfigure}

\caption{\textbf{Misclassification and \ood detection \vs} 
% mIoU.
\textbf{Robustness} for different segmentation models and datasets.
%
% \textbf{Misclassification (Top)}: 
\textbf{(Top) PRR $(\uparrow)$ \vs mIoU $(\uparrow)$:} 
ResNet-based models (\textcolor{sb_blue}{$\medbullet$},\textcolor{sb_orange}{$\times$}) outperform more recent models (other markers) in \id misclassification detection (CS, left), but the trend is opposite under strong domain shifts (ACDC, right). 
%
\textbf{(Bottom) AUROC $(\uparrow)$ \vs mIoU $(\uparrow)$:} There is no free-lunch between robustness and \ood detection in any considered domain.
% : in terms of \ood detection alone, a small ResNet-18 performs best (\textcolor{sb_blue}{$\medbullet$}).
}

\label{figure:misc_ood_all_models}
% \vspace{-5pt}
\end{figure}

\subsection{Misclassification detection} \label{sec:misclassification}

In \cref{figure:misc_ood_all_models} (top) 
% we compare \textit{misclassification detection} \vs \textit{robustness}
% ---PRR score $(\uparrow)$ \vs mIoU $(\uparrow)$---for all models.
we compare all models in terms of \textit{misclassification detection} \vs \textit{robustness}
---PRR score $(\uparrow)$ \vs mIoU $(\uparrow)$.
%  (note in this case higher is better in both axis). 
In domain, we observe a clear trend:
% for several model families: 
% where 
within the same model family, better performing models tend to also show better PRR. However, 
% between different model families, 
when considering all models,
higher mIoU does not generally imply higher PRR and ResNet-based backbones perform significantly better
than more recent architectures.
As we increase the domain shift, the trend changes: for ACDC, recent models perform best both in terms of mIoU and PRR. Moreover, out of domain, ResNet families show a negative correlation where better mIoU leads to worse PRR. 
%
Overall, \textit{recent 
% segmentation 
models seem to improve misclassification detection under strong domain shifts, but underperform baselines in domain.}


\subsection{Out-of-domain detection} \label{sec:OOD}

In \cref{figure:misc_ood_all_models} (bottom) we compare 
\textit{\ood detection} \vs \textit{robustness}
(AUROC score \vs mIoU) for all models. To measure \ood detection we separate the \id images (CS) from the \ood ones (IDD and ACDC). Therefore, the y-axis is the same in all three plots and only the mIoU changes. For \ood detection, there is a marked negative trend between CS mIoU and AUROC. When looking at IDD and ACDC, the negative trend continues but there seems to be a distinction between ResNet baselines and other recent models: 
% where recent models 
the latter
% are strongly shifted towards better mIoU, but have
perform better in terms of mIoU, but at the same time show
a drop in AUROC.
%
In short, \textit{there is no free-lunch between robustness and \ood detection. In terms of \ood detection, a small ResNet-18
(\textcolor{sb_blue}{$\medbullet$})
performs best.
}

\begin{figure}
\centering
\begin{subfigure}[t]{\linewidth}
\includegraphics[width=\linewidth]{Figures/misclassification_calibration_no_legend_new_colors.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\includegraphics[width=\linewidth]{Figures/OOD_calibration_new_colors.pdf}
\end{subfigure}

\caption{\textbf{Misclassification and \ood detection after calibration} for several models after applying different calibration techniques using CS samples.
\textbf{Misclassification -- PRR~$(\uparrow)$ (first 3 rows)}: Under strong domain shifts (ACDC), LTS calibration significantly improves PRR. 
\textbf{\ood detection -- AUROC~$(\uparrow)$ (last row)}: Both clustering and LTS yield improvements in \ood detection.
}
\label{figure:misc_ood_after_calibration}
% \vspace{-5pt}
\end{figure}

\subsection{Can calibration improve misclassification and out-of-domain detection?} \label{sec:improve_misc_ood}
In \cref{figure:misc_ood_after_calibration} we show misclassification (top) and \ood (bottom) detection metrics for different models after calibration using only CS samples. For misclassification, we observe a sharp PRR improvement on ACDC after we calibrate models with LTS. This is 
% very encouraging result, 
encouraging
as it indicates that the calibration network learned in LTS can help discern correct from incorrect predictions given its output temperature. We do not observe significant improvements in other datasets or with other methods. This is reasonable, since the largest calibration gain was 
observed with
% by testing 
LTS on ACDC.
(see \cref{figure:calib_methods_comparison}).

Regarding \ood detection (\cref{figure:misc_ood_after_calibration} bottom) we observe that both clustering and LTS calibration can improve \ood detection. 
% This is quite interesting, 
We find this interesting,
since clustering on CS did not improve \ood calibration significantly. Although the clusters using only CS images are not representative enough to produce adequate temperatures for IDD or ACDC, \ood samples are assigned to clusters which have larger temperatures. This is enough to decrease the confidence for \ood samples compared to \id and leads to better \ood detection. Similarly for LTS, the calibration network assigns larger temperatures to \ood images. 

In conclusion, \textit{adaptive \ts techniques are a promising avenue to improve \ood detection and misclassification detection under strong domain shifts.}



