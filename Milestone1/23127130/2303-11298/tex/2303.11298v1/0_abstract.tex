
%%%%%%%%% ABSTRACT
\begin{abstract}
Motivated by the increasing popularity of transformers in computer vision, in recent times
there has been a rapid development of novel architectures. 
While in-domain performance follows a constant, upward trend, properties like robustness or uncertainty estimation are 
less explored---leaving doubts about advances in model \textit{reliability}. Studies along these axes exist, but they are mainly limited to classification models.
In contrast, we 
% study 
carry out a study on
semantic segmentation, a relevant task for many real-world applications where model reliability is paramount. We analyze a broad variety of models, spanning from older ResNet-based architectures to novel transformers and assess their reliability based on four metrics: robustness, calibration, misclassification detection and out-of-distribution 
(\ood) detection. 
We find that while recent models are significantly more robust, they are not overall more reliable in terms of uncertainty estimation. We further explore methods that can come to the rescue and show that improving calibration can also help with other uncertainty metrics such as misclassification or \ood detection. This is the first study on modern segmentation models focused on both robustness and uncertainty estimation and 
we hope it will help practitioners and researchers interested in this fundamental vision task\footnote{Code available at \url{https://github.com/naver/relis}}.
% \rict{Code available at \url{https://github.com/naver/relis}}.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%
% Motivated by the increasing popularity of transformers in computer vision, in recent years there has been a rapid development of novel architectures. 
% While in-domain performance follows a constant, upward trend, properties like robustness or uncertainty estimation are 
% less explored \rict{-- leaving doubts about advances in model reliability}.

% In this work, we focus on recent models for semantic segmentation, a relevant task for many real-world applications where robustness and uncertainty estimation is paramount. We pose ourselves the following question: \emph{Are we moving in the right direction?} We carry out a large scale meta-study on a broad variety of segmentation models, spanning from older Resnet-based architectures to state-of-the-art transformers and assess their robustness to domain shifts, calibration, misclassification and out-of-distribution detection. 
% %
% We find that while recent models are significantly more robust, they are not overall more reliable when it comes to uncertainty metrics. Yet, we individuate methods that can come to the rescue and
% show that improving calibration can also help with other uncertainty metrics such as misclassification or out-of-distribution detection.
% This is the first meta-study on segmentation models focused on both robustness and uncertainty, and we hope it will help practitioners and researchers interested in this important task.
%%%%%%%%%%%%%%%%%%%%%%


% %%%%%%%%% OLD ABSTRACT
% \begin{abstract}
% Motivated by the increasing popularity of transformers in computer vision, in recent years there has been a rapid development of novel architectures. 
% While most of these works are showing promising results in terms of in-distribution performance, 
% other properties like robustness or uncertainty estimation are often less explored \rict{-- leaving doubts about advances in model reliability}.
% Moreover, the main focus of recent studies along these axes is to compare Transformers vs CNN architectures and only employ classification models. In this work, we focus on semantic segmentation, a relevant task for many real-world applications where robustness and uncertainty estimation is paramount. Additionally, our main goal is \textit{not} to compare self-attention vs convolutions, but recent models vs well established baselines (\eg ResNet); the key question being: \textit{Are we moving in the right direction?}. We leverage recent self-driving datasets captured in challenging settings to study robustness to natural covariate shifts and reliability in terms of calibration, misclassification and out-of-distribution detection. We find that while all recent models are significantly more robust than ResNet baselines, they are not overall more reliable. We further explore methods that can improve reliability and identify shortcomings and promising directions for out-of-domain calibration. Moreover, we show that improving calibration can also help with other uncertainty estimation problems such as misclassification or out-of-distribution detection.
% \end{abstract}