In \cref{figure:val_set_clustering} we observed that even when we evaluate the ECE on the calibration set, the calibration error does not monotonically decrease as we increase the number of clusters. This is somewhat counterintuitive as one would think that, with more clusters, the temperatures can be more fine-grained and evaluating on the calibration set there are no issues with overfitting. However, it is indeed possible since the temperatures of each cluster are optimized independently. In the following we present a theorem and proof to show that decreasing the ECE for several disjoint subsets of images (clusters) independently does not guarantee that the ECE on the union set will decrease.

First, we introduce some preliminaries and notation. Consider a classifier $f : \mathcal{X} \rightarrow \mathcal{Y}$ with $\mathcal{Y} = \{1, 2, 3, \dots, k\}$. We model our classifier as $f = \textrm{argmax}\ \hat{p}(\mathbf{y} | \mathbf{x})$ where $\hat{p}(\mathbf{y} | \mathbf{x})$ are the pseudo-probabilities estimated by the model for each class given the input. We say that a model is calibrated when
\begin{equation}
P \left( \mathbf{y}\ |\ \hat{p}(\mathbf{y} | \mathbf{x}) = p \right) = p, \quad \forall \mathbf{x} \sim \mathcal{D}
\end{equation}
where $P$ is the true probability of the classes and $\mathcal{D}$ the data distribution. However, most works focus on a simplification of this problem where only the probability of the predicted class is taken into account, that is:
\begin{equation}
P \left( y = \textrm{argmax}\ \hat{p}(\mathbf{y} | \mathbf{x})\ |\ \textrm{max}\ \hat{p}(\mathbf{y} | \mathbf{x}) = p^* \right) = p^* \quad \forall \mathbf{x}, y \sim \mathcal{D}.
\end{equation}

The most common metric to measure calibration is the \textit{Expected Calibration Error (ECE)}. Which looks at the expected difference between the predicted and actual probabilities:

\begin{equation}
\mathbb{E} \left [\ \left|\ p^*  - \mathbb{E} \left [\ P(\textrm{argmax}\ \hat{p}(\textbf{y} | \mathbf{x}) = y)\ |\  \textrm{max}\ \hat{p}(\textbf{y} | \mathbf{x}) = p^*\ \right ]\ \right| \ \right ].
\end{equation}

In order to empirically estimate the ECE, it is standard practice to quantize the output probabilities given by the model and compute the mean probability (confidence) and accuracy in each bin.  That is,  

\begin{equation}
\widehat{\textrm{ECE}}_{f} = \sum_{i=1}^m \frac{\#B_i}{n} \ |\textrm{accuracy}(B_i) - \textrm{confidence}(B_i)|
\end{equation}

where $\#B_i$ denotes the number of elements in the $i^{th}$ bin,  $m$ denotes the number of bins and $n = \sum_{i=1}^m \#B_i$ the total number of elements used to estimate the ECE.  We also use $f$ to indicate the dependency of the ECE on the classifier.

Consider now,  that we split the data into two different sets and we quantize it in bins $\mathcal{B} = \{B_i\}$ and $ \mathcal{B'} = \{ B'_i   \}$ with the same boundaries so that for each pair $B_i$ and $B'_i$  the range of confidence values are the same.  Moreover,  consider now the respective ECE computed for each subset of data independently --- denoted $\widehat{\textrm{ECE}}_{f}(\mathcal{B})$ and $\widehat{\textrm{ECE}}_{f}(\mathcal{B'})$ --- and on the full set of points $\widehat{\textrm{ECE}}_{f}(\mathcal{B}+\mathcal{B'})$ where $\mathcal{B}+\mathcal{B'}$ is an abuse of notation to indicate the union of elements in the bins for each index $i$. 

\begin{theorem}
With the notation described above, consider a model $f_{\textrm{oracle}}$ such that an ``oracle" splits the input according to whether it belongs to $\mathcal{B}$ or $\mathcal{B'}$. Moreover, $f_{\textrm{oracle}}$ uses two calibration strategies (one for $\mathcal{B}$ and one for $\mathcal{B'}$) in a way that it improves it's ECE on each subset $\mathcal{B}$, $\mathcal{B'}$ individually compared to some baseline model $f$ (e.g. by means of temperature scaling with a different temperature for each subset).  This does not necessarily imply that the oracle model ($f_{\textrm{oracle}}$) will be better calibrated on the full set of points $\mathcal{B}+\mathcal{B'}$ than the baseline model $f$. That is, given that:
$$
(a) \quad \widehat{\textrm{ECE}}_{f_{\textrm{oracle}}}(\mathcal{B}) \leq \widehat{\textrm{ECE}}_{f}(\mathcal{B})\ \ \  \textrm{and} \ \ \ \widehat{\textrm{ECE}}_{f_{\textrm{oracle}}}(\mathcal{B'}) \leq \widehat{\textrm{ECE}}_{f}(\mathcal{B'})
$$

Then,  condition $(a)$ is not sufficient to claim:
$$
 (b) \quad \widehat{\textrm{ECE}}_{f_{\textrm{oracle}}}(\mathcal{B + B'}) \leq \widehat{\textrm{ECE}}_{f}(\mathcal{B + B'})
$$
\end{theorem}

\begin{proof}
In order to proof the theorem we will construct a counter-example where condition $(a)$ is satisfied but condition $(b)$ is not. Consider the accuracy and confidence for the full set of points in a given bin:

\begin{align*}
    \textrm{acc}(B_i + B'_i) &= \frac{\#B_i\ \textrm{acc}(B_i) + \#B'_i\ \textrm{acc}(B'_i) }{\#B_i + \#B'_i} \\
    \textrm{conf}(B_i + B'_i) &= \frac{\#B_i\ \textrm{conf}(B_i) + \#B'_i\ \textrm{conf}(B'_i) }{\#B_i + \#B'_i}
\end{align*}

Then the ECE of the full set of points will be:

\begin{align*}
\widehat{\textrm{ECE}}_{f}(\mathcal{B + B'})  &= \sum_{i=1}^m \frac{\#B_i + \#B'_i}{n + n'} \left|\textrm{acc}_{f}(B_i + B'_i) - \textrm{conf}_{f}(B_i + B'_i)\ \right| \\
&= \sum_{i=1}^m \frac{1}{n + n'}\ \left|\ \#B_i (\textrm{acc}_{f}(B_i) - \textrm{conf}_{f}(B_i)) + \#B'_i(\textrm{acc}_{f}(B'_i) - \textrm{conf}_{f}(B'_i)) \ \right|.
\end{align*}

To simplify the notation,  let us define $r_i(f) = \textrm{acc}_{f}(B_i) - \textrm{conf}_{f}(B_i)$ and similarly for $r'_i(f)$.  Then,  we can write:

\begin{align*}
\widehat{\textrm{ECE}}_{f}(\mathcal{B + B'}) =&  \sum_{i=1}^m \frac{ 1}{n + n'}\  |\#B_i\ r_i(f)  + \#B'_i\ r'_i(f) |. \\
\widehat{\textrm{ECE}}_{f}(\mathcal{B})  =&  \sum_{i=1}^m \frac{\#B_i}{n} \ |r_i(f) |. \\
\widehat{\textrm{ECE}}_{f}(\mathcal{B'}) =&  \sum_{i=1}^m \frac{\#B'_i }{n'} \ |r'_i(f) |. \\
\end{align*}

Now let us consider a setting where $r_i(f) = r $ and $r_i(f_{\textrm{oracle}}) = -0.5 r $ for some $r \neq 0$ while $r'_i(f) = -r$ and $r'_i(f_{\textrm{oracle}}) = -0.5r $. Moreover, consider $\#B_i = \#B'_i$ which implies $n = n'$, then this setting would satisfy condition $(a)$ since 

\begin{align*}
&\widehat{\textrm{ECE}}_{f}(\mathcal{B})  =  \sum_{i=1}^m \frac{\#B_i }{n} \ |r |  \geq \sum_{i=1}^m \frac{\#B_i}{n} \ |-0.5r | = \widehat{\textrm{ECE}}_{f_{\textrm{oracle}}}(\mathcal{B})  \quad \textrm{and} \\
&\widehat{\textrm{ECE}}_{f}(\mathcal{B'}) =  \sum_{i=1}^m \frac{\#B'_i }{n'} \ |-r | \geq   \sum_{i=1}^m \frac{\#B'_i}{n'} \ |-0.5r |  =  \widehat{\textrm{ECE}}_{f_{\textrm{oracle}}}(\mathcal{B'}).
\end{align*}

However,  this same setting would not satisfy condition $(b)$ since 

\begin{align*}
\widehat{\textrm{ECE}}_{f}(\mathcal{B + B'}) &=  \sum_{i=1}^m \frac{\#B_i}{2n}\  |r  - r | = 0 \quad \textrm{and} \\
\widehat{\textrm{ECE}}_{f_{\textrm{oracle}}}(\mathcal{B + B'}) &= \sum_{i=1}^m \frac{\#B_i}{2n}\  |-0.5r  - 0.5r | > 0.
\end{align*}

Thus, we have showed that condition $(a)$ does not imply $(b)$.
\end{proof}
This result implies that minimizing the ECE for different subsets of the data independently (e.g.  each cluster of images) does not necessarily lead to an overall improvement of the ECE. Moreover,  we have assumed only two sets of samples without loss of generalization since if $(a)$ implied $(b)$ for an arbitrary number of data splits it would in particular imply it for two. Finally, note that our result is valid for either image classifiers or segmentors. In the first case we would each prediction would be the class of a whole image while in the second case the each pixel in an image would have a different prediction.




