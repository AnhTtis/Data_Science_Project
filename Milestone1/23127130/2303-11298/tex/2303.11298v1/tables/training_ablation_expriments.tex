% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[ht]
\centering
\begin{tabular}{@{}lcccccc@{}}
\toprule
                 & \multicolumn{3}{c}{\textbf{mIoU} ($\uparrow$)} & \multicolumn{3}{c}{\textbf{ECE} ($\uparrow$)} \\ 
\midrule
\textbf{Architecture}        & \textbf{CS}    & \textbf{IDD}   & \textbf{ACDC}  & \textbf{CS}   & \textbf{IDD}  & \textbf{ACDC}  \\ 
\midrule
ConvNext-B (IN-1k)  & 80.39 & 64.11 & 58.77 & 0.77 & 5.53 & 18.95 \\
ConvNext-B (IN-21k) & 81.56 & 65.70 & 60.59 & 0.81 & 5.27 & 20.07 \\
BiT-RN50 (IN-1k)    & 76.36 & 57.46 & 47.47 & 0.67 & 5.18 & 19.47 \\
BiT-RN50 (IN-21k)   & 76.49 & 57.23 & 46.70 & 0.65 & 5.20 & 19.63 \\
\midrule
\midrule
Segmenter (160k)    & 76.19 & 61.96 & 63.24 & 0.83 & 4.14 & 18.59 \\
Segmenter (80k)     & 76.22 & 60.74 & 62.96 & 0.71 & 4.28 & 18.36 \\
SegF-B5 (160k)      & 80.94 & 62.47 & 59.43 & 1.93 & 6.46 & 21.37 \\
SegF-B5 (80k)       & 80.15 & 62.95 & 57.00 & 1.07 & 5.32 & 21.27 \\ \bottomrule
\end{tabular}
\caption{Ablations of different training settings. On the top we compare ConvNext and BiT-RN50 architectures with the backbones pre-trained on either ImageNet 1k or 21k datasets. We observe that the benefits of a larger pre-training dataset are small in comparison to the gap between architectures. On the bottom we compare the amount of training iterations, again, we observe that although longer training schedules do improve the performance, changes are also small in comparison to architecture differences.}
\label{tab:training_ablations}
\end{table}