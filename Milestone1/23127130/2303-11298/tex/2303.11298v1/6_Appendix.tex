\section{Ablation of calibration metrics}
\label{sec:ablation_calibration_metrics}
In the main paper we present calibration results computing the Expected Calibration Error with equally spaced bins, however, alternative calibration metrics have been suggested. In \cref{figure:ablation_calibration_metrics} we compare the results obtained with: ECE with equally spaced bins (ECE), ECE with equally populated bins (Ada ECE) and the Kolmogorov-Smirnov Error (KS Error). For further details see \cref{sec:reliability_metrics}. We observe that the three different metrics yield almost identical results.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{Figures/calib_metrics_comparison.pdf}
\caption{\textbf{Comparison of calibration error metrics $(\downarrow)$} Calibration error for different datasets and networks computed with different metrics. All different metrics yield very similar result. 
}
\label{figure:ablation_calibration_metrics}
% \vspace{-5pt}
\end{figure}

\section{Ablation of number of pixels for calibration}
\label{sec:ablation_number_pixels_calibration}
As discussed in \cref{sec:reliability_metrics}, in segmentation, the number of 
samples to be taken into account for calibration scales with the number of pixels in an image. In order to be more cost-effective when testing different calibration metrics and strategies, we use a random subset of pixels within each image rather than the full image. In \cref{figure:ablation_num_samples_calibration}, we ablate the evolution of the different calibration metrics as we vary the number of sampled pixels. We can see that from 10k datapoints on, the metrics stabilizes; therefore, we chose to use 20k randomly sampled pixels per image for our experiments.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\linewidth]{Figures/ablation_num_samples_calibration.pdf}
\caption{\textbf{Pixels-per-image ablation.} Evolution of calibration metrics as we vary the number of pixels sampled at random from each image (as opposed to the full image). We observe that when sampling more than 10k pixels all calibration metrics are very similar and the calibration error remains stable. We use 20k random samples in our experiments.
}
\label{figure:ablation_num_samples_calibration}
% \vspace{-5pt}
\end{figure}


\section{Ablation of confidence score: max probability \vs entropy}
Misclassification detection and \ood detection both rely on a metric to evaluate how confident a model is on its predictions. The most straightforward metric would be the pseudo-probability of the predicted class (i.e. the max probability). If the probability is high it is reasonable to assume that the network is confident (this is precisely what we want to impose in the calibration task). Other metrics which involve all the logits have been suggested, negative entropy being the most popular. In \cref{figure:ablation_prob_vs_entropy} we compare the results obtained with probabiliy and entropy as confidence metrics and observe that there is not a significant difference between the two. Therefore, the simpler confidence metric based on the predicted class probability is used for other experiments by default.
\label{sec:ablation_prob_vs_entropy}
\begin{figure}[ht]
\centering
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=0.9\linewidth]{Figures/entropy_vs_prob_misclassification.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=0.9\linewidth]{Figures/entropy_vs_prob_ood.pdf}
\end{subfigure}
\caption{\textbf{Ablation of confidence score: max probability \vs entropy} Comparison of misclassification (top) and \ood (bottom) detection when using probability or negative entropy as confidence metrics. We observe that there is no significant difference between the two metrics, therefore we use the simpler probability as the default.
}
\label{figure:ablation_prob_vs_entropy}
% \vspace{-5pt}
\end{figure}


\section{Ablation number of clusters}
One of the main hyperparameters in Gong~\etal~\cite{gong2021confidence} is the number of clusters. In \cref{figure:ablation_num_clusters}, we ablate the number of clusters for different test datasets (columns) and calibration datasets (rows). Although not all networks evolve in the same way, we observe that after 16 clusters, performance is more or less stable.

\label{sec:ablation_num_clusters}
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{Figures/ablation_num_clusters.pdf}
\caption{\textbf{Ablation number of clusters.} ECE $(\downarrow)$ for different models and datasets as we vary the number of clusters computed for calibration. We find 16 clusters to be relatively stable.
}
\label{figure:ablation_num_clusters}
% \vspace{-5pt}
\end{figure}

\section{Visualization of cluster samples}
\label{sec:visualization_cluster_samples}
In \cref{sec:improving_calibration} we observe that adaptive temperature scaling via clustering does not significantly improve calibration under distribution shift -- especially when the shift is strong. 
% An implicit assumption for 
The method by Gong~\etal~\cite{gong2021confidence} makes the implicit assumption that the different domains captured in the clusters during calibration will be representative of the domains encountered at test time. In order to have a better intuition, we visualize a few samples randomly picked from each cluster. We show images from both the calibration set (used to compute the clusters and calibrate the models) and the test set (used to evaluate the calibration error). In our visualizations, the test set comprises images of the three datasets (CS, IDD and ACDC), while the calibration set changes for each Figure. In~\cref{figure:cluster_viz_All_dsets,figure:cluster_viz_Cityscapes,figure:cluster_viz_CS_aug}, respectively, we show representatives from clusters in \textit{Clust All} (all datasets used during calibration), \textit{Clust CS} (CS images used) and \textit{Clust CS aug} (augmented CS images used). Qualitatively, when all datasets are used for calibration, the cluster assignments appear quite reasonable (\eg night ACDC images are assigned to night images from calibration). However, when calibrating on CS and CS augmented, we observe that the calibration clusters are not diverse enough for the test images and the cluster assignments do not appear so intuitive.

\clearpage 

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/val_All_dsets_8_clusters_test_All_dsets_ConvNext_compressed.pdf}
\caption{\textbf{Visualization of clusters (Clust All).} Sample images from clusters computed for \cite{gong2021confidence}. In this case, the calibration set (where clusters are computed) contains imaged from all datasets and we qualitatively observe the cluster assignments to align with human intuition.
}
\label{figure:cluster_viz_All_dsets}
% \vspace{-5pt}
\end{figure}

\clearpage 

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/val_Cityscapes_8_clusters_test_All_dsets_ConvNext_compressed.pdf}
\caption{\textbf{Visualization of clusters (Clust CS).} Sample images from clusters computed for \cite{gong2021confidence}. In this case, the calibration set (where clusters are computed) contains imaged from CS only. We qualitatively observe that the clusters are not representative of the test distribution.
}
\label{figure:cluster_viz_Cityscapes}
% \vspace{-5pt}
\end{figure}

\clearpage 

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/val_Cityscapes_augmented_8_clusters_test_All_dsets_ConvNext_compressed.pdf}
\caption{\textbf{Visualization of clusters (Clust CS aug).} Sample images from clusters computed for \cite{gong2021confidence}. In this case, the calibration set (where clusters are computed) contains imaged from CS augmented only. Even if data augmentations introduce variability to the dataset, it is still not representative of the test distribution.
}
\label{figure:cluster_viz_CS_aug}
% \vspace{-5pt}
\end{figure}

\clearpage 

% \section{Subset calibration}
\section{Theoretical insights on adaptive temperature via clustering}
\label{sec:proof_subset_ECE}

\input{6.1_Partial_calibration_proof}

\clearpage 

\section{Per-class clustering}
\label{sec:per_class_clustering}
In \cref{figure:calib_methods_comparison}, we have observed that adaptive temperature via clustering \cite{gong2021confidence} does not significantly help improving out-of-domain calibration compared to local temperature scaling (LTS) \cite{ding2021local}. One important difference between the methods is that LTS computes a temperature for each pixel in the image while clustering is performed at the image level -- using a single temperature per image. This motivates us to perform an ablation where, on top of the image level clustering, pixels in a given image are grouped according to their predicted class. Intuitively, we are looking for a temperature for regions in the image that look alike to the network (since they are assigned to the same class). In \cref{figure:per_class_clustering} we compare standard per-image clustering (top) with the aforementioned per-class clustering (bottom). Note that per-class clustering always groups pixels according to the predicted class, therefore if $k=1$ then there are 19 clusters (corresponding to the CS classes). Calibration images are from the CS dataset.

Similarly to per-image clustering, increasing the number of clusters does not seem to always help when using per-class clustering. Moreover, we do not find that per-class clustering significantly improves calibration except for SegFormer architecture. 
%
We are not stating here that finer-grained clustering may not yield further improvements (and reach similar performance to LTS). However, given that improving ECE in different subdomains independently is not guaranteed to improve overall calibration (see \cref{sec:proof_subset_ECE}), perhaps a different approach to finding the temperatures and clusters taking into account both local and global calibration error would be needed.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{Figures/per_class_clustering_cityscapes.pdf}
\caption{\textbf{Per-class clustering ablation.} ECE $(\downarrow)$ for different models and datasets as we vary the number of clusters computed for calibration. We compare per-image clustering (where all pixels in an image are given the same temperature) \vs per-class clustering (where different pixels in an image are given a temperature depending on their predicted class). Overall, we do not observe consistent improvements when using per-class clustering except in SegFormer architecture.
}
\label{figure:per_class_clustering}
% \vspace{-5pt}
\end{figure}

\clearpage


\section{Ablation LTS: image \vs logits}
\label{sec:LTS_image_logits_ablation}
LTS \cite{ding2021local} employs a small-weight calibration network which receives both the image and predicted logits as input and it returns a temperature map to scale the logits (with a different temperature for each pixel in the image). Given its remarkable performance (see~\cref{figure:LTS_ablation,figure:calib_methods_comparison}) and, to get further insights into this method, in \cref{figure:LTS_image_logits_ablation} we perform an ablation where the calibration network only receives the image or the logits as input. To carry out this experiment, we modify the network in \cite{ding2021local} so that both input branches (logits and image) receive the same input, either both logits or both image. All calibration networks have been trained in CS images only. 

Interestingly, we observe that, in distribution (ECE CS), the better performing method for most networks is the LTS variant that uses the logits only. On the other hand, for \ood calibration, the better performing variant in most cases is the one that relies on both logit and image information. Moreover, under strong domain shifts (ECE ACDC), LTS yields better results by using only the image information than by using only the logits. However, this is subject to variability as results vary across different architectures. Further investigations on 
% the calibration network architecture and 
how logit and image signals are combined may constitute a promising direction to further improve calibration results. 

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/LTS_logits_vs_image_ablation_Cityscapes.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/LTS_logits_vs_image_ablation_IDD.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/LTS_logits_vs_image_ablation_ACDC.pdf}
\end{subfigure}
\caption{\textbf{LTS with image \vs logits information} ECE $(\downarrow)$ after calibration with three LTS variants: the original method combining image and logits (LTS), an LTS version only using logit information (LTS logits) and the complementary version only using image information (LTS image). We observe that in distribution, using only the logits seems to perform better while out of distribution using information from both image and logits works best.
}
\label{figure:LTS_image_logits_ablation}
% \vspace{-5pt}
\end{figure}

\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation of the effect of training settings}
\label{ap:training_ablations}

Our goal is to assess the reliability of the best available models for segmentation â€” the ones used by practitioners. Therefore, to test a model at its best, we need to use its most favorable setup (original pre-training, optimizer, etc.), as done in previous work \cite{pinto2022impartial, minderer2021revisiting}. Nevertheless, in this section we ablate the effects of two different training settings: the dataset used to pre-train the backbone and the number of training iterations. Most recent models were pre-trained on ImageNet 21k, however, ResNet backbones are usually pre-trained on ImageNet 1k. In the top rows of \cref{tab:training_ablations} we compare the performance of a BiT-ResNet50 \cite{kolesnikov2020big} and a ConvNext-B \cite{LiuCVPR22AConvNet4The2020s} backbones pre-trained with either ImageNet 1k or 21k. We observe that the benefits of a larger pre-training dataset are small in comparison to the gap between architectures.

On the other hand, most models were trained using 80k iterations (as is quite standard for CS dataset, but Segmenter and SegFormer models' original training schedule uses 160k iterations. In the bottom rows of \cref{tab:training_ablations} we compare Segmenter and SegFormer-B5 networks trained with either 160k or 80k iterations, again the differences are minor compared to the gap between architectures.

\input{tables/training_ablation_expriments}


\section{Architectures for Universal Image Segmentation}
\label{ap:mask2former}
With the appearance of transformers, and in particular motivated by DETR \cite{carion2020end}, some architectures have been proposed with the objective to solve the three main Image Segmentation tasks, that is: Semantic Segmentation, Instance Segmentation and Panoptic Segmentation \cite{cheng2021perpixel, zhang2021knet, ChengCVPR22MaskedAttentionMaskTransformer4UniversalSiS}. Here we will focus on Mask2Former \cite{ChengCVPR22MaskedAttentionMaskTransformer4UniversalSiS} which is based on MaskFormer \cite{cheng2021perpixel} and is the best performing universal architecture to the best of our knowledge. Interestingly, to be able to solve the different segmentation tasks jointly, these architectures do not output the standard per-pixel logits when it comes to semantic segmentation. Instead, they predict a set of $N$ object masks (where $N$ is fixed) and the class probabilities for each object. Then, to obtain the per-pixel class probabilities they marginalize over all the possible objects a pixel could belong to, we refer the reader to \cite{cheng2021perpixel, ChengCVPR22MaskedAttentionMaskTransformer4UniversalSiS} for further details. 

Although this final output can be regarded as per-pixel class probabilities, they way it is obtained differ from the standard \textit{logits + softmax} setting that all calibration methods rely on, therefore it is not straightforward to compare these universal architectures with other models with temperature scaling. Nevertheless, given the good performance and wider applicability of these models, we include them in our study comparing only off-the-shelf performance in terms of mIoU, calibration, OOD detection and misclassification. The best performing model from \cite{ChengCVPR22MaskedAttentionMaskTransformer4UniversalSiS} is based on a Swin transformer (Swin Large) \cite{LiuICCV21SwinTransformerHierarchicalViTShiftedWindows}, therefore, we also include a Swin transformer model with UpperNet to the comparison for completeness.

\subsection{Calibration with Mask2Former}
In \ref{figure:calib_mask2former} we present the ECE vs segmentation error (100 - mIoU) for the different models. Interestingly, we observe that Mask2Former seems to be the best-performing model in terms of mIoU in all datasets. In terms of calibration error Mask2Former is poorly calibrated in distribution (CS) but has a milder increase in ECE as the distribution shift becomes stronger. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{Figures/cvpr_rebuttal/calib.pdf}
\caption{
% Expected calibration error $(\downarrow)$  \vs mIoU error $(\downarrow)$  
\textbf{mIoU error $(\downarrow)$ \vs Expected calibration error $(\downarrow)$}. All models were trained on CS. Markersize proportional to number of parameters. Interestingly, we observe that Mask2Former seems to be the best-performing model in terms of mIoU in all datasets. In terms of calibration error Mask2Former is poorly calibrated in distribution (CS) but has a milder increase in ECE as the distribution shift becomes stronger. 
}
\label{figure:calib_mask2former}
% \vspace{-5pt}
\end{figure}


\subsection{OOD detection with Mask2Former}
In \ref{figure:ood_mask2former} we present the OOD vs mIoU for the different models. Here we observe that Mask2Former and Swin transformer align with the negative trend observed in previous models where models with better mIoU tend to perform worse at OOD detection. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{Figures/cvpr_rebuttal/ood_image.pdf}
\caption{
% Expected calibration error $(\downarrow)$  \vs mIoU error $(\downarrow)$  
\textbf{mIoU $(\uparrow)$ \vs OOD - AUROC $(\uparrow)$} for different model families. All models trained on CS. Markersize proportional to number of parameters. We observe that Mask2Former and Swin transformer align with the negative trend observed in previous models where models with better mIoU tend to perform worse at OOD detection. 
}
\label{figure:ood_mask2former}
% \vspace{-5pt}
\end{figure}

\subsection{PRR with Mask2Former}

In \ref{figure:prr_mask2former} we present the OOD vs mIoU for the different models. Although Mask2Former is significantly better than other models in terms of misclassification detection in distribution, it seems to perform significantly worse under strong domain shifts (ACDC). This seems to be contrary to the trend followed by other models where robustness seems to be correlated with misclassification under domain shift.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{Figures/cvpr_rebuttal/prr.pdf}
\caption{
% Expected calibration error $(\downarrow)$  \vs mIoU error $(\downarrow)$  
\textbf{mIoU $(\uparrow)$ \vs prediction rejection ratio $(\uparrow)$} for different model families. All models trained on CS. Markersize proportional to number of parameters. Mask2Former is significantly better than other models in terms of misclassification detection in distribution, it seems to perform significantly worse under strong domain shifts (ACDC).
}
\label{figure:prr_mask2former}
% \vspace{-5pt}
\end{figure}

\section{Visualization of uncertainty and temperature maps}
As opposed to classification, semantic segmentation models can assign different confidence to regions of the image. That can allow, among other applications, to detect regions with low confidence that may correspond to novel classes or weird instances of a known class. ACDC shares the same classes as Cityscapes, however, IDD includes some novel classes which are not included in the Cityscapes dataset. In \ref{figure:viz_t_scaling} we illustrate some examples of IDD images with confidence maps before and after LTS, together with the temperature scaling maps predicted by the calibration network. We observe how different \ood classes (\eg autorickshaw, bridge, billboard) or weird instances are highlighted in the calibration maps. In \cref{ap:local_ood} we quantify how useful are the calibration maps in order to perform local \ood detection. 

\begin{figure}[h!]
\centering
\includegraphics[width=0.65\linewidth]{Figures/cvpr_rebuttal/Figure_temperature_maps.pdf}
\caption{
% Expected calibration error $(\downarrow)$  \vs mIoU error $(\downarrow)$  
\textbf{Calibration and temperature maps} for different IDD images. We observe how different \ood classes (\eg autorickshaw, bridge, billboard) or weird instances are highlighted in the calibration maps.
}
\label{figure:viz_t_scaling}
% \vspace{-5pt}
\end{figure}

\clearpage

\section{Local OOD detection}
\label{ap:local_ood}
In this section we perform \ood detection at the pixel level to find regions of the image that belong to unknown classes (autorickshaw, guardrail, billboard, bridge). We define pixels of unknown classes as \ood while those corresponding to CS classes are in distribution. In \cref{figure:local_ood} we present the results of local \ood detection vs mIoU. We make two observations: i) Mask2Former has the best local \ood detection; ii) Differences between models are smaller for local \ood: numbers are roughly in the $0.7-0.8$ range \vs $ 0.4-1.0$ range for image-based \ood.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\linewidth]{Figures/cvpr_rebuttal/region_OOD_All_classes.pdf}
\caption{
\textbf{mIoU $(\uparrow)$ \vs local ood $(\uparrow)$} for different model families. All models trained on CS. Markersize proportional to number of parameters. Mask2Former achieves the best local \ood detection performance.
}
\label{figure:local_ood}
% \vspace{-5pt}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage


\section{Additional plots: adaptive temperature via clustering}
\label{sec:additional_plots_clustering}
In \cref{figure:cluster_ablation} we analyzed the calibration error after applying the method by Gong~\etal~\cite{gong2021confidence}, which clusters the images in the calibration set and computes a different temperature per cluster. Due to space constraints we only showed the best performing model for each family, in \cref{figure:clustering_methods_all_models} we show the results for all models.


\begin{figure}[ht]
\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/clustering_methods_all_architectures_Cityscapes.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/clustering_methods_all_architectures_IDD.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/clustering_methods_all_architectures_ACDC.pdf}
\end{subfigure}
\caption{\textbf{ECE $(\downarrow)$ after clustering \ts} Extension of \cref{figure:cluster_ablation} in the main paper where we show results for all models.
}
\label{figure:clustering_methods_all_models}
% \vspace{-5pt}
\end{figure}

\clearpage

Complementary to \cref{figure:clustering_methods_all_models} where we show the different calibration methods for a given architecture with barplots, in \cref{figure:clustering_methods_ece_vs_miou} we present the same results but we group them by calibration method (instead of by model). For each test dataset (rows), we plot the ECE \vs mIoU after calibrating the models with the corresponding method (columns).

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/custer_methods_vs_ece_all_models.pdf}
\caption{\textbf{ECE $(\downarrow)$ \vs mIoU error $(\downarrow)$ after calibration with clustering \ts} considering different calibration sets. This plot provides a different visualization of the results in \cref{figure:clustering_methods_all_models}.}
\label{figure:clustering_methods_ece_vs_miou}
% \vspace{-5pt}
\end{figure}

\clearpage

\section{Additional plots: local temperature scaling}
\label{sec:additional_plots_LTS}
In \cref{figure:LTS_ablation} we analyzed the calibration error after applying the method by Ding~\etal~\cite{ding2021local} which learns a calibration network that predicts the temperature as a function of the image and segmentation model logits. Due to space constraints we only showed the best performing model for each family, in \cref{figure:lts_methods_all_models} we show the results for all models.

\begin{figure}[ht]
\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/lts_methods_all_architectures_Cityscapes.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/lts_methods_all_architectures_IDD.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/lts_methods_all_architectures_ACDC.pdf}
\end{subfigure}
\caption{\textbf{ECE $(\downarrow)$ after Local \ts} Extension of \cref{figure:LTS_ablation} in the main paper where we show results for all models.
}
\label{figure:lts_methods_all_models}
% \vspace{-5pt}
\end{figure}


\clearpage

Complementary to \cref{figure:lts_methods_all_models} where we show the different calibration methods for a given architecture with barplots, in \cref{figure:lts_methods_ece_vs_miou} we present the same results but we group them by calibration method (instead of by model). For each test dataset (rows), we plot the ECE \vs mIoU after calibrating the models with the corresponding method (columns).

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/lts_methods_vs_miou_all_models.pdf}
\caption{\textbf{ECE $(\downarrow)$ \vs mIoU error $(\downarrow)$ after calibration with Local \ts} considering different calibration sets. This plot is showing the same results as \cref{figure:lts_methods_all_models} but in a different visualization.
}
\label{figure:lts_methods_ece_vs_miou}
% \vspace{-5pt}
\end{figure}

\clearpage


\section{Additional plots: Comparison calibration methods}
\label{sec:additional_plots_calib_methods_comparison}
In \cref{figure:calib_methods_comparison} we compared the calibration error after calibrating with TS CS, Clust CS and LTS CS \vs the uncalibrated baseline. Due to space constraints we only showed the best performing model for each family, in \cref{figure:calib_methods_all_models} we show the results for all models.

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/calib_methods_all_architectures_Cityscapes.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/calib_methods_all_architectures_IDD.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/calib_methods_all_architectures_ACDC.pdf}
\end{subfigure}
\caption{\textbf{ECE $(\downarrow)$ after calibration with different methods} Extension of \cref{figure:calib_methods_comparison} in the main paper where we show results for all models.
}
\label{figure:calib_methods_all_models}
% \vspace{-5pt}
\end{figure}

\clearpage

Complementary to \cref{figure:calib_methods_all_models} where we show the different calibration methods for a given architecture with barplots, in \cref{figure:calib_methods_ece_vs_miou} we present the same results but we group them by calibration method (instead of by model). For each test dataset (rows), we plot the ECE \vs mIoU after calibrating the models with the corresponding method (columns).

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/calib_methods_comparison_all_models_appendix.pdf}
\caption{\textbf{ECE $(\downarrow)$ \vs mIoU error $(\downarrow)$ after calibration with different methods} on the CS calibration set. This plot is showing the same results as \cref{figure:calib_methods_all_models} but in a different visualization.
}
\label{figure:calib_methods_ece_vs_miou}
% \vspace{-5pt}
\end{figure}

\clearpage

\section{Additional plots: misclassification detection}
\label{sec:additional_plots_misclassification}
In \cref{figure:misc_ood_after_calibration} we compared the misclassification detection and \ood detection performance of the networks after calibrating with TS CS, Clust CS and LTS CS \vs the uncalibrated baseline. Due to space constraints we only showed the best performing model for each family, in \cref{figure:misc_calib_all_models} we show the misclassification detection results for all models.

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/Misc_calibration_all_architectures_Cityscapes.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/Misc_calibration_all_architectures_IDD.pdf}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{\linewidth}
\centering
\includegraphics[width=0.75\linewidth]{Figures/Misc_calibration_all_architectures_ACDC.pdf}
\end{subfigure}
\caption{\textbf{Misc detection: PRR $(\uparrow)$ after calibration with different methods} Extension of \cref{figure:misc_ood_after_calibration} in the main paper where we show misclassification results for all models.
}
\label{figure:misc_calib_all_models}
% \vspace{-5pt}
\end{figure}

\clearpage

Complementary to \cref{figure:misc_calib_all_models} where we show the different calibration methods for a given architecture with barplots, in \cref{figure:misc_calib_ece_vs_miou} we present the same results but we group them by calibration method (instead of by model). For each test dataset (rows), we plot the ECE \vs mIoU after calibrating the models with the corresponding method (columns).

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{Figures/misc_after_calibration_all_models_dsets.pdf}
\caption{\textbf{PRR $(\uparrow)$ \vs mIoU $(\uparrow)$ after calibration with different methods} on the CS calibration set. This plot is showing the same results as \cref{figure:misc_calib_all_models} but in a different visualization.
}
\label{figure:misc_calib_ece_vs_miou}
% \vspace{-5pt}
\end{figure}

\clearpage

\section{Additional plots: out-of-distribution detection}
\label{sec:additional_plots_ood}
In \cref{figure:misc_ood_after_calibration} we compared the misclassification detection and ood detection performance of the networks after calibrating with TS CS, Clust CS and LTS CS \vs the uncalibrated baseline. Due to space constraints we only showed the best performing model for each family, in \cref{figure:misc_calib_all_models} we show the \ood detection results for all models.


\begin{figure}[ht]
\centering
\begin{subfigure}[t]{\linewidth}
\centering
\includegraphics[width=0.7\linewidth]{Figures/ood_calibration_all_architectures.pdf}
\end{subfigure}
\caption{\textbf{\ood detection: AUROC $(\uparrow)$ after calibration with different methods} Extension of \cref{figure:misc_ood_after_calibration} in the main paper where we show \ood detection results for all models.
}
\label{figure:ood_calib_all_models}
% \vspace{-5pt}
\end{figure}

Complementary to \cref{figure:ood_calib_all_models} where we show the different calibration methods for a given architecture with barplots, in \cref{figure:ood_calib_ece_vs_miou} we present the same results but we group them by calibration method (instead of by model). For each test dataset (rows), we plot the ECE \vs mIoU after calibrating the models with the corresponding method (columns).

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\linewidth]{Figures/OOD_after_calibration_all_models_dsets.pdf}
\caption{\textbf{AUROC $(\uparrow)$ \vs mIoU $(\uparrow)$ after calibration with different methods} on the CS calibration set. This plot is showing the same results as \cref{figure:ood_calib_all_models} but in a different visualization.
}
\label{figure:ood_calib_ece_vs_miou}
% \vspace{-5pt}
\end{figure}

