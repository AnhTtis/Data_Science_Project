\section{Experimental settings and preliminaries}
\label{sec:experimental_settings}

% \subsection{Datasets and domain shifts} 
\subsection{Datasets} 

As discussed in Section~\ref{sec:related_work}, we use different datasets for semantic segmentation of urban scenes to model natural domain shifts---inspired by prior art~\cite{YueICCV19DomainRandomizationPyramidConsistencySimulationToReal,ChoiCVPR21RobustNetImprovingDGUrbanSiSInstanceSelectiveWhitening,VolpiCVPR22OnRoadOnlineAdaptSiS}. 

% \looseness=-1

\noindent
\textbf{Cityscapes (CS)~\cite{CordtsCVPR16CityscapesDataset}} contains images taken across 50 European cities at day time with overall good weather. Training and validation sets use sequences from disjoint sets of cities. Following this protocol, we further split validation cities into a calibration and a test set. 
% \rict{Each image is accompanied with semantic maps with 19 classes}.
% Given the popularity of this dataset, 
Since CS is a mainstream benchmark in semantic segmentation, we use it as our training set (\id) to leverage available trained 
% models. 
weights.

\noindent
% \rict{We detail in the following the datasets for the \ood setting.}
\textbf{IDD~\cite{VarmaWACV19IDDDatasetExploringADUnconstrainedEnvironments}} 
% stands for India Driving Dataset and 
was captured in the cities of Hyderabad, Bangalore and their outskirts. Given the different geographical location, it poses a clear domain shift for CS models.
% It was generated with the intent to capture an unstructured driving environment and therefore poses a clear domain shift. 
%
%

\noindent
\textbf{ACDC~\cite{SakaridisICCV21ACDCAdverseConditionsDatasetSIS}} 
% stands for Adverse Conditions Dataset with Correspondences. It 
contains images captured in adverse conditions 
% which are divided into 
(Fog, Rain, Snow and Night),
% The images were captured in Switzerland, therefore there is not a large gap with CS in terms of location but the adversarial conditions
which translate into strong domain shifts. 
%Similarly to IDD, we use the validation set for calibration and the training for test.
%
Similarly to previous work~\cite{pinto2022impartial,zhou2022understanding, XieNIPS21SegFormerSemSegmTransformers}, we focus on \textit{covariate} shifts, \ie, changes in the input distribution---keeping the 
% labels 
label set
fixed. In practice, for \ood settings (IDD and ACDC) we consider the 19 classes from CS, ignoring the others. 
The only exception is one experiment in~\cref{ap:local_ood}, where we consider label shifts.
% Shifts in the label set (\eg, considering unseen classes) is an interesting future direction.



% \rict{while topics such as open-set recognition~\cite{ScheirerPAMI13TowardsOSR} and anomaly detection~\cite{LisICCV19DetectingTheUnexpectedViaImageResynthesis} are also interesting case studies for model reliability, we leave this for future work.
% }


\subsection{Architectures} 
\label{sec:arch}
% We test several recent transformer- and CNN-based segmentation models as well as the popular ResNet baseline \cite{he2016deep}.

% Below, we detail the models used in our study.
We implement our models with MMsegmentation~\cite{mmseg2020}. Following prior work~\cite{pinto2022impartial, minderer2021revisiting}, we use the original training recipes for each model to compare them at their best.
For completeness, we also explore the effects of pre-training dataset and number of training iterations in \cref{ap:training_ablations}.
% \footnote{\paut{We add two experiments on the effects of pre-training dataset and number of training iterations in \cref{ap:training_ablations}}}.

\myparagraph{\textcolor{sb_red}{SETR} \cite{ZhengCVPR21RethinkingSiSSeq2SeqPerspTransformers}} The first convolution-free segmentation model. It uses a ViT backbone \cite{DosovitskiyICLR21AnImageIsWorth16x16WordsTransformersAtScale} and 
% multiple 
different
decoders (SETR-Naive, SETR-MLA and SETR-PUP).
%to replace traditional convolutional decoders.
We use the ViT-Large backbone and analyze all three decoders.\looseness=-1

\myparagraph{\textcolor{sb_brown}{Segmenter} \cite{StrudelICCV21SegmenterTransformerForSemSegm}} Similarly to SETR, it also uses a ViT backbone; yet, unlike the simpler SETR decoders, 
% this one is transformer-based. 
it carries a transformer-based one.
We also test ViT-Large.
% We also explore the variant based on the ViT-Large backbone.

\myparagraph{\textcolor{sb_violet}{SegFormer} \cite{XieNIPS21SegFormerSemSegmTransformers}} This 
% work proposes 
model incorporates
an original self-attention mechanism and several architectural changes to be more efficient. 
% Aside from the ResNet baselines, this is the only model that does not pre-train on ImageNet-21K. 
We evaluate all models from this family (B0--B5), gradually increasing the number of parameters.

\myparagraph{\textcolor{sb_green}{ConvNeXt} \cite{LiuCVPR22AConvNet4The2020s}} 
Convolutional model with
% gradually 
changes inspired by transformers.
% obtains a convolutional model that performs 
% yields a performance
% on par or even better than them.
We use ConvNeXt-Large, comparable in size to ViT-Large, 
% To build a segmentation model 
with an UPerNet decoder~\cite{XiaoECCV18UnifiedPerceptualParsingSceneUnderstanding}.

\myparagraph{ResNet-based~\cite{he2019bag}}
We use ResNet-V1c model, the default ResNet in MMsegmentation library~\cite{mmseg2020}. Compared to vanilla ResNet\cite{he2016deep} it uses a stem with three 3x3 convs (instead of a 7x7 conv). We use ResNet-18/50/101 models and two popular decoders: \textbf{\textcolor{sb_orange}{DLV3+}}~\cite{ChenX17RethinkingAtrousConvolutionSemSegm} and \textbf{\textcolor{sb_blue}{UPerNet}}~\cite{XiaoECCV18UnifiedPerceptualParsingSceneUnderstanding}
% (also used by ConvNeXt).

Additionally,
in \cref{ap:mask2former}
we explore \textbf{Mask2Former}, an architecture for \textit{universal image segmentation} \cite{ChengCVPR22MaskedAttentionMaskTransformer4UniversalSiS} that does not follow the conventional \textit{logits + softmax} paradigm.
%However, it is not straightforward to compare these universal architectures with other models with temperature scaling. See further discussion and results 


\subsection{Reliability metrics}\label{sec:reliability_metrics}

\noindent
We evaluate model reliability on four aspects: robustness, calibration, misclassification detection and \ood detection.


\myparagraph{Robustness}
We measure robustness by evaluating the standardized mean
Intersection-over-Union (mIoU) performance in \ood settings, \ie, on ACDC and IDD. We also provide \id performance,
evaluating models on CS.


\myparagraph{Calibration} A model is said to be calibrated when the predictive probabilities (\ie, the logits after a softmax) correspond to the true probabilities. For instance, if we group all samples where the predicted probability is 
90\%,
% $90\%$,
we would expect that 
90\% 
% $90\%$ 
of those 
% samples 
predictions
are correct. The most common calibration metric is the \textit{Expected Calibration Error (ECE)} \cite{naeini2015obtaining}, which looks at the expected difference between the predicted and actual probabilities. To estimate the ECE we quantize the predicted probabilities and compare the accuracy with the mean probability in each bin. Since the binning strategy can affect the results, we 
% use 
test
ECE with equally spaced bins \cite{naeini2015obtaining}, equally populated bins \cite{nixon2019measuring, nguyen2015posterior} and the Kolmogorov-Smirnov test \cite{gupta2020calibration}, which gets rid of the binning strategy altogether. We report results with standard ECE, but find that all three aforementioned metrics 
% to be extremely similar, 
yield similar conclusions
(see~\cref{sec:ablation_calibration_metrics}).

Given that in segmentation we have per-pixel predictions, the number of calibration samples explodes (a single CS image contains 2048 $\times$ 1024 $\simeq $ 2M pixels). We ablate the different calibration metrics as we sub-sample the number of pixels per image and observe that 20k pixels per image in enough to estimate the ECE (see~\cref{sec:ablation_number_pixels_calibration}).

\myparagraph{Misclassification detection} 
% Although calibration is a very popular metric, it has some blind spots. For instance, a random binary classifier that always predicts with $50\%$ confidence will result perfectly calibrated in a balanced set, yet the model is by no means reliable.
%yet, its confidence does not help yield any information.
% This illustrates that a reasonable request for a 
A desiderata for a reliable model is to assign a larger confidence
to correct outputs than incorrect ones\footnote{We use max softmax as confidence in the paper; in \cref{sec:ablation_prob_vs_entropy} we present similar results with negative entropy.}. 
In the ideal case, if we sorted all predictions from least to most confident, we would have all the incorrect predictions first and correct ones later. \textit{Misclassification detection} measures how far away are we from such an ideal case. This can be measured with \textit{Rejection-Accuracy curves} \cite{fumera2002support, hendrycks2021natural}: we reject samples with low confidence and compute the accuracy \vs amount of rejected samples. However, these are biased in favor of better-performing models,
% (as 
since
the base accuracy 
% will already be higher). 
is higher in the first place.
To avoid that, we follow Malinin~\etal~\cite{malinin2019ensemble} 
% that normalizes 
and normalize
the area under the curve by that of an oracle and subtracts a baseline score with randomly sorted samples. The resulting metric, known as the Prediction Rejection Ratio (PRR), will be positive if the confidence is better than the random baseline and will have a maximum score of 100\% when the model matches the oracle.\looseness=-1
% an ``oracle'' sorting. 

\myparagraph{Out-of-domain detection} Another important aspect in reliability is that models are aware of their ``domain of expertise'' (\ie, their training domain). When a sample differs significantly from the training 
% domain, 
samples,
we would expect the model to be more uncertain of its prediction. Similarly to misclassification detection, in \ood detection we try to separate \id from \ood images based on the network confidence. This has 
% clear applications, 
broad applicability,
% such as 
\eg,
generating alerts for samples too far from the training domain
or, connecting to active learning, gathering them for further review and annotation. As in the rest of our work, we consider a whole image to be out of domain if it presents a significant 
% covariate 
domain
shift; that is, we consider CS in domain and IDD/ACDC out of domain. Since we define \id and \ood samples at the image level, we consider the average confidence of all pixels in a given image.
% As discussed in \cref{sec:related_work}, 
We use the Area Under the Receiver Operating Characteristic curve (AUROC) \cite{murphy2012machine}, which goes from 0 to 1 (1 being the best score). 
% \paut{Additionally, in \cref{ap:local_ood} we consider regions in an image to be \ood by using classes in the IDD dataset not present in CS.}
Additionally, in \cref{ap:local_ood} we consider \ood detection at a region level, considering classes in the IDD dataset not present in CS, \ie, \textit{rickshaw}, \textit{billboard}, \textit{guard rail}, \textit{tunnel} and \textit{bridge}.











% \subsection{Reliability metrics}

% We evaluate the reliability of segmentation models on three main aspects: model calibration, misclassification detection and out of domain detection.


% \paragraph{Calibration} Consider a classifier $f : \mathcal{X} \rightarrow \mathcal{Y}$ with $\mathcal{Y} = \{1, \dots, k\}$. We model our classifier as $f = \textrm{argmax}\ \hat{p}(\mathbf{y} | \mathbf{x})$ where $\hat{p}(\mathbf{y} | \mathbf{x})$ are the pseudo-probabilities estimated by the model for each class given the input. We say that a model is calibrated when
% \begin{equation}
% P \left( y = f( \mathbf{x})\ |\ \textrm{max}\ \hat{p}(\mathbf{y} | \mathbf{x}) = p^* \right) = p^* \quad \forall \mathbf{x}, y \sim \mathcal{D},
% \end{equation}
% where $P$ is the true probability and $\mathcal{D}$ the data domain \cite{GuoICML17OnCalibrationOfModernNNs}.\footnote{A more strict notion of calibration can be used which involves all logits, however, this one is by far the most popular.} In a nutshell, if we group all samples where the probability of the model was, for instance, $90\%$. Then, we would expect that $90\%$ of those samples are correct. Note that we used the notation of a classifier for simplicity, however, segmentation can be considered as per-pixel classification, therefore these notations can be trivially extended to segmentation where we consider the output for each pixel as a different prediction.

% The most common metric to measure calibration is the \textit{Expected Calibration Error (ECE)} \cite{naeini2015obtaining} which looks at the expected difference between the predicted and actual probabilities. To empirically estimate the ECE, it is standard practice to quantize the output probabilities given by the model and compute the mean probability (confidence) and accuracy in each bin. That is,  

% \begin{equation}
% \widehat{\textrm{ECE}} = \sum_{i=1}^m \frac{\#B_i}{n} \ |\textrm{accuracy}(B_i) - \textrm{confidence}(B_i)|,
% \end{equation}

% where $\#B_i$ denotes the number of elements in the $i^{th}$ bin, $m$ denotes the number of bins and $n = \sum_{i=1}^m \#B_i$ the total number of elements used to estimate the ECE. Despite its popularity, ECE metric is tied to its binning strategy, which can potentially affect results. Although ECE with equally spaced bins is the most commonly reported metric, an alternative was suggested that uses adaptive bin size to include the same number of samples per bin \cite{nixon2019measuring, nguyen2015posterior}. Alternatively, one can also use the Kolmogorov-Smirnov test to get rid of the binning strategy altogether \cite{gupta2020calibration}. We report results with standard ECE, but evaluate all main calibration results with the three aforementioned strategies and find all metrics to be extremely similar, see \cref{sec:ablation_calibration_metrics}.

% One peculiarity of calibration for segmentation models is that, given an image, each pixel is considered as a different prediction. This leads to a dramatic explosion of the number of calibration samples since a single CS image, of size $2048\times1024$, contains roughly 2M pixels. In \cref{sec:ablation_number_pixels_calibration} we ablate the variation of the different calibration metrics as we change the number of samples per image and observe that 20k pixels per image already offers a stable estimate of the ECE.

% \paragraph{Misclassification detection:} Although calibration error is a widely used reliability measure, it is not without flaws. As discussed in \cref{sec:related_work}, a random classifier can be perfectly calibrated. This illustrates that a reasonable request for a reliable model is to assign a larger confidence\footnote{We use max softmax as confidence in the paper but in \cref{sec:ablation_prob_vs_entropy} we present similar results with entropy.} to correct outputs than incorrect ones (on average). In the ideal case, if we sorted all predictions from least to most confident, we would have all the incorrect predictions first and then only correct ones. Misclassification detection measures how far away are we from such an ideal case. One of the standard metrics to measure is the Rejection-Accuracy curves \cite{fumera2002support, hendrycks2021natural}: we reject samples from least to most confident and compute the area under the accuracy \vs amount of rejected samples curve. Nevertheless, there is a caveat in this metric, the base accuracy (\ie the accuracy when no sample has been rejected) will be higher for well-performing classifiers, thus, this measure will be biased towards models with higher accuracy. Recently, a simple correction was suggested which normalizes the area under the curve by that of an oracle that sorts all incorrect samples first, moreover it also subtracts the baseline score where samples are sorted randomly \cite{malinin2019ensemble}. The resulting metric, known as the Prediction Rejection Ratio (PRR), will be positive if the model confidence is better than the random baseline and will have a maximum score of 100\% which would correspond to an ``oracle'' sorting. 

% \paragraph{Out-of-domain detection:} Another important aspect in reliability is that models are aware of their ``domain of expertise'' (\ie their training domain). When a sample differs significantly from the training domain, we would expect the model to be more uncertain of its prediction. Similarly to misclassification detection, in OOD detection we try to separate \id from \ood images based on the network confidence. This has 
% % clear applications, 
% \rict{broad applicability}
% such as 
% % rejecting strongly OOD samples 
% \rict{generating alerts for samples too far from the training} domain
% or, \rict{connecting to active learning}, gathering them for further review and annotation. As in the rest of our work, we consider a whole image to be out of domain if it presents a significant covariate shift, that is, CS would be considered in domain while IDD and ACDC out. Since we define in/out of domain samples at the image level, we consider the average confidence of all pixels in a given image. As discussed in \cref{sec:related_work}, we use the AUROC metric which goes from 0 to 1 (1 being the best score).


