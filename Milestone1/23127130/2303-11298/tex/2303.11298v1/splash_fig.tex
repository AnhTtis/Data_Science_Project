\begin{figure}
\centering
\includegraphics[width=\linewidth]{Figures/splash_v2_compressed.pdf}

\caption{\textbf{Top: mIoU and ECE \vs domain shift.} Errors are normalized with respect to the lowest error on the training distribution (Cityscapes). We compare recent segmentation models, both transformer-based (SETR \cite{ZhengCVPR21RethinkingSiSSeq2SeqPerspTransformers}, SegFormer \cite{XieNIPS21SegFormerSemSegmTransformers} and Segmenter \cite{StrudelICCV21SegmenterTransformerForSemSegm}) and convolution-based (ConvNext \cite{LiuCVPR22AConvNet4The2020s}) with ResNet baselines (UPerNet\cite{XiaoECCV18UnifiedPerceptualParsingSceneUnderstanding} and DLV3+\cite{ChenX17RethinkingAtrousConvolutionSemSegm}). All recent models (both transformers and CNNs) are remarkably more robust than ResNet baselines (whose lines in mIoU overlap), however, ECE increases sharply for all methods. \textbf{Bottom:} Sample images for each dataset.
}
\vspace{-5pt}
\label{figure:splash}
\end{figure}

%Comments from when it was in Introduction
% Greg --  I would detail which methods are transformer / Conv / ResNet (with citations) so it's stand alone for non experts too
% Greg -- couldn't we make the samples a bit bigger? putting the dataset name below for instance