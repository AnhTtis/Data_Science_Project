\section{Related work}

\label{sec:related_work}
% \rict{
% In this section we cover prior art closely related to our study. First, we discuss the state of the art in semantic segmentation. Next, we provide an overview of works tackling the robustness of neural networks. We follow by covering the state of the art in calibration techniques---namely methods to reduce the overconfidence of neural networks. We conclude with a paragraph summarizing other studies that share similarities with ours.
% }
We study robustness and uncertainty in 
% image 
semantic
segmentation. In doing so, we touch several fields, which we cover in the following. We further discuss related studies.

% \rict{Many different problem formulations fall under the large hood of ``uncertainty''. For what concerns \textit{model calibration},}

\myparagraph{Segmentation models} 
% \rict{Provided with an image, the segmentation task is that of predicting the semantic class each pixel belongs to.}
Modern segmentation pipelines typically consist of encoder-decoder architectures~\cite{CsurkaNow22SemanticImageSegmentation2DecasesOfResearch, BadrinarayananPAMI17SegnetDeepConvEncoderDecoder, NohICCV15LearningDeconvolutionNNSegmentation, RonnebergerMICCAI15UNetSegmentation}. Decoders are usually designed \textit{ad hoc} for segmentation, with DeepLab~\cite{ChenPAMI17DeeplabSemanticImgSegmentationDeepFullyConnectedCRF, ChenX17RethinkingAtrousConvolutionSemSegm, ChenECCV18EncoderDecoderAtrousSeparableConvSemSegm} and UPerNet~\cite{XiaoECCV18UnifiedPerceptualParsingSceneUnderstanding} being two of the most prominent. On the other hand, the evolution of 
% backbone models 
encoders
has been closely related to that of classification 
% models.
% ResNet~\cite{he2016deep} has been the most popular for years;
models, with ResNet~\cite{he2016deep} being one of the most popular for years.
%
The rise of transformers in computer vision~\cite{DosovitskiyICLR21AnImageIsWorth16x16WordsTransformersAtScale} has led to a flurry of works leveraging self-attention for 
% different tasks \cite{DosovitskiyICLR21AnImageIsWorth16x16WordsTransformersAtScale, touvron2021training, carion2020end, ZhengCVPR21RethinkingSiSSeq2SeqPerspTransformers, XieNIPS21SegFormerSemSegmTransformers, StrudelICCV21SegmenterTransformerForSemSegm, zhou2022understanding}. 
segmentation~\cite{ZhengCVPR21RethinkingSiSSeq2SeqPerspTransformers,StrudelICCV21SegmenterTransformerForSemSegm,XieNIPS21SegFormerSemSegmTransformers}.
% This has also lead to 
Novel convolutional architectures inspired by transformers have also risen~\cite{LiuCVPR22AConvNet4The2020s}. We compare several recent segmentation models 
% to 
against
ResNet baselines, in terms of both robustness and uncertainty.

\myparagraph{Robustness} The brittleness of neural networks to changes in the input domain is a well-studied problem and many sub-formulations exist~\cite{TaoriNeurIPS20MeasuringRobustnessToNaturalDistributionShifts}. 
Robustness against synthetic shifts takes into account 
% domains generated
samples crafted
by artificially altering images, for example injecting noise or blur (corruption robustness~\cite{HendrycksICLR19BenchmarkingNNRobustnessCommonCorruptionsPerturbations,KamannCVPR20BenchmarkingRobustnessSemSegmModels}), or crafting imperceptible perturbations to induce model failure (adversarial robustness~\cite{GoodfellowICLR15ExplainingHarnessingAdvExamples}).
% and many solutions have been devised 
% \cite{CsurkaBC17AComprehensiveSurveyDAForVisualApplications, VolpiNIPS19GeneralizingUnseenDomainsAdvDataAugm, ToldoX20UnsupervisedDASSReview}. 
Robustness against \textit{natural} shifts focuses on changes that may arise naturally, without human intervention~\cite{RechtICLR19DuImageNetClassifiersGeneralizeToImageNet,hendrycks2021natural}.

In this work we are interested in comparing the robustness of different off-the-shelf segmentation models under \textit{natural} domain shifts, since these are particularly relevant in real-world applications.
% \rict{since we have real-world applications in mind.}
In particular, we focus on semantic segmentation of urban scenes, hence, 
we evaluate models on samples from unseen geographical locations~\cite{VarmaWACV19IDDDatasetExploringADUnconstrainedEnvironments} and weather conditions~\cite{SakaridisICCV21ACDCAdverseConditionsDatasetSIS}.
% For a discussion of recent robustness studies see our analyses paragraph.
Segmentation robustness against natural shifts has been studied before~\cite{YueICCV19DomainRandomizationPyramidConsistencySimulationToReal,VolpiCVPR22OnRoadOnlineAdaptSiS}, yet not in tandem with uncertainty and within a large-scale study taking 
% in
into
consideration several recent models.\looseness=-1

% from rel work
% While it is relatively extended practice to measure robustness against simulated image corruptions~\cite{HendrycksICLR19BenchmarkingNNRobustnessCommonCorruptionsPerturbations, zhou2022understanding,KamannCVPR20BenchmarkingRobustnessSemSegmModels} (\eg blur, noise, etc) such synthetic interventions may not be representative of natural shifts \cite{taori2020measuring}. Thus, we argue that our setting is closer to real-world deployment scenarios. \todo{move to rel work}



% \myparagraph{Reliability}
\myparagraph{Uncertainty}
% \myparagraph{\rict{Calibration}}
Guo \etal \cite{GuoICML17OnCalibrationOfModernNNs} have shown that deep 
% learning 
models 
% tend to be 
are
overconfident. They have proposed a simple, yet effective solution known as temperature scaling (\ts) where the output logits are divided by a temperature parameter before 
% applying 
the softmax layer. Other calibration methods have been proposed (\eg, \cite{naeini2015obtaining, kull2019beyond, gupta2020calibration}), but \ts is still
% the most 
very
popular due to its simplicity and the fact that it does not alter predictions. 

Calibration with \ts is effective
% works reasonably well 
in \id settings; yet, Ovadia \etal \cite{ovadia2019can} have shown that model calibration degrades significantly out of domain. Some methods have been proposed that address this problem~\cite{pampari2020unsupervised, park2020calibrated, wang2020transferable}, by assuming access to unlabeled \ood images beforehand.
%---a constraint for real-world deployment. 
% To the best of our knowledge, only 
On the other hand, Gong~\etal~\cite{gong2021confidence} have proposed methods that improve \ood calibration without any data from the target domain. 
%\rict{(domain generalization settings)}. 
They propose to cluster the calibration set in different ``domains'' and find a different temperature value for each. At test time, images are calibrated using the temperature from the closest cluster. 
% On the other hand, while originally devised for \id calibration, 
%
\textit{Ad hoc} for semantic segmentation, Ding~\etal~\cite{ding2021local} propose a content-dependent calibration strategy that learns a small calibration network to predict a temperature for each pixel in an image.

In our study we test \id and \ood performance of several calibration methods, focusing on techniques that do not require access to \ood samples~\cite{GuoICML17OnCalibrationOfModernNNs,gong2021confidence,ding2021local}---as generally robustness is evaluated on unseen domains~\cite{HendrycksICLR19BenchmarkingNNRobustnessCommonCorruptionsPerturbations,KamannCVPR20BenchmarkingRobustnessSemSegmModels,TaoriNeurIPS20MeasuringRobustnessToNaturalDistributionShifts,pinto2022impartial}

% \looseness=-1

\myparagraph{Previous analyses}
In~\cref{tab:comparison} we 
compare
related studies 
% focused 
on different aspects of reliability. Several works have suggested that transformer-based \textit{classifiers} are more robust than CNNs \cite{BhojanapalliICCV21UnderstandingRobustnessTransformersImageClass, naseer2021intriguing, bai2021transformers, mao2022towards, paul2022vision}. Yet, the recent ConvNeXt \cite{LiuCVPR22AConvNet4The2020s} has challenged this result and later work have suggested that further investigation is needed~\cite{pinto2022impartial}. 
Minderer \etal \cite{minderer2021revisiting} have compared calibration of several classifiers, concluding that convolution-free models are more robust \textit{and} better calibrated. In contrast, Pinto \etal \cite{pinto2022impartial} have compared recent transformers and CNNs, arguing there is ``no clear winner''. 
Some works have compared the robustness of transformers and CNNs 
% in
for
segmentation~\cite{XieNIPS21SegFormerSemSegmTransformers, zhou2022understanding}---but only against \textit{synthetic} domain shifts.
% (\eg noise, blur).
We broadly study robustness \textit{and} uncertainty in segmentation under \textit{natural} domain shifts. Similarly to \cite{pinto2022impartial}, we do not observe a single model family which is better calibrated in all scenarios. In contrast with \cite{minderer2021revisiting} though, we observe that robustness and calibration \textit{do not} go hand in hand. 
This shows that not all trends observed in classification transfer to segmentation, confirming the importance of task-specific studies like ours.





% \myparagraph{Uncertainty estimation} To be safely deployed in the real world, deep learning models should be reliable. We evaluate the reliability of segmentation models on three main aspects, model calibration, misclassification detection and out of domain detection.

% \myparagraph{Calibration} Ideally, a well calibrated model should be more confident for correct predictions than incorrect ones,
% %this is often measured by the Expected Calibration Error (ECE).
% Guo \etal \cite{GuoICML17OnCalibrationOfModernNNs} observed that deep learning models tend to be overconfident and propose a simple, yet effective solution known as \ts where the output logits are divided by a ``temperature'' parameter before applying the softmax layer. Although other calibration methods have been proposed \cite{naeini2015obtaining, kull2019beyond, gupta2020calibration} \ts is still the most popular due to its simplicity and that it does not 
% % change 
% alter
% the predicted labels. 

% Ovadia \etal \cite{ovadia2019can} showed that calibration of models degrades under domain shifts. Some methods have been proposed to improve calibration under covariate shift \cite{pampari2020unsupervised, park2020calibrated, wang2020transferable}, but they assume access to the out of domain images (without labels) beforehand, which is a strong constraint for real-world deployment. To the best of our knowledge, only one technique has been proposed to improve calibration without any data from the target domain \cite{gong2021confidence} which relies on clustering the calibration set to find different \ts parameters depending on the image features. On the other hand, while originally devised to improve calibration \id, Ding \etal propose a content-dependent calibration strategy, specifically designed for segmentation, that learns a small calibration network to predict a temperature for each pixel in an image \cite{ding2021local}. In our experiments we study these two techniques with several segmentation models to improve calibration out of domain.

% Recently, Minderer \etal \cite{minderer2021revisiting} compared the calibration of several classification models and concluded transformers are better calibrated than CNNs, however, the more recent ConvNeXt \cite{LiuCVPR22AConvNet4The2020s} was not in the analysis and Pinto \etal \cite{pinto2022impartial} later observed there was ``no clear winner''. Unlike previous analyses, our work is centered on segmentation with special emphasis on domain shifts and we explore methods to improve calibration out of domain. 

% \myparagraph{Misclassification detection} Although calibration is an intuitive and reasonable metric, it has some blind spots, for instance, a random binary classifier that always predicts with $50\%$ confidence will be perfectly calibrated in a balanced set but its confidence is completely uninformative. The idea of the misclassification detection task is to predict the samples in which the prediction is wrong. Following \cite{malinin2019ensemble} we use the Prediction Rejection Ration (PRR) since they showed other metrics can be biased by the model performance.

% \myparagraph{Out of domain detection} Aside from correctly assessing the uncertainty in their predictions, perhaps an even more fundamental task for reliable models should be to detect when a certain input does not fall into the training domain, we use the Area Under the Receiver Operating Characteristic curve (AUROC) \cite{murphy2012machine} since it is agnostic to unbalanced classes and we have different number of in and \ood images \cite{pinto2022impartial}.
