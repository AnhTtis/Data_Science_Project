\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage[natbib=true, style=ieee,sorting=none, isbn=false]{biblatex}
\usepackage{doi}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[misc]{ifsym} % for envolope sign
\addbibresource{library.bib}


\title{Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{
 \Letter \hspace{0.3em} Johannes Thalhammer$^{1-4}$, Manuel Schultheiß$^\mathrm{{1-3}}$, Tina Dorosti$^\mathrm{{1-3}}$, Tobias Lasser$^\mathrm{{2,5}}$,\\ \textbf{Franz Pfeiffer$^\mathrm{{1-4}}$,
  Daniela Pfeiffer$^\mathrm{{1-4}}$, Florian Schaff$^\mathrm{{1,2}}$}\\\\
  1 Chair of Biomedical Physics, Department of Physics, School of Natural Sciences\\
  2 Munich Institute of Biomedical Engineering\\
  3 Department of Diagnostic and Interventional Radiology, School of Medicine, Klinikum rechts der Isar\\
  4 Institute for Advanced Study\\
  5 Computational Imaging and Inverse Problems, Department of Informatics, School of Computation,\\ Information, and Technology\\\\
  Technical University of Munich, Germany\\
  %\texttt{\{Author1, Author2\}email@email} \\
 \Letter \hspace{0.3em} \texttt{johannes.thalhammer@tum.de} \\
  }

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Improving Hemorrhage Detection in Sparse-view CT via CNN based Artifact Reduction}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Johannes Thalhammer},
pdfkeywords={Artifact reduction, computed tomography (CT), deep learning, hemorrhage detection, sparse-view CT, U-Net},
}

\begin{document}
\maketitle

\begin{abstract}
Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. 
For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. 
However, the increased health risk caused by radiation is a concern. 
The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. 
Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality.
In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. 
We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. 
Our results suggest that the automated classification and detection accuracy of hemorrhages in sparse-view CCTs can be improved substantially by the U-Net. 
This demonstrates the feasibility of rapid automated hemorrhage detection on low-dose CT data to assist radiologists in routine clinical practice.
\end{abstract}


% keywords can be removed
\keywords{Artifact reduction \and computed tomography (CT) \and deep learning \and hemorrhage detection \and sparse-view CT \and U-Net}


\section{Introduction}
Intracranial hemorrhage (ICH) refers to a pathological bleeding within the cranial vault. 
It is a potentially life-threatening disease with a median 30-day fatality rate of 40.4\%, accounting for approximately 10\% to 30\% of strokes annually \cite{vanAsch2010IncidenceMeta-analysis} \cite{Feigin2009WorldwideReview}. 
Its most important risk factors include hypertension \cite{Ikram2012InternationalHemorrhage}, cerebral amyloid angiopathy \cite{Charidimou2012SporadicSpectrum} and anticoagulation treatment \cite{ODonnell2010RiskStudy}. 
Furthermore, factors like drug abuse \cite{Martin-Schild2010IntracerebralUsers} and excessive alcohol consumption \cite{Ikram2012InternationalHemorrhage} are linked to an increased risk. 
In the event of an ICH, rapid and accurate diagnosis is crucial for an optimal treatment. The clinical standard for diagnosis is an intracranial Computed Tomography (CT) scan \cite{Currie2016ImagingInjury}. 
However, with the increasing use of medical CT, not only for intracranial scans, concerns over the associated health risk caused by radiation exposure are gaining more and more attention \cite{Brenner2007ComputedExposure} \cite{Smith-Bindman2009RadiationCancer}. 
Especially for CCTs, the associated overall median effective dose of 2 mSV for a single routine head CT is close to the natural radiation accumulated per year and affects not only the brain but also nearby areas, for example the lenses of the eye, which have a particularly high sensitivity to radiation \cite{Smith-Bindman2009RadiationCancer} \cite{BfS_Radiation} \cite{Poon2019RadiationBrain}.\\

Besides lowering the X-ray tube current, sparse-view CT is a promising approach to reduce dose by decreasing the number of views. However, this decrease leads to severe artifacts in filtered backprojection (FBP) reconstruction. Hence, adequate processing is required to restore image quality to an acceptable level for diagnostic CT images.
In the past, compressed sensing (CS) and iterative reconstruction (IR) approaches have been widely investigated, which generally minimize a CS-based regularization term and a data-fidelity term that ensures data consistency. While these approaches have been proven to produce good results, they are also very computationally expensive due to the repeated update steps during iterative optimization \cite{Hu2017AnReconstruction} \cite{Ritschl2011ImprovedData} \cite{Lauzier2013CharacterizationReduction}.

Recently, machine learning approaches using deep neural networks have gathered vast scientific attention. It has been shown that in the context of artifact reduction in sparse-view CT, excellent results can be achieved with comparably low computational effort during inference \cite{Han2018FramingCT} \cite{Jin2017DeepImaging} \cite{Shen2019Patient-specificLearning}.
Approaches that combine deep learning and iterative techniques can also be found in literature \cite{Wu2021DRONE:Reconstruction} \cite{Chen20204D-AirNet:Learning}.

On the other hand, extensive research has been conducted on utilizing deep learning techniques for automated detection and classification of pathological features in CT images to aid radiologists in clinical practice \cite{Wang2021AScans} \cite{Wang2021DeepTomography} \cite{Naik2021LungLearning} \cite{Subhalakshmi2022DeepImages}. Despite their outstanding performance, the presence of artifacts in sparse-view CT images can impair the accuracy of these algorithms. 

The excellent results achieved by convolutional neural networks in reducing artifacts, coupled with their efficient computational performance, served as a motivation for our study. We aim to explore the potential benefits of deep learning based artifact reduction in sparse-view CCTs and to determine whether this approach could enhance automated hemorrhage detection.

For that, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. The performance of the U-Net is compared to an analytical approach based on total variation (TV) \cite{Chambolle2004AnApplications}. We evaluate the hemorrhage detectability of the predicted full-view CCTs by a hemorrhage classification convolutional neural network, trained on full-view data to detect and classify different types of intracranial hemorrhages. To quantify the classification performance, we use the area under the receiver operator characteristic curve (AUC-ROC) \cite{Hanley1982TheCurve}.
\section{Materials and Methods}
The code associated with this publication can be found at \url{https://github.com/ThalhammerJ/Sparse-View-Cranial-CT-Reconstruction}.
\subsection{Network Architectures}

%U-Net
The network architecture used by us follows the U-Net architecture with an additional skip connection between the input and output of the network, similar to the implementation by Jin et al. \cite{Ronneberger2015U-net:Segmentation} \cite{Jin2017DeepImaging}. The architecture is depicted in figure \ref{fig:U-Net}. 
A 512x512 input image is processed by a series of convolutional layers and downsampled by four pooling layers, leading to a resolution of 32x32 at the bottleneck. 
The pooling is performed by strided convolutions with stride (2, 2), followed by a 1x1 convolution. 
The unpooling is implemented with transposed strided convolutions, again with stride (2, 2). 
The use of strided convolutions was adapted from the U-Net implementation by Guo et al. \cite{Guo2019TowardPhotographs}. \\ 
As described by Han et al., one of the most important features of the multi-resolution architecture of the U-Net is the exponentially large receptive field due to the pooling and unpooling layers \cite{Han2018FramingCT}. This feature allows the U-Net to deal with streak artifacts that occur in sparse-view CT and typically spread over a large portion of the image.

For the classification of the hemorrhage types, the EfficientNetB2 architecture implemented in Keras was used \cite{Tan2019EfficientNet:Networks} \cite{chollet2015keras}. Because of the shown benefits of using pre-trained models in image recognition tasks in terms of convergence, robustness and accuracy, the network was initialized with ImageNet pre-trained weights \cite{Hendrycks2019UsingUncertainty} \cite{Pondenkandath2018IdentifyingMotifs} \cite{Yu2022TransferSurvey} \cite{Deng2009ImageNet:Database}. The final dense layer with originally 1000 output classes was substituted by a dense layer with 6 output classes. Due to the use of pre-trained weights, the network expects three input channels of shape 260x260 pixels in the [0, 255] range.

\begin{figure*}[ht]
\centerline{\includegraphics[width=1.\columnwidth]{Figures/architecture_u_net.pdf}}
\caption{Architecture of the used U-Net for a 512x512 input.}
\label{fig:U-Net}
\end{figure*}

%Detailed description too much I guess.
%The input image with dimensions 512x512 is passed to a block of four convolutional layers with 64 feature channels, respectively.
%The output from this block is downsampled by a strided convolution with stride (2, 2) and 256 output feature channels. Subsequently, a convolution with a 1x1 kernel and 128 feature channels is performed. After that, another block consisting of three convolutional layers, each with 128 feature channels is introduced. This procedure is repeated three times, where the respective spatial dimensions are halved, while the number of feature channels are doubled compared to the ones of the previous level. 
%The resulting feature map of size 64x64 with 512 feature channels is then again downsampled by a strided convolution with stride (2, 2) and 2048 output feature channels. 
%After a 1x1 convolution, a block of 7 convolutional layers each with 1024 feature channels is introduced. Subsequently, a convolutional layer with 2048 layers is deployed. The output is then upsampled by a strided transposed convolution with stride (2, 2) and 512 output feature channels.
%These feature maps are then summed with the output of the previous 

\subsection{Dataset}
The dataset used was taken from the RSNA 2019 Brain CT Hemorrhage Challenge \cite{RSNAChallenge}. 
It was compiled from the clinical PACS (picture and archiving and communication system) archives from three institutions, namely: Stanford University, Universidade Federal de S$\tilde{\text{a}}$o Paulo and Thomas Jefferson University Hospital.
The annotation was performed in a collaborative effort between the RSNA and the American Society of Neuroradiology (ASNR). 
The label for each CT image is a six dimensional vector with binary entries, where the first entry indicates whether a hemorrhage is present or not, and the remaining five labels indidicate which of the five possible subtypes (subarachnoid, intraventricular, subdural, epidural, and intraparenchymal) is present. Each image is provided in DICOM format \cite{Flanders2020ConstructionChallenge}.
In total, there are 21,784 labeled CT scans in the dataset from 18,938 different patients with a total of 752,803 individual images. 
After sorting out all images which were either not 512x512 or produced some different error during the data generation process, 696,842 images from 18,545 patients remained. \\
For the training of the U-Net, 4000 patients were randomly selected. The data from 2400 patients (95384 individual images) served as training data, the data from 600 patients (23842 individual images) was used for validation. The remaining 1000 patients (39425 individual images) were held back for testing purposes. 
The pixel intensities in the DICOM format are usually stored as 12-bit numbers, leading to possible intensity values in the range [0-4095]. To produce sparse-view CTs, the data was normalized to range [0, 1] by division by 4095. After that, sinograms with 4096 views were created under parallel beam geometry from the CT dataset using the Astra Toolbox \cite{Palenstijn2011PerformanceGPUs}. For the ground truth images, CT images were reconstructed again via FBP using 4096 views.
Six corresponding sparse-view CT data subsets at varying levels of undersampling were generated using FBP with only 64, 128, 256, 512, 1024 and 2048 views, respectively. 


Regarding the training of the EfficientNetB2 classification network, the same subset of 1000 patients was used for testing as for the U-Net. Because overfitting was much more of a problem for the EfficientNetB2, the training set from before was expanded by the remaining data. Therefore, the training set for the classification task consisted of 17,545 individual patients with a total of 657,417 images. 
The individual images were first scaled to Hounsfield Units (HUs) using the rescale slope and intercept provided in the DICOM metadata. After that, the images were clipped to the diagnostically relevant brain window ranging from 0 HU to 80 HU. To meet the requirements of the pre-trained ImageNet weights, the images were rescaled to range [0, 255] by multiplying the pixel values with $\frac{255}{80}$, resized to 260x260 pixels by the biliniar resize function from the tensorflow library, and transformed to three channel images by concatenating three neighboring CT images \cite{Abadi2019Tensorflow:2016}. \\
Figure \ref{fig:dataset} depicts the distribution of the labeled hemorrhage subtypes in the used training set of the EfficientNet, the U-Net and the test set in descending order. The majority of the used images have no hemorrhage labeled. The subtypes subdural, intraparenchymatous, subarachnoid, and intraventricular occur in the same order of magnitude, whereas the epidural subtype is much less common.

 \begin{figure}[ht]
    \centerline{\includegraphics[width=.5\columnwidth]{Figures/dataset_statistics.png}}
    \caption{Distribution of the labeled hemorrhage subtypes in the used training set of the EfficientNetB2, the U-Net and in the test set. Note the logarithmic scaling of the x-axis.}
    \label{fig:dataset}
\end{figure}

\subsection{Training}
The U-Net architecture was trained separately for each subset of the sparse-view data, ranging from 64 to 2048 views, receiving the sparse-view images as input and the fully sampled image as ground truth. 
The U-Net was trained with mean squared error loss for 75 epochs with a mini-batchsize of 32. From each image, a patch of size 256x256 was randomly selected and randomly rotated by either 0$^\circ$, 90$^\circ$, 180$^\circ$, or 270$^\circ$.
The learning rate $lr$ of the networks was determined for each epoch by $lr=10^{-4}/(epoch +1)$. \\
The training scheme for the EfficientNetB2 was adapted from Wang et al. \cite{Wang2021AScans}.
The model was trained with k-fold cross validation with k=5 and binary crossentropy loss. 
The learning rate was determined using a cosine decay schedule with restarts after epoch one, three, and seven with a initial learning rate of $5\cdot10^{-4}$ and a minimal learning rate of $10^{-5}$. The network was trained with a mini-batchsize of 32 for 15 epochs.

\subsection{Total Variation} \label{sec:tv}

In this work, the total variation (TV) denoising method by Chambolle implemented in the scikit-image library was used \cite{Chambolle2004AnApplications} \cite{vanderWalt2014Scikit-image:Python}.
The optimal weight for the TV algorithm was chosen for each sparse subset by randomly sampling 1000 images from the training set and iterating through weights ranging from 0.001 to 1.000 in 0.001 increments.
For each level of subsampling, the weight that resulted in the best score on the structural similarity index measure (SSIM) was then used to calculate the metrics on the test set \cite{Wang2004ImageSimilarity}.

\subsection{AUC-ROC} \label{sec:AUC-ROC}

To quantify the hemorrhage classification performance, the AUC-ROC metric was used \cite{Hanley1982TheCurve}. 
First, predictions on the test dataset for the raw and post-processed reconstructions are generated for each level of sparse sampling. 
The ROC curves are then created by plotting the true positve rate (TPR) against the false positve rate (FPR) at different classification thresholds. To obtain the final AUC-ROC values, the area under the respective curves are calculated. 
For all these operations the scikit-learn library was used \cite{Pedregosa2011Scikit-learn:Python}.

\section{Results}

\begin{figure*}[ht]
    \centerline{\includegraphics[width=1.\columnwidth]{Figures/Fig_diseased.pdf}}
    \caption{CT image from the test set labeled with intraparenchymal (cyan arrow) and intraventricular (orange arrow) hemorrhage. Image a) shows the image reconstructed from 4096 views using FBP. Images b) - e) show the same image reconstructed from 512, 256, 128, and 64 views, respectively. Images e)- h) show the U-Net predictions of the corresponding sparse-view images in the upper row. All images are presented in the brain window ranging from 0 HU to 80 HU. Both inserts are 80x80 pixels.}
    \label{fig:diseased}
\end{figure*}

In the following, the results of the artifact reduction by the U-Net is presented. The outcomes are compared with artifact reduction by TV by visual impressions as well as in terms of peak-signal-to-noise ratio (PSNR) and SSIM values. Subsequently, the hemorrhage detection and classification performance on the raw sparse-view images and the images post-processed by either the U-Net or the TV method is presented. 
\subsection{Artifact Reduction}



Figure \ref{fig:diseased} shows an example of the artifact reduction by the U-Net on a CT image from the test set, reconstructed from a varying number of views. 
According to the provided label, an intraparenchymal and an intraventricular hemorrhage is present in the depicted example. The extracts depicted below the respective images display the zoomed in hemorrhages.
The shown images were rescaled to HU and clipped to the brain window. 
In a) the image reconstructed from 4096 views which serves as a ground truth image is shown. 
Here, the labeled intraparenchymal hemorrhage (cyan arrow) is clearly visible. 
The intraventricular subtype (orange arrow) is more difficult to detect but still stands out from the background tissue. 
Images b) - e) depict the same image reconstructed with 512, 256, 128, and 64 views, respectively. 
In b) it can be seen, that the image quality deteriorates and artifacts become visible. However, the hemorrhages are still recognizable. 
In image c) the streak artifacts become more pronounced, making it more difficult to distinguish small features. 
The intraparenchymal subtype is still distinguishable, the intraventricualar subtype on the other hand is only barely recognizable. 
In images d) and e) the streak artifacts are even more dominant and no features of the tissue inside the cranial vault can be recognized anymore. The hemorrhages are also not detectable. 
In images f) - i) the predictions of the U-Net of the corresponding sparse-view images are depicted.
In all cases we note a clear reduction in streak artifacts compared to the respective input images b) - e). With increasingly sparse-sampled input, the prediction also tend to become smoother, i.e. sharp image features are not retained. Despite the image smoothing during U-Net processing, image quality of the sparse-view CTs still improves, making the predictions appear much more similar to the full-view images. 
This improves the visibility of pathological features, as seen at the hemorrhages present.
The contours of the intraparenchymal hemorrhage can be recognized up to including 128-view sparse-sampling (h), those of the intraventricular hemorrhage up to including 256-view (g).\\

\subsection{Comparison with Total Variation (TV)}

\begin{figure*}[ht]
\centerline{\includegraphics[width=1.\columnwidth]{Figures/Fig_healthy.pdf}}
\caption{CT image from the test set labeled as ”healthy”. Images a) - f) shows the FBP reconstruction from 2048, 1024, 512, 256, 128, and 64 views, respectively. Images g) - m) show the U-Net predictions of the respective images in the upper row, images n) - s) show the results of the TV method. The presented SSIM and PSNR values were calculated over the entire CT image scaled to (0, 1) from the full range. All images are presented in the brain window ranging from 0 HU to 80 HU. The insert is 100x100 pixels.}
\label{fig:healthy}
\end{figure*}

\begin{table*}[ht]
    \centering
    \scalebox{0.87}{
    \begin{tabular}{c|cccccc}
        SSIM & 2048 views & 1024 views & 512 views & 256 views & 128 views & 64 views\\ 
        \hline
        FBP & 1.000 $\pm$ 0.000 & 0.994 $\pm$ 0.006 & 0.974 $\pm$ 0.025 & 0.906 $\pm$ 0.078 & 0.747 $\pm$ 0.149 & 0.574 $\pm$ 0.190\\
        U-Net & 1.000 $\pm$ 0.000 & \textbf{0.999 $\pm$ 0.001} & \textbf{0.997 $\pm$ 0.003}& \textbf{0.990 $\pm$ 0.008} & \textbf{0.985 $\pm$ 0.010} & \textbf{0.960 $\pm$ 0.024}\\
        TV & 1.000 $\pm$ 0.000 & 0.997 $\pm$ 0.003 & 0.991 $\pm$ 0.007 & 0.981 $\pm$ 0.012 & 0.946 $\pm$ 0.034 & 0.873 $\pm$ 0.073 \\
        \hline\hline
        PSNR[dB] & 2048 views & 1024 views & 512 views & 256 views & 128 views & 64 views\\ 
        \hline
        FBP & 68.102 $\pm$ 10.194 & 54.797 $\pm$ 12.674 & 45.268 $\pm$ 12.334 & 35.860 $\pm$ 10.751 & 27.631 $\pm$ 9.687 & 20.766 $\pm$ 9.372\\
        U-Net & \textbf{70.490 $\pm$ 8.476} & \textbf{59.812 $\pm$ 9.030} & \textbf{52.575 $\pm$ 10.292} & \textbf{44.707 $\pm$ 8.042} & \textbf{41.300 $\pm$ 5.239} & \textbf{35.256 $\pm$ 4.376}\\    
        TV & 61.991 $\pm$ 6.661 & 53.350 $\pm$ 10.061 & 44.323 $\pm$ 10.155 & 36.293 $\pm$ 9.849 & 29.464 $\pm$ 9.798 & 23.590 $\pm$ 9.780\\
    
    \end{tabular}
    }
    \caption{SSIM and PSNR values of the sparse-view CT images reconstructed by FBP, the predictions of the U-Net, and the TV method. The values were calculated on the test set with with respect to the ground truth reconstructions.}
    \label{tab:values}
\end{table*}

\begin{figure*}[ht]
\centerline{\includegraphics[width=1.\columnwidth]{Figures/fig_auc_plots.pdf}}
\caption{Results of the EfficientNetB2 classification network. Images a) - f) depict the AUC values of the ROC curves associated with the any, subdural, subarachnoid, intraparenchymal, intraventricular, and epidural class, respectively.}
\label{fig:aucs}
\end{figure*}

To put the performance of the U-Net into perspective, we also implemented artifact reduction using the TV method for comparison. TV is commonly applied to in these type of undersampling problems \cite{Sidky2008ImageMinimization} \cite{Zhang2014Few-viewVariation}.\\
Figure \ref{fig:healthy} compares results of one image labeled "healthy" from the test set that was processed in various different ways. 
The shown SSIM and PSNR values were calculated with respect to the image reconstructed from 4096 views. 
Images a) - f) show the image reconstructed using FBP from 2048, 1024, 512, 256, 128 and 64 views without any post-processing, respectively. 
While the images reconstructed from 2048 views (a) shows no artifacts, the image quality deteriorates if the number of views is further reduced. 
For the 64-view reconstruction only the shape of the skull is recognizable, but not the tissue inside the cranial vault. 
 In images g) - m) the respective predictions by the U-Net of the sparse-view CTs in the upper row are shown. 
Consistent with the results shown in figure 3, the UNet is able to reduce the artifacts considerably. 
We once again note that the resulting images are increasingly smoothed as the number of views decreases.
In images n) - s) the results of TV post-processing are depicted. 
TV is able to reduce streak artifacts comparatively well down to 256 views sparse sampling. 
However, the results for 128- and 64-sparse view data are sub-par compared to the results from U-Net processing. 
While both post-processing methods are able to reduce the artifacts and improve the image quality compared to the raw sparse-view FBP reconstruction, the U-Net produces superior results both visually as well as in terms of the calculated SSIM and PSNR values.\\
Table \ref{tab:values} depicts the average SSIM and PSNR values of the reconstructed images calculated on the entire test set. The individual values were calculated with respect to the ground truth reconstructions. 
Comparison of the SSIM values of the unprocessed, U-Net processed, and TV processed sparse-view CTs confirm quantitatively that both methods increased image similarity to the fully sampled reconstructions by reducing streak artifacts. 
The quantitative assessment is in agreement with the visual result of U-Net post-processing depicted in figure \ref{fig:diseased}.\\
The PSNR values are also improved by U-Net post-processing. Interestingly, no clear trend in the PSNR values can be identified for TV processing.
This is most likely due to the fact that the weights for the TV method were set to optimize the SSIM, rather than PSNR, as described in section \ref{sec:tv}.
The direct comparison of SSIM and PSNR values between U-Net and TV processing reveals a stronger performance of the U-Net in all investigated cases.\\


\subsection{Detection of Hemorrhage Subtypes}

Finally, we evaluate the results of the artifact reduction via the EfficientNetB2 hemorrhage classification network, trained on fully sampled CT images.\\
In figure \ref{fig:aucs} the AUC-ROC values for the raw images (blue) and the images post-processed by either TV (green) or the U-Net (orange) are shown for varying levels of subsampling.
In a) the AUC-ROC values of the "any" class are depicted, which indicates whether any kind of hemorrhage is present in the CT image or not. 
In subfigures b) - f) the AUC-ROC plots of the classified subtypes subdural, subarachnoid, intraparenchymal, intraventricular, and epidural are shown. 
In all cases the classification performance declines as the number of views used for the reconstruction decreases. The drop is most pronounced in the raw images, followed by the images post-processed by TV and least proncounced by the images post-processed by the U-Net.
For the raw reconstructions, the 2048-view images yield almost identical AUC-ROC values as the full-view images. When the number of views is reduced to 1024, the classification performance decreases slightly. By further decreasing the number of views, the AUC-ROC values drop substantially. 
For the images post-processed by the TV algorithm, the AUC-ROC values decrease slightly down to 512-views and decline substaintially for fewer views in subfigures a) - e).
For the epidural subtype in f) the hemorrhage detection performance of the raw and TV post-processed images are nearly identical down to 256 views. 
By further decreasing the number of views, the classification network performs substantially worse on the TV images than on the raw sparse-view ones. A possible explanation for this is that the artifacts smoothed by the TV algorithm get classified as epidural hemorrhages, leading to a high false positive rate. 
With the U-Net, the number of views can be reduced from 4096 views down to 512 views with almost no decrease in classification performance and to 256 views with a slight performance decrease. Below 256 views, a distinct decline in classification performance is also perceptible.
In all cases, the AUC-ROC values obtained from the images post-processed by the U-Net are superior compared to the TV processed and raw sparse-view images. 
When looking at Figure f), it is also noticeable that the detection of epidural hemorrhages is significantly worse compared to the other subtypes. This is most likely due to the fact that this type of hemorrhage was much less represented in the dataset than the other subtypes (cf. figure \ref{fig:dataset}). \\
The individual ROC curves of the EfficientNetB2 classification results used for the calculation of the AUC-ROC values are plotted in figure \ref{fig:rocs}.

\begin{figure*}[ht]
\centerline{\includegraphics[width=1.\columnwidth]{Figures/roc_table.pdf}}
\caption{Results of the EfficientNetB2 hemorrhage classification network. Depicted are the ROC curves of the classes any, subdural, subarachnoid, intraparenchymal, intraventricular, and epidural, for the different levels of subsampling. In addition to the results of the raw images (blue), the classification results on the images post-processed by either U-Net (orange) or TV (green) are shown.}
\label{fig:rocs}
\end{figure*}

\section{Conclusion}

In this work it was shown, that a deep, U-Net-type neural network is able to substantially improve the quality of cranial sparse-view CTs visually, as well as in terms of calculated PSNR and SSIM values. 
A hemorrhage detection and classification network was trained on fully sampled data and applied to the sparse-view data with and without post-processing by the U-Net. 
With the U-Net, the number of views can be reduced from 4096 views to 512 views with almost no decrease in classification performance and to 256 views with a slight performance decrease.
The results of the U-Net were compared with an analytical approach based on TV.
The U-Net was found to perform superior to TV post-processing with respect to image quality parameters and automated hemorrhage diagnosis. 
Our results suggest that the classification accuracy of hemorrhages in sparse-view CCTs can be improved substantially by deep learning methods. This demonstrates the feasibility of rapid automated hemorrhage classification on sparse-view CCT data to assist radiologists in routine clinical practice.

\section{Acknowledgements}

This work was funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the Länder, the German Research Foundation, as well as by the Technical University of Munich–Institute for Advanced Study.

%\bibliographystyle{unsrtnat}
 %\bibliography{library}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .
\printbibliography


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
 % \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
