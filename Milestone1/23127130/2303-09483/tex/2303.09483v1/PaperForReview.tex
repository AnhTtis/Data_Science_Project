% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[accsupp]{axessibility}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{6919} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


%%%%%%%%%%Custum new package & command
\usepackage{color, colortbl}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{graphicx,wrapfig,lipsum}

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output


\newcommand\norm[1]{\lVert#1\rVert}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\definecolor{Gray}{gray}{0.9}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning}

\author{Sanghwan Kim \quad  Lorenzo Noci  \quad Antonio Orvieto  \quad Thomas Hofmann\\
ETH Zürich\\
Zürich, Switzerland\\
{\tt\small \{sanghwan.kim, lorenzo.noci, antonio.orvieto, thomas.hofmann\}@inf.ethz.ch}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and Lorenzo Noci 
%\and Antonio Orvieto 
%\and Thomas Hofmann
%Lorenzo Noci\\
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from \emph{catastrophic forgetting}, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (\emph{plasticity}) while still achieving high accuracy on the previous tasks (\emph{stability}). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose \emph{Auxiliary Network Continual Learning} (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and stability, surpassing strong baselines on task incremental and class incremental scenarios. Through extensive analyses on ANCL solutions, we identify some essential principles beneath the stability-plasticity trade-off. The code implementation of our work is available at \url{https://github.com/kim-sanghwan/ANCL}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}\label{sec:introduction}

The continual learning (CL) model aims to learn from current data while still maintaining the information from previous training data. The naive approach of continuously fine-tuning the model on sequential tasks, however, suffers from \emph{catastrophic forgetting} \cite{mccloskey1989catastrophic, goodfellow2013empirical}. Catastrophic forgetting occurs in a gradient-based neural network because the updates made with the current task are likely to override the model weights that have been changed by the gradients from the old tasks.

Catastrophic forgetting can be understood in terms of \textit{stability-plasticity dilemma} \cite{mermillod2013stability}, one of the well-known challenges in continual learning. Specifically, the model not only has to generalize well on past data (\textit{stability}) but also learn new concepts (\textit{plasticity}). Focusing on stability will hinder the neural network from learning the new data, whereas too much plasticity will induce more forgetting of the previously learned weights. Therefore, CL model should strike a balance between stability and plasticity. 

There are various ways to define the problem of CL. Generally speaking, it can be categorized into three scenarios \cite{van2019three} : \emph{Task Incremental Learning} (TIL), \emph{Domain Incremental Learning} (DIL), and \emph{Class Incremental Learning} (CIL). In TIL, the model is informed about the task that needs to be solved; the task identity is given to the model during the training session and the test time. In DIL, the model is required to solve only one task at hands without the task identity. In CIL, the model should solve the task itself and infer the task identity. Since the model should discriminate all classes that have been seen so far, it is usually regarded as the hardest continual learning scenario. Our study performs extensive evaluations on TIL and CIL setting which will be further explained in \cref{sec:experiment}.

Recently, several papers~\cite{wang2021afec, zhang2020class, liu2021adaptive, lin2022towards} proposed the usage of an auxiliary network or an extra module that is solely trained on the current dataset, with the purpose of combining this additional structure with the previous network or module that has been continuously trained on the old datasets. For example, \emph{Active Forgetting with synaptic Expansion-Convergence} (AFEC) \cite{wang2021afec} regularizes the weights relevant to the current task through a new set of network parameters called the expanded parameters based on weight regularization methods. The expanded parameters are solely optimized on the current task and are allowed to forget the previous ones. As a result, AFEC can reduce potential negative transfer by selectively merging the old parameters with the expanded parameters. The stability-plasticity balance in AFEC is adjusted via hyperparameters which scale the regularization terms for remembering the old tasks and learning the new tasks. 

The authors of the above papers propose to mitigate the stability-plasticity dilemma by infusing plasticity through the auxiliary network or module (detailed explanation in Appendix A). However, a precise characterization of the interactive mechanism between the previous model and the auxiliary model is still missing in the literature. Therefore, in this paper, we first formalize the framework of CL that adopts the auxiliary network called \emph{Auxiliary Network Continual Learning} (ANCL). Given this environment, we then investigate the stability-plasticity trade-off through various analyses from both a theoretical and empirical point of view.

Our main contributions can be summarized as follows:
\begin{itemize}
    \item We propose the framework of \emph{Auxiliary Network Continual Learning} (ANCL) that can naturally incorporate the auxiliary network into a variety of CL approaches as a plug-in method (\cref{subsec:formulation_of_ANCL}).

    \item We empirically show that ANCL outperforms existing CL baselines on both CIFAR-100 \cite{krizhevsky2009learning} and Tiny ImageNet \cite{le2015tiny} (\cref{sec:experiment}).   
    \item Furthermore, we perform three analyses to investigate the stability-plasticity trade-off within ANCL (\cref{sec:trade-off_analysis}): \emph{Weight Distance}, \emph{Centered Kernel Alignment}, and \emph{Mean Accuracy Landscape}.
\end{itemize}



\section{Related Work}
\label{sec:related_work}
Continual learning approaches can be roughly categorized into weight regularization \cite{kirkpatrick2017overcoming, aljundi2018memory, chaudhry2018riemannian, wang2021afec}, knowledge distillation \cite{li2017learning, jung2016less, dhar2019learning, zhang2020class}, memory replay \cite{rebuffi2017icarl, castro2018end}, bias correction \cite{wu2019large, zhao2020maintaining, hou2019learning, douillard2020podnet}, and dynamic structure \cite{liu2021adaptive, abati2020conditional, yan2021dynamically}.

\textbf{Weight Regularization Method: } A standard way to alleviate catastrophic forgetting is to include a regularization term which binds the dynamics of each network's parameter to the corresponding parameter of the old network. For example, \textit{Elastic Weight Consolidation} (EWC) \cite{kirkpatrick2017overcoming} calculates the regularizer through the approximation of Fisher Information Matrix (FIM). \textit{Memory Aware Synapses} (MAS) \cite{aljundi2018memory} proposes the regularizer which accumulates the changes of each parameter throughout the update history. Recently, \cite{wang2021afec} suggests a biologically inspired argument to propose Active Forgetting with synaptic Expansion-Convergence (AFEC) where an additional regularization term associated with expanded parameters (or auxiliary network) is added to the loss of EWC.


\textbf{Knowledge Distillation Method: } A separate line of work adopts knowledge distillation \cite{bucilua2006model, hinton2015distilling} which was originally designed to train a more compact student network from a larger teacher network. In this way, the main network can emulate the activation or logit of the previous (or old) network while learning a new task. For instance, \textit{Learning without Forgetting} (LwF) \cite{li2017learning} proposes to learn the soft target generated by the old network while \textit{less-forgetting learning} (LFL) \cite{jung2016less} regularizes the difference between the activations of the main network and the old network. Based on LwF, \textit{Learning without Memorizing} (LwM) \cite{dhar2019learning} takes advantage of the attention of the previous network to train the current network. A recent distillation approach called \textit{Deep Model Conolidation} (DMC) \cite{zhang2020class}  proposes \textit{double distillation loss} to resolve the asymmetric property of training between old and new classes using a new network (or auxiliary network) and an unlabeled auxiliary dataset.

\textbf{Memory Replay Method: } Unlike the previous methods, replay-based methods keep a part of the previous data (or exemplars) in a memory buffer. Then, a model is trained on the current dataset and the previous exemplars to prevent the forgetting of the previous tasks. \emph{Incremental Classifier and Representation Learning} (iCaRL) \cite{rebuffi2017icarl} proposes the usage of the memory buffer derived from LwF \cite{li2017learning}. Then, iCaRL calculates the mean feature representations for each class and selects the exemplars iteratively so that the mean of the exemplars is closer to the class mean in feature space, which is called \emph{herding} sampling strategy. Another replay-based approach named \emph{End-to-End Incremental Learning} (EEIL) \cite{castro2018end} introduces an additional stage called \emph{balanced training} to fine-tune the model on a balanced dataset. The balanced dataset consists of the equal number of exemplars from each class that have been seen so far.

\textbf{Bias Correction Method: } In memory replay methods, the network is trained on the highly unbalanced dataset composed of the few exemplars from the previous task and fresh new samples from the new ones. As a result, the network is biased towards the data of new tasks, and this can lead to distorted predictions of the model, which is called \textit{task-recency bias}. To solve this problem, \emph{Bias Correction} (BiC) \cite{wu2019large} introduces a two-stage training where they perform the main training in the first stage and subsequently mitigate the bias through a linear transformation. Likewise, Weight Aligning (WA) \cite{zhao2020maintaining} proposes two-stage training. The first stage is equal to that of BiC and they normalize the weight vectors of the new classes and the old classes to reduce the bias in the second stage. Another bias correction method called \emph{Learning a Unified Classifier Incrementally via Rebalancing} (LUCIR) \cite{hou2019learning} alleviates task-recency bias by including three components into their training: cosine normalization, less-forget constraint, and inter-class. Built upon LUCIR, \emph{Pooled Outputs Distillation Network} (PODNet) \cite{douillard2020podnet} applies pooled out distillation loss and local similarity classifier.

\textbf{Dynamic Structure Method: } Dynamic structure approaches use masking for each task or expansion of the model to prevent forgetting and increase the model capacity to learn a new task. For instance, \emph{Conditional Channel Gated Networks} (CCGN)~\cite{abati2020conditional} dynamically adds an extra convolutional layer whenever the model learns a new task and it is only optimized for the new data. \emph{Adaptive Aggregation Networks} (AANets) \cite{liu2021adaptive} expands a Residual Network (ResNet) \cite{he2016deep} to have the two types of residual block at each residual level to balance stability and plasticity: a stable block that is trained on a first task and frozen afterward and a plastic block that is freely trained on a current task. Another dynamic structure method called \emph{Dynamically Expandable Representation Learning} (DER) \cite{yan2021dynamically} suggests to expand a feature extractor. The new feature extractor is trained solely on the current dataset with channel level masking and the whole model is fine-tuned on balanced dataset.




\section{Method}
\label{sec:method}

In this Section, we propose \textit{Auxiliary Network Continual Learning} (ANCL), a framework which combines original \textit{Continual Learning} (CL) approaches with an auxiliary network (\cref{subsec:formulation_of_ANCL}). In addition, we explain the detailed training steps of ANCL (\cref{subsec:algorithm_of_ANCL}).

\begin{figure*}
%\centering
\centerline{\includegraphics[scale=.45]{Figures/tiny_CL_DCL_Concept_figure.jpg}}
\caption{Conceptual comparison of \emph{Continual Learning} (CL) and \emph{Auxiliary Network Continual Learning} (ANCL) (ours) on task $t$. (1) CL: the previous weights $\theta^{CL}_{t-1}$ are frozen in the old network as $\theta_{1:t-1}^*$ and the old network regularizes the main training through $\lambda$. (2) ANCL: the auxiliary network initialized by $\theta_{t-1}^{ANCL}$ is trained on the dataset $D_t$ and then frozen as $\theta_t^*$. It regularizes the main training via $\lambda_a$ in addition to the regularization of the old network.}
\label{fig:CL_ANCL_concept}
\end{figure*}

\subsection{The Formulation of Auxiliary Network Continual Learning}\label{subsec:formulation_of_ANCL}


ANCL applies the auxiliary network trained on the current task to the continually learned previous network to achieve a balance between stability and plasticity. \cref{fig:CL_ANCL_concept} illustrates the conceptual difference between CL and ANCL, where CL can be any continual learning method that includes a regularizer that depends on the old network. Before training on the dataset $D_t$ of task $t$, CL freezes and copies the previous continual model $\theta^{CL}_{t-1}$ that has been trained until task $t-1$ as the old network $\theta_{1:t-1}^*$. Then, the old network regularizes the main training through the regularization strength $\lambda$. We can formally define the loss of CL on task $t$ as follows:
\begin{equation}
    \mathcal{L}_\text{CL} = \mathcal{L}_\text{t}(\theta) + \Omega(\theta ; \theta_{1:t-1}^*, \lambda), \label{eq:CL_loss}
\end{equation}
where the first term denotes a task-specific loss with respect to main network weights $\theta\in\mathbb{R}^P$ and the second term represents the regularizer that binds the dynamic of the network parameters $\theta$ to the old network parameters $\theta_{1:t-1}^*\in\mathbb{R}^P$. $\lambda\in\mathbb{R}$ is the regularization strength which is usually selected by a grid search procedure. These two loss terms can be calculated on the current dataset $D_t$ or on the combined dataset $D_t^+$ (current dataset $D_t$ + previous exemplars $P_{1:t-1}$) depending on the method to which it is applied. In classification problems, the task-specific loss becomes cross-entropy loss.  
The original CL approaches mainly focus on retaining the old knowledge obtained from the previous tasks by preventing large updates that would depart significantly from the old weights $\theta_{1:t-1}^*$. However, this might harmfully restrict the model's ability to learn the new knowledge, which will hinder the right balance between stability and plasticity. 

On the contrary, ANCL keeps two types of network to maintain this balance: (1) the auxiliary network $\theta_t^*$, which is solely optimized on the current task $t$ allowing for forgetting (\textit{plasticity}) and (2) the old network $\theta_{1:t-1}^*$ that has been sequentially trained until task $t-1$ (\textit{stability}). 
Then, both models are used to construct the regularizers in the following objective:
\begin{equation}
    \mathcal{L}_\text{ANCL} = \mathcal{L}_\text{t}(\theta) + \Omega(\theta ; \theta_{1:t-1}^*, \lambda) + \Omega(\theta ; \theta_t^*, \lambda_a)\label{eq:ANCL_loss} ,
\end{equation}
where the first two terms are the same as in \cref{eq:CL_loss} and the last term promotes the learning of the new task $t$ based on the parameters of the auxiliary network $\theta_t^*\in\mathbb{R}^P$ and the regularization strength $\lambda_a\in\mathbb{R}$. Note that the new regularizer $\Omega(\theta ; \theta_t^*, \lambda_a)$ is obtained in the same way as the original method, and thus we expect our model to naturally merge the old feature representation (or weight itself) with the new one. This is mathematically explained in Appendix E where we analyze and compare the gradient of CL and ANCL. 
Moreover, we initialize the auxiliary network with the old network parameters so that the auxiliary model is weakly biased toward the old model, thus facilitating the integration of the two models in the corresponding regularizers of Eq. \ref{eq:ANCL_loss}.



\begin{table}
\centering
\begin{tabular}{cc}
\hline
Methods                              & $\Omega(\theta ; \theta^*, \lambda)$         \\ \hline
EWC \cite{kirkpatrick2017overcoming} & $\frac{\lambda}{2}\sum_{i} F_{i}(\theta_{i}-\theta_{i}^*)^2$     \\
MAS \cite{aljundi2018memory}         & $\frac{\lambda}{2}\sum_{i} M_{i}(\theta_{i}-\theta_{i}^*)^2$     \\
LwF \cite{li2017learning}            & $\lambda \sum_{c=1}^{C_{1:t}} -y^c(x;\theta^*) \log{y^c(x;\theta)}$ \\
LFL \cite{jung2016less}              & $\lambda \norm{f(x; \theta) - f(x;\theta^*)}_2^2$  \\ \hline
iCaRL \cite{rebuffi2017icarl}        & $\lambda \sum_{c=1}^{C_{1:t}} -y^c(x;\theta^*) \log{y^c(x;\theta)}$ \\
BiC \cite{wu2019large}               & $\lambda \sum_{c=1}^{C_{1:t}} -y^c(x;\theta^*) \log{y^c(x;\theta)}$ \\
LUCIR \cite{hou2019learning}         &   $\lambda (1-\langle\bar{f}(x;\theta), \bar{f}(x;\theta^*)\rangle)$   \\
PODNet \cite{douillard2020podnet}    &  \begin{tabular}[r]{@{}r@{}} $\lambda [ \sum_{l=1}^{L-1} \mathcal{L}_\text{POD-spatial}(f_l(x;\theta), f_l(x;\theta^*))$ \\  $+ \mathcal{L}_\text{POD-flat}(f_L(x;\theta), f_L(x;\theta^*))]$ \end{tabular}   \\ \hline
\end{tabular}
\caption{The definition of $\Omega(\theta ; \theta^*, \lambda)$ depends on different methods. The first four methods (EWC, MAS, LwF, and LFL) are calculated on the current dataset $D_t$ while the last four methods (iCaRL, BiC, LUCIR, and PODNet) are measured on the combined dataset $D_t^+$ with the memory buffer. Detailed explanation and loss function of each method can be found in Appendix B.} 
\label{table:how_to_calculate_Omega}
\end{table}

In \cref{table:how_to_calculate_Omega}, we show how $\Omega(\theta ; \theta^*, \lambda)$ materializes in selected CL methods, given the current network parameters $\theta$, the reference network parameters $\theta^*$ (the old or auxiliary network), and the regularization strength $\lambda$. For example, the original CL loss of EWC can be expressed as follows by applying \cref{table:how_to_calculate_Omega} to \cref{eq:CL_loss}: 
\begin{equation}
    \mathcal{L}_\text{EWC} = \mathcal{L}_\text{t}(\theta) + \frac{\lambda}{2}\sum_{i} F_{1:t-1,i}(\theta_{i}-\theta_{1:t-1,i}^*)^2 \label{eq:ewc_loss}
\end{equation}
where $F_{t}$ is the approximation of the Fisher Information Matrix of the old network parameters $\theta_{1:t-1}^*$ and the regularization term calculates the difference between the network parameter $\theta_i$~($i = 1,\dots, P$) and the corresponding old network parameter $\theta_{1:t-1,i}^*$.
Next, if we apply ANCL to EWC to build the loss of the so-called \emph{Auxiliary Network EWC} (A-EWC), we get:
\begin{equation}
    \mathcal{L}_\text{A-EWC} = \mathcal{L}_\text{EWC}+ \frac{\lambda_a}{2}\sum_{i} F_{t,i}(\theta_{i}-\theta_{t,i}^*)^2\label{eq:a-ewc_loss}
\end{equation}
which adds the new regularizer built upon the auxiliary network parameter $\theta_{t,i}^*$. The application of ANCL to other methods in \cref{table:how_to_calculate_Omega} can be found in Appendix C.

In ANCL, the auxiliary network accounts for plasticity while the old network stands for stability. Furthermore, both networks are equally reflected through the regularization term $\Omega$, thus preventing bias toward either network. Adjusting both regularizers via $\lambda$ and $\lambda_a$, ANCL is more likely to achieve a better stability-plasticity balance than CL, under proper hyperparameter tuning. How ANCL solutions appropriately weigh the old network and the auxiliary network is further investigated in \cref{sec:trade-off_analysis}. Furthermore, we mathematically analyze and compare the gradient of CL and ANCL losses in terms of the stability-plasticity trade-off in Appendix E.


\paragraph{Comparison with AFEC} The auxiliary network of ANCL works similarly to the expanded parameter of AFEC with respect to adding an additional loss term, but ANCL uses a \textit{method-dependent} regularizer compared to the \textit{fixed and independent} regularizer of AFEC based on Fisher Information Matrix. In other words, while AFEC plugs in the same loss term calculated on the expanded parameter to every method, ANCL generates the loss term from the auxiliary network in the same way as the original CL where ANCL is applied. ANCL adopts two regularizers of the same type to equally represent stability and plasticity which is explicitly controlled by the scaling hyperparameters ($\lambda$ and $\lambda_a$ in \cref{eq:ANCL_loss}). If the two regularizers are of different types like in AFEC, each regularizer will change in different magnitude at every epoch. Consequently, it is less likely that the model will arrive at the best equilibrium. In Appendix D, we empirically show that ANCL outperforms AFEC.

\subsection{Algorithm} \label{subsec:algorithm_of_ANCL}
Detailed training steps of our ANCL framework is summarized in Alg.\ \ref{alg:DCL}. This is applicable to all ANCL methods if an appropriate ANCL loss is substituted in the algorithm. Given the training over total $N$ tasks, Lines 3-4 shows the training of the main network weight $\theta$ with task-specific loss $\mathcal{L}_\text{t}$ on the dataset of task $1$. Then, the optimal weight $\theta^*$ for task $1$ is saved as the old weight $\theta_{1:1}^*$ in Line 5. On task $t (>1)$, the auxiliary weight $\theta_t$ is initialized by the previous old weight $\theta_{1:t-1}^*$ and trained with task-specific loss $\mathcal{L}_\text{t}$ (Lines 7-9). In Line 10, the auxiliary weight $\theta_t^*$ is frozen and saved. Subsequently, the main network is trained with ANCL loss explained in \cref{eq:ANCL_loss} (Lines 11-12). The optimal main network on task $t$ is frozen and saved as an old weight $\theta_{1:t-1}^*$ for the next loop (Line 13). If Lines 7-10 are skipped and "ANCL Loss (\cref{eq:ANCL_loss})" in Line 12 is replaced with "CL Loss (\cref{eq:CL_loss})", Alg.\ \ref{alg:DCL} becomes the original CL algorithm. 

\begin{algorithm}[!ht]  
\DontPrintSemicolon 
  \KwInput{Main network weight $\theta$, Auxiliary network weight $\theta_t^*$, Old network weight $\theta_{1:t-1}^*$, Hyperparameters $\lambda$, $\lambda_a$}
  \KwOutput{Optimal main network weight $\theta^*$}
    \For{task t = 1, 2, .., N}    
        { \If{$t = 1$}
            {
                \tcp{Train main network}
                \For{epoch e = 1, 2, .., E} 
                {
                Train $\theta$ with task-specific loss $\mathcal{L}_\text{t}$ to obtain $\theta^*$ on task 1
            	}   
                \tcp{Save main network weight as old network weight}
                Freeze and save $\theta^*$ as $\theta_{1:1}^*$   
            } 
        
          \Else
            {
            \tcp{Initialize auxiliary network}
            $\theta_t$ = copy($\theta_{1:t-1}^*$) \\
            \tcp{Train auxiliary network}
            \For{epoch e = 1, 2, .., E} 
            {
            Train $\theta_t$ with task-specific loss $\mathcal{L}_\text{t}$ to obtain $\theta_t^*$ on task $t$ } 
            \tcp{Save auxiliary network weight}
            Freeze and save $\theta_t^*$ \\
            \tcp{Train main network}
            \For{epoch e = 1, 2, .., E} 
            {
            Train $\theta$ with ANCL Loss (\cref{eq:ANCL_loss}) to obtain $\theta^*$ on task $t$
        	} 
            \tcp{Save main network weight as old network weight}
            Freeze and save $\theta^*$ as $\theta_{1:t-1}^*$ 
            }
        }  
        \caption{ANCL Algorithm} \label{alg:DCL}
\end{algorithm} 


\section{Experiment}
\label{sec:experiment}

\begin{table*}
\centering
\begin{tabular}{ccccc}
\hline
                                          & \multicolumn{2}{c}{CIFAR-100}               & \multicolumn{2}{c}{Tiny ImageNet}          \\
Methods                                   & (1)               & (2)                  & (3)                    & (4)                   \\ \hline
Fine-tuning                               & $38.90_{\pm1.59}$    & $27.81_{\pm0.80}$    & $28.51_{\pm0.75}$     & $20.35_{\pm1.70}$     \\
Joint                                     & $89.64_{\pm0.37}$    & $93.42_{\pm0.27}$    & $67.98_{\pm1.15}$     & $70.02_{\pm2.63}$     \\
LwM \cite{dhar2019learning}                & $78.46_{\pm1.11}$    & $78.27_{\pm0.38}$    & $59.04_{\pm0.63}$     & $59.78_{\pm1.08}$     \\
DMC \cite{zhang2020class}                  & $51.90_{\pm0.91}$    & $53.72_{\pm1.11}$    & $45.65_{\pm0.15}$     & $44.50_{\pm0.73}$     \\
\hline
EWC \cite{kirkpatrick2017overcoming}       & $58.13_{\pm0.87}$    & $60.03_{\pm1.23}$    & $50.10_{\pm0.78}$     & $52.53_{\pm0.91}$     \\
\rowcolor{Gray}
w/ ANCL (ours)                             & $60.86_{\pm1.46}$    & $62.47_{\pm0.65}$    & $52.49_{\pm0.71}$     & $53.86_{\pm0.88}$     \\ \hline
MAS \cite{aljundi2018memory}               & $60.56_{\pm0.82}$    & $59.35_{\pm1.09}$    & $49.50_{\pm1.18}$     & $51.79_{\pm0.51}$     \\
\rowcolor{Gray}
w/ ANCL (ours)                             & $64.43_{\pm1.17}$    & $60.70_{\pm1.11}$    & $50.11_{\pm1.09}$     & $53.58_{\pm0.73}$     \\ \hline
LwF \cite{li2017learning}                  & $78.87_{\pm0.69}$    & $76.96_{\pm0.83}$    & $59.04_{\pm0.62}$     & $62.09_{\pm0.59}$     \\
\rowcolor{Gray}
w/ ANCL (ours)                             & $79.42_{\pm0.57}$ & $79.99_{\pm0.59}$ & $60.96_{\pm0.76}$     & $63.79_{\pm0.41}$     \\ \hline
LFL \cite{jung2016less}                    & $74.50_{\pm0.57}$    & $74.27_{\pm0.72}$    & $60.20_{\pm0.66}$     & $58.47_{\pm0.95}$     \\
\rowcolor{Gray}
w/ ANCL (ours)                             & $75.23_{\pm0.67}$    & $74.68_{\pm1.04}$    & $61.32_{\pm0.68}$     & $58.98_{\pm0.74}$     \\ \hline
\end{tabular}
\caption{The averaged accuracy (\%) on the benchmarks (1)-(4). Reported metrics are averaged over 3 runs (averaged accuracy $\pm$ standard error). ANCL methods are colored gray.} 
\label{table:mean_acc}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{ccccc}
\hline
                              & \multicolumn{2}{c}{CIFAR-100}          & \multicolumn{2}{c}{Tiny ImageNet} \\
Methods                       & (5)                  & (6)                     & (7)                   & (8)               \\ \hline
Fine-tuning                   & $45.78_{\pm0.90}$    & $43.57_{\pm1.33}$    & $27.44_{\pm0.85}$     & $24.18_{\pm0.98}$     \\
Joint                         & $67.84_{\pm1.35}$    & $66.40_{\pm0.86}$    & $46.85_{\pm0.74}$     & $46.02_{\pm0.55}$     \\
EEIL \cite{castro2018end}     & $49.81_{\pm1.12}$     & $48.65_{\pm0.94}$     & $28.68_{\pm0.93}$     & $28.00_{\pm0.73}$ \\ \hline
iCaRL \cite{rebuffi2017icarl} & $58.05_{\pm0.94}$         & $57.11_{\pm0.77}$        & $39.04_{\pm0.61}$             & $37.90_{\pm0.98}$          \\
\rowcolor{Gray}
w/ ANCL (ours)             & $61.22_{\pm0.88}$         & $59.13_{\pm0.68}$        & $41.46_{\pm0.85}$             & $39.91_{\pm1.02}$        \\
BiC \cite{wu2019large}      & $56.74_{\pm1.33}$         & $55.73_{\pm1.21}$        & $40.56_{\pm0.44}$             & $39.21_{\pm0.69}$      \\
\rowcolor{Gray}
w/ ANCL (ours)                    & $58.32_{\pm1.27}$         & $58.23_{\pm1.44}$        & $42.61_{\pm0.65}$             & $40.56_{\pm0.51}$        \\ \hline
LUCIR \cite{hou2019learning}     & $56.06_{\pm0.45}$         & $57.91_{\pm0.57}$        & $35.17_{\pm0.58}$             & $30.02_{\pm0.13}$         \\
\rowcolor{Gray}
w/ ANCL (ours)                    & $60.20_{\pm0.78}$         & $60.04_{\pm0.80}$        & $37.89_{\pm0.74}$             & $31.65_{\pm0.25}$         \\ \hline
PODNet \cite{douillard2020podnet} & $61.80_{\pm0.77}$         & $59.22_{\pm0.93}$        & $40.28_{\pm0.36}$             & $38.50_{\pm0.49}$            \\
\rowcolor{Gray}
w/ ANCL (ours)                    & $63.15_{\pm0.62}$         & $60.44_{\pm0.67}$        & $41.11_{\pm0.23}$             & $40.11_{\pm0.64}$           \\ \hline
\end{tabular}
\caption{The averaged incremental accuracy (\%) on the benchmarks (5)-(8). Reported metrics are averaged over 3 runs (averaged accuracy $\pm$ standard error). ANCL methods are colored gray.} 
\label{table:mean_acc_replay}
\end{table*}

\textbf{Benchmark:} CIFAR-100 \cite{krizhevsky2009learning} and Tiny ImageNet \cite{le2015tiny} are chosen to evaluate ANCL. CIFAR-100 contains 60,000 colored images from 100 classes with the size of $32 \times 32$. For task incremental scenario, CIFAR-100 is divided into 10 tasks of 10 classes each and 20 tasks of 5 classes each to construct two benchmarks: \textbf{(1) CIFAR-100/10} and \textbf{(2) CIFAR-100/20}. In addition, we build two more benchmarks for class incremental scenario: \textbf{(5) CIFAR-100/6} and \textbf{(6) CIFAR-100/11}. In these settings, 50 classes are learned at an initial phase and the rest classes are learned sequentially with 10 classes or 5 classes per phase after the initial one. Tiny ImageNet consists of 110,000 colored images (size $64 \times 64$) from 200 classes which are resized as $32 \times 32$ for both training and inference. We equally divide Tiny ImageNet into 10 and 20 tasks to build two benchmarks for task incremental scenario: \textbf{(3) TinyImagenet-200/10} and \textbf{(4) TinyImagenet-200/20}. For class incremental scenario, the model is trained on 100 classes at an initial phase and then trained continuously on 10 classes or 5 classes per phase after the initial one: \textbf{(7) TinyImagenet-200/11} and \textbf{(8) TinyImagenet-200/21}. 

\textbf{Architecture:} We select Resnet32 \cite{he2016deep} for all benchmarks which is commonly chosen in the literature of continual learning \cite{rebuffi2017icarl,wu2019large, hou2019learning,zhao2020maintaining,douillard2020podnet}. For task incremental scenario, multi-head layer is deployed instead of the last layer in Resnet32 to generate an output with a task identity. In class incremental scenario, single-head evaluation is adopted due to the absence of task identity during inference. 

\textbf{Implementation:} The model is trained from scratch and every experiment is carried out 3 times with different seeds to generate averaged metrics. SGD optimizer with momentum 0.9 and batch size 128 is applied to all experiments. In task incremental learning, we evaluate our methods on a strict setting of continual learning where the previous data is not visited again. In class incremental learning, we relax the regularization of accessing previous data. 20 exemplars per class of the old training data are selected by herding sampling strategy and stored in the memory buffer (more details in Appendix F.1). 

\textbf{Gridsearch on Parameters:} We conduct a comprehensive hyperparameter search for all methods and report the best scores for a fair comparison. We follow the way AFEC \cite{wang2021afec} performs the grid search on $\lambda$ and $\lambda_a$. First, an extensive grid search is made on $\lambda$ using the original CL loss and $\lambda$ is fixed afterward. Then, we use ANCL loss to conduct the grid search of $\lambda_a$. Grid search result of $\lambda$ and $\lambda_a$ for all benchmarks can be found in Appendix F.4

\textbf{Evaluation Metrics:} In task incremental scenario, averaged accuracy ($AAC$) for $T$ task is calculated after the training of all tasks. In class incremental scenario, averaged incremental accuracy ($AIAC$) is used instead:
\begin{equation}
    AAC = \frac{1}{T}\sum_{i=1}^T A_{T,i}, \quad AIAC = \frac{1}{N+1}\sum_{i=0}^N A_{i}.
\end{equation}
In AAC, $A_{j,k}$ is the test accuracy of task $k$ after the continual learning of task $j$. In AIAC, $A_i$ denotes the test accuracy of the classes seen so far at the $i$th phase for the benchmark consisting of $N+1$ phases including the initial one.


\begin{figure*}[htbp]
\centerline{\includegraphics[scale=.45]{Figures/Analysis_figures.jpg}}
\caption{Anaylsis figures on (1) CIFAR-100/10: weight distance (top row), centered kernel alignment (middle row), and mean accuracy landscape (bottom row). The set of $\lambda_a$ for each ANCL is as follows ($\lambda$ is fixed): (a) A-EWC ($\lambda=10000$) - $\lambda_a \in$ [10, 100, 1000, 10000, 20000, 40000], (b) A-MAS ($\lambda=50$) - $\lambda_a \in$ [1, 5, 10, 50, 100, 200], (c) A-LwF ($\lambda=10$) - $\lambda_a \in$ [0.05, 0.1, 0.5, 1, 5, 10] and (d) A-LFL ($\lambda=400$) - $\lambda_a \in$ [10, 50, 100, 200, 400, 800].}
\label{fig:trade-off_figures}
\end{figure*}


\textbf{Baseline:} Fine-tuning is the naive approach that a model is fine-tuned on each task (or each phase), which is regarded as a lowerbound and joint uses the whole dataset to train the model, which becomes an upperbound. In task incremental setting, we evaluate EWC \cite{kirkpatrick2017overcoming}, MAS \cite{aljundi2018memory}, LwF \cite{li2017learning}, LFL \cite{jung2016less}, LwM \cite{dhar2019learning}, and DMC \cite{zhang2020class}. For a fair comparison, DMC is modified to only use the original dataset like other methods instead of an unlabeled auxiliary dataset. Then, we apply ANCL to the original CL approaches. 
In class incremental setting, we test EEIL \cite{castro2018end}, iCaRL \cite{rebuffi2017icarl}, BiC \cite{wu2019large}, LUCIR \cite{hou2019learning}, and PODNet \cite{douillard2020podnet} with their applications to ANCL.

\textbf{Evaluation on Task Incremental Scenario:} \cref{table:mean_acc} shows that applying ANCL consistently gives an extra boost in accuracy by 1-3 \% compared to naive CL and A-LwF achieves the best accuracy in all benchmarks. ANCL can be more compatible with specific methods than others. For example in benchmark (1), applying ANCL outperforms MAS baseline by 3.87 \% while it improves LFL baseline only by 0.73 \%. This is because ANCL is more effective when the two regularizers in \cref{eq:ANCL_loss} are well suited to each other and CL has less plasticity at the beginning. The detail accuracy for all tasks can be found in Appendix F.2.

\textbf{Evaluation on Class Incremental Scenario:} In \cref{table:mean_acc_replay}, we can clearly see that ANCL surpasses CL baselines in all methods by 1-3 \% including state-of-the-art (SOTA) methods such as BiC \cite{wu2019large}, LUCIR \cite{hou2019learning}, and PODNet \cite{douillard2020podnet}. Similarly to \cref{table:mean_acc}, ANCL is more compatible with LUCIR and iCaRL compared to others thereby A-iCaRL being able to compete with or even outperform the stronger baseline of PODNet. We also plot how each method's accuracy at each phase changes and report the final accuracy in Appendix F.3.



\section{Stability-Plasticity Trade-off Analysis}\label{sec:trade-off_analysis}


In this chapter, we perform three analyses on (1) CIFAR-100/10 to study how the stability-plasticity dilemma is solved through ANCL: \emph{Weight Distance}, \emph{Centered Kernel Alignment}, and \emph{Mean Accuracy Landscape}. For simplification, $\lambda$ is first selected by grid search using CL loss on current task $t=2$ and then fixed. Then, ANCL solutions with different $\lambda_a$ are compared in various analyses. A training regime similar to the one in \cite{mirzadeh2020linear} is adopted for a fair comparison, which is explained in detail in Appendix G.1.


\subsection{Weight Distance} \label{subsec:weigth_distance_analysis}
If the parameters change less, it is reasonable to expect that less forgetting will occur. According to \cite{mirzadeh2020understanding}, forgetting $\mathcal{F}_1$ on task $1$ is bounded using Taylor expansion of the loss as follows:
\begin{align}
    \mathcal{F}_1 &= \mathcal{L}_1(\hat{\theta}_2) - \mathcal{L}_1(\hat{\theta}_1) \\&\approx \frac{1}{2} (\hat{\theta}_2 - \hat{\theta}_1)^T \nabla^2 \mathcal{L}_1(\hat{\theta}_1) (\hat{\theta}_2 - \hat{\theta}_1) \\ 
    &\le \frac{1}{2} \lambda_1^{max} \norm{\hat{\theta}_2 - \hat{\theta}_1}_2^2 
\end{align}
where $\mathcal{L}_1$ is the empirical loss on task $1$ and $\nabla^2 \mathcal{L}_1(\hat{\theta}_1)$ is the Hessian for $\mathcal{L}_1$ at $\hat{\theta}_1$. $\lambda_1^{max}$ is the maximum eigenvalue of $\nabla^2 \mathcal{L}_1(\hat{\theta}_1)$. Above inequality implies that the bound of forgetting $\mathcal{F}_1$ is determined by the norm of the difference between two weights near the minima of task 1 loss.

On task $t$, we measure the weight distance (WD) from the weights of the ANCL models $\theta_t^{ANCL}$ to the weights of the old model $\theta_{t-1}^{old}$ and the auxiliary model $\theta_{t}^{aux}$ respectively:
\begin{align}
    WD_{old} &= \norm{\theta_t^{ANCL} - \theta_{t-1}^{old}}_2, \\
    WD_{aux} &= \norm{\theta_t^{ANCL} - \theta_{t}^{aux}}_2.
    \label{eq:weight_distance_def}
\end{align}
WD analysis is shown in the top row of \cref{fig:trade-off_figures}. We calculate WD with different $\lambda_a$ which directly adjusts the stability-plasticity trade-off while $\lambda$ is fixed. The model parameters remain close to the old parameters when $\lambda_a$ is small, which can be seen on the left side of all WD figures. For A-EWC and A-MAS, $WD_{aux}$ decreases and $WD_{old}$ increases as $\lambda_a$ becomes larger. This result implies a direct interpolation between the old and auxiliary networks, which is consistent with the analysis of the ANCL gradient in Appendix E. For A-LwF and A-LFL, $WD_{aux}$ becomes relatively smaller than $WD_{old}$ with increasing $\lambda_a$ but $WD_{old}$ and $WD_{aux}$ are both growing. Unlike EWC and MAS which directly regularize the weights itself, LwF and LFL have more flexibility to remember the previous knowledge by utilizing loss terms based on activations or logits. Therefore, for the distillation approaches, the model weights tends to move relatively closer to the auxiliary weights with increasing $\lambda_a$ but not directly toward it like EWC or MAS. The difference between the regularization and distillation CL methods and the effect of $\lambda_a$ on the stability-plasticity trade-off is studied further in the following analyses.


\subsection{Centered Kernel Alignment}\label{subsec:CKA_analysis}
Centered Kernel Alignment (CKA) \cite{kornblith2019similarity} measures the similarity of two-layer representations on the same set of data. Given $N$ data and $p$ neurons, the layer activation matrices $R_1 \in \mathbb{R}^{N \times p}$ and $R_2 \in \mathbb{R}^{N \times p}$ are generated by two layers from two independent networks. Then, CKA is defined as:
\begin{align}
    CKA(R_1, R_2) = \frac{HSIC(R_1, R_2)}{\sqrt{HSIC(R_1, R_1)}\sqrt{HSIC(R_2, R_2)}}
\end{align}
where $HSIC$ stands for Hilbert-Schmidt Independence Criterion \cite{gretton2005measuring}. We use linear $HSIC$ to implement CKA. 
It is well known that lower layers have relatively higher CKA scores than deeper layers and deeper layers generally contribute to forgetting \cite{ramasesh2020anatomy}. In this analysis, we measure three CKA similarity:
\begin{align}
    CKA_{old} &= \frac{1}{L}\sum_{l=1}^L CKA(R_{t,l}^{ANCL}, R_{t-1,l}^{old}),\label{eq:CKA_def1}\\
    CKA_{aux} &= \frac{1}{L}\sum_{l=1}^L CKA(R_{t,l}^{ANCL}, R_{t,l}^{aux}), \label{eq:CKA_def2}\\
    CKA_{multi} &= \frac{1}{L}\sum_{l=1}^L CKA(R_{t,l}^{ANCL}, R_{t,l}^{multi}). \label{eq:CKA_def3}
\end{align}
where CKA is calculated and averaged over the set of layers $\{1, \dots, L\}$ in Resnet32. Resnet32 consists of 1 initial convolution layer and 3 residual blocks. In order to measure the output similarity of two networks, we select 10 convolution layers in the last residual block of Resnet32 as our set. $R_{t}^{ANCL}$, $R_{t-1}^{old}$ and $R_{t}^{aux}$ are the activation matrices of the ANCL network, the old network, and the auxiliary network, respectively. $R_{t}^{multi}$ is the activation output of the multitask model trained on the entire dataset $D_{1:t}$ until the task $t$. If $CKA_{multi}$ is high, the model generates layer activations similar to those of the multitask model. Then, the model is highly likely to perform well on all tasks like the multitask model, which is the main goal of continual learning.

The middle row of \cref{fig:trade-off_figures} shows three CKA similarities with different $\lambda_a$. In all methods, increasing $\lambda_a$ results in higher $CKA_{aux}$ and lower $CKA_{old}$, which can be interpreted to mean that the representations of the ANCL network become more similar to that of the auxiliary network and less similar to that of the old network. We can clearly see that the stability-plasticity trade-off is controlled by $\lambda_a$ through the interaction between the old and auxiliary networks. On the other hand, if $CKA_{multi}$ reaches the highest score at specific $\lambda_a$, that model is highly likely to have the best trade-off. For example, (b) A-MAS and (d) A-LFL achieve the highest $CKA_{multi}$ at $\lambda_a = 10$ and $\lambda_a = 400$ respectively. In general, $CKA_{multi}$ of the distillation methods is higher than that of the regularization methods, which corresponds to the results in \cref{table:mean_acc} where the distillation methods achieved a higher averaged accuracy compared to the regularization methods.



\subsection{Mean Accuracy Landscape}\label{subsec:mean_acc_landscape_analysis}

Lastly, we visualize mean accuracy landscape of task $1$ and $2$ in weight vector space following \cite{mirzadeh2020linear} (details in Appendix G.3). $\theta_1^{old}$, $\theta_2^{aux}$, and $\theta_2^{multi}$ are used to build two-dimensional subspace denoting the weights of the old network, the auxiliary network and the multitask network, respectively. Multitask network is trained on whole dataset $D_{1:2}$ until task $2$ and thus $\theta_2^{multi}$ is located in the highest contour indicating the highest mean accuracy. We project CL (blue) and ANCL (red) weight vectors on the subspace to see how ANCL parameters are shifted on the accuracy landscape with different $\lambda_a$. ANCL weights with the lowest $\lambda_a$ are denoted as a brown circle and $\lambda_a$ increases following the red dot line. Finally, the red dot line reaches a brown star which indicates ANCL weights with the highest $\lambda_a$.

In A-EWC and A-MAS, it is clearly observed that $\lambda_a$ adjusts the interpolation between the CL weights $\theta_{CL}$ and the auxiliary weights $\theta_2^{aux}$. The large $\lambda_a$ drifts the ANCL weights $\theta_{ANCL}$ directly toward $\theta_2^{aux}$ and the ANCL with sufficiently small $\lambda_a$ converges to CL methods. At the interpolation of the old weights $\theta_1^{old}$ and the auxiliary weights $\theta_2^{aux}$, the ANCL weight achieves higher mean accuracy located in the higher contour. Similarly in A-LwF and A-LFL, $\theta_{ANCL}$ with the lowest $\lambda_a$ starts near $\theta_{CL}$ and tends to move toward the region between $\theta_1^{old}$ and $\theta_2^{aux}$. As the distillation methods have more flexibility to retain the previous knowledge, the weights of A-LwF and A-LFL do not directly move toward $\theta_2^{aux}$ like those of A-EWC and A-MAS. Because of its flexibility, ANCL with distillation methods can deviate from the interpolation line and climb to the higher contour of mean accuracy. As a result, the best trade-off is made at somewhere between $\theta_1^{old}$ and $\theta_2^{aux}$. Again, the mean accuracy landscape figures show the projection of weight in the two-dimensional subspace built by three weights ($\theta_1^{old}$, $\theta_2^{aux}$, and $\theta_2^{multi}$). Therefore, it approximates the relative positions of CL and ANCL weights but does not reflect the exact positions of them in the weight space.

As a result, three analyses strongly support the notion that ANCL is able to achieve a better stability-plasticity trade-off where $CKA_{multi}$ and mean accuracy are the highest. The trade-off is mainly adjusted by the ratio between $\lambda$ and $\lambda_a$. ANCL with high $\lambda_a$ infuses more plasticity into the model, while ANCL with low $\lambda_a$ seeks more stability. These results coincides with the analysis of ANCL in Appendix E where the solutions of A-EWC and A-MAS indicate the explicit interpolation between the old and auxiliary weights and the gradients of A-LwF and A-LFL derive the activation (or logit) of the main network toward the interpolated activation (or logit) between the old and auxiliary networks. 

\section{Conclusion}
\label{sec:conclusion}
In our paper, we propose a novel framework called ANCL to pursue the proper balance between stability and plasticity inspired by the recent works \cite{wang2021afec, zhang2020class, liu2021adaptive, lin2022towards} adopting an auxiliary network. Our method outperforms the original baselines, including SOTA methods on CIFAR-100 \cite{krizhevsky2009learning} and Tiny ImageNet \cite{le2015tiny}. To investigate the underlying mechanism of ANCL, we extensively conduct analyses and confirm that the balance is resolved via the interpolation between the old and auxiliary weights. In summary, our work provides a deeper understanding of the interaction between the old network and the auxiliary network, which is the key to recent research on continual learning. 

Although ANCL can achieve better stability-plasticity trade-off compare to CL, it should be supported by enough hyperparameter search of $\lambda$ and $\lambda_a$. Therefore, extra computational burdens are required to search appropriate hyperparameters for each method, and results can be variant depending on the scope of grid search. In the future, we will investigate a better way to find these hyperparameters such as in data-driven fashion or inside the optimization process. 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{PaperForReview}
}

\end{document}
