
@article{calheiros_cloudsim_2011,
	title = {{CloudSim} : a toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms},
	volume = {41},
	issn = {0038-0644},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spe.995},
	doi = {10.1002/spe.995},
	number = {1},
	journal = {Software: Practice and Experience},
	author = {Calheiros, Rodrigo N and Ranjan, Rajiv and Beloglazov, Anton and De Rose, César A. F. and Buyya, Rajkumar},
	month = jan,
	year = {2011},
	keywords = {application scheduling, cloud computing, ment, modelling and simulation, performance evaluation, resource manage-},
	pages = {23--50},
}

@article{liu_qucloud_2022,
	title = {{QuCloud}+: {A} {Holistic} {Qubit} {Mapping} {Scheme} for {Single}/{Multi}-programming on {2D}/{3D} {NISQ} {Quantum} {Computers}},
	url = {http://arxiv.org/abs/2207.14483},
	abstract = {Qubit mapping is essential to quantum computing's fidelity and quantum computers' resource utilization. Yet, the existing qubit mapping schemes meet some challenges (e.g., crosstalk, SWAP overheads, diverse device topologies, etc.), leading to qubit resource under-utilization, high error rate, and low fidelity in computing results. This paper presents QuCloud+, a new qubit mapping scheme capable of handling these challenges. QuCloud+ has several new designs. (1) QuCloud+ enables multi-programming quantum computing on quantum chips with 2D/3D topology. (2) It partitions physical qubits for concurrent quantum programs with the crosstalk-aware community detection technique and further allocates qubits according to qubit degree, improving fidelity and resource utilization. (3) QuCloud+ includes an X-SWAP mechanism that avoids SWAPs with high crosstalk errors and enables inter-program SWAPs to reduce the SWAP overheads. (4) QuCloud+ schedules concurrent quantum programs to be mapped and executed based on estimated fidelity for the best practice. QuCloud+ outperforms the previous multi-programming work on various devices by 6.84\% on fidelity and saves 40.9\% additional gates required during mapping transition.},
	author = {Liu, Lei and Dou, Xinglei},
	note = {arXiv: 2207.14483},
}

@article{liu_qucloud_2021,
	title = {{QuCloud}: {A} {New} {Qubit} {Mapping} {Mechanism} for {Multi}-programming {Quantum} {Computing} in {Cloud} {Environment}},
	volume = {2021-Febru},
	issn = {15300897},
	doi = {10.1109/HPCA51647.2021.00024},
	abstract = {For a specific quantum chip, multi-programming improves overall throughput and resource utilization. Previous studies on mapping multiple programs often lead to resource under-utilization, high error rate, and low fidelity. This paper proposes QuCloud, a new approach for mapping quantum programs in the cloud environment. We have three new designs in QuCloud. (1) We leverage the community detection technique to partition physical qubits among concurrent quantum programs, avoiding the waste of robust resources. (2) We design X-SWAP scheme that enables inter-program SWAPs and prioritizes SWAPs associated with critical gates to reduce the SWAP overheads. (3) We propose a compilation task scheduler that schedules concurrent quantum programs to be compiled and executed based on estimated fidelity for the best practice. We evaluate our work on publicly available quantum computer IBMQ16 and a simulated quantum chip IBMQ50. Our work outperforms the state-of-The-Art work for multi-programming on fidelity and compilation overheads by 9.7\% and 11.6\%, respectively.},
	journal = {Proceedings - International Symposium on High-Performance Computer Architecture},
	author = {Liu, Lei and Dou, Xinglei},
	year = {2021},
	keywords = {n/a},
	pages = {167--178},
}

@article{dahlberg_simulaqron_2019,
	title = {{SimulaQron} - {A} simulator for developing quantum internet software},
	volume = {4},
	issn = {20589565},
	doi = {10.1088/2058-9565/aad56e},
	abstract = {We introduce a simulator of a quantum internet with the specific goal to support software development. A quantum internet consists of local quantum processors, which are interconnected by quantum communication channels that enable the transmission of qubits between the different processors. While many simulators exist for local quantum processors, there is presently no simulator for a quantum internet tailored towards software development. Quantum internet protocols require both classical as well as quantum information to be exchanged between the network nodes, next to the execution of gates and measurements on a local quantum processor. This requires quantum internet software to integrate classical communication programming practises with novel quantum ones. SimulaQron is built to enable application development and explore software engineering practises for a quantum internet. SimulaQron can be run on one or more classical computers to simulate local quantum processors, which are transparently connected in the background to enable the transmission of qubits or the generation of entanglement between remote processors. Application software can access the simulated local quantum processors to execute local quantum instructions and measurements, but also to transmit qubits to remote nodes in the network. SimulaQron features a modular design that performs a distributed simulation based on any existing simulation of a quantum computer capable of integrating with Python. Programming libraries for Python and C are provided to facilitate application development.},
	number = {1},
	journal = {Quantum Science and Technology},
	author = {Dahlberg, Axel and Wehner, Stephanie},
	year = {2019},
	keywords = {quantum internet, quantum network, simulator, software development},
	pages = {0--15},
}

@article{bian_pas_2021,
	title = {{PAS}: {A} new powerful and simple quantum computing simulator},
	volume = {53},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3049},
	doi = {https://doi.org/10.1002/spe.3049},
	abstract = {Abstract In recent years, many researchers have been using CPU for quantum computing simulation. However, in reality, the simulation efficiency of the large-scale simulator is low on a single node. Therefore, striving to improve the simulator efficiency on a single node has become a serious challenge that many researchers need to solve. After many experiments, we found that much computational redundancy and frequent memory access are important factors that hinder the efficient operation of the CPU. This paper proposes a new powerful and simple quantum computing simulator: PAS (power and simple). Compared with existing simulators, PAS introduces four novel optimization methods: efficient hybrid vectorization, fast bitwise operation, memory access filtering, and quantum tracking. In the experiment, we tested the QFT (quantum Fourier transform) and RQC (random quantum circuits) of 21 to 30 qubits and selected the state-of-the-art simulator QuEST (quantum exact simulation toolkit) as the benchmark. After experiments, we have concluded that PAS compared with QuEST can achieve a mean speedup of (QFT), (RQC) (up to , ) on the Intel Xeon E5-2670 v3 CPU.},
	number = {1},
	journal = {Software: Practice and Experience},
	author = {Bian, Haodong and Huang, Jianqiang and Tang, Jiahao and Dong, Runting and Wu, Li and Wang, Xiaoying},
	year = {2023},
	keywords = {CPU, PAS, QuEST, quantum Fourier transform, quantum computing simulator, random quantum circuits},
	pages = {142--159},
}

@article{wright_benchmarking_2019,
	title = {Benchmarking an 11-qubit quantum computer},
	volume = {10},
	issn = {20411723},
	doi = {10.1038/s41467-019-13534-2},
	abstract = {The field of quantum computing has grown from concept to demonstration devices over the past 20 years. Universal quantum computing offers efficiency in approaching problems of scientific and commercial interest, such as factoring large numbers, searching databases, simulating intractable models from quantum physics, and optimizing complex cost functions. Here, we present an 11-qubit fully-connected, programmable quantum computer in a trapped ion system composed of 13 171Yb+ ions. We demonstrate average single-qubit gate fidelities of 99.5\%, average two-qubit-gate fidelities of 97.5\%, and SPAM errors of 0.7\%. To illustrate the capabilities of this universal platform and provide a basis for comparison with similarly-sized devices, we compile the Bernstein-Vazirani and Hidden Shift algorithms into our native gates and execute them on the hardware with average success rates of 78\% and 35\%, respectively. These algorithms serve as excellent benchmarks for any type of quantum hardware, and show that our system outperforms all other currently available hardware.},
	number = {1},
	journal = {Nature Communications},
	author = {Wright, K. and Beck, K. M. and Debnath, S. and Amini, J. M. and Nam, Y. and Grzesiak, N. and Chen, J. S. and Pisenti, N. C. and Chmielewski, M. and Collins, C. and Hudek, K. M. and Mizrahi, J. and Wong-Campos, J. D. and Allen, S. and Apisdorf, J. and Solomon, P. and Williams, M. and Ducore, A. M. and Blinov, A. and Kreikemeier, S. M. and Chaplin, V. and Keesan, M. and Monroe, C. and Kim, J.},
	month = dec,
	year = {2019},
	keywords = {★},
}

@article{q-api-gateway,
	title = {Quantum {Software} as a {Service} {Through} a {Quantum} {API} {Gateway}},
	volume = {26},
	issn = {19410131},
	doi = {10.1109/MIC.2021.3132688},
	abstract = {As quantum computers mature, the complexity of quantum software increases. As we move from the initial standalone quantum algorithms toward complex solutions combining quantum algorithms with traditional software, new software engineering methods and abstractions are needed. Nowadays, quantum computers are usually offered in the cloud, under a pay-per-use model, leading to the adoption of the service-oriented good practices that dominate the cloud today. However, specific adaptations are needed to reap the benefits of service-oriented computing while dealing with quantum hardware limitations. In this article, we propose the Quantum API Gateway-an adaptation of the API Gateway pattern that takes into account the fact that quantum services cannot be deployed as traditional services. Instead, the Quantum API Gateway recommends the best quantum computer to run a specific quantum service at run time. As proof of concept, we provide an implementation of the Quantum API Gateway for the Amazon Braket platform.},
	number = {1},
	journal = {IEEE Internet Computing},
	author = {Garcia-Alonso, Jose and Rojo, Javier and Valencia, David and Moguel, Enrique and Berrocal, Javier and Murillo, Juan Manuel},
	year = {2022},
	pages = {34--41},
}

@incollection{weder_quantum_lifecycle_2022,
	address = {Cham},
	title = {Quantum {Software} {Development} {Lifecycle}},
	isbn = {978-3-031-05324-5},
	url = {https://doi.org/10.1007/978-3-031-05324-5_4},
	abstract = {This chapter discusses that the development of quantum applications typically incorporates the development of quantum programs, classical programs, and workflows to orchestrate them. Thus, the lifecycles of these software artifacts with their included phases, related concepts, and tools are described. Finally, the points of connection between the various lifecycles are identified, and they are integrated into the overall quantum software development lifecycle.},
	booktitle = {Quantum {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Weder, Benjamin and Barzen, Johanna and Leymann, Frank and Vietz, Daniel},
	editor = {Serrano, Manuel A. and Pérez-Castillo, Ricardo and Piattini, Mario},
	year = {2022},
	pages = {61--83},
}

@article{nisq-preskill,
	title = {Quantum computing in the {NISQ} era and beyond},
	volume = {2},
	issn = {2521327X},
	url = {https://quantum-journal.org/papers/q-2018-08-06-79/},
	doi = {10.22331/q-2018-08-06-79},
	abstract = {Noisy Intermediate-Scale Quantum (NISQ) technology will be available in the near future. Quantum computers with 50-100 qubits may be able to perform tasks which surpass the capabilities of today's classical digital computers, but noise in quantum gates will limit the size of quantum circuits that can be executed reliably. NISQ devices will be useful tools for exploring many-body quantum physics, and may have other useful applications, but the 100-qubit quantum computer will not change the world right away - we should regard it as a significant step toward the more powerful quantum technologies of the future. Quantum technologists should continue to strive for more accurate quantum gates and, eventually, fully fault-tolerant quantum computing.},
	journal = {Quantum},
	author = {Preskill, John},
	month = aug,
	year = {2018},
	pages = {79},
}

@article{dastjerdi_fog_2016,
	title = {Fog {Computing}: {Helping} the {Internet} of {Things} {Realize} {Its} {Potential}},
	volume = {49},
	issn = {00189162},
	doi = {10.1109/MC.2016.245},
	abstract = {The Internet of Things (IoT) could enable innovations that enhance the quality of life, but it generates unprecedented amounts of data that are difficult for traditional systems, the cloud, and even edge computing to handle. Fog computing is designed to overcome these limitations.},
	number = {8},
	journal = {Computer},
	author = {Dastjerdi, Amir Vahid and Buyya, Rajkumar},
	year = {2016},
	keywords = {Cloud Cover, Internet of Things, IoT, cloud computing, fog computing},
	pages = {112--116},
}

@article{qcaas,
	title = {A {Review} on {Progress} and {Problems} of {Quantum} {Computing} as a {Service} ({QCaaS}) in the {Perspective} of {Cloud} {Computing}},
	volume = {15},
	abstract = {Cloud computing is a global established system. Quantum computing is hypothetical model which is still in tentative analysis. Cloud system has some weakness in security, processing, backup and vicinity. Somehow quantum computing illustrates some revolutionary solution to overcome cloud weakness. Most researchers are optimistic in quantum computing that it will improve cloud system. It is not easy to combine these two different systems along. We will show two quantum approaches; quantum cryptography and blind quantum computing to secure cloud computing. Quantum cryptography will secure the user data transmission and communication through cloud form hackers. And blind computing will secure the instant eavesdropping or accessing of data processing in cloud from any vicious cloud provider or third party. This paper’s major target is to show advantages and disadvantages of quantum computing in the viewpoint to integrate it with cloud system. Also review some current improvement of quantum computing and computer.},
	number = {4},
	journal = {Global Journal of Computer Science and Technology: B Cloud and Distributed},
	author = {V, Rama Satish K and Kavya, N P},
	year = {2015},
	keywords = {1, 2, anewefficientcloudmodelfordataintensiveapplication, big data, big data, scheduling, iterative mapreduce, microso, c, compliance and regulations of, e, gjcst-b classification, hadoop, iterative mapreduce, microso, microsoft azure, scheduling, strictly as per the, ★},
}

@inproceedings{ma_hybrid_2022,
	address = {San Diego, United States},
	title = {Hybrid quantum edge computing network},
	isbn = {978-1-5106-5460-0 978-1-5106-5461-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12238/2633020/Hybrid-quantum-edge-computing-network/10.1117/12.2633020.full},
	doi = {10.1117/12.2633020},
	abstract = {Edge computing network and quantum network are two emerging technologies in current communication fields. Edge computing has emerged to support the computational demand of delay-sensitive applications in which substantial computing and storage are deployed at the network edge close to data sources. Quantum network supports distributed quantum computing, which could provide exponentially computation capabilities for certain problems. The vision of a hybrid quantum-edge is to provide a fundamentally new computing paradigm by expanding the computing capabilities and security of edge computing with quantum computing and quantum communications. The distributed nature of edge computing networks will also enable new scalable quantum networking schemes and applications. Such a hybrid computing paradigm will achieve unparalleled capabilities that are provably impossible by using only classical computing or quantum computing schemes alone. In this paper, we introduce the concept of hybrid quantum-edge computing network and discuss its challenges and opportunities.},
	language = {en},
	urldate = {2023-03-01},
	booktitle = {Quantum {Communications} and {Quantum} {Imaging} {XX}},
	publisher = {SPIE},
	author = {Ma, Lijun and Ding, Leah},
	month = oct,
	year = {2022},
	pages = {29},
}

@article{debenedictis_future_2018,
	title = {A {Future} with {Quantum} {Machine} {Learning}},
	volume = {51},
	issn = {00189162},
	doi = {10.1109/MC.2018.1451646},
	abstract = {Could combining quantum computing and machine learning with Moore's law produce a true 'rebooted computer'? This article posits that a three-technology hybrid-computing approach might yield sufficiently improved answers to a broad class of problems such that energy efficiency will no longer be the dominant concern.},
	number = {2},
	journal = {Computer},
	author = {Debenedictis, Erik P.},
	month = feb,
	year = {2018},
	keywords = {Moore's law, Rebooting Computing, energy efficiency, future of computing, green computing, history of computing, machine learning, quantum computing},
	pages = {68--71},
}

@article{adhikari_survey_2019,
	title = {A survey on scheduling strategies for workflows in cloud environment and emerging trends},
	volume = {52},
	issn = {15577341},
	doi = {10.1145/3325097},
	abstract = {Workflow scheduling is one of the challenging issues in emerging trends of the distributed environment that focuses on satisfying various quality of service (QoS) constraints. The cloud receives the applications as a form of a workflow, consisting of a set of interdependent tasks, to solve the large-scale scientific or enterprise problems. Workflow scheduling in the cloud environment has been studied extensively over the years, and this article provides a comprehensive review of the approaches. This article analyses the characteristics of various workflow scheduling techniques and classifies them based on their objectives and execution model. In addition, the recent technological developments and paradigms such as serverless computing and Fog computing are creating new requirements/opportunities for workflow scheduling in a distributed environment. The serverless infrastructures are mainly designed for processing background tasks such as Internet-of-Things (IoT), web applications, or event-driven applications. To address the ever-increasing demands of resources and to overcome the drawbacks of the cloud-centric IoT, the Fog computing paradigm has been developed. This article also discusses workflow scheduling in the context of these emerging trends of cloud computing.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Adhikari, Mainak and Amgoth, Tarachand and Srirama, Satish Narayana},
	month = aug,
	year = {2019},
	keywords = {Cloud computing, Fog computing, QoS constraint, Scientific problems, Serverless computing, Workflow scheduling},
}

@article{mahmud_ifogsim2_2022,
	title = {{iFogSim2}: {An} extended {iFogSim} simulator for mobility, clustering, and microservice management in edge and fog computing environments},
	volume = {190},
	issn = {01641212},
	url = {https://doi.org/10.1016/j.jss.2022.111351},
	doi = {10.1016/j.jss.2022.111351},
	abstract = {Internet of Things (IoT) has already proven to be the building block for next-generation Cyber–Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software have been developed that can imitate the physical behavior of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed modular simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing based on real datasets and extended the basic components of iFogSim, a widely used Edge/Fog computing simulator for their ease of adoption as iFogSim2. The performance of iFogSim2 and its built-in service management policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that our simulator consumes less memory and minimizes simulation time by an average of 28\% when compared to other simulators.},
	journal = {Journal of Systems and Software},
	author = {Mahmud, Redowan and Pallewatta, Samodha and Goudarzi, Mohammad and Buyya, Rajkumar},
	year = {2022},
	keywords = {Clustering, Edge/Fog computing, Internet of Things, Microservices, Mobility, Simulation, ★},
	pages = {111351},
}

@article{coopmans_netsquid_2021,
	title = {{NetSquid}, a {NETwork} {Simulator} for {QUantum} {Information} using {Discrete} events},
	volume = {4},
	issn = {23993650},
	doi = {10.1038/s42005-021-00647-8},
	abstract = {In order to bring quantum networks into the real world, we would like to determine the requirements of quantum network protocols including the underlying quantum hardware. Because detailed architecture proposals are generally too complex for mathematical analysis, it is natural to employ numerical simulation. Here we introduce NetSquid, the NETwork Simulator for QUantum Information using Discrete events, a discrete-event based platform for simulating all aspects of quantum networks and modular quantum computing systems, ranging from the physical layer and its control plane up to the application level. We study several use cases to showcase NetSquid’s power, including detailed physical layer simulations of repeater chains based on nitrogen vacancy centres in diamond as well as atomic ensembles. We also study the control plane of a quantum switch beyond its analytically known regime, and showcase NetSquid’s ability to investigate large networks by simulating entanglement distribution over a chain of up to one thousand nodes.},
	number = {1},
	journal = {Communications Physics},
	author = {Coopmans, Tim and Knegjens, Robert and Dahlberg, Axel and Maier, David and Nijsten, Loek and de Oliveira Filho, Julio and Papendrecht, Martijn and Rabbie, Julian and Rozpedek, Filip and Skrzypczyk, Matthew and Wubben, Leon and de Jong, Walter and Podareanu, Damian and Torres-Knoop, Ariana and Elkouss, David and Wehner, Stephanie},
	year = {2021},
}

@misc{piveteau_circuit_2023,
	title = {Circuit knitting with classical communication},
	url = {http://arxiv.org/abs/2205.00016},
	abstract = {The scarcity of qubits is a major obstacle to the practical usage of quantum computers in the near future. To circumvent this problem, various circuit knitting techniques have been developed to partition large quantum circuits into subcircuits that ﬁt on smaller devices, at the cost of a simulation overhead. In this work, we study a particular method of circuit knitting based on quasiprobability simulation of nonlocal gates with operations that act locally on the subcircuits. We investigate whether classical communication between these local quantum computers can help. We provide a positive answer by showing that for circuits containing n nonlocal CNOT gates connecting two circuit parts, the simulation overhead can be reduced from O(9n) to O(4n) if one allows for classical information exchange. Similar improvements can be obtained for general Cliﬀord gates and, at least in a restricted form, for other gates such as controlled rotation gates.},
	language = {en},
	urldate = {2023-03-24},
	publisher = {arXiv},
	author = {Piveteau, Christophe and Sutter, David},
	month = feb,
	year = {2023},
	note = {arXiv:2205.00016 [quant-ph]},
	keywords = {Quantum Physics},
}

@inproceedings{ravi_quantum_2021,
	address = {Storrs, CT, USA},
	title = {Quantum {Computing} in the {Cloud}: {Analyzing} job and machine characteristics},
	isbn = {978-1-66544-173-5},
	shorttitle = {Quantum {Computing} in the {Cloud}},
	url = {https://ieeexplore.ieee.org/document/9668289/},
	doi = {10.1109/IISWC53511.2021.00015},
	urldate = {2023-03-03},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	publisher = {IEEE},
	author = {Ravi, Gokul Subramanian and Smith, Kaitlin N. and Gokhale, Pranav and Chong, Frederic T.},
	month = nov,
	year = {2021},
	pages = {39--50},
}

@article{ravi_quantum_2022,
	title = {Quantum {Computing} in the {Cloud}: {Analyzing} job and machine characteristics},
	doi = {10.1109/IISWC53511.2021.00015},
	abstract = {As the popularity of quantum computing continues to grow, quantum machine access over the cloud is critical to both academic and industry researchers across the globe. And as cloud quantum computing demands increase exponentially, the analysis of resource consumption and execution characteristics are key to efficient management of jobs and resources at both the vendor-end as well as the client-end. While the analysis of resource consumption and management are popular in the classical HPC domain, it is severely lacking for more nascent technology like quantum computing. This paper is a first-of-its-kind academic study, analyzing various trends in job execution and resources consumption / utilization on quantum cloud systems. We focus on IBM Quantum systems and analyze characteristics over a two year period, encompassing over 6000 jobs which contain over 600,000 quantum circuit executions and correspond to almost 10 billion "shots" or trials over 20+ quantum machines. Specifically, we analyze trends focused on, but not limited to, execution times on quantum machines, queuing/waiting times in the cloud, circuit compilation times, machine utilization, as well as the impact of job and machine characteristics on all of these trends. Our analysis identifies several similarities and differences with classical HPC cloud systems. Based on our insights, we make recommendations and contributions to improve the management of resources and jobs on future quantum cloud systems.},
	author = {Ravi, Gokul Subramanian and Smith, Kaitlin N. and Gokhale, Pranav and Chong, Frederic T.},
	month = mar,
	year = {2022},
}

@article{devitt_performing_2016,
	title = {Performing quantum computing experiments in the cloud},
	volume = {94},
	issn = {24699934},
	doi = {10.1103/PhysRevA.94.032329},
	abstract = {Quantum computing technology has reached a second renaissance in the past five years. Increased interest from both the private and public sector combined with extraordinary theoretical and experimental progress has solidified this technology as a major advancement in the 21st century. As anticipated my many, some of the first realizations of quantum computing technology has occured over the cloud, with users logging onto dedicated hardware over the classical internet. Recently, IBM has released the Quantum Experience, which allows users to access a five-qubit quantum processor. In this paper we take advantage of this online availability of actual quantum hardware and present four quantum information experiments. We utilize the IBM chip to realize protocols in quantum error correction, quantum arithmetic, quantum graph theory, and fault-tolerant quantum computation by accessing the device remotely through the cloud. While the results are subject to significant noise, the correct results are returned from the chip. This demonstrates the power of experimental groups opening up their technology to a wider audience and will hopefully allow for the next stage of development in quantum information technology.},
	number = {3},
	journal = {Physical Review A},
	author = {Devitt, Simon J.},
	year = {2016},
	pages = {1--13},
}

@article{steiger_projectq_2018,
	title = {{ProjectQ}: {An} {Open} {Source} {Software} {Framework} for {Quantum} {Computing}},
	volume = {2},
	issn = {2521-327X},
	shorttitle = {{ProjectQ}},
	doi = {10.22331/q-2018-01-31-49},
	abstract = {We introduce ProjectQ, an open source software effort for quantum computing. The first release features a compiler framework capable of targeting various types of hardware, a high-performance simulator with emulation capabilities, and compiler plug-ins for circuit drawing and resource estimation. We introduce our Python-embedded domain-specific language, present the features, and provide example implementations for quantum algorithms. The framework allows testing of quantum algorithms through simulation and enables running them on actual quantum hardware using a back-end connecting to the IBM Quantum Experience cloud service. Through extension mechanisms, users can provide back-ends to further quantum hardware, and scientists working on quantum compilation can provide plug-ins for additional compilation, optimization, gate synthesis, and layout strategies.},
	language = {en},
	urldate = {2023-02-26},
	journal = {Quantum},
	author = {Steiger, Damian S. and Häner, Thomas and Troyer, Matthias},
	month = jan,
	year = {2018},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Programming Languages, Quantum Physics},
	pages = {49},
}

@article{shi_edge_2016,
	title = {Edge {Computing}: {Vision} and {Challenges}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Edge {Computing}},
	url = {http://ieeexplore.ieee.org/document/7488250/},
	doi = {10.1109/JIOT.2016.2579198},
	abstract = {The proliferation of Internet of Things (IoT) and the success of rich cloud services have pushed the horizon of a new computing paradigm, edge computing, which calls for processing the data at the edge of the network. Edge computing has the potential to address the concerns of response time requirement, battery life constraint, bandwidth cost saving, as well as data safety and privacy. In this paper, we introduce the deﬁnition of edge computing, followed by several case studies, ranging from cloud ofﬂoading to smart home and city, as well as collaborative edge to materialize the concept of edge computing. Finally, we present several challenges and opportunities in the ﬁeld of edge computing, and hope this paper will gain attention from the community and inspire more research in this direction.},
	language = {en},
	number = {5},
	urldate = {2023-03-02},
	journal = {IEEE Internet of Things Journal},
	author = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
	month = oct,
	year = {2016},
	pages = {637--646},
}

@article{wang_architectural_2020,
	title = {Architectural {Design} {Alternatives} {Based} on {Cloud}/{Edge}/{Fog} {Computing} for {Connected} {Vehicles}},
	volume = {22},
	issn = {1553-877X, 2373-745X},
	url = {https://ieeexplore.ieee.org/document/9184917/},
	doi = {10.1109/COMST.2020.3020854},
	abstract = {OF V2X have fueled a plethora of innovations in various areas, including computing, communication, and caching. Due to the limited on-board battery and computation capacity in vehicles, in order to execute a large number of computations in limited time, ofﬂoading power-intensive time-consuming computation tasks to other more powerful servers may signiﬁcantly improve the performance of many applications of CVs, such as intelligent driving, cruise assist, and high-resolution map creation. Therefore, cloud computing, edge computing, and fog computing are proposed to realize such computation ofﬂoading.},
	language = {en},
	number = {4},
	urldate = {2023-03-02},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Wang, Haoxin and Liu, Tingting and Kim, BaekGyu and Lin, Chung-Wei and Shiraishi, Shinichi and Xie, Jiang and Han, Zhu},
	year = {2020},
	pages = {2349--2377},
}

@article{porambage_survey_2018,
	title = {Survey on {Multi}-{Access} {Edge} {Computing} for {Internet} of {Things} {Realization}},
	volume = {20},
	issn = {1553-877X, 2373-745X},
	url = {https://ieeexplore.ieee.org/document/8391395/},
	doi = {10.1109/COMST.2018.2849509},
	abstract = {The Internet of Things (IoT) has recently advanced from an experimental technology to what will become the backbone of future customer value for both product and service sector businesses. This underscores the cardinal role of IoT on the journey toward the ﬁfth generation of wireless communication systems. IoT technologies augmented with intelligent and big data analytics are expected to rapidly change the landscape of myriads of application domains ranging from health care to smart cities and industrial automations. The emergence of multi-access edge computing (MEC) technology aims at extending cloud computing capabilities to the edge of the radio access network, hence providing real-time, high-bandwidth, low-latency access to radio network resources. IoT is identiﬁed as a key use case of MEC, given MEC’s ability to provide cloud platform and gateway services at the network edge. MEC will inspire the development of myriads of applications and services with demand for ultralow latency and high quality of service due to its dense geographical distribution and wide support for mobility. MEC is therefore an important enabler of IoT applications and services which require real-time operations. In this survey, we provide a holistic overview on the exploitation of MEC technology for the realization of IoT applications and their synergies. We further discuss the technical aspects of enabling MEC in IoT and provide some insight into various other integration technologies therein.},
	language = {en},
	number = {4},
	urldate = {2023-03-02},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Porambage, Pawani and Okwuibe, Jude and Liyanage, Madhusanka and Ylianttila, Mika and Taleb, Tarik},
	year = {2018},
	pages = {2961--2991},
}

@misc{aravanis_transpiling_2022,
	title = {Transpiling {Quantum} {Circuits} using the {Pentagon} {Equation}},
	url = {http://arxiv.org/abs/2209.14356},
	abstract = {We consider the application of the pentagon equation in the context of quantum circuit compression. We show that if solutions to the pentagon equation are found, one can transpile a circuit involving non-Heisenberg-type interactions to a circuit involving only Heisenberg-type interactions while, in parallel, reducing the depth of a circuit. In this context, we consider a model of non-local two-qubit operations of Zhang {\textbackslash}emph\{et. al.\} (the \$A\$ gate), and show that for certain parameters it is a solution of the pentagon equation.},
	language = {en},
	urldate = {2023-02-28},
	publisher = {arXiv},
	author = {Aravanis, Christos and Korpas, Georgios and Marecek, Jakub},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14356 [quant-ph]},
	keywords = {Mathematics - Quantum Algebra, Quantum Physics},
}

@article{altman_quantum_2021,
	title = {Quantum {Simulators}: {Architectures} and {Opportunities}},
	volume = {2},
	issn = {2691-3399},
	shorttitle = {Quantum {Simulators}},
	url = {https://link.aps.org/doi/10.1103/PRXQuantum.2.017003},
	doi = {10.1103/PRXQuantum.2.017003},
	language = {en},
	number = {1},
	urldate = {2023-02-28},
	journal = {PRX Quantum},
	author = {Altman, Ehud and Brown, Kenneth R. and Carleo, Giuseppe and Carr, Lincoln D. and Demler, Eugene and Chin, Cheng and DeMarco, Brian and Economou, Sophia E. and Eriksson, Mark A. and Fu, Kai-Mei C. and Greiner, Markus and Hazzard, Kaden R.A. and Hulet, Randall G. and Kollár, Alicia J. and Lev, Benjamin L. and Lukin, Mikhail D. and Ma, Ruichao and Mi, Xiao and Misra, Shashank and Monroe, Christopher and Murch, Kater and Nazario, Zaira and Ni, Kang-Kuen and Potter, Andrew C. and Roushan, Pedram and Saffman, Mark and Schleier-Smith, Monika and Siddiqi, Irfan and Simmonds, Raymond and Singh, Meenakshi and Spielman, I.B. and Temme, Kristan and Weiss, David S. and Vučković, Jelena and Vuletić, Vladan and Ye, Jun and Zwierlein, Martin},
	month = feb,
	year = {2021},
	pages = {017003},
}

@article{moll_quantum_2018,
	title = {Quantum optimization using variational algorithms on near-term quantum devices},
	volume = {3},
	issn = {2058-9565},
	url = {https://iopscience.iop.org/article/10.1088/2058-9565/aab822},
	doi = {10.1088/2058-9565/aab822},
	abstract = {Universal fault-tolerant quantum computers will require error-free execution of long sequences of quantum gate operations, which is expected to involve millions of physical qubits. Before the full power of such machines will be available, near-term quantum devices will provide several hundred qubits and limited error correction. Still, there is a realistic prospect to run useful algorithms within the limited circuit depth of such devices. Particularly promising are optimization algorithms that follow a hybrid approach: the aim is to steer a highly entangled state on a quantum system to a target state that minimizes a cost function via variation of some gate parameters. This variational approach can be used both for classical optimization problems as well as for problems in quantum chemistry. The challenge is to converge to the target state given the limited coherence time and connectivity of the qubits. In this context, the quantum volume as a metric to compare the power of near-term quantum devices is discussed. With focus on chemistry applications, a general description of variational algorithms is provided and the mapping from fermions to qubits is explained. Coupled-cluster and heuristic trial wave-functions are considered for efﬁciently ﬁnding molecular ground states. Furthermore, simple error-mitigation schemes are introduced that could improve the accuracy of determining ground-state energies. Advancing these techniques may lead to near-term demonstrations of useful quantum computation with systems containing several hundred qubits.},
	language = {en},
	number = {3},
	urldate = {2023-02-28},
	journal = {Quantum Science and Technology},
	author = {Moll, Nikolaj and Barkoutsos, Panagiotis and Bishop, Lev S and Chow, Jerry M and Cross, Andrew and Egger, Daniel J and Filipp, Stefan and Fuhrer, Andreas and Gambetta, Jay M and Ganzhorn, Marc and Kandala, Abhinav and Mezzacapo, Antonio and Müller, Peter and Riess, Walter and Salis, Gian and Smolin, John and Tavernelli, Ivano and Temme, Kristan},
	month = jul,
	year = {2018},
	pages = {030503},
}

@article{krantz_quantum_2019,
	title = {A quantum engineer's guide to superconducting qubits},
	volume = {6},
	issn = {1931-9401},
	url = {http://aip.scitation.org/doi/10.1063/1.5089550},
	doi = {10.1063/1.5089550},
	abstract = {The aim of this review is to provide quantum engineers with an introductory guide to the central concepts and challenges in the rapidly accelerating ﬁeld of superconducting quantum circuits. Over the past twenty years, the ﬁeld has matured from a predominantly basic research endeavor to a one that increasingly explores the engineering of larger-scale superconducting quantum systems. Here, we review several foundational elements—qubit design, noise properties, qubit control, and readout techniques—developed during this period, bridging fundamental concepts in circuit quantum electrodynamics and contemporary, state-of-the-art applications in gate-model quantum computation.},
	language = {en},
	number = {2},
	urldate = {2023-02-28},
	journal = {Applied Physics Reviews},
	author = {Krantz, P. and Kjaergaard, M. and Yan, F. and Orlando, T. P. and Gustavsson, S. and Oliver, W. D.},
	month = jun,
	year = {2019},
	pages = {021318},
}

@article{ladd_quantum_2010,
	title = {Quantum computers},
	volume = {464},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature08812},
	doi = {10.1038/nature08812},
	language = {en},
	number = {7285},
	urldate = {2023-02-28},
	journal = {Nature},
	author = {Ladd, T. D. and Jelezko, F. and Laflamme, R. and Nakamura, Y. and Monroe, C. and O’Brien, J. L.},
	month = mar,
	year = {2010},
	pages = {45--53},
}

@article{nagata_necessary_2019,
	title = {Necessary and {Sufficient} {Condition} for {Quantum} {Computing}},
	volume = {58},
	issn = {0020-7748, 1572-9575},
	url = {http://link.springer.com/10.1007/s10773-018-3917-x},
	doi = {10.1007/s10773-018-3917-x},
	language = {en},
	number = {1},
	urldate = {2023-02-28},
	journal = {International Journal of Theoretical Physics},
	author = {Nagata, Koji and Nakamura, Tadao and Farouk, Ahmed and Diep, Do Ngoc},
	month = jan,
	year = {2019},
	pages = {136--142},
}

@article{brennan_qxtools_2022,
	title = {{QXTools}: {A} {Julia} framework for distributed quantum circuit simulation},
	volume = {7},
	issn = {2475-9066},
	shorttitle = {{QXTools}},
	url = {https://joss.theoj.org/papers/10.21105/joss.03711},
	doi = {10.21105/joss.03711},
	abstract = {QXTools is a framework for simulating quantum circuits using tensor network methods. Weak simulation is the primary use case where given a quantum circuit and input state QXTools will efficiently calculate the probability amplitude of a given output configuration or set of configurations. Given this ability one can sample from the output distribution using random sampling approaches. QXTools is intended to be used by researchers interested in simulating circuits larger than those possible with full wave-function simulators or those interested in research and development of tensor network circuit simulation methods. See Brennan et al. (2021) for more complete background and scaling results and Brayford et al. (2021) for details about deploying in containerised environments.},
	language = {en},
	number = {70},
	urldate = {2023-02-28},
	journal = {Journal of Open Source Software},
	author = {Brennan, John and O’Riordan, Lee and Hanley, Kenneth and Doyle, Myles and Allalen, Momme and Brayford, David and Iapichino, Luigi and Moran, Niall},
	month = feb,
	year = {2022},
	pages = {3711},
}

@article{hauke_perspectives_2020,
	title = {Perspectives of quantum annealing: methods and implementations},
	volume = {83},
	issn = {0034-4885, 1361-6633},
	shorttitle = {Perspectives of quantum annealing},
	url = {https://iopscience.iop.org/article/10.1088/1361-6633/ab85b8},
	doi = {10.1088/1361-6633/ab85b8},
	abstract = {Quantum annealing is a computing paradigm that has the ambitious goal of ef ciently solving large-scale combinatorial optimization problems of practical importance. However, many challenges have yet to be overcome before this goal can be reached. This perspectives article rst gives a brief introduction to the concept of quantum annealing, and then highlights new pathways that may clear the way towards feasible and large scale quantum annealing. Moreover, since this eld of research is to a strong degree driven by a synergy between experiment and theory, we discuss both in this work. An important focus in this article is on future perspectives, which complements other review articles, and which we hope will motivate further research.},
	language = {en},
	number = {5},
	urldate = {2023-02-27},
	journal = {Reports on Progress in Physics},
	author = {Hauke, Philipp and Katzgraber, Helmut G and Lechner, Wolfgang and Nishimori, Hidetoshi and Oliver, William D},
	month = may,
	year = {2020},
	pages = {054401},
}

@article{heim_quantum_2020,
	title = {Quantum programming languages},
	volume = {2},
	issn = {2522-5820},
	url = {https://www.nature.com/articles/s42254-020-00245-7},
	doi = {10.1038/s42254-020-00245-7},
	abstract = {Quantum programming languages are essential to translate ideas into instructions that can be executed by a quantum computer. Not only are they crucial to the programming of quantum computers at scale but also they can facilitate the discovery and development of quantum algorithms even before hardware exists that is capable of executing them. Quantum programming languages are used for controlling existing physical devices, for estimating the execution costs of quantum algorithms on future devices, for teaching quantum computing concepts, or for verifying quantum algorithms and their implementations. They are used by newcomers and seasoned practitioners, researchers and developers working on the next ground-breaking discovery or applying known concepts to real-w orld problems. This variety in purpose and target audiences is reflected in the design and ecosystem of the existing quantum programming languages, depending on which factors a language prioritizes. In this Review, we highlight important aspects of quantum programming and how it differs from conventional programming. We overview a selection of several state-o f-the-art quantum programming languages, highlight their salient features, and provide code samples for each of the languages and Docker files to facilitate installation of the software packages.},
	language = {en},
	number = {12},
	urldate = {2023-02-27},
	journal = {Nature Reviews Physics},
	author = {Heim, Bettina and Soeken, Mathias and Marshall, Sarah and Granade, Chris and Roetteler, Martin and Geller, Alan and Troyer, Matthias and Svore, Krysta},
	month = nov,
	year = {2020},
	pages = {709--722},
}

@article{jones_quest_2019,
	title = {{QuEST} and {High} {Performance} {Simulation} of {Quantum} {Computers}},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-47174-9},
	doi = {10.1038/s41598-019-47174-9},
	abstract = {Abstract
            We introduce QuEST, the Quantum Exact Simulation Toolkit, and compare it to ProjectQ, qHipster and a recent distributed implementation of Quantum++. QuEST is the first open source, hybrid multithreaded and distributed, GPU accelerated simulator of universal quantum circuits. Embodied as a C library, it is designed so that a user’s code can be deployed seamlessly to any platform from a laptop to a supercomputer. QuEST is capable of simulating generic quantum circuits of general one and two-qubit gates and multi-qubit controlled gates, on pure and mixed states, represented as state-vectors and density matrices, and under the presence of decoherence. Using the ARCUS and ARCHER supercomputers, we benchmark QuEST’s simulation of random circuits of up to 38 qubits, distributed over up to 2048 compute nodes, each with up to 24 cores. We directly compare QuEST’s performance to ProjectQ’s on single machines, and discuss the differences in distribution strategies of QuEST, qHipster and Quantum++. QuEST shows excellent scaling, both strong and weak, on multicore and distributed architectures.},
	language = {en},
	number = {1},
	urldate = {2023-02-26},
	journal = {Scientific Reports},
	author = {Jones, Tyson and Brown, Anna and Bush, Ian and Benjamin, Simon C.},
	month = jul,
	year = {2019},
	pages = {10736},
}

@article{johansson_qutip_2012,
	title = {{QuTiP}: {An} open-source {Python} framework for the dynamics of open quantum systems},
	volume = {183},
	issn = {00104655},
	shorttitle = {{QuTiP}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010465512000835},
	doi = {10.1016/j.cpc.2012.02.021},
	abstract = {We present an object-oriented open-source framework for solving the dynamics of open quantum systems written in Python. Arbitrary Hamiltonians, including time-dependent systems, may be built up from operators and states deﬁned by a quantum object class, and then passed on to a choice of master equation or Monte Carlo solvers. We give an overview of the basic structure for the framework before detailing the numerical simulation of open system dynamics. Several examples are given to illustrate the build up to a complete calculation. Finally, we measure the performance of our library against that of current implementations. The framework described here is particularly well suited to the ﬁelds of quantum optics, superconducting circuit devices, nanomechanics, and trapped ions, while also being ideal for use in classroom instruction.},
	language = {en},
	number = {8},
	urldate = {2023-02-26},
	journal = {Computer Physics Communications},
	author = {Johansson, J.R. and Nation, P.D. and Nori, Franco},
	month = aug,
	year = {2012},
	pages = {1760--1772},
}

@article{diadamo_qunetsim_2021,
	title = {{QuNetSim}: {A} {Software} {Framework} for {Quantum} {Networks}},
	volume = {2},
	issn = {2689-1808},
	shorttitle = {{QuNetSim}},
	url = {https://ieeexplore.ieee.org/document/9465750/},
	doi = {10.1109/TQE.2021.3092395},
	abstract = {As quantum network technologies develop, the need for teaching and engineering tools such as simulators and emulators rises. QuNetSim addresses this need. QuNetSim is a Python software framework that delivers an easy-to-use interface for simulating quantum networks at the network layer, which can be extended at little effort of the user to implement the corresponding link layer protocols. The goal of QuNetSim is to make it easier to investigate and test quantum networking protocols over various quantum network configurations and parameters. The framework incorporates many known quantum network protocols so that users can quickly build simulations using a quantum-networking toolbox in a few lines of code and so that beginners can easily learn to implement their own quantum networking protocols. Unlike most current tools, QuNetSim simulates with real time and is, therefore, well suited to control laboratory hardware. Here, we present a software design overview of QuNetSim and demonstrate examples of protocols implemented with it. We describe ongoing work, which uses QuNetSim as a library, and describe possible future directions for the development of QuNetSim.},
	language = {en},
	urldate = {2023-02-26},
	journal = {IEEE Transactions on Quantum Engineering},
	author = {Diadamo, Stephen and Notzel, Janis and Zanger, Benjamin and Bese, Mehmet Mert},
	year = {2021},
	pages = {1--12},
}

@article{earnest_pulse-efficient_2021,
	title = {Pulse-efficient circuit transpilation for quantum applications on cross-resonance-based hardware},
	volume = {3},
	issn = {2643-1564},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.043088},
	doi = {10.1103/PhysRevResearch.3.043088},
	language = {en},
	number = {4},
	urldate = {2023-02-22},
	journal = {Physical Review Research},
	author = {Earnest, Nathan and Tornow, Caroline and Egger, Daniel J.},
	month = oct,
	year = {2021},
	pages = {043088},
}

@misc{younis_quantum_2022,
	title = {Quantum {Circuit} {Optimization} and {Transpilation} via {Parameterized} {Circuit} {Instantiation}},
	url = {http://arxiv.org/abs/2206.07885},
	abstract = {Parameterized circuit instantiation is a common technique encountered in the generation of circuits for a large class of hybrid quantum-classical algorithms. Despite being supported by popular quantum compilation infrastructures such as IBM Qiskit and Google Cirq, instantiation has not been extensively considered in the context of circuit compilation and optimization pipelines. In this work, we describe algorithms to apply instantiation during two common compilation steps: circuit optimization and gate-set transpilation. When placed in a compilation workﬂow, our circuit optimization algorithm produces circuits with an average of 13\% fewer gates than other optimizing compilers. Our gate-set transpilation algorithm can target any gate-set, even sets with multiple two-qubit gates, and produces circuits with an average of 12\% fewer two-qubit gates than other compilers. Overall, we show how instantiation can be incorporated into a compiler workﬂow to improve circuit quality and enhance portability, all while maintaining a reasonably low compile time overhead.},
	language = {en},
	urldate = {2023-02-22},
	publisher = {arXiv},
	author = {Younis, Ed and Iancu, Costin},
	month = jun,
	year = {2022},
	note = {arXiv:2206.07885 [quant-ph]},
	keywords = {Computer Science - Emerging Technologies, Quantum Physics},
}

@article{chatterjee_qurzon_2022,
	title = {Qurzon: {A} {Prototype} for a {Divide} and {Conquer}-{Based} {Quantum} {Compiler} for {Distributed} {Quantum} {Systems}},
	volume = {3},
	issn = {2661-8907},
	shorttitle = {Qurzon},
	url = {https://link.springer.com/10.1007/s42979-022-01207-9},
	doi = {10.1007/s42979-022-01207-9},
	abstract = {When working with algorithms on quantum devices, quantum memory becomes a crucial bottleneck due to low qubit count in NISQ-era devices. In this context, the concept of ‘divide and compute’, wherein a quantum circuit is broken into several subcircuits and executed separately, while stitching the results of the circuits via classical post-processing, becomes a viable option, especially in NISQ-era devices. This paper introduces Qurzon, a proposed novel quantum compiler that incorporates the marriage of techniques of divide and compute with the state-of-the-art algorithms of optimal qubit placement for executing on real quantum devices. A scheduling algorithm is also introduced within the compiler that can explore the power of distributed quantum computing while paving the way for quantum parallelism for large algorithms. Several benchmark circuits have been executed using the compiler, thereby demonstrating the power of the divide and compute when working with real NISQ-era quantum devices.},
	language = {en},
	number = {4},
	urldate = {2023-02-22},
	journal = {SN Computer Science},
	author = {Chatterjee, Turbasu and Das, Arnav and Mohtashim, Shah Ishmam and Saha, Amit and Chakrabarti, Amlan},
	month = jul,
	year = {2022},
	pages = {323},
}

@inproceedings{gemeinhardt_towards_2021,
	address = {Madrid, Spain},
	title = {Towards {Model}-{Driven} {Quantum} {Software} {Engineering}},
	isbn = {978-1-66544-462-0},
	url = {https://ieeexplore.ieee.org/document/9474563/},
	doi = {10.1109/Q-SE52541.2021.00010},
	abstract = {Quantum technologies are emerging. Dedicated languages for programming Quantum machines are emerging as well and already used in different settings. Orthogonal to this development, Model-Driven Engineering (MDE) is explored to ease the development of software systems by applying modeling techniques such as Domain-Speciﬁc Modeling Languages and generative techniques such as code generation.},
	language = {en},
	urldate = {2023-02-14},
	booktitle = {2021 {IEEE}/{ACM} 2nd {International} {Workshop} on {Quantum} {Software} {Engineering} ({Q}-{SE})},
	publisher = {IEEE},
	author = {Gemeinhardt, Felix and Garmendia, Antonio and Wimmer, Manuel},
	month = jun,
	year = {2021},
	pages = {13--15},
}

@inproceedings{barbosa_software_2020,
	address = {Seoul Republic of Korea},
	title = {Software engineering for 'quantum advantage'},
	isbn = {978-1-4503-7963-2},
	url = {https://dl.acm.org/doi/10.1145/3387940.3392184},
	doi = {10.1145/3387940.3392184},
	abstract = {Software is a critical factor in the reliability of computer systems. While the development of hardware is assisted by mature science and engineering disciplines, software science is still in its infancy. This situation is likely to worsen in the future with quantum computer systems. Actually, if quantum computing is quickly coming of age, with potential groundbreaking impacts on many different fields, such benefits come at a price: quantum programming is hard and finding new quantum algorithms is far from straightforward. Thus, the need for suitable formal techniques in quantum software development is even bigger than in classical computation. A lack of reliable approaches to quantum computer programming will put at risk the expected quantum advantage of the new hardware. This position paper argues for the need for a proper quantum software engineering discipline benefiting from precise foundations and calculi, capable of supporting algorithm development and analysis.},
	language = {en},
	urldate = {2023-02-14},
	booktitle = {Proceedings of the {IEEE}/{ACM} 42nd {International} {Conference} on {Software} {Engineering} {Workshops}},
	publisher = {ACM},
	author = {Barbosa, Luis S.},
	month = jun,
	year = {2020},
	pages = {427--429},
}

@article{fingerhuth_open_2018,
	title = {Open source software in quantum computing},
	volume = {13},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0208561},
	doi = {10.1371/journal.pone.0208561},
	abstract = {Open source software is becoming crucial in the design and testing of quantum algorithms. Many of the tools are backed by major commercial vendors with the goal to make it easier to develop quantum software: this mirrors how well-funded open machine learning frameworks enabled the development of complex models and their execution on equally complex hardware. We review a wide range of open source software for quantum computing, covering all stages of the quantum toolchain from quantum hardware interfaces through quantum compilers to implementations of quantum algorithms, as well as all quantum computing paradigms, including quantum annealing, and discrete and continuous-variable gate-model quantum computing. The evaluation of each project covers characteristics such as documentation, licence, the choice of programming language, compliance with norms of software engineering, and the culture of the project. We find that while the diversity of projects is mesmerizing, only a few attract external developers and even many commercially backed frameworks have shortcomings in software engineering. Based on these observations, we highlight the best practices that could foster a more active community around quantum computing software that welcomes newcomers to the field, but also ensures high-quality, welldocumented code.},
	language = {en},
	number = {12},
	urldate = {2023-02-14},
	journal = {PLOS ONE},
	author = {Fingerhuth, Mark and Babej, Tomáš and Wittek, Peter},
	editor = {Mueck, Leonie Anna},
	month = dec,
	year = {2018},
	pages = {e0208561},
}

@article{ali_when_2022,
	title = {When software engineering meets quantum computing},
	volume = {65},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3512340},
	doi = {10.1145/3512340},
	language = {en},
	number = {4},
	urldate = {2023-02-14},
	journal = {Communications of the ACM},
	author = {Ali, Shaukat and Yue, Tao and Abreu, Rui},
	month = apr,
	year = {2022},
	pages = {84--88},
}

@article{kim_scalable_2023,
	title = {Scalable error mitigation for noisy quantum circuits produces competitive expectation values},
	issn = {1745-2473, 1745-2481},
	url = {https://www.nature.com/articles/s41567-022-01914-3},
	doi = {10.1038/s41567-022-01914-3},
	language = {en},
	urldate = {2023-02-14},
	journal = {Nature Physics},
	author = {Kim, Youngseok and Wood, Christopher J. and Yoder, Theodore J. and Merkel, Seth T. and Gambetta, Jay M. and Temme, Kristan and Kandala, Abhinav},
	month = feb,
	year = {2023},
}

@article{quantum_technology_and_application_consortium__qutac_industry_2021,
	title = {Industry quantum computing applications},
	volume = {8},
	issn = {2662-4400, 2196-0763},
	url = {https://epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00114-x},
	doi = {10.1140/epjqt/s40507-021-00114-x},
	abstract = {Quantum computing promises to overcome computational limitations with better and faster solutions for optimization, simulation, and machine learning problems. Europe and Germany are in the process of successfully establishing research and funding programs with the objective to advance the technology’s ecosystem and industrialization, thereby ensuring digital sovereignty, security, and competitiveness. Such an ecosystem comprises hardware/software solution providers, system integrators, and users from research institutions, start-ups, and industry. The vision of the Quantum Technology and Application Consortium (QUTAC) is to establish and advance the quantum computing ecosystem, supporting the ambitious goals of the German government and various research programs. QUTAC is comprised of ten members representing diﬀerent industries, in particular automotive manufacturing, chemical and pharmaceutical production, insurance, and technology. In this paper, we survey the current state of quantum computing in these sectors as well as the aerospace industry and identify the contributions of QUTAC to the ecosystem. We propose an application-centric approach for the industrialization of the technology based on proven business impact. This paper identiﬁes 24 diﬀerent use cases. By formalizing high-value use cases into well-described reference problems and benchmarks, we will guide technological progress and eventually commercialization. Our results will be beneﬁcial to all ecosystem participants, including suppliers, system integrators, software developers, users, policymakers, funding program managers, and investors.},
	language = {en},
	number = {1},
	urldate = {2023-02-10},
	journal = {EPJ Quantum Technology},
	author = {{Quantum Technology and Application Consortium – QUTAC} and Bayerstadler, Andreas and Becquin, Guillaume and Binder, Julia and Botter, Thierry and Ehm, Hans and Ehmer, Thomas and Erdmann, Marvin and Gaus, Norbert and Harbach, Philipp and Hess, Maximilian and Klepsch, Johannes and Leib, Martin and Luber, Sebastian and Luckow, Andre and Mansky, Maximilian and Mauerer, Wolfgang and Neukart, Florian and Niedermeier, Christoph and Palackal, Lilly and Pfeiffer, Ruben and Polenz, Carsten and Sepulveda, Johanna and Sievers, Tammo and Standen, Brian and Streif, Michael and Strohm, Thomas and Utschig-Utschig, Clemens and Volz, Daniel and Weiss, Horst and Winter, Fabian},
	month = dec,
	year = {2021},
	pages = {25},
}

@article{vietz_exploratory_nodate,
	title = {An {Exploratory} {Study} on the {Challenges} of {Engineering} {Quantum} {Applications} in the {Cloud}},
	abstract = {The rapid evolution of quantum computation in the cloud creates considerable opportunities for multiple real-world application scenarios, including chemical simulation, optimization, and machine learning. Typical quantum applications are hybrid as they consist of both classical and quantum components. The latter require quantum computers for execution, which are often offered as cloud services. Thus, to implement quantum applications, developers need to have expertise in integration of quantum and classical components of the application, as well as understanding the relevant cloud-specific challenges and limitations. In this work, we explore the challenges which can be encountered when designing and implementing hybrid quantum applications in the cloud and identify which limitations of current quantum cloud services make such integration complex. To achieve this, we (i) implemented four quantum applications highlighting different scenarios of using quantum software components in cloud applications and (ii) analyzed the challenges and limitations encountered during the implementation process and documented the key observations. In addition, we discuss open research questions and ways to address them to improve the process of developing quantum applications in the cloud.},
	language = {en},
	author = {Vietz, Daniel},
}

@article{nguyen_qfaas_nodate,
	title = {{QFaaS}: {A} {Serverless} {Function}-as-a-{Service} {Framework} for {Quantum} {Computing}},
	copyright = {All rights reserved},
	abstract = {Quantum computing is rapidly reaching a point in which its application design and engineering aspects have to be seriously considered. However, quantum software engineering is still in its infancy with numerous challenges, especially in dealing with the diversity of quantum programming languages and noisy intermediate-scale quantum systems. To alleviate these challenges, we propose QFaaS, a novel Quantum Function-as-a-Service framework, which leverages the advantages of the serverless model and state-of-the-art techniques to advance practical quantum computing in the NISQ era. Our framework provides essential elements of a serverless quantum system to streamline service-oriented quantum application development in cloud environments, such as combining hybrid quantum-classical computation, automating the backend selection, and adapting quantum DevOps techniques. QFaaS offers the first full-stack and unified quantum serverless platform by integrating multiple well-known quantum software development kits (Qiskit, Q\#, Cirq, and Braket), quantum simulators, and cloud providers (IBM Quantum and Amazon Braket). This paper proposes the concept of quantum functions, software life cycle, system design, operation workflows, and implementation of QFaaS. We also present two practical use cases and performance evaluations on today’s quantum computers and simulators to demonstrate our framework’s capability to ease the burden on traditional engineers to expedite the ongoing quantum software transition.},
	language = {en},
	author = {Nguyen, Hoa T and Usman, Muhammad and Buyya, Rajkumar},
}

@article{habaebi_extending_2023,
	title = {Extending {CloudSim} to simulate sensor networks},
	volume = {99},
	issn = {0037-5497, 1741-3133},
	url = {http://journals.sagepub.com/doi/10.1177/00375497221105530},
	doi = {10.1177/00375497221105530},
	abstract = {With the enormous growth of sensing devices tending to the use of Internet of everything, data aggregated by these devices are the biggest data streams generated in the history of IT. Thus, aggregating such data in the cloud for leveraging powerful cloud computing processing and storage is essential, and it eventually led to the emergence of Sensor-Cloud concept. This has allowed aggregation of the sensors’ data to the cloud for further processing, storage, and visualization. Furthermore, virtualization makes the sensors accessible to other end-user applications that require such data. All of these features are expected to be provided by the Sensor-Cloud invisibly, without the end-user application developer being aware of the sensor location or hardware specifications. For these reasons, a simulation platform where SensorCloud infrastructure agents and components may be modeled, scheduling policies defined, and execution time assessed is essential to assure performance and quality of service. The aim of this study is to develop such a platform by enhancing CloudSim, the most well-known and powerful simulation tool for cloud computing. A user-friendly Java Script Swingbased graphical user interface (GUI) has been carefully designed and implemented for this purpose. The user can then utilize the specific interface to define the Cloudlet type as well as the scheduling on a single virtual machine. Finally, a simulation study is carried out on the platform to demonstrate its efficiency and accuracy. We were able to fully model the needed scenarios and acquire real-time results, displaying good accuracy in terms of application response time with a mean absolute percentage error (MAPE) of 3.37\%, demonstrating the increased proposed platform’s proper operation.},
	language = {en},
	number = {1},
	urldate = {2023-01-31},
	journal = {SIMULATION},
	author = {Habaebi, Mohamed Hadi and Merrad, Yaçine and Islam, Md Rafiqul and Elsheikh, Elfatih A A and Sliman, F M and Mesri, Mokhtaria},
	month = jan,
	year = {2023},
	pages = {3--22},
}

@inproceedings{guimaraes_towards_2022,
	address = {Barcelona, Spain},
	title = {Towards a layered architecture for error mitigation in quantum computation},
	isbn = {978-1-66548-134-2},
	url = {https://ieeexplore.ieee.org/document/9860214/},
	doi = {10.1109/QSW55613.2022.00022},
	abstract = {In the past few years, the ﬁrst commercially available quantum computers have emerged, in an early stage of development, the so-called Noisy Intermediate-Scale Quantum (NISQ) era. Although these devices are still very prone to errors of different natures, they also have shown to deal successfully with small computational problems. Nowadays, one of the challenges in quantum computation is exactly to be able to show that quantum computers are useful, whereby mitigating the effects of the faulty hardware is pivotal. Recently, a wide range of quantum error mitigation techniques have been proposed and successfully implemented, alternative to quantum error correction codes. Herein, we discuss several types of noise in a quantum computer and techniques available to mitigate them, as well as their limitations and conditions of applicability. We also suggest an hierarchy for them, towards the conception of a layered software framework of error mitigation techniques, and implement some of them in a quantum simulation of the Heisenberg model on an IBM quantum computer, improving the ﬁdelity of the simulation by 2.8x.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2022 {IEEE} {International} {Conference} on {Quantum} {Software} ({QSW})},
	publisher = {IEEE},
	author = {Guimaraes, Jose D. and Tavares, Carlos},
	month = jul,
	year = {2022},
	pages = {41--51},
}

@inproceedings{ahmad_towards_2022,
	address = {Barcelona, Spain},
	title = {Towards {Process} {Centered} {Architecting} for {Quantum} {Software} {Systems}},
	isbn = {978-1-66548-134-2},
	url = {https://ieeexplore.ieee.org/document/9860221/},
	doi = {10.1109/QSW55613.2022.00019},
	abstract = {Quantum Software Engineering (QSE) is a recent trend - focused on unifying the principles of quantum mechanics and practices of software engineering - to design, develop, validate, and evolve quantum age software systems and applications. Software architecture for quantum computing (a.k.a. quantum software architectures (QSA)) supports the design, development, and maintenance etc. phases of quantum software systems using architectural components and connectors. QSA can enable quantum software designers and developers to map the operations of Qubits to architectural components and connectors for implementing quantum software. This research aims to explore the role of QSAs by investigating (i) architectural process having architecting activities, and (ii) human roles that can exploit available tools to automate and customise architecturecentric implementation of quantum software. Results of this research can facilitate knowledge transfer, enabling researchers and practitioners, to address challenges of architecture-centric implementation of quantum software systems.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2022 {IEEE} {International} {Conference} on {Quantum} {Software} ({QSW})},
	publisher = {IEEE},
	author = {Ahmad, Aakash and Khan, Arif Ali and Waseem, Muhammad and Fahmideh, Mahdi and Mikkonen, Tommi},
	month = jul,
	year = {2022},
	pages = {26--31},
}

@inproceedings{scheerer_fault-tolerant_2022,
	address = {Barcelona, Spain},
	title = {Fault-tolerant {Hybrid} {Quantum} {Software} {Systems}},
	isbn = {978-1-66548-134-2},
	url = {https://ieeexplore.ieee.org/document/9860169/},
	doi = {10.1109/QSW55613.2022.00023},
	abstract = {We are currently experiencing the Noise Intermediate-Scale Quantum (NISQ) era where quantum algorithms are forced to be small in terms of qubits and gates used. This limitation makes it infeasible to apply quantum error correction. Consequently, running quantum algorithms on NISQ devices leads to gate and measurement errors and thus unreliable calculations. In this work, we approach this problem at the software architecture level. More precisely, we instantiate well-known and established architectural patterns for fault-tolerant systems in the context of quantum computing. Our preliminary results indicate that this approach is very promising as the patterns are applicable to the context of quantum programming and allow to gain more reliable results.},
	language = {en},
	urldate = {2023-01-30},
	booktitle = {2022 {IEEE} {International} {Conference} on {Quantum} {Software} ({QSW})},
	publisher = {IEEE},
	author = {Scheerer, Max and Klamroth, Jonas and Denninger, Oliver},
	month = jul,
	year = {2022},
	pages = {52--57},
}

@article{buyya_cloud_2009,
	title = {Cloud computing and emerging {IT} platforms: {Vision}, hype, and reality for delivering computing as the 5th utility},
	volume = {25},
	issn = {0167739X},
	url = {http://dx.doi.org/10.1016/j.future.2008.12.001},
	doi = {10.1016/j.future.2008.12.001},
	abstract = {With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing 'Storage Clouds' for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision. © 2008 Elsevier B.V. All rights reserved.},
	number = {6},
	journal = {Future Generation Computer Systems},
	author = {Buyya, Rajkumar and Yeo, Chee Shin and Venugopal, Srikumar and Broberg, James and Brandic, Ivona},
	year = {2009},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cloud computing, Data Centers, Market-oriented resource allocation, Utility computing, Virtualization},
	pages = {599--616},
}

@misc{qcloud-dwave,
	title = {D-{Wave} {Systems} {Leap} - {Quantum} {Computing} {Service}},
	url = {https://www.dwavesys.com/solutions-and-products/cloud-platform/},
	author = {D-Wave System},
	year = {2022},
}

@inproceedings{eismann_sizeless_2021,
	address = {Québec city Canada},
	title = {Sizeless: predicting the optimal size of serverless functions},
	isbn = {978-1-4503-8534-3},
	shorttitle = {Sizeless},
	url = {https://dl.acm.org/doi/10.1145/3464298.3493398},
	doi = {10.1145/3464298.3493398},
	abstract = {Serverless functions are an emerging cloud computing paradigm that is being rapidly adopted by both industry and academia. In this cloud computing model, the provider opaquely handles resource management tasks such as resource provisioning, deployment, and auto-scaling. The only resource management task that developers are still in charge of is selecting how much resources are allocated to each worker instance. However, selecting the optimal size of serverless functions is quite challenging, so developers often neglect it despite its significant cost and performance benefits. Existing approaches aiming to automate serverless functions resource sizing require dedicated performance tests, which are time-consuming to implement and maintain.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Proceedings of the 22nd {International} {Middleware} {Conference}},
	publisher = {ACM},
	author = {Eismann, Simon and Bui, Long and Grohmann, Johannes and Abad, Cristina and Herbst, Nikolas and Kounev, Samuel},
	month = dec,
	year = {2021},
	pages = {248--259},
}

@incollection{ryan_post-quantum_2016,
	address = {Berlin, Heidelberg},
	title = {Post-{Quantum} {Cryptography}: {State} of the {Art}},
	volume = {9100},
	isbn = {978-3-662-49300-7 978-3-662-49301-4},
	shorttitle = {Post-{Quantum} {Cryptography}},
	url = {http://link.springer.com/10.1007/978-3-662-49301-4_6},
	urldate = {2023-01-12},
	booktitle = {The {New} {Codebreakers}},
	publisher = {Springer Berlin Heidelberg},
	author = {Buchmann, Johannes A. and Butin, Denis and Göpfert, Florian and Petzoldt, Albrecht},
	editor = {Ryan, Peter Y. A. and Naccache, David and Quisquater, Jean-Jacques},
	year = {2016},
	doi = {10.1007/978-3-662-49301-4_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {88--108},
}

@article{bernstein_post-quantum_2017,
	title = {Post-quantum cryptography},
	volume = {549},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature23461},
	doi = {10.1038/nature23461},
	language = {en},
	number = {7671},
	urldate = {2023-01-12},
	journal = {Nature},
	author = {Bernstein, Daniel J. and Lange, Tanja},
	month = sep,
	year = {2017},
	pages = {188--194},
}

@inproceedings{iftikhar_quantum_2021,
	address = {Portsmouth, United Kingdom},
	title = {Quantum {Safe} {Cloud} {Computing} {Using} {Hash}-based {Digital} {Signatures}},
	isbn = {978-1-86043-557-7},
	url = {https://ieeexplore.ieee.org/document/9594228/},
	doi = {10.23919/ICAC50006.2021.9594228},
	abstract = {Cloud computing provides its clients with ondemand resources, such as storage and performance. The idea behind cloud computing is sharing of resources. With growth of cloud computing, the risks of security and privacy are arising. Also, the cloud resources are not secure against quantum computers, as quantum computing threatens the best security controls implemented today, that include ECC (Elliptic Curve Cryptography), RSA, DSA (Digital Signature Algorithm) and ECDSA (Elliptic Curve Digital Signature Algorithm). Fortunately, there are quantum-safe digital signatures called Hashbased Signatures, which can also protect integrity, authentication and privacy on the cloud. In this work, a hash-based digital signature scheme called WOTS is implemented to avoid security risks imposed by quantum computing on cloud computing. The implementation is carried out in CloudSim simulator. Apart from achieving quantum safety, the results show that our proposed solution is computationally cost effective.},
	language = {en},
	urldate = {2023-01-12},
	booktitle = {2021 26th {International} {Conference} on {Automation} and {Computing} ({ICAC})},
	publisher = {IEEE},
	author = {Iftikhar, Zainab and Iftikhar, Malayka and Shah, Munam Ali},
	month = sep,
	year = {2021},
	pages = {1--6},
}

@article{wallden_cyber_2019,
	title = {Cyber security in the quantum era},
	volume = {62},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3241037},
	doi = {10.1145/3241037},
	abstract = {Quantum systems will significantly affect the field of cyber security research.},
	language = {en},
	number = {4},
	urldate = {2023-01-12},
	journal = {Communications of the ACM},
	author = {Wallden, Petros and Kashefi, Elham},
	month = mar,
	year = {2019},
	pages = {120--120},
}

@article{roetteler_quantum_2018,
	title = {Quantum {Computing}: {Codebreaking} and {Beyond}},
	volume = {16},
	issn = {1540-7993, 1558-4046},
	shorttitle = {Quantum {Computing}},
	url = {https://ieeexplore.ieee.org/document/8490171/},
	doi = {10.1109/MSP.2018.3761710},
	language = {en},
	number = {5},
	urldate = {2023-01-12},
	journal = {IEEE Security \& Privacy},
	author = {Roetteler, Martin and Svore, Krysta M.},
	month = sep,
	year = {2018},
	pages = {22--36},
}

@article{gong_grover_2020,
	title = {Grover algorithm-based quantum homomorphic encryption ciphertext retrieval scheme in quantum cloud computing},
	volume = {19},
	issn = {1570-0755, 1573-1332},
	url = {http://link.springer.com/10.1007/s11128-020-2603-0},
	doi = {10.1007/s11128-020-2603-0},
	abstract = {Existing classical ciphertext retrieval schemes are mainly developed according to the homomorphic encryption that is a cryptographic technique based on computational complexity theory of mathematical puzzles. In quantum computing, on the one hand, most of traditional asymmetric encryption can be quickly cracked as the computational capacity of quantum computer is much higher than that of traditional digital computer. Hence, the quantum homomorphic encryption scheme is widely used in data encryption for the issue of privacy protection in quantum computing. But on the other the retrieval efﬁciency of homomorphic encrypted data is insufﬁcient especially in quantum cloud computing. Therefore, this paper proposes a novel quantum homomorphic encryption ciphertext retrieval (QHECR) scheme basing on the Grover algorithm to solve the problem of homomorphic encrypted ciphertext data retrieval in quantum cloud computing. Our scheme is to improve such retrieval efﬁciency mentioned above where the interaction process is not required even if the T -gate exists in the circuit. In the experiment, two qubits without the T -gate are conducted on both simulations and real quantum devices by using IBM quantum information science kit (Qiskit). The results show that the proposed QHECR scheme is capable of achieving the retrieval encrypted data when the T -gate does not exist in the evaluation circuit in ciphertext environment. Moreover, a veriﬁcation experiment about the T -gate key update algorithm is implemented to verify the feasibility and reliability of the proposed scheme in the Qiskit as well, which indicates that the QHECR is still available when the T -gate exists in the circuit. Since the decryption is inefﬁcient when there are exponential T -gates in the circuit, our proposed scheme is suitable for low T -gate complexity.},
	language = {en},
	number = {3},
	urldate = {2023-01-12},
	journal = {Quantum Information Processing},
	author = {Gong, Changqing and Du, Juan and Dong, Zhaoyang and Guo, Zhenzhou and Gani, Abdullah and Zhao, Liang and Qi, Han},
	month = mar,
	year = {2020},
	pages = {105},
}

@article{phalak_quantum_2021,
	title = {Quantum {PUF} for {Security} and {Trust} in {Quantum} {Computing}},
	volume = {11},
	issn = {2156-3357, 2156-3365},
	url = {https://ieeexplore.ieee.org/document/9420762/},
	doi = {10.1109/JETCAS.2021.3077024},
	language = {en},
	number = {2},
	urldate = {2023-01-12},
	journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	author = {Phalak, Koustubh and Saki, Abdullah Ash- and Alam, Mahabubul and Topaloglu, Rasit Onur and Ghosh, Swaroop},
	month = jun,
	year = {2021},
	pages = {333--342},
}

@article{kahanamoku-meyer_forging_2019,
	title = {Forging quantum data: classically defeating an {IQP}-based quantum test},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Forging quantum data},
	url = {https://arxiv.org/abs/1912.05547},
	doi = {10.48550/ARXIV.1912.05547},
	abstract = {In 2009, Shepherd and Bremner proposed a "test of quantum capability" arXiv:0809.0847 that is attractive because the quantum machine's output can be verified efficiently by classical means. While follow-up papers gave evidence that directly simulating the quantum prover is classically hard, the security of the protocol against other (non-simulating) classical attacks has remained an open question. In this paper, I demonstrate that the protocol is not secure against classical provers. I describe a classical algorithm that can not only convince the verifier that the (classical) prover is quantum, but can in fact can extract the secret key underlying a given protocol instance. Furthermore, I show that the algorithm is efficient in practice for problem sizes of hundreds of qubits. Finally, I provide an implementation of the algorithm, and give the secret vector underlying the "\$25 challenge" posted online by the authors of the original paper.},
	urldate = {2023-01-12},
	author = {Kahanamoku-Meyer, Gregory D.},
	year = {2019},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Cryptography and Security (cs.CR), FOS: Computer and information sciences, FOS: Physical sciences, Quantum Physics (quant-ph)},
}

@article{shepherd_temporally_2009,
	title = {Temporally unstructured quantum computation},
	volume = {465},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2008.0443},
	doi = {10.1098/rspa.2008.0443},
	abstract = {We examine theoretic architectures and an abstract model for a restricted class of quantum computation, called here
              temporally unstructured
              (‘
              instantaneous
              ’)
              quantum computation
              because it allows for essentially no temporal structure within the quantum dynamics. Using the theory of binary matroids, we argue that the paradigm is rich enough to enable sampling from probability distributions that cannot, classically, be sampled efficiently and accurately. This paradigm also admits simple interactive proof games that may convince a sceptic of the existence of truly quantum effects. Furthermore, these effects can be created using significantly fewer qubits than are required for running Shor's algorithm.},
	language = {en},
	number = {2105},
	urldate = {2023-01-12},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Shepherd, Dan and Bremner, Michael J.},
	month = may,
	year = {2009},
	pages = {1413--1439},
}

@article{chen_experimental_2021,
	title = {Experimental cryptographic verification for near-term quantum cloud computing},
	volume = {66},
	issn = {20959273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S209592732030534X},
	doi = {10.1016/j.scib.2020.08.013},
	abstract = {An important task for quantum cloud computing is to make sure that there is a real quantum computer running, instead of classical simulation. Here we explore the applicability of a cryptographic veriﬁcation scheme for verifying quantum cloud computing. We provided a theoretical extension and implemented the scheme on a 5-qubit NMR quantum processor in the laboratory and a 5-qubit and 16-qubit processors of the IBM quantum cloud. We found that the experimental results of the NMR processor can be veriﬁed by the scheme with about 1:4\% error, after noise compensation by standard techniques. However, the ﬁdelity of the IBM quantum cloud is currently too low to pass the test (about 42\% error). This veriﬁcation scheme shall become practical when servers claim to offer quantum-computing resources that can achieve quantum supremacy.},
	language = {en},
	number = {1},
	urldate = {2023-01-12},
	journal = {Science Bulletin},
	author = {Chen, Xi and Cheng, Bin and Li, Zhaokai and Nie, Xinfang and Yu, Nengkun and Yung, Man-Hong and Peng, Xinhua},
	month = jan,
	year = {2021},
	pages = {23--28},
}

@article{gong_quantum_2021,
	title = {Quantum k-means algorithm based on trusted server in quantum cloud computing},
	volume = {20},
	issn = {1570-0755, 1573-1332},
	url = {https://link.springer.com/10.1007/s11128-021-03071-7},
	doi = {10.1007/s11128-021-03071-7},
	abstract = {Quantum k-means algorithm is widely used in solving clustering problems. However, the repeated calculation of the core subroutines overloads the quantum computer with mass computing tasks. In addition, due to the decoherence of the quantum state, the computing power of personal quantum computer is limited. Therefore, it is necessary to use high-performance quantum cloud computing to complete data processing. This paper proposes a quantum k-means algorithm in quantum cloud computing, which uploads the data to the server to execute the core subroutines SwapT est (calculating the similarity between ciphertext quantum states) and Gr over O ptim (quantum minimization algorithm) of quantum k-means algorithm and then sends the ciphertext to the client for further decryption using the decryption key, which reduces the load of the client. An improved quantum homomorphic encryption by t-gate updating in trusted server and data processing in semi-trusted server is introduced as well. In this scheme, the trusted server assists the semi-trusted server to execute the T -gate existing in the quantum circuit, which separates the key from the ciphertext and ensure the security of the ciphertext data processing in the quantum cloud. In our experiment, the two subroutines, SwapT est and Gr over O ptim, are conducted in ciphertext and plaintext conditions by using IBM Qiskit to evaluate the reliability and feasibility. The result shows that the decrypted ciphertext result is the same as the plaintext. We conclude that the proposed algorithm has a better performance in reducing the cost of load in client and protecting privacy data in cloud by contrast with the original quantum k-means algorithm.},
	language = {en},
	number = {4},
	urldate = {2023-01-12},
	journal = {Quantum Information Processing},
	author = {Gong, Changqing and Dong, Zhaoyang and Gani, Abdullah and Qi, Han},
	month = apr,
	year = {2021},
	pages = {130},
}

@inproceedings{singh_quantum_2014,
	address = {Faridabad, Haryana, India},
	title = {The {Quantum} way of {Cloud} {Computing}},
	isbn = {978-1-4799-2995-5 978-1-4799-3958-9 978-1-4799-2996-2},
	url = {http://ieeexplore.ieee.org/document/6798362/},
	doi = {10.1109/ICROIT.2014.6798362},
	abstract = {Quantum Computing and Cloud Computing are the technologies which have the capability to shape the future of computing. Quantum computing focuses on creating super-fast computers using the concepts of quantum physics whereas Cloud computing allows the computing power to be provided as a service. This paper presents a theoretical approach towards the possibility of a Quantum-Cloud i.e. quantum computing as a service. This will combine the fields of quantum computing and cloud computing, resulting into an evolutionary technology. Also, this paper discusses the possible advantages of this in the near future.},
	language = {en},
	urldate = {2023-01-12},
	booktitle = {2014 {International} {Conference} on {Reliability} {Optimization} and {Information} {Technology} ({ICROIT})},
	publisher = {IEEE},
	author = {Singh, Harpreet and Sachdev, Abha},
	month = feb,
	year = {2014},
	pages = {397--400},
}

@article{singh_quantum_internet_2021,
	title = {Quantum {Internet}—{Applications}, {Functionalities}, {Enabling} {Technologies}, {Challenges}, and {Research} {Directions}},
	volume = {23},
	issn = {1553-877X, 2373-745X},
	url = {https://ieeexplore.ieee.org/document/9528843/},
	doi = {10.1109/COMST.2021.3109944},
	abstract = {The advanced notebooks, mobile phones, and Internet applications in today’s world that we use are all entrenched in classical communication bits of zeros and ones. Classical Internet has laid its foundation originating from the amalgamation of mathematics and Claude Shannon’s theory of information. However, today’s Internet technology is a playground for eavesdroppers. This poses a serious challenge to various applications that rely on classical Internet technology, and it has motivated the researchers to switch to new technologies that are fundamentally more secure. By exploring the quantum effects, researchers paved the way into quantum networks that provide security, privacy, and range of capabilities such as quantum computation, communication, and metrology. The realization of Quantum Internet (QI) requires quantum communication between various remote nodes through quantum channels guarded by quantum cryptographic protocols. Such networks rely upon quantum bits (qubits) that can simultaneously take the value of zeros and ones. Due to the extraordinary properties of qubits such as superposition, entanglement, and teleportation, it gives an edge to quantum networks over traditional networks in many ways. At the same time, transmitting qubits over long distances is a formidable task and extensive research is going on satellite-based quantum communication, which will deliver breakthroughs for physically realizing QI in near future. In this paper, QI functionalities, technologies, applications and open challenges have been extensively surveyed to help readers gain a basic understanding of the infrastructure required for the development of the global QI.},
	language = {en},
	number = {4},
	urldate = {2023-01-11},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Singh, Amoldeep and Dev, Kapal and Siljak, Harun and Joshi, Hem Dutt and Magarini, Maurizio},
	year = {2021},
	pages = {2218--2247},
}

@misc{loke_distributed_2022,
	title = {From {Distributed} {Quantum} {Computing} to {Quantum} {Internet} {Computing}: an {Overview}},
	shorttitle = {From {Distributed} {Quantum} {Computing} to {Quantum} {Internet} {Computing}},
	url = {http://arxiv.org/abs/2208.10127},
	abstract = {The possibility of quantum computing has been proposed decades ago, at least as far back as the 1980s, and distributed quantum computing has been studied around two decades ago. Recent times have seen experimental successes and advances in quantum computer hardware and in quantum networking, leading towards the quantum Internet. We provide in this paper an overview of concepts and ideas in distributed quantum computing since over two decades ago as well as look at recent efforts in the area, and consider how, with the development of the quantum Internet, distributed quantum computing is evolving into quantum Internet computing.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Loke, Seng W.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.10127 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Emerging Technologies},
}

@article{cuomo_towards_2020,
	title = {Towards a distributed quantum computing ecosystem},
	volume = {1},
	issn = {2632-8925, 2632-8925},
	url = {https://onlinelibrary.wiley.com/doi/10.1049/iet-qtc.2020.0002},
	doi = {10.1049/iet-qtc.2020.0002},
	abstract = {The Quantum Internet, by enabling quantum communications among remote quantum nodes, is a network capable of supporting functionalities with no direct counterpart in the classical world. Indeed, with the network and communications functionalities provided by the Quantum Internet, remote quantum devices can communicate and cooperate for solving challenging computational tasks by adopting a distributed computing approach. The aim of this study is to provide the reader with an overview about the main challenges and open problems arising in the design of a distributed quantum computing ecosystem. For this, the authors provide a survey, following a bottom-up approach, from a communications engineering perspective. They start by introducing the Quantum Internet as the fundamental underlying infrastructure of the distributed quantum computing ecosystem. Then they go further, by elaborating on a high-level system abstraction of the distributed quantum computing ecosystem. They describe such an abstraction through a set of logical layers. Thereby, they clarify dependencies among the aforementioned layers and, at the same time, a road-map emerges.},
	language = {en},
	number = {1},
	urldate = {2023-01-11},
	journal = {IET Quantum Communication},
	author = {Cuomo, Daniele and Caleffi, Marcello and Cacciapuoti, Angela Sara},
	month = jul,
	year = {2020},
	pages = {3--8},
}

@incollection{kumar_comprehensive_2022,
	address = {Cham},
	title = {A {Comprehensive} {Overview} of {Quantum} {Internet}: {Architecture}, {Protocol} and {Challenges}},
	volume = {133},
	isbn = {978-3-031-04612-4 978-3-031-04613-1},
	shorttitle = {A {Comprehensive} {Overview} of {Quantum} {Internet}},
	url = {https://link.springer.com/10.1007/978-3-031-04613-1_8},
	abstract = {The Quantum Internet is deﬁned as the internetwork of remote quantum devices through Quantum communications. The major advantages of Quantum Internet include the independence of legacy internet, a wide range of secured information transmission, and the capability of cutting-edge bling computing. This chapter focuses on four primary objectives. The ﬁrst section focuses on exploring the fundaments behind Quantum Computing and the Quantum Internet. The Quantum entanglement, Quantum bits, and Quantum states will be discussed in this section. The second section focuses on envisioning the Quantum Internet standard architecture. The third objective is to explore Quantum Teleportation as a Quantum communication protocol for Quantum Internet. Finally, the challenges of Quantum Internet will be discussed. The major challenges include de-coherence of quantum information, non-cloning information theory, quantum state collapsing, distributed networking of quantum devices.},
	language = {en},
	urldate = {2023-01-11},
	booktitle = {Quantum and {Blockchain} for {Modern} {Computing} {Systems}: {Vision} and {Advancements}},
	publisher = {Springer International Publishing},
	author = {Krishnamurthi, Rajalakshmi and Bhatt, Arpita Jadhav and Sardana, Neetu},
	editor = {Kumar, Adarsh and Gill, Sukhpal Singh and Abraham, Ajith},
	year = {2022},
	doi = {10.1007/978-3-031-04613-1_8},
	note = {Series Title: Lecture Notes on Data Engineering and Communications Technologies},
	pages = {223--247},
}

@article{illiano_quantum_2022,
	title = {Quantum {Internet} protocol stack: {A} comprehensive survey},
	volume = {213},
	issn = {13891286},
	shorttitle = {Quantum {Internet} protocol stack},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128622002250},
	doi = {10.1016/j.comnet.2022.109092},
	abstract = {Classical Internet evolved exceptionally during the last five decades, from a network comprising a few static nodes in the early days to a leviathan interconnecting billions of devices. This has been possible by the separation of concern principle, for which the network functionalities are organized as a stack of layers, each providing some communication functionalities through specific network protocols. In this survey, we aim at highlighting the impossibility of adapting the classical Internet protocol stack to the Quantum Internet, due to the marvels of quantum mechanics. Indeed, the design of the Quantum Internet requires a major paradigm shift of the whole protocol stack for harnessing the peculiarities of quantum entanglement and quantum information. In this context, we first overview the relevant literature about Quantum Internet protocol stack. Then, stemming from this, we sheds the light on the open problems and required efforts toward the design of an effective and complete Quantum Internet protocol stack. To the best of authors’ knowledge, a survey of this type is the first of its own. What emerges from this analysis is that the Quantum Internet, though still in its infancy, is a disruptive technology whose design requires an inter-disciplinary effort at the border between quantum physics, computer and telecommunications engineering.},
	language = {en},
	urldate = {2023-01-11},
	journal = {Computer Networks},
	author = {Illiano, Jessica and Caleffi, Marcello and Manzalini, Antonio and Cacciapuoti, Angela Sara},
	month = aug,
	year = {2022},
	pages = {109092},
}

@misc{caleffi_distributed_2022,
	title = {Distributed {Quantum} {Computing}: a {Survey}},
	shorttitle = {Distributed {Quantum} {Computing}},
	url = {http://arxiv.org/abs/2212.10609},
	abstract = {Nowadays, quantum computing has reached the engineering phase, with fully-functional quantum processors integrating hundred of noisy qubits available. Yet – to fully unveil the potential of quantum computing out of the labs and into business reality – the challenge ahead is to substantially scale the qubit number, reaching orders of magnitude exceeding the thousands (if not millions) of noise-free qubits. To this aim, there exists a broad consensus among both academic and industry communities about considering the distributed computing paradigm as the key solution for achieving such a scaling, by envision multiple moderate-to-smallscale quantum processors communicating and cooperating to execute computational tasks exceeding the computational resources available within a single processing device. The aim of this survey is to provide the reader with an overview about the main challenges and open problems arising with distributed quantum computing, and with an easy access and guide towards the relevant literature and the prominent results from a computer/communications engineering perspective.},
	language = {en},
	urldate = {2023-01-11},
	publisher = {arXiv},
	author = {Caleffi, Marcello and Amoretti, Michele and Ferrari, Davide and Cuomo, Daniele and Illiano, Jessica and Manzalini, Antonio and Cacciapuoti, Angela Sara},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10609 [quant-ph]},
	keywords = {Computer Science - Networking and Internet Architecture, Quantum Physics},
}

@article{gyongyosi_survey_2019,
	title = {A {Survey} on quantum computing technology},
	volume = {31},
	issn = {15740137},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1574013718301709},
	doi = {10.1016/j.cosrev.2018.11.002},
	abstract = {The power of quantum computing technologies is based on the fundamentals of quantum mechanics, such as quantum superposition, quantum entanglement, or the no-cloning theorem. Since these phenomena have no classical analogue, similar results cannot be achieved within the framework of traditional computing. The experimental insights of quantum computing technologies have already been demonstrated, and several studies are in progress. Here we review the most recent results of quantum computation technology and address the open problems of the field.},
	language = {en},
	urldate = {2023-01-11},
	journal = {Computer Science Review},
	author = {Gyongyosi, Laszlo and Imre, Sandor},
	month = feb,
	year = {2019},
	pages = {51--71},
}

@article{magann_digital_2021,
	title = {Digital quantum simulation of molecular dynamics and control},
	volume = {3},
	issn = {2643-1564},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.3.023165},
	doi = {10.1103/PhysRevResearch.3.023165},
	language = {en},
	number = {2},
	urldate = {2023-01-11},
	journal = {Physical Review Research},
	author = {Magann, Alicia B. and Grace, Matthew D. and Rabitz, Herschel A. and Sarovar, Mohan},
	month = jun,
	year = {2021},
	pages = {023165},
}

@article{gobato_calculations_2017,
	title = {Calculations {Using} {Quantum} {Chemistry} for {Inorganic} {Molecule} {Simulation} {BeLi}\&lt;sub\&gt;2\&lt;/sub\&gt;{SeSi}},
	volume = {5},
	issn = {2376-8045},
	url = {http://www.sciencepublishinggroup.com/journal/paperinfo?journalid=223&doi=10.11648/j.sjac.20170505.13},
	doi = {10.11648/j.sjac.20170505.13},
	abstract = {Inorganic crystals have been used in the most diverse electronic systems since the nineteenth century, which apply to the wide variety of technological applications, which are the quartz crystals are the most used. Elements such as beryllium, lithium, silicon and selenium are widely used. The difficulty of finding such crystals from the combination of elements in nature or synthesized, suggest an advanced study of the same. In this sense, these elements were chosen because of the physical-chemical properties of each one, to simulate a seed molecule whose arrangement would be formed by the combination of these, aiming at the future development of a crystal to be used technologically. A study using computer programs with ab initio method was applied and the quantum chemistry was utilized through Molecular Mechanics, HartreeFock, Møller-Plesset and Density Functional Theory, on several bases. The main focus was to obtain a stable molecular structure acceptable to quantum chemistry. As a result of the likely molecular structure of the arrangement of a crystal was obtained, beyond the dipole moment, thermal energy, heat of vaporization and entropy of the molecule. The simulated molecule has a cationic molecular structure, in the atoms Selenium and Silicon. As a consequence, it has a strong electric dipole moment. Due to its geometry, it presents a probable formation structure of a crystal with a tetrahedral and hexahedral crystal structure.},
	language = {en},
	number = {5},
	urldate = {2023-01-11},
	journal = {Science Journal of Analytical Chemistry},
	author = {Gobato, Ricardo},
	year = {2017},
	pages = {76},
}

@article{batra_quantum_2021,
	title = {Quantum {Machine} {Learning} {Algorithms} for {Drug} {Discovery} {Applications}},
	volume = {61},
	issn = {1549-9596, 1549-960X},
	url = {https://pubs.acs.org/doi/10.1021/acs.jcim.1c00166},
	doi = {10.1021/acs.jcim.1c00166},
	abstract = {The growing quantity of public and private data sets focused on small molecules screened against biological targets or whole organisms provides a wealth of drug discovery relevant data. This is matched by the availability of machine learning algorithms such as Support Vector Machines (SVM) and Deep Neural Networks (DNN) that are computationally expensive to perform on very large data sets with thousands of molecular descriptors. Quantum computer (QC) algorithms have been proposed to oﬀer an approach to accelerate quantum machine learning over classical computer (CC) algorithms, however with signiﬁcant limitations. In the case of cheminformatics, which is widely used in drug discovery, one of the challenges to overcome is the need for compression of large numbers of molecular descriptors for use on a QC. Here, we show how to achieve compression with data sets using hundreds of molecules (SARS-CoV-2) to hundreds of thousands of molecules (whole cell screening data sets for plague and M. tuberculosis) with SVM and the data reuploading classiﬁer (a DNN equivalent algorithm) on a QC benchmarked against CC and hybrid approaches. This study illustrates the steps needed in order to be “quantum computer ready” in order to apply quantum computing to drug discovery and to provide the foundation on which to build this ﬁeld.},
	language = {en},
	number = {6},
	urldate = {2023-01-11},
	journal = {Journal of Chemical Information and Modeling},
	author = {Batra, Kushal and Zorn, Kimberley M. and Foil, Daniel H. and Minerali, Eni and Gawriljuk, Victor O. and Lane, Thomas R. and Ekins, Sean},
	month = jun,
	year = {2021},
	pages = {2641--2647},
}

@article{cao_potential_2018,
	title = {Potential of quantum computing for drug discovery},
	volume = {62},
	issn = {0018-8646, 0018-8646},
	url = {https://ieeexplore.ieee.org/document/8585034/},
	doi = {10.1147/JRD.2018.2888987},
	language = {en},
	number = {6},
	urldate = {2023-01-11},
	journal = {IBM Journal of Research and Development},
	author = {Cao, Y. and Romero, J. and Aspuru-Guzik, A.},
	month = nov,
	year = {2018},
	pages = {6:1--6:20},
}

@article{zinner_quantum_2021,
	title = {Quantum computing's potential for drug discovery: {Early} stage industry dynamics},
	volume = {26},
	issn = {13596446},
	shorttitle = {Quantum computing's potential for drug discovery},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1359644621002750},
	doi = {10.1016/j.drudis.2021.06.003},
	language = {en},
	number = {7},
	urldate = {2023-01-11},
	journal = {Drug Discovery Today},
	author = {Zinner, Maximillian and Dahlhausen, Florian and Boehme, Philip and Ehlers, Jan and Bieske, Linn and Fehring, Leonard},
	month = jul,
	year = {2021},
	pages = {1680--1688},
}

@inproceedings{de_stefano_towards_2022,
	address = {Singapore Singapore},
	title = {Towards {Quantum}-algorithms-as-a-service},
	isbn = {978-1-4503-9458-1},
	url = {https://dl.acm.org/doi/10.1145/3549036.3562056},
	doi = {10.1145/3549036.3562056},
	abstract = {Quantum computing is an emerging field of high interest. Many companies have started to work on developing more powerful and stable quantum computers. However, developers still struggle to master the art of programming with a quantum computer. One of the major challenges faced is the integration of quantum parts of a system with the classical one. This paper proposes a novel development model called Quantum-Algorithms-as-a-Service (QAaaS). This new model aims to allow developers to abstract the quantum components away from the design of the software they are building. The model leverages Software-as-a-Service and Functionas-a-Service to support multiple quantum cloud providers and run their algorithms regardless of the underlying hardware.},
	language = {en},
	urldate = {2023-01-11},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Quantum} {Programming} for {Software} {Engineering}},
	publisher = {ACM},
	author = {De Stefano, Manuel and Di Nucci, Dario and Palomba, Fabio and Taibi, Davide and De Lucia, Andrea},
	month = nov,
	year = {2022},
	pages = {7--10},
}

@article{el_azzaoui_blockchain-based_2022,
	title = {Blockchain-based delegated {Quantum} {Cloud} architecture for medical big data security},
	volume = {198},
	issn = {10848045},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1084804521002952},
	doi = {10.1016/j.jnca.2021.103304},
	abstract = {Smart Healthcare systems compromise complex computations such as visualization of molecules, analysis of DNA, and therapy determination. These are considered to be complex problems that today’s supercomputers are still facing. On the other hand, Quantum computing promises fast, efficient, and scalable computing resources that are sufficient to compute large and complex operations in exponential time. It is a fact that Quantum computing will adequately innovate the computation perspective. However, it is not a feasible solution yet as it is likely to be rare and highly expensive to produce. This paper presents a Quantum Cloud-as-a-service for an efficient, scalable, and secure solution for complex Smart Healthcare computations. Our novelty resides in the usage of Quantum Terminal Machines (QTM) and Blockchain technology to enhance the feasibility and security of the proposed architecture. Experimental results prove the feasibility of the architecture and the absolute se­ curity of the implemented Q-OTP encryption.},
	language = {en},
	urldate = {2023-01-11},
	journal = {Journal of Network and Computer Applications},
	author = {EL Azzaoui, Abir and Sharma, Pradip Kumar and Park, Jong Hyuk},
	month = feb,
	year = {2022},
	pages = {103304},
}

@article{planqk,
	title = {{PlanQK} — {Quantum} {Computing} {Meets} {Artificial} {Intelligence}: {How} to make an ambitious idea reality},
	volume = {4},
	issn = {2569-1996},
	shorttitle = {{PlanQK} — {Quantum} {Computing} {Meets} {Artificial} {Intelligence}},
	url = {http://link.springer.com/10.1007/s42354-020-0257-9},
	doi = {10.1007/s42354-020-0257-9},
	language = {en},
	number = {2},
	urldate = {2023-01-11},
	journal = {Digitale Welt},
	author = {Linnhoff-Popien, Claudia},
	month = apr,
	year = {2020},
	pages = {28--35},
}

@article{kumar_quantum_2022,
	title = {Quantum true random number generation on {IBM}’s cloud platform},
	volume = {34},
	issn = {13191578},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1319157822000283},
	doi = {10.1016/j.jksuci.2022.01.015},
	abstract = {Random numbers are the lifeline of any cryptographic operation in modern computing. Quantum True Random Number Generator (QTRNG) can yield real random data to replace random-looking periodic sequences. Here, we present the fastest QTRNG on factual Quantum Computational Devices (QCD) locally on our device to save time. To construct a random number generator, the IBM Q Experience called Qiskit is used. The ease of our source Hadamard gate pooled with its veriﬁably sole randomness is imperative for attaining quantum random number generators at no cost. The inner matrix product of the square of the Hadamard gate is proved to be identiﬁed in the quantum lab. Applying the H gate twice on the quantum register cancelled out the H gate pair. It acted as a single Hadamard gate that was practically visualised using the IMB Q experience. The 24-qubit random number is experimented with and investigated in this work. The min-entropy of such generated random numbers is 0.0007244 with the worst-case entropy value of 0.999507. Besides, subsequent veriﬁcation of genuine randomness was conﬁrmed by steering restart experimentation, \& the statistical properties of TRNG were evaluated over autocorrelation study, National Institute of Standards and Technology (NIST) SP 800-90B \& 800-22 tests.},
	language = {en},
	number = {8},
	urldate = {2023-01-11},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Kumar, Vaishnavi and Rayappan, John Bosco Balaguru and Amirtharajan, Rengarajan and Praveenkumar, Padmapriya},
	month = sep,
	year = {2022},
	pages = {6453--6465},
}

@article{garcia-perez_ibm_2020,
	title = {{IBM} {Q} {Experience} as a versatile experimental testbed for simulating open quantum systems},
	volume = {6},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-019-0235-y},
	doi = {10.1038/s41534-019-0235-y},
	abstract = {Abstract
            The advent of noisy intermediate-scale quantum (NISQ) technology is changing rapidly the landscape and modality of research in quantum physics. NISQ devices, such as the IBM Q Experience, have very recently proven their capability as experimental platforms accessible to everyone around the globe. Until now, IBM Q Experience processors have mostly been used for quantum computation and simulation of closed systems. Here, we show that these devices are also able to implement a great variety of paradigmatic open quantum systems models, hence providing a robust and flexible testbed for open quantum systems theory. During the last decade an increasing number of experiments have successfully tackled the task of simulating open quantum systems in different platforms, from linear optics to trapped ions, from nuclear magnetic resonance (NMR) to cavity quantum electrodynamics. Generally, each individual experiment demonstrates a specific open quantum system model, or at most a specific class. Our main result is to prove the great versatility of the IBM Q Experience processors. Indeed, we experimentally implement one and two-qubit open quantum systems, both unital and non-unital dynamics, Markovian and non-Markovian evolutions. Moreover, we realise proof-of-principle reservoir engineering for entangled state generation, demonstrate collisional models, and verify revivals of quantum channel capacity and extractable work, caused by memory effects. All these results are obtained using IBM Q Experience processors publicly available and remotely accessible online.},
	language = {en},
	number = {1},
	urldate = {2023-01-11},
	journal = {npj Quantum Information},
	author = {García-Pérez, Guillermo and Rossi, Matteo A. C. and Maniscalco, Sabrina},
	month = jan,
	year = {2020},
	pages = {1},
}

@inproceedings{ibmq-experience,
	address = {Canada},
	title = {Introduction to the {IBM} {Q} experience and {Quantum} {Computing}},
	doi = {10.5555/3291291.3291362},
	language = {en},
	booktitle = {Proceedings of the 28th {Annual} {International} {Conference} on {Computer} {Science} and {Software} {Engineering}},
	publisher = {IBM Corp},
	author = {Bozzo-Rey, Mehdi and Loredo, Robert},
	month = oct,
	year = {2018},
	pages = {410--412},
}

@inproceedings{tang_cutqc_2021,
	address = {Virtual USA},
	title = {{CutQC}: using small {Quantum} computers for large {Quantum} circuit evaluations},
	isbn = {978-1-4503-8317-2},
	shorttitle = {{CutQC}},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446758},
	doi = {10.1145/3445814.3446758},
	abstract = {Quantum computing (QC) is a new paradigm offering the potential of exponential speedups over classical computing for certain computational problems. Each additional qubit doubles the size of the computational state space available to a QC algorithm. This exponential scaling underlies QC’s power, but today’s Noisy Intermediate-Scale Quantum (NISQ) devices face significant engineering challenges in scalability. The set of quantum circuits that can be reliably run on NISQ devices is limited by their noisy operations and low qubit counts.},
	language = {en},
	urldate = {2023-01-10},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Tang, Wei and Tomesh, Teague and Suchara, Martin and Larson, Jeffrey and Martonosi, Margaret},
	month = apr,
	year = {2021},
	pages = {473--486},
}

@article{eddins_doubling_2022,
	title = {Doubling the {Size} of {Quantum} {Simulators} by {Entanglement} {Forging}},
	volume = {3},
	issn = {2691-3399},
	url = {https://link.aps.org/doi/10.1103/PRXQuantum.3.010309},
	doi = {10.1103/PRXQuantum.3.010309},
	language = {en},
	number = {1},
	urldate = {2023-01-10},
	journal = {PRX Quantum},
	author = {Eddins, Andrew and Motta, Mario and Gujarati, Tanvi P. and Bravyi, Sergey and Mezzacapo, Antonio and Hadfield, Charles and Sheldon, Sarah},
	month = jan,
	year = {2022},
	pages = {010309},
}

@article{tuckman_developmental_1965,
	title = {Developmental sequence in small groups.},
	volume = {63},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0022100},
	doi = {10.1037/h0022100},
	language = {en},
	number = {6},
	urldate = {2022-12-09},
	journal = {Psychological Bulletin},
	author = {Tuckman, Bruce W.},
	month = jun,
	year = {1965},
	pages = {384--399},
}

@article{hattie_power_2007,
	title = {The {Power} of {Feedback}},
	volume = {77},
	issn = {0034-6543, 1935-1046},
	url = {http://journals.sagepub.com/doi/10.3102/003465430298487},
	doi = {10.3102/003465430298487},
	abstract = {Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms.},
	language = {en},
	number = {1},
	urldate = {2022-12-09},
	journal = {Review of Educational Research},
	author = {Hattie, John and Timperley, Helen},
	month = mar,
	year = {2007},
	pages = {81--112},
}

@article{tuckman_stages_1977,
	title = {Stages of {Small}-{Group} {Development} {Revisited}},
	volume = {2},
	issn = {0364-1082},
	url = {http://journals.sagepub.com/doi/10.1177/105960117700200404},
	doi = {10.1177/105960117700200404},
	abstract = {The purpose of this review was to examine published research on small-group development done in the last ten years that would constitute an empirical test of Tuckman's (1965) hypothesis that groups go through the stages of "forming," "storming," "norming," and "performing." Of the twenty-two studies reviewed, only one set out to directly test this hypothesis, although many of the others could be related to it. Following a review of these studies, a fifth stage, "adjourning," was added to the hypothesis, and more empirical work was recommended.},
	language = {en},
	number = {4},
	urldate = {2022-12-09},
	journal = {Group \& Organization Studies},
	author = {Tuckman, Bruce W. and Jensen, Mary Ann C.},
	month = dec,
	year = {1977},
	pages = {419--427},
}

@misc{qcloud-ionq,
	title = {{IonQ} {Cloud} {Service}},
	url = {https://ionq.com/},
	author = {IonQ},
	year = {2022},
}

@misc{qcloud-xanadu,
	title = {Xanadu {Cloud} {Service}},
	url = {https://www.xanadu.ai/},
	author = {Xanadu},
	year = {2022},
}

@article{mills_application-motivated_2021,
	title = {Application-{Motivated}, {Holistic} {Benchmarking} of a {Full} {Quantum} {Computing} {Stack}},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2006.01273},
	doi = {10.22331/q-2021-03-22-415},
	abstract = {Quantum computing systems need to be benchmarked in terms of practical tasks they would be expected to do. Here, we propose 3 "application-motivated" circuit classes for benchmarking: deep (relevant for state preparation in the variational quantum eigensolver algorithm), shallow (inspired by IQP-type circuits that might be useful for near-term quantum machine learning), and square (inspired by the quantum volume benchmark). We quantify the performance of a quantum computing system in running circuits from these classes using several figures of merit, all of which require exponential classical computing resources and a polynomial number of classical samples (bitstrings) from the system. We study how performance varies with the compilation strategy used and the device on which the circuit is run. Using systems made available by IBM Quantum, we examine their performance, showing that noise-aware compilation strategies may be beneficial, and that device connectivity and noise levels play a crucial role in the performance of the system according to our benchmarks.},
	language = {en},
	urldate = {2022-12-08},
	journal = {Quantum},
	author = {Mills, Daniel and Sivarajah, Seyon and Scholten, Travis L. and Duncan, Ross},
	month = mar,
	year = {2021},
	note = {arXiv:2006.01273 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {415},
}

@misc{qcloud-quantinuum,
	title = {Quantinuum {Cloud} {Service}},
	url = {https://www.quantinuum.com/},
	author = {Quantinuum},
	year = {2022},
}

@article{zhu_quantum_2021_security,
	title = {Quantum {Fully} {Homomorphic} {Encryption} {Scheme} for {Cloud} {Privacy} {Data} {Based} on {Quantum} {Circuit}},
	issn = {15729575},
	doi = {10.1007/s10773-021-04879-w},
	abstract = {How to make cloud computing ensure the privacy of data while ensuring its availability in the process of computing data is a big problem faced by cloud computing. Under the background of quantum information processing, a quantum fully homomorphic encryption (QFHE) scheme is proposed. QFHE based on universal quantum circuit (UQC) allows arbitrary quantum transformation. QFHE allows the operation of encrypted data without decryption, which greatly ensures the security of data. In this scheme, the encryption key is constructed by GHZ-like state. Both the encryption key and the decryption key use quantum one-time pad (QOTP), and the decryption key is different from the encryption key. In this scheme, the evaluation algorithm is independent of the encryption key, which is a powerful tool to protect data privacy in the cloud environment. In addition, security analysis and complexity analysis also show that the scheme is suitable for privacy data processing.},
	journal = {International Journal of Theoretical Physics},
	author = {Zhu, Hongfeng and Wang, Chaonan and Wang, Xueying},
	year = {2021},
	note = {Publisher: Springer},
	keywords = {GHZ-like state, Quantum fully homomorphic encryption, Quantum one-time pad, Universal quantum circuit},
}

@inproceedings{javadiabhari_scaffcc_2014,
	address = {Cagliari Italy},
	title = {{ScaffCC}: a framework for compilation and analysis of quantum computing programs},
	isbn = {978-1-4503-2870-8},
	shorttitle = {{ScaffCC}},
	url = {https://dl.acm.org/doi/10.1145/2597917.2597939},
	doi = {10.1145/2597917.2597939},
	abstract = {Quantum computing is a promising technology for highperformance computation, but requires mature toolﬂows that can map large-scale quantum programs onto targeted hardware. In this paper, we present a scalable compiler for largescale quantum applications, and show the opportunities for reducing compilation and analysis time, as well as output code size. We discuss the similarities and di↵erences between compiling for a quantum computer as opposed to a classical computer, and present a state-of-the-art approach for compilation of classical circuits into quantum circuits. Our work also highlights the importance of high-level quantum compilation for logical circuit translation, quantitative analysis of algorithms, and optimization of circuit lengths.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 11th {ACM} {Conference} on {Computing} {Frontiers}},
	publisher = {ACM},
	author = {JavadiAbhari, Ali and Patil, Shruti and Kudrow, Daniel and Heckey, Jeff and Lvov, Alexey and Chong, Frederic T. and Martonosi, Margaret},
	month = may,
	year = {2014},
	pages = {1--10},
}

@article{li_quantum_2021_FL,
	title = {Quantum federated learning through blind quantum computing},
	volume = {64},
	issn = {18691927},
	url = {http://arxiv.org/abs/2103.08403%0Ahttp://dx.doi.org/10.1007/s11433-021-1753-3},
	doi = {10.1007/s11433-021-1753-3},
	abstract = {Private distributed learning studies the problem of how multiple distributed entities collaboratively train a shared deep network with their private data unrevealed. With the security provided by the protocols of blind quantum computation, the cooperation between quantum physics and machine learning may lead to unparalleled prospect for solving private distributed learning tasks. In this paper, we introduce a quantum protocol for distributed learning that is able to utilize the computational power of the remote quantum servers while keeping the private data safe. For concreteness, we first introduce a protocol for private single-party delegated training of variational quantum classifiers based on blind quantum computing and then extend this protocol to multiparty private distributed learning incorporated with differential privacy. We carry out extensive numerical simulations with different real-life datasets and encoding strategies to benchmark the effectiveness of our protocol. We find that our protocol is robust to experimental imperfections and is secure under the gradient attack after the incorporation of differential privacy. Our results show the potential for handling computationally expensive distributed learning tasks with privacy guarantees, thus providing a valuable guide for exploring quantum advantages from the security perspective in the field of machine learning with real-life applications.},
	number = {10},
	author = {Li, Weikang and Lu, Sirui and Deng, Dong-Ling},
	year = {2021},
	note = {arXiv: 2103.08403
ISBN: 1143302117},
}

@misc{fang_quantum_2022,
	title = {Quantum {NETwork}: from theory to practice},
	shorttitle = {Quantum {NETwork}},
	url = {http://arxiv.org/abs/2212.01226},
	abstract = {The quantum internet is envisioned as the ultimate stage of the quantum revolution, which surpasses its classical counterpart in various aspects, such as the efficiency of data transmission, the security of network services, and the capability of information processing. Given its disruptive impact on the national security and the digital economy, a global race to build scalable quantum networks has already begun. With the joint effort of national governments, industrial participants and research institutes, the development of quantum networks has advanced rapidly in recent years, bringing the first primitive quantum networks within reach. In this work, we aim to provide an up-to-date review of the field of quantum networks from both theoretical and experimental perspectives, contributing to a better understanding of the building blocks required for the establishment of a global quantum internet. We also introduce a newly developed quantum network toolkit to facilitate the exploration and evaluation of innovative ideas. Particularly, it provides dual quantum computing engines, supporting simulations in both the quantum circuit and measurement-based models. It also includes a compilation scheme for mapping quantum network protocols onto quantum circuits, enabling their emulations on real-world quantum hardware devices. We showcase the power of this toolkit with several featured demonstrations, including a simulation of the Micius quantum satellite experiment, a testing of a four-layer quantum network architecture with resource management, and a quantum emulation of the CHSH game. We hope this work can give a better understanding of the state-of-the-art development of quantum networks and provide the necessary tools to make further contributions along the way.},
	language = {en},
	urldate = {2022-12-07},
	publisher = {arXiv},
	author = {Fang, Kun and Zhao, Jingtian and Li, Xiufan and Li, Yifei and Duan, Runyao},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01226 [quant-ph]},
	keywords = {Computer Science - Networking and Internet Architecture, Quantum Physics},
}

@article{qxmd-gan,
	title = {Quantum maximum mean discrepancy {GAN}},
	volume = {454},
	issn = {18728286},
	url = {https://doi.org/10.1016/j.neucom.2021.04.091},
	doi = {10.1016/j.neucom.2021.04.091},
	abstract = {Generative adversarial network (GAN) has shown profound power in machine learning. It inspires many researchers from other fields to create powerful tools for various tasks, including quantum state preparation, quantum circuit translation, and so on. It is known as classical techniques cannot efficiently simulate the quantum system, and the existing works haven't investigated the quantum version of maximum mean discrepancy as the metric in learning models and applied it to quantum data. In this paper, we propose a metric named quantum maximum mean discrepancy (qMMD), which can be used to measure the distance between quantum data in Hilbert space. Based on the qMMD, we then design a quantum generative adversarial model, named qMMD-GAN, under the hybrid quantum–classical methods. We also provide the construction of qMMD-GAN that can be easily implemented on a quantum device. We demonstrate the power of our qMMD-GAN by applying it to a crucial real-world application that is generating an unknown quantum state. Our numerical experiments show that qMMD-GAN has a competitive performance compared to existing results. We believe that the hybrid-based models will not only be applied to physics research but provide a new direction for improving classical data processing tasks.},
	journal = {Neurocomputing},
	author = {Huang, Yiming and Lei, Hang and Li, Xiaoyu and Yang, Guowu},
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {Generative adversarial network, Maximum mean discrepancy, Quantum computing},
	pages = {88--100},
}

@article{ohkura_simultaneous_2022,
	title = {Simultaneous {Execution} of {Quantum} {Circuits} on {Current} and {Near}-{Future} {NISQ} {Systems}},
	volume = {3},
	issn = {2689-1808},
	url = {https://ieeexplore.ieee.org/document/9749894/},
	doi = {10.1109/TQE.2022.3164716},
	abstract = {In the noisy intermediate-scale quantum (NISQ) era, the idea of quantum multiprogramming, running multiple quantum circuits (QCs) simultaneously on the same hardware, helps to improve the throughput of quantum computation. However, the crosstalk, unwanted interference between qubits on NISQ processors, may cause performance degradation when using multiprogramming. To address this challenge, we introduce palloq (parallel allocation of QCs), a novel compilation protocol. Palloq improves the performance of quantum multiprogramming on NISQ processors, while paying attention to 1) the combination of QCs chosen for parallel execution and 2) the assignment of program qubit variables to physical qubits, to reduce unwanted interference among the active set of QCs. We also propose a software-based crosstalk detection protocol using a new combination of randomized benchmarking methods. Our method successfully characterizes the suitability of hardware for multiprogramming with relatively low detection costs. We found a tradeoff between the success rate and execution time of the multiprogramming. Our results will be of value when device throughput becomes a significant bottleneck. Until service providers have enough quantum processors available to more than meet demand, this approach will be attractive to the service providers and users who want to optimize job management and throughput of the processor.},
	language = {en},
	urldate = {2022-12-06},
	journal = {IEEE Transactions on Quantum Engineering},
	author = {Ohkura, Yasuhiro and Satoh, Takahiko and Van Meter, Rodney},
	year = {2022},
	pages = {1--10},
}

@article{ma_qenclave_2022,
	title = {{QEnclave} - {A} practical solution for secure quantum cloud computing},
	volume = {8},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-022-00612-5},
	doi = {10.1038/s41534-022-00612-5},
	abstract = {Abstract
            We introduce a secure hardware device named a QEnclave that can secure the remote execution of quantum operations while only using classical controls. This device extends to quantum computing from the classical concept of a secure enclave that isolates a computation from its environment to provide privacy and tamper-resistance. Remarkably, our QEnclave only performs single qubit rotations but can nevertheless be used to secure an arbitrary quantum computation even if the qubit source is controlled by an adversary. More precisely, by attaching a QEnclave to a quantum computer, a remote client controlling the QEnclave can securely delegate its computation to the server solely using classical communication. We investigate the security of our QEnclave by modeling it as an ideal functionality named remote state rotation (RSR). We show that this resource, similar to the previously introduced functionality of remote state preparation, allows blind delegated quantum computing with perfect security. Our proof under the Abstract Cryptography framework shows the construction of remote state preparation from remote state rotation while preserving security. An immediate consequence is the weakening of the requirements for blind delegated computation. While previous delegated protocols relied on a client that can either generate or measure quantum states, we show that this same functionality can be achieved with a client that only transforms quantum states without generating or measuring them.},
	language = {en},
	number = {1},
	urldate = {2022-12-04},
	journal = {npj Quantum Information},
	author = {Ma, Yao and Kashefi, Elham and Arapinis, Myrto and Chakraborty, Kaushik and Kaplan, Marc},
	month = nov,
	year = {2022},
	pages = {128},
}

@misc{nguyen_software_2022,
	title = {Software for {Massively} {Parallel} {Quantum} {Computing}},
	url = {http://arxiv.org/abs/2211.13355},
	abstract = {Quantum computing has the potential to oﬀer substantial computational advantages over conventional computing. Recent advances in quantum computing hardware and algorithms have enabled a class of classically parallel quantum workloads, whereby individual quantum circuits can execute independently on many quantum processing units. Here, we present the full-stack software framework developed at Quantum Brilliance to enable multi-modal parallelism for hybrid quantum workloads. Our software provides the capability to distribute quantum workloads across multiple quantum accelerators hosted by nodes of a locally-networked cluster, via the industry-standard MPI (Message Passing Interface) protocol, or to distribute workloads across a large number of cloud-hosted quantum accelerators.},
	language = {en},
	urldate = {2022-12-02},
	publisher = {arXiv},
	author = {Nguyen, Thien and Arya, Daanish and Doherty, Marcus and Herrmann, Nils and Kuhlmann, Johannes and Preis, Florian and Scott, Pat and Yin, Simon},
	month = nov,
	year = {2022},
	note = {arXiv:2211.13355 [quant-ph]},
	keywords = {Quantum Physics},
}

@inproceedings{salama_iotnetsim_2019,
	address = {Auckland New Zealand},
	title = {{IoTNetSim}: {A} {Modelling} and {Simulation} {Platform} for {End}-to-{End} {IoT} {Services} and {Networking}},
	isbn = {978-1-4503-6894-0},
	shorttitle = {{IoTNetSim}},
	url = {https://dl.acm.org/doi/10.1145/3344341.3368820},
	doi = {10.1145/3344341.3368820},
	abstract = {Internet-of-Things (IoT) systems are becoming increasingly complex, heterogeneous and pervasive, integrating a variety of physical devices and virtual services that are spread across architecture layers (cloud, fog, edge) using different connection types. As such, research and design of such systems have proven to be challenging. Despite the influx in IoT research and the significant benefits of simulation-based approaches in supporting research, there is a general lack of appropriate modelling and simulation platforms to create a detailed representation of end-to-end IoT services, i.e. from the underlying IoT nodes to the application layer in the cloud along with the underlying networking infrastructure. To aid researchers and practitioners in overcoming these challenges, we propose IoTNetSim, a novel self-contained extendable platform for modelling and simulation of end-to-end IoT services. The platform supports modelling heterogeneous IoT nodes (sensors, actuators, gateways, etc.) with their fine-grained details (mobility, energy profile, etc.), as well as different models of application logic and network connectivity. The proposed work is distinct from the current literature, being an all-in-one tool for end-to-end IoT services with a multi-layered architecture that allows modelling IoT systems with different structures. We experimentally validate and evaluate our IoTNetSim implementation using two very large-scale real-world cases from the natural environment and disaster monitoring IoT domains.},
	language = {en},
	urldate = {2022-11-23},
	booktitle = {Proceedings of the 12th {IEEE}/{ACM} {International} {Conference} on {Utility} and {Cloud} {Computing}},
	publisher = {ACM},
	author = {Salama, Maria and Elkhatib, Yehia and Blair, Gordon},
	month = dec,
	year = {2019},
	pages = {251--261},
}

@article{curran_perfectionism_2019,
	title = {Perfectionism is increasing over time: {A} meta-analysis of birth cohort differences from 1989 to 2016.},
	volume = {145},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Perfectionism is increasing over time},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000138},
	doi = {10.1037/bul0000138},
	abstract = {From the 1980s onward, neoliberal governance in the United States, Canada, and the United Kingdom has emphasized competitive individualism and people have seemingly responded, in kind, by agitating to perfect themselves and their lifestyles. In this study, the authors examine whether cultural changes have coincided with an increase in multidimensional perfectionism in college students over the last 27 years. Their analyses are based on 164 samples and 41,641 American, Canadian, and British college students, who completed the Multidimensional Perfectionism Scale (Hewitt \& Flett, 1991) between 1989 and 2016 (70.92\% female, Mage ϭ 20.66). Cross-temporal meta-analysis revealed that levels of selforiented perfectionism, socially prescribed perfectionism, and other-oriented perfectionism have linearly increased. These trends remained when controlling for gender and between-country differences in perfectionism scores. Overall, in order of magnitude of the observed increase, the findings indicate that recent generations of young people perceive that others are more demanding of them, are more demanding of others, and are more demanding of themselves.},
	language = {en},
	number = {4},
	urldate = {2022-11-18},
	journal = {Psychological Bulletin},
	author = {Curran, Thomas and Hill, Andrew P.},
	month = apr,
	year = {2019},
	pages = {410--429},
}

@article{hewitt_perfectionism_nodate,
	title = {Perfectionism in the {Self} and {Social} {Contexts}: {Conceptualization}, {Assessment}, and {Association} {With} {Psychopathology}},
	language = {en},
	author = {Hewitt, Paul L and Flett, Gordon L},
	pages = {15},
}

@article{blume-kohout_volumetric_2020,
	title = {A volumetric framework for quantum computer benchmarks},
	volume = {4},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/1904.05546},
	doi = {10.22331/q-2020-11-15-362},
	abstract = {We propose a very large family of benchmarks for probing the performance of quantum computers. We call them volumetric benchmarks (VBs) because they generalize IBM's benchmark for measuring quantum volume {\textbackslash}cite\{Cross18\}. The quantum volume benchmark defines a family of square circuits whose depth \$d\$ and width \$w\$ are the same. A volumetric benchmark defines a family of rectangular quantum circuits, for which \$d\$ and \$w\$ are uncoupled to allow the study of time/space performance trade-offs. Each VB defines a mapping from circuit shapes -- \$(w,d)\$ pairs -- to test suites \${\textbackslash}mathcal\{C\}(w,d)\$. A test suite is an ensemble of test circuits that share a common structure. The test suite \${\textbackslash}mathcal\{C\}\$ for a given circuit shape may be a single circuit \$C\$, a specific list of circuits \${\textbackslash}\{C\_1{\textbackslash}ldots C\_N{\textbackslash}\}\$ that must all be run, or a large set of possible circuits equipped with a distribution \$Pr(C)\$. The circuits in a given VB share a structure, which is limited only by designers' creativity. We list some known benchmarks, and other circuit families, that fit into the VB framework: several families of random circuits, periodic circuits, and algorithm-inspired circuits. The last ingredient defining a benchmark is a success criterion that defines when a processor is judged to have "passed" a given test circuit. We discuss several options. Benchmark data can be analyzed in many ways to extract many properties, but we propose a simple, universal graphical summary of results that illustrates the Pareto frontier of the \$d\$ vs \$w\$ trade-off for the processor being benchmarked. [1] A. Cross, et al., Phys. Rev. A, 100, 032328, September 2019.},
	language = {en},
	urldate = {2022-11-17},
	journal = {Quantum},
	author = {Blume-Kohout, Robin and Young, Kevin C.},
	month = nov,
	year = {2020},
	note = {arXiv:1904.05546 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {362},
}

@article{qiu_distributed_2021,
	title = {Distributed and {Collective} {Deep} {Reinforcement} {Learning} for {Computation} {Offloading}: {A} {Practical} {Perspective}},
	volume = {32},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {Distributed and {Collective} {Deep} {Reinforcement} {Learning} for {Computation} {Offloading}},
	url = {https://ieeexplore.ieee.org/document/9288861/},
	doi = {10.1109/TPDS.2020.3042599},
	abstract = {Mobile edge computing (MEC) is a promising solution to support resource-constrained devices by ofﬂoading tasks to the edge servers. However, traditional approaches (e.g., linear programming and game-theory methods) for computation ofﬂoading mainly focus on the immediate performance, potentially leading to performance degradation in the long run. Recent breakthroughs regarding deep reinforcement learning (DRL) provide alternative methods, which focus on maximizing the cumulative reward. Nonetheless, there exists a large gap to deploy real DRL applications in MEC. This is because: 1) training a well-performed DRL agent typically requires data with large quantities and high diversity, and 2) DRL training is usually accompanied by huge costs caused by trial-and-error. To address this mismatch, we study the applications of DRL on the multi-user computation ofﬂoading problem from a more practical perspective. In particular, we propose a distributed and collective DRL algorithm called DC-DRL with several improvements: 1) a distributed and collective training scheme that assimilates knowledge from multiple MEC environments, which not only greatly increases data amount and diversity but also spreads the exploration costs, 2) an updating method called adaptive n-step learning, which can improve training efﬁciency without suffering from the high variance caused by distributed training, and 3) combining the advantages of deep neuroevolution and policy gradient to maximize the utilization of multiple environments and prevent the premature convergence. Lastly, evaluation results demonstrate the effectiveness of our proposed algorithm. Compared with the baselines, the exploration costs and ﬁnal system costs are reduced by at least 43 and 9.4 percent, respectively.},
	language = {en},
	number = {5},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Qiu, Xiaoyu and Zhang, Weikun and Chen, Wuhui and Zheng, Zibin},
	month = may,
	year = {2021},
	pages = {1085--1101},
}

@article{liu_deep_2022,
	title = {Deep {Reinforcement} {Learning} for {Load}-{Balancing} {Aware} {Network} {Control} in {IoT} {Edge} {Systems}},
	volume = {33},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/9555233/},
	doi = {10.1109/TPDS.2021.3116863},
	abstract = {Load balancing is directly associated with the overall performance of a parallel and distributed computing system. Although the relevant problems in communication and computation have been well studied in data center environments, few works have considered the issues in an Internet of Things (IoT) edge scenario. In fact, processing data in a load balancing way for the latter case is more challenging.},
	language = {en},
	number = {6},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Liu, Qingzhi and Xia, Tiancong and Cheng, Long and van Eijk, Merijn and Ozcelebi, Tanir and Mao, Ying},
	month = jun,
	year = {2022},
	pages = {1491--1502},
}

@article{kardani-moghaddam_adrl_2021,
	title = {{ADRL}: {A} {Hybrid} {Anomaly}-{Aware} {Deep} {Reinforcement} {Learning}-{Based} {Resource} {Scaling} in {Clouds}},
	volume = {32},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {{ADRL}},
	url = {https://ieeexplore.ieee.org/document/9203981/},
	doi = {10.1109/TPDS.2020.3025914},
	abstract = {The virtualization concept and elasticity feature of cloud computing enable users to request resources on-demand and in the pay-as-you-go model. However, the high ﬂexibility of the model makes the on-time resource scaling problem more complex. A variety of techniques such as threshold-based rules, time series analysis, or control theory are utilized to increase the efﬁciency of dynamic scaling of resources. However, the inherent dynamicity of cloud-hosted applications requires autonomic and adaptable systems that learn from the environment in real-time. Reinforcement Learning (RL) is a paradigm that requires some agents to monitor the surroundings and regularly perform an action based on the observed states. RL has a weakness to handle high dimensional state space problems. Deep-RL models are a recent breakthrough for modeling and learning in complex state space problems. In this article, we propose a Hybrid Anomaly-aware Deep Reinforcement Learning-based Resource Scaling (ADRL) for dynamic scaling of resources in the cloud. ADRL takes advantage of anomaly detection techniques to increase the stability of decision-makers by triggering actions in response to the identiﬁed anomalous states in the system. Two levels of global and local decision-makers are introduced to handle the required scaling actions. An extensive set of experiments for different types of anomaly problems shows that ADRL can signiﬁcantly improve the quality of service with less number of actions and increased stability of the system.},
	language = {en},
	number = {3},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Kardani-Moghaddam, Sara and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
	month = mar,
	year = {2021},
	pages = {514--526},
}

@article{yi_efficient_2020,
	title = {Efficient {Compute}-{Intensive} {Job} {Allocation} in {Data} {Centers} via {Deep} {Reinforcement} {Learning}},
	volume = {31},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/8964275/},
	doi = {10.1109/TPDS.2020.2968427},
	abstract = {Reducing the energy consumption of the servers in a data center via proper job allocation is desirable. Existing advanced job allocation algorithms, based on constrained optimization formulations capturing servers’ complex power consumption and thermal dynamics, often scale poorly with the data center size and optimization horizon. This article applies deep reinforcement learning to build an allocation algorithm for long-lasting and compute-intensive jobs that are increasingly seen among today’s computation demands. Speciﬁcally, a deep Q-network is trained to allocate jobs, aiming to maximize a cumulative reward over long horizons. The training is performed ofﬂine using a computational model based on long short-term memory networks that capture the servers’ power and thermal dynamics. This ofﬂine training approach avoids slow online convergence, low energy efﬁciency, and potential server overheating during the agent’s extensive state-action space exploration if it directly interacts with the physical data center in the usually adopted online learning scheme. At run time, the trained Q-network is forward-propagated with little computation to allocate jobs. Evaluation based on eight months’ physical state and job arrival records from a national supercomputing data center hosting 1,152 processors shows that our solution reduces computing power consumption by more than 10 percent and processor temperature by more than 4 C without sacriﬁcing job processing throughput.},
	language = {en},
	number = {6},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Yi, Deliang and Zhou, Xin and Wen, Yonggang and Tan, Rui},
	month = jun,
	year = {2020},
	pages = {1474--1485},
}

@article{fan_dras_2022,
	title = {{DRAS}: {Deep} {Reinforcement} {Learning} for {Cluster} {Scheduling} in {High} {Performance} {Computing}},
	volume = {33},
	issn = {1045-9219, 1558-2183, 2161-9883},
	shorttitle = {{DRAS}},
	url = {https://ieeexplore.ieee.org/document/9894371/},
	doi = {10.1109/TPDS.2022.3205325},
	abstract = {Cluster schedulers are crucial in high-performance computing (HPC). They determine when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with speciﬁc HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backﬁlling. An efﬁcient training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a speciﬁc scheduling objective given by the system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. We implement DRAS into a HPC scheduling platform called CQGym. CQGym provides a common platform allowing users to ﬂexibly evaluate DRAS and other scheduling methods such as heuristic and optimization methods. The experiments using CQGym with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 50\%.},
	language = {en},
	number = {12},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Fan, Yuping and Li, Boyang and Favorite, Dustin and Singh, Naunidh and Childers, Taylor and Rich, Paul and Allcock, William and Papka, Michael E. and Lan, Zhiling},
	month = dec,
	year = {2022},
	pages = {4903--4917},
}

@article{zhang_robustness_2022,
	title = {Robustness {Analysis} and {Enhancement} of {Deep} {Reinforcement} {Learning}-based {Schedulers}},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/9937194/},
	doi = {10.1109/TPDS.2022.3218649},
	abstract = {Dependency-aware jobs, such as the big data analytic workflows, are commonly executed on the cloud. They are compiled to directed acyclic graphs, with tasks linked in regarding the dependency. The cloud scheduler, which maintains a large number of resources, is responsible to execute tasks in parallel. To resolve the complex dependencies, Deep Reinforcement Learning (DRL) based schedulers are widely applied. However, we find that the DRL-based schedulers are vulnerable to the perturbations in the input jobs and may generate falsified decisions to benefit a particular job while delaying the others. By perturbation, we mean a slight adjustment to the job’s node features or dependencies, while not changing its functionality. In this paper, we first explore the vulnerability of DRL-based schedulers to job perturbations without accessing the information of the DRL models used in the scheduler. We devise the black-box perturbation system, in which, a proxy model is trained to mimic the DRL-based scheduling policy. We show that the high-faith proxy model can help to craft effective perturbations. The DRL-based schedulers can be as high as 60\% likely to be badly affected by the perturbations. Then, we investigate the solution to improve the robustness of DRL-based schedulers to such perturbations. We propose an adversarial training framework to force the neural model to adapt to the perturbation patterns during training so as to eliminate the potential damage during applications. Experiments show that the adversarial-trained scheduler is more robust, reducing the chance of being affected to 3-fold less and the potential bad effects halved.},
	language = {en},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Zhang, Shaojun and Wang, Chen and Zomaya, Albert Y.},
	year = {2022},
	pages = {1--12},
}

@article{chen_adaptive_2022,
	title = {Adaptive and {Efficient} {Resource} {Allocation} in {Cloud} {Datacenters} {Using} {Actor}-{Critic} {Deep} {Reinforcement} {Learning}},
	volume = {33},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/9635652/},
	doi = {10.1109/TPDS.2021.3132422},
	abstract = {The ever-expanding scale of cloud datacenters necessitates automated resource provisioning to best meet the requirements of low latency and high energy-efﬁciency. However, due to the dynamic system states and various user demands, efﬁcient resource allocation in cloud faces huge challenges. Most of the existing solutions for cloud resource allocation cannot effectively handle the dynamic cloud environments because they depend on the prior knowledge of a cloud system, which may lead to excessive energy consumption and degraded Quality-of-Service (QoS). To address this problem, we propose an adaptive and efﬁcient cloud resource allocation scheme based on Actor-Critic Deep Reinforcement Learning (DRL). First, the actor parameterizes the policy (allocating resources) and chooses actions (scheduling jobs) based on the scores assessed by the critic (evaluating actions). Next, the resource allocation policy is updated by using gradient ascent while the variance of policy gradient is reduced with an advantage function, which improves the training efﬁciency of the proposed method. We conduct extensive simulation experiments using real-world data from Google cloud datacenters. The results show that our method can obtain the superior QoS in terms of latency and job dismissing rate with enhanced energy-efﬁciency, compared to two advanced DRL-based and ﬁve classic cloud resource allocation methods.},
	language = {en},
	number = {8},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Chen, Zheyi and Hu, Jia and Min, Geyong and Luo, Chunbo and El-Ghazawi, Tarek},
	month = aug,
	year = {2022},
	pages = {1911--1923},
}

@article{mann_decentralized_2022,
	title = {Decentralized {Application} {Placement} in {Fog} {Computing}},
	volume = {33},
	issn = {1045-9219, 1558-2183, 2161-9883},
	url = {https://ieeexplore.ieee.org/document/9706284/},
	doi = {10.1109/TPDS.2022.3148985},
	abstract = {In recent years, cloud computing concepts have been extended towards the network edge, leading to paradigms like fog and edge computing. As a result, applications can be placed on a variety of resources, including fog nodes and cloud data centers. Application placement has signiﬁcant impact on important metrics like latency. Finding an optimal application placement is computationally challenging, particularly because of the potentially huge number of infrastructure nodes and application components. To overcome the limited scalability of application placement algorithms, optimization can be decentralized, i.e., performed separately for different parts of the infrastructure. The infrastructure can be split into fog colonies, where a fog colony consists of the computational resources in a given geographical region. Application placement can then be performed for the individual fog colonies, thus mitigating the scalability problem. However, independent optimization of application placement in different fog colonies may lead to missed synergies and thus to sub-optimal overall results. Hence, some kind of coordination between fog colonies may be beneﬁcial. In this article, we analyze the effects of decentralization and coordination on the optimization results. In particular, we compare empirically four different approaches: (i) centralized decision-making, where decisions are made in one go for the entire infrastructure, (ii) independent fog colonies, where optimization is carried out in each fog colony independently from each other, (iii) fog colonies with communication, where excess application components in one fog colony can be sent to a neighboring fog colony, and (iv) fog colonies with overlaps, where shared resources may be dynamically distributed between neighboring fog colonies. Our experiments show that, for large problem instances, decentralization combined with coordination leads to the best results.},
	language = {en},
	number = {12},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Mann, Zoltan Adam},
	month = dec,
	year = {2022},
	pages = {3262--3273},
}

@inproceedings{goudarzi_distributed_2021,
	title = {A {Distributed} {Application} {Placement} and {Migration} {Management} {Techniques} for {Edge} and {Fog} {Computing} {Environments}},
	url = {https://annals-csis.org/proceedings/2021/drp/005.html},
	doi = {10.15439/2021F005},
	abstract = {Fog/Edge computing model allows harnessing of resources in the proximity of the Internet of Things (IoT) devices to support various types of latency-sensitive IoT applications. However, due to the mobility of users and a wide range of IoT applications with different resource requirements, it is a challenging issue to satisfy these applications’ requirements. The execution of IoT applications exclusively on one fog/edge server may not be always feasible due to limited resources, while the execution of IoT applications on different servers requires further collaboration and management among servers. Moreover, considering user mobility, some modules of each IoT application may require migration to other servers for execution, leading to service interruption and extra execution costs. In this article, we propose a new weighted cost model for hierarchical fog computing environments, in terms of the response time of IoT applications and energy consumption of IoT devices, to minimize the cost of running IoT applications and potential migrations. Besides, a distributed clustering technique is proposed to enable the collaborative execution of tasks, emitted from application modules, among servers. Also, we propose an application placement technique to minimize the overall cost of executing IoT applications on multiple servers in a distributed manner. Furthermore, a distributed migration management technique is proposed for the potential migration of applications’ modules to other remote servers as the users move along their path. Besides, failure recovery methods are embedded in the clustering, application placement, and migration management techniques to recover from unpredicted failures. The performance results demonstrate that our technique signiﬁcantly improves its counterparts in terms of placement deployment time, average execution cost of tasks, the total number of migrations, the total number of interrupted tasks, and cumulative migration cost.},
	language = {en},
	urldate = {2022-11-08},
	author = {Goudarzi, Mohammad and Palaniswami, Marimuthu and Buyya, Rajkumar},
	month = sep,
	year = {2021},
	pages = {37--56},
}

@article{goudarzi_distributed_2021-1,
	title = {A {Distributed} {Deep} {Reinforcement} {Learning} {Technique} for {Application} {Placement} in {Edge} and {Fog} {Computing} {Environments}},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/9591490/},
	doi = {10.1109/TMC.2021.3123165},
	abstract = {Fog/Edge computing is a novel computing paradigm supporting resource-constrained Internet of Things (IoT) devices by placement of their tasks on edge and/or cloud servers. Recently, several Deep Reinforcement Learning (DRL)-based placement techniques have been proposed in fog/edge computing environments, which are only suitable for centralized setups. The training of well-performed DRL agents requires manifold training data while obtaining training data is costly. Hence, these centralized DRL-based techniques lack generalizability and quick adaptability, thus failing to efﬁciently tackle application placement problems. Moreover, many IoT applications are modeled as Directed Acyclic Graphs (DAGs) with diverse topologies. Satisfying dependencies of DAG-based IoT applications incur additional constraints and increase the complexity of placement problem. To overcome these challenges, we propose an actor-critic-based distributed application placement technique, working based on the IMPortance weighted Actor-Learner Architectures (IMPALA). IMPALA is known for efﬁcient distributed experience trajectory generation that signiﬁcantly reduces exploration costs of agents. Besides, it uses an adaptive off-policy correction method for faster convergence to optimal solutions. Our technique uses recurrent layers to capture temporal behaviors of input data and a replay buffer to improve the sample efﬁciency. The performance results, obtained from simulation and testbed experiments, demonstrate that our technique signiﬁcantly improves execution cost of IoT applications up to 30\% compared to its counterparts.},
	language = {en},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Goudarzi, Mohammad and Palaniswami, Marimuthu S and Buyya, Rajkumar},
	year = {2021},
	pages = {1--1},
}

@article{goudarzi_application_2021,
	title = {An {Application} {Placement} {Technique} for {Concurrent} {IoT} {Applications} in {Edge} and {Fog} {Computing} {Environments}},
	volume = {20},
	issn = {1536-1233, 1558-0660, 2161-9875},
	url = {https://ieeexplore.ieee.org/document/8960404/},
	doi = {10.1109/TMC.2020.2967041},
	abstract = {Fog/Edge computing emerges as a novel computing paradigm that harnesses resources in the proximity of the Internet of Things (IoT) devices so that, alongside with the cloud servers, provide services in a timely manner. However, due to the everincreasing growth of IoT devices with resource-hungry applications, fog/edge servers with limited resources cannot efﬁciently satisfy the requirements of the IoT applications. Therefore, the application placement in the fog/edge computing environment, in which several distributed fog/edge servers and centralized cloud servers are available, is a challenging issue. In this article, we propose a weighted cost model to minimize the execution time and energy consumption of IoT applications, in a computing environment with multiple IoT devices, multiple fog/edge servers and cloud servers. Besides, a new application placement technique based on the Memetic Algorithm is proposed to make batch application placement decision for concurrent IoT applications. Due to the heterogeneity of IoT applications, we also propose a lightweight pre-scheduling algorithm to maximize the number of parallel tasks for the concurrent execution. The performance results demonstrate that our technique signiﬁcantly improves the weighted cost of IoT applications up to 65 percent in comparison to its counterparts.},
	language = {en},
	number = {4},
	urldate = {2022-11-08},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Goudarzi, Mohammad and Wu, Huaming and Palaniswami, Marimuthu and Buyya, Rajkumar},
	month = apr,
	year = {2021},
	pages = {1298--1311},
}

@misc{meyer_survey_2022,
	title = {A {Survey} on {Quantum} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2211.03464},
	abstract = {Quantum reinforcement learning is an emerging ﬁeld at the intersection of quantum computing and machine learning. While we intend to provide a broad overview of the literature on quantum reinforcement learning (our interpretation of this term will be clariﬁed below), we put particular emphasis on recent developments. With a focus on already available noisy intermediate-scale quantum devices, these include variational quantum circuits acting as function approximators in an otherwise classical reinforcement learning setting. In addition, we survey quantum reinforcement learning algorithms based on future fault-tolerant hardware, some of which come with a provable quantum advantage. We provide both a birds-eye-view of the ﬁeld, as well as summaries and reviews for selected parts of the literature.},
	language = {en},
	urldate = {2022-11-08},
	publisher = {arXiv},
	author = {Meyer, Nico and Ufrecht, Christian and Periyasamy, Maniraman and Scherer, Daniel D. and Plinge, Axel and Mutschler, Christopher},
	month = nov,
	year = {2022},
	note = {arXiv:2211.03464 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{bantysh_quantum_2021,
	title = {Quantum tomography benchmarking},
	volume = {20},
	issn = {1570-0755, 1573-1332},
	url = {https://link.springer.com/10.1007/s11128-021-03285-9},
	doi = {10.1007/s11128-021-03285-9},
	abstract = {Recent advances in quantum computers and simulators are steadily leading us toward full-scale quantum computing devices. Due to the fact that debugging is necessary to create any computing device, quantum tomography (QT) is a critical milestone on this path. In practice, the choice between different QT methods faces the lack of comparison methodology. Modern research provides a wide range of QT methods, which differ in their application areas, as well as experimental and computational complexity. Testing such methods is also being made under different conditions, and various efﬁciency measures are being applied. Moreover, many methods have complex programming implementations; thus, comparison becomes extremely difﬁcult. In this study, we have developed a general methodology for comparing quantum-state tomography methods. The methodology is based on an estimate of the resources needed to achieve the required accuracy. We have developed a software library (in MATLAB and Python) that makes it easy to analyze any QT method implementation through a series of numerical experiments. The conditions for such a simulation are set by the number of tests corresponding to real physical experiments. As a validation of the proposed methodology and software, we analyzed and compared a set of QT methods. The analysis revealed some method-speciﬁc features and provided estimates of the relative efﬁciency of the methods.},
	language = {en},
	number = {10},
	urldate = {2022-11-07},
	journal = {Quantum Information Processing},
	author = {Bantysh, B. I. and Chernyavskiy, A. Yu. and Bogdanov, Yu. I.},
	month = oct,
	year = {2021},
	pages = {339},
}

@article{larose_mitiq_2022,
	title = {Mitiq: {A} software package for error mitigation on noisy quantum computers},
	volume = {6},
	issn = {2521-327X},
	shorttitle = {Mitiq},
	url = {http://arxiv.org/abs/2009.04417},
	doi = {10.22331/q-2022-08-11-774},
	abstract = {We introduce Mitiq, a Python package for error mitigation on noisy quantum computers. Error mitigation techniques can reduce the impact of noise on near-term quantum computers with minimal overhead in quantum resources by relying on a mixture of quantum sampling and classical post-processing techniques. Mitiq is an extensible toolkit of different error mitigation methods, including zero-noise extrapolation, probabilistic error cancellation, and Clifford data regression. The library is designed to be compatible with generic backends and interfaces with different quantum software frameworks. We describe Mitiq using code snippets to demonstrate usage and discuss features and contribution guidelines. We present several examples demonstrating error mitigation on IBM and Rigetti superconducting quantum processors as well as on noisy simulators.},
	language = {en},
	urldate = {2022-11-07},
	journal = {Quantum},
	author = {LaRose, Ryan and Mari, Andrea and Kaiser, Sarah and Karalekas, Peter J. and Alves, Andre A. and Czarnik, Piotr and Mandouh, Mohamed El and Gordon, Max H. and Hindy, Yousef and Robertson, Aaron and Thakre, Purva and Wahl, Misty and Samuel, Danny and Mistri, Rahul and Tremblay, Maxime and Gardner, Nick and Stemen, Nathaniel T. and Shammah, Nathan and Zeng, William J.},
	month = aug,
	year = {2022},
	note = {arXiv:2009.04417 [quant-ph]},
	keywords = {Computer Science - Emerging Technologies, Quantum Physics},
	pages = {774},
}

@article{cross_validating_2019,
	title = {Validating quantum computers using randomized model circuits},
	volume = {100},
	issn = {2469-9926, 2469-9934},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.100.032328},
	doi = {10.1103/PhysRevA.100.032328},
	language = {en},
	number = {3},
	urldate = {2022-11-07},
	journal = {Physical Review A},
	author = {Cross, Andrew W. and Bishop, Lev S. and Sheldon, Sarah and Nation, Paul D. and Gambetta, Jay M.},
	month = sep,
	year = {2019},
	keywords = {quantum volume},
	pages = {032328},
}

@article{noauthor_quantum_nodate,
	title = {Quantum {Machine} {Learning} - {State} of the {Art} and {Future} {Directions}},
	language = {en},
	pages = {122},
}

@misc{wack_quality_2021,
	title = {Quality, {Speed}, and {Scale}: three key attributes to measure the performance of near-term quantum computers},
	shorttitle = {Quality, {Speed}, and {Scale}},
	url = {http://arxiv.org/abs/2110.14108},
	abstract = {Defining the right metrics to properly represent the performance of a quantum computer is critical to both users and developers of a computing system. In this white paper, we identify three key attributes for quantum computing performance: quality, speed, and scale. Quality and scale are measured by quantum volume and number of qubits, respectively. We propose a speed benchmark, using an update to the quantum volume experiments that allows the measurement of Circuit Layer Operations Per Second (CLOPS) and identify how both classical and quantum components play a role in improving performance. We prescribe a procedure for measuring CLOPS and use it to characterize the performance of some IBM Quantum systems.},
	language = {en},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Wack, Andrew and Paik, Hanhee and Javadi-Abhari, Ali and Jurcevic, Petar and Faro, Ismael and Gambetta, Jay M. and Johnson, Blake R.},
	month = oct,
	year = {2021},
	note = {arXiv:2110.14108 [quant-ph]},
	keywords = {Quantum Physics},
}

@incollection{mahalingam_conceptual_2022,
	title = {A {Conceptual} {Framework} for {Scaling} and {Security} in {Serverless} {Environments} {Using} {Blockchain} and {Quantum} {Key} {Distribution}},
	volume = {133},
	isbn = {978-3-031-04613-1},
	url = {http://dx.doi.org/10.1007/978-3-031-04613-1_5},
	abstract = {Serverless computing is one of the most preferred deployment strategies in today’s world. It refers to a concept where execution environment is not predefined during deployment, but scaled based on demand. The concept has gained importance with improvements in cloud computing, which banks on this concept. Nowadays, cloud service providers are able to expose this concept as a cloud service itself, with application like GCP Cloud Functions and AWS Lambda. But as services go online, securing them becomes a challenge. Serverless computing was originally envisaged as a stateless environment, but now it has evolved to semi-stateful entities, making them prone to state analysis and hijacking. With current cryptosystems being outmatched at a fast pace, and quantum computing expected to break almost all cryptographic algorithms in the coming decade, it is imperative that serverless communications be encrypted in a much more secure fashion. A concept based on quantum key distribution and blockchain is proposed to improve application scaling and security. Blockchain platforms like Ethereum work based on smart contracts, which can execute pieces of code on the nodes and place results on the blockchain after consensus. The same can be adapted to serverless computing because smart contract codebase is deployed on the node itself, and is triggered by user requests. The user-defined function can then be deployed as a smart contract. When demand is huge, more nodes can be spun up, which contain the codebase for the smart contract, and they can initiate execution. In order to enhance security, the communication between endpoints can be protected by using Quantum Key Distribution, and even among the nodes, selections can be done using principles behind QKD. After execution is complete, the result is placed on the blockchain itself, which acts as a log of execution, as well as carry the result.},
	booktitle = {A {Conceptual} {Framework} for {Scaling} and {Security} in {Serverless} {Environments} {Using} {Blockchain} and {Quantum} {Key} {Distribution}},
	publisher = {Springer International Publishing},
	author = {Mahalingam, P. R.},
	year = {2022},
	doi = {10.1007/978-3-031-04613-1_5},
	note = {Publication Title: Lecture Notes on Data Engineering and Communications Technologies
ISSN: 23674520},
}

@article{vardoyan_quantum_2021,
	title = {On the {Quantum} {Performance} {Evaluation} of {Two} {Distributed} {Quantum} {Architectures}},
	url = {http://arxiv.org/abs/2107.12246},
	abstract = {Distributed quantum applications impose requirements on the quality of the quantum states that they consume. When analyzing architecture implementations of quantum hardware, characterizing this quality forms an important factor in understanding their performance. Fundamental characteristics of quantum hardware lead to inherent tradeoffs between the quality of states and traditional performance metrics such as throughput. Furthermore, any real-world implementation of quantum hardware exhibits time-dependent noise that degrades the quality of quantum states over time. Here, we study the performance of two possible architectures for interfacing a quantum processor with a quantum network. The first corresponds to the current experimental state of the art in which the same device functions both as a processor and a network device. The second corresponds to a future architecture that separates these two functions over two distinct devices. We model these architectures as Markov chains and compare their quality of executing quantum operations and producing entangled quantum states as functions of their memory lifetimes, as well as the time that it takes to perform various operations within each architecture. As an illustrative example, we apply our analysis to architectures based on Nitrogen-Vacancy centers in diamond, where we find that for present-day device parameters one architecture is more suited to computation-heavy applications, and the other for network-heavy ones. Besides the detailed study of these architectures, a novel contribution of our work are several formulas that connect an understanding of waiting time distributions to the decay of quantum quality over time for the most common noise models employed in quantum technologies. This provides a valuable new tool for performance evaluation experts, and its applications extend beyond the two architectures studied in this work.},
	author = {Vardoyan, Gayane and Skrzypczyk, Matthew and Wehner, Stephanie},
	year = {2021},
	note = {arXiv: 2107.12246},
	pages = {1--35},
}

@article{higgott_pymatching_2022,
	title = {{PyMatching}: {A} {Python} {Package} for {Decoding} {Quantum} {Codes} with {Minimum}-{Weight} {Perfect} {Matching}},
	volume = {3},
	issn = {2643-6809},
	doi = {10.1145/3505637},
	abstract = {This article introduces PyMatching, a fast open-source Python package for decoding quantum error-correcting codes with the minimum-weight perfect matching (MWPM) algorithm. PyMatching includes the standard MWPM decoder as well as a variant, which we call local matching , that restricts each syndrome defect to be matched to another defect within a local neighborhood. The decoding performance of local matching is almost identical to that of the standard MWPM decoder in practice, while reducing the computational complexity. We benchmark the performance of PyMatching, showing that local matching is several orders of magnitude faster than implementations of the full MWPM algorithm using NetworkX or Blossom V for problem sizes typically considered in error correction simulations. PyMatching and its dependencies are open-source, and it can be used to decode any quantum code for which syndrome defects come in pairs using a simple Python interface. PyMatching supports the use of weighted edges, hook errors, boundaries and measurement errors, enabling fast decoding, and simulation of fault-tolerant quantum computing.},
	number = {3},
	journal = {ACM Transactions on Quantum Computing},
	author = {Higgott, Oscar},
	year = {2022},
	note = {arXiv: 2105.13082},
	pages = {1--16},
}

@article{fitzsimons_private_2017,
	title = {Private quantum computation: an introduction to blind quantum computing and related protocols},
	volume = {3},
	issn = {2056-6387},
	url = {http://dx.doi.org/10.1038/s41534-017-0025-3},
	doi = {10.1038/s41534-017-0025-3},
	abstract = {Quantum technologies hold the promise of not only faster algorithmic processing of data, via quantum computation, but also of more secure communications, in the form of quantum cryptography. In recent years, a number of protocols have emerged which seek to marry these concepts for the purpose of securing computation rather than communication. These protocols address the task of securely delegating quantum computation to an untrusted device while maintaining the privacy, and in some instances the integrity, of the computation. We present a review of the progress to date in this emerging area.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Fitzsimons, Joseph F.},
	year = {2017},
	note = {arXiv: 1611.10107
Publisher: Springer US},
	pages = {1--10},
}

@techreport{qasm,
	title = {Open {Quantum} {Assembly} {Language}},
	url = {http://arxiv.org/abs/1707.03429},
	abstract = {This document describes a quantum assembly language (QASM) called OpenQASM that is used to implement experiments with low depth quantum circuits. OpenQASM represents universal physical circuits over the CNOT plus SU(2) basis with straight-line code that includes measurement, reset, fast feedback, and gate subroutines. The simple text language can be written by hand or by higher level tools and may be executed on the IBM Q Experience.},
	author = {Cross, Andrew and Bishop, Lev and Smolin, John and Gambetta, Jay},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.03429},
}

@article{varghese_next_2018,
	title = {Next generation cloud computing: {New} trends and research directions},
	volume = {79},
	issn = {0167739X},
	url = {http://dx.doi.org/10.1016/j.future.2017.09.020},
	doi = {10.1016/j.future.2017.09.020},
	abstract = {The landscape of cloud computing has significantly changed over the last decade. Not only have more providers and service offerings crowded the space, but also cloud infrastructure that was traditionally limited to single provider data centers is now evolving. In this paper, we firstly discuss the changing cloud infrastructure and consider the use of infrastructure from multiple providers and the benefit of decentralising computing away from data centers. These trends have resulted in the need for a variety of new computing architectures that will be offered by future cloud infrastructure. These architectures are anticipated to impact areas, such as connecting people and devices, data-intensive computing, the service space and self-learning systems. Finally, we lay out a roadmap of challenges that will need to be addressed for realising the potential of next generation cloud systems.},
	journal = {Future Generation Computer Systems},
	author = {Varghese, Blesson and Buyya, Rajkumar},
	year = {2018},
	note = {arXiv: 1707.07452
Publisher: Elsevier B.V.},
	keywords = {Cloud computing, Cloud security, Cloudlet, Fog computing, Multi-cloud, Serverless computing},
	pages = {849--861},
}

@article{weder_modulo_2021,
	title = {{MODULO}: {Modeling}, {Transformation}, and {Deployment} of {Quantum} {Workflows}},
	issn = {15417719},
	doi = {10.1109/EDOCW52865.2021.00067},
	abstract = {Quantum applications are usually hybrid, i.e., they require executing quantum programs and classical programs, e.g., performing pre- or post-processing tasks. To benefit from advantages, such as robustness, reliability, or scalability, these programs can be orchestrated using quantum workflows. However, modeling quantum-specific tasks in workflows is complex and requires deep technical and mathematical knowledge. Furthermore, additional steps have to be performed before executing the workflow, e.g., the deployment and binding of the needed services. In this demonstration, we present the MODULO framework, providing a workflow modeling extension easing the modeling of quantum workflows. It comprises an integrated toolchain to graphically model quantum workflows, transform and package them in a self-contained archive, and automatically deploying the workflows together with their required services.},
	journal = {Proceedings - IEEE International Enterprise Distributed Object Computing Workshop, EDOCW},
	author = {Weder, Benjamin and Barzen, Johanna and Leymann, Frank},
	year = {2021},
	note = {Publisher: IEEE
ISBN: 9781665444880},
	keywords = {Quantum Computing, Quantum Workflows, Service Deployment Automation, Workflow Technology},
	pages = {341--344},
}

@article{truong_machine_2018,
	title = {Machine {Learning} {Cryptanalysis} of a {Quantum} {Random} {Number} {Generator}},
	volume = {14},
	issn = {15566013},
	doi = {10.1109/TIFS.2018.2850770},
	abstract = {Random number generators (RNGs) that are crucial for cryptographic applications have been the subject of adversarial attacks. These attacks exploit environmental information to predict generated random numbers that are supposed to be truly random and unpredictable. Though quantum random number generators (QRNGs) are based on the intrinsic indeterministic nature of quantum properties, the presence of classical noise in the measurement process compromises the integrity of a QRNG. In this paper, we develop a predictive machine learning (ML) analysis to investigate the impact of deterministic classical noise in different stages of an optical continuous variable QRNG. Our ML model successfully detects inherent correlations when the deterministic noise sources are prominent. After appropriate filtering and randomness extraction processes are introduced, our QRNG system, in turn, demonstrates its robustness against ML. We further demonstrate the robustness of our ML approach by applying it to uniformly distributed random numbers from the QRNG and a congruential RNG. Hence, our result shows that ML has potentials in benchmarking the quality of RNG devices.},
	number = {2},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Truong, Nhan Duy and Haw, Jing Yan and Assad, Syed Muhamad and Lam, Ping Koy and Kavehei, Omid},
	year = {2018},
	note = {arXiv: 1905.02342
Publisher: IEEE},
	keywords = {Quantum random number generator, cryptoanalysis, machine learning},
	pages = {403--414},
}

@article{guerreschi_intel_2020,
	title = {Intel {Quantum} {Simulator}: a cloud-ready high-performance simulator of quantum circuits},
	volume = {5},
	issn = {2058-9565},
	shorttitle = {Intel {Quantum} {Simulator}},
	url = {https://iopscience.iop.org/article/10.1088/2058-9565/ab8505},
	doi = {10.1088/2058-9565/ab8505},
	abstract = {Classical simulation of quantum computers will continue to play an essential role in the progress of quantum information science, both for numerical studies of quantum algorithms and for modeling noise and errors. Here we introduce the latest release of Intel Quantum Simulator (IQS), formerly known as qHiPSTER. The high-performance computing (HPC) capability of the software allows users to leverage the available hardware resources provided by supercomputers, as well as available public cloud computing infrastructure. To take advantage of the latter platform, together with the distributed simulation of each separate quantum state, IQS allows to subdivide the computational resources to simulate a pool of related circuits in parallel. We highlight the technical implementation of the distributed algorithm and details about the new pool functionality. We also include some basic benchmarks (up to 42 qubits) and performance results obtained using HPC infrastructure. Finally, we use IQS to emulate a scenario in which many quantum devices are running in parallel to implement the quantum approximate optimization algorithm, using particle swarm optimization as the classical subroutine. The results demonstrate that the hyperparameters of this classical optimization algorithm depends on the total number of quantum circuit simulations one has the bandwidth to perform. Intel Quantum Simulator has been released open-source with permissive licensing and is designed to simulate a large number of qubits, to emulate multiple quantum devices running in parallel, and/or to study the effects of decoherence and other hardware errors on calculation results.},
	language = {en},
	number = {3},
	urldate = {2022-08-30},
	journal = {Quantum Science and Technology},
	author = {Guerreschi, Gian Giacomo and Hogaboam, Justin and Baruffa, Fabio and Sawaya, Nicolas P D},
	month = may,
	year = {2020},
	keywords = {cloud computing, particle swarm optimization, quantum emulation, quantum information, quantum simulation, variational quantum algorithm},
	pages = {034007},
}

@article{basu_i_2022,
	title = {i -{QER}: {An} {Intelligent} {Approach} {Towards} {Quantum} {Error} {Reduction}},
	volume = {3},
	issn = {2643-6809},
	doi = {10.1145/3539613},
	abstract = {Quantum computing has become a promising computing approach because of its capability to solve certain problems, exponentially faster than classical computers. A n -qubit quantum system is capable of providing 2  n  computational space to a quantum algorithm. However, quantum computers are prone to errors. Quantum circuits that can reliably run on today’s Noisy Intermediate-Scale Quantum (NISQ) devices are not only limited by their qubit counts but also by their noisy gate operations. In this article, we have introduced i -QER, a scalable machine learning-based approach to evaluate errors in a quantum circuit and reduce these without using any additional quantum resources. The i -QER predicts possible errors in a given quantum circuit using supervised learning models. If the predicted error is above a pre-specified threshold, it cuts the large quantum circuit into two smaller sub-circuits using an error-influenced fragmentation strategy for the first time to the best of our knowledge. The proposed fragmentation process is iterated until the predicted error reaches below the threshold for each sub-circuit. The sub-circuits are then executed on a quantum device. Classical reconstruction of the outputs obtained from the sub-circuits can generate the output of the complete circuit. Thus, i -QER also provides classical control over a scalable hybrid computing approach, which is a combination of quantum and classical computers. The i -QER tool is available at https://github.com/SaikatBasu90/i-QER .},
	number = {4},
	journal = {ACM Transactions on Quantum Computing},
	author = {Basu, Saikat and Saha, Amit and Chakrabarti, Amlan and Sur-Kolay, Susmita},
	year = {2022},
	note = {arXiv: 2110.06347},
	pages = {1--18},
}

@article{weder_hybrid_2021,
	title = {Hybrid {Quantum} {Applications} {Need} {Two} {Orchestrations} in {Superposition}: {A} {Software} {Architecture} {Perspective}},
	doi = {10.1109/ICWS53863.2021.00015},
	abstract = {Quantum applications are most often hybrid, i.e., they are not only made of implementations of pure quantum algorithms but also of classical programs as well as workflows and topologies as key artifacts, and data they process. Since workflows and topologies are referred to as 'orchestrations' in modern terminology (but with very different meanings), two orchestrations that go hand-in-hand are required to realize quantum applications. We motivate this by means of a nontrivial example, sketch these orchestration technologies, and reveal the overall structure of non-trivial quantum applications. Furthermore, we discuss the implied architecture of a runtime environment for such quantum applications. To validate the introduced architecture, we present a prototypical implementation based on the Camunda workflow engine, its associated modeling tool, as well as the OpenTOSCA ecosystem.},
	journal = {Proceedings - 2021 IEEE International Conference on Web Services, ICWS 2021},
	author = {Weder, Benjamin and Barzen, Johanna and Leymann, Frank and Zimmermann, Michael},
	year = {2021},
	note = {arXiv: 2103.04320
Publisher: IEEE
ISBN: 9781665416818},
	keywords = {Hybrid Quantum-Classical Applications, NISQ, Quantum Computing, Runtime for Quantum Applications, Software Engineering of Quantum Applications, applications, engineering of quantum applications, hybrid quantum-classical, nisq, quantum computing, quantum software, runtime for quantum applications, software, ★},
	pages = {1--13},
}

@article{huang_homomorphic_2017,
	title = {Homomorphic encryption experiments on {IBM}’s cloud quantum computing platform},
	volume = {12},
	issn = {20950470},
	doi = {10.1007/s11467-016-0643-9},
	abstract = {Quantum computing has undergone rapid development in recent years. Owing to limitations on scalability, personal quantum computers still seem slightly unrealistic in the near future. The first practical quantum computer for ordinary users is likely to be on the cloud. However, the adoption of cloud computing is possible only if security is ensured. Homomorphic encryption is a cryptographic protocol that allows computation to be performed on encrypted data without decrypting them, so it is well suited to cloud computing. Here, we first applied homomorphic encryption on IBM’s cloud quantum computer platform. In our experiments, we successfully implemented a quantum algorithm for linear equations while protecting our privacy. This demonstration opens a feasible path to the next stage of development of cloud quantum information technology.},
	number = {1},
	journal = {Frontiers of Physics},
	author = {Huang, He Liang and Zhao, You Wei and Li, Tan and Li, Feng Guang and Du, Yu Tao and Fu, Xiang Qun and Zhang, Shuo and Wang, Xiang and Bao, Wan Su},
	month = feb,
	year = {2017},
	note = {arXiv: 1612.02886
Publisher: Higher Education Press},
	keywords = {IBM quantum experience, cloud computing, homomorphic encryption, ibmq, linear equations, quantum computing, quantum security},
}

@article{Shen2020,
	title = {From distributed machine learning to federated learning: {In} the view of data privacy and security},
	issn = {15320634},
	doi = {10.1002/cpe.6002},
	abstract = {Federated learning is an improved version of distributed machine learning that further offloads operations which would usually be performed by a central server. The server becomes more like an assistant coordinating clients to work together rather than micromanaging the workforce as in traditional DML. One of the greatest advantages of federated learning is the additional privacy and security guarantees it affords. Federated learning architecture relies on smart devices, such as smartphones and IoT sensors, that collect and process their own data, so sensitive information never has to leave the client device. Rather, clients train a submodel locally and send an encrypted update to the central server for aggregation into the global model. These strong privacy guarantees make federated learning an attractive choice in a world where data breaches and information theft are common and serious threats. This survey outlines the landscape and latest developments in data privacy and security for federated learning. We identify the different mechanisms used to provide privacy and security, such as differential privacy, secure multiparty computation and secure aggregation. We also survey the current attack models, identifying the areas of vulnerability and the strategies adversaries use to penetrate federated systems. The survey concludes with a discussion on the open challenges and potential directions of future work in this increasingly popular learning paradigm.},
	number = {February},
	journal = {Concurrency Computation},
	author = {Shen, Sheng and Zhu, Tianqing and Wu, Di and Wang, Wei and Zhou, Wanlei},
	year = {2020},
	note = {arXiv: 2010.09258},
	keywords = {data privacy, distributed machine learning, federated learning, security, ★},
	pages = {1--19},
}

@article{Lim2020,
	title = {Federated {Learning} in {Mobile} {Edge} {Networks}: {A} {Comprehensive} {Survey}},
	volume = {22},
	issn = {1553877X},
	doi = {10.1109/COMST.2020.2986024},
	abstract = {In recent years, mobile devices are equipped with increasingly advanced sensing and computing capabilities. Coupled with advancements in Deep Learning (DL), this opens up countless possibilities for meaningful applications, e.g., for medical purposes and in vehicular networks. Traditional cloud-based Machine Learning (ML) approaches require the data to be centralized in a cloud server or data center. However, this results in critical issues related to unacceptable latency and communication inefficiency. To this end, Mobile Edge Computing (MEC) has been proposed to bring intelligence closer to the edge, where data is produced. However, conventional enabling technologies for ML at mobile edge networks still require personal data to be shared with external parties, e.g., edge servers. Recently, in light of increasingly stringent data privacy legislations and growing privacy concerns, the concept of Federated Learning (FL) has been introduced. In FL, end devices use their local data to train an ML model required by the server. The end devices then send the model updates rather than raw data to the server for aggregation. FL can serve as an enabling technology in mobile edge networks since it enables the collaborative training of an ML model and also enables DL for mobile edge network optimization. However, in a large-scale and complex mobile edge network, heterogeneous devices with varying constraints are involved. This raises challenges of communication costs, resource allocation, and privacy and security in the implementation of FL at scale. In this survey, we begin with an introduction to the background and fundamentals of FL. Then, we highlight the aforementioned challenges of FL implementation and review existing solutions. Furthermore, we present the applications of FL for mobile edge network optimization. Finally, we discuss the important challenges and future research directions in FL.},
	number = {3},
	journal = {IEEE Communications Surveys and Tutorials},
	author = {Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
	year = {2020},
	note = {arXiv: 1909.11875},
	keywords = {Federated learning, communication cost, data privacy, data security, mobile edge networks, resource allocation, ★},
	pages = {2031--2063},
}

@article{liu_federated_2020,
	title = {Federated learning for {6G} communications: {Challenges}, methods, and future directions},
	volume = {17},
	issn = {16735447},
	doi = {10.23919/JCC.2020.09.009},
	abstract = {As the 5G communication networks are being widely deployed worldwide, both industry and academia have started to move beyond 5G and explore 6G communications. It is generally believed that 6G will be established on ubiquitous Artificial Intelligence (AI) to achieve data-driven Machine Learning (ML) solutions in heterogeneous and massive-scale networks. However, traditional ML techniques require centralized data collection and processing by a central server, which is becoming a bottleneck of large-scale implementation in daily life due to significantly increasing privacy concerns. Federated learning, as an emerging distributed AI approach with privacy preservation nature, is particularly attractive for various wireless applications, especially being treated as one of the vital solutions to achieve ubiquitous AI in 6G. In this article, we first introduce the integration of 6G and federated learning and provide potential federated learning applications for 6G. We then describe key technical challenges, the corresponding federated learning methods, and open problems for future research on federated learning in the context of 6G communications.},
	number = {9},
	journal = {China Communications},
	author = {Liu, Yi and Yuan, Xingliang and Xiong, Zehui and Kang, Jiawen and Wang, Xiaofei and Niyato, Dusit},
	year = {2020},
	note = {arXiv: 2006.02931},
	keywords = {6G communication, Federated learning, Privacy protection, Security, federated learning, security and privacy protection},
	pages = {105--118},
}

@article{yussupov_faasten_2021,
	title = {{FaaSten} your decisions: {A} classification framework and technology review of function-as-a-{Service} platforms},
	volume = {175},
	issn = {01641212},
	doi = {10.1016/j.jss.2021.110906},
	abstract = {Function-as-a-Service (FaaS) is a cloud service model enabling developers to offload event-driven executable snippets of code. The execution and management of such functions becomes a FaaS provider's responsibility, therein included their on-demand provisioning and automatic scaling. Key enablers for this cloud service model are FaaS platforms, e.g., AWS Lambda, Microsoft Azure Functions, or OpenFaaS. At the same time, the choice of the most appropriate FaaS platform for deploying and running a serverless application is not trivial, as various organizational and technical aspects have to be taken into account. In this work, we present (i) a FaaS platform classification framework derived using a multivocal review and (ii) a technology review of the ten most prominent FaaS platforms, based on the proposed classification framework. We also present a FaaS platform selection support system, called FAASTENER, which can help researchers and practitioners to choose the FaaS platform most suited for their requirements.},
	journal = {Journal of Systems and Software},
	author = {Yussupov, Vladimir and Soldani, Jacopo and Breitenbücher, Uwe and Brogi, Antonio and Leymann, Frank},
	month = may,
	year = {2021},
	note = {arXiv: 2004.00969
Publisher: Elsevier Inc.},
	keywords = {Classification framework, FaaS, Function-as-a-Service, Platform, Serverless, Technology review, ★},
}

@article{nguyen_extending_2022,
	title = {Extending {Python} for {Quantum}-classical {Computing} via {Quantum} {Just}-in-time {Compilation}},
	volume = {3},
	issn = {2643-6809},
	doi = {10.1145/3544496},
	abstract = {Python is a popular programming language known for its flexibility, usability, readability, and focus on developer productivity. The quantum software community has adopted Python on a number of large-scale efforts due to these characteristics, as well as the remote nature of near-term quantum processors. The use of Python has enabled quick prototyping for quantum code that directly benefits pertinent research and development efforts in quantum scientific computing. However, this rapid prototyping ability comes at the cost of future performant integration for tightly coupled CPU-QPU architectures with fast-feedback. Here, we present a language extension to Python that enables heterogeneous quantum-classical computing via a robust C++ infrastructure for quantum just-in-time (QJIT) compilation. Our work builds off the QCOR C++ language extension and compiler infrastructure to enable a single-source, quantum hardware-agnostic approach to quantum-classical computing that retains the performance required for tightly coupled CPU-QPU compute models. We detail this Python extension, its programming model and underlying software architecture, and provide a robust set of examples to demonstrate the utility of our approach.},
	number = {4},
	journal = {ACM Transactions on Quantum Computing},
	author = {Nguyen, Thien and McCaskey, Alexander J.},
	year = {2022},
	note = {arXiv: 2105.04671},
	pages = {1--25},
}

@article{qsdk-qcor,
	title = {Extending {C}++ for {Heterogeneous} {Quantum}-{Classical} {Computing}},
	volume = {2},
	issn = {2643-6809},
	doi = {10.1145/3462670},
	abstract = {We present qcor—a language extension to C++ and compiler implementation that enables heterogeneous quantum-classical programming, compilation, and execution in a single-source context. Our work provides a first-of-its-kind C++ compiler enabling high-level quantum kernel (function) expression in a quantum-language agnostic manner, as well as a hardware-agnostic, retargetable compiler workflow targeting a number of physical and virtual quantum computing backends. qcor leverages novel Clang plugin interfaces and builds upon the XACC system-level quantum programming framework to provide a state-of-the-art integration mechanism for quantum-classical compilation that leverages the best from the community at-large. qcor translates quantum kernels ultimately to the XACC intermediate representation, and provides user-extensible hooks for quantum compilation routines like circuit optimization, analysis, and placement. This work details the overall architecture and compiler workflow for qcor, and provides a number of illuminating programming examples demonstrating its utility for near-term variational tasks, quantum algorithm expression, and feed-forward error correction schemes.},
	number = {2},
	journal = {ACM Transactions on Quantum Computing},
	author = {Mccaskey, Alexander and Nguyen, Thien and Santana, Anthony and Claudino, Daniel and Kharazi, Tyler and Finkel, Hal},
	month = jul,
	year = {2021},
	note = {arXiv: 2010.03935
Publisher: Association for Computing Machinery (ACM)},
	pages = {1--36},
}

@article{nguyen_diot_2019,
	title = {D{ÏoT}: {A} federated self-learning anomaly detection system for {IoT}},
	volume = {2019-July},
	doi = {10.1109/ICDCS.2019.00080},
	abstract = {IoT devices are increasingly deployed in daily life. Many of these devices are, however, vulnerable due to insecure design, implementation, and configuration. As a result, many networks already have vulnerable IoT devices that are easy to compromise. This has led to a new category of malware specifically targeting IoT devices. However, existing intrusion detection techniques are not effective in detecting compromised IoT devices given the massive scale of the problem in terms of the number of different types of devices and manufacturers involved. In this paper, we present DÏoT, an autonomous self-learning distributed system for detecting compromised IoT devices. DÏoT builds effectively on device-type-specific communication profiles without human intervention nor labeled data that are subsequently used to detect anomalous deviations in devices' communication behavior, potentially caused by malicious adversaries. DÏoT utilizes a federated learning approach for aggregating behavior profiles efficiently. To the best of our knowledge, it is the first system to employ a federated learning approach to anomaly-detection-based intrusion detection. Consequently, DÏoT can cope with emerging new and unknown attacks. We systematically and extensively evaluated more than 30 off-the-shelf IoT devices over a long term and show that DÏoT is highly effective (95.6\% detection rate) and fast (257 ms) at detecting devices compromised by, for instance, the infamous Mirai malware. DÏoT reported no false alarms when evaluated in a real-world smart home deployment setting.},
	journal = {Proceedings - International Conference on Distributed Computing Systems},
	author = {Nguyen, Thien Duc and Marchal, Samuel and Miettinen, Markus and Fereidooni, Hossein and Asokan, N. and Sadeghi, Ahmad Reza},
	year = {2019},
	note = {arXiv: 1804.07474
Publisher: IEEE
ISBN: 9781728125190},
	keywords = {Anomaly detection, Federated deep learning, Internet of Things, IoT malware, IoT security, Self-learning},
	pages = {756--767},
}

@techreport{proctor_demonstrating_nodate,
	title = {Demonstrating {Scalable} {Benchmarking} of {Quantum} {Computers} {PRESENTED} {BY}},
	author = {Proctor, Timothy and Rudinger, Kenny and Young, Kevin and Nielsen, Erik and Blume-Kohout, Robin},
}

@article{funcke_best-approximation_2021,
	title = {Best-approximation error for parametric quantum circuits},
	doi = {10.1109/ICWS53863.2021.00096},
	abstract = {In Variational Quantum Simulations, the construction of a suitable parametric quantum circuit is subject to two counteracting effects. The number of parameters should be small for the device noise to be manageable, but also large enough for the circuit to be able to represent the solution. Dimensional expressivity analysis can optimize a candidate circuit considering both aspects. In this article, we will first discuss an inductive construction for such candidate circuits. Furthermore, it is sometimes necessary to choose a circuit with fewer parameters than necessary to represent all relevant states. To characterize such circuits, we estimate the best-approximation error using Voronoi diagrams. Moreover, we discuss a hybrid quantum-classical algorithm to estimate the worst-case best-approximation error, its complexity, and its scaling in state space dimensionality. This allows us to identify some obstacles for variational quantum simulations with local optimizers and underparametrized circuits, and we discuss possible remedies.},
	journal = {Proceedings - 2021 IEEE International Conference on Web Services, ICWS 2021},
	author = {Funcke, Lena and Hartung, Tobias and Jansen, Karl and Kuhn, Stefan and Schneider, Manuel and Stornati, Paolo},
	year = {2021},
	note = {arXiv: 2107.07378
ISBN: 9781665416818},
	keywords = {Voronoi diagrams, best-approximation error, dimensional expressivity analysis, parametric quantum circuits, variational quantum simulations},
	pages = {693--702},
}

@techreport{kozlowski_architectural_2022,
	title = {Architectural {Principles} for a {Quantum} {Internet}},
	url = {https://datatracker.ietf.org/doc/draft-irtf-qirg-principles/11/},
	abstract = {The vision of a quantum internet is to enhance existing Internet technology by enabling quantum communication between any two points on Earth. To achieve this goal, a quantum network stack should be built from the ground up to account for the fundamentally new properties of quantum entanglement. The first quantum entanglement networks have been realised [Pompili21.1], but there is no practical proposal for how to organise, utilise, and manage such networks. In this draft, we attempt to lay down the framework and introduce some basic architectural principles for a quantum internet. This is intended for general guidance and general interest, but also to provide a foundation for discussion between physicists and network specialists. This document is a product of the Quantum Internet Research Group (QIRG).},
	institution = {Internet Engineering Task Force},
	author = {Kozlowski, Wojciech and Wehner, Stephanie and Meter, Rodney Van and Rijsman, Bruno and Cacciapuoti, Angela Sara and Caleffi, Marcello and Nagayama, Shota},
	month = aug,
	year = {2022},
	note = {Issue: draft-irtf-qirg-principles-11},
}

@article{Lo2021,
	title = {Architectural {Patterns} for the {Design} of {Federated} {Learning} {Systems}},
	url = {http://arxiv.org/abs/2101.02373},
	abstract = {Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, and four model aggregation patterns. The patterns are associated to particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems.},
	author = {Lo, Sin Kit and Lu, Qinghua and Zhu, Liming and Paik, Hye-young and Xu, Xiwei and Wang, Chen},
	year = {2021},
	note = {arXiv: 2101.02373},
	keywords = {Artificial intelligence, Federated learning, Machine learning, Pattern, Software architecture, ★},
}

@article{yung_anti-forging_2020,
	title = {Anti-{Forging} {Quantum} {Data}: {Cryptographic} {Verification} of {Quantum} {Cloud} {Computing}},
	url = {http://arxiv.org/abs/2005.01510},
	abstract = {Quantum cloud computing is emerging as a popular model for users to experience the power of quantum computing through the internet, enabling quantum computing as a service (QCaaS). The question is, when the scale of the computational problems becomes out of reach of classical computers, how can users be sure that the output strings sent by the server are really from a quantum hardware? In 2008, Shepherd and Bremner proposed a cryptographic verification protocol based on a simplified circuit model called IQP (instantaneous quantum polynomial-time), which can potentially be applied to most existing quantum cloud platforms. However, the Shepherd-Bremner protocol has recently been shown to be insecure by Kahanamoku-Meyer. Here we present a generalized model of cryptographic verification protocol, where the Shepherd-Bremner model can be regarded as a special case. This protocol not only can avoid the attack by Kahanamoku-Meyer but also provide several additional security measures for anti-forging quantum data. In particular, our protocol admits a simultaneous encoding of multiple secret strings, strengthening significantly the hardness for classical hacking. Furthermore, we provide methods for estimating the correlation functions associated with the secret strings, which are the key elements in our verification protocol.},
	author = {Yung, Man-Hong and Cheng, Bin},
	month = may,
	year = {2020},
	note = {arXiv: 2005.01510},
}

@article{quan_simplified_2021,
	title = {A simplified verifiable blind quantum computing protocol with quantum input verification},
	volume = {3},
	issn = {2577-0470},
	doi = {10.1002/que2.58},
	number = {1},
	journal = {Quantum Engineering},
	author = {Quan, Junyu and Li, Qin and Liu, Chengdong and Shi, Jinjing and Peng, Yu},
	year = {2021},
	keywords = {and, blind quantum computing, central south university, delegated quantum computing, engineering, quantum cryptography, school of computer science},
	pages = {1--10},
}

@article{liu_secure_2020,
	title = {A {Secure} {Federated} {Learning} {Framework} for {5G} {Networks}},
	volume = {27},
	issn = {15580687},
	doi = {10.1109/MWC.01.1900525},
	abstract = {Federated learning (FL) has recently been proposed as an emerging paradigm to build machine learning models using distributed training datasets that are locally stored and maintained on different devices in 5G networks while providing privacy preservation for participants. In FL, the central aggregator accumulates local updates uploaded by participants to update a global model. However, there are two critical security threats: poisoning and membership inference attacks. These attacks may be carried out by malicious or unreliable participants, resulting in the construction failure of global models or privacy leakage of FL models. Therefore, it is crucial for FL to develop security means of defense. In this article, we propose a blockchain-based secure FL framework to create smart contracts and prevent malicious or unreliable participants from being involved in FL. In doing so, the central aggregator recognizes malicious and unreliable participants by automatically executing smart contracts to defend against poisoning attacks. Further, we use local differential privacy techniques to prevent membership inference attacks. Numerical results suggest that the proposed framework can effectively deter poisoning and membership inference attacks, thereby improving the security of FL in 5G networks.},
	number = {4},
	journal = {IEEE Wireless Communications},
	author = {Liu, Yi and Peng, Jialiang and Kang, Jiawen and Iliyasu, Abdullah M. and Niyato, Dusit and El-Latif, Ahmed A.Abd},
	year = {2020},
	note = {arXiv: 2005.05752},
	pages = {24--31},
}

@article{ittah_qiro_2022,
	title = {{QIRO}: {A} {Static} {Single} {Assignment}-based {Quantum} {Program} {Representation} for {Optimization}},
	volume = {(Just Accepted)},
	issn = {2643-6809},
	url = {https://dl.acm.org/doi/10.1145/3491247},
	doi = {10.1145/3491247},
	abstract = {{\textless}p{\textgreater} We propose an IR for quantum computing that directly exposes quantum and classical data dependencies for the purpose of optimization. The {\textless}italic{\textgreater}Quantum Intermediate Representation for Optimization{\textless}/italic{\textgreater} (QIRO) consists of two dialects, one input dialect and one that is specifically tailored to enable quantum-classical co-optimization. While the first employs a perhaps more intuitive memory-semantics (quantum operations act on qubits via side-effects), the latter uses value-semantics (operations consume and produce states) to integrate quantum dataflow in the IR’s Static Single Assignment (SSA) graph. Crucially, this allows for a host of optimizations that leverage dataflow analysis. We discuss how to map existing quantum programming languages to the input dialect and how to lower the resulting IR to the optimization dialect. We present a prototype implementation based on MLIR that includes several quantum-specific optimization passes. Our benchmarks show that significant improvements in resource requirements are possible even through static optimization. In contrast to circuit optimization at run time, this is achieved while incurring only a small constant overhead in compilation time, making this a compelling approach for quantum program optimization at application scale. {\textless}/p{\textgreater}},
	journal = {ACM Transactions on Quantum Computing},
	author = {Ittah, David and Häner, Thomas and Kliuchnikov, Vadym and Hoefler, Torsten},
	month = feb,
	year = {2022},
	pages = {1--29},
}

@inproceedings{griffin_quantum_2021,
	address = {Chicago, IL, USA},
	title = {Quantum {Computing} for {Supply} {Chain} {Finance}},
	isbn = {978-1-66541-683-2},
	url = {https://ieeexplore.ieee.org/document/9592468/},
	doi = {10.1109/SCC53864.2021.00066},
	booktitle = {2021 {IEEE} {International} {Conference} on {Services} {Computing} ({SCC})},
	publisher = {IEEE},
	author = {Griffin, Paul and Sampat, Ritesh},
	month = sep,
	year = {2021},
	keywords = {credit, quantum computing, supply chainfinance, trade finance},
	pages = {456--459},
}

@book{qbook-qiskit,
	address = {Online},
	title = {Qiskit {Textbook}},
	url = {https://qiskit.org/textbook/},
	publisher = {IBM},
	author = {{IBM Quantum}},
	year = {2022},
}

@article{huang_quantum_2022,
	title = {Quantum advantage in learning from experiments},
	volume = {376},
	issn = {0036-8075},
	url = {https://www.science.org/doi/10.1126/science.abn7293},
	doi = {10.1126/science.abn7293},
	abstract = {Quantum technology promises to revolutionize how we learn about the physical world. An experiment that processes quantum data with a quantum computer could have substantial advantages over conventional experiments in which quantum states are measured and outcomes are processed with a classical computer. We proved that quantum machines could learn from exponentially fewer experiments than the number required by conventional experiments. This exponential advantage is shown for predicting properties of physical systems, performing quantum principal component analysis, and learning about physical dynamics. Furthermore, the quantum resources needed for achieving an exponential advantage are quite modest in some cases. Conducting experiments with 40 superconducting qubits and 1300 quantum gates, we demonstrated that a substantial quantum advantage is possible with today’s quantum processors.},
	number = {6598},
	journal = {Science},
	author = {Huang, Hsin-Yuan and Broughton, Michael and Cotler, Jordan and Chen, Sitan and Li, Jerry and Mohseni, Masoud and Neven, Hartmut and Babbush, Ryan and Kueng, Richard and Preskill, John and McClean, Jarrod R.},
	month = jun,
	year = {2022},
	pmid = {35679419},
	note = {arXiv: 2112.00778},
	pages = {1182--1186},
}

@article{wehner_quantum_2018,
	title = {Quantum internet: {A} vision for the road ahead},
	volume = {362},
	issn = {10959203},
	doi = {10.1126/science.aam9288},
	abstract = {The internet-a vast network that enables simultaneous long-range classical communication-has had a revolutionary impact on our world. The vision of a quantum internet is to fundamentally enhance internet technology by enabling quantum communication between any two points on Earth. Such a quantum internet may operate in parallel to the internet that we have today and connect quantum processors in order to achieve capabilities that are provably impossible by using only classical means. Here, we propose stages of development toward a full-blown quantum internet and highlight experimental and theoretical progress needed to attain them.},
	number = {6412},
	journal = {Science},
	author = {Wehner, Stephanie and Elkouss, David and Hanson, Ronald},
	year = {2018},
	pmid = {30337383},
}

@inproceedings{caleffi_quantum_2018,
	title = {Quantum internet: {From} communication to distributed computing!: {Invited} paper},
	isbn = {1-4503-5711-3},
	doi = {10.1145/3233188.3233224},
	abstract = {In this invited paper, the authors discuss the exponential computing speed-up achievable by interconnecting quantum computers through a quantum internet. They also identify key future research challenges and open problems for quantum internet design and deployment.},
	booktitle = {Proceedings of the 5th {ACM} {International} {Conference} on {Nanoscale} {Computing} and {Communication}, {NANOCOM} 2018},
	publisher = {Association for Computing Machinery, Inc},
	author = {Caleffi, Marcello and Cacciapuoti, Angela Sara and Bianchi, Giuseppe},
	month = sep,
	year = {2018},
	keywords = {Distributed Quantum Computing, Quantum Cloud, Quantum Communications, Quantum Internet, Quantum Networks},
}

@article{cacciapuoti_quantum_2020,
	title = {Quantum {Internet}: {Networking} {Challenges} in {Distributed} {Quantum} {Computing}},
	volume = {34},
	issn = {1558156X},
	doi = {10.1109/MNET.001.1900092},
	abstract = {The Quantum Internet, a network interconnecting remote quantum devices through quantum links in synergy with classical ones, is envisioned as the final stage of the quantum revolution, opening fundamentally new communications and computing capabilities. But the Quantum Internet is governed by the laws of quantum mechanics. Phenomena with no counterpart in classical networks, such as no-cloning, quantum measurement, entanglement and quantum teleportation, impose new challenging constraints for network design. Specifically, classical network functionalities are based on the assumption that classical information can be safely read and copied. However, this assumption does not hold in the Quantum Internet. As a consequence, its design requires a major network-paradigm shift to harness the quantum mechanics specificities. The goal of this work is to shed light on the challenges and open problems of Quantum Internet design. We first introduce some basic knowledge of quantum mechanics, needed to understand the differences between a classical and a quantum network. Then, we introduce quantum teleportation as the key strategy for transmitting quantum information without physically transferring the particle that stores the quantum information or violating the principles of quantum mechanics. Finally, the key research challenges to design quantum communication networks are discussed.},
	number = {1},
	journal = {IEEE Network},
	author = {Cacciapuoti, Angela Sara and Caleffi, Marcello and Tafuri, Francesco and Cataliotti, Francesco Saverio and Gherardini, Stefano and Bianchi, Giuseppe},
	year = {2020},
	note = {arXiv: 1810.08421},
	keywords = {★},
	pages = {137--143},
}

@article{li_quantum_2021,
	title = {Quantum random number generator using a cloud superconducting quantum computer based on source-independent protocol},
	volume = {11},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-03286-9},
	doi = {10.1038/s41598-021-03286-9},
	abstract = {Quantum random number generator (QRNG) relies on the intrinsic randomness of quantum mechanics to produce true random numbers which are important in information processing tasks. Due to the presence of the superposition state, a quantum computer can be used as a true random number generator. However, in practice, the implementation of the quantum computer is subject to various noise sources, which affects the randomness of the generated random numbers. To solve this problem, we propose a scheme based on the quantum computer which is motivated by the source-independent QRNG scheme in optics. By using a method to estimate the upper bound of the superposition state preparation error, the scheme can provide certified randomness in the presence of readout errors. To increase the generation rate of random bits, we also provide a parameter optimization method with a finite data size. In addition, we experimentally demonstrate our scheme on the cloud superconducting quantum computers of IBM.},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Li, Yuanhao and Fei, Yangyang and Wang, Weilong and Meng, Xiangdong and Wang, Hong and Duan, Qianheng and Ma, Zhi},
	month = dec,
	year = {2021},
	pages = {23873},
}

@article{pedone_quantum_2022,
	title = {Quantum {Key} {Distribution} in {Kubernetes} {Clusters}},
	volume = {14},
	issn = {1999-5903},
	url = {https://www.mdpi.com/1999-5903/14/6/160},
	doi = {10.3390/fi14060160},
	abstract = {Quantum Key Distribution (QKD) represents a reasonable countermeasure to the advent of Quantum Computing and its impact on current public-key cryptography. So far, considerable efforts have been devoted to investigate possible application scenarios for QKD in several domains such as Cloud Computing and NFV. This paper extends a previous work whose main objective was to propose a new software stack, the Quantum Software Stack (QSS), to integrate QKD into software-deﬁned infrastructures. The contribution of this paper is twofold: enhancing the previous work adding functionalities to the ﬁrst version of the QSS, and presenting a practical integration of the QSS in Kubernetes, which is the de-facto standard for container orchestration.},
	language = {en},
	number = {6},
	urldate = {2022-08-30},
	journal = {Future Internet},
	author = {Pedone, Ignazio and Lioy, Antonio},
	month = may,
	year = {2022},
	keywords = {Quantum Cryptography, Quantum Key Distribution, software-defined infrastructures},
	pages = {160},
}

@book{ganguly_quantum_2021,
	address = {Berkeley, CA},
	title = {Quantum {Machine} {Learning}: {An} {Applied} {Approach}},
	isbn = {978-1-4842-7097-4},
	url = {https://link.springer.com/10.1007/978-1-4842-7098-1},
	publisher = {Apress},
	author = {Ganguly, Santanu},
	year = {2021},
	doi = {10.1007/978-1-4842-7098-1},
}

@article{tan_quantum_2016,
	title = {Quantum private comparison protocol with cloud quantum computing},
	volume = {28},
	issn = {15320626},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.3490},
	doi = {10.1002/cpe.3490},
	abstract = {HPCTOOLKIT is an integrated suite of tools that supports measurement, analysis, attribution, and presentation of application performance for both sequential and parallel programs. HPCTOOLKIT can pinpoint and quantify scalability bottlenecks in fully optimized parallel programs with a measurement overhead of only a few percent. Recently, new capabilities were added to HPCTOOLKIT for collecting call path profiles for fully optimized codes without any compiler support, pinpointing and quantifying bottlenecks in multithreaded programs, exploring performance information and source code using a new user interface, and displaying hierarchical space-time diagrams based on traces of asynchronous call path samples. This paper provides an overview of HPCTOOLKIT and illustrates its utility for performance analysis of parallel applications. Copyright © 2009 John Wiley \& Sons, Ltd.},
	number = {10},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Tan, Xiaoqing and Zhang, Xiaoqian},
	month = jul,
	year = {2016},
	keywords = {Binary analysis, Call path profiling, Execution monitoring, Performance tools, Tracing, entanglement swapping, entangle–measure attack, fidelity, intercept-and-resend attack, intercept–measure–resend attack, quantum private comparison},
	pages = {3006--3020},
}

@article{huang_quantum_2021,
	title = {Quantum random number cloud platform},
	volume = {7},
	issn = {2056-6387},
	url = {http://www.nature.com/articles/s41534-021-00442-x},
	doi = {10.1038/s41534-021-00442-x},
	abstract = {Randomness lays the foundation for information security. Quantum random number generation based on various quantum principles has been proposed to provide true randomness in the last two decades. We integrate four different types of quantum random number generators on the Alibaba Cloud servers to enhance cybersecurity. Post-processing modules are integrated into the quantum platform to extract true random numbers. We employ improved authentication protocols where original pseudo-random numbers are replaced with quantum ones. Users from the Alibaba Cloud, such as Ant Financial and Smart Access Gateway, request random numbers from the quantum platform for various cryptographic tasks. For cloud services demanding the highest security, such as Alipay at Ant Financial, we combine the random numbers from four quantum devices by XOR the outputs to enhance practical security. The quantum platform has been continuously run for more than a year.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Huang, Leilei and Zhou, Hongyi and Feng, Kai and Xie, Chongjin},
	month = dec,
	year = {2021},
	keywords = {Usman, ★},
	pages = {107},
}

@article{arute_quantum_2019,
	title = {Quantum supremacy using a programmable superconducting processor},
	volume = {574},
	issn = {14764687},
	doi = {10.1038/s41586-019-1666-5},
	abstract = {The promise of quantum computers is that certain computational tasks might be executed exponentially faster on a quantum processor than on a classical processor1. A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits2–7 to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 253 (about 1016). Measurements from repeated experiments sample the resulting probability distribution, which we verify using classical simulations. Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit a million times—our benchmarks currently indicate that the equivalent task for a state-of-the-art classical supercomputer would take approximately 10,000 years. This dramatic increase in speed compared to all known classical algorithms is an experimental realization of quantum supremacy8–14 for this specific computational task, heralding a much-anticipated computing paradigm.},
	number = {7779},
	journal = {Nature},
	author = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G.S.L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandrà, Salvatore and McClean, Jarrod R. and McEwen, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
	month = oct,
	year = {2019},
	pmid = {31645734},
	note = {Publisher: Nature Publishing Group},
	keywords = {Google},
	pages = {505--510},
}

@article{zhou_quantum_2018,
	title = {Quantum technique for access control in cloud computing {II}: {Encryption} and key distribution},
	volume = {103},
	issn = {10958592},
	url = {https://doi.org/10.1016/j.jnca.2017.11.012},
	doi = {10.1016/j.jnca.2017.11.012},
	abstract = {This is the second paper of the series of papers dealing with access control problems in cloud computing by adopting quantum techniques. In this paper we study the application of quantum encryption and quantum key distribution in the access control problem. We formalize our encryption scheme and protocol for key distribution in the setting of categorical quantum mechanics (CQM). The graphical language of CQM is used in this paper. The quantum scheme/protocol we propose possesses several advantages over existing schemes/protocols proposed in the state of the art for the same purpose. They are informationally secure and implementable by the current technology.},
	number = {December 2017},
	journal = {Journal of Network and Computer Applications},
	author = {Zhou, Lu and Wang, Quanlong and Sun, Xin and Kulicki, Piotr and Castiglione, Arcangelo},
	year = {2018},
	note = {Publisher: Elsevier Ltd},
	keywords = {Access control, Categorical quantum mechanics, Quantum encryption, Quantum key distribution},
	pages = {178--184},
}

@article{chen_quantum_2021,
	title = {Quantum {Token} for {Network} {Authentication}},
	doi = {10.1109/ICWS53863.2021.00095},
	abstract = {Classical token-based authentication can play a significant role in the web security check without accessing the database. For example, JSON Web Tokens (JWT) has been used to support scenarios such as single-sign-on. However, with the development of quantum computer, the security of JWT relying on the RSA algorithm would be compromised. Therefore, we propose a protocol to realize network authentication utilizing quantum token. Inspired by the structure of classical JWT, the structure of quantum token also consists of three parts: header, payload and quantum information. After the user logs in successfully, the quantum token can be issued by the server. If the user presents the quantum token to access again during the validity period, the server can verify whether the quantum token is valid. Our quantum token protocol can detect eavesdropping and achieve identity authentication. We also conduct a security analysis of the proposed protocol by addressing possible motives of an Eavesdropper and conclude the approach to be resilient against a broad range of attacks.},
	journal = {Proceedings - 2021 IEEE International Conference on Web Services, ICWS 2021},
	author = {Chen, Huimin and Jia, Hengyue and Wu, Xia and Wang, Xiuli and Wang, Maoning},
	year = {2021},
	note = {Publisher: IEEE
ISBN: 9781665416818},
	keywords = {JWT token, detect eavesdropping, identity authentication, jwt, network authentication, quantum token},
	pages = {688--692},
}

@article{qsdk-quingo,
	title = {Quingo: {A} {Programming} {Framework} for {Heterogeneous} {Quantum}-{Classical} {Computing} with {NISQ} {Features}},
	volume = {2},
	issn = {2643-6809},
	doi = {10.1145/3483528},
	abstract = {The increasing control complexity of Noisy Intermediate-Scale Quantum (NISQ) systems underlines the necessity of integrating quantum hardware with quantum software. While mapping heterogeneous quantum-classical computing (HQCC) algorithms to NISQ hardware for execution, we observed a few dissatisfactions in quantum programming languages (QPLs), including difficult mapping to hardware, limited expressiveness, and counter-intuitive code. In addition, noisy qubits require repeatedly performed quantum experiments, which explicitly operate low-level configurations, such as pulses and timing of operations. This requirement is beyond the scope or capability of most existing QPLs.We summarize three execution models to depict the quantum-classical interaction of existing QPLs. Based on the refined HQCC model, we propose the Quingo framework to integrate and manage quantum-classical software and hardware to provide the programmability over HQCC applications and map them to NISQ hardware. We propose a six-phase quantum program life-cycle model matching the refined HQCC model, which is implemented by a runtime system. We also propose the Quingo programming language, an external domain-specific language highlighting timer-based timing control and opaque operation definition, which can be used to describe quantum experiments. We believe the Quingo framework could contribute to the clarification of key techniques in the design of future HQCC systems.},
	number = {4},
	journal = {ACM Transactions on Quantum Computing},
	author = {Fu, X. and Yu, Jintao and Su, Xing and Jiang, Hanru and Wu, Hua and {et. al.}},
	year = {2021},
	note = {arXiv: 2009.01686},
	pages = {1--37},
}

@article{almudever_realizing_2020,
	title = {Realizing {Quantum} {Algorithms} on {Real} {Quantum} {Computing} {Devices}},
	doi = {10.23919/DATE48585.2020.9116240},
	abstract = {Quantum computing is currently moving from an academic idea to a practical reality. Quantum computing in the cloud is already available and allows users from all over the world to develop and execute real quantum algorithms. However, companies which are heavily investing in this new technology such as Google, IBM, Rigetti, Intel, IonQ, and Xanadu follow diverse technological approaches. This led to a situation where we have substantially different quantum computing devices available thus far. They mostly differ in the number and kind of qubits and the connectivity between them. Because of that, various methods for realizing the intended quantum functionality on a given quantum computing device are available. This paper provides an introduction and overview into this domain and describes corresponding methods, also referred to as compilers, mappers, synthesizers, transpilers, or routers.},
	journal = {Proceedings of the 2020 Design, Automation and Test in Europe Conference and Exhibition, DATE 2020},
	author = {Almudever, Carmen G. and Lao, Lingling and Wille, Robert and Guerreschi, Gian G.},
	year = {2020},
	note = {arXiv: 2007.01000
ISBN: 9783981926347},
	pages = {864--872},
}

@article{usman_recent_2021,
	title = {Recent progress in atomistic modelling and simulations of donor spin qubits in silicon},
	volume = {193},
	issn = {09270256},
	url = {https://doi.org/10.1016/j.commatsci.2021.110280},
	doi = {10.1016/j.commatsci.2021.110280},
	abstract = {Electron or nuclear spins associated with dopant atoms, such as phosphorus impurities in silicon (Si:P), have been shown to form excellent qubits with promising potential for scale-up towards a fault-tolerant quantum computer architecture. The remarkable progress in the design and characterisation of Si:P qubits and quantum gates has been led by recent experimental demonstrations. Equally importantly, advances in theoretical modelling and simulations over a number of years have underpinned the experimental efforts through the fundamental understanding of dopant physics and by providing crucial interpretation of the experimental evidence. This brief review article provides highlights of our research on developing atomistic theoretical methods and their application to the understanding, characterisation and scale-up of Si:P qubits in silicon. We have established a state-of-the-art theoretical framework which is capable of performing electronic structure simulations over millions of atoms. This includes a comprehensive set of central-cell corrections within atomistic tight-binding theory to simulate dopant energy spectra and electronic wave functions with high precision. When integrated with Bardeen's tunnelling formalism and Chen's derivative rule, the theoretical simulations were able to reproduce the measured spatially resolved scanning tunnelling microscope (STM) images of dopant wave functions, providing an unprecedented access to the dopant physics in silicon. A systematic examination of the STM image features (brightness and symmetry) allowed pinpointing of the dopant atom positions in silicon lattice with an exact atom precision and for dopant depths up to 5 nm below the silicon surface. The scale-up of the metrology technique was demonstrated by training a machine learning algorithm such as convolutional neural network. For the design and implementation of high-fidelity two-qubit quantum gates, we investigated exchange interaction between dopant pairs and showed that the application of a small lattice strain could provide a full control in the presence of one-lattice site donor position variations. The state-of-the-art computational capability developed by our team is a culmination of more than five years of research efforts – it has been well-benchmarked against several different experimental measurements and is expected to play an important role in design and characterisation of quantum gates and scale-up architectures in the coming years.},
	number = {January},
	journal = {Computational Materials Science},
	author = {Usman, Muhammad},
	month = jun,
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {Machine learning, Quantum computer architecture, Scanning tunnelling microscope, Silicon donor qubits, Tight-binding},
	pages = {110280},
}

@article{qu_secure_2022,
	title = {Secure quantum fog computing model based on blind quantum computation},
	volume = {13},
	issn = {18685145},
	url = {https://doi.org/10.1007/s12652-021-03402-7},
	doi = {10.1007/s12652-021-03402-7},
	abstract = {As a computing service platform closer to users, fog computing has many advantages such as extremely low latency, good mobility, accurate location perception and wide distribution. It has developed rapidly in recent years. However, due to the wide distribution of fog nodes, complex network environments, and limited resources, the security of fog nodes is vulnerable to a variety of attacks, such as denial of service and abuse of resources. In order to effectively deal with these attacks, this paper proposes a quantum fog computing model based on blind quantum computation and verifiable quantum secret sharing. The model mainly relies on blind quantum computation to realize the security joint operation characteristics of multiple fog nodes, and the identity verifiable and channel detection protection features provided by the quantum secret sharing protocol, which can not only efficiently perform the functions of the classic fog computing, but also guarantee the security of information transmission and data calculation. Through the complete security analysis, the new quantum fog computing model proposed in this paper can effectively resist on a variety of fog computing attacks, thus achieving information security protection in both the content and process of fog computing.},
	number = {8},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Qu, Zhiguo and Wang, Kunyi and Zheng, Min},
	year = {2022},
	note = {Publisher: Springer Berlin Heidelberg
ISBN: 1265202103402},
	keywords = {Blind quantum computation, Fog computing, Quantum identity authentication, Quantum secret sharing, Usman, ★},
	pages = {3807--3817},
}

@article{kumar_securing_2021,
	title = {Securing the future internet of things with post-quantum cryptography},
	doi = {10.1002/spy2.200},
	number = {November},
	author = {Kumar, Adarsh and Ottaviani, Carlo and Gill, Sukhpal Singh and Buyya, Rajkumar},
	year = {2021},
	keywords = {centre for quantum technologies, cryptography, department of computer science, internet of things, post-quantum cryptography, quantum computing, security, york},
	pages = {1--10},
}

@article{qsdk-strawberry-fields,
	title = {Strawberry {Fields}: {A} {Software} {Platform} for {Photonic} {Quantum} {Computing}},
	volume = {3},
	issn = {2521-327X},
	url = {https://quantum-journal.org/papers/q-2019-03-11-129/},
	doi = {10.22331/q-2019-03-11-129},
	abstract = {We introduce Strawberry Fields, an open-source quantum programming architecture for light-based quantum computers, and detail its key features. Built in Python, Strawberry Fields is a full-stack library for design, simulation, optimization, and quantum machine learning of continuous-variable circuits. The platform consists of three main components: (i) an API for quantum programming based on an easy-to-use language named Blackbird; (ii) a suite of three virtual quantum computer backends, built in NumPy and TensorFlow, each targeting specialized uses; and (iii) an engine which can compile Blackbird programs on various backends, including the three built-in simulators, and - in the near future - photonic quantum information processors. The library also contains examples of several paradigmatic algorithms, including teleportation, (Gaussian) boson sampling, instantaneous quantum polynomial, Hamiltonian simulation, and variational quantum circuit optimization.},
	journal = {Quantum},
	author = {Killoran, Nathan and Izaac, Josh and Quesada, Nicolás and Bergholm, Ville and Amy, Matthew and Weedbrook, Christian},
	month = mar,
	year = {2019},
	pages = {129},
}

@inproceedings{Weder2020QuantumLifecycle,
	title = {The {Quantum} software lifecycle},
	volume = {1},
	doi = {10.1145/3412451.3428497},
	abstract = {Quantum computing is an emerging paradigm that enables to solve a variety of problems more efficiently than it is possible on classical computers. As the first quantum computers are available, quantum algorithms can be implemented and executed on real quantum hardware. However, the capabilities of today's quantum computers are very limited and quantum computations are always disturbed by some error. Thus, further research is needed to develop or improve quantum algorithms, quantum computers, or required software tooling support. Due to the interdisciplinary nature of quantum computing, a common understanding of how to develop and execute a quantum software application is needed. However, there is currently no methodology or lifecycle comprising all relevant phases that can occur during the development and execution process. Hence, in this paper, we introduce the quantum software lifecycle consisting of ten phases a gate-based quantum software application should go through. We analyze the purpose of each phase, the available methods and tools that can be applied, and the open problems or research questions. Therefore, the lifecycle can be used as a baseline for discussions and future research.},
	booktitle = {The 1st {ACM} {SIGSOFT} {Workshop} on {Architectures} and {Paradigms} for {Engineering} {Quantum} {Software}},
	publisher = {ACM},
	author = {Weder, Benjamin and Barzen, Johanna and Leymann, Frank and Salm, Marie and Vietz, Daniel},
	year = {2020},
	note = {Issue: 1},
	keywords = {NISQ, Quantum Applications, Quantum Computing, Quantum Software Development, Software Engineering, Software Lifecycle, ★},
	pages = {2--9},
}

@article{piattini_talavera_2020,
	title = {The {Talavera} manifesto for quantum software engineering and programming},
	volume = {2561},
	issn = {16130073},
	abstract = {This paper presents the Talavera Manifesto for quantum software engineering and programming. This manifesto collects some principles and commitments about the quantum software engineering and programming field, as well as some calls for action. This is the result of the discussion and different viewpoints of academia and industry practitioners who joined at the first International Workshop on QuANtum SoftWare Engineering \& pRogramming (QANSWER).},
	journal = {CEUR Workshop Proceedings},
	author = {Piattini, Mario and Peterssen, Guido and Pérez-Castillo, Ricardo and Hevia, Jose Luis and Serrano, Manuel A. and Hernández, Guillermo and de Guzmán, Ignacio García Rodríguez and Paradela, Claudio Andrés and Polo, Macario and Murina, Ezequiel and Jiménez, Luis and Marqueño, Juan Carlos and Gallego, Ramsés and Tura, Jordi and Phillipson, Frank and Murillo, Juan M. and Niño, Alfonso and Rodríguez, Moisés},
	year = {2020},
	keywords = {Manifesto, Quantum Computing, Quantum Software Engineering, Talavera},
	pages = {1--5},
}

@article{wille_tools_2022,
	title = {Tools for {Quantum} {Computing} {Based} on {Decision} {Diagrams}},
	volume = {3},
	issn = {2643-6809},
	doi = {10.1145/3491246},
	abstract = {With quantum computers promising advantages even in the near-term NISQ era, there is a lively community that develops software and toolkits for the design of corresponding quantum circuits. Although the underlying problems are different, expertise from the design automation community, which developed sophisticated design solutions for the conventional realm in the past decades, can help here. In this respect, decision diagrams provide a promising foundation for tackling many design tasks such as simulation, synthesis, and verification of quantum circuits. However, users of the corresponding tools often do not have a proper background or an intuition about how these methods based on decision diagrams work and what their strengths and limits are. In this work, we first review the concepts of how decision diagrams can be employed, e.g., for the simulation and verification of quantum circuits. Afterwards, in an effort to make decision diagrams for quantum computing more accessible, we then present a visualization tool for quantum decision diagrams, which allows users to explore the behavior of decision diagrams in the design tasks mentioned above. Finally, we present decision diagram-based tools for simulation and verification of quantum circuits using the methods discussed above as part of the open-source Munich Quantum Toolkit (MQT)—a set of tools for quantum computing developed at the Technical University of Munich and the Johannes Kepler University Linz and released under the MIT license. More information about the corresponding tools is available at https://github.com/cda-tum/ddsim . By this, we provide an introduction of the concepts and tools for potential users who would like to work with them as well as potential developers aiming to extend them.},
	number = {3},
	journal = {ACM Transactions on Quantum Computing},
	author = {Wille, Robert and Hillmich, Stefan and Burgholzer, Lukas},
	year = {2022},
	pages = {1--17},
}

@article{pedone_towards_2021,
	title = {Towards a complete software stack to integrate {Quantum} {Key} {Distribution} in a cloud environment},
	volume = {9},
	issn = {21693536},
	doi = {10.1109/ACCESS.2021.3102313},
	abstract = {The coming advent of Quantum Computing promises to jeopardize current communications security, undermining the effectiveness of traditional public-key based cryptography. Different strategies (Post-Quantum or Quantum Cryptography) have been proposed to address this problem. Many techniques and algorithms based on quantum phenomena have been presented in recent years; the most relevant example is the introduction of Quantum Key Distribution (QKD). This approach allows to exchange cryptographic keys among parties and does not suffer from the development of quantum computation. Problems arise when this technique has to be deployed and combined with modern distributed infrastructures that heavily depend on cloud and virtualisation paradigms. This paper addresses this issue by presenting a new software stack that effortlessly introduces QKD in such environments and involves a simulation tool for Quantum Key Distribution. This software stack allows for agnostic integration, monitoring, and management of QKD, independent from a specific vendor or technology. Furthermore, a QKD simulator is presented, designed, and tested. This latter contribution is suitable as a low-level testing device, as an independent software module to check QKD protocols, and as a testbed to identify future practical enhancements.},
	journal = {IEEE Access},
	author = {Pedone, Ignazio and Atzeni, Andrea and Canavese, Daniele and Lioy, Antonio},
	year = {2021},
	keywords = {Cloud computing, Protocols, QKD, Quantum Communication, Quantum Cryptography, Quantum computing, Quantum cryptography, Security, Software, Softwarised Infrastructures, Testing, ★},
}

@article{arjona_triggerflow_2021,
	title = {Triggerflow: {Trigger}-based orchestration of serverless workflows},
	volume = {124},
	issn = {0167739X},
	doi = {10.1016/j.future.2021.06.004},
	abstract = {As more applications are being moved to the Cloud thanks to serverless computing, it is increasingly necessary to support the native life cycle execution of those applications in the data center. But existing cloud orchestration systems either focus on short-running workflows (like IBM Composer or Amazon Step Functions Express Workflows) or impose considerable overheads for synchronizing massively parallel jobs (Azure Durable Functions, Amazon Step Functions). None of them are open systems enabling extensible interception and optimization of custom workflows. We present Triggerflow: an extensible Trigger-based Orchestration architecture for serverless workflows. We demonstrate that Triggerflow is a novel serverless building block capable of constructing different reactive orchestrators (State Machines, Directed Acyclic Graphs, Workflow as code, Federated Learning orchestrator). We also validate that it can support high-volume event processing workloads, auto-scale on demand with scale down to zero when not used, and transparently guarantee fault tolerance and efficient resource usage when orchestrating long running scientific workflows.},
	journal = {Future Generation Computer Systems},
	author = {Arjona, Aitor and López, Pedro García and Sampé, Josep and Slominski, Aleksander and Villard, Lionel},
	month = nov,
	year = {2021},
	note = {arXiv: 2106.00583
Publisher: Elsevier B.V.},
	keywords = {Event-based, Orchestration, Serverless},
	pages = {215--229},
}

@article{li_verifiable_2020,
	title = {Verifiable quantum cloud computation scheme based on blind computation},
	volume = {8},
	issn = {21693536},
	doi = {10.1109/ACCESS.2020.2982090},
	abstract = {In this paper, we propose a verifiable quantum cloud computation scheme based on blind quantum computation which effectively solves the privacy and verifiability problem for clients when interfacing with quantum computer in cloud computation. Different from most of present works (e.g. using brickwork state as resource state), we use cluster state with grid distributed structure to realize the universal and cryptographic calculation which is more suitable for the distributed architecture of cloud computation. Throughout the process, the random key is always held by the client that effectively promote the confidentiality. Moreover, we propose a new method to make the client Alice verifies the correctness of the results through only classical calculation. Security analysis shows that cloud server Bob can only obtain the dimension of input state and cluster state from the calculation, but cannot obtain any information of the input, the output and the algorithm. Dishonest Bob will be detected during the verification.},
	journal = {IEEE Access},
	author = {Li, Jing and Zhao, Yang and Yang, Yubo and Lin, Yongjun},
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Blind quantum computation, Cloud computation, Cluster state, Measurement-based quantum computation, ★},
	pages = {56921--56926},
}

@article{qcloud-rigetti,
	title = {A quantum-classical cloud platform optimized for variational hybrid algorithms},
	volume = {5},
	issn = {20589565},
	doi = {10.1088/2058-9565/ab7559},
	abstract = {In order to support near-term applications of quantum computing, a new compute paradigm has emerged-the quantum-classical cloud-in which quantum computers (QPUs) work in tandem with classical computers (CPUs) via a shared cloud infrastructure. In this work, we enumerate the architectural requirements of a quantum-classical cloud platform, and present a framework for benchmarking its runtime performance. In addition, we walk through two platform-level enhancements, parametric compilation and active qubit reset, that specifically optimize a quantum-classical architecture to support variational hybrid algorithms, the most promising applications of near-term quantum hardware. Finally, we show that integrating these two features into the Rigetti Quantum Cloud Services platform results in considerable improvements to the latencies that govern algorithm runtime.},
	number = {2},
	journal = {Quantum Science and Technology},
	author = {Karalekas, Peter J. and Tezak, Nikolas A. and Peterson, Eric C. and Ryan, Colm A. and Da Silva, Marcus P. and Smith, Robert S.},
	year = {2020},
	note = {arXiv: 2001.04449
Publisher: IOP Publishing},
	keywords = {cloud-based quantum computing, near-term quantum algorithms, quantum software engineering},
	pages = {0--13},
}

@article{song_verifiable_2020,
	title = {A verifiable (t, n) threshold quantum state sharing scheme on {IBM} quantum cloud platform},
	volume = {19},
	issn = {15731332},
	doi = {10.1007/s11128-020-02846-8},
	abstract = {In current verifiable quantum state sharing schemes, the dishonest behaviors of certain participants can be verified. However, these schemes have either high resource consumption, low verification efficiency or lack simulation implementations. To compensate for the shortcomings of current schemes, a new verifiable (t, n) threshold quantum state sharing scheme on the IBM quantum cloud platform is proposed. To reduce resource consumption, the proposed scheme prepares only one secret particle and then performs the Lagrange unitary operator on it, and the transformed secret particle is shared in the authorized subset of participants. To improve verification efficiency, the scheme performs the composite rotation unitary operator on the received message particle, such that it not only verifies the validity of the message particle but also reconstructs the original secret particle. The correctness of the proposed scheme is verified by not only mathematic proof but also experimental simulation on the IBM quantum cloud platform. Compared with the two other schemes based on the rotation unitary operator, the proposed scheme provides stronger verification security by using the private shadow key and rotation key. Compared with the two other schemes based on the verification mechanism, the proposed scheme has lower resource consumption and higher verification efficiency.},
	number = {9},
	journal = {Quantum Information Processing},
	author = {Song, Xiuli and Liu, Yanbing and Xiao, Min and Deng, Hongyao and Yang, Shuai},
	month = aug,
	year = {2020},
	note = {Publisher: Springer},
	keywords = {(t, n) threshold, Lagrange unitary operator, Quantum cloud platform, Rotation unitary operator, Verifiable quantum state sharing, quantum security},
}

@article{dahlberg_link_2019,
	title = {A link layer protocol for quantum networks},
	doi = {10.1145/3341302.3342070},
	abstract = {Quantum communication brings radically new capabilities that are provably impossible to attain in any classical network. Here, we take the first step from a physics experiment to a quantum internet system. We propose a functional allocation of a quantum network stack, and construct the first physical and link layer protocols that turn ad-hoc physics experiments producing heralded entanglement between quantum processors into a well-defined and robust service. This lays the groundwork for designing and implementing scalable control and application protocols in platform-independent software. To design our protocol, we identify use cases, as well as fundamental and technological design considerations of quantum network hardware, illustrated by considering the state-of-the-art quantum processor platform available to us (Nitrogen-Vacancy (NV) centers in diamond). Using a purpose built discrete-event simulator for quantum networks, we examine the robustness and performance of our protocol using extensive simulations on a supercomputing cluster. We perform a full implementation of our protocol in our simulator, where we successfully validate the physical simulation model against data gathered from the NV hardware. We first observe that our protocol is robust even in a regime of exaggerated losses of classical control messages with only little impact on the performance of the system. We proceed to study the performance of our protocols for 169 distinct simulation scenarios, including trade-offs between traditional performance metrics such as throughput, and the quality of entanglement. Finally, we initiate the study of quantum network scheduling strategies to optimize protocol performance for different use cases.},
	number = {Figure 1},
	journal = {SIGCOMM 2019 - Proceedings of the 2019 Conference of the ACM Special Interest Group on Data Communication},
	author = {Dahlberg, Axel and Skrzypczyk, Matthew and Coopmans, Tim and Wubben, Leon and Rozpdek, Filip and Pompili, Matteo and Stolk, Arian and Pawelczak, Przemyslaw and Knegjens, Robert and De Oliveira Filho, Julio and Hanson, Ronald and Wehner, Stephanie},
	year = {2019},
	note = {arXiv: 1903.09778
ISBN: 9781450359566},
	keywords = {Link Layer, Quantum Internet, Quantum Networks},
	pages = {159--173},
}

@article{helsen_benchmarking_2021,
	title = {A benchmarking procedure for quantum networks},
	url = {http://arxiv.org/abs/2103.01165},
	abstract = {We propose network benchmarking: a procedure to efficiently benchmark the quality of a quantum network link connecting quantum processors in a quantum network. This procedure is based on the standard randomized benchmarking protocol and provides an estimate for the fidelity of a quantum network link. We provide statistical analysis of the protocol as well as a simulated implementation inspired by NV-center systems using Netsquid, a special purpose simulator for noisy quantum networks.},
	author = {Helsen, Jonas and Wehner, Stephanie},
	year = {2021},
	note = {arXiv: 2103.01165},
	pages = {1--10},
}

@article{proctor_scalable_2022,
	title = {Scalable {Randomized} {Benchmarking} of {Quantum} {Computers} {Using} {Mirror} {Circuits}},
	volume = {129},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.129.150502},
	doi = {10.1103/PhysRevLett.129.150502},
	abstract = {The performance of quantum gates is often assessed using some form of randomized benchmarking. However, the existing methods become infeasible for more than approximately five qubits. Here we show how to use a simple and customizable class of circuits -- randomized mirror circuits -- to perform scalable, robust, and flexible randomized benchmarking of Clifford gates. We show that this technique approximately estimates the infidelity of an average many-qubit logic layer, and we use simulations of up to 225 qubits with physically realistic error rates in the range 0.1-1\% to demonstrate its scalability. We then use up to 16 physical qubits of a cloud quantum computing platform to demonstrate that our technique can reveal and quantify crosstalk errors in many-qubit circuits.},
	number = {15},
	journal = {Physical Review Letters},
	author = {Proctor, Timothy and Seritan, Stefan and Rudinger, Kenneth and Nielsen, Erik and Blume-Kohout, Robin and Young, Kevin},
	month = oct,
	year = {2022},
	keywords = {★},
	pages = {150502},
}

@article{martiel_benchmarking_2021,
	title = {Benchmarking quantum co-processors in an application-centric, hardware-agnostic and scalable way},
	url = {http://arxiv.org/abs/2102.12973},
	doi = {10.1109/TQE.2021.3090207},
	abstract = {Existing protocols for benchmarking current quantum co-processors fail to meet the usual standards for assessing the performance of High-Performance-Computing platforms. After a synthetic review of these protocols -- whether at the gate, circuit or application level -- we introduce a new benchmark, dubbed Atos Q-score (TM), that is application-centric, hardware-agnostic and scalable to quantum advantage processor sizes and beyond. The Q-score measures the maximum number of qubits that can be used effectively to solve the MaxCut combinatorial optimization problem with the Quantum Approximate Optimization Algorithm. We give a robust definition of the notion of effective performance by introducing an improved approximation ratio based on the scaling of random and optimal algorithms. We illustrate the behavior of Q-score using perfect and noisy simulations of quantum processors. Finally, we provide an open-source implementation of Q-score that makes it easy to compute the Q-score of any quantum hardware.},
	author = {Martiel, Simon and Ayral, Thomas and Allouche, Cyril},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12973},
}

@article{helsen_matchgate_2022,
	title = {Matchgate benchmarking: {Scalable} benchmarking of a continuous family of many-qubit gates},
	volume = {6},
	issn = {2521327X},
	doi = {10.22331/Q-2022-02-21-657},
	abstract = {We propose a method to reliably and efficiently extract the fidelity of many-qubit quantum circuits composed of continuously parametrized two-qubit gates called matchgates. This method, which we call matchgate benchmarking, relies on advanced techniques from randomized benchmarking as well as insights from the representation theory of matchgate circuits. We argue the formal correctness and scalability of the protocol, and moreover deploy it to estimate the performance of matchgate circuits generated by two-qubit XY spin interactions on a quantum processor.},
	journal = {Quantum},
	author = {Helsen, Jonas and Nezami, Sepehr and Reagor, Matthew and Walter, Michael},
	year = {2022},
	note = {arXiv: 2011.13048
Publisher: Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
}

@article{kaewpuang_stochastic_2022,
	title = {Stochastic {Qubit} {Resource} {Allocation} for {Quantum} {Cloud} {Computing}},
	url = {http://arxiv.org/abs/2210.12343},
	abstract = {Quantum cloud computing is a promising paradigm for efficiently provisioning quantum resources (i.e., qubits) to users. In quantum cloud computing, quantum cloud providers provision quantum resources in reservation and on-demand plans for users. Literally, the cost of quantum resources in the reservation plan is expected to be cheaper than the cost of quantum resources in the on-demand plan. However, quantum resources in the reservation plan have to be reserved in advance without information about the requirement of quantum circuits beforehand, and consequently, the resources are insufficient, i.e., under-reservation. Hence, quantum resources in the on-demand plan can be used to compensate for the unsatisfied quantum resources required. To end this, we propose a quantum resource allocation for the quantum cloud computing system in which quantum resources and the minimum waiting time of quantum circuits are jointly optimized. Particularly, the objective is to minimize the total costs of quantum circuits under uncertainties regarding qubit requirement and minimum waiting time of quantum circuits. In experiments, practical circuits of quantum Fourier transform are applied to evaluate the proposed qubit resource allocation. The results illustrate that the proposed qubit resource allocation can achieve the optimal total costs.},
	author = {Kaewpuang, Rakpong and Xu, Minrui and Niyato, Dusit and Yu, Han and Xiong, Zehui and Kang, Jiawen},
	month = oct,
	year = {2022},
	note = {arXiv: 2210.12343},
}

@article{tilly_variational_2022,
	title = {The {Variational} {Quantum} {Eigensolver}: {A} review of methods and best practices},
	volume = {986},
	issn = {03701573},
	doi = {10.1016/j.physrep.2022.08.003},
	abstract = {The variational quantum eigensolver (or VQE), first developed by Peruzzo et al. (2014), has received significant attention from the research community in recent years. It uses the variational principle to compute the ground state energy of a Hamiltonian, a problem that is central to quantum chemistry and condensed matter physics. Conventional computing methods are constrained in their accuracy due to the computational limits facing exact modeling of the exponentially growing electronic wavefunction for these many-electron systems. The VQE may be used to model these complex wavefunctions in polynomial time, making it one of the most promising near-term applications for quantum computing. One important advantage is that variational algorithms have been shown to present some degree of resilience to the noise in the quantum hardware. Finding a path to navigate the relevant literature has rapidly become an overwhelming task, with many methods promising to improve different parts of the algorithm, but without clear descriptions of how the diverse parts fit together. The potential practical advantages of the algorithm are also widely discussed in the literature, but with varying conclusions. Despite strong theoretical underpinnings suggesting excellent scaling of individual VQE components, studies have pointed out that their various pre-factors could be too large to reach a quantum computing advantage over conventional methods. This review aims at disentangling the relevant literature to provide a comprehensive overview of the progress that has been made on the different parts of the algorithm, and to discuss future areas of research that are fundamental for the VQE to deliver on its promises. All the different components of the algorithm are reviewed in detail. These include the representation of Hamiltonians and wavefunctions on a quantum computer, the optimization process to find ground state energies, the post processing mitigation of quantum errors, and suggested best practices. We identify four main areas of future research: (1) optimal measurement schemes for reduction of circuit repetitions required; (2) large scale parallelization across many quantum computers; (3) ways to overcome the potential appearance of vanishing gradients in the optimization process for large systems, and how the number of iterations required for the optimization scales with system size; (4) the extent to which VQE suffers for quantum noise, and whether this noise can be mitigated in a tractable manner. The answers to these open research questions will determine the routes for the VQE to achieve quantum advantage as the quantum computing hardware scales up and as the noise levels are reduced.},
	journal = {Physics Reports},
	author = {Tilly, Jules and Chen, Hongxiang and Cao, Shuxiang and Picozzi, Dario and Setia, Kanav and Li, Ying and Grant, Edward and Wossnig, Leonard and Rungger, Ivan and Booth, George H. and Tennyson, Jonathan},
	month = nov,
	year = {2022},
	note = {arXiv: 2111.05176
Publisher: Elsevier B.V.},
	keywords = {Electronic structure, Many-body physics, Noisy intermediate scale quantum devices, Quantum chemistry, Quantum computing, Variational methods},
	pages = {1--128},
}

@inproceedings{swarup_task_2021,
	title = {Task scheduling in cloud using deep reinforcement learning},
	volume = {184},
	doi = {10.1016/j.procs.2021.03.016},
	abstract = {Cloud computing is an emerging technology used in many applications such as data analysis, storage, and Internet of Things (IoT). Due to the increasing number of users in the cloud and the IoT devices that are being integrated with the cloud, the amount of data generated by these users and these devices is increasing ceaselessly. Managing this data over the cloud is no longer an easy task. It is almost impossible to move all data to the cloud datacenters, and this will lead to excessive bandwidth usage, latency, cost, and energy consumption. This makes it evident that allocating resources to users' tasks is an essential quality feature in cloud computing. This is because it provides the customers or the users with high Quality of Service (QoS) with the best response time, and it also respects the established Service Level Agreement. Therefore, there is a great importance of efficient utilization of computing resources for which an optimal strategy for task scheduling is required. This paper focuses on the problem of task scheduling of cloud-based applications and aims to minimize the computational cost under resource and deadline constraints. Towards this end, we propose a clipped double deep Q-learning algorithm utilizing the target network and experience relay techniques, as we as using the reinforcement learning approach.},
	booktitle = {Procedia {Computer} {Science}},
	publisher = {Elsevier B.V.},
	author = {Swarup, Shashank and Shakshuki, Elhadi M. and Yasar, Ansar},
	year = {2021},
	note = {ISSN: 18770509},
	keywords = {Clipped double deep Q-learning (CDDQL), Computational cost, Deep reinforcement learning, Energy consumption, Task scheduling},
	pages = {42--51},
}

@article{zheng_deep_2022,
	title = {Deep {Reinforcement} {Learning}-{Based} {Workload} {Scheduling} for {Edge} {Computing}},
	volume = {11},
	issn = {2192113X},
	doi = {10.1186/s13677-021-00276-0},
	abstract = {Edge computing is a new paradigm for providing cloud computing capacities at the edge of network near mobile users. It offers an effective solution to help mobile devices with computation-intensive and delay-sensitive tasks. However, the edge of network presents a dynamic environment with large number of devices, high mobility of users, heterogeneous applications and intermittent traffic. In such environment, edge computing often suffers from unbalance resource allocation, which leads to task failure and affects system performance. To tackle this problem, we proposed a deep reinforcement learning(DRL)-based workload scheduling approach with the goal of balancing the workload, reducing the service time and the failed task rate. Meanwhile, We adopt Deep-Q-Network(DQN) algorithms to solve the complexity and high dimension of workload scheduling problem. Simulation results show that our proposed approach achieves the best performance in aspects of service time, virtual machine(VM) utilization, and failed tasks rate compared with other approaches. Our DRL-based approach can provide an efficient solution to the workload scheduling problem in edge computing.},
	number = {1},
	journal = {Journal of Cloud Computing},
	author = {Zheng, Tao and Wan, Jian and Zhang, Jilin and Jiang, Congfeng},
	month = dec,
	year = {2022},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Computation offloading, Deep reinforcement learning, Edge computing, Workload scheduling},
}

@inproceedings{murali_noise-adaptive_2019,
	title = {Noise-{Adaptive} {Compiler} {Mappings} for {Noisy} {Intermediate}-{Scale} {Quantum} {Computers}},
	isbn = {978-1-4503-6240-5},
	doi = {10.1145/3297858.3304075},
	abstract = {A massive gap exists between current quantum computing (QC) prototypes, and the size and scale required for many proposed QC algorithms. Current QC implementations are prone to noise and variability which affect their reliability, and yet with less than 80 quantum bits (qubits) total, they are too resource-constrained to implement error correction. The term Noisy Intermediate-Scale Quantum (NISQ) refers to these current and near-term systems of 1000 qubits or less. Given NISQ's severe resource constraints, low reliability, and high variability in physical characteristics such as coherence time or error rates, it is of pressing importance to map computations onto them in ways that use resources efficiently and maximize the likelihood of successful runs. This paper proposes and evaluates backend compiler approaches to map and optimize high-level QC programs to execute with high reliability on NISQ systems with diverse hardware characteristics. Our techniques all start from an LLVM intermediate representation of the quantum program (such as would be generated from high-level QC languages like Scaffold) and generate QC executables runnable on the IBM Q public QC machine. We then use this framework to implement and evaluate several optimal and heuristic mapping methods. These methods vary in how they account for the availability of dynamic machine calibration data, the relative importance of various noise parameters, the different possible routing strategies, and the relative importance of compile-time scalability versus runtime success. Using realsystem measurements, we show that fine grained spatial and temporal variations in hardware parameters can be exploited to obtain an average 2.9x (and up to 18x) improvement in program success rate over the industry standard IBM Qiskit compiler. Despite small qubit counts, NISQ systems will soon be large enough to demonstrate "quantum supremacy," i.e., an advantage over classical computing. Tools like ours provide significant improvements in program reliability and execution time, and offer high leverage in accelerating progress towards quantum supremacy.},
	booktitle = {International {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems} - {ASPLOS}},
	publisher = {Association for Computing Machinery},
	author = {Murali, Prakash and Baker, Jonathan M. and Abhari, Ali Javadi and Chong, Frederic T. and Martonosi, Margaret},
	month = apr,
	year = {2019},
	note = {arXiv: 1901.11054},
	keywords = {noise-adaptive compilation, qubit mapping},
	pages = {1015--1029},
}

@article{ravi_vaqem_2021,
	title = {{VAQEM}: {A} {Variational} {Approach} to {Quantum} {Error} {Mitigation}},
	url = {http://arxiv.org/abs/2112.05821},
	abstract = {Variational Quantum Algorithms (VQAs) are relatively robust to noise, but errors are still a significant detriment to VQAs on near-term quantum machines. It is imperative to employ error mitigation techniques to improve VQA fidelity. While existing error mitigation techniques built from theory provide substantial gains, the disconnect between theory and real machine execution limits their benefits. Thus, it is critical to optimize mitigation techniques to explicitly suit the target application as well as the noise characteristics of the target machine. We propose VAQEM, which dynamically tailors existing error mitigation techniques to the actual, dynamic noisy execution characteristics of VQAs on a target quantum machine. We do so by tuning specific features of these mitigation techniques similar to the traditional rotation angle parameters - by targeting improvements towards a specific objective function which represents the VQA problem at hand. In this paper, we target two types of error mitigation techniques which are suited to idle times in quantum circuits: single qubit gate scheduling and the insertion of dynamical decoupling sequences. We gain substantial improvements to VQA objective measurements - a mean of over 3x across a variety of VQA applications, run on IBM Quantum machines. More importantly, the proposed variational approach is general and can be extended to many other error mitigation techniques whose specific configurations are hard to select a priori. Integrating more mitigation techniques into the VAQEM framework can lead to potentially realizing practically useful VQA benefits on today's noisy quantum machines.},
	author = {Ravi, Gokul Subramanian and Smith, Kaitlin N. and Gokhale, Pranav and Mari, Andrea and Earnest, Nathan and Javadi-Abhari, Ali and Chong, Frederic T.},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.05821},
}

@inproceedings{tomesh_supermarq_2022,
	title = {{SupermarQ}: {A} {Scalable} {Quantum} {Benchmark} {Suite}},
	volume = {2022-April},
	isbn = {978-1-66542-027-3},
	doi = {10.1109/HPCA53966.2022.00050},
	abstract = {The emergence of quantum computers as a new computational paradigm has been accompanied by speculation concerning the scope and timeline of their anticipated revolutionary changes. While quantum computing is still in its infancy, the variety of different architectures used to implement quantum computations make it difficult to reliably measure and compare performance. This problem motivates our introduction of SupermarQ, a scalable, hardware-agnostic quantum benchmark suite which uses application-level metrics to measure performance. SupermarQ is the first attempt to systematically apply techniques from classical benchmarking methodology to the quantum domain. We define a set of feature vectors to quantify coverage, select applications from a variety of domains to ensure the suite is representative of real workloads, and collect benchmark results from the IBM, IonQ, and AQT@LBNL platforms. Looking forward, we envision that quantum benchmarking will encompass a large cross-community effort built on open source, constantly evolving benchmark suites. We introduce SupermarQ as an important step in this direction.},
	booktitle = {Proceedings - {International} {Symposium} on {High}-{Performance} {Computer} {Architecture}},
	publisher = {IEEE Computer Society},
	author = {Tomesh, Teague and Gokhale, Pranav and Omole, Victory and Ravi, Gokul Subramanian and Smith, Kaitlin N. and Viszlai, Joshua and Wu, Xin Chuan and Hardavellas, Nikos and Martonosi, Margaret R. and Chong, Frederic T.},
	year = {2022},
	note = {arXiv: 2202.11045
ISSN: 15300897},
	keywords = {Benchmarking, Program Characterization, Quantum Computing, ★},
	pages = {587--603},
}

@article{smith_timestitch_2022,
	title = {{TimeStitch}: {Exploiting} {Slack} to {Mitigate} {Decoherence} in {Quantum} {Circuits}},
	issn = {2643-6809},
	doi = {10.1145/3548778},
	abstract = {Quantum systems have potential to demonstrate significant computational advantage, but current quantum devices suffer from the rapid accumulation of error that prevents the storage of quantum information over extended periods. The unintentional coupling of qubits to their environment and each other adds significant noise to computation, and improved methods to combat decoherence are required to boost the performance of quantum algorithms on real machines. While many existing techniques for mitigating error rely on adding extra gates to the circuit [13, 20, 56], calibrating new gates [50], or extending a circuit’s runtime [32], this paper’s primary contribution leverages the gates already present in a quantum program without extending circuit duration. We exploit circuit slack for single-qubit gates that occur in idle windows, scheduling the gates such that their timing can counteract some errors.Spin-echo corrections that mitigate decoherence on idling qubits act as inspiration for this work. Theoretical models, however, fail to capture all sources of noise in NISQ devices, making practical solutions necessary that better minimize the impact of unpredictable errors in quantum machines. This paper presents TimeStitch: a novel framework that pinpoints the optimum execution schedules for single-qubit gates within quantum circuits. TimeStitch, implemented as a compilation pass, leverages the reversible nature of quantum computation to boost the success of circuits on real quantum machines. Unlike past approaches that apply reversibility properties to improve quantum circuit execution [35], TimeStitch amplifies fidelity without violating critical path frontiers in either the slack tuning procedures or the final rescheduled circuit. On average, compared to a state-of-the-art baseline, a practically constrained TimeStitch achieves a mean 38\% relative improvement in success rates, with a maximum of 106\%, while observing bounds on circuit depth. When unconstrained by depth criteria, TimeStitch produces a mean relative fidelity increase of 50\% with a maximum of 256\%. Finally, when TimeStitch intelligently leverages periodic dynamical decoupling within its scheduling framework, a mean 64\% improvement is observed over the baseline, relatively outperforming standalone dynamical decoupling by 19\%, with a maximum of 287\%.},
	journal = {ACM Transactions on Quantum Computing},
	author = {Smith, Kaitlin N. and Ravi, Gokul Subramanian and Murali, Prakash and Baker, Jonathan M. and Earnest, Nathan and Javadi-Abhari, Ali and Chong, Frederic T.},
	month = mar,
	year = {2022},
	note = {Publisher: Association for Computing Machinery (ACM)},
}

@inproceedings{ravi_adaptive_2021,
	title = {Adaptive job and resource management for the growing quantum cloud},
	isbn = {978-1-66541-691-7},
	url = {https://ieeexplore.ieee.org/document/9605297/},
	doi = {10.1109/QCE52317.2021.00047},
	abstract = {As the popularity of quantum computing continues to grow, efficient quantum machine access over the cloud is critical to both academic and industry researchers across the globe. And as cloud quantum computing demands increase exponentially, the analysis of resource consumption and execution characteristics are key to efficient management of jobs and resources at both the vendor-end as well as the client-end. While the analysis and optimization of job / resource consumption and management are popular in the classical HPC domain, it is severely lacking for more nascent technology like quantum computing.This paper proposes optimized adaptive job scheduling to the quantum cloud taking note of primary characteristics such as queuing times and fidelity trends across machines, as well as other characteristics such as quality of service guarantees and machine calibration constraints. Key components of the proposal include a) a prediction model which predicts fidelity trends across machine based on compiled circuit features such as circuit depth and different forms of errors, as well as b) queuing time prediction for each machine based on execution time estimations.Overall, this proposal is evaluated on simulated IBM machines across a diverse set of quantum applications and system loading scenarios, and is able to reduce wait times by over 3x and improve fidelity by over 40\% on specific usecases, when compared to traditional job schedulers.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Quantum} {Computing} and {Engineering} ({QCE})},
	publisher = {IEEE},
	author = {Ravi, Gokul Subramanian and Smith, Kaitlin N. and Murali, Prakash and Chong, Frederic T.},
	month = oct,
	year = {2021},
	keywords = {★},
	pages = {301--312},
}

@article{ngoenriang_optimal_2022,
	title = {Optimal {Stochastic} {Resource} {Allocation} for {Distributed} {Quantum} {Computing}},
	url = {http://arxiv.org/abs/2210.02886},
	abstract = {With the advent of interconnected quantum computers, i.e., distributed quantum computing (DQC), multiple quantum computers can now collaborate via quantum networks to perform massively complex computational tasks. However, DQC faces problems sharing quantum information because it cannot be cloned or duplicated between quantum computers. Thanks to advanced quantum mechanics, quantum computers can teleport quantum information across quantum networks. However, challenges to utilizing efficiently quantum resources, e.g., quantum computers and quantum channels, arise in DQC due to their capabilities and properties, such as uncertain qubit fidelity and quantum channel noise. In this paper, we propose a resource allocation scheme for DQC based on stochastic programming to minimize the total deployment cost for quantum resources. Essentially, the two-stage stochastic programming model is formulated to handle the uncertainty of quantum computing demands, computing power, and fidelity in quantum networks. The performance evaluation demonstrates the effectiveness and ability of the proposed scheme to balance the utilization of quantum computers and on-demand quantum computers while minimizing the overall cost of provisioning under uncertainty.},
	author = {Ngoenriang, Napat and Xu, Minrui and Supittayapornpong, Sucha and Niyato, Dusit and Yu, Han and {Xuemin} and {Shen}},
	month = sep,
	year = {2022},
	note = {arXiv: 2210.02886},
	keywords = {★},
}

@article{cicconetti_resource_2022,
	title = {Resource {Allocation} in {Quantum} {Networks} for {Distributed} {Quantum} {Computing}},
	url = {http://arxiv.org/abs/2203.05844},
	doi = {10.1109/SMARTCOMP55677.2022.00032},
	abstract = {The evolution of quantum computing technologies has been advancing at a steady pace in the recent years, and the current trend suggests that it will become available at scale for commercial purposes in the near future. The acceleration can be boosted by pooling compute infrastructures to either parallelize algorithm execution or solve bigger instances that are not feasible on a single quantum computer, which requires an underlying Quantum Internet: the interconnection of quantum computers by quantum links and repeaters to exchange entangled quantum bits. However, Quantum Internet research so far has been focused on provisioning point-to-point flows only, which is suitable for (e.g.) quantum sensing and metrology, but not for distributed quantum computing. In this paper, after a primer on quantum computing and networking, we investigate the requirements and objectives of smart computing on distributed nodes from the perspective of quantum network provisioning. We then design a resource allocation strategy that is evaluated through a comprehensive simulation campaign, whose results highlight the key features and performance issues, and lead the way to further investigation in this direction.},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.05844},
}

@article{buyya_manifesto_2019,
	title = {A manifesto for future generation cloud computing: {Research} directions for the next decade},
	volume = {51},
	issn = {15577341},
	doi = {10.1145/3241737},
	abstract = {The Cloud computing paradigm has revolutionised the computer science horizon during the past decade and has enabled the emergence of computing as the fifth utility. It has captured significant attention of academia, industries, and government bodies. Now, it has emerged as the backbone of modern economy by offering subscription-based services anytime, anywhere following a pay-as-you-go model. This has instigated (1) shorter establishment times for start-ups, (2) creation of scalable global enterprise applications, (3) better cost-to-value associativity for scientific and high-performance computing applications, and (4) different invocation/execution models for pervasive and ubiquitous applications. The recent technological developments and paradigms such as serverless computing, software-defined networking, Internet of Things, and processing at network edge are creating new opportunities for Cloud computing. However, they are also posing several new challenges and creating the need for new approaches and research strategies, as well as the re-evaluation of the models that were developed to address issues such as scalability, elasticity, reliability, security, sustainability, and application models. The proposed manifesto addresses them by identifying the major open challenges in Cloud computing, emerging trends, and impact areas. It then offers research directions for the next decade, thus helping in the realisation of Future Generation Cloud Computing.},
	number = {5},
	journal = {ACM Computing Surveys},
	author = {Buyya, Rajkumar and Srirama, Satish Narayana and Casale, Giuliano and Calheiros, Rodrigo and Simmhan, Yogesh and Varghese, Blesson and Gelenbe, Erol and Javadi, Bahman and Vaquero, Luis Miguel and Netto, Marco A.S. and Toosi, Adel Nadjaran and Rodriguez, Maria Alejandra and Llorente, Ignacio M. and De Capitani Di Vimercati, Sabrina and Samarati, Pierangela and Milojicic, Dejan and Varela, Carlos and Bahsoon, Rami and Dias De Assuncao, Marcos and Rana, Omer and Zhou, Wanlei and Jin, Hai and Gentzsch, Wolfgang and Zomaya, Albert Y. and Shen, Haiying},
	month = jan,
	year = {2019},
	note = {arXiv: 1711.09123
Publisher: Association for Computing Machinery},
	keywords = {Application development, Cloud computing, Cloud economics, Data management, Fog computing, InterCloud, Scalability, Serverless computing, Sustainability},
}

@article{berenberg_deployment_2023,
	title = {Deployment {Archetypes} for {Cloud} {Applications}},
	volume = {55},
	issn = {0360-0300},
	doi = {10.1145/3498336},
	abstract = {This is a survey article that explores six Cloud-based deployment archetypes for Cloud applications and the tradeoffs between them to achieve high availability, low end-user latency, and acceptable costs. These are (1) Zonal, (2) Regional, (3) Multi-regional, (4) Global, (5) Hybrid, and (6) Multi-cloud deployment archetypes. The goal is to classify cloud applications into a set of deployment archetypes and deployment models that tradeoff their needs around availability, latency, and geographical constraints with a focus on serving applications. This enables application owners to better examine the tradeoffs of each deployment model and what is needed for achieving the availability and latency goals for their application.},
	number = {3},
	journal = {ACM Computing Surveys},
	author = {Berenberg, Anna and Calder, Brad},
	month = apr,
	year = {2023},
	note = {arXiv: 2105.00560
Publisher: Association for Computing Machinery (ACM)},
	pages = {1--48},
}

@article{moghaddam_performance-aware_2019,
	title = {Performance-aware management of cloud resources: {A} taxonomy and future directions},
	volume = {52},
	issn = {15577341},
	doi = {10.1145/3337956},
	abstract = {The dynamic nature of the cloud environment has made the distributed resource management process a challenge for cloud service providers. The importance of maintaining quality of service in accordance with customer expectations and the highly dynamic nature of cloud-hosted applications add new levels of complexity to the process. Advances in big-data learning approaches have shifted conventional static capacity planning solutions to complex performance-aware resource management methods. It is shown that the process of decision-making for resource adjustment is closely related to the behavior of the system, including the utilization of resources and application components. Therefore, a continuous monitoring of system attributes and performance metrics provides the raw data for the analysis of problems affecting the performance of the application. Data analytic methods, such as statistical and machine-learning approaches, offer the required concepts, models, and tools to dig into the data and find general rules, patterns, and characteristics that define the functionality of the system. Obtained knowledge from the data analysis process helps to determine the changes in the workloads, faulty components, or problems that can cause system performance to degrade. A timely reaction to performance degradation can avoid violations of service level agreements, including performing proper corrective actions such as auto-scaling or other resource adjustment solutions. In this article, we investigate the main requirements and limitations of cloud resource management, including a study of the approaches to workload and anomaly analysis in the context of performance management in the cloud. A taxonomy of the works on this problem is presented that identifies main approaches in existing research from the data analysis side to resource adjustment techniques. Finally, considering the observed gaps in the general direction of the reviewed works, a list of these gaps is proposed for future researchers to pursue.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Moghaddam, Sara Kardani and Buyya, Rajkumar and Ramamohanarao, Kotagiri},
	month = aug,
	year = {2019},
	note = {arXiv: 1808.02254
Publisher: Association for Computing Machinery},
	keywords = {Anomaly detection, Big-data analytics, Performance management, Resource management},
}

@article{costa_orchestration_2023,
	title = {Orchestration in {Fog} {Computing}: {A} {Comprehensive} {Survey}},
	volume = {55},
	issn = {15577341},
	doi = {10.1145/3486221},
	abstract = {Fog computing is a paradigm that brings computational resources and services to the network edge in the vicinity of user devices, lowering latency and connecting with cloud computing resources. Unlike cloud computing, fog resources are based on constrained and heterogeneous nodes whose connectivity can be unstable. In this complex scenario, there is a need to define and implement orchestration processes to ensure that applications and services can be provided, considering the settled agreements. Although some publications have dealt with orchestration in fog computing, there are still some diverse definitions and functional intersection with other areas, such as resource management and monitoring. This article presents a systematic review of the literature with focus on orchestration in fog computing. A generic architecture of fog orchestration is presented, created from the consolidation of the analyzed proposals, bringing to light the essential functionalities addressed in the literature. This work also highlights the main challenges and open research questions.},
	number = {2},
	journal = {ACM Computing Surveys},
	author = {Costa, Breno and Bachiega, Joao and De Carvalho, Leonardo Rebouças and Araujo, Aleteia P.F.},
	month = mar,
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Fog computing, monitoring, orchestration, resource management},
}

@article{hua_edge_2022,
	title = {Edge {Computing} with {Artificial} {Intelligence}: {A} {Machine} {Learning} {Perspective}},
	issn = {0360-0300},
	doi = {10.1145/3555802},
	abstract = {Recent years have witnessed the widespread popularity of Internet of things (IoT). By providing sufficient data for model training and inference, IoT has promoted the development of artificial intelligence (AI) to a great extent. Under this background and trend, the traditional cloud computing model may nevertheless encounter many problems in independently tackling the massive data generated by IoT and meeting corresponding practical needs. In response, a new computing model called edge computing (EC) has drawn extensive attention from both industry and academia. With the continuous deepening of the research on EC, however, scholars have found that traditional (non-AI) methods have their limitations in enhancing the performance of EC. Seeing the successful application of AI in various fields, EC researchers start to set their sights on AI, especially from a perspective of machine learning (ML), a branch of AI which has gained increased popularity in the past decades. In this paper, we first explain the formal definition of EC and the reasons why EC has become a favorable computing model. Then, we discuss the problems of interest in EC. We summarize the traditional solutions and hightlight their limitations. By explaining the research results of using AI to optimize EC and applying AI to other fields under the EC architecture, this paper can serve as a guide to explore new research ideas in these two aspects while enjoying the mutually beneficial relationship between AI and EC.},
	journal = {ACM Computing Surveys},
	author = {Hua, Haochen and Li, Yutong and Wang, Tonghe and Dong, Nanqing and Li, Wei and Cao, Junwei},
	month = aug,
	year = {2022},
	note = {Publisher: Association for Computing Machinery (ACM)},
}

@article{zhong_machine_2022,
	title = {Machine {Learning}-based {Orchestration} of {Containers}: {A} {Taxonomy} and {Future} {Directions}},
	volume = {54},
	issn = {0360-0300},
	doi = {10.1145/3510415},
	abstract = {Containerization is a lightweight application virtualization technology, providing high environmental consistency, operating system distribution portability, and resource isolation. Existing mainstream cloud service providers have prevalently adopted container technologies in their distributed system infrastructures for automated application management. To handle the automation of deployment, maintenance, autoscaling, and networking of containerized applications, container orchestration is proposed as an essential research problem. However, the highly dynamic and diverse feature of cloud workloads and environments considerably raises the complexity of orchestration mechanisms. Machine learning algorithms are accordingly employed by container orchestration systems for behavior modeling and prediction of multi-dimensional performance metrics. Such insights could further improve the quality of resource provisioning decisions in response to the changing workloads under complex environments. In this article, we present a comprehensive literature review of existing machine learning-based container orchestration approaches. Detailed taxonomies are proposed to classify the current researches by their common features. Moreover, the evolution of machine learning-based container orchestration technologies from the year 2016 to 2021 has been designed based on objectives and metrics. A comparative analysis of the reviewed techniques is conducted according to the proposed taxonomies, with emphasis on their key characteristics. Finally, various open research challenges and potential future directions are highlighted.},
	number = {10s},
	journal = {ACM Computing Surveys},
	author = {Zhong, Zhiheng and Xu, Minxian and Rodriguez, Maria Alejandra and Xu, Chengzhong and Buyya, Rajkumar},
	month = jan,
	year = {2022},
	note = {arXiv: 2106.12739
Publisher: Association for Computing Machinery (ACM)},
	pages = {1--35},
}

@article{beaumont_scheduling_2020,
	title = {Scheduling on {Two} {Types} of {Resources}: {A} {Survey}},
	volume = {53},
	issn = {15577341},
	doi = {10.1145/3387110},
	abstract = {The evolution in the design of modern parallel platforms leads to revisit the scheduling jobs on distributed heterogeneous resources. The goal of this survey is to present the main existing algorithms, to classify them based on their underlying principles, and to propose unified implementations to enable their fair comparison, in terms of running time and quality of schedules, on a large set of common benchmarks that we made available for the community. Beyond this comparison, our goal is also to understand the main difficulties that heterogeneity conveys and the shared principles that guide the design of efficient algorithms.},
	number = {3},
	journal = {ACM Computing Surveys},
	author = {Beaumont, Olivier and Canon, Louis Claude and Eyraud-Dubois, Lionel and Lucarelli, Giorgio and Marchal, Loris and Mommessin, Clément and Simon, Bertrand and Trystram, Denis},
	month = jun,
	year = {2020},
	note = {arXiv: 1909.11365
Publisher: Association for Computing Machinery},
	keywords = {Scheduling, heterogeneity, makespan minimization, online scheduling, performance evaluation, resource allocation},
}

@article{le_duc_machine_2019,
	title = {Machine learning methods for reliable resource provisioning in edge-cloud computing: {A} survey},
	volume = {52},
	issn = {15577341},
	doi = {10.1145/3341145},
	abstract = {Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use. This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article.},
	number = {5},
	journal = {ACM Computing Surveys},
	author = {Le Duc, Thang and Leiva, Rafael García and Casari, Paolo and Östberg, Per Olov},
	month = sep,
	year = {2019},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Autoscaling, Cloud computing, Consolidation, Distributed systems, Edge computing, Machine learning, Optimization, Placement, Reliability, Remediation},
}

@article{mampage_holistic_2022,
	title = {A {Holistic} {View} on {Resource} {Management} in {Serverless} {Computing} {Environments}: {Taxonomy} and {Future} {Directions}},
	volume = {54},
	issn = {0360-0300},
	doi = {10.1145/3510412},
	abstract = {Serverless computing has emerged as an attractive deployment option for cloud applications in recent times. The unique features of this computing model include rapid auto-scaling, strong isolation, fine-grained billing options, and access to a massive service ecosystem, which autonomously handles resource management decisions. This model is increasingly being explored for deployments in geographically distributed edge and fog computing networks as well, due to these characteristics. Effective management of computing resources has always gained a lot of attention among researchers. The need to automate the entire process of resource provisioning, allocation, scheduling, monitoring, and scaling has resulted in the need for specialized focus on resource management under the serverless model. In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements that influence these aspects, encompassing characteristics of system design, workload attributes, and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog, and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identifies gaps in literature and highlights future research directions for improving capabilities of this computing model.},
	number = {11s},
	journal = {ACM Computing Surveys},
	author = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
	month = jan,
	year = {2022},
	note = {arXiv: 2105.11592
Publisher: Association for Computing Machinery (ACM)},
	pages = {1--36},
}

@article{mohamed_software-defined_2021,
	title = {Software-defined networks for resource allocation in cloud computing: {A} survey},
	volume = {195},
	issn = {13891286},
	doi = {10.1016/j.comnet.2021.108151},
	abstract = {Cloud computing has a shared set of resources, including physical servers, networks, storage, and user applications. Resource allocation is a critical issue for cloud computing, especially in Infrastructure-as-a-Service (IaaS). The decision-making process in the cloud computing network is non-trivial as it is handled by switches and routers. Moreover, the network concept drifts resulting from changing user demands are among the problems affecting cloud computing. The cloud data center needs agile and elastic network control functions with control of computing resources to ensure proper virtual machine (VM) operations, traffic performance, and energy conservation. Software-Defined Network (SDN) proffers new opportunities to blueprint resource management to handle cloud services allocation while dynamically updating traffic requirements of running VMs. The inclusion of an SDN for managing the infrastructure in a cloud data center better empowers cloud computing, making it easier to allocate resources. In this survey, we discuss and survey resource allocation in cloud computing based on SDN. It is noted that various related studies did not contain all the required requirements. This study is intended to enhance resource allocation mechanisms that involve both cloud computing and SDN domains. Consequently, we analyze resource allocation mechanisms utilized by various researchers; we categorize and evaluate them based on the measured parameters and the problems presented. This survey also contributes to a better understanding of the core of current research that will allow researchers to obtain further information about the possible cloud computing strategies relevant to IaaS resource allocation.},
	journal = {Computer Networks},
	author = {Mohamed, Arwa and Hamdan, Mosab and Khan, Suleman and Abdelaziz, Ahmed and Babiker, Sharief F. and Imran, Muhammad and Marsono, M. N.},
	month = aug,
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {5G, Cloud computing, Data Center Network, Edge-Computing, Resource allocation, Software-defined networks},
}

@article{ahsan_designing_2015,
	title = {Designing a million-qubit quantum computer using a resource performance simulator},
	volume = {12},
	issn = {15504840},
	doi = {10.1145/2830570},
	abstract = {The optimal design of a fault-Tolerant quantum computer involves finding an appropriate balance between the burden of large-scale integration of noisy components and the load of improving the reliability of hardware technology. This balance can be evaluated by quantitatively modeling the execution of quantum logic operations on a realistic quantum hardware containing limited computational resources. In this work, we report a complete performance simulation software tool capable of (1) searching the hardware design space by varying resource architecture and technology parameters, (2) synthesizing and scheduling a fault-Tolerant quantum algorithm within the hardware constraints, (3) quantifying the performance metrics such as the execution time and the failure probability of the algorithm, and (4) analyzing the breakdown of these metrics to highlight the performance bottlenecks and visualizing resource utilization to evaluate the adequacy of the chosen design. Using this tool, we investigate a vast design space for implementing key building blocks of Shor's algorithm to factor a 1,024-bit number with a baseline budget of 1.5 million qubits. We show that a trapped-ion quantum computer designed with twice as many qubits and one-Tenth of the baseline infidelity of the communication channel can factor a 2,048-bit integer in less than 5 months.},
	number = {4},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	author = {Ahsan, Muhammad and Van Meter, Rodney and Kim, Jungsang},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.00796
Publisher: Association for Computing Machinery},
}

@article{bhatia_quantum-based_2019,
	title = {Quantum-based predictive fog scheduler for {IoT} applications},
	volume = {111},
	issn = {01663615},
	doi = {10.1016/j.compind.2019.06.002},
	abstract = {Load scheduling across distributed fog computing nodes has been a major challenge to meet the increased demand of real-time data analysis, and time-sensitive decision-making. This study presents a Quantum Computing-inspired (QCi) optimized load scheduling technique in fog computing environments for real-time IoT applications. In addition to this, QCi-Neural Network Model is used as a predictive model to determine the optimal computational node for enabling real-time service delivery. For validation, simulations were performed using 3, 5, 7, and 10 fog nodes at different instances to schedule nearly 600 heterogeneous tasks to obtain the respective results. Results were compared with other state-of-the-art scheduling models like Min–Max, Minimum Completion time, and Round Robin. Better results were registered for the proposed technique in terms of minimum average task completion time (28.15 s for 10 nodes) and average energy consumption (3.48 J for 10 nodes). Moreover, higher rates of statistical parameters like sensitivity (88.66\%), specificity (91.28\%), precision (92.25\%), and coverage (95.66\%) were acquired depicting enhanced overall performance.},
	journal = {Computers in Industry},
	author = {Bhatia, Munish and Sood, Sandeep K. and Kaur, Simranpreet},
	month = oct,
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Fog computing, Industrial IoT, Load scheduling, Quantum computing},
	pages = {51--67},
}

@techreport{satoh_resource_nodate,
	title = {Resource {Allocation} {Policy} for {Noisy} {Distributed} {Quantum} {Computing} {Resource} {Allocation} {Policy} for {Noisy} {Distributed} {Quantum} {Computers}},
	abstract = {Multicore processing and distributed computing are crucial applications for the success of these day's computation. Since classical CPU's physical limitation gets close, as shown in Moor's law, quantum computing is expected as the next-generation computing. Similar to classical computing, multi-processing and distributed computing can be thought of for future quantum computing applications. This thesis aims to establish a resource allocation method that efficiently prepares a distributed quantum state, especially a graph state, which is an important resource for quantum computing. A key element of this method is a decision tree composed of three different graph algorithms. By preparing three different graph algorithms, the allocator is able to deal with various types of graphs. This resource allocation method outperformed the random allocation in terms of the quality of the final state in the presence of noise. This allocator is four times better than the naive allocation method in terms of error tolerance in a simple situation. Even with a very complex graph, this allocator still keeps generating a better state than the other allocation methods. As for the scalability of this method, it works in a very straightforward way. However, the time complexity of an entire system is not small. By setting a threshold for search space, it is possible to suppress the time complexity of the entire process. These results show us the path to efficient resource allocation in a practical situation.},
	author = {Satoh, Ryosuke},
	keywords = {1 Quantum Computing, 2 Distributed Quantum Computing, 3 Quantum Networking, 4 Graph State 5 Graph Theory},
}

@inproceedings{das_case_2019,
	title = {A case for multi-programming-antum computers},
	isbn = {978-1-4503-6938-1},
	doi = {10.1145/3352460.3358287},
	abstract = {Existing and near-term quantum computers face signi?cant reliability challenges because of high error rates caused by noise. Such machines are operated in the Noisy Intermediate Scale Quantum (NISQ) model of computing. As NISQ machines exhibit high error-rates, only programs that require a few qubits can be executed reliably. Therefore, NISQ machines tend to underutilize its resources. In this paper, we propose to improve the throughput and utilization of NISQ machines by using multi-programming and enabling the NISQ machine to concurrently execute multiple workloads. Multi-programming a NISQ machine is non-trivial. This is because, a multi-programmed NISQ machine can have an adverse impact on the reliability of the individual workloads. To enable multi-programming in a robust manner, we propose three solutions. First, we develop methods to partition the qubits into multiple reliable regions using error information from machine calibration so that each program can have a fair allocation of reliable qubits. Second, we observe that when two programs are of unequal lengths, measurement operations can impact the reliability of the co-running program. To reduce this interference, we propose a Delayed Instruction Scheduling (DIS) policy that delays the start of the shorter program so that all the measurement operations can be performed at the end. Third,we develop an Adaptive Multi-Programming (AMP) design that monitors the reliability at runtime and reverts to single program mode if the reliability impact of multi-programming is greater than a prede?ned threshold. Our evaluations with IBMQ16 show that our proposals can improve resource utilization and throughput by up to 2x, while limiting the impact on reliability.},
	booktitle = {Proceedings of the {Annual} {International} {Symposium} on {Microarchitecture}, {MICRO}},
	publisher = {IEEE Computer Society},
	author = {Das, Poulami and Tannu, Swamit S. and Nair, Prashant J. and Qureshi, Moinuddin},
	month = oct,
	year = {2019},
	note = {ISSN: 10724451},
	keywords = {Multi-programming, NISQ, Quantum computer, Reliability},
	pages = {291--303},
}

@book{european_university_cyprus_iwcmc_nodate,
	title = {{IWCMC} 2020 : 2020 16th {International} {Wireless} {Communications} \& {Mobile} {Computing} {Conference} ({IWCMC}) : {Limassol}, {Cyprus}, {June} 15 -19, 2020},
	isbn = {978-1-72813-129-0},
	abstract = {Conference held online due to COVID-19.},
	author = {{European University Cyprus} and {Jāmiʻah al-Lubnānīyah al-Amīrikīyah} and {Institute of Electrical and Electronics Engineers} and {Institute of Electrical and Electronics Engineers. Lebanon Section} and {Institute of Electrical and Electronics Engineers. Cyprus Section}},
}

@article{alnas_optimal_2022,
	title = {Optimal resource allocation for flexible-grid entanglement distribution networks},
	volume = {30},
	issn = {10944087},
	doi = {10.1364/oe.458358},
	abstract = {We use a genetic algorithm (GA) as a design aid for determining the optimal provisioning of entangled photon spectrum in flex-grid quantum networks with arbitrary numbers of channels and users. After introducing a general model for entanglement distribution based on frequency-polarization hyperentangled biphotons, we derive upper bounds on fidelity and entangled bit rate for networks comprising one-to-one user connections. Simple conditions based on user detector quality and link efficiencies are found that determine whether entanglement is possible. We successfully apply a GA to find optimal resource allocations in four different representative network scenarios and validate features of our model experimentally in a quantum local area network in deployed fiber. Our results show promise for the rapid design of large-scale entanglement distribution networks.},
	number = {14},
	journal = {Optics Express},
	author = {Alnas, Jude and Alshowkan, Muneer and Rao, Nageswara S. V. and Peters, Nicholas A. and Lukens, Joseph M.},
	month = jul,
	year = {2022},
	note = {arXiv: 2204.06642
Publisher: Optica Publishing Group},
	pages = {24375},
}

@article{kaewpuang_adaptive_2022,
	title = {Adaptive {Resource} {Allocation} in {Quantum} {Key} {Distribution} ({QKD}) for {Federated} {Learning}},
	url = {http://arxiv.org/abs/2208.11270},
	abstract = {Increasing privacy and security concerns in intelligence-native 6G networks require quantum key distribution-secured federated learning (QKD-FL), in which data owners connected via quantum channels can train an FL global model collaboratively without exposing their local datasets. To facilitate QKD-FL, the architectural design and routing management framework are essential. However, effective implementation is still lacking. To this end, we propose a hierarchical architecture for QKD-FL systems in which QKD resources (i.e., wavelengths) and routing are jointly optimized for FL applications. In particular, we focus on adaptive QKD resource allocation and routing for FL workers to minimize the deployment cost of QKD nodes under various uncertainties, including security requirements. The experimental results show that the proposed architecture and the resource allocation and routing model can reduce the deployment cost by 7.72{\textbackslash}\% compared to the CO-QBN algorithm.},
	author = {Kaewpuang, Rakpong and Xu, Minrui and Niyato, Dusit and Yu, Han and Xiong, Zehui and Shen, Xuemin Sherman},
	month = aug,
	year = {2022},
	note = {arXiv: 2208.11270},
}

@article{narottama_quantum_2022,
	title = {Quantum {Neural} {Networks} for {Resource} {Allocation} in {Wireless} {Communications}},
	volume = {21},
	issn = {15582248},
	doi = {10.1109/TWC.2021.3102139},
	abstract = {This study exploits a quantum neural network (QNN) for resource allocation in wireless communications. A QNN is presented to reduce time complexity while still maintaining performance. Moreover, a reinforcement-learning- inspired QNN (RL-QNN) is presented to improve the perfor- mance. Quantum circuit design of the QNN is presented to ensure the practical implementation in noisy intermediate-scale quantum (NISQ) computers. For the QNN, the complexity and the number of required qubits are analyzed as well. As a particular use case, the QNN is utilized for user grouping in non-orthogonal multiple access. The results reveal that the QNN schemes have lower complexities and similar performance in terms of the achievable sum rate when compared with that of the classical neural network.},
	number = {2},
	journal = {IEEE Transactions on Wireless Communications},
	author = {Narottama, Bhaskara and Shin, Soo Young},
	month = feb,
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {6G, B5G, non-orthogonal multiple access, quantum neural networks, wireless communications},
	pages = {1103--1116},
}

@article{salazar_optimal_2021,
	title = {Optimal allocation of quantum resources},
	volume = {5},
	issn = {2521327X},
	doi = {10.22331/Q-2021-03-10-407},
	abstract = {The optimal allocation of resources is a crucial task for their efficient use in a wide range of practical applications in science and engineering. This paper investigates the optimal allocation of resources in multipartite quantum systems. In particular, we show the relevance of proportional fairness and optimal reliability criteria for the application of quantum resources. Moreover, we present optimal allocation solutions for an arbitrary number of qudits using measurement incompatibility as an exemplary resource theory. Besides, we study the criterion of optimal equitability and demonstrate its relevance to scenarios involving several resource theories such as nonlocality vs local contextuality. Finally, we highlight the potential impact of our results for quantum networks and other multi-party quantum information processing, in particular to the future Quantum Internet.},
	journal = {Quantum},
	author = {Salazar, Roberto and Biswas, Tanmoy and Czartowski, Jakub and Życzkowski, Karol and Horodecki, Paweł},
	month = mar,
	year = {2021},
	note = {arXiv: 2006.16134
Publisher: Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
}

@article{resch_benchmarking_2022,
	title = {Benchmarking {Quantum} {Computers} and the {Impact} of {Quantum} {Noise}},
	volume = {54},
	issn = {15577341},
	doi = {10.1145/3464420},
	abstract = {Benchmarking is how the performance of a computing system is determined. Surprisingly, even for classical computers this is not a straightforward process. One must choose the appropriate benchmark and metrics to extract meaningful results. Different benchmarks test the system in different ways, and each individual metric may or may not be of interest. Choosing the appropriate approach is tricky. The situation is even more open ended for quantum computers, where there is a wider range of hardware, fewer established guidelines, and additional complicating factors. Notably, quantum noise significantly impacts performance and is difficult to model accurately. Here, we discuss benchmarking of quantum computers from a computer architecture perspective and provide numerical simulations highlighting challenges that suggest caution.},
	number = {7},
	journal = {ACM Computing Surveys},
	author = {Resch, Salonik and Karpuzcu, Ulya R.},
	year = {2022},
	note = {arXiv: 1912.00546},
	keywords = {Benchmarking quantum computers, quantum noise, ★},
}

@article{openja_technical_2022,
	title = {Technical debts and faults in open-source quantum software systems: {An} empirical study},
	volume = {193},
	issn = {01641212},
	doi = {10.1016/j.jss.2022.111458},
	abstract = {Quantum computing is a rapidly growing field attracting the interest of both researchers and software developers. Supported by its numerous open-source tools, developers can now build, test, or run their quantum algorithms. Although the maintenance practices for traditional software systems have been extensively studied, the maintenance of quantum software is still a new field of study but a critical part to ensure the quality of a whole quantum computing system. In this work, we set out to investigate the distribution and evolution of technical debts in quantum software and their relationship with fault occurrences. Understanding these problems could guide future quantum development and provide maintenance recommendations for the key areas where quantum software developers and researchers should pay more attention. In this paper, we empirically studied 118 open-source quantum projects, which were selected from GitHub. The projects are categorized into 10 categories. We found that the studied quantum software suffers from the issues of code convention violation, error-handling, and code design. We also observed a statistically significant correlation between code design, redundant code or code convention, and the occurrences of faults in quantum software.},
	journal = {Journal of Systems and Software},
	author = {Openja, Moses and Morovati, Mohammad Mehdi and An, Le and Khomh, Foutse and Abidi, Mouna},
	month = nov,
	year = {2022},
	note = {Publisher: Elsevier Inc.},
	keywords = {Quantum computing, Software bugs, Software maintenance, Software reliability, Technical debts},
}

@article{malawski_serverless_2020,
	title = {Serverless execution of scientific workflows: {Experiments} with {HyperFlow}, {AWS} {Lambda} and {Google} {Cloud} {Functions}},
	volume = {110},
	issn = {0167739X},
	url = {https://doi.org/10.1016/j.future.2017.10.029},
	doi = {10.1016/j.future.2017.10.029},
	abstract = {Scientific workflows consisting of a high number of interdependent tasks represent an important class of complex scientific applications. Recently, a new type of serverless infrastructures has emerged, represented by such services as Google Cloud Functions and AWS Lambda, also referred to as the Function-as-a-Service model. In this paper we take a look at such serverless infrastructures, which are designed mainly for processing background tasks of Web and Internet of Things applications, or event-driven stream processing. We evaluate their applicability to more compute- and data-intensive scientific workflows and discuss possible ways to repurpose serverless architectures for execution of scientific workflows. We have developed prototype workflow executor functions using AWS Lambda and Google Cloud Functions, coupled with the HyperFlow workflow engine. These functions can run workflow tasks in AWS and Google infrastructures, and feature such capabilities as data staging to/from S3 or Google Cloud Storage and execution of custom application binaries. We have successfully deployed and executed the Montage astronomy workflow, often used as a benchmark, and we report on initial results of its performance evaluation. Our findings indicate that the simple mode of operation makes this approach easy to use, although there are costs involved in preparing portable application binaries for execution in a remote environment. While our solution is an early prototype, we find the presented approach highly promising. We also discuss possible future steps related to execution of scientific workflows in serverless infrastructures. Finally, we perform a cost analysis and discuss implications with regard to resource management for scientific applications in general.},
	journal = {Future Generation Computer Systems},
	author = {Malawski, Maciej and Gajek, Adam and Zima, Adam and Balis, Bartosz and Figiela, Kamil},
	month = sep,
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cloud functions, FaaS, Scientific workflows, Serverless architectures},
	pages = {502--514},
}

@techreport{qsdk-cirq,
	title = {Cirq {Framework}},
	url = {https://zenodo.org/record/5182845},
	urldate = {2022-03-28},
	author = {{Google}},
	month = aug,
	year = {2021},
	doi = {10.5281/zenedo.5182845},
}

@inproceedings{qalgo-grover,
	address = {New York, USA},
	title = {A fast quantum mechanical algorithm for database search},
	volume = {75},
	isbn = {0-89791-785-5},
	doi = {10.1145/237814.237866},
	abstract = {We present a detailed study of Higgs boson production in association with a single top quark at the LHC, at next-to-leading order accuracy in QCD. We consider total and differential cross sections, at the parton level as well as by matching short distance events to parton showers, for both t-channel and s-channel production. We provide predictions relevant for the LHC at 13 TeV together with a thorough evaluation of the residual uncertainties coming from scale variation, parton distributions, strong coupling constant and heavy quark masses. In addition, for t-channel production, we compare results as obtained in the 4-flavour and 5-flavour schemes, pinning down the most relevant differences between them. Finally, we study the sensitivity to a non-standard-model relative phase between the Higgs couplings to the top quark and to the weak bosons.},
	booktitle = {Proceedings of the 28th {ACM} {Symposium} on {Theory} of computing  - {STOC} '96},
	publisher = {ACM},
	author = {Grover, Lov K.},
	month = jun,
	year = {1996},
	note = {arXiv: 1504.00611
Issue: 6},
	keywords = {★},
	pages = {212--219},
}

@techreport{qsdk-qiskit,
	title = {Qiskit: {An} {Open}-source {Framework} for {Quantum} {Computing}},
	url = {https://zenodo.org/record/2562111},
	urldate = {2022-09-13},
	author = {Aleksandrowicz, Gadi and Alexander, Thomas and Barkoutsos, Panagiotis and Bello, Luciano and Ben-Haim, Yael and Bucher, David and Cabrera-Hernández, Francisco Jose and {et. al.}},
	month = jan,
	year = {2019},
	doi = {10.5281/ZENODO.2562111},
	keywords = {qiskit, quantum computing, quantum programming language, sdk},
}

@techreport{qsdk-forest-rigetti,
	title = {A {Practical} {Quantum} {Instruction} {Set} {Architecture}},
	url = {http://arxiv.org/abs/1608.03355},
	abstract = {We introduce an abstract machine architecture for classical/quantum computations---including compilation---along with a quantum instruction language called Quil for explicitly writing these computations. With this formalism, we discuss concrete implementations of the machine and non-trivial algorithms targeting them. The introduction of this machine dovetails with ongoing development of quantum computing technology, and makes possible portable descriptions of recent classical/quantum algorithms.},
	author = {Smith, Robert S. and Curtis, Michael J. and Zeng, William J.},
	month = aug,
	year = {2016},
	doi = {10.48550/arXiv.1608.03355},
	note = {arXiv: 1608.03355},
}

@techreport{qsdk-pennylane,
	title = {{PennyLane}: {Automatic} differentiation of hybrid quantum-classical computations},
	url = {http://arxiv.org/abs/1811.04968},
	abstract = {PennyLane is a Python 3 software framework for differentiable programming of quantum computers. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for hardware providers including the Xanadu Cloud, Amazon Braket, and IBM Quantum, allowing PennyLane optimizations to be run on publicly accessible quantum devices. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, JAX, and Autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.},
	author = {Bergholm, Ville and Izaac, Josh and Schuld, Maria and {et. al.}},
	month = nov,
	year = {2018},
	doi = {10.48550/arXiv.1811.04968},
	note = {arXiv: 1811.04968},
}

@techreport{qalgo-qaoa,
	title = {A {Quantum} {Approximate} {Optimization} {Algorithm}},
	url = {https://arxiv.org/abs/1411.4028},
	abstract = {We introduce a quantum algorithm that produces approximate solutions for
combinatorial optimization problems. The algorithm depends on a positive
integer p and the quality of the approximation improves as p is increased. The
quantum circuit that implements the algorithm consists of unitary gates whose
locality is at most the locality of the objective function whose optimum is
sought. The depth of the circuit grows linearly with p times (at worst) the
number of constraints. If p is fixed, that is, independent of the input size,
the algorithm makes use of efficient classical preprocessing. If p grows with
the input size a different strategy is proposed. We study the algorithm as
applied to MaxCut on regular graphs and analyze its performance on 2-regular
and 3-regular graphs for fixed p. For p = 1, on 3-regular graphs the quantum
algorithm always finds a cut that is at least 0.6924 times the size of the
optimal cut.},
	urldate = {2022-09-13},
	author = {Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
	month = nov,
	year = {2014},
	doi = {10.48550/arxiv.1411.4028},
	note = {arXiv: 1411.4028},
}

@misc{openfaas,
	title = {{OpenFaaS} - {Serverless} {Functions} {Made} {Simple}},
	url = {https://github.com/openfaas/faas},
	urldate = {2022-10-21},
	author = {Ellis, Alex},
	year = {2022},
}

@article{gimenez-alventosa_framework_2019,
	title = {A framework and a performance assessment for serverless {MapReduce} on {AWS} {Lambda}},
	volume = {97},
	issn = {0167739X},
	url = {https://doi.org/10.1016/j.future.2019.02.057},
	doi = {10.1016/j.future.2019.02.057},
	abstract = {MapReduce is one of the most widely used programming models for analysing large-scale datasets, i.e. Big Data. In recent years, serverless computing and, in particular, Functions as a Service (FaaS) has surged as an execution model in which no explicit management of servers (e.g. virtual machines) is performed by the user. Instead, the Cloud provider dynamically allocates resources to the function invocations and fine-grained billing is introduced depending on the execution time and allocated memory, as exemplified by AWS Lambda. In this article, a high-performant serverless architecture has been created to execute MapReduce jobs on AWS Lambda using Amazon S3 as the storage backend. In addition, a thorough assessment has been carried out to study the suitability of AWS Lambda as a platform for the execution of High Throughput Computing jobs. The results indicate that AWS Lambda provides a convenient computing platform for general-purpose applications that fit within the constraints of the service (15 min of maximum execution time, 3008 MB of RAM and 512 MB of disk space) but it exhibits an inhomogeneous performance behaviour that may jeopardise adoption for tightly coupled computing jobs.},
	journal = {Future Generation Computer Systems},
	author = {Giménez-Alventosa, V. and Moltó, Germán and Caballer, Miguel},
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cloud computing, Elasticity, MapReduce, Serverless, ★},
	pages = {259--274},
}

@article{yao_performance_2023,
	title = {Performance optimization of serverless edge computing function offloading based on deep reinforcement learning},
	volume = {139},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X2200293X},
	doi = {10.1016/j.future.2022.09.009},
	journal = {Future Generation Computer Systems},
	author = {Yao, Xuyi and Chen, Ningjiang and Yuan, Xuemei and Ou, Pingjie},
	month = feb,
	year = {2023},
	pages = {74--86},
}

@techreport{qfw-scq-ibm,
	title = {A {Serverless} {Cloud} {Integration} {For} {Quantum} {Computing}},
	url = {http://arxiv.org/abs/2107.02007},
	abstract = {Starting from the idea of Quantum Computing which is a concept that dates back to 80s, we come to the present day where we can perform calculations on real quantum computers. This sudden development of technology opens up new scenarios that quickly lead to the desire and the real possibility of integrating this technology into current software architectures. The usage of frameworks that allow computation to be performed directly on quantum hardware poses a series of challenges. This document describes a an architectural framework that addresses the problems of integrating an API exposed Quantum provider in an existing Enterprise architecture and it provides a minimum viable product (MVP) solution that really merges classical quantum computers on a basic scenario with reusable code on GitHub repository. The solution leverages a web-based frontend where user can build and select applications/use cases and simply execute it without any further complication. Every triggered run leverages on multiple backend options, that include a scheduler managing the queuing mechanism to correctly schedule jobs and final results retrieval. The proposed solution uses the up-to-date cloud native technologies (e.g. Cloud Functions, Containers, Microservices) and serves as a general framework to develop multiple applications on the same infrastructure.},
	author = {Grossi, M. and Crippa, L. and Aita, A. and Bartoli, G. and Sammarco, V. and Picca, E. and Said, N. and Tramonto, F. and Mattei, F.},
	month = jul,
	year = {2021},
	doi = {10.48550/arXiv.2107.02007},
	note = {arXiv: 2107.02007},
}

@techreport{qfw-algo2qpu,
	title = {A framework for algorithm deployment on cloud-based quantum computers},
	url = {http://arxiv.org/abs/1810.10576},
	abstract = {In recent years, the field of quantum computing has significantly developed in both the improvement of hardware as well as the assembly of various software tools and platforms, including cloud access to quantum devices. Unfortunately, many of these resources are rapidly changing and thus lack accessibility and stability for robust algorithm prototyping and deployment. Effectively leveraging the array of hardware and software resources at a higher level, that can adapt to the rapid development of software and hardware, will allow for further advancement and democratization of quantum technologies to achieve useful computational tasks. As a way to approach this challenge, we present a flexible, high-level framework called algo2qpu that is well-suited for designing and testing instances of algorithms for near-term quantum computers on the cloud. Algorithms that employ adaptive protocols for optimizations of algorithm parameters can be grouped under the umbrella of "adaptive hybrid quantum-classical" (AHQC) algorithms. We demonstrate the utility of algo2qpu for near-term algorithm development by applying the framework to implement proof-of-principle instances of two AHQC algorithms that have applications in quantum chemistry and/or quantum machine learning, namely the quantum autoencoder and the variational quantum classifier, using Rigetti Computing's Forest platform.},
	author = {Sim, Sukin and Cao, Yudong and Romero, Jonathan and Johnson, Peter D. and Aspuru-Guzik, Alan},
	month = oct,
	year = {2018},
	doi = {10.48550/arXiv.1810.10576},
	note = {arXiv: 1810.10576},
}

@article{scheuner_function-as--service_2020,
	title = {Function-as-a-{Service} performance evaluation: {A} multivocal literature review},
	volume = {170},
	issn = {01641212},
	url = {https://doi.org/10.1016/j.jss.2020.110708},
	doi = {10.1016/j.jss.2020.110708},
	abstract = {Function-as-a-Service (FaaS) is one form of the serverless cloud computing paradigm and is defined through FaaS platforms (e.g., AWS Lambda) executing event-triggered code snippets (i.e., functions). Many studies that empirically evaluate the performance of such FaaS platforms have started to appear but we are currently lacking a comprehensive understanding of the overall domain. To address this gap, we conducted a multivocal literature review (MLR) covering 112 studies from academic (51) and grey (61) literature. We find that existing work mainly studies the AWS Lambda platform and focuses on micro-benchmarks using simple functions to measure CPU speed and FaaS platform overhead (i.e., container cold starts). Further, we discover a mismatch between academic and industrial sources on tested platform configurations, find that function triggers remain insufficiently studied, and identify HTTP API gateways and cloud storages as the most used external service integrations. Following existing guidelines on experimentation in cloud systems, we discover many flaws threatening the reproducibility of experiments presented in the surveyed studies. We conclude with a discussion of gaps in literature and highlight methodological suggestions that may serve to improve future FaaS performance evaluation studies.},
	journal = {Journal of Systems and Software},
	author = {Scheuner, Joel and Leitner, Philipp},
	year = {2020},
	note = {arXiv: 2004.03276
Publisher: Elsevier Inc.},
	keywords = {Benchmarking, Cloud computing, Function-as-a-Service, Multivocal literature review, Performance, Serverless},
	pages = {110708},
}

@article{barcelona-pons_benchmarking_2021,
	title = {Benchmarking parallelism in {FaaS} platforms},
	volume = {124},
	issn = {0167739X},
	url = {https://doi.org/10.1016/j.future.2021.06.005},
	doi = {10.1016/j.future.2021.06.005},
	abstract = {Serverless computing has seen a myriad of work exploring its potential. Some systems tackle Function-as-a-Service (FaaS) properties on automatic elasticity and scale to run highly-parallel computing jobs. However, they focus on specific platforms and convey that their ideas can be extrapolated to any FaaS runtime. An important question arises: do all FaaS platforms fit parallel computations? In this paper, we argue that not all of them provide the necessary means to host highly-parallel applications. To validate our hypothesis, we create a comparative framework and categorize the architectures of four cloud FaaS offerings, emphasizing parallel performance. We attest and extend this description with an empirical experiment that consists in plotting in deep detail the evolution of a parallel computing job on each service. The analysis of our results evinces that FaaS is not inherently good for parallel computations and architectural differences across platforms are decisive to categorize their performance. A key insight is the importance of virtualization technologies and the scheduling approach of FaaS platforms. Parallelism improves with lighter virtualization and proactive scheduling due to finer resource allocation and faster elasticity. This causes some platforms like AWS and IBM to perform well for highly-parallel computations, while others such as Azure present difficulties to achieve the required parallelism degree. Consequently, the information in this paper becomes of special interest to help users choose the most adequate infrastructure for their parallel applications.},
	journal = {Future Generation Computer Systems},
	author = {Barcelona-Pons, Daniel and García-López, Pedro},
	year = {2021},
	note = {arXiv: 2010.15032
Publisher: Elsevier B.V.},
	keywords = {Benchmark, FaaS, Parallelism, Serverless},
	pages = {268--284},
}

@misc{ibmq-127qubit,
	title = {{IBM} {Quantum} breaks the 100‑qubit processor barrier},
	url = {https://research.ibm.com/blog/127-qubit-quantum-processor-eagle},
	abstract = {IBM Quantum delivers 127 qubits on a single processor for the first time, and previews the design for its next-gen IBM Quantum System Two.},
	author = {Chow, Jerry M.; Dial, Oliver; Gambetta, Jay},
	month = nov,
	year = {2021},
	note = {Pages: Last visited: 2022-06-19},
	keywords = {IBM},
}

@article{qalgo-dj,
	title = {Rapid solution of problems by quantum computation},
	volume = {439},
	issn = {0962-8444},
	doi = {10.1098/rspa.1992.0167},
	abstract = {A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.},
	number = {1907},
	journal = {Proceedings of the Royal Society of London. Series A: Mathematical and Physical Sciences},
	author = {Deutsch, David and Jozsa, Richard},
	month = dec,
	year = {1992},
	keywords = {★},
	pages = {553--558},
}

@book{qbook-nielsen-chuang,
	title = {Quantum {Computation} and {Quantum} {Information}},
	isbn = {978-1-107-00217-3},
	abstract = {10th anniversary ed. This is an introduction to the main ideas and techniques of the field of quantum computation and quantum information. It asks the question: what are the ultimate physical limits to computation and communication? Fundamental Concepts -- Introduction and overview -- Introduction to quantum mechanics -- Introduction to computer science -- Quantum Computation -- Quantum circuits -- The quantum Fourier transform and its applications -- Quantum search algorithms -- Quantum computers: physical realization -- Quantum Information -- Quantum noise and quantum operations -- Distance measures for quantum information -- Quantum error-correction -- Entropy and information -- Quantum information theory -- Notes on basic probability theory -- Group theory -- The Solovay-Kitaev theorem -- Number theory -- Public-key cryptography and the RSA cryptosystem -- Proof of Lieb's theorem.},
	publisher = {Cambridge University Press},
	author = {Nielsen, Michael A. and Chuang, Isaac L.},
	month = jun,
	year = {2012},
	doi = {10.1017/CBO9780511976667},
}

@article{qfw-xacc,
	title = {A backend-agnostic, quantum-classical framework for simulations of chemistry in {C} ++},
	issn = {2643-6809},
	doi = {10.1145/3523285},
	abstract = {As quantum computing hardware systems continue to advance, the research and development of performant, scalable, and extensible software architectures, languages, models, and compilers is equally as important in order to bring this novel coprocessing capability to a diverse group of domain computational scientists. For the field of quantum chemistry, applications and frameworks exist for modeling and simulation tasks that scale on heterogeneous classical architectures, and we envision the need for similar frameworks on heterogeneous quantum-classical platforms. Here we present the XACC system-level quantum computing framework as a platform for prototyping, developing, and deploying quantum-classical software that specifically targets chemistry applications. We review the fundamental design features in XACC, with special attention to its extensibility and modularity for key quantum programming workflow interfaces, and provide an overview of the interfaces most relevant to simulations of chemistry. A series of examples demonstrating some of the state-of-the-art chemistry algorithms currently implemented in XACC are presented, while also illustrating the various APIs that would enable the community to extend, modify, and devise new algorithms and applications in the realm of chemistry.},
	journal = {ACM Transactions on Quantum Computing},
	author = {Claudino, Daniel and McCaskey, Alexander J. and Lyakh, Dmitry I.},
	year = {2022},
	note = {arXiv: 2105.01619},
}

@article{q-tket,
	title = {t{\textbar}ket: a retargetable compiler for {NISQ} devices},
	volume = {6},
	issn = {20589565},
	doi = {10.1088/2058-9565/ab8e92},
	abstract = {We present t{\textbar}ket, a quantum software development platform produced by Cambridge Quantum Computing Ltd. The heart of t{\textbar}ket is a language-agnostic optimising compiler designed to generate code for a variety of NISQ devices, which has several features designed to minimise the influence of device error. The compiler has been extensively benchmarked and outperforms most competitors in terms of circuit optimisation and qubit routing.},
	number = {1},
	journal = {Quantum Science and Technology},
	author = {Sivarajah, Seyon and Dilkes, Silas and Cowtan, Alexander and Simmons, Will and Edgington, Alec and Duncan, Ross},
	month = jan,
	year = {2021},
	note = {Publisher: IOP Publishing Ltd},
	keywords = {Compilers, Quantum circuit optimisation, Quantum computing, Quantum software, Qubit mapping, ★},
}

@article{qsurvey-raj-2022,
	title = {Quantum computing: {A} taxonomy, systematic review and future directions},
	volume = {52},
	issn = {1097024X},
	doi = {10.1002/spe.3039},
	abstract = {Quantum computing (QC) is an emerging paradigm with the potential to offer significant computational advantage over conventional classical computing by exploiting quantum-mechanical principles such as entanglement and superposition. It is anticipated that this computational advantage of QC will help to solve many complex and computationally intractable problems in several application domains such as drug design, data science, clean energy, finance, industrial chemical development, secure communications, and quantum chemistry. In recent years, tremendous progress in both quantum hardware development and quantum software/algorithm has brought QC much closer to reality. Indeed, the demonstration of quantum supremacy marks a significant milestone in the Noisy Intermediate Scale Quantum (NISQ) era—the next logical step being the quantum advantage whereby quantum computers solve a real-world problem much more efficiently than classical computing. As the quantum devices are expected to steadily scale up in the next few years, quantum decoherence and qubit interconnectivity are two of the major challenges to achieve quantum advantage in the NISQ era. QC is a highly topical and fast-moving field of research with significant ongoing progress in all facets. A systematic review of the existing literature on QC will be invaluable to understand the state-of-the-art of this emerging field and identify open challenges for the QC community to address in the coming years. This article presents a comprehensive review of QC literature and proposes taxonomy of QC. The proposed taxonomy is used to map various related studies to identify the research gaps. A detailed overview of quantum software tools and technologies, post-quantum cryptography, and quantum computer hardware development captures the current state-of-the-art in the respective areas. The article identifies and highlights various open challenges and promising future directions for research and innovation in QC.},
	number = {1},
	journal = {Software - Practice and Experience},
	author = {Gill, Sukhpal Singh and Kumar, Adarsh and Singh, Harvinder and Singh, Manmeet and Kaur, Kamalpreet and Usman, Muhammad and Buyya, Rajkumar},
	year = {2022},
	note = {arXiv: 2010.15559},
	keywords = {conceptual model, future directions, methodical analysis, quantum computing, qubits, research challenges, taxonomy},
	pages = {66--114},
}

@article{qcloud-braket,
	title = {Cloud based {QC} with {Amazon} {Braket}},
	volume = {5},
	issn = {2569-1996},
	url = {http://link.springer.com/10.1007/s42354-021-0330-z},
	doi = {10.1007/s42354-021-0330-z},
	abstract = {The approaches to solutions in quantum computing technology are auspicious. Quantum computers are already promising an improved architecture to solve problems that conventional computers can no longer handle. The voices asking how this new technology can be made commercially viable are getting louder. The US cloud computing provider Amazon Web Services has launched “Amazon Braket”, a fully managed quantum computing service that harnesses the potential of quantum computing. Constantin Gonzalez is Principal Solutions Architect at Amazon Web Services and speaks about AWS’s ambitious goals to make quantum computing a breakthrough.},
	number = {2},
	journal = {Digitale Welt},
	author = {Gonzalez, Constantin},
	month = apr,
	year = {2021},
	pages = {14--17},
}

@article{rojo_trials_2021,
	title = {Trials and tribulations of developing hybrid quantum-classical microservices systems},
	volume = {3008},
	issn = {16130073},
	abstract = {Quantum computing holds great promise to solve problems where classical computers cannot reach. To the point where it already arouses the interest of both scientific and industrial communities. Thus, it is expected that hybrid systems will start to appear where quantum software interacts with classical systems. Such coexistence can be fostered by service computing. Unfortunately, how quantum code can be offered as a service still misses out on many of the potential benefits of service computing. This paper takes the traveling salesman problem and tackles the challenge of giving it an implementation in the form of a quantum microservice. Then it is used to detect which of the benefits of service computing are lost in the process. The conclusions help to measure the distance between the current state of technology and the state that would be desirable to have a real quantum service engineering.},
	journal = {CEUR Workshop Proceedings},
	author = {Rojo, Javier and Moguel, Enrique and Valencia, David and Berrocal, Javier and García-Alonso, Jose and Murillo, Juan M.},
	year = {2021},
	note = {arXiv: 2105.04421},
	pages = {38--53},
}

@article{kandala_hardware-efficient_2017,
	title = {Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
	volume = {549},
	issn = {14764687},
	doi = {10.1038/nature23879},
	abstract = {Quantum computers can be used to address electronic-structure problems and problems in materials science and condensed matter physics that can be formulated as interacting fermionic problems, problems which stretch the limits of existing high-performance computers. Finding exact solutions to such problems numerically has a computational cost that scales exponentially with the size of the system, and Monte Carlo methods are unsuitable owing to the fermionic sign problem. These limitations of classical computational methods have made solving even few-atom electronic-structure problems interesting for implementation using medium-sized quantum computers. Yet experimental implementations have so far been restricted to molecules involving only hydrogen and helium. Here we demonstrate the experimental optimization of Hamiltonian problems with up to six qubits and more than one hundred Pauli terms, determining the ground-state energy for molecules of increasing size, up to BeH2. We achieve this result by using a variational quantum eigenvalue solver (eigensolver) with efficiently prepared trial states that are tailored specifically to the interactions that are available in our quantum processor, combined with a compact encoding of fermionic Hamiltonians and a robust stochastic optimization routine. We demonstrate the flexibility of our approach by applying it to a problem of quantum magnetism, an antiferromagnetic Heisenberg model in an external magnetic field. In all cases, we find agreement between our experiments and numerical simulations using a model of the device with noise. Our results help to elucidate the requirements for scaling the method to larger systems and for bridging the gap between key problems in high-performance computing and their implementation on quantum hardware.},
	number = {7671},
	journal = {Nature},
	author = {Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M. and Gambetta, Jay M.},
	month = sep,
	year = {2017},
	pmid = {28905916},
	note = {arXiv: 1704.05018
Publisher: Nature Publishing Group},
	pages = {242--246},
}

@article{leitner_mixed-method_2019,
	title = {A mixed-method empirical study of {Function}-as-a-{Service} software development in industrial practice},
	volume = {149},
	issn = {01641212},
	doi = {10.1016/j.jss.2018.12.013},
	abstract = {Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers.},
	journal = {Journal of Systems and Software},
	author = {Leitner, Philipp and Wittern, Erik and Spillner, Josef and Hummer, Waldemar},
	month = mar,
	year = {2019},
	note = {Publisher: Elsevier Inc.},
	keywords = {Cloud computing, Empirical research, Function-as-a-Service, Serverless},
	pages = {340--359},
}

@article{xu_edgeworkflow_2022,
	title = {{EdgeWorkflow}: {One} click to test and deploy your workflow applications to the edge},
	volume = {193},
	issn = {01641212},
	doi = {10.1016/j.jss.2022.111456},
	abstract = {In recent years, edge computing has become the ideal computing paradigm for various smart systems, such as smart logistics, smart health and smart transportation. This is due to its advantages including fast response times, energy efficiency and cost effectiveness over conventional cloud computing platforms. However, running complex computational scientific workflow tasks is still a very challenging issue at the edge, due to its typical three-layered computing environment consisting of an end device layer, an edge server layer, and a cloud server layer. A large number of recent studies have proposed different solutions for optimizing such computing resource management problems in an edge computing environment. However, since evaluation of most such studies is conducted through simulation, the effectiveness cannot be guaranteed in a real world environment. Therefore, to advance research on efficient execution and deployment problems for real world workflow applications using edge computing, an open-source edge workflow management system with comprehensive empirical evaluation capabilities is urgently required. This paper presents the first edge workflow system (named EdgeWorkflow) that is able to deploy user-created workflow applications to a real-world edge computing environment with “one-click” after optimizing the configuration with the simulation tool. With the aid of EdgeWorkflow, the user can automate the generation of specific edge computing environments, easily model and generate executable workflow applications with a visual modelling tool, effectively select various resource management methods included in the systems or apply their own resource management and task scheduling algorithms, efficiently monitor the statuses of computational tasks and obtain comprehensive reports on the execution results (such as those regarding time, cost and energy). We use an edge computing-based unmanned aerial vehicle (UAV) last-mile delivery system as a real-world case study, and a number of representative scientific workflows are employed for our experiments. Our experimental results show that EdgeWorkflow can effectively evaluate the performance of different resource management and workflow task scheduling algorithms and efficiently deploy and execute user-defined scientific workflow applications to user-specified edge computing environments.},
	journal = {Journal of Systems and Software},
	author = {Xu, Jia and Ding, Ran and Liu, Xiao and Li, Xuejun and Grundy, John and Yang, Yun},
	month = nov,
	year = {2022},
	note = {Publisher: Elsevier Inc.},
	keywords = {Edge computing, One-click deployment, Resource management, Workflow system},
}

@article{karabey_aksakalli_deployment_2021,
	title = {Deployment and communication patterns in microservice architectures: {A} systematic literature review},
	volume = {180},
	issn = {01641212},
	doi = {10.1016/j.jss.2021.111014},
	abstract = {Context: Microservice is an architectural style that separates large systems into small functional units to provide better modularity. A key challenge of microservice architecture design frequently discussed in the literature is the identification and decomposition of the service modules. Besides this, two other key challenges can be identified, including the deployment of the modules on the corresponding execution platform, and adopted communication patterns. Objective: This study aims to identify and describe the reported deployment approaches, and the communication platforms for microservices in the current literature. Furthermore, we aim to describe the identified obstacles of these approaches as well as the corresponding research directions. Method: A systematic literature review (SLR) is conducted using a multiphase study selection process in which we reviewed a total of 239 papers. Among these, we selected 38 of them as primary studies related to the described research questions. Results: Based on our study, we could identify three types of deployment approaches and seven different communication patterns. Moreover, we have identified eight challenges related to the deployment and six challenges related to the communication concerns. Conclusion: Our study shows that in addition to the identification of modules, the deployment and communication approaches are equally crucial for a successful application of the microservice architecture style. Various deployment approaches and communication patterns appear to be useful for different contexts. The identified research directions in the literature study show that still more research is needed to cope with the current challenges.},
	journal = {Journal of Systems and Software},
	author = {Karabey Aksakalli, Işıl and Çelik, Turgay and Can, Ahmet Burak and Teki̇nerdoğan, Bedir},
	month = oct,
	year = {2021},
	note = {Publisher: Elsevier Inc.},
	keywords = {Communication concerns, Communication patterns of microservices, Deployment challenges, Microservice architectures, Microservice deployment, Research directions},
}

@article{poojara_serverless_2022,
	title = {Serverless data pipeline approaches for {IoT} data in fog and cloud computing},
	volume = {130},
	issn = {0167739X},
	doi = {10.1016/j.future.2021.12.012},
	abstract = {With the increasing number of Internet of Things (IoT) devices, massive amounts of raw data is being generated. The latency, cost, and other challenges in cloud-based IoT data processing have driven the adoption of Edge and Fog computing models, where some data processing tasks are moved closer to data sources. Properly dealing with the flow of such data requires building data pipelines, to control the complete life cycle of data streams from data acquisition at the data source, edge and fog processing, to Cloud side storage and analytics. Data analytics tasks need to be executed dynamically at different distances from the data sources and often on very heterogeneous hardware devices. This can be streamlined by the use of a Serverless (or FaaS) cloud computing model, where tasks are defined as virtual functions, which can be migrated from edge to cloud (and vice versa) and executed in an event-driven manner on data streams. In this work, we investigate the benefits of building Serverless data pipelines (SDP) for IoT data analytics and evaluate three different approaches for designing SDPs: (1) Off-the-shelf data flow tool (DFT) based, (2) Object storage service (OSS) based and (3) MQTT based. Further, we applied these strategies on three fog applications (Aeneas, PocketSphinx, and custom Video processing application) and evaluated the performance by comparing their processing time (computation time, network communication and disk access time), and resource utilization. Results show that DFT is unsuitable for compute-intensive applications such as video or image processing, whereas OSS is best suitable for this task. However, DFT is nicely fit for bandwidth-intensive applications due to the minimum use of network resources. On the other hand, MQTT-based SDP is observed with increase in CPU and Memory usage as the number of users rose, and experienced a drop in data units in the pipeline for PocketSphinx and custom video processing applications, however it performed well for Aeneas which had low size data units.},
	journal = {Future Generation Computer Systems},
	author = {Poojara, Shivananda R. and Dehury, Chinmaya Kumar and Jakovits, Pelle and Srirama, Satish Narayana},
	month = may,
	year = {2022},
	note = {arXiv: 2112.09974
Publisher: Elsevier B.V.},
	keywords = {Cloud computing, Data pipelines, Edge computing, Fog computing, Internet of things, Serverless computing},
	pages = {91--105},
}

@article{cassel_serverless_2022,
	title = {Serverless computing for {Internet} of {Things}: {A} systematic literature review},
	volume = {128},
	issn = {0167739X},
	doi = {10.1016/j.future.2021.10.020},
	abstract = {Serverless computing, or Function as a Service (FaaS), represents a research trend where applications are built and deployed as a group of stateless functions. Although initially proposed for the cloud, serverless computing has also found its place on Internet of Things (IoT) while bringing functions closer to the devices, in order to reduce latency and avoid unnecessary energy and resource consumption. It is interesting that solutions can work in an integrated manner on edge, fog, and cloud layers. Mission-critical functions can be executed on edge and fog in order to benefit from low-latency responses, while heavy functions can be executed on the cloud to process huge amount of data produced by IoT sensors, as long as Internet connection is available. Existing surveys focus on serverless computing for specific layers and do not address a broad, integrated, and systematic vision regarding how IoT benefits from serverless on edge, fog, and cloud. With this in mind, this paper provides a comprehensive Systematic Literature Review that, after the selection process, covers 60 papers on the field of serverless computing for IoT on the three layers. This gives us insights about how functions are offloaded to different devices and how they interact with each other. We bring main components employed to incubate and execute functions, as well as the main challenges and open questions for this subject. Protocols, programming languages, and storage services related to the solutions are also presented. Finally, we show a rich taxonomy summarizing all characteristics in a single figure, along with a discussion about the overall architecture of serverless applications for IoT. We conclude that serverless computing is a promising technology for IoT applications, but several improvements still need to be made to popularize this concept and make it easier to use.},
	journal = {Future Generation Computer Systems},
	author = {Cassel, Gustavo André Setti and Rodrigues, Vinicius Facco and da Rosa Righi, Rodrigo and Bez, Marta Rosecler and Nepomuceno, Andressa Cruz and André da Costa, Cristiano},
	month = mar,
	year = {2022},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cloud, Edge, Fog, Function as a Service, Internet of Things, Serverless},
	pages = {299--316},
}

@article{zhao_mobile-aware_2019,
	title = {Mobile-aware service function chain migration in cloud–fog computing},
	volume = {96},
	issn = {0167739X},
	doi = {10.1016/j.future.2019.02.031},
	abstract = {Network Function Virtualization (NFV) provides a good paradigm for sharing the resources of the physical network. The deployment problem of Service Function Chains (SFCs) composed of a specific order of Virtual Network Functions (VNFs) has become the focus of research. Moreover, to solve the facing challenges of the centralized cloud computing, the researchers have proposed the distributed fog computing. When the mobile user moves among different fog-based radio access networks, the SFC must be migrated. Therefore, in the paper, we research the problem of SFCs migration/remapping caused by the user movement in cloud–fog computing environments. We firstly model the migration problem of SFCs as an integer linear program; then we propose two SFC migration strategies: the minimum number of VNFs migration strategy and the two-step migration strategy, to reduce the reconfiguration cost, the migration time and downtime of SFCs and improve the remapping success ratio of SFCs; and we have designed a two-step migration algorithm to migrate SFCs. We use the cloud–fog computing environment to evaluate our proposed algorithms. The reconfiguration cost, the remapping success ratio, the migration time and the downtime of our proposed algorithms are more excellent than that of benchmark algorithm.},
	journal = {Future Generation Computer Systems},
	author = {Zhao, Dongcheng and Sun, Gang and Liao, Dan and Xu, Shizhong and Chang, Victor},
	month = jul,
	year = {2019},
	note = {Publisher: Elsevier B.V.},
	keywords = {Cloud–fog computing, Live migration, Network Function Virtualization, Service Function Chain},
	pages = {591--604},
}

@article{vaquero_research_2019,
	title = {Research challenges in nextgen service orchestration},
	volume = {90},
	issn = {0167739X},
	doi = {10.1016/j.future.2018.07.039},
	abstract = {Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.},
	journal = {Future Generation Computer Systems},
	author = {Vaquero, Luis M. and Cuadrado, Felix and Elkhatib, Yehia and Bernal-Bernabe, Jorge and Srirama, Satish N. and Zhani, Mohamed Faten},
	month = jan,
	year = {2019},
	note = {arXiv: 1806.00764
Publisher: Elsevier B.V.},
	keywords = {Churn, Edge, FaaS, Fog, Large scale, NFV, NVM, Orchestration, SDN, Serverless},
	pages = {20--38},
}

@article{logesh_hybrid_2018,
	title = {A hybrid quantum-induced swarm intelligence clustering for the urban trip recommendation in smart city},
	volume = {83},
	issn = {0167739X},
	doi = {10.1016/j.future.2017.08.060},
	abstract = {The development of internet technologies has brought digital services to the hands of common man. In the selection process of relevant digital services to the active target user, recommender systems have proved its efficiency as a successful decision support tool. Among many successful techniques incorporated to generate recommendations, collaborative filtering has been widely used to make similarity-based predictions for the recommendation of the relevant list of items to the users. As an advancement, utilizing clustering mechanisms with collaborative filtering for grouping similar users as clusters can enhance the efficiency of the recommendation generated. Though many clustering mechanisms have been employed to group similar users in the existing works, incorporation of bio-inspired clustering has yet to be explored for the generation of optimal recommendations. In this paper, a novel user clustering approach based on Quantum-behaved Particle Swarm Optimization (QPSO) has been proposed for the collaborative filtering based recommender system. The proposed recommendation approach has been evaluated on real-world large-scale datasets of Yelp and TripAdvisor for hit-rate, precision, recall, f-measure, and accuracy. The obtained results illustrate the advantageous performance of proposed approach over its peer works of recent times. We have also developed a new mobile recommendation framework XplorerVU for the urban trip recommendation in smart cities, to evaluate the proposed recommendation approach and the real-time implementation details of the mobile application in the smart-cities are also presented. The evaluation results prove the usefulness of the generated recommendations and depict the users’ satisfaction on the proposed recommendation approach.},
	journal = {Future Generation Computer Systems},
	author = {Logesh, R. and Subramaniyaswamy, V. and Vijayakumar, V. and Gao, Xiao Zhi and Indragandhi, V.},
	month = jun,
	year = {2018},
	note = {Publisher: Elsevier B.V.},
	keywords = {Bio-inspired computing, Clustering, Collaborative filtering, Quantum-behaved particle swarm optimization, Recommender system, Swarm intelligence},
	pages = {653--673},
}

@article{eismann_case_2022,
	title = {A case study on the stability of performance tests for serverless applications},
	volume = {189},
	issn = {01641212},
	doi = {10.1016/j.jss.2022.111294},
	abstract = {Context: While in serverless computing, application resource management and operational concerns are generally delegated to the cloud provider, ensuring that serverless applications meet their performance requirements is still a responsibility of the developers. Performance testing is a commonly used performance assessment practice; however, it traditionally requires visibility of the resource environment. Objective: In this study, we investigate whether performance tests of serverless applications are stable, that is, if their results are reproducible, and what implications the serverless paradigm has for performance tests. Method: We conduct a case study where we collect two datasets of performance test results: (a) repetitions of performance tests for varying memory size and load intensities and (b) three repetitions of the same performance test every day for ten months. Results: We find that performance tests of serverless applications are comparatively stable if conducted on the same day. However, we also observe short-term performance variations and frequent long-term performance changes. Conclusion: Performance tests for serverless applications can be stable; however, the serverless model impacts the planning, execution, and analysis of performance tests.},
	journal = {Journal of Systems and Software},
	author = {Eismann, Simon and Costa, Diego Elias and Liao, Lizhi and Bezemer, Cor Paul and Shang, Weiyi and van Hoorn, André and Kounev, Samuel},
	month = jul,
	year = {2022},
	note = {arXiv: 2107.13320
Publisher: Elsevier Inc.},
	keywords = {FaaS, Function-as-a-Service, Performance, Reproducibility, Serverless, Stability},
}

@article{beauregard_circuit_2003,
	title = {Circuit for {Shor}'s {Algorithm} {Using} 2n+3 {Qubits}},
	volume = {3},
	issn = {1533-7146},
	abstract = {We try to minimize the number of qubits needed to factor an integer of n bits using Shor's algorithm on a quantum computer. We introduce a circuit which uses 2n + 3 qubits and 0(n3lg(n)) elementary quantum gates in a depth of 0(n3) to implement the factorization algorithm. The circuit is computable in polynomial time on a classical computer and is completely general as it does not rely on any property of the number to be factored.},
	number = {2},
	journal = {Quantum Info. Comput.},
	author = {Beauregard, Stephane},
	month = mar,
	year = {2003},
	note = {Publisher: Rinton Press, Incorporated
Place: Paramus, NJ},
	keywords = {factorization, modular arithmetics, quantum circuits},
	pages = {175--185},
}

@article{monz_realization_2016,
	title = {Realization of a scalable {Shor} algorithm},
	volume = {351},
	issn = {0036-8075},
	url = {https://www.science.org/doi/10.1126/science.aad9480},
	doi = {10.1126/science.aad9480},
	abstract = {{\textless}p{\textgreater} A quantum computer is expected to outperform its classical counterpart in certain tasks. One such task is the factorization of large integers, the technology that underpins the security of bank cards and online privacy. Using a small-scale quantum computer comprising five trapped calcium ions, Monz {\textless}italic{\textgreater}et al.{\textless}/italic{\textgreater} implement a scalable version of Shor's factorization algorithm. With the function of ions being recycled and the architecture scalable, the process is more efficient than previous implementations. The approach thus provides the potential for designing a powerful quantum computer, but with fewer resources. {\textless}/p{\textgreater}},
	number = {6277},
	journal = {Science},
	author = {Monz, Thomas and Nigg, Daniel and Martinez, Esteban A. and Brandl, Matthias F. and Schindler, Philipp and Rines, Richard and Wang, Shannon X. and Chuang, Isaac L. and Blatt, Rainer},
	month = mar,
	year = {2016},
	pages = {1068--1070},
}

@article{lanyon_experimental_2007,
	title = {Experimental {Demonstration} of a {Compiled} {Version} of {Shor}’s {Algorithm} with {Quantum} {Entanglement}},
	volume = {99},
	issn = {0031-9007},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.99.250505},
	doi = {10.1103/PhysRevLett.99.250505},
	abstract = {Shor's powerful quantum algorithm for factoring represents a major challenge in quantum computation and its full realization will have a large impact on modern cryptography. Here we implement a compiled version of Shor's algorithm in a photonic system using single photons and employing the non-linearity induced by measurement. For the first time we demonstrate the core processes, coherent control, and resultant entangled states that are required in a full-scale implementation of Shor's algorithm. Demonstration of these processes is a necessary step on the path towards a full implementation of Shor's algorithm and scalable quantum computing. Our results highlight that the performance of a quantum algorithm is not the same as performance of the underlying quantum circuit, and stress the importance of developing techniques for characterising quantum algorithms.},
	number = {25},
	journal = {Physical Review Letters},
	author = {Lanyon, B. P. and Weinhold, T. J. and Langford, N. K. and Barbieri, M. and James, D. F. V. and Gilchrist, A. and White, A. G.},
	month = dec,
	year = {2007},
	pages = {250505},
}

@inproceedings{suchara_qure_2013,
	title = {{QuRE}: {The} quantum resource estimator toolbox},
	isbn = {978-1-4799-2987-0},
	doi = {10.1109/ICCD.2013.6657074},
	abstract = {We describe QuRE, the Quantum Resource Estimator. QuRE is a layout estimation tool that estimates the cost of practical implementations of quantum circuits in a variety of competing physical quantum technologies and with a variety of strategies for fault tolerant encoding. For each specified algorithm, QuRE estimates quantities such as number of physical qubits, execution time, probability of success of the computation, and physical gate counts for elementary quantum gate types of a specified technology. Out of the box, QuRE supports estimation for six physical quantum technologies, seven quantum algorithms, and with error correction using the Steane [1], [2], Bacon-Shor [3], Knill [4] or surface [5], [6] error correction codes. Moreover, QuRE is extendable and can easily accommodate other choices. After describing QuRE, we use it to investigate the tradeoff between concatenated and surface error correction coding techniques, demonstrating the existence of a crossover point for the Ground State Estimation Algorithm [7]. © 2013 IEEE.},
	booktitle = {2013 {IEEE} 31st {International} {Conference} on {Computer} {Design}, {ICCD} 2013},
	publisher = {IEEE Computer Society},
	author = {Suchara, Martin and Kubiatowicz, John and Faruque, Arvin and Chong, Frederic T. and Lai, Ching Yi and Paz, Gerardo},
	year = {2013},
	keywords = {Quantum Computation, Resource Estimation, ★},
	pages = {419--426},
}

@techreport{the_eu_quantum_flagship_initiative_quantum_2016,
	title = {Quantum {Manifesto}: {A} {New} {Era} of {Technology}},
	urldate = {2022-09-14},
	author = {{The EU Quantum Flagship Initiative}},
	month = may,
	year = {2016},
}

@inproceedings{leymann_towards_2019,
	title = {Towards a {Platform} for {Sharing} {Quantum} {Software}},
	url = {http://www2.informatik.uni-stuttgart.de/cgi-bin/NCSTRL/NCSTRL_view.pl?id=INPROC-2019-24&engl=1},
	abstract = {Quantum computers solving real-world problems are expected 
to become general
available within the next few years. But software for quantum 
computers require
very different skills compared to creating software for traditional 
computers
or networks. Thus, a community-driven approach to creating software 
for quantum
computers will foster a wide-spread use of this innovative 
technology. Also, a
platform for quantum software may provide a business model for 
several user
groups.},
	language = {English},
	booktitle = {Proceedings of the 13th {Advanced} {Summer} {School} on {Service}  {Oriented} {Computing} (2019)},
	publisher = {University of Stuttgart, Faculty of Computer Science,  Electrical Engineering, and Information Technology, Germany},
	author = {Leymann, Frank and Barzen, Johanna and Falkenthal, Michael},
	month = sep,
	year = {2019},
	note = {Series Title: IBM Technical Report (RC25685)},
	keywords = {Quantum computing; software engineering; middleware; 
platforms; cloud computing},
	pages = {70--74},
}

@incollection{salm_nisq_2020,
	title = {The {NISQ} {Analyzer}: {Automating} the {Selection} of {Quantum} {Computers} for {Quantum} {Algorithms}},
	volume = {1310},
	url = {https://link.springer.com/10.1007/978-3-030-64846-6_5},
	abstract = {Quantum computing can enable a variety of breakthroughs in research and industry in the future. Although some quantum algorithms already exist that show a theoretical speedup compared to the best known classical algorithms, the implementation and execution of these algorithms come with several challenges. The input data determines, for example, the required number of qubits and gates of a quantum algorithm. A quantum algorithm implementation also depends on the used Software Development Kit which restricts the set of usable quantum computers. Because of the limited capabilities of current quantum computers, choosing an appropriate one to execute a certain implementation for a given input is a difficult challenge that requires immense mathematical knowledge about the implemented quantum algorithm as well as technical knowledge about the used Software Development Kits. In this paper, we present a concept for the automated analysis and selection of implementations of quantum algorithms and appropriate quantum computers that can execute a selected implementation with a certain input data. The practical feasibility of the concept is demonstrated by the prototypical implementation of a tool that we call NISQ Analyzer.},
	booktitle = {Communications in {Computer} and {Information} {Science}},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Salm, Marie and Barzen, Johanna and Breitenbücher, Uwe and Leymann, Frank and Weder, Benjamin and Wild, Karoline},
	year = {2020},
	doi = {10.1007/978-3-030-64846-6_5},
	keywords = {Decision support, Hardware selection, Implementation selection, NISQ analyzer, Quantum algorithms, Quantum computing, resource allocation, ★},
	pages = {66--85},
}

@inproceedings{weder_integrating_2020,
	title = {Integrating {Quantum} {Computing} into {Workflow} {Modeling} and {Execution}},
	isbn = {978-0-7381-2394-3},
	url = {https://ieeexplore.ieee.org/document/9302814/},
	doi = {10.1109/UCC48980.2020.00046},
	abstract = {Quantum computing has the potential to significantly impact many application domains, as several quantum algorithms are promising to solve problems more efficiently than possible on classical computers. However, various complex pre- and post-processing tasks have to be performed when executing a quantum circuit, which require immense mathematical and technical knowledge. For example, calculations on today's quantum computers are noisy and require an error mitigation task after the execution. Hence, integrating classical applications with quantum circuits is a difficult challenge. In this paper, we introduce a modeling extension for imperative workflow languages to enable the integration of quantum computations and ease the orchestration of classical applications and quantum circuits. Further, we show how the extension can be mapped to native modeling constructs of extended workflow languages to retain the portability of the workflows. We validate the practical feasibility of our approach by applying our proposed extension to BPMN and introduce Quantum4BPMN.},
	booktitle = {2020 {IEEE}/{ACM} 13th {International} {Conference} on {Utility} and {Cloud} {Computing} ({UCC})},
	publisher = {IEEE},
	author = {Weder, Benjamin and Breitenbucher, Uwe and Leymann, Frank and Wild, Karoline},
	month = dec,
	year = {2020},
	keywords = {Modeling Extension, Quantum Applications, Quantum Computing, Quantum Software, Workflow Technology, hybrid qc, ★},
	pages = {279--291},
}

@incollection{bera_crown_2020,
	address = {Singapore},
	title = {The {Crown} {Jewels} of {Quantum} {Algorithms}},
	isbn = {978-981-15-2471-4},
	url = {https://doi.org/10.1007/978-981-15-2471-4_10},
	abstract = {This chapter provides detailed descriptions of the most intellectually valued algorithms in quantum computing, including Peter Shor's factoring algorithmQuantum algorithmsShor's factoring algorithmand Lov Grover search algorithm, among others. An attempt is made to explain the subtle aspects of the algorithms and why such algorithms are valued.},
	booktitle = {The {Amazing} {World} of {Quantum} {Computing}},
	publisher = {Springer Nature Singapore},
	author = {Bera, Rajendra K},
	year = {2020},
	doi = {10.1007/978-981-15-2471-4_10},
	pages = {207--240},
}

@article{zhang_poisoning_2019,
	title = {Poisoning attack in federated learning using generative adversarial nets},
	doi = {10.1109/TrustCom/BigDataSE.2019.00057},
	abstract = {Federated learning is a novel distributed learning framework, where the deep learning model is trained in a collaborative manner among thousands of participants. The shares between server and participants are only model parameters, which prevent the server from direct access to the private training data. However, we notice that the federated learning architecture is vulnerable to an active attack from insider participants, called poisoning attack, where the attacker can act as a benign participant in federated learning to upload the poisoned update to the server so that he can easily affect the performance of the global model. In this work, we study and evaluate a poisoning attack in federated learning system based on generative adversarial nets (GAN). That is, an attacker first acts as a benign participant and stealthily trains a GAN to mimic prototypical samples of the other participants' training set which does not belong to the attacker. Then these generated samples will be fully controlled by the attacker to generate the poisoning updates, and the global model will be compromised by the attacker with uploading the scaled poisoning updates to the server. In our evaluation, we show that the attacker in our construction can successfully generate samples of other benign participants using GAN and the global model performs more than 80\% accuracy on both poisoning tasks and main tasks.},
	journal = {Proceedings - 2019 18th IEEE International Conference on Trust, Security and Privacy in Computing and Communications/13th IEEE International Conference on Big Data Science and Engineering, TrustCom/BigDataSE 2019},
	author = {Zhang, Jiale and Chen, Junjun and Wu, Di and Chen, Bing and Yu, Shui},
	year = {2019},
	note = {ISBN: 9781728127767},
	keywords = {Federated learning, Generative adversarial nets, Poisoning attack, Privacy, Security},
	pages = {374--380},
}

@article{li_serverless_2022,
	title = {The {Serverless} {Computing} {Survey}: {A} {Technical} {Primer} for {Design} {Architecture}},
	issn = {0360-0300},
	doi = {10.1145/3508360},
	abstract = {The development of cloud infrastructures inspires the emergence of cloud-native computing. As the most promising architecture for deploying microservices, serverless computing has recently attracted more and more attention in both industry and academia. Due to its inherent scalability and flexibility, serverless computing becomes attractive and more pervasive for ever-growing Internet services. Despite the momentum in the cloud-native community, the existing challenges and compromises still wait for more advanced research and solutions to further explore the potentials of the serverless computing model. As a contribution to this knowledge, this article surveys and elaborates the research domains in the serverless context by decoupling the architecture into four stack layers: Virtualization, Encapsule, System Orchestration, and System Coordination. Inspired by the security model, we highlight the key implications and limitations of these works in each layer, and make suggestions for potential challenges to the field of future serverless computing.},
	journal = {ACM Computing Surveys},
	author = {Li, Zijun and Guo, Linsong and Cheng, Jiagan and Chen, Quan and He, BingSheng and Guo, Minyi},
	year = {2022},
	note = {arXiv: 2112.12921},
	keywords = {serverless computing, architecture design, FaaS, L},
	pages = {1--34},
}

@article{shafiei_serverless_2022,
	title = {Serverless {Computing}: {A} {Survey} of {Opportunities}, {Challenges}, and {Applications}},
	issn = {0360-0300},
	doi = {10.1145/3510611},
	abstract = {The emerging serverless computing paradigm has attracted attention from both academia and industry. This paradigm brings benefits such as less operational complexity, a pay-as-you-go pricing model, and an auto-scaling feature. The paradigm opens up new opportunities and challenges for cloud application developers. In this paper, we present a comprehensive overview of the past development as well as the recent advances in research areas related to serverless computing. First, we survey serverless applications introduced in the literature. We categorize applications in eight domains and separately discuss the objectives and the viability of the serverless paradigm along with challenges in each of those domains. We then classify those challenges into nine topics and survey the proposed solutions. Finally, we present the areas that need further attention from the research community and identify open problems.},
	journal = {ACM Computing Surveys},
	author = {Shafiei, Hossein and Khonsari, Ahmad and Mousavi, Payam},
	year = {2022},
	note = {arXiv: 1911.01296},
	keywords = {Cloud Services, Serverless Computing, Function-as-},
	pages = {1--31},
}

@article{banaei_etas_2022,
	title = {{ETAS}: predictive scheduling of functions on worker nodes of {Apache} {OpenWhisk} platform},
	volume = {78},
	issn = {15730484},
	url = {https://doi.org/10.1007/s11227-021-04057-z},
	doi = {10.1007/s11227-021-04057-z},
	abstract = {Fast execution of functions is an inevitable challenge in the serverless computing landscape. Inefficient dispatching, fluctuations in invocation rates, burstiness of workloads, and wide range of execution times of serverless functions result in load imbalances and overloading of worker nodes of serverless platforms that impose high latency on invocations. This paper is concentrated on function scheduling within worker nodes of Apache OpenWhisk serverless platform and presents ETAS, a predictive scheduling scheme to reduce response times of invocations besides increasing workers throughputs and resource utilizations. ETAS schedules functions using their execution times, estimated by their previous execution’s history, arrival times, and containers’ status. We have implemented ETAS in Apache OpenWhisk and show that, compared to the OpenWhisk worker scheduler and several queue scheduling schemes, ETAS outperforms others by reducing the average waiting time by 30\% and increasing the throughput by 40\%.},
	number = {4},
	journal = {Journal of Supercomputing},
	author = {Banaei, Ali and Sharifi, Mohsen},
	year = {2022},
	note = {Publisher: Springer US},
	keywords = {Apache OpenWhisk, Function scheduling, Latency, Predictive scheduling, Serverless computing platforms},
	pages = {5358--5393},
}

@article{wang_achieving_2009,
	title = {Achieving high and consistent rendering performance of java {AWT}/{Swing} on multiple platforms},
	volume = {39},
	issn = {00380644},
	doi = {10.1002/spe},
	abstract = {Wang et al. (Softw. Pract. Exper. 2007; 37(7):727-745) observed a phenomenon of performance inconsistency in the graphics of Java Abstract Window Toolkit (AWT)/Swing among different Java runtime environments (JREs) on Windows XP. This phenomenon makes it difficult to predict the performance of Java game applications. Therefore, they proposed a portable AWT/Swing architecture, called CYC Window Toolkit (CWT), to provide programmers with high and consistent rendering performance for Java game development among different JREs. They implemented a DirectX version to demonstrate the feasibility of the architecture. This paper extends the above research to other environments in two aspects. First, we evaluate the rendering performance of the original Java AWT with different combinations of JREs, image application programming interfaces, system properties and operating systems (OSs), including Windows XP, Windows Vista, Fedora and Mac OS X. The evaluation results indicate that the performance inconsistency of Java AWT also exists among the four OSs, even if the same hardware configuration is used. Second, we design an OpenGL version of CWT, named CWT-GL, to take advantage of modern 3D graphics cards, and compare the rendering performance of CWT with Java AWT/Swing. The results show that CWT-GL achieves more consistent and higher rendering performance in JREs 1.4to 1.6 on the four OSs. The results also hint at two approaches: (a) decouple the rendering pipelines of Java AWT/Swing from the JREs for faster upgrading and supporting old JREs and (b) use other graphics libraries, such as CWT, instead of Java AWT/Swing to develop cross-platform Java games with higher and more consistent rendering performance © 2009 John Wiley \& Sons, Ltd.},
	number = {7},
	journal = {Software - Practice and Experience},
	author = {Wang, Yi Hsien and Wu, I. Chen},
	year = {2009},
	keywords = {CYC Window Toolkit, Directx, Linux, Mac OS x, OpenGL, Windows},
	pages = {701--736},
}

@article{assuncao_big_2015,
	title = {Big {Data} computing and clouds: {Trends} and future directions},
	volume = {79-80},
	issn = {07437315},
	url = {http://dx.doi.org/10.1016/j.jpdc.2014.08.003},
	doi = {10.1016/j.jpdc.2014.08.003},
	abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Assunção, Marcos D. and Calheiros, Rodrigo N. and Bianchi, Silvia and Netto, Marco A.S. and Buyya, Rajkumar},
	year = {2015},
	note = {arXiv: 1312.4722
Publisher: Elsevier Inc.},
	keywords = {Analytics, Big Data, Cloud computing, Data management},
	pages = {3--15},
}

@article{versluis_survey_2021,
	title = {A survey of domains in workflow scheduling in computing infrastructures: {Community} and keyword analysis, emerging trends, and taxonomies},
	volume = {123},
	issn = {0167739X},
	url = {https://doi.org/10.1016/j.future.2021.04.009},
	doi = {10.1016/j.future.2021.04.009},
	abstract = {Workflows are prevalent in today's computing infrastructures as they support many domains. Different Quality of Service (QoS) requirements of both users and providers makes workflow scheduling challenging. Meeting the challenge requires an overview of state-of-art in workflow scheduling. Sifting through literature to find the state-of-art can be daunting, for both newcomers and experienced researchers. Surveys are an excellent way to address questions regarding the different techniques, policies, emerging areas, and opportunities present, yet they rarely take a systematic approach and publish their tools and data on which they are based. Moreover, the communities behind these articles are rarely studied. We attempt to address these shortcomings in this work. We introduce and open-source an instrument used to combine and store article meta-data. Using this meta-data, we characterize and taxonomize the workflow scheduling community and four areas within workflow scheduling: (1) the workflow formalism, (2) workflow allocation, (3) resource provisioning, and (4) applications and services. In each characterization, we obtain important keywords overall and per year, identify keywords growing in importance, get insight into the structure and relations within each community, and perform a systematic literature survey per part to validate and complement our taxonomies},
	journal = {Future Generation Computer Systems},
	author = {Versluis, Laurens and Iosup, Alexandru},
	month = oct,
	year = {2021},
	keywords = {Allocation, Applications, Cloud, Cluster, Community, Formalism, Grid, Meta-analysis, Policies, Provisioning, Scheduling, Services, Survey, Taxonomy, Workflow},
	pages = {156--177},
}

@article{jonas_cloud_2019,
	title = {Cloud {Programming} {Simplified}: {A} {Berkeley} {View} on {Serverless} {Computing}},
	url = {http://arxiv.org/abs/1902.03383},
	abstract = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.},
	author = {Jonas, Eric and Schleier-Smith, Johann and Sreekanti, Vikram and Tsai, Chia-Che and Khandelwal, Anurag and Pu, Qifan and Shankar, Vaishaal and Carreira, Joao and Krauth, Karl and Yadwadkar, Neeraja and Gonzalez, Joseph E and Popa, Raluca Ada and Stoica, Ion and Patterson, David A},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.03383},
	keywords = {★},
}

@inproceedings{eskandani_wonderless_2021,
	title = {The {Wonderless} {Dataset} for {Serverless} {Computing}},
	isbn = {978-1-72818-710-5},
	url = {https://ieeexplore.ieee.org/document/9463099/},
	doi = {10.1109/MSR52588.2021.00075},
	abstract = {Function as a Service (FaaS) has grown in popularity in recent years, with an increasing number of applications following the Serverless computing model. Serverless computing supports out of the box autoscaling in a pay-as-you-go manner, letting developers focus on the application logic rather than worrying about resource management. With the increasing adoption of the this model, researchers have started studying a wide variety of aspects of Serverless computing, including communication, security, performance, and cost optimization. Yet, we still know very little of how Serverless computing is used in practice.In this paper, we introduce Wonderless, a novel dataset of open-source Serverless applications. Wonderless consists of 1, 877 real-world Serverless applications extracted from GitHub, and it can be used as a data source for further research in the Serverless ecosystem, such as performance evaluation and software mining. To the best of our knowledge, Wonderless is currently the most diverse and largest dataset for research on Serverless computing.},
	booktitle = {2021 {IEEE}/{ACM} 18th {International} {Conference} on {Mining} {Software} {Repositories} ({MSR})},
	publisher = {IEEE},
	author = {Eskandani, Nafise and Salvaneschi, Guido},
	month = may,
	year = {2021},
	keywords = {Cloud Computing, FaaS, Function as a Service, Serverless},
	pages = {565--569},
}

@article{wang_peeking_2020,
	title = {Peeking behind the curtains of serverless platforms},
	abstract = {Serverless computing is an emerging paradigm in which an application's resource provisioning and scaling are managed by third-party services. Examples include AWS Lambda, Azure Functions, and Google Cloud Functions. Behind these services' easy-to-use APIs are opaque, complex infrastructure and management ecosystems. Taking on the viewpoint of a serverless customer, we conduct the largest measurement study to date, launching more than 50,000 function instances across these three services, in order to characterize their architectures, performance, and resource management efficiency. We explain how the platforms isolate the functions of different accounts, using either virtual machines or containers, which has important security implications. We characterize performance in terms of scalability, coldstart latency, and resource efficiency, with highlights including that AWS Lambda adopts a bin-packing-like strategy to maximize VM memory utilization, that severe contention between functions can arise in AWS and Azure, and that Google had bugs that allow customers to use resources for free.},
	journal = {Proceedings of the 2018 USENIX Annual Technical Conference, USENIX ATC 2018},
	author = {Wang, Liang and Li, Mengyuan and Zhang, Yinqian and Ristenpart, Thomas and Swift, Michael},
	year = {2020},
	note = {ISBN: 9781939133021},
	keywords = {★},
	pages = {133--145},
}

@article{castro_rise_2019,
	title = {The rise of serverless computing},
	volume = {62},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3368454},
	doi = {10.1145/3368454},
	abstract = {The server is dead, long live the server.},
	number = {12},
	journal = {Communications of the ACM},
	author = {Castro, Paul and Ishakian, Vatche and Muthusamy, Vinod and Slominski, Aleksander},
	month = nov,
	year = {2019},
	keywords = {★},
	pages = {44--54},
}

@article{van_eyk_serverless_2018,
	title = {Serverless is {More}: {From} {PaaS} to {Present} {Cloud} {Computing}},
	volume = {22},
	issn = {19410131},
	doi = {10.1109/MIC.2018.053681358},
	abstract = {In the late-1950s, leasing time on an IBM 704 cost hundreds of dollars per minute. Today, cloud computing, that is, using IT as a service, on-demand and pay-per-use, is a widely used computing paradigm that offers large economies of scale. Born from a need to make platform as a service (PaaS) more accessible, fine-grained, and affordable, serverless computing has garnered interest from both industry and academia. This article aims to give an understanding of these early days of serverless computing: what it is, where it comes from, what is the current status of serverless technology, and what are its main obstacles and opportunities.},
	number = {5},
	journal = {IEEE Internet Computing},
	author = {Van Eyk, Erwin and Toader, Lucian and Talluri, Sacheendra and Versluis, Laurens and Uta, Alexandru and Iosup, Alexandru},
	year = {2018},
	keywords = {cloud computing, function-as-a-service, internet, internet computing, serverless, workflows, ★},
	pages = {8--17},
}

@article{aditya_will_2019,
	title = {Will {Serverless} {Computing} {Revolutionize} {NFV}?},
	volume = {107},
	issn = {0018-9219},
	url = {https://ieeexplore.ieee.org/document/8653379/},
	doi = {10.1109/JPROC.2019.2898101},
	abstract = {Communication networks need to be both adaptive and scalable. The last few years have seen an explosive growth of software-defined networking (SDN) and network function virtualization (NFV) to address this need. Both technologies help enable networking software to be decoupled from the hardware so that software functionality is no longer constrained by the underlying hardware and can evolve independently. Both SDN and NFV aim to advance a software-based approach to networking, where networking functionality is implemented in software modules and executed on a suitable cloud computing platform. Achieving this goal requires the virtualization paradigm used in these services that play an important role in the transition to software-based networks. Consequently, the corresponding computing platforms accompanying the virtualization technologies need to provide the required agility, robustness, and scalability for the services executed. Serverless computing has recently emerged as a new paradigm in virtualization and has already significantly changed the economics of offloading computations to the cloud. It is considered as a low-latency, resource-efficient, and rapidly deployable alternative to traditional virtualization approaches, such as those based on virtual machines and containers. Serverless computing provides scalability and cost reduction, without requiring any additional configuration overhead on the part of the developer. In this paper, we explore and survey how serverless computing technology can help building adaptive and scalable networks and show the potential pitfalls of doing so.},
	number = {4},
	journal = {Proceedings of the IEEE},
	author = {Aditya, Paarijaat and Akkus, Istemi Ekin and Beck, Andre and Chen, Ruichuan and Hilt, Volker and Rimac, Ivica and Satzke, Klaus and Stein, Manuel},
	month = apr,
	year = {2019},
	keywords = {Application virtualization, cloud computing, edge computing, network function virtualization (NFV), serverless computing, software-defined networking (SDN)},
	pages = {667--678},
}

@article{jindal_function_2021,
	title = {Function delivery network: {Extending} serverless computing for heterogeneous platforms},
	volume = {51},
	issn = {1097024X},
	doi = {10.1002/spe.2966},
	abstract = {Serverless computing has rapidly grown following the launch of Amazon's Lambda platform. Function-as-a-Service (FaaS) a key enabler of serverless computing allows an application to be decomposed into simple, standalone functions that are executed on a FaaS platform. The FaaS platform is responsible for deploying and facilitating resources to the functions. Several of today's cloud applications spread over heterogeneous connected computing resources and are highly dynamic in their structure and resource requirements. However, FaaS platforms are limited to homogeneous clusters and homogeneous functions and do not account for the data access behavior of functions before scheduling. We introduce an extension of FaaS to heterogeneous clusters and to support heterogeneous functions through a network of distributed heterogeneous target platforms called Function Delivery Network (FDN). A target platform is a combination of a cluster of homogeneous nodes and a FaaS platform on top of it. FDN provides Function-Delivery-as-a-Service (FDaaS), delivering the function to the right target platform. We showcase the opportunities such as varied target platform's characteristics, possibility of collaborative execution between multiple target platforms, and localization of data that the FDN offers in fulfilling two objectives: Service Level Objective (SLO) requirements and energy efficiency when scheduling functions by evaluating over five distributed target platforms using the FDNInspector, a tool developed by us for benchmarking distributed target platforms. Scheduling functions on an edge target platform in our evaluation reduced the overall energy consumption by 17× without violating the SLO requirements in comparison to scheduling on a high-end target platform.},
	number = {9},
	journal = {Software - Practice and Experience},
	author = {Jindal, Anshul and Gerndt, Michael and Chadha, Mohak and Podolskiy, Vladimir and Chen, Pengfei},
	year = {2021},
	note = {arXiv: 2102.02330},
	keywords = {cloud computing, edge computing, function delivery network, function-as-a-service, heterogeneous Faas, heterogeneous platforms, high-performance computing, serverless computing, ★},
	pages = {1936--1963},
}

@article{schleier-smith_what_2021,
	title = {What serverless computing is and should become},
	volume = {64},
	issn = {15577317},
	doi = {10.1145/3406011},
	number = {5},
	journal = {Communications of the ACM},
	author = {Schleier-Smith, Johann and Sreekanti, Vikram and Khandelwal, Anurag and Carreira, Joao and Yadwadkar, Neeraja J. and Popa, Raluca Ada and Gonzalez, Joseph E. and Stoica, Ion and Patterson, David A.},
	year = {2021},
	keywords = {★},
	pages = {76--84},
}

@article{risco_serverless_2021,
	title = {Serverless {Workflows} for {Containerised} {Applications} in the {Cloud} {Continuum}},
	volume = {19},
	issn = {15729184},
	doi = {10.1007/s10723-021-09570-2},
	abstract = {This paper introduces an open-source platform to support serverless computing for scientific data-processing workflow-based applications across the Cloud continuum (i.e. simultaneously involving both on-premises and public Cloud platforms to process data captured at the edge). This is achieved via dynamic resource provisioning for FaaS platforms compatible with scale-to-zero approaches that minimise resource usage and cost for dynamic workloads with different elasticity requirements. The platform combines the usage of dynamically deployed auto-scaled Kubernetes clusters on on-premises Clouds and automated Cloud bursting into AWS Lambda to achieve higher levels of elasticity. A use case in public health for smart cities is used to assess the platform, in charge of detecting people not wearing face masks from captured videos. Faces are blurred for enhanced anonymity in the on-premises Cloud and detection via Deep Learning models is performed in AWS Lambda for this data-driven containerised workflow. The results indicate that hybrid workflows across the Cloud continuum can efficiently perform local data processing for enhanced regulations compliance and perform Cloud bursting for increased levels of elasticity.},
	number = {3},
	journal = {Journal of Grid Computing},
	author = {Risco, Sebastián and Moltó, Germán and Naranjo, Diana M. and Blanquer, Ignacio},
	year = {2021},
	keywords = {Cloud computing, Containers, Serverless computing, Workflow},
}

@article{eismann_serverless_2021,
	title = {Serverless {Applications}: {Why}, {When}, and {How}?},
	volume = {38},
	issn = {0740-7459},
	url = {https://ieeexplore.ieee.org/document/9190031/},
	doi = {10.1109/MS.2020.3023302},
	abstract = {Why do so many companies adopt serverless? When are serverless applications well suited? How are they currently implemented? To address these questions, we analyze 89 serverless applications from open source projects, industrial sources, academic literature, and scientific computing-presenting the most extensive study to date.},
	number = {1},
	journal = {IEEE Software},
	author = {Eismann, Simon and Scheuner, Joel and van Eyk, Erwin and Schwinger, Maximilian and Grohmann, Johannes and Herbst, Nikolas and Abad, Cristina L. and Iosup, Alexandru},
	month = jan,
	year = {2021},
	note = {arXiv: 2009.08173},
	keywords = {★},
	pages = {32--39},
}

@article{barcelona-pons_faas_2019,
	title = {On the {FaaS} track: {Building} stateful distributed applications with serverless architectures},
	doi = {10.1145/3361525.3361535},
	abstract = {Serverless computing is an emerging paradigm that greatly simplifies the usage of cloud resources and suits well to many tasks. Most notably, Function-as-a-Service (FaaS) enables programmers to develop cloud applications as individual functions that can run and scale independently. Yet, due to the disaggregation of storage and compute resources in FaaS, applications that require fine-grained support for mutable state and synchronization, such as machine learning and scientific computing, are hard to build. In this work, we present Crucial, a system to program highly-concurrent stateful applications with serverless architectures. Its programming model keeps the simplicity of FaaS and allows to port effortlessly multi-threaded algorithms to this new environment. Crucial is built upon the key insight that FaaS resembles to concurrent programming at the scale of a data center. As a consequence, a distributed shared memory layer is the right answer to the need for fine-grained state management and coordination in serverless. We validate our system with the help of micro-benchmarks and various applications. In particular, we implement two common machine learning algorithms: k-means clustering and logistic regression. For both cases, Crucial obtains superior or comparable performance to an equivalent Spark cluster.},
	journal = {Middleware 2019 - Proceedings of the 2019 20th International Middleware Conference},
	author = {Barcelona-Pons, Daniel and Sánchez-Artigas, Marc and París, Gerard and Sutra, Pierre and García-López, Pedro},
	year = {2019},
	note = {ISBN: 9781450370097},
	keywords = {FaaS, In-memory, Serverless, Stateful, Synchronization, ★},
	pages = {41--54},
}

@article{cicconetti_uncoordinated_2020,
	title = {Uncoordinated access to serverless computing in {MEC} systems for {IoT}},
	volume = {172},
	issn = {13891286},
	url = {https://doi.org/10.1016/j.comnet.2020.107184},
	doi = {10.1016/j.comnet.2020.107184},
	abstract = {Edge computing is a promising solution to enable low-latency Internet of Things (IoT) applications, by shifting computation from remote data centers to local devices, less powerful but closer to the end user devices. However, this creates the challenge on how to best assign clients to edge nodes offering compute capabilities. So far, two antithetical architectures are proposed: centralized resource orchestration or distributed overlay. In this work we explore a third way, called uncoordinated access, which consists in letting every device exploring multiple opportunities, to opportunistically embrace the heterogeneity of network and load conditions towards diverse edge nodes. In particular, our contribution is intended for emerging serverless IoT applications, which do not have a state on the edge nodes executing tasks. We model the proposed system as a set of M/M/1 queues and show that it achieves a smaller delay than single edge node allocation. Furthermore, we compare uncoordinated access with state-of-the-art centralized and distributed alternatives in testbed experiments under more realistic conditions. Based on the results, our proposed approach, which requires a tiny fraction of the complexity of the alternatives in both the device and network components, is very effective in using the network resources, while incurring only a small penalty in terms of increased compute load and high percentiles of delay.},
	number = {October 2019},
	journal = {Computer Networks},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	year = {2020},
	note = {Publisher: Elsevier B.V.},
	keywords = {Computation offloading, Distributed cloud, Internet of Things, Mobile Edge Computing, Online job dispatching, Performance evaluation, Serverless computing},
	pages = {107184},
}

@article{denninnart_harnessing_2022,
	title = {Harnessing the {Potential} of {Function}-{Reuse} in {Multimedia} {Cloud} {Systems}},
	volume = {33},
	issn = {15582183},
	doi = {10.1109/TPDS.2021.3097911},
	abstract = {Cloud-based computing systems can get oversubscribed due to the budget constraints of their users or limitations in certain resource types. The oversubscription can, in turn, degrade the users perceived Quality of Service (QoS). The approach we investigate to mitigate both the oversubscription and the incurred cost is based on smart reusing of the computation needed to process the service requests (i.e., tasks). We propose a reusing paradigm for the tasks that are waiting for execution. This paradigm can be particularly impactful in serverless platforms where multiple users can request similar services simultaneously. Our motivation is a multimedia streaming engine that processes the media segments in an on-demand manner. We propose a mechanism to identify various types of 'mergeable' tasks and aggregate them to improve the QoS and mitigate the incurred cost. We develop novel approaches to determine when and how to perform task aggregation such that the QoS of other tasks is not affected. Evaluation results show that the proposed mechanism can improve the QoS by significantly reducing the percentage of tasks missing their deadlines and reduce the overall time (and subsequently the incurred cost) of utilizing cloud services by more than 9 percent.},
	number = {3},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Denninnart, Chavit and Salehi, Mohsen Amini},
	year = {2022},
	note = {arXiv: 2104.04474
Publisher: IEEE},
	keywords = {Task aggregation, cloud computing, over-subscription, serverless computing, video stream processing},
	pages = {617--629},
}

@article{martins_benchmarking_2020,
	title = {Benchmarking {Serverless} {Computing} {Platforms}},
	volume = {18},
	issn = {15729184},
	doi = {10.1007/s10723-020-09523-1},
	abstract = {We propose a benchmarking test suite to evaluate performance of cloud serverless platforms and an open source software tool to automate the test process. Additionally, we used this setup to compare the commercial offers of Amazon, Google, Microsoft, and IBM. The work builds on ideas and experiments reported in the literature that, nevertheless, did not offer a “standard” set of coherent and comprehensive tests, capable of analyzing diverse serverless platforms, across the same set of features. To that end, we defined seven tests that cover scalability (latency and throughput), the effect of allocated memory, the performance for CPU-bound cases, the impact of payload size, the influence of the programming language, resource management (e.g., reuse of containers), and overall platform overhead. We created a software tool to deploy the test code and collect metrics in a manner agnostic to the serverless platforms under study. At a time when serverless computing popularity is rising, this benchmarking suite and software test tool enable developers to take informed decisions on the suitability and performance of each provider’s serverless offer. Additionally, they also help to identify attributes in need for improvement in the existing platforms.},
	number = {4},
	journal = {Journal of Grid Computing},
	author = {Martins, Horácio and Araujo, Filipe and da Cunha, Paulo Rupino},
	year = {2020},
	note = {ISBN: 1072302009523},
	keywords = {Benchmarking, Cloud computing, Serverless computing},
	pages = {691--709},
}

@article{cicconetti_toward_2020,
	title = {Toward {Distributed} {Computing} {Environments} with {Serverless} {Solutions} in {Edge} {Systems}},
	volume = {58},
	issn = {15581896},
	doi = {10.1109/MCOM.001.1900498},
	abstract = {Computation offloading through stateless applications is gaining momentum thanks to the emergence of serverless frameworks with inherent scalability properties. However, adoption of a serverless framework in an edge computing system requires careful consideration to keep its advantages unscathed. In the cloud, micro-services are scaled automatically according to demands, but in edge computing this would incur a significantly higher cost than in a data center and cannot be as fluid. This is especially relevant in scenarios where edge nodes are spread across large areas and have relatively small computation capabilities. In this article we propose to overcome this issue by adapting the allocation of demands to the currently allocated micro-services at short timescales, with two alternative mechanisms designed for different target scenarios, both aimed at enabling distributed computing environments. The proposed solution can be integrated within the ETSI MEC standard, which specifies a reference architecture and open service interfaces. Our contribution is validated in a proof-of-concept scenario with a prototype implementation released as open source.},
	number = {3},
	journal = {IEEE Communications Magazine},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea and Sabella, Dario},
	year = {2020},
	note = {Publisher: IEEE},
	pages = {40--46},
}

@article{cicconetti_decentralized_2021,
	title = {A {Decentralized} {Framework} for {Serverless} {Edge} {Computing} in the {Internet} of {Things}},
	volume = {18},
	issn = {19324537},
	doi = {10.1109/TNSM.2020.3023305},
	abstract = {Serverless computing is becoming widely adopted among cloud providers, thus making increasingly popular the Function-as-a-Service (FaaS) programming model, where the developers realize services by packaging sequences of stateless function calls. The current technologies are very well suited to data centers, but cannot provide equally good performance in decentralized environments, such as edge computing systems, which are expected to be typical for Internet of Things (IoT) applications. In this article, we fill this gap by proposing a framework for efficient dispatching of stateless tasks to in-network executors so as to minimize the response times while exhibiting short- and long-term fairness, also leveraging information from a virtualized network infrastructure when available. Our solution is shown to be simple enough to be installed on devices with limited computational capabilities, such as IoT gateways, especially when using a hierarchical forwarding extension. We evaluate the proposed platform by means of extensive emulation experiments with a prototype implementation in realistic conditions. The results show that it is able to smoothly adapt to the mobility of clients and to the variations of their service request patterns, while coping promptly with network congestion.},
	number = {2},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Cicconetti, Claudio and Conti, Marco and Passarella, Andrea},
	year = {2021},
	keywords = {Internet of Things services, computer simulation experiments, overlay networks, software-defined networking},
	pages = {2166--2180},
}

@article{deng_master_2021,
	title = {Master {Graduation} {Thesis}: {A} {Lightweight} and {Distributed} {Container}-based {Framework}},
	url = {http://arxiv.org/abs/2108.03562},
	abstract = {Edge/Fog computing is a novel computing paradigm that provides resource-limited Internet of Things (IoT) devices with scalable computing and storage resources. Compared to cloud computing, edge/fog servers have fewer resources, but they can be accessed with higher bandwidth and less communication latency. Thus, integrating edge/fog and cloud infrastructures can support the execution of diverse latency-sensitive and computation-intensive IoT applications. Although some frameworks attempt to provide such integration, there are still several challenges to be addressed, such as dynamic scheduling of different IoT applications, scalability mechanisms, multi-platform support, and supporting different interaction models. To overcome these challenges, we propose a lightweight and distributed container-based framework, called FogBus2. It provides a mechanism for scheduling heterogeneous IoT applications and implements several scheduling policies. Also, it proposes an optimized genetic algorithm to obtain fast convergence to well-suited solutions. Besides, it offers a scalability mechanism to ensure efficient responsiveness when either the number of IoT devices increases or the resources become overburdened. Also, the dynamic resource discovery mechanism of FogBus2 assists new entities to quickly join the system. We have also developed two IoT applications, called Conway's Game of Life and Video Optical Character Recognition to demonstrate the effectiveness of FogBus2 for handling real-time and non-real-time IoT applications. Experimental results show FogBus2's scheduling policy improves the response time of IoT applications by 53{\textbackslash}\% compared to other policies. Also, the scalability mechanism can reduce up to 48{\textbackslash}\% of the queuing waiting time compared to frameworks that do not support scalability.},
	number = {August},
	author = {Deng, Qifan and Buyya, Rajkumar},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.03562},
}

@article{gill_quantum_2021,
	title = {Quantum and blockchain based {Serverless} edge computing: {A} vision, model, new trends and future directions},
	issn = {2476-1508},
	doi = {10.1002/itl2.275},
	abstract = {Serverless computing has become an important cloud model delivering service to users based on the amount of resources consumed by the Internet of Things (IoT) applications. Edge computing can be utilized for Serverless computing to process small or deadline oriented jobs efficiently at edge devices helping to reduce latency. Furthermore, security and computation speed have become important challenges for Serverless edge computing. Blockchain and Quantum computing can be used to provide secure and reliable edge Serverless service while also improving security and computation speed respectively. This letter identifies and integrates various emerging paradigms to develop a conceptual model for Quantum and Blockchain based Serverless edge computing. Promising future research directions and open challenges are also discussed.},
	number = {December 2020},
	journal = {Internet Technology Letters},
	author = {Gill, Sukhpal Singh},
	year = {2021},
	keywords = {artificial intelligence, blockchain, edge computing, internet of things, quantum computing},
	pages = {4--9},
}

@article{aaronson_limits_2008,
	title = {The {Limits} of {Quantum} {Computers}},
	volume = {298},
	issn = {0036-8733},
	url = {https://www.scientificamerican.com/article/the-limits-of-quantum-computers},
	doi = {10.1038/scientificamerican0308-62},
	number = {3},
	journal = {Scientific American},
	author = {Aaronson, Scott},
	month = mar,
	year = {2008},
	pages = {62--69},
}

@misc{ibmq-roadmap,
	title = {{IBM} {Quantum} {Development} {Roadmap} 2022},
	url = {https://www.ibm.com/quantum/roadmap},
	author = {{IBM Quantum}},
	year = {2022},
}

@article{qdevops,
	title = {Quantum {DevOps}: {Towards} {Reliable} and {Applicable} {NISQ} {Quantum} {Computing}},
	doi = {10.1109/GCWkshps50303.2020.9367411},
	abstract = {Quantum Computing is emerging as one of the great hopes for boosting current computational resources and enabling the application of ICT for optimizing processes and solving complex and challenging domain specific problems. However, the Quantum Computing technology has not matured to a level where it can provide a clear advantage over high performance computing yet. Towards achieving this 'quantum advantage', a larger number of Qubits is required, leading inevitably to a more complex topology of the computing Qubits. This raises additional difficulties with decoherence times and implies higher Qubit error rates. Nevertheless, the current Noisy Intermediate-Scale Quantum (NISQ) computers can prove useful despite the intrinsic uncertainties on the quantum hardware layer. In order to utilize such error-prone computing resources, various concepts are required to address Qubit errors and to deliver successful computations. In this paper describe and motivate the need for the novel concept of Quantum DevOps. which entails regular checking of the reliability of NISQ Quantum Computing (QC) instances. By means of testing the computational reliability of basic quantum gates and computations (C-NOT, Hadamard, etc.)it consequently estimates the likelihood for a large scale critical computation (e.g. calculating hourly traffic flow models for a city) to provide results of sufficient quality. Following this approach to select the best matching (cloud) QC instance and having it integrated directly with the processes of development, testing and finally the operations of quantum based algorithms and systems enables the Quantum DevOps concept.},
	journal = {2020 IEEE Globecom Workshops, GC Wkshps 2020 - Proceedings},
	author = {Gheorghe-Pop, Ilie Daniel and Tcholtchev, Nikolay and Ritter, Tom and Hauswirth, Manfred},
	year = {2020},
	note = {ISBN: 9781728173078},
	keywords = {DevOps, Framework, IT, Quantum Computing, Quantum DevOps, Testing, ★},
}

@misc{qsdk-qsharp,
	title = {Q\# {Quantum} {Programming} {Language}},
	url = {https://github.com/microsoft/qsharp-language},
	urldate = {2022-03-28},
	author = {{Microsoft}},
	year = {2021},
}

@article{qsdk-tensorflow-quantum,
	title = {{TensorFlow} {Quantum}: {A} {Software} {Framework} for {Quantum} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2003.02989},
	abstract = {We introduce TensorFlow Quantum (TFQ), an open source library for the rapid prototyping of hybrid quantum-classical models for classical or quantum data. This framework offers high-level abstractions for the design and training of both discriminative and generative quantum models under TensorFlow and supports high-performance quantum circuit simulators. We provide an overview of the software architecture and building blocks through several examples and review the theory of hybrid quantum-classical neural networks. We illustrate TFQ functionalities via several basic applications including supervised learning for quantum classification, quantum control, simulating noisy quantum circuits, and quantum approximate optimization. Moreover, we demonstrate how one can apply TFQ to tackle advanced quantum learning tasks including meta-learning, layerwise learning, Hamiltonian learning, sampling thermal states, variational quantum eigensolvers, classification of quantum phase transitions, generative adversarial networks, and reinforcement learning. We hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage.},
	author = {Broughton, Michael and Verdon, Guillaume and McCourt, Trevor and Martinez, Antonio J. and Yoo, Jae Hyeon and Isakov, Sergei V. and Massey, Philip and Halavati, Ramin and Niu, Murphy Yuezhen and Zlokapa, Alexander and Peters, Evan and Lockwood, Owen and Skolik, Andrea and Jerbi, Sofiene and Dunjko, Vedran and Leib, Martin and Streif, Michael and Von Dollen, David and Chen, Hongxiang and Cao, Shuxiang and Wiersema, Roeland and Huang, Hsin-Yuan and McClean, Jarrod R. and Babbush, Ryan and Boixo, Sergio and Bacon, Dave and Ho, Alan K. and Neven, Hartmut and Mohseni, Masoud},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.02989},
}

@misc{qfw-strangeworks,
	title = {Strangeworks {Quantum} {Computing} {Platform}},
	url = {https://app.quantumcomputing.com/},
	author = {{Strangeworks}},
	year = {2022},
}

@misc{qfw-qiskit-runtime,
	title = {Qiskit {Runtime}},
	url = {https://github.com/Qiskit/qiskit-ibm-runtime},
	urldate = {2022-03-29},
	author = {{IBM Quantum}},
	year = {2021},
}

@article{qfw-quantumpath,
	title = {{QuantumPath} : {A} quantum software development platform},
	volume = {2021},
	issn = {0038-0644},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/spe.3064},
	doi = {10.1002/spe.3064},
	abstract = {Quantum computing has experienced a breakthrough. Several companies are taking up the challenge of designing and manufacturing quantum computers, and the supply of tools for quantum software development is growing all the time. This article addresses quantum software development toolkits and introduces the ‘QuantumPath’ platform. In developing QuantumPath, our aim is to fulfil certain principles such as: agnosticism, extensibility, integration, independency, optimisation, scalability, security, usability and software engineering support. This article presents both the architecture itself as well as the main tools that compose QuantumPath, in order to illustrate the support which platform provides to the development and execution of quantum software.},
	number = {December},
	journal = {Software: Practice and Experience},
	author = {Hevia, Jose Luis and Peterssen, Guido and Piattini, Mario},
	month = dec,
	year = {2021},
	keywords = {QuantumPath, quantum computing, quantum toolkits, ★},
	pages = {1--14},
}

@article{seo_measurement_2022,
	title = {Measurement {Crosstalk} {Errors} in {Cloud}-{Based} {Quantum} {Computing}},
	volume = {26},
	issn = {19410131},
	doi = {10.1109/MIC.2021.3133437},
	abstract = {Quantum technologies available currently contain noise in general, often dubbed noisy intermediate-scale quantum systems. We here present the verification of noise in measurement readout errors in cloud-based quantum computing services, IBMQ and Rigetti, by directly performing quantum detector tomography, and show that there exist measurement crosstalk errors. We provide the characterization and the quantification of noise in a quantum measurement of multiple qubits. We remark that entanglement is found as a source of crosstalk errors in a measurement of three qubits.},
	number = {1},
	journal = {IEEE Internet Computing},
	author = {Seo, Seungchan and Bae, Joonwoo},
	year = {2022},
	note = {arXiv: 2112.00984
Publisher: IEEE},
	pages = {26--33},
}

@article{berganza_gomez_towards_2022,
	title = {Towards {AutoQML}: {A} {Cloud}-{Based} {Automated} {Circuit} {Architecture} {Search} {Framework}},
	doi = {10.1109/ICSA-C54293.2022.00033},
	abstract = {The learning process of classical machine learning algorithms is tuned by hyperparameters that need to be customized to best learn and generalize from an input dataset. In recent years, Quantum Machine Learning (QML) has been gaining traction as a possible application of quantum computing which may provide quantum advantage in the future. However, quantum versions of classical machine learning algorithms introduce a plethora of additional parameters and circuit variations that have their own intricacies in being tuned.In this work, we take the first steps towards Automated Quantum Machine Learning (AutoQML). We propose a concrete description of the problem, and then develop a classical-quantum hybrid cloud architecture that allows for parallelized hyperparameter exploration and model training.As an application use-case, we train a quantum Generative Adversarial neural Network (qGAN) to generate energy prices that follow a known historic data distribution. Such a QML model can be used for various applications in the energy economics sector.},
	journal = {2022 IEEE 19th International Conference on Software Architecture Companion, ICSA-C 2022},
	author = {Berganza Gomez, Raul and O'Meara, Corey and Cortiana, Giorgio and Mendl, Christian B. and Bernabe-Moreno, Juan},
	year = {2022},
	note = {arXiv: 2202.08024
ISBN: 9781665494939},
	keywords = {cloud computing, parametrized quantum circuit, quantum machine learning, quantum neural network, software architecture},
	pages = {129--136},
}

@article{blinov_comparison_2021,
	title = {Comparison of cloud-based ion trap and superconducting quantum computer architectures},
	volume = {3},
	issn = {26390213},
	doi = {10.1116/5.0058187},
	abstract = {Quantum computing represents a radical departure from conventional approaches to information processing, offering the potential for solving problems that can never be approached classically. While large-scale quantum computer hardware is still in development, several quantum computing systems have recently become available as commercial cloud services. The authors compare the performance of IBMQ-16-Melbourne, IBMQ-Vigo, and Rigetti Aspen-8 superconducting systems, and IonQ ion trap systems on several simple quantum circuits and algorithms and examine component performance in the context of each system's architecture.},
	number = {3},
	journal = {AVS Quantum Science},
	author = {Blinov, S. and Wu, B. and Monroe, C.},
	year = {2021},
	note = {arXiv: 2102.00371},
}

@article{zhu_quantum_2021,
	title = {Quantum autoencoders for communication-efficient quantum cloud computing},
	url = {http://arxiv.org/abs/2112.12369},
	abstract = {In the model of quantum cloud computing, the server executes a computation on the quantum data provided by the client. In this scenario, it is important to reduce the amount of quantum communication between the client and the server. A possible approach is to transform the desired computation into a compressed version that acts on a smaller number of qubits, thereby reducing the amount of data exchanged between the client and the server. Here we propose quantum autoencoders for quantum gates (QAEGate) as a method for compressing quantum computations. We illustrate it in concrete scenarios of single-round and multi-round communication and validate it through numerical experiments. A bonus of our method is it does not reveal any information about the server's computation other than the information present in the output.},
	author = {Zhu, Yan and Bai, Ge and Wang, Yuexuan and Li, Tongyang and Chiribella, Giulio},
	year = {2021},
	note = {arXiv: 2112.12369},
}

@article{soeparno_cloud_2021,
	title = {Cloud {Quantum} {Computing} {Concept} and {Development}: {A} {Systematic} {Literature} {Review}},
	volume = {179},
	issn = {18770509},
	url = {https://doi.org/10.1016/j.procs.2021.01.084},
	doi = {10.1016/j.procs.2021.01.084},
	abstract = {A cloud quantum computer is a quantum computer that can be accessed in a cloud environment through a network. Today, there are numbers of cloud quantum computing services that can be accessed by users. They are used to solve complex problems that require powerful computing. Different cloud quantum computing services deliver different architecture and performances. In our study, we conducted a research on some services to test and evaluate the performances of different cloud quantum computing services and make a comparison out of it. The test will be conducted using two different methods such as visual programming and qiskit. From the result, we can see that the amount of qubit per backend and shots per run pretty much affect the execution time of a cloud quantum computing. This test will give the users some insight and enables them to decide which cloud quantum computing services deliver better performance or faster execution time based on the specification each cloud quantum computer offers.},
	number = {2019},
	journal = {Procedia Computer Science},
	author = {Soeparno, Haryono and Perbangsa, Anzaludin Samsinga},
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {Benchmarking, Cloud, Comparison, Quantum Computing},
	pages = {944--954},
}

@misc{johnson_introducing_2021,
	title = {Introducing {Quantum} {Serverless}, a new programming model for leveraging quantum and classical resources},
	url = {https://research.ibm.com/blog/quantum-serverless-programming},
	author = {Johnson, Blake and Faro, Ismael and Behrendt, Michael and Gambetta, Jay},
	month = may,
	year = {2021},
}

@inproceedings{kaiiali_cloud_2019,
	title = {Cloud computing in the quantum era},
	volume = {2019-Janua},
	isbn = {978-1-5386-7117-7},
	url = {https://ieeexplore.ieee.org/document/8952589/},
	doi = {10.1109/CNS44998.2019.8952589},
	abstract = {Cloud computing has become the prominent technology of this era. Its elasticity, dynamicity, availability, heterogeneity, and pay as you go pricing model has attracted several companies to migrate their businesses' services into the cloud. This gives them more time to focus solely on their businesses and reduces the management and backup overhead leveraging the flexibility of cloud computing. On the other hand, quantum technology is developing very rapidly. Experts are expecting to get an efficient quantum computer within the next decade. This has a significant impact on several sciences including cryptography, medical research, and other fields. This paper analyses the reciprocal impact of quantum technology on cloud computing and vice versa.},
	booktitle = {2019 {IEEE} {Conference} on {Communications} and {Network} {Security} ({CNS})},
	publisher = {IEEE},
	author = {Kaiiali, Mustafa and Sezer, Sakir and Khalid, Ayesha},
	month = jun,
	year = {2019},
	keywords = {Cloud Computing, Homomorphic Encryption, Post-Quantum Cryptography, Quantum Annealing, Quantum Computing, Quantum Driven PUF, Quantum Resistant TPM},
	pages = {1--4},
}

@article{dumitrescu_cloud_2018,
	title = {Cloud {Quantum} {Computing} of an {Atomic} {Nucleus}},
	volume = {120},
	issn = {10797114},
	url = {https://doi.org/10.1103/PhysRevLett.120.210501},
	doi = {10.1103/PhysRevLett.120.210501},
	abstract = {We report a quantum simulation of the deuteron binding energy on quantum processors accessed via cloud servers. We use a Hamiltonian from pionless effective field theory at leading order. We design a low-depth version of the unitary coupled-cluster ansatz, use the variational quantum eigensolver algorithm, and compute the binding energy to within a few percent. Our work is the first step towards scalable nuclear structure computations on a quantum processor via the cloud, and it sheds light on how to map scientific computing applications onto nascent quantum devices.},
	number = {21},
	journal = {Physical Review Letters},
	author = {Dumitrescu, E. F. and McCaskey, A. J. and Hagen, G. and Jansen, G. R. and Morris, T. D. and Papenbrock, T. and Pooser, R. C. and Dean, D. J. and Lougovski, P.},
	year = {2018},
	pmid = {29883142},
	note = {arXiv: 1801.03897
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevLett.120.210501 url:https://doi},
	pages = {210501},
}

@misc{qcloud-ibm,
	title = {{IBM} {Quantum}},
	url = {https://quantum-computing.ibm.com/},
	urldate = {2022-03-28},
	author = {{IBM}},
	year = {2021},
}

@misc{qcloud-google,
	title = {Google {Quantum} {Computing} {Service}},
	url = {https://quantumai.google/quantum-computing-service},
	author = {{Google}},
	year = {2022},
}

@misc{qcloud-azure,
	title = {Azure {Quantum}},
	url = {https://azure.microsoft.com/en-us/services/quantum},
	author = {{Microsoft}},
	year = {2021},
}

@inproceedings{niu_how_2022,
	title = {How {Parallel} {Circuit} {Execution} {Can} {Be} {Useful} for {NISQ} {Computing}?},
	isbn = {978-3-9819263-6-1},
	url = {https://ieeexplore.ieee.org/document/9774512/},
	doi = {10.23919/DATE54114.2022.9774512},
	abstract = {Quantum computing is performed on Noisy Intermediate-Scale Quantum (NISQ) hardware in the short term. Only small circuits can be executed reliably on a quantum machine due to the unavoidable noisy quantum operations on NISQ devices, leading to the under-utilization of hardware resources. With the growing demand to access quantum hardware, how to utilize it more efficiently while maintaining output fidelity is becoming a timely issue. A parallel circuit execution technique has been proposed to address this problem by executing multiple programs on hardware simultaneously. It can improve the hardware throughput and reduce the overall runtime. However, accumulative noises such as crosstalk can decrease the output fidelity in parallel workload execution. In this paper, we first give an in-depth overview of state-of-the-art parallel circuit execution methods. Second, we propose a Quantum Crosstalk-aware Parallel workload execution method (QuCP) without the overhead of crosstalk characterization. Third, we investigate the trade-off between hardware throughput and fidelity loss to explore the hardware limitation with parallel circuit execution. Finally, we apply parallel circuit execution to VQE and zero-noise extrapolation error mitigation method to showcase its various applications on advancing NISQ computing.},
	booktitle = {2022 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	publisher = {IEEE},
	author = {Niu, Siyuan and Todri-Sanial, Aida},
	month = mar,
	year = {2022},
	note = {arXiv: 2112.00387},
	pages = {1065--1070},
}

@article{dreher_prototype_2019,
	title = {Prototype container-based platform for extreme quantum computing algorithm development},
	doi = {10.1109/HPEC.2019.8916430},
	abstract = {Recent advances in the development of the first generation of quantum computing devices have provided researchers with computational platforms to explore new ideas and reformulate conventional computational codes suitable for a quantum computer. Developers can now implement these reformulations on both quantum simulators and hardware platforms through a cloud computing software environment. For example, the IBM Q Experience provides the direct access to their quantum simulators and quantum computing hardware platforms. However these current access options may not be an optimal environment for developers needing to download and modify the source codes and libraries. This paper focuses on the construction of a Docker container environment with Qiskit source codes and libraries running on a local cloud computing system that can directly access the IBM Q Experience. This prototype container based system allows single user and small project groups to do rapid prototype development, testing and implementation of extreme capability algorithms with more agility and flexibility than can be provided through the IBM Q Experience website. This prototype environment also provides an excellent teaching environment for labs and project assignments within graduate courses in cloud computing and quantum computing. The paper also discusses computer security challenges for expanding this prototype container system to larger groups of quantum computing researchers.},
	journal = {2019 IEEE High Performance Extreme Computing Conference, HPEC 2019},
	author = {Dreher, Patrick and Ramasami, Madhuvanti},
	year = {2019},
	note = {Publisher: IEEE
ISBN: 9781728150208},
	keywords = {Cloud Computing, Docker Containers, IBM Q Experience, Quantum Computing, containerization},
}

@article{qalgo-vqe,
	title = {A variational eigenvalue solver on a photonic quantum processor},
	volume = {5},
	issn = {20411723},
	doi = {10.1038/ncomms5213},
	abstract = {Quantum computers promise to efficiently solve important problems that are intractable on a conventional computer. For quantum systems, where the physical dimension grows exponentially, finding the eigenvalues of certain operators is one such intractable problem and remains a fundamental challenge. The quantum phase estimation algorithm efficiently finds the eigenvalue of a given eigenvector but requires fully coherent evolution. Here we present an alternative approach that greatly reduces the requirements for coherent evolution and combine this method with a new approach to state preparation based on ansätze and classical optimization. We implement the algorithm by combining a highly reconfigurable photonic quantum processor with a conventional computer. We experimentally demonstrate the feasibility of this approach with an example from quantum chemistry - calculating the ground-state molecular energy for He-H+. The proposed approach drastically reduces the coherence time requirements, enhancing the potential of quantum resources available today and in the near future. © 2014 Macmillan Publishers Limited. All rights reserved.},
	number = {May},
	journal = {Nature Communications},
	author = {Peruzzo, Alberto and McClean, Jarrod and Shadbolt, Peter and Yung, Man Hong and Zhou, Xiao Qi and Love, Peter J. and Aspuru-Guzik, Alán and O'Brien, Jeremy L.},
	year = {2014},
	keywords = {★},
}

@article{qalgo-shor,
	title = {Polynomial-{Time} {Algorithms} for {Prime} {Factorization} and {Discrete} {Logarithms} on a {Quantum} {Computer}},
	volume = {26},
	issn = {0097-5397},
	url = {http://epubs.siam.org/doi/10.1137/S0097539795293172},
	doi = {10.1137/S0097539795293172},
	abstract = {A digital computer is generally believed to be an efficient universal computing device; that is, it is believed able to simulate any physical computing device with an increase in computation time by at most a polynomial factor. This may not be true when quantum mechanics is taken into consideration. This paper considers factoring integers and finding discrete logarithms, two problems which are generally thought to be hard on a classical computer and which have been used as the basis of several proposed cryptosystems. Efficient randomized algorithms are given for these two problems on a hypothetical quantum computer. These algorithms take a number of steps polynomial in the input size, e.g., the number of digits of the integer to be factored.},
	number = {5},
	journal = {SIAM Journal on Computing},
	author = {Shor, Peter W.},
	month = oct,
	year = {1997},
	note = {arXiv: quant-ph/9508027},
	keywords = {Algorithmic number theory, Church's thesis, Discrete logarithms, Foundations of quantum mechanics, Fourier transforms, Prime factorization, Quantum computers, Spin systems, ★},
	pages = {1484--1509},
}

@article{leymann_bitter_2020,
	title = {The bitter truth about gate-based quantum algorithms in the {NISQ} era},
	volume = {5},
	issn = {20589565},
	doi = {10.1088/2058-9565/abae7d},
	abstract = {Implementing a gate-based quantum algorithm on an noisy intermediate scale quantum (NISQ) device has several challenges that arise from the fact that such devices are noisy and have limited quantum resources. Thus, various factors contributing to the depth and width as well as to the noise of an implementation of a gate-based algorithm must be understood in order to assess whether an implementation will execute successfully on a given NISQ device. In this contribution, we discuss these factors and their impact on algorithm implementations. Especially, we will cover state preparation, oracle expansion, connectivity, circuit rewriting, and readout: these factors are very often ignored when presenting a gate-based algorithm but they are crucial when implementing such an algorithm on near-term quantum computers. Our contribution will help developers in charge of realizing gate-based algorithms on such machines in (i) achieving an executable implementation, and (ii) assessing the success of their implementation on a given machine.},
	number = {4},
	journal = {Quantum Science and Technology},
	author = {Leymann, Frank and Barzen, Johanna},
	year = {2020},
	note = {Publisher: IOP Publishing},
	keywords = {NISQ, Quantum computing, Quantum software, Software engineering of quantum applications},
	pages = {0--28},
}

@article{preskill_quantum_2021,
	title = {Quantum computing 40 years later},
	url = {http://arxiv.org/abs/2106.10522},
	abstract = {Forty years ago, Richard Feynman proposed harnessing quantum physics to build a more powerful kind of computer. Realizing Feynman's vision is one of the grand challenges facing 21st century science and technology. In this article, we'll recall Feynman's contribution that launched the quest for a quantum computer, and assess where the field stands 40 years later.},
	author = {Preskill, John},
	year = {2021},
	note = {arXiv: 2106.10522},
	pages = {1--49},
}

@article{serrano_quantum_2022,
	title = {Quantum {Software} {Components} and {Platforms}: {Overview} and {Quality} {Assessment}},
	issn = {0360-0300},
	doi = {10.1145/3548679},
	abstract = {Quantum computing is the latest revolution in computing and will probably come to be seen an advance as important as the steam engine or the information society. In the last few decades, our understanding of quantum computers has expanded and multiple efforts have been made to create languages, libraries, tools, and environments to facilitate their programming. Nonetheless, quantum computers are complex systems at the bottom of a stack of layers that programmers need to understand. Hence, efforts towards creating quantum programming languages and computing environments that can abstract low-level technology details have become crucial steps to achieve a useful quantum computing technology. However, most of these environments still lack many of the features that would be desirable, such as those outlined in The Talavera Manifesto for Quantum Software Engineering and Programming. For advancing quantum computing, we will need to develop quantum software engineering techniques and tools to ensure the feasibility of this new type of quantum software. To contribute to this goal, this paper provides a review of the main quantum software components and platforms. We also propose a set of quality requirements for the development of quantum software platforms and the conduct of their quality assessment.},
	journal = {ACM Computing Surveys},
	author = {Serrano, Manuel A. and Cruz-Lemus, José A. and Pérez-Castillo, Ricardo and Piattini, Mario},
	year = {2022},
}

@article{aburaed_advances_2017,
	title = {Advances in the quantum theoretical approach to image processing applications},
	volume = {49},
	issn = {15577341},
	doi = {10.1145/3009965},
	abstract = {In this article, a detailed survey of the quantum approach to image processing is presented. Recently, it has been established that existing quantum algorithms are applicable to image processing tasks allowing quantum informational models of classical image processing. However, efforts continue in identifying the diversity of its applicability in various image processing domains. Here, in addition to reviewing some of the critical image processing applications that quantum mechanics have targeted, such as denoising, edge detection, image storage, retrieval, and compression, this study will also highlight the complexities in transitioning from the classical to the quantum domain. This article shall establish theoretical fundamentals, analyze performance and evaluation, draw key statistical evidence to support claims, and provide recommendations based on published literature mostly during the period from 2010 to 2015.},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Abura'ed, Nour and Khan, Faisal Shah and Bhaskar, Harish},
	year = {2017},
	keywords = {Edge detection, Image compression, Image denoising, Image processing, Image retrieval, Image storage, Image watermarking, Quantum computing},
}

@article{bruss_quantum_2007,
	title = {Quantum cryptography: {A} survey},
	volume = {39},
	issn = {03600300},
	doi = {10.1145/1242471.1242474},
	abstract = {We survey some results in quantum cryptography. After a brief introduction to classical cryptography, we provide the quantum-mechanical background needed to present some fundamental protocols from quantum cryptography. In particular, we review quantum key distribution via the BB84 protocol and its security proof, as well as the related quantum bit commitment protocol and its proof of insecurity. © 2007 ACM.},
	number = {2},
	journal = {ACM Computing Surveys},
	author = {Bruss, Dagmar and Erdélyi, Gábor and Meyer, Tim and Riege, Tobias and Rothe, Jörg},
	year = {2007},
	keywords = {Quantum bit commitment, Quantum cryptography, Quantum key distribution},
}

@article{mehic_quantum_2020,
	title = {Quantum {Key} {Distribution}: {A} {Networking} {Perspective}},
	volume = {53},
	issn = {15577341},
	doi = {10.1145/3402192},
	abstract = {The convergence of quantum cryptography with applications used in everyday life is a topic drawing attention from the industrial and academic worlds. The development of quantum electronics has led to the practical achievement of quantum devices that are already available on the market and waiting for their first application on a broader scale. A major aspect of quantum cryptography is the methodology of Quantum Key Distribution (QKD), which is used to generate and distribute symmetric cryptographic keys between two geographically separate users using the principles of quantum physics. In previous years, several successful QKD networks have been created to test the implementation and interoperability of different practical solutions. This article surveys previously applied methods, showing techniques for deploying QKD networks and current challenges of QKD networking. Unlike studies focusing on optical channels and optical equipment, this survey focuses on the network aspect by considering network organization, routing and signaling protocols, simulation techniques, and a software-defined QKD networking approach.},
	number = {5},
	journal = {ACM Computing Surveys},
	author = {Mehic, Miralem and Niemiec, Marcin and Rass, Stefan and Ma, Jiajun and Peev, Momtchil and Aguado, Alejandro and Martin, Vicente and Schauer, Stefan and Poppe, Andreas and Pacher, Christoph and Voznak, Miroslav},
	year = {2020},
	keywords = {Quantum key distribution, cryptography, network organization, security},
}

@article{massoli_leap_2022,
	title = {A {Leap} among {Quantum} {Computing} and {Quantum} {Neural} {Networks}: {A} {Survey}},
	issn = {0360-0300},
	doi = {10.1145/3529756},
	abstract = {In recent years, Quantum Computing witnessed massive improvements in terms of available resources and algorithms development. The ability to harness quantum phenomena to solve computational problems is a long-standing dream that has drawn the scientific community’s interest since the late 80s. In such a context, we propose our contribution. First, we introduce basic concepts related to quantum computations, and then we explain the core functionalities of technologies that implement the Gate Model and Adiabatic Quantum Computing paradigms. Finally, we gather, compare and analyze the current state-of-the-art concerning Quantum Perceptrons and Quantum Neural Networks implementations.},
	journal = {ACM Computing Surveys},
	author = {Massoli, Fabio Valerio and Vadicamo, Lucia and Amato, Giuseppe and Falchi, Fabrizio},
	year = {2022},
	note = {arXiv: 2107.03313},
	keywords = {Quantum Computing, Quantum Machine Learning, Quant},
}

@article{moguel_quantum_2022,
	title = {Quantum service-oriented computing: current landscape and challenges},
	issn = {15731367},
	url = {https://doi.org/10.1007/s11219-022-09589-y},
	doi = {10.1007/s11219-022-09589-y},
	abstract = {The development that quantum computing technologies are achieving is beginning to attract the interest of companies that could potentially be users of quantum software. Thus, it is perfectly feasible that during the next few years hybrid systems will start to appear integrating both the classical software systems of companies and new quantum ones providing solutions to problems that still remain unmanageable today. A natural way to support such integration is Service-Oriented Computing. While conceptually the invocation of a quantum software service is similar to that of a classical one, technically there are many differences and technological limitations, which refer to platform independence, decoupling, scalability, etc. To highlight these differences and the difficulties to develop quality quantum services, this paper takes a well-known problem to which a quantum solution can be provided, integer factorization, making use of the Amazon Braket quantum service platform. The exercise of trying to provide the factorization as a quantum service is carried out following the best practices, design patterns and standards existing in the implementation of classical services. This case study is used to highlight the rough edges and limitations that arise in the integration of classical-quantum hybrid systems using service-oriented computing. The conclusion of the study allows us to point out directions in which to focus research efforts in order to achieve effective quantum service-oriented computing.},
	number = {0123456789},
	journal = {Software Quality Journal},
	author = {Moguel, Enrique and Rojo, Javier and Valencia, David and Berrocal, Javier and Garcia-Alonso, Jose and Murillo, Juan M.},
	year = {2022},
	note = {Publisher: Springer US
ISBN: 0123456789},
	keywords = {Classical services, Hybrid classical-quantum software, Quality, Quantum services},
}

@article{de_wolf_potential_2017,
	title = {The potential impact of quantum computers on society},
	volume = {19},
	issn = {15728439},
	url = {http://dx.doi.org/10.1007/s10676-017-9439-z},
	doi = {10.1007/s10676-017-9439-z},
	abstract = {This paper considers the potential impact that the nascent technology of quantum computing may have on society. It focuses on three areas: cryptography, optimization, and simulation of quantum systems. We will also discuss some ethical aspects of these developments, and ways to mitigate the risks.},
	number = {4},
	journal = {Ethics and Information Technology},
	author = {de Wolf, Ronald},
	year = {2017},
	note = {arXiv: 1712.05380
Publisher: Springer Netherlands
ISBN: 0123456789},
	keywords = {Ethical impact, Quantum computing, Societal impact},
	pages = {271--276},
}

@article{saki_survey_2021,
	title = {A {Survey} and {Tutorial} on {Security} and {Resilience} of {Quantum} {Computing}},
	volume = {2021-May},
	issn = {15581780},
	doi = {10.1109/ETS50041.2021.9465397},
	abstract = {Present-day quantum computers suffer from various noises or errors such as, gate error, relaxation, dephasing, readout error, and crosstalk. Besides, they offer a limited number of qubits with restrictive connectivity. Therefore, quantum programs running these computers face resilience issues and low output fidelities. The noise in the cloud-based access of quantum computers also introduce new modes of security and privacy issues. Furthermore, quantum computers face several threat models from insider and outsider adversaries including input tampering, program misallocation, fault injection, Reverse Engineering (RE) and Cloning. This paper provides an overview of various assets embedded in quantum computers and programs, vulnerabilities and attack models and the relation between resilience and security. We also cover countermeasures against the reliability and security issues and present future outlook for security of quantum computing.},
	journal = {Proceedings of the European Test Workshop},
	author = {Saki, Abdullah Ash and Alam, Mahabubul and Phalak, Koustubh and Suresh, Aakarshitha and Topaloglu, Rasit Onur and Ghosh, Swaroop},
	year = {2021},
	note = {arXiv: 2106.06081
Publisher: IEEE
ISBN: 9781665418492},
	keywords = {Fault Injection, Privacy, Quantum Computing, Resilience, Reverse Engineering, Security},
}

@book{ahmad_empirical_2022,
	title = {Empirical {Analysis} of {Security} {Enabled} {Quantum} {Computing} for {Cloud} {Environment}},
	volume = {133},
	isbn = {978-3-031-04613-1},
	url = {http://dx.doi.org/10.1007/978-3-031-04613-1_3},
	abstract = {Although quantum cloud computing (CC) is based on theoretical notions, it still requires experimentation. CC has emerged as an on-demand accessible tool in different practical applications such as digital industry, academics, manufacturing, health sector, and others. Backup, security, processing, and localization are all issues that the cloud is dealing with. As a result, quantum computing has seen tremendous growth in several domains. In literature, the majority of those researchers working on quantum believe it will solve the cloud service issues. Quantum cloud computing (QCC) entails the deployment of quantum computation sources in a cloud environment to overcome the following challenges and concerns that the existing model of simple cloud computation faces. However, not everyone is interested in combining physics-based atomic computing with software-based CC. This study will demonstrate all of the probable benefits and drawbacks of quantum computing (QC) when used in conjunction with a cloud system. The same also cover some of the most recent updates, as well as some uncertain prospects for quantum as a cloud service. This work demonstrates the current state of the artwork in the quantum services domain. Further, a comparative analysis with similar areas is conducted.},
	publisher = {Springer International Publishing},
	author = {Ahmad, Shahnawaz and Mehfuz, Shabana and Beg, Javed},
	year = {2022},
	doi = {10.1007/978-3-031-04613-1_3},
	note = {Publication Title: Lecture Notes on Data Engineering and Communications Technologies
Issue: Cc
ISSN: 23674520},
	keywords = {And quantum cloud, Cloud computing, Cryptography, Quantum cloud, Quantum computing, Quantum information processing},
}

@article{mohseni_commercialize_2017,
	title = {Commercialize early quantum technologies},
	volume = {543},
	issn = {0028-0836},
	abstract = {Masoud Mohseni, Peter Read, Hartmut Neven and colleagues at Google's Quantum AI Laboratory set out investment opportunities on the road to the ultimate quantum machines. F rom aspects of quantum entangle-ment to chemical reactions with large molecules, many features of the world cannot be described efficiently with con-ventional computers based on binary logic. The solution, as physicist Richard Feynman realized three decades ago 1 , is to use quan-tum processors that adopt a blend of classical states simultaneously, as matter does. Many technical hurdles must be overcome for such quantum machines to be practical, however. These include noise control and improving the fidelity of operations acting on the quan-tum states that encode the information. The quantum-computing community is channelling most of its efforts towards building the ultimate machine: a digital quantum computer that tolerates noise and errors, and that in principle can be applied to any problem. In theory, such a machine — which will need large processors comprising many quantum bits, or qubits — should be able to calculate faster than a conventional computer. Such capability is at least a decade away 2 . Correcting for errors requires redun-dancy, and the number of qubits needed quickly mounts. For example, factorizing a 2,000-bit number in one day, a task believed to be intractable using classical computers 3 , would take 100 million qubits, even if indi-vidual quantum operations failed just once in every 10,000 operations. We have yet to assemble digital quantum processors with tens of qubits. This conservative view of quantum computing gives the impression that inves-tors will benefit only in the long term. We contend that short-term returns are possi-ble with the small devices that will emerge within the next five years, even though these will lack full error correction. A lack of theoretical guarantees need not preclude success. Heuristic 'hybrid' methods that blend quantum and classical approaches could be the foundation for powerful future applications. The recent success of neural net-works in machine learning is a good exam-ple. In the 1990s, when the computing power required to train deep neural networks was unavailable, it was fashionable in the field to focus on 'convex' methods (based on func-tions with a clear minimum solution) that had a strong theoretical basis. Today, these methods are no match for deep learning. The underlying algorithms of neural networks Google's cryostats reach temperatures of 10 millikelvin to run its quantum processors.},
	journal = {Nature},
	author = {Mohseni, Masoud and Read, Peter and Neven, Hartmut},
	year = {2017},
	pages = {171--174},
}

@article{brooks_beyond_2019,
	title = {Beyond quantum supremacy: the hunt for useful quantum computers},
	volume = {574},
	issn = {0028-0836},
	url = {http://www.nature.com/articles/d41586-019-02936-3},
	doi = {10.1038/d41586-019-02936-3},
	abstract = {With decades still to go until the first general-purpose quantum computers the race is on to make today’s systems useful.},
	number = {7776},
	journal = {Nature},
	author = {Brooks, Michael},
	month = oct,
	year = {2019},
	note = {Publisher: Cambridge University Press},
	pages = {19--21},
}

@techreport{commonwealth_of_australia_2021_army_2021,
	title = {Army {Quantum} {Technology} {Roadmap}},
	author = {{Commonwealth of Australia 2021}},
	year = {2021},
	note = {Issue: April},
}

@misc{ibmroadmap,
	title = {{IBM}’s roadmap for scaling quantum technology},
	url = {https://research.ibm.com/blog/ibm-quantum-roadmap},
	urldate = {2022-03-28},
	journal = {IBM Research},
	author = {Gambetta, Jay},
	month = sep,
	year = {2020},
	keywords = {IBM},
}

@article{shor_early_2022,
	title = {The {Early} {Days} of {Quantum} {Computation}},
	url = {http://arxiv.org/abs/2208.09964},
	abstract = {I recount some of my memories of the early development of quantum computation, including the discovery of the factoring algorithm, of error correcting codes, and of fault tolerance.},
	author = {Shor, Peter W.},
	year = {2022},
	note = {arXiv: 2208.09964},
	pages = {1--10},
}

@article{jnane_multicore_2022,
	title = {Multicore {Quantum} {Computing}},
	url = {http://arxiv.org/abs/2201.08861},
	abstract = {Any architecture for practical quantum computing must be scalable. An attractive approach is to create multiple cores, computing regions of fixed size that are well-spaced but interlinked with communication channels. This exploded architecture can relax the demands associated with a single monolithic device: the complexity of control, cooling and power infrastructure as well as the difficulties of cross-talk suppression and near-perfect component yield. Here we explore interlinked multicore architectures through analytic and numerical modelling. While elements of our analysis are relevant to diverse platforms, our focus is on semiconductor electron spin systems in which numerous cores may exist on a single chip. We model shuttling and microwave-based interlinks and estimate the achievable fidelities, finding values that are encouraging but markedly inferior to intra-core operations. We therefore introduce optimsed entanglement purification to enable high-fidelity communication, finding that \$99.5{\textbackslash}\%\$ is a very realistic goal. We then assess the prospects for quantum advantage using such devices in the NISQ-era and beyond: we simulate recently proposed exponentially-powerful error mitigation schemes in the multicore environment and conclude that these techniques impressively suppress imperfections in both the inter- and intra-core operations.},
	author = {Jnane, Hamza and Undseth, Brennan and Cai, Zhenyu and Benjamin, Simon C and Koczor, Bálint},
	year = {2022},
	note = {arXiv: 2201.08861},
	pages = {1--26},
}

@article{gesek_uniform_2021,
	title = {A {Uniform} {Quantum} {Computing} {Model} based on {Virtual} {Quantum} {Processors}},
	doi = {10.1109/ICWS53863.2021.00018},
	abstract = {What else than Hybrid Quantum Computing will dominate data centers, just within a decade? But until now, it is unclear how precisely such high-performance computers will be constructed, even the fundamental functionality is in question. Thus, it is very hard to prepare today, from a programmer perspective, in-depth for the anticipated near-term programming paradigm shift due to quantum computers. Furthermore, the industry is reluctant to invest heavily into quantum software development right now, since under this uncertainty regarding the future development of hardware and Cloud services, no one would like to see all these efforts to be ruined by a novel technology, an unforeseen new machine, which would eradicate the software in place, which the industry had developed so costly. This calls for a more generic view on Hybrid Quantum Computing, on one hand hardware agnostic, but still anticipating the fundamental laws of nature which rule any future quantum computing system, regardless of its engineered excellence. This generalization can be done with the introduction of a Virtual Quantum Processor, a piece of imaginary hardware, which is described detailed enough, to emulate a generic hybrid quantum machine based on a set of instructions within a Turing machine. If this could be accomplished, we would retrieve a Uniform Computing Model for Hybrid Quantum Software, which can be applied later to any physical representation of quantum computing hardware, but would run already today on our current machines.},
	journal = {Proceedings - 2021 IEEE International Conference on Web Services, ICWS 2021},
	author = {Gesek, Georg},
	year = {2021},
	note = {Publisher: IEEE
ISBN: 9781665416818},
	keywords = {high-performance computing, hybrid, programming model, quantum, uniform, virtual processor},
	pages = {32--41},
}

@article{rabbie_designing_2022,
	title = {Designing quantum networks using preexisting infrastructure},
	volume = {8},
	issn = {20566387},
	doi = {10.1038/s41534-021-00501-3},
	abstract = {We consider the problem of deploying a quantum network on an existing fiber infrastructure, where quantum repeaters and end nodes can only be housed at specific locations. We propose a method based on integer linear programming (ILP) to place the minimal number of repeaters on such an existing network topology, such that requirements on end-to-end entanglement-generation rate and fidelity between any pair of end-nodes are satisfied. While ILPs are generally difficult to solve, we show that our method performs well in practice for networks of up to 100 nodes. We illustrate the behavior of our method both on randomly-generated network topologies, as well as on a real-world fiber topology deployed in the Netherlands.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Rabbie, Julian and Chakraborty, Kaushik and Avis, Guus and Wehner, Stephanie},
	year = {2022},
	note = {arXiv: 2005.14715
Publisher: Springer US},
}

@article{jackson_quantum_2007,
	title = {Quantum communication},
	volume = {57},
	issn = {01638998},
	doi = {10.1146/annurev.nucl.57.090506.123030},
	abstract = {Few would dispute that the science of particle physics in the United States has reached a crossroads. Policies, decisions, and events of the coming decade will be critical in determining whether the United States continues to carry out a competitive program of leadership in this field of fundamental science. The field of particle physics has responded to this reality by fundamentally changing its model of communication from "business as usual" to a strategic and collaborative method that is clearly focused on achieving a healthy future for the science. Over the past half-dozen years, the particle physics community has gone from being an oft-cited example of how not to communicate effectively, to a frequently cited - and emulated - model for science communication. This review outlines the new approach toward communication in particle physics and then goes into detail about three case studies. Copyright © 2007 by Annual Reviews. All rights reserved.},
	journal = {Annual Review of Nuclear and Particle Science},
	author = {Jackson, Judy and Calder, Neil},
	year = {2007},
	note = {arXiv: 1507.05157
ISBN: 082431557X},
	keywords = {Collaboration, Fermilab, Interactions, International, Particle physics, SLAC, Symmetry},
	pages = {441--462},
}

@article{moguel_roadmap_2020,
	title = {A roadmap for quantum software engineering: {Applying} the lessons learned from the classics},
	volume = {2705},
	issn = {16130073},
	abstract = {Quantum Computing is one of the emerging areas of computing that currently generates more expectations. However, there are many doubts about its actual future projection. On the one hand, the industry shows reluctance to invest in it. The main reasons are the high costs of the hardware, together with the fact that current commercial quantum computers offer a potential that goes little beyond experimentation. On the other hand, there is controversy in the research community about the feasibility of creating and programming powerful and reliable quantum computers. The possibility of having reliable hardware with a reasonable number of Qubits seems still distant. Finally, current quantum programming tools are still at almost logic gate level, which limits the possibility of creating real complex quantum software systems. If we look back in time, this situation is reminiscent of the Software Crisis experienced by classical computing in the 60’s. This talk starts from this analogy and, analyzing the advances and the lessons learned in the field of Software Engineering in the last 60 years, raises the directions that could help to develop the future Quantum Software Engineering.},
	journal = {CEUR Workshop Proceedings},
	author = {Moguel, Enrique and Berrocal, Javier and García-Alonso, Jose and Murillo, Juan Manuel},
	year = {2020},
	note = {ISBN: 0000000210072},
	keywords = {Future quantum software engineering, Quantum computing, Quantum software crisis},
	pages = {5--13},
}

@incollection{leymann_towards_2019,
	address = {New York, New York, USA},
	title = {Towards a {Pattern} {Language} for {Quantum} {Algorithms}},
	isbn = {978-1-4503-3416-7},
	url = {http://dx.doi.org/10.1007/978-3-030-14082-3_19},
	booktitle = {Proceedings of the 19th {European} {Conference} on {Pattern} {Languages} of {Programs} - {EuroPLoP} '14},
	publisher = {ACM Press},
	author = {Leymann, Frank},
	year = {2019},
	doi = {10.1007/978-3-030-14082-3_19},
	keywords = {Pattern languages, Quantum algorithms, Software engineering, pattern languages, quantum algorithms, software engineering},
	pages = {218--230},
}

@article{zhao_practical_2021,
	title = {Practical distributed quantum information processing with {LOCCNet}},
	volume = {7},
	issn = {20566387},
	doi = {10.1038/s41534-021-00496-x},
	abstract = {Distributed quantum information processing is essential for building quantum networks and enabling more extensive quantum computations. In this regime, several spatially separated parties share a multipartite quantum system, and the most natural set of operations is Local Operations and Classical Communication (LOCC). As a pivotal part in quantum information theory and practice, LOCC has led to many vital protocols such as quantum teleportation. However, designing practical LOCC protocols is challenging due to LOCC’s intractable structure and limitations set by near-term quantum devices. Here we introduce LOCCNet, a machine learning framework facilitating protocol design and optimization for distributed quantum information processing tasks. As applications, we explore various quantum information tasks such as entanglement distillation, quantum state discrimination, and quantum channel simulation. We discover protocols with evident improvements, in particular, for entanglement distillation with quantum states of interest in quantum information. Our approach opens up new opportunities for exploring entanglement and its applications with machine learning, which will potentially sharpen our understanding of the power and limitations of LOCC. An implementation of LOCCNet is available in Paddle Quantum, a quantum machine learning Python package based on PaddlePaddle deep learning platform.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Zhao, Xuanqiang and Zhao, Benchi and Wang, Zihe and Song, Zhixin and Wang, Xin},
	year = {2021},
	note = {arXiv: 2101.12190
Publisher: Springer US},
}

@article{singh_quantum_2021,
	title = {A {Quantum} {Approach} {Towards} the {Adaptive} {Prediction} of {Cloud} {Workloads}},
	volume = {32},
	issn = {1045-9219},
	url = {https://ieeexplore.ieee.org/document/9428529/},
	doi = {10.1109/TPDS.2021.3079341},
	abstract = {This work presents a novel Evolutionary Quantum Neural Network (EQNN) based workload prediction model for Cloud datacenter. It exploits the computational efficiency of quantum computing by encoding workload information into qubits and propagating this information through the network to estimate the workload or resource demands with enhanced accuracy proactively. The rotation and reverse rotation effects of the Controlled-NOT (C-NOT) gate serve activation function at the hidden and output layers to adjust the qubit weights. In addition, a Self Balanced Adaptive Differential Evolution (SB-ADE) algorithm is developed to optimize qubit network weights. The accuracy of the EQNN prediction model is extensively evaluated and compared with seven state-of-the-art methods using eight real world benchmark datasets of three different categories. Experimental results reveal that the use of the quantum approach to evolutionary neural network substantially improves the prediction accuracy up to 91.6\% over the existing approaches.},
	number = {12},
	journal = {IEEE Transactions on Parallel and Distributed Systems},
	author = {Singh, Ashutosh Kumar and Saxena, Deepika and Kumar, Jitendra and Gupta, Vrinda},
	month = dec,
	year = {2021},
	pages = {2893--2905},
}

@article{buyya_software-defined_2014,
	title = {Software-{Defined} {Cloud} {Computing}: {Architectural} elements and open challenges},
	doi = {10.1109/ICACCI.2014.6968661},
	abstract = {The variety of existing cloud services creates a challenge for service providers to enforce reasonable Software Level Agreements (SLA) stating the Quality of Service (QoS) and penalties in case QoS is not achieved. To avoid such penalties at the same time that the infrastructure operates with minimum energy and resource wastage, constant monitoring and adaptation of the infrastructure is needed. We refer to Software-Defined Cloud Computing, or simply Software-Defined Clouds (SDC), as an approach for automating the process of optimal cloud configuration by extending virtualization concept to all resources in a data center. An SDC enables easy reconfiguration and adaptation of physical resources in a cloud infrastructure, to better accommodate the demand on QoS through a software that can describe and manage various aspects comprising the cloud environment. In this paper, we present an architecture for SDCs on data centers with emphasis on mobile cloud applications. We present an evaluation, showcasing the potential of SDC in two use cases - QoS-aware bandwidth allocation and bandwidth-aware, energy-efficient VM placement - and discuss the research challenges and opportunities in this emerging area.},
	journal = {Proceedings of the 2014 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2014},
	author = {Buyya, Rajkumar and Calheiros, Rodrigo N. and Son, Jungmin and Dastjerdi, Amir Vahid and Yoon, Young},
	year = {2014},
	note = {arXiv: 1408.6891
ISBN: 9781479930791},
	keywords = {Cloud Computing, Data Centers, Software-Defined Clouds, Software-Defined Networks, Virtualization},
	pages = {1--12},
}

@article{whaiduzzaman_privacy-preserving_2020,
	title = {A {Privacy}-preserving {Mobile} and {Fog} computing framework to trace and prevent {COVID}-19 community transmission},
	volume = {24},
	issn = {2168-2194},
	doi = {10.1109/jbhi.2020.3026060},
	abstract = {To slow down the spread of COVID-19, governments around the world are trying to identify infected people and to contain the virus by enforcing isolation and quarantine. However, it is difficult to trace people who came into contact with an infected person, which causes widespread community transmission and mass infection. To address this problem, we develop an e-government Privacy Preserving Mobile and Fog computing framework entitled PPMF that can trace infected and suspected cases nationwide. We use personal mobile devices with contact tracing app and two types of stationary fog nodes, named Automatic Risk Checkers (ARC) and Suspected User Data Uploader Node (SUDUN), to trace community transmission alongside maintaining user data privacy. Each user’s mobile device receives a Unique Encrypted Reference Code (UERC) when registering on the central application. The mobile device and the central application both generate Rotational Unique Encrypted Reference Code (RUERC), which broadcasted using the Bluetooth Low Energy (BLE) technology. The ARCs are placed at the entry points of buildings, which can immediately detect if there are positive or suspected cases nearby. If any confirmed case is found, the ARCs broadcast pre-cautionary messages to nearby people without revealing the identity of the infected person. The SUDUNs are placed at the health centers that report test result to the central cloud application. The reported data is later used to map between infected and suspected cases. Therefore, using our proposed PPMF framework, governments can let organizations continue their economic activities without complete lockdown.},
	number = {12},
	journal = {arXiv},
	author = {Whaiduzzaman, Md and Hossain, Md Razon and Shovon, Ahmedur Rahman and Roy, Shanto and Laszka, Aron and Buyya, Rajkumar and Barros, Alistair},
	year = {2020},
	pmid = {32966223},
	note = {arXiv: 2006.13364},
	keywords = {COVID-19, Community Transmission, Contact Tracing, Data Privacy, Fog Computing, Mobile App},
	pages = {3564--3575},
}

@inproceedings{leymann_quantum_2020,
	title = {Quantum in the {Cloud}: {Application} {Potentials} and {Research} {Opportunities}},
	isbn = {978-989-758-424-4},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0009819800090024},
	doi = {10.5220/0009819800090024},
	abstract = {Quantum computers are becoming real, and they have the inherent potential to significantly impact many application domains. We sketch the basics about programming quantum computers, showing that quantum programs are typically hybrid consisting of a mixture of classical parts and quantum parts. With the advent of quantum computers in the cloud, the cloud is a fine environment for performing quantum programs. The tool chain available for creating and running such programs is sketched. As an exemplary problem we discuss efforts to implement quantum programs that are hardware independent. A use case from machine learning is outlined. Finally, a collaborative platform for solving problems with quantum computers that is currently under construction is presented.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Cloud} {Computing} and {Services} {Science}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Leymann, Frank and Barzen, Johanna and Falkenthal, Michael and Vietz, Daniel and Weder, Benjamin and Wild, Karoline},
	month = mar,
	year = {2020},
	keywords = {Cloud Computing, Hybrid Applications, Quantum Computing, general, survey, ★},
	pages = {9--24},
}

@article{Kairouz2021,
	title = {Advances and {Open} {Problems} in {Federated} {Learning}},
	volume = {14},
	issn = {1935-8237},
	url = {http://www.nowpublishers.com/article/Details/MAL-083},
	doi = {10.1561/2200000083},
	abstract = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
	number = {1–2},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kairouz, Peter and McMahan, H. Brendan and Avent, Brendan and Bellet, Aurélien and Bennis, Mehdi and Nitin Bhagoji, Arjun and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and D’Oliveira, Rafael G. L. and Eichner, Hubert and El Rouayheb, Salim and Evans, David and Gardner, Josh and Garrett, Zachary and Gascón, Adrià and Ghazi, Badih and Gibbons, Phillip B. and Gruteser, Marco and Harchaoui, Zaid and He, Chaoyang and He, Lie and Huo, Zhouyuan and Hutchinson, Ben and Hsu, Justin and Jaggi, Martin and Javidi, Tara and Joshi, Gauri and Khodak, Mikhail and Konecný, Jakub and Korolova, Aleksandra and Koushanfar, Farinaz and Koyejo, Sanmi and Lepoint, Tancrède and Liu, Yang and Mittal, Prateek and Mohri, Mehryar and Nock, Richard and Özgür, Ayfer and Pagh, Rasmus and Qi, Hang and Ramage, Daniel and Raskar, Ramesh and Raykova, Mariana and Song, Dawn and Song, Weikang and Stich, Sebastian U. and Sun, Ziteng and Suresh, Ananda Theertha and Tramèr, Florian and Vepakomma, Praneeth and Wang, Jianyu and Xiong, Li and Xu, Zheng and Yang, Qiang and Yu, Felix X. and Yu, Han and Zhao, Sen},
	month = dec,
	year = {2021},
	pages = {1--210},
}

@article{saini_quantum_2020,
	title = {Quantum {Driven} {Machine} {Learning}},
	volume = {59},
	issn = {0020-7748, 1572-9575},
	url = {http://link.springer.com/10.1007/s10773-020-04656-1},
	doi = {10.1007/s10773-020-04656-1},
	abstract = {Quantum computing is proving to be very beneficial for solving complex machine learning problems. Quantum computers are inherently excellent in handling and manipulating vectors and matrix operations. The ever increasing size of data has started creating bottlenecks for classical machine learning systems. Quantum computers are emerging as potential solutions to tackle big data related problems. This paper presents a quantum machine learning model based on quantum support vector machine (QSVM) algorithm to solve a classification problem. The quantum machine learning model is practically implemented on quantum simulators and real-time superconducting quantum processors. The performance of quantum machine learning model is computed in terms of processing speed and accuracy and compared against its classical counterpart. The breast cancer dataset is used for the classification problem. The results are indicative that quantum computers offer quantum speed-up.},
	language = {en},
	number = {12},
	urldate = {2021-05-27},
	journal = {International Journal of Theoretical Physics},
	author = {Saini, Shivani and Khosla, Pk and Kaur, Manjit and Singh, Gurmohan},
	month = dec,
	year = {2020},
	pages = {4013--4024},
}

@article{biamonte_quantum_2017,
	title = {Quantum machine learning},
	volume = {549},
	issn = {14764687},
	doi = {10.1038/nature23474},
	abstract = {Fuelled by increasing computer power and algorithmic advances, machine learning techniques have become powerful tools for finding patterns in data. Quantum systems produce atypical patterns that classical systems are thought not to produce efficiently, so it is reasonable to postulate that quantum computers may outperform classical computers on machine learning tasks. The field of quantum machine learning explores how to devise and implement quantum software that could enable machine learning that is faster than that of classical computers. Recent work has produced quantum algorithms that could act as the building blocks of machine learning programs, but the hardware and software challenges are still considerable.},
	number = {7671},
	journal = {Nature},
	author = {Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth},
	month = sep,
	year = {2017},
	pmid = {28905917},
	note = {arXiv: 1611.09347
Publisher: Nature Publishing Group},
	pages = {195--202},
}

@article{Lo2021,
	title = {A {Systematic} {Literature} {Review} on {Federated} {Machine} {Learning}},
	volume = {54},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3450288},
	doi = {10.1145/3450288},
	abstract = {Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.},
	number = {5},
	journal = {ACM Computing Surveys},
	author = {Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Paik, Hye-Young and Zhu, Liming},
	month = jun,
	year = {2021},
	pages = {1--39},
}

@article{Mothukuri2021,
	title = {A survey on security and privacy of federated learning},
	volume = {115},
	issn = {0167739X},
	url = {https://doi.org/10.1016/j.future.2020.10.007},
	doi = {10.1016/j.future.2020.10.007},
	abstract = {Federated learning (FL) is a new breed of Artificial Intelligence (AI) that builds upon decentralized data and training that brings learning to the edge or directly on-device. FL is a new research area often referred to as a new dawn in AI, is in its infancy, and has not yet gained much trust in the community, mainly because of its (unknown) security and privacy implications. To advance the state of the research in this area and to realize extensive utilization of the FL approach and its mass adoption, its security and privacy concerns must be first identified, evaluated, and documented. FL is preferred in use-cases where security and privacy are the key concerns and having a clear view and understanding of risk factors enable an implementer/adopter of FL to successfully build a secure environment and gives researchers a clear vision on possible research areas. This paper aims to provide a comprehensive study concerning FL's security and privacy aspects that can help bridge the gap between the current state of federated AI and a future in which mass adoption is possible. We present an illustrative description of approaches and various implementation styles with an examination of the current challenges in FL and establish a detailed review of security and privacy concerns that need to be considered in a thorough and clear context. Findings from our study suggest that overall there are fewer privacy-specific threats associated with FL compared to security threats. The most specific security threats currently are communication bottlenecks, poisoning, and backdoor attacks while inference-based attacks are the most critical to the privacy of FL. We conclude the paper with much needed future research directions to make FL adaptable in realistic scenarios.},
	journal = {Future Generation Computer Systems},
	author = {Mothukuri, Viraaji and Parizi, Reza M. and Pouriyeh, Seyedamin and Huang, Yan and Dehghantanha, Ali and Srivastava, Gautam},
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {Artificial intelligence, Distributed learning, Federated learning, Federated machine learning, Machine learning, Privacy, Security, ★},
	pages = {619--640},
}

@incollection{mazzara_microservices_2017,
	address = {Cham},
	title = {Microservices: {Yesterday}, {Today}, and {Tomorrow}},
	isbn = {978-3-319-67424-7 978-3-319-67425-4},
	shorttitle = {Microservices},
	url = {http://link.springer.com/10.1007/978-3-319-67425-4_12},
	abstract = {Microservices is an architectural style inspired by service-oriented computing that has recently started gaining popularity. Before presenting the current state of the art in the ﬁeld, this chapter reviews the history of software architecture, the reasons that led to the diffusion of objects and services ﬁrst, and microservices later. Finally, open problems and future challenges are introduced. This survey primarily addresses newcomers to the discipline, while offering an academic viewpoint on the topic. In addition, we investigate some practical issues and point out a few potential solutions.},
	language = {en},
	urldate = {2022-08-30},
	booktitle = {Present and {Ulterior} {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Dragoni, Nicola and Giallorenzo, Saverio and Lafuente, Alberto Lluch and Mazzara, Manuel and Montesi, Fabrizio and Mustafin, Ruslan and Safina, Larisa},
	editor = {Mazzara, Manuel and Meyer, Bertrand},
	year = {2017},
	doi = {10.1007/978-3-319-67425-4_12},
	pages = {195--216},
}

@article{siqueira_service_2022,
	title = {Service {Computing} for {Industry} 4.0: {State} of the {Art}, {Challenges}, and {Research} {Opportunities}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Service {Computing} for {Industry} 4.0},
	url = {https://dl.acm.org/doi/10.1145/3478680},
	doi = {10.1145/3478680},
	abstract = {Recent advances in the large-scale adoption of information and communication technologies in manufacturing processes, known as Industry 4.0 or Smart Manufacturing, provide us a window into how the manufacturing sector will evolve in the coming decades. As a result of these initiatives, manufacturing firms have started to integrate a series of emerging technologies into their processes that will change the way products are designed, manufactured, and consumed. This article provides a comprehensive review of how service-oriented computing is being employed to develop the required software infrastructure for Industry 4.0 and identifies the major challenges and research opportunities that ensue. Particular attention is paid to the microservices architecture, which is increasingly recognized as offering a promising approach for developing innovative industrial applications. This literature review is based on the current state of the art on service computing for Industry 4.0 as described in a large corpus of recently published research papers, which helped us to identify and explore a series of challenges and opportunities for the development of this emerging technology frontier, with the goal of facilitating its widespread adoption.},
	language = {en},
	number = {9},
	urldate = {2022-08-30},
	journal = {ACM Computing Surveys},
	author = {Siqueira, Frank and Davis, Joseph G.},
	month = dec,
	year = {2022},
	pages = {1--38},
}

@article{leite_survey_2020,
	title = {A {Survey} of {DevOps} {Concepts} and {Challenges}},
	volume = {52},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3359981},
	doi = {10.1145/3359981},
	abstract = {DevOpsis a collaborative and multidisciplinary organizational effort to automate continuous delivery of new software updates while guaranteeing their correctness and reliability. The present survey investigates and discusses DevOps challenges from the perspective of engineers, managers, and researchers. We review the literature and develop a DevOps conceptual map, correlating the DevOps automation tools with these concepts. We then discuss their practical implications for engineers, managers, and researchers. Finally, we critically explore some of the most relevant DevOps challenges reported by the literature.},
	language = {en},
	number = {6},
	urldate = {2022-08-30},
	journal = {ACM Computing Surveys},
	author = {Leite, Leonardo and Rocha, Carla and Kon, Fabio and Milojicic, Dejan and Meirelles, Paulo},
	month = nov,
	year = {2020},
	pages = {1--35},
}

@misc{van_meter_quantum_2021,
	title = {A {Quantum} {Internet} {Architecture}},
	url = {http://arxiv.org/abs/2112.07092},
	abstract = {Entangled quantum communication is advancing rapidly, with laboratory and metropolitan testbeds under development, but to date there is no unifying Quantum Internet architecture. We propose a Quantum Internet architecture centered around the Quantum Recursive Network Architecture (QRNA), using RuleSet-based connections established using a two-pass connection setup. Scalability and internetworking (for both technological and administrative boundaries) are achieved using recursion in naming and connection control. In the near term, this architecture will support end-to-end, two-party entanglement on minimal hardware, and it will extend smoothly to multi-party entanglement and the use of quantum error correction on advanced hardware in the future. For a network internal gateway protocol, we recommend (but do not require) qDijkstra with seconds per Bell pair as link cost for routing; the external gateway protocol is designed to build recursively. The strength of our architecture is shown by assessing extensibility and demonstrating how robust protocol operation can be conﬁrmed using the RuleSet paradigm.},
	language = {en},
	urldate = {2022-08-30},
	publisher = {arXiv},
	author = {Van Meter, Rodney and Satoh, Ryosuke and Benchasattabuse, Naphan and Matsuo, Takaaki and Hajdušek, Michal and Satoh, Takahiko and Nagayama, Shota and Suzuki, Shigeya},
	month = dec,
	year = {2021},
	note = {arXiv:2112.07092 [quant-ph]},
	keywords = {Computer Science - Networking and Internet Architecture, Quantum Physics},
}

@misc{nguyen_qfaas_2022,
	title = {{QFaaS}: {A} {Serverless} {Function}-as-a-{Service} {Framework} for {Quantum} {Computing}},
	copyright = {All rights reserved},
	shorttitle = {{QFaaS}},
	url = {http://arxiv.org/abs/2205.14845},
	abstract = {Recent breakthroughs in quantum hardware are creating opportunities for its use in many applications. However, quantum software engineering is still in its infancy with many challenges, especially dealing with the diversity of quantum programming languages and hardware platforms. To alleviate these challenges, we propose QFaaS, a novel Quantum Function-as-a-Service framework, which leverages the advantages of the serverless model and the state-of-the-art software engineering approaches to advance practical quantum computing. Our framework provides essential components of a quantum serverless platform to simplify the software development and adapt to the quantum cloud computing paradigm, such as combining hybrid quantum-classical computation, containerizing functions, and integrating DevOps features. We design QFaaS as a unified quantum computing framework by supporting well-known quantum languages and software development kits (Qiskit, Q\#, Cirq, and Braket), executing the quantum tasks on multiple simulators and quantum cloud providers (IBM Quantum and Amazon Braket). This paper proposes architectural design, principal components, the life cycle of hybrid quantum-classical function, operation workflow, and implementation of QFaaS. We present two practical use cases and perform the evaluations on quantum computers and simulators to demonstrate our framework’s ability to ease the burden on traditional engineers to expedite the ongoing quantum software transition.},
	language = {en},
	urldate = {2022-08-29},
	publisher = {arXiv},
	author = {Nguyen, Hoa T. and Usman, Muhammad and Buyya, Rajkumar},
	month = may,
	year = {2022},
	note = {arXiv:2205.14845 [quant-ph]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Emerging Technologies, Quantum Physics},
}

@inproceedings{huang_analysis_2022,
	address = {Dalian, China},
	title = {Analysis on the recent development of quantum computer and quantum neural network technology},
	isbn = {978-1-66549-991-0},
	url = {https://ieeexplore.ieee.org/document/9844614/},
	doi = {10.1109/ICAICA54878.2022.9844614},
	abstract = {Many countries have invested a large amount of scientific research funds into supporting quantum information technology and proposed many policies to support the three main quantum technology fields: quantum sensing, quantum communication and quantum computing. Many science and technology giants have also provided quantum cloud computing experimental platforms that can provide quantum access online. With the investment of various technologies and resources, quantum information technology has developed rapidly. The combination of quantum computing and neural networks is a research branch that has attracted much attention in the field of quantum computing in recent years. It mainly involves the study of how to exploit the computational acceleration advantage of quantum computing to deal with various problems solved by neural networks in traditional artificial intelligence. In this paper, the recent development of the quantum computer is widely investigated, and a quantum neural network is briefly analyzed. Finally, the performance of the quantum neural network and quantum optimization is discussed, and future research directions are proposed.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {2022 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Computer} {Applications} ({ICAICA})},
	publisher = {IEEE},
	author = {Huang, Zhiguo and Qian, Ling and Cai, Dunbo},
	month = jun,
	year = {2022},
	pages = {680--684},
}

@inproceedings{larasati_quantum_2022,
	address = {Xi'an, China},
	title = {Quantum {Federated} {Learning}: {Remarks} and {Challenges}},
	isbn = {978-1-66548-066-6},
	shorttitle = {Quantum {Federated} {Learning}},
	url = {https://ieeexplore.ieee.org/document/9842983/},
	doi = {10.1109/CSCloud-EdgeCom54986.2022.00010},
	abstract = {As the development of quantum computing hardware is on the rise, its potential application to various research areas has been investigated, including to machine learning. Recently, there have been several initiatives to expand the work to quantum federated learning (QFL). However, challenges arise due to the fact that quantum computation poses different characteristics from classical computation, giving an even more challenge for a federated setting. In this paper, we present a highlevel overview of the current state of research in QFL. Furthermore, we also describe in brief about quantum computation and discuss its present limitations in relation to QFL development. Additionally, possible approaches to deploy QFL are explored. Lastly, remarks and challenges of QFL are also presented.},
	language = {en},
	urldate = {2022-08-09},
	booktitle = {2022 {IEEE} 9th {International} {Conference} on {Cyber} {Security} and {Cloud} {Computing} ({CSCloud})/2022 {IEEE} 8th {International} {Conference} on {Edge} {Computing} and {Scalable} {Cloud} ({EdgeCom})},
	publisher = {IEEE},
	author = {Larasati, Harashta Tatimma and Firdaus, Muhammad and Kim, Howon},
	month = jun,
	year = {2022},
	pages = {1--5},
}

@article{j_quantum_2022,
	title = {Quantum {Algorithm} {Implementations} for {Beginners}},
	issn = {2643-6809, 2643-6817},
	url = {https://dl.acm.org/doi/10.1145/3517340},
	doi = {10.1145/3517340},
	abstract = {As quantum computers become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classical computer programs for most of their careers. While currently available quantum computers have less than 100 qubits, quantum computing hardware is widely expected to grow in terms of qubit count, quality, and connectivity. This review aims to explain the principles of quantum programming, which are quite different from classical programming, with straightforward algebra that makes understanding of the underlying fascinating quantum mechanical principles optional. We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succinct and self-contained fashion. We show how these algorithms can be implemented on IBM’s quantum computer, and in each case, we discuss the results of the implementation with respect to differences between the simulator and the actual hardware runs. This article introduces computer scientists, physicists, and engineers to quantum algorithms and provides a blueprint for their implementations.},
	language = {en},
	urldate = {2022-06-22},
	journal = {ACM Transactions on Quantum Computing},
	author = {J., Abhijith and Adedoyin, Adetokunbo and Ambrosiano, John and Anisimov, Petr and Casper, William and Chennupati, Gopinath and Coffrin, Carleton and Djidjev, Hristo and Gunter, David and Karra, Satish and Lemons, Nathan and Lin, Shizeng and Malyzhenkov, Alexander and Mascarenas, David and Mniszewski, Susan and Nadiga, Balu and O’Malley, Daniel and Oyen, Diane and Pakin, Scott and Prasad, Lakshman and Roberts, Randy and Romero, Phillip and Santhi, Nandakishore and Sinitsyn, Nikolai and Swart, Pieter J. and Wendelberger, James G. and Yoon, Boram and Zamora, Richard and Zhu, Wei and Eidenbenz, Stephan and Bärtschi, Andreas and Coles, Patrick J. and Vuffray, Marc and Lokhov, Andrey Y.},
	month = mar,
	year = {2022},
	pages = {3517340},
}

@misc{tian_recent_2022,
	title = {Recent {Advances} for {Quantum} {Neural} {Networks} in {Generative} {Learning}},
	url = {http://arxiv.org/abs/2206.03066},
	abstract = {Quantum computers are next-generation devices that hold promise to perform calculations beyond the reach of classical computers. A leading method towards achieving this goal is through quantum machine learning, especially quantum generative learning. Due to the intrinsic probabilistic nature of quantum mechanics, it is reasonable to postulate that quantum generative learning models (QGLMs) may surpass their classical counterparts. As such, QGLMs are receiving growing attention from the quantum physics and computer science communities, where various QGLMs that can be efﬁciently implemented on near-term quantum machines with potential computational advantages are proposed. In this paper, we review the current progress of QGLMs from the perspective of machine learning. Particularly, we interpret these QGLMs, covering quantum circuit Born machines, quantum generative adversarial networks, quantum Boltzmann machines, and quantum autoencoders, as the quantum extension of classical generative learning models. In this context, we explore their intrinsic relation and their fundamental differences. We further summarize the potential applications of QGLMs in both conventional machine learning tasks and quantum physics. Last, we discuss the challenges and further research directions for QGLMs.},
	language = {en},
	urldate = {2022-06-10},
	publisher = {arXiv},
	author = {Tian, Jinkai and Sun, Xiaoyu and Du, Yuxuan and Zhao, Shanshan and Liu, Qing and Zhang, Kaining and Yi, Wei and Huang, Wanrong and Wang, Chaoyue and Wu, Xingyao and Hsieh, Min-Hsiu and Liu, Tongliang and Yang, Wenjing and Tao, Dacheng},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.03066
arXiv:2206.03066 [quant-ph]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantum Physics},
}

@article{Qayyum2021,
	title = {Collaborative {Federated} {Learning} {For} {Healthcare}: {Multi}-{Modal} {COVID}-19 {Diagnosis} at the {Edge}},
	url = {http://arxiv.org/abs/2101.07511},
	abstract = {Despite significant improvements over the last few years, cloud-based healthcare applications continue to suffer from poor adoption due to their limitations in meeting stringent security, privacy, and quality of service requirements (such as low latency). The edge computing trend, along with techniques for distributed machine learning such as federated learning, have gained popularity as a viable solution in such settings. In this paper, we leverage the capabilities of edge computing in medicine by analyzing and evaluating the potential of intelligent processing of clinical visual data at the edge allowing the remote healthcare centers, lacking advanced diagnostic facilities, to benefit from the multi-modal data securely. To this aim, we utilize the emerging concept of clustered federated learning (CFL) for an automatic diagnosis of COVID-19. Such an automated system can help reduce the burden on healthcare systems across the world that has been under a lot of stress since the COVID-19 pandemic emerged in late 2019. We evaluate the performance of the proposed framework under different experimental setups on two benchmark datasets. Promising results are obtained on both datasets resulting in comparable results against the central baseline where the specialized models (i.e., each on a specific type of COVID-19 imagery) are trained with central data, and improvements of 16{\textbackslash}\% and 11{\textbackslash}\% in overall F1-Scores have been achieved over the multi-modal model trained in the conventional Federated Learning setup on X-ray and Ultrasound datasets, respectively. We also discuss in detail the associated challenges, technologies, tools, and techniques available for deploying ML at the edge in such privacy and delay-sensitive applications.},
	author = {Qayyum, Adnan and Ahmad, Kashif and Ahsan, Muhammad Ahtazaz and Al-Fuqaha, Ala and Qadir, Junaid},
	year = {2021},
	note = {arXiv: 2101.07511},
	pages = {1--10},
}

@article{He2020,
	title = {{FedML}: {A} research library and benchmark for federated machine learning},
	issn = {23318422},
	abstract = {Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at https://FedML.ai.},
	journal = {arXiv},
	author = {He, Chaoyang and Li, Songze and So, Jinhyun and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and Shen, Li and Zhao, Peilin and Kang, Yan and Liu, Yang and Raskar, Ramesh and Yang, Qiang and Annavaram, Murali and Avestimehr, Salman},
	year = {2020},
	note = {arXiv: 2007.13518},
	keywords = {★},
}

@article{Lo2020,
	title = {A {Systematic} literature review on federated {Machine} {Learning}: {From} a software engineering perspective},
	volume = {37},
	issn = {23318422},
	abstract = {Federated learning is an emerging machine learning paradigm where multiple clients train models locally to formulate a global model under the coordination of a central server. To identify the state-of-the-art in federated learning from a software engineering perspective, we performed a systematic literature review with the extracted 231 primary studies. The results show that most of the known motivations of federated learning appear to be the most studied federated learning challenges, including data privacy, communication efficiency, and statistical heterogeneity. Also, there are only a few real-world applications of federated learning. More studies are needed before production-level adoption can take place.},
	number = {4},
	journal = {arXiv},
	author = {Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Hye-Young, Paik and Zhu, Liming},
	year = {2020},
	note = {arXiv: 2007.11354},
	keywords = {Artificial intelligence, Communication, Computing methodologies, Distributed computing, Edge computing, Federated learning, Machine learning, Privacy, Security and privacy, Software and its engineering, Software architecture, Software engineering, Systematic literature review, ★},
}

@article{Asad2020,
	title = {A critical evaluation of privacy and security threats in federated learning},
	volume = {20},
	issn = {14248220},
	doi = {10.3390/s20247182},
	abstract = {With the advent of smart devices, smartphones, and smart everything, the Internet of Things (IoT) has emerged with an incredible impact on the industries and human life. The IoT consists of millions of clients that exchange massive amounts of critical data, which results in high privacy risks when processed by a centralized cloud server. Motivated by this privacy concern, a new machine learning paradigm has emerged, namely Federated Learning (FL). Specifically, FL allows for each client to train a learning model locally and performs global model aggregation at the centralized cloud server in order to avoid the direct data leakage from clients. However, despite this efficient distributed training technique, an individual’s private information can still be compromised. To this end, in this paper, we investigate the privacy and security threats that can harm the whole execution process of FL. Additionally, we provide practical solutions to overcome those attacks and protect the individual’s privacy. We also present experimental results in order to highlight the discussed issues and possible solutions. We expect that this work will open exciting perspectives for future research in FL.},
	number = {24},
	journal = {Sensors (Switzerland)},
	author = {Asad, Muhammad and Moustafa, Ahmed and Yu, Chao},
	year = {2020},
	pmid = {33333854},
	keywords = {Attacks, Federated learning, Privacy, Security, Threats, ★},
	pages = {1--15},
}

@article{Mcmahan2018,
	title = {L {Earning} {D} {Ifferentially} {P} {Rivate} {R} {Ecurrent}},
	number = {2014},
	journal = {CoRR},
	author = {Mcmahan, H Brendan and Ramage, Daniel},
	year = {2018},
	note = {arXiv: 1710.06963v3},
	pages = {1--14},
}

@article{Bonawitz2017,
	title = {Practical secure aggregation for privacy-preserving machine learning},
	issn = {15437221},
	doi = {10.1145/3133956.3133982},
	abstract = {We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers 1.73× communication expansion for 210 users and 220-dimensional vectors, and 1.98× expansion for 214 users and 224-dimensional vectors over sending data in the clear.},
	journal = {Proceedings of the ACM Conference on Computer and Communications Security},
	author = {Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H. Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
	year = {2017},
	note = {ISBN: 9781450349468},
	keywords = {Federated Learning, Machine Learning, Privacy-Preserving Protocols, Secure Aggregation, ★},
	pages = {1175--1191},
}

@article{BrendanMcMahan2017,
	title = {Communication-efficient learning of deep networks from decentralized data},
	volume = {54},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100x as compared to synchronized stochastic gradient descent.},
	journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
	author = {Brendan McMahan, H. and Moore, Eider and Ramage, Daniel and Hampson, Seth and Agüera y Arcas, Blaise},
	year = {2017},
	note = {arXiv: 1602.05629},
	keywords = {★},
}

@article{Carlini2018,
	title = {The secret sharer: {Evaluating} and testing unintended memorization in neural networks},
	abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models—a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users’ private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google’s Smart Compose, a commercial text-completion neural network trained on millions of users’ email messages.},
	journal = {arXiv},
	author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
	year = {2018},
	note = {arXiv: 1802.08232v3},
}

@article{Konecny2016,
	title = {Federated {Learning}: {Strategies} for {Improving} {Communication} {Efficiency}},
	url = {http://arxiv.org/abs/1610.05492},
	abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
	author = {Konečný, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richtárik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	year = {2016},
	note = {arXiv: 1610.05492},
	pages = {1--10},
}

@article{Sedjelmaci2020,
	title = {Cyber {Security} {Based} on {Artificial} {Intelligence} for {Cyber}-{Physical} {Systems}},
	volume = {34},
	issn = {1558156X},
	doi = {10.1109/MNET.2020.9105926},
	number = {3},
	journal = {IEEE Network},
	author = {Sedjelmaci, Hichem and Guenab, Fateh and Senouci, Sidi Mohammed and Moustafa, Hassnaa and Liu, Jiajia and Han, Shuai},
	year = {2020},
	pages = {6--7},
}

@article{Qu2020,
	title = {Decentralized privacy using blockchain-enabled federated learning in fog computing},
	volume = {7},
	issn = {23274662},
	doi = {10.1109/JIOT.2020.2977383},
	abstract = {As the extension of cloud computing and a foundation of IoT, fog computing is experiencing fast prosperity because of its potential to mitigate some troublesome issues, such as network congestion, latency, and local autonomy. However, privacy issues and the subsequent inefficiency are dragging down the performances of fog computing. The majority of existing works hardly consider a reasonable balance between them while suffering from poisoning attacks. To address the aforementioned issues, we propose a novel blockchain-enabled federated learning (FL-Block) scheme to close the gap. FL-Block allows local learning updates of end devices exchanges with a blockchain-based global learning model, which is verified by miners. Built upon this, FL-Block enables the autonomous machine learning without any centralized authority to maintain the global model and coordinates by using a Proof-of-Work consensus mechanism of the blockchain. Furthermore, we analyze the latency performance of FL-Block and further derive the optimal block generation rate by taking communication, consensus delays, and computation cost into consideration. Extensive evaluation results show the superior performances of FL-Block from the aspects of privacy protection, efficiency, and resistance to the poisoning attack.},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Qu, Youyang and Gao, Longxiang and Luan, Tom H. and Xiang, Yong and Yu, Shui and Li, Bai and Zheng, Gavin},
	year = {2020},
	note = {Publisher: IEEE},
	keywords = {Blockchain, Federated learning, Fog computing, Privacy protection},
	pages = {5171--5183},
}

@article{Yang2019,
	title = {Federated {Machine} {Learning}},
	volume = {10},
	issn = {2157-6904},
	url = {https://dl.acm.org/doi/10.1145/3298981},
	doi = {10.1145/3298981},
	abstract = {Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
	number = {2},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
	month = feb,
	year = {2019},
	pages = {1--19},
}

@article{Asad2020a,
	title = {{FedOpt}: {Towards} communication efficiency and privacy preservation in federated learning},
	volume = {10},
	issn = {20763417},
	doi = {10.3390/APP10082864},
	abstract = {Artificial Intelligence (AI) has been applied to solve various challenges of real-world problems in recent years. However, the emergence of new AI technologies has brought several problems, especially with regard to communication efficiency, security threats and privacy violations. Towards this end, Federated Learning (FL) has received widespread attention due to its ability to facilitate the collaborative training of local learning models without compromising the privacy of data. However, recent studies have shown that FL still consumes considerable amounts of communication resources. These communication resources are vital for updating the learning models. In addition, the privacy of data could still be compromised once sharing the parameters of the local learning models in order to update the global model. Towards this end, we propose a new approach, namely, Federated Optimisation (FedOpt) in order to promote communication efficiency and privacy preservation in FL. In order to implement FedOpt, we design a novel compression algorithm, namely, Sparse Compression Algorithm (SCA) for efficient communication, and then integrate the additively homomorphic encryption with differential privacy to prevent data from being leaked. Thus, the proposed FedOpt smoothly trade-offs communication efficiency and privacy preservation in order to adopt the learning task. The experimental results demonstrate that FedOpt outperforms the state-of-the-art FL approaches. In particular, we consider three different evaluation criteria; model accuracy, communication efficiency and computation overhead. Then, we compare the proposed FedOpt with the baseline configurations and the state-of-the-art approaches, i.e., Federated Averaging (FedAvg) and the paillier-encryption based privacy-preserving deep learning (PPDL) on all these three evaluation criteria. The experimental results show that FedOpt is able to converge within fewer training epochs and a smaller privacy budget.},
	number = {8},
	journal = {Applied Sciences (Switzerland)},
	author = {Asad, Muhammad and Moustafa, Ahmed and Ito, Takayuki},
	year = {2020},
	keywords = {Artificial intelligence, Communication efficiency, Federated learning, Privacy preserving},
}

@article{Bonawitz2019,
	title = {Towards federated learning at scale: {System} design},
	abstract = {Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.},
	journal = {arXiv},
	author = {Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloé and Konečný, Jakub and Mazzocchi, Stefano and Brendan McMahan, H. and Van Overveldt, Timon and Petrou, David and Ramage, Daniel and Roselander, Jason},
	year = {2019},
	note = {arXiv: 1902.01046},
}

@article{Silva2020,
	title = {Towards {Federated} {Learning} in {Edge} {Computing} for {Real}-{Time} {Traffic} {Estimation} in {Smart} {Cities}},
	volume = {1},
	abstract = {The wide proliferation of sensors and devices of Internet of Things (IoT), together with Artificial Intelligence (AI), has created the so-called Smart Environments. From a network perspective, these solutions suffer from high la- tency and increased data transmission. This paper proposes a Federated Learn- ing (FL) architecture for Real-Time Traffic Estimation, supported by Roadside Units (RSU’s) for model aggregation. The solution envisages that learning will be done on clients with their local data, and fully distributed on the Edge, with high learning rates, low latency, and less bandwidth usage. To achieve that, this paper discusses tools and requirements for FL implementation towards a model for real-time traffic estimation, as well as how such solution could be evaluated using VANET and network simulators. As a first practical step, we show a preliminary evaluation ofa learning model using a data set ofcars that demonstrate a distributed learning strategy. In the future, we will use a similar distributed strategy within our proposed architecture.},
	journal = {Workshop of Urban Computation (CoUrb) 2020},
	author = {Silva, Matteus V S and Bittencourt, Luiz F},
	year = {2020},
}

@article{Zhou2020,
	title = {Privacy-{Preserving} {Federated} {Learning} in {Fog} {Computing}},
	volume = {7},
	issn = {23274662},
	doi = {10.1109/JIOT.2020.2987958},
	abstract = {Federated learning can combine a large number of scattered user groups and train models collaboratively without uploading data sets, so as to avoid the server collecting user sensitive data. However, the model of federated learning will expose the training set information of users, and the uneven amount of data owned by users in multiple users' scenarios will lead to the inefficiency of training. In this article, we propose a privacy-preserving federated learning scheme in fog computing. Acting as a participant, each fog node is enabled to collect Internet-of-Things (IoT) device data and complete the learning task in our scheme. Such design effectively improves the low training efficiency and model accuracy caused by the uneven distribution of data and the large gap of computing power. We enable IoT device data to satisfy -differential privacy to resist data attacks and leverage the combination of blinding and Paillier homomorphic encryption against model attacks, which realize the security aggregation of model parameters. In addition, we formally verified our scheme can not only guarantee both data security and model security but completely resist collusion attacks launched by multiple malicious entities. Our experiments based on the Fashion-MNIST data set prove that our scheme is highly efficient in practice.},
	number = {11},
	journal = {IEEE Internet of Things Journal},
	author = {Zhou, Chunyi and Fu, Anmin and Yu, Shui and Yang, Wei and Wang, Huaqun and Zhang, Yuqing},
	year = {2020},
	note = {Publisher: IEEE},
	keywords = {Federated learning, Internet of Things (IoT), fog computing, privacy preserving},
	pages = {10782--10793},
}

@article{Zhao2020,
	title = {Intelligent intrusion detection based on federated learning aided long short-term memory},
	volume = {42},
	issn = {18744907},
	url = {https://doi.org/10.1016/j.phycom.2020.101157},
	doi = {10.1016/j.phycom.2020.101157},
	abstract = {Deep learning based intelligent intrusion detection (IID) methods have been received strongly attention for computer security protection in cybersecurity. All these learning models are trained at either a single user server or centralized server. For one thing, it is almost impossible to train a powerful deep learning model at a single user. For other, it will encounter intrusion risks at centre server and violate user privacy if collecting dataset from all of user servers. In order to solve these problems, this paper proposes an effective IID method based on federated learning (FL) aided long short-term memory (FL-LSTM) framework. First, the initial LSTM global model is deployed at all of user servers. Second, each user trains its single model and then uploads its model parameters to central server. Finally, the central server performs model parameters aggregation to form a new global model and distributes it to user servers. Use this step as a loop for communication to complete the training of the intrusion detection model. Simulation results show that our proposed method achieves a higher accuracy and better consistency than conventional methods.},
	journal = {Physical Communication},
	author = {Zhao, Ruijie and Yin, Yue and Shi, Yong and Xue, Zhi},
	year = {2020},
	note = {Publisher: Elsevier B.V.
ISBN: 0000000161688},
	keywords = {Deep learning, Federated learning, Intrusion detection, Long short-term memory},
	pages = {101157},
}

@article{Jiang2020,
	title = {Federated learning in smart city sensing: {Challenges} and opportunities},
	volume = {20},
	issn = {14248220},
	doi = {10.3390/s20216230},
	abstract = {Smart Cities sensing is an emerging paradigm to facilitate the transition into smart city services. The advent of the Internet of Things (IoT) and the widespread use of mobile devices with computing and sensing capabilities has motivated applications that require data acquisition at a societal scale. These valuable data can be leveraged to train advanced Artificial Intelligence (AI) models that serve various smart services that benefit society in all aspects. Despite their effectiveness, legacy data acquisition models backed with centralized Machine Learning models entail security and privacy concerns, and lead to less participation in large-scale sensing and data provision for smart city services. To overcome these challenges, Federated Learning is a novel concept that can serve as a solution to the privacy and security issues encountered within the process of data collection. This survey article presents an overview of smart city sensing and its current challenges followed by the potential of Federated Learning in addressing those challenges. A comprehensive discussion of the state-of-the-art methods for Federated Learning is provided along with an in-depth discussion on the applicability of Federated Learning in smart city sensing; clear insights on open issues, challenges, and opportunities in this field are provided as guidance for the researchers studying this subject matter.},
	number = {21},
	journal = {Sensors (Switzerland)},
	author = {Jiang, Ji Chu and Kantarci, Burak and Oktug, Sema and Soyata, Tolga},
	year = {2020},
	pmid = {33142863},
	keywords = {Federated learning, Internet of things, Machine learning, Privacy, Security, Smart cities sensing},
	pages = {1--29},
}

@article{Zhao2020a,
	title = {Detecting and mitigating poisoning attacks in federated learning using generative adversarial networks},
	issn = {15320634},
	doi = {10.1002/cpe.5906},
	abstract = {In the age of the Internet of Things (IoT), large numbers of sensors and edge devices are deployed in various application scenarios; Therefore, collaborative learning is widely used in IoT to implement crowd intelligence by inviting multiple participants to complete a training task. As a collaborative learning framework, federated learning is designed to preserve user data privacy, where participants jointly train a global model without uploading their private training data to a third party server. Nevertheless, federated learning is under the threat of poisoning attacks, where adversaries can upload malicious model updates to contaminate the global model. To detect and mitigate poisoning attacks in federated learning, we propose a poisoning defense mechanism, which uses generative adversarial networks to generate auditing data in the training procedure and removes adversaries by auditing their model accuracy. Experiments conducted on two well-known datasets, MNIST and Fashion-MNIST, suggest that federated learning is vulnerable to the poisoning attack, and the proposed defense method can detect and mitigate the poisoning attack.},
	number = {December 2019},
	journal = {Concurrency Computation},
	author = {Zhao, Ying and Chen, Junjun and Zhang, Jiale and Wu, Di and Blumenstein, Michael and Yu, Shui},
	year = {2020},
	keywords = {federated learning, generative adversarial networks, model security, poisoning attacks, ★},
	pages = {1--12},
}

@article{Ma2019,
	title = {On safeguarding privacy and security in the framework of federated learning},
	abstract = {Motivated by the advancing computational capacity of wireless end-user equipment (UE), as well as the increasing concerns about sharing private data, a new machine learning (ML) paradigm has emerged, namely federated learning (FL). Specifically, FL allows a decoupling of data provision at UEs and ML model aggregation at a central unit. By training model locally, FL is capable of avoiding data leakage from the UEs, thereby preserving privacy and security to some extend. However, even if raw data are not disclosed from UEs, individual's private information can still be extracted by some recently discovered attacks in the FL architecture. In this work, we analyze the privacy and security issues in FL, and raise several challenges on preserving privacy and security when designing FL systems. In addition, we provide extensive simulation results to illustrate the discussed issues and possible solutions.},
	journal = {arXiv},
	author = {Ma, Chuan and Li, Jun and Ding, Ming and Yang, Howard H. and Shu, Feng and Quek, Tony Q.S. and Vincent, H.},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {Federated Learning, Privacy, Security},
	pages = {1--7},
}

@article{Tolpegin2020,
	title = {Data poisoning attacks against federated learning systems},
	volume = {12308 LNCS},
	issn = {16113349},
	doi = {10.1007/978-3-030-58951-6_24},
	abstract = {Federated learning (FL) is an emerging paradigm for distributed training of large-scale deep neural networks in which participants’ data remains on their own devices with only model updates being shared with a central server. However, the distributed nature of FL gives rise to new threats caused by potentially malicious participants. In this paper, we study targeted data poisoning attacks against FL systems in which a malicious subset of the participants aim to poison the global model by sending model updates derived from mislabeled data. We first demonstrate that such data poisoning attacks can cause substantial drops in classification accuracy and recall, even with a small percentage of malicious participants. We additionally show that the attacks can be targeted, i.e., they have a large negative impact only on classes that are under attack. We also study attack longevity in early/late round training, the impact of malicious participant availability, and the relationships between the two. Finally, we propose a defense strategy that can help identify malicious participants in FL to circumvent poisoning attacks, and demonstrate its effectiveness.},
	number = {2},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Tolpegin, Vale and Truex, Stacey and Gursoy, Mehmet Emre and Liu, Ling},
	year = {2020},
	note = {arXiv: 2007.08432
ISBN: 9783030589509},
	keywords = {Adversarial machine learning, Data poisoning, Deep learning, Federated learning, Label flipping, ★},
	pages = {480--501},
}

@inproceedings{ChulinXie2019,
	title = {{DBA}: {Distributed} {Backdoor} {Attacks} against {Federated} {Learning}},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {{Chulin Xie} and Huang, Keli and Chen, Pin-Yu and Li, Bo},
	year = {2019},
	pages = {1--19},
}

@book{Tolpegin2020a,
	title = {Federated {Learning} {Systems}},
	isbn = {978-3-030-58951-6},
	url = {http://dx.doi.org/10.1007/978-3-030-58951-6_24},
	publisher = {Springer International Publishing},
	author = {Tolpegin, Vale and B, Stacey Truex and Gursoy, Mehmet Emre and Liu, Ling},
	year = {2020},
	doi = {10.1007/978-3-030-58951-6},
	keywords = {Federated learning,Adversarial machine learning,La, adversarial machine learning, data poisoning, deep learning, federated learning, flipping},
}

@inproceedings{Zhang2019a,
	title = {{PEFL}: {A} {Privacy}-{Enhanced} {Federated} {Learning} {Scheme} for {Big} {Data} {Analytics}},
	isbn = {978-1-72810-962-6},
	url = {https://ieeexplore.ieee.org/document/9014272/},
	doi = {10.1109/GLOBECOM38437.2019.9014272},
	booktitle = {2019 {IEEE} {Global} {Communications} {Conference} ({GLOBECOM})},
	publisher = {IEEE},
	author = {Zhang, Jiale and Chen, Bing and Yu, Shui and Deng, Hai},
	month = dec,
	year = {2019},
	pages = {1--6},
}

@techreport{Odena2017,
	title = {Conditional {Image} {Synthesis} with {Auxiliary} {Classifier} {GANs}},
	url = {https://github.com/openai/improved-gan/.},
	abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 × 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 × 128 samples are more than twice as discriminable as artificially resized 32 × 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
	author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
	year = {2017},
}

@techreport{Bhagoji,
	title = {Analyzing {Federated} {Learning} through an {Adversarial} {Lens}},
	url = {https://archive.ics.uci.edu/ml/datasets/},
	abstract = {Federated learning distributes model training among a multitude of agents, who, guided by privacy concerns, perform training using their local data but share only model parameter updates, for iterative aggregation at the server to train an overall global model. In this work, we explore how the federated learning setting gives rise to a new threat, namely model poisoning, different from traditional data poisoning. Model poisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to mis-classify a set of chosen inputs with high confidence. We explore a number of attack strategies for deep neural networks, starting with targeted model poisoning using boosting of the malicious agent's update to overcome the effects of other agents. We also propose two critical notions of stealth to detect malicious updates. We bypass these by including them in the adversar-ial objective to carry out stealthy model poisoning. We improve attack stealth with the use of an alternating minimization strategy which alternately optimizes for stealth and the adversarial objective. We also empirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results show that effective and stealthy model poisoning attacks are possible, highlighting vulnerabilities in the fed-erated learning setting.},
	author = {Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
}

@inproceedings{Zhao2020c,
	title = {{PDGAN}: {A} {Novel} {Poisoning} {Defense} {Method} in {Federated} {Learning} {Using} {Generative} {Adversarial} {Network}},
	volume = {11944 LNCS},
	isbn = {978-3-030-38990-1},
	doi = {10.1007/978-3-030-38991-8_39},
	abstract = {Federated learning can complete an enormous training task efficiently by inviting participants to train a deep learning model collaboratively, and the user privacy will be well preserved for the users only upload model parameters to the centralized server. However, the attackers can initiate poisoning attacks by uploading malicious updates in federated learning. Therefore, the accuracy of the global model will be impacted significantly after the attack. To address this vulnerability, we propose a novel poisoning defense generative adversarial network (PDGAN) to defend the poising attack. The PDGAN can reconstruct training data from model updates and audit the accuracy for each participant model by using the generated data. Precisely, the participant whose accuracy is lower than a predefined threshold will be identified as an attacker and model parameters of the attacker will be removed from the training procedure in this iteration. Experiments conducted on MNIST and Fashion-MNIST datasets demonstrate that our approach can indeed defend the poisoning attacks in federated learning.},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	publisher = {Springer},
	author = {Zhao, Ying and Chen, Junjun and Zhang, Jiale and Wu, Di and Teng, Jian and Yu, Shui},
	year = {2020},
	note = {ISSN: 16113349},
	keywords = {Federated learning, Generative adversarial network, Poisoning defense},
	pages = {595--609},
}

@article{Zhang2021,
	title = {{PoisonGAN}: {Generative} {Poisoning} {Attacks} against {Federated} {Learning} in {Edge} {Computing} {Systems}},
	volume = {8},
	issn = {23274662},
	doi = {10.1109/JIOT.2020.3023126},
	abstract = {Edge computing is a key-enabling technology that meets continuously increasing requirements for the intelligent Internet-of-Things (IoT) applications. To cope with the increasing privacy leakages of machine learning while benefiting from unbalanced data distributions, federated learning has been wildly adopted as a novel intelligent edge computing framework with a localized training mechanism. However, recent studies found that the federated learning framework exhibits inherent vulnerabilities on active attacks, and poisoning attack is one of the most powerful and secluded attacks where the functionalities of the global model could be damaged through attacker's well-crafted local updates. In this article, we give a comprehensive exploration of the poisoning attack mechanisms in the context of federated learning. We first present a poison data generation method, named Data\_Gen, based on the generative adversarial networks (GANs). This method mainly relies upon the iteratively updated global model parameters to regenerate samples of interested victims. Second, we further propose a novel generative poisoning attack model, named PoisonGAN, against the federated learning framework. This model utilizes the designed Data\_Gen method to efficiently reduce the attack assumptions and make attacks feasible in practice. We finally evaluate our data generation and attack models by implementing two types of typical poisoning attack strategies, label flipping and backdoor, on a federated learning prototype. The experimental results demonstrate that these two attack models are effective in federated learning.},
	number = {5},
	urldate = {2021-05-28},
	journal = {IEEE Internet of Things Journal},
	author = {Zhang, Jiale and Chen, Bing and Cheng, Xiang and Binh, Huynh Thi Thanh and Yu, Shui},
	month = mar,
	year = {2021},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Backdoor attack, federated learning, generative adversarial nets, label flipping, poisoning attacks},
	pages = {3310--3322},
}

@article{li_edgefed_2020,
	title = {{EdgeFed} : {Optimized} {Federated} {Learning} {Based} on {Edge} {Computing}},
	doi = {10.1109/ACCESS.2020.3038287},
	author = {Li, Shen and Liu, Fang and Tang, Yonghao and Hu, Wanting},
	year = {2020},
	pages = {209191--209198},
}

@article{Hosseinalipour2020,
	title = {From {Federated} to {Fog} {Learning}: {Distributed} {Machine} {Learning} over {Heterogeneous} {Wireless} {Networks}},
	volume = {58},
	issn = {15581896},
	doi = {10.1109/MCOM.001.2000410},
	abstract = {Machine learning (ML) tasks are becoming ubiquitous in today's network applications. Federated learning has emerged recently as a technique for training ML models at the network edge by leveraging processing capabilities across the nodes that collect the data. There are several challenges with employing conventional federated learning in contemporary networks, due to the significant heterogeneity in compute and communication capabilities that exist across devices. To address this, we advocate a new learning paradigm called fog learning, which will intelligently distribute ML model training across the continuum of nodes from edge devices to cloud servers. Fog learning enhances federated learning along three major dimensions: network, heterogeneity, and proximity. It considers a multi-layer hybrid learning framework consisting of heterogeneous devices with various proximities. It accounts for the topology structures of the local networks among the heterogeneous nodes at each network layer, orchestrating them for collaborative/cooperative learning through device-to-device communications. This migrates from star network topologies used for parameter transfers in federated learning to more distributed topologies at scale. We discuss several open research directions toward realizing fog learning.},
	number = {12},
	journal = {IEEE Communications Magazine},
	author = {Hosseinalipour, Seyyedali and Brinton, Christopher G. and Aggarwal, Vaneet and Dai, Huaiyu and Chiang, Mung},
	year = {2020},
	note = {arXiv: 2006.03594},
	pages = {41--47},
}

@article{liu_client-edge-cloud_2020,
	title = {Client-{Edge}-{Cloud} {Hierarchical} {Federated} {Learning}},
	volume = {2020-June},
	issn = {15503607},
	doi = {10.1109/ICC40277.2020.9148862},
	abstract = {Federated Learning is a collaborative machine learning framework to train a deep learning model without accessing clients' private data. Previous works assume one central parameter server either at the cloud or at the edge. The cloud server can access more data but with excessive communication overhead and long latency, while the edge server enjoys more efficient communications with the clients. To combine their advantages, we propose a client-edge-cloud hierarchical Federated Learning system, supported with a HierFAVG algorithm that allows multiple edge servers to perform partial model aggregation. In this way, the model can be trained faster and better communication-computation trade-offs can be achieved. Convergence analysis is provided for HierFAVG and the effects of key parameters are also investigated, which lead to qualitative design guidelines. Empirical experiments verify the analysis and demonstrate the benefits of this hierarchical architecture in different data distribution scenarios. Particularly, it is shown that by introducing the intermediate edge servers, the model training time and the energy consumption of the end devices can be simultaneously reduced compared to cloud-based Federated Learning.},
	journal = {IEEE International Conference on Communications},
	author = {Liu, Lumin and Zhang, Jun and Song, S. H. and Letaief, Khaled B.},
	year = {2020},
	note = {arXiv: 1905.06641
ISBN: 9781728150895},
	keywords = {Edge Learning, Federated Learning, Mobile Edge Computing},
	pages = {0--5},
}

@article{zhang_efficient_2020,
	title = {Efficient {Federated} {Learning} for {Cloud}-{Based} {AIoT} {Applications}},
	volume = {X},
	issn = {19374151},
	doi = {10.1109/TCAD.2020.3046665},
	abstract = {As a promising method for central model training on decentralized device data without compromising user privacy, Federated Learning (FL) is becoming more and more popular in Internet of Things (IoTs) design. However, due to limited computing and memory resources of devices that restrict the capabilities of hosted deep learning models, existing FL approaches for AI IoT (AIoT) applications suffer from inaccurate prediction results. To address this problem, this paper presents a collaborative Big.Little branch architecture to enable efficient FL for AIoT applications. Inspired by the architecture of BranchyNet which has multiple prediction branches, our approach deploys Deep Neural Network (DNN) models across both cloud and AIoT devices. Our Big.Little branch model has two branches, where the big branch is deployed on cloud for strengthened prediction accuracy, and the little branches are used to fit for AIoT devices. When AIoT devices cannot make the prediction with high confidence using local little branches, they will resort to the big branch for further inference. To increase both prediction accuracy and early exit rate of Big.Little branch model, we propose a two-stage training and co-inference scheme, which considers the local characteristics of AIoT scenarios. Comprehensive experiment results obtained from a real AIoT environment demonstrate the efficiency and effectiveness of our approach in terms of prediction accuracy and average inference time.},
	number = {c},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Zhang, Xinqian and Hu, Ming and Xia, Jun and Wei, Tongquan and Chen, Mingsong and Hu, Shiyan},
	year = {2020},
	keywords = {AIoT, BranchyNet., Cloud computing, Collaborative work, Computational modeling, Computer architecture, Data models, Predictive models, Servers, cloud computing, deep neural network, federated learning, inference accuracy},
	pages = {1--13},
}

@article{xia_survey_2021,
	title = {A survey of federated learning for edge computing: {Research} problems and solutions},
	volume = {1},
	issn = {26672952},
	url = {https://doi.org/10.1016/j.hcc.2021.100008},
	doi = {10.1016/j.hcc.2021.100008},
	abstract = {Federated Learning is a machine learning scheme in which a shared prediction model can be collaboratively learned by a number of distributed nodes using their locally stored data. It can provide better data privacy because training data are not transmitted to a central server. Federated learning is well suited for edge computing applications and can leverage the the computation power of edge servers and the data collected on widely dispersed edge devices. To build such an edge federated learning system, we need to tackle a number of technical challenges. In this survey, we provide a new perspective on the applications, development tools, communication efficiency, security \& privacy, migration and scheduling in edge federated learning.},
	number = {1},
	journal = {High-Confidence Computing},
	author = {Xia, Qi and Ye, Winson and Tao, Zeyi and Wu, Jindi and Li, Qun},
	year = {2021},
	note = {Publisher: Elsevier B.V.},
	keywords = {"Edge computing", "Federated learning", ★},
	pages = {100008},
}

@article{khan_federated_2021,
	title = {Federated {Learning} for {Internet} of {Things}: {Recent} {Advances}, {Taxonomy}, and {Open} {Challenges}},
	doi = {10.1109/comst.2021.3090430},
	abstract = {The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithms for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Third, we propose two IoT use cases of dispersed federated learning that can offer better privacy preservation than federated learning. Finally, we present several open research challenges with their possible solutions.},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Khan, Latif U. and Saad, Walid and Han, Zhu and Hossain, Ekram and Hong, Choong Seon},
	month = jun,
	year = {2021},
	note = {arXiv: 2009.13012
Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {1--1},
}

@article{khan_federated_2020,
	title = {Federated {Learning} for {Edge} {Networks}: {Resource} {Optimization} and {Incentive} {Mechanism}},
	volume = {58},
	issn = {15581896},
	doi = {10.1109/MCOM.001.1900649},
	abstract = {Recent years have witnessed a rapid proliferation of smart Internet of Things (IoT) devices. IoT devices with intelligence require the use of effective machine learning paradigms. Federated learning can be a promising solution for enabling IoT-based smart applications. In this article, we present the primary design aspects for enabling federated learning at the network edge. We model the incentive- based interaction between a global server and participating devices for federated learning via a Stackelberg game to motivate the participation of the devices in the federated learning process. We present several open research challenges with their possible solutions. Finally, we provide an outlook on future research.},
	number = {10},
	journal = {IEEE Communications Magazine},
	author = {Khan, Latif U. and Pandey, Shashi Raj and Tran, Nguyen H. and Saad, Walid and Han, Zhu and Nguyen, Minh N.H. and Hong, Choong Seon},
	month = oct,
	year = {2020},
	note = {arXiv: 1911.05642
Publisher: Institute of Electrical and Electronics Engineers Inc.},
	pages = {88--93},
}

@article{Vorobeychik2018,
	title = {Adversarial {Machine} {Learning}},
	volume = {12},
	issn = {1939-4608},
	url = {https://www.morganclaypool.com/doi/10.2200/S00861ED1V01Y201806AIM039},
	doi = {10.2200/S00861ED1V01Y201806AIM039},
	number = {3},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Vorobeychik, Yevgeniy and Kantarcioglu, Murat},
	month = aug,
	year = {2018},
	note = {ISBN: 9781681733951},
	pages = {1--169},
}

@article{Chen2017,
	title = {Targeted backdoor attacks on deep learning systems using data poisoning},
	issn = {23318422},
	abstract = {Deep learning models have achieved high performance on many tasks, and thus have been applied to many security-critical scenarios. For example, deep learning-based face recognition systems have been used to authenticate users to access many security-sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work, we consider a new type of attacks, called backdoor attacks, where the attacker’s goal is to create a backdoor into a learning-based authentication system, so that he can easily circumvent the system by leveraging the backdoor. Specifically, the adversary aims at creating backdoor instances, so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular, we study backdoor poisoning attacks, which achieve backdoor attacks using poisoning strategies. That is, the attacker injects poisoning samples into the training set to achieve his adversarial goal. Different from all existing work, our studied poisoning strategies can apply under a very weak threat model: (1) the adversary has no knowledge of the model and the training set used by the victim system; (2) the attacker is allowed to inject only a small amount of poisoning samples; (3) the backdoor key is hard to notice even by human beings to achieve stealthiness. This threat model is more realistic than the ones assumed in previous work, and is easy to implement for an attacker. Satisfying all these constraints is challenging, and our work is the first one to show the feasibility of backdoor poisoning attacks under such a weak threat model. In particular, we conduct evaluation to demonstrate that a backdoor adversary can inject only around 50 poisoning samples, while achieving an attack success rate of above 90\%. We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system, and thus highlights the importance of further investigation and proposing defense strategies against them.},
	journal = {arXiv},
	author = {Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
	year = {2017},
	note = {arXiv: 1712.05526},
}

@article{zhang_recent_2020,
	title = {Recent advances in quantum machine learning},
	volume = {2},
	issn = {2577-0470},
	doi = {10.1002/que2.34},
	abstract = {Here we discuss advances in the field of quantum machine learning. The following document offers a hybrid discussion; both reviewing the field as it is currently, and suggesting directions for further research. We include both algorithms and experimental implementations in the discussion. The field's outlook is generally positive, showing significant promise. However, we believe there are appreciable hurdles to overcome before one can claim that it is a primary application of quantum computation.},
	number = {1},
	journal = {Quantum Engineering},
	author = {Zhang, Yao and Ni, Qiang},
	month = mar,
	year = {2020},
	note = {Publisher: Wiley},
	keywords = {★},
}

@article{oquinn_quantum_2020,
	title = {Quantum machine learning: {Recent} advances and outlook},
	volume = {27},
	issn = {15580687},
	doi = {10.1109/MWC.001.1900341},
	abstract = {Quantum computing is currently at the nexus of physics and engineering. Although current generation quantum processors are small and noisy, advancements are happening at an astounding rate. In addition, machine learning has played a crucial role in many recent advances. The combination of these two fields, Quantum Machine Learning, is a small but extremely promising new field with the possibility of unlimited abilities. This work seeks to provide an introduction to this emerging field, along with a discussion of recent advances as well as problems that are yet to be solved.},
	number = {3},
	journal = {IEEE Wireless Communications},
	author = {O'Quinn, Wesley and Mao, Shiwen},
	month = jun,
	year = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	pages = {126--131},
}

@article{kardashin_quantum_2021,
	title = {Quantum {Machine} {Learning} {Tensor} {Network} {States}},
	volume = {8},
	issn = {2296424X},
	doi = {10.3389/fphy.2020.586374},
	abstract = {Tensor network algorithms seek to minimize correlations to compress the classical data representing quantum states. Tensor network algorithms and similar tools—called tensor network methods—form the backbone of modern numerical methods used to simulate many-body physics and have a further range of applications in machine learning. Finding and contracting tensor network states is a computational task, which may be accelerated by quantum computing. We present a quantum algorithm that returns a classical description of a rank-r tensor network state satisfying an area law and approximating an eigenvector given black-box access to a unitary matrix. Our work creates a bridge between several contemporary approaches, including tensor networks, the variational quantum eigensolver (VQE), quantum approximate optimization algorithm (QAOA), and quantum computation.},
	number = {March},
	journal = {Frontiers in Physics},
	author = {Kardashin, Andrey and Uvarov, Alexey and Biamonte, Jacob},
	year = {2021},
	note = {arXiv: 1804.02398},
	keywords = {ground state, machine learning, properties, quantum algorithms and circuits, quantum computing, quantum information, tensor network algorithms},
	pages = {1--6},
}

@article{hu_quantum_2019,
	title = {Quantum machine learning with {D}‐wave quantum computer},
	volume = {1},
	issn = {2577-0470},
	doi = {10.1002/que2.12},
	abstract = {Summary The new era of artificial intelligence (AI) aims to entangle the relationships among models (characterizations), algorithms, and implementations toward the high-level intelligence with general cognitive ability, strong robustness, and interpretability, which is intractable for machine learning (ML). Quantum computer provides a new computing paradigm for ML. Although universal quantum computers are still in infancy, special-purpose D-Wave machine hopefully becomes the breaking point of commercialized quantum computing. The core principle, quantum annealing (QA), enables the quantum system to naturally evolve toward the low-energy states. D-Wave's quantum computer has developed some applications of quantum ML based on quantum-assisted ML algorithms, quantum Boltzmann machine, etc. Additionally, working with CPUs, quantum processing units is likely to advance ML in a quantum-inspired way. Thus, a new advanced computing architecture, quantum-classical hybrid approach consisting of QA, classical computing, and brain-inspired cognitive science, is required to explore its superiority to universal quantum algorithms and classical ML algorithms. It is important to explore hybrid quantum/classical approaches to overcome the defects of ML such as high dependence on training data, low robustness to the noises, and cognitive impairment. The new framework is expected to gradually form a highly effective, accurate, and adaptive intelligent computing architecture for the next generation of AI.},
	number = {2},
	journal = {Quantum Engineering},
	author = {Hu, Feng and Wang, Ban‐Nan and Wang, Ning and Wang, Chao},
	month = jun,
	year = {2019},
	note = {Publisher: Wiley},
}

@article{nawaz_quantum_2019,
	title = {Quantum {Machine} {Learning} for {6G} {Communication} {Networks}: {State}-of-the-{Art} and {Vision} for the {Future}},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2909490},
	abstract = {The upcoming fifth generation (5G) of wireless networks is expected to lay a foundation of intelligent networks with the provision of some isolated artificial intelligence (AI) operations. However, fully intelligent network orchestration and management for providing innovative services will only be realized in Beyond 5G (B5G) networks. To this end, we envisage that the sixth generation (6G) of wireless networks will be driven by on-demand self-reconfiguration to ensure a many-fold increase in the network performance and service types. The increasingly stringent performance requirements of emerging networks may finally trigger the deployment of some interesting new technologies, such as large intelligent surfaces, electromagnetic-orbital angular momentum, visible light communications, and cell-free communications, to name a few. Our vision for 6G is a massively connected complex network capable of rapidly responding to the users' service calls through real-time learning of the network state as described by the network edge (e.g., base-station locations and cache contents), air interface (e.g., radio spectrum and propagation channel), and the user-side (e.g., battery-life and locations). The multi-state, multi-dimensional nature of the network state, requiring the real-time knowledge, can be viewed as a quantum uncertainty problem. In this regard, the emerging paradigms of machine learning (ML), quantum computing (QC), and quantum ML (QML) and their synergies with communication networks can be considered as core 6G enablers. Considering these potentials, starting with the 5G target services and enabling technologies, we provide a comprehensive review of the related state of the art in the domains of ML (including deep learning), QC, and QML and identify their potential benefits, issues, and use cases for their applications in the B5G networks. Subsequently, we propose a novel QC-assisted and QML-based framework for 6G communication networks while articulating its challenges and potential enabling technologies at the network infrastructure, network edge, air interface, and user end. Finally, some promising future research directions for the quantum- and QML-assisted B5G networks are identified and discussed.},
	number = {Ml},
	journal = {IEEE Access},
	author = {Nawaz, Syed Junaid and Sharma, Shree Krishna and Wyne, Shurjeel and Patwary, Mohammad N. and Asaduzzaman, Md},
	year = {2019},
	note = {Publisher: IEEE},
	keywords = {6G, B5G, machine learning, quantum communications, quantum machine learning},
	pages = {46317--46350},
}

@article{huang_power_2021,
	title = {Power of data in quantum machine learning},
	volume = {12},
	issn = {20411723},
	doi = {10.1038/s41467-021-22539-9},
	abstract = {The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. However, machine learning tasks where data is provided can be considerably different than commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be easily predicted by classical machines learning from data. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing potential quantum advantage in learning tasks. The bounds are tight asymptotically and empirically predictive for a wide range of learning models. These constructions explain numerical results showing that with the help of data, classical machine learning models can be competitive with quantum models even if they are tailored to quantum problems. We then propose a projected quantum model that provides a simple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. For near-term implementations, we demonstrate a significant prediction advantage over some classical models on engineered data sets designed to demonstrate a maximal quantum advantage in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits.},
	number = {1},
	journal = {Nature Communications},
	author = {Huang, Hsin Yuan and Broughton, Michael and Mohseni, Masoud and Babbush, Ryan and Boixo, Sergio and Neven, Hartmut and McClean, Jarrod R.},
	month = dec,
	year = {2021},
	pmid = {33976136},
	note = {arXiv: 2011.01938
Publisher: Nature Research},
}

@article{adcock_advances_2015,
	title = {Advances in quantum machine learning},
	url = {http://arxiv.org/abs/1512.02900},
	abstract = {Here we discuss advances in the field of quantum machine learning. The following document offers a hybrid discussion; both reviewing the field as it is currently, and suggesting directions for further research. We include both algorithms and experimental implementations in the discussion. The field's outlook is generally positive, showing significant promise. However, we believe there are appreciable hurdles to overcome before one can claim that it is a primary application of quantum computation.},
	author = {Adcock, Jeremy and Allen, Euan and Day, Matthew and Frick, Stefan and Hinchliff, Janna and Johnson, Mack and Morley-Short, Sam and Pallister, Sam and Price, Alasdair and Stanisic, Stasja},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.02900},
}

@article{haug_large-scale_2021,
	title = {Large-scale quantum machine learning},
	url = {http://arxiv.org/abs/2108.01039},
	abstract = {Quantum computers promise to enhance machine learning for practical applications. Quantum machine learning for real-world data has to handle extensive amounts of high-dimensional data. However, conventional methods for measuring quantum kernels are impractical for large datasets as they scale with the square of the dataset size. Here, we measure quantum kernels using randomized measurements to gain a quadratic speedup in computation time and quickly process large datasets. Further, we efficiently encode high-dimensional data into quantum computers with the number of features scaling linearly with the circuit depth. The encoding is characterized by the quantum Fisher information metric and is related to the radial basis function kernel. We demonstrate the advantages and speedups of our methods by classifying images with the IBM quantum computer. Our approach is exceptionally robust to noise via a complementary error mitigation scheme. Using currently available quantum computers, the MNIST database can be processed within 220 hours instead of 10 years which opens up industrial applications of quantum machine learning.},
	author = {Haug, Tobias and Self, Chris N. and Kim, M. S.},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.01039},
	keywords = {★},
}

@incollection{pedro_dos_santos_goncalves_quantum_2019,
	title = {Quantum {Neural} {Machine} {Learning}: {Theory} and {Experiments}},
	volume = {i},
	url = {https://www.intechopen.com/books/artificial-intelligence-applications-in-medicine-and-biology/quantum-neural-machine-learning-theory-and-experiments},
	abstract = {Abstract Long-haul travel does not constitute an obstacle for tourists to travel and is fast gaining the attention of tourists in new and unique experiences. This study was conducted to identify the long-haul travel motivation by international tourists to Penang. A total of 400 respondents participated in this survey, conducted around the tourist attractions in Penang, using cluster random sampling. However, only 370 questionnaires were only used for this research. Data were analysed using SPSS software 22 version. The findings, ‘knowledge and novelty seeking’ were the main push factors that drove long-haul travel by international tourists to Penang. Meanwhile, the main pull factor that attracts long- haul travel by international tourists to Penang was its ‘culture and history’. Additionally, there were partly direct and significant relationships between socio-demographic, trip characteristics and travel motivation (push factors and pull factors). Overall, this study identified the long-haul travel motivations by international tourists to Penang based on socio-demographic, trip characteristics and travel motivation and has indirectly helped in understanding the long-haul travel market particularly for Penang and Southeast Asia. This research also suggested for an effective marketing and promotion strategy in pro- viding useful information that is the key to attract international tourists to travel long distances. Keywords:},
	booktitle = {Artificial {Intelligence} - {Applications} in {Medicine} and {Biology}},
	publisher = {IntechOpen},
	author = {Pedro dos Santos Gonçalves, Carlos},
	month = jul,
	year = {2019},
	doi = {10.5772/intechopen.84149},
	note = {Issue: tourism},
	keywords = {tourism},
	pages = {13},
}

@article{ferrari_efficient_2018,
	title = {Efficient and effective quantum compiling for entanglement-based machine learning on {IBM} {Q} devices},
	volume = {16},
	issn = {02197499},
	doi = {10.1142/S0219749918400063},
	abstract = {Quantum compiling means fast, device-aware implementation of quantum algorithms (i.e. quantum circuits, in the quantum circuit model of computation). In this paper, we present a strategy for compiling IBM Q-aware, low-depth quantum circuits that generate Greenberger-Horne-Zeilinger (GHZ) entangled states. The resulting compiler can replace the QISKit compiler for the specific purpose of obtaining improved GHZ circuits. It is well known that GHZ states have several practical applications, including quantum machine learning. We illustrate our experience in implementing and querying a uniform quantum example oracle based on the GHZ circuit, for solving the classically hard problem of learning parity with noise.},
	number = {8},
	journal = {International Journal of Quantum Information},
	author = {Ferrari, Davide and Amoretti, Michele},
	year = {2018},
	note = {arXiv: 1801.02363},
	keywords = {IBM Q, Quantum compiling, parity learning},
}

@article{khan_machine_2020,
	title = {Machine {Learning}: {Quantum} vs {Classical}},
	volume = {8},
	issn = {21693536},
	doi = {10.1109/ACCESS.2020.3041719},
	abstract = {Encouraged by growing computing power and algorithmic development, machine learning technologies have become powerful tools for a wide variety of application areas, spanning from agriculture to chemistry and natural language processing. The use of quantum systems to process classical data using machine learning algorithms has given rise to an emerging research area, i.e. quantum machine learning. Despite its origins in the processing of classical data, quantum machine learning also explores the use of quantum phenomena for learning systems, the use of quantum computers for learning on quantum data and how machine learning algorithms and software can be formulated and implemented on quantum computers. Quantum machine learning can have a transformational effect on computer science. It may speed up the processing of information well beyond the existing classical speeds. Recent work has seen the development of quantum algorithms that could serve as foundations for machine learning applications. Despite its great promise, there are still significant hardware and software challenges that need to be resolved before quantum machine learning becomes practical. In this paper, we present an overview of quantum machine learning in the light of classical approaches. Departing from foundational concepts of machine learning and quantum computing, we discuss various technical contributions, strengths and similarities of the research work in this domain. We also elaborate upon the recent progress of different quantum machine learning approaches, their complexity, and applications in various fields such as physics, chemistry and natural language processing.},
	journal = {IEEE Access},
	author = {Khan, Tariq M. and Robles-Kelly, Antonio},
	year = {2020},
	keywords = {QuBit, Quantum machine learning, quantum algorithms, quantum computing},
	pages = {219275--219294},
}

@article{perdomo-ortiz_opportunities_2018,
	title = {Opportunities and challenges for quantum-assisted machine learning in near-term quantum computers},
	volume = {3},
	issn = {20589565},
	doi = {10.1088/2058-9565/aab859},
	abstract = {With quantum computing technologies nearing the era of commercialization and quantum supremacy, machine learning (ML) appears as one of the promising 'killer' applications. Despite significant effort, there has been a disconnect between most quantum ML proposals, the needs of ML practitioners, and the capabilities of near-term quantum devices to demonstrate quantum enhancement in the near future. In this contribution to the focus collection 'What would you do with 1000 qubits?', we provide concrete examples of intractable ML tasks that could be enhanced with near-term devices. We argue that to reach this target, the focus should be on areas where ML researchers are struggling, such as generative models in unsupervised and semi-supervised learning, instead of the popular and more tractable supervised learning techniques. We also highlight the case of classical datasets with potential quantum-like statistical correlations where quantum models could be more suitable. We focus on hybrid quantum-classical approaches and illustrate some of the key challenges we foresee for near-term implementations. Finally, we introduce the quantum-assisted Helmholtz machine (QAHM), an attempt to use near-term quantum devices to tackle high-dimensional datasets of continuous variables. Instead of using quantum computers to assist deep learning, as previous approaches do, the QAHM uses deep learning to extract a low-dimensional binary representation of data, suitable for relatively small quantum processors which can assist the training of an unsupervised generative model. Although we illustrate this concept on a quantum annealer, other quantum platforms could benefit as well from this hybrid quantum-classical framework.},
	number = {3},
	journal = {Quantum Science and Technology},
	author = {Perdomo-Ortiz, Alejandro and Benedetti, Marcello and Realpe-Gómez, John and Biswas, Rupak},
	year = {2018},
	note = {arXiv: 1708.09757
Publisher: IOP Publishing},
	keywords = {hybrid algorithms, near-term quantum computers, quantum annealing, quantum machine learning, quantum-assisted machine learning, unsupervised generative models, unsupervised learning, ★},
}

@article{sergioli_quantum_2020,
	title = {Quantum and quantum-like machine learning: a note on differences and similarities},
	volume = {24},
	issn = {14337479},
	url = {https://doi.org/10.1007/s00500-019-04429-x},
	doi = {10.1007/s00500-019-04429-x},
	abstract = {In the past few decades, researchers have extensively investigated the applications of quantum computation and quantum information to machine learning with remarkable results. This, in turn, has led to the emergence of quantum machine learning as a separate discipline, whose main goal is to transform standard machine learning algorithms into quantum algorithms which can be implemented on quantum computers. One further research programme has involved using quantum information to create new quantum-like algorithms for classical computers (Sergioli et al. in Int J Theor Phys 56(12):3880–3888, 2017; PLoS ONE 14:e0216224, 2019. https://doi.org/10.1371/journal.pone.0216224; Int J Quantum Inf 16(8):1840011, 2018a; Soft Comput 22(3):691–705, 2018b). This brief survey summarises and compares both approaches and also outlines the main motivations behind them.},
	number = {14},
	journal = {Soft Computing},
	author = {Sergioli, Giuseppe},
	year = {2020},
	note = {Publisher: Springer Berlin Heidelberg},
	keywords = {Binary classification, Quantum information, Quantum machine learning},
	pages = {10247--10255},
}

@article{lloyd_quantum_2021,
	title = {Quantum {Machine} {Learning} for {Data} {Classification}},
	volume = {14},
	issn = {1943-2879},
	doi = {10.1103/physics.14.79},
	journal = {Physics},
	author = {Lloyd, Seth},
	year = {2021},
	pages = {1--3},
}

@article{stein_qugan_2020,
	title = {{QuGAN}: {A} {Generative} {Adversarial} {Network} {Through} {Quantum} {States}},
	url = {http://arxiv.org/abs/2010.09036},
	abstract = {Tremendous progress has been witnessed in artificial intelligence, where neural network backed deep learning systems have been used, with applications in almost every domain. As a representative deep learning framework, Generative Adversarial Network (GAN) has been widely used for generating artificial images, text-to-image or image augmentation across areas of science, arts and video games. However, GANs are computationally expensive, sometimes computationally prohibitive. Furthermore, training GANs may suffer from convergence failure and modal collapse. Aiming at the acceleration of use cases for practical quantum computers, we propose QuGAN, a quantum GAN architecture that provides stable convergence, quantum-states based gradients and significantly reduced parameter sets. The QuGANarchitecture runs both the discriminator and the generator purely on quantum state fidelity and utilizes the swap test on qubits to calculate the values of quantum-based loss functions. Built on quantum layers, QuGAN achieves similar performance with a 94.98\% reduction on the parameter set when compared to classical GANs. With the same number of parameters, addition-ally, QuGAN outperforms state-of-the-art quantum based GANsin the literature providing a 48.33\% improvement in system performance compared to others attaining less than 0.5\% in terms of similarity between generated distributions and original data sets.},
	author = {Stein, Samuel A. and Baheri, Betis and Chen, Daniel and Mao, Ying and Guan, Qiang and Li, Ang and Fang, Bo and Xu, Shuai},
	year = {2020},
	note = {arXiv: 2010.09036},
}

@article{information_quantum_nodate,
	title = {Quantum {Wasserstein} {GANs} {Shouvanik} - {Supplementary} {Materials}},
	volume = {0},
	author = {Information, Quantum},
	pages = {13--23},
}

@article{chakrabarti_quantum_2019,
	title = {Quantum wasserstein {GANs}},
	volume = {32},
	issn = {10495258},
	abstract = {The study of quantum generative models is well motivated, not only because of its importance in quantum machine learning and quantum chemistry but also because of the perspective of its implementation on near-term quantum machines. Inspired by previous studies on the adversarial training of classical and quantum generative models, we propose the first design of quantum Wasserstein Generative Adversarial Networks (WGANs), which has been shown to improve the robustness and the scalability of the adversarial training of quantum generative models even on noisy quantum hardware. Specifically, we propose a definition of the Wasserstein semimetric between quantum data, which inherits a few key theoretical merits of its classical counterpart. We also demonstrate how to turn the quantum Wasserstein semimetric into a concrete design of quantum WGANs that can be efficiently implemented on quantum machines. Our numerical study, via classical simulation of quantum systems, shows the more robust and scalable numerical performance of our quantum WGANs over other quantum GAN proposals. As a surprising application, our quantum WGAN has been used to generate a 3-qubit quantum circuit of {\textasciitilde}50 gates that well approximates a 3-qubit 1-d Hamiltonian simulation circuit that requires over 10k gates using standard techniques.},
	number = {NeurIPS},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chakrabarti, Shouvanik and Huang, Yiming and Li, Tongyang and Feizi, Soheil and Wu, Xiaodi},
	year = {2019},
	pages = {1--12},
}

@article{amin_quantum_2021,
	title = {Quantum {Machine} {Learning} {Architecture} for {COVID}-19 {Classification} {Based} on {Synthetic} {Data} {Generation} {Using} {Conditional} {Adversarial} {Neural} {Network}},
	issn = {1866-9956},
	url = {https://link.springer.com/10.1007/s12559-021-09926-6},
	doi = {10.1007/s12559-021-09926-6},
	journal = {Cognitive Computation},
	author = {Amin, Javaria and Sharif, Muhammad and Gul, Nadia and Kadry, Seifedine and Chakraborty, Chinmay},
	month = aug,
	year = {2021},
}

@article{li_quantum_2020,
	title = {Quantum generative adversarial network: {A} survey},
	volume = {64},
	issn = {15462226},
	doi = {10.32604/CMC.2020.010551},
	abstract = {Generative adversarial network (GAN) is one of the most promising methods for unsupervised learning in recent years. GAN works via adversarial training concept and has shown excellent performance in the fields image synthesis, image super-resolution, video generation, image translation, etc. Compared with classical algorithms, quantum algorithms have their unique advantages in dealing with complex tasks, quantum machine learning (QML) is one of the most promising quantum algorithms with the rapid development of quantum technology. Specifically, Quantum generative adversarial network (QGAN) has shown the potential exponential quantum speedups in terms of performance. Meanwhile, QGAN also exhibits some problems, such as barren plateaus, unstable gradient, model collapse, absent complete scientific evaluation system, etc. How to improve the theory of QGAN and apply it that have attracted some researcher. In this paper, we comprehensively and deeply review recently proposed GAN and QAGN models and their applications, and we discuss the existing problems and future research trends of QGAN.},
	number = {1},
	journal = {Computers, Materials and Continua},
	author = {Li, Tong and Zhang, Shibin and Xia, Jinyue},
	year = {2020},
	keywords = {Generative adversarial network, Mode collapse, Quantum generative adversarial network, Quantum machine learning},
	pages = {401--438},
}

@article{hu_quantum_2019,
	title = {Quantum generative adversarial learning in a superconducting quantum circuit},
	volume = {5},
	issn = {23752548},
	doi = {10.1126/sciadv.aav2761},
	abstract = {Generative adversarial learning is one of the most exciting recent breakthroughs in machine learning. It has shown splendid performance in a variety of challenging tasks such as image and video generation. More recently, a quantum version of generative adversarial learning has been theoretically proposed and shown to have the potential of exhibiting an exponential advantage over its classical counterpart. Here, we report the first proof-of-principle experimental demonstration of quantum generative adversarial learning in a superconducting quantum circuit. We demonstrate that, after several rounds of adversarial learning, a quantum-state generator can be trained to replicate the statistics of the quantum data output from a quantum channel simulator, with a high fidelity (98.8\% on average) so that the discriminator cannot distinguish between the true and the generated data. Our results pave the way for experimentally exploring the intriguing long-sought-after quantum advantages in machine learning tasks with noisy intermediate–scale quantum devices.},
	number = {1},
	journal = {Science Advances},
	author = {Hu, Ling and Wu, Shu Hao and Cai, Weizhou and Ma, Yuwei and Mu, Xianghao and Xu, Yuan and Wang, Haiyan and Song, Yipu and Deng, Dong Ling and Zou, Chang Ling and Sun, Luyan},
	year = {2019},
	pmid = {30746476},
	note = {arXiv: 1808.02893},
	pages = {1--7},
}

@article{lloyd_quantum_2018,
	title = {Quantum {Generative} {Adversarial} {Learning}},
	volume = {121},
	issn = {10797114},
	url = {https://doi.org/10.1103/PhysRevLett.121.040502},
	doi = {10.1103/PhysRevLett.121.040502},
	abstract = {Generative adversarial networks represent a powerful tool for classical machine learning: a generator tries to create statistics for data that mimics those of a true data set, while a discriminator tries to discriminate between the true and fake data. The learning process for generator and discriminator can be thought of as an adversarial game, and under reasonable assumptions, the game converges to the point where the generator generates the same statistics as the true data and the discriminator is unable to discriminate between the true and the generated data. This Letter introduces the notion of quantum generative adversarial networks, where the data consist either of quantum states or of classical data, and the generator and discriminator are equipped with quantum information processors. We show that the unique fixed point of the quantum adversarial game also occurs when the generator produces the same statistics as the data. Neither the generator nor the discriminator perform quantum tomography; linear programing drives them to the optimal. Since quantum systems are intrinsically probabilistic, the proof of the quantum case is different from - and simpler than - the classical case. We show that, when the data consist of samples of measurements made on high-dimensional spaces, quantum adversarial networks may exhibit an exponential advantage over classical adversarial networks.},
	number = {4},
	journal = {Physical Review Letters},
	author = {Lloyd, Seth and Weedbrook, Christian},
	year = {2018},
	pmid = {30095952},
	note = {arXiv: 1804.09139
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevLett.121.040502 url:https://doi, ★},
	pages = {40502},
}

@article{zoufal_quantum_2019,
	title = {Quantum {Generative} {Adversarial} {Networks} for learning and loading random distributions},
	volume = {5},
	issn = {20566387},
	url = {http://dx.doi.org/10.1038/s41534-019-0223-2},
	doi = {10.1038/s41534-019-0223-2},
	abstract = {Quantum algorithms have the potential to outperform their classical counterparts in a variety of tasks. The realization of the advantage often requires the ability to load classical data efficiently into quantum states. However, the best known methods require O(2 n) gates to load an exact representation of a generic data structure into an n-qubit state. This scaling can easily predominate the complexity of a quantum algorithm and, thereby, impair potential quantum advantage. Our work presents a hybrid quantum-classical algorithm for efficient, approximate quantum state loading. More precisely, we use quantum Generative Adversarial Networks (qGANs) to facilitate efficient learning and loading of generic probability distributions - implicitly given by data samples - into quantum states. Through the interplay of a quantum channel, such as a variational quantum circuit, and a classical neural network, the qGAN can learn a representation of the probability distribution underlying the data samples and load it into a quantum state. The loading requires O(poly(n)) gates and can thus enable the use of potentially advantageous quantum algorithms, such as Quantum Amplitude Estimation. We implement the qGAN distribution learning and loading method with Qiskit and test it using a quantum simulation as well as actual quantum processors provided by the IBM Q Experience. Furthermore, we employ quantum simulation to demonstrate the use of the trained quantum channel in a quantum finance application.},
	number = {1},
	journal = {npj Quantum Information},
	author = {Zoufal, Christa and Lucchi, Aurélien and Woerner, Stefan},
	year = {2019},
	note = {arXiv: 1904.00043
Publisher: Springer US},
}

@article{lu_quantum_2019,
	title = {Quantum {Adversarial} {Machine} {Learning}},
	volume = {2},
	issn = {23318422},
	url = {https://doi.org/10.1103/PhysRevResearch.2.033212},
	doi = {10.1103/physrevresearch.2.033212},
	abstract = {Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and has attracted tremendous attention across different communities recently. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as, ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals a novel perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.},
	number = {3},
	journal = {arXiv},
	author = {Lu, Sirui and Duan, Lu Ming and Deng, Dong Ling},
	year = {2019},
	note = {arXiv: 2001.00030
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevResearch.2.033212 url:https://d},
	pages = {33212},
}

@article{anand_noise_2021,
	title = {Noise {Robustness} and {Experimental} {Demonstration} of a {Quantum} {Generative} {Adversarial} {Network} for {Continuous} {Distributions}},
	volume = {4},
	issn = {25119044},
	doi = {10.1002/qute.202000069},
	abstract = {The potential advantage of machine learning in quantum computers is a topic of intense discussion in the literature. Theoretical, numerical, and experimental explorations will most likely be required to understand its power. There have been different algorithms proposed to exploit the probabilistic nature of variational quantum circuits for generative modeling. In this paper, a hybrid architecture for quantum generative adversarial networks (QGANs) is employed and their robustness in the presence of noise is studied. A simple way of adding different types of noise to the quantum generator circuit is devised, and the noisy hybrid QGANs (HQGANs) are simulated numerically to learn continuous probability distributions, and to show that the performance of HQGANs remains unaffected. The effect of different parameters on the training time is also investigated to reduce the computational scaling of the algorithm and simplify its deployment on a quantum computer. The training on Rigetti's Aspen-4-2Q-A quantum processing unit is then performed, and the results from the training are presented. The authors' results pave the way for experimental exploration of different quantum machine learning algorithms on noisy intermediate-scale quantum devices.},
	number = {5},
	journal = {Advanced Quantum Technologies},
	author = {Anand, Abhinav and Romero, Jonathan and Degroote, Matthias and Aspuru-Guzik, Alán},
	year = {2021},
	note = {arXiv: 2006.01976},
	keywords = {quantum computing, quantum machine learning, variational quantum algorithms},
	pages = {1--11},
}

@article{wall_generative_2021,
	title = {Generative machine learning with tensor networks: {Benchmarks} on near-term quantum computers},
	volume = {3},
	issn = {0031-899X},
	url = {https://doi.org/10.1103/PhysRevResearch.3.023010},
	doi = {10.1103/physrevresearch.3.023010},
	abstract = {Noisy, intermediate-scale quantum (NISQ) computing devices have become an industrial reality in the last few years, and cloud-based interfaces to these devices are enabling exploration of near-term quantum computing on a range of problems. As NISQ devices are too noisy for many of the algorithms with a known quantum advantage, discovering impactful applications for near-term devices is the subject of intense research interest. We explore quantum-assisted machine learning (QAML) on NISQ devices through the perspective of tensor networks (TNs), which offer a robust platform for designing resource-efficient and expressive machine learning models to be dispatched on quantum devices. In particular, we lay out a framework for designing and optimizing TN-based QAML models using classical techniques, and then compiling these models to be run on quantum hardware, with demonstrations for generative matrix product state (MPS) models. We put forth a generalized canonical form for MPS models that aids in compilation to quantum devices, and demonstrate greedy heuristics for compiling with a given topology and gate set that outperforms known generic methods in terms of the number of entangling gates, e.g., CNOTs, in some cases by an order of magnitude. We present an exactly solvable benchmark problem for assessing the performance of MPS QAML models, and also present an application for the canonical MNIST handwritten digit dataset. The impacts of hardware topology and day-to-day experimental noise fluctuations on model performance are explored by analyzing both raw experimental counts and statistical divergences of inferred distributions. We also present parametric studies of depolarization and readout noise impacts on model performance using hardware simulators.},
	number = {2},
	journal = {Physical Review Research},
	author = {Wall, Michael L. and Abernathy, Matthew R. and Quiroz, Gregory},
	year = {2021},
	note = {arXiv: 2010.03641
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevResearch.3.023010 url:https://d},
	pages = {23010},
}

@article{kaur_generative_2021,
	title = {Generative {Adversarial} {Networks} with {Quantum} {Optimization} {Model} for {Mobile} {Edge} {Computing} in {IoT} {Big} {Data}},
	issn = {0929-6212},
	url = {https://doi.org/10.1007/s11277-021-08706-7},
	doi = {10.1007/s11277-021-08706-7},
	number = {Cc},
	journal = {Wireless Personal Communications},
	author = {Kaur, Inderjeet and Lydia, E. Laxmi and Nassa, Vinay Kumar and Shrestha, Bhanu and Nebhen, Jamel and Malebary, Sharaf and Joshi, Gyanendra Prasad},
	year = {2021},
	note = {Publisher: Springer US
ISBN: 0123456789},
	keywords = {Big data,IoT,Mobile edge computing,Deep learning,F, big data, deep learning, feature selection, iot, mobile edge computing, ★},
}

@article{huang_experimental_2020,
	title = {Experimental {Quantum} {Generative} {Adversarial} {Networks} for {Image} {Generation}},
	volume = {024051},
	issn = {23317019},
	url = {http://arxiv.org/abs/2010.06201%0Ahttp://dx.doi.org/10.1103/PhysRevApplied.16.024051},
	doi = {10.1103/PhysRevApplied.16.024051},
	abstract = {Quantum machine learning is expected to be one of the first practical applications of near-term quantum devices. Pioneer theoretical works suggest that quantum generative adversarial networks (GANs) may exhibit a potential exponential advantage over classical GANs, thus attracting widespread attention. However, it remains elusive whether quantum GANs implemented on near-term quantum devices can actually solve real-world learning tasks. Here, we devise a flexible quantum GAN scheme to narrow this knowledge gap. In principle, this scheme has the ability to complete image generation with high-dimensional features and could harness quantum superposition to train multiple examples in parallel. We experimentally achieve the learning and generating of real-world handwritten digit images on a superconducting quantum processor. Moreover, we utilize a gray-scale bar dataset to exhibit competitive performance between quantum GANs and the classical GANs based on multilayer perceptron and convolutional neural network architectures, respectively, benchmarked by the Fréchet distance score. Our work provides guidance for developing advanced quantum generative models on near-term quantum devices and opens up an avenue for exploring quantum advantages in various GAN-related learning tasks.},
	author = {Huang, He-Liang and Du, Yuxuan and Gong, Ming and Zhao, Youwei and Wu, Yulin and Wang, Chaoyue and Li, Shaowei and Liang, Futian and Lin, Jin and Xu, Yu and Yang, Rui and Liu, Tongliang and Hsieh, Min-Hsiu and Deng, Hui and Rong, Hao and Peng, Cheng-Zhi and Lu, Chao-Yang and Chen, Yu-Ao and Tao, Dacheng and Zhu, Xiaobo and Pan, Jian-Wei},
	year = {2020},
	note = {arXiv: 2010.06201},
	pages = {1--20},
}

@article{niu_entangling_2021,
	title = {Entangling {Quantum} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2105.00080},
	abstract = {Generative adversarial networks (GANs) are one of the most widely adopted semisupervised and unsupervised machine learning methods for high-definition image, video, and audio generation. In this work, we propose a new type of architecture for quantum generative adversarial networks (entangling quantum GAN, EQ-GAN) that overcomes some limitations of previously proposed quantum GANs. Leveraging the entangling power of quantum circuits, EQ-GAN guarantees the convergence to a Nash equilibrium under minimax optimization of the discriminator and generator circuits by performing entangling operations between both the generator output and true quantum data. We show that EQ-GAN has additional robustness against coherent errors and demonstrate the effectiveness of EQ-GAN experimentally in a Google Sycamore superconducting quantum processor. By adversarially learning efficient representations of quantum states, we prepare an approximate quantum random access memory (QRAM) and demonstrate its use in applications including the training of quantum neural networks.},
	author = {Niu, Murphy Yuezhen and Zlokapa, Alexander and Broughton, Michael and Boixo, Sergio and Mohseni, Masoud and Smelyanskyi, Vadim and Neven, Hartmut},
	year = {2021},
	note = {arXiv: 2105.00080},
	pages = {1--10},
}

@article{liu_hybrid_2021,
	title = {A hybrid quantum-classical conditional generative adversarial network algorithm for human-centered paradigm in cloud},
	issn = {1687-1499},
	url = {https://doi.org/10.1186/s13638-021-01898-3},
	doi = {10.1186/s13638-021-01898-3},
	journal = {EURASIP Journal on Wireless Communications and Networking},
	author = {Liu, Wenjie and Zhang, Ying and Deng, Zhiliang and Zhao, Jiaojiao and Tong, Lian},
	year = {2021},
	note = {Publisher: Springer International Publishing},
	keywords = {Cloud computing, Conditional generative adversarial network, Human-centered computing, Parameterized quantum circuits, Quantum generative adversarial network, adversarial network, cloud computing, conditional generative, human-centered computing, parameterized, quantum generative adversarial network, ★},
}

@article{xia_quantumfed_nodate,
	title = {{QuantumFed} : {A} {Federated} {Learning} {Framework} for {Collaborative} {Quantum} {Training}},
	author = {Xia, Qi},
	note = {arXiv: 2106.09109v2},
}

@article{chehimi_quantum_2021,
	title = {Quantum {Federated} {Learning} with {Quantum} {Data}},
	url = {http://arxiv.org/abs/2106.00005},
	abstract = {Quantum machine learning (QML) has emerged as a promising field that leans on the developments in quantum computing to explore large complex machine learning problems. Recently, some purely quantum machine learning models were proposed such as the quantum convolutional neural networks (QCNN) to perform classification on quantum data. However, all of the existing QML models rely on centralized solutions that cannot scale well for large-scale and distributed quantum networks. Hence, it is apropos to consider more practical quantum federated learning (QFL) solutions tailored towards emerging quantum network architectures. Indeed, developing QFL frameworks for quantum networks is critical given the fragile nature of computing qubits and the difficulty of transferring them. On top of its practical momentousness, QFL allows for distributed quantum learning by leveraging existing wireless communication infrastructure. This paper proposes the first fully quantum federated learning framework that can operate over quantum data and, thus, share the learning of quantum circuit parameters in a decentralized manner. First, given the lack of existing quantum federated datasets in the literature, the proposed framework begins by generating the first quantum federated dataset, with a hierarchical data format, for distributed quantum networks. Then, clients sharing QCNN models are fed with the quantum data to perform a classification task. Subsequently, the server aggregates the learnable quantum circuit parameters from clients and performs federated averaging. Extensive experiments are conducted to evaluate and validate the effectiveness of the proposed QFL solution. This work is the first to combine Google's TensorFlow Federated and TensorFlow Quantum in a practical implementation.},
	author = {Chehimi, Mahdi and Saad, Walid},
	year = {2021},
	note = {arXiv: 2106.00005},
	keywords = {federated learning, qcnn, qml, ★},
	pages = {1--13},
}

@article{larose_quantum_2020,
	title = {Quantum {Federated} {Learning} for {Wireless} {Communications}},
	volume = {102},
	issn = {2469-9926},
	doi = {10.1103/PhysRevA.102.032420},
	abstract = {Data representation is crucial for the success of machine learning models. In the context of quantum machine learning with near-term quantum computers, equally important considerations of how to efficiently input (encode) data and effectively deal with noise arise. In this work, we study data encodings for binary quantum classification and investigate their properties both with and without noise. For the common classifier we consider, we show that encodings determine the classes of learnable decision boundaries as well as the set of points which retain the same classification in the presence of noise. After defining the notion of a robust data encoding, we prove several results on robustness for different channels, discuss the existence of robust encodings, and prove an upper bound on the number of robust points in terms of fidelities between noisy and noiseless states. Numerical results for several example implementations are provided to reinforce our findings.},
	number = {3},
	journal = {Physical Review A},
	author = {LaRose, Ryan and Coyle, Brian},
	month = sep,
	year = {2020},
	note = {arXiv: 2003.01695
ISBN: 5384153849},
	pages = {032420},
}

@article{chen_federated_2021,
	title = {Federated quantum machine learning},
	volume = {23},
	issn = {10994300},
	doi = {10.3390/e23040460},
	abstract = {Distributed training across several quantum computers could significantly improve the training time and if we could share the learned model, not the data, it could potentially improve the data privacy as the training would happen where the data is located. One of the potential schemes to achieve this property is the federated learning (FL), which consists of several clients or local nodes learning on their own data and a central node to aggregate the models collected from those local nodes. However, to the best of our knowledge, no work has been done in quantum machine learning (QML) in federation setting yet. In this work, we present the federated training on hybrid quantum-classical machine learning models although our framework could be generalized to pure quantum machine learning model. Specifically, we consider the quantum neural network (QNN) coupled with classical pre-trained convolutional model. Our distributed federated learning scheme demonstrated almost the same level of trained model accuracies and yet significantly faster distributed training. It demonstrates a promising future research direction for scaling and privacy aspects.},
	number = {4},
	journal = {Entropy},
	author = {Chen, Samuel Yen Chi and Yoo, Shinjae},
	year = {2021},
	note = {arXiv: 2103.12010},
	keywords = {Federated learning, Privacy-preserving AI, Quantum machine learning, Quantum neural networks, Variational quantum circuits, federated learning, qfl, ★},
}

@article{sheng_distributed_2017,
	title = {Distributed secure quantum machine learning},
	volume = {62},
	issn = {20959281},
	url = {http://dx.doi.org/10.1016/j.scib.2017.06.007},
	doi = {10.1016/j.scib.2017.06.007},
	abstract = {Distributed secure quantum machine learning (DSQML) enables a classical client with little quantum technology to delegate a remote quantum machine learning to the quantum server with the privacy data preserved. Moreover, DSQML can be extended to a more general case that the client does not have enough data, and resorts both the remote quantum server and remote databases to perform the secure machine learning. Here we propose a DSQML protocol that the client can classify two-dimensional vectors to different clusters, resorting to a remote small-scale photon quantum computation processor. The protocol is secure without leaking any relevant information to the Eve. Any eavesdropper who attempts to intercept and disturb the learning process can be noticed. In principle, this protocol can be used to classify high dimensional vectors and may provide a new viewpoint and application for future “big data”.},
	number = {14},
	journal = {Science Bulletin},
	author = {Sheng, Yu Bo and Zhou, Lan},
	year = {2017},
	note = {Publisher: Science China Press},
	keywords = {Big data, Quantum communication, Quantum computation, Quantum machine learning, ★},
	pages = {1025--1029},
}

@article{perrier_qdataset_2021,
	title = {{QDataset}: {Quantum} {Datasets} for {Machine} {Learning}},
	url = {http://arxiv.org/abs/2108.06661},
	abstract = {The availability of large-scale datasets on which to train, benchmark and test algorithms has been central to the rapid development of machine learning as a discipline and its maturity as a research discipline. Despite considerable advancements in recent years, the field of quantum machine learning (QML) has thus far lacked a set of comprehensive large-scale datasets upon which to benchmark the development of algorithms for use in applied and theoretical quantum settings. In this paper, we introduce such a dataset, the QDataSet, a quantum dataset designed specifically to facilitate the training and development of QML algorithms. The QDataSet comprises 52 high-quality publicly available datasets derived from simulations of one- and two-qubit systems evolving in the presence and/or absence of noise. The datasets are structured to provide a wealth of information to enable machine learning practitioners to use the QDataSet to solve problems in applied quantum computation, such as quantum control, quantum spectroscopy and tomography. Accompanying the datasets on the associated GitHub repository are a set of workbooks demonstrating the use of the QDataSet in a range of optimisation contexts.},
	author = {Perrier, Elija and Youssry, Akram and Ferrie, Chris},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.06661},
}

@article{kouda_qubit_2005,
	title = {Qubit neural network and its learning efficiency},
	volume = {14},
	issn = {09410643},
	doi = {10.1007/s00521-004-0446-8},
	abstract = {Neural networks have attracted much interest in the last two decades for their potential to realistically describe brain functions, but so far they have failed to provide models that can be simulated in a reasonable time on computers; rather they have been limited to toy models. Quantum computing is a possible candidate for improving the computational efficiency of neural networks. In this framework of quantum computing, the Qubit neuron model, proposed by Matsui and Nishimura, has shown a high efficiency in solving problems such as data compression. Simulations have shown that the Qubit model solves learning problems with significantly improved efficiency as compared to the classical model. In this paper, we confirm our previous results in further detail and investigate what contributes to the efficiency of our model through 4-bit and 6-bit parity check problems, which are known as basic benchmark tests. Our simulations suggest that the improved performance is due to the use of superposition of neural states and the use of probability interpretation in the observation of the output states of the model. © Springer-Verlag London Limited 2005.},
	number = {2},
	journal = {Neural Computing and Applications},
	author = {Kouda, Noriaki and Matsui, Nobuyuki and Nishimura, Haruhiko and Peper, Ferdinand},
	year = {2005},
	keywords = {Back-propagation, Learning, Neural network, Parity check, Quantum computing, Qubit},
	pages = {114--121},
}

@article{bernstein_quantum_1997,
	title = {Quantum complexity theory},
	volume = {26},
	issn = {00975397},
	doi = {10.1137/S0097539796300921},
	abstract = {In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch's model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97-117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension. We also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that O(log T) bits of precision suffice to support a T step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. We give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church-Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class BPP. The class BQP of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies BPP ⊆ BQP ⊆ P\#P. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory.},
	number = {5},
	journal = {SIAM Journal on Computing},
	author = {Bernstein, Ethan and Vazirani, Umesh},
	year = {1997},
	keywords = {Fourier sampling, Quantum Turing machines, Quantum computation, Quantum polynomial time, Reversibility, Universal quantum Turing machine},
	pages = {1411--1473},
}

@article{j_quantum_2018,
	title = {Quantum {Algorithm} {Implementations} for {Beginners}},
	issn = {2331-8422},
	url = {http://arxiv.org/abs/1804.03719},
	abstract = {As quantum computers become available to the general public, the need has arisen to train a cohort of quantum programmers, many of whom have been developing classical computer programs for most of their careers. While currently available quantum computers have less than 100 qubits, quantum computing hardware is widely expected to grow in terms of qubit count, quality, and connectivity. This review aims to explain the principles of quantum programming, which are quite different from classical programming, with straightforward algebra that makes understanding of the underlying fascinating quantum mechanical principles optional. We give an introduction to quantum computing algorithms and their implementation on real quantum hardware. We survey 20 different quantum algorithms, attempting to describe each in a succinct and self-contained fashion. We show how these algorithms can be implemented on IBM's quantum computer, and in each case, we discuss the results of the implementation with respect to differences between the simulator and the actual hardware runs. This article introduces computer scientists, physicists, and engineers to quantum algorithms and provides a blueprint for their implementations.},
	author = {J., Abhijith and Adedoyin, Adetokunbo and Ambrosiano, John and Anisimov, Petr and Bärtschi, Andreas and Casper, William and Chennupati, Gopinath and Coffrin, Carleton and Djidjev, Hristo and Gunter, David and Karra, Satish and Lemons, Nathan and Lin, Shizeng and Malyzhenkov, Alexander and Mascarenas, David and Mniszewski, Susan and Nadiga, Balu and O'Malley, Daniel and Oyen, Diane and Pakin, Scott and Prasad, Lakshman and Roberts, Randy and Romero, Phillip and Santhi, Nandakishore and Sinitsyn, Nikolai and Swart, Pieter J. and Wendelberger, James G. and Yoon, Boram and Zamora, Richard and Zhu, Wei and Eidenbenz, Stephan and Coles, Patrick J. and Vuffray, Marc and Lokhov, Andrey Y.},
	year = {2018},
	note = {arXiv: 1804.03719},
}

@article{gisin_quantum_2007,
	title = {Quantum communication},
	volume = {1},
	journal = {Nature Photonics},
	author = {Gisin, Nicolas and Thew, Rob},
	year = {2007},
	pages = {165--171},
}

@article{gyongyosi_resource_2020,
	title = {Resource prioritization and balancing for the quantum internet},
	volume = {10},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-020-78960-5},
	doi = {10.1038/s41598-020-78960-5},
	abstract = {{\textless}p{\textgreater}The quantum Internet enables networking based on the fundamentals of quantum mechanics. Here, methods and procedures of resource prioritization and resource balancing are defined for the quantum Internet. We define a model for resource consumption optimization in quantum repeaters, and a strongly-entangled network structure for resource balancing. We study the resource-balancing efficiency of the strongly-entangled structure. We prove that a strongly-entangled quantum network is two times more efficient in a resource balancing problem than a full-mesh network of the traditional Internet.{\textless}/p{\textgreater}},
	number = {1},
	journal = {Scientific Reports},
	author = {Gyongyosi, Laszlo and Imre, Sandor},
	month = dec,
	year = {2020},
	note = {Publisher: Nature Publishing Group UK},
	keywords = {mathematical},
	pages = {22390},
}

@article{rabbie_designing_2020,
	title = {Designing {Quantum} {Networks} {Using} {Preexisting} {Infrastructure}},
	issn = {2331-8422},
	url = {http://arxiv.org/abs/2005.14715},
	abstract = {We consider the problem of deploying a quantum network on an existing fiber infrastructure, where quantum repeaters and end nodes can only be housed at specific locations. We propose a method based on integer linear programming (ILP) to place the minimal number of repeaters on such an existing network topology, such that requirements on end-to-end entanglement-generation rate and fidelity between any pair of end-nodes are satisfied. While ILPs are generally difficult to solve, we show that our method performs well in practice for networks of up to 100 nodes. We illustrate the behavior of our method both on randomly-generated network topologies, as well as on a real-world fiber topology deployed in the Netherlands.},
	author = {Rabbie, Julian and Chakraborty, Kaushik and Avis, Guus and Wehner, Stephanie},
	year = {2020},
	note = {arXiv: 2005.14715},
	pages = {1--30},
}

@article{kozlowski_p4_2020,
	title = {A {P4} {Data} {Plane} for the {Quantum} {Internet}},
	doi = {10.1145/3426744.3431321},
	abstract = {The quantum technology revolution brings with it the promise of a quantum internet. A new - - quantum - - network stack will be needed to account for the fundamentally new properties of quantum entanglement. The first realisations of quantum networks are imminent and research interest in quantum network protocols has started growing. In the non-quantum world, programmable data planes have broken the pattern of ossification of the protocol stack and enabled a new - - software-defined - - network software architecture. Similarly, a programmable quantum data plane could pave the way for a software-defined quantum network architecture. In this paper, we demonstrate how we use P416 to explore abstractions and device architectures for quantum networks.},
	journal = {EuroP4 2020 - Proceedings of the 3rd P4 Workshop in Europe, Part of CoNEXT 2020},
	author = {Kozlowski, Wojciech and Kuipers, Fernando and Wehner, Stephanie},
	year = {2020},
	note = {arXiv: 2010.11263
ISBN: 9781450381819},
	keywords = {P4, programmable networks, quantum communication, quantum data plane, quantum internet, quantum networks},
	pages = {49--51},
}

@article{lipinska_secure_2020,
	title = {Secure multiparty quantum computation with few qubits},
	volume = {102},
	issn = {24699934},
	doi = {10.1103/PhysRevA.102.022405},
	abstract = {We consider the task of secure multiparty distributed quantum computation on a quantum network. We propose a protocol based on quantum error correction which reduces the number of necessary qubits. That is, each of the n nodes in our protocol requires an operational workspace of n2+4n qubits, as opposed to the previously shown ω(n3+n2s2)logn qubits, where s is a security parameter. Additionally, we reduce the communication complexity by a factor of O(n3log(n)) qubits per node compared to existing protocols. To achieve universal computation, we develop a distributed procedure for verifying magic states, which allows us to apply distributed gate teleportation and which may be of independent interest. We showcase our protocol in a small example for a seven-node network.},
	number = {2},
	journal = {Physical Review A},
	author = {Lipinska, Victoria and Ribeiro, Jérémy and Wehner, Stephanie},
	year = {2020},
	note = {arXiv: 2004.10486
Publisher: American Physical Society},
	keywords = {doi:10.1103/PhysRevA.102.022405 url:https://doi.or},
	pages = {1--15},
}

@article{liu_quantum_2019,
	title = {Quantum searchable encryption for cloud data based on full-blind quantum computation},
	volume = {7},
	issn = {21693536},
	doi = {10.1109/ACCESS.2019.2960592},
	abstract = {Searchable encryption (SE) is a positive way to protect users sensitive data in cloud computing setting, while preserving search ability on the server side, i.e., it allows the server to search encrypted data without leaking information about the plaintext data. In this paper, a multi-client universal circuit-based full-blind quantum computation (FBQC) model is proposed. In order to meet the requirements of multi-client accessing or computing encrypted cloud data, all clients with limited quantum ability outsource the key generation to a trusted key center and upload their encrypted data to the data center. Considering the feasibility of physical implementation, all quantum gates in the circuit are replaced with the combination of π 8 rotation operator set Rz(π/4), Ry(π/4), CRz(π/4), CRy(π/4), CCRz(π/4), CCRy(π/4). In addition, the data center is only allowed to perform one π/8\vphantom{\{}\} rotation operator each time, but does not know the structure of the circuit (i.e., quantum computation), so it can guarantee the blindness of computation. Then, through combining this multi-client FBQC model and Grover searching algorithm, we continue to propose a quantum searchable encryption scheme for cloud data. It solves the problem of multi-client access mode under searchable encryption in the cloud environment, and has the ability to resist against some quantum attacks. To better demonstrate our scheme, an example of our scheme to search on encrypted 2-qubit state is given in detail. Furthermore, the security of our scheme is analysed from two aspects: external attacks and internal attacks, and the result indicates that it can resist against such kinds of attacks and also guarantee the blindness of data and computation.},
	journal = {IEEE Access},
	author = {Liu, Wenjie and Xu, Yinsong and Liu, Wen and Wang, Haibin and Lei, Zhibin},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Grover algorithm, Quantum searchable encryption, cloud data, full-blind quantum computation, multi-client access, trusted key center, π/8 rotation operator},
	pages = {186284--186295},
}

@article{sutradhar_efficient_2021,
	title = {An efficient simulation for quantum secure multiparty computation},
	volume = {11},
	issn = {20452322},
	doi = {10.1038/s41598-021-81799-z},
	abstract = {The quantum secure multiparty computation is one of the important properties of secure quantum communication. In this paper, we propose a quantum secure multiparty summation (QSMS) protocol based on (t, n) threshold approach, which can be used in many complex quantum operations. To make this protocol secure and realistic, we combine both the classical and quantum phenomena. The existing protocols have some security and efficiency issues because they use (n, n) threshold approach, where all the honest players need to perform the quantum multiparty summation protocol. We however use a (t, n) threshold approach, where only t honest players need to compute the quantum summation protocol. Compared to other protocols our proposed protocol is more cost-effective, realistic, and secure. We also simulate it using the IBM corporation’s online quantum computer, or quantum experience.},
	number = {1},
	journal = {Scientific Reports},
	author = {Sutradhar, Kartick and Om, Hari},
	month = dec,
	year = {2021},
	pmid = {33500501},
	note = {Publisher: Nature Research},
}

@techreport{montanaro_past_2015,
	title = {The past, present, and future history of quantum computing},
	author = {Montanaro, Ashley},
	year = {2015},
}

@article{wolf_quantum_nodate,
	title = {Quantum {Computing} : {Lecture} {Notes}},
	author = {Wolf, Ronald De},
	note = {arXiv: 1907.09415v2},
}

@misc{qiskit,
	title = {Qiskit: {An} {Open}-source {Framework} for {Quantum} {Computing}},
	url = {https://github.com/Qiskit},
	author = {Abraham, Hector and {AduOffei} and Agarwal, Rochisha and Agliardi, Gabriele and Aharoni, Merav and Akhalwaya, Ismail Yunus and Aleksandrowicz, Gadi and Alexander, Thomas and Amy, Matthew and Anagolum, Sashwat and Arbel, Eli and Asfaw, Abraham and Athalye, Anish and Avkhadiev, Artur and Azaustre, Carlos and {et al.}},
	year = {2021},
	doi = {10.5281/zenodo.2573505},
}

@article{qcor,
	title = {{QCOR}: {A} {Language} {Extension} {Specification} for the {Heterogeneous} {Quantum}-{Classical} {Model} of {Computation}},
	volume = {16},
	issn = {1550-4832},
	url = {https://dl.acm.org/doi/10.1145/3380964},
	doi = {10.1145/3380964},
	abstract = {{\textless}p{\textgreater} Quantum computing (QC) is an emerging computational paradigm that leverages the laws of quantum mechanics to perform elementary logic operations. Existing programming models for QC were designed with fault-tolerant hardware in mind, envisioning stand-alone applications. However, the susceptibility of near-term quantum computers to noise limits their stand-alone utility. To better leverage limited computational strengths of noisy quantum devices, hybrid algorithms have been suggested whereby quantum computers are used in tandem with their classical counterparts in a heterogeneous fashion. This {\textless}italic{\textgreater}modus operandi{\textless}/italic{\textgreater} calls out for a programming model and a high-level programming language that natively and seamlessly supports heterogeneous quantum-classical hardware architectures in a single-source-code paradigm. Motivated by the lack of such a model, we introduce a language extension specification, called {\textless}italic{\textgreater}QCOR{\textless}/italic{\textgreater} , which enables single-source quantum-classical programming. Programs written using the QCOR library–based language extensions can be compiled to produce functional hybrid binary executables. After defining QCOR’s programming model, memory model, and execution model, we discuss how QCOR enables variational, iterative, and feed-forward QC. QCOR approaches quantum-classical computation in a hardware-agnostic heterogeneous fashion and strives to build on best practices of high-performance computing. The high level of abstraction in the language extension is intended to accelerate the adoption of QC by researchers familiar with classical high-performance computing. {\textless}/p{\textgreater}},
	number = {2},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	author = {Mintz, Tiffany M. and McCaskey, Alexander J. and Dumitrescu, Eugene F. and Moore, Shirley V. and Powers, Sarah and Lougovski, Pavel},
	month = apr,
	year = {2020},
	pages = {1--17},
}

@misc{qsharp,
	title = {Q\# {Language}},
	url = {https://github.com/microsoft/qsharp-language},
	urldate = {2022-03-28},
	author = {{Microsoft}},
	year = {2021},
}

@misc{rigetti_forest_2021,
	title = {Forest {Framework}},
	url = {https://github.com/rigetti/forest-software},
	urldate = {2022-03-28},
	author = {{Rigetti}},
	year = {2021},
}

@misc{cirq,
	title = {Cirq},
	url = {https://zenodo.org/record/5182845},
	urldate = {2022-03-28},
	author = {Developers, Cirq},
	month = aug,
	year = {2021},
	doi = {10.5281/zenedo.5182845},
}

@book{qbook-nielsenchuang,
	address = {Cambridge},
	title = {Quantum {Computation} and {Quantum} {Information}},
	isbn = {978-0-511-97666-7},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511976667},
	abstract = {One of the most cited books in physics of all time, Quantum Computation and Quantum Information remains the best textbook in this exciting field of science. This 10th anniversary edition includes an introduction from the authors setting the work in context. This comprehensive textbook describes such remarkable effects as fast quantum algorithms, quantum teleportation, quantum cryptography and quantum error-correction. Quantum mechanics and computer science are introduced before moving on to describe what a quantum computer is, how it can be used to solve problems faster than 'classical' computers and its real-world implementation. It concludes with an in-depth treatment of quantum information. Containing a wealth of figures and exercises, this well-known textbook is ideal for courses on the subject, and will interest beginning graduate students and researchers in physics, computer science, mathematics, and electrical engineering.},
	urldate = {2022-03-29},
	publisher = {Cambridge University Press},
	author = {Nielsen, Michael A. and Chuang, Isaac L.},
	month = dec,
	year = {2010},
	doi = {10.1017/CBO9780511976667},
}
