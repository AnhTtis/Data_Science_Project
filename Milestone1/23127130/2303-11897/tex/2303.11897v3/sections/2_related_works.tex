\section{Related Work}
\label{sec:related}

We compare \NAME to other image and language generation evaluation metrics.

\noindent\textbf{Prior image generation evaluation}
Prior work usually compares image generation models via pairwise comparison by humans. How to design automatic evaluation metrics to approximate human assessment of the quality of machine-generated images has always been a major challenge in computer vision. There are two aspects to evaluate, namely image quality and image-text alignment.~\textbf{Inception Score}~\cite{Salimans2016ImprovedTF} and \textbf{FID}~\cite{Heusel2017GANsTB} are the most widely adopted metrics for image quality. They compare the features of the generated images and gold images extracted from a pre-trained Inception-V3 model~\cite{Szegedy2015RethinkingTI} to evaluate the fidelity and diversity of generated images. However, they rely on ground-truth images and are based on a classification model, which makes them not suitable for complex datasets~\cite{Frolov2021AdversarialTS}. For image-text alignment, prior metrics are mainly based on CLIP~\cite{radford2021learning}, captioning, and object detection models. \textbf{CLIPScore}~\cite{Hessel2021CLIPScoreAR} and \textbf{CLIP-R}~\cite{park2021benchmark} are based on the cosine similarity of image and text CLIP~\cite{radford2021learning} embeddings. \cite{Cho2022DALLEvalPT, Hinz2019SemanticOA, Hong2018InferringSL} first convert the images using a captioning model, and then compare the image caption with the text using metrics like CIDEr~\cite{vedantam2015cider} and SPICE~\cite{anderson2016spice}. \textbf{SOA}~\cite{Hinz2019SemanticOA} and \textbf{DALL-Eval}~\cite{Cho2022DALLEvalPT} employ object detection models to determine if objects, attributes, and relations in the text input are in the generated image. However, this approach only works on synthesized text inputs and measures faithfulness on limited axes (object, counting, color, spatial relation), missing elements like material, shape, activities, and context. In contrast, thanks to the flexibility of questions, \NAME works on any text inputs and evaluates faithfulness across a broad spectrum of dimensions.

\noindent\textbf{Summarization evaluation in NLP}
\NAME is inspired by the summarization evaluation methods based on question answering (QA) \cite{Wang2020AskingAA, singh2019towards}. Given a summary, a language model generates a set of questions about the text. A QA model checks if the same answer can be inferred from the text and the summary. These QA-based metrics have much higher correlations with human judgments on the factual consistency of summarization than other automatic metrics \cite{Wang2020AskingAA, singh2019towards}.
\NAME can be seen as treating the text input for the text-to-image model as a summary of the generated image.
%In our work, we are treating the text input for the text-to-image model as a summary of the generated image.

% \nascomment{I am not sure what this means; there are factuality evaluations, but I don't know what ``factual faithfulness'' is}