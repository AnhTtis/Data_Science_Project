\begin{appendices}

\section{Qualitative Examples}
\label{appendix:example}

Here we show three examples on how \NAME is computed. For each text input, we show the questions and answers generated by GPT-3~\cite{brown2020language} and filtered by UnifiedQA~\cite{Khashabi2020UnifiedQACF}. 
We also show the VQA model's answer to each vision question given each generated image.
The VQA model used is mPLUG~\cite{Li2022mPLUGEA}. The first text input comes from COCO~\cite{lin2014microsoft} and the second and third text inputs come from DrawBench~\cite{Saharia2022PhotorealisticTD}.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.37\textwidth]{figures/demo11.pdf}
  \label{fig:separate_error}
\end{figure}


\begin{figure}[ht]
\centering
  \includegraphics[width=0.37\textwidth]{figures/demo2.pdf}
  \label{fig:separate_error}
\end{figure}


\begin{figure}[ht]
\centering
  \includegraphics[width=0.37\textwidth]{figures/demo3.pdf}
  \label{fig:separate_error}
\end{figure}


\pagebreak



\section{Detailed Results}
\label{appendix:results}
\input{tables/detailed_scores}

Table~\ref{tab:detailed_scores} shows the detailed evaluation results of the text-to-image models we use on the \NAME v1.0 benchmark.
We show the VQA accuracy of each question category, \NAME score on each text source, and the overall \NAME score. 
We can see that Stable Diffusion v2.1~\cite{Rombach2021HighResolutionIS} gets the highest overall score and also scores the highest in most categories. Nonetheless, the CLIP~\cite{radford2021learning} and VQGAN~\cite{Esser2020TamingTF} based minDALL-E~\cite{kakaobrain2021minDALL-E} gets the highest accuracy on ``shape", ``other", ``food", and VQ-Diffusion~\cite{Gu2021VectorQD} gets the highest accuracy on ``color".



\section{Annotation Details}
\label{appendix:annotation}

\subsection{Likert Scale on Text-to-Image Faithfulness}
\paragraph{Guidelines}
The annotation guideline is as follows:
\begin{itemize}
    \item On a scale of 1-5, score "does the image match the prompt?".
    \item The ranking of each image given the same text input is important. If you believe the current scoring criteria cannot reflect your ranking preference, pick scores that are consistent with your ranking. Ties are allowed.
    \item To evaluate the generated image, there are two aspects: image quality and text-image match. Here we only care about text-image match, which is referred to as “faithfulness”.
    \item There are several kinds of elements in the text: object, attribute, relation, and context. Measure the consistency by counting how many elements are missed/misrepresented in the generated image.
    \item For some elements, e.g. ``train conductor's hat", if you can see there is a hat but not a train conductor's hat, consider half of the element is missed/misrepresented in the generated image.
    \item Objects are the most important elements. If an object is missing, then consider all related attributes, activity, and attributes missing.
    \item When you cannot tell what the object/attribute/activity/context is, consider the element missing. (e.g., can't tell if an object is a microwave)
\end{itemize}

Given the above guideline, suppose the text input contains $n$ elements, and $x$ elements are missed or misrepresented. 
$n$ and $x$ are all counted by the annotators.
The reference scoring guideline is as follows:
\begin{itemize}
    \item 5: The image perfectly matches the prompt.
    \item 4:  $x \le 2$ and $x \le n/3$. A few elements are missed/misrepresented.
    \item 3: $\min \{2, n/3\} < x \le n/2$ elements are missed/misrepresented.
    \item 2: $x > n/2$. More than half of the elements are missed/misrepresented.
    \item 1: None of the major objects are correctly presented in the image.
\end{itemize}

\paragraph{Details}
We collect 1600 annotations on 800 generated images from 160 text inputs. Each image is scored by 2 annotators, and we collect the scores from 20 graduate students. We average the scores as the final faithfulness score of the image. The inter-annotator agreement measured by Krippendorf's $\alpha$ is 0.67, indicating ``substantial" agreement.
The images are generated by the five most recent text-to-image models in our study, including VQ-Diffusion~\cite{Gu2021VectorQD}, minDALL-E~\cite{kakaobrain2021minDALL-E}, and Stable Diffusion~\cite{Saharia2022PhotorealisticTD} v1.1, v1.5, and v2.1. 
For each text input, we present the five images together, making it easier for the annotators to give faithfulness scores that reflect their ranking preference.
We will release the annotation scores on publication.

\subsection{Human VQA}

\paragraph{Guidelines}
Given an image, a question, and a set of choices, choose the correct choice according to the image content. There are two types of questions. One has two options: "(A) yes (B) no". Another type of question has four choices. We also add the fifth option ``None of the above". If you believe none of the four choices is correct, choose the fifth one. 
Some images are of low quality. Just select the choice according to your instinct.
For ambiguous cases, for example, the question is ``is there a man?", and the image contains a human but it is unclear whether the human is a man, answer ``no".

\paragraph{Details}
We collect annotations of 1029 questions on 126 generated images. The images are from images used in the Likert Scale annotation. Each question is answered by two annotators, and we have the same 20 graduate students as the annotators. The inter-annotator agreement measured by Krippendorf's $\alpha$ is 0.88. A third annotator is involved if the two annotators disagree. And the final answer is given by the majority vote. We will release the annotated VQA answers.

\section{Common Q \& A}
\label{appendix:qa}

\paragraph{Any possible extension to \NAME?}
As discussed in \S\ref{sec:intro}, one extension of our work will be customized versions of the \NAME benchmark focusing on one aspect of image generation. For example, we can make a \NAME benchmark that only contains questions about ``counting"; Or a benchmark consists of text inputs synthesized to test text-to-image models' ability in composing multiple objects. Another possible extension is to use \NAME on other generation tasks, e.g., text-to-3D and text-to-video.

\paragraph{The OpenAI APIs are too expensive. Can we generate questions by local models?}
Yes. Our approach works on any language model. Please refer to \S\ref{sec:question_generation_llama} on question generation with our fine-tuned LLaMA 2 checkpoint in.  Also, we would like to emphasize that all questions in \NAME v1.0 benchmark are pre-generated by GPT-3, and there is no need to re-generate those questions for evaluation.




\begin{figure}[h]
\centering
  \includegraphics[width=0.4\textwidth]{figures/tifa_v1_len.jpg}
  \caption{
Distribution of the lengths of \NAME v1.0 text inputs.
}
  \vspace{-4mm}
  \label{fig:tifa_len}
  
\end{figure}


\paragraph{More details on \NAME v1.0 text inputs?}
The distribution of the number of words in the text inputs is shown in ~\ref{fig:tifa_len}. Most text inputs have around 10 words. We also conduct bias analysis on \NAME v1.0 text inputs. Among the 4K text inputs, regarding gender expression, 400 are perceived as ``male" and 239 are perceived as ``female". We find that the bias in gender distribution comes from captions sampled from COCO dataset~\cite{lin2014microsoft}.


\section{Text-to-Image Model Details}
\label{appendix:t2i}


\paragraph{AttnGAN} 
AttnGan~\cite{Xu2017AttnGANFT} is a text-to-image model that is introduced in 2017. It is based on an attention mechanism that allows the model to focus on different parts of the input text when generating an image. AttnGAN has been shown to generate high-quality images for a variety of datasets and has been widely used in a number of applications. However, the attention mechanism can be computationally expensive, and the model can be difficult to train. 

\paragraph{X-LXMERT} 
X-LXMERT~\cite{Cho2020XLXMERTPC} is an enhanced version of LXMERT~\cite{Tan2019LXMERTLC}. It is introduced in 2020 and incorporates several training refinements. These refinements involve discretizing visual representations, utilizing uniform masking with a wide range of masking ratios, and aligning appropriate pre-training datasets to respective objectives.

\paragraph{minDALL-E}
MinDALL-E~\cite{kakaobrain2021minDALL-E} is a fast, minimal port of Boris Dayma's DALL·E Mini (with mega weights).
DALL·E Mini is an attempt to reproduce OpenAI's DALL-E~\cite{Ramesh2021ZeroShotTG} with a smaller architecture. DALL-E can generate high-quality new images from any text prompt. The checkpoint we use is DALL·E Mega, the latest version of DALL·E Mini.

\paragraph{VQ-Diffusion}
VQ-Diffusion~\cite{Gu2021VectorQD} is a generative model that combines vector quantization (VQ) and diffusion-based models for image synthesis. VQ-Diffusion builds upon the framework of diffusion-based generative models, which involves simulating a stochastic process that gradually transforms a simple noise distribution into the target data distribution. In VQ-Diffusion, the image data is first quantized into discrete codes using a VQ algorithm, which maps each image patch to the nearest code in a codebook. This allows the model to represent complex data distributions with a compact set of discrete codes, rather than continuous probability densities.

\paragraph{Stable Diffusion}
Stable Diffusion is a pre-trained diffusion model for text-to-image generation. It is based on Latent Diffusion model (LDM)~\cite{Rombach2021HighResolutionIS}. LDM is designed to learn the underlying structure of a dataset by mapping it to a lower-dimensional latent space. This latent space represents the data in which the relationships between different data points are more easily understood and analyzed, and reduces the amount of computational resources needed for training diffusion models. Specifically, we use three versions of Stable Diffusion, v1.1, v1.5, and v2.1. Each version is trained with a different number of steps and amount of data.


\section{Prompt}
\label{appendix:prompt}

For demonstration purposes, we show part of the prompt for question generation with GPT-3 in-context learning. The whole prompt will be released with our codes. The prompt contains instructions and several in-context examples. The examples cover all element categories.

\begin{tiny}
\begin{lstlisting}
Given an image description, generate multiple-choice questions that verify if the image description is correct.

First extract elements from the image description. Then classify each element into a category (object, human, animal, food, activity, attribute, counting, color, material, spatial, location, shape, other). Finally, generate questions for each element.

Description: A man posing for a selfie in a jacket and bow tie.
Entities: man, selfie, jacket, bow tie
Activities: posing
Colors:
Counting:
Other attributes:
Questions and answers are below:
About man (human):
Q: is this a man?
Choices: yes, no
A: yes
Q: who is posing for a selfie?
Choices: man, woman, boy, girl
A: man
About selfie (activity):
Q: is the man taking a selfie?
Choices: yes, no
A: yes
Q: what type of photo is the person taking?
Choices: selfie, landscape, sports, portrait
A: selfie
About jacket (object):
Q: is the man wearing a jacket?
Choices: yes, no
A: yes
Q: what is the man wearing?
Choices:jacket, t-shirt, tuxedo, sweater
A: jacket
About bow tie (object):
Q: is the man wearing a bow tie?
Choices: yes, no
A: yes
Q: is the man wearing a bow tie or a neck tie?
Choices: bow tie, neck tie, cravat, bolo tie
A: bow tie
About posing (activity):
Q: is the man posing for the selfie?
Choices: yes, no
A: yes
Q: what is the man doing besides taking the selfie?
Choices: posing, waving, nothing, shaking
A: posing

Description: A horse and several cows feed on hay.
Entities: horse, cows, hay
Activities: feed on
Colors:
Counting: several
Other attributes:
Questions and answers are below:
About horse (animal):
Q: is there a horse?
Choices: yes, no
A: yes
About cows (animal):
Q: are there cows?
Choices: yes, no
A: yes
About hay (object):
Q: is there hay?
Choices: yes, no
A: yes
Q: what is the horse and cows feeding on?
Choices: hay, grass, leaves, twigs
A: hay
About feed on (activity):
Q: are the horse and cows feeding on hay?
Choices: yes, no
A: yes
About several (counting):
Q: are there several cows?
Choices: yes, no
A: yes

Description: A red colored dog.
Entities: dog
Activities:
Colors: red
Counting:
Other attributes:
Questions and answers are below:
About dog (animal):
Q: is this a dog?
Choices: yes, no
A: yes
Q: what animal is in the picture?
Choices: dog, cat, bird, fish
A: dog
About red (color):
Q: is the dog red?
Choices: yes, no
A: yes
Q: what color is the dog?
Choices: red, black, white, yellow
A: red

Description: Here are motorcyclists parked outside a Polish gathering spot for women
Entities: motorcyclists, gathering spot, women
Activities: parked
Colors:
Counting:
Other attributes: outside, polish
Questions and answers are below:
About motorcyclists (human):
Q: are there motorcyclists?
Choices: yes, no
A: yes
About gathering spot (location):
Q: is this a gathering spot?
Choices: yes, no
A: yes
About women (human):
Q: are there women?
Choices: yes, no
A: yes
Q: who are in the gathering spot?
Choices: women, men, boys, girls
A: women
About parked (activity):
Q: are the motorcyclists parked?
Choices: yes, no
A: yes
About outside (spatial):
Q: have the motorcyclists parked outside the gathering spot?
Choices: yes, no
A: yes
Q: are the motorcyclists outside or inside of the gathering spot?
Choices: outside, inside, on the roof, in the basement
A: outside
About Polish (other):
Q: is this a Polish gathering spot?
Choices: yes, no
A: yes
Q: is this a Polish or a Chinese gathering spot?
Choices: Polish, American, Chinese, Japanese
A: Polish
\end{lstlisting}
\end{tiny}


\end{appendices}