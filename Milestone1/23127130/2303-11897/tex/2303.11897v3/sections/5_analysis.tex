\section{Discussion}

\noindent\textbf{Why does \NAME work better than CLIPScore?}
Our experimental results and human evaluations in Section~\ref{sec:exp} show that \NAME is more accurate than prior metrics (CLIP~\cite{radford2021learning} and captioning-based approaches) for evaluating text-to-image faithfulness. We hypothesize that the major challenge of these prior metrics is that they summarize the image outputs and text inputs into a single representation (embedding/caption). In contrast, \NAME exploits the power of the language models to decompose the text input into fine-grained probes, which allows VQA to capture more nuanced aspects of the text input and the generated image.

\noindent\textbf{How do current VQA models perform on \NAME?}
One limitation is that \NAME requires VQA models to work reasonably well, which is true given the current models and \NAME v1.0, as shown in Section~\ref{sec:exp}. 
Nevertheless, the assumption might not hold for current models in domains like anime and abstract art.
TIFA is a modularized evaluation framework.
The VQA models used within the framework can be updated as stronger VQA models become available in the future. 
For instance, we plan to incorporate GPT-4 once its image API is made public since it is likely to improve TIFA. Another possible solution is to ensemble multiple image understanding models. For example, one may employ expert models on art concepts. We leave this for future work.

\vspace{1mm}
\noindent\textbf{Other limitations.}
Another limitation of \NAME is its runtime. Answering multiple visual questions is slower than one CLIP inference. In the scenario described in Section~\ref{sec:exp:comparisonVQA}, mPLUG takes 1.6s to evaluate one image (without batching). Also, our question generation pipeline needs one inference on a modern language model for each text input.
The run time is not a critical issue for benchmarking purposes, but may not be computationally feasible for the kind of large-scale data filtering done, for example, in LAION-5B~\cite{schuhmann2022laion}.
Nevertheless, we would like to point out that our evaluation is much faster than the image generation process of diffusion models.
Thus, we believe it is feasible to perform reranking and reinforcement learning with \NAME on diffusion models.

