\begin{table}[h]
\small
\centering
\caption{
Correlations between each evaluation metric and human judgment on text-to-image faithfulness, measured by Spearman's $\rho$ and Kendall's $\tau$. %BLEU and ROUGE have lower correlations with human judgments than METEOR.
}
% \begin{tabular}{P{1.7cm}P{1.2cm}P{1.2cm}P{1.0cm}P{1.2cm}P{1.2cm}P{1.2cm}P{1.1cm}P{1.1cm}P{1.1cm}}
\begin{tabular}{lcc}
\toprule[1.2pt]
 & Spearman's $\rho$ & Kendall's $\tau$ \\
\midrule
\textbf{Caption-Based} \\
BLEU-4 & 18.3 & 18.8\\
ROUGE-L & 32.9 & 24.5\\
METEOR & 34.0 & 27.4\\
SPICE & 32.8 & 23.2\\
\midrule
CLIPScore & 33.2 & 23.1\\
\midrule
\textbf{Ours} \\
% \NAME (PromptCap-UQA) & 38.2 \\
\NAME (VILT) & 49.3 & 38.2\\
\NAME (OFA) & 49.6 & 37.2\\
\NAME (GIT) & 54.5 & 42.6\\
\NAME (BLIP-2) & 55.9 & 43.6\\
\textbf{\NAME (mPLUG)} & \textbf{59.7} & \textbf{47.2}\\
%\midrule
%\midrule
%Inter-Annotator Correlation & 70.3 \\
\bottomrule[1.2pt]
\end{tabular}
\label{tab:human_correlation}
\vspace{-0.1in}
% \bottomrule
\end{table}