\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{figures/tifa_teaser.pdf}
  \caption{
Illustration of how \NAME works, and comparison with the widely-used CLIPScore and SPICE metrics.
Given the text input, \NAME uses GPT-3 to generate several question-answer pairs, and a QA model filters them (3 out of 14 questions for this text input are shown). 
\NAME measures whether VQA models can accurately answer these questions given the generated image.
In this example, \NAME indicates that the image generated by Stable Diffusion v2.1 is better than that by v1.5, while CLIP and SPICE yield the opposite result.
The text input is from MSCOCO validation set.
}
  \vspace{-4mm}
  \label{fig:teaser}
\end{figure}


\begin{figure*}[t]
\centering
  \includegraphics[width=0.95\textwidth]{figures/tifa_overview.pdf}
  \caption{
\textbf{(a) Overview of how \NAME evaluates the faithfulness of a synthesized image.} \NAME uses a language model (LM), a question-answering (QA) model, and a visual-question-answering (VQA) model. Given a text input, we generate several question-answer pairs with the LM and then filter them via the QA model. To evaluate the faithfulness of a synthesized image to the text input, a VQA model answers these visual questions using the image, and we check the answers for correctness.
\textbf{(b) \NAME v1.0 benchmark.} While \NAME is applicable to any text prompt, to allow direct comparison across different studies, and for ease of use, we introduce the \NAME v1.0 benchmark, a repository of text inputs along with pre-generated question-answer pairs. 
% Here ``Q" is the question, ``C" are the choices, and ``A" is the answer inferred from the text.
To evaluate a text-to-image model, a user first produces the images for the text inputs in \NAME v1.0 and then performs VQA with our provided tools on generated images to compute \NAME.
% \jungo{Thank you for your revision. This looks much clearer Small visual things: make the vertial line next to VQA model a straight vertical line (visually when there's a line that looks different from the rest, it stands out. I don't think that's your intention. On the other hand, your "compare" thing is working in this sense. It should stand out). You can highlight TV in the prompt and Q. Also maybe be consistent between TV vs.\ tv?}
% \yushi{have addressed Mari's comments and the following Ranjay's comments.}
% \rk{This needs refinement. The workflow that most people understand is: (1) user comes in with text input. (2) Generative model produces image. (3) model needs to be evaluated. During evaluation, I am not sure if there is a user involved at all. Let's drop the user and make the production of the image clear.}
% \rk{Second, there are two things happening here and we need to tease them apart. First, we need to generate a benchmark. and second, we need to explain how TIFA evaluates models. Since this figure is trying to do both, we need to make that more obvious. Make the top row ``TIFA benchmark generation''. The bottom row ``TIFA evaluation''.}
% \rk{For the benchmark generation step, make it obvious that we produce a repository of text inputs, questions, and answers.}
% \rk{What is also missing in the benchmark generation phase is that you take the prompt and break it up into multiple answers. Each answer produces a question and then is verified by a QA.}
% \rk{What is missing in evaluation is the generative model itself. You need to sample from the repository, generate an image, answer the questiosn and calculate the score. These steps need to be laid out.}
}
  \vspace{-4mm}
  \label{fig:overview}
\end{figure*}


\vspace{-0.2in}
\section{Introduction}
\label{sec:intro}

While we welcome artistic freedom when we commission art from artists, images produced by deep generative models~\cite{Ramesh2021ZeroShotTG, Rombach2021HighResolutionIS, Ramesh2022HierarchicalTI, Saharia2022PhotorealisticTD, Yu2022ScalingAM} should conform closely to our requests. 
% Deep generative models have shown impressive results on generating high-quality images from text prompts.
Despite the advances in generative models, it is still challenging for models to produce images faithful to users' intentions~\cite{petsiuk2022human, Feng2022TrainingFreeSD, Lee2023AligningTM, Liu2022CompositionalVG, Liu2022CharacterAwareMI}. For example, current models often fail to compose multiple objects~\cite{petsiuk2022human, Feng2022TrainingFreeSD, Liu2022CompositionalVG}, bind attributes to the wrong objects~\cite{Feng2022TrainingFreeSD}, and struggle in generating visual text~\cite{Liu2022CharacterAwareMI}.
Today, there are efforts to address these challenges: researchers are imposing linguistic structure with diffusion guidance to produce images with multiple objects~\cite{Feng2022TrainingFreeSD}; others are designing reward models trained using human feedback to better align generations with user intention~\cite{Lee2023AligningTM}.
However, progress is difficult to quantify without accurate and interpretable evaluation measures that explain when and how models struggle.

% A critical bottleneck in this line of work is the lack of reliable automatic evaluation metrics for text-to-image generation faithfulness.
% Common metrics like Inception Score~\cite{Salimans2016ImprovedTF} and FID~\cite{Heusel2017GANsTB} focus on image quality and diversity.
% The most commonly used metrics for image-text alignment is CLIPScore~\cite{Hessel2021CLIPScoreAR}, which is based on the cosine similarity between CLIP~\cite{radford2021learning} embeddings. However, prior works have shown that CLIP is not good at counting objects~\cite{radford2021learning}, measuring distance~\cite{radford2021learning}, or reasoning compositionally~\cite{Ma2022CREPECV}. Such weaknesses make evaluations based on CLIP unreliable. Another family of evaluation metrics is captioning-based, in which an image captioning model first converts the image into text, and then the textual similarity between the image caption and the text prompt is computed. But image captions often miss visual details in the images, and the text-based similarity metrics are problematic \nascomment{too vague, be more clear here} ~\cite{Hessel2021CLIPScoreAR}. The most recent image-text alignment metric is DALL-Eval~\cite{Cho2022DALLEvalPT}, which employs object detection to determine if the objects in the prompt are in the generated images. However, because of the limitation of object detection, the evaluation only works on synthesized text prompts and probing limited aspects (object, counting, color, and spatial relation) in text-to-image generation. \nascomment{not clear to me that these problems don't also apply to our approach!  spell out whatever I'm missing}

A critical bottleneck, therefore, is the lack of reliable automatic evaluation metrics for text-to-image generation faithfulness.
One of the popular metrics is CLIPScore~\cite{Hessel2021CLIPScoreAR}, which measures the cosine similarity between the CLIP embeddings~\cite{radford2021learning} of the text input and the generated image.
However, since CLIP is not effective at counting objects~\cite{radford2021learning}, or reasoning compositionally~\cite{Ma2022CREPECV}, CLIPScore is unreliable and often inaccurate.
Another family of evaluation metrics uses image captions, in which an image captioning model first converts the image into text, and then the image caption is evaluated by comparing it against the text input.
Unfortunately, using captioning models is insufficient since they might decide to ignore salient information in images or focus on other non-essential image regions~\cite{Kasai2021TransparentHE}; for example, a captioning model might say that the images in Figure~\ref{fig:teaser} are ``a field of grass with trees in the background''. Moreover, evaluating text (caption) generation is inherently challenging~\cite{Kasai2021billboard,khashabi-etal-2022-genie}.
Another recent text-to-image evaluation is DALL-Eval~\cite{Cho2022DALLEvalPT}, which employs object detection to determine if the objects in the texts are in the generated images. 
However, this approach only works on synthesized text and measures faithfulness along the limited axes of objects, counting, colors, and spatial relationships but misses activities, geolocation, weather, time, materials, shapes, sizes, and other potential categories we often ask about when we recall images from memory~\cite{Krishna2019InformationMV}.


% To address the above challenges, we introduce \NAME, a new large-scale benchmark to evaluate text-to-image generation faithfulness. Our evaluation approach is illustrated in Figure~\ref{fig:teaser}.
% Given a text prompt, we first generate a set of question-answer pairs via a language model. All answers should be easily inferred given the text prompt. There are two types of questions, binary questions that should be answered ``yes,'' and multiple-choice questions. For example, for the prompt \emph{``a person sitting on a horse in the air over the gate in the grass with people and trees in the background''}, sample question-answer pairs may be \emph{Q: ``what is the animal?'' A: ``horse''} and \emph{Q: ``are
% there people in the background?'' A: ``yes''}. During evaluation, given an image, we use a pre-trained image-to-text model to perform visual question answering (VQA) on these questions. If the image-to-text model is perfect, and the generated image is consistent with the text prompt, then all visual questions should be answered correctly.

To address the above challenges, we introduce \NAME, a new metric to evaluate text-to-image generation faithfulness. Our approach is illustrated in Figure~\ref{fig:overview}. Given a repository of text inputs, we automatically generate question-answer pairs for each text via a language model (here, GPT-3~\cite{brown2020language}). 
A question-answering (QA) system (here, UnifiedQA~\cite{Khashabi2020UnifiedQACF}) is subsequently used to verify and filter these question-answer pairs.
To evaluate a generated image, we use a visual-question-answering (VQA) system (here, mPLUG-large~\cite{Li2022mPLUGEA}) to answer the questions given the generated image.
We measure the image's faithfulness to the text input as the accuracy of the answers generated by the VQA system. 
% Our evaluation metric is based on the assumption that if the VQA is performant, and the generated image is faithful to the text, then all visual questions should be answered correctly.
% Naturally, this assumption is violated today since all VQA models make mistakes. However, regardless of this limitation, we empirically find that \NAME has much higher correlations with human judgments than CLIPScore (Spearman's $\rho =$ 0.50 vs. 0.26) and captioning-based approaches (0.50 vs. 0.14).
While the accuracy of \NAME is dependent on the accuracy of the VQA model, our experiments show that \NAME has much higher correlation with human judgments than CLIPScore (Spearman's $\rho =$ 0.60 vs. 0.33) and captioning-based approaches (Spearman's $\rho =$ 0.60 vs. 0.34).
Additionally, since VQA models will continue to improve, we hypothesize that \NAME will continue to be more reliable over time.
Also, our metrics can automatically detect when elements are missing in the generation: in Figure~\ref{fig:overview}, \NAME detects that the generated image does not contain a TV.


To promote the use of our new evaluation metric, we release \NAME v1.0,  
a large-scale text-to-image generation benchmark containing 4K diverse text inputs, sampled from the MSCOCO captions~\cite{lin2014microsoft}, DrawBench~\cite{Saharia2022PhotorealisticTD}, PartiPrompts~\cite{Yu2022ScalingAM}, and PaintSkill~\cite{Cho2022DALLEvalPT}. 
Each input comes with a pre-generated set of question-answer pairs, resulting in 25K questions covering 4.5K distinct elements.
These questions have been automatically generated and pre-filtered using a question-answering model.
This benchmark also comes with different VQA models~\cite{wang2022git, kim2021vilt, wang2022ofa, Li2023BLIP2BL, Li2022mPLUGEA, Hu2022PromptCapPT} that can be used to evaluate generative models and can be easily extended to use future VQA models when they become available.


We conduct a comprehensive evaluation of current text-to-image models using \NAME v1.0.
Thanks to \NAME's ability to detect fine-grained unfaithfulness in images, we find that current state-of-the-art models are good at rendering common objects, animals, and colors, but still struggle in composing multiple objects, reasoning about spatial relations, and binding the correct activity for each entity.
In addition, our ablation experiments show that \NAME is robust to different VQA models.
% \rk{Add a final sentence about the implications of your results. How will future researchers use \NAME? What should current generative models focus on?} \yushi{added below, need some improvement.}
Future researchers can use \NAME v1.0 to compare their text-to-image models' faithfulness across different studies. 
Also, future generative models may focus on addressing the weaknesses of current models that \NAME discovered.
In addition, with \NAME, users can customize evaluations with their own text inputs and questions~\cite{ethayarajh-jurafsky-2020-utility}; for example, a future \NAME benchmark could focus on counting or scene text.



% Because of the flexibility of natural language, our benchmark is able to probe multiple aspects of the text-to-image models. Our \NAME benchmark contains 4K diverse prompts, 25K questions, and covers 4.5K distinct entities, attributes, etc. \yushi{Here please refer to Figure 3. How should we call these? I am using "elements" or "concepts", but I think there might be a better word} \nascomment{elements seems ok} The questions are classified into 12 categories  \nascomment{right now we make a big thing out of this but I'm not yet sure whether it's worth it; does it reveal interesting differences across systems later on? if not it might not deserve discussion in the intro}  (e.g., object, activity, counting, color, etc.), and the text-to-image models' accuracy on each set of questions reflects model's ability on the corresponding category. We find the current state-of-the-art models are good at rendering common objects, animals, and colors, but still struggle in composing multiple objects, reasoning about spatial relations, and binding the correct activity for each entity. In addition, our metrics can automatically detect fine-grained unfaithfulness between the image and the text. For example, for the left image in Figure~\ref{fig:teaser}, image-to-text models can detect that the image misses the ``gate,'' and the horse is not ``in air.''

% One major concern for our metrics is that the image-to-text models are not perfect and may introduce errors. To address this concern, we conduct extensive analysis and human evaluation, showing that our metric is robust to different pre-trained image-to-text models, and has much higher correlations with human judgments than CLIPScore (Spearman's $\rho =$ 0.51 vs. 0.26). We attribute this to  advances in vision-language models since Flamingo~\cite{alayrac2022flamingo, wang2022git, kim2021vilt, wang2022ofa, Hu2022PromptCapPT, Li2023BLIP2BL}, which makes our evaluation metric possible. Also, different from CLIP, which generates a single embedding for the whole image, visual questions are more fine-grained probes that allow image-to-text models to yield high accuracy in answering them.


%We attribute this to  advances in vision-language models since Flamingo~\cite{alayrac2022flamingo, wang2022git, kim2021vilt, wang2022ofa, Hu2022PromptCapPT, Li2023BLIP2BL}, which makes our evaluation metric possible.
% Also, different from CLIP, which generates a single embedding for the whole image, visual questions are more fine-grained probes that allow VQA models to yield high accuracy in answering them.

% \rk{You don't need a summary paragraph. Good introductions don't need to repeat themselves!}
% In summary, our contributions are as follows:
% \begin{itemize}
%     \item We introduce \NAME (\S\ref{sec:method}), a novel automatic text-to-image faithfulness evaluation metric based on visual question answering (VQA). 
%     %This new metric is reference-free, fine-grained, explainable, and provides an easy way to detect image deficiency automatically.
%     \item Based on \NAME, we introduce a large-scale text-to-image benchmark (\S\ref{sec:benchmark}) that contains 4K prompts and 25K questions, and present a comprehensive study of current text-to-image models against \NAME. (\S\ref{sec:exp})
%     \jungo{maybe say any future text2image models can be evaluated by these question-answer pairs.}
%     \item We conduct a detailed analysis and human evaluation (\S\ref{sec:exp}), showing that our evaluation metric is robust to different VQA models, and has much higher correlations with human judgments than prior metrics.
% \end{itemize}

% \nascomment{in general intro feels long.  consider moving details about alternative metrics, or types of questions generated, or all the stuff about the 12 categories, later in the paper.  the important thing the intro should do is to state clearly what existing models/tools the TIFA method exploits (e.g., a language model to produce questions, a QA system to check that they are valid given the prompt, a VQA system, maybe more) and -- this is really important -- that these can be upgraded later as they get better!  the intro should also summarize the most important takeaways from the experimental analysis.  try to keep it to one page at most (including the opening figure)} 
% \yushi{Great advice! Have modified a lot according to this comment}
% \jungo{I think you addressed this in the current version! Nice!}
% \jungo{left off here}
