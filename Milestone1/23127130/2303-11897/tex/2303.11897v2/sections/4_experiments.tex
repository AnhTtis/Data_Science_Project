


\section{Experiments}
\label{sec:exp}

In this section, we first show that \NAME has substantially higher correlations with human judgments than prior metrics on text-to-image faithfulness (\S\ref{sec:exp:corr}). Then we present a comprehensive evaluation of existing text-to-image models using \NAME v1.0, highlighting the challenges of current text-to-image models (\S\ref{sec:exp:findings}). Finally, we conduct an analysis of \NAME's robustness against different VQA models (\S\ref{sec:exp:comparisonVQA}). For all experiments, we use \textbf{mPLUG} as the VQA model for \NAME unless stated otherwise.
The models we evaluate include AttnGAN~\cite{Xu2017AttnGANFT}, X-LXMERT~\cite{Cho2020XLXMERTPC}, VQ-Diffusion~\cite{Gu2021VectorQD}, minDALL-E~\cite{kakaobrain2021minDALL-E}, and Stable Diffusion v1.1, v1.5, and v2.1~\cite{Rombach2021HighResolutionIS}. Details are in Appendix~\ref{appendix:t2i}.


\subsection{Correlation with Human Judgements}
\label{sec:exp:corr}

To compare \NAME with prior evaluation metrics, we first conduct human evaluations of the text-to-image models on the 1-5 Likert scale on text-to-image faithfulness.
Then we compare \NAME with other metrics based on their correlation with human judgments.

\vspace{-0.1in}
\paragraph{Likert scale on text-to-image faithfulness}
Annotators are asked to answer on a scale of 1 (worst) to 5 (best) to the question \emph{``Does the image match the text?"}.
The detailed annotation guidelines are in Appendix~\ref{appendix:annotation}.
Annotators are asked to focus on text-to-image faithfulness rather than image quality.
The Likert scale should be based on how many elements in the text prompt are missed or misrepresented in the image.
Objects are considered more important than the other categories.
If an object is missed in the image, then all related attributes, activities, relations, etc. are also considered lost.
An example is given in Figure~\ref{fig:human_eval}. 

We collect annotations of 800 generated images on 160 text inputs from \NAME v1.0. For each prompt, we sample an image from the 5 most recent generative models we evaluated, i.e., minDALL-E, VQ-Diffusion, Stable Diffusion v1.1, v1.5, and v2.1.  We collect 2 annotations per image and average over the scores as the single ``faithfulness" score. The inter-annotator agreement measured by Krippendorf's $\alpha$ is 0.67, indicating ``substantial" agreement.



\begin{figure}[h]
\centering
  \includegraphics[width=0.35\textwidth]{figures/human_evaluation.pdf}
  \caption{
Illustration of our Likert scale annotation guideline. Annotators are asked to give a score of 1 to 5 based on how many elements in the text prompt are missed or misrepresented in the image. 
% Here we mark all the entities in the text prompt that are present in the image by bounding boxes with corresponding colors. 
The missed elements are \textcolor{red}{\underline{underlined}}.
% \jungo{TODO: remove boundary boxes}
% %\jungo{so this boundary box is only for this illusration figure right? Maybe a bit confusing because we don't do any of this during our annotaitons?}
}
  \vspace{-4mm}
  \label{fig:human_eval}
\end{figure}

\vspace{-0.1in}
\paragraph{Baselines} 
We compare our evaluation with two families of reference-free metrics on text-image match introduced in Section~\ref{sec:related}. The first is the \textbf{caption-based method}.
We use the state-of-the-art \textbf{BLIP-2 FlanT5-XL}~\cite{Li2023BLIP2BL} as the captioning model.
The second approach is \textbf{CLIPScore}~\cite{Hessel2021CLIPScoreAR, radford2021learning}. We use CLIP (VIT-B/32)~\cite{radford2021learning} to compute the score.

\input{tables/human_correlation}


\vspace{-0.1in}
\paragraph{\NAME has a much higher correlation with human judgments than prior metrics.}
The correlations between each evaluation metric and human judgment are shown in Table~\ref{tab:human_correlation}. For caption-based evaluations, we use metrics 
BLEU-4~\cite{papineni2002bleu}, ROUGE-L~\cite{lin-2004-rouge}, METEOR~\cite{banerjee2005meteor}, and SPICE~\cite{anderson2016spice}.
\NAME has higher correlations with human judgments than all previous evaluation metrics on all VQA models.
\NAME(mPLUG) yields the highest correlation with human judgments among all VQA models. 
The inter-annotator agreement (Krippendorf's $\alpha$) is 0.67 (substantial agreement).



\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{figures/tifa_leaderboard.pdf}
  \caption{
Average \NAME score of text-to-image models on the \NAME v1.0 benchmark.
The horizontal axis shows their release dates.
% \yushi{how to make this prettier?}
% \yizhong{Is it possible to be fit into one-column? This looks quite sparse now. Also, the improvement will look sharper after you narrow the x axis.}\yushi{Mari says she also think it can be one-column.}
% \rk{Oof, this figure is cool but needs to be pretty-fied. Also, do we have any stds associated with the TIFA score}\yushi{Yeah, I computed stds on images, but the error bars will be long (17 - 22). I think we'd rather make the model names bigger and make it like a cool leaderboard}
}
  \vspace{-4mm}
  \label{fig:leaderboard}
\end{figure}

\begin{figure*}[ht]
\centering
  \includegraphics[width=0.95\textwidth]{figures/question_type.pdf}
  \caption{
 %\yushi{How to better present these? Also, think of a better description instead of ``other text inputs"} 
 Accuracy on each type of question in the \NAME v1.0 benchmark. The text-to-image models are Stable Diffusion v1.1, v1.5, and v2.1. We order the categories by the average score Stable Diffusion v2.1 gets on corresponding questions. For COCO captions, we also include the accuracy of the ground-truth images for reference. 
}
  \vspace{-4mm}
  \label{fig:type}
\end{figure*}





\subsection{Benchmarking Text-to-Image Models}

Figure~\ref{fig:leaderboard} shows the average \NAME score text-to-image models get on \NAME v1.0. The detailed scores with each VQA model on each element type are provided in Appendix~\ref{appendix:results}. We can see a clear trend of how text-to-image models evolve over time. There is a jump in \NAME score after DALL-E~\cite{Ramesh2021ZeroShotTG} is released, from about 60\% to 75\%. Qualitative examples of our evaluation metric are in Appendix~\ref{appendix:example}.

\subsection{Findings on Current Text-to-Image Models}
\label{sec:exp:findings}

Figure~\ref{fig:type} shows accuracy on each type of question in \NAME v1.0 for Stable Diffusion v1.1, v1.5, and v2.1. The score on each type reflects the text-to-image models' faithfulness in each type of visual element.
To the best of our knowledge, \NAME is the only automatic evaluation method that can provide such a detailed fine-grained analysis of image generation. We separate the scores on COCO captions and other text inputs. For COCO captions, we also include the accuracy on the ground-truth images for reference. We summarize our findings in the following paragraphs.

\vspace{-0.1in}
%\paragraph{Image captions are easier to generate than other text inputs.}
\paragraph{Generating images from captions vs.\ free-form text}
%\nascomment{I got confused by this header; you aren't really generating image captions.  I think it should say somethng like:  ``It is easier to generate images from image captions than from other text inputs.''}
%\jungo{how about: Generating from image captions vs.\ free-form text}
From Figure \ref{fig:type}, we can see that VQA accuracy is higher on the COCO captions than on other text inputs. The reason is that COCO captions correspond to real images, while other text inputs may correspond to compositions that cannot be found in real-world photos (e.g.~``a blue apple").

\vspace{-0.1in}
\paragraph{What elements are text-to-image models struggling with?}
Based on the scores of each category in Figure~\ref{fig:type}, we can see that Stable Diffusion models are performing well on material, animal/human, color, and location in terms of text-to-image faithfulness.
However, they yield low accuracy on questions involving \textbf{shapes}, \textbf{counting}, and \textbf{spatial relations}. ``other" mainly contains \textbf{abstract art notions}, and models are also struggling with them.
There is also a big gap between the synthesized images and real images on the COCO captions. Future work can explore various directions (e.g., training data/loss and model architecture)  to improve text-to-image models' faithfulness in these aspects.
%\jungo{Is there any part of their text-to-image training pipeline that suggests this limitation? Training loss? If any, worth mentioning.}

\vspace{-0.1in}
\paragraph{Why are ground-truth images not getting perfect scores?}
Real images in COCO do not get perfect scores because 1) the COCO captions contain a substantial amount of noise from crowd workers \cite{Kasai2021TransparentHE} and 2) VQA models are not perfect.
Despite the fact that real images have higher accuracy in most of the categories, we find Stable Diffusion v2.1 gets higher accuracy than real images on material, color, and location.
It is left to future work to improve the assessments of text-to-image models in these categories.
%\jungo{Actually not sure about this. COCO captining does have non-neglible noise from crowdworkers. So it's possible that captions are simply wrong in some cases.}

\vspace{-0.1in}
\paragraph{Stable Diffusion is evolving.}
We can see the consistent trend that Stable Diffusion models are improving in their later versions in most of the element categories. The exceptions are ``shape" for both prompt sources, ``other" and ``food" for the free-form text inputs without gold images.


\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{figures/compositionality}
  \caption{
\NAME vs.\ numbers of entities (objects, animals/humans, and food) in the text input. The accuracy starts to drop when more than 5 entities are added to the text, showing that compositionality is hard for text-to-image models. Meanwhile, \NAME scores for COCO ground-truth (GT) images remain consistent.
}
\vspace{-0.1in}
  \label{fig:compositionality}
\end{figure}

\vspace{-0.2in}
\paragraph{Composing multiple objects is challenging.}
Figure~\ref{fig:compositionality} shows how the number of entities (objects, animals/humans, food) in the text input affects the average \NAME score.
When there are more than 5 entities, \NAME score starts to drop radically for all text-to-image models, consistent with similar findings in other vision-language evaluations~\cite{grunde2021agqa,gandhi2022measuring}. For reference, we also add the real images in COCO in this figure. \NAME score on real images is rather consistent and does not change as the number of entities increases. This quantitatively shows that composing multiple objects is challenging for current text-to-image models.





% \nascomment{propose you break this into separate subsections, one for Likert scale measurements of faithfulness by humans, first explain the method then give the results; then a separate section for humans doing VQA, with results; and so on.}

% We collect two types of human annotations. The first is the Likert scale (1--5) on text-to-image faithfulness. These scores help investigate the correlation between various evaluation metrics and human judgments.
% The second type is the quality evaluation of the visual questions. We let annotators decide if a visual question is reasonable. Also, annotators choose their answers to the visual questions. These annotations help analyze the quality of \NAME question-answer pairs and also evaluate the accuracy of each VQA model on our generated questions.


\subsection{Analysis of VQA Models}
\label{sec:exp:comparisonVQA}
One major concern of \NAME is that VQA models can introduce some errors.
Table~\ref{tab:human_correlation} shows that \NAME has a much higher correlation with human judgment than the previous metrics, regardless of the choice of the VQA models; here we conduct a more detailed analysis.
%In addition, we conduct an ablation study of using different VQA models on \NAME.

\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{figures/VQA_sensitivity.pdf}
  \caption{
Several text-to-image models' \NAME score on COCO captions, measured by different VQA models. We also include the accuracy of ground-truth images for reference. 
% \rk{in both figures 8 and 9, I am sure you already know that the font size needs to increase to match the font size in the rest of the paper. Also for figures 7, 8, 9, 10 can we use seaborn colorblind colorpalette?}
}
  \vspace{-4mm}
  \label{fig:vqa_models}
\end{figure}



\vspace{-0.1in}
\paragraph{Sensitivity of \NAME to VQA models}
Figure~\ref{fig:vqa_models} shows several recent text-to-image models' \NAME scores on the COCO captions in \NAME v1.0, measured by different VQA models.
We also include the \NAME scores on the ground-truth COCO images for reference.
\NAME scores computed by different VQA models show a similar trend on these text-to-image models.
Also, the ground-truth images get the highest \NAME score.
We also computed Spearman's $\rho$ of \NAME scores given by different VQA models.
The pairwise correlation between all VQA models is greater than 0.6. 
%The correlation matrix is in Appendix~\ref{appendix:qa}. 

\vspace{-0.1in}
\paragraph{Humans performing VQA}
\label{sec:exp:humanVQA}

To conduct further analysis on the VQA models, we ask annotators to answer the multiple-choice visual questions in \NAME v1.0. These annotations help us evaluate the accuracy of each VQA model. For multiple-choice questions, we add the option ``None of the above" for human evaluation.
Annotation guidelines are in Appendix~\ref{appendix:annotation}.

We collect annotations of 1029 questions on 126 images. Each question is answered by two annotators. The inter-annotator agreement measured by Krippendorf's $\alpha$ is 0.88. A third annotator is involved if two annotators disagree, and the final answer is chosen by the majority vote.

\input{tables/vqa_acc}

\vspace{-0.1in}
\paragraph{Which VQA model should we use?}

Table~\ref{tab:vqa} reports the accuracy of each VQA model and the correlation between \NAME scores calculated by VQA model answers and human answers.  We observe that 
 higher model performance is directly related to the \NAME score's correlation with human judgments.
\textbf{mPLUG} has the highest accuracy.

Another important factor to consider is the runtime. We measure the inference speed of each VQA model on NVIDIA A40 GPU with batch size 1 over the Stable Diffusion v2.1 images ($768 \times 768$ pixels).
For one question, VILT takes 0.08s on average; OFA, GIT, and mPLUG all take about 0.25s; BLIP-2 takes 0.73s.
Based on the above results, we choose \textbf{mPLUG} as the default VQA model for \NAME v1.0 because it is the most accurate while being reasonably fast.

\vspace{-0.1in}
\paragraph{Separation of Text-to-Image Errors and VQA Errors }
\label{sec:exp:vqa_errors}
Suppose an image gets a wrong answer given a visual question.
Then the image generation or the VQA model might have made an error.
Based on the human VQA results, we separate these two kinds of errors in Figure~\ref{fig:separate_error}.
If human VQA gives the wrong answer, then we suspect the generated image has an error. Otherwise, the image is correct but the VQA model is making an error.
Figure~\ref{fig:separate_error} shows that the majority of errors are made by the text-to-image models.
For mPLUG, less than 25\% errors are due to the VQA model.
This suggests that the \NAME framework is a viable evaluation method despite its inherent challenges.

%\jungo{what's the implication on \NAME? How about: Therefore, the \NAME framework is a viable evaluation method for text-to-image synthesis.}

\begin{figure}[ht]
\centering
  \includegraphics[width=0.45\textwidth]{figures/error_split.pdf}
  \caption{
Source of the error when VQA gets the wrong answer.
}
  \vspace{-4mm}
  \label{fig:separate_error}
\end{figure}



