




\section{The \NAME metric}
\label{sec:method}

We introduce a framework for automatically estimating the faithfulness of an image to its text prompt.
Given a text input $T$, we aim to measure the faithfulness of the generated image $I$.
An overview of our metric is illustrated in Figure~\ref{fig:overview}.
From $T$, we generate $N$ multiple-choice question-answer pairs $\{Q_i, C_i, A_i\}_{i=1}^N$, in which $Q_i$ is a question, $C_i$ is a set of answer choices, and $A_i \in C_i$ is the gold answer. The answer $A_i$ can be inferred given $T$, $Q_i$, and $C_i$.
% Our goal is to evaluate the faithfulness between $T$ and $I$ using a visual question answering (VQA) model.
Next, for each question $Q_i$, we use a VQA model to produce an answer $A_i^{\text{VQA}} = \max_{a \in C_i} p(a \mid I, Q_i)$.
We define the faithfulness between the text $T$ and image $I$ as the VQA accuracy:
\begin{equation}
\vspace{-0.1in}
    \mathit{faithfulness}(T, I) = \frac{1}{N}\sum_{i=1}^N \mathbbm{1}[A_i^{\text{VQA}} = A_i]
\end{equation}
The range of our faithfulness score is $[0,1]$. It is maximized when we have a performant VQA model, and the image $I$ accurately covers the information in the text $T$ so that for any question $Q$, which can be answered given $T$ can also be answered given $I$.
Several key design decisions will be addressed in later sections: how to generate questions (\S\ref{sec:method:qg}), how to control the question quality (\S\ref{sec:filtering}), and how to answer those questions (\S\ref{sec:answering}).

%\rk{We need a short paragraph here that says: with the above metric defined, we need to make a few design decisions: (1) how to generate questions and (2) how do you filter out bad questions and (3) how to answer those questions using VQA. Those are addressed in the subsections below.} \yushi{Added below}

%\jungo{I edited the following paragraph a bit. I think we can make it even more compact and merge it into the preceding paragraph, if we need more space (e.g., how to generate questions -> just say question generation, control.. -> quality control, answer -> automatic visual question answering}
%With the above metric defined, some design decisions need to be addressed: (1) how to generate questions, (2) how to control the question quality, and (3) how to answer those questions. These are addressed in the subsections below.
%These are addressed in the subsections below.

% We define $p(Q, C|T)$ as the distribution of multiple-choice questions given the text prompt $T$, in which $Q$ is the question and $C$ is the set of choices. Let $p(A | T, Q, C)$ $p( A | I, Q, C)$ be the distribution of possible answers to the question $Q$ given either the text $T$ or the image $I$. 
% We constrain the answer in the set of choices $C$.
% We define the faithfulness between the text $T$ and image $I$ as
% \begin{equation*}
%     E_{Q,C \sim p(Q,C | T)}[D(p(A|T,Q,C), p(A | I, Q, C))]
% \end{equation*}
% Where $D$ is a measurement of the difference between two distributions.






\subsection{Question-Answer Generation}
\label{sec:method:qg}
%\rk{Setup up the technical challenge here: What are the challenges? Why isn't it straightforward to generate questions given text? Once you set up the problem, say how you solve it. Only after that, describe things with the notation below.} \yushi{added below, need improvement.}

Our main challenge is to generate diverse questions that cover all elements of the text input evenly. We also simplify the question-generation pipeline into a single GPT-3~\cite{brown2020language} completion, so that \NAME can exploit the power of recent language models (LM) and work with updated black-box LMs (e.g., ChatGPT) in the future.

Inspired by prior work~\cite{Changpinyo2022AllYM}, given a text prompt $T$, we generate the question-answer pairs $\{Q_i, C_i, A_i\}_{i=1}^N$ via the pipeline illustrated in Figure~\ref{fig:question_generation}.
Different from prior work, which relies on multiple components, our pipeline is completed by a single inference run via in-context learning with GPT-3 \cite{brown2020language, wei2022chain, hu2022incontext, press2022measuring, su-etal-2023}, thereby avoiding the need for intermediate human annotations.
%\jungo{can you state some benefits of this approach, if any? Maybe simplicity? fully automatic and not need for human interventions?}
% \nascomment{not obvious to me how this is chain of thought and not just few shot prompting.  if anything the extra structure you're imposing looks more like Ofir's self-ask than chain of thought ...}
We annotate 15 examples and use them as in-context examples for GPT-3 to follow.
Here we take the text \emph{``A photo of three dogs.''} as an example.
Each in-context example contains the following steps:


\begin{figure}[t]
\centering
  \includegraphics[width=0.42\textwidth]{figures/tifa_approach.pdf}
  \caption{
Our question-answer pair generation pipeline. The whole pipeline can be executed via a single inference of GPT-3 via in-context learning. Given the text prompt, GPT-3 first extracts the elements and then generates two questions for each element. The GPT-3 output is then parsed and filtered by UnifiedQA.
%\yushi{addressed the following Jungo's comments}
%\jungo{Need to simplify this. First make it clear that this is just one-shot LM generation! (visually maybe GPT-3 is speaking (like a bubble/callout might be effective). No human intervention, which is very cool. Remove all text colors and highlight different components. Remove "steps" because it's one-shot LM generation. Colors would make it look like you are interativng with GPT-3.}
} 
  \vspace{-4mm}
  \label{fig:question_generation}
\end{figure}


\vspace{-0.15in}
\paragraph{Element extraction}
Given text prompt $T$, GPT-3  will first extract all elements $\{v_i\}_{i=1}^m$ following prior work~\cite{Changpinyo2022AllYM} (for the in-context examples, we perform element extraction manually).
The elements include noun phrases (including named entities), verbs, adjectives, adverbs, and parse tree spans with no more than 3 words altogether. For the above example, the elements are \emph{photo, three, dogs}.
%\nascomment{clarify if this is done manually or automatically; if automatic, what methods are applied? from the figure, I think this is also part of the GPT-3 generation task, so you do it manually for the prompt examples but the rest is left to the model (like element category classification)?}
%\jungo{+1. Explicitly state what's done by GPT-3 automatically and what's done by humans. My understanding is our approach only needs a few in-context examples written manually, which makes it very appealing. Highlight that. }

\vspace{-0.15in}
\paragraph{Element category classification} 
%\nascomment{this is not clear; how do we do this classification?  are you training a classifier?  is it done manually?}  
For each element $v_i$, following~\cite{Krishna2019InformationMV}, we classify the elements into one of the following 12 categories: \emph{object, activity, animal, food, counting, color, material, spatial, location, shape, attribute}, and {other}. 
As shown in Figure~\ref{fig:question_generation}, text generated from GPT-3 contains the category corresponding to each question.%classification is done by GPT-3 during generation.
%\jungo{Okay so this is confusing (unless my understanding is incorrect). This reads like you're using GPT-3 as a classifier. No, you're just using it for single-run generation, and GPT-3 will implicitly categorize questions in generation.}
%\jungo{How about: As shown in..., text generated from GPT-3 contains the category corresponding to each question.}
For example, ``three'' is ``counting'', and ``dogs'' is classified as ``animal.''
This step allows a detailed analysis of the text-to-image model's ability in each category.

\vspace{-0.15in}
\paragraph{Question generation conditioned on elements}
For each element $v_i$, we generate two questions. The first is a question that should be answered ``yes'' for a faithful generated image, and the second question has $v_i$ as its answer. For example, two questions are generated for the element \emph{``three''}. The first is ``are there three dogs?'', and the choices are \{yes, no\}. The second is ``how many dogs are there?'', and the choices are \{1,2,3,4\}.
These two types of questions make our evaluations diverse and robust to surface-level differences.
%\jungo{any reason why we need these two types? They are technically asking about the same thing, right? Is it a consistency/sanity check for VQA?}

\vspace{-0.15in}
\paragraph{Completing above steps by prompting GPT-3 once}
As mentioned earlier, for each text $T$, the whole pipeline can be completed by one GPT-3 inference. 
We annotated 15 in-context examples that cover all types of questions. The full prompt is in the Appendix.
The prompt format is shown in Figure~\ref{fig:question_generation}. 
Our in-context examples follow the same format, and identical examples are used for all text inputs, leading to a fixed and limited amount of human annotation cost.
%\nascomment{are all 15 examples always used in every prompt?  maybe hint that more/better instances could be used in the future to improve or customize the approach}
We use \textit{code-davinci-002} engine for question generation, and the decoding temperature is 0.


\subsection{Question Filtering}
\label{sec:filtering}
% \yizhong{probably give more explanation or argument on why you chose UnifiedQA instead of GPT-3 itself?}\yushi{Here we have already generated the answer with GPT-3, it's more like double-checking the answer. I change the phrasing to "verify".}
To ensure the quality of generated images, we use UnifiedQA~\cite{Khashabi2020UnifiedQACF} to verify the GPT-3 generated question-answer pairs and filter out the ones that GPT-3 and UnifiedQA do not agree on.
UnifiedQA\footnote{Model checkpoint we use: \url{https://huggingface.co/allenai/unifiedqa-v2-t5-large-1363200}.} is a state-of-the-art multi-task question-answering model that can answer both multiple-choice and free-form questions. 
Denote the UnifiedQA model as $\mathit{QA}$. Given the text $T$, question $Q_i$, choices $C_i$, and answer $A_i$, Let $A_i^{f} = QA(T, Q_i)$ be the free-form answer, and $A_i^{mc} = \mathit{QA}(T, Q_i, C_i)$ be the multiple-choice answer.
We keep the question if $A_i = A_i^{mc}$ and the word-level $F_1$ score between $A_i^{f}$ and $A_i$ is greater than 0.7.
%\jungo{I wonder if you need this mathematical formulation here. Just say if their multiple-choice answers are the same or free-text answers overlpa with 0.7 F1 points or more?}
We conduct a human evaluation on 1000 filtered question-answer pairs.
Only 7 are considered not reasonable (e.g., generated choices do not include a correct answer).
%\jungo{what does not reasonable mean here? 7 are not consistent with the image prompt? Be explicit if you can!}.
Details are in Appendix~\ref{appendix:annotation}.


\subsection{VQA Models}
\label{sec:answering}
Since our questions contain a diverse set of visual elements (e.g., activity, art style), we use open-domain pre-trained vision-language models as our VQA model (rather than closed-class classification models fine-tuned on VQAv2~\cite{goyal2017making}). We provide tools to easily perform VQA on arbitrary images and questions, based on 5 state-of-the-art VQA models trained with distinct data and strategies. 

\vspace{-0.15in}
\paragraph{Vision-language models}
The general pre-trained vision-language model are \textbf{GIT-large}~\cite{wang2022git}, \textbf{VILT-B/32}~\cite{kim2021vilt}, \textbf{OFA-large}~\cite{wang2022ofa}, and \textbf{mPLUG-large}~\cite{Li2022mPLUGEA}. 
These models are pre-trained on a large amount of image-text pairs, and downstream image-to-text tasks like image captioning and visual question answering.
Notice that these models have not been trained to answer multiple-choice questions. For each question, we first decode the free-form answer and then choose the choice that has the highest similarity with the decoded answer, measured by SBERT~\cite{Reimers2019SentenceBERTSE}.
%\jungo{Oh this seems a rather odd design choice. Why not take perpelxity/average log loss of each choice. Choose the lowest choice? }
Another model we use is \textbf{BLIP-2 FlanT5-XL}~\cite{Li2023BLIP2BL}, in which a VIT~\cite{dosovitskiy2021an} is connected with a frozen FlanT5~\cite{Chung2022ScalingIL} via a lightweight transformer. This model allows for performing multiple-choice VQA directly due to the flexibility of the LM.

% \vspace{-0.15in}
% \paragraph{Visual encoder + frozen LM} Another family of image-to-text models we use is based on bootstrapping a visual encoder with a frozen language model (LM). 
% Two models are used. The first is \textbf{PromptCap + UnifiedQA}~\cite{Hu2022PromptCapPT, Khashabi2020UnifiedQACF}, in which the image is first converted into text via the task-aware captioning model PromptCap, and then UnifiedQA performs QA given the question and image description. The second is \textbf{BLIP-2 FlanT5xl}~\cite{Li2023BLIP2BL}, in which a VIT~\cite{dosovitskiy2021an} is connected with a frozen FlanT5~\cite{Chung2022ScalingIL} via a lightweight transformer.

\vspace{-0.15in}
\paragraph{Recommended VQA model} Based on considerations over the accuracy, correlation with human judgments, and run time, we currently suggest using \textbf{mPLUG-large} as the VQA model for \NAME. Analysis is given in Section~\ref{sec:exp:comparisonVQA}.
Like the LM and QA components, the VQA component can be updated in the future as the technology improves.


\begin{figure*}[t]
\centering
  \includegraphics[width=0.98\textwidth]{figures/tifa_benchmark_new.pdf}
  \caption{
Statistics and diversity of \NAME v1.0. The text inputs contain elements from 12 categories (e.g., object, spatial, and counting). We show the most common elements from each category. In addtion, we also show some example text inputs on the sides.
%\yizhong{What do category and element mean? The caption needs to be self-contained.}
}
  \vspace{-4mm}
  \label{fig:benchmark}
  
\end{figure*}


\section{\NAME v1.0: Benchmark for Text-to-Image Generation Faithfulness}
\label{sec:benchmark}

%\nascomment{I tried to think of a simpler, shorter name for the benchmark.  I failed.  you might want to call it TIFA-benchmark v.~1.0 to hint at/allow future versions.  could also just call it TIFA v.~1.0 and say it's an instantiation of the approach ...}

In this section, we introduce \NAME v1.0, a text-to-image generation faithfulness benchmark based on the evaluation method discussed in Section~\ref{sec:method}. The benchmark consists of 4,081 diverse text inputs paired with 25,829 question-answer pairs. Each question is classified into one of the categories discussed in Section~\ref{sec:method:qg}. The benchmark also comes with Python pip-installable APIs to perform VQA with various state-of-the-art VQA models on arbitrary visual questions.
%\jungo{APIs? That's super nice!! Is it just pip install tifa and users can do all evaluations?}
The overall \NAME for each text-to-image model is computed by averaging \NAME scores of images generated from each text input in the benchmark.


\subsection{Text Collections}
% \yizhong{The subsection title reminds me of PromptSource. Change the name to Dataset/Prompt Collections?} 
We collect 4,081 text inputs to benchmark text-to-image models' generation ability on diverse tasks.
2,000 text inputs are image captions from \textbf{COCO} validation set~\cite{lin2014microsoft}.
These captions have corresponding gold images. 
%that are helpful for analysis \nascomment{how so?  too vague}.
Since text-to-image models are often used to create abstract art, we also collect 2,081 text inputs from previous works that do not correspond to any real image.
All text inputs we use contain $\ge 3$ words.
We include 161 from \textbf{DrawBench} used in Imagen~\cite{Saharia2022PhotorealisticTD} (texts that are categorized as ``misspellings'' and ``rare words'' are removed); 1420 from \textbf{PartiPrompt} used in Parti~\cite{Yu2022ScalingAM} (texts in category ``abstract'' are removed); and 500 texts from \textbf{PaintSkill} used in DALL-Eval~\cite{Cho2022DALLEvalPT}.


\input{tables/statistics}

\subsection{Statistics and Diversity}
Table~\ref{tab:statistics} shows the basic statistics of the \NAME v1.0 benchmark.
% bar charts.
% \jungo{remove bar chars? Incomplete sentence, haha.}
We demonstrate \NAME v1.0's diversity in Figure~\ref{fig:benchmark}.
%\nascomment{this figure would be worth the real estate if it had examples}.
\NAME v1.0 contains questions about 4,550 distinct elements, which are categorized into 12 categories. The biggest genre is ``Object", which occurs 7,854 times in the text inputs. The ``Other" genre contains mostly abstract art notions, such as ``starry night'' and ``steampunk.''
% The number of times each type of element occurs in the text input is object (7,854), animal/human (3,501), attribute (3,399), activity (2,851), spatial (2,265), location (1,840), color (1,743), counting (986), food (911), material (209), shape (69), and other (201). 
The accuracy of VQA on a particular genre measures the text-to-image model's ability in the corresponding aspect.
% \jungo{can cut these numbers if we need more space. Figure tells us the same thing.}

% \rk{You have more information right? Like the question types? Can you put those statistics in here too?}\yushi{addressed. Anything more that should be included?}

