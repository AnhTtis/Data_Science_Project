
% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{framed}
\usepackage{bm}
\usepackage{bbm}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{cite}
\usepackage{xparse}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{url}


\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.88,0.95,1}

\def \ie {\emph{i.e.}}
\def \eg {\emph{e.g.}}
\def \etal {\emph{et al.}}

\newcommand{\ours}{PAC-S\xspace}
\newcommand{\oursref}{RefPAC-S\xspace}

\newcommand{\tit}[1]{\smallbreak\noindent\textbf{#1.}}
\newcommand{\tinytit}[1]{\noindent\textbf{#1.}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{***} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Positive-Augmented Contrastive Learning for\\Image and Video Captioning Evaluation}

\author{Sara Sarto$^1$ \quad Manuele Barraco$^1$ \quad Marcella Cornia$^1$ \quad Lorenzo Baraldi$^1$ \quad Rita  Cucchiara$^{1,2}$  \\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
$^1$University of Modena and Reggio Emilia, Modena, Italy \quad $^2$IIT-CNR, Pisa, Italy\\
% First line of institution2 address\\
{\tt\small \{name.surname\}@unimore.it}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly available at: {\url{https://github.com/aimagelab/pacscore}}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\input{sections/01_introduction.tex}

\section{Related Work}
\label{sec:related}
\input{sections/02_related.tex}

\section{Positive-Augmented Contrastive Learning}
\label{sec:method}
\input{sections/03_method.tex}

\section{Experimental Evaluation}
\label{sec:experiments}
\input{sections/04_experiments.tex}

\section{Conclusion}
\label{sec:conclusion}
\input{sections/05_conclusion.tex}

\section*{Acknowledgments}
\small{We thank CINECA for providing computational resources. Work conducted under a research grant co-funded by Leonardo S.p.A. and supported by the projects: PNRR-M4C2 (PE00000013) ``FAIR - Future Artificial Intelligence Research'' funded by the European Commission, ``ELSA - European Lighthouse on Secure and Safe AI'' funded by the EU (GA 101070617), and the PRIN ``CREATIVE: CRoss-modal understanding and gEnerATIon of Visual and tExtual content'' co-funded by the Italian Ministry of University and Research (CUP B87G22000460001).}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{bibliography}
}

\newpage
\newpage
\input{supplementary}

\end{document}
