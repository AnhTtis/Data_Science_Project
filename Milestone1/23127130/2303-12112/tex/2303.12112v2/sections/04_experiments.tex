\subsection{Implementation details}\label{sec:details}
\tinytit{Architecture and training details} In continuity with existing literature~\cite{hessel2021clipscore,kim2022mutual,shi2022emscore}, we use CLIP ViT-B/32~\cite{radford2021learning} as backbone to encode images (or video frames) and textual sentences. We finetune the visual and textual final projections of the model using the approach described in Sec.~\ref{sec:method} on the COCO dataset~\cite{lin2014microsoft}, which contains more than 120k images annotated with five captions. In particular, we employ the splits introduced by Karpathy~\etal~\cite{karpathy2015deep}, where 5,000 images are used for validation, 5,000 images are used for test and the rest for training. 
During finetuning, we use AdamW~\cite{loshchilov2019decoupled} as optimizer with a learning rate equal to 0.0001 and a batch size of 256. The $\lambda_v$ and $\lambda_t$ values are selected with a grid search, choosing the combination that provides the best average across datasets. Specifically, we set $\lambda_v$ to 0.05 and $\lambda_t$ to 0.1, and stop the training stage when the validation loss stops decreasing for 1,500 iterations.

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{.3em}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lc cc c cc}
\toprule
& & \multicolumn{2}{c}{\textbf{Flickr8k-Expert}} & & \multicolumn{2}{c}{\textbf{Flickr8k-CF}} \\
\cmidrule{3-4} \cmidrule{6-7}
& & Kendall $\tau_b$ & Kendall $\tau_c$  & & Kendall $\tau_b$ & Kendall $\tau_c$ \\
\midrule
BLEU-1~\cite{papineni2002bleu} & & 32.2 & 32.3 & & 17.9 & 9.3 \\
BLEU-4~\cite{papineni2002bleu} & & 30.6 & 30.8 & & 16.9 & 8.7 \\
ROUGE~\cite{lin2004rouge} & & 32.1 & 32.3 & & 19.9 & 10.3 \\
METEOR~\cite{banerjee2005meteor} & & 41.5 & 41.8 & & 22.2 & 11.5 \\
CIDEr~\cite{vedantam2015cider} & & 43.6 & 43.9 & & 24.6 & 12.7 \\
SPICE~\cite{spice2016} & & 51.7 & 44.9 & & 24.4 & 12.0 \\
\midrule
BERT-S~\cite{zhang2019bertscore} & & - & 39.2 & & 22.8 & - \\
LEIC~\cite{cui2018learning} & & 46.6 & - & & 29.5 & - \\
BERT-S++~\cite{yi2020improving} & & - & 46.7 & & - & - \\
UMIC~\cite{lee2021umic} & & - & 46.8 & & - & - \\
TIGEr~\cite{jiang2019tiger} & & - & 49.3 & & - & - \\
ViLBERTScore~\cite{lee2020vilbertscore} & & - & 50.1 & & - & - \\
MID~\cite{kim2022mutual} & & - & 54.9  & & 37.3 & - \\
\midrule
CLIP-S~\cite{hessel2021clipscore} & & 51.1 & 51.2 & & 34.4 & 17.7 \\
\rowcolor{LightCyan}
& & \underline{53.9} & \underline{54.3} & & \underline{36.0} & \underline{18.6}  \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+2.8}) & (\textcolor{blue}{+3.1}) & & (\textcolor{blue}{+1.6}) & (\textcolor{blue}{+0.9}) \\
\midrule
RefCLIP-S~\cite{hessel2021clipscore} & & 52.6 & 53.0 & & 36.4 & 18.8 \\
\rowcolor{LightCyan}
& & \underline{\textbf{55.5}} & \underline{\textbf{55.9}} & & \underline{\textbf{37.6}} & \underline{\textbf{19.5}}  \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\oursref}} & & (\textcolor{blue}{+2.9}) & (\textcolor{blue}{+2.9}) & & (\textcolor{blue}{+1.2}) & (\textcolor{blue}{+0.7}) \\
\bottomrule
\end{tabular}
 }
 \vspace{-0.1cm}
\caption{Human judgment correlation scores on Flickr8k-Expert and Flickr8k-CF~\cite{hodosh2013framing}. The overall best scores are in bold.}
\label{tab:flickr}
\vspace{-0.35cm}
\end{table}

\tit{Positive image-text generation} To augment the training set with new positive examples, we use Stable Diffusion\footnote{\url{https://github.com/CompVis/stable-diffusion}}~\cite{rombach2022high} for generating new visual data and the BLIP model~\cite{li2022blip} for generating new textual descriptions. Specifically, to generate images, we employ the model pre-trained on the English image-text pairs of the LAION-5B dataset~\cite{schuhmann2022laion} and finetuned at a resolution equal to $512\times512$ on the LAION-Aesthetics subset\footnote{\url{https://laion.ai/blog/laion-aesthetics/}}, which has been filtered with aesthetic requirements. 
During generation, we employ the safety checker module to reduce the probability of explicit images and disable the invisible watermarking of the outputs to avoid easy identification of the images as machine-generated. To generate text, instead, we use the ViT-L/14 version\footnote{\url{https://github.com/salesforce/BLIP}} of the BLIP model pre-trained on 129M image-text pairs and finetuned on the COCO dataset. After this generation phase, we get a new version of the COCO dataset in which each image is additionally associated with a machine-generated caption and each human-annotated caption is instead associated with a newly generated image. Sample image-text data employed for finetuning are shown in Fig.~\ref{fig:dataset}. % More examples can be found in the supplementary material.

\subsection{Correlation with human judgment}
To evaluate the correlation of the proposed metric with human ratings, we conduct experiments on both image and video captioning datasets. Specifically, we employ the Flickr8k-Expert, Flickr8k-CF, and Composite datasets~\cite{hodosh2013framing,aditya2015images} for the image setting and the VATEX-EVAL dataset~\cite{shi2022emscore} to evaluate video-caption pairs.

\tit{Image captioning results} We first evaluate our solution on the Flickr8k-Expert and Flickr8k-CF datasets~\cite{hodosh2013framing} which include image-caption pairs with corresponding human ratings. In particular, Flickr8k-Expert contains 17k expert annotations for visual-textual pairs, with a total of 5,664 different images. The pairs are evaluated with a score from 1 to 4, where 1 indicates that the caption does not correlate with the image and 4 indicates that the caption describes the corresponding image without errors. Flickr8k-CF, instead, is composed of 145k binary quality judgments, collected from CrowdFlower, for 48k image-caption pairs (with 1,000 unique images). Each pair is annotated with at least three binary scores, where ``yes'' indicates that the caption correlates with the image. To measure the correlation with human judgment, we compute the mean proportion of ``yes'' annotations as the score for each pair. 

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{.55em}
\resizebox{.68\linewidth}{!}{
\begin{tabular}{lc cc}
\toprule
& & \multicolumn{2}{c}{\textbf{Composite}} \\
\cmidrule{3-4}
& & Kendall $\tau_b$ & Kendall $\tau_c$ \\
\midrule
BLEU-1~\cite{papineni2002bleu} & & 29.0 & 31.3 \\
BLEU-4~\cite{papineni2002bleu} & & 28.3 & 30.6 \\
ROUGE~\cite{lin2004rouge} & & 30.0 & 32.4 \\
METEOR~\cite{banerjee2005meteor} & & 36.0 & 38.9 \\
CIDEr~\cite{vedantam2015cider} & & 34.9 & 37.7 \\
SPICE~\cite{spice2016} & & 38.8 & 40.3 \\
\midrule
BERT-S~\cite{zhang2019bertscore} & & - & 30.1 \\
BERT-S++~\cite{yi2020improving} & & - & 44.9 \\
TIGEr~\cite{jiang2019tiger} & & - & 45.4 \\
ViLBERTScore~\cite{lee2020vilbertscore} & & - & 52.4 \\
FAIEr~\cite{wang2021faier} & & - & 51.4 \\
\midrule
CLIP-S~\cite{hessel2021clipscore} & & 49.8 & 53.8 \\
\rowcolor{LightCyan}
 & & \underline{51.5} & \underline{55.7} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+1.7}) & (\textcolor{blue}{+1.9}) \\
\midrule
RefCLIP-S~\cite{hessel2021clipscore} & & 51.2 & 55.4 \\
\rowcolor{LightCyan}
 & & \underline{\textbf{53.0}} & \underline{\textbf{57.3}} \\
 \rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\oursref}} & & (\textcolor{blue}{+1.8}) & (\textcolor{blue}{+1.9}) \\
\bottomrule
\end{tabular}
}
\vspace{-0.1cm}
\caption{Human judgment correlation scores on the Composite dataset~\cite{aditya2015images}. The overall best scores are in bold.}
\label{tab:composite}
\vspace{-0.35cm}
\end{table}

\begin{table*}[t]
\small
\centering
\begin{minipage}{0.6\linewidth}
\setlength{\tabcolsep}{.3em}
\resizebox{\linewidth}{!}{
\begin{tabular}{lc cc c cc c cc}
\toprule
& & \multicolumn{2}{c}{\textbf{No Ref}} & & \multicolumn{2}{c}{\textbf{1 Ref}} & & \multicolumn{2}{c}{\textbf{9 Refs}} \\
\cmidrule{3-4} \cmidrule{6-7} \cmidrule{9-10}
& & Kendall $\tau_b$ & Spearman $\rho$ & & Kendall $\tau_b$ & Spearman $\rho$ & & Kendall $\tau_b$ & Spearman $\rho$ \\
\midrule
BLEU-1~\cite{papineni2002bleu} & & - & - & & 12.2 & 15.9 & & 28.9 & 37.0 \\
BLEU-4~\cite{papineni2002bleu} & & - & - & & 12.6 & 16.4 & & 22.4 & 29.5 \\
ROUGE~\cite{lin2004rouge} & & - & - & & 12.5 & 16.3 & & 23.8 & 30.9 \\
METEOR~\cite{banerjee2005meteor} & & - & - & & 16.4 & 21.5 & & 27.6 & 35.7 \\
CIDEr~\cite{vedantam2015cider} & & - & - & &  17.3 & 22.6 & & 27.8 & 36.1 \\
\midrule
BERT-S~\cite{zhang2019bertscore} & & - & - & & 18.2 & 23.7 & & 29.3 & 37.8 \\
BERT-S++~\cite{yi2020improving} & & - & - & & 15.2 & 19.8 & & 24.4 & 31.7 \\
\midrule
EMScore~\cite{shi2022emscore} & & 23.2 & 30.3 & & 28.6 & 37.1 & & 36.8 & 47.2 \\
\rowcolor{LightCyan}
& & \underline{\textbf{25.1}} & \underline{\textbf{32.6}} & & \underline{\textbf{31.4}} & \underline{\textbf{40.5}} & & \underline{\textbf{38.1}} & \underline{\textbf{48.8}} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\ours~/ \oursref}} & & (\textcolor{blue}{+1.9}) & (\textcolor{blue}{+2.3}) & & (\textcolor{blue}{+2.8}) & (\textcolor{blue}{+3.4}) & & (\textcolor{blue}{+1.3}) & (\textcolor{blue}{+1.6})  \\
\bottomrule
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}{0.37\linewidth}
\includegraphics[width=0.95\textwidth]{images/vatex_scores.pdf}
\end{minipage}
\vspace{-0.1cm}
\caption{Human judgment correlation scores on the VATEX-EVAL dataset~\cite{shi2022emscore}. The overall best scores are in bold. On the right, we show Kendall $\tau_b$ correlation score at varying of the number of reference captions.}
\label{tab:vatex}
\vspace{-0.3cm}
\end{table*}

Following previous works~\cite{zhang2019bertscore,lee2020vilbertscore,lee2021umic,hessel2021clipscore}, we compute Kendall correlation scores in both $\tau_b$ and $\tau_c$ versions. Results are reported in Table~\ref{tab:flickr} comparing the proposed \ours metric with respect to both standard captioning evaluation scores (\ie~BLEU~\cite{papineni2002bleu}, ROUGE~\cite{lin2004rouge}, METEOR~\cite{banerjee2005meteor}, CIDEr~\cite{vedantam2015cider}, and SPICE~\cite{spice2016}) and more recent solutions that either exploit text-only or cross-modal learned embeddings, such as BERT-S~\cite{zhang2019bertscore}, BERT-S++~\cite{yi2020improving}, LEIC~\cite{cui2018learning}, TIGEr~\cite{jiang2019tiger}, UMIC~\cite{lee2021umic}, VilBERTScore~\cite{lee2020vilbertscore}, MID~\cite{kim2022mutual}, and CLIP-S~\cite{hessel2021clipscore}. While CLIP-S is reported in both reference-free and reference-based versions, all other metrics require reference captions. The only exception is the MID score which is positioned between a reference-free and a reference-based metric since it utilizes the mean and covariance of the correct captions. 

From the results, it can be seen that the proposed score achieves the best correlation with human judgment on both considered datasets, demonstrating its effectiveness compared to previously proposed metrics. In particular, when comparing our score with CLIP-S and RefCLIP-S, we can notice an improvement in terms of Kendall $\tau_b$ of 2.8 and 2.9 points on Flickr8k-Expert, and 1.6 and 1.2 points on Flickr8k-CF, respectively. Similar improvements can be also observed in terms of Kendall $\tau_c$ correlation score. It is also important to note that the reference-free version of \ours overcomes by a large margin the correlation scores achieved by traditional reference-based metrics such as CIDEr and SPICE (\eg~+10.3/10.4 points with respect to the CIDEr metric on Flickr8k-Expert).

We also conduct experiments on the Composite dataset~\cite{aditya2015images} which contains 12k human judgments for image-caption pairs taken from COCO~\cite{lin2014microsoft} (2,007 images), Flickr8k~\cite{hodosh2013framing} (997 images), and Flickr30k~\cite{young2014image} (991 images). Each image-caption pair is evaluated with a score, given by humans, between 1 and 5 to estimate the correspondence of the caption with the associated image. Experimental results are shown in Table~\ref{tab:composite}, again in terms of Kendall $\tau_b$ and Kendall $\tau_c$ correlation scores. Also in this case, our metric achieves a better correlation with human ratings than that obtained by both traditional and more recent evaluation scores, confirming its effectiveness even when compared to CLIP-S and RefCLIP-S.

\tit{Video captioning results} To evaluate the correlation with humans in the context of video-caption pairs, we consider the VATEX-EVAL dataset~\cite{shi2022emscore} which includes 3,000 videos from the VATEX~\cite{wang2019vatex} validation set, each of them associated with six captions of mixed quality. Each video-caption pair has been evaluated by three human annotators with a score from 1 (to denote inconsistency between the video and the caption) to 5 (to denote consistency). Overall, the dataset contains 54k human ratings for 18k video-caption pairs. Following recent literature~\cite{shi2022emscore}, we compute Kendall $\tau_b$ and Spearman $\rho$ rank correlation coefficients, considering a different number of reference sentences when measuring correlation (\ie~zero, one, or nine). Correlation scores are reported in Table~\ref{tab:vatex} in comparison with standard evaluation metrics, BERT-S, BERT-S++, and the only video-specific captioning metric existing in literature, \ie~EMScore. On the right, we also report the correlation scores at varying the number of reference captions. It can be seen that PAC-S achieves the best correlation scores in all settings, improving EMScore of 2.3, 3.4, and 1.6 Spearman $\rho$ points respectively with no references, one reference, and nine reference sentences. These results further confirm the appropriateness of our positive-augmented contrastive learning strategy to improve captioning evaluation also when considering videos instead of static images.

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{.55em}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lc cccc c c}
\toprule
 & & HC & HI & HM & MM & & Mean \\
\midrule
length & & 51.7 & 52.3 & 63.6 & 49.6 & & 54.3 \\
BLEU-1~\cite{papineni2002bleu} & & 64.6 & 95.2 & 91.2 & 60.7 & & 77.9 \\
BLEU-4~\cite{papineni2002bleu} & & 60.3 & 93.1 & 85.7 & 57.0 & & 74.0 \\
ROUGE~\cite{lin2004rouge} & & 63.9 & 95.0 & 92.3 & 60.9 & & 78.0 \\
METEOR~\cite{banerjee2005meteor} & & 66.0 & 97.7 & 94.0 & 66.6 & & 81.1 \\
CIDEr~\cite{vedantam2015cider} & & 66.5 & 97.9 & 90.7 & 65.2 & & 80.1 \\
\midrule
BERT-S$^\dagger$~\cite{zhang2019bertscore} & & 65.4 & 96.2 & 93.3 & 61.4 & & 79.1 \\
BERT-S++$^\dagger$~\cite{yi2020improving} & & 65.4 & 98.1 & 96.4 & 60.3 & & 80.1 \\
TIGEr$^\dagger$~\cite{jiang2019tiger} & & 56.0 & 99.8 & 92.8 & 74.2 & & 80.7 \\
ViLBERTScore$^\dagger$~\cite{lee2020vilbertscore} & & 49.9 & 99.6 & 93.1 & 75.8 & & 79.6\\
FAIEr$^\dagger$~\cite{wang2021faier} & & 59.7 & {\textbf{99.9}} & 92.7 & 73.4 & & 81.4 \\
MID$^\dagger$~\cite{kim2022mutual} & & 67.0 & 99.7 & \textbf{97.4} & \textbf{76.8} & & \textbf{85.2} \\
\midrule
CLIP-S~\cite{hessel2021clipscore} & & 55.9 & \underline{99.3} & 96.5 & 72.0 & & 80.9 \\
\rowcolor{LightCyan}
 & & \underline{60.6} & \underline{99.3} & \underline{96.9} & \underline{72.9} & & \underline{82.4} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+4.7}) & (+0.0) & (\textcolor{blue}{+0.4}) & (\textcolor{blue}{+0.9}) & & (\textcolor{blue}{+1.5}) \\
\midrule
RefCLIP-S~\cite{hessel2021clipscore} & & 64.9 & 99.5 & 95.5 & 73.3 & & 83.3 \\
\rowcolor{LightCyan}
& & \underline{\textbf{67.7}} & \underline{99.6} & \underline{96.0} & \underline{75.6} & & \underline{84.7} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\textbf{\oursref}}} & & (\textcolor{blue}{+2.8}) & (\textcolor{blue} {+0.1}) & (\textcolor{blue}{+0.5}) & (\textcolor{blue}{+2.3}) & & (\textcolor{blue}{+1.4}) \\
\bottomrule
\end{tabular}
}
\caption{Accuracy results on the Pascal-50S dataset~\cite{vedantam2015cider} obtained by averaging the scores over five random draws of reference captions (except for reference-free metrics). The $\dagger$ marker indicates scores reported in previous works, which may differ in terms of selected reference captions. We refer to the text for the definition of HC, HI, HM, and MM. The overall best scores are in bold.}
\label{tab:pascal}
\vspace{-0.35cm}
\end{table}

\subsection{Caption pairwise ranking}
We assess the effectiveness of the proposed metric on the Pascal-50S dataset~\cite{vedantam2015cider}, which reports pairwise preference judgments between two captions. Specifically, the dataset comprises 4k sentence pairs, each of them associated with an image from the UIUC Pascal sentence dataset~\cite{rashtchian2010collecting}. 
For each pair, 48 human judgments have been collected, in which each evaluation expresses which sentence best describes the given image. Sentence pairs are divided into four different categories: two human-written and correct captions (HC), two human-written captions where one is correct and the other is wrong (HI), two correct captions but one written by humans and the other machine-generated (HM), two machine-generated and correct captions (MM).



\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{.55em}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lcc cc lcc}
\toprule
& & Mean & & & & & Mean\\
\midrule
% length & & . & . & & . \\
% \midrule
CLIP-S~\cite{hessel2021clipscore} & & 68.2 & & & RefCLIP-S~\cite{hessel2021clipscore} & & 75.8 \\
\rowcolor{LightCyan}
 & & \underline{69.7} & & & & & \underline{\textbf{76.9}} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+1.5}) & & &  \multirow{-2}{*}{\textbf{\oursref}} & & (\textcolor{blue}{+1.1})  \\
\bottomrule
\end{tabular}
}
 \vspace{-0.1cm}
\caption{Accuracy results on the Abstract-50S dataset~\cite{vedantam2015cider}.}
\label{tab:abstract}
\vspace{-0.35cm}
\end{table}

In this setting, instead of computing correlation scores, we compute accuracy by considering for each pair the caption preferred by the majority of human ratings as correct (where ties are broken randomly) and measuring how often the evaluation metric assigns a higher score to the selected caption. For each evaluation, we randomly sample five reference captions (among the 48 provided by the dataset) and average the results over five different draws. Accuracy values are reported in Table~\ref{tab:pascal} in comparison with previously proposed metrics. Similarly to previous works, we also include the results of a length-based baseline in which the longer caption is always considered the better one. From the results, we can observe that \ours and \oursref respectively perform better than CLIP-S and RefCLIP-S in almost all categories, with an increase of 1.5 points in terms of averaged accuracy. Also, our results are generally higher than those of the other metrics, with the only exception of the MID score which achieves slightly better accuracy. However, our results are not directly comparable to the ones reported in previous works (as, for example, FAIEr and MID), given the random selection of ground-truth sentences used to compute reference-based metrics.

As a further analysis, we evaluate the results on the Abstract-50S dataset~\cite{vedantam2015cider} which contains clip-art images from~\cite{zitnick2013bringing} associated with 48 human-annotated reference sentences. Similar to Pascal-50S, each image is associated with a pair of candidate captions and 48 human judgments, collected asking to select which candidate caption is most similar to a given reference sentence. Overall, the dataset is composed of 400 candidate caption pairs, of which 200 describe the corresponding image (\ie~both captions are correct) and 200 instead contain one correct caption and one caption of another image. Again, we compute accuracy scores by considering the most preferred caption as correct, averaging the results over five random draws of reference sentences. Table~\ref{tab:abstract} shows the results of our score in comparison with CLIP-S. In both reference-free and reference-based versions, \ours achieves better accuracy scores than CLIP-S, demonstrating its effectiveness also in this challenging setting of non-photographic images.

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{.4em}
\resizebox{.94\linewidth}{!}{
\begin{tabular}{lc cc c c}
\toprule
 & & \multicolumn{2}{c}{\textbf{FOIL}} & & \textbf{ActivityNet-FOIL} \\
 \cmidrule{3-4} \cmidrule{6-6}
 & & Acc. (1 Ref) & Acc. (4 Refs) & & Accuracy \\
\midrule
BLEU-1~\cite{papineni2002bleu} & & 65.7 & 85.4 & & 60.1 \\
BLEU-4~\cite{papineni2002bleu} & & 66.2 & 87.0 & & 66.1 \\
ROUGE~\cite{lin2004rouge} & & 54.6 & 70.4 & & 56.7 \\
METEOR~\cite{banerjee2005meteor} & & 70.1 & 82.0 & & 72.9 \\
CIDEr~\cite{vedantam2015cider} & & 85.7 & 94.1 & & 77.9 \\
MID~\cite{kim2022mutual} & & 90.5 & 90.5 & & - \\
\midrule
CLIP-S~\cite{hessel2021clipscore} & & 87.2 & 87.2 & & - \\
EMScore~\cite{shi2022emscore} & & - & - & & 89.5 \\
\rowcolor{LightCyan}
& & \underline{89.9} & \underline{89.9} & & \underline{90.1} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+2.7}) & (\textcolor{blue}{+2.7}) & & (\textcolor{blue}{+0.6}) \\
\midrule
RefCLIP-S~\cite{hessel2021clipscore} & & 91.0 & 92.6 & & - \\
EMScoreRef~\cite{shi2022emscore} & & - & - & & 92.4 \\
\rowcolor{LightCyan}
& & \underline{\textbf{93.7}} & \underline{\textbf{94.9}} & & \underline{\textbf{93.5}} \\
\rowcolor{LightCyan}
\multirow{-2}{*}{\textbf{\oursref}} & & (\textcolor{blue}{+2.7}) & (\textcolor{blue}{+2.3}) & & (\textcolor{blue}{+1.1}) \\
\bottomrule
\end{tabular}
}
 \vspace{-0.1cm}
\caption{Accuracy results on the FOIL~\cite{shekhar2017foil} and ActivityNet-FOIL~\cite{shi2022emscore} hallucination detection datasets. The overall best scores are in bold.}
\label{tab:foil}
\vspace{-0.2cm}
\end{table}

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{.32em}
\resizebox{\linewidth}{!}{
\begin{tabular}{lc ccc ccc}
\toprule
& & B-4 & M & C & CLIP-S & \textbf{\ours} & \textbf{\oursref} \\
\midrule
Show and Tell~\cite{vinyals2015show} & & 31.4 & 25.0 & 97.2 & 0.572 & \cellcolor[rgb]{0.88,0.95,1} 0.772 & \cellcolor[rgb]{0.88,0.95,1} 0.826 \\
Show, Attend and Tell~\cite{xu2015show} & & 33.4 & 26.2 & 104.6 & 0.582 & \cellcolor[rgb]{0.88,0.95,1} 0.785 & \cellcolor[rgb]{0.88,0.95,1} 0.837 \\
Up-Down~\cite{anderson2018bottom} & &  36.7 & 27.9 & 122.7 & 0.592 & \cellcolor[rgb]{0.88,0.95,1} 0.794 & \cellcolor[rgb]{0.88,0.95,1} 0.847 \\
SGAE~\cite{yang2019auto} & & 39.0 & 28.4 & 129.1 & 0.600 & \cellcolor[rgb]{0.88,0.95,1} 0.803 & \cellcolor[rgb]{0.88,0.95,1} 0.854 \\
AoANet~\cite{huang2019attention} & & 38.9 & 29.2 & 129.8 &  0.602 & \cellcolor[rgb]{0.88,0.95,1} 0.805 & \cellcolor[rgb]{0.88,0.95,1} 0.856 \\
$\mathcal{M}^2$ Transformer~\cite{cornia2020meshed} & & 39.1 & 29.2 & 131.2 & 0.605 & \cellcolor[rgb]{0.88,0.95,1} 0.806 & \cellcolor[rgb]{0.88,0.95,1} 0.854 \\
X-Transformer~\cite{pan2020x} & & 39.7 & 29.5 & 132.8 & 0.610 & \cellcolor[rgb]{0.88,0.95,1} 0.812 & \cellcolor[rgb]{0.88,0.95,1} 0.859 \\
VinVL~\cite{zhang2021vinvl} & & \textbf{41.0} & \textbf{31.1} & \textbf{140.9} & \textbf{0.627} & \cellcolor[rgb]{0.88,0.95,1} \textbf{0.821} & \cellcolor[rgb]{0.88,0.95,1} \textbf{0.869} \\
\midrule
\textit{Humans} & & - & \textit{24.1} & \textit{87.6} & \textit{0.626} & \cellcolor[rgb]{0.88,0.95,1} \textit{0.818} & \cellcolor[rgb]{0.88,0.95,1} \textit{0.857} \\
\bottomrule
\end{tabular}
}
 \vspace{-0.1cm}
\caption{Evaluation scores of state-of-the-art captioning models on COCO test set~\cite{lin2014microsoft}.}
\label{tab:sota}
\vspace{-0.35cm}
\end{table}



\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{.35em}
\resizebox{\linewidth}{!}{
\begin{tabular}{clc cc c cc c cc c c c c c c}
\toprule
& & & \multicolumn{2}{c}{\textbf{Flickr8k-Expert}} & & \multicolumn{2}{c}{\textbf{Flickr8k-CF}} & & \multicolumn{2}{c}{\textbf{VATEX-EVAL}} & & \textbf{Pascal-50S} & & \textbf{FOIL} & & \textbf{ActivityNet-FOIL} \\
\cmidrule{4-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-13} \cmidrule{15-15} \cmidrule{17-17}
& & & Kendall $\tau_b$ & Kendall $\tau_c$  & & Kendall $\tau_b$ & Kendall $\tau_c$ & & Kendall $\tau_b$ & Spearman $\rho$ & & Accuracy & & Accuracy & & Accuracy \\
\midrule
& CLIP-S~\cite{hessel2021clipscore} &  &  51.7 &  52.1 &  &  34.9 &  18.0 &  &  - &  - &  &  81.1 &  &  90.6 &  &  - \\
& EMScore~\cite{shi2022emscore}  & & - & - & & - & - & & 24 1& 31.4 & & - & & - & & 90.0 \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} & &  &  \textbf{54.5} &  \textbf{54.9} &  &  \textbf{35.9} &  \textbf{18.5} &  & \textbf{26.8} & \textbf{34.7} &  &  \textbf{82.9} &  &  \textbf{91.1} &  &  \textbf{90.7} \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} \multirow{-4}{*}{\textbf{CLIP ViT-B/16}} & \multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+2.8}) & (\textcolor{blue}{+2.8}) & & (\textcolor{blue}{+1.0}) & (\textcolor{blue}{+0.5}) & & (\textcolor{blue}{+2.7}) & (\textcolor{blue}{+3.3}) & & (\textcolor{blue}{+1.8}) & & (\textcolor{blue}{+0.5}) & & (\textcolor{blue}{+0.7}) \\
\midrule
 & CLIP-S~\cite{hessel2021clipscore} & & 52.6 & 53.0 & & 35.2 & 18.2 & & - & - & & 81.7 & & 90.9 & & - \\
& EMScore~\cite{shi2022emscore}  & & - & - & & - & - & & 26.7 & 34.7 & & - & & - & & 89.0 \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} & &  &  \textbf{55.1} &  \textbf{55.5} &  &  \textbf{36.8} &  \textbf{19.0} &  &  \textbf{28.9} &  \textbf{37.4} &  &  \textbf{82.2} &  &  \textbf{91.9} &  &  \textbf{91.2} \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} \multirow{-4}{*}{\textbf{CLIP ViT-L/14}} & \multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+2.5}) & (\textcolor{blue}{+2.5}) & & (\textcolor{blue}{+1.6}) & (\textcolor{blue}{+0.8}) & & (\textcolor{blue}{+2.2}) & (\textcolor{blue}{+2.7}) & & (\textcolor{blue}{+0.5}) & & (\textcolor{blue}{+1.0}) & & (\textcolor{blue}{+2.2}) \\
\midrule
& CLIP-S~\cite{hessel2021clipscore} & & 52.3 & 52.6 & & 35.4 & 18.3 & & - & - & & 81.2 & & 88.9 & & - \\
\textbf{OpenCLIP} & EMScore~\cite{shi2022emscore} & & - & - & & - & - & & 24.8 & 32.2 & & - & & - & & 88.2 \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} \textbf{ViT-B/32} & &  &  \textbf{53.6} &  \textbf{53.9} &  &  \textbf{36.1} &  \textbf{18.6} &  & \textbf{25.4} & \textbf{33.1} &  &  \textbf{82.1} &  &  \textbf{90.1} &  &  \textbf{89.5} \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} & \multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+1.3}) & (\textcolor{blue}{+1.3}) & & (\textcolor{blue}{+0.7}) & (\textcolor{blue}{+0.3}) & & (\textcolor{blue}{+0.6}) & (\textcolor{blue}{+0.9}) & & (\textcolor{blue}{+0.9}) & & (\textcolor{blue}{+1.2}) & & (\textcolor{blue}{+1.3}) \\
\midrule
& CLIP-S~\cite{hessel2021clipscore}  & & 54.4 & 54.5 & & 36.6 & 18.9 & & - & - & & 82.5 & & 92.2 & & - \\
\textbf{OpenCLIP} & EMScore~\cite{shi2022emscore}  & & - & - & & - & - & & 27.0 & 35.0 & & - & & - & & 90.7 \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} \textbf{ViT-L/14} &  &  &  \textbf{55.3} &  \textbf{55.7} &  &  \textbf{37.0} &  \textbf{19.1} &  &  \textbf{27.8} & \textbf{36.1} &  &  \textbf{82.8} &  &  \textbf{93.1} &  &  \textbf{91.2} \\
\rowcolor{LightCyan}
\cellcolor[rgb]{1,1,1} & \multirow{-2}{*}{\textbf{\ours}} & & (\textcolor{blue}{+0.9}) & (\textcolor{blue}{+1.2}) & & (\textcolor{blue}{+0.4}) & (\textcolor{blue}{+0.2}) & & (\textcolor{blue}{+0.8}) & (\textcolor{blue}{+1.1}) & & (\textcolor{blue}{+0.3}) & & (\textcolor{blue}{+0.9}) & & (\textcolor{blue}{+0.5}) \\
\bottomrule
\end{tabular}
}
 \vspace{-0.1cm}
\caption{Human correlation and accuracy scores on both image and video captioning datasets using different cross-modal backbones.}
\label{tab:features}
\vspace{-0.3cm}
\end{table*}

\subsection{Sensitivity to object hallucination}
Correctly identifying captions with potential object hallucinations (\ie~with objects that are not present in the image or video) is fundamental for the captioning task~\cite{rohrbach2018object}. Therefore, we extend our analysis to two datasets for detecting hallucinations in textual sentences, namely FOIL~\cite{shekhar2017foil} and ActivityNet-FOIL~\cite{shi2022emscore}. In particular, the FOIL dataset is composed of image-caption pairs from the COCO dataset~\cite{lin2014microsoft}. In this case, captions are perturbed by creating modified versions that are highly similar to the original ones but contain one single error (\ie~a foil word). For a fair comparison, we take the subset of the validation set that does not overlap with the portion of COCO used to finetune our model thus obtaining 8k images, each associated with a foil-correct textual pair. The ActivityNet-FOIL dataset, instead, contains video-text pairs from the ActivityNet test set~\cite{zhou2019grounded}. Each video comes with two annotated paragraphs, one used to construct foil-correct pair and the other used as ground-truth for reference-based metrics. To create a foil caption, a noun phrase in the original caption is replaced with a similar but incorrect visual concept. Overall, the dataset is composed of 1,900 foil-correct paragraph pairs.  

Since each image or video is associated with a foil-correct caption pair, we measure the portion of times in which the correct caption obtains a higher score than the foil one. Table~\ref{tab:foil} shows the accuracy results on the considered datasets. As it can be seen, \ours achieves better results than previous solutions, increasing the accuracy score of 2.7 and 0.6 points compared to CLIP-S and EMScore, respectively. Similar improvements can also be observed in the reference-based version, demonstrating the capabilities of our metric to correctly identify hallucinated objects.

\subsection{System-level correlation}
After demonstrating the benefits of using \ours over other evaluation metrics, we also analyze its effectiveness when evaluating existing captioning methods. To this aim, we consider different popular captioning models and compute their predictions on images coming from the COCO test set. Results are reported in Table~\ref{tab:sota} in terms of BLEU-4, METEOR, CIDEr, CLIP-S, and our \ours, in both reference-free and reference-based versions. We also include the results of a human-based baseline, in which for each sample one human-annotated sentence (among the five provided by the COCO dataset) is randomly selected as candidate caption and compared with the remaining references\footnote{The BLEU-4 score of the human-based baseline is not reported due to its sensitivity to the number of references used for evaluation.}. As shown in the table, our metric well correlates with previous ones in identifying the best captioning model. Interestingly, \ours can also effectively evaluate human-annotated sentences, unlike for example the METEOR and CIDEr scores which rank human captions even below those generated by early captioning approaches~\cite{xu2015show,vinyals2015show}.



\subsection{Analyzing other cross-modal features}\label{sec:diffFeats}
Finally, we report in Table~\ref{tab:features} captioning evaluation results when using different cross-modal features. In particular, we employ ViT-B/16 and ViT-L/14 models of CLIP~\cite{radford2021learning} and the ViT-B/32 and ViT-L/14 versions of the open source implementation (\ie~OpenCLIP~\cite{wortsman2022robust}\footnote{\url{https://github.com/mlfoundations/open_clip}}) trained on the English subset of the LAION-5B dataset~\cite{schuhmann2022laion}. For all backbones, we employ the same finetuning procedure and training settings described in Sec~\ref{sec:details}. We conduct the analysis on the majority of the datasets considered in the previous experiments and compare the proposed \ours with CLIP-S and EMScore, respectively for image and video captioning datasets. Noticeably, \ours achieves the best results across all cross-modal backbones and all datasets, overcoming correlation and accuracy scores of other metrics by a large margin. When comparing the results when using different backbones, both ViT-L/14 models outperform other considered architectures as well as the standard CLIP ViT-B/32 model used in previous experiments, thus demonstrating the usefulness of using more powerful cross-modal models to evaluate captioning predictions.



