We are interested in devising an image and video captioning metric based on a shared embedding space in which both visual data and text can be projected and compared. To this aim, we start from the dual-encoder architecture popularized by CLIP~\cite{radford2021learning}, which comprises an image encoder~\cite{he2016deep,dosovitskiy2020image} and a text encoder~\cite{vaswani2017attention}. In this architecture, the multimodal interaction is performed in a late fusion fashion, by projecting the output of both encoders to a common dimensionality and then on the $\ell_2$ hypersphere via normalization. The visual and the textual inputs can then be compared via cosine similarity.

Starting from a trained embedding space, an evaluation metric for image captioning can be defined by simply scaling, and eventually thresholding, the similarity computed inside of the embedding itself. For instance, given a visual embedding $v$ and a textual embedding $t$, Hessel~\etal~\cite{hessel2021clipscore} define the evaluation score as
\begin{equation}
    \label{eq:clip_score}
    \text{Score}(t, v) = w \cdot \max (\cos (t, v), 0),
\end{equation}
where $\cos$ indicates the cosine similarity computed inside of the embedding space and $w$ is a scaling factor to enhance numerical readability.

Large-scale contrastive models like CLIP~\cite{radford2021learning} are trained on web-collected image-caption pairs. These provide a large-scale source of supervision for learning scalable low-level and semantic visual and textual features, as testified by their zero-shot classification performance and by their adaptability to different tasks~\cite{ramesh2022hierarchical,barraco2022unreasonable,materzynska2022disentangling,khandelwal2022simple}. Nevertheless, it shall be noted that the textual annotations contained in alt-tags are far from the quality level that a captioning evaluator should look for, and that the distribution of web-scale images might not be properly aligned with those on which image captioning systems are evaluated.

To solve this issue, one might think of learning the metric directly on cleaned data sources. However, recent attempts of learning contrastive-based evaluation metrics on cleaned datasets like COCO~\cite{lin2014microsoft} perform poorly when compared to traditional metrics, potentially because of the lack of training data~\cite{jiang2019tiger}. We, therefore, advocate the usage of synthetic generators of both visual and textual data, which showcase sufficiently high quality levels when generating both images and texts, do lack in terms of style, and are controllable in terms of visual distribution. 

Specifically, given a positive image-text pair $(v, t)$, we augment it by generating a synthetic caption $t'$ from $v$ using an image captioner~\cite{li2022blip}, and a synthetic image $v'$ from $t$ via a diffusion-based text-to-image model~\cite{rombach2022high}, thus building a dataset consisting of tuples of four elements $(v, t, v', t')$. As in Eq.~\ref{eq:clip_score}, we represent $t'$ and $v'$ via their respective text and image embedding. We then train our evaluation model by jointly taking into account contrastive relationships between real and generated matching image-caption pairs (Fig.~\ref{fig:model}). To lower the computational requirements, we start with pre-trained CLIP visual and textual encoders and only train the projection toward the embedding space. 

Formally, given a batch of $N$ real images $\mathcal{V} = \left[ v_1, v_2, ..., v_N \right]$ and their corresponding captions $\mathcal{T} = \left[ t_1, t_2, ..., t_N \right]$, generated images $\mathcal{V}' = \left[ v_1', v_2', ..., v_N' \right]$ and generated texts $\mathcal{T}' = \left[ t_1', t_2', ..., t_N' \right]$, we define multiple $N \times N$ matrices containing pairwise cosine similarities between the different inputs. We then adopt a symmetric InfoNCE loss~\cite{oord2018representation} which aims at maximizing the cosine similarity between the $N$ matching pairs and minimize those of the $N^2 - N$ non-matching pairs. The loss which compares real images $\mathcal{V}$ with respect to real texts $\mathcal{T}$ can be defined, for instance, as
\begin{gather}
    L_{\mathcal{V}, \mathcal{T}} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp(\cos(v_i, t_i) / \tau}{\sum_{j=1}^N \exp(\cos(v_i, t_j) / \tau} + \\
    -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp(\cos(v_i, t_i) / \tau}{\sum_{j=1}^N \exp(\cos(v_j, t_i) / \tau},
\end{gather}
where $\tau$ is a temperature parameter. In addition to a loss term between real images and real texts, $L_{\mathcal{V}, \mathcal{T}}$, we also add symmetrical loss terms between cross-modal generated and real pairs, \ie~between generated images and human-annotated texts, and between original images and generated texts. In this way, generated items act as additional positive samples for the real matching pairs, thus adding a supervisory signal without paying the cost of the noisy data on which contrastive-based features extractors like CLIP are learned. In summary, the final loss is a weighted combination of the three loss terms, \ie 
\begin{equation}
    L = L_{\mathcal{V}, \mathcal{T}} + \lambda_v L_{\mathcal{V}', \mathcal{T}} + \lambda_t  L_{\mathcal{V}, \mathcal{T}'},
\end{equation}
where $L_{\mathcal{V}', \mathcal{T}}$ is the loss between generated images and real texts and $L_{\mathcal{V}, \mathcal{T}'}$ its counterpart between generated texts and real images.


\subsection{Captioning evaluation score for images}
After training with positive-augmented contrastive learning, we employ two evaluation scores for evaluating images in both a reference-free and a reference-based setting. Specifically, we employ Eq.~\ref{eq:clip_score} with $w=2$\footnote{To stretch the range of the score distribution in $\left[0, 1\right]$.} as our reference-free score. Then, we follow the approach proposed in~\cite{hessel2021clipscore} to include reference ground-truth captions in the evaluation process. Specifically, we compute the representation of each reference caption using the textual encoder. Then, we compute the harmonic mean between the reference-free score (Eq.~\ref{eq:clip_score}) and the maximum cosine similarity between the candidate caption and all reference captions. Formally, given a set of reference captions $R = \{ r_1, r_2, ..., r_m\}$, the score is computed as
\begin{gather}
    \text{Ref-Score}(t, v, R) = \text{H-Mean}(\text{Score}(t, v), \\
    \max(0, \max_{r \in R} \cos (c, r))),
\end{gather}
where $\text{Score}(\cdot)$ indicates the reference-free evaluation score as reported by our positive-augmented embedding space, and $\text{H-Mean}(\cdot)$ indicates the harmonic mean.

\subsection{Captioning evaluation score for videos}
To test the proposed positive-augmented strategy for evaluating video captions, we extend the above defined metric following the approach of~\cite{shi2022emscore}. In this case, matching scores are computed at two granularity levels, \ie~a coarse-grained level in which the global representation of the candidate caption is compared with the global representation of the video, and a fine-grained level in which the embeddings of single words are compared to those of single frames.

Specifically, we use the positive-augmented CLIP visual encoder to extract the embeddings of single frames and average-pool them to get the representation of the entire video. Similarly, we employ the corresponding textual encoder to get single tokens and whole caption embeddings. The fine-grained score is then computed by taking the F1-score of pairwise word-frame similarities and TF-IDF~\cite{robertson2004understanding} weighting, and the coarse-grained score is computed as the similarity between the global video and caption representations. Given a source video $V$ and a candidate caption $c$, the overall score is defined as
\begin{equation}
    \label{eq:emscore}
    \text{Score}(c, V) = \frac{\text{Score}(c, V)_c + \text{Score}(c, V)_f}{2},
\end{equation}
where $\text{Score}_c$ represents the coarse-grained embedding matching and $\text{Score}_f$ stands for the fine-grained similarity.
Finally, to include a set of reference captions $R$, we follow the reference version of the aforementioned approach:
\begin{equation}
    \label{eq:ref_emscore}
    \text{Ref-Score}(c, V, r) = \frac{\text{Score}(c, V) + \max_{r \in R}\text{Score}(c, r)}{2},
\end{equation}
where $\text{Score}(c, r)$ is computed as defined in Eq.~\ref{eq:emscore} by using the word-level embeddings of the reference caption.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/dataset_samples.pdf}
\caption{Sample real and generated image-text data used for positive-augmented contrastive learning.}
\label{fig:dataset}
\vspace{-.35cm}
\end{figure}