Image and video captioning solutions have been traditionally evaluated using a set of standard evaluation metrics, specifically BLEU~\cite{papineni2002bleu}, METEOR~\cite{banerjee2005meteor}, ROUGE~\cite{lin2004rouge}, CIDEr~\cite{vedantam2015cider}, and SPICE~\cite{spice2016}. Some of them have been originally introduced to evaluate NLP tasks such as machine translation and summarization, while others have been specifically designed for the captioning task. 

Recently, research efforts have been made to introduce additional metrics that can capture different aspects of generated textual sentences, like diversity~\cite{shetty2017speaking,van2018measuring,wang2019describing,wang2020diversity}, robustness of object hallucination~\cite{rohrbach2018object}, uniqueness~\cite{wang2020towards}, and coverage of ground-truth named entities~\cite{cornia2019show,cornia2020smart}. A new trend, instead, is to exploit the capabilities of pre-trained models to compare textual-only~\cite{zhang2019bertscore,yi2020improving} or visual-textual contents~\cite{jiang2019tiger,jiang2019reo,lee2020vilbertscore,wang2021faier,lee2021umic,hessel2021clipscore}. Among them, the BERT score~\cite{zhang2019bertscore} and its improved version~\cite{yi2020improving} use pre-trained BERT embeddings~\cite{devlin2018bert} to represent and compare word tokens in the generated and ground-truth sentences.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\linewidth]{images/model.pdf}
    \caption{Overview of our positive-augmented constrastive learning approach.}
    \label{fig:model}
    \vspace{-.3cm}
\end{figure*}

In addition to these text-based metrics, other solutions leverage the multimodal nature of vision-and-language models to exploit not only textual information but also the visual content of images and potentially video frames. For example, Jiang~\etal~\cite{jiang2019tiger} introduced the TIGEr metric, which considers the similarities between words and image regions computed according to a cross-modal matching model~\cite{lee2018stacked} trained on COCO~\cite{lin2014microsoft}. Other approaches, instead, exploit the effectiveness of web-scale vision-and-language models such as VilBERT~\cite{lu2019vilbert}, UNITER~\cite{chen2020uniter}, and CLIP~\cite{radford2021learning}, pre-trained on millions or even billions of image-text pairs, to obtain more robust metrics~\cite{lee2020vilbertscore,lee2021umic,hessel2021clipscore,kim2022mutual}. Among them, the recent CLIP-Score~\cite{hessel2021clipscore} is based on a modified cosine similarity between image and candidate caption representations coming from the CLIP model. Recently, Kim~\etal~\cite{kim2022mutual} proposed using CLIP visual-textual features to compute the negative Gaussian cross-mutual information, obtaining a more effective evaluation metric.

While all the aforementioned evaluation metrics have originally been introduced for image captioning, there is only one attempt to evaluate video descriptions through learnable metrics also taking into account the visual content appearing in video frames. In particular, Shi~\etal~\cite{shi2022emscore} presented the EMScore, in its both reference-free and reference-based versions, that computes fine-grained similarities between video frames and words of the candidate caption using CLIP visual-textual embeddings. 

Another related work is that proposed in~\cite{zhu2023imagine} where diffusion models are used to evaluate text-only tasks. Differently from our proposal, the introduced metric exploits similarities between machine-generated images obtained by a visual generator~\cite{rombach2022high} starting from reference and candidate textual items during evaluation.  
