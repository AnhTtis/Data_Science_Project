The task of image captioning, which requires an algorithm to describe visual contents with natural language sentences, has been gaining considerable attention from the research community in the past few years~\cite{xu2015show,vinyals2015show,karpathy2015deep}. As such, the task has witnessed methodological and architectural innovations, ranging from the usage of self-attentive models~\cite{huang2019attention,herdade2019image,cornia2020meshed,pan2020x} to the development of better connections between visual and textual modalities with the addition of objects~\cite{anderson2018bottom,zhang2021vinvl,yang2019auto} and tags~\cite{yao2017boosting,li2020oscar} or the use of more powerful cross-modal features~\cite{barraco2022unreasonable,barraco2022camel,sarto2022retrieval}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/first_page.pdf}
    \caption{Evaluation scores generated by our proposed metric, \ours, in comparison with existing metrics for captioning. The caption highlighted in green is the one preferred by humans.}
    \label{fig:firstpage}
    \vspace{-.35cm}
\end{figure}

Together with an increase in generation quality, the automatic evaluation of captions has also witnessed a significant effort. While early evaluation scores were based on translation metrics~\cite{papineni2002bleu,lin2004rouge,banerjee2005meteor}, more effective text-based~\cite{vedantam2015cider,spice2016,zhang2019bertscore} and multimodal solutions~\cite{jiang2019tiger,wang2021faier} have been proposed in the last few years. Among these, the usage of cross-modal models in which both visual and textual data can be matched has proven to be a viable strategy that can lead to high quality metrics~\cite{lee2020vilbertscore,lee2021umic,hessel2021clipscore,kim2022mutual}. Recently, the large-scale CLIP model~\cite{radford2021learning} was tested for image captioning evaluation, resulting in the CLIP-Score~\cite{hessel2021clipscore} which proved to have a significant correlation with human judgment.

While these advancements demonstrate the appropriateness of using contrastive-based embedding spaces for evaluating image captions, large-scale models pre-trained on web-collected data also have limitations, due to the lack in style of captions collected from alt-tags and of the distribution of web-scale images which is not aligned with those on which captioning systems are evaluated. While cleaned data sources, on the contrary, are limited in size, recent advances in both image~\cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2022high,gafni2022make} and text generation~\cite{zhang2021vinvl,wang2022simvlm,li2022blip} have made it possible to synthetically generate data in both modalities, with controlled style and quality.

Following this insight, in this paper we propose a learnable metric that fuses together the advantages of both these scenarios, by leveraging the quality of the pre-training on web-collected data and that of cleaned data, and also regularizing the training by considering additional positive samples hailing from visual and textual generators. Specifically, our proposed metric, PAC-S, is trained via a newly conceived positive-augmented contrastive learning approach, in which pairs of generated images and texts act as additional positives in addition to real images and human-annotated captions taken from a cleaned data source. We demonstrate that the combination of these factors, \ie~the usage of a cleaned data source and the pairing with multimodal generated data, when used to finetune a large-scale contrastive model, results in an embedding space with significantly higher alignment with the human judgment (Fig.~\ref{fig:firstpage}). We apply the resulting metric to evaluate both images and videos, both in reference-based and reference-free settings.

We investigate the quality of the proposed metric by conducting extensive experiments on a variety of image and video datasets, including Flickr8k-Expert and Flickr8k-CF~\cite{hodosh2013framing}, Composite~\cite{aditya2015images}, Pascal-50S, and Abstract-50S~\cite{vedantam2015cider} for the image scenario and the VATEX-EVAL dataset~\cite{shi2022emscore} to evaluate video-caption pairs. Further, we verify its sensitivity to object hallucination on the FOIL~\cite{shekhar2017foil} and ActivityNet-FOIL~\cite{shi2022emscore} datasets and compare the performance of state-of-the-art caption generators with respect to the proposed metric. Our proposal outperforms previous reference-based and reference-free metrics and showcases superior performance with respect to CLIP-Score~\cite{hessel2021clipscore} and the corresponding video-based version (\ie~EMScore~\cite{shi2022emscore}), which also employ a contrastive-based embedding space. Overall, our metric ranks first in terms of correlation with human judgment with respect to all existing image and video captioning metrics.

To sum up, the main contribution of this paper is a novel metric for image and video captioning, based on a positive-augmented training of a multimodal embedding space, which exploits both curated image-caption pairs and additional synthetically generated positives. Extensive experiments on several datasets demonstrate a higher correlation with human judgment and an increased sensitivity to object hallucination.
