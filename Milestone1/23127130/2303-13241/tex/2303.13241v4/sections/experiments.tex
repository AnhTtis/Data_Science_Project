\section{Experiments}
In this section, we first detail our experimental setup with which we competed in the post-mortem \gls{SPEC} competition and achieved state-of-the-art performance. In addition, we perform extensive experiments on the SPEED+~\cite{Park2021-jr} and TUD-L~\cite{hodan_bop_2018} datasets to investigate various aspects of our pose estimation system.

\subsection{SPEED+ Satellite Pose Estimation Challenge}
The Next-Generation \underline{S}pacecraft \underline{P}os\underline{E} \underline{E}stimation \underline{D}ataset (SPEED+) was the subject of the 6D pose estimation challenge %\footnote{https://kelvins.esa.int/pose-estimation-2021/home/} 
\gls{SPEC} organized by the \gls{ESA} and Stanford University. 
After its inception in 2021, the official part of the challenge concluded on 03/31/2022. Afterward, for post-mortem competition, the leaderboards remained open and competitors were able to submit results before the competition ultimately closed on 12/31/2022. 

The purpose of the competition was to determine the current state-of-the-art performance on the challenging task of satellite pose estimation as well as closing the Sim2Real gap. 
Hence, the training and validation samples of the dataset are $59960$ computer-rendered images of the Tango satellite annotated with the 6D pose.
The test set contains $9531$ \gls{HIL} mockup-satellite images split into two subsets %-- called \texttt{lightbox} and \texttt{sunlamp} -- 
with distinct visual conditions.
The \texttt{lightbox} split simulates albedo conditions using light boxes with diffuser plates and the \texttt{sunlamp} split entails a high-intensity sun simulator to mimic direct homogeneous light from the sun~\cite{Park2021-jr}.

\subsubsection{Synthetic Training Data}
\label{subsubsec:exp:data}
Our work is inspired by the issue of missing the satellite's CAD model but requiring one to train the proposed network architecture.
%The dataset does not include a ground truth CAD model of the satellite which motivates our approach to the problem.
To this end, we established a coarse wireframe model of the satellite by inspection and used this faulty model in combination with the 6D pose to generate the missing annotations (i.e. $\bm{I_{\tilde{q}}}, \bm{I}_o, \bm{I}_r$) for the original training images. 
In addition, we rectified each sample and reduced the resolution to $640 \times 400$ pixels.

\subsubsection{Closing the Sim2Real Gap} 
\label{subsubsec:exp:sim2real}
A central obstacle of the dataset is closing the Sim2Real gap. 
In this work, we focus on data augmentations which have long been a standard practice to train neural networks~\cite{devries2017improved} and are one of the simplest, yet effective steps to close the domain gap. 
%We augmented each training image by an array of randomized augmentations, the full stack of augmentations with all values can be found in the github repository. 
We applied an array of randomized augmentations to each training image\footnote{For details we would like to refer to our GitHub repository (soon available)}. %, the full stack of augmentations with all values can be found in the github repository.
Besides common augmentations (e.g. add, multiply, invert, brightness, blur, gaussian noise, and dropout)~\cite{Sundermeyer2018-fl} we use ~\gls{DL}-based style augmentations~\cite{Jackson2018-ye}. In addition, we crafted space-specific augmentations to tackle the unique effects seen in the test data. 
For instance, we implemented a synthetic specular reflection module that searches for connected regions in the image that are brighter than the rest of the image and adds a very bright blurry structure on top. This structure resembles features similar to specular reflection seen in orbit. 
Simultaneously, we make the rest of the image darker to mimic the exceedance of the dynamic range of the camera. 
This dramatically improves the prediction performances on the \texttt{sunlamp} test set (see ~\ref{exp:speedplus_ablation:domain_gap}). 

\subsubsection{Object Detection and Bounding Box Refinement} \label{subsubsec:exp:object_detection}
In the scope of the challenge, we trained an off-the-shelf object detector to predict 2D bounding boxes obtained by our created mask annotations $\bm{I}_o$. 
We used a Mask R-CNN~\cite{He2017-ko} with a pre-trained ResNet-50~\cite{He2015-hv} backbone and similar augmentations as described in~\ref{subsubsec:exp:sim2real}. 
The detector was trained for $130$ epochs using stochastic gradient descent with a learning rate of $0.01$, momentum $0.9$, and weight decay $0.0005$. In addition, we decayed the learning rate by $0.1$ every $3$ epochs. 
For two-stage approaches, the performance of 6D pose estimation algorithms correlates with the quality of the 2D object detection~\cite{sundermeyer2023bop}. % performance. 
Hence, we propose an iterative bounding box refinement during inference. % step for our approach. 
%For each sample, we first used the bounding box from the detector and predicted the error map $\bm{\widehat{I_e}}$. 
%If the standard deviation of the predicted error is below a threshold, we assume that the model is certain to a degree about its prediction. 
%In that case, we use the wireframe from ~\ref{subsubsec:exp:data} to change the bounding box such that it fits the model perfectly. 
We render the model wireframe (cf. \ref{subsubsec:exp:data}) if the standard deviation of the predicted error map is below a threshold and update the bounding box. 
% Thus, we exploit this value to determine good predictions.
% For samples below the threshold, we render the wireframe (cf. \ref{subsubsec:exp:data}) and update the bounding box.

\subsubsection{Test Set Augmentations}
%In addition to bounding box refinement, we added another step to trade-off extra compute for higher performance in the scope of the challenge. 
%We observed that pose estimation results were not identical when rotating input images, probably due to biases in the training data. 
Another empirical finding which leads to a \gls{SPEC} specific method adaptation was deviating results if the input images were rotated.
We hypothesize that this is caused by a bias in the training data.
Thus, we create an ensemble and forward each sample four times to our model, rotated by $\varrho \in [0, \frac{\pi}{2}, \pi$, $\frac{3\pi}{2}]$, respectively. 
After prediction, the rotations of all outputs are reversed. 

We generate $n_\varepsilon$ hypothesis (cf.~\ref{subsec:multi_hypothesis}) for each rotation, leading to a total of $4 \cdot n_\varepsilon$ hypotheses refinements
At last, we average the four resulting confidence maps $\widehat{\bm{I}}_o = \frac{1}{4}\sum_{i=1}^4 \widehat{\bm{I}}_{o,i}$ before refining each individual hypothesis.

\subsubsection{Training} \label{subsubsec:exp:training}
The network architecture is inspired by \cite{liu2022gdrnpp_bop}, a state-of-the-art 6D pose estimation method. As the backbone, we use ConvNeXt-Base~\cite{Liu_undated-mg}, pre-trained on ImageNet-1k. We extend the decoder of~\cite{liu2022gdrnpp_bop} by two additional upsampling steps but keep the kernel size of $3$ and feature dimensions of $256$ for each convolution the same. 
The group normalization~\cite{Wu2018-re} and GELU~\cite{Hendrycks2016-hm} activation are applied to every layer, except the output layer. In the last layer, we apply the sigmoid activation to $\widehat{\bm{I}}_o$ and $\widehat{\bm{I}_r}$. 

The model is trained for $2\mathrm{e}6$ steps with a batch size of $32$ using the Adam~\cite{Kingma2014-hu} optimizer with a base learning rate of $1\mathrm{e}{-3}$. The learning rate is warmed up for the first $1000$ steps and annealed after $72\%$ of the training is complete with a cosine scheduler~\cite{Wang2021-sj}.
We predict $8$ surface regions for $\bm{I}_r$ and scale the loss with weights $\alpha, \beta, \gamma = 1$ and $\delta=0.1$.

% We predict $8$ surface regions for $\bm{I}_r$. Therefore, the complete target tensor is composed of $\bigl[\bm{I_{\tilde{q}}} \conc \bm{I}_o \conc \bm{I}_e \conc \bm{I}_r\bigr] \in \mathbb{R}^{256 \times 256 \times 13}$ and scale the loss with weights $\alpha, \beta, \gamma = 1$ and $\delta=0.1$.

\subsubsection{Evaluation Metrics}
In the scope of the SPEED+ challenge, translational and rotational errors are used to evaluate a 6D pose~\cite{Park2021-jr}. 
The normalized position error $e_{\bm{t}}$ and rotation error $e_{\bm{R}}$ are defined as
\begin{equation}
    e_{\bm{t}} = \frac{\norm{\bm{\hat{t}} - \bm{t}}}{\bm{t}} , ~~ e_{\bm{R}} = 2\cdot \arccos{\abs{\langle \bm{\hat{a}}, \bm{a} \rangle}}
\end{equation}
% \begin{equation}
%     e_{\bm{R}} = 2\cdot \arccos{\abs{\langle \bm{\hat{a}}, \bm{a} \rangle}} ,
% \end{equation}
% \begin{equation}
%     e_{\text{pose}} = e_{\bm{t}} + e_{\bm{R}},
% \end{equation}
with quaternion representation $\bm{a}$ of rotation matrix $\bm{R}$. 
If the errors are below the calibration accuracy $e_{\bm{t}} < 0.002173$ and $e_{\bm{R}} < 0.169^{\circ}$ of the \gls{HIL} facility they are set to zero. 
Both error scores combined result in $e_{\text{pose}} = e_{\bm{t}} + e_{\bm{R}}$.
\subsubsection{Challenge Results}
Until the conclusion of the challenge, the ground truth labels of the test set have not been publicly available. 
To add an entry to the challenge, a spreadsheet of the results had to be submitted to the official challenge website. 
A summary of the results of the best submissions is shown in Table~\ref{tab:speedplus_results}. 
%The table is a compilation of each team's \textit{best} result from the official submissions\footnote{\url{https://kelvins.esa.int/pose-estimation-2021/}} to the live and post-mortem challenge or any publication. 
The table is a compilation of each team's \textit{best} result either from the official submission\footnote{\url{https://kelvins.esa.int/pose-estimation-2021/}} tables (live or post-mortem) or any publication.
Our scores are taken from the official post-mortem leaderboard under the team name \textit{mystery\_team}.
Not all top-performing teams have made their method public yet.

Compared to the best submissions on SPEED+, our \method~ achieves overall state-of-the-art. 
In the \texttt{lightbox} category, our \method~ performs best on all three metrics. % $e_{\bm{t}}, e_{\bm{R}}$, and $e_{\text{pose}}$. 
In the \texttt{sunlamp}, our method is third in regards of $e_{\bm{t}}$ and $e_{\text{pose}}$, however performs best in rotational error $e_{\bm{R}}$.
Fig.~\ref{fig:speedplus_samples} shows some qualitative results.
\begin{table}
    \begin{center}
        \caption{Comparison with state-of-the-art on SPEED+. Values depict the best-found results in a publication, the live, or post-mortem challenge.}\label{tab:speedplus_results} 
        \resizebox{\columnwidth}{!}{
        \begin{tabular}[t]{lcccccccc}
            \toprule[1pt]
            & \multicolumn{3}{c}{lightbox}  &\multicolumn{3}{c}{sunlamp} \\ 
            \cmidrule(lr){2-4} \cmidrule(lr){5-7}
             & $e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$  &$e_{\bm{t}}$ & $e_{\bm{R}}$ &$e_{\text{pose}}$ & $\mu$\\
            \cmidrule(lr){1-8}
            \text{lava1302~\cite{Wang2022-rs}} &0.0464	&0.1163	&0.1627 &\textbf{0.0069} &0.0476	&\textbf{0.0545} &0.1086 \\
            \text{prow}                 &0.0196 &0.0944 &0.1140 &0.0133 &0.0840 &0.0972 &0.1056\\
            \text{VPU~\cite{Perez-Villar_undated-ly}}                  &0.0215 &0.0799 &0.1014 &0.0118 &0.0493 &0.0612 &0.0813  \\
            \text{TangoUnchained}       &0.0161 &0.0519 &0.0679 &0.0150  &0.0750 &0.0900 &0.0790\\
            \text{haoranhuang\_njust}   &0.0138 &0.0515 &0.0652 &0.0110 &0.0479 &0.0589 &0.0621\\
            \method~(ours)                     &\textbf{0.0085} &\textbf{0.0305} &\textbf{0.0390} &0.0126 &\textbf{0.0465} &0.0590 &\textbf{0.0490} \\
            \bottomrule[1pt]
        \end{tabular}     
        }
    \end{center}
    \vspace{-4mm}
\end{table}

\subsection{SPEED+ Ablation Studies}\label{exp:speedplus_ablation}
To further investigate some factors of our method and dataset, we present several ablation experiments with a smaller pre-trained ResNet-34~\cite{He2015-hv} as the backbone of \method. 
Moreover, we reduce the size of the decoder layers significantly and use skip connections in each layer. % at each downsampling step of the backbone to the upsampling step of the decoder. 
We trained the model for $1\mathrm{e}6$ steps, using the same optimizer, batch size, and training regiment as in~\ref{subsubsec:exp:training}.
%Moreover, we used the refined object detection bounding boxes from~\ref{subsubsec:exp:object_detection} that we computed for the challenge for all ablation studies.
As input, we consider the refined bounding boxes that we computed for the challenge.
\begin{table}%[b]
    \begin{center}
        \caption{Ablation Study on SPEED+. \method~trained with different targets (normalized model coordinates $\bm{I_{\tilde{q}}}$, object mask $\bm{I}_o$, predicted error $\bm{I}_e$, and surface region segmentation $\bm{I}_r$) and pose estimates with error policies $\varepsilon$.}\label{tab:ablation_error} 
        \begin{tabular}[t]{P{0.01cm} P{0.01cm} P{0.01cm} P{0.01cm} P{0.4cm} P{0.5cm} P{0.5cm} P{0.5cm} P{0.5cm} P{0.5cm} P{0.5cm}}%{ccccc|cccccc}
            \toprule[1pt]
            \multicolumn{5}{c}{} & \multicolumn{3}{c}{lightbox}  &\multicolumn{3}{c}{sunlamp} \\ 
            \cmidrule(lr){6-8} \cmidrule(lr){9-11}
            $\bm{I_{\tilde{q}}}$ & $\bm{I}_o$ & $\bm{I}_e$ & $\bm{I}_r$ & $\epsilon$ & $e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$ &$e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$ \\
            \cmidrule(lr){1-11}
             \cmark & \cmark & \xmark & \xmark & 1.0   &0.037 &0.190 &0.228 &0.056 &0.387 &0.443  \\ [1ex] %baseline
            % \setlength\extrarowheight{10pt} 
             \cmark & \cmark & \cmark & \xmark & 1.0    &0.043 &0.187 &0.230 &0.070 &0.370 &0.442    \\   %model a epsilon=1.0
             \cmark & \cmark & \cmark & \xmark & $f_{\alpha}$   &0.036 &0.137 &0.174 &0.062 &0.266 &0.328    \\   [1ex]%model a epsilon=$f_{\alpha}$
             \cmark & \cmark & \cmark & \cmark & 1.0    &0.039 &0.161 &0.200 &0.063 &0.327 &0.390    \\ %model b epsilon=1.0
             \cmark & \cmark & \cmark & \cmark & $f_{\alpha}$  &\textbf{0.033} &\textbf{0.117} &\textbf{0.150} &\textbf{0.056} &\textbf{0.237} &\textbf{0.293}    \\ %model b epsilon=$f_{\alpha}$
            \bottomrule[1pt]
        \end{tabular}     
    \end{center}
\vspace{-5mm}
\end{table}

\subsubsection{Effectiveness of Error Prediction and Region Surfaces}\label{subsubsec:exp:error}
We demonstrate the effectiveness of the error prediction map $\widehat{\bm{I}}_e$ and surface region segmentations $\widehat{\bm{I}}_r$. To this end, we train three models: 
\begin{enumerate*}[label=(\roman*)]
    \item a baseline trained with $\bigl[\bm{I_{\tilde{q}}} \conc \bm{I}_o\bigr]$ only,
    \item an error-aware model trained with $\bigl[\bm{I_{\tilde{q}}} \conc \bm{I}_o \conc \bm{I}_e\bigr]$, and
    \item an error-aware, geometry-guided model trained with $\bigl[\bm{I_{\tilde{q}}} \conc \bm{I}_o \conc \bm{I}_e \conc \bm{I}_r\bigr]$.
\end{enumerate*} 
In this experiment, we focus on the quality of \gls{PnP} pose estimates given a set of 2D-3D correspondences and do not use the multi-hypothesis refinement step. 
Moreover, we want to explore the potential of using $\widehat{\bm{I}_e}$ to select different sets of correspondences. 
Hence, we employ an adaptive error threshold $f_{\alpha}(\bm{I}, \widehat{\bm{I}}_e) = \epsilon$ with ${\epsilon \in [1.0, 0.5, 0.3, 0.1, 0.075, 0.05, 0.025]}$ which chooses the best threshold given an image $\bm{I}$ and error map $\widehat{\bm{I}}_e$ statistics. 

As shown in Table~\ref{tab:ablation_error}, the employment of the error prediction output without considering it during \gls{PnP} ($\epsilon = 1.0$) leads to marginally worse metric scores.
However, considering the output to discard erroneous correspondences the performance improves.
%As shown in Table~\ref{tab:ablation_error}, if we add the extra error prediction target $\bm{I}_e$ but do not make any use of it $\epsilon = 1.0$, the performance gets marginally worse. However, if we utilize the error threshold $\epsilon=f_\alpha$, we can improve the accuracy significantly by discarding erroneous correspondences. 
% It should be noted, that $\epsilon=f_\alpha$ should not be considered an upper bound for the performance due to the fixed set of discrete values for $\epsilon$.
Adding the region segmentation as an auxiliary task further improves the results.

\subsubsection{Bridging the Domain Gap with Data Augmentations} \label{exp:speedplus_ablation:domain_gap}
%As mentioned in~\ref{subsubsec:exp:sim2real}, the domain gap was one of the central obstacles to overcome in the dataset. 
As mentioned the Sim2Real gap is one of the central obstacles of SPEED+.
Table~\ref{tab:ablation_augmentations} depicts five models confronted with different data augmentations of increasing severity to investigate their impact on bridging the domain gap.
\begin{table}[b]
    \begin{center}
        \vspace{-3mm}
        \caption{Ablation Study on Different Data Augmentations Configurations on SPEED+.}\label{tab:ablation_augmentations} 
        \begin{tabular}[t]{lcccccc}
            \toprule[1pt]
            & \multicolumn{3}{c}{lightbox}  &\multicolumn{3}{c}{sunlamp} \\ 
            \cmidrule(lr){2-4} \cmidrule(lr){5-7}
             & $e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$ &$e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$ \\
            \cmidrule(lr){1-7}
             \texttt{aug1}   &0.050 &0.216 &0.266 &0.122 &0.557 &0.679  \\
             \texttt{aug2}   &0.033 &0.121  &0.154  &0.058 &0.223  &0.281   \\
             \texttt{aug3}   &0.039 &0.138  &0.178  &0.034  &0.142  &0.176    \\
             \texttt{aug4}   &0.038 &0.128  &0.166  &\textbf{0.033}  &\textbf{0.133}  &\textbf{0.166}    \\
             \texttt{aug5}   &\textbf{0.032} &\textbf{0.112}  &\textbf{0.145}  &0.035  &0.138  &0.173   \\
            \bottomrule[1pt]
        \end{tabular}     
    \end{center}
    \vspace{-4mm}
\end{table}
%To investigate the impact of different data augmentation modules, we trained five models with a different set of data augmentations of increasing severity. 
Therefore, we train
\begin{enumerate*}[label=(\roman*)]
    \item \texttt{aug1} with blur, sharpen, emboss, additive gaussian noise, coarse dropout, linear contrast, add, invert, and multiply,
    \item \texttt{aug2} increases the number of augmentations and adds superpixels, edge detections, and dropout,
    \item \texttt{aug3} adds simplex noise, image corruptions (fog, snow), and space-specific augmentations (specular reflection, high contrast),
    \item \texttt{aug4} adds an extra brightness module and further image corruptions (saturation, contrast),
    \item at last \texttt{aug5} separates the different classes of augmentations into brightness, blur, corruptions, and general augmentations and samples these groups individually.\footnote{A full list of data augmentations can be found in our github repository (soon available)}
\end{enumerate*} 

One can observe the vital role of data augmentation in addressing the Sim2Real gap~\cite{Sundermeyer2018-fl}. 
% As shown in Table~\ref{tab:ablation_augmentations}, data augmentations play a vital role in bridging the domain gap. 
The results also indicate that test data-inspired augmentations, such as synthetic specular reflections, can drastically improve performance. % of a model. 
This can be especially seen in the \texttt{sunlamp} test set. 
At last, we see a difference between \texttt{aug4} and \texttt{aug5} which mainly differ in additional logic applying augmentations. 
% We hypothesize that adding too many augmentations of the same type can deteriorate the information contained in a sample to the point that no information remains. 
Model capacity plays an integral role in the selection and amount of data augmentations.
Hence, while this study might offer some clues, future research is required to find definitive factors in data augmentations for orbital perception.
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{images/dragons.png}
    \caption{Illustration of the three models $q_1, q_2$, and $q_3$ that we use in the model quality ablation study. (left) ground truth 3D CAD model; (middle) deformed model; (right) model of geometric primitives.}
    \label{fig:tudl_dragons}
    \vspace{-5mm}
\end{figure}

\subsubsection{Multi-hypothesis Testing and Refinement}
Finally, we demonstrate the effects of refinement, multi-hypotheses testing, and test set augmentations. 
For this study, the \texttt{aug5} model is used and %trained for the data augmentation ablation study is used. 
%We investigate several different scenarios:
several scenarios are investigated:
\begin{enumerate*}[label=(\roman*)]
    \item no refinement with $\epsilon=1.0$,
    \item single hypothesis refinement with $\epsilon=1.0$,
    \item multi-hypothesis refinement of all hypotheses with predicted, 
    \item ground truth, or %multi-hypothesis refinement of all available hypothesis with the ground truth segmentation $\bm{I}_o$, 
    \item no learned object segmentation map; %multi-hypothesis refinement without the learned confidence map,
    \item multi-hypothesis refinement with predicted segmentation and test set augmentations.
\end{enumerate*}

\begin{table}[b]
\vspace{-4mm}
    \begin{center}
        \caption{Ablation Study on Different Refinement Design Choices on SPEED+.}\label{tab:ablation_refinement} 
        \vspace{-2mm}

        \begin{tabular}[t]{lcccccc}
            \toprule[1pt]
            & \multicolumn{3}{c}{lightbox}  &\multicolumn{3}{c}{sunlamp} \\ 
            \cmidrule(lr){2-4} \cmidrule(lr){5-7}
              & $e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$ &$e_{\bm{t}}$ & $e_{\bm{R}}$ & $e_{\text{pose}}$ \\
            \cmidrule(lr){1-7}
             (i) no refinement   &0.038   &0.157  &0.194  &0.038  &0.192  &0.230  \\
             (ii) single $h_i$   &0.035 &0.133 &0.168 &0.036 &0.164 &0.200   \\
             (iii) multi $h_i$; $\widehat{\bm{I}}_o$   &0.030	&0.126	&0.156	&0.032 &0.159 &0.191    \\
             (iv) multi $h_i$; $\bm{I}_o$    &0.024 &0.104 &0.127 &0.024 &0.138 &0.162   \\
             (v) without $\bm{I}_o$    &0.037 &0.166 &0.203 &0.042 &0.257 &0.299 \\
             (vi) test set augs     &\textbf{0.019}	&\textbf{0.088}	&\textbf{0.107} &\textbf{0.022} &\textbf{0.097} &\textbf{0.120}    \\
            \bottomrule[1pt]
        \end{tabular}     
    \end{center}
\vspace{-4mm}
\end{table}

As shown in Table~\ref{tab:ablation_refinement}, the refinement significantly improves the prediction accuracy, even with only one hypothesis ((i) vs. (ii)). 
This improvement increases even further if multiple-hypothesis refinement is applied.
However, the refinement performance directly correlates with the quality of the segmentation map ((iii) vs. (iv)).
At last, the (vi) test set augmentations drastically improve the accuracy by another $35\%$ on average. 
Besides the fact, that we refine four times more hypotheses, the ensemble significantly improves the quality of the segmentation mask $\widehat{\bm{I}}_o = \frac{1}{4}\sum_{i=1}^4 \widehat{\bm{I}}_{o,i}$. 

\subsection{Experiments on TUD-L Dataset}
In the last experiment, we demonstrate the impact of object model quality on correspondence-based pose estimation methods. 
More importantly, we showcase how \method~ is still able to make accurate predictions.
For this experiment, we use the TUD-L dataset~\cite{hodan_bop_2018} -- similar to SPEED+ -- contains challenging lighting effects on the real test set. 
\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{figures/tudl_ablation.pdf}
    \caption{Results of the ablation study on TUD-L. The graph shows the fraction of samples within ADD thresholds. The larger the area under the curve, the better the performance. Methods using $\varepsilon=f_\alpha$ clearly outperform the other methods and can accurately estimate the pose.}\label{fig:tudl_ablation}
\vspace{-4mm}
\end{figure}

To investigate the impact of the object's model quality, we create two approximate object models (see Fig.~\ref{fig:tudl_dragons}). 
The first model $q_1$ acts as a baseline and is not changed. 
For the second model $q_2$, we taper the model using Blenders \texttt{Taper} modifier. 
Such a model deterioration could arise if the 3D model used for learning is generated using 3D shape reconstruction. 
At last, the model $q_3$ is approximating the real object with coarse geometric primitives which corresponds to a reduced modelling effort. 
To quantify the model deformity, we calculate distances from vertices in $q_1$ to the nearest neighbor vertices in $q_2$ and $q_3$. On average, $q_2$ has a distance error of $5 \mathrm{mm}$ per model point and model $q_3$ of $7 \mathrm{mm}$.

We train the models purely on synthetic images generated with BlenderProc~\cite{denninger_blenderproc_nodate} with the same training parameters as in \ref{exp:speedplus_ablation} and basic data augmentations \texttt{aug2}. For each object quality we train two models,
\begin{enumerate*}[label=(\roman*)]
    \item a \textbf{baseline} with $\bigl[\bm{I_{\tilde{q}}} \conc \bm{I}_o\bigl]$, and
    \item an \textbf{error-aware} model with $\bigl[\bm{I_{\tilde{q}}} \conc \bm{I}_o \conc \bm{I}_e\bigl]$.
\end{enumerate*} 

We use the ADD metric for evaluation given the ground truth model. It measures whether the average distances of the transformed model points deviate less than a certain threshold of the object's diameter. As shown in Fig.~\ref{fig:tudl_ablation}, error-aware models that use the adaptive error threshold $\epsilon=f_\alpha$ perform significantly better than the other methods. They achieve an ADD$(0.1\mathrm{d})$, less than $10$\% of their diameter, for between $65$\% and $70$\% of the test images. In contrast, the baseline models achieved an ADD$(0.1\mathrm{d})$ between $43$\% and $46$\%. Surprisingly, error-aware models with $\epsilon=1.0$ significantly outperform the baseline methods with an ADD$(0.1\mathrm{d})$ between $45$\% and $55$\% without using the estimated errors.

