%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\input{math_commands.tex}
\usepackage{booktabs}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{ANEDL: Adaptive Negative Evidential Deep Learning \\ for Open-Set Semi-Supervised Learning}
\author{
    Yang Yu \textsuperscript{\rm 1},
    Danruo Deng \textsuperscript{\rm 1},
    Furui Liu \textsuperscript{\rm 2},
    Yueming Jin \textsuperscript{\rm 3},\\
    Qi Dou \textsuperscript{\rm 1},
    Guangyong Chen \textsuperscript{\rm 2},
    Pheng-Ann Heng \textsuperscript{\rm 1},
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}The Chinese University of Hong Kong,
    \textsuperscript{\rm 2}Zhejiang Lab,
    \textsuperscript{\rm 3}National University of Singapore\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % 1900 Embarcadero Road, Suite 101\\
    % Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    
    yangyu@cse.cuhk.edu.hk
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi


%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrated empirically, our proposed method outperforms existing state-of-the-art methods across four datasets. 
% Our code will be released once this paper is accepted.
\end{abstract}

\section{Introduction}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.47\textwidth]{edl_detector2.pdf}
% 	% \vspace{-1mm}
% 	\caption{The ANEDL we introduce can quantify different types of uncertainty, including epistemic uncertainty due to lack of knowledge, and aleatoric uncertainty from the complexity of samples in distribution. Based on these uncertainties, we design different metrics for self-training and inference. (Images with blue and yellow backgrounds indicate inliers and outliers, respectively.)}
%  % The EDL detector we introduce can quantify different types of uncertainty and can be used to design more targeted training and inference strategies.
% 	\label{fig:edl_detector}
%  \vspace{-2mm}
% \end{figure}

\begin{figure*}[t]
% \vspace{-2mm}
	%\setlength{\abovecaptionskip}{0.1cm}	
	%\setlength{\belowcaptionskip}{-0.3cm}
	\centering
 \includegraphics[width=0.98\textwidth]{anedl2.pdf}
	%\vspace{-4mm}
	\caption{\textbf{Overview of our proposed Adaptive Negative Evidential Deep Learning (ANEDL).} (a) The framework of ANEDL consists of a shared feature extractor, a Softmax head and an EDL head. EDL head is used to detect outliers, while Softmax head is used to classify. To effectively leverage the information of inlier and outlier from unlabeled samples, we introduce Negative Optimization to explicitly regularize our EDL detector to output low evidence values for uncertain classes, while proposing adaptive loss weights to encourage the detector to pay more attention to these uncertain classes. (b) Three cases of unlabeled samples. Our model can quantify different types of uncertainty, including epistemic uncertainty due to lack of knowledge, and aleatoric uncertainty due to the complexity of samples from the distribution. Among them, only samples with confident predictions are used jointly with labeled samples to train the classification head.}
	% \vspace{-2mm}
	\label{fig:anedl}
\end{figure*}

Semi-supervised learning (SSL) has recently witnessed significant progress by propagating the label information from labeled data to unlabeled data \cite{berthelot2019mixmatch, xu2021dash, wang2022freematch, zheng2022simmatch}. Despite the success, SSL methods are deeply rooted in the closed-set assumption that labeled data, unlabeled data and test data share the same predefined label set. In reality \cite{yu2020multi}, such an assumption may not always hold as we can only accurately control the label set of labeled data, while unlabeled and test data may include outliers that belong to the novel classes that are not seen in labeled data. During self-training, imprecisely propagating label information to these outliers shall interfere with model learning and lead to performance degradation. To this end, open-set semi-supervised learning (Open-set SSL) \cite{yu2020multi} has been emerging to tackle the problem. It first detects out-of-distribution (OOD) samples as outliers and then recognizes in-distribution (ID) samples as belonging to the classes from the predefined label set.
%\furui{should write it more formally. This version reads somewhat like informal conclusions}

Existing Open-set SSL methods \cite{guo2020safe, chen2020semi} generally consist of an outlier detector and a classifier. To avoid propagating wrong label information to outliers during SSL training of the classifier, they first detect outliers and then optimize SSL loss only for unlabeled instances taken as inliers. To detect outliers, they aim to estimate confidence score of a sample being an inlier (reversely, uncertainty score for outliers). For example, T2T \cite{huang2021trash} proposes to detect outliers by cross-modal matching over binary detector.
% classifier. 
OpenMatch \cite{saito2021openmatch} deploys a one-vs-all (OVA) classifier \cite{saito2021ovanet} as outlier detector to output the confidence score of considering a sample as an inlier and reject outliers with confidence score below a threshold for self-training in SSL. With outliers rejected, they can improve recognition accuracy of SSL methods. 



We identify two main limitations in current state-of-the-art literature to tackle this task.
%However, several limitations hinder them from further promoting outlier detection performance and recognition accuracy. 
First, most previous works estimate confidence scores based on binary likelihoods generated from binary detectors activated by Softmax function. Such softmax-based networks only provide a point estimation of predicted class probabilities and 
% cannot quantify different types of uncertainties \YJ{for different classes in the multi-class recognition task?} 
cannot quantify whether the uncertainty stems from the lack of knowledge of outliers (epistemic uncertainty) or from the complexity of the samples within the distribution (aleatoric uncertainty) \cite{malinin2018predictive}.
% \cite{sensoy2018evidential, malinin2018predictive}. 
% which cannot distinguish \textit{aleatoric uncertainty} and \textit{epistemic uncertainty}. 
Moreover, when we tackle a K-way classification problem with a large K, the binary detectors are less robust to identify outliers from such a complex dataset that contains multi-class information \cite{carbonneau2018multiple}.
One advanced method, evidential deep learning (EDL) \cite{sensoy2018evidential} can explicitly quantify the classification uncertainty corresponding to the unknown class, by treating the network's output as evidence for parameterizing the Dirichlet distribution according to subjective logic \cite{josang2016subjective}. 
Compared with Softmax which only provides probabilistic comparison values between classes, EDL models second-order probability and uncertainty for each class.
In this work, we take the \emph{first} step to introduce EDL into the outlier detector. As shown in Figure \ref{fig:anedl}, EDL enables a multinomial detector to directly output 
% uncertainty and 
evidence for each class, which can alleviate the insufficient scalability of existing binary detectors. Moreover, different types of uncertainties can be distinguished by the output vector,
% concentration parameters, 
so we propose to design different uncertainty metrics for self-training and inference.
% more targeted training and inference strategies,  
Note that though EDL shows an impressive performance in uncertainty quantification, we empirically find its deficiency in learning representation preferred by classification tasks and consequent low performance in classification accuracy. Therefore, we propose to take advantage of both Softmax and EDL, by keeping Softmax for representation learning. 

The other limitation of prior methods is that they do not effectively leverage the information contained in unlabeled data. The detector should output low confidence scores for outliers, but this information is underutilized for model regularization, which may result in suboptimal performance \cite{malinin2018predictive}.
In this regard, we propose \emph{adaptive negative optimization} tailored for unlabeled data, where the negative optimization is to explicitly regulate our EDL detector to output K low evidence values for a given outlier sample. However, negative optimization can not directly apply to inliers in unlabeled data.
Therefore, we derive an adaptive loss weight for encouraging the model to separately treat inliers and outliers in the unlabeled data in the learning process. It can largely avoid the interference between the ID and OOD features as well as the leading problem of uncertainty availability reduction.
Specifically, we achieve this by using the Fisher information matrix (FIM) introduced by $\mathcal{I}$-EDL \cite{deng2023uncertainty} to identify the amount of information for each class of unlabeled samples, while imposing smaller constraints on the confident classes with high evidence. Therefore, such optimization can sufficiently compress evidence values of unlabeled outliers without interfering with the learning of unlabeled inliers. 

To our knowledge, we are the first to exploit evidential deep learning in Open-set SSL and propose a novel Adaptive Negative Evidential Deep Learning (ANEDL) framework. Our main contributions can
be summarized as follows:
\begin{itemize}
\item We introduce EDL as an outlier detector for Open-set SSL, which enables quantification of different types of uncertainty and design of different uncertainty metrics for self-training and inference, respectively.
\item We propose a novel adaptive negative optimization strategy, which tailors EDL on the unlabeled dataset containing both inliers and outliers.
\item Extensive experiments on four public datasets, including CIFAR-10, CIFAR-100, ImageNet-30, and Mini-ImageNet, show that our method outperforms state-of-the-art methods in different settings.
\end{itemize}

\section{Related Works}

\subsection{Semi-supervised Learning}
Semi-supervised learning (SSL) aims to ease deep networks' reliance on massive and expensive labeled data by utilizing limited labeled data together with a large amount of unlabeled data. Existing works can be generally categorized into two paradigms: consistency regularization and pseudo labeling \cite{lee2013pseudo}. Consistency regularization assumes that the network should be invariant to different perturbations of the same instance. Pseudo-labeling takes confidently pseudo-labeled unlabeled data as labeled data to train the network. And many pseudo labeling methods like FixMatch \cite{sohn2020fixmatch}, FlexMatch \cite{zhang2021flexmatch} and AdaMatch \cite{berthelot2021adamatch} focus on how to select confidently pseudo-labeled data. Although pseudo-labeling methods have achieved tremendous success in SSL, they are deeply rooted in the close-set assumption and can not be directly applied to open-set SSL \cite{yu2020multi, saito2021openmatch}. Because noisy pseudo-labeled outliers may degrade the performance of self-supervised training.
% On the other hand, consistency regularization which gets rid of the close-set assumption can be kept for open-set SSL. Following OpenMatch, we also apply soft consistency regularization to our outlier detector. 
On the other hand, OpenMatch \cite{saito2021openmatch} proposes that soft consistency regularization by minimizing the distance between its predictions on two augmentations of the same image can help SSL methods get rid of the close-set assumption can be kept for open-set SSL. Following OpenMatch, our method also applies soft consistency regularization to the outlier detector.

\subsection{Open-set Semi-supervised Learning}
Open-set SSL \cite{guo2020safe, chen2020semi} aims to detect outliers while categorizing inliers into correct close-set classes during training and inference. Existing open-set SSL methods recognize the weakness of SSL methods in terms of detecting outliers and they develop an extra outlier detector to exclude outliers from self-supervised training. Their outlier detectors are based on binary classifier activated by softmax function. For example, MTC \cite{yu2020multi} deploys a binary classifier to predict the probability of the instance belonging to outliers and updates the network parameters and the outlier score alternately. OpenMatch \cite{saito2021openmatch} takes one-vs-all (OVA) classifier as its outlier detector and proposes to apply soft consistency regularization to the outlier detector. An OVA classifier is composed of K binary classifiers and each classifier predicts the probability of the sample belonging to a specific class. 
% We identify two limitations of binary classifier for outlier detection in open-set SSL. First, softmax function is notorious to be over-confident. Second, when the class number is large, the modality that each singleton (e.g. positive and negative class) of binary classifier should recognize will be very complicated. Therefore, we adopt EDL as our outlier detection method, with a better uncertainty quantification capacity compared with softmax. On the other hand, EDL gets rid of binary classifer and can achieve good performance even with large class number.
However, softmax-based networks are notorious for inflating the probabilities of predicted classes \cite{szegedy2016rethinking, guo2017calibration, wilson2020bayesian}, and binary detectors perform poorly on tasks with a large number of classes \cite{carbonneau2018multiple}. To address these limitations, we adopt evidential deep learning (EDL) \cite{sensoy2018evidential} as the outlier detector for Open-set SSL.

\subsection{Evidential Deep Learning}
Evidential neural networks \cite{sensoy2018evidential} model the outputs as evidence to quantify belief masses and uncertainty based on the Dempster-Shafer Theory of Evidence (DST) \cite{sentz2002combination} and subjective logic \cite{josang2016subjective}. Similar to this work, \cite{malinin2018predictive} propose Prior Networks that explicitly consider the distributional uncertainty to quantify the mismatch of ID and OOD distribution. Compared with the standard neural network classifiers direct output the probability distribution of each sample, EDL obtains the density of classification probability assignments by parameterizing the Dirichlet distribution. Therefore, EDL can use the properties of Dirichlet distribution to distinguish different types of uncertainties, which shows an impressive performance in uncertainty quantification and is widely used in various applications. For example, \cite{zhao2020uncertainty} proposes a multi-source uncertainty framework combined with DST for semi-supervised node classification with GNNs. \cite{soleimany2021evidential} introduces evidential priors over the original Gaussian likelihood function to model the uncertainty of regression networks. \cite{bao2021evidential, bao2022opental} propose a general framework based on EDL for Open Set Recognition (OSR) and Open Set Temporal Action Localization (OSTAL), respectively. Compared with previous efforts, our work is the first to exploit EDL to Open-set SSL. For Open-set SSL, in addition to needing to distinguish unknown and known classes at test time, more importantly is how to better utilize the unlabeled data during training to learn the features of known classes.

\section{Method}

\subsection{Problem Setting}
Open-set semi-supervised learning (Open-set SSL) aims to learn a classifier by using a set of labeled and unlabeled datasets. Unlike traditional semi-supervised learning (SSL), the unlabeled dataset of Open-set SSL contains both ID and OOD samples, i.e., inliers and outliers. More specifically, for a K-way classification problem, let $\mathcal{D}_{l}=\{(\boldsymbol{x}^l_j, y^l_j)\}_{j=1}^{N_l}$ be the labeled dataset, containing $N_l$ labeled samples randomly generated from a latent distribution $\mathcal{P}$. The labeled example $\boldsymbol{x}^l_j$ can be classified into one of K classes, meaning that its corresponding label $y^l_j \in \{c_1, \cdots, c_K\}$. Let $\mathcal{D}_{u}= \{\boldsymbol{x}^i_j\}_{j=1}^{N_i} \cup \{\boldsymbol{x}^o_j\}_{j=1}^{N_o}$ be the unlabeled dataset consisting of $N_i + N_o$ examples with labels either from the K known classes (inliers) or never been seen (outliers). The goal of Open-set SSL is to detect outliers while classifying inliers into the correct class.

\subsection{Adaptive Negative Evidential Deep Learning}

% \noindent\textbf{Network Overview} 
\noindent\textbf{Approach Overview.} 
As shown in Figure \ref{fig:anedl}, our proposed framework contains three components: 
% (1) an EDL head $D(\cdot)$ that quantifies uncertainty by outputting K evidence values to detect outliers in unlabeled data;
% % detects outliers \dr{in unlabeled data} by directly outputting K evidence values to quantify  belief mass or uncertainty; 
% (2) a Softmax head $C(\cdot)$
% % categorizes inliers by outputting K probabilities; 
% classifies all inlier samples by outputting K-way probabilities, including labeled data and inliers in unlabeled data; 
% (3) a feature extractor $F(\cdot)$ shared by outlier detector and classifier.
% the uncertainty head and the classification head. 
% To use our model, the EDL head first predicts evidence vector and a low-evidence sample will be detected as an outlier. Conversely, a high-evidence sample will be taken as an inlier and categorized into pre-defined K categories by the classification head. 
(1) a shared feature extractor $F(\cdot)$;
(2) a Softmax head $C(\cdot)$ that classifies samples by outputting K-way probabilities;
(3) an EDL head $D(\cdot)$ that quantifies uncertainty by outputting K evidence values to detect outliers in unlabeled data.
To adopt our framework to Open-set SSL, we propose an Adaptive Negative Optimization strategy to train EDL head by using all labeled and unlabeled samples. We introduce negative optimization to explicitly regularize our EDL detector to output low evidence values for uncertain classes, while deriving an adaptive loss weight to encourage the detector to pay more attention to these uncertain classes. For unlabeled data with confident prediction, we will use them jointly to train the Softmax classifier with labeled data.

\noindent\textbf{Evidential Outlier Detector.}
% Traditional classification networks use softmax function to predict class probabilities and achieve impressive classification performance. However, such softmax-based networks only provide a point estimation of probabilities of predicted classes and are not able to quantify different types of uncertainties. To tackle the limitations of softmax, evidential deep learning (EDL) proposes to integrate deep classification networks into the evidence framework of 
% %the Dempster-Shafer Theory (DST) of evidence and 
% subjective logic, to jointly predict classification probabilities and quantify uncertainty. 
Traditional outlier detectors for Open-set SSL use Softmax activation function to predict confidence scores and achieve impressive performance. However, such softmax-based detectors only provide a point estimation of outlier probabilities, unable to quantify different types of uncertainty. To tackle the limitations of Softmax, we introduce evidential deep learning (EDL) \cite{sensoy2018evidential} to our framework as an outlier detection module. EDL proposes to integrate deep classification networks into the evidence framework of subjective logic \cite{josang2016subjective}, to jointly predict classification probabilities and quantify uncertainty.
Specifically, for K-way classification, EDL uses a network $g(\boldsymbol{\theta})$ to calculate its evidence vector $\boldsymbol{e}= g(\boldsymbol{x}|\boldsymbol{\theta})$. Note that an activation function, e.g. Relu or Softplus, is on top of $g(\boldsymbol{\theta})$ to guarantee non-negative evidence. Then EDL correlates its derived evidence vector $\boldsymbol{e}\in \mathbb{R}_{+}^{K}$ to the concentration parameter $\boldsymbol{\alpha} \in \mathbb{R}_{+}^{K}$ of Dirichlet distribution $D(\boldsymbol{p}|\boldsymbol{\alpha})$ with the equality ${\alpha}_i = e_i + 1$. 
Dirichlet distribution parameterized by $\boldsymbol{\alpha}$ models the density of classification probability assignments and uncertainty.
% The expected probability of the $k^{th}$ class can be expressed by the mean of Dirichlet distribution ${\hat{p}}_k = \frac{{\alpha}_k}{S} $ and the predictive uncertainty can be deterministically computed as $u = \frac{K}{S} $, where $S$ is the Dirichlet strength and defined as $S=\sum_{k=1}^K {\alpha}_k$. 
The expected probability of the $k^{th}$ class and uncertainty can be derived by
$$
{\hat{p}}_k = \frac{{\alpha}_k}{\alpha_{0}} \quad \text{and} \quad u = \frac{K}{\alpha_{0}}, 
$$
where 
% $S$ is the precision of Dirichlet distribution and defined as 
$\alpha_{0}=\sum_{k=1}^K {\alpha}_k$. 
Higher values of $S$ lead to %sharper and 
more confident distributions.
% And ${\alpha}_k$ represents the evidence that a sample belonging to $k^{th}$ class collected from data. 
Given a sample, ${\alpha}_k$ is incremented to update the Dirichlet distribution of the sample when evidence of $k^{th}$ class is observed.
%Therefore, EDL can jointly predict classification probabilities and quantify uncertainty with a deterministic network.

%Given a sample $x_i$ and its one-hot label $y_i$, evidential network predicts concentration parameters ${\alpha}_i$ of corresponding Dirichlet distribution. 

\noindent\textbf{Joint Optimization with Softmax and EDL.} Although EDL-related works \cite{sensoy2018evidential, deng2023uncertainty} are capable of jointly classifying and quantifying uncertainty, we empirically find its deficiency in optimizing a network to learn representation for classification. 
That is, the classification performance of the evidential network is lower than that of a network activated by Softmax and optimized with only cross-entropy loss.
% That is to say the network optimized network with only EDL loss will have a lower classification performance than that activated by softmax and optimized with only cross-entropy loss. 
In this way, we propose to take the strength of both Softmax and EDL and jointly optimize their losses. In our proposed framework, Softmax head takes charge of learning representation and predicting classification probabilities, while EDL is responsible for quantifying uncertainty. During training, with EDL excluding outliers, we adopt an SSL method (i.e., FixMatch following OpenMatch \cite{saito2021openmatch}) to our Softmax head to enhance representation quality and classification accuracy. We also utilize counterfactual reasoning and adaptive margins proposed in DebiasPL \cite{wang2022debiased} to remove the bias of pseudo label in FixMatch.

% \subsection{Adaptive Negative EDL}
\noindent\textbf{Adaptive Negative Optimization.}
The core challenge of Open-set SSL is that unlabeled data contains novel categories that are not seen in labeled training data.
% i.e., outliers. 
To enhance the separation between inliers and outliers, detectors should output low confidence scores for outliers, but traditional binary detectors do not sufficiently leverage this information for model regularization, which may result in suboptimal performance \cite{malinin2018predictive}.
% Since EDL-related methods learn optimal parameters by maximizing the expected likelihood of the observed labels, directly training EDL outlier detector with \yy{inaccurate pseudo-labels} would mislead the learning of the evidence, reducing the availability of uncertainty. 
% If these outliers cannot be detected, it can severely impair the performance of the detector.  
Inspired by negative learning \cite{ishida2017learning}, we propose Adaptive Negative Optimization (ANO) to avoid providing misinformation to the EDL detector by focusing on uncertain classes. 
% In particular, we derive an adaptive loss weight to encourage EDL detector to separately handle inliers and outliers in unlabeled data during the learning process. For the identified outlier samples, we introduce the negative optimization to explicitly regulate our detector to output K low evidence values.
In particular, we introduce the negative optimization to explicitly regulate our detector to output low evidence values for uncertain classes. Meanwhile, adaptive loss weights are proposed to encourage EDL detectors to pay more attention to uncertain classes during the learning process.
% For the identified outlier samples, we introduce the negative optimization to explicitly regulate our detector to output K low evidence values.


More specifically, we adopt the Fisher information matrix (FIM) introduced by $\mathcal{I}$-EDL \cite{deng2023uncertainty} to identify the amount of information contained in each class of each sample, while imposing explicit constraints on the informative uncertainty classes. 
% We adopt the Fisher Information Matrix (FIM) introduced by $\mathcal{I}$-EDL to identify the amount of information contained in each class of each sample, where class labels with higher evidence correspond to less Fisher information. More specifically, we constrain the uncertain classes of unlabeled samples by the KL divergence term, making their distribution more uniform. The objective function is defined as
$\mathcal{I}$-EDL proves that the evidence of a class exhibits a negative correlation with its Fisher information, i.e., the class with more evidence corresponds to less Fisher information, and shows an impressive performance in uncertainty quantification \cite{deng2023uncertainty}. Therefore, we propose to use FIM as an indicator to distinguish inliers from outliers to adaptively regulate our model to pay more attention to uncertain classes.
%\furui{need to think about the rigorousness of the statement that "evidence of a class is negatively correlated with its Fisher information" better add some citations}
For those uncertain classes with less evidence in unlabeled samples, we impose explicit constraints to output K low evidence values through the Kullback-Leibler (KL) divergence term. 
% Following $\mathcal{I}$-EDL, we use the inverse of the FIM ($\mathcal{I}(\boldsymbol{\alpha})^{-1}$) as the variance of the generative distribution of $\boldsymbol{y}$.
The objective function for unlabeled data is defined as
\begin{equation}
\begin{aligned}
\label{obj_unlabel}
% \min_{\boldsymbol{\theta}} \ &\mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}_{L}} \left[ - \log \mathbb{E}_{\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})} [ \mathcal{N}(\boldsymbol{y}|\boldsymbol{p}, \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1}) ]\right] \\
% &\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_{U}} \left[ D_{KL}( \mathcal{U} \Vert \mathcal{N}( \mathbb{E}_{Dir(\boldsymbol{\alpha})}[\boldsymbol{p}], \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1})\right]
\min_{\boldsymbol{\theta}} \ \mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_{u}} \left[ D_{KL}( \mathcal{U} \Vert \mathcal{N}(\mathbb{E}_{Dir(\boldsymbol{\alpha})}[\boldsymbol{p}], \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1})\right]
\end{aligned}
\end{equation}
where $\mathcal{U}=\mathcal{N}(\boldsymbol{1}/K, \lambda^2 \mI)$, $\boldsymbol{\alpha} = g(\boldsymbol{x}|\boldsymbol{\theta}) + \boldsymbol{1}$, and $\mathcal{I}(\boldsymbol{\alpha})$ denotes the FIM of $Dir(\boldsymbol{\alpha})$ (the closed-form expression is provided in Appendix A.1). 
Eq.(\ref{obj_unlabel}) can be understood through the lens of probabilistic graphical models, where the observed labels $\hat{\boldsymbol{y}}$ are generated from the Dirichlet distribution $\boldsymbol{p}$ with its parameter $\boldsymbol{\alpha}$ calculated by passing the input sample $\boldsymbol{x}$ through the network. Since our unlabeled data contains both novel and known classes, we assume $\hat{\boldsymbol{y}} \sim \mathcal{N}(\boldsymbol{p}, \sigma^2 \mathcal{I}(\boldsymbol{\alpha})^{-1} )$ to make the model pay more attention to uncertain classes. For the target variable $\boldsymbol{y}$, we expect its Dirichlet distribution $\boldsymbol{p}$ to obey a uniform distribution, i.e. $\boldsymbol{y} \sim \mathcal{N}(\boldsymbol{1}/K, \lambda^2 \mI)$. Finally, we use KL divergence to constrain the observed labels $\hat{\boldsymbol{y}}$ and the target variable $\boldsymbol{y}$ to obey the same distribution. Due to the analyzable nature of the Dirichlet distribution, Eq.(\ref{obj_unlabel}) can be simplified as
% Given training set $\mathcal{D}_{u} = \{\boldsymbol{x}_i\}_{i=1}^{N_u}$, by the definition of KL divergence, Eq. (\ref{obj_unlabel}) can be simplified as 
% According to the definition of KL divergence, the loss function for an unlabeled sample $\boldsymbol{x}_j \in \mathcal{D}_u$ in Eq. (\ref{obj_unlabel}) can be simplified as 
\begin{equation}
\begin{aligned}
\label{obj_nedl}
& \mathcal{L}_{j}^{\text{N-EDL}} = \sum_{k=1}^{K}  (\frac{1}{K} - \frac{\alpha_{j k}}{\alpha_{j 0}} )^2 \psi^{(1)}(\alpha_{j k}) - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_j)|,
% & \frac{1}{N_u} \sum_{i=1}^{N_u} \left( \sum_{j=1}^{K}  (\frac{1}{K} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 \psi^{(1)}(\alpha_{i j}) - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_i)| \right),
\end{aligned}
\end{equation}
where 
% $$
% \log |\mathcal{I}(\boldsymbol{\alpha}_j)| = \sum_{k=1}^{K} \log \psi^{(1)}(\alpha_{j k}) + \log (1 - \sum_{k=1}^{K} \frac{\psi^{(1)}(\alpha_{j0})}{\psi^{(1)}(\alpha_{j k})}),
% $$
${\alpha}_{j0} = \sum_{k=1}^{K}{\alpha}_{j k}$, and $\psi^{(1)}(\cdot)$ represents the \textit{trigamma} function, defined as $\psi^{(1)}(x) = d \psi(x) / dx = d^2 \ln \Gamma(x) / dx^2 $ (See Appendix A.2 for the detailed derivation process). We name the above weighted loss function as adaptive negative EDL loss. Since $\psi^{(1)}(x)$ is a monotonically decreasing function when $x > 0$, class labels with less evidence would be subject to greater penalties to achieve a flatter output. Conversely, once a certain class of evidence is learned, the weight will be reduced (see Fig.\ref{fig:anedl}), so that the inlier features in the unlabeled dataset do not interfere too much with the learning of outlier features. We also use labeled dataset to enhance the learning of inlier features. We follow $\mathcal{I}$-EDL \cite{deng2023uncertainty} to impose constraints on the uncertain class of labeled samples, outputting a sharp distribution that fits the ground truth.
% $$
% \min_{\boldsymbol{\theta}} \ \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}_l} \left[-\log \mathbb{E}_{\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})} [ \mathcal{N}(\boldsymbol{y}|\boldsymbol{p}, \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1}) ]\right].
% $$
The loss function for an labeled sample $(\boldsymbol{x}^l_j, \boldsymbol{y}^l_j) \in \mathcal{D}_l$ can be expressed as:
\begin{equation}
\begin{aligned}
\label{obj_pedl}
% L_i^{\text{P-EDL}} = &\sum_{j=1}^{K} \left( (y_{i j} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 + \frac{p_{i j}( 1 - p_{i j})}{\alpha_{i 0} + 1} \right) \psi^{(1)}(\alpha_{i j}) \\
% & - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_i)|), \\
\mathcal{L}_{j}^{\text{P-EDL}}=&\sum_{k=1}^{K} \left( (y_{j k} - \frac{\alpha_{j k}}{\alpha_{j 0}} )^2 + \frac{\alpha_{j k}( \alpha_{j 0} - \alpha_{j k})}{\alpha_{j 0}^2(\alpha_{j 0} + 1)} \right) \psi^{(1)}(\alpha_{j k}) \\
& - \lambda_2 \log |\mathcal{I}(\boldsymbol{\alpha}_j)|,
% \min_{\boldsymbol{\theta}} \frac{1}{N_l} \sum_{i=1}^{N_l} &( \sum_{j=1}^{K} \left( (y_{i j} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 + \frac{\alpha_{i j}( \alpha_{i 0} - \alpha_{i j})}{\alpha_{i 0}^2(\alpha_{i 0} + 1)} \right) \psi^{(1)}(\alpha_{i j}) \\
% &- \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_i)|),
\end{aligned}
\end{equation}
where $\boldsymbol{y}^l_j$ denotes one-hot encoded ground-truth of $\boldsymbol{x}^l_j$.

% where $p_{ij} = \alpha_{i j}/\alpha_{i 0}$.

\noindent\textbf{Strengthened KL Loss.} 
% Shared by EDL and $\mathcal{I}$-EDL, $\mathcal{L}_{i}^{\text{KL-ORI}}$ aims to reduce the evidence of non-labeled classes and leaves the evidence corresponding to labeled class ignored. To help increase the evidence of labeled class, we propose to utilize KL-divergence to force our predicted Dirichlet distribution $Dir(p_i | {\alpha}_i)$ to approach a new target Dirichlet distribution $Dir(p_i | \beta)$, where ${\beta} = [1, \cdots, P, \cdots, 1] \in \mathbb{R}^{K}_+$ and $P$ is a large target evidence value, e.g. 100. 
% We define our EDL loss function for labeled data as:
%Previous EDL methods use KL divergence loss only to shrink misleading evidence by changing ${\alpha}_i$ to $\hat{\alpha}_i = {\alpha}_i \odot(1 - y_i) + y_i$ and setting target Dirichlet Distribution as $Dir(p_i | \boldsymbol{1}).
KL divergence loss $\mathcal{L}_{j}^{\text{KL-ORI}} $ as part of classical EDL loss, aims to penalize evidence for misleading classes that a sample does not belong to: %that \YJ{do not fit the training data}:
\begin{equation}
\label{obj_kl_ori}
\mathcal{L}_{j}^{\text{KL-ORI}} 
= D_{KL}(Dir(\boldsymbol{p}_j | \hat{\boldsymbol{\alpha}_j}) \Vert Dir(\boldsymbol{p}_j | \boldsymbol{1})).
\end{equation}
where $\hat{\boldsymbol{\alpha}}_j = \boldsymbol{\alpha}_j \odot(1 - \boldsymbol{y}^l_j) + \boldsymbol{y}^l_j$.
However, $\mathcal{L}_{j}^{\text{KL-ORI}} $ only shrinks misleading evidence but leaves non-misleading evidence ignored. To help enhance non-misleading evidence, we propose to utilize a strengthened KL-divergence to force our predicted Dirichlet distribution $Dir( \boldsymbol{p}_j | {\boldsymbol{\alpha}}_j)$ to approach a new target Dirichlet distribution $Dir(\boldsymbol{p}_j | \boldsymbol{\beta})$. In particular, we introduce a new KL divergence term as follows,
\begin{equation}
%\begin{aligned}
\label{obj_kl}
\mathcal{L}_{j}^{\text{KL}} 
= D_{KL}(Dir(\boldsymbol{p}_j | \boldsymbol{\alpha}_j) \Vert Dir(\boldsymbol{p}_j | \boldsymbol{\beta})). \\
%= & \log \Gamma(\alpha_{j 0}) - \sum_{k=1}^{K} \log \Gamma(\alpha_{j k})  - \log \Gamma(\beta_{j 0}) + \sum_{k=1}^{K} \log \Gamma(\beta_{j k}) \\
%& + \sum_{k=1}^{K}(\alpha_{j k} - \beta_{k}) \left[\psi(\alpha_{j k}) - \psi(\alpha_{j 0}) \right].
%\end{aligned}
\end{equation}
For labeled data, we set $\boldsymbol{\beta} = [1, \cdots, P, \cdots, 1] \in \mathbb{R}^{K}_+$, where $P$ is a large target evidence value for ground-truth, e.g. 100. 
% Our strengthened KL-Divergence also applies to unlabeled data by using 
For unlabeled data, we set $\boldsymbol{\beta} = \boldsymbol{1}$ as the target distribution. The detailed closed-form expression of $\mathcal{L}_{j}^{\text{KL}}$ can be found in Appendix A.3.
%To help learn the evidence of both labeled and unlabeled data, 

% \begin{equation}
% \label{obj_final}
% \mathcal{L}_{i}^{\mathcal{P}\text{-EDL}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{I}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\text{KL}},
% \end{equation}

% where

% \begin{equation}
% \begin{aligned}
% \label{obj_final_kl}
% \mathcal{L}_{i}^{\text{KL}} 
% = & D_{KL}(Dir(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert Dir(\boldsymbol{p}_i | \boldsymbol{\beta})) \\
% = & \log \Gamma(\sum_{j=1}^{K} \alpha_{i j}) - \log \Gamma(\sum_{j=1}^{K} \beta_{j}) \\
% &+ \sum_{j=1}^{K} \log \Gamma(\beta_{j}) - \sum_{j=1}^{K} \log \Gamma(\alpha_{i j}) \\
% &+ \sum_{j=1}^{K}(\alpha_{i j} - \beta_{j}) \left[ \psi(\alpha_{i j}) - \psi( \sum_{k=1}^{K} \alpha_{i k}) \right],
% \end{aligned}
% \end{equation}
% and $\mathcal{L}_{i}^{\mathcal{I}\text{-MSE}}$ and $\mathcal{L}_{i}^{|\mathcal{I}|}$ are the same as in \eqref{obj_iedl}. 

% One key feature of open-set SSL is that unlabeled data contains outliers 
% %whose predictions should be a vector of K low-evidence values (i.e. uncertain outliers). 
% and that if a unlabeled sample belongs to outliers keeps unknown. 

% For outliers, our EDL network is expected to output K low-evidence values. Previous works i.e. EDL and $\mathcal{I}$-EDL, focus on optimizing the network to fit a one-hot label $y_i$ given an inlier $x_i$. Therefore, their loss functions can not be directly applied to outliers. On the other hand, negative learning explicitly regulates the network to output low possibilities for part of non-labeled classes. Inspired by negative learning, we propose to explicitly regulate EDL network to output K low evidence values for these outliers. Accordingly, we set the learning target ${\alpha}_{LOW}$ regarding these outliers as $\boldsymbol{1} = [1, \cdots, 1] \in \mathbb{R}^{K}$. However, the problem of outliers selection from unlabeled data arises. Since selection with thresholding causes a trade-off between quantity and quality, we propose to apply low-evidence regulation to all unlabeled data. However, the low-evidence regulation is inappropriate for inliers in unlabeled data. To avoid these inliers being over-penalized, inspired by $\mathcal{I}$-EDL, we introduce FIM to measure the informativeness of evidence carried by each sample and derive a weighted loss function to reduce the weight of unlabeled samples with high evidence. We name the weighted loss function as adaptive negative EDL loss and it is defined as:

% \begin{equation}
% \label{obj_final}
%  \mathcal{L}_{i}^{\mathcal{N}\text{-EDL}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{N}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\mathcal{N}\text{KL}},
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \label{obj_neg_mse}
% &\mathcal{L}_{i}^{\mathcal{N}\text{-MSE}} 
% = \sum_{j=1}^{K}  (\frac{1}{K} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 \cdot \psi^{(1)}(\alpha_{i j}), \\
% \end{aligned}
% \end{equation}

% where

% \begin{equation}
% \begin{aligned}
% \label{obj_final_kl}
% \mathcal{L}_{i}^{\mathcal{N}\text{-KL}} 
% = & D_{KL}(Dir(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert Dir(\boldsymbol{p}_i | \boldsymbol{1})) \\
% = & \log \Gamma(\sum_{j=1}^{K} \alpha_{i j}) - \log \Gamma(K) - \sum_{j=1}^{K} \log \Gamma(\alpha_{i j}) \\
% &+ \sum_{j=1}^{K}(\alpha_{i j} - 1) \left[ \psi(\alpha_{i j}) - \psi( \sum_{k=1}^{K} \alpha_{i k}) \right],
% \end{aligned}
% \end{equation}

% and $\mathcal{L}_{i}^{|\mathcal{I}|}$ are the same as in \eqref{obj_iedl}
% %the uniform Dirichlet distribution $Dir(p_i | \boldsymbol{1})$. 

% To optimize evidential networks, EDL \cite{sensoy2018evidential} derives three kinds of loss functions i.e. mean squared error (MSE), cross-entropy and negative log marginal likelihood. Among them, MSE loss is empirically demonstrated to perform best. Besides MSE loss, EDL introduces a Kullback-Leibler (KL) divergence loss 
% % as the other term of the final loss function to reduce evidence of other $K-1$ non-labeled classes. 
% \dr{as the regular term to penalize evidence for classes that do not fit the training data. Recently proposed $\mathcal{I}$-EDL further introduces Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which the objective loss terms are dynamically re-weighted to force the network to focus more on uncertain classes (i.e. low-evidence classes). More specifically, $\mathcal{I}$-EDL assumes that the target variable $\boldsymbol{y}$ follows a multivariate Gaussian distribution 
% $
% \boldsymbol{y} \sim \mathcal{N}(\boldsymbol{p} , \sigma^2 \mathcal{I}(\boldsymbol{\alpha})^{-1}),
% $
% where $\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})$, $\mathcal{I}(\boldsymbol{\alpha})$ denotes Fisher information matrix(FIM) of $\boldsymbol{p}$, and $\sigma^2$ is the scalar hyperparameter. The optimal parameters $\boldsymbol{\theta}$ are learned by maximizing the marginal likelihood as follows
% $$
% \max_{\boldsymbol{\theta}} \ \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{P}} \left[\log \mathbb{E}_{\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})} [ \mathcal{N}(\boldsymbol{y}|\boldsymbol{p}, \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1}) ]\right].
% $$
% }
% Given training set $\mathcal{D}_{L} = \{(\boldsymbol{x}_i, \boldsymbol{y}_i)\}_{i=1}^{N}$, the final loss function of $\mathcal{I}$-EDL can be reformulated as:
% \begin{equation}
% \label{obj_iedl}
% \min_{\boldsymbol{\theta}} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{I}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\text{KL-ORI}},
% \end{equation}
% where 
% \begin{equation}
% \begin{aligned}
% \label{obj_final_mse}
% &\mathcal{L}_{i}^{\mathcal{I}\text{-MSE}} 
% = \sum_{j=1}^{K} \left( (y_{i j} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 + \frac{\alpha_{i j}( \alpha_{i 0} - \alpha_{i j})}{\alpha_{i 0}^2(\alpha_{i 0} + 1)} \right) \psi^{(1)}(\alpha_{i j}), \\
% \end{aligned}
% \end{equation}
% $$
% $$
% \begin{equation}
% \begin{aligned}
% \label{obj_final_det}
% &\mathcal{L}_{i}^{|\mathcal{I}|}
% = \sum_{j=1}^{K} \log  \psi^{(1)}(\alpha_{i j}) + \log \left(1 - \sum_{j=1}^{K} \frac{ \psi^{(1)}(\alpha_{i 0})}{ \psi^{(1)}(\alpha_{i j})} \right),
% \end{aligned}
% \end{equation}
% $$
% $$
% \begin{equation}
% %\begin{aligned}
% \label{obj_final_kl}
% \mathcal{L}_{i}^{\text{KL-ORI} }
% = D_{KL}(Dir(\boldsymbol{p}_i | \hat{\boldsymbol{\alpha}}_i) \Vert Dir(\boldsymbol{p}_i | \boldsymbol{1}))
% %= & \log \Gamma(\sum_{j=1}^{K} \hat{\alpha}_{i j}) - \log \Gamma(K) - \sum_{j=1}^{K} \log \Gamma(\hat{\alpha}_{i j}) \\
% %& + \sum_{j=1}^{K}(\hat{\alpha}_{i j} - 1) \left[ \psi(\hat{\alpha}_{i j}) - \psi( \sum_{k=1}^{K} \hat{\alpha}_{i k}) \right],
% %\end{aligned}
% \end{equation}
% In $\mathcal{L}_{i}^{\mathcal{I}\text{-MSE}}$ and $\mathcal{L}_{i}^{|\mathcal{I}|}$, ${\alpha}_{i0} = \sum_{j=1}^{K}{\alpha}_{ij} $ and $\psi^{(1)}(\cdot)$ represents the \textit{trigamma} function, defined as $\psi^{(1)}(x) = d \psi(x) / dx = d^2 \ln \Gamma(x) / dx^2 $. \dr{Note that $\psi^{(1)}(x)$ is a monotonically decreasing function when $x > 0$.} 
% % the weight $\psi^{(1)}(\alpha_{i j})$ decreases with the increase of $\alpha_{i j}$. 
% In $\mathcal{L}_{i}^{\text{KL-ORI}} $, 
% % $\psi$ and $\gamma$ denote the \textit{digamma} function and \textit{Gamma} function respectively.  
% $Dir(p_i | \boldsymbol{1})$ is the uniform Dirichlet distribution and $\hat{\alpha}_i = {\alpha}_i \odot(1 - y_i) + y_i$ is the Dirichlet parameters after removal of the evidence corresponding to the labeled class. 

\noindent\textbf{Overall Loss Function.} 
For the EDL outlier detector, except Eq. (\ref{obj_nedl}), (\ref{obj_pedl}), (\ref{obj_kl}), we add a consistency loss $\mathcal{L}_\text{CON}$ to enhance the smoothness of detector, which is defined as
\begin{equation}
\begin{aligned}
% \mathcal{L}_\text{CON}(\mathcal{D}_{u}) = \frac{1}{N_i + N_o} \sum_{i=1}^{N_i + N_o} \sum_{j=1}^{K} (\alpha_{i j}^{s} - \alpha_{i j}^{w})^2,
\mathcal{L}_\text{CON}(\mathcal{D}_{u}) = \frac{1}{N_i + N_o} \sum_{j=1}^{N_i + N_o} \|\boldsymbol{\alpha}_{j}^{s} - \boldsymbol{\alpha}_{j}^{w}\|^2_2,
\end{aligned}
\end{equation}
% where $\alpha_{i j}^{s}, \alpha_{i j}^{w}$
where $\boldsymbol{\alpha}_{j}^{s}, \boldsymbol{\alpha}_{j}^{w}$ represent the predicted evidence vector of strong and weak augmentations of the same unlabeled sample. For the Softmax classifier, we compute the standard cross-entropy loss $\mathcal{L}_\text{CE}$ for labeled data and the FixMatch loss $\mathcal{L}_\text{FM}$ for the high-certainty inliers $\mathcal{I}_{u}$ identified by the EDL detector.
Therefore, the overall loss function of our proposed ANEDL can be expressed as
\begin{equation}
\begin{aligned}
\label{obj_final}
\mathcal{L}_\text{ANEDL} = &\mathcal{L}_\text{CE}(\mathcal{D}_{l}) + \mathcal{L}_\text{FM}(\mathcal{I}_{u}) \\
+ & \mathcal{L}_\text{ANO}(\mathcal{D}_{l}, \mathcal{D}_{u}) + \lambda_\text{CON} \mathcal{L}_\text{CON}(\mathcal{D}_{u}),
\end{aligned}
\end{equation}
where 
\begin{equation}
\begin{aligned}
\mathcal{L}_\text{ANO}(\mathcal{D}_{l}, \mathcal{D}_{u}) = 
& \lambda_\text{P-EDL} \frac{1}{N_l} \sum_{j=1}^{N_l} (\mathcal{L}_j^{\text{P-EDL}} + \mathcal{L}_{j}^{\text{KL}}) \\
+ & \lambda_\text{N-EDL} \frac{1}{N_i + N_o} \sum_{j=1}^{N_i + N_o} (\mathcal{L}_j^{\text{N-EDL}} + \mathcal{L}_{j}^{\text{KL}}),
\end{aligned}
\end{equation}
$\lambda_\text{P-EDL}, \lambda_\text{N-EDL}$ and $\lambda_\text{CON}$ are the hyperparameters used to control the trade-off for each objective.

To minimize our overall loss, we train our model for two stages. In the first stage, we pre-train our model with $\mathcal{L} = \mathcal{L}_\text{CE}(\mathcal{D}_{l}) +  \mathcal{L}_\text{ANO}(\mathcal{D}_{l}, \mathcal{D}_{u}) + \lambda_\text{CON} \mathcal{L}_\text{CON}(\mathcal{D}_{u})$ which drops $\mathcal{L}_\text{FM}$ for $E_{FM}$ epochs. In second stage, we start self-training of classifier and use $\mathcal{L}_\text{ANEDL}$ as loss function. Before every self-training epoch, we calibrate softmax head and EDL head to calculate uncertainty metric to select the inliers for softmax head to conduct self-training. 
% \begin{equation}
% \begin{aligned}
% \mathcal{L}_\text{CON}(\mathcal{D}_{u}) = \frac{1}{N_u} \sum_{i=1}^{N_u} \sum_{j=1}^{K} (\alpha_{i j}^{s} - \alpha_{i j}^{w})^2,
% \end{aligned}
% \end{equation}
% $\alpha_{i j}^{s}, \alpha_{i j}^{w}$ represent the predicted evidence of strong and weak augmentation of unlabeled sample, and $\mathcal{I}$ denotes the high-certainty inliers we identified by the EDL detector. Note that the consistency loss $\mathcal{L}_\text{CON}$ is used to enhance the smoothness of EDL outlier detector. 

% \noindent\textbf{Overall Framework.}

% We design a framework consisting of an EDL head and a softmax head for outlier detection and classification, respectively. To optimize the EDL head, we minimize $\mathcal{L}_{i}^{\text{EDL}}$ for labeled data and $\mathcal{L}_{i}^{\text{EDL}}$ for unlabeled data. To optimize the softmax head, we compute cross-entropy loss for labeled data. Since a randomly initialized network can not select inliers from unlabeled data, we first pre-train the network for $E_{fm}$ epochs with L. After the pre-training, we start self-training of classifier and select the inliers before every self-training epoch. To select the inliers, we do not derive confidence metric solely from EDL prediction, which ignores the fact that selected inliers are used by softmax head. Instead, we calibrate softmax prediction and EDL prediction to give confidence metric. Specifically, given a unlabeled sample $x_u$, the softmax head first predict pseudo label $\hat{k} = {argmax}_j C(F(x_u))$. Then we use the $\hat{k}^{th}$ evidence value ${\alpha}_{\hat{k}}$ predicted by the EDL head as confidence metric and take top-M most confident unlabeled samples as inliers. Here, M is a hyper-parameter. With the calibration, pseudo-labeled class (i.e. $\hat{k}^{th}$ class) which will be used for self-training, is predicted to have high evidence collected from labeled data. With inliers selected from unlabeled data, we compute FixMatch loss $\mathcal{L}_{fm}$ to optimize our network. The following is the final loss function of our method:

% \begin{equation}
% \label{obj_final}
%  \mathcal{L}_{i}^{\text{ALL}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{N}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\mathcal{N}\text{KL}},
% \end{equation}

% Our trained network is also required to detect ouliers in test data. Different from detecting outliers in unlabeled data, detected inliers will not be used for self-training. Thus taking softmax prediction into consideration is not necessary regarding confidence metric. Based on the evidence vector $\alpha$ predicted by our EDL head, we can derive many kinds of metrics e.g. mutual information, expected entropy, and total evidence. They empirically find total evidence ${\alpha}_{i0} = \sum_{j=1}^{K}{\alpha}_{ij}$
% achieves best uncertainty estimation performance. For a descending sorted evidence vector, we think only top-k evidence values can effectively reflect the amount of support collected from data. While the last evidence values tend to be randomly sampled due to the large difference between the sample and corresponding classes. These random sampled values will bring considerable noise to confidence metric when class number $K$ is large. Therefore, we propose to take total top-k evidence as our confidence metric.

%However,  DST \cite{chen2022debiased} argues that pseudo labels generated for self-training are biased. To avoid biased pseudo labels accumulate the bias of classification head which is used for inference, DST \cite{chen2022debiased} introduces a completely parameter independent pseudo head for pseudo labeled data. So that classification head is only trained with labeled data and pseudo-labeled data is only used for representation learning. However, a completely parameter independent pseudo classification head has a different classification boundary with classification head, and thus learns a representation that can not perfectly fit classification head. To sovle this problem, we propose to copy the parameter of classification head to pseudo classification head 

%DST \cite{chen2022debiased}

%Therefore, we use counterfactual reasoning and adaptive marginal loss both proposed in DebiasPL \cite{wang2022debiased}, to handle biased pseudo labels. 

%DebiasPL \cite{wang2022debiased} and
%------------------------------------------------------------------------
\begin{table*}[t]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{clccclcclcclc}
\toprule
Dataset                  &  & \multicolumn{3}{c}{CIFAR-10}                  &  & \multicolumn{2}{c}{CIFAR-100}             &  & \multicolumn{2}{c}{CIFAR-100}             &  & ImageNet-30         \\ \cmidrule{1-1} \cmidrule{3-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-13} 
Inlier/Outlier Classes &  & \multicolumn{3}{c}{6/4}                       &  & \multicolumn{2}{c}{55/45}                 &  & \multicolumn{2}{c}{80/20}                 &  & 20/10               \\ \cmidrule{1-1} \cmidrule{3-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-13} 
No. of labeled samples   &  & 50         & 100        & 400                 &  & 50                  & 100                 &  & 50                  & 100                 &  & 10\%                \\ 
\midrule
FixMatch  
%\cite{sohn2020fixmatch}              
&  & 56.1$\pm$0.6 & 60.4$\pm$0.4 & 71.8$\pm$0.4          &  & 72.0$\pm$1.3          & 79.9$\pm$0.9          &  & 64.3$\pm$1.0          & 66.1$\pm$0.5          &  & 88.6$\pm$0.5          \\
MTC  
% \cite{yu2020multi}                  
&  & 96.6$\pm$0.6 & 98.2$\pm$0.9 & 98.9$\pm$0.1         &  & 81.2$\pm$3.4          & 80.7$\pm$4.6          &  & 79.4$\pm$1.0          & 73.2$\pm$3.5          &  & 93.8$\pm$0.8          \\
T2T 
% \cite{saito2021openmatch}      
&  & 43.1$\pm$8.8 & 43.1$\pm$14.1 & 56.2$\pm$1.4         &  & 60.4$\pm$8.9          & 62.2$\pm$4.4          &  & 74.2$\pm$6.3          & 65.4$\pm$13.4          &  & 55.7$\pm$10.8        \\
OpenMatch 
% \cite{saito2021openmatch}      
&  & \textbf{99.3$\pm$0.9} & \textbf{99.7$\pm$0.2} & 99.3$\pm$0.2         &  & 87.0$\pm$1.1          & 86.5$\pm$2.1          &  & 86.2$\pm$0.6          & 86.8$\pm$1.4          &  & 96.4$\pm$0.7          \\ 
\midrule
Ours                     &  &  98.5$\pm$0.2          &   99.0$\pm$0.2         & \textbf{99.4$\pm$0.1} &  & \textbf{90.3$\pm$0.1} & \textbf{91.1$\pm$0.4} &  & \textbf{89.2$\pm$0.5} & \textbf{91.1$\pm$1.0} &  & \textbf{96.6$\pm$0.3} \\ 
\bottomrule
\end{tabular}}
\caption{Mean and standard deviation of AUROC (\%) on CIFAR-10, CIFAR-100 and ImageNet-30. Higher is better. For CIFAR-10 and CIFAR-100, the three runs correspond to three different folds. For ImageNet-30, the three runs are conducted on the same folder but with different random seeds.}
\label{tab:auroc}
\end{center}
% \vspace{-2mm}
\end{table*}

\begin{table*}[t]
% \centering
% \vspace{-2mm}
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{clccclcclcclc}
\toprule
Dataset                  &  & \multicolumn{3}{c}{CIFAR-10}          &  & \multicolumn{2}{c}{CIFAR-100} &  & \multicolumn{2}{c}{CIFAR-100} &  & ImageNet-30 \\ \cmidrule{1-1} \cmidrule{3-5} \cmidrule{7-8} \cmidrule{10-11} \cmidrule{13-13} 
Inlier/Outlier Classes &  & \multicolumn{3}{c}{6/4}               &  & \multicolumn{2}{c}{55/45}     &  & \multicolumn{2}{c}{80/20}     &  & 20/10       \\ \midrule
No. of labeled samples   &  & 50         & 100        & 400         &  & 50            & 100           &  & 50            & 100           &  & 10\%        \\ \midrule
FixMatch 
% \cite{sohn2020fixmatch}     
&  & 43.2$\pm$1.2 & 29.8$\pm$0.6 & 16.3$\pm$0.5  &  & 35.4$\pm$0.7    & 27.3$\pm$0.8    &  & 41.2$\pm$0.7    & 34.1$\pm$0.4    &  & 12.9$\pm$0.4  \\
MTC 
% \cite{yu2020multi}            
&  & 20.3$\pm$0.9 & 13.7$\pm$0.9 & 9.0$\pm$0.5 &  & 33.5$\pm$1.2    & 27.9$\pm$0.5    &  & 40.1$\pm$0.8    & 33.6$\pm$0.3    &  & 13.6$\pm$0.7  \\
T2T 
% \cite{yu2020multi}            
&  & 10.4$\pm$1.2 & 11.3$\pm$1.3 & 9.5$\pm$1.4 &  & 29.1$\pm$0.9    & 27.0$\pm$0.4    &  & 35.7$\pm$1.4    & 31.6$\pm$1.0    &  & 12.2$\pm$0.9 \\
OpenMatch  
% \cite{saito2021openmatch}    
&  & 10.4$\pm$0.9 & 7.1$\pm$0.5  & 5.9$\pm$0.5 &  & 27.7$\pm$0.4    & 24.1$\pm$0.6    &  & 33.4$\pm$0.2    & 29.5$\pm$0.3    &  & \textbf{10.4$\pm$1.0}  \\ \midrule
Ours                     &  &   \textbf{9.6$\pm$0.4}         &  \textbf{7.0$\pm$0.4}          & \textbf{5.5$\pm$0.2}   &  & \textbf{26.7$\pm$0.2}    & \textbf{23.2$\pm$0.6}    &  & \textbf{32.3$\pm$0.3}    & \textbf{28.5$\pm$0.2}    &  & 11.3$\pm$0.4  \\ \bottomrule
\end{tabular}}
\caption{Error rate (\%) corresponding to Table \ref{tab:auroc}. Lower is better.}
\label{tab:errorrate}
\end{center}
% \vspace{-2mm}
\end{table*}


\subsection{Uncertainty Metric for Open-set SSL}
\label{method:unc}
\noindent\textbf{Inlier Selection in Self-training.} To help better enhance recognition performance with unlabeled data, we aim to select inliers with accurate pseudo labels from unlabeled data for self-training. In FixMatch, pseudo label is generated by Softmax head and will also be used to softmax head as a supervision signal. Therefore, we propose to 
% utilize the prediction of 
leverage EDL head to quantify the uncertainty of the class predicted by softmax head
%\dr{unlabeled samples}
% the pseudo label 
 and select inliers with 
% low uncertainty 
confident prediction which are more likely to be accurately pseudo-labeled. 
%To select the inliers, we do not derive confidence metric solely from EDL prediction, which ignores the fact that selected inliers are used by softmax head. Instead, we calibrate softmax prediction and EDL prediction to give confidence metric. 
% Specifically, given an unlabeled sample $\boldsymbol{x}_u$, the Softmax head first predicts pseudo label $\hat{k} = \arg \max_k C(F(\boldsymbol{x}_u))$. We use the 
% % $\hat{k}^{th}$ evidence value of $\boldsymbol{\alpha}_{u}$ 
% % % ${\alpha}_{\hat{k}}$ 
% % predicted by the EDL head as confidence metric 
% $1 / (\boldsymbol{\alpha}_{u})_{\hat{k}}$ as uncertainty metric
% % , where $(\boldsymbol{\alpha}_{u})_{\hat{k}}$ is $\hat{k}^{th}$ evidence value of $\boldsymbol{\alpha}_{u}$ predicted by the EDL head.
% and select top-O most certain unlabeled samples as inliers. Here, O is a hyperparameter. 
%With the calibration, pseudo-labeled class (i.e. $\hat{k}^{th}$ class) which will be used for self-training, is predicted to have high evidence collected from labeled data. With inliers selected from unlabeled data,
Specifically, given an unlabeled sample $\boldsymbol{x}_u$, the Softmax head  produce a one-hot pseudo label $\hat{\boldsymbol{y}} \in \{0,1\}^{K}$, the EDL head output evidence $\boldsymbol{\alpha} \in \mathbf{R}^{K}_{+}$. Then we use 
$$\text{M}_\text{Self-training} = \hat{\boldsymbol{y}} \cdot \boldsymbol{\alpha}$$
as the confidence value for $\boldsymbol{x}_u$. Larger values mean higher confidence, and we select top-$O$ most certain unlabeled samples as inliers. Here, $O$ is a hyperparameter. 


\noindent\textbf{Outlier Detection for Inference.}
% Our trained network is also required to detect ouliers in test data. 
Detecting outliers in test data is also one of the tasks of Open-set SSL. 
% Unlike selecting inliers from unlabeled data, detected inliers in test data will not be used for self-training. 
Unlike selecting inliers from unlabeled data, detected inliers in test data do not have to consider calibrating with Softmax. Therefore, 
% our target 
the uncertainty metric for inference is to accurately quantify epistemic uncertainty for better separation between inliers and outliers.
%Thus taking softmax prediction into consideration is not necessary regarding confidence metric. 
Based on the evidence vector $\boldsymbol{\alpha}$ predicted by EDL head, we can derive various uncertainty metrics, i.e. mutual information, differential entropy, and total evidence.
% many kinds of uncertainty metrics, e.g. mutual information, expected entropy, and total evidence. 
Previous EDL methods 
% empirically find 
demonstrate total evidence 
% ${\alpha}_{i0} = \sum_{j=1}^{K}{\alpha}_{ij}$
${\alpha}_{0} = \sum_{k=1}^{K}{\alpha}_{k}$
achieves the best outlier detection performance \cite{sensoy2018evidential, deng2023uncertainty}. 
However, we empirically find that classes with small evidence values may form a long-tail effect because the EDL detector assigns a non-negative evidence value to each class. Although these evidence values are small, their total amount is not negligible, especially when dealing with tasks with a large number of classes.
% especially in classification tasks with a large number of classes. 
To avoid these long-tailed evidence values disturbing the separation between inliers and outliers, we propose to sort the evidence vectors in descending order and use only the sum of the top-M evidence values as the confidence value for inference, which is defined as
% to calculate our uncertainty metric for inference. The uncertainty metric is defined as:
\begin{equation}
\label{eq:uncertainty}
    \text{M}_\text{Inference} = \sum_{i=1}^{M} \bar{\alpha}_i,
\end{equation}
where $\bar{\boldsymbol{\alpha}}$ is the descending order of $\boldsymbol{\alpha}$.
% For a descending sorted evidence vector, we think only top-M evidence values can effectively reflect the amount of support collected from data. While the last $K-M$ evidence values tend to be small random numbers with noise, due to the large difference between the sample and corresponding classes. Accumulated noise of these evidence values will disturb separation between inliers and outliers. Therefore, we propose to remove these evidence values and take summation of top-M evidence values as our uncertainty metric for inference.





% \centering




\section{Experiments}

In this section, we conduct extensive experiments to compare the performance of our method with previous SOTA methods and analyze several key components of our method. Additional experimental results are provided in Appendix D. 
%-------------------------------------------------------------------------
\subsection{Datasets}
We evaluate our method on four datasets, including CIFAR-10, CIFAR-100 \cite{krizhevsky2009learning}, ImageNet-30 \cite{hendrycks2016baseline} and Mini-ImageNet \cite{vinyals2016matching}. Appendix C.1 provides details about the dataset. We follow OpenMatch \shortcite{saito2021openmatch} to tailor the dataset for Open-set SSL. Given a dataset, we first split its class set into inlier and outlier class sets. Next, we randomly select inliers to form labeled, unlabeled, validation, and test dataset. Then all outliers will be randomly sampled into unlabeled and test datasets. For the four datasets, we set several experimental settings with different numbers of labeled samples and different amounts of inliers/outliers classes. 


%\YJ{We choose 20 classes as inlier classes delete?} 

%-------------------------------------------------------------------------
\subsection{Implementation Details}
 We employ a randomly initialized Wide ResNet-28-2 \cite{zagoruyko2016wide} to conduct experiments on CIFAR-10 and CIFAR-100, and a randomly initialized ResNet-18 \cite{he2016deep} for experiments on ImageNet-30 and Mini-ImageNet. The Softmax head is a linear layer and the EDL head is non-linear, which is composed of 4 MLP layers. For the pre-training stage, we set its length $E_{FM}$ as 10 for all experiments.
 %and $\lambda_{FM}$ is set to 0 in pre-training stage and 1 in self-training stage.
 We set the length of an epoch as 1024 steps, which means we select a new set of inliers every 1024 steps during self-training. For hyper-parameters of FixMatch, we set them the same as in OpenMatch \cite{saito2021openmatch}. We report the hyper-parameters of our loss function in Appendix. We adopt SGD optimizer with 0.0001 weight decay and 0.9 momentum to train our model by setting initial learning rate as 0.03 with the cosine decrease policy. We set the batch size of labeled and unlabeled samples as 64 and 128 respectively for all experiments. Experiments on ImageNet-30 are conducted with a single 24-GB GPU and other experiments are conducted with a single 12-GB GPU. All experiments in this work are conducted over three runs with different random seeds, and we report the mean and standard derivation of three runs.


%\begin{table}[]
%\centering
%\begin{tabular}{cc|c|c}
%\midrule
%$\alpha$ & $\beta$ & Error rate & AUROC      \\ \midrule
%$\hat{\boldsymbol{\alpha}}$ & $\boldsymbol{1}$& 27.0$\pm$0.2 & 89.9$\pm$0.3 \\
%$\boldsymbol{\alpha}$ & $\boldsymbol{{\beta}_{100}}$& \textbf{26.7$\pm$0.2} & %\textbf{90.3$\pm$0.1} \\
%$\boldsymbol{\alpha}$ & $\boldsymbol{{\beta}_{200}}$ &            &            \\ \midrule
%\end{tabular}
%\end{table}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.46\textwidth]{mini1.pdf}
% 	%\vspace{-3mm}
% 	\caption{Comparison with OpenMatch on Mini-ImageNet. We use the same fold to conduct three runs and report the mean of three runs at the top of each bar.}
% 	\label{fig:mini}
% 	%\vspace{-3mm}
% \end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.99\textwidth]{exp_fig2.pdf}
	%\vspace{-3mm}
	\caption{(a) AUROC ($\%$) and (b) Error rate ($\%$) compared to OpenMatch on Mini-ImageNet. (c) The effect of $M$ using in $\text{M}_\text{Inference}$ during inference. We use the same fold to conduct three runs and report the mean of three runs at the top of each bar. }
	\label{fig:exp_fig}
	%\vspace{-3mm}
\end{figure*}

\subsection{Comparison with Other Methods}

To illustrate the effectiveness of our method on Open-set SSL,
% To illustrate the efficacy of our method, 
we compare it against several baseline  %\YJ{state-of-the-art?} 
methods, including FixMatch \cite{sohn2020fixmatch}, MTC \cite{yu2020multi}, T2T \cite{huang2021trash} and OpenMatch \cite{saito2021openmatch}. 
% To evaluate the performance, we follow the open-set assumption of Open-set SSL that the test dataset contains both inliers and outliers. 
We use classification error rate to evaluate the performance of classifier in inliers
% . To evaluate the performance of outlier detection, we use AUROC which is the general evaluation metric of novelty detection \cite{hendrycks2016baseline}. 
and AUROC to evaluate the performance of outlier detection. AUROC is the general evaluation metric for novelty detection \cite{hendrycks2016baseline}. 
% The results of baseline methods in Table \ref{tab:errorrate} and Table \ref{tab:auroc} comes from OpenMatch \cite{saito2021openmatch}. And the results of OpenMatch in Figure \ref{fig:mini} are obtained based on the author's implementation. 
Table \ref{tab:auroc} and Table \ref{tab:errorrate} show the AUROC and error rate on the CIFAR-10, CIFAR-100, and ImageNet-30, respectively, where the results of other methods are quoted from OpenMatch \cite{saito2021openmatch}. Figure \ref{fig:exp_fig} (a) and (b) reports the experimental results on Mini-ImageNet, where the results of OpenMatch are obtained based on the author's own implementation.

% Table \ref{tab:errorrate} and Table \ref{tab:auroc} show the error rate and AUROC repsectively. In 
% \dr{As shown in} Table \ref{tab:errorrate}, the comparison between Open-set SSL methods (i.e. MTC, OpenMatch and ours) and SSL method (i.e. FixMatch) shows that by excluding outliers in unlabeled data from self-training, Open-set SSL methods can mitigate the problem of incorrect pseudo label of outliers. And thus inlier recognition performance can be largely promoted. Among Open-set SSL methods, overall speaking, our method achieves the best performance in terms of error rate. Because we can better quantify the classification uncertainty and select inliers preferred by self-training with our proposed method \yy{and we also remove bias in pseudo label with method proposed in DebiasPL \cite{wang2022debiased}}. 
% \furui{relate this results with statements from previous sections. your goal is to prove your claim by empirical results.}

As shown in Table \ref{tab:auroc}, we can see that our method outperforms the SOTA method, i.e., OpenMatch, in most experiment settings with respect to AUROC.
% In particular, the improvements of our methods are more significant when the number of classes is larger. Moreover,
In Table \ref{tab:errorrate}, our proposed method also achieves SOTA performance in terms of error rate in 7 out of 8 settings. These results illustrate that our method is more advanced in epistemic uncertainty quantification and 
% has better separation between outliers and inliers. 
has superior performance in outlier detection as well as inlier classification.
Notably, in the more challenging settings with a large number of classes, i.e., settings on CIFAR-100, our method 
% shows a clear advantage over OpenMatch with a more than $\textbf{3\%}$ margin. 
significantly outperforms OpenMatch in both error rate and AUROC, with improvements of $\sim \textbf{1\%}$ and $\sim \textbf{3\%}$. It shows our multinomial detector is more robust 
% to complex modalities 
than the binary detector in identifying outliers from complex modalities when dealing with K-way classification tasks with large K. To further demonstrate our superiority in settings with a large number of inlier classes, we compare our method with OpenMatch 
% in 60/80 inlier classes and 50 labeled samples per class settings on Mini-ImageNet. Mini-ImageNet is more challenge than CIFAR-100 with larger image size and more complex patterns. Again, our method consistently outperforms OpenMatch with respect to both error rate and AUROC as shown in Figure \ref{fig:mini}.
on Mini-ImageNet, which is even more difficult than CIFAR-100 with larger image size and more complex patterns. %We set up experiments with $\{50, 60, 80\}$ inlier classes and 50 labeled samples per class. 
As shown in Figure \ref{fig:exp_fig} (a) and (b), our method consistently outperforms OpenMatch with respect to both AUROC and error rate.

%which means K is large for K-way classification

%-------------------------------------------------------------------------



\subsection{Ablation Studies}
%We next conduct ablation studies to evaluate our key components. 
% All the experiments are conducted in 55 inlier classes and 50 labeled samples setting on Cifar-100.
All experiments of our ablation studies are conducted under the setting of 55 inlier classes and 50 labeled samples per class on CIFAR-100.
We report the mean and standard deviation of three runs on three different folds. 



\noindent\textbf{Component Analysis of ANO.}
Our ANO loss consists of two components, i.e. negative optimization and adaptive loss weight. For ``without NO and Adaptive'', we directly replace $\mathcal{L}_\text{ANO}$ to classical EDL loss, which means we apply no explicit constraints to outliers in unlabeled data. Since the information contained in unlabeled data keeps underutilized, 
% the performance is suboptimal. 
both classification and outlier detection performance are suboptimal as shown in Table \ref{table:ano}.
For ``without Adaptive'', we remove our adaptive loss weight from $\mathcal{L}_\text{ANO}$, which means 
% we do not use 
not using the FIM to identify the amount of information for each class of unlabeled samples. As a result, all unlabeled samples including inliers are equally regulated to output K-low evidence values. Although negative optimization compresses evidence values of unlabeled outliers, 
% it also interferes with the learning of unlabeled inliers and the
it simultaneously depresses the evidence of unlabeled inliers, making inlier features and outlier features interfere with each other. Therefore, its improvement compared to ``without NO and Adaptive'' is still limited. 
Our proposed $\mathcal{L}_\text{ANO}$, i.e., ``with NO and  Adaptive'', peaks the best performance by sufficiently compressing evidence values of unlabeled outliers without interfering with the learning of unlabeled inliers.
%-------------------------------------------------------------------------
\begin{table}[t]
\begin{center}
% \resizebox{0.40\textwidth}{!}{
\begin{tabular}{cc|cc}
\toprule
NO & Adaptive & AUROC & Error rate     \\ \midrule
     &          & 88.4$\pm$0.4 & 27.0$\pm$0.3\\
$\checkmark$     &          & 89.0$\pm$0.3 & 26.9$\pm$0.3\\
$\checkmark$     & $\checkmark$ & \textbf{90.3$\pm$0.1} & \textbf{26.7$\pm$0.2}\\ \bottomrule
\end{tabular}
\caption{Component analysis of Adaptive Negative Optimization (ANO).}
\label{table:ano}
% }
\end{center}
%\vspace{-2mm}
\end{table}

\noindent\textbf{Evaluation of Uncertainty Metric.} 
To illustrate the effectiveness of designing different uncertainty metrics for self-training and inference, we conduct an ablation study and summarize it in Table \ref{table:metric}. Results show that using $\text{M}_\text{Self-training}$ and $\text{M}_\text{Inference}$ for self-training and inference, respectively, achieves the best performance in both AUROC and error rate. Since $\text{M}_\text{Self-training}$ cannot distinguish outliers and inliers with high aleatoric uncertainty, the AUROC using $\text{M}_\text{Self-training}$ is significantly lower than that of $\text{M}_\text{Inference}$. However, the effect of using $\text{M}_\text{Inference}$ during self-training outperforms using $\text{M}_\text{Self-training}$, probably due to the fact that inliers with high aleatoric uncertainty interfere with self-training. Note that using different metrics during inference only affects the effectiveness of outlier detection, not inlier classification. 
Furthermore, since $\text{M}_\text{Inference}$ uses the sum of top-$M$ evidence values as the metric, we also conduct an ablation study to evaluate the effect of $M$. As shown in Figure \ref{fig:exp_fig} (c), the sum of top $M=25$ evidence values is a better metric to detect outliers, compared with the sum of all $K=55$ evidence values. Note that the improvements steadily appear in each trained model. Comparison with other values of $M$ also shows that too small $M$ will miss useful evidence value and lead to insufficient measurement of uncertainty.

% \noindent\textbf{Evaluation of Uncertainty Metric During Inference.} 
% Previous EDL methods propose to use the sum of all K evidence values as a metric to measure epistemic uncertainty, while we propose to use the sum of top-M evidence values as the metric. For each experiment, we use the same trained model to evaluate performance and only change how uncertainty metric is calculated. We also compare the configuration of the uncertainty metric used for inlier selection in self-training, denoted as ``Calibration''. Since uncertainty metric during inference influences outlier detection more, we report AUROC in Figure \ref{fig:exp_fig} (c). Results show that the sum of top $M=25$ evidence values is a better metric to quantify epistemic uncertainty, compared with the sum of all $K=55$ evidence values. Note that the improvements steadily appear in each trained model. Comparison between $M=25$ and $M=3$ (or $1$) also shows that a small M misses useful evidence values and is insufficient to measure the uncertainty. 



% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.48\textwidth]{ab1.pdf}
% 	\caption{Evaluation of uncertainty metric during inference. Only AUROC is reported. We show the mean of three runs at the top of each box. M denotes the sum of top-M evidence values. Calibration denotes uncertainty metric used in ST.}
% 	\label{fig:cmtest}
% \end{figure}

\begin{table}[t]
\begin{center}
% \resizebox{0.40\textwidth}{!}{
\begin{tabular}{cc|cc}
\toprule
Self-training &  Inference    &    AUROC       &   Error rate             \\ \midrule
$\text{M}_\text{Inference}$ & $\text{M}_\text{Inference}$ &  89.8$\pm$0.2 &  27.8$\pm$0.2        \\ 
$\text{M}_\text{Self-training}$ & $\text{M}_\text{Self-training}$ &  88.4$\pm$0.3 &  26.7$\pm$0.2        \\ 
$\text{M}_\text{Self-training}$ & $\text{M}_\text{Inference}$   & \textbf{90.3$\pm$0.1} &  \textbf{26.7$\pm$0.2} \\ \bottomrule
\end{tabular}
\caption{Ablation study of uncertainty metric in self-training and inference. 
% ``w/o calibration'' denotes the sum of top-25 evidence values (M=25) as uncertainty metric, which is used for inference.
}
\label{table:metric}
% }
\end{center}
\end{table}

% \noindent\textbf{Evaluation of Uncertainty Metric in Self-training.} The choice of uncertainty metric is important to inliers selection in self-training (ST). 
% To illustrate the effectiveness of our proposed uncertainty metric for ST (denoted as ``with calibration'') , we compare it  with the sum of the top 25 evidence values (denoted as ``without calibration'') which achieves the best outlier detection performance during inference. %(see Figure \ref{fig:cmtest}). 
% We use these two different metrics for self-training respectively and conduct inference with the latter one.
% %as uncertainty measure of ST and denote ``without calibration''. 
% % For ``without calibration'', we use the summation of top-25 evidence values which has the best outlier detection performance as shown in 
% % Figure \ref{fig:cmtest}. 
% %It effectively measures the epistemic uncertainty of the whole sample. The metric we propose in Section \ref{method:unc} is then denoted ``with calibration".
% %As shown in Table \ref{table:conf}, 
% Although ``without calibration'' shows superior performance during inference, it performs poorly if directly used in ST as shown in Table \ref{table:conf}. Whereas our proposed ``with calibration'', which quantifies epistemic uncertainty of the class predicted by Softmax head, shows clear advantages over "without calibration". 
% % Despite its superiority in outlier detection, it does not consider the prediction of Softmax head, which leads to a sub-optimal performance. For ``with calibration'', we calculate the metric as claimed in section 3.3. It quantifies epistemic uncertainty of the class predicted by Softmax head and shows a clear superiority compared with ``without calibration''. 
% The superiority indicates that for inlier selection in self-training, we should calibrate Softmax head and EDL head to quantify epistemic uncertainty of predicted class, instead of the whole sample.% estimated by the sum of top-25 evidence values.   



%Despite its superiority in outlier detection, it does not take the prediction of softmax head into consideration which leads to a sub-optimal performance with repect to both error rate and AUROC. 
%Although it selects low epistemic uncertainty samples, 



\section{Conclusion}

In this work, we propose a novel framework, adaptive negative evidential deep learning (ANEDL), for open-set semi-supervised learning. ANEDL adopts evidential deep learning (EDL) to Open-set SSL for the first time and designs novel adaptive negative optimization method. In particular, EDL, as an advanced uncertainty quantification method, is deployed to estimate different types of uncertainty for inliers selection in self-training and outlier detection in inference. Furthermore, to enhance the separation between inliers and outliers, we propose adaptive negative optimization to explicitly compress the evidence value of outliers in unlabeled data and avoid interfering with the learning of inliers in unlabeled data with adaptive loss weight. Our extensive experiments on four datasets demonstrate that our method is superior to other SOTAs.% state-of-the-art methods.


\bibliography{aaai24}


\appendix

\clearpage

\input{supplementary}

\end{document}
