%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Adaptive Negative Evidential Deep Learning \\ for Open-set Semi-supervised Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%\author{Ben Trovato}
%\authornote{Both authors contributed equally to this research.}
%\email{trovato@corporation.com}
%\orcid{1234-5678-9012}

\author{Yang Yu}
%\authornotemark[1]
\affiliation{%
  \institution{The Chinese University of Hong Kong}
  \city{Hong Kong}
  %\state{State}
  \country{China}
}
\email{yangyu@cse.cuhk.edu.hk}

\author{Danruo Deng}
%\authornotemark[1]
\affiliation{%
  \institution{The Chinese University of Hong Kong}
  \city{Hong Kong}
  %\state{State}
  \country{China}
}
\email{drdeng@cse.cuhk.edu.hk}

\author{Furui Liu}
%\authornotemark[1]
\affiliation{%
  \institution{Zhejiang Lab}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\email{liufurui@zhejianglab.com}

\author{Yueming Jin}
%\authornotemark[1]
\affiliation{%
  \institution{National University of Singapore}
  %\city{Hangzhou}
  %\state{Zhejiang}
  \country{Singapore}
}
\email{ymjin@nus.edu.sg}

\author{Qi Dou}
%\authornotemark[1]
\affiliation{%
  \institution{The Chinese University of Hong Kong}
  \city{Hong Kong}
  %\state{State}
  \country{China}
}
\email{qidou@cuhk.edu.hk}

\author{Guangyong Chen}
%\authornotemark[1]
\affiliation{%
  \institution{Zhejiang Lab}
  \city{Hangzhou}
  \state{Zhejiang}
  \country{China}
}
\email{gychen@link.cuhk.edu.hk}

\author{Pheng-Ann Heng}
%\authornotemark[1]
\affiliation{%
  \institution{The Chinese University of Hong Kong}
  \city{Hong Kong}
  %\state{State}
  \country{China}
}
\email{pheng@cse.cuhk.edu.hk}


%\author{Anonymous ACM-MM submission}
%\authornotemark[1]
%\email{email@email.com}
%\affiliation{%
%  \institution{Department Name \\ Institute/University Name}
%  \city{City}
%  \state{State}
%  \country{Country}
%}

%\author{Anonymous ACM-MM submission}
%\authornotemark[1]
%\email{email@email.com}
%\affiliation{%
%  \institution{Department Name \\ Institute/University Name}
%  \city{City}
%  \state{State}
%  \country{Country}
%}

%\author{Anonymous ACM-MM submission}
%\affiliation{%
%  \institution{Paper ID }
%}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{First and second author, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrated empirically, our proposed method outperforms existing state-of-the-art methods across four datasets. Our code will be released once this paper is accepted.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Object recognition}
\ccsdesc[500]{Computing methodologies~Object recognition}
%\ccsdesc[300]{Object recognition}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Open-set Semi-supervised Learning, Evidential Deep Learning, Adaptive Negative Optimization}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.47\textwidth]{edl_detector.pdf}
	% \vspace{-1mm}
	\caption{The EDL detector we introduce can quantify different types of uncertainty, including epistemic uncertainty due to lack of knowledge, and aleatoric uncertainty from the complexity of samples in distribution. Based on these uncertainties, we design different metrics for self-training and inference. (Images with blue and yellow backgrounds indicate inliers and outliers, respectively.)}
 % The EDL detector we introduce can quantify different types of uncertainty and can be used to design more targeted training and inference strategies.
	\label{fig:edl_detector}
 %\vspace{-2mm}
\end{figure}

Semi-supervised learning (SSL) has recently witnessed significant progress by propagating the label information from labeled data to unlabeled data \cite{berthelot2019mixmatch, xu2021dash, wang2022freematch, zheng2022simmatch}. Despite the success, SSL methods are deeply rooted in the closed-set assumption that labeled data, unlabeled data and test data share the same predefined label set. In reality \cite{yu2020multi}, such an assumption may not always hold as we can only accurately control the label set of labeled data, while unlabeled and test data may include outliers that belong to the novel classes that are not seen in labeled data. During self-training, imprecisely propagating label information to these outliers shall interfere with model learning and lead to performance degradation. To this end, open-set semi-supervised learning (Open-set SSL) \cite{yu2020multi} has been emerging to tackle the problem. It first detects out-of-distribution (OOD) samples as outliers and then recognizes in-distribution (ID) samples as belonging to the classes from the predefined label set.
%\furui{should write it more formally. This version reads somewhat like informal conclusions}

Existing Open-set SSL methods \cite{guo2020safe, chen2020semi} generally consist of an outlier detector and a classifier. To avoid propagating wrong label information to outliers during SSL training of the classifier, they first detect outliers and then optimize SSL loss only for unlabeled instances taken as inliers. To detect outliers, they aim to estimate confidence score of a sample being an inlier (reversely, uncertainty score for outliers). For example, T2T \cite{huang2021trash} proposes to detect outliers by cross-modal matching over binary detector.
% classifier. 
OpenMatch \cite{saito2021openmatch} deploys a one-vs-all (OVA) classifier \cite{saito2021ovanet} as outlier detector to output the confidence score of considering a sample as an inlier and reject outliers with confidence score below a threshold for SSL training. With outliers rejected, they improve recognition accuracy of SSL methods. 



We identify two main limitations in current state-of-the-art literature to tackle this task.
%However, several limitations hinder them from further promoting outlier detection performance and recognition accuracy. 
First, most previous works estimate confidence scores based on binary likelihoods generated from binary detectors activated by Softmax function. Such softmax-based networks only provide a point estimation of predicted class probabilities and 
% cannot quantify different types of uncertainties \YJ{for different classes in the multi-class recognition task?} 
cannot quantify whether the uncertainty stems from the lack of knowledge of outliers (epistemic uncertainty) or from the complexity of the samples within the distribution (aleatoric uncertainty) \cite{sensoy2018evidential, malinin2018predictive, parkactive}. 
% which cannot distinguish \textit{aleatoric uncertainty} and \textit{epistemic uncertainty}. 
Moreover, when we tackle a K-way classification problem with a large K, the binary detectors are less robust to identify outliers from such a complex dataset that contains multi-class information \cite{carbonneau2018multiple}.

One advanced method, evidential deep learning (EDL) \cite{sensoy2018evidential} can explicitly quantify the classification uncertainty corresponding to the unknown class, by treating the network's output as evidence for parameterizing the Dirichlet distribution according to subjective logic \cite{josang2016subjective}. 
Compared with Softmax which only provides probabilistic comparison values between classes, EDL models second-order probability and uncertainty for each class.

In this work, we take the \emph{first} step to introduce EDL into the outlier detector. As shown in Figure \ref{fig:edl_detector}, EDL enables a multinomial detector to directly output uncertainty and evidence for each class, which can alleviate the insufficient scalability of existing binary detectors. Moreover, different types of uncertainties can be distinguished by concentration parameters, so we propose to design different uncertainty metrics for self-training and inference.
% more targeted training and inference strategies,  
Note that though EDL shows an impressive performance in uncertainty quantification, we empirically find its deficiency in learning representation preferred by classification tasks and consequent low performance in classification accuracy. Therefore, we propose to take the advantage of both Softmax and EDL, by keeping Softmax for representation learning. 

\begin{figure*}[t]
	%\setlength{\abovecaptionskip}{0.1cm}	
	%\setlength{\belowcaptionskip}{-0.3cm}
	\centering
	\includegraphics[width=0.97\textwidth]{method1.pdf}
	%\vspace{-4mm}
	\caption{Overview of our proposed Adaptive Negative Evidential Deep Learning (ANEDL). It consists of a shared feature extractor, a Softmax head and an EDL head. EDL head is used to detect outliers, while Softmax head is used to classify. We introduce Negative Optimization to explicitly regularize our EDL detector to output low evidence values for uncertain classes, while proposing an Adaptive loss weight to encourage the detector to pay more attention to  these uncertain classes. Unlabeled samples with confident predictions are used jointly with labeled samples to train the classification head.}
	%\vspace{-2mm}
	\label{fig:method}
\end{figure*}

The other limitation of prior methods is that they do not effectively leverage the information contained in unlabeled data. The detector should output low confidence scores for outliers, but this information is underutilized for model regularization, which may result in suboptimal performance \cite{malinin2018predictive}.
In this regard, we propose \emph{adaptive negative optimization} tailored for unlabeled data, where the Negative optimization is to explicitly regulate our EDL detector to output K low evidence values for a given outlier sample. However, negative optimization can not directly apply to inliers in unlabeled data.
Therefore, we derive an adaptive loss weight for encouraging the model to separately treat inliers and outliers in the unlabeled data in the learning process. It can largely avoid the interference between the ID and OOD features as well as the leading problem of uncertainty availability reduction.
Specifically, we achieve this by using the Fisher information matrix (FIM) introduced by $\mathcal{I}$-EDL \cite{deng2023uncertainty} to identify the amount of information for each class of unlabeled samples, while imposing smaller constraints on the confident classes with high evidence. Therefore, such optimization can sufficiently compress evidence values of unlabeled outliers without interfering with the learning of unlabeled inliers. 

To our knowledge, we are the first to exploit evidential deep learning in the Open-set SSL task, and propose a novel Adaptive Negative Evidential Deep Learning (ANEDL) framework. Below, we summarize the major contributions of this work:
\begin{itemize}
	\item 
 % We propose a two-head framework that deploys EDL for uncertainty estimation and softmax for representation learning and classification. The framework can detect outliers while recognizing inliers into correct classes.
 We introduce EDL as an outlier detector for Open-set SSL, which enables quantification of different types of uncertainty and 
 % more refined training and inference strategies.
  design different uncertainty metrics for self-training and inference.
	% \item We set a low-confidence learning target for outliers in unlabeled dataset and derive a FIM weighted loss function to achieve the target.
    \item We propose a novel adaptive negative optimization strategy, which 
    % can learn to quantify uncertainty 
    tailors EDL on the unlabeled dataset containing both inliers and outliers.
	\item Extensive experiments on four public datasets, i.e., CIFAR-10, CIFAR-100, ImageNet-30, and Mini-ImageNet show that our method outperforms the state-of-the-art methods in different settings.
%state-of-the-arts in different settings .
\end{itemize}

\section{Related Works}

\subsection{Semi-Supervised Learning}
Semi-supervised learning (SSL) aims to ease deep network's reliance on massive and expensive labeled data by utilizing limited labeled data together with a large amount of unlabeled data. Existing works can be generally categorized into two paradigms: consistency regularization and pseudo labeling \cite{lee2013pseudo}. Consistency regularization assumes that the network should be invariant to different perturbations of the same instance. Pseudo-labeling takes confidently pseudo-labeled unlabeled data as labeled data to train the network. And many pseudo labeling methods like FixMatch \cite{sohn2020fixmatch}, FlexMatch \cite{zhang2021flexmatch} and AdaMatch \cite{berthelot2021adamatch} focus on how to select confidently pseudo-labeled data. Although pseudo-labeling methods have achieved tremendous success in SSL, they are deeply rooted in the close-set assumption and can not be directly applied to open-set SSL \cite{yu2020multi, saito2021openmatch}. Because noisy pseudo-labeled outliers may degrade the performance of self-supervised training.
% On the other hand, consistency regularization which gets rid of the close-set assumption can be kept for open-set SSL. Following OpenMatch, we also apply soft consistency regularization to our outlier detector. 
On the other hand, OpenMatch \cite{saito2021openmatch} proposes that soft consistency regularization by minimizing the distance between its predictions on two augmentations of the same image can help SSL methods get rid of the close-set assumption can be kept for open-set SSL. Following OpenMatch, our method also applies soft consistency regularization to the outlier detector.

\subsection{Open-set Semi-Supervised Learning}
Open-set SSL \cite{guo2020safe, chen2020semi} aims to detect outliers while categorizing inliers into correct close-set classes. Existing open-set SSL methods recognize the weakness of SSL methods in terms of detecting outliers and they develop an extra outlier detector to exclude outliers from self-supervised training. Their outlier detectors are based on binary classifier activated by softmax function. For example, MTC \cite{yu2020multi} deploys a binary classifier to predict the probability of the instance belonging to outliers and updates the network parameters and the outlier score alternately. OpenMatch \cite{saito2021openmatch} takes one-vs-all (OVA) classifier as its outlier detector and proposes to apply soft consistency regularization to the outlier detector. An OVA classifier is composed of K binary classifiers and each classifier predicts the probability of the sample belonging to a specific class. 
% We identify two limitations of binary classifier for outlier detection in open-set SSL. First, softmax function is notorious to be over-confident. Second, when the class number is large, the modality that each singleton (e.g. positive and negative class) of binary classifier should recognize will be very complicated. Therefore, we adopt EDL as our outlier detection method, with a better uncertainty quantification capacity compared with softmax. On the other hand, EDL gets rid of binary classifer and can achieve good performance even with large class number.
However, softmax-based networks are notorious for inflating the probabilities of predicted classes \cite{szegedy2016rethinking, guo2017calibration, wilson2020bayesian}, and binary detectors perform poorly on tasks with a large number of classes \cite{carbonneau2018multiple}. To address these limitations, we adopt evidential deep learning (EDL) \cite{sensoy2018evidential} as the outlier detector for Open-set SSL.

\subsection{Evidential Deep Learning}
Evidential neural networks \cite{sensoy2018evidential} model the outputs as evidence to quantify belief masses and uncertainty based on the Dempster-Shafer Theory of Evidence (DST) \cite{sentz2002combination} and subjective logic \cite{josang2016subjective}. Similar to this work, \cite{malinin2018predictive} propose Prior Networks that explicitly consider the distributional uncertainty to quantify the mismatch of ID and OOD distribution. Compared with the standard neural network classifiers direct output the probability distribution of each sample, EDL obtains the density of classification probability assignments by parameterizing the Dirichlet distribution. Therefore, EDL can use the properties of Dirichlet distribution to distinguish different types of uncertainties, which shows an impressive performance in uncertainty quantification and is widely used in various applications. For example, \cite{zhao2020uncertainty} proposes a multi-source uncertainty framework combined with DST for semi-supervised node classification with GNNs. \cite{soleimany2021evidential} introduces evidential priors over the original Gaussian likelihood function to model the uncertainty of regression networks. \cite{bao2021evidential, bao2022opental} propose a general framework based on EDL for Open Set Recognition (OSR) and Open Set Temporal Action Localization (OSTAL), respectively. Compared with previous efforts, our work is the first to exploit EDL to Open-set SSL.

\section{Method}

\subsection{Problem Setting}
Open-set semi-supervised learning (Open-set SSL) aims to learn a classifier by using a set of labeled and unlabeled datasets. Unlike traditional semi-supervised learning (SSL), the unlabeled dataset of Open-set SSL contains both ID and OOD samples, i.e., inliers and outliers. More specifically, for a K-way classification problem, let $\mathcal{D}_{l}=\{(\boldsymbol{x}^l_j, y^l_j)\}_{j=1}^{N_l}$ be the labeled dataset, containing $N_l$ labeled samples randomly generated from a latent distribution $\mathcal{P}$. The labeled example $\boldsymbol{x}^l_j$ can be classified into one of K classes, meaning that its corresponding label $y^l_j \in \{c_1, \cdots, c_K\}$. Let $\mathcal{D}_{u}= \{\boldsymbol{x}^i_j\}_{j=1}^{N_i} \cup \{\boldsymbol{x}^o_j\}_{j=1}^{N_o}$ be the unlabeled dataset consisting of $N_i + N_o$ examples with labels either from the K known classes (inliers) or never been seen (outliers). The goal of Open-set SSL is to detect outliers while classifying inliers into the correct class.

\subsection{Adaptive Negative Evidential Deep Learning}

% \noindent\textbf{Network Overview} 
\noindent\textbf{Approach Overview.} 
As shown in Figure \ref{fig:method}, our proposed framework contains three components: (1) an EDL head $D(\cdot)$ that quantifies uncertainty by outputting K evidence values to detect outliers in unlabeled data;
% detects outliers \dr{in unlabeled data} by directly outputting K evidence values to quantify  belief mass or uncertainty; 
(2) a Softmax head $C(\cdot)$
% categorizes inliers by outputting K probabilities; 
classifies all inlier samples by outputting K-way probabilities, including labeled data and inliers in unlabeled data; 
(3) a feature extractor $F(\cdot)$ shared by outlier detector and classifier.
% the uncertainty head and the classification head. 
% To use our model, the EDL head first predicts evidence vector and a low-evidence sample will be detected as an outlier. Conversely, a high-evidence sample will be taken as an inlier and categorized into pre-defined K categories by the classification head. 
To adopt our framework to Open-set SSL, we propose an Adaptive Negative Optimization strategy to train EDL head by using all labeled and unlabeled samples. We introduce negative optimization to explicitly regularize our EDL detector to output low evidence values for uncertain classes, while deriving an adaptive loss weight to encourage the detector to pay more attention to these uncertain classes. For unlabeled data with confident prediction, we will use them jointly to train the Softmax classifier with labeled data.

\noindent\textbf{Evidential Outlier Detector.}
% Traditional classification networks use softmax function to predict class probabilities and achieve impressive classification performance. However, such softmax-based networks only provide a point estimation of probabilities of predicted classes and are not able to quantify different types of uncertainties. To tackle the limitations of softmax, evidential deep learning (EDL) proposes to integrate deep classification networks into the evidence framework of 
% %the Dempster-Shafer Theory (DST) of evidence and 
% subjective logic, to jointly predict classification probabilities and quantify uncertainty. 
Traditional outlier detectors for Open-set SSL use Softmax activation function to predict confidence scores and achieve impressive performance. However, such softmax-based detectors only provide a point estimation of outlier probabilities, unable to quantify different types of uncertainty. To tackle the limitations of Softmax, we introduce evidential deep learning (EDL) \cite{sensoy2018evidential} to our framework as an outlier detection module. EDL proposes to integrate deep classification networks into the evidence framework of subjective logic \cite{josang2016subjective}, to jointly predict classification probabilities and quantify uncertainty.
Specifically, for K-way classification, EDL uses a network $g(\boldsymbol{\theta})$ to calculate its evidence vector $\boldsymbol{e}= g(\boldsymbol{x}|\boldsymbol{\theta})$. Note that an activation function, e.g. Relu or Softplus, is on top of $g(\boldsymbol{\theta})$ to guarantee non-negative evidence. Then EDL correlates its derived evidence vector $\boldsymbol{e}\in \mathbb{R}_{+}^{K}$ to the concentration parameter $\boldsymbol{\alpha} \in \mathbb{R}_{+}^{K}$ of Dirichlet distribution $D(\boldsymbol{p}|\boldsymbol{\alpha})$ with the equality ${\alpha}_i = e_i + 1$. 
Dirichlet distribution parameterized by $\boldsymbol{\alpha}$ models the density of classification probability assignments and uncertainty.
% The expected probability of the $k^{th}$ class can be expressed by the mean of Dirichlet distribution ${\hat{p}}_k = \frac{{\alpha}_k}{S} $ and the predictive uncertainty can be deterministically computed as $u = \frac{K}{S} $, where $S$ is the Dirichlet strength and defined as $S=\sum_{k=1}^K {\alpha}_k$. 
The expected probability of the $k^{th}$ class and uncertainty can be derived by
$$
{\hat{p}}_k = \frac{{\alpha}_k}{S} \quad \text{and} \quad u = \frac{K}{\alpha_{0}}, 
$$
where $S$ is the precision of Dirichlet distribution and defined as $S=\sum_{k=1}^K {\alpha}_k$. Higher values of $S$ lead to sharper, more confident distributions.
% And ${\alpha}_k$ represents the evidence that a sample belonging to $k^{th}$ class collected from data. 
Given a sample, ${\alpha}_k$ is incremented to update the Dirichlet distribution of the sample when the evidence of $k^{th}$ class is observed.
%Therefore, EDL can jointly predict classification probabilities and quantify uncertainty with a deterministic network.

%Given a sample $x_i$ and its one-hot label $y_i$, evidential network predicts concentration parameters ${\alpha}_i$ of corresponding Dirichlet distribution. 

\noindent\textbf{Joint Optimization with Softmax and EDL.} Although EDL-related works \cite{sensoy2018evidential, deng2023uncertainty} are capable of jointly classifying and quantifying uncertainty, we empirically find its deficiency in optimizing a network to learn representation for classification. 
That is, the classification performance of the evidential network is lower than that of a network activated by Softmax and optimized with only cross-entropy loss.
% That is to say the network optimized network with only EDL loss will have a lower classification performance than that activated by softmax and optimized with only cross-entropy loss. 
In this way, we propose to take the strength of both Softmax and EDL and jointly optimize their losses. In our proposed framework, Softmax head takes charge of learning representation and predicting classification probabilities, while EDL is responsible for quantifying uncertainty. During training, with EDL excluding outliers, we adopt an SSL method (i.e., FixMatch following OpenMatch \cite{saito2021openmatch}) to our Softmax head to enhance representation quality and classification accuracy. We also utilize counterfactual reasoning and adaptive margins proposed in DebiasPL \cite{wang2022debiased} to remove the bias of pseudo label in FixMatch.

% \subsection{Adaptive Negative EDL}
\noindent\textbf{Adaptive Negative Optimization.}
The core challenge of Open-set SSL is that unlabeled data contains novel categories that are not seen in labeled training data, i.e., outliers. 
To enhance the separation between inliers and outliers, detectors should output low confidence scores for outliers, but traditional binary detectors do not sufficiently leverage this information for model regularization, which may result in suboptimal performance \cite{malinin2018predictive}.
% Since EDL-related methods learn optimal parameters by maximizing the expected likelihood of the observed labels, directly training EDL outlier detector with \yy{inaccurate pseudo-labels} would mislead the learning of the evidence, reducing the availability of uncertainty. 
% If these outliers cannot be detected, it can severely impair the performance of the detector.  
Inspired by negative learning \cite{ishida2017learning}, we propose Adaptive Negative Optimization (ANO) to avoid providing misinformation to the EDL detector by focusing on uncertain classes. 
% In particular, we derive an adaptive loss weight to encourage EDL detector to separately handle inliers and outliers in unlabeled data during the learning process. For the identified outlier samples, we introduce the negative optimization to explicitly regulate our detector to output K low evidence values.
In particular, we introduce the negative optimization to explicitly regulate our detector to output low evidence values for uncertain classes. Meanwhile, adaptive loss weights are proposed to encourage EDL detectors to pay more attention to uncertain classes during the learning process.
% For the identified outlier samples, we introduce the negative optimization to explicitly regulate our detector to output K low evidence values.


More specifically, we adopt the Fisher information matrix (FIM) introduced by $\mathcal{I}$-EDL \cite{deng2023uncertainty} to identify the amount of information contained in each class of each sample, while imposing explicit constraints on the informative uncertainty classes. 
% We adopt the Fisher Information Matrix (FIM) introduced by $\mathcal{I}$-EDL to identify the amount of information contained in each class of each sample, where class labels with higher evidence correspond to less Fisher information. More specifically, we constrain the uncertain classes of unlabeled samples by the KL divergence term, making their distribution more uniform. The objective function is defined as
$\mathcal{I}$-EDL proves that the evidence of a class exhibits a negative correlation with its Fisher information, i.e., the class with more evidence corresponds to less Fisher information, and shows an impressive performance in uncertainty quantification \cite{deng2023uncertainty}. Therefore, we propose to use FIM as an indicator to distinguish inliers from outliers to adaptively regulate our model to pay more attention to uncertain classes.
%\furui{need to think about the rigorousness of the statement that "evidence of a class is negatively correlated with its Fisher information" better add some citations}
For those uncertain classes with less evidence in unlabeled samples, we impose explicit constraints to output K low evidence values through the Kullback-Leibler (KL) divergence term. 
% Following $\mathcal{I}$-EDL, we use the inverse of the FIM ($\mathcal{I}(\boldsymbol{\alpha})^{-1}$) as the variance of the generative distribution of $\boldsymbol{y}$.
The objective function for unlabeled data is defined as
\begin{equation}
\begin{aligned}
\label{obj_unlabel}
% \min_{\boldsymbol{\theta}} \ &\mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}_{L}} \left[ - \log \mathbb{E}_{\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})} [ \mathcal{N}(\boldsymbol{y}|\boldsymbol{p}, \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1}) ]\right] \\
% &\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_{U}} \left[ D_{KL}( \mathcal{U} \Vert \mathcal{N}( \mathbb{E}_{Dir(\boldsymbol{\alpha})}[\boldsymbol{p}], \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1})\right]
\min_{\boldsymbol{\theta}} \ \mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}_{u}} \left[ D_{KL}( \mathcal{U} \Vert \mathcal{N}(\mathbb{E}_{Dir(\boldsymbol{\alpha})}[\boldsymbol{p}], \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1})\right]
\end{aligned}
\end{equation}
where $\mathcal{U}=\mathcal{N}(1/K, \lambda^2)$, $\boldsymbol{\alpha} = g(\boldsymbol{x}|\boldsymbol{\theta}) + \boldsymbol{1}$, and $\mathcal{I}(\boldsymbol{\alpha})$ denotes the FIM of $Dir(\boldsymbol{\alpha})$ (the closed-form expression is provided in Appendix). 
% Given training set $\mathcal{D}_{u} = \{\boldsymbol{x}_i\}_{i=1}^{N_u}$, by the definition of KL divergence, Eq. (\ref{obj_unlabel}) can be simplified as 
According to the definition of KL divergence, the loss function for an unlabeled sample $\boldsymbol{x}_j \in \mathcal{D}_u$ in Eq. (\ref{obj_unlabel}) can be simplified as 
\begin{equation}
\begin{aligned}
\label{obj_nedl}
& \mathcal{L}_{j}^{\text{N-EDL}} = \sum_{k=1}^{K}  (\frac{1}{K} - \frac{\alpha_{j k}}{\alpha_{j 0}} )^2 \psi^{(1)}(\alpha_{j k}) - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_j)|,
% & \frac{1}{N_u} \sum_{i=1}^{N_u} \left( \sum_{j=1}^{K}  (\frac{1}{K} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 \psi^{(1)}(\alpha_{i j}) - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_i)| \right),
\end{aligned}
\end{equation}
where 
$$
\log |\mathcal{I}(\boldsymbol{\alpha}_j)| = \sum_{k=1}^{K} \log \psi^{(1)}(\alpha_{j k}) + \log (1 - \sum_{k=1}^{K} \frac{\psi^{(1)}(\alpha_{j0})}{\psi^{(1)}(\alpha_{j k})}),
$$
${\alpha}_{j0} = \sum_{k=1}^{K}{\alpha}_{j k}$, and $\psi^{(1)}(\cdot)$ represents the \textit{trigamma} function, defined as $\psi^{(1)}(x) = d \psi(x) / dx = d^2 \ln \Gamma(x) / dx^2 $ (See Appendix for the detailed derivation process). We name the above weighted loss function as adaptive negative EDL loss. Since $\psi^{(1)}(x)$ is a monotonically decreasing function when $x > 0$, class labels with less evidence would be subject to greater penalties to achieve a flatter output. Conversely, once a certain class of evidence is learned, the weight will be reduced (see red number in Firgure \ref{fig:method}), so that the inlier features in the unlabeled dataset do not interfere too much with the learning of outlier features. We also use labeled dataset to enhance the learning of inlier features. We follow $\mathcal{I}$-EDL \cite{deng2023uncertainty} to impose constraints on the uncertain class of labeled samples, outputting a sharp distribution that fits the ground truth.
% $$
% \min_{\boldsymbol{\theta}} \ \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{D}_l} \left[-\log \mathbb{E}_{\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})} [ \mathcal{N}(\boldsymbol{y}|\boldsymbol{p}, \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1}) ]\right].
% $$
The loss function for an labeled sample $(\boldsymbol{x}^l_j, \boldsymbol{y}^l_j) \in \mathcal{D}_l$ can be expressed as:
\begin{equation}
\begin{aligned}
\label{obj_pedl}
% L_i^{\text{P-EDL}} = &\sum_{j=1}^{K} \left( (y_{i j} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 + \frac{p_{i j}( 1 - p_{i j})}{\alpha_{i 0} + 1} \right) \psi^{(1)}(\alpha_{i j}) \\
% & - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_i)|), \\
\mathcal{L}_{j}^{\text{P-EDL}}=&\sum_{k=1}^{K} \left( (y_{j k} - \frac{\alpha_{j k}}{\alpha_{j 0}} )^2 + \frac{\alpha_{j k}( \alpha_{j 0} - \alpha_{j k})}{\alpha_{j 0}^2(\alpha_{j 0} + 1)} \right) \psi^{(1)}(\alpha_{j k}) \\
& - \lambda_2 \log |\mathcal{I}(\boldsymbol{\alpha}_j)|,
% \min_{\boldsymbol{\theta}} \frac{1}{N_l} \sum_{i=1}^{N_l} &( \sum_{j=1}^{K} \left( (y_{i j} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 + \frac{\alpha_{i j}( \alpha_{i 0} - \alpha_{i j})}{\alpha_{i 0}^2(\alpha_{i 0} + 1)} \right) \psi^{(1)}(\alpha_{i j}) \\
% &- \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_i)|),
\end{aligned}
\end{equation}
where $\boldsymbol{y}^l_j$ denotes one-hot encoded ground-truth of $\boldsymbol{x}^l_j$.

% where $p_{ij} = \alpha_{i j}/\alpha_{i 0}$.

\noindent\textbf{Strengthened KL Loss.} 
% Shared by EDL and $\mathcal{I}$-EDL, $\mathcal{L}_{i}^{\text{KL-ORI}}$ aims to reduce the evidence of non-labeled classes and leaves the evidence corresponding to labeled class ignored. To help increase the evidence of labeled class, we propose to utilize KL-divergence to force our predicted Dirichlet distribution $Dir(p_i | {\alpha}_i)$ to approach a new target Dirichlet distribution $Dir(p_i | \beta)$, where ${\beta} = [1, \cdots, P, \cdots, 1] \in \mathbb{R}^{K}_+$ and $P$ is a large target evidence value, e.g. 100. 
% We define our EDL loss function for labeled data as:
%Previous EDL methods use KL divergence loss only to shrink misleading evidence by changing ${\alpha}_i$ to $\hat{\alpha}_i = {\alpha}_i \odot(1 - y_i) + y_i$ and setting target Dirichlet Distribution as $Dir(p_i | \boldsymbol{1}).
KL divergence loss $\mathcal{L}_{j}^{\text{KL-ORI}} $ as part of classical EDL loss, aims to penalize evidence for misleading classes that a sample does not belong to: %that \YJ{do not fit the training data}:
\begin{equation}
\label{obj_kl_ori}
\mathcal{L}_{j}^{\text{KL-ORI}} 
= D_{KL}(Dir(\boldsymbol{p}_j | \hat{\boldsymbol{\alpha}_j}) \Vert Dir(\boldsymbol{p}_j | \boldsymbol{1})).
\end{equation}
where $\hat{\boldsymbol{\alpha}}_j = \boldsymbol{\alpha}_j \odot(1 - \boldsymbol{y}^l_j) + \boldsymbol{y}^l_j$.
However, $\mathcal{L}_{j}^{\text{KL-ORI}} $ only shrinks misleading evidence but leaves non-misleading evidence ignored. To help enhance non-misleading evidence, we propose to utilize a strengthened KL-divergence to force our predicted Dirichlet distribution $Dir( \boldsymbol{p}_j | {\boldsymbol{\alpha}}_j)$ to approach a new target Dirichlet distribution $Dir(\boldsymbol{p}_j | \boldsymbol{\beta})$. In particular, we introduce a new KL divergence term as follows,
\begin{equation}
\begin{aligned}
\label{obj_kl}
\mathcal{L}_{j}^{\text{KL}} 
= & D_{KL}(Dir(\boldsymbol{p}_j | \boldsymbol{\alpha}_j) \Vert Dir(\boldsymbol{p}_j | \boldsymbol{\beta})) \\
= & \log \Gamma(\alpha_{j 0}) - \sum_{k=1}^{K} \log \Gamma(\alpha_{j k})  - \log \Gamma(\beta_{j 0}) + \sum_{k=1}^{K} \log \Gamma(\beta_{j k}) \\
& + \sum_{k=1}^{K}(\alpha_{j k} - \beta_{k}) \left[\psi(\alpha_{j k}) - \psi(\alpha_{j 0}) \right].
\end{aligned}
\end{equation}
For labeled data, we set $\boldsymbol{\beta} = [1, \cdots, P, \cdots, 1] \in \mathbb{R}^{K}_+$, where $P$ is a large target evidence value for ground-truth, e.g. 100. Our strengthened KL-Divergence also applies to unlabeled data by using $\boldsymbol{\beta} = \boldsymbol{1}$ as the target distribution. The detailed closed-form expression of $\mathcal{L}_{j}^{\text{KL}}$ can be found in Appendix.
%To help learn the evidence of both labeled and unlabeled data, 

% \begin{equation}
% \label{obj_final}
% \mathcal{L}_{i}^{\mathcal{P}\text{-EDL}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{I}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\text{KL}},
% \end{equation}

% where

% \begin{equation}
% \begin{aligned}
% \label{obj_final_kl}
% \mathcal{L}_{i}^{\text{KL}} 
% = & D_{KL}(Dir(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert Dir(\boldsymbol{p}_i | \boldsymbol{\beta})) \\
% = & \log \Gamma(\sum_{j=1}^{K} \alpha_{i j}) - \log \Gamma(\sum_{j=1}^{K} \beta_{j}) \\
% &+ \sum_{j=1}^{K} \log \Gamma(\beta_{j}) - \sum_{j=1}^{K} \log \Gamma(\alpha_{i j}) \\
% &+ \sum_{j=1}^{K}(\alpha_{i j} - \beta_{j}) \left[ \psi(\alpha_{i j}) - \psi( \sum_{k=1}^{K} \alpha_{i k}) \right],
% \end{aligned}
% \end{equation}
% and $\mathcal{L}_{i}^{\mathcal{I}\text{-MSE}}$ and $\mathcal{L}_{i}^{|\mathcal{I}|}$ are the same as in \eqref{obj_iedl}. 

% One key feature of open-set SSL is that unlabeled data contains outliers 
% %whose predictions should be a vector of K low-evidence values (i.e. uncertain outliers). 
% and that if a unlabeled sample belongs to outliers keeps unknown. 

% For outliers, our EDL network is expected to output K low-evidence values. Previous works i.e. EDL and $\mathcal{I}$-EDL, focus on optimizing the network to fit a one-hot label $y_i$ given an inlier $x_i$. Therefore, their loss functions can not be directly applied to outliers. On the other hand, negative learning explicitly regulates the network to output low possibilities for part of non-labeled classes. Inspired by negative learning, we propose to explicitly regulate EDL network to output K low evidence values for these outliers. Accordingly, we set the learning target ${\alpha}_{LOW}$ regarding these outliers as $\boldsymbol{1} = [1, \cdots, 1] \in \mathbb{R}^{K}$. However, the problem of outliers selection from unlabeled data arises. Since selection with thresholding causes a trade-off between quantity and quality, we propose to apply low-evidence regulation to all unlabeled data. However, the low-evidence regulation is inappropriate for inliers in unlabeled data. To avoid these inliers being over-penalized, inspired by $\mathcal{I}$-EDL, we introduce FIM to measure the informativeness of evidence carried by each sample and derive a weighted loss function to reduce the weight of unlabeled samples with high evidence. We name the weighted loss function as adaptive negative EDL loss and it is defined as:

% \begin{equation}
% \label{obj_final}
%  \mathcal{L}_{i}^{\mathcal{N}\text{-EDL}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{N}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\mathcal{N}\text{KL}},
% \end{equation}

% \begin{equation}
% \begin{aligned}
% \label{obj_neg_mse}
% &\mathcal{L}_{i}^{\mathcal{N}\text{-MSE}} 
% = \sum_{j=1}^{K}  (\frac{1}{K} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 \cdot \psi^{(1)}(\alpha_{i j}), \\
% \end{aligned}
% \end{equation}

% where

% \begin{equation}
% \begin{aligned}
% \label{obj_final_kl}
% \mathcal{L}_{i}^{\mathcal{N}\text{-KL}} 
% = & D_{KL}(Dir(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert Dir(\boldsymbol{p}_i | \boldsymbol{1})) \\
% = & \log \Gamma(\sum_{j=1}^{K} \alpha_{i j}) - \log \Gamma(K) - \sum_{j=1}^{K} \log \Gamma(\alpha_{i j}) \\
% &+ \sum_{j=1}^{K}(\alpha_{i j} - 1) \left[ \psi(\alpha_{i j}) - \psi( \sum_{k=1}^{K} \alpha_{i k}) \right],
% \end{aligned}
% \end{equation}

% and $\mathcal{L}_{i}^{|\mathcal{I}|}$ are the same as in \eqref{obj_iedl}
% %the uniform Dirichlet distribution $Dir(p_i | \boldsymbol{1})$. 

% To optimize evidential networks, EDL \cite{sensoy2018evidential} derives three kinds of loss functions i.e. mean squared error (MSE), cross-entropy and negative log marginal likelihood. Among them, MSE loss is empirically demonstrated to perform best. Besides MSE loss, EDL introduces a Kullback-Leibler (KL) divergence loss 
% % as the other term of the final loss function to reduce evidence of other $K-1$ non-labeled classes. 
% \dr{as the regular term to penalize evidence for classes that do not fit the training data. Recently proposed $\mathcal{I}$-EDL further introduces Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which the objective loss terms are dynamically re-weighted to force the network to focus more on uncertain classes (i.e. low-evidence classes). More specifically, $\mathcal{I}$-EDL assumes that the target variable $\boldsymbol{y}$ follows a multivariate Gaussian distribution 
% $
% \boldsymbol{y} \sim \mathcal{N}(\boldsymbol{p} , \sigma^2 \mathcal{I}(\boldsymbol{\alpha})^{-1}),
% $
% where $\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})$, $\mathcal{I}(\boldsymbol{\alpha})$ denotes Fisher information matrix(FIM) of $\boldsymbol{p}$, and $\sigma^2$ is the scalar hyperparameter. The optimal parameters $\boldsymbol{\theta}$ are learned by maximizing the marginal likelihood as follows
% $$
% \max_{\boldsymbol{\theta}} \ \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \mathcal{P}} \left[\log \mathbb{E}_{\boldsymbol{p} \sim Dir(\boldsymbol{\alpha})} [ \mathcal{N}(\boldsymbol{y}|\boldsymbol{p}, \sigma^2\mathcal{I}(\boldsymbol{\alpha})^{-1}) ]\right].
% $$
% }
% Given training set $\mathcal{D}_{L} = \{(\boldsymbol{x}_i, \boldsymbol{y}_i)\}_{i=1}^{N}$, the final loss function of $\mathcal{I}$-EDL can be reformulated as:
% \begin{equation}
% \label{obj_iedl}
% \min_{\boldsymbol{\theta}} \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{I}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\text{KL-ORI}},
% \end{equation}
% where 
% \begin{equation}
% \begin{aligned}
% \label{obj_final_mse}
% &\mathcal{L}_{i}^{\mathcal{I}\text{-MSE}} 
% = \sum_{j=1}^{K} \left( (y_{i j} - \frac{\alpha_{i j}}{\alpha_{i 0}} )^2 + \frac{\alpha_{i j}( \alpha_{i 0} - \alpha_{i j})}{\alpha_{i 0}^2(\alpha_{i 0} + 1)} \right) \psi^{(1)}(\alpha_{i j}), \\
% \end{aligned}
% \end{equation}
% $$
% $$
% \begin{equation}
% \begin{aligned}
% \label{obj_final_det}
% &\mathcal{L}_{i}^{|\mathcal{I}|}
% = \sum_{j=1}^{K} \log  \psi^{(1)}(\alpha_{i j}) + \log \left(1 - \sum_{j=1}^{K} \frac{ \psi^{(1)}(\alpha_{i 0})}{ \psi^{(1)}(\alpha_{i j})} \right),
% \end{aligned}
% \end{equation}
% $$
% $$
% \begin{equation}
% %\begin{aligned}
% \label{obj_final_kl}
% \mathcal{L}_{i}^{\text{KL-ORI} }
% = D_{KL}(Dir(\boldsymbol{p}_i | \hat{\boldsymbol{\alpha}}_i) \Vert Dir(\boldsymbol{p}_i | \boldsymbol{1}))
% %= & \log \Gamma(\sum_{j=1}^{K} \hat{\alpha}_{i j}) - \log \Gamma(K) - \sum_{j=1}^{K} \log \Gamma(\hat{\alpha}_{i j}) \\
% %& + \sum_{j=1}^{K}(\hat{\alpha}_{i j} - 1) \left[ \psi(\hat{\alpha}_{i j}) - \psi( \sum_{k=1}^{K} \hat{\alpha}_{i k}) \right],
% %\end{aligned}
% \end{equation}
% In $\mathcal{L}_{i}^{\mathcal{I}\text{-MSE}}$ and $\mathcal{L}_{i}^{|\mathcal{I}|}$, ${\alpha}_{i0} = \sum_{j=1}^{K}{\alpha}_{ij} $ and $\psi^{(1)}(\cdot)$ represents the \textit{trigamma} function, defined as $\psi^{(1)}(x) = d \psi(x) / dx = d^2 \ln \Gamma(x) / dx^2 $. \dr{Note that $\psi^{(1)}(x)$ is a monotonically decreasing function when $x > 0$.} 
% % the weight $\psi^{(1)}(\alpha_{i j})$ decreases with the increase of $\alpha_{i j}$. 
% In $\mathcal{L}_{i}^{\text{KL-ORI}} $, 
% % $\psi$ and $\gamma$ denote the \textit{digamma} function and \textit{Gamma} function respectively.  
% $Dir(p_i | \boldsymbol{1})$ is the uniform Dirichlet distribution and $\hat{\alpha}_i = {\alpha}_i \odot(1 - y_i) + y_i$ is the Dirichlet parameters after removal of the evidence corresponding to the labeled class. 

\noindent\textbf{Overall Loss Function.} 
For the EDL outlier detector, except Eq. (\ref{obj_nedl}), (\ref{obj_pedl}), (\ref{obj_kl}), we add a consistency loss $\mathcal{L}_\text{CON}$ to enhance the smoothness of detector, which is defined as
\begin{equation}
\begin{aligned}
% \mathcal{L}_\text{CON}(\mathcal{D}_{u}) = \frac{1}{N_i + N_o} \sum_{i=1}^{N_i + N_o} \sum_{j=1}^{K} (\alpha_{i j}^{s} - \alpha_{i j}^{w})^2,
\mathcal{L}_\text{CON}(\mathcal{D}_{u}) = \frac{1}{N_i + N_o} \sum_{j=1}^{N_i + N_o} \|\boldsymbol{\alpha}_{j}^{s} - \boldsymbol{\alpha}_{j}^{w}\|^2_2,
\end{aligned}
\end{equation}
% where $\alpha_{i j}^{s}, \alpha_{i j}^{w}$
where $\boldsymbol{\alpha}_{j}^{s}, \boldsymbol{\alpha}_{j}^{w}$ represent the predicted evidence vector of strong and weak augmentations of the same unlabeled sample. For the Softmax classifier, we compute the standard cross-entropy loss $\mathcal{L}_\text{CE}$ for labeled data and the FixMatch loss $\mathcal{L}_\text{FM}$ for the high-certainty inliers $\mathcal{I}_{u}$ identified by the EDL detector.
Therefore, the overall loss function of our proposed ANEDL can be expressed as
\begin{equation}
\begin{aligned}
\label{obj_final}
\mathcal{L}_\text{ANEDL} = &\mathcal{L}_\text{CE}(\mathcal{D}_{l}) + \mathcal{L}_\text{FM}(\mathcal{I}_{u}) \\
+ & \mathcal{L}_\text{ANO}(\mathcal{D}_{l}, \mathcal{D}_{u}) + \lambda_\text{CON} \mathcal{L}_\text{CON}(\mathcal{D}_{u}),
\end{aligned}
\end{equation}
where 
\begin{equation}
\begin{aligned}
\mathcal{L}_\text{ANO}(\mathcal{D}_{l}, \mathcal{D}_{u}) = 
& \lambda_\text{P-EDL} \frac{1}{N_l} \sum_{j=1}^{N_l} (\mathcal{L}_j^{\text{P-EDL}} + \mathcal{L}_{j}^{\text{KL}}) \\
+ & \lambda_\text{N-EDL} \frac{1}{N_i + N_o} \sum_{j=1}^{N_i + N_o} (\mathcal{L}_j^{\text{N-EDL}} + \mathcal{L}_{j}^{\text{KL}}),
\end{aligned}
\end{equation}
$\lambda_\text{P-EDL}, \lambda_\text{N-EDL}$ and $\lambda_\text{CON}$ are the hyperparameters used to control the trade-off for each objective.

To minimize our overall loss, we train our model for two stages. In the first stage, we pre-train our model with $\mathcal{L} = \mathcal{L}_\text{CE}(\mathcal{D}_{l}) +  \mathcal{L}_\text{ANO}(\mathcal{D}_{l}, \mathcal{D}_{u}) + \lambda_\text{CON} \mathcal{L}_\text{CON}(\mathcal{D}_{u})$ which drops $\mathcal{L}_\text{FM}$ for $E_{FM}$ epochs. In second stage, we start self-training of classifier and use $\mathcal{L}_\text{ANEDL}$ as loss function. Before every self-training epoch, we calibrate softmax head and EDL head to calculate uncertainty metric to select the inliers for softmax head to conduct self-training. 
% \begin{equation}
% \begin{aligned}
% \mathcal{L}_\text{CON}(\mathcal{D}_{u}) = \frac{1}{N_u} \sum_{i=1}^{N_u} \sum_{j=1}^{K} (\alpha_{i j}^{s} - \alpha_{i j}^{w})^2,
% \end{aligned}
% \end{equation}
% $\alpha_{i j}^{s}, \alpha_{i j}^{w}$ represent the predicted evidence of strong and weak augmentation of unlabeled sample, and $\mathcal{I}$ denotes the high-certainty inliers we identified by the EDL detector. Note that the consistency loss $\mathcal{L}_\text{CON}$ is used to enhance the smoothness of EDL outlier detector. 

% \noindent\textbf{Overall Framework.}

% We design a framework consisting of an EDL head and a softmax head for outlier detection and classification, respectively. To optimize the EDL head, we minimize $\mathcal{L}_{i}^{\text{EDL}}$ for labeled data and $\mathcal{L}_{i}^{\text{EDL}}$ for unlabeled data. To optimize the softmax head, we compute cross-entropy loss for labeled data. Since a randomly initialized network can not select inliers from unlabeled data, we first pre-train the network for $E_{fm}$ epochs with L. After the pre-training, we start self-training of classifier and select the inliers before every self-training epoch. To select the inliers, we do not derive confidence metric solely from EDL prediction, which ignores the fact that selected inliers are used by softmax head. Instead, we calibrate softmax prediction and EDL prediction to give confidence metric. Specifically, given a unlabeled sample $x_u$, the softmax head first predict pseudo label $\hat{k} = {argmax}_j C(F(x_u))$. Then we use the $\hat{k}^{th}$ evidence value ${\alpha}_{\hat{k}}$ predicted by the EDL head as confidence metric and take top-M most confident unlabeled samples as inliers. Here, M is a hyper-parameter. With the calibration, pseudo-labeled class (i.e. $\hat{k}^{th}$ class) which will be used for self-training, is predicted to have high evidence collected from labeled data. With inliers selected from unlabeled data, we compute FixMatch loss $\mathcal{L}_{fm}$ to optimize our network. The following is the final loss function of our method:

% \begin{equation}
% \label{obj_final}
%  \mathcal{L}_{i}^{\text{ALL}} = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}_{i}^{\mathcal{N}\text{-MSE}} -\lambda_1 \mathcal{L}_{i}^{|\mathcal{I}|} + \lambda_2 \mathcal{L}_{i}^{\mathcal{N}\text{KL}},
% \end{equation}

% Our trained network is also required to detect ouliers in test data. Different from detecting outliers in unlabeled data, detected inliers will not be used for self-training. Thus taking softmax prediction into consideration is not necessary regarding confidence metric. Based on the evidence vector $\alpha$ predicted by our EDL head, we can derive many kinds of metrics e.g. mutual information, expected entropy, and total evidence. They empirically find total evidence ${\alpha}_{i0} = \sum_{j=1}^{K}{\alpha}_{ij}$
% achieves best uncertainty estimation performance. For a descending sorted evidence vector, we think only top-k evidence values can effectively reflect the amount of support collected from data. While the last evidence values tend to be randomly sampled due to the large difference between the sample and corresponding classes. These random sampled values will bring considerable noise to confidence metric when class number $K$ is large. Therefore, we propose to take total top-k evidence as our confidence metric.

%However,  DST \cite{chen2022debiased} argues that pseudo labels generated for self-training are biased. To avoid biased pseudo labels accumulate the bias of classification head which is used for inference, DST \cite{chen2022debiased} introduces a completely parameter independent pseudo head for pseudo labeled data. So that classification head is only trained with labeled data and pseudo-labeled data is only used for representation learning. However, a completely parameter independent pseudo classification head has a different classification boundary with classification head, and thus learns a representation that can not perfectly fit classification head. To sovle this problem, we propose to copy the parameter of classification head to pseudo classification head 

%DST \cite{chen2022debiased}

%Therefore, we use counterfactual reasoning and adaptive marginal loss both proposed in DebiasPL \cite{wang2022debiased}, to handle biased pseudo labels. 

%DebiasPL \cite{wang2022debiased} and
%------------------------------------------------------------------------
\begin{table*}[t]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{clccclcclcclc}
\hline
Dataset                  &  & \multicolumn{3}{c}{CIFAR-10}                  &  & \multicolumn{2}{c}{CIFAR-100}             &  & \multicolumn{2}{c}{CIFAR-100}             &  & ImageNet-30         \\ \cline{1-1} \cline{3-5} \cline{7-8} \cline{10-11} \cline{13-13} 
Inlier/Outlier Classes &  & \multicolumn{3}{c}{6/4}                       &  & \multicolumn{2}{c}{55/45}                 &  & \multicolumn{2}{c}{80/20}                 &  & 20/10               \\ \cline{1-1} \cline{3-5} \cline{7-8} \cline{10-11} \cline{13-13} 
No. of labeled samples   &  & 50         & 100        & 400                 &  & 50                  & 100                 &  & 50                  & 100                 &  & 10\%                \\ \hline
FixMatch  \cite{sohn2020fixmatch}               &  & 56.1$\pm$0.6 & 60.4$\pm$0.4 & 71.8$\pm$0.4          &  & 72.0$\pm$1.3          & 79.9$\pm$0.9          &  & 64.3$\pm$1.0          & 66.1$\pm$0.5          &  & 88.6$\pm$0.5          \\
MTC  \cite{yu2020multi}                    &  & 96.6$\pm$0.6 & 98.2$\pm$0.9 & 98.9$\pm$ 0.1         &  & 81.2$\pm$3.4          & 80.7$\pm$4.6          &  & 79.4$\pm$1.0          & 73.2$\pm$3.5          &  & 93.8$\pm$0.8          \\
OpenMatch \cite{saito2021openmatch}               &  & \textbf{99.3$\pm$0.9} & \textbf{99.7$\pm$0.2} & 99.3$\pm$0.2         &  & 87.0$\pm$1.1          & 86.5$\pm$2.1          &  & 86.2$\pm$0.6          & 86.8$\pm$1.4          &  & 96.4$\pm$0.7          \\ \hline
Ours                     &  &  98.5$\pm$0.2          &   99.0$\pm$0.2         & \textbf{99.4$\pm$0.1} &  & \textbf{90.3$\pm$0.1} & \textbf{91.1$\pm$0.4} &  & \textbf{89.2$\pm$0.5} & \textbf{91.1$\pm$1.0} &  & \textbf{96.6$\pm$0.3} \\ \hline
\end{tabular}}
\end{center}
%\vspace{-1mm}
\caption{Mean and standard deviation of AUROC (\%) on CIFAR-10, CIFAR-100 and ImageNet-30. Higher is better. For CIFAR-10 and CIFAR-100, the three runs correspond to three different folds. For ImageNet-30, the three runs are conducted on the same folder but with different random seeds. The best results are in bold.}
\label{tab:auroc}
\end{table*}

\begin{table*}[t]
% \centering
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{clccclcclcclc}
\hline
Dataset                  &  & \multicolumn{3}{c}{CIFAR-10}          &  & \multicolumn{2}{c}{CIFAR-100} &  & \multicolumn{2}{c}{CIFAR-100} &  & ImageNet-30 \\ \cline{1-1} \cline{3-5} \cline{7-8} \cline{10-11} \cline{13-13} 
Inlier/Outlier Classes &  & \multicolumn{3}{c}{6/4}               &  & \multicolumn{2}{c}{55/45}     &  & \multicolumn{2}{c}{80/20}     &  & 20/10       \\ \hline
No. of labeled samples   &  & 50         & 100        & 400         &  & 50            & 100           &  & 50            & 100           &  & 10\%        \\ \hline
FixMatch \cite{sohn2020fixmatch}                &  & 43.2$\pm$1.2 & 29.8$\pm$0.6 & 16.3$\pm$0.5  &  & 35.4$\pm$0.7    & 27.3$\pm$0.8    &  & 41.2$\pm$0.7    & 34.1$\pm$0.4    &  & 12.9$\pm$0.4  \\
MTC \cite{yu2020multi}                      &  & 20.3$\pm$0.9 & 13.7$\pm$0.9 & 9.0$\pm$0.5 &  & 33.5$\pm$1.2    & 27.9$\pm$0.5    &  & 40.1$\pm$0.8    & 33.6$\pm$0.3    &  & 13.6$\pm$0.7  \\
OpenMatch  \cite{saito2021openmatch}              &  & 10.4$\pm$0.9 & 7.1$\pm$0.5  & 5.9$\pm$0.5 &  & 27.7$\pm$0.4    & 24.1$\pm$0.6    &  & 33.4$\pm$0.2    & 29.5$\pm$0.3    &  & \textbf{10.4$\pm$1.0}  \\ \hline
Ours                     &  &   \textbf{9.6$\pm$0.4}         &  \textbf{7.0$\pm$0.4}          & \textbf{5.5$\pm$0.2}   &  & \textbf{26.7$\pm$0.2}    & \textbf{23.2$\pm$0.6}    &  & \textbf{32.3$\pm$0.3}    & \textbf{28.5$\pm$0.2}    &  & 11.3$\pm$0.4  \\ \hline
\end{tabular}}
\end{center}
%\vspace{-2mm}
\caption{Error rate (\%) corresponding to Table \ref{tab:auroc}. Lower is better. The best results are in bold.}
\label{tab:errorrate}
\end{table*}


\subsection{Uncertainty Metric for Open-set SSL}
\label{method:unc}
\noindent\textbf{Inlier Selection in Self-training.} To help better enhance recognition performance with unlabeled data, we aim to select inliers with accurate pseudo labels from unlabeled data for self-training. In FixMatch, pseudo label is generated by Softmax head and will also be used to softmax head as a supervision signal. Therefore, we propose to 
% utilize the prediction of 
leverage EDL head to quantify the uncertainty of the class predicted by softmax head
%\dr{unlabeled samples}
% the pseudo label 
 and select inliers with 
% low uncertainty 
confident prediction which are more likely to be accurately pseudo-labeled. 
%To select the inliers, we do not derive confidence metric solely from EDL prediction, which ignores the fact that selected inliers are used by softmax head. Instead, we calibrate softmax prediction and EDL prediction to give confidence metric. 
Specifically, given an unlabeled sample $\boldsymbol{x}_u$, the Softmax head first predicts pseudo label $\hat{k} = \arg \max_k C(F(\boldsymbol{x}_u))$. Then we use the $\hat{k}^{th}$ evidence value of $\boldsymbol{\alpha}_{u}$ 
% ${\alpha}_{\hat{k}}$ 
predicted by the EDL head as uncertainty metric and select top-O most certain unlabeled samples as inliers. Here, O is a hyperparameter. 
%With the calibration, pseudo-labeled class (i.e. $\hat{k}^{th}$ class) which will be used for self-training, is predicted to have high evidence collected from labeled data. With inliers selected from unlabeled data,

\noindent\textbf{Outlier Detection for Inference.}
% Our trained network is also required to detect ouliers in test data. 
Detecting outliers in test data is also one of the tasks of Open-set SSL. 
% Unlike selecting inliers from unlabeled data, detected inliers in test data will not be used for self-training. 
Unlike selecting inliers from unlabeled data, detected inliers in test data do not have to consider calibrating with Softmax. Therefore, 
% our target 
the uncertainty metric for inference is to accurately quantify epistemic uncertainty for better separation between inliers and outliers.
%Thus taking softmax prediction into consideration is not necessary regarding confidence metric. 
Based on the evidence vector $\boldsymbol{\alpha}$ predicted by EDL head, we can derive 
various uncertainty metrics, i.e. mutual information, differential entropy, and total evidence.
% many kinds of uncertainty metrics, e.g. mutual information, expected entropy, and total evidence. 
Previous EDL methods 
% empirically find 
demonstrate total evidence 
% ${\alpha}_{i0} = \sum_{j=1}^{K}{\alpha}_{ij}$
${\alpha}_{0} = \sum_{k=1}^{K}{\alpha}_{k}$
achieves the best outlier detection performance \cite{sensoy2018evidential, deng2023uncertainty}. 
However, we empirically find that classes with small evidence values may form a long-tail effect because the EDL detector assigns a non-negative evidence value to each class. Although these evidence values are small, their total amount is not negligible, especially when dealing with tasks with a large number of classes.
% especially in classification tasks with a large number of classes. 
To avoid these long-tailed evidence values disturbing the separation between inliers and outliers, we propose to sort the evidence vectors in descending order and use only the sum of the top-M evidence values as our uncertainty metric for inference.
% For a descending sorted evidence vector, we think only top-M evidence values can effectively reflect the amount of support collected from data. While the last $K-M$ evidence values tend to be small random numbers with noise, due to the large difference between the sample and corresponding classes. Accumulated noise of these evidence values will disturb separation between inliers and outliers. Therefore, we propose to remove these evidence values and take summation of top-M evidence values as our uncertainty metric for inference.





% \centering




\section{Experiments}

In this section, we conduct extensive experiments to compare the performance of our method with previous SOTA methods and analyze several key components of our method.
%-------------------------------------------------------------------------
\subsection{Datasets}
We evaluate our method on four public datasets. Given a dataset, we first split its class set into inlier and outlier class set. Next, we randomly select inliers to form labeled, unlabeled, validation and test dataset as in SSL. Then all outliers will be randomly arranged into unlabeled dataset and test dataset. With four datasets, we set several experimental settings with different numbers of labeled samples and different amounts of inliers/outliers classes. 

\noindent\textbf{CIFAR-10} \cite{krizhevsky2009learning} has 10 categories and each category has 6,000 images of size $32 \times 32$. 1,000 images of each category are used for test. We choose 6/4 classes as inlier/outlier classes. The amount of labeled images of each inlier class varies in $\{50,100,400\}$. And the number of validation images of each inlier class is 50.

\noindent\textbf{CIFAR-100} \cite{krizhevsky2009learning} is composed of 60,000 images of size $32 \times 32$  from 100 classes. 100 images of each class are used for test. We choose 55/80 classes as inlier classes, resulting in 45/20 outliers classes. The amount of labeled images of each inlier class varies in $\{50,100\}$. And the number of validation images of each inlier class is 50.

\noindent\textbf{ImageNet-30} \cite{hendrycks2016baseline} is a subset of ImageNet \cite{deng2009imagenet}. It has 30 classes and each class contains 1,300 images of size $224 \times 224$. 100 images of each class are used for test. We choose 20 classes as inlier classes. The amounts of labeled and validation images of each inlier class are both 130. 

\noindent\textbf{Mini-ImageNet} \cite{vinyals2016matching} is also a subset of ImageNet. It has 100 classes and each class contains 600 images of size $84 \times 84$. 100 images of each class are used for test. The number of inlier classes varies in $\{50,60,80\}$. The amounts of labeled and validation images of each inlier class are both 50. 
%\YJ{We choose 20 classes as inlier classes delete?} 

%-------------------------------------------------------------------------
\subsection{Implementation Details}
 We employ a randomly initialized Wide ResNet-28-2 \cite{zagoruyko2016wide} to conduct experiments on CIFAR-10 and CIFAR-100, and a randomly initialized ResNet-18 \cite{he2016deep} for experiments on ImageNet-30 and Mini-ImageNet. The Softmax head is a linear layer and the EDL head is non-linear, which is composed of 4 MLP layers. For the pre-training stage, we set its length $E_{FM}$ as 10 for all experiments.
 %and $\lambda_{FM}$ is set to 0 in pre-training stage and 1 in self-training stage.
 We set the length of an epoch as 1024 steps, which means we select a new set of inliers every 1024 steps during self-training. For hyper-parameters of FixMatch, we set them the same as in OpenMatch \cite{saito2021openmatch}. We report the hyper-parameters of our loss function in the Appendix. We adopt SGD optimizer with 0.0001 weight decay and 0.9 momentum to train our model by setting initial learning rate as 0.03 with the cosine decrease policy. We set the batch size of labeled and unlabeled samples as 64 and 128 respectively for all experiments. Experiments on ImageNet-30 are conducted with a single 24-GB GPU and other experiments are conducted with a single 12-GB GPU. All experiments in this work are conducted over 3 runs with different random seeds, and we report their mean and standard derivation.


%\begin{table}[]
%\centering
%\begin{tabular}{cc|c|c}
%\hline
%$\alpha$ & $\beta$ & Error rate & AUROC      \\ \hline
%$\hat{\boldsymbol{\alpha}}$ & $\boldsymbol{1}$& 27.0$\pm$0.2 & 89.9$\pm$0.3 \\
%$\boldsymbol{\alpha}$ & $\boldsymbol{{\beta}_{100}}$& \textbf{26.7$\pm$0.2} & %\textbf{90.3$\pm$0.1} \\
%$\boldsymbol{\alpha}$ & $\boldsymbol{{\beta}_{200}}$ &            &            \\ \hline
%\end{tabular}
%\end{table}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.46\textwidth]{mini1.pdf}
	%\vspace{-3mm}
	\caption{Comparison with OpenMatch on Mini-ImageNet. We use the same fold to conduct three runs and report the mean of three runs at the top of each bar.}
	\label{fig:mini}
	%\vspace{-3mm}
\end{figure}

\subsection{Comparison with Other Methods}

To illustrate the effectiveness of our method on Open-set SSL,
% To illustrate the efficacy of our method, 
we compare it against several baseline  %\YJ{state-of-the-art?} 
methods, including FixMatch \cite{sohn2020fixmatch}, MTC \cite{yu2020multi} and OpenMatch \cite{saito2021openmatch}. 
% To evaluate the performance, we follow the open-set assumption of Open-set SSL that the test dataset contains both inliers and outliers. 
We use classification error rate to evaluate the performance of classifier in inliers
% . To evaluate the performance of outlier detection, we use AUROC which is the general evaluation metric of novelty detection \cite{hendrycks2016baseline}. 
and AUROC to evaluate the performance of outlier detection. AUROC is the general evaluation metric for novelty detection \cite{hendrycks2016baseline}. 
% The results of baseline methods in Table \ref{tab:errorrate} and Table \ref{tab:auroc} comes from OpenMatch \cite{saito2021openmatch}. And the results of OpenMatch in Figure \ref{fig:mini} are obtained based on the author's implementation. 
Table \ref{tab:auroc} and Table \ref{tab:errorrate} show the AUROC and error rate on the CIFAR-10, CIFAR-100, and ImageNet-30, respectively, where the results of other methods are quoted from OpenMatch \cite{saito2021openmatch}. Figure \ref{fig:mini} reports the experimental results on Mini-ImageNet, where the results of OpenMatch are obtained based on the author's own implementation.

% Table \ref{tab:errorrate} and Table \ref{tab:auroc} show the error rate and AUROC repsectively. In 
% \dr{As shown in} Table \ref{tab:errorrate}, the comparison between Open-set SSL methods (i.e. MTC, OpenMatch and ours) and SSL method (i.e. FixMatch) shows that by excluding outliers in unlabeled data from self-training, Open-set SSL methods can mitigate the problem of incorrect pseudo label of outliers. And thus inlier recognition performance can be largely promoted. Among Open-set SSL methods, overall speaking, our method achieves the best performance in terms of error rate. Because we can better quantify the classification uncertainty and select inliers preferred by self-training with our proposed method \yy{and we also remove bias in pseudo label with method proposed in DebiasPL \cite{wang2022debiased}}. 
% \furui{relate this results with statements from previous sections. your goal is to prove your claim by empirical results.}

As shown in Table \ref{tab:auroc}, we can see that our method outperforms the state-of-the-art method, i.e., OpenMatch, in most experiment settings with respect to AUROC.
% In particular, the improvements of our methods are more significant when the number of classes is larger. Moreover,
In Table \ref{tab:errorrate}, our proposed method also achieves state-of-the-art performance in terms of error rate in 7 out of 8 settings. These results illustrate that our method is more advanced in epistemic uncertainty quantification and 
% has better separation between outliers and inliers. 
has superior performance in outlier detection as well as inlier classification.
Notably, in the more challenging settings with a large number of classes, i.e., settings on CIFAR-100, our method 
% shows a clear advantage over OpenMatch with a more than $\textbf{3\%}$ margin. 
significantly outperforms OpenMatch in both error rate and AUROC, with improvements of $\sim \textbf{1\%}$ and $\sim \textbf{3\%}$. It shows our multinomial detector is more robust 
% to complex modalities 
than the binary detector in identifying outliers from complex modalities when dealing with K-way classification tasks with large K. To further demonstrate our superiority in settings with a large number of inlier classes, we compare our method with OpenMatch 
% in 60/80 inlier classes and 50 labeled samples per class settings on Mini-ImageNet. Mini-ImageNet is more challenge than CIFAR-100 with larger image size and more complex patterns. Again, our method consistently outperforms OpenMatch with respect to both error rate and AUROC as shown in Figure \ref{fig:mini}.
on Mini-ImageNet, which is even more difficult than CIFAR-100 with larger image size and more complex patterns. %We set up experiments with $\{50, 60, 80\}$ inlier classes and 50 labeled samples per class. 
As shown in Figure \ref{fig:mini}, our method consistently outperforms OpenMatch with respect to both AUROC and error rate.

%which means K is large for K-way classification

%-------------------------------------------------------------------------
\subsection{Ablation Studies}
We next conduct ablation studies to evaluate each component of our method. 
% All the experiments are conducted in 55 inlier classes and 50 labeled samples setting on Cifar-100.
All experiments are conducted under the setting of 55 inlier classes and 50 labeled samples per class on CIFAR-100.
We report the mean and standard deviation of three runs on three different folds.





\noindent\textbf{Component analysis of ANO.}
Our ANO loss consists of two components, i.e. negative optimization and adaptive loss weight. For ``without NO and Adaptive'', we directly replace $\mathcal{L}_\text{ANO}$ to classical EDL loss, which means we apply no explicit constraints to outliers in unlabeled data. Since the information contained in unlabeled data keeps underutilized, 
% the performance is suboptimal. 
both classification and outlier detection performance are suboptimal as shown in Table \ref{table:ano}.
For ``without Adaptive'', we remove our adaptive loss weight from $\mathcal{L}_\text{ANO}$, which means 
% we do not use 
not using the FIM to identify the amount of information for each class of unlabeled samples. As a result, all unlabeled samples including inliers are equally regulated to output K-low evidence values. Although negative optimization compresses evidence values of unlabeled outliers, 
% it also interferes with the learning of unlabeled inliers and the
it simultaneously depresses the evidence of unlabeled inliers, making inlier features and outlier features interfere with each other. Therefore, its improvement compared to ``without NO and Adaptive'' is still limited. 
Our proposed $\mathcal{L}_\text{ANO}$, i.e., ``with NO and  Adaptive'', peaks the best performance by sufficiently compressing evidence values of unlabeled outliers without interfering with the learning of unlabeled inliers.
%-------------------------------------------------------------------------
\begin{table}[t]
\begin{center}
% \resizebox{0.40\textwidth}{!}{
\begin{tabular}{cc|c|c}
\hline
NO & Adaptive & AUROC & Error rate     \\ \hline
     &          & 88.4$\pm$0.4 & 27.0$\pm$0.3\\
$\checkmark$     &          & 89.0$\pm$0.3 & 26.9$\pm$0.3\\
$\checkmark$     & $\checkmark$ & \textbf{90.3$\pm$0.1} & \textbf{26.7$\pm$0.2}\\ \hline
\end{tabular}
% }
\end{center}
%\vspace{-2mm}
\caption{Component analysis of Adaptive Negative Optimization (ANO).}
\label{table:ano}
\end{table}


\begin{table}[t]
\begin{center}
% \resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c}
\hline
KL divergence & AUROC & Error rate     \\ \hline
$D_{KL}(D(\boldsymbol{p}_i | \hat{\boldsymbol{\alpha}}_i) \Vert D(\boldsymbol{p}_i | \boldsymbol{1}))$& 89.9$\pm$0.3 & 27.0$\pm$0.2\\
$D_{KL}(D(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert D(\boldsymbol{p}_i | \boldsymbol{{\beta}_{100}}))$& \textbf{90.3$\pm$0.1} &  \textbf{26.7$\pm$0.2} \\
$D_{KL}(D(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert D(\boldsymbol{p}_i | \boldsymbol{{\beta}_{200}}))$ &  89.9$\pm$0.9        &     26.9$\pm$0.2       \\ \hline
\end{tabular}
% }
\end{center}
%\vspace{-2mm}
\caption{Evaluation on KL divergence.}
\label{table:kl}
\end{table}
\noindent\textbf{Evaluation on KL divergence.}
%Previous EDL methods use KL divergence loss only to shrink misleading evidence by changing ${\alpha}_i$ to $\hat{\alpha}_i = {\alpha}_i \odot(1 - y_i) + y_i$ and setting target Dirichlet Distribution as $Dir(p_i | \boldsymbol{1})$. 
Besides shrinking misleading evidence, we propose to boost non-misleading evidence with KL divergence loss to improve the separation of inliers and outliers. In Eq. (\ref{obj_kl}), we keep $\boldsymbol{\alpha}_i$ unchanged and setting target Dirichlet Distribution to $Dir(\boldsymbol{p}_i | \boldsymbol{\beta})$, where $\boldsymbol{\beta} = [1, \cdots, P, \cdots, 1] $. $\boldsymbol{{\beta}_{100}}$ and $\boldsymbol{{\beta}_{200}}$ denote $P=100$ and $P=200$, respectively. Table \ref{table:kl}
% Results 
shows that our strengthen KL divergence can effectively improve the separation of inliers and outliers by boosting non-misleading evidence. However, a too large $P$ may cause an unstable training process, 
% and thus 
resulting in suboptimal performance.


\noindent\textbf{Evaluation of uncertainty metric during inference.} Previous EDL methods propose to use the sum of all K evidence values as a metric to measure epistemic uncertainty, %Given a sorted evidence vector, 
%we assume that last few evidence values corresponding to less relative classes are noisy and the accumulated noise may interfer the separation of inliers and outliers. Therefore, 
while we propose to use the sum of top-M evidence values as the metric. For each experiment, we use the same trained model to evaluate performance and only change how uncertainty metric is calculated. We also compare the configuration of the uncertainty metric used for inlier selection in self-training, denoted as ``Calibration''. Since uncertainty metric during inference influences outlier detection more, we report AUROC in Figure \ref{fig:cmtest}. Results show that the sum of top $M=25$ evidence values is a better metric to quantify epistemic uncertainty, compared with the sum of all $K=55$ evidence values. Note that the improvements steadily appear in each trained model. Comparison between $M=25$ and $M=3$ (or $1$) also shows that a small M misses useful evidence values and is insufficient to measure epistemic uncertainty. 

\begin{figure}[t]
	\centering
	\includegraphics[width=0.5\textwidth]{ab1.pdf}
	\caption{Evaluation of uncertainty metric during inference. Only AUROC is reported. We show the mean of three runs at the top of each box. M denotes the sum of top-M evidence values. Calibration denotes uncertainty metric used in ST.}
	\label{fig:cmtest}
\end{figure}

\begin{table}[t]
\begin{center}
% \resizebox{0.40\textwidth}{!}{
\begin{tabular}{c|c|c}
\hline
Uncertainty Metric in ST        &    AUROC       &   Error rate             \\ \hline
w/o calibration &  89.8$\pm$0.2 &  27.8$\pm$0.2        \\ \hline
w calibration   & \textbf{90.3$\pm$0.1} &  \textbf{26.7$\pm$0.2} \\ \hline
\end{tabular}
% }
\end{center}
\caption{Evaluation of uncertainty metric in ST. ``w/o calibration'' denotes the sum of top-25 evidence values (M=25) as uncertainty metric, which is used for inference.}
\label{table:conf}
\end{table}

\noindent\textbf{Evaluation of uncertainty metric in self-training.} The choice of uncertainty metric is important to inliers selection in self-training (ST). 
To illustrate the effectiveness of our proposed uncertainty metric for ST (denoted as ``with calibration'') , we compare it  with the sum of the top 25 evidence values (denoted as ``without calibration'') which achieves the best outlier detection performance during inference (see Figure \ref{fig:cmtest}). We use these two different metrics for self-training respectively and conduct inference with the latter one.
%as uncertainty measure of ST and denote ``without calibration''. 
% For ``without calibration'', we use the summation of top-25 evidence values which has the best outlier detection performance as shown in 
% Figure \ref{fig:cmtest}. 
%It effectively measures the epistemic uncertainty of the whole sample. The metric we propose in Section \ref{method:unc} is then denoted ``with calibration".
%As shown in Table \ref{table:conf}, 
Although ``without calibration'' shows superior performance during inference, it performs poorly if directly used in ST as shown in Table \ref{table:conf}. Whereas our proposed ``with calibration'', which quantifies epistemic uncertainty of the class predicted by Softmax head, shows clear advantages over "without calibration". 
% Despite its superiority in outlier detection, it does not consider the prediction of Softmax head, which leads to a sub-optimal performance. For ``with calibration'', we calculate the metric as claimed in section 3.3. It quantifies epistemic uncertainty of the class predicted by Softmax head and shows a clear superiority compared with ``without calibration''. 
The superiority indicates that for inlier selection in self-training, we should calibrate Softmax head and EDL head to quantify epistemic uncertainty of predicted class, instead of the whole sample estimated by the sum of top-25 evidence values.   



\noindent\textbf{Evaluation of DebiasPL.} As mentioned in ``Joint Optimization with Softmax and EDL'' of Section 3.2, we utilize counterfactual reasoning and adaptive margins proposed in DebiasPL \cite{wang2022debiased} to remove the bias of pseudo label in Fixmatch. In Table \ref{table:debias}, we study the influence of DebiasPL. The results of OpenMatch come from Table \ref{tab:auroc} and \ref{tab:errorrate}, and OpenMatch is trained without DebiasPL. Therefore we remove DebiasPL from our method for a fair comparison. We can see that without DebiasPL, our method still outperforms OpenMatch with respect to both AUROC and error rate, because we select better inliers for self-training with proper uncertainty metric. Also, the comparison between ``with'' and ``without DebiasPL'' proves that DebiasPL can help further improves our performance with bias in pseudo label alleviated.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c}
\hline
Method           & AUROC      & Error rate \\ \hline
OpenMatch        & 87.0$\pm$1.1 & 27.7$\pm$0.4 \\ \hline
Ours(w/o DebiasPL) & 90.0$\pm$0.5 & 27.0$\pm$0.2 \\ \hline
Ours(w DebiasPL)   & \textbf{90.3$\pm$0.1} & \textbf{26.7$\pm$0.2} \\ \hline
\end{tabular}
\caption{Evaluation on DebiasPL.}
\label{table:debias}
\end{table}

%Despite its superiority in outlier detection, it does not take the prediction of softmax head into consideration which leads to a sub-optimal performance with repect to both error rate and AUROC. 
%Although it selects low epistemic uncertainty samples, 


\section{Conclusion}

In this work, we propose a novel framework, adaptive negative evidential deep learning (ANEDL), for open-set semi-supervised learning (Open-set SSL). ANEDL adopts evidential deep learning (EDL) to Open-set SSL for the first time and designs novel adaptive negative optimization method. In particular, EDL, as an advanced uncertainty quantification method, is deployed to estimate different types of uncertainty for inliers selection in self-training and outlier detection in inference. Furthermore, to enhance the separation between inliers and outliers, we propose adaptive negative optimization to explicitly compress the evidence value of outliers in unlabeled data and avoid interfering with the learning of inliers in unlabeled data with adaptive loss weight. Our extensive experiments on four datasets demonstrate that our method is superior to other state-of-the-art methods.

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
