
\section*{A.Derivation}

This section provides some derivation presented in the main text, including the log determinant of the Fisher Information Matrix (FIM) of Dirichlet distribution, and the close-form expression of $\mathcal{L}_{j}^{\text{N-EDL}}$ and $\mathcal{L}_{j}^{\text{KL}}$ in Eq.(\ref{obj_nedl}) and Eq.(\ref{obj_kl}), respectively. 

\subsection*{A.1.FIM Derivation for Dirichlet Distribution}
\label{app_obj_fisher}

According to the definition of the Fisher Information Matrix (FIM), we can obtain the FIM of Dirichlet distribution as:
\begin{equation}
\begin{aligned}
\mathcal{I}(\boldsymbol{\alpha}) = \E \left[ \frac{\partial \ell}{\partial \boldsymbol{\alpha}} \frac{\partial \ell}{\partial \boldsymbol{\alpha}^{T}}\right] \in \mathbb{R}^{K \times K},
\end{aligned}
\tag{9}
\end{equation}
where $\ell = \log Dir(\vp|\boldsymbol{\alpha})$ is the log-likelihood function, and $Dir(\vp|\boldsymbol{\alpha})=\frac{\Gamma\left(\alpha_{0}\right)}{\prod_{k=1}^{K} \Gamma\left(\alpha_{k}\right)} \prod_{k=1}^{K} p_{k}^{\alpha_{k}-1}, \alpha_{0}=\sum_{k=1}^{K} \alpha_{k}$.
% , and $\boldsymbol{\alpha} = f(\vx) + \boldsymbol{1} \in \mathbb{R}_{+}^{K}$. 
By applying Lemma 5.3 in \cite{lehmpointann2006theory}, the FIM can be expressed as $\mathcal{I}(\boldsymbol{\alpha}) = \E_{Dir(\vp|\boldsymbol{\alpha})} \left[ - \partial^2 \ell /  \partial \boldsymbol{\alpha} \boldsymbol{\alpha}^{T} \right]$. Hence, we can calculate each element by
% Calculate $\mathcal{I}(\boldsymbol{\alpha})$:
$$
% \begin{equation}
\begin{aligned}
 \relax[\mathcal{I}(\boldsymbol{\alpha})]_{i j} &= \E_{Dir(\vp|\boldsymbol{\alpha})} \left[ - \frac{\partial^2}{\partial \alpha_i \partial \alpha_j } \log Dir(\vp|\boldsymbol{\alpha}) \lvert \boldsymbol{\alpha} \right] \\
& = \E_{Dir(\vp|\boldsymbol{\alpha})} \left[ - \frac{\partial}{\partial \alpha_j } \left( \psi\left(\alpha_{0}\right) - \psi\left(\alpha_{i}\right) + \log p_{i} \right) \right] \\
& = \begin{cases}
\psi^{(1)}\left(\alpha_{i}\right) - \psi^{(1)}\left(\alpha_{0}\right), & i=j, \\ 
-\psi^{(1)}\left(\alpha_{0}\right), & i \neq j,
\end{cases}
\end{aligned}
% \end{equation}
$$
where $\Gamma(\cdot)$ is the \textit{gamma} function, $\psi(\cdot)$ is the \textit{digamma} function, $\psi^{(1)}(\cdot)$ is the \textit{trigamma} function, defined as $\psi^{(1)}(x) = d \psi(x) / dx = d^2 \ln \Gamma(x) / dx^2$. Then, we can get the matrix form of the FIM:
% $$
\begin{equation}
\begin{aligned}
\label{fisher_matrix}
% \mathcal{I}(\boldsymbol{\alpha}) = &
% \begin{bmatrix}
% \psi^{(1)}\left(\alpha_{1}\right) - \psi^{(1)}\left(\alpha_{0}\right) & - \psi^{(1)}\left(\alpha_{0}\right) & \cdots & - \psi^{(1)}\left(\alpha_{0}\right) \\
% - \psi^{(1)}\left(\alpha_{0}\right) & \psi^{(1)}\left(\alpha_{2}\right) - \psi^{(1)}\left(\alpha_{0}\right) & \cdots & - \psi^{(1)}\left(\alpha_{0}\right) \\
% \vdots & \vdots & \ddots & \vdots \\
% - \psi^{(1)}\left(\alpha_{0}\right) & - \psi^{(1)}\left(\alpha_{0}\right) & \cdots & \psi^{(1)}\left(\alpha_{K}\right) - \psi^{(1)}\left(\alpha_{0}\right)
% \end{bmatrix} \\
% = & \ \text{diag}([\psi^{(1)}(\alpha_{1}), \cdots, \psi^{(1)}(\alpha_{K})])- \psi^{(1)}(\alpha_{0}) \boldsymbol{1}\boldsymbol{1}^T,
\mathcal{I}(\boldsymbol{\alpha}) = &
\begin{bmatrix}
\psi^{(1)}\left(\alpha_{1}\right) - \psi^{(1)}\left(\alpha_{0}\right) & \cdots & - \psi^{(1)}\left(\alpha_{0}\right) \\
- \psi^{(1)}\left(\alpha_{0}\right) & \cdots & - \psi^{(1)}\left(\alpha_{0}\right) \\
\vdots & \ddots & \vdots \\
- \psi^{(1)}\left(\alpha_{0}\right) & \cdots & \psi^{(1)}\left(\alpha_{K}\right) - \psi^{(1)}\left(\alpha_{0}\right)
\end{bmatrix} \\
= & \ \text{diag}([\psi^{(1)}(\alpha_{1}), \cdots, \psi^{(1)}(\alpha_{K})])- \psi^{(1)}(\alpha_{0}) \boldsymbol{1}\boldsymbol{1}^T,
\end{aligned}
\tag{10}
\end{equation}
% $$
where $\boldsymbol{1} = [1; \cdots; 1] \in \mathbb{R}^{K}$.

Next, we derive the close-form of log determinant of Eq. (\ref{fisher_matrix}). Let $\vb=[ \psi^{(1)}(\alpha_{1}), \cdots, \psi^{(1)}(\alpha_{K})]^T$, by applying Matrix-Determinant Lemma, we have
\begin{equation}
\begin{aligned}
|\mathcal{I}(\boldsymbol{\alpha})| & = |\text{diag}(\vb)| \cdot (1 - \psi^{(1)}(\alpha_{0}) \boldsymbol{1}^T \text{diag}(\vb)^{-1}\boldsymbol{1}) \\
& = \prod_{k=1}^{K} \psi^{(1)}(\alpha_{k}) (1 - \sum_{j=1}^{K} \frac{\psi^{(1)}(\alpha_{0})}{\psi^{(1)}(\alpha_{j})}).
\end{aligned}
\tag{11}
\end{equation}
Therefore, we can obtain
\begin{equation}
\begin{aligned}
\label{log_det_fisher}
\log |\mathcal{I}(\boldsymbol{\alpha})| = \sum_{k=1}^{K} \log \psi^{(1)}(\alpha_{k}) + \log (1 - \sum_{k=1}^{K} \frac{\psi^{(1)}(\alpha_{0})}{\psi^{(1)}(\alpha_{k})}).
\end{aligned}
\tag{12}
\end{equation}



\subsection*{A.2.Simplification of $\mathcal{L}_{j}^{\text{N-EDL}}$ in Eq.(\ref{obj_nedl})}
\label{app_obj_nedl}

Since $\mathcal{L}_{j}^{\text{N-EDL}} = \KL(\mathcal{U} \Vert \mathcal{N}( \E_{Dir(\boldsymbol{\alpha}_{j})}[\vp], \sigma^2\mathcal{I}(\boldsymbol{\alpha}_{j})^{-1})$,
where $\mathcal{U} = \mathcal{N}(\boldsymbol{1}/K, \lambda^2 \mI)$.
Then, we have
$$
% \begin{equation}
\begin{aligned}
& \KL( \mathcal{N}(\boldsymbol{1}/K, \lambda^2 \mI) \Vert \mathcal{N}( \E_{Dir(\boldsymbol{\alpha}_{j})}[\vp], \sigma^2\mathcal{I}(\boldsymbol{\alpha}_{j})^{-1}) \\
\propto & \frac{1}{2\sigma^2} (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}})^T \mathcal{I}(\boldsymbol{\alpha}_{j})(\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}}) - \frac{1}{2} \log |\mathcal{I}(\boldsymbol{\alpha}_j)| \\
\propto & \sum_{k=1}^{K}(\frac{1}{K} - \frac{\alpha_{j k}}{\alpha_{j 0}})^2 \psi^{(1)}(\alpha_{j k}) - \lambda_1 \log |\mathcal{I}(\boldsymbol{\alpha}_j)|.
\end{aligned}
% \end{equation}
$$
The second formula is derived from the formula of KL divergence between two multivariate Gaussians, and the last formula is derived on the closed-form of $\mathcal{I}(\boldsymbol{\alpha}_{j})$ in Eq. (\ref{fisher_matrix}) 
$$
% \begin{equation}
\begin{aligned}
& (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}})^T \mathcal{I}(\boldsymbol{\alpha}_{j})(\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}}) \\
= & (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}})^T \text{diag}([\psi^{(1)}(\alpha_{j1}), \cdots, \psi^{(1)}(\alpha_{jK})]) (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}})  \\
& - (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}})^T \psi^{(1)}(\alpha_{j0}) \boldsymbol{1}\boldsymbol{1}^T (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}}) \\
= & \sum_{k=1}^{K}(\frac{1}{K} - \frac{\alpha_{j k}}{\alpha_{j 0}})^2 \psi^{(1)}(\alpha_{j k}),
\end{aligned}
% \end{equation}
$$
where the last formula comes from the fact that
$$
% \begin{equation}
\begin{aligned}
& (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}})^T \psi^{(1)}(\alpha_{j 0}) \boldsymbol{1}\boldsymbol{1}^T (\frac{1}{K} - \frac{\boldsymbol{\alpha}_{j}}{\alpha_{j 0}}) \\
= & \psi^{(1)}(\alpha_{j 0})\left( \sum_{i=1}^K (\frac{1}{K} - \frac{\alpha_{j i}}{\alpha_{j 0}})\right)^2 \\
= & \psi^{(1)}(\alpha_{j 0})\left( 1 - \sum_{i=1}^K \frac{\alpha_{j i}}{\alpha_{j 0}}\right)^2 = 0.
\end{aligned}
% \end{equation}
$$

Figure \ref{fig:prob} also provides the lens of probabilistic graphical models to understand $\mathcal{L}^{\text{N-EDL}}$.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.46\textwidth]{prob_fig.pdf}
	%\vspace{-4mm}
	\caption{Graphical model representation of $\mathcal{L}^{\text{N-EDL}}$.}
	%\vspace{-2mm}
	\label{fig:prob}
\end{figure}

\subsection*{A.3.Closed-form of $\mathcal{L}_{j}^{\text{KL}}$ in Eq. (5)}
\label{app_obj_kl}

Since $\mathcal{L}_{j}^{\text{KL}} 
= \KL(Dir(\vp_j | \boldsymbol{\alpha}_j) \Vert Dir(\vp_j | \boldsymbol{\beta})),$
where
$$
Dir(\vp_j|\boldsymbol{\alpha}_j)=\frac{1}{B(\boldsymbol{\alpha}_j)} \prod_{k=1}^{K} p_{j k}^{\alpha_{k}-1}, B(\boldsymbol{\alpha}_j) = \frac{\prod_{k=1}^{K} \Gamma\left(\alpha_{j k}\right)}{\Gamma\left(\sum_{k=1}^{K} \alpha_{j k}\right)}.
$$
Then, we have
\begin{equation}
\begin{aligned}
\label{app_kl}
\mathcal{L}_{j}^{\text{KL}} 
= & \KL(Dir(\vp_j | \boldsymbol{\alpha}_j) \Vert Dir(\vp_j | \boldsymbol{\beta})) \\
= & \E_{Dir(\vp_j | \boldsymbol{\alpha}_j)} \left[\log \frac{Dir(\vp_j | \boldsymbol{\alpha}_j)}{Dir(\vp_j | \boldsymbol{\beta}))} \right] \\
= & \E_{Dir(\vp_j | \boldsymbol{\alpha}_j)} \left[\log Dir(\vp_j | \boldsymbol{\alpha}_j) - \log Dir(\vp_j | \boldsymbol{\beta})) \right]
\\
= & \E_{Dir(\vp_j | \boldsymbol{\alpha}_j)} \left[ - \log B(\boldsymbol{\alpha}_j) + \sum_{k=1}^{K}(\alpha_{j k} - 1) \log p_{j k} \right] \\
& - \E_{Dir(\vp_j | \boldsymbol{\alpha}_j)} \left[ - \log B(\boldsymbol{\beta}) + \sum_{k=1}^{K}(\beta_{k} - 1) \log p_{j k}\right] \\
= & \log \frac{B(\boldsymbol{\beta})}{B(\boldsymbol{\alpha}_j)} + \E_{Dir(\vp_j | \boldsymbol{\alpha}_j)} \left[\sum_{k=1}^{K}(\alpha_{j k} - \beta_{k}) \log p_{j k} 
 \right].
\end{aligned}
\tag{13}
\end{equation}

Therefore, we obtain 
\begin{equation}
\begin{aligned}
\mathcal{L}_{j}^{\text{KL}} = & \log \Gamma(\alpha_{j 0}) - \sum_{k=1}^{K} \log \Gamma(\alpha_{j k}) \\
& - \log \Gamma(\beta_{0}) + \sum_{k=1}^{K} \log \Gamma(\beta_{k}) \\
&+ \sum_{k=1}^{K}(\alpha_{j k} - \beta_{k}) \left[\psi(\alpha_{j k}) - \psi(\alpha_{j 0}) \right],
\end{aligned}
\tag{14}
\end{equation}
where $\alpha_{j 0} = \sum_{k=1}^{K} \alpha_{j k}$, $\beta_{0} = \sum_{k=1}^{K} \beta_{k}$.


\begin{algorithm*}[t]
   \caption{Adaptive Negative Evidential Deep Learning}
   \label{alg:train}
    \begin{algorithmic}[]
   \STATE {\bfseries Input:} Labeled dataset $\mathcal{D}_{l}=\{(\boldsymbol{x}^l_j, y^l_j)\}_{j=1}^{N_l}$, unlabeled dataset $\mathcal{D}_{u}= \{\boldsymbol{x}^i_j\}_{j=1}^{N_i} \cup \{\boldsymbol{x}^o_j\}_{j=1}^{N_o}$, pseudo-inlier set $\mathcal{I}_{u} = \emptyset$, learning rate $\beta$, total epochs $E$, steps in an epoch $S$, trade-off parameters 
   ${\lambda}_\text{P-EDL}$, ${\lambda}_\text{N-EDL}$ and 
   ${\lambda}_\text{CON}$, initialized $\vtheta$
   \FOR{$e=1,\cdots, E$}
       \IF{$e > E_{FM}$}
        \STATE \textbf{Update} $\mathcal{I}_{u} = Select(\theta, \mathcal{D}_{u})$ \COMMENT{See Section: Uncertainty Metric for Open-set SSL}
        \ENDIF
        \FOR{$s=1,\cdots, S$}
            \STATE \textbf{Sample} a batch of labeled data $\mathcal{B}_{l} \in \mathcal{D}_{l}$ and unlabeled data $\mathcal{B}_{u} \in \mathcal{D}_{u}$
            \STATE \textbf{Compute} $\mathcal{L}_\text{ANO}(\mathcal{B}_{l}, \mathcal{B}_{u}) = \lambda_\text{P-EDL} (\mathcal{L}^{\text{P-EDL}}(\mathcal{B}_{l}) + \mathcal{L}^{\text{KL}}(\mathcal{B}_{l})) + \lambda_\text{N-EDL} (\mathcal{L}^{\text{N-EDL}}(\mathcal{B}_{u}) + \mathcal{L}^{\text{KL}}(\mathcal{B}_{u}))$ \COMMENT{See Eq.(8)}
            \STATE \textbf{Compute} $\mathcal{L} = \mathcal{L}_\text{CE}(\mathcal{B}_{l}) +  \mathcal{L}_\text{ANO}(\mathcal{B}_{l}, \mathcal{B}_{u}) + \lambda_\text{CON} \mathcal{L}_\text{CON}(\mathcal{B}_{u})$ \COMMENT{See Eq.(7)}
            \IF{$e > E_{FM}$}
            \STATE \textbf{Sample} a batch of pseudo-inliers $\mathcal{B}_{u}^{\mathcal{I}}\in \mathcal{I}_{l}$
            \STATE \textbf{Compute} $\mathcal{L} += \mathcal{L}_\text{FM}(\mathcal{B}_{u}^{\mathcal{I}})$
            \ENDIF
            \STATE $\vtheta \leftarrow \vtheta - \beta \nabla_\vtheta \mathcal{L}$ 
            \ENDFOR
    \ENDFOR
   % \UNTIL{$noChange$ is $true$}
    \end{algorithmic}
\end{algorithm*}

\section*{B.Pseudo-code of ANEDL}
\label{app_pseudo_code}
The complete pseudo-code of ANEDL is outlined in Algorithm \ref{alg:train}.



\section*{C.Experimental Details}
\label{app_exp}

\subsection*{C.1.Datasets}
\noindent\textbf{CIFAR-10} \cite{krizhevsky2009learning} has 10 categories and each category has 6,000 images of size $32 \times 32$. 1,000 images of each category are used for test. We choose 6/4 classes as inlier/outlier classes. The amount of labeled images of each inlier class varies in $\{50,100,400\}$. And the number of validation images of each inlier class is 50.

\noindent\textbf{CIFAR-100} \cite{krizhevsky2009learning} is composed of 60,000 images of size $32 \times 32$  from 100 classes. 100 images of each class are used for test. We choose 55/80 classes as inlier classes, resulting in 45/20 outliers classes. The amount of labeled images of each inlier class varies in $\{50,100\}$. The number of validation images of each inlier class is 50.

\noindent\textbf{ImageNet-30} \cite{hendrycks2016baseline} is a subset of ImageNet \cite{deng2009imagenet}. It has 30 classes and each class contains 1,300 images of size $224 \times 224$. 100 images per class are used for test. Inlier class number is 20. The amounts of labeled and validation images per inlier class are both 130. 

\noindent\textbf{Mini-ImageNet} \cite{vinyals2016matching} is also a subset of ImageNet. It has 100 classes and each class contains 600 images of size $84 \times 84$. 100 images of each class are used for test. The number of inlier classes varies in $\{50,60,80\}$. The amounts of labeled and validation images of each inlier class are both 50. 


%The default hyperparameter values for each dataset are shown in Table \ref{tab:para}. Since there are several settings in each dataset, we further fine-tune hyperparameters in a small range for each setting for better results. We only change part of hyperparameters of four settings, and the others settings use default values in Table \ref{tab:para}. For 50 labeled samples per classes of Cifar-10, we set ${\lambda}_{P-EDL}=0.008$ and ${\lambda}_{CON}=0.08$. For 55 inlier classes and 100 labeled samples per classes of Cifar-100, we set ${\lambda}_{P-EDL}=0.02$ and ${\lambda}_{CON}=0.02$. For 80 inlier classes and 50 labeled samples per classes of Cifar-100, we set ${\lambda}_{CON}=0.06$. For 50 inlier classes Mini-imageNet, we set ${\lambda}_{P-EDL}=0.01$. 

%\begin{table}[]
%\setcounter{table}{5}
%\centering
%\begin{tabular}{c|c|c|c}
%\toprule
%Dataset       &  ${\lambda}_{P-EDL}$  & ${\lambda}_{CON}$     & ${\lambda}_{2}$    \\ \midrule
%Cifar-10      & 0.005 & 0.005 & 0.002 \\ \midrule
%Cifar-100     & 0.03 & 0.04  & 0.01  \\ \midrule
%Mini-ImageNet   & 0.03 & 0.01  & 0.002 \\ \midrule
%ImageNet-30 & 0.08 & 0.01  & 0.01  \\ \bottomrule
%\end{tabular}
%\caption{List of hyperparameter.}
%\label{tab:para}
%\end{table}



\section*{D.Additional Experimental Results}

\subsection*{D.1.Evaluation on KL Divergence.}

%Previous EDL methods use KL divergence loss only to shrink misleading evidence by changing ${\alpha}_i$ to $\hat{\alpha}_i = {\alpha}_i \odot(1 - y_i) + y_i$ and setting target Dirichlet Distribution as $Dir(p_i | \boldsymbol{1})$. 
Besides shrinking misleading evidence, we propose to boost non-misleading evidence with KL divergence loss to improve the separation of inliers and outliers. In Eq. (\ref{obj_kl}), we keep $\boldsymbol{\alpha}_i$ unchanged and setting target Dirichlet Distribution to $Dir(\boldsymbol{p}_i | \boldsymbol{\beta})$, where $\boldsymbol{\beta} = [1, \cdots, P, \cdots, 1] $. $\boldsymbol{{\beta}_{100}}$ and $\boldsymbol{{\beta}_{200}}$ denote $P=100$ and $P=200$, respectively. Table \ref{table:kl}
% Results 
shows that our strengthen KL divergence can effectively improve the separation of inliers and outliers by boosting non-misleading evidence. However, a too large $P$ may cause an unstable training process, 
% and thus 
resulting in suboptimal performance.



\subsection*{D.2.Evaluation of DebiasPL}

As mentioned in ``Joint Optimization with Softmax and EDL '' of Section 3.2, we utilize DebiasPL \cite{wang2022debiased} to remove the bias of pseudo label in Fixmatch. In Table \ref{table:debias}, we study the influence of DebiasPL. Since OpenMatch is trained without DebiasPL, we remove DebiasPL from our method for a fair comparison with OpenMatch. We can see that, even trained without DebiasPL, our method still outperforms OpenMatch with respect to both AUROC and error rate. %Also, the comparison between ``with'' and ``without DebiasPL'' proves that DebiasPL can help further improves our performance with bias in pseudo label alleviated.




\subsection*{D.3.Case Study}
 We further conduct a case study under the setting of 55 inlier classes and 50 labeled samples per class on CIFAR-100, as shown in Figure \ref{fig:case}. 
 % We derive uncertainty value of our method by Eq.(\ref{eq:uncertainty}) while obtaining that of OpenMatch with $p_{ood}$, which is the outlier possibility predicted by its binary detector. 
 We derive uncertainty value of our method by Eq.(\ref{eq:uncertainty}), while the value of OpenMatch is derived from the outlier likelihood $p_{ood}$ predicted by its binary detector. We sort uncertainty values in descending order and report the order in Figure \ref{fig:case}. We can see that compared with OpenMatch, our method can better distinguish between inliers and outliers. 


\begin{table}[t]
\begin{center}
% \resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|c|c}
\toprule
KL divergence & AUROC & Error rate     \\ \midrule
$D_{KL}(D(\boldsymbol{p}_i | \hat{\boldsymbol{\alpha}}_i) \Vert D(\boldsymbol{p}_i | \boldsymbol{1}))$& 89.9$\pm$0.3 & 27.0$\pm$0.2\\
$D_{KL}(D(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert D(\boldsymbol{p}_i | \boldsymbol{{\beta}_{100}}))$& \textbf{90.3$\pm$0.1} &  \textbf{26.7$\pm$0.2} \\
$D_{KL}(D(\boldsymbol{p}_i | \boldsymbol{\alpha}_i) \Vert D(\boldsymbol{p}_i | \boldsymbol{{\beta}_{200}}))$ &  89.9$\pm$0.9        &     26.9$\pm$0.2       \\ \bottomrule
\end{tabular}
% }
\caption{Evaluation on KL divergence.}
\label{table:kl}
\end{center}
%\vspace{-2mm}

\end{table}

\begin{table}[t]
\centering
\begin{tabular}{c|c|c}
\toprule
Method           & AUROC      & Error rate \\ \midrule
OpenMatch        & 87.0$\pm$1.1 & 27.7$\pm$0.4 \\ 
Ours(w/o DebiasPL) & 90.0$\pm$0.5 & 27.0$\pm$0.2 \\
Ours(w DebiasPL)   & \textbf{90.3$\pm$0.1} & \textbf{26.7$\pm$0.2} \\ \bottomrule
\end{tabular}
\caption{Evaluation on DebiasPL}
\label{table:debias}
\end{table}

\begin{figure*}[t]
	%\setlength{\abovecaptionskip}{0.1cm}	
	%\setlength{\belowcaptionskip}{-0.3cm}
	\centering
	\includegraphics[width=0.98\textwidth]{case.pdf}
	%\vspace{-4mm}
	\caption{Case study. U. and O. represent uncertainty and order, respectively. The uncertainty of our method can better distinguish between inliers and outliers than that of OpenMatch. (Samples with blue and yellow backgrounds indicate inliers and outliers, respectively.)}
	%\vspace{-2mm}
	\label{fig:case}
\end{figure*}