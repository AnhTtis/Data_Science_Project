\section{Details of the methodology}
\subsection{Algorithm for deleting one data point} \label{appendix:method_delete_data}
Here, we provide details for the algorithm for deleting one data point from Section~\ref{subsec:method_2}.
Removing the $j$-th data point corresponds to removing the term $\frac{1}{2}\lambda_{j}\|\Phi_{j} \weightvec - \MLy_{j}\|_2^2 $ in the loss function~\eqref{eq:loss_function} or, equivalently, removing the Hamiltonian $\frac{1}{2}\|\Phi_{j} \weightvec - \MLy_{j}\|_2^2$ from the multi-time HJ PDE and removing the pieces $L_{j}(s, \HJu) = \frac{1}{2}\HJu^T\HJu - \MLy_{j}$ and $f(s,\HJu) = \Phi_{j}^T\HJu$ from the running cost and dynamics, respectively, of the corresponding piecewise LQR problem.
Hence, numerically, we can remove the $j$-th data point by solving the following Riccati ODE
\begin{equation}\label{eqt:regression_2Riccati}
    \begin{dcases}
    \dot{\tilde\Sxx}(\HJt) =  -\tilde\Sxx(\HJt)^T\MLbasismat_{j}^T\MLbasismat_{j}\tilde\Sxx(\HJt) &\HJt<\lambda_j,\\
    \dot{\tilde\Sx}(\HJt) = -\tilde\Sxx(\HJt)^T\MLbasismat_{j}^T(\MLbasismat_{j}\tilde\Sx(\HJt) - \MLy_{j})&\HJt<\lambda_j,
    \end{dcases}
\end{equation}
with terminal condition $\tilde\Sxx(\lambda_j) = \Sxx\left(T_N\right)$ and $\tilde\Sx(\lambda_j) = \Sx\left(T_N\right)$, where $\Sxx\left(T_N\right)$ and $\Sx\left(T_N\right)$ are obtained from solving the learning problem~\eqref{eq:loss_function} with all $N$ data points. Then, the solution to the new learning problem with the $j$-th point removed is given by~\eqref{eqt:sec42_newoptimizer}, where $\tilde\Sxx = \tilde\Sxx(0)$ and $\tilde \Sx = \tilde \Sx(0)$ are the solution to~\eqref{eqt:regression_2Riccati}.


\subsection{Algorithm for tuning the regularization weights}\label{appendix:hyperparam_tuning}
Here, we provide details for the algorithm for deleting one data point from Section~\ref{subsec:method_3}.
We consider the case where we change each regularization parameter $\MLreg_k$ to $\tilde\MLreg_k$. This change can be regarded as two steps: first, we change all parameters $\MLreg_k$ for the indices $k$ such that $\tilde\MLreg_k > \MLreg_k$, and then we change the other parameters. Define the index set $\mathcal{K} $ to be $\mathcal{K} = \{k\colon \tilde\MLreg_k > \MLreg_k\}$. 

The first step is equivalent to adding the term $\sum_{k\in \mathcal{K}}\frac{\tilde\MLreg_k - \MLreg_k}{2}(\weight_k - \MLcenter_k)^2$ to the loss function~\eqref{eq:loss_function}. We can interpret this as adding an $(N+1)$-th Hamiltonian $\frac{1}{2}\weightvec^T\MLregmat_{+}\weightvec$ with corresponding time variable $\HJt_{N+1} = 1$ to the multi-time HJ PDE, where $\MLregmat_+$ is a diagonal matrix whose $k$-th diagonal element is $\tilde\MLreg_k - \MLreg_k$ if $k\in\mathcal{K}$ and $0$ otherwise.
Therefore, the solution to this new multi-time HJ PDE can be solved by the following Riccati equation:
\begin{equation}\label{eqt:riccati_increase_regweight}
\begin{dcases}
\dot{\Sxx}_+(\HJt) =  -\Sxx_+(\HJt)^T\MLregmat_+\Sxx_+(\HJt) &\HJt\in \left(0,1\right),\\
\dot{\Sx}_+(\HJt) = -\Sxx_+(\HJt)^T\MLregmat_+\Sx_+(\HJt)&\HJt\in \left(0, 1\right),
\end{dcases}
\end{equation}
with initial condition $\Sxx_+(0)$ and $\Sx_+(0)$, which are the corresponding solutions to the Riccati equations before changing the weights $\MLreg_k, k\in\mathcal{K}$. In other words, we set $\Sxx_+(0) = \Sxx\left(T_N\right)$ and $\Sx_+(0) = \Sx\left(T_N\right)$, where $\Sxx(T_N), \Sx(T_N)$ are obtained from solving the original learning problem~\eqref{eq:loss_function} with the original values of $\MLreg_k$.

The second step is equivalent to removing the term $\sum_{k\not\in\mathcal{K}}\frac{\MLreg_k - \tilde\MLreg_k}{2}(\weight_k - \MLcenter_k)^2$ from the loss function~\eqref{eq:loss_function}. This is equivalent to solving a single-time HJ PDE with a terminal condition at time $1$ and Hamiltonian $\frac{1}{2}\weightvec^T\MLregmat_{-}\weightvec$, where $\MLregmat_-$ is a diagonal matrix whose $k$-th diagonal element is $\MLreg_k - \tilde\MLreg_k$ if $k\not\in\mathcal{K}$ and $0$ otherwise.
Then, the solution can be obtained by solving the following Riccati equation:
\begin{equation}\label{eqt:riccati_decrease_reg_weight}
\begin{dcases}
\dot{\Sxx}_-(\HJt) =  -\Sxx_-(\HJt)^T\MLregmat_-\Sxx_-(\HJt) &\HJt\in \left(0,1\right),\\
\dot{\Sx}_-(\HJt) = -\Sxx_-(\HJt)^T\MLregmat_-\Sx_-(\HJt)&\HJt\in \left(0, 1\right),
\end{dcases}
\end{equation}
with terminal condition $\Sxx_-(1) = \Sxx_+(1)$ and $\Sx_-(1) = \Sx_+(1)$, where $\Sxx_+, \Sx_+$ are obtained from the solution to~\eqref{eqt:riccati_increase_regweight}.

Finally, the minimizer of the new learning problem after changing all of the weights $\MLreg_k$ in the regularization term is given by ${\Sxx}_-(0) \tilde\MLregmat \MLcentervec+ {\Sx}_-(0)$, where ${\Sxx}_-, {\Sx}_-$ are obtained from the solution to~\eqref{eqt:riccati_decrease_reg_weight} and $\tilde\MLregmat$ is a diagonal matrix whose $k$-th diagonal element is the new regularization parameter $\tilde\MLreg_k$.




\section{Additional results for example 2}\label{appendix:1}
In Section~\ref{subsec:example_2}, we consider a 1D steady-state linear reaction-diffusion equation and discuss three different types of post-training calibrations: adding new data points to compensate for a lack of knowledge in the regular training, enforcing the fitting of some data points by increasing the weights $\lambda_i$ of their respective terms in the loss function~\eqref{eq:calibration:loss}, and removing some data points so that their  effects are eliminated. In this section, we present the results for the last case. In Figure~\ref{fig:calibration:2}, we remove two outliers one-by-one and observe that their removal does successfully increase the accuracy of the learned model. Again, using our Riccati-based approach, the removal of these points is done using only knowledge about the point to be removed and the results of the previous training step. 

\begin{figure}[ht]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.3\textwidth]{calibration/f_1_new.png}
        \includegraphics[width = 0.3\textwidth]{calibration/f_2_new.png}
        \includegraphics[width = 0.3\textwidth]{calibration/f_3_new.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.3\textwidth]{calibration/u_1_new.png}
        \includegraphics[width = 0.3\textwidth]{calibration/u_2_new.png}
        \includegraphics[width = 0.3\textwidth]{calibration/u_3_new.png}
        \caption{}
    \end{subfigure}
    \caption{Results of solving the 1D steady-state reaction-diffusion equation~\eqref{eq:reaction} with noisy measurements of the source term $f$ in the domain and noiseless measurements of the solution $u$ on the boundary. (a) results for $f$; (b) results for $u$. \textbf{Left}: results of regular training using our Riccati-based method in Section~\ref{subsec:method_1}; \textbf{middle} and \textbf{right}: calibrating the results of regular training by eliminating two outlier measurements of $f$ using the methodology described in Section~\ref{subsec:method_2}. Calibrations are employed without re-training or access to the data from the previous training.}
    \label{fig:calibration:2}
\end{figure}

