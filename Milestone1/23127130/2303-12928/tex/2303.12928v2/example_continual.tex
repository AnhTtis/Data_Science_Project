\subsection{Function approximation in continual learning}\label{subsec:example_1}
In this section, we test our Riccati-based approach (as described in Section~\ref{subsec:example_1}) on a pedagogical function approximation example in continual learning \cite{parisi2019continual, kirkpatrick2017overcoming, van2019three} to demonstrate its computational and memory advantages.
Under the continual learning framework, data is accessed in a stream and the trainable model parameters are updated incrementally as new data becomes available.
In some cases, the historical data may also become inaccessible after new data is received, which can often lead to catastrophic forgetting \cite{kirkpatrick2017overcoming, parisi2019continual}, which refers to the abrupt degradation in performance of learned models on previous tasks upon training on new tasks. 
In this example, we show how our Riccati-based approach naturally coincides with the continual learning framework, while also inherently avoiding catastrophic forgetting even if the historical data is inaccessible.


Our example set-up is as follows. Our goal is to regress the function $y(\tau) = 0.1 \tau + 0.05\sin(10\tau)$ given noisy data $\{(\tau_i, y_i\approx y(\tau_i))\}_{i=1}^N\subset\R\times\R$.
Following the continual learning framework, we assume that a new noisy data point is available every $\Delta \tau= 0.01$ (i.e., $\tau_i = (i-1)\Delta \tau\in [0, 10]$, $i = 1, \dots, N$).
We regress $y(\tau)$ using the linear model $y(\tau) = \sum_{k=1}^n \weight_k \phi_k(\tau)$, where $n=10$ and the basis functions are given by
\begin{equation}\label{eq:continual:basis}
    \{\phi_k(\tau)\}_{k=1}^n = \{1, \tau, \tau^2, \tau^3, \sin(\tau), \sin(5\tau), \sin(8\tau), \sin(9\tau), \sin(10\tau), \sin(12\tau)\}.
\end{equation}
The coefficients $\weightvec = [\weight_1, \dots, \weight_n]^T$ of our linear model are learned by minimizing the following loss function:
\begin{equation}\label{eq:continual:loss}
    \mathcal{L}(\weightvec) = \frac{1}{2}\sum_{i=1}^N \lambda_i \left|\sum_{k=1}^{n} \weight_k\phi_k(\tau_i) - y_i \right|^2 + \frac{1}{2}\sum_{k=1}^{n} \MLreg_k|\weight_k|^2,
\end{equation}
where $\lambda_i, i=1,...,N$ and $\MLreg_k, k=1,...,n$ are weights for the data loss and $\ell_2$ regularization terms, respectively, and we update our learned coefficients every time a new data point is available. In our numerical experiments, we use additive Gaussian noise with zero mean and standard deviation $0.01$, and we set $\lambda_i=1, \forall i$ and $\MLreg_k=0.1, \forall k$. 


Although this loss function~\eqref{eq:continual:loss} could be minimized using conventional machine learning techniques (e.g., the method of least squares), these methods typically require access to and training on the entire dataset $\{(\tau_i, y_i)\}_{i=1}^N$, which conflicts with the assumptions in continual learning.
However, note that this loss function~\eqref{eq:continual:loss} is of the form~\eqref{eq:loss_function}. Thus, we can instead apply our Riccati-based approach (as described in Sections~\ref{subsec:method_1} and~\ref{subsec:method_2}) to solve this learning problem. In particular, the methodology described in Section~\ref{subsec:method_2} matches the data streaming paradigm of continual learning; we incrementally update our learned coefficients by considering each new data point as the time evolution of a corresponding multi-time HJ PDE. 
As a result, in contrast to conventional machine learning methods, our Riccati-based approach does not require storage of previous data points and its memory and computational complexity is constant for each added data point. As such, our Riccati-based methodology may be well-suited to online learning applications.
Additionally, since this time evolution of the HJ PDE (i.e., the addition of a new data point) requires knowledge about the solution to the HJ PDE at the previous time (i.e., the results of the previous training), our Riccati-based approach also inherently avoids catastrophic forgetting.


\begin{figure}[htbp]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.5\textwidth]{continual_learning/fig1_evolution.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.3\textwidth]{continual_learning/fig1_200.png}
        \includegraphics[width = 0.3\textwidth]{continual_learning/fig1_400.png}
        \includegraphics[width = 0.3\textwidth]{continual_learning/fig1_800.png}
        \caption{}
    \end{subfigure}
    \caption{Evolution and continual learning of the function approximation learned using our Riccati-based approach as more data becomes available. (a) shows the evolution of the learned coefficients $\weight_k, k=1,...,n$ as more data is incorporated into the model; the horizontal dotted lines denote the exact reference values. (b) shows the inferences of $y$ after the 200th, 400th, and 800th noisy data point becomes available. Our Riccati-based approach allows us to incrementally update the learned coefficients as more data becomes available without requiring access to the previous data or re-training on the entire dataset, which provides advantages in both memory and computations over conventional learning methods. }
    \label{fig:continual}
\end{figure}

The results of applying our Riccati-based method to solve this learning problem~\eqref{eq:calibration:loss} are shown in Figure~\ref{fig:continual} and Table~\ref{tab:continual}. 
Figure~\ref{fig:continual}(a) depicts the evolution of the coefficients $\{\weight_k\}_{k=1}^n$ as more data points become available. We observe that the learned coefficients $\weight_k, k=1,...,n$ converge to their true values as more data points are incorporated into the model.
Figure~\ref{fig:continual}(b) displays three inferences at  different times $\tau = 2, 4, 8$, which demonstrate that our Riccati-based approach is capable of real-time inferences without catastrophic forgetting, even though each inference is made using only one data point.
Note that due to an appropriate choice of basis functions, our learned model is able to provide accurate extrapolations as well. 

Table~\ref{tab:continual} displays the numerical errors of the minimizer $\weightvec^*$ of the loss function~\eqref{eq:continual:loss} obtained using the Riccati-based methodology from Section~\ref{sec:method} after the last data point becomes available at $\tau=10$. We observe that the accuracy of our approach increases as we decrease the step size $h$ of RK4, which indicates that the errors of $\weightvec^*$ stem from the accuracy of RK4 in solving the corresponding Riccati ODEs.
The reference solution is obtained by minimizing~\eqref{eq:continual:loss} directly using the method of least squares and assuming that all $N=1000$ data points are accessible. 



\begin{table}[ht]
    \footnotesize
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & $h=0.2$ & $h=0.1$ & $h=0.01$ \\
       \hline
       %$\ell^2$ error of $c^*$ & $6.4290\times 10^{-7}$ & $3.5060\times10^{-9}$ & $2.2395\times10^{-12}$\\
       %\hline
       %$L_2$ relative error of $c^*$ & $5.7623\times 10^{-6}$ & $3.1424\times 10^{-8}$ & $2.0073\times 10^{-11}$  \\
       %\hline
       $\ell_1$ error of $\weightvec^*$ & $1.1210\times 10^{-6}$ & $6.0924\times10^{-9}$ & $4.2222\times10^{-12}$\\
       \hline
       $\ell_1$ relative error of $\weightvec^*$ & $7.3622\times 10^{-6}$ & $4.0012\times10^{-8}$ & $2.7729\times10^{-11}$\\
       \hline
    \end{tabular}
\caption{Errors in computing the minimizer $\weightvec^*$ of the function approximation loss~\eqref{eq:continual:loss} using our Riccati-based approach. We use RK4 to solve the Riccati ODEs~\eqref{eqt:sequentialRiccatiODEs} and \eqref{eqt:regression_1Riccati} with double precision and various step sizes $h$. The reference is obtained by using the method of least squares to minimize the loss function~\eqref{eq:continual:loss} directly. }
    \label{tab:continual}
\end{table}
