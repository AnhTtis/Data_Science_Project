\section{Generalized Hopf formula}\label{sec:Hopf}
In this section, we provide some mathematical background on the single- and multi-time Hopf formulas. Specifically, we review the well-known connections between the Hopf formula, the solution to the HJ PDEs, and the solution to the corresponding optimal control problems. We then present a novel theoretical connection between the Hopf formula and certain learning problems. Through this connection, we establish that when we solve certain learning problems, we are actually evaluating the solution to certain HJ PDEs and their corresponding optimal control problems, and vice versa.

\subsection{Introduction to the Hopf formula}

The single-time HJ PDE is 
\begin{equation}\label{eqt:singletimeHJPDE}
    \begin{dcases}
    \frac{\partial S(\HJx,\HJt)}{\partial \HJt} + \Hamiltonian(\nabla_\HJx S(\HJx,\HJt)) = 0 & \HJx\in \HJstatespace, \HJt > 0, \\
    S(\HJx,0) = \HJIC(\HJx) & \HJx\in\HJstatespace,
    \end{dcases}
\end{equation}
where $\Hamiltonian:\Rn\to \R$ is the Hamiltonian and $\HJIC:\Rn\to \R$ is the initial condition. Assume that $\Hamiltonian$ and $\HJIC$ are convex. Then, the viscosity solution to the single-time HJ PDE~\eqref{eqt:singletimeHJPDE} is given by the Hopf formula \cite{hopf1965hopfformula}:
\begin{equation}\label{eqt:singletime_Hopf}
\begin{aligned}
S(\HJx,\HJt) & = \sup_{\HJmom\in\HJstatespace} \{\langle \HJx,\HJmom\rangle - \HJt \Hamiltonian(\HJmom) - \HJIC^*(\HJmom)\} = -\inf_{\HJmom\in\HJstatespace} \{ \HJt \Hamiltonian(\HJmom) + \HJIC^*(\HJmom)-\langle \HJx,\HJmom\rangle\},
\end{aligned}
\end{equation}
where $f^*$ denotes the Fenchel-Legendre transform of the function $f$; i.e., $f^*(p) = \sup_{x\in\Rn} \{\langle x,p\rangle - f(x)\}$.

The Hopf formula~\eqref{eqt:singletime_Hopf} also solves the following optimal control problem: 
\begin{equation}\label{eqt:optimal_control_standardform}
\min_{\bx(\cdot)} \left\{\int_{0}^{\HJt}L(\HJu(s)) ds + \HJIC(\bx(\HJt))\colon \dot\bx(s) = f(\HJu(s)) \forall s\in(0,\HJt], \bx(0) = \HJx\right\},
\end{equation}
where the running cost $L$ and the source term $f$ of the dynamics are related to the Hamiltonian $\Hamiltonian$ by $\Hamiltonian(\HJmom) = \sup_{\HJu\in\Rn} \{\langle -f(\HJu), \HJmom\rangle - L(\HJu)\}$ and, in this context, we interpret $\HJIC$ to be the terminal cost.


A natural generalization of this formulation to the multi-time case is as follows. Let $\Hamiltonian_1,\dots, \Hamiltonian_\numt$ be convex Hamiltonians, such that $\dom \Hamiltonian_i = \Rn$ for all $i = 1, \dots, \numt$, and let $\HJIC$ be a convex initial condition. Then, the multi-time HJ PDE is given by
\begin{equation}\label{eqt:multitimeHJPDE}
    \begin{dcases}
    \frac{\partial S(\HJx,\HJt)}{\partial \HJt_i} + \Hamiltonian_i(\nabla_\HJx S(\HJx,\HJt)) = 0 \text{ for } i\in\{1, \dots, \numt\} & \HJx\in \HJstatespace, \HJt_1, \dots, \HJt_\numt > 0, \\
    S(\HJx,0, \dots, 0) = \HJIC(\HJx) & \HJx\in\HJstatespace,
    \end{dcases}
\end{equation}
and the solution to the multi-time HJ PDE~\eqref{eqt:multitimeHJPDE} can be represented by the following generalized (multi-time) Hopf formula \cite{lions1986hopf}:
\begin{equation} \label{eqt:multitime_Hopf}
\begin{aligned}
S(\HJx,\HJt_1,\dots, \HJt_\numt) & = \sup_{\HJmom\in\HJstatespace} \left\{\langle \HJx, \HJmom\rangle - \sum_{i=1}^\numt \HJt_i\Hamiltonian_i(\HJmom) - \HJIC^*(\HJmom)\right\} \\
&= -\inf_{\HJmom\in\HJstatespace} \left\{ \sum_{i=1}^\numt \HJt_i\Hamiltonian_i(\HJmom) + \HJIC^*(\HJmom) -\langle \HJx, \HJmom\rangle \right\}.\\
\end{aligned}
\end{equation}



Moreover, the generalized Hopf formula~\eqref{eqt:multitime_Hopf} also solves an optimal control problem in the form of~\eqref{eqt:optimal_control_standardform}
with terminal time $\HJt = \sum_{j=1}^\numt \HJt_j$ and where the running cost $L$ and the source term $f$ of the dynamics are defined piecewise by $L(s, \HJu) = L_i(\HJu)$ and $f(s, \HJu) = f_i(\HJu)$, respectively, for $s \in \left(\sum_{j=1}^{i-1}\HJt_j, \sum_{j=1}^{i}\HJt_j\right]$ and $i= 1, \dots, \numt$. The piecewise running costs $L_i$ and piecewise source terms $f_i$ of the dynamics are related to the Hamiltonians $\Hamiltonian_i$ by $\Hamiltonian_i(\HJmom) = \sup_{\HJu\in\Rn} \{\langle -f_i(\HJu), \HJmom\rangle - L_i(\HJu)\}$.




\subsection{Connection between the Hopf formula and learning problems}\label{sec:general_connection_Hopf}

\begin{figure}[htbp]
    \centering   
    \begin{adjustbox}{width=\textwidth}
\begin{tikzpicture}[node distance=2cm]
    \node (min) [nobox, yshift=-0.2cm] {$\min$};
    \node (minarg) [boxsmall, right of=min, xshift=-1.5cm, yshift=-0.37cm, draw=cyan!60, fill=cyan!5] {$\text{}_{\weightvec\in\weightspace}$};
    \node (sum) [nobox, right of=min, xshift=-0.5cm] {$\sum_{i=1}^\numt$};
    \node (param) [box, right of=sum, xshift=-1.25cm, draw=magenta!60, fill=magenta!5]{$\param_i$};
    \node (loss) [box, right of=param, xshift=-0.2cm, draw=green!60, fill=green!5] {$\lossfunc_i(\mathcal{A}F(\bz_i;\HJmom), \by_i)$};
    \node (plus) [nobox, right of=loss, xshift=1.03cm] {$+$};
    \node (regLeft) [nobox, right of=loss, xshift=1.1cm] {};
    \node (regularization) [box, right of=regLeft, xshift=0.1cm, draw=blue!60, fill=blue!5] {$\regfunc(\weightvec)$};
    \node (regRight) [nobox, right of=regularization, xshift=-0.1cm] {};
    
    \node (sup) [nobox, below of=min, xshift=-0cm] {$\min$};
    \node (suparg) [boxsmall, right of=sup, xshift=-1.5cm, yshift=-0.4cm, draw=cyan!60, fill=cyan!5] {$\text{}_{\HJmom\in\HJstatespace}$};
    \node (sum2) [nobox, below of=sum] {$\sum_{i=1}^\numt$};
    \node (equal) [nobox, left of=sup, xshift=1.4cm] {$=$};
    \node (S) [box, left of=equal, xshift=0.45cm, draw=red!60, fill=red!5] {$S(\HJx, \HJt_1, \dots, \HJt_n)$};
    \node (minus) [nobox, left of=S, xshift=0.5cm] {$-$};
    \node (time) [box, below of=param, draw=magenta!60, fill=magenta!5]{$\HJt_i$};
    \node (Hamiltonian) [box, below of=loss, draw=green!60, fill=green!5] {$\Hamiltonian_i(\HJmom)$};
    \node (plus2) [nobox, right of=Hamiltonian, xshift=-0.44cm] {$+$};
    \node (IC) [box, below of=regLeft, draw=blue!60, fill=blue!5] {$\HJIC^*(\HJmom)$};
    \node (linear) [box, below of=regRight, draw=blue!60, fill=blue!5] {$-\langle \HJx, \HJmom\rangle$};

    \node (OCmin) [nobox, below of=sup, xshift=0.2cm] {$\min$};
    \node (OCS) [box, below of=S, xshift=0cm, draw=red!60, fill=red!5] {$S(\HJx, \HJt_1, \dots, \HJt_n)$};
    \node (leftbracket) [nobox, right of=OCmin, xshift=-1.3cm] {$\Bigg\{$};
    \node (OCequal) [nobox, below of=equal] {$=$};
    \node (OCint) [nobox, below of=sum2, xshift=0.05cm] {$\int_{0}^{\sum_{i=1}^{\numt}}$};
    \node (OCintupperbd) [boxsmall, below of=time, yshift=0.15cm, draw=magenta!60, fill=magenta!5] {${}^{\HJt_i}$};
    \node (OCHamiltonian) [box, below of=Hamiltonian, draw=green!60, fill=green!5] {$L(\HJu(s)) $};
    \node (OCds) [nobox, right of=OCHamiltonian, xshift=-1cm] {$ds$};
    \node (OCplus) [nobox, right of=OCds, xshift=-1.6cm] {$+$};
    \node (OCIC) [box, below of=IC, draw=blue!60, fill=blue!5] {$\HJIC\left(\bx\left(\sum_{i=1}^{\numt}\HJt_i\right)\right)$};
    \node (colon) [nobox, right of=OCIC, xshift=-0.5cm] {:};
    \node(OCdynamics) [box, right of=OCIC, xshift=2.4cm, draw=green!60, fill=green!5] {$\dot\bx(s) = f(\HJu(s)), \forall s \in \left(0, \sum_{i=1}^{\numt}\HJt_i\right]$};
    \node(comma) [nobox, right of=OCdynamics, xshift=0.8cm, yshift=-0.11cm] {,};
    \node (terminalposition) [box, right of=OCdynamics, xshift=1.8cm, draw=blue!60, fill=blue!5] {$\bx\left(0\right) = \HJx$};
    \node (rightbracket) [nobox, right of=terminalposition, xshift=-0.9cm] {$\Bigg\}$};

    \node (LPequal) [nobox, above of=equal] {$=$};
    \node (minloss) [box, above of=S, draw=red!60, fill=red!5] {$\min_{\weightvec\in\weightspace}\mathcal{L}(\weightvec)$};
    
    \draw [doublearrow] (param) -- (time);
    \draw [doublearrow] (loss) -- (Hamiltonian);
    \draw [dottedarrow] (regularization) -- (IC);
    \draw [dottedarrow] (regularization) -- (linear);
    \draw [doublearrow] (minarg) -- (suparg);
    \draw [doublearrow] (S) -- (OCS);
    \draw [doublearrow] (time) -- (OCintupperbd);
    \draw [dottedarrow] (IC) -- (OCIC);
    \draw [dottedarrow] (Hamiltonian) -- (OCHamiltonian);
    \draw [dottedarrow] (Hamiltonian) -- (OCdynamics);
    \draw [dottedarrow] (linear) -- (terminalposition);
    \draw[dottedarrow](minloss) -- (S);
\end{tikzpicture}
\end{adjustbox}

    \caption{(See Section~\ref{sec:Hopf}) Mathematical formulation describing the connection between a regularized learning problem (\textbf{top}), the multi-time Hopf formula (\textbf{middle}), and an optimal control problem (\textbf{bottom}). The content of this illustration matches that of Figure~\ref{fig:intro_connection_in_words} by replacing each term in Figure~\ref{fig:intro_connection_in_words} with its corresponding mathematical expression. The colors indicate the associated quantities between each problem. The solid-line arrows denote direct equivalences. The dotted arrows represent additional mathematical relations.}
    \label{fig:connection_multitimeHopf}
\end{figure}

In this section, we connect the Hopf formulas~\eqref{eqt:singletime_Hopf} and~\eqref{eqt:multitime_Hopf} with certain learning problems. Consider a learning problem with data points $\{(\bz_i,\by_i)\}_{i=1}^\numt \subset \LPinspace\times \LPoutspace$. The goal of the learning problem is to find a function $F(\bz;\weightvec)$ with input $\bz\in\LPinspace$ and unknown parameter $\weightvec\in\weightspace$, such that $\mathcal{A}F(\bz_i;\weightvec)$ approximately equals $\by_i$ at every $\bz_i$. Here $\mathcal{A}$ is an operator acting on the function $F$. 
For instance, $\mathcal{A}$ could be the identity operator (as in the regression problem \cite{weisberg2005applied}) or a differential operator (as in PINNs \cite{raissi2019physics}).
Then, the learning problem is given by the following optimization problem:
\begin{equation}\label{eqt:general_learning}
\min_{\weightvec\in \weightspace} \sum_{i=1}^\numt \lambda_i\lossfunc_i(\mathcal{A}F(\bz_i;\weightvec), \by_i) + \regfunc(\weightvec).
\end{equation}
The above loss function consists of two parts: each of $\lossfunc_i(\mathcal{A}F(\bz_i;\weightvec), \by_i)$ is a data fitting term at $(\bz_i, \by_i)$ (where $\lossfunc_i(\ba,\bb)$ is a function measuring the discrepancy between $\ba$ and $\bb$) and $\regfunc$ is a regularization term. In this paper, we assume the function $\weightvec \mapsto \lossfunc_i(\mathcal{A}F(\bz_i;\weightvec), \by_i)$ is convex for all $i=1,\dots, \numt$.

Then, the connection between the learning problem~\eqref{eqt:general_learning}, the Hopf formulas~\eqref{eqt:singletime_Hopf} and~\eqref{eqt:multitime_Hopf}, and the optimal control problem~\eqref{eqt:optimal_control_standardform} is illustrated in Figure~\ref{fig:connection_multitimeHopf}. 
Specifically, if there is only one data point ($\numt =1$), the learning problem~\eqref{eqt:general_learning} is related to the single-time Hopf formula~\eqref{eqt:singletime_Hopf} by setting $\Hamiltonian(\HJmom) = \lossfunc_1(\mathcal{A}F(\bz_1;\HJmom), \by_1)$, $\HJt = \param_1$, and $\regfunc(\HJmom) = \HJIC^*(\HJmom) - \langle \HJx,\HJmom\rangle$. Hence, when we solve these single-point learning problems, we simultaneously evaluate the solution to the HJ PDE~\eqref{eqt:singletimeHJPDE} at the point $(\HJx,\HJt)$, or equivalently, we solve the corresponding optimal control problem~\eqref{eqt:optimal_control_standardform}. Conversely, when we solve the HJ PDE~\eqref{eqt:singletimeHJPDE}, the spatial gradient $\nabla_\HJx S(\HJx, \HJt)$ of the solution gives the minimizer $\weightvec^*$ to the single-point learning problem~\eqref{eqt:general_learning}.

If there are $\numt > 1$ data points, then the learning problem~\eqref{eqt:general_learning} is related to the multi-time Hopf formula~\eqref{eqt:multitime_Hopf} by setting $\Hamiltonian_i(\HJmom) = \lossfunc_i(\mathcal{A}F(\bz_i;\HJmom), \by_i)$, $\HJt_i = \param_i$, and $\regfunc(\HJmom) = \HJIC^*(\HJmom) - \langle \HJx,\HJmom\rangle$. 
Hence, solving these multi-point learning problems is equivalent to evaluating the solution to the HJ PDE~\eqref{eqt:multitime_Hopf} at the point $(\HJx,\HJt_1, \dots, \HJt_\numt)$, which is also equivalent to solving the corresponding optimal control problem~\eqref{eqt:optimal_control_standardform} with terminal time $\HJt = \sum_{j=1}^\numt \HJt_j$ and piecewise running costs $L_i$ and dynamics $f_i$ related via $\Hamiltonian_i(\HJmom) = \sup_{\HJu\in\Rn} \{\langle -f_i(\HJu), \HJmom\rangle - L_i(\HJu)\}$. Similarly to the single-time case, when we solve the multi-time HJ PDE~\eqref{eqt:multitimeHJPDE}, the spatial gradient $\nabla_\HJx S(\HJx, \HJt_1, \dots, \HJt_\numt)$ of the solution gives the minimizer $\weightvec^*$ to the multi-point learning problem~\eqref{eqt:general_learning}.






