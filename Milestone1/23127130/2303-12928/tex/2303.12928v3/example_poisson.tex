\subsection{Poisson equation using PINNs and transfer learning}\label{subsec:example_3}
In this example, we demonstrate the versatility of our Riccati-based method by combining it with existing machine learning techniques to fit the last layer of a PINN. We also show that when we perform hyper-parameter tuning by solving the associated Riccati ODEs (Section~\ref{subsec:method_3}), we not only provide the solution to the updated problem but also a continuum of solutions along a 1D curve on the Pareto front of the data fitting losses and regularization.
Consider the 2D Poisson equation with Dirichlet boundary conditions, which is given by
\begin{equation}\label{eq:poisson}
\begin{dcases}
  \frac{\partial^2 u}{\partial x^2}(x,y) + \frac{\partial^2 u}{\partial y^2}(x,y) = f(x,y) & (x, y) \in \Omega,\\
  u(x, y) = 0 & (x, y) \in \partial \Omega,
\end{dcases}
\end{equation}
where $\Omega := [0, 1]^2$ and $f$ is a source term. We solve this equation using transfer learning and PINNs. Consider the scenario where we only have access to measurements $\{(x_i, y_i, f_i = f(x_i, y_i))\}_{i=1}^N$ of $f$ at limited sampling points. Transfer learning compensates for this lack of knowledge by transferring the knowledge from models pre-trained on similar problems to  solve this new problem of interest. Here, we learn the linear model $u(x,y) = \sum_{k=1}^n\weight_k\phi_k(x,y)$, where each basis function $\phi_k(x, y), k=1, \dots, n$ is the PINN solution of a neural network 
pre-trained to solve the 2D Poisson equation~\eqref{eq:poisson} with similar source terms. 
Transfer learning using pre-trained neural networks as basis functions, as we do here, has recently grown in popularity in the scientific machine learning community \cite{desai2021one, zou2023hydra, goswami2022deep} and has been shown to provide efficient yet accurate inferences even given very limited data \cite{zou2023hydra}. 
We learn the coefficients $\weightvec$ of our linear model by minimizing the following PINN-type loss:
\begin{equation}\label{eq:loss:poisson}
    \mathcal{L}(\weightvec) = \frac{1}{2}\sum_{i=1}^N \lambda_i \left|\sum_{k=1}^n \weight_k\left(\frac{\partial^2\phi_k}{\partial x^2} + \frac{\partial^2\phi_k}{\partial y^2}\right)(x_i, y_i) - f_i \right|^2 + \frac{1}{2}\sum_{k=1}^n \MLreg_k|\weight_k|^2,
\end{equation}
where $\lambda_i=1, i=1,...,N$ and $\MLreg_k = \MLreg, k=1,...,n$ are weights for the data fitting and $\ell_2$-regularization terms, respectively. Note that minimizing~\eqref{eq:loss:poisson} with respect to $\weightvec$ is equivalent to fitting the last layer of a neural network given previous, pre-trained  nonlinear layers as the basis functions.

In our numerical experiments, we use transfer learning to solve the 2D Poisson equation~\eqref{eq:poisson}  with source term  $f(x, y) = \sin(2.5\pi x)\sin(2.5\pi y)$ using $n=100$ basis functions and $N=100$ random measurements of $f$.
We use the multi-head PINN method \cite{zou2023hydra} to obtain the basis functions, which correspond to the shared nonlinear hidden layers of the pre-trained PINN solutions to~\eqref{eq:poisson} with source terms $f(x, y) = \sin(k\pi x)\sin(k\pi y), k=1,2,3,4$. \updatetwo{We note that the boundary condition is hard-encoded in the basis functions, and hence, in \eqref{eq:loss_function}, no penalty term involving the boundary condition is included.} We then solve the learning problem~\eqref{eq:loss:poisson} using our Riccati-based method from Section~\ref{subsec:method_1}.


In Table~\ref{tab:poisson}, we compare the errors of the minimizer $\weightvec^*$ of~\eqref{eq:loss:poisson} and the solution $u$ of the PDE~\eqref{eq:poisson} as we decrease the weight $\MLreg (=\MLreg_k,\forall k)$ of the regularization term from 1 to  1e-5. The reference for $\weightvec^*$ is obtained from minimizing~\eqref{eq:loss:poisson} directly for each value of $\MLreg$ using the method of least squares. The reference for $u$ is computed using a finite difference method with a five-point stencil \updatetwo{and a $257\times257$ uniform grid on $\Omega$} to solve~\eqref{eq:poisson}. The same grid is used to evaluate our trained models. 
We originally minimize~\eqref{eq:loss:poisson} using $\MLreg = 1$ and the Riccati-based approach in Section~\ref{subsec:method_1}. We then compute the solutions for the other values of $\MLreg$ by incrementally decreasing $\MLreg$ by a factor of 10 and using the methodology from Section~\ref{subsec:method_3} to reuse the results of training with the previous value of $\MLreg$ to compute the solution for the new value of $\MLreg$. Consequently, we see that the error in $\weightvec^*$ increases as we decrease $\MLreg$ due to error accumulation from repeated applications of RK4. However, the error of $u$ generally decreases as we decrease $\MLreg$ with the lowest error being achieved when $\MLreg=$ 1e-4.


\begin{table}[ht]
    \footnotesize
    \centering
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         & $\MLreg = \updatetwo{10^0}$ & $\MLreg = \updatetwo{10^{-1}}$ & $\MLreg =\updatetwo{10^{-2}} $ & $\MLreg =\updatetwo{10^{-3}} $ & $\MLreg = \updatetwo{10^{-4}}$ & $\MLreg=\updatetwo{10^{-5}}$  \\
         \hline
       $\ell_1$ error of $\weightvec^*$ & $5.9707\times 10^{-10}$ & $4.2905\times 10^{-8}$ & $1.0725\times 10^{-6}$ & $1.7235 \times 10^{-5}$ & $2.0490\times 10^{-4}$ & $5.8749\times 10^{-3}$\\
       \hline
       $L^2$ relative error of $u$ & $6.5247\%$ & $5.6793\%$ & $3.0390\%$ & $1.7280\%$ & $1.1662\%$ & $1.4736\%$ \\
       \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Errors of the minimizer $\weightvec^*$ of~\eqref{eq:loss:poisson} and the solution $u$ to the 2D Poisson equation~\eqref{eq:poisson} using transfer learning and our Riccati-based approach. The reference for $\weightvec^*$ is given by minimizing~\eqref{eq:loss:poisson} with the corresponding value of $\MLreg$ directly using the method of least squares, and the reference for $u$ is given by solving~\eqref{eq:poisson} using a finite difference method. Since we decrease $\MLreg$ incrementally from 1 to 1e-5, the error of $\weightvec^*$ accumulates due to successive applications of RK4.} 
    \label{tab:poisson}
\end{table}

From the results in Table~\ref{tab:poisson}, we see that our choice of the hyper-parameter $\MLreg$ can greatly influence the accuracy of our learned model. Note that since we fix $\lambda_i = 1, \forall i$ and 
$\MLreg_k = \MLreg, \forall k$, we can view~\eqref{eq:loss:poisson} as a bi-objective loss, where the two objectives are the weighted data fitting term $\frac{1}{2}\sum_{i=1}^N \left|\sum_{k=1}^n \weight_k\left(\frac{\partial^2\phi_k}{\partial x^2} + \frac{\partial^2\phi_k}{\partial y^2}\right)(x_i, y_i)- f_i\right|^2$ and the regularization term $ \frac{1}{2}\sum_{k=1}^n|\weight_k|^2$. 
To better understand the effects of tuning $\MLreg$, in Figure~\ref{fig:poisson}, we explore the Pareto front of these two objectives. Traditional scalarization-based approaches for computing the Pareto front typically rely on discrete samplings of the Pareto front corresponding to discrete choices of $\MLreg$ \cite{Jin2008pareto}.
While our Riccati-based methodology from Section~\ref{subsec:method_3} also recovers discrete points on the Pareto front corresponding to particular choices of $\MLreg$, note that when we change $\MLreg$, e.g., from $\MLreg=1$ to $\MLreg=0.1$, we also recover a one-dimensional curve along the Pareto front corresponding to every  $\MLreg\in[0.1, 1]$. We obtain this 1D curve theoretically via the flow of solutions obtained from the corresponding Riccati ODEs and numerically via the intermediate steps of RK4.
The left plot of Figure~\ref{fig:poisson} shows the 1D curve along the Pareto front recovered by our Riccati-based approach (although note that in this example, the Pareto front is also one-dimensional and hence is equivalent to the exposed 1D curve), where the flow of solutions corresponding to decreasing $\MLreg$ is represented by the arrows.
Thus, although in general our methodology cannot compute the entire Pareto front, every time we change the value of the hyper-parameters, our approach recovers a continuous 1D curve along the Pareto front. In the right plot of Figure~\ref{fig:poisson}, we also visualize how the $L^2$ error of our learned solution $u$ changes as we decrease $\MLreg$. 





\begin{figure}[htbp]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.4\textwidth]{poisson/pareto_front.png}
        \includegraphics[width = 0.4\textwidth]{poisson/error.png}
    \end{subfigure}
    \caption{Results of changing the regularization weight $\MLreg$ when solving the 2D Poisson equation using PINNs and transfer learning. We incrementally decrease the value of $\MLreg$ by evolving the corresponding Riccati ODEs backwards in time. As a result, we obtain a flow of solutions, the direction of which is represented by the arrows in each figure. In the left figure, this flow of solutions gives us that each change in the value of $\MLreg$ from $\hat\MLreg$ to $\tilde\MLreg$ results in the recovery of every point of the Pareto front along the one-dimensional curve parameterized by $\MLreg\in[\hat\MLreg, \tilde\MLreg]$.}
    \label{fig:poisson}
\end{figure}
