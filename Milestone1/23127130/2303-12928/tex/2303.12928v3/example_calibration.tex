\subsection{1D steady-state reaction-diffusion equation and post-training calibration}\label{subsec:example_2}
In this example, we use our Riccati-based approach to apply post-training calibrations when solving a PDE. Specifically, we leverage the methodology in Section~\ref{subsec:method_2} to add or remove data and the methodology in Section~\ref{subsec:method_3} to enforce the boundary conditions of the PDE without retraining the entire learned model.
Consider the following 1D steady-state reaction-diffusion equation:
\begin{equation}\label{eq:reaction}
\begin{dcases}
  D\frac{\partial^2 u}{\partial x^2}(x) + \kappa u(x) = f(x), x\in [0, 1],\\
  u(0) = u(1) = 0,
\end{dcases}
\end{equation}
where $D=0.01$ is the diffusion coefficient, $\kappa=-1$, and $f(x)$ is the source term of which noisy measurements $\{(x_i, f_i\approx f(x_i))\}_{i=1}^N\subset\R\times\R$ are available.
We consider the scenario where regular training has been employed but with either insufficient data or sufficient data with outliers, both of which yield inaccurate inferences of the solution. 
We further assume that extra information is provided after the regular training and seek to perform post-training calibrations to incorporate this extra information into the already-trained models without losing information from the original training.
In the literature, it is well-established that post-training calibrations can significantly increase the performance of deployed machine learning methods \cite{psaros2023uncertainty, zou2022neuraluq}.
However, designing computationally efficient methods for performing these post-calibrations is still of great interest.

In this example, we solve this PDE~\eqref{eq:reaction} by reformulating the PDE as an optimization problem \cite{raissi2019physics, sirignano2018dgm, han2018solving}.
We use a linear model to approximate the solution, i.e. $u(x) = \sum_{k=1}^n \weight_k\phi_k(x)$, where $n=21$ and $\{\phi_k(x)\}_{k=1}^n=\{1\}\cup\{\sin(2l\pi x),$ $ \cos(2l\pi x)\}_{l=1}^{(n-1)/2}$ are the truncated Fourier basis functions on $[0, 1]$. 
We learn the coefficients $\weightvec = [\weight_1, \dots, \weight_n]^T$ of the linear model by minimizing the following loss:
\begin{equation}\label{eq:calibration:loss}
\begin{aligned}
    \mathcal{L}(\weightvec) & = \frac{1}{2}\sum_{i=1}^N \lambda_i \Bigg|D\sum_{k=1}^{n} \weight_k\frac{\partial^2 \phi}{\partial x^2}(x_i) + \kappa \sum_{k=1}^{n} \weight_k \phi_k(x_i) - f_i \Bigg|^2 \\
    & + \frac{1}{2}\lambda_b\left|\sum_{k=1}^{n} \weight_k \phi_k(0) - 0\right|^2 + \frac{1}{2}\lambda_b\left|\sum_{k=1}^{n} \weight_k \phi_k(1) - 0\right|^2 + \frac{1}{2}\sum_{k=1}^{n} \MLreg_k|\weight_k|^2,
\end{aligned}
\end{equation}
where $\lambda_i, i=1,...,N$, $\lambda_b$, and $\MLreg_k, k=1,..,n$ are balancing weights for the PDE residual, the boundary conditions, and the regularization term, respectively. In our numerical experiments, we assume the exact solution to be $u(x) = \sin^3(2\pi x)$ (and $f$ to be defined by~\eqref{eq:reaction}, accordingly) and the noise to be additive Gaussian with zero mean and standard deviation $0.1$. For the regular training, we set $\lambda_i=1$, $\lambda_b=1$, and $\MLreg_k=1$ and apply our Riccati-based approach (see Section~\ref{subsec:method_1}) to get our original estimate of the minimizer $\weightvec^*$ of the loss function~\eqref{eq:calibration:loss}. Using our Riccati-based approach yields an  $\ell_1$ error of $8.9950\times 10^{-10}$ and relative $\ell_1$ error of $1.4288\times 10^{-9}$ in $\weightvec^*$, where the reference is obtained by minimizing~\eqref{eq:calibration:loss} directly using the method of least squares.


In the leftmost column of Figure~\ref{fig:calibration:1}, we see that the accuracy of both $u$ and $f$ as inferred by the regular training is impaired by a lack of data around the highest peak and lowest valley of the exact functions. 
To compensate, we first calibrate our model by adding some new noisy measurements of $f$ in these regions where the data points are sparse. This calibration uses the methodology described in Section~\ref{subsec:method_2}, and the results are shown in the middle column of Figure~\ref{fig:calibration:1}. 
Next, we note that the inferred $u$ still disagrees with the exact solution at the boundary points. 
Hence, we further calibrate our model by increasing the value of the boundary weight $\lambda_b$ from $1$ to $10$ to enforce the boundary conditions of the PDE. This calibration is done using the methodology described in Section~\ref{subsec:method_3} and the results of this second calibration are presented in the rightmost column of Figure~\ref{fig:calibration:1}. In both cases, we observe that the calibrations successfully improve the accuracy of the learned model. \updatethree{These results are also reflected in the relative $L^2$ errors shown in Table~\ref{tab:calibration:1}.}


\begin{table}[ht]
    \footnotesize
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & Regular training & First calibration & Second calibration \\
       \hline
       relative $L^2$  error of $f$ & $66.40\%$ & $7.15\%$ & $6.49\%$\\
       \hline
      relative $L^2$  error of $u$ & $57.29\%$ & $7.35\%$ & $5.57\%$\\
       \hline
    \end{tabular}
\caption{Errors in solving the 1D steady-state reaction-diffusion equation~\eqref{eq:reaction} and regressing $f$ at different stages of training, using our Riccati-based approach. Regular training is done with insufficient data and provides inaccurate inferences. The first calibration adds new measurements of $f$, and the second calibration increases the weights $\lambda_b$ of the boundary conditions in \eqref{eq:calibration:loss}. Both calibration steps successfully improve the accuracy of our inferences without requiring retraining on or access to the previous data. Qualitative results can be found in Figure~\ref{fig:calibration:1}.}
\label{tab:calibration:1}
\end{table}

\begin{figure}[ht]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.3\textwidth]{calibration/f_1.png}
        \includegraphics[width = 0.3\textwidth]{calibration/f_2.png}
        \includegraphics[width = 0.3\textwidth]{calibration/f_3.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.3\textwidth]{calibration/u_1.png}
        \includegraphics[width = 0.3\textwidth]{calibration/u_2.png}
        \includegraphics[width = 0.3\textwidth]{calibration/u_3.png}
        \caption{}
    \end{subfigure}
    \caption{Results of solving the 1D steady-state reaction-diffusion equation~\eqref{eq:reaction} with noisy measurements of the source term $f$ in the domain and noiseless measurements of the solution $u$ on the boundary. (a) results for $f$; (b) results for $u$. \textbf{Left}: results of regular training; \textbf{middle}: calibrating the results with some additional noisy measurements of $f$; \textbf{right}: calibrating the results further by enforcing the boundary conditions by increasing the value of $\lambda_b$ in the loss function~\eqref{eq:calibration:loss}. The regular training uses our Riccati-based method in Section~\ref{subsec:method_1} to minimize~\eqref{eq:calibration:loss}, while the calibrations use the adaptations of our method in Section~\ref{subsec:method_2}. Calibrations are employed without re-training or access to the data from the previous training, which demonstrates the advantages in both memory storage and computational complexity of our Riccati-based approach over conventional machine learning methods.}
    \label{fig:calibration:1}
\end{figure}

Note that our Riccati-based approach allows us to perform each of these calibration steps using only the new or changed values in that step and the results of the previous training step. In other words, each step of this training process (including the original training and each subsequent calibration step) is done without sharing data between training steps, which exactly matches the framework of federated learning \cite{li2020federated}. Thus, our methodology may be relevant to distributed training or collaborative learning applications, where data privacy is of concern.



Next, we discuss another post-training calibration technique.
In this case, we assume the data for the regular training is sufficient but contains outliers due to large noise. We again assume the regular training is performed using the Riccati-based approach from Section~\ref{subsec:method_1}. Then, we eliminate these outliers using the methodology described in Section~\ref{subsec:method_2}, which only requires knowledge about the outliers to be removed and the results of the regular training.
The results of this post-training outlier removal show that eliminating these points successfully improves the accuracy of the learned models (see Appendix \ref{appendix:1}, Figure~\ref{fig:calibration:2}). \updatethree{Finally, in Appendix~\ref{appendix:1}, Figure~\ref{fig:example_2:3}, we also provide a large-scale continual learning example for solving~\eqref{eq:reaction} to further demonstrate the performance of our Riccati-based approach in large data settings.}


