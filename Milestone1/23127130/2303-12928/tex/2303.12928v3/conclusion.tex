\section{Summary}\label{sec:conclusion}
In this paper, we established a novel theoretical connection between \updateone{regularized} learning problems and the multi-time Hopf formula. In doing so, we showed that when we solve these learning problems, we actually solve certain multi-time HJ PDEs and their corresponding optimal control problems. In this work, we focused on the development of the connection between regularized linear regression and the LQR problem. By leveraging this connection, we developed new methodology based on solving Riccati ODEs that allows us to design new training approaches for certain machine learning applications, such as continual learning, post-training calibration, hyper-parameter tuning and exploration of the associated Pareto front, and sparse dynamics identification. We also showed that our Riccati-based approach yields some promising computational advantages over conventional learning methods; after the original training, the models learned using our approach can be continually updated without having to retrain the entire model or having access to all of the previous data, which could be particularly useful in continual learning \cite{parisi2019continual, kirkpatrick2017overcoming, van2019three} and federated learning \cite{li2020federated, kairouz2021advances}.
We note that our connection shares some similarities to the approach of NeuralODEs \cite{chen2018neural}, which also connects ODEs to machine learning; however, NeuralODEs focuses on establishing new neural networks architectures using ODEs, whereas we make connections between (Riccati) ODEs (that arise from optimal control theory) and the training/optimization process of existing machine learning methods and neural networks.
\updateeditor{Our connection is also related to~\cite{todorov2008general}, which connects LQR problems with Kalman filters; however,~\cite{todorov2008general} does not make any connections to PDEs or learning problems.}

Thus, our theoretical connection and Riccati-based methodology present many exciting opportunities. Some possible future directions are as follows. While our Riccati-based methodology allows us to alter the hyper-parameters and data points used in the learning problem without having to retrain on all previous data, it also requires that the original training be done using our Riccati-based methodology. It would allow for increased versatility if we could more easily combine our Riccati-based approach with other training methods. 
Additionally, in Sections~\ref{subsec:method_4} and~\ref{subsec:example_4}, we showed that our Riccati-based approach allows computations to be reused when using non-quadratic regularizations. However, in this case, the training process still had to be restarted if the hyper-parameters, dataset, or regularization type is changed. It would allow for more flexibility if we could develop more adaptive processes for changing these aspects of the learning problem when the regularization is not quadratic. 

In Section~\ref{sec:LQR}, we focused on LQR problems, where the dynamics are independent of the trajectory, but it would be worthwhile to investigate what connections LQR problems with  general linear dynamics may yield (e.g., LQR problems with general linear dynamics may be reformulated as LQR problems with state-independent dynamics using a change of variable \cite{Darbon2016Algorithms}). 
Another natural extension would be to consider nonlinear models, which currently pose challenges in both scientific machine learning and optimal control and hence, connections drawn in this case would benefit both fields.
\updateone{It would also be interesting to explore applications of our connection between general learning problems and general optimal control problems (i.e., the connection in Section~\ref{sec:general_connection_Hopf}). 
In general, these optimal control problems can be tackled by solving characteristic line equations, which are represented by two-point boundary value problems (TPBVPs) and thus are more challenging to solve than initial value problems like the Riccati equation. However, working with these TPBVPs may be bypassed in some cases by considering numerical solvers such as those in~\cite{chen2021hopf,chen2021lax,darbon2015convex,darbon2019decomposition,Darbon2016Algorithms,yegorov2017perspectives}.}

Alternatively, if we relax our assumption on convexity by allowing for nonconvex loss functions (or equivalently, nonconvex Hamiltonians), we could also extend our connection to be between learning problems and differential games \cite{evans1984differentialgames} instead of optimal control. However, even with convex losses, there are already many interesting potential applications in machine learning to pursue. 
\updateeditor{For example, in the regularized linear regression case, it may be of interest to explore adjusting methods such as recursive least squares to perform hyperparameter estimation, e.g., similarly to~\cite{chung2020sampled}.} 
More generally, the theoretical connection presented in Section~\ref{sec:general_connection_Hopf} for general convex losses provides a formula for the optimal learning parameters $\weightvec^*$ in terms of the hyper-parameters. This connection could be leveraged to simplify the bi-level optimization in meta-learning applications to a single-level, constrained optimization problem. 
Finally, we could generalize our theory to instead consider viscous HJ PDEs, which add a Laplacian term to the right-hand side of the HJ PDEs~\eqref{eqt:singletimeHJPDE} and~\eqref{eqt:multitimeHJPDE}. Then, by leveraging the recently established connection between viscous HJ PDEs and Bayesian modeling \cite{langlois2021HJvariational}, we could extend to applications in Bayesian inference.

