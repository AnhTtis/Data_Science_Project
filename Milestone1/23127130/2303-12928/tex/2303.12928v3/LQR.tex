\section{Linear Quadratic Regulator}\label{sec:LQR}
In this section, we develop our theoretical connection from Section~\ref{sec:general_connection_Hopf} in the specific case where the optimal control problem~\eqref{eqt:optimal_control_standardform} corresponds to the LQR problem  \cite{TrentelmanControlLinearSystems, anderson2007optimal}. We show that solving certain LQR problems is equivalent to solving learning problems with linear models, quadratic data fitting losses, and quadratic regularization (i.e., an $\ell_2$-regularized linear regression problem). Although broader classes of learning problems are of interest, we restrict to linear regression problems as a starting point for demonstrating the potential of our theoretical connection. Specifically, we leverage our theoretical connection to show how established techniques for solving HJ PDEs (e.g., the Riccati ODEs \cite{mceneaney2006max}) can be reused to solve this class of learning problems. 

\subsection{Introduction to the Linear Quadratic Regulator and Riccati equation}\label{sec:intro_LQR}

The finite-horizon, continuous-time LQR is given by
\begin{multline}\label{eqt:LQR_general_control}
    S(\HJx,\HJt) = \min_{\updateone{\HJu(\cdot)}} \Bigg\{\int_0^\HJt \left(\frac{1}{2}\bx(s)^T\LQRxx\bx(s) + \frac{1}{2}\HJu(s)^T\LQRuu\HJu(s) + \bx(s)^T\LQRxu\HJu(s)\right) ds \\ + \frac{1}{2}\bx(\HJt)^T\LQRTC\bx(\HJt): 
    \dot\bx(s) = \LQRA\bx(s) + \LQRB\HJu(s) \forall s\in(0,t], \bx(0) = \HJx\Bigg\},
\end{multline}
where $\LQRxx, \LQRTC \in\R^{n\times n}$ and $\LQRuu\in\R^{m\times m}$ are symmetric positive definite, $\LQRxu\in\R^{n\times m}$, $\LQRA\in\R^{n\times n}$, and $\LQRB\in\R^{n\times m}$. The corresponding HJ PDE is 
\begin{equation}\label{eqt:LQR_HJPDE}
    \begin{dcases}
    \frac{\partial S(\HJx,\HJt)}{\partial \HJt} + \Hamiltonian(\HJx,\nabla_\HJx S(\HJx,\HJt)) = 0 & \HJx\in\R^n, \HJt > 0, \\
    S(\HJx,0) = \HJIC(\HJx)  & \HJx\in\R^n,
    \end{dcases}
\end{equation}
where the initial data of the HJ PDE is given by the terminal cost $\HJIC(\HJx) := \frac{1}{2}\HJx^T\LQRTC\HJx$ of the optimal control problem and the Hamiltonian $\Hamiltonian$ is defined by
\begin{equation}
    \begin{aligned}
    \Hamiltonian(\HJx,\HJmom) & = \sup_{\HJu\in\R^m} \langle -f(\HJx,\HJu), \HJmom \rangle - L(\HJx,\HJu) \\
     &=  -\langle \LQRA\HJx, \HJmom\rangle - \frac{1}{2}\langle \HJx, \LQRxx\HJx\rangle + \frac{1}{2}\langle \LQRB^T\HJmom + \LQRxu^T\HJx, \LQRuu^{-1}(\LQRB^T\HJmom + \LQRxu^T\HJx) \rangle,
    \end{aligned}
\end{equation}
where $f(\HJx,\HJu) = \LQRA\HJx + \LQRB\HJu$ is the source term of the dynamics and $L(\HJx,\HJu) = \frac{1}{2}\HJx^T\LQRxx\HJx + \frac{1}{2}\HJu^T\LQRuu\HJu + \HJx^T\LQRxu\HJu$ is the running cost. Note that because of the spatial dependence in the Hamiltonian $\Hamiltonian$, the Hopf formula cannot be applied directly to the above LQR problem without additional assumptions. In Sections~\ref{sec:LQR_1ptregression} and~\ref{sec:LQR_multiptregression}, we discuss some assumptions under which $\Hamiltonian$ is independent of the spatial variable $\HJx$ and the corresponding learning problems that can be solved via our connection through the Hopf formula (see, for example, Figure~\ref{fig:connection_LQR}).


It is well-known that this LQR problem can be solved using the Riccati equation as follows \cite{mceneaney2006max}. Define $\Cpp = \LQRB \LQRuu^{-1}\LQRB^T$, $\Cxx = -\LQRxu\LQRuu^{-1}\LQRxu^T + \LQRxx$, and $\Cxp = \LQRA - \LQRB \LQRuu^{-1}\LQRxu^T$. 
Then, the solution is also given by
$S(\HJx,\HJt) = \frac{1}{2} \HJx^T \Sxx(\HJt)\HJx$, 
where the function $\Sxx: [0,\infty)\to\R^{n\times n}$ takes values in the space of symmetric positive definite matrices and solves the following Riccati equation:
\begin{equation} \label{eqt: odeP}
{\small
    \begin{dcases}
    \dot{\Sxx}(\HJt) =  -\Sxx(\HJt)^T\Cpp\Sxx(\HJt) + \Sxx(\HJt)^T\Cxp + \Cxp^T\Sxx(\HJt) + \Cxx &\HJt\in(0,+\infty),\\
    \Sxx(0) = \LQRTC.
    \end{dcases}
    }
\end{equation}

When $\Hamiltonian$ does not depend on $\HJx$, we can use our connection to modify the corresponding LQR problem to consider different HJ PDEs and, hence, to accommodate different learning problems. For example, adding lower order terms in the running cost $L$ and/or the source term $f$ of the dynamics corresponds to adding lower order terms in the Hamiltonian of the HJ PDE or, equivalently, in the data fitting term of the learning problem. 
Similarly, adding lower order terms in the 
terminal cost $\HJIC$ corresponds to adding lower order terms in the initial condition of the HJ PDE or, equivalently, in the regularization term of the learning problem. \updatetwo{In Appendix~\ref{appendix:general_LQR}, we present a more general LQR problem with lower order terms that is more amenable to forming our connection to linear regression problems.}


\subsection{Connection to single-point regularized linear regression problems}\label{sec:LQR_1ptregression}


In this section, we establish a relation between the linear regression problem with only one data point and the LQR problem~\eqref{eqt:LQR_general_control} with $\LQRA = \LQRxx = 0$ and $\LQRxu = 0$. Note that, to do this, we also add in some lower order terms to the original LQR problem~\eqref{eqt:LQR_general_control} \updatetwo{(e.g., see Appendix~\ref{appendix:general_LQR})}.
In this case, the LQR problem becomes
    \begin{multline}\label{eqt:optctrl_1pt}
        S(\HJx,\HJt) = \min_{\updateone{\HJu(\cdot)}}  \Bigg\{\int_0^\HJt \left(\frac{1}{2}\HJu(s)^T\LQRuu\HJu(s) -\ba^T\HJu(s)\right)ds \\ + \frac{1}{2}\bx(\HJt)^T\LQRTC\bx(\HJt)
        + \bb^T \bx(\HJt)\colon 
         \dot\bx(s) = \LQRB\HJu(s) \forall s\in(0,\HJt], \bx(0) = \HJx\Bigg\},
    \end{multline}
    and the corresponding HJ PDE is given by~\eqref{eqt:LQR_HJPDE}, where $\HJIC(\HJx) = \frac{1}{2}\HJx^T\LQRTC\HJx + \bb^T \HJx$ is the initial data/terminal cost whose Fenchel transform is given by
    $$\HJIC^*(\HJmom) = \sup_{\HJx\in\Rn} \langle \HJx, \HJmom\rangle - \HJIC(\HJx) = \frac{1}{2}\left\|\LQRTC^{-1/2} (\HJmom - \bb)\right\|_2^2$$
    and the Hamiltonian $\Hamiltonian$ is given by 
    $$
    \Hamiltonian(\HJmom) 
    = \sup_{\HJu\in\R^m} \langle -\LQRB \HJu, \HJmom\rangle - \frac{1}{2}\HJu ^T\LQRuu\HJu + \ba^T \HJu = \frac{1}{2}\left\|\LQRuu^{-1/2}(\LQRB^T\HJmom- \ba)\right\|_2^2, $$
    where $f(\HJu) = \LQRB\HJu$ is the source term of the dynamics and $L(\HJu) = \frac{1}{2}\HJu^T\LQRuu\HJu$ is the running cost. Then, using the single-time Hopf formula~\eqref{eqt:singletime_Hopf}, we have that the solution to the HJ PDE is given by
    \begin{equation}\label{eqt:hopf_1ptregression}
        S(\HJx,\HJt) = \sup_{\HJmom\in\Rn} \left\{\langle \HJx, \HJmom\rangle - \frac{\HJt}{2}\left\|\LQRuu^{-1/2}( \LQRB^T \HJmom - \ba)\right\|_2^2 - \frac{1}{2}\left\|\LQRTC^{-1/2} (\HJmom - \bb)\right\|_2^2 \right\}.
    \end{equation}
    In this case, we can compute the maximizer in the above Hopf formula explicitly, which can be done numerically using the method of least squares.
   
    Alternatively, this LQR problem can also be solved via the Riccati ODEs, which are given by 
    \begin{equation}
    \dot{\Sxx}(\HJt) =  -\Sxx(\HJt)^T\LQRB\LQRuu^{-1}\LQRB^T\Sxx(\HJt), \quad 
    \dot{\Sx}(\HJt) = -\Sxx(\HJt)^T\LQRB\LQRuu^{-1}(\LQRB^T\Sx(\HJt) - \ba),
    \end{equation}
    $$\dot{\Sc}(\HJt) = -\frac{1}{2}\left\|\LQRuu^{-1/2}(\LQRB^T\Sx(\HJt)- \ba)\right\|_2^2$$
    with initial conditions $\Sxx(0) = \LQRTC$, $\Sx(0) = \bb$, and $\Sc(0) = 0$.


The above Hopf formula~\eqref{eqt:hopf_1ptregression} is related to the learning problem~\eqref{eqt:general_learning} with quadratic data fidelity term and quadratic regularization.
Specifically, let $\HJt = \param$ and $\HJmom = \weightvec$. Then, solving the above maximization problem~\eqref{eqt:hopf_1ptregression}  is equivalent to minimizing the following loss function with respect to $\weightvec = [\weight_1, \dots, \weight_n]^T$:
    \begin{equation}\label{eqt:loss_1ptregression}
        \lossfunc(\weightvec) = \frac{\param}{2}\left\|\LQRuu^{-1/2} (\LQRB^T \weightvec - \ba)\right\|_2^2 + \frac{1}{2}\left\|\LQRTC^{-1/2} (\weightvec - (\bb + \LQRTC\HJx))\right\|_2^2.
    \end{equation}
This loss function corresponds to a one-point linear regression problem, where the regularization term is given by $\frac{1}{2}\left\|\LQRTC^{-1/2} (\weightvec - (\bb + \LQRTC\HJx))\right\|_2^2$ and the data fitting term is given by $\frac{\param}{2}\left\|\LQRuu^{-1/2} (\LQRB^T \weightvec - \ba)\right\|_2^2$; i.e., set $\numt = 1$ in Figure~\ref{fig:connection_LQR}.
The minimizer $\weightvec^*$ of $\lossfunc$ is related to the solution of the Riccati equation via
\begin{equation}\label{eqt:singlept_minimizer}
\weightvec^* (=\HJmom^*) = \nabla_\HJx S(\HJx,\param) = \Sxx(\param)\bx + \Sx(\param),
\end{equation}
where $\HJmom^*$ is the minimizer in the Hopf formula~\eqref{eqt:hopf_1ptregression}.
Note that if we only need to recover the minimzer $\weightvec^*$, then we only need to solve two ODEs (namely, the ODEs for $\Sxx, \Sx$) since~\eqref{eqt:singlept_minimizer} does not depend on $\Sc$.




\subsection{Connection to multi-point regularized linear regression problems}\label{sec:LQR_multiptregression}

\begin{figure}[htbp]
    \centering
\begin{adjustbox}{width=\textwidth}
\begin{tikzpicture}[node distance=2cm]
    \node (min) [nobox, yshift=-0.2cm] {$\min$};
    \node (minarg) [boxsmall, right of=min, xshift=-1.5cm, yshift=-0.37cm, draw=cyan!60, fill=cyan!5] {$\text{}_{\weightvec\in\weightspace}$};
    \node (sum) [nobox, right of=minarg, xshift=-0.5cm] {$\sum_{i=1}^\numt$};
    \node (param) [box, right of=sum, xshift=-0.7cm, draw=magenta!60, fill=magenta!5] {$\param_i$};
    \node (loss) [box, right of=param, xshift=0.5cm, draw=green!60, fill=green!5] {$\frac{1}{2}\left\|\LQRuu_i^{-1/2}(\LQRB_i^T\weightvec - \ba_i)\right\|_2^2$ };
    \node (plus) [nobox, right of=loss, xshift=1.2cm] {$+$};
    \node (regLeft) [nobox, right of=loss, xshift=3.5cm] {};
    \node (regularization) [box, right of=regLeft, xshift=-0.2cm, draw=blue!60, fill=blue!5] { $\frac{1}{2}\left\|\LQRTC^{-1/2} (\weightvec - (\bb + \LQRTC\HJx))\right\|_2^2$};
    \node (regRight) [nobox, right of=regularization, xshift=-0.1cm] {};
    
    \node (sup) [nobox, below of=min, xshift=-0cm] {$\min$};
    \node (suparg) [boxsmall, right of=sup, xshift=-1.5cm, yshift=-0.37cm, draw=cyan!60, fill=cyan!5] {$\text{}_{\HJmom\in\HJstatespace}$};
    \node (sum) [nobox, below of=sum] {$\sum_{i=1}^\numt$};
    \node (time) [box, below of=param, xshift=-0cm, draw=magenta!60, fill=magenta!5] {$\HJt_i$};
    \node (equals) [nobox, left of=sup, xshift=1.45cm] {$=$};
    \node (S) [box, left of=equals, xshift=0.5cm, draw=red!60, fill=red!5] {$S(\HJx, \HJt_1, \dots, \HJt_\numt)$};
    \node (minus2) [nobox, left of=S, xshift=0.5cm] {$-$};
    \node (Hamiltonian) [box, below of=loss, draw=green!60, fill=green!5] {$\frac{1}{2}\left\|\LQRuu_i^{-1/2}(\LQRB_i^T\HJmom- \ba_i)\right\|_2^2$};
    \node (plus2) [nobox, right of=Hamiltonian,xshift=0.65cm] {$+$};
    \node (IC) [box, below of=regLeft, draw=blue!60, fill=blue!5] {$\frac{1}{2}\left\|\LQRTC^{-1/2}(\HJmom - \bb)\right\|_2^2$};
    \node (linear) [box, below of=regRight, draw=blue!60, fill=blue!5] {$-\langle \HJx, \HJmom\rangle$};

    \node (OCmin) [nobox, below of=sup, yshift=-0.3cm] {$\min$};
    \node (OCminarg) [boxsmall, right of=OCmin, xshift=-1.5cm, yshift=-0.37cm, draw=cyan!60, fill=cyan!5] {$\text{}_{\HJu(\cdot)}$};
    \node (OCS) [box, below of=S, xshift=0cm, yshift=-0.35cm, draw=red!60, fill=red!5] {$S(\HJx, \HJt_1, \dots, \HJt_\numt)$};
    \node (OCequals) [nobox, below of=equals, xshift=0cm, yshift=-0.35cm] {$=$};
    \node (OCint) [nobox, below of=time, xshift=-1.1cm] {$\sum_{i=1}^\numt \int_{\sum_{j=1}^{i-1}\HJt_j}^{\sum_{j=1}^{i}} $};
    \node (OCt) [boxsmall, right of=OCint, xshift=-0.9cm, yshift=0.2cm, draw=magenta!60, fill=magenta!5] {$\text{}^{\HJt_j}$};
    \node (OCHamiltonian) [box, below of=Hamiltonian, xshift=-0cm, draw=green!60, fill=green!5] {$\left(\frac{1}{2}\HJu(s)^T\LQRuu_i\HJu(s) -\ba_i^T\HJu(s)\right)$};
    \node (OCds) [nobox, right of=OCHamiltonian, xshift=0.4cm] {$ds$};
    \node (OCplus) [nobox, right of=OCds, xshift=-1.6cm] {$+$};
    \node (OCIC) [box, below of=IC, draw=blue!60, fill=blue!5] {$\frac{1}{2}\bx(T_\numt)^T\LQRTC\bx(T_\numt) + \bb^T \bx(T_\numt)$};
    \node (OCdynam) [box, right of=OCIC, xshift=3.05cm, draw=green!60, fill=green!5] {$\dot\bx(s) = \LQRB_i\HJu(s)\forall s\in \left(T_{i-1}, T_i\right]$};
    \node (OCinitialposition) [box, right of=OCdynam, xshift=1.5cm, draw=blue!60, fill=blue!5] {$\bx(0) = \HJx$};
    \node (leftbracket) [nobox, left of=OCint, xshift=0.8cm] {$\Bigg\{$};
    \node (colon) [nobox, right of=OCIC, xshift=0.55cm] {$:$};
    \node (comma) [nobox, right of=OCdynam, xshift=0.45cm, yshift=-0.11cm] {,};
    \node (rightbracket) [nobox, right of=OCinitialposition, xshift=-1.05cm] {$\Bigg\}$};

    \node (LPequal) [nobox, above of=equal] {$=$};
    \node (minloss) [box, above of=S, draw=red!60, fill=red!5] {$\min_{\weightvec\in\weightspace}\mathcal{L}(\weightvec)$};
    \draw [dottedarrow] (S) -- (minloss);
    
    \draw [doublearrow] (loss) -- (Hamiltonian);
    \draw [dottedarrow] (regularization) -- (IC);
    \draw [dottedarrow] (regularization) -- (linear);
    \draw [doublearrow] (minarg) -- (suparg);
    \draw [doublearrow] (OCS) -- (S);
    \draw [doublearrow] (OCt) -- (time);
    \draw [dottedarrow] (OCHamiltonian) -- ++(0,1.5cm);
    \draw [dottedarrow] (OCIC) -- (IC);
    \draw [dottedarrow] (OCdynam) -- (Hamiltonian);
    \draw [doublearrow] (time) -- (param);
    \draw [dottedarrow] (linear) -- (OCinitialposition);
    \draw [dottedarrow] (suparg) -- (OCminarg);
\end{tikzpicture}
\end{adjustbox}
    \caption{(See Section~\ref{sec:LQR}) Mathematical formulation describing the connection between a regularized linear regression problem  (\textbf{top}), the multi-time Hopf formula (\textbf{middle}), and a piecewise LQR problem with $\LQRA = \LQRxx = 0$ and $\LQRxu = 0$ on each piece (\textbf{bottom}). Note that $T_i = \sum_{j=1}^i t_j$. The content of this illustration is a special case of the connection in Figure~\ref{fig:connection_multitimeHopf} using quadratic data fitting losses and quadratic regularization. The colors indicate the associated quantities between each problem. The solid-line arrows denote direct equivalences. The dotted arrows represent additional mathematical relations.}
    \label{fig:connection_LQR}
\end{figure}

If the running cost is piecewise quadratic with $\LQRxx = 0, \LQRxu = 0$ on each piece and the dynamics are piecewise linear with $\LQRA = 0$ on each piece, then we get a more general piecewise LQR problem in the form of~\eqref{eqt:optimal_control_standardform}
with piecewise running costs $L_i(\HJu) = \frac{1}{2}\HJu ^T\LQRuu_i\HJu - \ba_i^T \HJu$, where each $\LQRuu_i\in \R^{m\times m}$ is symmetric positive definite, piecewise dynamics $f_i(\HJu) = \LQRB_i \HJu$, terminal cost $\HJIC(\HJx) = \frac{1}{2}\HJx^T\LQRTC \HJx + \bb^T\HJx$, and terminal time $\HJt = \sum_{j=1}^\numt \HJt_j$. Recall that this optimal control problem corresponds to the multi-time HJ PDE~\eqref{eqt:multitimeHJPDE} with Hamiltonians $\Hamiltonian_i(\HJmom) = \sup_{\HJu\in\R^m} \{\langle -f_i(\HJu), \HJmom\rangle - L_i(\HJu)\}$ and initial data $\HJIC$. Then, the solution to this LQR problem and corresponding HJ PDE is given by the following  multi-time Hopf formula:
\begin{equation} \label{eqt:piece_LQR_Hopf}
S(\HJx,\HJt_1, \dots, \HJt_\numt) = \sup_{\HJmom\in\Rn} \left\{\langle \HJx, \HJmom\rangle - \sum_{i=1}^\numt \frac{\HJt_i}{2}\|\LQRuu_i^{-1/2}(\LQRB_i^T\HJmom- \ba_i)\|_2^2 - \frac{1}{2}\|\LQRTC^{-1/2}(\HJmom - \bb)\|_2^2\right\}.
\end{equation} 


Define $T_i$ to be
\begin{equation}
T_i = \sum_{j=1}^i t_j.
\end{equation}
Then, the multi-time HJ PDE and piecewise LQR problem can also be solved using the following Riccati equation: 
$S(\HJx,\HJt_1,\cdots, \HJt_N) = \frac{1}{2} \HJx^T \Sxx(T_N)\HJx + \Sx(T_N)^T\HJx + \Sc(T_N)$, where the function $\Sxx:[0,\infty)\to\R^{n\times n}$ takes values in the space of symmetric positive definite matrices and solves the following piecewise Riccati equation:
\begin{equation} \label{eqt: odeP_piecewise}
    \begin{dcases}
    \dot{\Sxx}(s) =  -\Sxx(s)^T\LQRB_i\LQRuu_i^{-1}\LQRB_i^T\Sxx(s) &s\in \left(T_{i-1}, T_i\right)\\
    \Sxx(0) = \LQRTC,
    \end{dcases}
\end{equation}
the function $\Sx:[0,\infty)\to\Rn$ solves the following piecewise linear ODE:
\begin{equation} \label{eqt: odeq_piecewise}
    \begin{dcases}
    \dot{\Sx}(s) = -\Sxx(s)^T\LQRB_i\LQRuu_i^{-1}(\LQRB_i^T\Sx(s) - \ba_i)&s\in \left(T_{i-1}, T_i\right),\\
    \Sx(0) = -\bb,
    \end{dcases}
\end{equation}
and the function $\Sc:[0,\infty)\to\R$ solves the following piecewise ODE:
\begin{equation} \label{eqt: oder_piecewise}
    \begin{dcases}
    \dot{\Sc}(s) = -\frac{1}{2}\left\|\LQRuu_i^{-1/2}(\LQRB_i^T\Sx(s)- \ba_i)\right\|_2^2 &s\in \left(T_{i-1}, T_i\right),\\
    \Sc(0) = 0.
    \end{dcases}
\end{equation}

The above multi-time Hopf formula~\eqref{eqt:piece_LQR_Hopf} can also be regarded as a linear regression problem with multiple data points, as illustrated in Figure~\ref{fig:connection_LQR}. Let  $\frac{1}{2}\|\LQRTC^{-1/2}(\HJmom - \bb)\|_2^2$ be the the regularization term and  $\frac{\HJt_i}{2}\|\LQRuu_i^{-1/2}(\LQRB_i^T\HJmom- \ba_i)\|_2^2$ be the data fitting term at the $i$-th data point. 
Then, the multi-time LQR problem~\eqref{eqt:piece_LQR_Hopf} is equivalent to the learning problem $\min_{\weightvec}\lossfunc(\weightvec)$, where the loss function $\lossfunc:\Rn\to\R$ is given by
\begin{equation} \label{eqt:regression_multidata}
\lossfunc(\weightvec) = \sum_{i=1}^\numt \frac{\param_i}{2}\left\|\LQRuu_i^{-1/2}(\LQRB_i^T\weightvec - \ba_i)\right\|_2^2 + \frac{1}{2}\left\|\LQRTC^{-1/2} (\weightvec - (\bb + \LQRTC\HJx))\right\|_2^2.
\end{equation}
In Section~\ref{subsec:method_1}, we discuss a specific example of the learning problem~\eqref{eqt:regression_multidata}, which is more readily recognizable as the standard linear regression problem. The minimizer $\weightvec^*$ of the learning problem~\eqref{eqt:regression_multidata} (and $\HJmom^*$ of the Hopf formula~\eqref{eqt:piece_LQR_Hopf}) is given by 
\begin{equation}\label{eqt:multipt_minimizer}
\weightvec^* (=\HJmom^*) = \nabla_{\HJx} S(\HJx,\HJt_1,\dots, \HJt_N) = \Sxx\left(T_N\right)\HJx + \Sx\left(T_N\right).
\end{equation}
\updatethree{For more details, we refer readers to~\cite{bardi1984hopf}. Note that one could also use the Pontryagin maximum principle to compute the gradient of the solution to the HJ PDE, which gives the same result as in~\eqref{eqt:multipt_minimizer}.}


