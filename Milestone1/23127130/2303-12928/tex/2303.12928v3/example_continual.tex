\subsection{Function approximation in continual learning}\label{subsec:example_1}
In this section, we test our Riccati-based approach (as described in Section~\ref{subsec:example_1}) on a pedagogical function approximation example in continual learning \cite{parisi2019continual, kirkpatrick2017overcoming, van2019three} to demonstrate its computational and memory advantages.
Under the continual learning framework, data are accessed in a stream and the trainable model parameters are updated incrementally as new data become available. 
In some cases, the historical data may also become inaccessible after new data are received, which can often lead to catastrophic forgetting \cite{kirkpatrick2017overcoming, parisi2019continual}, which refers to the abrupt degradation in performance of learned models on previous tasks upon training on new tasks. 
In this example, we show how our Riccati-based approach naturally coincides with the continual learning framework, while also inherently avoiding catastrophic forgetting even if the historical data are inaccessible.

Our goal is to regress the function $y(x) = \sin(10 x)$ given noisy data $\{x_i, y_i\approx y(x_i)\}_{i=1}^N$, where the data are corrupted by additive Gaussian noise with relatively large noise scale. Hence, a large amount of data is required in order to obtain an accurate inference. In this example, we follow the continual learning framework and assume that the data are accessed in a stream, so that each new data point must be incorporated into the learned model as soon as it becomes available. Specifically, each new data point comes from a uniformly random sample point $x\in[0, 10]$ and is corrupted by additive Gaussian noise with noise scale one. We regress $y(x)$ using the linear model $y(x) = \sum_{k=1}^n \weight_k \phi_k(x)$, where $n=10$ and 
\begin{equation*}\label{eq:continual:basis}
    \{\phi_k(x)\}_{k=1}^n = \{1, x, x^2, x^3, \sin(\tau), \sin(5x), \sin(8x), \sin(9x), \sin(10x), \sin(12x)\}.
\end{equation*}
We learn the coefficients $\weightvec = [\weight_1, \dots, \weight_n]^T$ by minimizing the following loss function:
\begin{equation}\label{eq:continual:loss}
    \mathcal{L}(\weightvec) = \frac{1}{2}\sum_{i=1}^N \lambda_i \left|\sum_{k=1}^{n} \weight_k\phi_k(x_i) - y_i \right|^2 + \frac{1}{2}\sum_{k=1}^{n} \MLreg_k|\weight_k|^2,
\end{equation}
where $\lambda_i, i=1,...,N$ and $\MLreg_k, k=1,...,n$ are weights for the data loss and $\ell_2$ regularization terms, respectively, and we update our learned coefficients every time a new data point is available. In our numerical experiments, we set $\lambda_i=1, \forall i$ and $\MLreg_k=100, \forall k$.

\begin{figure}[htbp]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.38\textwidth]{continual_learning/fig1_evolution_new.png}
        \includegraphics[width = 0.38\textwidth]{continual_learning/fig1_errs_new.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width = 0.3\textwidth]{continual_learning/fig1_200_new.png}
        \includegraphics[width = 0.3\textwidth]{continual_learning/fig1_1000_new.png}
        \includegraphics[width = 0.3\textwidth]{continual_learning/fig1_50000_new.png}
        \caption{}
    \end{subfigure}
    \caption{Evolution and continual learning of the function approximation learned using our Riccati-based approach as more data becomes available. (a) shows the evolution of the learned coefficients $\weight_k, k=1,...,n$ and the relative $L^2$ error of the inferred $y$ as more data is incorporated into the model; the horizontal dotted lines denote the exact reference values. (b) shows the inferences of $y$ after the 200th, 1,000th, and 50,000th noisy data point becomes available. Our Riccati-based approach allows us to incrementally update the learned coefficients as more data becomes available without accessing the previous data or re-training on the entire dataset, which provides both memory and computational advantages over conventional learning methods.}
    \label{fig:continual}
\end{figure}

\begin{table}[ht]
    \footnotesize
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         & $h=0.001$ & $h=0.0005$ & $h=0.0001$ \\
       \hline
       $\ell_1$ error of $\weightvec^*$ & $2.7814\times 10^{-7}$ & $4.9461\times 10^{-10}$ & $3.0801\times 10^{-12}$ \\
       \hline
       $\ell_1$ relative error of $\weightvec^*$ & $2.7196\times 10^{-7}$ & $4.8361\times 10^{-10}$ & $3.0116\times 10^{-12}$\\
       \hline
    \end{tabular}
\caption{Errors in computing the minimizer $\weightvec^*$ of the function approximation loss~\eqref{eq:continual:loss} using our Riccati-based approach. We use RK4 to solve the Riccati ODEs~\eqref{eqt:sequentialRiccatiODEs} and \eqref{eqt:regression_1Riccati} with double precision and various step sizes $h$. The reference is obtained by using the method of least squares to minimize the loss function~\eqref{eq:continual:loss} directly. }
    \label{tab:continual}
\end{table}

Although this loss function~\eqref{eq:continual:loss} could be minimized using conventional machine learning techniques (e.g., the method of least squares), these methods typically require access to and training on the entire dataset $\{(x_i, y_i)\}_{i=1}^N$, which conflicts with the assumptions in continual learning.
However, note that this loss function~\eqref{eq:continual:loss} is of the form~\eqref{eq:loss_function}. Thus, we can instead apply our Riccati-based approach (as described in Sections~\ref{subsec:method_1} and~\ref{subsec:method_2}) to solve this learning problem. In particular, the methodology described in Section~\ref{subsec:method_2} matches the data streaming paradigm of continual learning; we incrementally update our learned coefficients by considering each new data point as the time evolution of a corresponding multi-time HJ PDE. 
As a result, in contrast to conventional machine learning methods, our Riccati-based approach does not require storage of previous data points, and its memory and computational complexity is constant for each added data point. As such, our Riccati-based methodology may be well-suited to online learning applications.
Additionally, since this time evolution of the HJ PDE (i.e., the addition of a new data point) requires knowledge about the solution to the HJ PDE at the previous time (i.e., the results of the previous training), our Riccati-based approach also inherently avoids catastrophic forgetting.

The results of applying our Riccati-based method to solve this learning problem~\eqref{eq:calibration:loss} are shown in Figure~\ref{fig:continual} and Table~\ref{tab:continual}. 
Figure~\ref{fig:continual}(a) depicts the evolution of the coefficients $\{\weight_k\}_{k=1}^n$ as more data points become available. We observe that the learned coefficients $\weight_k, k=1,...,n$ converge to their true values as more data points are incorporated into the model.
Figure~\ref{fig:continual}(b) displays the inferences obtained when the $200$th, $1,000$th, and $50,000$th noisy data point become available, which demonstrates that our Riccati-based approach is capable of real-time inferences without catastrophic forgetting, even though each inference is made using only one data point. 
As shown, the regression using data with high-level noise becomes more accurate as more data is incorporated into the learning process.

Table~\ref{tab:continual} displays the numerical errors of the minimizer $\weightvec^*$ of the loss function~\eqref{eq:continual:loss} obtained using the Riccati-based methodology from Section~\ref{sec:method} after the last data point becomes available. We observe that the accuracy of our approach increases as we decrease the step size $h$ of RK4, which indicates that the errors of $\weightvec^*$ stem from the accuracy of RK4 in solving the corresponding Riccati ODEs.
The reference solution is obtained by minimizing~\eqref{eq:continual:loss} directly using the method of least squares and assuming that all $N=50,000$ data points are accessible.