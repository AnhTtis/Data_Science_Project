\documentclass[10pt,onecolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}     % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{mmstyle}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[table]{xcolor}
\input{macro}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{10301} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE
\title{Supplementary Materials for Co-Speech Gesture Synthesis using Discrete Gesture Token Learning }

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}




\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\linewidth]{figure/VQCodes_Vis.png}
  \caption{Visualization of exemplar discrete gesture tokens with corresponding code indices. Each token consist of 4 motion frames showing representative gesture poses. With a properly learned codebook, a variety of gesture types are compactly represented as different discrete latent codes. }
  \label{fig:vq_vis}
\end{figure}

\section{Visualization of Discrete Gesture Tokens}
To examine the learned gesture tokens visually, we decode each gesture latent vector in the codebook into a 4-frame gesture segment and render the motions. Figure \ref{fig:vq_vis} shows the visualization results from a subset of gesture codes learned from Trinity dataset with codebook size $N=256$. Note that the residual quantization (RQ) combines multiple codes together to form a more accurate reconstruction of the input gesture data. Thus even with a relatively small codebook size $N=256$, it can produce $O(N^4)$ code combinations to form different gesture segments.  



\section{User Study Interface Details}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\linewidth]{figure/hemvip_interface.png}
  \caption{HEMVIP interface \cite{jonell2021hemvip} used in the user studies.}
  \label{fig:hemvip_interface}
\end{figure}

Figure \ref{fig:hemvip_interface} shows the user interface that we used in the user studies. We used the question ``How humanlike does the gesture motion appear?'' for human-likeness studies, and we used ``How well do the character's movements reflect what the character says?'' for the appropriateness studies.

\section{Additional Experiments}
We have experimented on training RQ-VAE and its corresponding transformers using different codebook size $N$ and temporal reduction factor $s$. We also investigated how varying $k$ in the top-$k$ sampling affects the resulting gestures. The Trinity dataset \cite{IVA:2018} was used for all of the experiments. 

Table \ref{tab:RQVAE} shows the results of using different codebook size $N$ and temporal reduction factor $s$. It tested how varying these parameters could affect the quality of the generated gesture clips. Inuitively, larger $N$ or smaller $s$ should result in lower reconstruction errors since the latent space model would have better capacity to represent the input data. However, in practice we found that it might result in codebook collapse and did not always produce better results. In general, increasing the number of RQ layers tend to reduce the reconstruction errors and the decoded gestures would be closer to the ground truth gestures. Based on these quantitative results, we picked the RQ-VAE model with codebook size $N=256$ and temporal factor $s=4$ for training the subsequent transformer used in the objective evaluations of the main paper. 

Table \ref{tab:RQTransformer} shows how varying the parameter $k$ in top-$k$ sampling affects the quality in the synthesized gesture clips. Instead of greedily picking the most probable tokens, the top-$k$ sampling randomly select a token from the highest $k$ probabilities during inference. From the results, we found that for all conditions, applying top-$k$ sampling usually improved the results. By using $k=50$, the model produced diverse gesture results with better quality than using smaller $k$.


\begin{table}
\begin{tabular}
{ 
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X | }
 \hline
 \multicolumn{5}{|c|}{Experiments for RQ-VAE Reconstruction } \\
 \hline
 Codebook Size $N$ & Temporal Factor $s$ & Num RQ Layer & FGD& L1 Loss\\
 \hline
 128 & 4 & 4 & 0.349 & 0.024 \\
 256 & 4 & 4 & 0.163 & 0.022 \\
 512 & 4 & 4 & 0.360 & 0.023 \\
 1024 & 4 & 4 & 0.239 & 0.022 \\
 256 & 2 & 2 & 0.381 & 0.730 \\
 256 & 2 & 4 & 0.406 & 0.641 \\
 %256 & 4 & 1 & 0.849 & 0.031 \\
 256 & 4 & 2 & 0.515 & 0.027 \\
 %256 & 4 & 3 & 0.277 & 0.023 \\
 \hline
\end{tabular}
 \caption{Experimental results on varying codebook size $N$, temporal factor $s$, and RQ layers when training RQ-VAE. Both FGD and L1 loss were computed based on RQ-VAE reconstruction results.}
 \label{tab:RQVAE}
\end{table}


\begin{table}
\begin{tabular}
{ 
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X
  | >{\centering\arraybackslash}X | }
 \hline
 \multicolumn{7}{|c|}{Experiments for Transformer Inference} \\
 \hline 
 Codebook Size $N$ & Temporal Factor $s$ & RQ Layer & FGD ($k=1$) & FGD ($k=5$) & FGD ($k=10$) & FGD ($k=50$)\\
 \hline
 %256 & 4 & 1 & 1.340 & 1.612 & 1.661 & 1.645 \\
 256 & 4 & 2 & 1.642 & 1.669 & 1.620 & 1.604 \\
 %256 & 4 & 3 & 1.638 & 1.672 & 1.682 & 1.565 \\
 256 & 4 & 4 & 1.920 & 1.743 & 1.664 & 1.523 \\
 256 & 2 & 2 & 1.846 & 1.803 & 1.686 & 1.680 \\
 256 & 2 & 4 & 2.036 & 1.873 & 1.733 & 1.605 \\
 \hline
\end{tabular}
 
 \caption{Experimental results on varying top-$k$ parameters during inference. Here the FGD score in each column is computed using different $k$ in top-$k$ sampling.}
  \label{tab:RQTransformer}
\end{table}



% need to add participant demographic information?
\section{Model Architectures}
We listed the detailed model architectures in Table \ref{tbl:component}, \ref{tbl:vqvae}, and \ref{tbl:transformer}. Among them, Table \ref{tbl:component} describes the common building blocks that will be re-used in both models. 

Table \ref{tbl:vqvae} describes the RQ-VAE architecture for learning the discrete gesture tokens. For RQ-VAE with temporal reduction factor $s=4$, the encoder includes stacks of residual connections and 2 downsampling layers (convolution layer with stride of 2). The decoder has a symmetrical architecture with stacks of residual connections and 2 upsampling layers(transposed convolution layer).

Table \ref{tbl:transformer} describes the RQ-Transformer architecture that learns the conditional prior probabilities of gesture tokens. The model mainly consists of 4 parts, the audio encoder, the text encoder, the spatial transformer, and the depth transformer. The audio encoder is a network with 1D convolutions. Text encoder is based on a temporal convolutional network(TCN) with stacks of dilated convolution layers \cite{BaiTCN2018}. Spatial transformer and depth transformer are both stacks of self-attention layers \cite{vaswani2017attention}.

We have included the model source code as part of the supplementary material. The full source code will also be publicly available later via GitHub.

%Table \ref{tbl:vqvae} elaborates the model architecture we used for gesture reconstruction with 4 RQ residual codebooks and 4x downsampling. For RQ-VAE with temporal reduction factor $s=4$, the encoder includes stacks of residual connections and 2 downsampling layers (convolution layer with stride of 2). The decoder has a symmetrical architecture with stacks of residual connections and 2 upsampling layers(transposed convolution layer).

%Table \ref{tbl:transformer} elaborates the transformer architecture we used for gesture generation. The model mainly consists of 4 parts, the audio encoder, the text encoder, the spatial transformer, and the depth transformer. The audio encoder is a network with 1D convolutions. Text encoder is based on a temporal convolutional network(TCN) with stacks of dilated convolution layers \cite{BaiTCN2018}. Spatial transformer and depth transformer are both stacks of self-attention layers \cite{vaswani2017attention}.

% Need a bit more description for each table.

\begin{center}

\begin{longtable}{lll}
\caption[Block Components]{Block Components} \label{Block Components} \\

\hline \multicolumn{1}{c}{\textbf{Components}} & \multicolumn{1}{c}{\textbf{Architecture}}\\ 
\hline 
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline \multicolumn{1}{c}{\textbf{Components}} &
\multicolumn{1}{c}{\textbf{Architecture}}\\ \hline 
\endhead
            
\hline
\multirow{5}{4em}{ResnetBlock}
& 	(norm1): GroupNorm(32, 64, eps=1e-06) \\
&            	(conv1): Conv1d(64, 64, kernel\textunderscore size=(3,), stride=(1,), padding=(1,)) \\
&            	(norm2): GroupNorm(32, 64, eps=1e-06) \\
&            	(dropout): Dropout(p=0.0) \\
&            	(conv2): Conv1d(64, 64, kernel\textunderscore size=(3,), stride=(1,), padding=(1,)) \\

\hline
\multirow{5}{4em}{ConvAttnBlock}
&            	(norm): GroupNorm(32, 256, eps=1e-06) \\
&            	(q): Conv1d(256, 256, kernel\textunderscore size=(1,), stride=(1,)) \\
&            	(k): Conv1d(256, 256, kernel\textunderscore size=(1,), stride=(1,)) \\
&            	(v): Conv1d(256, 256, kernel\textunderscore size=(1,), stride=(1,)) \\
&            	(proj\textunderscore out): Conv1d(256, 256, kernel\textunderscore size=(1,), stride=(1,)) \\

\hline
\multirow{18}{4em}{TemporalBlock}
&        	(conv1): Conv1d(300,300,kernel\textunderscore size=(2,), stride=(1,), padding=(1,)) \\
&        	(chomp1): Chomp1d() \\
&        	(relu1): ReLU() \\
&        	(dropout1): Dropout(p=0.1, inplace=False) \\
&        	(conv2): Conv1d(300, 300, kernel\textunderscore size=(2,), stride=(1,), padding=(1,)) \\
&        	(chomp2): Chomp1d() \\
&        	(relu2): ReLU() \\
&        	(dropout2): Dropout(p=0.1, inplace=False) \\
&        	(net): Sequential( \\
&          	(0): Conv1d(300, 300, kernel\textunderscore size=(2,), stride=(1,), padding=(1,)) \\
&          	(1): Chomp1d() \\
&          	(2): ReLU() \\
&          	(3): Dropout(p=0.1, inplace=False) \\
&          	(4): Conv1d(300, 300, kernel\textunderscore size=(2,), stride=(1,), padding=(1,)) \\
&          	(5): Chomp1d() \\
&          	(6): ReLU() \\
&          	(7): Dropout(p=0.1, inplace=False) \\
&        	) \\
&        	(relu): ReLU() \\

\hline
\multirow{10}{4em}{SelfAttnBlock}
& (ln1): LayerNorm((256,), eps=1e-05) \\
&        	(ln2): LayerNorm((256,), eps=1e-05) \\
&        	(attn): MultiSelfAttention( \\
&          	(key): Linear(in\textunderscore features=256, out\textunderscore features=256) \\
&          	(query): Linear(in\textunderscore features=256, out\textunderscore features=256) \\
&       	(value): Linear(in\textunderscore features=256, out\textunderscore features=256) \\
&          	(attn\textunderscore drop): Dropout(p=0.1) \\
&          	(resid\textunderscore drop): Dropout(p=0.1) \\
&         	(proj): Linear(in\textunderscore features=256, out\textunderscore features=256) \\
\hline
\hline
\label{tbl:component}
\end{longtable}
\end{center}

\begin{center}
\begin{longtable}{lll}
\caption[RQVAE Architecture]{RQVAE Architecture} \label{RQVAE Architecture} \\

\hline \multicolumn{1}{c}{\textbf{Components}} & \multicolumn{1}{c}{\textbf{Architecture}}\\ 
\hline 
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline \multicolumn{1}{c}{\textbf{Components}} &
\multicolumn{1}{c}{\textbf{Architecture}}\\ \hline 
\endhead
            
\hline
\multirow{16}{4em}{VAE Encoder}
& (conv\textunderscore in): Conv1d(126, 64, kernel\textunderscore size=(3,), stride=(1,), padding=(1,)) \\ 
& (res\textunderscore block0): ResnetBlock() \\
& (res\textunderscore block1): ResnetBlock() \\
& (down\textunderscore sample0): Downsample(
          	Conv1d(64, 64, kernel\textunderscore size=(3,), stride=(2,))
        	) \\
& (res\textunderscore block2): ResnetBlock() \\
& (res\textunderscore block3): ResnetBlock() \\
& (down\textunderscore sample0): Downsample(
          	Conv1d(128, 128, kernel\textunderscore size=(3,), stride=(2,))
        	) \\
& (res\textunderscore block4): ResnetBlock() \\
& (res\textunderscore block5): ResnetBlock() \\

& (conv\textunderscore attn\textunderscore block0): ConvAttnBlock() \\
& (conv\textunderscore attn\textunderscore block1): ConvAttnBlock() \\
& (res\textunderscore block6): ResnetBlock() \\
& (conv\textunderscore attn\textunderscore block2): ConvAttnBlock() \\
& (res\textunderscore block7): ResnetBlock() \\
& (norm\textunderscore out): GroupNorm(32, 256, eps=1e-06) \\
& (conv\textunderscore out): Conv1d(256, 64, kernel\textunderscore size=(3,), stride=(1,), padding=(1,)) \\ 

\hline
\multirow{4}{4em}{Residual Codebook}
& (0): VQEmbedding(257, 64, padding\textunderscore idx=256) \\ 
& (1): VQEmbedding(257, 64, padding\textunderscore idx=256) \\ 
& (2): VQEmbedding(257, 64, padding\textunderscore idx=256) \\
& (3): VQEmbedding(257, 64, padding\textunderscore idx=256) \\    

\hline
\multirow{12}{4em}{VAE Decoder}
& (conv\textunderscore in): Conv1d(64, 256, kernel\textunderscore size=(3,), stride=(1,), padding=(1,)) \\ 
& (res\textunderscore block0): ResnetBlock() \\
& (conv\textunderscore attn\textunderscore block0): ConvAttnBlock() \\
& (res\textunderscore block1): ResnetBlock() \\
& (res\textunderscore block2): ResnetBlock() \\
& (res\textunderscore block3): ResnetBlock() \\
& (res\textunderscore block4): ResnetBlock() \\
& (res\textunderscore block5): ResnetBlock() \\
& (res\textunderscore block6): ResnetBlock() \\
& (res\textunderscore block7): ResnetBlock() \\
& (up\textunderscore sample0): Upsample(
          	Conv1d(128, 128, kernel\textunderscore size=(3,), stride=(1,))
        	) \\
& (res\textunderscore block8): ResnetBlock() \\
& (res\textunderscore block9): ResnetBlock() \\
& (res\textunderscore block10): ResnetBlock() \\
& (conv\textunderscore attn\textunderscore block1): ConvAttnBlock() \\
& (conv\textunderscore attn\textunderscore block2): ConvAttnBlock() \\
& (conv\textunderscore attn\textunderscore block3): ConvAttnBlock() \\
& (up\textunderscore sample0): Upsample(
          	Conv1d(256, 256, kernel\textunderscore size=(3,), stride=(1,))
        	) \\
& (norm\textunderscore out): GroupNorm(32, 64, eps=1e-06) \\
& (conv\textunderscore out): Conv1d(64, 126, kernel\textunderscore size=(3,), stride=(1,), padding=(1,)) \\ 
\hline
\label{tbl:vqvae}
\end{longtable}
\end{center}



\begin{center}
\begin{longtable}{lll}
\caption[Transformer Architecture]{Transformer Architecture} \label{Transformer Architecture} \\

\hline \multicolumn{1}{c}{\textbf{Components}} & \multicolumn{1}{c}{\textbf{Architecture}}\\ 
\hline 
\endfirsthead

\multicolumn{2}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline \multicolumn{1}{c}{\textbf{Components}} &
\multicolumn{1}{c}{\textbf{Architecture}}\\ \hline 
\endhead
            
            
\hline
\multirow{13}{4em}{Audio Encoder}
& (Feat Extractor): Sequential( \\
& (0): Conv1d(1, 16, kernel\textunderscore size=(15,), stride=(4,), padding=(1600,)) \\
& (1): BatchNorm1d(16, eps=1e-05, momentum=0.1) \\
& (2): LeakyReLU(negative\textunderscore slope=0.3, inplace=True) \\
& (3): Conv1d(16, 32, kernel\textunderscore size=(15,), stride=(5,)) \\
& (4): BatchNorm1d(32, eps=1e-05, momentum=0.1) \\
& (5): LeakyReLU(negative\textunderscore slope=0.3, inplace=True) \\
& (6): Conv1d(32, 64, kernel\textunderscore size=(15,), stride=(6,), padding=(2,)) \\
& (7): BatchNorm1d(64, eps=1e-05, momentum=0.1) \\
& (8): LeakyReLU(negative\textunderscore slope=0.3, inplace=True) \\
& (9): Conv1d(64, 32, kernel\textunderscore size=(15,), stride=(7,), padding=(2,)) \\
& ) \\
\hline

            \multirow{12}{4em}{Text Encoder}
& (Embedding): Embedding(4040, 300) \\
& (TCN): Sequential( \\
&      	(0): TemporalBlock() \\
&        (1): TemporalBlock() \\
&        (2): TemporalBlock() \\
&        (3): TemporalBlock() \\
&        (4): TemporalBlock() \\
&        (5): TemporalBlock() \\
&        (6): TemporalBlock() \\
&        (7): TemporalBlock() \\
& (decoder): Linear(in\textunderscore features=300, out\textunderscore features=32, bias=True) \\
&  	(drop): Dropout(p=0.1, inplace=False) \\
&	) \\
\hline
 \multirow{10}{4em}{Spacial Transformer}
& (blocks): SelfAttnBlock( \\
& (0): SelfAttnBlock() \\
& (1): SelfAttnBlock() \\
& (2): SelfAttnBlock() \\
& (3): SelfAttnBlock() \\
& (4): SelfAttnBlock() \\
& (5): SelfAttnBlock() \\
& (6): SelfAttnBlock() \\
& (7): SelfAttnBlock() \\
&      	) \\

\hline
 \multirow{6}{4em}{Depth Transformer}
& (blocks): SelfAttnBlock( \\
& (0): SelfAttnBlock() \\
& (1): SelfAttnBlock() \\
& (2): SelfAttnBlock() \\
& (3): SelfAttnBlock() \\
&      	) \\

\hline
 \multirow{6}{4em}{Classifier}
& (classifier) Sequential( \\
&    	(layer\textunderscore norm): LayerNorm((256,), eps=1e-05) \\
&    	(linear): Linear(in\textunderscore features=256, out\textunderscore features=256, bias=True) \\
&    	(logit\textunderscore mask): LogitMask() \\
&  	) \\

\hline
\label{tbl:transformer}
\end{longtable}
\end{center}




{\small
\bibliographystyle{ieee_fullname}
\bibliography{shortstrings,bibliography}
}


\end{document}