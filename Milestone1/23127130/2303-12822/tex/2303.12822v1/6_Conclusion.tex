\section{Conclusion}
% need to re-write conclusion and discussion 
In this work, we proposed to use discrete gesture tokens for learning gesture synthesis model. The method utilizes RQ-VAE for learning gesture units and a two-level autoregressive transformer for learning conditional latent code priors. By using discrete tokens as gesture representations, it reduces the problem of gesture synthesis into learning the probability distributions for categorical token indices. Such setup naturally addresses the one-to-many mapping issue in gesture synthesis by assigning different probabilities to multiple gesture tokens even if they are corresponding to the same speech. Since the method generates gestures by selecting the next likely tokens based on the learned probability distributions, it is able to produce varying gesture motions given the same speech context. Both the objective and subjective evaluation results show that the method is able to generate diverse gesture motions with adequate human-likeness.  

\textbf{Limitations and Future Works}
While our method performed relatively well in the user evaluation studies, some limitations need to be considered and addressed in future works. First, vector quantization approximates the original data and might produce lower-quality motions with less variety. Although we partially addressed this issue by using residual quantization to reduce the reconstruction errors, information would still be lost during quantization. Second, since each token contains multiple frames of gesture motions, it may limit the temporal resolution of resulting motions since gestures could only change after predicting the next token. Thus the synthesized gestures might be less aligned with input speech compared to methods that synthesize the motions frame-by-frame. Finally, while residual quantization produces better reconstruction results, it requires more burdens on learning the autoregressive priors and thus may need more powerful networks to handle the complicated relationships between many tokens when $D$ is large. %since more gesture tokens result in more complicated dependencies between different tokens.  
%While our method performed relatively well in the user evaluation studies, some limitations need to be considered and addressed in future works. First, vector quantization approximates the original data and might produce lower-quality motions with less variety. Although we addressed this issue to some degree using residual quantization, which reduces reconstruction errors by adding residual layers, the latent codebook still might not have enough expressiveness for representing the full gesture motions from the dataset. Further improving the discrete latent space representation for more faithful reconstructions would be important future works. Second, from the discrete latent space, each token would contain multiple frames of gesture motions. Such representation may limit the temporal resolution of resulting motions since gestures could only change after predicting the next token. Thus the synthesized gestures might be less aligned with input speech compared to methods that synthesize the motions frame-by-frame. Finally, while residual quantization produces better reconstruction results, it also puts more burdens on learning the autoregressive priors since more gesture tokens mean longer training and more complicated dependencies between different tokens. The two-level transformer architecture addressed the efficiency issues but does not necessarily make the learning simpler. This phenomenon could be seen in our experiments on RQ-VAE with varying residual code depth. While RQ-VAE produces substantially better reconstruction results than VQ-VAE, the better discrete representations are not fully reflected in gesture synthesis results and we only observe minor improvements in both FGD and diversity. Moreover, from the subjective evaluations, while the difference between the natural human motions and our synthesized results was small, the gap is much bigger in terms of appropriateness to speech context. More advanced models that can handle these longer dependencies or better utilize semantic input would be useful future improvements. 

%One key issue is that the VQ-VAE training is not as stable as we originally expected due to codebook collapse. Thus we would hope to develop a more robust process for training the VQ-VAE, especially for different datasets. During our initial development, we used the Trinity dataset \cite{ferstl2018investigating} to allow faster iterations in model training and parameter tuning. However, when we switched to the TWH dataset and applied the same hyper-parameters ($|V|=64$ and $s=4$) that were working well on the Trinity dataset, we found that the results were worse than expected and we had to re-do the parameter tuning to build an acceptable latent cookbook. Since the codebook collapse issue is still not fully solved for VQ-VAE, this might explain why the results are not as stable across different datasets with a varying number of joints and motion quality. We also hope to investigate newer techniques \cite{yu2022vectorquantized} that mitigate the codebook collapse issue to make the training less vulnerable to hyper-parameter changes. 

%We would also hope to improve the autoregressive model to address the sequential nature of gesture motions. Due to the timeline for the challenge, we simplified the transformer implementation and adapted the typical architecture from image synthesis for our method. Thus it only works at a fixed-size sliding window without considering longer gesture sequences. This not only makes the system less flexible but also introduces potential artifacts between adjacent windows. A specialized model for handling time-series data of arbitrary length should work better for gesture synthesis. 
%Finally, since our method is inspired by retrieval-based methods, we would hope to perform further study and analysis between different systems. In particular, it would be interesting to learn the limit of our method 

%Finally, in our submission, we were not able to model the root translations and for simplicity, we fixed the root joint positions for all of our results. Thus the lower body movements from our submission are less natural compared with the other top results. Implementing the techniques of full-body synthesis such as the one proposed in \cite{Alexanderson2020} to handle the root translations will enhance our method to generate long sequences of full-body gestures with realistic lower body motions.

%1. better VQ-VAE training with better codebook utilization
%2. adjust autoregressive model
%\section*{Acknowledgments}
%The projects or efforts depicted were or are sponsored by the U. S. Army. The content or information presented does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.