\section{Methods}
%In this section, we will briefly introduce the theory of VQ-VAE and its variant, RQ-VAE and how they are applied to gesture synthesis.
%Then, we introduce our pose-transformer, which consists of text, audio feature encoder and a RQ-transformer.
In this section, we briefly describe Vector Quantization and RQ-VAEs and how we applied them to the gesture synthesis problem. Then, we introduce our gesture synthesis system which consists of a text encoder, an audio encoder, and a motion generator based on transformers. 

% \paragraph{Data Pre-Processing}
In our training data, each speech-gesture sequence consists of speech audio, transcribed speech text, and gesture motion. Motion data is represented as a sequence of upper-body poses. For each speech-gesture sequence, we first down-sampled the motion to 20 fps and applied a sliding window of 64 frames with 10 frames step size to produce training gesture samples. Each gesture sample is represented as a tensor of size $F \times J \times R$, where $F=64$ is the sliding window size, $J$ is the number of upper-body joints (e.g.\, head, shoulder, wrist), and $R$ is the size for joint rotation representation. We also use $R=6$ as the representation for joint rotations based on previous research \cite{Zhou2019} to prevent singularities and reduce rotation approximation errors. To maintain the continuity of output gestures during synthesis, we include a 10-frame overlap between each clip for consecutive syntheses.

 %The input data is a gesture clip of 64 frames obtained using a sliding window to segment the speech and gesture into sample clips from a full speech and gesture sequence. To maintain the continuity of output gestures, we include a 10-frame overlap between each clip for consecutive syntheses.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\linewidth]{figure/Overview.drawio_V2.pdf}
  \caption{Overview of the two-stage model architecture for learning gesture synthesis using discrete gesture tokens. The stage 1 model learns discrete tokens via residual quantization to reconstruct input gesture motions. The stage 2 model uses the transformers to learn the autoregressive priors using speech text and audio as conditions. It first infers the spatial context $h_t$ at time step $t$ using a temporal transformer. The depth transformer then autoregressively predicts the depth token $S_{t+1, d}$ using spatial context and previously predicted depth tokens $(S_{t+1,1} \dots S_{t+1,d-1})$ at time step $t+1$. The predicted gesture tokens are then used to reconstruct the gesture motions via RQ-VAE decoder. }
  \label{fig:overview}
\end{figure*}

\subsection{Overview}
% The proposed method is motivated by the recent works in cross-modal text-to-image synthesis \cite{dalle2021, esser2021} that utilize VQ-VAE as latent space representation for image patches and generate new images via autoregressive models to predict discrete tokens for each patch. As a high-level analogy for gesture generations, this could be seen as extracting a smaller set of gesture units from the training motions and learning the conditional probability distribution for these gesture units based on speech context and previous gestures.

% One motivation for learning the gesture units as discrete latent vectors is because the generation process can be seen as sampling from the codebook instead of interpolation within a continuous latent space and thus we hypothesize that the resulting gestures are more likely to retain their motion quality from original data. Moreover, learning the probability distribution in the discrete codebook will naturally handle the issue when different gesture motions are associated with the same speech in the training data since during inference the model can randomly pick one of these gesture units instead of outputting their average. 

 Our method is summarized in Figure \ref{fig:overview}. Our method mainly consists of two stages that learn the gesture representations and the conditional probability distributions respectively. The first stage involves training the RQ-VAE model to learn discrete feature representation. In the second stage, we freeze the weight of RQ-VAE to treat it as a gesture encoder and use a transformer to learn the probability distribution over the discrete latent space with corresponding conditional speech features.



% \subsection{Vector Quantization and VAE on Gestures}
% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figure/VQVAE.drawio.pdf}
%   \caption{VQ-VAE architecture}
% \end{figure*}
%Inspired by Trimodal \cite{Yoon2020Speech}, 
\subsection{Gesture Token Learning}
Discrete latent space model is adopted to extract small gesture segments as tokens from raw gesture motions (Stage 1 in Figure \ref{fig:overview}). We assume both the input and output include only gesture samples of size $F \times P$, where $F$ is the number of frames per sample and $P = J \times R$ is the pose feature size. 
The input gesture motion $x \in \mathbb{R}^{F \times P}$ is first encoded into a lower-dimensional tensor $Z_e(x) = [z_1, z_2,\ldots, z_T] $ with size $ T \times p$, where $T < F$, and $T$ indicates the number of latent vectors $z_t$ after the encoding layers. In our implementation, we use a varying number of pooling layers in the encoder so $T=F/s$ and $s$ is the temporal reduction factor. 
Then each $p$-dimensional latent vector from $Z_e(x)$ is quantized to the nearest embedding in a learnable codebook $V = \{e_{1} ,e_2,\ldots,e_{|V|}\}$, with embedding dimension $p$ and codebook size $|V|$. During the quantization stage, each feature vector from the encoder output $Z_e(x)$ is replaced by the index of the nearest vector $e_{k}$ in the codebook. %The gradient of the decoder is passed  through the encoder using straight-through gradient estimator.
The quantization step $\Psi$ can be summarized as:
\begin{equation}
\Psi( Z_e(x)) \ =\ e_{k} \ 
\text{where}\ k=\underset{j}{argmin} ||Z_e(x) -e_{j} ||
\end{equation}
During the reconstruction stage, the decoder takes the quantized latent vector $e$ and maps it back to the reconstructed gesture $\hat{x}$ in the original dimension. Besides the $L1$ reconstruction loss, it also includes two additional loss terms. The codebook loss helps the codebook variable training and the commitment loss is for updating encoder weights stably. The objective function for training the quantization model is then defined as the following:
\begin{equation}
L( x,\  \hat{x}) =||x-\hat{x} ||+||sg[ Z_e(x)] -e||_{2}^{2} +\beta ||sg[e] -Z_e(x) ||_{2}^{2}
\end{equation}
The operator $sg$ refers to the stop-gradient operator and $\beta$ is a hyperparameter that controls the weight of commitment loss.
The quantized result can be represented by a sequence of  the vector indices $k$ in the codebook. The decoder maps and upsamples the quantized vectors back to reconstruct the original input.
However, one big issue for training the discrete latent space model is the codebook collapse. This problem happens when only a small subset of codes are utilized in the codebook during training and will result in a latent space with less representational power. In our experiments, we found that increasing the codebook size could lead to worse reconstruction performance because of a lower code usage rate and unstable training. 

%\subsection{Residual Quantization}
To improve code utilization, residual quantization (RQ) \cite{lee2022autoregressive} is used to represent a latent vector with multiple residual codes. It makes better usage of the codebook by recursively approximating the latent vectors with multiple codes. Specifically, for a quantization of total depth $D$, RQ recursively computes the residual code $S_{t,d}$ for the $t$-th latent vector at depth $d$ as
\begin{align}
S_{t,d}&=\Psi(r_{t,d-1}) \\
r_{t,d}&=r_{t,d-1} - \Psi( r_{t,d-1})
\end{align}
, where the 0-th residual vector is defined as the original latent vector $r_{t,0}=z_t$. $S$ is the 2D array of latent codes with size $T$ and depth $D$.
The sum of all RQ layers is denoted as $\hat{z}_t = \sum_{d=1}^D{S_{t,d}}$ which produces the final quantized vector. 
By applying the recursive quantization process, reconstruction error decreases with the increase of RQ depth. And by using a shared codebook at all depth, we are able to achieve better feature extraction under the same codebook size without increasing the number of parameters of the model, and also avoid overfitting. In our experiment, both the RQ reconstruction and inference achieve better FGD score (explained in Section \ref{sec:objective}) and lower L1 error as the number of residual layers increases. %The training of RQ is similar to a single layer vector quantization model except for changing the commitment loss to be layer-wise. %We did not use adversarial training as their authors point out since we have already achieved good enough reconstruction results.

To further prevent codebook collapse, we also utilized several strategies proposed in previous work for learning the codebook. First, we apply exponential moving averages for the codebook learning, which places a greater weight update on the most recent codebook vectors \cite{van2017neural}. In addition, we reset the codes that are not used to random values to allow them a better chance to be utilized in the next iterations, as proposed in the Jukebox paper \cite{dhariwal2020jukebox}.
We also found that dividing the variance of the dataset when calculating MSE reconstruction loss does have a small improvement for the training \cite{yan2021videogpt}.
% mention training configuration

% \subsection{Transformer-based Pose Generator}
% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=.95\linewidth]{figure/RQ_Transformer.drawio.pdf}
%   \caption{Transformer architecture}
% \end{figure*}
\subsection{Learning Conditional Priors of Gesture Token}
In the gesture synthesis stage (the second stage in Figure \ref{fig:overview}), we aim to predict the next gesture token based on previous tokens and condition vectors. The input conditions for our auto-regressive model are raw speech audio and text. We used a text encoder and an audio encoder for getting conditional feature vectors. For the speech audio features, the raw audio waveform goes through one-dimensional (1D) convolutional layers to generate a sequence of audio feature vectors. For text features, we first padded the speech text with padding tokens to make them the same length as gesture frames. 
We then used FastText~\cite{bojanowski2016enriching}, a pretrained word embedding, for speech text embedding; their weights were updated during training. And we used a temporal convolutional network (TCN)~\cite{BaiTCN2018} to process the word embeddings through a series of causal and dilated convolutions, which are shown to have advantages over traditional recurrent neural networks.
After encoding data from each modality through separate encoders, we concatenate both outputs into a single condition feature tensor $f_c$ with size $F \times (|C_{1}|+|C_{2}|)$, where $C_{1}$ and $C_{2}$ are audio and text feature vectors.

We take the feature vectors for text and audio as the condition on the transformer and do left-to-right prediction of tokens, similar to other language modeling tasks on the discrete latent codes based on transformer architecture adapted from. Specifically, we model the discrete latent variables on the residual layers as 
\begin{equation}
p( s) =\prod _{t=1}^{T}\prod _{d=1}^{D} p( S_{t,d} |S_{< t,d} S_{t,< d}, f_c)
\end{equation}
As the equation shows, we could autoregressively predict the stack of residual codes using a single transformer by expanding time and depth dimensions. To improve efficiency, we utilize the two-level architecture similar to \cite{lee2022autoregressive} by first using a Temporal Transformer (stacks of self-attention) to get context features of the tokens from previous time steps and condition vectors.
\begin{gather}
u_{t} =\textrm{PE}_T( t) +\sum _{d=1}^{D} e( S_{t-1,d} ) \ for\ t >1\\
h_{t} =\textrm{TemporalTransformer}( u_{< t}, f_c)
\end{gather}
, where PE stands for positional encodings. With the context vector on the corresponding time step, we utilize another transformer to predict code on each residual layer based on the given context vectors.
\begin{gather}
v_{t,d} =\textrm{PE}_D(d) +\sum _{d'=1}^{d-1} e( S_{t,d'} ) \ \ for\ d >1\\
v_{t,1} =\textrm{PE}_{D}(1) +h_{t} \\
p_{t,d} =\textrm{DepthTransformer}( v_{t,1}, \dots v_{t,d})
\end{gather}
Here the autoregressive steps are applied at each position $t = 1, \dotsc, T$ for the temporal transformer to get context from the previous time steps. Then the temporal context are used to predict residual layers $d = 1, \dotsc, D$ with the depth transformer and a softmax function with output probability $p_{td}$ for residual codes. In our experiment, we set temporal reduction factor $s=4$ and depth $D=4$ to allow each gesture token to represent about $0.2$ second of gesture motion. The loss is calculated using negative log-likelihood at each position of the residual layers.

%$X$ is the concatenated condition features $(f, D)$ with previous gesture tokens in the form of VQ-VAE codebook indices
\subsection{Inference}
While the model used a fixed input speech length ($F=64$ frames; 3.2 seconds), we can synthesize gesture motion for a longer speech than the fixed length by predicting $F$ frames at a time and merging them into the final gesture sequence. Each sliding window has a 10 frames overlap with the previous window and the poses are merged in the overlapped area to maintain motion continuity. Specifically, for each new synthesized gesture segment from a sliding window, its poses from the first 10 frames are linearly interpolated with the last 10 frames of the previous gesture segment before concatenation. The gesture tokens within each sliding window are predicted in a similar autoregressive manner by inferring the probability of the next token as $p_{t+1,d}$ for next time step $t+1$ and depth $d$. To allow more variety in the resulting gestures, instead of selecting the latent code with the highest probability from the codebook, we randomly sample it using the top-k probabilities where $k=10$. 
% mention training configuration

% \begin{table*}
% \centering
%   \begin{tabular}{lcc|ccc}
%     \toprule
%      &
%       \multicolumn{2}{c}{VAE Reconstruction} &
%       \multicolumn{3}{c}{Transformer Inference }  \\
%                     & FGD $\downarrow$ & L1 $\downarrow$ & FGD $\downarrow$ & Diversity $\uparrow$ & Beat-Consistency $\uparrow$
%                     \\
%       \midrule
%     VQ         & 0.85 & 0.031 & 1.66 & 1.61 & 0.733 \\
%     RQ(layer 2)  & 0.51 & 0.027 & 1.60 & 1.61 & 0.739 \\
%     RQ(layer 3)  & 0.28 & 0.023 & 1.63 & 1.60  & \textbf{0.751} \\
%     RQ(layer 4)  & \textbf{0.16} & \textbf{0.022} & \textbf{1.55} & \textbf{1.70}  & 0.744 \\
%     \bottomrule
%   \end{tabular}  
%   \caption{Caption for results. }
%   \label{tab:ablation}
% \end{table*}
