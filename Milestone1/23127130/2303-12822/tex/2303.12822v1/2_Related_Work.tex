\section{Related Work}
% To-do : need to rewrite the related work text since it's currently copied from GENEA paper

%We first review traditional gesture generation methods and later introduce some work related to VQ-VAE. 
\subsection{Co-Speech Gesture Generation}
The early works in co-speech gesture synthesis utilized pre-defined sets of manually created gesture units to build a gesture database. The new gestures were generated via keyword matching or prosody analysis to find the best corresponding gesture units from the database \cite{kopp2003max, kopp2009, marsella2013}. The gesture unit database can also be created automatically from speech-gesture data by segmenting and clustering gesture motions based on the similarity of motions and speech contents. During synthesis, the desired gesture property and speech attributes are used to search for the best gesture segment in the database that matches the input speech content. Our method is motivated by these ideas from early works, but instead of manually building a gesture database, we used residual quantization to implicitly learn the discrete codebook of gesture tokens. 

Recent learning-based methods train an end-to-end model from speech-gesture datasets to predict gesture motions from speech. The methods based on direct regressions find a deterministic mapping from speech to gestures \cite{kucherenko2020gesticulator, ginosar2019gestures, Yoon2020Speech, bhattacharya2021speech2affectivegestures}. Since these methods do not handle the issue of one-to-many mapping, the adversarial scheme is sometimes utilized by training the model with an additional discriminator to improve the resulting motion qualities. Recent methods further improve synthesized results by using hierarchical architecture to model multi-level skeletal poses \cite{liu2022learning} or adding semantic prompter to force semantic alignment in output gestures \cite{liang2022seeg}. 

Probabilistic frameworks were also used in the recent gesture generation works to handle the gesture ambiguities \cite{ahuja2019language2pose, Alexanderson2020, Qian2021, Li2021, bhattacharya2021text2gestures}. This type of methods learns a latent space generative model and could generate multiple gesture motions from the same speech input using conditional sampling from the latent space during inference. Similar to the previous methods, our method utilizes a latent space model and conditional sampling to handle the one-to-many problems for gesture synthesis. However, instead of learning a continuous latent space, we applied residual quantization to learn \textit{discrete} latent codes. The discrete latent codebook provides a compact representation for gesture units and the inference process is reduced to selecting the most probable latent code from the codebook. Thus by learning the conditional prior distribution over the discrete latent codes, the method naturally handles the mapping from one speech input to multiple different gestures with varying probabilities.  

\subsection{Discrete Latent Space Learning}

Vector-quantized variational autoencoder (VQ-VAE) \cite{van2017neural} learns discrete representations as a codebook from images. In the two-staged generative architecture like in Video GPT \cite{yan2021videogpt}, an autoregressive prior can then be trained to model the categorical distributions for these discrete latent codes. It was first introduced for image synthesizing or compression tasks and is able to produce sharper and higher quality image synthesis results. It was further improved in \cite{razavi2019generating} using a multi-scale hierarchical architecture to model higher-resolution images. 

%One issue that often affects the reconstruction quality when training VQ-VAE is codebook collapse. It happens when the model only learns to use a small subset of the codes in the codebook, leaving a majority of the codes unused. This limits the expressiveness of the model and results in lower-quality results. Several methods and techniques have been proposed to prevent codebook collapse. Jukebox \cite{dhariwal2020jukebox} introduced re-initializing unused codes to a random vector to prevent dead codes during each training iteration. Video GPT \cite{yan2021videogpt} finds normalizing MSE for the reconstruction loss also mitigates codebook collapse. Hierarchical models were proposed for better codebook utilization in VQ-VAE2 \cite{razavi2019generating} by first extracting bottom and top features unconditionally to mitigate the codebook collapse. RQ-VAE~\cite{lee2022autoregressive} uses a fixed size of codebook to recursively quantize the feature map represented as a stacked map of discrete codes, which reduces the codebook size and stabilizes the codebook training. 

One issue that often affects the reconstruction quality when training VQ-VAE is codebook collapse, which leaves a majority of the codes unused and limits the expressiveness of the model. Several methods and techniques have been proposed to prevent codebook collapse. Jukebox \cite{dhariwal2020jukebox} introduced re-initializing unused codes to a random vector to prevent dead codes during each training iteration. Video GPT \cite{yan2021videogpt} finds normalizing MSE for the reconstruction loss also mitigates codebook collapse. Hierarchical models were proposed for better codebook utilization in VQ-VAE2 \cite{razavi2019generating} by first extracting bottom and top features unconditionally to mitigate the codebook collapse. RQ-VAE~\cite{lee2022autoregressive} uses a fixed size of codebook to recursively quantize the feature map represented as a stacked map of discrete codes, which reduces the codebook size and stabilizes the codebook training. 

The discrete latent space has also been applied for text-to-image synthesis, which generates new images based on input textual description using a two-stage architecture. It first learns a discrete representation for image patches and modeling the auto-regressive priors using transformers. Learning with discrete codes is more efficient over raw pixels since the transformer may not learn the fully dependencies between pixels. The work by Esser \etal \cite{esser2021} further applied adversarial training to learn VQ-GAN that produces a perceptually rich codebook. 

In addition to image synthesis, discrete latent space model is also known to be one of the state-to-the-art methods for modeling time-series data such as audio. Jukebox \cite{dhariwal2020jukebox} utilized VQ-VAE to generate singing music. It trained multi-level networks to compress audio in different resolutions into discrete space and then used autoregressive transformers to learn the latent codes for music generation. The same idea was also adapted to generate repetitive rhythms of music by learning from extracted music loops. Multi-Instrumentalist Net \cite{su2020multi} was proposed to generate multi-instrumental music from videos, which trained VQ-VAE along with an autoregressive prior conditioned on the musicianâ€™s body key points movements. 

%Our method is motivated by the recent success of applying a discrete latent space model in cross-modal synthesis tasks. 
Compared to previous works, the proposed co-speech gesture synthesis methods utilize RQ-VAE for modeling discrete gesture tokens and a two-level RQ-Transformer architecture to model conditional priors for gesture-generating tasks. The evaluation results show its potential for retaining motion quality while allowing non-deterministic motion synthesis from the same speech input.


% In our work, we do suffers from the codebook collapse problem in the beginning, output gestures are limited. We then applied used random restart of the codebook, exponential moving average updates for the codebook and adjusting MSE weight, and the results are much better. We may consider hierarchical architecture for VQ-VAE in the future as well.

%\subsection{Multi-modal Text-to-Image Synthesis}


