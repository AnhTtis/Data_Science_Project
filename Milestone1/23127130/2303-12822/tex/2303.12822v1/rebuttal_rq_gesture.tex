\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}
%\newcommand{\RoyalBlue}[1]{\textcolor{RoyalBlue}{#1}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{10301} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Rebuttal for CVPR Submission 10301}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

We thank the reviewers for their valuable comments. We address each concern of the reviewers in the following.

\noindent \textbf{(Reviewer IDs: R1=pjYM, R2=8HCA, R3=nQM2)}

\noindent \textbf{Previous motion synthesis works using VQ-VAE (R1, R2, R3).} 
We appreciate the comments about the missing related works that apply discrete tokens for motion generations. 
We will add the references in the paper revision. We also want to highlight that our method uses residual quantization (RQ-VAE), which combines multiple discrete codes to form more accurate gesture reconstructions than a single code from VQ-VAE. The experiments show our method produces better results than vanilla VQ-VAE.

\noindent \textbf{Benefits of discrete codes compared with other baselines such as Normalizing Flows (Moglow), Random, and NN (R1, R2, R3).} %Using discrete codes may provide direct control over token selections during synthesis. For example, we could explicitly exclude certain tokens from being selected and thus adjust overall gesture styles. W
The main benefit is to turn the gesture synthesis into a classification problem, which may allow direct control over token selections during synthesis. For example, we could explicitly exclude certain tokens from being selected and thus adjust overall gesture styles. Such a mechanism would be more difficult in the continuos latent space of Moglow. Table \ref{tab:new_baseline} shows the experiment results where our method produces better FGD and Diversity than Moglow. When compared with simple baselines like Random and NN, our method also performs better in FGD and Beat.

\noindent \textbf{No hand poses (R2).} We omitted hand poses since the datasets do not have high-quality hand motions, but our method can be easily extended to handle this by learning additional hand gesture tokens and a separate transformer. %Due to the complexity of hand motions , we also believe it would be more efficient to learn different sets of conditional priors for body and hand poses separately. This allows the hand poses to depend on both speech context and current arm poses for better generalizability.

\noindent \textbf{Analysis of gestures that relate to the content of the speech such as metaphorical gestures (R2).} This is one of the general limitations for learning-based methods to perform such analysis as the existing gesture datasets do not provide such annotations for gesture semantics. While we are not able to explicitly perform such analysis quantitatively, the user study shows that our method produces a higher gesture appropriateness score than other baselines but not as well as the ground truth motions.

%\noindent \textbf{Additional baseline comparisons with Random and NN (R2).} 
%Thank you for the suggestions. 
%We have implemented Random and NN baselines according to the reviewer's suggestion and evaluated their performance. Table \ref{tab:new_baseline} shows both baselines perform far worse in FGD as they can only select one motion segment at a time while other methods produce gestures on a per-frame basis.

\noindent \textbf{Implementation details for the baselines--Trimodal and Speech2Gesture (R2).} We re-implemented both baselines based on the original codebase. We’ve only changed the input dimensions to make them compatible with skeletons with an increased number of 3D joints used in the present paper. We will clarify this in the paper and code. Although the dataset and input dimension were changed, we didn’t perform a hyperparameter tuning for the baselines (used the same ones in the original implementation), so it might not reproduce the full quality of the baselines. 
%We also note that both baselines are GAN-based models that are known to be hyperparameter sensitive and particularly difficult to reproduce.

\noindent \textbf{Comparison with Trimodal for style control in synthesized gestures (R3).} 
%In our implementations, 
We did not handle the speaker style control in the paper, but we expect this could be achieved by either using the style embedding vector as part of the conditional input or by explicitly adjusting the priors for some gesture tokens to follow certain styles. We also hope to point out that Trimodal is a deterministic method and thus does not handle the one-to-many mapping issue. Our method also performed better than Trimodal in both objective and subjective studies. %based on desired styles.  For example, it could lower the probabilities for some gesture tokens so the resulting gestures would follow a certain gesturing styles.

\noindent \textbf{Number of network parameters for different depths $D$ (R3).} Compared to VQ model ($D=1$), RQ ($D>1$)
has an additional 3.1M trainable parameters for depth transformers (the total number of parameters is 10M). The number of parameters does not change when varying depth $D$ in RQ.

\noindent \textbf{Parameter selection for codebook size $N$ and temporal reduction factor $s$ (R3).} We selected the best ones by performing a parameter search. Table 1 on page 3 of the supplemental document provides experimental results with these parameters and shows how changing these parameters affects gesture synthesis performance.

\noindent \textbf{FastText vs BERT/CLIP (R3).} 
Sentence embeddings like CLIP may lose temporal information of words in speech, and 
%in our generation setting, which divides a long speech into smaller chunks, 
the text encoder often cannot receive a complete sentence due to a chunk-divided synthesis. Thus, we chose word embeddings instead of sentence embeddings. 
%Another motivation for using FastText was its multilingual support for 157 languages (although we only applied it to English datasets in this paper), and FastText has been successfully used in previous Trimodal papers. 
%We agree that the model could be improved with more recent word embeddings such as BERT.
Using more recent word embeddings such as BERT would be an interesting modification.
%, and analyzing the effect of different word embeddings would be interesting future work.

%\noindent \textbf{Q11: Deterministic regression vs NLL loss (R1)?} 
%Since we explicitly model the gesture synthesis as token prediction, we applied top-k sampling by randomly choosing the next gesture token based on the prior probability predicted by the transformer. Therefore instead of deterministically selecting the most probable token as in a classification problem, our method would produce different token sequences each time from the same input.
\noindent \textbf{Deterministic regression for conflicting samples (R1).}
If a regressor learns from different gestures for the same speech, it would tend to converge to the mean gesture, which is the best choice with L1 or L2 losses. We mitigated this problem by explicitly modeling the gesture synthesis as classification (i.e. token prediction) and using NLL loss.

\noindent \textbf{The codebook is only learned with the gesture features (R1).} We followed the paradigm of a two-stage model similar to frameworks used in image synthesis. The first stage focuses on learning a compact discrete representation and the speech is only used in the second stage to model the conditional prior of tokens. %for the gesture motions, which is independent of the input audio and speech text. The audio and text features are only utilized in the second stage for modeling the conditional prior and predicting the toke sequences. %Modulating the codebook decoding with speech condition would be an interesting future direction. 

%\noindent \textbf{Q13: The readability of the paper can be improved. Better explanation for the depth and spatial parts of the transformer (R2).} Thank you for the comments and we will add more explanations for the two-level transformers architecture to improve the readability in the revision. %The spatial transformer is similar to VQ-Transformer that mainly predicts the temporal token sequences while the depth transformer is added to predict the RQ tokens at each time step for refining the results.

\vspace{0.1cm}

We will revise the paper to address the above concerns. The readability issue for the description of the depth and spatial part of the transformer will be also addressed. 
We would kindly ask you to consider adjusting your review score, taking the rebuttal into account. 
Thank you again for taking the time to review the paper.


\vspace*{-\baselineskip}
\begin{table}[t]
\centering
%\small{
  \begin{tabular}{@{}l|c@{\hskip5pt}c@{\hskip5pt}c@{}}
    \toprule
      Methods & FGD $\downarrow$ & Diversity $\uparrow$ & Beat $\uparrow$
                    \\
      \midrule
    Random   &  4.05 & 1.64 & 0.715 \\
    NN (Audio) &  6.02 & 1.58 & 0.713 \\
    NN (Text) &  3.85 & \textbf{1.70} & 0.709 \\
    Normalizing Flows  &  9.46 & 1.25  & \textbf{0.791} \\  
    RQ (Ours)  &  \textbf{1.52} & 1.68  & 0.745 \\    
    \bottomrule
  \end{tabular}  
%}
  \caption{Comparisons with Random, NN, and Normalizing Flows.}
  \label{tab:new_baseline}
\end{table}



%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{shortstrings,bibliography}
% }

\end{document}
