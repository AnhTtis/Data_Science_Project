%Speech2Gesture
@InProceedings{ginosar2019gestures,
  author={S. Ginosar and A. Bar and G. Kohavi and C. Chan and A. Owens and J. Malik},
  title = {Learning Individual Styles of Conversational Gesture},
  booktitle = cvpr,
  publisher = {IEEE},
  year={2019},
  month=jun
}

% FID
@inproceedings{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle=nips,
  year={2017}
}

% hierarchical gesture
@inproceedings{liu2022learning,
  title={Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation},
  author={Liu, Xian and Wu, Qianyi and Zhou, Hang and Xu, Yinghao and Qian, Rui and Lin, Xinyi and Zhou, Xiaowei and Wu, Wayne and Dai, Bo and Zhou, Bolei},
  booktitle=cvpr,
  pages={10462--10472},
  year={2022}
}

% SEEG
@inproceedings{liang2022seeg,
  title={SEEG: Semantic Energized Co-Speech Gesture Generation},
  author={Liang, Yuanzhi and Feng, Qianyu and Zhu, Linchao and Hu, Li and Pan, Pan and Yang, Yi},
  booktitle=cvpr,
  pages={10473--10482},
  year={2022}
}

%Text2Gesture
@INPROCEEDINGS{
  yoonICRA19,
  title={Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots},
  author={Yoon, Youngwoo and Ko, Woo-Ri and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong and Lee, Geehyuk},
  booktitle=icra,
  year={2019}
}

%Trimodal
@article{Yoon2020Speech,
  title={Speech Gesture Generation from the Trimodal Context of Text, Audio, and Speaker Identity},
  author={Youngwoo Yoon and Bok Cha and Joo-Haeng Lee and Minsu Jang and Jaeyeon Lee and Jaehong Kim and Geehyuk Lee},
  journal={ACM Transactions on Graphics},
  year={2020},
  volume={39},
  number={6},
}

%VQ-VAE
@INPROCEEDINGS{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  booktitle=nips,
  pages={6309--6318},
  year={2017}
}

@inproceedings{IVA:2018,
  title={IVA: Investigating the use of recurrent motion modelling for speech gesture generation},
  author={Ferstl, Ylva and McDonnell, Rachel},
  booktitle = {IVA '18 Proceedings of the 18th International Conference on Intelligent Virtual Agents},
  year={2018},
  month = {Nov},
  url = {https://trinityspeechgesture.scss.tcd.ie},
  month_numeric = {11}
}

%Jukebox
@article{dhariwal2020jukebox,
  title={Jukebox: A generative model for music},
  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2005.00341},
  year={2020}
}

%SQ-VAE
@article{takida2022sq,
  title={SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization},
  author={Takida, Yuhta and Shibuya, Takashi and Liao, WeiHsiang and Lai, Chieh-Hsin and Ohmura, Junki and Uesaka, Toshimitsu and Murata, Naoki and Takahashi, Shusuke and Kumakura, Toshiyuki and Mitsufuji, Yuki},
  journal={arXiv preprint arXiv:2205.07547},
  year={2022}
}

%RQ-VAE
@inproceedings{lee2022autoregressive,
  title={Autoregressive Image Generation using Residual Quantization},
  author={Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin},
  booktitle=cvpr,
  pages={11523--11532},
  year={2022}
}

%VideoGPT
@article{yan2021videogpt,
  title={Videogpt: Video generation using vq-vae and transformers},
  author={Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  journal={arXiv preprint arXiv:2104.10157},
  year={2021}
}

%Transformer
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal=nips,
  volume={30},
  year={2017}
}


%CogView 
@article{ding2021cogview,
  title={Cogview: Mastering text-to-image generation via transformers},
  author={Ding, Ming and Yang, Zhuoyi and Hong, Wenyi and Zheng, Wendi and Zhou, Chang and Yin, Da and Lin, Junyang and Zou, Xu and Shao, Zhou and Yang, Hongxia and others},
  journal=nips,
  volume={34},
  pages={19822--19835},
  year={2021}
}

%GPT2
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{hostetter2012effects,
  title={Effects of personality and social situation on representational gesture production},
  author={Hostetter, Autumn B and Potthoff, Andrea L},
  journal={Gesture},
  volume={12},
  number={1},
  pages={62--83},
  year={2012},
  publisher={John Benjamins}
}

@book{kipp2005gesture,
  title={Gesture generation by imitation: From human behavior to computer character animation},
  author={Kipp, Michael},
  year={2005},
  publisher={Universal-Publishers}
}

@article{neff2008,
author = {Neff, Michael and Kipp, Michael and Albrecht, Irene and Seidel, Hans-Peter},
title = {Gesture Modeling and Animation Based on a Probabilistic Re-Creation of Speaker Style},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/1330511.1330516},
doi = {10.1145/1330511.1330516},
journal = {ACM Trans. Graph.},
month = {mar},
articleno = {5},
numpages = {24},
keywords = {Human modeling, character animation, gesture}
}

@article{kopp2003max,
  title={Max-a multimodal assistant in virtual reality construction.},
  author={Kopp, Stefan and Jung, Bernhard and Lessmann, Nadine and Wachsmuth, Ipke},
  journal={KI},
  volume={17},
  number={4},
  pages={11},
  year={2003}
}

@inproceedings{kopp2009,
author = {Bergmann, Kirsten and Kopp, Stefan},
title = {Increasing the Expressiveness of Virtual Agents: Autonomous Generation of Speech and Gesture for Spatial Description Tasks},
year = {2009},
isbn = {9780981738161},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
booktitle = {Proc. AAMAS},
pages = {361–368},
numpages = {8},
keywords = {embodied conversational agents, multimodal output, expressiveness, language, gesture},
location = {Budapest, Hungary},
}

@inproceedings{marsella2013,
author = {Marsella, Stacy and Xu, Yuyu and Lhommet, Margaux and Feng, Andrew and Scherer, Stefan and Shapiro, Ari},
title = {Virtual Character Performance from Speech},
year = {2013},
isbn = {9781450321327},
publisher = {ACM},
url = {https://doi.org/10.1145/2485895.2485900},
doi = {10.1145/2485895.2485900},
booktitle = {Proc. ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)},
pages = {25–35},
numpages = {11},
keywords = {animation, conversational agent, behavior, gestures},
location = {Anaheim, California}
}

@article{su2020multi,
  title={Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements},
  author={Su, Kun and Liu, Xiulong and Shlizerman, Eli},
  journal={arXiv preprint arXiv:2012.03478},
  year={2020}
}

@inproceedings{Zhou2019,
   author = {Yi Zhou and Connelly Barnes and Jingwan Lu and Jimei Yang and Hao Li},
   doi = {10.1109/CVPR.2019.00589},
   isbn = {9781728132938},
   issn = {10636919},
   booktitle = cvpr,
   keywords = {3D from Single Image,And Body Pose,Deep Learning,Face,Gesture,Motion and Tracking,Robotics + Driving},
   month = {6},
   pages = {5738-5746},
   publisher = {IEEE},
   title = {On the continuity of rotation representations in neural networks},
   year = {2019},
}

@INPROCEEDINGS{lee2019talking,
  author={Lee, Gilwoo and Deng, Zhiwei and Ma, Shugao and Shiratori, Takaaki and Srinivasa, Siddhartha and Sheikh, Yaser},
  booktitle=iccv, 
  title={Talking With Hands 16.2M: A Large-Scale Dataset of Synchronized Body-Finger Motion and Audio for Conversational Motion Analysis and Synthesis}, 
  year={2019},
  volume={},
  number={},
  pages={763-772},
  doi={10.1109/ICCV.2019.00085}}

  
  
 
@inproceedings{Qian2021,
   author = {Shenhan Qian and Zhi Tu and Yihao Zhi and Wen Liu and Shenghua Gao},
   doi = {10.1109/ICCV48922.2021.01089},
   isbn = {978-1-6654-2812-5},
   booktitle = iccv,
   month = {10},
   pages = {11057-11066},
   publisher = {IEEE Computer Society},
   title = {Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates},
   year = {2021},
}

@article{OpenPose2019,
  author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},
  journal = pami,
  title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year = {2019}
}

% SMPLX
@inproceedings{SMPL-X:2019,
  title = {Expressive Body Capture: {3D} Hands, Face, and Body from a Single Image},
  author = {Pavlakos, Georgios and Choutas, Vasileios and Ghorbani, Nima and Bolkart, Timo and Osman, Ahmed A. A. and Tzionas, Dimitrios and Black, Michael J.},
  booktitle = cvpr,
  pages     = {10975--10985},
  year = {2019}
}

% multi-modality metrics
@inproceedings{guo2020action2motion,
  title={Action2motion: Conditioned generation of 3d human motions},
  author={Guo, Chuan and Zuo, Xinxin and Wang, Sen and Zou, Shihao and Sun, Qingyao and Deng, Annan and Gong, Minglun and Cheng, Li},
  booktitle={Proc ACM Int. Conf. on Multimedia},
  pages={2021--2029},
  year={2020}
}

@inproceedings{Li2021,
   author = {Jing Li and Di Kang and Wenjie Pei and Xuefei Zhe and Ying Zhang and Zhenyu He and Linchao Bao},
   doi = {10.1109/ICCV48922.2021.01110},
   isbn = {978-1-6654-2812-5},
   booktitle = iccv,
   month = {10},
   pages = {11273-11282},
   publisher = {IEEE},
   title = {Audio2Gestures: Generating Diverse Gestures from Speech Audio with Conditional Variational Autoencoders},
   year = {2021},
}

@article{Ferstl2021,
   author = {Ylva Ferstl and Michael Neff and Rachel McDonnell},
   doi = {10.1002/CAV.2016},
   issn = {1546-427X},
   issue = {3-4},
   journal = {Computer Animation and Virtual Worlds},
   keywords = {computer animation,conversational agents,expressive agents,gesture generation,motion matching,perception},
   month = {6},
   pages = {e2016},
   publisher = {John Wiley & Sons, Ltd},
   title = {ExpressGesture: Expressive gesture generation from speech through database matching},
   volume = {32},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/cav.2016 https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.2016 https://onlinelibrary.wiley.com/doi/10.1002/cav.2016},
   year = {2021},
}

@inproceedings{gesturematching22,
Author = {Habibie, Ikhsanul and Elgharib, Mohamed and Sarkar, Kripashindu and Abdullah, Ahsan and Nyatsanga, Simbarashe and Neff, Michael and Theobalt, Christian},
Title = {A Motion Matching-based Framework for Controllable Gesture Synthesis from Speech},
Booktitle = {SIGGRAPH ’22 Conference Proceedings},
Year = {2022},
Eprint = {Todo},
}

@article{Alexanderson2020,
   abstract = {Automatic synthesis of realistic gestures promises to transform the fields of animation, avatars and communicative agents. In off-line applications, novel tools can alter the role of an animator to that of a director, who provides only high-level input for the desired animation; a learned network then translates these instructions into an appropriate sequence of body poses. In interactive scenarios, systems for generating natural animations on the fly are key to achieving believable and relatable characters. In this paper we address some of the core issues towards these ends. By adapting a deep learning-based motion synthesis method called MoGlow, we propose a new generative model for generating state-of-the-art realistic speech-driven gesticulation. Owing to the probabilistic nature of the approach, our model can produce a battery of different, yet plausible, gestures given the same input speech signal. Just like humans, this gives a rich natural variation of motion. We additionally demonstrate the ability to exert directorial control over the output style, such as gesture level, speed, symmetry and spacial extent. Such control can be leveraged to convey a desired character personality or mood. We achieve all this without any manual annotation of the data. User studies evaluating upper-body gesticulation confirm that the generated motions are natural and well match the input speech. Our method scores above all prior systems and baselines on these measures, and comes close to the ratings of the original recorded motions. We furthermore find that we can accurately control gesticulation styles without unnecessarily compromising perceived naturalness. Finally, we also demonstrate an application of the same method to full-body gesticulation, including the synthesis of stepping motion and stance.},
   author = {Simon Alexanderson and Gustav Eje Henter and Taras Kucherenko and Jonas Beskow},
   doi = {10.1111/CGF.13946},
   issn = {14678659},
   issue = {2},
   journal = {Computer Graphics Forum},
   keywords = {CCS Concepts,Character control,Data-driven animation,Gestures,Motion capture,Neural networks,Probabilistic models,• Computing methodologies → Motion capture; Animation},
   month = {5},
   pages = {487-496},
   publisher = {Blackwell Publishing Ltd},
   title = {Style-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows},
   volume = {39},
   year = {2020},
}

@article{kucherenko2021moving,
author = {Taras Kucherenko and Dai Hasegawa and Naoshi Kaneko and Gustav Eje Henter and Hedvig Kjellström},
title = {Moving Fast and Slow: Analysis of Representations and Post-Processing in Speech-Driven Automatic Gesture Generation},
journal = {International Journal of Human–Computer Interaction},
volume = {37},
number = {14},
pages = {1300-1316},
year  = {2021},
publisher = {Taylor & Francis},
doi = {10.1080/10447318.2021.1883883},
URL = {https://doi.org/10.1080/10447318.2021.1883883},
eprint = {https://doi.org/10.1080/10447318.2021.1883883}
}

@inproceedings{ferstl2018investigating,
  title={Investigating the use of recurrent motion modelling for speech gesture generation},
  author={Ferstl, Ylva and McDonnell, Rachel},
  booktitle={Proc. IVA},
  pages={93--98},
  year={2018}
}

@inproceedings{ahuja2019language2pose,
  title={Language2pose: Natural language grounded pose forecasting},
  author={Ahuja, Chaitanya and Morency, Louis-Philippe},
  booktitle={Proc. Int. Conf. on 3D Vision (3DV)},
  pages={719--728},
  year={2019},
  organization={IEEE}
}

@inproceedings{yoon2022genea,
author={Yoon, Youngwoo and Wolfert, Pieter and
Kucherenko, Taras and Viegas, Carla and
Nikolov, Teodor and Tsakov, Mihail and
Henter, Gustav Eje},
title={{T}he {GENEA} {C}hallenge 2022: {A} large
evaluation of data-driven co-speech gesture
generation},
booktitle = icmi,
publisher = {ACM},
series = {ICMI '22},
year={2022}
}

@inproceedings{
yu2022vectorquantized,
title={Vector-quantized Image Modeling with Improved {VQGAN}},
author={Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},
booktitle=iclr,
year={2022},
url={https://openreview.net/forum?id=pfNyExj7z2}
}

%VQ-VAE2
@InProceedings{razavi2019generating,
  title={Generating diverse high-fidelity images with vq-vae-2},
  author={Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
  booktitle=nips,
  pages={14866--14876},
  year={2019}
}

@InProceedings{dalle2021,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 icml,
  pages = 	 {8821--8831},
  year = 	 {2021},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
}

@INPROCEEDINGS {esser2021,
author = {P. Esser and R. Rombach and B. Ommer},
booktitle = cvpr,
title = {Taming Transformers for High-Resolution Image Synthesis},
year = {2021},
volume = {},
issn = {},
pages = {12868-12878},
keywords = {vocabulary;image segmentation;computer vision;image synthesis;computer architecture;transformers;rendering (computer graphics)},
doi = {10.1109/CVPR46437.2021.01268},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.01268},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{kucherenko2020gesticulator,
  title={Gesticulator: A framework for semantically-aware speech-driven gesture generation},
  author={Kucherenko, Taras and Jonell, Patrik and van Waveren, Sanne and Henter, Gustav Eje and Alexandersson, Simon and Leite, Iolanda and Kjellstr{\"o}m, Hedvig},
  booktitle=icmi,
  pages={242--250},
  year={2020}
}

@inproceedings{bhattacharya2021text2gestures,
  title={Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents},
  author={Bhattacharya, Uttaran and Rewkowski, Nicholas and Banerjee, Abhishek and Guhan, Pooja and Bera, Aniket and Manocha, Dinesh},
  booktitle={Proc. IEEE Virtual Reality and 3D User Interfaces (VR)},
  pages={1--10},
  year={2021},
  organization={IEEE}
}


@inproceedings{bhattacharya2021speech2affectivegestures,
  title={Speech2affectivegestures: Synthesizing co-speech gestures with generative adversarial affective expression learning},
  author={Bhattacharya, Uttaran and Childs, Elizabeth and Rewkowski, Nicholas and Manocha, Dinesh},
  booktitle=ACMMM,
  pages={2027--2036},
  year={2021}
}

@inproceedings{kucherenko2021large,
  title={A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020},
  author={Kucherenko, Taras and Jonell, Patrik and Yoon, Youngwoo and Wolfert, Pieter and Henter, Gustav Eje},
  booktitle={Proc. ACM Annual Conference on Intelligent User Interfaces (IUI)},
  publisher={ACM},
  pages={11--21},
  doi={10.1145/3397481.3450692},
  year={2021}
}

@inproceedings{jonell2021hemvip,
  title={{HEMVIP}: Human evaluation of multiple videos in parallel},
  author={Jonell, Patrik and Yoon, Youngwoo and Wolfert, Pieter and Kucherenko, Taras and Henter, Gustav Eje},
  booktitle=icmi,
  publisher={ACM},
  pages={707--711},
  doi={10.1145/3462244.3479957},
  year={2021}
}

@inproceedings{yoon2021sgtoolkit,
  title={SGToolkit: An interactive gesture authoring toolkit for embodied conversational agents},
  author={Yoon, Youngwoo and Park, Keunwoo and Jang, Minsu and Kim, Jaehong and Lee, Geehyuk},
  booktitle={Proc. ACM UIST},
  pages={826--840},
  publisher={ACM},
  doi={10.1145/3472749.3474789},
  year={2021}
}

@article{ghorbani2022zeroeggs,
  title={ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech},
  author={Ghorbani, Saeed and Ferstl, Ylva and Holden, Daniel and Troje, Nikolaus F and Carbonneau, Marc-Andr{\'e}},
  journal={arXiv preprint arXiv:2209.07556},
  year={2022}
}

@inproceedings{feng2022tool,
author = {Feng, Andrew and Shin, Samuel and Yoon, Youngwoo},
title = {A Tool for Extracting 3D Avatar-Ready Gesture Animations from Monocular Videos},
year = {2022},
isbn = {9781450398886},
publisher = {ACM},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561975.3562953},
doi = {10.1145/3561975.3562953},
booktitle = {Proc. ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG)},
articleno = {7},
numpages = {7},
keywords = {human mesh recovery, gesture dataset, gesture synthesis},
location = {Guanajuato, Mexico}
}

@article{bojanowski2016enriching,
  title={Enriching Word Vectors with Subword Information},
  author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={135--146},
  year={2017},
  publisher={MIT Press}
}

@article{BaiTCN2018,
	author    = {Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
	title     = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
	journal   = {arXiv:1803.01271},
	year      = {2018},
}