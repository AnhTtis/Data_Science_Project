%\section{Background VQ-VAE}
% the background text for introducing VQ-VAE & RQ-VAE?
%The proposed method is motivated by the recent works in cross-modal text-to-image synthesis \cite{dalle2021, esser2021} that utilize VQ-VAE as latent space representation for image patches and generate new images via autoregressive models to predict discrete tokens for each patch. As a high-level analogy for gesture generations, this could be seen as extracting a smaller set of gesture units from the training motions and learning the conditional probability distribution for these gesture units based on speech context and previous gestures. One motivation for learning the gesture units as discrete latent vectors is because the generation process can be seen as sampling from the codebook instead of interpolation within a continuous latent space and thus we hypothesize that the resulting gestures are more likely to retain their motion quality from original data. Moreover, learning the probability distribution in the discrete codebook will naturally handle the issue when different gesture motions are associated with the same speech in the training data since during inference the model can randomly pick one of these gesture units instead of outputting their average. 



