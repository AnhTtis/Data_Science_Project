% to-do : Update ceretain paragraphs for a better social robot motivations.
\section{Introduction}

Co-speech gesture synthesis enhances the realism of the robots or virtual agents by generating gesture motions that are plausible and synchronous to the corresponding speech input. It also plays an important role in making the robot move and communicate more like a real human for social interaction scenarios. Therefore realistic gesture generations would enhance the acceptances of social robots by human users and could find applications in education, training, and medical use cases. However, generating gestures that are both realistic and semantically appropriate for the input speech is a challenging and unresolved problem.

One of the challenges for generating co-speech gestures is that motion synthesis from speech is a one-to-many mapping problem, which means multiple different gestures could correspond to the same speech input. For instance, for a given speech that corresponds to a beat motion, a person could use his left, right, or both hands to perform the beat. All of them are plausible and will be deemed appropriate motions for a human user. However, learning from such data with one-to-many mapping requires more careful consideration for the model architectures as a deterministic regression will not likely learn either of the gestures but the average of them. Previous works formulated the problem as a deterministic process and therefore utilized convolutional neural network \cite{ginosar2019gestures} or recurrent neural network \cite{yoonICRA19, Yoon2020Speech} to map speech directly to gesture motions. Thus these methods tend to produce more damped gesture motions that are less interesting for human perceptions. Adding adversarial training schemes \cite{ginosar2019gestures, Yoon2020Speech} improved the results, but at the cost of a more elaborated training process that requires careful tuning. More recent works explored the probabilistic framework \cite{Alexanderson2020, Li2021} to address the non-deterministic nature of gesture synthesis and sample new gestures during inferences. This line of work builds a latent space model (\emph{e.g.}, normalizing flows) from gesture motions to learn its conditional probability distributions. Then during inference,  the gestures are randomly sampled from the latent space conditioned on speech input as the control signals. 

In this work, we tackled the issue of one-to-many mapping in gesture synthesis using discrete latent space learning. Specifically, we proposed to utilize residual quantization \cite{lee2022autoregressive} to learn discrete latent codes from gesture motions. Such discrete codes used as gesture representations allow sampling of various gesture tokens by learning an auto-regressive prior conditioned on speech context. Therefore the model is able to sample varying hand gestures associated with the same speech input with different probabilities, instead of their averaged gestures as in deterministic regressions. Residual quantization uses multiple codes to represent a gesture and reduces the reconstruction errors when using the discrete latent codes. The conditional sampling is via a two-level transformer architecture to handle the residual codes and learn the priors for gesture tokens that synthesizes diverse gesture motions. 

The proposed method is motivated by the recent success in cross-modal text-to-image synthesis \cite{dalle2021, esser2021} that utilize VQ-VAE as latent space representation for image patches and generate new images via autoregressive models to predict discrete tokens for each patch. As a high-level analogy for gesture generations, this could be considered as extracting a smaller set of gesture units from the training motions and learning the conditional probability distribution for these gesture units conditioned on speech context and previous gestures.

Our objective and subjective evaluation showed that the proposed method produces gesture motions with higher fidelity and retains the motion quality from human motion. Since the gesture synthesis process is formulated as sampling the next likely token in the codebook, multiple gesture sequences could be produced from the same speech input using top-k sampling during inference. Therefore the method does not suffer from the over-smoothed or damped gesture motions caused by the deterministic mapping of the regression models in previous methods. 

Our contributions are summarized as the following: 1) We proposed to learn discrete latent vectors as gesture motion representation through quantization to model short gesture segments as tokens. To reduce reconstruction errors and better approximate original gestures, we utilize residual quantization (RQ) during the quantization process. 2) Modeling the distribution of learned gesture tokens by using the two-level transformers to handle residual codes. With stochastic sampling, the model naturally generates multi-modal gesture motions from the same speech context. 
%To our best knowledge, this is the first attempt to synthesize co-speech gesture motion using discrete gesture tokens learned from data. 
%To our best knowledge, this is the first attempt to apply sequence modeling with learned discrete gesture tokens for speech gesture synthesis.
3) We evaluated our method in a comparison with state-of-the-art methods using both objective metrics and subjective user study. The results demonstrated that our method outperforms the previous methods in both gesture quality and diversity metrics. The user evaluation results also showed that the proposed approach can produce gesture motions with better human likeness and gesture appropriateness than the previous methods. The full source code of our implementation will be publicly available via GitHub for future research.


%In summary, our contribution is a novel two-stage method for co-speech gesture generation from multi-modal context information including audio and text. Firstly, we proposed to utilize a VQ-VAE model for modeling smaller gesture units as codebook vectors. Secondly, we proposed an autoregressive model based on the GPT-2 transformer to model the distribution on the discrete latent space of VQ-VAE and to sample new gesture sequences based on the given speech context. The user evaluation results showed the proposed method can produce gesture motions with reasonable human likeness and gesture appropriateness. 

% motivation goes here, should briefly talk about why this task is interesting.
% Current work limits the usage of model to either convolutional neural network \cite{ginosar2019gestures} or recurrent neural network \cite{yoonICRA19}, both of which lack the ability of capturing long term dependency in sequential data. Thus, our  proposal is to make use of transformer architecture \cite{vaswani2017attention},which is good at understanding the relationship between long-ranged elements. 



% In the experiment we found Mapping gestures to hidden codebook not only mitigates the problem of overfitting, but also increase the fluency of continuous poses.
% Whatâ€™s more, in generation we can sample from the possible poses distribution, with the same context and preposes we could generate multiple pose sequences, which is exactly the case because people have different gestures even speaking the exact same thing \cite{hostetter2012effects}.

% In the present study, we proposes two training stages with two models based on the the above facts.
% The first stage is to train the VQ-VAE, and the inputs and outputs are pose gestures only. Our main purpose is to get good feature representation of pose gestures.We builds our generative model in the second stage, which is primary based on the architecture of Trimodal \cite{Yoon2020Speech}. We keep the TextEncoder and the WaveEncoder structure the same to get feature representation of the speech and audio information. Instead of using GRU for generation, we uses a stack of masked self-attentions for medeling the hidden code.

% Our contributions can be summarized as the following:
% To summary, our system is a two stage method for speech gesture generation which utilizes multimodal context information include audio, speech text, etc.
% Firstly, We proposed a VQ-VAE model for human poses feature extraction and reconstruction.
% Secondly, we proposed a conditioned GPT-2 transformer to model the distribution on the latent space of VQ-VAE and to predict new poses based on preposes and given context.