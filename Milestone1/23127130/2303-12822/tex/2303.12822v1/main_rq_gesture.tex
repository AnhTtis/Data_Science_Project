\pdfoutput=1
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}

\IEEEoverridecommandlockouts
\overrideIEEEmargins

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
% \usepackage{setspace}
%\usepackage{booktabs}
\usepackage{svg}
\newcommand{\etal}{\textit{et al.}}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{hyperref}
\hypersetup{breaklinks,colorlinks}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{10301} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\LARGE \bf
Co-Speech Gesture Synthesis using Discrete Gesture Token Learning
}
% Another candidate for the title
% Co-Speech Gesture Synthesis using Discrete Gesture Token Learning

\author{Shuhong Lu$^{1}$, Youngwoo Yoon$^{2}$, and Andrew Feng$^{1}$
%\thanks{This work was supported by ...}
\thanks{$^{1}$Shuhong Lu and Andrew Feng are with Institute for Creative Technologies, University of Southern California, Los Angeles, USA
{\tt\small \{shuhongl, feng\}@usc.edu}}%
\thanks{$^{2}$Electronics and Telecommunications Research Institute, Daejeon, Republic of Korea
{\tt\small youngwoo@etri.re.kr}}%
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    
   Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes conditioned on input speech context. Since the inference is formulated as token sampling, multiple gesture sequences could be generated given the same speech input using top-k sampling. The quantitative results and the user study showed the proposed method outperforms the previous methods and is able to generate realistic and diverse gesture motions. 
\end{abstract}

%%%%%%%%% BODY TEXT
\input{1_Introduction}
\input{2_Related_Work}
\input{3_Background_VQVAE}
\input{4_Method}
\input{5_Experiment}
\input{6_Conclusion}

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{shortstrings,bibliography}
}

\end{document}
