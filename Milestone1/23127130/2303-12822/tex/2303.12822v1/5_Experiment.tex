


\section{Experiments}
\subsection{Datasets and Training} %and Pre-processing}
\noindent \textbf{TED Gesture} 
The dataset \cite{yoonICRA19, Yoon2020Speech} is a collection of monologue speech gestures from TED talks. The dataset consists of 1,766 videos that are processed via OpenPose \cite{OpenPose2019} to identify shots of interest. It contains $97$ hours of valid gesture motions and corresponding transcripts. For our experiments, we used the refined version \cite{feng2022tool} of the dataset by re-processing the original dataset into SMPL-X \cite{SMPL-X:2019} pose representation which gives more accurate motion. 

\noindent \textbf{Trinity} 
The dataset is a speech-gesture dataset of a single male speaker. The gestures were created via motion capture. It provides high-quality gesture motions but the dataset has a smaller size of about $5$ hours. For consistent pose representations, we retargeted the original motions to SMPL-X \cite{SMPL-X:2019} model before further data pre-processing.

% \paragraph{Training Details} 
We implemented our method using PyTorch framework. The codebook size was $1024$ for TED Gesture dataset and $256$ for Trinity dataset, and the temporal reduction factor was set to $s=4$ for both dataset. The RQ-VAE was trained for $500$ epoch and the RQ-Transformer was trained for $1000$ epochs. Both models were trained using AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.95$, and the learning rate was $0.0005$. We trained on a single RTX 3090 GPU for Trinity dataset using batch size $128$ and 4$\times$ RTX 8000 GPUs for TED Gesture dataset using batch size $512$.


\begin{figure}[b]
  \centering
\includegraphics[width=.95\linewidth]{figure/Fig_Comparison.pdf}
  \caption{Samples visualization of the human motion (GT) and synthesized motions for the same speech. From top to bottom, Joint Embedding (JE), Trimodal, Speech2Gesture (S2G), Seq2Seq, and our RQ-Transformer model (RQ). Please see the supplementary video for animated visualization. }
  \label{fig:comp}
\end{figure}


\subsection{Baseline Methods}% describe baseline methods for Seq2Seq, Speech2Gesture, Trimoda, Language2Pose, and VQ-Transformer.
%\noindent \textbf{Baseline Methods}
We compared our method with five baseline methods. 1) \textbf{Seq2Seq} \cite{yoonICRA19} utilizes LSTM network with an attention mechanism to generate gestures from speech text input. 2) \textbf{Speech2Gesture} \cite{ginosar2019gestures} uses a convolutional encoder-decoder network with adversarial training to generate gestures from speech audios. 3) \textbf{Trimodal} \cite{Yoon2020Speech} uses the combination of speech audio, text, and speaker ID to synthesize gestures. 4) \textbf{Joint Embedding} \cite{ahuja2019language2pose} trains two separate encoders to map both input speech text and target motions into the same latent space before decoding them into motions. During inference, only the speech is used to obtain a latent space vector for decoding motions. 5) \textbf{VQ-Transformer} is based on our proposed model architecture without using residual quantization, and it is considered the baseline implementation.  





% \textbf{4) Multi-modality} Multi-modality measures how many different gesture motions could by synthesized using the same speech input. Following \cite{guo2020action2motion}, we generate $K=20$ different gesture motions for each speech input and compute the average $L1$ distances between them.



\subsection{Objective Evaluations}\label{sec:objective}
\noindent \textbf{Evaluation Metrics}
For quantitative evaluations, we used the following metrics that were commonly used to measure the quality of synthesized gestures. \textbf{1) Fr\'echet Gesture Distance (FGD)} Similar to FID \cite{heusel2017gans} for measuring the perceptual quality of images, FGD \cite{Yoon2020Speech} measures how much the data distribution of the synthesized gestures is close to the distribution of human gestures. We followed the same encoder-decoder architectures in \cite{Yoon2020Speech} and trained the feature extractor as an autoencoder using both Trinity and TED Gesture datasets for calculating FGD.
\textbf{2) Diversity} Diversity measures how many different types of gestures are synthesized within a long sequence. We followed the formula in \cite{Li2021} by first splitting the output gestures into equal size segments and then computing the average $L1$ distances between all pairs of segments. Note that high diversity does not necessarily indicate good gestures as noisy motions could also produce high diversity results. Therefore the metric should be considered alongside FGD to ensure gesture qualities.
\textbf{3) Beat Consistency (Beat)} The metric measures the motion-audio correlations, which indicate whether the arm movements synchronize well with the beat sound in the speech audio \cite{liu2022learning}. Specifically, the shoulder, elbow, and wrist rotations are used to obtain the local optima as kinematic beats. The audio beats are then compared with kinematic beats to compute beat consistency scores as average distances between nearest pairs. 

% quantitative results
\begingroup
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\renewcommand{\arraystretch}{1} % Default value: 1
% \begin{singlespace}
\begin{table}
  \caption{Objective evaluation results of the proposed method and baseline methods. Div and Beat represent diversity and beat consistency.}
  \label{tab:baseline_comp}
\centering
  \begin{tabular}{|p{2.4cm}||c|c|c||c|c|c|}
    \hline
     &
      \multicolumn{3}{c||}{TED Gesture} &
      \multicolumn{3}{c|}{Trinity }  \\
                    & FGD$\downarrow$ & Div.$\uparrow$ & Beat$\uparrow$ & FGD$\downarrow$ & Div.$\uparrow$ & Beat$\uparrow$\\
      \hline\hline
    Seq2Seq \cite{yoonICRA19}         & 8.86 & 0.91 & 0.152 & 4.99 & 1.54 & 0.238 \\
    \hline
    Speech2Gesture \cite{ginosar2019gestures} & 5.02 & 0.51 & \textbf{0.812} & 3.87 & 1.21 & 0.620 \\
    \hline
    Trimodal \cite{Yoon2020Speech}         & 1.74 & 0.65 & 0.461 & 2.86  & 1.34 & 0.652 \\
    \hline
    Joint Embedding \cite{ahuja2019language2pose} & 2.24 & 0.66 & 0.084 & 2.60  & 1.33 & 0.247 \\
    \hline
    VQ-Transformer (Ours, $D=1$)       & \underline{1.66} & \underline{1.02} & 0.793 & \underline{1.64} & \underline{1.61} &  \underline{0.733}\\
    \hline
    RQ-Transformer (Ours, $D=4$)      & \textbf{0.40} & \textbf{1.36} & \underline{0.798} & \textbf{1.52} & \textbf{1.68} & \textbf{0.745}  \\
    \hline
  \end{tabular}  
  %\caption{Objective evaluation results of the proposed method and baseline methods. We report FID, Diversity, and Beat consistency measurements for both TED Gesture Dataset and Trinity Dataset. The best one is in \textbf{bold} and the second best is \underline{underlined}.}
\end{table}
% \end{singlespace}
\endgroup

Table \ref{tab:baseline_comp} shows the quantitative results of the proposed method and baseline methods. We report FGD, Diversity, and Beat consistency measurements for both TED Gesture Dataset and Trinity Dataset. The best one is in \textbf{bold} and the second best is \underline{underlined}. For TED Gesture dataset, the RQ-Transformer performed the best in both FGD and Diversity metrics, and showed the second-highest score for beat consistency. For Trinity dataset, the RQ-Transformer obtained the best results in all metrics, which indicate synthesized gestures from the proposed method are close to human gestures and have good synchronicity with input speech. Note that the VQ-Transformer, which is the proposed method without residual quantization, is also among the top-2 methods in all metrics. Since our method encodes gestures as discrete tokens, it is less likely to produce damped motions (\emph{i.e.} for Trinity dataset, our method showed 78\% of wrist motion speed to the human motion while the other methods showed only 50--60\%) and thus has lower FGD scores. On the other hand, by using top-k sampling, the gesture generation process is not limited to deterministic mapping and is able to sample various plausible gestures based on the autoregressive prior learned from the human motion. The quantitative evaluation results from both datasets are consistent with these capabilities and show that our method is able to produce diverse gestures while retaining the gesture quality from human motion. 

% Youngwoo: made it fit in a column
% Youngwoo: I move this table to be close to the corresponding text
\begin{table}
\caption{Experimental results on varying $D$ in Residual Quantizations (RQ).}
  \label{tab:ablation}
\centering
  \begin{tabular}{|l||c|c||c|c|c|}
    \hline
     &
      \multicolumn{2}{c||}{VAE Recon.} &
      \multicolumn{3}{c|}{Transformer Inference }  \\
                    & FGD $\downarrow$ & L1 $\downarrow$ & FGD $\downarrow$ & Diversity $\uparrow$ & Beat $\uparrow$
                    \\
      \hline\hline
    VQ ($D=1$)  & 0.85 & 0.031 & 1.64 & 1.61 & 0.733 \\
    \hline
    RQ ($D=2$)  & 0.51 & 0.027 & 1.60 & 1.61 & 0.737 \\
    \hline
    RQ ($D=3$)  & 0.28 & 0.023 & 1.57 & 1.62  & \textbf{0.751} \\
    \hline
    RQ ($D=4$)  & \textbf{0.16} & \textbf{0.022} & \textbf{1.52} & 
    \textbf{1.68}  & 0.745 \\
    \hline
  \end{tabular}  
  %\caption{Experimental results on how Residual Quantization (RQ) settings affect the gesture synthesis results by using a different number of residual layers $D$. RQ with $D=1$ means there is no residual quantization and it is equivalent to Vector Quantization (VQ). We report VAE reconstruction performance and gesture synthesis performance (Transformer-based gesture token inference).}
\end{table}


\begin{table}
%\caption{Experimental results on varying codebook size $N$, temporal factor $s$, and RQ layers when training RQ-VAE. Both FGD and L1 loss were computed based on RQ-VAE reconstruction results.}
\caption{Experimental results on varying codebook size $N$, temporal factor $s$, and RQ layers $D$ for RQ-VAE reconstructions.}
 \label{tab:RQVAE}
\centering
\begin{tabular}{|c|c|c||c|c|}
 \hline
 $N$ & $s$ & RQ Layers & FGD & L1 Loss\\
 \hline\hline
 128 & 4 & 4 & 0.349 & 0.024 \\\hline
 256 & 4 & 4 & \textbf{0.163} & \textbf{0.022} \\\hline
 512 & 4 & 4 & 0.360 & 0.023 \\\hline
 1024 & 4 & 4 & 0.239 & \textbf{0.022} \\\hline
 256 & 2 & 2 & 0.381 & 0.730 \\\hline
 256 & 2 & 4 & 0.406 & 0.641 \\\hline
 %256 & 4 & 1 & 0.849 & 0.031 \\
 256 & 4 & 2 & 0.515 & 0.027 \\\hline
 %256 & 4 & 3 & 0.277 & 0.023 \\
\end{tabular}
\end{table}


We also performed an additional experiment on how RQ settings affect the gesture synthesis results by using a different number of residual layers $D$. Table \ref{tab:ablation} shows the quantitative results from the RQ-VAE and its subsequent transformers by varying $D$. Note that $D=1$ is equivalent to VQ baseline setting without residual quantization. The VAE Reconstruction part shows the gesture reconstruction results of RQ-VAE models while the Transformer Inference part shows the results from RQ-Transformer for gesture synthesis. All models were trained on Trinity dataset with codebook size 256 and temporal reduction factor $s=4$. 

For VAE reconstruction, the results indicate that with the number of RQ layers increasing, the reconstructed gestures are closer to the ground truth gestures. This is consistent with our expectations that applying the residual quantization helps reduce the reconstruction errors compared to vanilla vector quantization given the same codebook size since more codes are used to reconstruct the gesture segment.

For RQ-Transformer, the results showed that the transformer performance also increases with the corresponding VAE encoder with more residual layers, though the improvements are less significant. While RQ with $D=4$ still performs better in both FGD and diversity, it does not fully reflect the advantages of the lower reconstruction errors from VAE. The results also indicate that while increasing the RQ layers $D$ would better approximate the original data, it may also require stronger model architectures to model the conditional priors since additional residual tokens would complicate the probabilistic mapping that needs to be learned. These results validate the use of RQ over VQ for representing the gesture tokens.

Table \ref{tab:RQVAE} shows the experiment results on Trinity dataset using different codebook size $N$ and temporal reduction factor $s$ for RQ-VAE reconstructions. Intuitively, larger $N$ or smaller $s$ should result in lower reconstruction errors since the latent space model would have better capacity to represent the input data. However, in practice we found that it might result in codebook collapse and did not always produce better results. Thus these parameters are selected based on the experiment results to train the RQ-VAE and subsequent RQ-Transformer. %Based on the experiment results, we selected codebook size $N=256$ and temporal factor $s=4$ to train RQ-VAE and RQ-Transformer for the Trinity dataset.
%for training the subsequent transformer used in the objective evaluations of the main paper.


% \begin{enumerate}
%   \item \textbf{FGD}.
%   \item \textbf{Diversity}.
%   \item \textbf{Multi-modality}.
% \end{enumerate}

%\subsubsection{Ablation Study}
% varying codebook size and/or 4x vs 2x downsampling for VQ and RQ.
% varying residual layers (1 to 4)
% need to add ablation study resultss





% human evaluation results
\subsection{Subjective Evaluations}

\iffalse
%two column version
\begin{figure*}[t]
  \centering
  \includesvg[width=0.40\linewidth]{figure/Human-likeness.svg} \includesvg[width=0.40\linewidth]{figure/Appropriateness.svg}
  \caption{} \label{fig:userstudy_results}
\end{figure*}

% one column version
\begin{figure}[t]
  \centering
  \includesvg[width=0.95\linewidth]{figure/Human-likeness_small.svg} 
  \includesvg[width=0.95\linewidth]{figure/Appropriateness_small.svg}
  \caption{Human evaluation results for (top) human-likeness and (bottom) appropriateness studies. Box plots for human motion and five gesture-generation systems. RQ-T represents the proposed RQ-Transformer, and the other condition names represent Joint Embedding \cite{ahuja2019language2pose}, Trimodal \cite{Yoon2020Speech}, Seq2Seq \cite{yoonICRA19}, and Speech2Gesture \cite{ginosar2019gestures}. The left figure is for the study of human-likeness of motion, and the right one is for motion appropriateness to input speech. Boxes cover 25th and 75th percentiles, and whiskers represent the 5th and 95th percentiles. Box notches represent median values.} \label{fig:userstudy_results}
\end{figure}
\fi

% merged version
\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figure/user_study_results_merged.png} 
  \caption{Evaluation results of two subjective user studies: 1) human-likeness of motion and 2) motion appropriateness to input speech. Box plots for human motion and five gesture-generation system conditions (ordered by descending appropriateness rating on the x-axis). RQ-T represents the proposed RQ-Transformer, and the other condition names represent Joint Embedding \cite{ahuja2019language2pose}, Trimodal \cite{Yoon2020Speech}, Seq2Seq \cite{yoonICRA19}, and Speech2Gesture \cite{ginosar2019gestures}. Boxes cover 25th and 75th percentiles, and whiskers represent the 5th and 95th percentiles. Box notches represent median values.} \label{fig:userstudy_results}
\end{figure}

% evaluation intro
In addition to the objective evaluation, we conducted a subjective evaluation in that human participants rate the gesture motion videos of a virtual character. We followed the evaluation scheme introduced in GENEA Challenge 2020 \cite{kucherenko2021large} and was used in related studies \cite{yoon2021sgtoolkit, ghorbani2022zeroeggs}. The evaluation scheme consists of two studies that measure human-likeness of generated motion and appropriateness of motion to the input speech. The web interface \cite{jonell2021hemvip} presents video stimulus on a page, and a participant rates each video in a scale of 0-100. For the human-likeness study, videos were muted so that a participant can rate only the motion quality, not how appropriate to the input speech.

% evaluation setup
There were six conditions of human motion, RQ-Transformer (ours), Joint Embedding \cite{ahuja2019language2pose}, Trimodal \cite{Yoon2020Speech}, Seq2Seq \cite{yoonICRA19}, and Speech2Gesture \cite{ginosar2019gestures}. We selected 50 random speech segments from the test set and generated video stimuli corresponding to each condition for each segment using the same visualization in Figure \ref{fig:comp}. We recruited 50 valid crowd workers for each study from Prolific crowd-sourcing platform. There were 100 participants in total excluding participants who failed attention checks (36 females, 64 males; ages ranged from 19 to 70 years). Each participant rated 46 videos (8 pages x 6 videos; 2 videos were replaced by attention checks), and we collected 2300 ratings for each study. The participants were compensated 4 USD. The responses from the participants who failed the attention checks were excluded from the result analysis. In the attention-check videos, an overlay text requesting to rate the current video to a specific number appears in the middle of the video playing.

% results description (incl. stat analysis)
Figure \ref{fig:userstudy_results} shows the results of two studies. The human motion condition showed the highest ratings in both studies (Median$=$67 for the human-likeness study, 69 for the appropriateness study). The RQ-Transformer showed the second highest ratings for both studies (Median$=$60, 55). Following them, the other conditions showed median ratings of 45, 40 (Joint Embedding); 38, 35 (Trimodal); 39, 30 (Seq2Seq); 21, 15 (Speech2Gesture). The ratings between the conditions were significantly different except for between Trimodal and Seq2Seq in the human-likeness study. We used Wilcoxon rank-sum tests with Holmâ€“Bonferroni corrections for multiple comparisons. The family-wise error rate was 0.05.

% a short discussion
The subjective evaluation showed a consistent result to the objective evaluation that the proposed RQ-Transformer is able to synthesize more natural and appropriate gestures than the previous methods. In both subjective evaluation studies, RQ-Transformer was rated higher than all the other generation methods by large margins. The difference between the natural human motion and the proposed method was small in terms of human-likeness of motion while the difference was apparent in terms of speech appropriateness. Conveying semantic gestures like iconic and metaphoric gestures is a challenging problem especially for data-driven gesture generation approaches because of the scarcity of data samples mapping speech to semantic gestures.

Figure \ref{fig:comp} showed examples of the qualitative results by comparing gesture results from our method and baseline methods with natural human motions. Here the keyframes from the synthesized gestures of our method and other baselines were visualized and compared against the ground truth motions. Joint Embedding \cite{ahuja2019language2pose} and Seq2Seq \cite{yoonICRA19} methods produce damped motions that are less synchronous to input speech. For Speech2Gesture \cite{ginosar2019gestures} and Trimodal \cite{Yoon2020Speech} methods, they tend to produce synchronous gesture motions but lack motion diversity when compared to ground truth. By using discrete gesture tokens as data representation, our model is able to produce more interesting and diverse gesture motions than other baselines. Please see the supplemental video for more qualitative comparison results. 
