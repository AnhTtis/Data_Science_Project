{
    "arxiv_id": "2303.12822",
    "paper_title": "Co-Speech Gesture Synthesis using Discrete Gesture Token Learning",
    "authors": [
        "Shuhong Lu",
        "Youngwoo Yoon",
        "Andrew Feng"
    ],
    "submission_date": "2023-03-04",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.RO"
    ],
    "abstract": "Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes conditioned on input speech context. Since the inference is formulated as token sampling, multiple gesture sequences could be generated given the same speech input using top-k sampling. The quantitative results and the user study showed the proposed method outperforms the previous methods and is able to generate realistic and diverse gesture motions.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.12822v1"
    ],
    "publication_venue": "8 pages, 3 figures, 3 tables"
}