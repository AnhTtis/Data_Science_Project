\documentclass{AUJarticle}
\usepackage[cmex10]{amsmath}
\usepackage[utf8x]{inputenc}
\usepackage[nocompress]{cite}
\usepackage{graphicx, multirow, booktabs, color, listings}
\usepackage{balance}


\usepackage{subcaption}
\usepackage{caption}
\usepackage{array}
\usepackage{multirow}
\usepackage{cleveref}
\usepackage{xurl}

\graphicspath{{figures/}}

\pagestyle{empty}

\pdfinfo{ /Title (Template for Ada User Journal)
  /Author (An Author, B. Another, Y. Other)
  /Keywords (Ada) }

\newcommand{\todo}{\textcolor{red}}

\hyphenation{}

\setcounter{page}{1}

\begin{document}

\title{Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm}


\addauthor{Bakary Badjie, José Cecílio, António Casimiro}
{LASIGE, Departamento de Informática, Faculdade de Ciências da Universidade Lisboa, Lisboa} 
  {\{bbadjie, jmcecilio, casim\}@ciencias.ulisboa.pt}

\issuev{1}
\issuen{1}
\issued{March 2023}

\shortauthor{B. Badjie et al.}
\shorttitle{Denoising Autoencoder-based Defensive Distillation}

\thispagestyle{plain}

\maketitle

\begin{abstract}



%The robustness of deep neural networks (DNNs) is significantly threatened by adversarial attacks. Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work suggests a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). Our technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data in order to assess the performance of our suggested method. Our experimental findings demonstrate that our method was successful in identifying and reconstructing the poisonous inputs while also considerably enhancing the DNN's resilience. Our proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data poisoning attacks are a concern. Thus, the defensive distillation technique's limitation posed by poisonous adversarial attacks is overcome.

Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defence mechanism for DNNs in various applications where data poisoning attacks are a concern. Thus, the defensive distillation technique's limitation posed by poisonous adversarial attacks is overcome.

Keywords: \textit{Deep Neural Network}, \textit{Denoising Autoencoder}, \textit{Defensive Distillatiomn}, \textit{Adversarial attacks and Robustmness}, \textit{Alibi-detect}

\end{abstract}



%% \linenumbers

%---------------- Introduction ------------------
\input{sections/introduction}
%------------------------------------------------


% %---------------- Related work ------------------
% \input{sections/relatedwork}
% %------------------------------------------------


% %------------------ Proposal --------------------
% \input{sections/proposal}
% %------------------------------------------------

% %------------------ Results ---------------------
% \input{sections/results}
% %------------------------------------------------

% %------------------ Conclusions --------------------
% \input{sections/conclusion}
% %------------------------------------------------


% \section*{Acknowledgments}
% This work was supported by the LASIGE Research Unit (ref. UIDB/00408/2020 and ref. UIDP/00408/2020), and by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 957197 (VEDLIoT project).

%\section*{References}

\bibliographystyle{ieeetr}
\bibliography{References}
\balance

\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
