
\section{Proofs for Section \ref{sxn:logistic_loss}}\label{sxn:proof_appendix}


\textbf{Proof of Theorem \ref{thm:space_lower_bound}}

\begin{proof}
Define $\Rcal(\beta) = \sum_{i=1}^n \max\{0, \xb_i^T\beta\}$, i.e., the ReLu loss. We will first lower bound the space needed by any data structure which approximates ReLu loss to $\epsilon$-relative error. Later, we will show that this implies a lower bound on the space complexity of any data structure $f(\cdot)$ for approximating logistic loss. Let $g(\cdot)$ approximate $\Rcal(\cdot)$ such that $(1-\epsilon) \Rcal(\beta) \leq g(\beta) \leq (1+\epsilon)\Rcal(\beta)$ for all $\beta \in \R^d$. We can rewrite $\Rcal(\beta)$ as follows:
\begin{flalign*}
    \Rcal(\beta) &= \sum_{i=1}^n \max\{0, \xb_i^T\beta\} 
    = \sum_{i=1}^n \nicefrac{1}{2}\cdot \xb_i^T\beta + \nicefrac{1}{2}\cdot |\xb_i^T\beta| 
    = \frac{1}{2}\one^T \Xb\beta + \frac{1}{2}\|\Xb\beta\|_1.
\end{flalign*}
We next use that $\Rcal(\beta) \leq \|\Xb\beta\|_1$.
\begin{flalign*}
    |\Rcal(\beta) - g(\beta)|
    = |\frac{1}{2}\one^T \Xb\beta + \frac{1}{2}\|\Xb\beta\|_1 - g(\beta)| \leq \epsilon \Rcal(\beta) \\
    \Rightarrow |\frac{1}{2}\one^T \Xb\beta + \frac{1}{2}\|\Xb\beta\|_1 - g(\beta)| \leq \epsilon\|\Xb\beta\|_1.
\end{flalign*}

We can store $\one^T\Xb$ exactly in $\Ocal(d)$ space as a length $d$ vector.  We define a new function $h(\beta) = 2g(\beta) - \one^T\Xb\beta$, and by the above inequality, $h(\beta)$ satisfies $|\|\Xb\beta\|_1 - h(\beta)| \leq 2\epsilon\|\Xb\beta\|_1$ for all $\beta \in \R^d$.  Therefore, $h(\beta)$ is an $\epsilon$-relative approximation to $\|\Xb\beta\|_1$ after adjusting for constants and solves the $\ell_1$-subspace sketch problem (see Definition 1.1 in \cite{li2021tight}). By Theorem 1.2 in \cite{li2021tight}, the data structure $h(\cdot)$ requires $\Omega\left(\frac{d}{\epsilon^2 \operatorname{polylog}(\epsilon^{-1})}\right)$ bits of space  if $d = \Omega(\log 1/\epsilon)$ and $n = \Omegatil\left(d\epsilon^{-2}\right)$. Therefore, we conclude that any data structure which approximates $\Rcal(\beta)$ to $\epsilon$-relative error for all $\beta \in \R^d$ must use $\Omegatil\left(\frac{d}{\epsilon^2}\right)$ bits in the worst case.

Next, we show that a data structure, $f(\cdot)$, which approximates logistic loss to relative error can be used to construct an approximation to the ReLu loss, $g(\cdot)$, by $g(\beta) = \frac{1}{t} \cdot f(t\cdot \beta)$ for large enough constant $t > 0$.  To show this, we first bound the approximation error of the logistic loss for a single point.  First, we derive the following inequality when $r>0$.
\begin{gather*}
    \frac{1}{t}\log(1 + e^{t\cdot r}) - r = \frac{1}{t}\left(\log(e^{rt}) + \log\left(\frac{1 + e^{rt}}{e^{rt}}\right)\right) - r
    = \frac{1}{t}\cdot\log\left(1 + \frac{1}{e^{rt}}\right) \leq \frac{1}{t \cdot e^{rt}}.
\end{gather*}
Therefore, if $\xb_i^T\beta > 0$, then $|\frac{1}{t}\log(1 + e^{t\cdot\xb_i^T\beta}) - \xb_i^T\beta| < \frac{1}{t\cdot e^{t\cdot\xb_i^T\beta}}$.  Next, we consider the case where $r \leq 0$. For the case $r < 0$ (in which case $\operatorname{ReLu}(r) = 0$), it directly follows that $\frac{1}{t}\log(1 + e^{t\cdot r})\leq \frac{e^{t\cdot r}}{t}$.  Therefore,
\begin{gather*}
    |\frac{1}{t}\cdot\log(1+e^{t\cdot r}) - \max\{0, r\}| \leq \frac{1}{t \cdot e^{t\cdot|r|}} \leq \frac{1}{t}.
\end{gather*}
We use this inequality to bound the difference in the transformed logistic loss and ReLu loss as follows.
\begin{gather}\label{eqn:logistic_to_ReLu}
    |\frac{1}{t}\cdot\Lcal(t\cdot\beta) - \Rcal(\beta)| = \Big|\sum_{i=1}^n \frac{1}{t}\log(1+e^{t\cdot\xb_i^T\beta}) - \max\{0, \xb_i^T\beta\}\Big|
    \leq \sum_{i=1}^n |\frac{1}{t} \log(1+e^{t\cdot\xb_i^T\beta}) - \max\{0, \xb_i^T\beta\}| \leq \frac{n}{t}.
\end{gather}
Therefore, if we set $t = \frac{n}{\epsilon \cdot \Rcal(\beta)}$, then $|\frac{1}{t} \Lcal(t\cdot \beta) - \Rcal(\beta)| \leq \epsilon\Rcal(\beta)$ for all $\beta \in \R^d$. However, we don't know $\Rcal(\beta)$ exactly, and furthermore, it is possible that $\Rcal(\beta) = 0$.  To handle these issues, we first observe that, for fixed dimensions $n$ and $d$, the set of possible input $(\Xb, \yb)$ is finite due to the bounded bit complexity of entries in $\Xb$.  Therefore, with bounded time complexity, we can compute $\Rcal_{\operatorname{min}}(\beta) = \inf_{\Xb, \yb : \Rcal(\beta; \Xb, \yb) > 0} \Rcal(\beta)$, that is, the minimum positive value of $\Rcal(\beta)$ over all possible input of a fixed dimension.  We then set $t = \frac{4n}{\epsilon \cdot \Rcal_{\operatorname{min}}(\beta)}$. By definition $\Rcal_{\min}(\beta)$ is the smallest possible non-zero value of $\Rcal(\beta)$, hence there are two possible cases 1) $\Rcal(\beta) \geq \Rcal_{\min}(\beta)$ and 2) $\Rcal(\beta) = 0$. First, we show that returning $\frac{1}{t}\Lcal(t\cdot\beta)$ gives an $\epsilon$-relative error approximation in the first case:
\begin{gather}\label{eq:relu_case_one}
    \Rcal(\beta) \geq \Rcal_{\min}(\beta) 
    \Rightarrow |\frac{1}{t} \Lcal(t\cdot \beta) - \Rcal(\beta)| \leq \frac{\epsilon}{4}\Rcal_{\min}(\beta) \leq \epsilon\Rcal(\beta).
\end{gather}
Next, we show that if we are in the second case, then $\frac{1}{t}\Lcal(t\cdot\beta)$ is sufficiently smaller than $\Rcal_{\min}(\beta)$ so that we can certify that $\Rcal(\beta) = 0$, in which case we can just return zero to get no error in approximating $\Rcal(\beta)$.
\begin{gather}\label{eq:relu_case_two}
    \Rcal(\beta) = 0
    \Rightarrow |\frac{1}{t} \Lcal(t\cdot \beta)| \leq \frac{\epsilon}{4}\Rcal_{\min}(\beta) 
    \Rightarrow |\frac{1}{t} \Lcal(t\cdot \beta) - \Rcal_{\min}(\beta)| > \frac{\epsilon}{4} \Rcal_{\min}(\beta)
\end{gather}

Above, we showed that $\Lcal(\beta)$ could be used to provide an $\epsilon$-relative error approximation to $\Rcal(\beta)$. Now, we show that if $f(\beta)$ is an $\epsilon$-relative error approximation of $\Lcal(\beta)$, then $g(\beta) = \frac{1}{t}\cdot f(t\cdot\beta)$ is a $3\epsilon$-relative error approximation to $\Rcal(\beta)$ for the value of $t$ defined above. First, we derive the following inequality using eqn. (\ref{eq:relu_case_one}) and the error guarantee of $f(\cdot)$.
\begin{gather*}
    \Big|\frac{1}{t} \Lcal(t \cdot \beta) - \frac{1}{t} f(t \cdot \beta)\Big|
    \leq \epsilon \cdot \frac{1}{t} \cdot \Lcal(t \cdot \beta)
    \leq \epsilon \cdot \left(\Rcal(\beta) + \frac{\epsilon}{4} \Rcal_{\min}(\beta)\right)
\end{gather*}
If $\Rcal(\beta) > 0$, then $\Rcal(\beta) \geq \Rcal_{\min}(\beta)$, and therefore we conclude $|\frac{1}{t} \Lcal(t \cdot \beta) - \frac{1}{t} f(t \cdot \beta)| \leq 2\epsilon\Rcal(\beta)$ from the previous equation.
We now use this result along with eqn. (\ref{eq:relu_case_one}) to prove that $g(\beta)$ is a $3\epsilon$-relative error approximation of $\Rcal(\beta)$ when $\Rcal(\beta) > 0$.
\begin{align*}
    |\frac{1}{t}\cdot f(t\cdot\beta) - \Rcal(\beta)|
    \leq |\frac{1}{t}\cdot f(t\cdot\beta) - \frac{1}{t}\cdot\Lcal(t\cdot \beta)| + |\frac{1}{t}\cdot \Lcal(t\cdot\beta) - \Rcal(\beta)| 
    \leq 3\epsilon \Rcal(\beta).
\end{align*}
Alternatively, if $\Rcal(\beta) = 0$, then:
\begin{gather*}
    \Big|\frac{1}{t} \Lcal(t \cdot \beta) - \frac{1}{t} f(t \cdot \beta)\Big|
    \leq \frac{\epsilon^2}{4}\Rcal_{\min}(\beta) 
    \quad\text{ and }\quad
    |\frac{1}{t} \Lcal(t\cdot \beta)| \leq \frac{\epsilon}{4}\Rcal_{\min}(\beta)\\
    \Rightarrow \Big| \frac{1}{t} f(t\cdot \beta) \Big| \leq \frac{\epsilon}{2}\Rcal_{\min} 
    \Rightarrow \Big|\frac{1}{t} f(t\cdot \beta) - \Rcal_{\min}(\beta) \Big| > \frac{\epsilon}{4}\Rcal(\beta).
\end{gather*}
Therefore, if $\frac{1}{t}f(t\cdot \beta) \leq \frac{\epsilon}{2}\Rcal_{\min}(\beta)$, then $\Rcal(\beta)$ must equal zero, and so we can return zero to get zero approximation error. After adjusting $\epsilon$ by a factor of three in the above proof, we conclude $f(\cdot)$ must use $\Omegatil(\frac{d}{\epsilon^2})$ bits of memory in the worst case.
\end{proof}



\textbf{Proof of Theorem \ref{thm:logistic_loss_upper}}
\begin{proof}

To simplify the notation, let $\sbb = \Xb\beta$ and $\db = (\Xb - \Xbtil)\beta$.  We can then write the difference in the log loss as:
\begin{flalign*}
    |\Lcal(\beta; \Xb) - \Lcal(\beta, \Xbtil)|
    &= \left(\sum_{i=1}^n \log\left( 1 + e^{\xb_i^T\beta}\right) + \frac{\lambda}{2} \|\beta\|_2^2\right) - \left(\sum_{i=1}^n \log\left( 1 + e^{\xbtil_i^T\beta}\right) + \frac{\lambda}{2} \|\beta\|_2^2\right) \\
    &=\left|\sum_{i=1}^n \log \left(\frac{1 + e^{\sbb_i}}{1 + e^{\sbb_i + \db_i}}  \right)\right| \\
    &\leq \left|\sum_{i=1}^n \log \left(\frac{1 + e^{\sbb_i}}{1 + e^{\sbb_i - |\db_i|}}  \right)\right| \\
    &= \left|\sum_{i=1}^n \log \left(\frac{1 + e^{\sbb_i}}{1 + e^{-|\db_i|}e^{\sbb_i}}  \right)\right| \\
    &\leq \left|\sum_{i=1}^n \log \left(\frac{1}{e^{-|\db_i|}}\frac{1 + e^{\sbb_i}}{1 + e^{\sbb_i}}  \right)\right| \\
    &= \sum_{i=1}^n  |\db_i| = \|\db\|_1.
\end{flalign*}

Therefore, we can conclude that $|\Lcal(\beta; \Xb) - \Lcal(\beta; \Xbtil)| \leq \|\db\|_1 \leq \sqrt{n}\|\db\|_2 \leq \sqrt{n} \|\Xbtil - \Xb\|_2 \|\beta\|_2$.


\end{proof}


\textbf{Proof of Theorem \ref{thm:logistic_loss_lower}}
\begin{proof}
To prove the theorems statement, we first consider the case of square matrices $(d = n)$.  In particular, first consider the case where $d = n = 1$, where $\Xb = [x]$ and $\Xbtil = [x + s]$, in which case $\|\Xb - \Xbtil\|_2 = s$.  Then,
\begin{gather*}
     \lim_{x \rightarrow \infty} \Lcal([1]; [x+s]) - \Lcal([1]; [x]) = \lim_{x \rightarrow \infty} \log(1+e^{x + s}) - \log(1 + e^x)  = s
\end{gather*}
Which shows that for $\beta = [1]$ and $x$ with large enough magnitude $\Lcal(\beta; \Xbtil) - \Lcal(\beta; \Xb) = (1-\delta)\|\Xb - \Xbtil\|_2$. Next, let $\Xb = x \cdot \Ib_n$, $\Xbtil = (x+s) \cdot \Ib_n$, and $\beta = \one_n$.  Then for all $i \in [n]$, $\xb_i^T\beta = x$ and $\xb_i^T\beta = x+s$.  Therefore, 
\begin{gather*}
     \lim_{x \rightarrow \infty} \Lcal(\beta; \Xbtil) - \Lcal(\beta; \Xb) = \lim_{x \rightarrow \infty} \sum_{i=1}^n \left[\log(1+e^{x + s}) - \log(1 + e^x)\right] = sn.
\end{gather*}
Since $\|\Xb - \Xbtil\|_2 = \|s \cdot \Ib\|_2 = s$.  $\|\beta\|_2 = \sqrt{n}$,
\begin{gather*}
    \lim_{x \rightarrow \infty} \Lcal(\beta; \Xbtil) - \Lcal(\beta; \Xb)
    = sn = \sqrt{n} \|\Xb - \Xbtil\|_2 \|\beta\|_2
\end{gather*}
Hence, we conclude the statement of the theorem for the case where $d = n$. To conclude the case for $d \geq n$, note that $\sqrt{n}\|\Xb - \Xbbar\|_2\|\beta\|_2$ does not change if we extend $\Xb$ and $\Xbbar$ with columns of zeroes and extend $\beta$ with entries of zero until $\Xb,\Xbbar \in \R^{n \times d}$ and $\R^d$.  This procedure also does not change the loss at $\beta$, hence we conclude the statement of the theorem.
\end{proof}

