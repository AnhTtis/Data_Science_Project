\section{Introduction}

Logistic regression is an indispensable tool in data analysis, dating back to at least the early 19th century. It was initially used to make predictions in social science and chemistry applications~\cite{PFV1838,PFV1845,Cramer2003}. Over the past 200 years it has been applied to all data-driven scientific domains, from economics and the social sciences to physics, medicine, and biology. At a high level, the (binary) logistic regression model is a statistical abstraction that models the probability of one of two alternatives or classes by expressing the log-odds (the logarithm of the odds) for the class as a linear combination of one or more predictor variables. 

Formally, consider the following regularized regression problem:
\begin{align}
    \beta_d &= \argmin_{\beta \in \R^d}\Lcal(\beta)  
    = \argmin_{\beta \in \R^d} \sum_{i=1}^n \log(1 + e^{-\yb_i\xb_i^T\beta}) + \frac{\lambda}{2}\|\beta\|_2^2,\label{eqn:blrr}
\end{align}
where $\Xb \in \R^{n \times d}$ is the data matrix ($n$ points in $\R^d$, with $\xb_i^T$ being the rows of $\Xb$); $\yb \in \{-1,1\}^n$ is the vector of labels whose entries are the $\yb_i$; and $\lambda > 0$ is the regularization parameter.

Over the past 25 years, matrix sketching and Randomized Linear Algebra~\cite{Drineas2016,Drineas2017,Halko2011,KannanV17,Martinsson2020} have advocated the use of sketching and sampling techniques to \textit{reduce} the dimensionality of the input matrix $\Xb$ (and the response vector $\yb$) in a variety of optimization problems in order to reduce running times and improve interpretability with a bounded accuracy loss. For example, if one considers the much more common setting of \textit{linear} regression (least-squares or $\ell_2$ regression and, more generally, $\ell_p$ regression), there has been an extensive body of work studying the effect of matrix sketching and sampling for \textit{preconditioning, coreset construction, feature selection, dimensionality reduction, etc.} for (regularized or not) under- and over-constrained regression problems~\cite{avron2010blendenpik,chowdhury2018iterative,clarkson2009numerical,DMM06,Drineas2011,Sarlos2006,Clarkson2013a,Clarkson2013b,Clarkson2016,Avron2016,Pilanci2014,Pilanci2016,Derezinski2017,Derezinski2021}. At a high level, matrix sketching and sampling can be used to speed up linear regression in theory and in practice, either by preconditioning the input and then applying standard solvers or by constructing and solving a ``smaller'' regression problem, with little loss in accuracy. Additionally, from an interpretability perspective, we know how to construct coresets or select/extract features, thus reducing the dimensionality of the constraint space or the feature space, so that solving the ``induced'' problem results in small accuracy loss. In many cases, tight upper and lower bounds are known for the aforementioned operations and, indeed, solving linear regression problems has been a massive success story for matrix sketching and Randomized Linear Algebra, both in theory and in practice.

Despite the central importance of logistic regression in statistics and machine learning, and unlike linear regression, relatively little is known about how the method behaves when matrix sketching and sampling are applied to its input.~\citet{munteanu2018coresets,pmlr-v139-munteanu21a} initiated the study of \textit{coresets} for logistic regression, and~\citet{mai2021coresets} provided the current state-of-the-art bounds for this problem. In these works, coreset constructions reduce the number of training points in the logistic regression problem, while guaranteeing that the reduced problem satisfies provable bounds on the \textit{in-sample} logistic loss. The reduction in the number of points is typically performed via sampling algorithms, where a subset of the original points is selected with respect to carefully constructed probability distributions, such as the Lewis scores. Coreset construction methods can be thought of as \textit{sketching} the input matrix $\Xb$ from the left, using a sketching matrix of canonical vectors, where each vector indicates an input point (row of $\Xb$) to be included in the coreset. It is worth noting that~\citet{pmlr-v139-munteanu21a} even considered oblivious sketching methods to construct coresets that are not a subset of the original set of points, but consist of linear combinations of the original points, thus moving beyond the canonical vector paradigm for the sketching matrix.

Even less is known for reducing the dimensionality of the $d$-dimensional feature space of the logistic regression problem of eqn.~(\ref{eqn:blrr}). To the best of our knowledge, only feature selection methods have been considered in prior work~\cite{Lozano2011,elenberg2018restricted} (see Section~\ref{sxn:related} for details). Again, feature selection methods can be thought of as sketching the input matrix $\Xb$ from the right, using a sketching matrix of canonical vectors, where each vector indicates a feature (column of $\Xb$) to be selected. Other dimensionality reduction and feature extraction methods, which use a smaller subset of \textit{artificial} instead of \textit{actual} features to operate on, have not been studied at all in prior work.

Our work makes progress in both fronts, resolving open problems from prior work and presenting novel bounds for the complexity of coreset construction methods. Importantly, we initiate the study of \textit{forward error bounds} for dimensionality reduction methods (including both feature selection and feature extraction) for the feature space of logistic regression. Our work presents tight bounds for \textit{linear} dimensionality reduction methods for logistic regression. Interestingly, our forward error bounds are also applicable to the much more general setting of Generalized Linear Models (GLMs). 

\subsection{Our contributions}\label{section:contributions}

\vspace{0.04in}\noindent\textbf{Coreset construction (row sketching).} In Section \ref{sxn:logistic_loss}, we consider the problem of approximating the logistic loss function $\Lcal(\beta)$ by a different function $\Lcaltil(\beta)$. This is the setting of prior work on coresets for logistic regression~\cite{munteanu2018coresets, mai2021coresets}, where the function $\Lcaltil(\beta)$ is evaluated, for example, only on a subset of the input points $\xb_i$ (the coreset). Just like prior work, we are interested in bounding the approximation error 
%
$|\Lcal(\beta) - \Lcaltil(\beta)|,$
%
as a function of $\beta \in \R^d$ and we present a number of novel results in this context.

First, we lower bound the worst-case space complexity of \emph{any} data structure that approximates logistic loss to $\epsilon$-relative error. Coresets for logistic regression introduced in prior work are special cases of a low-space data structure giving an $\epsilon$-relative error approximation to logistic loss. Our bound implies that the construction given by \citet{mai2021coresets} has optimal size up to a $\Ocaltil(d\cdot\mu_\yb(\Xb)^2)$ factor, where $\mu_\yb(\Xb)$ is a data-dependent complexity measure (Definition~\ref{def:complexity_measure}). Bridging this gap would imply that coreset constructions are actually optimal among all possible data structures that attempt to reduce the number of points (or even the number of bits used to sketch the input) that logistic regression operates on. Our bound is a first step towards this objective.

Our second contribution is an efficient linear programming formulation for computing the data complexity measure $\mu_\yb(\Xb)$ of Definition~\ref{def:complexity_measure}, which was previously conjectured to be hard to compute by~\citet{munteanu2018coresets}. This quantity is a critical component of existing theoretical results bounding the necessary size of a coreset for logistic regression. Therefore, an efficient and accurate method to evaluate this complexity measure was a critical missing piece and paves the road towards an improved empirical validation of coreset constructions for logistic regression.

Finally, we show that replacing the input matrix $\Xb$ by any other matrix $\tilde{\Xb}$ such that the two-norm\footnote{Recall that the two-norm, also called the spectral norm, of a matrix $\Xb \in \mathbb{R}^{n \times d}$ is $\|\Xb\|_2 = \max_{\xb \in \mathbb{R}^d,\ \|\xb\|_2=1} \|\Xb\xb\|_2 = \max_{\yb \in \mathbb{R}^n,\ \|\yb\|_2=1} \|\yb^T\Xb\|_2$.} error
%
$\|\Xb - \tilde{\Xb}\|_2$
%
is bounded can be used to obtain an additive error approximation to the logistic loss. We also prove that this additive error guarantee is tight in the worst case. Low-rank approximations to $\Xb$ are often used instead of $\Xb$ in logistic regression, either to reduce the intrinsic dimensionality\footnote{Reducing the intrinsic dimensionality (rank) of the input results in storage savings, since one can store only a few singular values and singular vectors instead of the whole matrix. Also, it could result in improved running times, since many numerical algorithms take advantage of low-rank matrices.} of the input or to denoise it. Such low-rank approximations do result in a bounded two-norm approximation: It is well-known that setting $\tilde{\Xb} = \Xb_k$, where $\Xb_k$ is the best rank-$k$ approximation to $\Xb$ computed, for example, via the Singular Value Decomposition (SVD)\footnote{Randomized Linear Algebra algorithms can be used to approximate the best low-rank approximation to $\Xb$ with bounded accuracy loss~\cite{Halko2011,Drineas2017,Drineas2018,kishore2017literature}.}, satisfies $\|\Xb - \Xb_k\|_2 =\sigma_{k+1}(\Xb)$\footnote{$\sigma_{k+1}(\Xb)$ is the $(k+1)$st singular value of $\Xb$.}. Somewhat surprisingly, such results were not known for logistic regression prior to our work.

\vspace{0.04in}\noindent\textbf{Reducing the feature-space dimensionality (column sketching).} 
%
Our most fundamental contribution is the development of novel, tight bounds for sketching the feature space (columns) of regularized logistic regression problems. More specifically, given the data matrix $\Xb \in \R^{n \times d}$ of eqn.~(\ref{eqn:blrr}), we study the error induced by projecting the columns (features) of $\Xb$ to an arbitrary $k$-dimensional subspace (with $k \ll d$) spanned by the columns of a $k \times d$ matrix, $\Pb_k$. Formally, consider the following, \textit{sketched} regression problem:
%
\begin{align}
    \beta_k &= \argmin_{\beta \in \R^k} \Lcal_k(\beta) 
    = \argmin_{\beta \in \R^k} \sum_{i=1}^n \log(1 + e^{-\yb_i\xb_i^T\Pb_k\beta}) + \frac{\lambda}{2} \|\beta\|_2^2,\label{eqn:sketchedregression1}
\end{align}
%
where $\Pb_k \in \R^{d \times k}$, $k \ll d$, has orthonormal columns; all other quantities are as in eqn.~(\ref{eqn:blrr}). This family of dimensionality reducing transformations is relevant to understanding several important topics for sketched logistic regression:
%
\begin{itemize}
    %
    \item Any feature selection method for logistic regression problems can be modelled as a matrix $\Pb_k$ whose columns are standard basis vectors. This includes popular methods that interpret the output of logistic regression by keeping only the $k$ largest magnitude coefficients of the learned parameter vector $\beta$~\cite{Wu2009,Held2016,Prive2019} and attempt to interpret the associated features.
    %
    \item Dimensionality reduction methods for logistic regression that project the feature matrix on its top $k$ right singular vectors, thus using those singular vectors as eigenfeatures. Empirical evidence suggests that reducing the features of a data set to its top principal components can greatly reduce the computational and memory burden of solving large-scale logistic regression problems with limited reduction in accuracy. This can be modeled in our framework as the case where $\Pb_k$ are the top $k$ right singular vectors of $\Xb$. 
\end{itemize}
%
As we shall see in Section~\ref{sxn:logistic_loss} and Theorem~\ref{thm:logistic_loss_lower}, there are barriers to getting relative error guarantees on the logistic loss when sketching the high-dimensional feature space of a data set, thus effectively reducing the rank of the input matrix. Motivated by such lower bounds, we consider the \emph{forward error} when sketching the feature space, namely the two-norm difference between the optimal parameters of the original and sketched problems: 
%
\begin{equation}\label{eqn:pd111}
%
\|\Pb_k\beta_k - \beta_d\|_2^2.
%
\end{equation}
%
We prove the following complementary upper and lower bounds on the forward error of eqn.~(\ref{eqn:pd111}). The bounds hold for \emph{any} valid tuple $(\Xb, \yb, \Pb_k, \lambda)$:
\begin{align*}
     \frac{4}{ \|\Xb\|_2^2 + \lambda} \cdot \Phi(\Xb,\yb,\Pb_k)
     \leq \|\Pb_k\beta_k - \beta_d\|_2^2
     \leq\frac{2}{\lambda}\Phi(\Xb,\yb,\Pb_k).
\end{align*}
%
In the above, $\Phi(\Xb,\yb,\Pb_k)$ is the following function\footnote{We omit $\beta_k$ and $\beta_d$ from the definition of $\Phi(\cdot)$ since they can be computed given the other inputs.}:
%
$$\Phi(\Xb,\yb,\Pb_k) = \beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb \wb(\Pb_k\beta_k),$$
%
where $\Db_\yb \in \R^{n \times n}$ denotes the diagonal matrix whose entries are $\yb_i$; $\wb(\beta) \in \R^n$ has entries $\wb(\beta)_i = \sigma(-\yb_i \xb_i^T\beta)$, for all $i=1\ldots n$; and $\sigma(u)$ denotes the logistic function $\sigma(u) = \nicefrac{1}{1 + e^{-u}}$.  
%
Note that our bounds are \textit{tight up to a constant factor} when $\|\Xb\|_2^2 \leq C\lambda$ for some constant $C > 0$.

To better understand the above bound, one could interpret $\wb(\Pb_k\beta_k)$ as a vector describing the distribution of the $i$-th label under the model given by $\Pb_k\beta_k$. The norm can be bounded naively by $\sqrt{n}$. Additional structural assumptions could restrict the norm further. Under generative model assumptions, one could bound the term $\sigma(\xb_i^T \Pb_k \Pb_k^T \beta_d)$ to probabilistically bound the entries of $\wb(\Pb_k\beta_k)$, and hence its norm, more sharply.

It is worth noting that forward error bounds are a natural accuracy metric in numerical analysis and theoretical computer science. For example, for linear regression, forward error bounds in the context of sensitivity analysis, to bound the accuracy of iterative approximation algorithms, or to analyze randomized linear regression algorithms, have been a topic of intense research for many years (see the works of~\citet{Higham2002,Woodruff2014,Drineas2017} and references therein). In the context of logistic regression, forward error bounds are optimization-based guarantees that are considered to be a precursor for generalization errors which typically require additional statistical assumptions ~\cite{loh2015regularized,negahban2012unified,elenberg2018restricted}. We do emphasize that related prior work also focused on error metrics such as in-sample logistic loss~\cite{munteanu2018coresets,mai2021coresets}, that have no known connection to the generalization error for logistic regression.


\vspace{0.04in}\noindent\textbf{Extensions.} We generalize our results in three different ways.  In Appendix~\ref{section:generative_model}, we show that generative assumptions on the data set $(\Xb, \yb)$ from prior work are enough to prove our forward error bounds \emph{without} any explicit regularization (i.e. with $\lambda=0$). In Appendix~ \ref{section:glm}, we show that our upper and lower forward error bounds generalize to a wide class of generalized linear models (GLMs) including linear and multinomial regression.

Finally, in Appendix~\ref{section:information_theoretic_bound}, we show that our upper bound can be naturally interpreted from an information theoretic perspective under the typical modeling assumptions of logistic regression. More specifically, given the matrix of $n$ points $\Xb$ and the logistic regression parameter $\beta$, let $\ybtil \in\{-1, 1\}^n$ be a random binary vector where the distribution of the $i$-th coordinate is the distribution given by the linear-log odds model parameterized by $\beta$. If we additionally have a true label vector $\yb\in\{-1,1\}^n$, then we can define a new random vector $\ybhat \in \{-1,1\}^n$ such that $\ybhat = \yb \cdot \ybtil$, where the multiplication is entry-wise.  Intuitively, $\PP(\ybhat_i = 1)$ is the probability that a label drawn from the linear log-odds model agrees with the true label, and this measures how well the model fits the true labels. We prove that if $\Dcal_p$ is the distribution of $\ybhat$ induced by $\beta = (\Ib - \Pb_k\Pb_k^T)\beta_d$ and $\Dcal_q$ is the distribution of $\ybhat$ induced by $\beta = \Pb_k\beta_k$, then 
%
$$\|\Pb_k\beta_k - \beta_d\|_2^2 \leq \frac{2}{\lambda} H(\Dcal_p, \Dcal_q),$$ 
%
where $H$ denotes the cross-entropy between the two distributions.

\subsection{Related Work}\label{sxn:related}

A line of research that motivated our work focused on the development of coresets for logistic regression. This was initiated by~\citet{munteanu2018coresets}, and the current state-of-the-art bounds for coreset construction for logistic regression appeared in~\cite{mai2021coresets}.  We already discussed the contributions of these two papers earlier in the introduction. 
Another line of research that is relevant to our work is that of feature selection for logistic regression. In this setup, the goal is to select $k$ features out of $d$, which can be construed as restricting the sketching matrix $\mathbf{P}_k$ to specifically be a selection matrix. Towards this goal,~\citet{elenberg2018restricted} provide approximation and recovery guarantees for performance of greedy selection of features vis-{\`a}-vis the best possible \emph{sparse} solution for \emph{general} functions. On the other hand, our results quantify recovery-like guarantees for the parameter $\beta$ for \emph{any} given $\mathbf{P}_k$ and \emph{any} possible $\beta_d$ but specifically for logistic regression. Since we generalize in different ways, our bounds are not directly comparable with theirs. Even if we specialize their results for logistic regression and choose our $\Pb_k$ to select the features selected by the greedy algorithm for the special case when $\beta_d$ is inherently $k$-sparse, our Theorem~\ref{thm:forward_error_upper_bound} still is incomparable to~\citet{elenberg2018restricted}[Theorem 6] because the two theorems upper bound slightly different quantities.~\citet{Lozano2011} also provide guarantees for logistic regression which are similar to those of~\citet{elenberg2018restricted} but under group-sparsity constraints, and are incomparable to our results. 
