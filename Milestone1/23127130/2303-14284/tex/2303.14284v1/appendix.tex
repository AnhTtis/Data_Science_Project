\section{Generative Model} \label{section:generative_model}

We remind the reader that, in the absence of some form of regularization, the worst case forward error for a dimensionally-reduced logistic problem could be unbounded. However, under mild assumptions on the data generating process, we show in this section that this phenomenon will not occur even without any form of regularization. Towards that end, the work of~\citet{loh2015regularized} demonstrated that a simple data generating model is sufficient to ensure that the logistic loss function satisfies a restricted strong convexity condition  with high probability.  This result was later used by~\citet{elenberg2018restricted} to guarantee that the performance of their feature extraction method did not deteriorate in the absence of regularization.  

We now describe the model setup and prove our results. Let the observed samples (the rows of the data matrix $\Xb \in \R^{n \times d}$) be drawn in i.i.d. trials from a zero-mean sub-gaussian distribution with parameter $\sigma_x$ and covariance matrix $\Sigmab$.  Furthermore, let the corresponding labels $\yb_i$ be generated according to the linear log-odds model, by fixing $\beta^*\in \R^d$ to be the true parameter of the logistic regression model and setting $\yb_i$ to one with probability $\nicefrac{1}{1+e^{-\xb_i^T\beta^*}}$ and to $-1$ otherwise.

Let $\beta_1$ and $\beta_2$ be vectors in $\R^d$ and let $\mathbb{B}_p(R) = \{\xb \in \R^n : \|\xb\|_p \leq R\}$. Under this data generating model,~\cite{loh2015regularized} showed that the \emph{Taylor error} around the vector $\beta_2$ in the direction $\beta_1 - \beta_2$, defined as\footnote{We use the notation $\langle \xb,\yb\rangle$ to denote the inner product $\xb^T\yb$ between the two vectors.}
%
\begin{align*}
\Tcal(\beta_1, \beta_2) = \Lcal(\beta_1) -
\Lcal(\beta_2) - \langle\nabla \Lcal(\beta_2), \beta_1 -
  \beta_2\rangle,
\end{align*}
%
is upper and lower bounded. We note that the following proposition holds for GLMs beyond logistic regression, where $\alpha_u$ and $\psi(\cdot)$ are defined as in Appendix~\ref{section:glm}; in the special case of logistic regression, $\alpha_u \leq 1$.
%
\begin{proposition}\label{prop:wainwright} (Proposition 1 in \cite{loh2015regularized})
There exists a constant $\alpha_\ell > 0$, depending only on the GLM and the parameters $(\sigma_x^2, \Sigmab)$, such that for all vectors $\beta_2 \in \mathbb{B}_2(3) \cap \mathbb{B}_1(R)$, we have
\begin{align*}
	\Tcal(\beta_1, \beta_2) \geq 
	 \frac{\alpha_\ell}{2} \|\beta_1 - \beta_2\|_2^2 - \frac{c^2
           \sigma_x^2}{2\alpha_\ell} \frac{\log d}{n} \|\beta_1 - \beta_2\|_1^2,
         \quad \text{for all $\|\beta_1 - \beta_2\|_2 \leq 3$,}  %
\end{align*}
%
with probability at least $1 - c_1 \exp (-c_2 n)$. With the bound
$\|\psi''\|_\infty \leq \alpha_u$, we also have
\begin{align}
\Tcal(\beta_1, \beta_2) & \leq \alpha_u \lambda_{max}(\Sigmab) \;
\left( \frac{3}{2} \|\beta_1 - \beta_2\|_2^2 + \frac{\log d}{n}
\|\beta_1 - \beta_2\|_1^2\right), \quad \mbox{for all $\beta_1, \beta_2 \in
  \R^d$},
\end{align}
with probability at least $1 - c_1 \exp(-c_2 n)$, where $c_1$ and $c_2$ are fixed constants.
\end{proposition}
%
We can use the above bound in the same way as we previously used strong convexity from the regularization parameter to bound the forward error of the dimensionally-reduced problem. Again, let $\vb$ be defined as the unit vector pointing from $\beta_d$ to $\Pb_k\beta_k$, i.e.,
%
\begin{align}
\vb = \frac{1}{\|\Pb_k\beta_k - \beta_d\|_2} \cdot \Pb_k\beta_k - \beta_d.
\end{align}
%
Let $\beta_2 = \Pb_k\beta_k$ and $\beta_1 = \Pb_k\beta_k - t\cdot \vb$, for some $t>0$. Using Proposition~\ref{prop:wainwright}, we get
%
\begin{gather*}
    \Lcal(\Pb_k\beta_k - t\cdot\vb) - \Lcal(\Pb_k\beta_k) - \langle\nabla \Lcal(\Pb_k\beta_k), -t\cdot\vb \rangle
    \geq \frac{\alpha_\ell}{2} \|t\cdot\vb\|_2^2 - \frac{c^2\sigma_x^2}{2\alpha_\ell} \frac{\log d}{n} \|t\cdot\vb\|_1^2.
\end{gather*}
%
We now solve for $t^*$, the minimum value of $t > 0$ that satisfies
%
\begin{gather*}
    \langle\nabla \Lcal(\Pb_k\beta_k), t^*\cdot\vb \rangle 
    =\frac{\alpha_\ell}{2} \|t^*\cdot\vb\|_2^2 - \frac{c^2\sigma_x^2}{2\alpha_\ell} \frac{\log d}{n} \|t^*\cdot\vb\|_1^2,
\end{gather*}
%
as this guarantees $\Lcal(\Pb_k\beta_k - t^*\cdot\vb) - \Lcal(\Pb_k\beta_k) \geq 0$.  We next substitute in the simplified gradient (without regularization) from eqn.~(\ref{eqn:pd1}).  Recall that $\Xbbar = -\Db_\yb\Xb$ and $[\wbbar(\beta)]_i = \sigma(\xbbar_i^T\beta)$ to get
%
\begin{gather*}
    t^*\cdot\vb^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k)
    = \frac{\alpha_\ell}{2} \|t^*\cdot\vb\|_2^2 - \frac{c^2\sigma_x^2}{2\alpha_\ell} \frac{\log d}{n} \|t^*\cdot\vb\|_1^2\\
    \Rightarrow 
     \frac{t^*}{\|\Pb_k\beta_k - \beta_d\|_2} \cdot (-\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k))
    = \frac{\alpha_\ell}{2} t^{*2}\cdot\|\vb\|_2^2 - \frac{c^2\sigma_x^2}{2\alpha_\ell} \frac{\log d}{n} t^{*2} \cdot \|\vb\|_1^2.
\end{gather*}
%
Re-arranging terms, we get:
%
\begin{gather*}
\|\Pb_k\beta_k - \beta_d\|_2 \cdot t^*=
    \left(\frac{\alpha_\ell}{2}  - \frac{c^2\sigma_x^2}{2\alpha_\ell} \frac{\log d}{n}  \frac{\|\Pb_k\beta_k - \beta_d\|_1^2}{\|\Pb_k\beta_k - \beta_d\|_2^2}\right)^{-1} \cdot   (-\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k)).
\end{gather*}
%
We again use that $\|\Pb_k\beta_k - \beta_d\|_2 \leq t^*$, since the loss decreases monotonically as $\beta$ goes from $\Pb_k\beta_k$  to $\beta_d$, to get:
%
\begin{gather*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 \leq
    \left(\frac{\alpha_\ell}{2}  - \frac{c^2\sigma_x^2}{2\alpha_\ell} \frac{\log d}{n}  \frac{\|\Pb_k\beta_k - \beta_d\|_1^2}{\|\Pb_k\beta_k - \beta_d\|_2^2}\right)^{-1} \cdot  (-\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k)).
\end{gather*}
%
We note that the ratio, $$\frac{\|\Pb_k\beta_k - \beta_d\|_1}{\|\Pb_k\beta_k - \beta_d\|_2,}$$ is upper bounded by the so-called \textit{subspace compatibility constant} of the range of $\Pb_k$, as defined by~\citet{negahban2012unified}.  For large enough $n$, relative to the subspace compatibility factor, there exists a universal constant $\tau > 0$ such that the following holds:
%
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\leq \frac{-1}{\tau}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k) \\
    &= \frac{1}{\tau}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb \wb(\Pb_k\beta_k).
\end{align*}
%
Note that the above upper bound holds without any regularization, i.e., $\lambda = 0$.  

We now proceed to provide a lower bound for the forward error under the aforementioned generative model. Using Proposition~\ref{prop:wainwright}, we get the following upper bound on $\Tcal(\Pb_k\beta_k - t\cdot\vb, \Pb_k\beta_k)$; again note that for the special case of logistic regression $\alpha_u \leq 1$:
%
\begin{align*}
    \Lcal(\Pb_k\beta_k - t\cdot\vb) - \Lcal(\Pb_k\beta_k) - \langle\nabla \Lcal(\Pb_k\beta_k), -t\cdot\vb \rangle
    &\leq \lambda_{max}(\Sigmab) \;
    \left( \frac{3}{2} \|t\cdot \vb\|_2^2 + \frac{\log d}{n} \|t\cdot \vb\|_2^2\right) \\
    &= t^2 \cdot \lambda_{max}(\Sigmab)\left(\frac{3}{2} + \frac{\log d}{n}\right).
\end{align*}
%
Moving the gradient term to the right side and setting $\alpha = 2\lambda_{max}(\Sigmab)\left(\frac{3}{2} + \frac{\log d}{n}\right)$ results in a formula with the same form as eqn.~(\ref{eq:intermediate_upper}):
%
\begin{gather*}
    \Lcal_d(\Pb_k\beta_k - t \cdot \vb) - \Lcal_d(\Pb_k\beta_k)
    \leq t\cdot (-\vb^T\nabla \Lcal_d(\Pb_k\beta)) + \frac{t^2 \cdot \alpha}{2}, \quad
    \text{for all }t \in [0,1].
\end{gather*}
%
From here on, we can follow the remainder of the proof in Section~\ref{sxn:forward_lower}, starting with eqn.~(\ref{eq:intermediate_upper}) and using $\alpha = 2\lambda_{max}(\Sigmab)\left(\frac{3}{2} + \frac{\log d}{n}\right)$. We eventually conclude that:
%
\begin{gather*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    \geq \left(2\lambda_{max}(\Sigmab)\left(\frac{3}{2} + \frac{\log d}{n}\right)\right)^{-1} \cdot (- \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xbbar^T  \wbbar(\Pb_k\beta_k)).
\end{gather*}
%
For $n \geq 2\log d$, the above equation simplifies to:
%
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\geq \frac{-1}{4\lambda_{max}(\Sigmab)} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xbbar^T  \wbbar(\Pb_k\beta_k) \\
    &= \frac{1}{4\lambda_{max}(\Sigmab)} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb  \wb(\Pb_k\beta_k).
\end{align*}
%
We are now ready to summarize our results for the generative model of~\cite{loh2015regularized} in the following lemma, which provides (almost) tight upper and lower bounds for the forward error for non-reguralized logistic regression.
%
\begin{lemma}\label{lemma:appA2}
If $\Pb_k\beta_k \in \mathbb{B}_2(3) \cap \mathbb{B}_1(R)$ and $\|\Pb_k\beta_k - \beta_d\|_2 \leq 3$, then for $n \geq \Ocal(\log d)$, there exists a constant, $\tau$, depending only on $(\sigma_x^2, \Sigmab)$ and the subspace compatibility constant of the range of $\Pb_k$\footnote{The $\ell_1$-$\ell_2$ \textit{subspace compatibility constant} defined by~\cite{negahban2012unified} for a subspace $\Mcal$ is $\Psi(\Mcal) = \sup_{\ub \in \Mcal} \frac{\|\ub\|_1}{\|\ub\|_2}$.}
%
such that
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\leq \frac{1}{\tau} \cdot \beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb \wb(\Pb_k\beta_k), \quad \text{and} \\
    %
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\geq \frac{1}{4\lambda_{max}(\Sigmab)} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xbbar^T\Db_\yb  \wb(\Pb_k\beta_k),
\end{align*}
hold with probability at least $1- 2c_1\exp(-c_2 n)$, where $c_1$ and $c_2$ are fixed constants.
\end{lemma}
%
Note that the upper and lower bounds are tight up to constants and a dependency on the largest singular value of the covariance matrix $\Sigmab$.

\section{Extension to GLMs}\label{section:glm}

\subsection{Forward error bound for GLMs}\label{sxn:glm_upper}

In this section, we show how our results can be generalized to hold for Generalized Linear Models (GLMs) beyond logistic regression.  We start by writing down the general formulas for the conditional distribution and prediction of an arbitrary GLM with linear parameter $\beta \in \R^d$, scale parameter $\sigma >0$, and cumulant function $\psi$. We follow the lines of~\citet{mccullagh2019generalized} to get:
%
\begin{align*}
    \PP(\yb_i | \xb_i, \beta, \sigma) &= \exp\left\{\frac{\yb_i\xb_i^T\beta - \psi(\xb_i^T\beta)}{c(\sigma)}\right\}, \quad \text{and}\\
    %
    \EE[\yb_i | \xb_i, \beta, \sigma] &= \psi'(\xb_i^T\beta).
\end{align*}
%
The empirical (un-normalized) log-loss function is given by:
%
\begin{align*}
    \Lcal_\text{log-loss}(\beta) = \sum_{i=1}^n \frac{-1}{c(\sigma)}(\yb_i\xb_i^T\beta - \psi(\xb_i^T\beta)).
\end{align*}
%
We can now define the original and dimensionally-reduced loss functions with $\ell_2^2$-regularization as follows: set $c(\sigma) = 1$ (without loss of generality):
%
\begin{align*}
    \beta_d &= \argmin_{\beta \in \R^d} \Lcal_d(\beta) = \argmin_{\beta \in \R^d} \sum_{i=1}^n -(\yb_i\xb_i^T\beta - \psi(\xb_i^T\beta)) + \frac{\lambda}{2}\|\beta\|_2^2,\quad \text{and} \\
    \beta_k &= \argmin_{\beta \in \R^k} \Lcal_k(\beta) = \argmin_{\beta \in \R^k} \sum_{i=1}^n -(\yb_i\xb_i^T\Pb_k\beta - \psi(\xb_i^T\Pb_k\beta)) + \frac{\mu}{2} \|\beta\|_2^2.
\end{align*}
%
We note that the above two equations are analogs of eqns.~(\ref{eqn:blrr}) and~(\ref{eqn:sketchedregression1}). The corresponding gradients of the above loss functions are given by
%
\begin{align*}
    \nabla \Lcal_d(\beta) &= \sum_{i=1}^n (\psi'(\xb_i^T\beta) - \yb_i)\xb_i + \lambda\cdot\beta, \quad \text{and}\\
    \nabla \Lcal_k(\beta) &= \sum_{i=1}^n (\psi'(\xb_i^T\Pb_k\beta) - \yb_i)\Pb_k^T\xb_i + \mu\cdot\beta.
\end{align*}
%
We now define $\wbbar(\beta) \in \R^n$ such that $[\wb(\beta)]_i = (\psi'(\xb_i^T\beta) - \yb_i)$.  We can now rewrite the above two gradients using matrix notation as
%
\begin{align*}
    \nabla \Lcal_d(\beta) &=\Xb^T\wbbar(\beta) + \lambda\cdot\beta, \quad \text{and}\\
    \nabla \Lcal_k(\beta) &= \Pb_k^T\Xb^T\wbbar(\Pb_k\beta) + \mu\cdot\beta.
\end{align*}
%
Notice that the above two formulas for the gradients are equivalent to eqn.~(\ref{eqn:grall}) except for the definition of $\wbbar(\cdot)$.  From here on, we can closely follow the remainder of the proof of Theorem~\ref{thm:forward_error_upper_bound} in Section~\ref{sxn:forward_upper} to get an analogous upper bound on the forward error. If we let $\wb(\beta) = -1\cdot \wbbar(\beta)$ and $\mu = \lambda$, then the final error bound is
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\leq \frac{2}{\lambda}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T \wb(\Pb_k\beta_k).
\end{align*}


\subsection{Lower bound for GLMs}

To extend our lower bounds to GLMs, let $\Db(\beta) \in \R^{n \times n}$ be a diagonal matrix whose $i$-th entry  for $i=1\ldots n$ is $\psi''(\xb_i^T\beta)$.  The Hessian of $\Lcal_d(\beta)$ is given by:
%
\begin{align*}
    \nabla^2\Lcal_d(\beta) &= \Xb^T\Db(\beta)\Xb + \lambda \cdot \Ib.
\end{align*}
%
In Section~\ref{sxn:forward_lower} we used the fact that $\|\Db(\beta)\|_2 \leq \nicefrac{1}{4}$ for the special case of logistic regression.  We now assume there exists a constant $\alpha_u > 0$ such that 
%
\begin{align}
\psi''(t) \leq \alpha_u  \label{eqn:GLMassume}
\end{align}
%
for all $t \in \R$. As discussed in Section 3.3 of~\citet{loh2015regularized}, this assumption holds for many GLMs, such as logistic regression, linear regression, and multinomial regression, but it does not hold in other cases, like Poissonn regression.  Using this assumption, we can bound the spectral norm of the Hessian:
\begin{gather*}
    \|\nabla^2 \Lcal_d(\beta)\|_2 
    = \|\Xb^T\Db(\beta)\Xb + \lambda \cdot \Ib\|_2
    \leq \|\Xb^T\|_2\|\Db(\beta)\|_2\|\Xb\|_2 + \lambda\|\Ib\|_2
    = \alpha_u \|\Xb\|_2^2 + \lambda.
\end{gather*}
Therefore, the log-loss of the GLM is $\alpha$-smooth, where $\alpha = \alpha_u \|\Xb\|_2^2 + \lambda$.  We can then follow the rest of the proof in Section \ref{sxn:forward_lower} starting from eqn. (\ref{eq:intermediate_upper}) using the definitions of $\wb(\cdot)$ and $\wbbar(\cdot)$ in Section \ref{sxn:glm_upper} to get:
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\geq \frac{1}{\alpha} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xb^T  \wb(\Pb_k\beta_k) \\
    &= \frac{1}{\alpha_u\|\Xb\|_2^2 + \lambda} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xb^T  \wb(\Pb_k\beta_k).
\end{align*}
%
Since $\alpha_u$ is a constant that only depends on the chosen GLM, we again conclude that, when the two-norm of the input matrix is constant, our upper and lower bounds differ by a factor of $\Ocal(\nicefrac{1}{\lambda})$.

\section{Information-theoretic bound on forward error}\label{section:information_theoretic_bound}


In this section, we show that our upper bound in Theorem \ref{thm:forward_error_upper_bound} implies an upper bound on the forward error of a logistic regression problem in terms of information theoretic quantities under standard statistical assumptions of logistic regression~\citep[Chapter 4]{mccullagh2019generalized}. This probabilistic interpretation requires no generative model assumption on the data points $(\yb_i, \xb_i)$.  It simply interprets the vectors $(\Ib - \Pb_k\Pb_k^T)\beta_d$ and $\Pb_k\beta_k$ as encoding distributions on the space of labels as described by the linear log-odds model.  

For a fixed point $\xb \in \R^d$, its corresponding label has the following distribution under the linear log-odds model with parameter $\beta$:
\begin{gather*}
    \PP\left(\yb = 1 | \xb, \beta\right) = \frac{1}{1+e^{-\xb^T\beta}}
\end{gather*}

For a fixed $\beta \in \R^d$, we can define another induced binary distribution that encodes the distribution of correct.  Let 
\begin{gather}
    q_i = \PP(\yb \neq \yb_i | \xb_i, (\Ib - \Pb_k\Pb_k^T)\beta_d) =  \frac{1}{1+e^{\yb_i\cdot\xb_i^T(\Ib - \Pb_k\Pb_k^T)\beta_d}} \label{eq:logistic_misclassification_probability_sketch}\\
    \text{and}\nonumber\\
    p_i = \PP(\yb \neq \yb_i | \xb_i, \Pb_k\beta_k) =  \frac{1}{1+e^{\yb_i\cdot\xb_i^T\Pb_k\beta_k}}. \label{eq:logistic_misclassification_probability_original} 
\end{gather}
Then, $q_i$ is the probability that a draw from the linear log-odds model at $\xb_i$ parameterized by $\beta = (\Ib - \Pb_k\Pb_k^T)\beta_d$ will not match the true observed label $\yb_i$.  The same intuition holds for $p_i$ except with $\beta = \Pb_k\beta_k$.  Therefore, we can denote the set of events $\{M, N\}$ called ``Match'' and ``No Match'' which indicate whether a draw from a linear log-odds model matches the true label $\yb_i$.  For a fixed tuple $(\xb_i, \yb_i, \beta_d, \beta_k, \Pb_k)$, we have two well defined distributions on $\{M,N\}$. Interestingly, the $\ell_2$-norm difference between $\beta_d$ and $(\Ib - \Pb_k\Pb_k^T)\beta_k$ is bounded by the cross-entropy of these two distributions.
\begin{theorem}
    Let $\Xb \in \R^{n \times d}$, $\yb \in \R^n$, $\beta_d \in \R^d$, and $\lambda > 0$ define the original logistic regression problem as defined in Section \ref{section:contributions}. Let $\Pb_k \in \R^{k \times d}$ be a sketching matrix such that $\Pb_k\Pb_k^T$ is a projection matrix, and let $\beta_k$ be the optimal solution of the sketched problem.  Let $\Dcal_q$ and $\Dcal_p$ be distributions on $\{-1, 1\}^n$ where the distribution on the $i$-th index is defined by eqns. \ref{eq:logistic_misclassification_probability_sketch} and \ref{eq:logistic_misclassification_probability_original} respectively. Then,
    \begin{gather*}
        \|\beta_d - \Pb_k\beta_k\|_2^2 \leq H(\Dcal_p, \Dcal_q),
    \end{gather*}
    where $H(\cdot, \cdot)$ denotes the cross-entropy between the two distributions.
\end{theorem}
\begin{proof}
By Theorem \ref{thm:forward_error_upper_bound}:
\begin{align*}
        \|\Pb_k\beta_k - \beta_d\|_2^2
        &\leq \frac{2}{\lambda} \sum_{i=1}^n \yb_i\cdot\beta_\lambda^T(\Ib - \Pb_k\Pb_k^T) \xb_i \cdot \sigma(-\yb_i\xb_i^T\Pb_k\beta_k),
    \end{align*}
We can rewrite a single summand in the bound by substituting in eqns. \ref{eq:logistic_misclassification_probability_sketch} and \ref{eq:logistic_misclassification_probability_original}:
\begin{align*}
    \yb_i\cdot\beta_\lambda^T(\Ib - \Pb_k\Pb_k^T) \xb_i \cdot \sigma(-\yb_i\xb_i^T\Pb_k\beta_k)
    &= \log \exp(\yb_i\cdot\beta_\lambda^T(\Ib - \Pb_k\Pb_k^T) \xb_i ) \cdot \frac{1}{1+e^{\yb_i\xb_i^T\Pb_k\beta_k}} \\
    &= \log \left(\frac{1}{q_i} - 1\right) \cdot p_i \\
    &= \log \left(\frac{1-q_i}{q_i}\right) \cdot p_i \\
    &= - p_i \log \frac{q_i}{1-q_i} \\
    &= -p_i \log q_i + p_i \log ( 1 - q_i) \\
    &= -p_i \log q_i - (1 - p_i) \log ( 1 - q_i) + \log(1-q_i) \\
    &\leq -p_i \log q_i - (1 - p_i) \log ( 1 - q_i).
\end{align*}

Let there exist two distributions with binary outcomes on the same event space.  If the probability of event one occurring is $p$ in the first distribution and $q$ in the second, the the cross-entropy of the first distribution relative to the second is $H(p,q) = -p\log q - (1-p) \log (1-q)$.  Therefore, we see that each summand of our forward error bound is bounded by the binary cross-entropy $H(p_i, q_i)$. Our new total bound can be written as:
\begin{align*}
        \|\Pb_k\beta_k - \beta_d\|_2^2
        &\leq \frac{2}{\lambda} \sum_{i=1}^n H(p_i, q_i)
\end{align*}
The binary cross entropy can be rewritten as $H(p, q) = H(p) + \dkl(p, q)$, where $\dkl$ denotes KL-divergence.  Both entropy and KL-divergence decompose additively over a product distribution of independent marginal distributions \cite{cover1999elements}. If $\qb \sim \Dcal_q$ and $\pb \sim \Dcal_p$, each coordinate of these vectors is independent and the $i$-th coordinates are distributed as Bernoulli random variable with parameters $q_i$ and $p_i$ respectively. Therefore,  
\begin{gather*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 \leq \frac{2}{\lambda} H(\Dcal_p, \Dcal_q).
\end{gather*}

\end{proof}
This bound is complementary to Theorem \ref{thm:forward_error_upper_bound} in that it intuitively captures the same relationship, but sacrifices tightness of the bound for interpretability. Fundamentally, the forward error must depend on the true labels of the data in a non-linear and non-smooth manner. The cross-entropy of the the modeled labels under the two models is one more familiar way to represent this relationship. Alternatively, one may view this result as a lower bound on the cross-entropy of the modeled labels in terms of the $\ell_2$-norm distance between the parameters of the original and sketched problem.

