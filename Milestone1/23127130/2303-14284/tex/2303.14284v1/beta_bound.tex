\section{Bounding the forward error}\label{sxn:forward_error}

In this section, we provide upper and lower bounds for the forward error when running logistic regression on the lower-dimensional data matrix $\Xb\Pb_k \in \R^{n \times k}$, with $k \ll d$, where $d$ is the dimensionality of the original data points (see Section~\ref{section:contributions} for notations). Recall that $\Pb_k \in \R^{d \times k}$ denotes the linear dimensionality reduction matrix, whose columns are pairwise orthogonal and normal ($\Pb_k^T \Pb_k = \Ib_k$). Using our notation, the solution to the original logistic regression problem is given by eqn.~(\ref{eqn:blrr}), while the solution to the dimensionally reduced logistic regression problem is as stated in eqn.~(\ref{eqn:sketchedregression1}).
%
Notice that at this point we assume that the regularization parameters for the two problems are not equal; we will see in our proof of Theorem~\ref{thm:forward_error_upper_bound} that setting $\mu = \lambda$ simplifies our bound without increasing the forward error, at least if no additional assumptions are made. 

\subsection{Upper bound}\label{sxn:forward_upper}
%
We now prove the following Theorem that upper bounds the forward error. We will use the notation introduced in Section~\ref{section:contributions}.
%
\begin{theorem}\label{thm:forward_error_upper_bound}
    Let the regularization terms of the dimensionally-reduced (eqn.~(\ref{eqn:sketchedregression1})) and the original (eqn.~(\ref{eqn:blrr})) logistic regression problems be equal, i.e., $\mu = \lambda$. Then, the forward error of the dimensionally-reduced logistic regression problem satisfies the following inequality: 
    \begin{align*}
        \|\Pb_k\beta_k - \beta_d\|_2^2
        &\leq \frac{2}{\lambda}\Phi(\Xb,\yb,\Pb_k),
    \end{align*}
    where $\Phi(\Xb,\yb,\Pb_k) = \beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb \wb(\Pb_k\beta_k)$.
    %
\end{theorem}
%
\begin{proof}
%
We start by computing the gradients of the loss functions for the original and the dimensionally-reduced problems. Recall that the loss function of eqn.~(\ref{eqn:blrr}) is a function over vectors $\beta \in \R^d$, while the loss function of eqn.~(\ref{eqn:sketchedregression1}) is over vectors $\beta \in \R^k$. For notational convenience, let $\Xbbar = -\Db_\yb\Xb$ and let $\wbbar(\beta) \in \R^n$ have entries $\wbbar(\beta)_i = \sigma(\xbbar_i^T\beta)$ (we do note that $\wb(\beta) = \wbbar(\beta)$). Using our notation, the gradients are:
%
\begin{gather}
    \nabla \Lcal_d(\beta) = \Xbbar^T \wbbar(\beta) + \lambda \cdot \beta
    \quad\text{and}\nonumber\\
    \nabla \Lcal_k(\beta) = \Pb_k^T\Xbbar^T \wbbar(\Pb_k\beta) + \mu \cdot \beta.\label{eqn:grall}
\end{gather}
%
After solving the dimensionally-reduced problem, we map its $k$-dimensional solution $\beta_k$ to the original $d$-dimensional domain by premultiplying by the dimensionality reduction matrix $\Pb_k$ to compute $\Pb_k\beta_k \in \R^d$. Since $\beta_k$ is the optimal solution to the sketched problem, we know that $\nabla \Lcal_k(\beta_k)$ is equal to zero and thus $\Pb_k\nabla \Lcal_k(\beta_k) = 0$.  This allows us to simplify the gradient of the original loss function $ \nabla\Lcal_d$ computed at $\Pb_k\beta_k$ as follows:
%
\begin{align}
    \nabla \Lcal_d(\Pb_k\beta_k)
    &= \Xbbar^T \wbbar(\Pb_k\beta_k) + \lambda \cdot \Pb_k\beta_k \\
    &= \Xbbar^T \wbbar(\Pb_k\beta_k) + \lambda \cdot \Pb_k\beta_k - \Pb_k\nabla \Lcal_k(\beta_k)\\
    &= \Xbbar^T \wbbar(\Pb_k\beta_k) + \lambda \cdot \Pb_k\beta_k \\
    &\quad\quad- \Pb_k[\Pb_k^T\Xbbar^T \wbbar(\Pb_k\beta_k) + \mu \cdot \beta_k] \nonumber\\
    &= (\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k) + (\lambda - \mu) \Pb_k \beta_k.\label{eqn:pd1}
\end{align} 
%
The above derivations are immediate using the gradients of the loss functions from eqn.~(\ref{eqn:grall}). Let 
%
\begin{align}
\vb = \frac{\Pb_k\beta_k - \beta_d}{\|\Pb_k\beta_k - \beta_d\|_2}.\label{eqn:pdv}
\end{align}
%
In words, $\vb$ is a unit vector pointing from $\beta_d$ to $\Pb_k\beta_k$, which allows us to bound the forward error $\|\Pb_k\beta_k - \beta_d\|_2$ using values $t>0$ that satisfy: 
%
\begin{gather}
    \Lcal_d(\Pb_k\beta_k - t \cdot \vb) - \Lcal_d(\Pb_k\beta_k) \geq t\cdot (-\vb^T\nabla \Lcal_d(\Pb_k\beta_k)) + \frac{t^2 \cdot \lambda}{2}.\label{eqn:vv1}
\end{gather}
%
In order to understand the above equation, note that $\Lcal_d(\beta)$ is $\lambda$-strongly convex.  Therefore, we can \textit{lower}-bound the loss at any point by using a truncated Taylor series and setting the Hessian term to $\lambda \cdot \Ib$~(see \cite{nesterov1998introductory} for definitions and properties of strong convexity).  As we increase $t$ in the truncated Taylor series, the point $\Pb_k\beta_k - t\cdot \vb$ will move from $\Pb_k\beta_k$ to $\beta_d$ and then past it.  If $\Lcal_d(\Pb_k\beta_k - t \cdot \vb) \geq \Lcal_d(\Pb_k\beta_k)$, then we know we have passed the point $\beta_d$ (the optimal solution to the original logistic regression problem), since $\beta_d$ minimizes the loss function $\Lcal_d(\cdot)$.  We can now derive our bound by first setting the left-hand side of eqn.~(\ref{eqn:vv1}) to zero and solving for $t^* >0$:
%
\begin{gather*}
    0 = -\vb^T\nabla \Lcal_d(\Pb_k\beta_k) + \nicefrac{t^* \cdot \lambda}{2} 
    \Rightarrow
     \nicefrac{t^* \cdot \lambda}{2} =  \vb^T\nabla \Lcal_d(\Pb_k\beta_k) \\
    \Rightarrow t^* = \nicefrac{2}{\lambda}\vb^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k)
    + \nicefrac{2(\lambda - \mu)}{\lambda} \vb^T \Pb_k \beta_k.
\end{gather*}
%
In the last derivation we used eqn.~(\ref{eqn:pd1}). We now proceed by expanding $\vb$ using eqn.~(\ref{eqn:pdv}) and 
%
\begin{align*}
    t^*\|\Pb_k\beta_k - \beta_d\|_2 &= \frac{2}{\lambda}(\Pb_k\beta_k - \beta_d)^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k) + \frac{2(\lambda - \mu)}{\lambda} (\Pb_k\beta_k - \beta_d)^T \Pb_k \beta_k \\
    %
    &= \frac{-2}{\lambda}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k) + \frac{2(\lambda - \mu)}{\lambda} (\|\Pb_k\beta_k\|_2^2 - \beta_d^T \Pb_k \beta_k).
\end{align*}
%
To derive the second equality, we used the fact that $\beta_k^T \Pb_k^T(\Ib - \Pb_k\Pb_k^T)$ is equal to zero. We now conclude the proof by using $t^*\geq\|\Pb_k\beta_k - \beta_d\|_2$, which must hold since the loss of $\beta$ decreases monotonically as $\beta$ goes from $\Pb_k\beta_k$ to the optimum $\beta_d$.
%
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\leq \frac{-2}{\lambda}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k) + \frac{2(\lambda - \mu)}{\lambda} (\|\beta_k\|_2^2 - \beta_d^T \Pb_k \beta_k) \\
    &\leq \frac{2}{\lambda}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb \wb(\Pb_k\beta_k) + \frac{2(\lambda - \mu)}{\lambda} (\|\beta_k\|_2^2 - \beta_d^T \Pb_k \beta_k).
\end{align*}
%
In the above derivation we also used the fact that $\|\Pb_k \beta_k\|_2 = \|\beta_k\|_2$, since $\Pb_k$ is a column-orthogonal matrix. We can now conclude the proof by setting $\lambda = \mu$, which cancels the second term in the right-hand side of the above inequality. We do note that this term scales as a function of the difference between the norm (squared) of the solution to the dimensionally-reduced problem $\beta_k$ and the inner product between the solution to the original problem $\beta_d$ and the ``mapped'' solution $\Pb_k \beta_k$. One could envision settings where this difference is sufficiently large so that using a value of $\mu$ that is not equal to $\lambda$ and makes the second term negative improves the overall bound. This seems challenging without additional assumptions on the behavior of the aforementioned difference, which in turn depends on our choice of $\mu$. Therefore, to simplify our bound and avoid additional complicated assumptions, we set $\mu$ equal to $\lambda$.
\end{proof}

%
\subsection{A matching lower bound}\label{sxn:forward_lower}

We now proceed to prove a matching (up to constants and $\lambda$-factors) lower bound for Theorem~\ref{thm:forward_error_upper_bound}. Towards that end, we start by stating two lemmas from prior work that will be critical in our proofs. The first lemma shows that upper bounding the $\ell_2$-norm of the Hessian also upper bounds the norm of the gradient of a function, when these quantities exist.

\vspace{0.02in}\begin{lemma}\label{lemma:lipschitz_gradient}
    \citep[Lemma 1.2.2]{nesterov1998introductory} A function $f(x)$ is twice differentiable on $\R^n$ and has an $L$-Lipschitz gradient if and only if
    $\|\nabla^2f(\xb)\|_2 \leq L$, for all $\xb \in \R^n$.
\end{lemma}
%
The second lemma shows that if the gradient of a function is Lipschitz, then the function can be upper bounded by a quadratic function.
%
\vspace{0.02in}\begin{lemma}\label{lemma:lipschitz_convexity}
    \citep[Lemma 1.2.3]{nesterov1998introductory} If a function $f(x)$ is differentiable on $\R^n$ and has an $L$-Lipschitz gradient, then for all $\xb,\yb \in \R^n$,
    \begin{gather*}
        |f(\yb) - f(\xb) - (\yb - \xb)^T \nabla f(\xb)| \leq \nicefrac{L}{2} \|\yb - \xb\|_2^2.
    \end{gather*}
\end{lemma}
%
We are now ready to present our lower-bound proof. 
%
\begin{theorem}\label{thm:forward_error_lower_bound}
    Let the regularization terms of the dimensionally-reduced (eqn.~(\ref{eqn:sketchedregression1})) and the original (eqn.~(\ref{eqn:blrr})) logistic regression problems be equal, i.e., $\mu = \lambda$. Then, the forward error of the dimensionally-reduced logistic regression problem satisfies the following inequality: 
    \begin{gather*}
         \|\Pb_k\beta_k - \beta_d\|_2^2 \geq \frac{4}{ \|\Xb\|_2^2 + \lambda} \cdot\Phi(\Xb,\yb,\Pb_k),
    \end{gather*}
    where $\Phi(\Xb,\yb,\Pb_k) = \beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb \wb(\Pb_k\beta_k)$.
\end{theorem}
%

\begin{proof}
    First, define the diagonal matrix $\Db(\beta) \in \R^{n\times n}$, whose diagonal entries $\Db(\beta)_{ii}$ are equal to $\sigma(\xb_i^T\beta)\sigma(-\xb_i^T\beta)$ for all $i=1\ldots n$; recall from the previous section that $\sigma(\cdot)$ is the logistic function. The Hessian of the regularized loss function of the original problem (see eqn.~(\ref{eqn:blrr}) is:
%
$
    \nabla^2 \Lcal_d(\beta) = \Xbbar^T \Db(\beta) \Xbbar + \lambda \cdot \Ib.
$
%
By the definition of $\sigma(\cdot)$ it follows that $\sigma(\xb_i^T\beta)\sigma(-\xb_i^T\beta) \in (0,1/4]$ and therefore $\|\Db(\beta)\|_2 \leq 1/4$. We can now upper-bound the spectral norm of the Hessian using sub-additivity and sub-multiplicativity of the spectral norm as follows:
\begin{align*}
    \|\nabla^2 \Lcal_d(\beta)\|_2 & = \|\Xbbar^T \Db(\beta) \Xbbar + \lambda \cdot \Ib\|_2 \\
    &\leq \|\Xbbar^T\|_2 \cdot \|\Db(\beta)\|_2 \cdot \|\Xbbar\|_2 + \lambda\cdot \|\Ib\|_2 \\
    &= \nicefrac{1}{4} \cdot \|\Xbbar\|_2^2 + \lambda.
\end{align*}

This shows that $\Lcal_d(\beta)$ is $\alpha$-smooth with $\alpha = \nicefrac{1}{4} \cdot \|\Xbbar\|_2^2 + \lambda$.  This implies two facts: first, using Lemma~\ref{lemma:lipschitz_gradient}, the gradient $\nabla \Lcal_d(\beta)$ is $\alpha$-Lipschitz with respect to the $\ell_2$-norm. 
Second, we can upper-bound $\Lcal_d(\beta)$ using  a quadratic function as follows:
%
\begin{gather*}\label{eq:intermediate_upper}
    \Lcal_d(\Pb_k\beta_k - t \cdot \vb) - \Lcal_d(\Pb_k\beta_k)
    \leq t\cdot (-\vb^T\nabla \Lcal_d(\Pb_k\beta_k)) + \frac{t^2 \cdot \alpha}{2}, \quad
\text{for all }t \in [0,1].
\end{gather*}
%
The above follows from Lemma~\ref{lemma:lipschitz_convexity}; 
$\vb$ is the same vector as in the previous section (see eqn.~(\ref{eqn:pdv})). Notice that both the left and the right hand side of the above inequality are functions of the variable $t$. We let $f(t) = \Lcal_d(\Pb_k\beta_k - t \cdot \vb) - \Lcal_d(\Pb_k\beta_k)$ denote the left-hand side of the inequality and we let $q(t) = t\cdot -\vb^T\nabla \Lcal_d(\Pb_k\beta_k) + \nicefrac{t^2 \cdot \alpha}{2}$ denote the quadratic function at the right-hand side of the inequality. We know that $f(t)$ is minimized when $\Pb_k\beta_k - t \cdot \vb = \beta_d$, since $\beta_d$ minimizes $\Lcal_d(\beta)$. It immediately follows that $f(t)$ is minimized by setting $t = \|\Pb_k\beta_k - \beta_d\|_2$.  We can therefore lower-bound the forward error by first showing that 
%
\begin{align}
\argmin_{t \geq 0} f(t) \geq \argmin_{t \geq 0} q(t).\label{eqn:pd3}
\end{align}
%
Towards that end, we express the derivatives of $f(t)$ and $q(t)$ as functions of the gradient of $\Lcal_d(\beta)$ as follows:
%
\begin{gather*}
    f'(t)  = -\vb^T \nabla \Lcal_d(\Pb_k\beta_k - t \cdot \vb), \quad \text{and} \quad \\
    q'(t) = -\vb^T\nabla \Lcal_d(\Pb_k\beta_k) + t \cdot \alpha.
\end{gather*}
%
Since $\nabla \Lcal_d(\beta)$ is $\alpha$-Lipschitz, $\| \nabla \Lcal_d(\Pb_k\beta_k - t \cdot \vb) -  \nabla \Lcal_d(\Pb_k\beta_k)\|_2 \leq \alpha \cdot t$. Using the fact that $\|\vb\|_2=1$, we get
%
\begin{align}
\vb^T\nabla &\Lcal_d(\Pb_k\beta_k - t \cdot \vb) - \vb^T\nabla \Lcal_d(\Pb_k\beta_k) \nonumber \\
&\leq \|\vb\|_2
\| \nabla \Lcal_d(\Pb_k\beta_k - t \cdot \vb) -  \nabla \Lcal_d(\Pb_k\beta_k)\|_2 \nonumber \\
%
&\leq \alpha \cdot t. \label{eqn:pd2}
%
\end{align}
%
We are now ready to show that the derivative of $q(t)$ upper-bounds the derivative of $f(t)$:
%
\begin{align*}
     q'(t) - f'(t)
     &= -\vb^T\nabla \Lcal_d(\Pb_k\beta_k) + t \cdot \alpha  + \vb^T \nabla \Lcal_d(\Pb_k\beta_k - t \cdot \vb) \\
     &= t \cdot \alpha - (\vb^T\nabla \Lcal_d(\Pb_k\beta_k - t \cdot \vb) - \vb^T\nabla \Lcal_d(\Pb_k\beta_k)) \\
     &\geq t\cdot \alpha - t \cdot \alpha = 0.
\end{align*}
%
The inequality follows from eqn.~(\ref{eqn:pd2}). Since initially $f'(0)$ and $q'(0)$ are both negative or zero, it must be the case that $q'(t) = 0$ first, hence $\argmin_{t>0} f(t) \geq \argmin_{t\geq 0} q(t)$, thus proving eqn.~(\ref{eqn:pd3}). Recall that the minimizer for $f(t)$ is $\argmin_{t>0} f(t) = \|\Pb_k\beta_k - \beta_d\|_2$, while we can directly solve for the minimizer for $q(t)$ to get $$\argmin_{t>0} q(t) = \frac{1}{\alpha} \vb^T\nabla \Lcal_d(\Pb_k\beta_k).$$ This gives the following bound on the forward error: 
%
\begin{gather*}
    \frac{1}{\alpha} \vb^T\nabla \Lcal_d(\Pb_k\beta_k)
    \leq \|\Pb_k\beta_k - \beta_d\|_2.
\end{gather*}
%
We can manipulate this lower bound so that it matches the upper bound of Theorem~\ref{thm:forward_error_upper_bound} up to constant factors. Using the definition of $\vb$ from eqn.~(\ref{eqn:pdv}) we get:
%
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2 &\geq \frac{1}{\alpha} \vb^T \nabla \Lcal_d(\Pb_k\beta_k) \\
    &=
    \frac{1}{\alpha \|\Pb_k\beta_k - \beta_d\|_2 }
    (\Pb_k\beta_k - \beta_d)^T\nabla \Lcal_d(\Pb_k\beta_k).
\end{align*}
%
Recall that, for $\mu = \lambda$, $\nabla \Lcal_d(\Pb_k\beta_k) = (\Ib - \Pb_k\Pb_k^T) \Xbbar^T \wbbar(\Pb_k\beta_k)$ and rewrite the above inequality to get:
%
\begin{align*}
    \|\Pb_k\beta_k& - \beta_d\|_2^2 \geq \frac{1}{\alpha} (\Pb_k\beta_k - \beta_d)^T\nabla \Lcal_d(\Pb_k\beta_k) \\
    &= \frac{1}{\alpha} (\Pb_k\beta_k - \beta_d)^T (\Ib - \Pb_k\Pb_k^T) \Xbbar^T  \wbbar(\Pb_k\beta_k)\\
    &=\frac{1}{\alpha} \cdot (- \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xbbar^T  \wbbar(\Pb_k\beta_k)) \\
    &=\frac{1}{\alpha} \cdot (\beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xb^T\Db_\yb  \wb(\Pb_k\beta_k)).
\end{align*}
%
The second-to-last equality follows since $\beta_k^T\Pb_k(\Ib - \Pb_k\Pb_k^T)$ is equal to zero.  Finally, using $\|\Xbbar\|_2 = \|\Xb\|_2$ to conclude the theorem statement.
\end{proof}
%
We note that the bound of Theorem~\ref{thm:forward_error_lower_bound} is useful only when $\|\Xb\|_2^2 \geq \lambda$. This is actually the only interesting setting, since if $\|\Xb\|_2^2 < \lambda$ then the solution to both the sketched and the original problem is trivial and equal to zero.

Finally, it immediately follows from Theorem~\ref{thm:forward_error_lower_bound} that if the two norm of the input matrix $\Xb$ is constant (i.e., $\|\Xb\|_2=O(1)$), our upper and lower bounds differ by a constant factor that only depends on $\lambda$:
%
\begin{align*}
\nicefrac{2}{\lambda} \cdot \alpha = \nicefrac{1}{2\lambda} \cdot \|\Xbbar\|_2^2 + 2 = \nicefrac{1}{2\lambda} \cdot \|\Xb\|_2^2 + 2 = O(\nicefrac{1}{\lambda}).\label{eqn:lowerbound}
\end{align*}
