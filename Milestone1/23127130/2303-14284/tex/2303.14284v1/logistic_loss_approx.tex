\section{Approximating Logistic Loss}\label{sxn:logistic_loss}

In this section, we consider approximations to the \emph{logistic loss} function. This type of error guarantees were previously provided by a body of work on coresets for logistic regression; see the works of~\citet{munteanu2018coresets, mai2021coresets} for state-of-the-art results and bounds and references therein for motivation and prior work on this topic.

We begin by proving a general lower bound on the space complexity of \emph{any} data structure which approximates logistic loss to $\epsilon$-relative error for every parameter vector $\beta \in \R^d$. Our lower bound shows that the coreset construction of \citet{mai2021coresets} has optimal space complexity up to an $\Ocaltil(d\cdot \mu_\yb(\Xb)^2)$ factor, where $\mu_\yb(\Xb)^2$ is a data complexity measure introduced by~\citet{munteanu2018coresets} (see also Definition~\ref{def:complexity_measure} below).  Furthermore, we provide an efficient linear programming formulation to compute this data complexity measure on real data sets, \textit{refuting an earlier conjecture that the factor was hard to compute}~\cite{munteanu2018coresets}.

Next, we show that any low-rank approximation $\Xbbar$ of the data matrix $\Xb$ can be used to approximate the logistic loss function $\Lcal(\beta)$ up to a $\sqrt{n}\|\Xb - \Xbbar\|_2\|\beta\|_2$ additive error. The factor $\|\Xb - \Xbbar\|_2\|\beta\|_2$ is the spectral norm (or two-norm) error of the low-rank approximation and we also prove that this bound is tight in the worst case. Low rank approximations are commonly used to reduce the time and space complexity of numerical algorithms, especially in settings where the data matrix $\Xb$ is numerically low-rank or has a decaying spectrum of singular values.

In this section, we assume without loss of generality that $\yb_i = -1$ for all $i=1\ldots n$. Recall that any logistic regression problem can be transformed to this standard form by multiplying both $\Xb$ and $\yb$ by $-\Db_\yb$ (see Section~\ref{section:contributions} for notation), in which case the logistic loss of the original problem is equal to that of the transformed problem for all $\beta\in \R^d$.  Furthermore, we assume in this section that the logistic loss has no regularization, i.e. $\lambda = 0$. This is because increasing the regularization parameter will only increase the quality of the approximation to the logistic loss in terms of relative error, as the regularization penalty can be computed exactly. 

\subsection{General space complexity lower bound}

We lower bound the space complexity of any data structure that approximates logistic loss to $\epsilon$-relative error. More specifically, assume we are given a logistic regression problem specified by $\Xb$ and $\yb$ with bounded bit complexity. We consider the problem of compressing the data to a small number of bits so that a data structure $f(\cdot)$ with access only to these small number of bits satisfies 
%
$$|f(\beta) - \Lcal(\beta)| \leq \epsilon \Lcal(\beta),$$ 
%
for all $\beta \in \R^d$. Recall that the running time of the computation compressing the data to a small number of bits and evaluating $f(\beta)$ for a given query $\beta$ is unbounded in this model. Hence, Theorem~\ref{thm:space_lower_bound} provides a strong lower bound on the space needed for \textit{any} compression of the data that can be used to compute an $\epsilon$-relative error approximation to the logistic loss, including, but not limited to, coresets. 

At a high level, our proof operates by showing that a relative error approximation to logistic loss can be used to obtain a relative error approximation to ReLu regression, which in turn can be used to construct a relative error $\ell_1$-subspace embedding. Previously,~\citet{li2021tight} lower bounded the worst case space complexity of any data structure that maintains an $\ell_1$-subspace embedding by reducing the problem to the well-known~\texttt{INDEX} problem in communication complexity. See Appendix~\ref{sxn:proof_appendix} for the proof of Theorem~\ref{thm:space_lower_bound}.
%
\begin{theorem}\label{thm:space_lower_bound}
    Let $\Xb \in \R^{n \times d}$ and $\yb \in \{-1, 1\}^n$ be the data matrix and label vector such that each entry of $\Xb$ is specified by $\Ocal(\log nd)$ bits. If $f(\cdot)$ is a data structure such that for all $\beta \in \R^d$, $|f(\beta) - \Lcal(\beta)| \leq \epsilon\Lcal(\beta),$ then $f(\cdot)$ requires $\Omegatil\left(\frac{d}{\epsilon^2}\right)$ space in the worst case, provided that $d = \Omega(\log 1/\epsilon)$ and $n = \Omegatil\left(d\epsilon^{-2}\right)$ and $\epsilon > 0$ is sufficiently small. 
\end{theorem}
%
Prior work on coreset construction for logistic regression critically depends on the data complexity measure $\mu_\yb(\Xb)$, which was first introduced in \cite{munteanu2018coresets}, and is defined as follows.
%
\begin{definition}\label{def:complexity_measure} (Classification Complexity Measure \cite{munteanu2018coresets}) 
    For any $\Xb \in \R^{n \times d}$ and $\yb \in \{-1, 1\}^n$, let $\mu_\yb(\Xb) = \sup_{\beta \neq \zero} \frac{\|(\Db_\yb\Xb\beta)^+\|_1}{\|(\Db_\yb\Xb\beta)^-\|_1}$, where $\Db_\yb$ is a diagonal matrix with $\yb$ as its diagonal, and $(\Db_\yb\Xb\beta)^+$ and $(\Db_\yb\Xb\beta)^-$ denote the positive and the negative entries of $\Db_\yb\Xb\beta$ respectively.
\end{definition}
%
The work of~\citet{mai2021coresets} showed that sampling $\Ocal\left(\frac{d \cdot \mu_\yb(\Xb)^2}{\epsilon^2}\right)$ rows of $\Xb$ yields an $\epsilon$-relative error coreset for logistic loss with high probability, where $\mu_\yb(\Xb)$ is the data complexity measure of Definition~\ref{def:complexity_measure}. This leaves a gap of $\Ocaltil(d\cdot\mu_\yb(\Xb)^2)$ between our space complexity lower bound and the best known upper bound for the space needed to approximate logistic loss to relative error. Closing this gap is an open problem for future work.

Understanding whether the complexity measure of Definition~\ref{def:complexity_measure} truly explains the performance of coresets on real-world data sets would help guide further improvements to coreset construction for logistic regression. However, prior work conjectured that $\mu_\yb(\Xb)$ was hard to compute, while providing a polynomial time algorithm to approximate the measure to within a $\operatorname{poly}(d)$-factor (see Theorem 3 of \citet{munteanu2018coresets}). We refute this conjecture by showing that the complexity measure $\mu_\yb(\Xb)$ can in fact be computed efficiently via linear programming.
%Note that if the data is linearly separable, then the complexity measure is infinite, and the optimal objective value of the provided linear program is infinite as well.
%
\begin{theorem}\label{thm:compute_complexity_measure}
    If the complexity measure $\mu_\yb(\Xb)$ of Definition~\ref{def:complexity_measure} is finite, it can be computed \textit{exactly} by solving a linear program with $2n$ variables and $4n$ constraints.
\end{theorem}
\begin{proof}
    We now derive a linear programming formulation formulation to compute the complexity measure in Definition \ref{def:complexity_measure}. Note we flip the numerator and denominator from Definition \ref{def:complexity_measure} without loss of generality. Define $\beta^* \in \R^d$ as, ($C$ is an arbitrary positive constant):
\begin{flalign*}
    \beta^* &= \argmax_{\beta \neq \zero} \frac{\|(\Db_\yb\Xb\beta)^-\|_1}{\|(\Db_\yb\Xb\beta)^+\|_1} \\
    &= \underset{\beta \in \R^d}{\argmax} ~ \|(\Db_\yb\Xb\beta)^-\|_1 \text{ such that } \|(\Db_\yb\Xb\beta)^+\|_1 \leq C \\
    &= \underset{\beta \in \R^d}{\argmax} ~ \|\Db_\yb\Xb\beta\|_1 \text{ such that } \|(\Db_\yb\Xb\beta)^+\|_1 \leq C.
\end{flalign*}

Now we reformulate the last constraint: 
%
\begin{align*}
\|(\Db_\yb\Xb\beta)^+\|_1 &= \sum_{i=1}^n \max\{[\Db_\yb\Xb\beta]_i, 0\} \\
&= \sum_{i=1}^n \frac{1}{2} [\Db_\yb\Xb\beta]_i + \frac{1}{2}|[\Db_\yb\Xb\beta]_i| \\
&= \frac{1}{2}\one^T\Db_\yb\Xb\beta + \frac{1}{2}\|\Db_\yb\Xb\beta\|_1.  
\end{align*}
Therefore, the above formulation is equivalent to:
\begin{flalign*}
    \beta^* = \underset{\beta \in \R^d}{\argmax}& ~ \|\Db_\yb\Xb\beta\|_1 \\
    \text{ such that }& \frac{1}{2}\one^T\Db_\yb\Xb\beta + \frac{1}{2}\|\Db_\yb\Xb\beta\|_1 \leq C \\ 
    = \underset{\beta \in \R^d}{\argmin}& ~ \one^T\Db_\yb\Xb\beta \text{ such that } \|\Db_\yb\Xb\beta\|_1 \leq C.
\end{flalign*}
Next, we replace $\Db_\yb\Xb\beta$ with a single vector $\zb \in \R^n$ and a linear constraint to guarantee that $\zb \in \operatorname{Range}(\Db_\yb\Xb)$. Let $\Pb_R \in \R^{n \times n}$ be the orthogonal projection to $\operatorname{Range}(\Db_\yb\Xb)$.
\begin{gather*}
    \zb^* = \underset{\beta \in \R^d}{\argmin} ~ \one^T\zb \\\text{ such that } \|\zb\|_1 \leq C, ~(\Ib - \Pb_R)\zb = \zero.
\end{gather*}
Next, we solve this formulation by constructing a linear program such that $[\zb_+, \zb_-] \in \R^{2n}$ corresponds to the absolute value of the positive and negative elements of $\zb$:  \begin{align*}
    \zb^* = \underset{\beta \in \R^d}{\argmin}& ~ \one_{n}^T(\zb_+ -\zb_-) \\ 
    \text{ such that }& \sum_{i=1}^{2n} [\zb_+, \zb_-]_i \leq C,\\ 
    &(\Ib - \Pb_R)(\zb_+ - \zb_-)  = \zero,\ \ 
    \zb_+,\zb_- \geq 0.
\end{align*}
Observe that this is a linear program with $2n$ variables and $4n$ constraints.  After solving this program for $\zb_+^*$ and $\zb_-^*$, we can compute $\zb^* = \zb_+^* - \zb_-^*$.  From this, we can compute $\beta^*$ by solving the linear system $\zb^* = \Db_\yb\Xb\beta^*$, which is guaranteed to have a solution by the linear constraint $(\Ib - \Pb_R)\zb^* = \zero$.

After solving for $\beta^*$, we can compute (recall that the numerator and denominator are flipped  readability without loss of generality) $$\mu_\yb(\Xb) = \frac{\|(\Db_\yb\Xb\beta^*)^-\|_1}{\|(\Db_\yb\Xb\beta^*)^+\|_1},$$ completing the proof.
\end{proof}
%
Prior experimental evaluation~\cite{munteanu2018coresets, mai2021coresets} of coreset constructions relied on estimates of $\mu_\yb(\Xb)$ using the method provided by \citet{munteanu2018coresets}. Given the large $\operatorname{poly}(d)$ approximation factor of this algorithm, it is unclear how reliable these estimates are. Our linear programming formulation provides a novel way to relate the proposed complexity measure and coreset performance.

\subsection{Low-rank approximations in logistic regression}

In this section, we consider the performance of low-rank approximations to the matrix $\Xb$ in order to approximately compute the logistic loss function. Low-rank approximations are often used to reduce the time complexity or to denoise and improve the performance of clustering and classification algorithms (such as logistic regression) by replacing a matrix with its low rank factors~\cite{Drineas2004,Paul2014}. To the best of our knowledge, its use for logistic regression has not been explicitly analyzed in prior work. 

Using low-rank approximations of $\Xb$ to estimate the logistic loss is appealing due to the extensive body work on fast constructions of low-rank approximations via sketching, sampling, and direct methods \cite{kishore2017literature}. We show that a spectral approximation provides an additive error guarantee for the logistic loss and that this guarantee is tight on worst-case inputs.

\begin{theorem}\label{thm:logistic_loss_upper}
    If $\Xb, \Xbtil \in \R^{n \times d}$, then for all $\beta \in \R^d$,
    \begin{gather*}
        |\Lcal(\beta; \Xb) - \Lcal(\beta, \Xbtil)| \leq \sqrt{n} \|\Xb - \Xbtil\|_2 \|\beta\|_2.
    \end{gather*}
\end{theorem}
%
We note that Theorem~\ref{thm:logistic_loss_upper} holds for any matrix $\Xbtil \in \R^{n \times d}$ that approximates $\Xb$ with respect to the spectral norm, and does not necessitate that $\Xbtil$ has low-rank. We now provide a matching lower-bound for the logistic loss function in the same setting.
%
\begin{theorem}\label{thm:logistic_loss_lower}
    For every $d,n \in \mathbb{N}$ where $d \geq n$, there exists a data matrix $\Xb \in \R^{n \times d}$, label vector $\yb \in \{-1, 1\}^n$, parameter vector $\beta \in \R^d$, and spectral approximation $\Xbtil \in \R^{n \times d}$ such that:
    \begin{gather*}
        |\Lcal(\beta; \Xbbar) - \Lcal(\beta; \Xb)| \geq (1-\delta)\sqrt{n}\|\Xb - \Xbbar\|_2\|\beta\|_2,
    \end{gather*}
    for every $\delta > 0$.  
    %
    Hence, the guarantee of Theorem \ref{thm:logistic_loss_upper} is tight in the worst case.
\end{theorem}