
\section{Extension to GLMs}

\subsection{Forward error bound for GLMs}\label{sxn:glm_upper}

In this section, we show how our results can be generalized to hold for Generalized Linear Models (GLMs) beyond logistic regression.  We start by writing down the general formulas for the conditional distribution and prediction of an arbitrary GLM with linear parameter $\beta \in \R^d$, scale parameter $\sigma >0$, and cumulant function $\psi$. We follow the lines of~\cite{mccullagh2019generalized} to get:
%
\begin{align*}
    \PP(\yb_i | \xb_i, \beta, \sigma) &= \exp\left\{\frac{\yb_i\xb_i^T\beta - \psi(\xb_i^T\beta)}{c(\sigma)}\right\}, \quad \text{and}\\
    %
    \EE[\yb_i | \xb_i, \beta, \sigma] &= \psi'(\xb_i^T\beta).
\end{align*}
%
The empirical (un-normalized) log-loss function is given by:
%
\begin{align*}
    \Lcal_\text{log-loss}(\beta) = \sum_{i=1}^n \frac{-1}{c(\sigma)}(\yb_i\xb_i^T\beta - \psi(\xb_i^T\beta)).
\end{align*}
%
We can now define the original and dimensionally-reduced loss functions with $\ell_2^2$-regularization as follows: set $c(\sigma) = 1$ (without loss of generality):
%
\begin{align*}
    \beta_d &= \argmin_{\beta \in \R^d} \Lcal_d(\beta) \\
    &= \argmin_{\beta \in \R^d} \sum_{i=1}^n -(\yb_i\xb_i^T\beta - \psi(\xb_i^T\beta)) + \frac{\lambda}{2}\|\beta\|_2^2,\quad \text{and,} \\
    \beta_k &= \argmin_{\beta \in \R^k} \Lcal_k(\beta) \\
    &= \argmin_{\beta \in \R^k} \sum_{i=1}^n -(\yb_i\xb_i^T\Pb_k\beta - \psi(\xb_i^T\Pb_k\beta)) + \frac{\mu}{2} \|\beta\|_2^2.
\end{align*}
%
We note that the above two equations are analogs of eqns.~(\ref{eqn:blrr}) and~(\ref{eqn:ll2}). The corresponding gradients of the above loss functions are given by
%
\begin{align*}
    \nabla \Lcal_d(\beta) &= \sum_{i=1}^n (\psi'(\xb_i^T\beta) - \yb_i)\xb_i + \lambda\cdot\beta, \quad \text{and}\\
    \nabla \Lcal_k(\beta) &= \sum_{i=1}^n (\psi'(\xb_i^T\Pb_k\beta) - \yb_i)\Pb_k^T\xb_i + \mu\cdot\beta.
\end{align*}
%
We now define $\wbbar(\beta) \in \R^n$ such that $[\wb(\beta)]_i = (\psi'(\xb_i^T\beta) - \yb_i)$.  We can now rewrite the above two gradients using matrix notation as
%
\begin{align*}
    \nabla \Lcal_d(\beta) &=\Xb^T\wbbar(\beta) + \lambda\cdot\beta, \quad \text{and}\\
    \nabla \Lcal_k(\beta) &= \Pb_k^T\Xb^T\wbbar(\Pb_k\beta) + \mu\cdot\beta.
\end{align*}
%
Notice that the above two formulas for the gradients are equivalent to eqn.~(\ref{eqn:grall}) except for the definition of $\wbbar(\cdot)$.  From here on, we can closely follow the remainder of the proof of Theorem~\ref{eqn:lemma1} in Section~\ref{sxn:forward_upper} to get an analogous upper bound on the forward error. If we let $\wb(\beta) = -1\cdot \wbbar(\beta)$ and $\mu = \lambda$, then the final error bound is
\begin{align*}
    \|\Pb_k\beta_k - \beta_d\|_2^2 
    &\leq \frac{2}{\lambda}\beta_d^T(\Ib - \Pb_k\Pb_k^T) \Xb^T \wb(\Pb_k\beta_k).
\end{align*}


\subsection{Lower bound for GLMs}

To extend our lower bounds to GLMs, let $\Db(\beta) \in \R^{n \times n}$ be a diagonal matrix whose $i$-th entry  for $i=1\ldots n$ is $\psi''(\xb_i^T\beta)$.  The Hessian of $\Lcal_d(\beta)$ is given by:
%
\begin{align*}
    \nabla^2\Lcal_d(\beta) &= \Xb^T\Db(\beta)\Xb + \lambda \cdot \Ib.
\end{align*}
%
In Section~\ref{sxn:forward_lower} we used the fact that $\|\Db(\beta)\|_2 \leq \nicefrac{1}{4}$ for the special case of logistic regression.  We now assume there exists a constant $\alpha_u > 0$ such that 
%
\begin{align}
\psi''(t) \leq \alpha_u  \label{eqn:GLMassume}
\end{align}
%
for all $t \in \R$. As discussed in Section 3.3 of~\citet{loh2015regularized}, this assumption holds for many GLMs, such as logistic regression, linear regression, and multinomial regression, but it does not hold in other cases, like Poissonn regression.  Using this assumption, we can bound the spectral norm of the Hessian:
\begin{gather*}
    \|\nabla^2 \Lcal_d(\beta)\|_2 
    = \|\Xb^T\Db(\beta)\Xb + \lambda \cdot \Ib\|_2
    \\leq \|\Xb^T\|_2\|\Db(\beta)\|_2\|\Xb\|_2 + \lambda\|\Ib\|_2
    = \alpha_u \|\Xb\|_2^2 + \lambda.
\end{gather*}
Therefore, the log-loss of the GLM is $\alpha$-smooth, where $\alpha = \alpha_u \|\Xb\|_2^2 + \lambda$.  We can then follow the rest of the proof in Section \ref{sxn:forward_lower} starting from eqn. (\ref{eq:intermediate_upper}) using the definitions of $\wb(\cdot)$ and $\wbbar(\cdot)$ in Section \ref{sxn:glm_upper} to get:
\begin{align*}
    \|\Pb_k\beta_k - &\beta_d\|_2^2 
    \geq \frac{1}{\alpha} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xb^T  \wb(\Pb_k\beta_k) \\
    &= \frac{1}{\alpha_u\|\Xb\|_2^2 + \lambda} \cdot  \beta_d^T (\Ib - \Pb_k\Pb_k^T) \Xb^T  \wb(\Pb_k\beta_k).
\end{align*}
%
Since $\alpha_u$ is a constant that only depends on the chosen GLM, we again conclude that, when the two-norm of the input matrix is constant, our upper and lower bounds differ by a factor of $\Ocal(\nicefrac{1}{\lambda})$.

