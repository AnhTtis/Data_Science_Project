% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}            
\usepackage[accsupp]{axessibility}


\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{fancyhdr}





% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
%\def\cvprPaperID{3212} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Multi-granularity Archaeological Dating of Chinese Bronze Dings Based on Knowledge-guided Relation Graph}
\title{Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph}

\author{
Rixin Zhou\textsuperscript{1} \qquad
Jiafu Wei\textsuperscript{1} \qquad
Qian Zhang\textsuperscript{3} \qquad
Ruihua Qi\textsuperscript{3} \qquad
Xi Yang\textsuperscript{1,2,}\textsuperscript{*} \qquad
Chuntao Li\textsuperscript{3,}\textsuperscript{*}\\
\textsuperscript{1}School of Artificial Intelligence, Jilin University\\
\textsuperscript{2}Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MoE, China \\
\textsuperscript{3}School of Archaeology, Jilin University\\
}
% \saythanks

% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% \and
% 3rd Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% \and
% 4th Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% \and
% 5th Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }


% \maketitle

%------------------


\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure1.pdf}
    \captionof{figure}{Typical examples of Chinese bronze dings from 4 dynasties and 11 periods. The columns from left to right show Shang (Early, Late), Western Zhou (Early, Mid, Late), Spring and Autumn (Early, Mid, Late), and Warring States (Early, Mid, Late). The timeline under the image is the time range for the corresponding periods, where B.C. indicates Before Christ.}
   % \captionof{figure}{Typical examples of Chinese bronze Dings from 4 dynasties and 11 periods, which coloums from left to right are Shang (Early, Late), Western Zhou (Early, Mid, Late), Spring and Autumn (Eerly, Mid, Late), Warring States (Early, Mid, Late). Timeline under the image is the time ranges for corresponding periods, where B.C. indicates Before Christ.}
  \label{fig1:examples}
\end{center}


}]
%------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% ABSTRACT
\begin{abstract}
   \vspace{-0.5cm}
   The archaeological dating of bronze dings has played a critical role in the study of ancient Chinese history. Current archaeology depends on trained experts to carry out bronze dating, which is time-consuming and labor-intensive. For such dating, in this study, we propose a learning-based approach to integrate advanced deep learning techniques and archaeological knowledge. To achieve this, we first collect a large-scale image dataset of bronze dings, which contains richer attribute information than other existing fine-grained datasets. Second, we introduce a multihead classifier and a knowledge-guided relation graph to mine the relationship between attributes and the ding era. Third, we conduct comparison experiments with various existing methods, the results of which show that our dating method achieves a state-of-the-art performance. We hope that our data and applied networks will enrich fine-grained classification research relevant to other interdisciplinary areas of expertise. The dataset and source code used are included in our supplementary materials, and will be open after submission owing to the anonymity policy. Source codes and data are available at: https://github.com/zhourixin/bronze-Ding.
\end{abstract}
\thispagestyle{fancy} % IEEE模板在\maketitle后会自动声明\thispagestyle{plain}，
                            % 导致第一页什么都没有。所以得把plain更改为fancy
      \fancyhead{} 
      \fancyfoot{} 
      \fancyfoot[L]{\footnotesize *Corresponding authors}
      \renewcommand{\headrulewidth}{0pt} %改为0pt即可去掉页眉下面的横线
      \renewcommand{\footrulewidth}{1pt} %改为0pt即可去掉页脚上面的横线
      \fancyfootoffset[R]{-14cm}
% \begin{abstract}
%    The archaeological dating of bronze Dings plays a critical role in the study of ancient Chinese history. Current archaeology depends on trained experts to carry out the dating of bronze Dings, which was time-consuming and labor-intensive. In this work, we propose a learning-based approach to integrate advanced deep learning techniques and archaeology knowledge in this dating task. To achieve this, we first collect a large-scale image dataset of bronze Dings, which contains richer attribute information than other existing fine-grained datasets. Second, we introduce a multi-head classifier and a knowledge-guided relation graph to mine the relation between attributes and the era of Dings. Third, we conduct comparison experiments with various existing methods, the results show that our method achieves state-of-the-art dating performance. We hope our data and 1 will enrich fine-grained classification research relevant to other interdisciplinary areas of expertise. The dataset and source code are included in our supplementary materials, and these will be open after submission due to the anonymous policy.
% \end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-10.5pt}
\section{Introduction}
\vspace{-4pt}
Dings are cauldrons used for cooking, storage, and ritual offerings to gods or ancestors in ancient China, and they are the most important species used in Chinese ritual bronzes~\cite{49}. The archaeological dating of dings has contributed to the study of ancient Chinese history. Although the excavated bronzes are massive, dating such artifacts depends on the long-term training and accumulation of expertise in archaeological typology~\cite{54}. In addition, some artifacts are easy to identify to a precise age and others are difficult to identify.
% Dings are cauldrons that are used for cooking, storage, and ritual offerings to the gods or to ancestors in ancient China, and it is the most important species used in Chinese ritual bronzes~\cite{49}. The archaeological dating of Dings contributes to studying ancient Chinese history. 
% Although the excavated bronzes are massive, dating these artifacts depends on the long term training and accumulation of experts currently in archaeological typology~\cite{54}. Besides, some artifacts are easy to be identified to a precise age, some are difficult. Even worse, due to the limited information, different judgments can be decided based on personal knowledge frequently. 

% \fancyfoot[LE]{Overleaf}

For the object, we focus on a ding, the features of which are similar and complicated in different eras, as shown in the columns of Figure~\ref{fig1:examples}. We therefore consider this dating task as a fine-grained classification problem. Simultaneously, research into fine-grained classification is close to that of other areas of expertise because it often requires expensive specialized data and knowledge areas, such as birds (zoology)~\cite{12,56} and flowers (botany)~\cite{55}.
% We focus on this one kind of object (Ding) whose features are similar and complicated from different eras, as shown in each column of Figure~\ref{fig1:examples}. Therefore, we deal with this dating task as a fine-grained classification problem. Simultaneously, fine-grained classification research is close to other areas of expertise, as they often require expensive specialized data and knowledge, such as birds (zoology)~\cite{12,56} and flowers (botany)~\cite{55}. 


Data features and domain knowledge, particularly in archaeology, vary in different fields. In addition to the common traits of the existing fine-grained datasets, our data are more challenging. First, our data are unbalanced and difficult to mitigate through their collection because they are determined based on an unearthed state. Second, there are more similarities between bronze dings of adjacent eras, leading to the possibility of misclassifying them into fine granularity adjacent eras beyond a coarse granularity. In other words, compared to other fine-grained classification data, our data have a larger intra-class difference and a smaller inter-class difference between adjacent eras. Third, the attributes and eras are intertwined and the relations are more complex. Each period of bronze dings has multiple shapes and characteristics, and each shape and characteristic correspond to multiple periods of bronze dings, leading to the impracticality of making simple judgments regarding the period based on the shape and characteristic. Existing fine-grained classification methods therefore struggle when applying our data.
% The data features and domain knowledge are different in different fields, especially archaeology. Besides the common traits of existing fine-grained datasets, our data is more challenging. First, Our data is more unbalanced and difficult to mitigate by collecting data, because they are determined by the unearthed situation. Second, more similarities between bronze Dings of adjacent eras, leading to the possibility of misclassifying into fine-granularity adjacent eras beyond coarse-granularity. In other words, compared to other fine-grained classification data, our data has a larger intra-class difference, and a smaller inter-class difference between adjacent eras. Thrid, attributes and eras are intertwined, and the relations are more complex. Each period of bronze Dings have multiple shape and characteristic, and each shape and characteristic correspond to multiple periods of bronze Ding, leading to the impracticality of making simple judgments about period based on shape and characteristic. Therefore, existing fine-grained classification methods struggle to perform well on our data.


To address these issues, we make the following contributions in this study:
% To deal with these issues, we make the following contributions in this paper:
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item We collect an image dataset of 3690 bronze dings with rich annotations made by bronze experts, including the era (4 course-grained dynasties and 11 fine-grained periods), attributes (29 shapes and 96 characteristics with bounding boxes), literature, location of excavation, and the museum where they are displayed. 
% \vspace{-4pt}
\item We build an end-to-end multihead network to solve this multi-granularity task. The two heads combine coarse- and fine-grained features in a bidirectional manner with a gradient truncated addition to improve the performance at both granularities. The outputs of other two heads, the shape and characteristic nodes, are added to a knowledge-guided relation graph to embed the domain knowledge into our network,  
\item We propose exploiting these rich attributes following archaeological knowledge by employing the focal-type probability classification loss and indicate the ineffectiveness of simply concatenating external information. 
\item We achieve the best performance in terms of the dating accuracy, outperforming other state-of-the-art (SOTA) fine-grained classification methods. 
\end{itemize}
% \begin{itemize}
% \setlength{\itemsep}{0pt}
% \setlength{\parsep}{0pt}
% \setlength{\parskip}{0pt}
% \item We collect an image dataset of 3690 bronze Dings with rich annotations by bronze experts, including eras (4 course-grained dynasties and 11 fine-grained periods), attributes (29 shapes and 96 characteristics with bounding boxes), literature, excavation, and museum. 
% % \vspace{-4pt}
% \item We build an end-to-end multi-head network to solve this multi-granularity task. Two heads combine coarse-grained and fine-grained features in a bi-directional manner with a gradient truncated addition to improve the performance at both granularities. The nodes of shape and characteristic are added in a knowledge guided graph to embed domain knowledge into our network. 
% \item We propose to exploit these rich attributes following archaeological knowledge by employing conditional probability classification loss, and indicate the ineffectiveness of simply concatenating external information. 
% \item We achieve the best performance on dating accuracy, outperforming that of other various state-of-the-art (SOTA) fine-grained classification methods. 
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
%------------------------------------------------------------------
\vspace{-4pt}
\subsection{Bronze Dings dating}
\vspace{-4pt}

In addition to manual inference~\cite{54, 28}, the chemical and physical properties of metals are also used to locate the exact year of a bronze ding~\cite{10,11,23, 24}. However, chemical and material science based techniques are time-consuming and difficult to manipulate, and they may cause irreversible damage to the bronze. Meanwhile, machine-learning-based methods have been used to explore bronze inscription recognition~\cite{15,21,22}. Although the meaning of the inscriptions on a bronze ding is important, it is insufficient for achieving an accurate dating. The automatic dating of bronze dings using artificial intelligence has been largely underexplored.
% Apart from manual inference~\cite{54, 28}, the chemical and physical properties of metals are also used to locate the exact year of the bronze Dings~\cite{10,11,23, 24}. However, the chemical and material science-based techniques are time-consuming and not easy to manipulate and may cause irreversible damage to the bronze. Meanwhile, machine learning-based methods have been used to explore bronze inscription recognition~\cite{15,21,22}. Although the meaning of inscriptions is important for bronze Dings, they are insufficient for accurate dating in themselves. The automatic dating of bronze Dings using artificial intelligence has largely been under-explored. 



\subsection{Fine-Grained Visual Classification}
\paragraph{Datasets.}
\vspace{-4pt}
Compared to a traditional image recognition task~\cite{3,4,5}, a fine-grained visual classification (FGVC) task is more challenging~\cite{44}. Although the variation between different categories of fine-grained data can be extremely small, the variation within the same category, owing to changes in pose and occlusions, is much broader, which leads to more difficulties. Several datasets have been proposed to address these challenges, including birds~\cite{12,56,57}, dogs~\cite{14}, airplanes~\cite{13}, flowers~\cite{55}, cars~\cite{58}, vegetables~\cite{59}, fruits~\cite{59}, foods~\cite{60}, fashion~\cite{61,62,41}, and retail products~\cite{63,64}.
% \paragraph{Datasets.} Compared with the traditional image recognition task~\cite{3,4,5}, the fine-grained visual classification(FGVC) task is more challenging~\cite{44}. The variation between different categories of fine-grained data can be very small, but the variation within the same category, due to pose changes and occlusions, is much broader, which leading to more difficulties. To address these challenges, there are many datasets already proposed, such as birds~\cite{12,56,57}, dogs~\cite{14}, airplanes~\cite{13}, flowers~\cite{55}, cars~\cite{58}, vegetables~\cite{59}, fruits~\cite{59}, foods~\cite{60}, fashion~\cite{61,62,41}, retail products~\cite{63,64}, etc.

\vspace{-10.5pt}


\paragraph{Single-Granularity Visual Classification.}
Single-granularity FGVC treats objects at the single-class level. Some studies~\cite{6,7,8,9,45,47} have been based on localization-classification networks to find the local features with differentiation and then combine the global features for fine-grained recognition. In addition, high-order feature interactions~\cite{29,30,31} and the design of specific loss functions~\cite{32,33,34,35,48} have resulted in significant improvements. In addition to conventional methods, to further assist fine-grained recognition tasks, some researchers leverage external information, such as attributes ~\cite{46,66,67,68,69}, web data~\cite{36,70,71}, multi-modal data~\cite{37,72,73}, or human-computer interactions~\cite{38,74}.
% Single-grained FGVC treat objects on a single class level. Some works~\cite{6,7,8,9,45,47} based on localisation-classification networks to find local features with differentiation and then combine global features for fine-grained recognition. Additionally, performing high-order feature interactions~\cite{29,30,31} and designing specific loss functions~\cite{32,33,34,35,48} have also resulted in significant improvements. Beyond the conventional methods, some researches leverage external information, such as attribute~\cite{46,66,67,68,69}, web data~\cite{36,70,71}, multi-modal data~\cite{37,72,73}, or human-computer interactions~\cite{38,74}, to further assist the fine-grained recognition task. 
\vspace{-10.5pt}

\paragraph{Multi-Granularity Visual Classification.}
Hierarchical multi-granularity structures can express richer information than single-granularity structures. We construct the network as a hierarchical structure, which has also been adapted in a number of other studies~\cite{39,40,1,20,2}. In protein function prediction, the output of each level is combined with the input of the next level, thus allowing the network to learn the features of each level jointly~\cite{39}. Parameters assigned for each task are used to encourage cross-task feature interactions~\cite{40}. Tree-structured tasks are constructed to integrate knowledge from the tree hierarchy~\cite{20,43} and conduct a feature transfer between levels~\cite{2,65}.
% Hierarchical multi-granularity structures can express richer information than single-granularity structures. We construct the network as a hierarchical structure, which has also been adapted in a number of works~\cite{39,40,1,20,2}. In protein function prediction, the output of each level is combined with the input of the next level, thus leading the network to learn the features of each level jointly~\cite{39}. Assigning parameters to each task is used to encourage cross-task feature interaction~\cite{40}. The tree-structured tasks are constructed to integrate the knowledge from tree hierarchy~\cite{20,43} and perform feature transfer between levels~\cite{2,65}. %Un
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bronze Ding Dataset}
\label{section3}
\vspace{-4pt}
\paragraph{Data Collection and annotation.}
\label{sec:data Collect and Anno}
We collect more than four thousand ding images from both five published archaeology books and four websites, and sort out 3690 images as our dataset. Some of these are line graphs. Because many dating results are controversial, we re-argue the era of each artifact through discussions with three bronze experts. The collection and labelling of data were carried out by an archaeologist and eight archaeology assists, who took approximately 8 months to complete.
% We collect more than more four images of Dings from 5 published archaeology books and 4 websites, and sort out 3690 consisting of our dataset. Few of them are line graphs. Due to many dating are controversial, we re-argue the era of each artifact discussing with 3 bronze experts. The collection and labelling of data is carried out by a archaeologist and 8 archaeology assists, taking about eight months to complete.

The collected dings belong to 4 course-grained dynasties and 11 fine-grained periods, and each image is annotated with additional annotations, as shown in Figure~\ref{fig3:bbox}, including:

\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item \textbf{Shape}: Single-category label for bronze ding shape, with 29 types in total.
\item \textbf{Characteristic}: Multi-category labels for the key components, decorations, and inscriptions, along with bounding boxes, with 96 types in total.
\item \textbf{Source}: Literature (studies to which these bronze ding images belong); excavation (location of excavation), and museum (current exhibition museums).
\end{itemize}
The detailed process used in the data annotation is described in the supplementary material.
% \begin{itemize}
% \item \textbf{Shape}: Single-category label for bronze Ding's shape, 29 types in total.
% \item \textbf{Characteristic}: Multi-category labels for key components, decorations and inscriptions, with Bounding boxes, 96 types in total.
% \item \textbf{Source}: Literature (The literatures to which these bronze Ding images belong); Excavation (Location of excavation); Museum (Current exhibition museums).
% \end{itemize}
% The detailed process of data annotation are described in the supplementary material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centering
  \includegraphics[width=0.98\linewidth]{figure3.pdf}
  \caption{Annotations of a bronze ding example. The characteristics are labeled by bounding boxes in the left figure, and the right shows the detailed information.}
  % \caption{The annotations of a bronze Ding example. The characteristics are labeled by bounding-boxes in the left figure, and the right shows the detailed information.}
  \label{fig3:bbox}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \vspace{-10.5pt}
\paragraph{Statistics.}
We also count the numbers of eras, shapes, and characteristic labels from different eras, the results of which are shown in Figure~\ref{fig4:bar}. The numbers of shapes and characteristic annotations varied considerably between eras.
% We also counted the number of era, shape, and characteristic labels in different eras, and the results are shown in Figure ~\ref{fig4:bar}. The number of shape and characteristic annotations varies considerably between eras.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\centering
  \includegraphics[width=0.98\linewidth]{figure4.pdf}
  \caption{Statistics showing the imbalance of our dataset.}
  % \caption{The statistics show the unbalance of our dataset.}
  \label{fig4:bar}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[t]
% \centering
% \caption{Comparison of our dataset and two existing datasets in terms of entropy, conditional entropy, and information gain. The information gain shows that the shape and characteristic annotations of our dataset provide richer information.}
% % \caption{Comparison of our dataset and two existing datasets in entropy, conditional entropy and information gain. The information gains show the shape and characteristic annotations in our dataset provide richer information.}
% \label{tab2:entropy}
% \resizebox{.48\textwidth}{!}{  % Here 1/2
% \begin{tabular}{c|cc|c|c}
% \hline\hline
%                                & \multicolumn{2}{c|}{Bronze Ding}                                          & CUB\_200\_2011~\cite{14}         & Deep Fashion~\cite{41}           \\ \hline\hline
% $H(D)$                       & \multicolumn{2}{c|}{3.459}                                           & 7.644                  & 5.644                  \\ \hline
% $H(D \mid A)$                   & \multicolumn{1}{c|}{1.826(characteristic)}       & 1.717(shape)           & 6.599                  & 4.789                  \\ \hline
% \multirow{2}{*}{$g(D, A)$} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{1.633}}} & \multirow{2}{*}{\textbf{1.742}} & \multirow{2}{*}{1.045} & \multirow{2}{*}{0.855} \\
%                                & \multicolumn{1}{c|}{}                       &                        &                        &                        \\ \hline
% \end{tabular}
% } 
% \end{table}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \setlength{\textfloatsep}{10pt}

\begin{table}[t]
\centering
\caption{Comparison of our dataset and five existing datasets in terms of category statistics, entropy, conditional entropy, and information gain. The information gain shows that the shape and characteristic annotations of our dataset provide richer information.}
\label{tab2:entropy}
\resizebox{0.48\textwidth}{!}{  % Here 1/2
\begin{tabular}{c|cc|c|cc|c|c|c}
\hline\hline
                                      & \multicolumn{2}{c|}{Bronze Ding}                                                                                                                                  & CUB\_200\_2011         & \multicolumn{2}{c|}{Deep Fashion}                                                                                                      & CompCars%~\cite{yang2015large} 
                                      & Stanford Dogs           & Food-101                \\ \hline\hline
Images                                & \multicolumn{2}{c|}{3690}                                                                                                                                         & 11788                  & \multicolumn{2}{c|}{289222}                                                                                                            & 136726                         & 20580                   & 101000                  \\ \hline
Image Categories                      & \multicolumn{2}{c|}{11}                                                                                                                                           & 200                    & \multicolumn{2}{c|}{50}                                                                                                                & 1716                           & 120                     & 101                     \\ \hline
Images per Category                   & \multicolumn{2}{c|}{$335\pm315$}                                                                                                                     & $59\pm3$  & \multicolumn{2}{c|}{$5784\pm11989$}                                                                                       & $79\pm48 $        & $171\pm23$ & $1000\pm0$ \\ \hline
\multirow{2}{*}{Attribute Categories} & \multicolumn{1}{c|}{96}                                                                        & 29                                                               & \multirow{2}{*}{312}   & \multicolumn{1}{c|}{1000}                                                     & 26                                                     & \multirow{2}{*}{218}           & \multirow{2}{*}{\textbf{$--$}}      & \multirow{2}{*}{\textbf{$--$}}      \\
                                      & \multicolumn{1}{c|}{$(characteristic)$}                                                          & $(shape)$                                                          &                        & \multicolumn{1}{c|}{$(coarse)$}                                                 & $(fine)$                                                 &                                &                         &                         \\ \hline
$H\left(D\right)$                                & \multicolumn{2}{c|}{3.459}                                                                                                                                        & 7.644                  & \multicolumn{2}{c|}{5.644}                                                                                                             & 10.745                         & \textbf{$--$}                       & \textbf{$--$}                       \\ \hline
\multirow{2}{*}{$H\left(D \mid A\right)$}        & \multicolumn{1}{c|}{1.826}                                                                     & 1.717                                                            & \multirow{2}{*}{6.599} & \multicolumn{1}{c|}{4.470}                                                    & 4.789                                                  & \multirow{2}{*}{9.570}         & \multirow{2}{*}{\textbf{$--$}}      & \multirow{2}{*}{\textbf{$--$}}      \\
                                      & \multicolumn{1}{c|}{$(characteristic)$}                                                          & $(shape)$                                                          &                        & \multicolumn{1}{c|}{$(coarse)$}                                                 & $(fine)$                                                 &                                &                         &                         \\ \hline
$g(D, A)$                             & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}1.633\\ $(characteristic)$\end{tabular}}} & \textbf{\begin{tabular}[c]{@{}c@{}}1.742\\ $(shape)$\end{tabular}} & 1.045                  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}1.174\\ $(coarse)$\end{tabular}} & \begin{tabular}[c]{@{}c@{}}0.855\\ $(fine)$\end{tabular} & 1.002                          & \textbf{$--$}                       & \textbf{$--$}                       \\ \hline
\end{tabular}
}
%\caption{\textcolor{red}{revise accordingly, remove caption? to save space} Comparison of our dataset and four existing datasets in terms of conditional entropy, information gain, etc.}
\end{table}
% \vspace{-20pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Comparison.}
To quantify the information provided by the additional annotations, we calculate the information gain of the shape and characteristic annotations of the era judgement, as shown in Equation (\ref{eq1:entropy}).
% In order to quantify the information provided by the additional annotations, we calculate the information gain of the shape and characteristic annotations to the era judgement, as Equation (\ref{eq1:entropy}). 

\begin{equation}
g(D, A)=H(D)-H(D \mid A)
\label{eq1:entropy}
\end{equation}
where $\mathrm{H}(\mathrm{D})$ is the entropy of the fine-grained labels on dataset $D$ and $\mathrm{H}(\mathrm{D}\mid\mathrm{A})$ is the conditional entropy of attribute $A$ on dataset $D$.
% Where $\mathrm{H}(\mathrm{D})$ is the entropy of fine-grained labels on dataset $D$, and $\mathrm{H}(\mathrm{D}\mid\mathrm{A})$ is the conditional entropy of attributes $A$ on dataset $D$.

For comparison, the entropy in CUB-200-2011~\cite{14} and Deep-Fashion~\cite{41} are also calculated. The results are presented in Table~\ref{tab2:entropy}. We find that the information gain in our dataset is more significant than that of the other datasets, which means that our shape and characteristic annotations provide richer information. Such information is therefore critical for improving the network performance.
% For comparison, those in datasets CUB-200-2011~\cite{14} and Deep-Fashion~\cite{41} also be calculated. The results are shown in Table~\ref{tab2:entropy}. We can find that the information gain in our dataset is more significant than that of others, which means our shape and characteristic annotations provide richer information. Therefore, this information is critical to improve the performance of the network. 







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htbp]
\centering
  \includegraphics[width=1.0\linewidth]{figure2.pdf}
  \caption{Overview of our network. We design four heads: two heads in MGM are responsible for extracting dynasty and period features at two granularities. Two other heads in KEM are responsible for extracting shape and characteristic features of the bronze dings. Then, the outputs of MGM and KEM jointly build our AKG to formulate the relationship between the eras and attributes of each ding through the graph loss. Simultaneously, the outputs are also used to compute cross-entropy and focal losses to enhance the learning of the annotations on each head.}
  \label{fig2:architecture}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\vspace{-4pt}
\subsection{Overview}
\vspace{-4pt}
To address the challenges to our task, we construct a multihead network for predicting the era of bronze dings on an archaeology knowledge-guided relation graph, as shown in Figure ~\ref{fig2:architecture}. The network consists of three parts: a multi-granularity module (MGM), knowledge extraction module (KEM), and archaeology knowledge-guided relation graph (AKG). Compared to HRN~\cite{2}, first, we additionally enhance the dynasty (coarse) features without affecting the period (fine) dating performance by adding the feature of period head to the dynasty head with gradient truncation, and then feeding into the next FC layer. Second, we implement KEM in our network and extend the relation graph to leverage the features of our dataset by considering attributes information (shape and characteristic) in the knowledge graph. Third, we design a focal-type probability classification loss to learn the relationship between attributes and eras from easy (shape) to difficult (characteristic), instead of just learning category information.


\subsection{Network Architecture}
\vspace{-4pt}
\paragraph{Multi-granularity Module.}

The MGM is built to combine dynasty features with period features and enhance them interactively. After the backbone network, dynasty and period features are separately extracted by two heads consisting of two convolution layers and two fully connected layers. Then, in addition to applying an element-wise addition from dynasty head to period head to enrich period information, an element-wise gradient truncation addition is applied in reverse to enhance dynasty features without affecting the period performance.

% The MGM is built to combine dynasty features with period features and enhance their features. In MGM, a bronze ding's dynasty and period features are extracted by a series of convolution layers and full connection layers. The dynasty features apply an element-wise addition to the period features to enrich the period information. Meanwhile, the period features apply an element-wise gradient truncated addition to the dynasty features, enhancing the dynasty features without undermining the performance of the period classification.

% The MGM is built to combine dynasty features with period features and enhance their granularity features. We use two heads to extract the dynasty and period features of a bronze ding and then add the dynasty features to the period features to enrich the period features. Specifically, the period features apply an element-wise gradient truncated addition to the dynasty features, enhancing the dynasty features without undermining the performance of the period classification.
% MGM is built to combine dynasty features with period features to enhance both granularity features. We use two heads to extract dynasty features and period features of the bronze Ding, and then add the dynasty features to the period features to enrich the period features. Specifically, the period features perform element-wise gradient truncated addition to the dynasty features, enhancing the dynasty features without undermining the performance of the period classification. 


For the outputs, dynasty head applies a $sigmoid$ projection, and then the result forms the dynasty node of the AKG. The period head applies $sigmoid$ and $softmax$ projections, where the $sigmoid$ output forms the period node of the AKG, and the $softmax$ output computes the cross-entropy loss $\mathcal{L}_{ce}$ to enhance the exclusive relation between the period nodes.
% The dynasty head of the MGM perform the $sigmoid$ projection and the result forms the dynasty node of the AKG. Then the period head of the MGM perform $sigmoid$ and $softmax$ projection respectively, where the $sigmoid$ output forms the period node of the AKG and the $softmax$ output computes the cross-entropy loss $\mathcal{L}_{ce}$ to enhance the exclusive relation between the period nodes.

\vspace{-10.5pt}
\paragraph{Knowledge Extraction Module.}
Because of the narrow inter-class differences and wide intra-class variations of the bronze ding, archaeologists must combine various factors to determine the era to which they belong. We therefore introduce domain knowledge to improve the performance of the network, including the shape and characteristic annotations. Specifically, we design a KEM for extracting the shape and characteristic information of a bronze ding, which consists of two separate heads for extracting knowledge-specific features.
% Because of the narrow inter-class differences and the wide intra-class variations of the bronze Ding images, even archaeologists have to combine various dimensions to determine the era to which they belong. We therefore introduce additional expert knowledge to help improve the performance of the network, including shape and characteristic annotations. Specifically, we designed the knowledge extraction module (KEM) to extract the shape and characteristic information of the bronze Ding, which consists of two separate heads for extracting the knowledge-specific features, respectively.


The shape head of the KEM applies $sigmoid$ and $softmax$ projections. From Figures~\ref{fig4:bar} (b) and (c), we can see that the shape and characteristic categories are unbalanced; therefore, we compute $\mathcal{L}_{\text {focal}}$~\cite{42} between its $softmax$ output and the shape labels to alleviate the category imbalance problem and enhance the exclusive relation between the shape nodes. Furthermore, because the classification of the characteristics is a multilabel classification task, the characteristic head of the KEM only applies a $sigmoid$ projection, and we compute the multilabel $\mathcal{L}_{\text {ml-focal}}$ between its $sigmoid$ output and the characteristic labels. The $sigmoid$ outputs of these two heads are also fed to the AKG.
% The shape head of the KEM perform $sigmoid$ and $softmax$ projection respectively. From the Figure ~\ref{fig4:bar} (b) and (c), we find that the shape and characteristic categories are unbalanced, therefore, we compute the $\mathcal{L}_{\text {focal}}$~\cite{42} between its $softmax$ output and shape labels to alleviate the category imbalance problem and model the exclusive relation between the shape nodes. As the classification of characteristic is a multi-label classification task, the characteristic head of the KEM only perform $sigmoid$ projectionwe. We compute the multi-label $\mathcal{L}_{\text {ml-focal}}$ between its $sigmoid$ output and characteristic labels. And the $sigmoid$ output of these two heads are also fed to the AKG.
%------------------------------------------------------------------


\subsection{Archaeology Knowledge Guided Relation Graph}
\vspace{-4pt}
\paragraph{The Formalism of Relation Graph.}
Inspired by the study in~\cite{2,20}, we develop an archaeological knowledge-guided relation graph embedded with domain knowledge to enable the network to synthetically learn the era, shape, and characteristic labels. The nodes of the relation graph are the types of eras and attributes from MGM and KEM, and a set of directed edges and undirected edges are defined between these nodes. A directed edge is a subsumption edge that indicates that the parent nodes subsume the child node. An undirected edge is an exclusion edge and indicates that the two nodes are mutually exclusive.
% Inspired by the work of~\cite{2,20}, we develop an archaeology knowledge guided relation graph embedded with expert knowledge to enable the network to synthetically learn the era, shape and characteristic labels. The nodes of relation graph are the types of eras and attributes from MGM and KEM, and a set of directed edges and undirected edges defined between these nodes. A directed edge is a subsumption edge, indicating that the parent nodes subsumes the child node. An undirected edge is an exclusion edge, denoting that two nodes are mutually exclusive. 




According to archaeological knowledge, we conclude that the relations of the edges and nodes are as follows:
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item Because a bronze ding cannot belong to two eras at the same time, any two dynasty or period nodes only have an exclusive edge between them. The relation of the era node subsumes in the dynasty node and can be expressed as a subsumption edge.
\item Although one bronze ding only contains one type of shape node, multiple dings within the same period contain multiple shape nodes. Therefore, the period and shape nodes have multiple subsumption edges. In addition, because a bronze ding cannot have two shapes, any two period nodes have exclusive edges between them.
\item A bronze ding may contain multiple characteristics, and the period and characteristic nodes therefore have multiple subsumption edges.
\end{itemize}
Based on these relations, we define an extended legal global assignment of all labels in the relation graph as binary-label vectors for an object. Beyond~\cite{2}, we consider the shape and characteristics of a node assignment. The set of all legal global assignments forms the era state space $S_{G_e} \subseteq \left\{0, 1\right\}^n$, era-shape combination state space $S_{G_{es}} \subseteq \left\{0, 1\right\}^{n+m}$, and era-characteristic combination state space $S_{G_{ec}} \subseteq \left\{0, 1\right\}^{n+k}$ of relation graph $G$, where $n$, $m$, and $k$ denote the number of nodes for the eras, shapes, and characteristics, respectively. Thus, we calculate the probabilistic classification loss on $G$, enabling the network to improve its judgement of the era by learning domain knowledge of the shapes and characteristics.
% According to archaeological knowledge, we conclude that the relations of edge-node are:
% \begin{itemize}
% \item Since a bronze Ding cannot belong to two eras at the same time, any two dynasty or period nodes only have exclusive edge between them. And the relation of era node subsumes in dynasty node can be expressed as subsumption edge.
% \item Although one bronze Ding only contains one type of shape node, multiple dings within the same period contain multiple shape nodes. Therefore, the period node and the shape node have multiple subsumption edges. Besides, since a bronze Ding cannot has two shapes, any two period nodes have exclusive edge between them.
% \item A bronze Ding may contain multiple characteristics, therefore, the period node and the characteristic node have multiple subsumption edges.
% \end{itemize}
% Based on these relations, we define a extended legal global assignment of all labels in the hierarchy as some binary label vectors for an object. Beyond~\cite{2}, we take the shape and characteristic into account for node assignment. The set of all legal global assignments forms the era state space $S_{G_e} \subseteq \left\{0, 1\right\}^n$,  the era-shape state space $S_{G_{es}} \subseteq \left\{0, 1\right\}^{n+m}$, the era-characteristic state space $S_{G_{ec}} \subseteq \left\{0, 1\right\}^{n+k}$ of relation graph $G$, where $n,m,k$ denote the number of nodes for eras, shapes and attributes, respectively. Thus, we can calculate the conditional probabilistic classification loss on $G$, enabling the network to improve its judgement of era by learning expert knowledge of shape and characteristic.

\vspace{-10.5pt}

% \paragraph{Conditional Probabilistic Classification Loss.}
\paragraph{Focal-type Probabilistic Classification Loss.} 
The two types of attributes have different effects on the dating by archaeologists. There are a few types of shapes, which are relatively easy to distinguish. However, the ding shape type is not decisive for dating. Meanwhile, some characteristic types can accurately define this era. However, the number of characteristic types is large, and they are both similar and complex. Based on this knowledge, we construct probabilistic classification losses to enable the network to learn information in the relation graph. 

During training, we obtain the predicted label in the relation graph and maximized its marginal probability in a step-by-step manner. Given an input image $\mathbf{x}$, the unnormalized era joint probability of all era nodes concerning the era label assignment $\mathbf{y_e}$ can be computed as $\tilde{P}_{e}(\mathbf{y_{e}} \!\!\mid\!\!  \mathbf{x})$.
The era joint probability is then normalized by $\operatorname{Pr_{e}}(\mathbf{y_{e}} \mid \mathbf{x})=\frac{\tilde{P_{e}}(\mathbf{y_{e}} \mid \mathbf{x})}{Z_e(\mathbf{x})}$, where $Z_e(\mathbf{x})$ is the era partition function that sums over all legal era assignments $\overline{\mathbf{y}}_e \in S_{G_e}$ in the era state space. If input image $\mathbf{x}$ has the $i$-th era label, we can obtain the era marginal probability $\operatorname{Pr_{e}}(y_{e_i}=1 \mid \mathbf{x})$ of era label $i$ by summing over all legal era assignments $\overline{\mathbf{y}}_e$ that include $\overline{y}_{e_i}$ = 1. Procedures for calculating normalized era-shape joint probability $\operatorname{Pr_{es}}(\mathbf{y}_{es} \mid \mathbf{x})$ and era-characteristic joint probability$\operatorname{Pr_{ec}}(\mathbf{y}_{ec} \mid \mathbf{x})$ are the same. The details of the calculations are described in the supplementary material.



% During training, we obtain the predicted label in the relation graph and maximized its marginal probability in a step-by-step manner. Given an input image $\mathbf{x}$, we first calculate the unnormalized era probability $\tilde{P}_{e}(\mathbf{y_{e}} \!\!\mid\!\!  \mathbf{x})$, the unnormalized era-shape combination probability $\tilde{P}_{es}(\mathbf{y_{es}} \!\!\mid\!\!  \mathbf{x})$, and the unnormalized era-characteristic combination probability $\tilde{P}_{ec}(\mathbf{y_{ec}} \!\!\mid\!\! \mathbf{x})$ of the corresponding nodes, respectively. The probability is then normalized by $\operatorname{Pr}(\mathbf{y} \mid \mathbf{x})=\frac{\tilde{P}(\mathbf{y} \mid \mathbf{x})}{Z(\mathbf{x})}$, where $Z(\mathbf{x})$ is the partition function that sums over all legal assignments $\overline{\mathbf{y}} \in \left\{ S_{G_e}, S_{G_{es}}, S_{G_{ec}}\right\}$ of $G$. After that, we can obtain the marginal probability of the labels of \mathbf{x}. The details of the calculations for the probability and partition functions are described in the supplementary material.
% Given an input image $x$, we calculate the unnormalized era probability $\tilde{P}_{e}(\mathbf{y_{e}} \!\!\mid\!\!  \mathbf{x})$, the unnormalized era-shape combination probability $\tilde{P}_{es}(\mathbf{y_{es}} \!\!\mid\!\!  \mathbf{x})$ and the unnormalized era-characteristic combination probability $\tilde{P}_{ec}(\mathbf{y_{ec}} \!\!\mid\!\! \mathbf{x})$ of corresponding nodes. The probability is then normalized by $\operatorname{Pr}(\mathbf{y} \mid \mathbf{x})=\frac{\tilde{P}(\mathbf{y} \mid \mathbf{x})}{Z(\mathbf{x})}$, where $Z(x)$ is the partition function that sums over all legal assignments $\overline{\mathbf{y}} \in \left\{ S_{G_e}, S_{G_{es}}, S_{G_{ec}}\right\}$ of $G$. Thus we can calculate the marginal probability of $x$'s labels. Details of the calculations for the probability and partition function are described in the supplementary material.


Given $m$ training samples, $\mathcal{D}=\left\{x^{l}, y_e^{l}, y_{es}^{l}, y_{ec}^{l}, g_{e}^{l}, \notag\right. \\ \left. g_{es}^{l}, g_{ec}^{l}\,\right\}$, $l=1, \ldots, m$, where $y_e^{l}$, $y_{es}^{l}$ and $y_{ec}^{l}$ are the ground-truth label vector of the era, era-shape combination, and era-characteristic combination, respectively. And $g_{e}^{l}  \in 
 \left\{1,\ldots , n\right\} $, $g_{es}^{l}\in \left\{1,\ldots , n+m\right\}$, $g_{ec}^{l}\in \left\{1,\ldots , n+k\right\}$ are the indices of the observed era, era-shape combination, and era-characteristic combination labels, respectively. Subsequently, the era probabilistic classification loss $\mathcal{L}_{\text {e}}(\mathcal{\!D\!})$ is defined as follows:
 \begin{equation}
-\frac{1}{m} \sum_{l=1}^{m} \ln (\operatorname{Pr_e}(y_{e_{g_{e} ^{l}}}^{l}=1 \mid \mathbf{x}^{l})\!)
\label{eq:Lp}
\end{equation}
 
% Given $m$ training samples, $\mathcal{D}=\left\{x^{l}, y_e^{l}, y_{es}^{l}, y_{ec}^{l}, g_{e}^{l}, \notag\right. \\ \left. g_{es}^{l}, g_{ec}^{l}\,\right\}$, $l=1, \ldots, m$, where $y_e^{l}$, $y_{es}^{l}$ and $y_{ec}^{l}$ are the ground-truth label vector of the era, era-shape combination, and era-characteristic combination, respectively. And $g_{e}^{l}$, $g_{es}^{l}$, $g_{ec}^{l}$ are the indices of the observed era, era-shape combination, and era-characteristic combination labels, respectively. Subsequently, the era probabilistic classification loss $\mathcal{L}_{\text {e}}(\mathcal{\!D\!})$ is defined as follows:
% Given $m$ training samples, $\mathcal{D}=\left\{x^{l}, y_e^{l}, y_{es}^{l}, y_{ec}^{l}, g_{e}^{l}, \notag\right. \\ \left. g_{es}^{l}, g_{ec}^{l}\,\right\}$, $l=1, \ldots, m$, where $y_e^{l}$ is the era ground-truth label vector, $y_{es}^{l}$ is the era-shape combination ground truth label vector, $y_{ec}^{l}$ is the era-characteristic combination ground truth label vector, and $g_{e}^{l}$, $g_{es}^{l}$, $g_{ec}^{l}$ are the indices of the observed era, era-shape combination, and era-characteristic combination labels, respectively. Subsequently, the era probabilistic classification loss $\mathcal{L}_{\text {e}}(\mathcal{\!D\!})$ is defined as follows:


Then, because of the different importance of attributes, we define the focal-type era-shape probabilistic classification loss $\mathcal{L}_{\text {es}}(\mathcal{\!D\!})$ and the era-shape-characteristic probabilistic classification loss $\mathcal{L}_{\text {esc}}(\mathcal{\!D\!})$ as follows:
\begin{equation}
-\frac{1}{m} \!\!\displaystyle\sum_{l=1}^{m}\!\!\left(\!\!(\!1\!-\!\operatorname{Pr_{e}}\!(y_{e_{g_{e} ^{l}}}^{l}\!\!\!=\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)^{	\alpha_1} \! \ln  (\operatorname{Pr_{es}}(y_{{es}_{g_{es} ^{l}}}^{l}\!\!\!\!\!\!=\!\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)\!\!\right)
\label{eq:Le-s}
\end{equation}

\vspace{-10.5pt}

\begin{equation}
-\frac{1}{m} \!\!\displaystyle\sum_{l=1}^{m}\!\!\left(\!\!(\!1\!\!-\!\!\operatorname{Pr_{es}}\!(y_{{es}_{g_{es} ^{l}}}^{l}\!\!\!\!\!\!=\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)^{	\alpha_2} \! \ln  (\!\operatorname{Pr_{ec}}(y_{{ec}_{g_{ec} ^{l}}}^{l}\!\!\!\!\!=\!\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)\!\!\right)
\label{eq:Le-a}
\end{equation}

Thus, when the network learns sufficient information through the era features of a given sample to determine its era, the influence of the shape and characteristics can be weakened by decay factor $\alpha_1$ and $\alpha_2$. When the network cannot learn a sufficient amount of information to determine the era of this sample, $\mathcal{L}_{\text {es}}(\mathcal{\!D\!})$ plays a supportive role. When neither the era features nor the shape features can provide sufficient information, $\mathcal{L}_{\text {esc}}(\mathcal{\!D\!})$ will contribute to determining the chronology of this sample by learning the relationship between the era and characteristics. In this manner, the network can adaptively adjust its learning of the shape and characteristics according to the amount of information possessed by different samples, thereby avoiding a disturbance of the main task.
% Given $m$ training samples $\mathcal{D}=\left\{x^{l}, y_e^{l}, y_{es}^{l}, y_{ec}^{l}, g_{e}^{l}, \notag\right. \\ \left. g_{es}^{l}, g_{ec}^{l}\,\right\}$, $l=1, \ldots, m$, where $y_e^{l}$ is the era ground truth label vector, $y_{es}^{l}$ is the era-shape combination ground truth label vector, $y_{ec}^{l}$ is the era-characteristic combination ground truth label vector, and $g_{e}^{l}$, $g_{es}^{l}$, $g_{ec}^{l}$ are the index of the observed era label, era-shape combination label, era-characteristic combination label, respectively. Then, the era probabilistic classification loss $\mathcal{L}_{\text {e}}(\mathcal{\!D\!})$ is defined as: 
%  \begin{equation}
% -\frac{1}{m} \sum_{l=1}^{m} \ln (\operatorname{Pr_e}(y_{g_{e} ^{l}}^{l}=1 \mid \mathbf{x}^{l})\!)
% \label{eq:Lp}
% \end{equation}
% The era-shape conditional probabilistic classification loss $\mathcal{L}_{\text {es}}(\mathcal{\!D\!})$ and era-characteristic conditional probabilistic classification loss $\mathcal{L}_{\text {ec}}(\mathcal{\!D\!})$ are defined as: 
% \begin{equation}
% -\frac{1}{m} \!\!\displaystyle\sum_{l=1}^{m}\!\left(\!\!(\!1\!-\!\operatorname{Pr_{e}}\!(y_{g_{e} ^{l}}^{l}\!\!\!=\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)^{	\alpha} \! \ln  (\operatorname{Pr_{es}}(y_{g_{es} ^{l}}^{l}\!\!\!\!\!\!\!=\!\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)\!\!\right)
% \label{eq:Le-s}
% \end{equation}
% \begin{equation}
% -\frac{1}{m} \!\!\displaystyle\sum_{l=1}^{m}\!\!\left(\!\!(\!1\!\!-\!\!\operatorname{Pr_{es}}\!(y_{g_{es} ^{l}}^{l}\!\!\!\!\!\!=\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)^{	\alpha} \! \ln  (\!\operatorname{Pr_{ec}}(y_{g_{ec} ^{l}}^{l}\!\!\!\!\!=\!\!1 \!\!\mid \!\!\mathbf{x}^{l})\!)\!\!\right)
% \label{eq:Le-a}
% \end{equation}
% Therefore, when the network learns enough information through the era features of a given sample to determine its era, the influence of shape and characteristic can be weakened by a decay factor $\alpha$. When the network is not able to learn enough information to determine the era of this sample, the $\mathcal{L}_{\text {es}}(\mathcal{\!D\!})$ will play a supportive role. And when neither the era features nor the shape features can provide enough information, the $\mathcal{L}_{\text {ec}}(\mathcal{\!D\!})$ will contribute to determine the chronology of this sample by learning the relation between era and characteristic. In this way, the network can adaptively adjust its learning of shape and characteristic according to the amount of information possessed by different samples, avoiding disturbing the main task. 



Finally, the aforementioned losses are added in a linear manner to form a complete probabilistic classification loss:
\begin{equation}
\mathcal{L}_{\text {graph}}(\mathcal{D})= \mathcal{L}_{\text {e}}(\mathcal{D}) + \beta \cdot (\mathcal{L}_{\text {es}}(\mathcal{D}) + \mathcal{L}_{\text {esc}}(\mathcal{D}))
\label{eq:graph}
\end{equation}
where $\beta$ denotes the weight used to balance the influence of the loss components. 
% Finally, the aforementioned losses are added in a linear manner to form a complete probabilistic classification loss:
% \begin{equation}
% \mathcal{L}_{\text {graph }}(\mathcal{D})= \mathcal{L}_{\text {e}}(\mathcal{D}) + \beta \cdot (\mathcal{L}_{\text {es}}(\mathcal{D}) + \mathcal{L}_{\text {ec}}(\mathcal{D}))
% \label{eq:graph}
% \end{equation}
% where $\beta$ is the weight to balance the influence of loss components. 



\subsection{Total Loss}
\vspace{-4pt}
In addition to the probabilistic classification loss $\mathcal{L}_{graph}$, we also use $\mathcal{L}_{ce}$, $\mathcal{L}_{focal}$, and $\mathcal{L}_{ml-focal}$ to enable the network to learn the era, shape, and characteristic categories of bronze dings, respectively. 
In summary, the total loss is given as a summation of the aforementioned losses with a trade-off parameter, $\lambda$:
\begin{equation}
\mathcal{L}_{\text {total }}(\!\mathcal{D}\!)\!=\!\mathcal{L}_{\text {graph }}(\!\mathcal{D}\!) \!+\! \mathcal{L}_{\text {ce}}(\!\mathcal{D}\!)\! +\! \lambda \!\cdot \!(\mathcal{L}_{\text {focal}}(\!\mathcal{D}\!) \!+\! \mathcal{L}_{\text {ml-focal}}(\!\mathcal{D}\!))
\label{eq:all}
\end{equation}
% Besides probabilistic classification loss $\mathcal{L}_{graph}$, we also use $\mathcal{L}_{ce}$, $\mathcal{L}_{focal}$, and $\mathcal{L}_{ml-focal}$ to enable the network to learn the era, shape, and characteristic categories of the bronze Dings, respectively. 

% In summary, the total loss is given as a summation of aforementioned losses with one trade-off parameter $\lambda$:
% \begin{equation}
% \mathcal{L}_{\text {total }}(\!\mathcal{D}\!)\!=\!\mathcal{L}_{\text {graph }}(\!\mathcal{D}\!) \!+\! \mathcal{L}_{\text {ce}}(\!\mathcal{D}\!)\! +\! \lambda \!\cdot \!(\mathcal{L}_{\text {focal}}(\!\mathcal{D}\!) \!+\! \mathcal{L}_{\text {ml-focal}}(\!\mathcal{D}\!))
% \label{eq:all}
% \end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%------------------------------------------------------------------
\vspace{-4pt}
\subsection{Data Preparation}
\vspace{-4pt}
We split our data into three sets: the scales of the training set, validation set, and test set are 4:1:5 (1470:363:1857), respectively, following the divisions of other fine-grained classification datasets~\cite{12,13}. Due to the data imbalance, we keep the proportions of each dynasty and period divided in the same way. For the pre-processing, we apply data augmentation to the collected images, including background removal and grayscale.
% We split our data into three sets, the scales of train set, validatation set, and test set are 4:1:5 (1470:363:1857), respectively, following the divisions of other fine-grained classification datasets~\cite{12,13}. Because of data unbalance, we keep the proportion of each dynasty and period are divided in the same way. For pre-processing, we apply data augmentation to the collected images to improve the prediction results, including background removal, gray-scale and feature line extraction. 

%------------------------------------------------------------------
\subsection{Implementation Details}
\label{ImDetails}
\vspace{-4pt}
We implement our network using PyTorch~\cite{51} and conduct experiments on a workstation equipped with an NVIDIA RTX 3090 GPU. For a fair comparison, we also adopt ResNet50 pretrained on ImageNet as our network backbone and resized the input images to 400$\times$400 throughout the experiments. We train each experiment for 64 epochs with early stopping and use Adam optimizer with a learning rate of 0.0001, adjusted using a cosine annealing strategy~\cite{27} to optimize our network. The batch size is set to 32. Besides, We set the decay factors $\alpha_1=2$ and $\alpha_2=3$ in Equations (\ref{eq:Le-s}) and (\ref{eq:Le-a}), balance weight $\beta=0.001$ in Equation (\ref{eq:graph}), and trade-off parameter $\lambda=0.1$ in Equation (\ref{eq:all}). And the parameter settings in $\mathcal{L}_{focal}$ and $\mathcal{L}_{ml-focal}$ follow~\cite{42}. The impact of hyper-parameters are also analysed in supplementary materials. Based on these implementation details, we set the HRN~\cite{2} as our baseline model.
% We implemented our network with PyTorch~\cite{51}, and conducted experiments on a workstation equipped with one NVIDIA RTX 3090 GPU. For fair comparison, we also adopt ResNet50 pre-trained on ImageNet as our network backbone and resize input images to 400$\times$400 throughout the experiments. We train each experiment for 64 epochs with early stopping and use adaptive moment estimation (Adam) with a learning rate of 0.0001 adjusted by the cosine annealing strategy~\cite{27} to optimize our network. The batch size is set to 32.  We set decay factor $\alpha=3$ in Equation (\ref{eq:Le-s}) and (\ref{eq:Le-a}), the balance weight $\beta=0.001$ in Equation (\ref{eq:graph}), and trade-off parameter $\lambda=0.1$ in Equation (\ref{eq:all}). Based on these implementation details, we set HRN~\cite{2} as our baseline model.

\vspace{-10.5pt}

\paragraph{Evaluation.} 
We use the overall accuracy ($OA$) and the area under the average precision and recall curve $AU(\overline{PRC})$ to evaluate the dating performance.
% We use the overall accuracy($OA$) and the area under the average precision and recall curve $AU(\overline{PRC})$ to evaluate the performance of dating performance. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table} 
\centering
\caption{Ablation study of each component in our network.} 
\label{tab:abalation}
\resizebox{.45\textwidth}{!}{  % Here 1/2
\begin{tabular}{c|c|cc|cccc|cc}
\hline\hline
\multirow{3}{*}{} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}MGM \\ w/ Truncated\end{tabular}} & \multicolumn{2}{c|}{KEM}                                 & \multicolumn{4}{c|}{AKG}                                                               & \multirow{3}{*}{Dynasty $OA$} & \multirow{3}{*}{Period $OA$} \\ \cline{3-8}
                  &                                                                              & \multirow{2}{*}{Shape} & \multirow{2}{*}{Characteristic} & \multicolumn{2}{c|}{Shape}                       & \multicolumn{2}{c|}{Characteristic} &                             &                            \\ \cline{5-8}
                  &                                                                              &                        &                                 & Concat       & \multicolumn{1}{c|}{Embed}    & Concat           & Embed        &                             &                            \\ \hline\hline
1                 &                                                                              &                        &                                 &              & \multicolumn{1}{c|}{}             &                  &                  & 85.28                       & 75.81                      \\ \hline
2                 & $\checkmark$                                                                 &                        &                                 &              & \multicolumn{1}{c|}{}             &                  &                  & 86.85                       & 77.05                      \\ \hline
3                 & $\checkmark$                                                                 & $\checkmark$           &                                 &              & \multicolumn{1}{c|}{}             &                  &                  & 84.54                       & 75.54                      \\ \hline
4                 & $\checkmark$                                                                 & $\checkmark$           &                                 & $\checkmark$ & \multicolumn{1}{c|}{}             &                  &                  & 86.53                       & 76.51                      \\ \hline
5                 & $\checkmark$                                                                 & $\checkmark$           &                                 &              & \multicolumn{1}{c|}{$\checkmark$} &                  &                  & 87.71                       & 77.86                      \\ \hline
6                 & $\checkmark$                                                                 &                        & $\checkmark$                    &              & \multicolumn{1}{c|}{}             &                  &                  & 87.23                       & 77.37                      \\ \hline
7                 & $\checkmark$                                                                 &                        & $\checkmark$                    &              & \multicolumn{1}{c|}{}             & $\checkmark$     &                  & 87.50                       & 77.10                      \\ \hline
8                 & $\checkmark$                                                                 &                        & $\checkmark$                    &              & \multicolumn{1}{c|}{}             &                  & $\checkmark$     & 87.72                       & 77.98                      \\ \hline
9                 & $\checkmark$                                                                 & $\checkmark$           & $\checkmark$                    & $\checkmark$ & \multicolumn{1}{c|}{}             & $\checkmark$     &                  & 86.80                       & 76.62                      \\ \hline
10                & $\checkmark$                                                                 & $\checkmark$           & $\checkmark$                    &              & \multicolumn{1}{c|}{$\checkmark$} &                  &                  & 87.12                       & 77.05                      \\ \hline
11                &                                                                              & $\checkmark$           & $\checkmark$                    &              & \multicolumn{1}{c|}{$\checkmark$} &                  & $\checkmark$     & 87.45                       & \textbf{78.83}             \\ \hline
12                & $\checkmark$                                                                 & $\checkmark$           & $\checkmark$                    &              & \multicolumn{1}{c|}{$\checkmark$} &                  & $\checkmark$     & \textbf{88.79}              & \textbf{78.83}             \\ \hline
\end{tabular}
} 
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation Study}
\vspace{-4pt}
We evaluate different combinations of the proposed components and followed the default parameter settings described in Section ~\ref{ImDetails}. The results are listed in Table ~\ref{tab:abalation}.
% We evaluated different combinations of the proposed components and follow the default parameter setting in Section ~\ref{ImDetails}. The results are shown in Table ~\ref{tab:abalation}.
% \vspace{-16pt}
\vspace{-10.5pt}
\paragraph{Multi-Granularity Module (MGM).} 
We apply an element-wise gradient truncated addition, from the period features to the dynasty features, forming a bi-directional interaction structure. Simple but efficient, this (1$\rightarrow$2) improves the dynasty dating accuracy of the network by $1.57\%$ and the period dating accuracy by $1.24\%$. The removal of the gradient truncated addition from the complete model (11$\rightarrow$12) leads to a $1.34\%$ decrease in the dynasty dating accuracy, while maintaining the period dating accuracy. %This result is also in line with our original intention, and strengthens the dynasty dating while not affecting the period dating.  
This result is also in line with our original intention, which strengthens the dynasty dating while not affecting the period dating.
% We perform element-wise gradient truncated addition from period features to dynasty features, forming a bi-directional interaction structure. Simple but efficient, this \textcolor{red}{(1$\rightarrow$2)} improves the network's dynasty dating accuracy by $1.57\%$ and period dating accuracy by $1.24\%$ respectively. And the removal of gradient truncated addition from the complete model (11$\rightarrow$12) leads to a $1.34\%$ dynasty dating accuracy decrease  while the period dating accuracy is maintained. This result is also in line with our original intention, which not only strengthens the dynasty dating but does not affect the period dating.
\vspace{-10.5pt}

\paragraph{Knowledge Extraction Module (KEM).}
With this module, we first verify the influence of the shape information on the dating performance. After adding the shape head of the KEM to the baseline (2$\rightarrow$3), the dynasty dating accuracy of the network is reduced by $2.31\%$, and the period dating accuracy is reduced by $1.51\%$. After concatenating the extracted shape features with the period features and using them together for period learning (3$\rightarrow$4), the accuracy of the dynasty dating increases by $1.99\%$, and the accuracy of the period dating increases by $0.97\%$.
% In this module, we first verify the influence of shape information on dating performance. After adding the shape head of KEM to the baseline (2$\rightarrow$3), the network's dynasty dating accuracy is reduced by $2.31\%$ and period dating accuracy is reduced by $1.51\%$. And after concatenating the extracted shape features with the period features and using them together for period learning (3$\rightarrow$4), the dynasty dating accuracy increases by $1.99\%$ and the period dating accuracy increases by $0.97\%$.



Second, we verify the influence of characteristic information on the dating performance. The addition of the characteristic head of the KEM (2$\rightarrow$6) improves the dating accuracy of the network by $0.38\%$ and $0.32\%$. After concatenating the extracted characteristic features with the period features and using them together for period learning (6$\rightarrow$7), the accuracy of the period dating decreases by $0.27\%$, whereas the accuracy of the dynasty dating improves by only $0.27\%$. When we simultaneously concatenate the shape and characteristic features into the features of the period for period learning (2$\rightarrow$9), but the accuracies of the dynasty dating and period dating decrease by $0.05\%$ and $0.43\%$, respectively.
% Second, we verify the influence of characteristic information on dating performance. The addition of the characteristic head of KEM (2$\rightarrow$6) improves the network's dating accuracy by $0.38\%$ and $0.32\%$. And after concatenating the extracted characteristic features with the period features and using them together for period learning (6$\rightarrow$7), the period dating accuracy decreases by $0.32\%$ while the dynasty dating accuracy improves by only $0.27\%$. When we simultaneously concatenate the features of shape and characteristic into the features of period for period learning (2$\rightarrow$9), the dynasty dating accuracy and period dating accuracy decrease by $0.05\%$ and $0.43\%$, respectively. 


These results demonstrate that concatenating the attribute information with the period information is inefficient. The external information is far from being fully utilized. In the following, we embed the shape and characteristic predictions into an archaeology knowledge-guided relation graph.
% These results demonstrate that simply concatenating the attribute information with the period information is inefficient. This external information is far from being fully utilized. In the following, we embed the shape and characteristic predictions into the archaeology knowledge-guided relation graph.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htbp]
\centering
\caption{Comparison of each method on the proposed datasets. Bold indicates the best results, and underlined values are the second best results. The single- and multi-granularity methods use single- and multi-granularity era labels as supervision, respectively. In addition, our method and $A^3$M~\cite{46} use additional attribute annotation information, and Part-based R-CNN~\cite{6} uses additional bounding box annotations.}
% \caption{Comparison of each method on the proposed datasets. The bolded ones are the best results, and the underlined ones are the second best results. \textcolor{red}{The single-granularity method and the multi-granularity method use single-granularity era labels and multi-granularity era labels as supervision, respectively. In addition, our method and $A^3$M~\cite{46} use additional attribute annotations information, and Part-based R-CNN~\cite{6} uses additional bounding box annotations.} }
\label{tab3:Period ACC}
\resizebox{0.98\textwidth}{!}{  % Here 1/2
\begin{tabular}{cc|c|cc|cc|ccc|ccc|ccc}
\hline\hline
\multicolumn{2}{c|}{}                                                                                           &                                 &                      &                                              & \multicolumn{2}{c|}{Shang}                  & \multicolumn{3}{c|}{Western Zhou}                                  & \multicolumn{3}{c|}{Spring and Autumn}                                                    & \multicolumn{3}{c}{Warring States}                                                        \\
\multicolumn{2}{c|}{\multirow{-2}{*}{Method}}                                                                   & \multirow{-2}{*}{w/ Attributes} & \multirow{-2}{*}{$OA$} & \multirow{-2}{*}{$AU(\overline{PRC})$}                    & Early                & Late                 & Early                & Mid                  & Late                 & Early                & Mid                  & Late                                        & Early                                       & Mid                  & Late                 \\ \hline\hline
\multicolumn{1}{c|}{}                                     & ConvNeXt~\cite{50}                  &                                 & 76.01                & {{\ul \textit{0.8397}}} & 73.04                & 82.31                & 75.21                & 82.56                & 80.00                & 77.44                & 64.17                & { {\ul \textit{67.76}}} & { {\ul \textit{56.90}}} & 41.18                & 63.37                \\ \cline{2-16} 
\multicolumn{1}{c|}{}                                     & Part-based R-CNN~\cite{6}           & BBox                    & 69.45                & 0.7796                                       & \textbf{92.59}       & {\ul \textit{84.47}} & 62.54                & \textbf{90.78}       & 67.74                & 62.03                & 62.79                & 46.89                                       & 38.46                                       & \textbf{73.33}       & \textbf{84.91}                \\ \cline{3-3}
\multicolumn{1}{c|}{}                                     & MCL~\cite{48}                       &                                 & 70.41                & 0.7742                                       & 79.55                & 78.56                & 66.85                & 79.58                & 76.28                & 71.57                & 54.23                & 66.02                                       & 35.42                                       & 31.18                & 60.11                \\ \cline{3-3}
\multicolumn{1}{c|}{}                                     & CrossX~\cite{30}                    &                                 & 70.54                & 0.7755                                       & 70.89                & 72.97                & 75.21                & 71.74                & 82.88                & 67.67                & 62.14                & 57.20                                       & 25.00                                       & 57.14                & 69.23                \\ \cline{3-3}
\multicolumn{1}{c|}{}                                     & BCNN~\cite{29}                      &                                 & 71.59                & 0.7402                                       & 71.15                & 80.48                & 70.92                & 76.82                & 78.60                & 73.14                & 57.53                & 59.00                                       & 0.00                                        & 0.00                 & 48.11                \\ \cline{3-3}
\multicolumn{1}{c|}{}                                     & NTS-Net~\cite{47}                   &                                 & 73.06                & 0.7890                                       & 71.15                & 80.93                & 67.28                & 79.17                & 78.15                & 81.56                & 58.24                & 60.48                                       & 54.17                                       & {\ul \textit{70.83}} & 67.78                \\ \cline{3-3}
\multicolumn{1}{c|}{}                                     & $A^3$M~\cite{46}                    & $\checkmark$                    & 75.12                & 0.8002                                       & 79.59                & 78.24                & 74.11                & 85.71                & 78.42                & 78.99                & 62.35                & 63.08                                       & 44.44                                       & 50.00                & 69.32                \\ \cline{3-3}
\multicolumn{1}{c|}{}                                     & SPS~\cite{31}                       &                                 & 76.94                & 0.8245                                       & 80.39                & 83.30                & 73.19                & {\ul \textit{86.51}} & {\ul \textit{85.61}} & 81.21                & 62.38                & 66.36                                       & 42.11                                       & 50.00                & 67.78                \\ \cline{3-3}
\multicolumn{1}{c|}{\multirow{-9}{*}{\rotatebox[origin=c]{90}{Single-Granularity}}} & P2PNet~\cite{45}                    &                                 & {\ul \textit{77.32}} & 0.8370                                       & 79.25                & 78.25                & \textbf{80.39}       & 78.80                & \textbf{88.37}       & \textbf{85.11}       & 64.52                & 67.24                                       & 50.00                                       & 52.78                & 71.59                \\ \hline
\multicolumn{1}{c|}{}                                     &                                                     &                                 & 84.85                & {\ul \textit{0.9125}}                        & \multicolumn{2}{c|}{{\ul \textit{84.37}}}   & \multicolumn{3}{c|}{84.09}                                         & \multicolumn{3}{c|}{{\ul \textit{87.25}}}                                                 & \multicolumn{3}{c}{84.97}                                                                 \\
\multicolumn{1}{c|}{}                                     & \multirow{-2}{*}{YourFL~\cite{1}}   & \multirow{-2}{*}{}              & 73.92                & 0.8019                                       & 79.25                & 80.20                & 69.38                & 82.21                & 83.59                & 82.14                & 56.38                & 62.90                                       & 53.85                                       & 51.72                & 60.67                \\ \cline{3-16} 
\multicolumn{1}{c|}{}                                     &                                                     &                                 & 84.43                & 0.8934                                       & \multicolumn{2}{c|}{82.07}                  & \multicolumn{3}{c|}{84.28}                                         & \multicolumn{3}{c|}{85.91}                                                                & \multicolumn{3}{c}{{\ul \textit{90.41}}}                                                  \\
\multicolumn{1}{c|}{}                                     & \multirow{-2}{*}{C-HMCNN~\cite{43}} & \multirow{-2}{*}{}              & 74.52                & 0.7766                                       & {\ul \textit{81.25}} & 79.29                & 70.04                & 85.57                & 81.95                & 75.00                & \textbf{67.47}       & 60.45                                       & \textbf{58.06}                              & 45.24                & {\ul \textit{77.46}}       \\ \cline{3-16} 
\multicolumn{1}{c|}{}                                     &                                                     &                                 & {\ul \textit{85.28}} & 0.9124                                       & \multicolumn{2}{c|}{81.08}                  & \multicolumn{3}{c|}{{\ul \textit{88.24}}}                          & \multicolumn{3}{c|}{85.98}                                                                & \multicolumn{3}{c}{85.71}                                                                 \\
\multicolumn{1}{c|}{}                                     & \multirow{-2}{*}{HRN~\cite{2}}      & \multirow{-2}{*}{}              & 75.81                & 0.8206                                       & 75.93                & 79.66                & 75.89                & 80.63                & 85.61                & 81.70                & 62.24                & 62.71                                       & 51.06                                       & 48.15                & 68.06                \\ \cline{3-16} 
\multicolumn{1}{c|}{}                                     &                                                     &                                 & \textbf{88.79}       & \textbf{0.9380}                              & \multicolumn{2}{c|}{\textbf{86.80}}         & \multicolumn{3}{c|}{\textbf{88.62}}                                & \multicolumn{3}{c|}{\textbf{91.57}}                                                       & \multicolumn{3}{c}{\textbf{90.45}}                                                        \\
\multicolumn{1}{c|}{\multirow{-8}{*}{\rotatebox[origin=c]{90}{Multi-Granularity}}}  & \multirow{-2}{*}{Ours}                              & \multirow{-2}{*}{$\checkmark$}  & \textbf{78.83}       & \textbf{0.8550}                              & 77.36                & \textbf{84.85}       & {\ul \textit{78.30}} & 81.28                & 85.31                & {\ul \textit{83.33}} & {\ul \textit{64.84}} & \textbf{68.85}                              & 45.95                                       & 53.13                & 75.31 \\ \hline
\end{tabular}
}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\caption{Comparison results of the dating performance showing the effect of the order of the shape and characteristic embedding.}
% \caption{The comparison  results of dating performance show the affect of the order of shape and characteristic embedding.}
\label{tab:order}
\resizebox{.32\textwidth}{!}{  % Here 1/2
\begin{tabular}{cl|cc}
\hline\hline
\multicolumn{2}{c|}{Embedding order}          & Dynasty $OA$     & Period $OA$      \\ \hline\hline
\multicolumn{2}{c|}{Era-Characteristic-Shape} & 86.37          & 76.13          \\ \hline
\multicolumn{2}{c|}{Era-Shape-Characteristic} & \textbf{88.79} & \textbf{78.83} \\ \hline
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\vspace{-10.5pt}

\paragraph{Archaeology Knowledge Guided Relation Graph (AKG).}  
To make better use of the additional information, we build an AKG by adding the shape and characteristic predictions extracted from the KEM to the relation graph. Experiment results show that embedding the shape into the relation graph (2$\rightarrow$5) improves the accuracy of the dynasty dating by $0.86\%$ and the accuracy of the period dating by $0.81\%$. Embedding the characteristics into the relation graph (2$\rightarrow$8) improves the accuracy of dynasty dating by $0.87\%$ and accuracy of period dating by $0.93\%$. In the complete model, embedding both the shape and characteristics (2$\rightarrow$12) improves the accuracy of the dynasty dating by $1.94\%$ and the accuracy of the period dating by $1.78\%$.
% To make better use of the additional information, we build the archaeology knowledge guided relation graph (AKG) by adding the shape and characteristic predictions extracted from the KEM to the relation graph. Experiments show that embedding shape into the relation graph (2$\rightarrow$5) improves dynasty dating accuracy by $0.86\%$ and period dating accuracy by$0.81\%$. And embedding characteristic into relation graph (2$\rightarrow$8) improves dynasty dating accuracy by $0.87\%$ and period dating accuracy by $0.93\%$ . In the complete model, embedding both shape and characteristic (2$\rightarrow$12) can improve dynasty dating accuracy by $1.94\%$ and period dating accuracy by $1.78\%$.


It is worth noting that the embedding order of the shape and characteristics also has an impact on the dating performance. As illustrated in Equations (\ref{eq:Le-s}) and (\ref{eq:Le-a}), in our network, we use the Era-Shape-Characteristic order to incrementally learn different information. To verify the influence of the embedding order, we also tested the opposite case. In the Era-Characteristic-Shape learning order, when the model is unable to accurately determine the era of a sample based on its era feature, it first increases the influence of the characteristic features and finally considers the shape feature. The comparison results are shown in Table~\ref{tab:order}, where the embedding learning order of the Era-Characteristic-Shape is $2.42\%$ and $2.70\%$ lower than that of our applied order. This confirms that when learning labels with different distributions, the network needs to learn the labels progressively from weak to strong and from easy to complex. This also explains why the strong supervised method (Part-based R-CNN~\cite{6}) did not achieve excellent dating results in the following comparison experiment.
% It is worth to notice that the embedding order of shape and characteristic also has an impact on the dating performance. As illustrated in Equation (\ref{eq:Le-s}), (\ref{eq:Le-a}), in our network, we use the Era-Shape-Characteristic order to learn the different information incrementally. To verify the influence of embedding order, we also test the opposite way. In the Era-Characteristic-Shape learning order, when the model is unable to accurately determine the era of a sample by the its era feature, it will first increase the influences of the characteristic feature and finally consider the shape feature. \textcolor{red}{In the Era-Characteristic-Shape learning order, when the network is not able
% to learn enough information to determine the era of this
% sample, the $\mathcal{L}_{\text {ec}}(\mathcal{\!D\!})$ will play a supportive role. And when neither the era features nor the characteristic features can provide enough information, the $\mathcal{L}_{\text {es}}(\mathcal{\!D\!})$ will contribute to determine the chronology of this sample by learning the relation be-
% tween era and shape.} The comparison results are shown in Table~\ref{tab:order}, the embedding learning order of Era-Shape-Characteristic is $1.08\%$ and $2.70\%$ higher compared to the other one. We believe this confirms that when learning the labels with different distributions, the network needs to learn the labels in a progressive order from weak to strong and from easy to complex. It also explains why the strong supervised method (Part-based R-CNN~\cite{6}) has not achieved excellent dating results in the following comparison experiment.


%------------------------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htbp]
\centering
  \includegraphics[width=1.0\linewidth]{figure6V2.pdf}
  \caption{Gradient-weighted class activation map of different methods on 11 period test samples. Compared to the other methods, our network is more concentrated on the discriminative regions of a bronze ding and is able to capture its key characteristics.}
  % \caption{The gradient-weighted class activation map of different method on 11 period test samples. Compared to the other methods, our network is more concentrated on the discriminative regions of bronze Ding and is able to capture the key characteristics on the bronze Ding.}
  \label{fig6:CAM}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\centering
  \includegraphics[width=0.9\linewidth]{figure7.pdf}
  \caption{Visualization of learned representations of different methods on our dataset using T-SNE. Compared to the other methods, the decision boundaries of our proposed network become more separated.}
  % \caption{Visualization of learned representations of different methods on our dataset by using T-SNE. Compared to the other methods, the decision boundaries of our proposed network become more separated.}
  \label{fig7:tsne}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparison with SOTA Methods}
\vspace{-4pt}
We compare our proposed network with other SOTA approaches under multi- and single-granularity settings. Under a multi-granularity setting, we train all multi-granularity methods with two-level labels of the bronze ding datasets. We report the $OA$ and $AU(\overline{PRC})$ results for each hierarchical level on the test set. Under a single-granularity setting, we train all single-granularity methods using fine-grained period labels of a bronze ding. Besides, to observe the classification performance of each dynasty and period independently, we calculate the precision of each approach on 4 coarse-grained dynasties as well as on 11 fine-grained periods recall is reported in our supplementary material. In addition to research related to fine-grained classification, we also compare our method with ConvNeXt~\cite{50}, which is an extremely popular approach in traditional classification tasks.
% We compare our proposed network with other SOTA approaches under multi-granularity setting and single-granularity setting respectively. In multi-granularity setting, we train all multi-granularity methods with two-level labels of the bronze Ding datasets. We report $OA$ and $AU(\overline{PRC})$ results of each hierarchical level on test sets. In single-granularity setting, we train all single-granularity methods with fine-grained period labels of the bronze Ding and report $OA$ and $AU(\overline{PRC})$ results on test sets. Additionally, in order to observe the classification performance on each dynasty and period independently, we calculate the classification accuracy of each method on 4 coarse-grained dynasties as well as on 11 fine-grained periods. In addition to the fine-grained classification related work, we also compare our method with the ConvNeXt~\cite{50}, which is very popular in traditional classification tasks.

As Table~\ref{tab3:Period ACC} shows, using the same backbone ResNet50, our method outperforms the state-of-the-art 
single-granularity method P2PNet~\cite{45} on the bronze ding dataset benchmark by more than $1.51\%$ $OA$ and $0.018$ $AU(\overline{PRC})$ for period dating. And our method outperforms the state-of-the-art 
multi-granularity method HRN~\cite{2} (our baseline) on the bronze ding dataset benchmark by more than $3.51\%$ $OA$ and $0.0256$ $AU(\overline{PRC})$ in terms of dynasty dating, and by more than $3.02\%$ $OA$ and $0.0344$ $AU(\overline{PRC})$ for period dating. Furthermore, we achieve the best performance for all 4 coarse-grained dynasties and 3 out of 11 fine-grained periods for each independent era classification.
% As Table~\ref{tab3:Period ACC} shows, using the same backbone ResNet50, our method outperforms the state-of-the-art method P2PNet~\cite{45} on the bronze ding dataset benchmark by more than $3.51\%$ $OA$ and $0.0255\%$ $AU(\overline{PRC})$ in terms of dynasty dating, and by more than $1.51\%$ $OA$ and $0.018\%$ $AU(\overline{PRC})$ for period dating. We achieved the best performance for all 4 coarse-grained dynasties and 3 out of 11 fine-grained periods for each independent period classification.

% As the Table~\ref{tab3:Period ACC} shows, with the same backbone ResNet50, our method outperforms the state-of-the-art methods P2PNet~\cite{45} on bronze Ding datasets benchmark by more than $3.51\%$ $OA$ and $0.0255\%$ $AU(\overline{PRC})$ on dynasty dating, and more than $1.51\%$ $OA$ and $0.018\%$ $AU(\overline{PRC})$ on period dating. We achieve the best performance for all four coarse-grained dynasties and three out of eleven fine-grained periods for each independent period classification.
\vspace{-10.5pt}

\paragraph{Visualization.}

To demonstrate that our network can capture important regions of interest useful for bronze ding dating, we adopt Grad-CAM~\cite{52} for an intuitive visualization. For comparison, we also conducte the same visualization for the single-granularity method P2PNet~\cite{45} and the multi-granularity methods HRN~\cite{2} and C-HMCNN~\cite{43}, which also exhibit competitive performances. As shown in Figure~\ref{fig6:CAM}, our network is more concentrated within the discriminative regions of a bronze ding. 
% Compared to the other methods, our network captures the key locations on a bronze ding when applying dating of the decorations and inscriptions, among other characteristics. 
Compared to the other methods, our network captures the key locations on a bronze ding when performing the dating, such as decorations and inscriptions.
% To demonstrate that our network can capture important regions of interests that are useful for bronze Ding dating, we also adopt Grad-CAM~\cite{52} to show intuitive visualization. For comparison, we also perform the same visualization for the single-granularity method P2PNet~\cite{45} and the multi-granularity method HRN~\cite{2} and C-HMCNN~\cite{43}, which also have competitive performance. It can be seen from Figure ~\ref{fig6:CAM} that our network is more concentrated on the discriminative regions of bronze Ding. Compared to the other methods, our network captures the key locations on the bronze Ding when performing the dating，such as decorations and inscriptions.


In addition, as shown in Figure~\ref{fig7:tsne}, we draw t-SNE~\cite{53} scatter plots from the learned high-dimensional period features of our network and some other comparison methods. For better visualization, we randomly select 40 images for each period. From the t-SNE plots, we can clearly see that our network extracts more discriminative period representations of different images.
% A shown in Figure~\ref{fig7:tsne}, we draw the t-SNE~\cite{53} scatter plot from the learned high dimensional period features of our network and some comparison methods. We randomly selecte 40 images in each period for visualization. From the t-SNE plot, we find that our network can extract more discriminative period representations of different images.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\vspace{-10.5pt}


\section{Conclusion and Further Work}
\vspace{-4pt}
In this study, we introduce a bronze ding dataset with rich archaeological labels. To address the challenges, we construct an end-to-end multihead network for predicting the era of bronze dings based on an AKG. Comprehensive experiments are conducted on our dataset, the results of which demonstrate the effectiveness of our proposed network in comparison to existing multi- and single-granularity FGVC methods. And, excitingly, our learning-based dating network achieve the same level of human experts. 

In a future study, we plan to collect and open up more types of Chinese bronze data to facilitate research through learning-based methods. We hope that our study will provide further contributions to both the archaeological and deep-learning communities.

% Paleography and Chinese Civilization Inheritance and Development Program
\vspace{-10.5pt}

\paragraph{Acknowledgement.} This work is supported by the "Paleography and Chinese Civilization Inheritance and Development Program" Collaborative Innovation Platform (Grant No.G3829) and Jilin University (Grant No.419021421665 and No.419021422B08).

% Thanks to the "Engineering Platform for the Inheritance and Development of Ancient Characters and Chinese Civilization" for funding the project "Construction of Artificial Intelligence Recognition System for Ancient Characters" (Grant No.G3829). This work is also supported by Jilin University (Grant No.419021421665).

% In this work, we introduce a bronze Ding dataset with rich archaeological labels. To address these challenges of our task, we construct an end-to-end multi-head network for predicting the era of bronze Dings based on an archaeology knowledge guided relation graph. Comprehensive experiments are conducted on our dataset, and the results demonstrated the effectiveness of our proposed network compared to the existing multi-granularity and single-granularity FGVC methods. Excitingly, our learning-based dating results have achieved the same level of human exerts. 

% In further work, we will collect and open up more types of Chinese bronze data to facilitate research with learning-based methods. We hope that our work will provide further contributions to both the archaeological and deep learning communities.
\input{main.bbl}
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}

}

\end{document}
