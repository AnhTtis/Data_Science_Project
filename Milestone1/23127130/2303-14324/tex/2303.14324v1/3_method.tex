\section{Proposed Method}
\label{sec:method}
In this section, we provide a detailed description of our proposed TCSR. Firstly, we introduce the general framework for SISR tasks. Subsequently, we present the implementation details of the proposed NA and EFFN modules. Finally, we provide further comparisons between Swin, convolution, and NA modules.
\subsection{Overall Architecture}
% %\vskip-5pt
As illustrated in \cref{fig:framework}, the proposed TCSR contains three components: the shallow feature extractor, the deep feature extraction module stacked by several NAT blocks, and the high-resolution reconstruction module. Given the LR input $I^{LR}\in \mathbb{R}^{H \times W\times 3}$where $H$, $W$ are image height, width, respectively. The shallow feature extractor $f_{s}$, based on a normal $3\times3$ convolutional layer, is firstly utilized to map the input image into the latent space and the primitive feature $F\in \mathbb{R}^{H\times W\times C}$ with $C$ channel dimensions is obtained as follows:
\begin{equation}
    F_{s} = f_{s}(I^{LR}).
\end{equation}
Then $F_{s}$ is sent into the deep feature extractor $f_{d}$ and deep the feature $F_{d}$ is formulated as follows:
\begin{equation}
    F_{d}= f_{d}(F_{s}).
\end{equation}
The feature $F_{d}$ is utilized for the final super-resolution by the HR reconstruction module, and the super-resolved image $I^{SR}$ is obtained as follows:
\begin{equation}
    I^{SR} = F_{HR}(F_{d}),
\end{equation}
where $F_{HR}$ presents the HR reconstruction module, based on the $3\times3$ convolution and a pixelshuffle layer.

\subsection{Neighborhood Attention Module}
% %\vskip-5pt
\paragraph{Self-Attention.}
Self-attention (SA) is an operation that employs a query (Q) and a set of key (K) and value (V) pairs. First, the dot product between the Q and K is computed and scaled, and the softmax function is utilized to obtain weighted coefficients. Then assembled feature can be obtained by combining the V with the coefficient. The SA is formulated as follows:
\begin{equation}
    \operatorname{SA}(Q, K, V)=\operatorname{SoftMax}\left(\frac{Q K^T}{\sqrt{d_k}}+RPB\right) V,
\end{equation}
where $d$ is the feature dimension, $\sqrt{d_{k}}$ is the scale factor, and $RPB$ is the learnable relative position bias. Furthermore, the multi-head self-attention is utilized, which translates the input into $h$ independently features by learnable linear projections, where $h$ is the number of headers, and SA is applied in parallel against each projection.




\paragraph{Neighborhood Attention.}
Based on SA, aggregated features can learn the relation between each pair of Q and K and easily obtain the long-range relation with a large scale of the key-value set. However, SA has a quadratic complexity to the number of tokens. To reduce the computational cost, the local attention mechanism is in demand. Considering the success of CNNs, the inductive bias in CNN is essential to modeling. To bridge the gap between the SA in transformer and the inductive bias in CNN, a sliding window-based local attention module is introduced here, which extracts the SA among local neighboring features around the target query pixel, named the neighborhood attention (NA).

The proposed NA applies SA with the sliding window like the normal convolutional layer. The analogy to the convolution with the kernel size $k$, for the $(i, j)$th pixel $p_{i,j}$ in the feature map, the key-value set is selected within local pixels around $p_{i,j}$, noted as $\rho^{k}_{i,j}$. Let us take the $3\times3$ kernel as the example, the corresponding key-value set is $\rho^{3}_{i,j}=\{p_{i+1, j-1}, p_{i+1, j}, p_{i+1, j+1}, p_{i, j-1}, p_{i, j}, p_{i, j+1}, p_{i-1, j-1}, \\ p_{i-1, j}, p_{i-1, j+1}\}$.

Based on the implementation of SA, the proposed NA is formulated as follows:
\begin{equation}
\small
    \operatorname{NA}(Q_{i,j}, K_{\rho^{k}_{i,j}}, V_{\rho^{k}_{i,j}})=\operatorname{SoftMax}\left(\frac{Q_{i,j} K^T_{\rho^{k}_{i,j}}}{\sqrt{d_k}}+RPB\right) V_{\rho^{k}_{i,j}}.
\end{equation}

It is worth noting that there is no patch splitting and patch embedding operation in the proposed NA. Feature extraction in NA is the same as the normal convolution with kernel size $k$, where we take 11 as the default kernel size, and more detailed ablations are presented in experiments.

\subsection{Enhanced Feed-Forward Network}
Following the feature aggregation NA module, a feed-forward network (FFN) containing two linear layers with a non-linear activation layer is utilized. We can find that pixel-wise interaction is extracted by FFN, which lacks feature aggregation with local neighboring pixels. A vanilla way is to take convolutional layers, such as the normal $3\times3$ convolution, but more parameter count and computational costs are involved. To address this problem and bring the local feature aggregation into the FFN, as illustrated in \cref{fig:framework}(a), we propose the enhanced feed-forward network (EFFN) with the spatial-shift operation, which is parameter-free and no extra FLOPs cost. 

\paragraph{Spatial-Shift Operation.} The spatial-shift operation manually exploits the feature aggregation against the channel dimension. As illustrated in \cref{fig:framework}(b), here we take the spatial-shift operation with 4 groups as the example. Given the input feature $F_{in}\in\mathbb{R}^{H\times W\times C}$, we first uniformly separate it into $N$ thinner groups along channel dimension. Then each thinner grouped feature is shifted in different directions with the shift 
stride $s$. Here we take the same feature aggregation pattern as the normal $3\times3$ convolution. In detail, given the input feature $F_{in}$, we uniformly split it into 8 groups along the channel dimension. Then each separated feature group is shifted in 8 directions with stride 1, and we take the constant value 0 as the default padding for 
borders.


By spatial-shift operation, local pixels are involved in the shifted feature among different channel groups. 

\subsection{Loss Function}

For image SR tasks, MAE (Mean Absolute Error) and MSE (Mean Squared Error) are two commonly used loss functions. MAE loss, also known as L1 loss, measures the absolute differences between the super-resolved image and the HR target. It is frequently used because it is more robust to outliers than MSE and produces sharper edges in the output image \cite{loss_in_IR}. In this paper, we adopt MAE loss to measure the differences between the SR images and the ground truth. Specifically, the loss function is:
\begin{equation}
    \mathcal{L}_1=\|I^{SR}-I^{HR}\|_{1},
\end{equation}
where $I^{SR}$ and $I^{HR}$ are super-resolved image and the HR target, respectively.

\subsection{Remark}

\paragraph{Comparison between Conv, Swin, and NA.} The proposed NA, a sliding window-based self-attention mechanism, brings the inductive bias in convolution to the vanilla self-attention module. Compared to the convolution, the parameter count of NA is independent of the kernel size, which is more flexible for extracting long-range relations. Local window-based self-attention is exploited by splitting the input into non-overlapping windows to reduce the computational cost of global self-attention. To obtain the cross-window connection, Swin proposed and achieved promising results. Compared to the Swin, the proposed NA is a more flexible operation to obtain the cross-region relation like the normal convolution.

%\vskip-5pt
\begin{table}[!ht]
    \caption{Comparison of computational cost.}
    %\vskip-5pt
    \label{sup:complexity_summary}
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Module} & \textbf{Computation} \\
        \midrule

        \textbf{Conv} & $\mathcal{O}\left( H W C^2 K^2 \right)$ \\
        \textbf{Swin} & $\mathcal{O}\left(  3 H W C^2 + 2 H W C K^2 \right)$ \\
        \textbf{NA} & $\mathcal{O}\left(  3 H W C^2 + 2 H W C K^2 \right)$ \\
        \bottomrule
    \end{tabular}
    %\vskip-20pt
\end{table}

\paragraph{Complexity Analysis.} Given the input feature $F\in \mathbb{R}^{H\times W\times C}$, where $H, W$, and $C$ are height, width, and channel dimension, respectively. The kernel size in convolution, the local window size in Swin, and the kernel size in NA are set as $K$. The number of headers in Swin and NA is set as 1, and the linear projection for Q, K, and V is contained. Results are presented in \cref{sup:complexity_summary}. One can find that NA has the same complexity as the Swin when they take the same spatial extent. Compared to the normal convolution, the computational cost of the NA grows slower than the convolution. If we take the channel dimension $C=64$ as the example, the computational cost in the normal $3\times3$ convolution is near to the NA with $K=13$.   

