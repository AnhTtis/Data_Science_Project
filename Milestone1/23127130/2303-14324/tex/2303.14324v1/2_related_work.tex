\section{Related Works}
\vskip-5pt
\label{sec:related_work}
In this section we will briefly introduce some related work about the image super-resolution, vision transformer, and modern convolutional networks.

\textbf{Image Super-Resolution.}
 Deep learning-based methods for SISR tasks achieved great breakthroughs in recent years \cite{survey,ACMComputingSurvey,21survey}. Dong \textit{et al.} proposed the SRCNN which takes three convolutional layers to learn the LR to HR mapping directly and obtains promising results compared to the classical approaches. Subsequently, many well designed CNN-based architectures were exploited and proposed for SISR task and achieved further improvement \cite{EDSR,RDN,RRDB,RCAN,NLSN}. Many efficient and lightweight SR models were proposed \cite{CARN,IMDN,LAPAR,SMSR,ECBSR,FDIWN,shufflemixer}. Liu \textit{et al.} proposed the ShuffleMixer, which exploits the large kernel in SR network and utilizes the channel shuffle operation to reduce the number of learnable features. In addition, Liang \textit{et al.} \cite{SwinIR} proposed the SwinIR, which involved the Swin Transformer \cite{SwinT} to image restoration tasks and achieved promising results.

\textbf{Modern Convolutional Network.}
In the last year, several work investigated some modern CNN-based architectures \cite{convnet,largekernel}. Liu \textit{et al.} \cite{convnet} revisited a modern design of the residual block and introduced larger kernels, where the $7\times7$ kernel size is utilized. Furthermore, Ding \textit{et al.} \cite{largekernel} exploited the kernel size upto 31. In \cite{largekernel}, the stable and scalable architectures were proposed based on the re-parameterizing strategy \cite{repVGG}, and a well-optimized implementation of the large kernel convolution was introduced, which makes it more practical. Compared to the standard $3\times3$ convolution, large kernels bring larger receptive fields that significantly improve the capabilities of CNN-based networks.

\textbf{Vision Transformer.}
%Vision transformers have attracted great attention in recent years. 
Dosovitskiy et al.\cite{ViT} first introduced the ViT, which proposed the Transformer-based architectures into vision tasks. Furthermore, Liu \textit{et al.} \cite{SwinT} brought some inductive bias in CNNs into the Transformer-based architecture design, and proposed a local self-attention mechanism, named Shifted Window-based (Swin) Transformer. Swin partitions the input and applies self-attention into each partition separately, which reduces the computational cost and makes the Transformer-based architecture more practical for vision tasks. Based on Swin, how to extract more effective cross-region relation has attracted great attention \cite{CSwin,SpaceShuffle}. In addition, Ramachandran \textit{et al.} \cite{SASA} proposed the sliding window-based self-attention mechanism and made an attempt to substitute normal convolutions. Most recently, Hassani \textit{et al.} \cite{NAT} proposed the neighborhood attention module and given an efficient implementation of the sliding window-based self-attention.

In this paper, we attempt to exploit more large kernel design in lightweight SR network. In contrast to focusing on the architecture design, we exploit large kernel attention by a sliding window-based self-attention pattern, which extracts long-range relation effectively while maintains inductive bias like the convolution. We first analyze the complementarity of the neighborhood attention (NA) against the normal convolution. As the aforementioned, NA contains inductive bias in the CNN, which can effectively extract the cross-region relation like the convolution by the dense feature extraction. Furthermore, we extend and propose the enhanced feed-forward network (EFFN). The proposed EFFN involves the spatial-shift operation to maintain more local feature aggregation along channel dimension.







