\section{Conclusion \label{sec:conclusion}}
In this paper, we proposed a new lightweight image super-resolution architecture named TCSR, which is a conv-like transformer architecture. TCSR combines the strengths of both convolution and self-attention mechanisms, leveraging the inductive bias of convolution for local feature aggregation in CNN and the long-range relation capabilities of self-attention. To further improve the feature enhancement capabilities of TCSR, we introduced an enhanced feed-forward network (EFFN) by utilizing the spatial-shift operation, which further improves the local feature aggregating and long-range modeling. Our extensive experiments demonstrate the effectiveness of TCSR, which outperforms existing lightweight SR networks. Moreover, we provide detailed ablation studies that reveal the scalability of TCSR. We believe that analyzing the difference between features extracted by convolution and self-attention and enhancing the fundamental architectures by interpolating convolution with self-attention is a promising research direction for the future. We  hope that our work will inspire further exploration of modern architecture in the near future, leading to more significant improvements in the field of lightweight image super-resolution.
