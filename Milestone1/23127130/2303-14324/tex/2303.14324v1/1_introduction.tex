% 我们的核心贡献： first introduce the Neighborhood Attention in SISR, which is the most like conv self-attention modeling.
% 计算密集和窗口选取、近邻信息缺失以及窗口size，本质上还是Size带来的好处，相比之下，Conv的新作大核卷积也验证了LKS更好，但是参数量和计算量是KS的二次项，Conv中难以实现naiev的大核过程，例如大核卷积用的DW以及low-rank的稀疏卷积核。

% The feature extraction and feature aggregation are decoupled. Since there is no parameter in aggregation stage and we can easily exploit the long-range relation with the large kernel size.

% 卷积可以看做9个独立的1x1 linear mapping得到不同位置先验的特征，fetaure aggregation 则是把不同位置处的的特征naeive相加得到ensembled 特征。One can find that， conv is parameter-dense, 因为参数量和计算量随着ks二次增长，hard and hamper 大核卷积。相比之下，Conv-like SA则有效的将Feature extraction和Feature aggregation过程中参数量和特征提取解耦开，不需要对不同位置的特征分别使用不同的linear mapping，而是一个linear mapping之后使用MHSA进行特征aggregation。这里长距离关系就很容易引入。
% remark: Due to the success of the CNN-based SR models, one can find that the local feature aggregation is of crucial importance. 

\section{Introduction}
\label{sec:intro}
Single image super-resolution (SISR) has enjoyed tremendous progress with the development of deep learning, especially in recent years. The goal of SISR is to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart. The pioneering work SRCNN \cite{SRCNN} first proposed a convolutional neural network (CNN) to learn the mapping from LR inputs to HR targets, and outperformed traditional SISR approaches by a large margin. Following \cite{SRCNN}, a series of well-designed CNN architectures \cite{VDSR,EDSR,RDN} and visual attention mechanisms \cite{RCAN,SAN,NLSN} were introduced to improve the CNN-based SISR performance. However, the above mentioned SISR methods rely heavily on complicated network architectures, which require substantial computational resources and are hard to be deployed on mobile and edge devices. Therefore, the design of efficient and lightweight SR models are highly demanded.

For practical SISR, many efforts have been made to reduce the number of parameters and floating-point operations (FLOPs) \cite{CARN,IDN,IMDN,LAPAR,SMSR,ECBSR,FDIWN,shufflemixer}. Since both the number of parameters and FLOPs grow quadratically with respect to the kernel size, $3\times3$ convolution is widely adopted in CNN-based models. Recently, modern CNN architectures were exploited by revisiting the effectiveness of large kernels \cite{convnet,largekernel}. \textit{Liu et al.} \cite{convnet} redesigned the basic residual block where the kernel size is crucial to the performance and $7\times7$ kernel was utilized. Ding \textit{et al.} \cite{largekernel} further extended the kernel size up to 31, which resulted in large effective receptive fields. Inspired by \cite{convnet,largekernel}, we are interested in designing an efficient SISR method by taking advantage of large kernels while enjoying a lightweight architecture, in order to get the best of both worlds.

%For lightweight SISR, Sun \textit{et al.} \textcolor{red}{ref.} leveraged the large kernel convolution, which utilizes the depth-wise convolution combined with the channel splitting and shuffling. 
%While large receptive fields can be obtained with large kernel sizes , the model size grows quadratically with respect to the kernel size. Thus, 
\begin{figure}
    \centering
    \includegraphics[width=0.485\textwidth]{figure/Conv_and_NA.pdf}
    \caption{Illustration of the feature extraction. (a) Local pixels are projected and summed as the target output. (b) Neighboring pixels are projected and assembled by the self-attention mechanism.}
    \label{fig:na_vs_conv}
    %\vskip-15pt
\end{figure}
% 与卷积不同 Transformer广泛使用SA，可以容易的获取long-range，但是ViT中Global的SA计算量过于复杂，SA难以应用到dense reconstruction task. Swin 引入了一种local attention机制，partions and calculate the SA within the local window. Furthermore, the shifted window operation is proposed to better extract the cross regions relation among different non-overlap windows. Swin 将ViT的复杂度降低到并且在多种vision任务重取得应用；以及SwinIR将Swin 应用到LLCV中；但是local window attention 中 cross regions relation is  hard to extract well that in SwinIR, extra Conv layer is needed to enhance the feature aggregation.此外，处理的图像大小需要被windows整除，并且Shift-window 这种对整幅图反复平移的操作影响执行效率,在实际应用中尤其是小模型中SwinIR-light 有很多局限；
% To reduce the computational cost, local attention-based 机制 is in demand, inspired from the success of the CNN-based models, we introduce a sliding-window-based self-attention 机制. which brings the inductive

%Specifically, we replace convolution operations in CNNs with multi-head self-attention (MSA) mechanism in vision transformers \cite{ViT}

%observe that  requires less parameters and 


%are better at modeling long-range relations compared with convolution operations. We 


%However, heavy computational cost is required to calculate the global self-attention. For efficient modeling, Liu \textit{et at.} \cite{SwinT} proposed a local window-based self-attention mechanism which partitioned the input into several non-overlapping regions and applied self-attention within each partition separately. Meanwhile, a shifted-window-based self-attention (Swin) and a pyramid-like architecture were proposed to further exploit the cross-region dependency. Subsequently, \textcolor{red}{Liang \textit{et al.} \cite{SwinIR} proposed SwinIR by applying the Swin Transformer to image restoration tasks, which took the normal convolution at the end of the residual Swin Transformer block (RSTB) for feature aggregation. On the other hand, limitations of the Swin, such as the input size must be divided by the local window size and there are redundant feature roll operations, making it inefficient for application.}

Specifically, in this paper, we propose a flexible and scalable neighborhood attention (NA) mechanism to substitute for standard convolutions. NA extracts feature relations in a sliding window pattern like standard convolutions. The corresponding feature extraction and aggregation compared to standard convolutions are shown in \cref{fig:na_vs_conv}. The number of parameters in the standard convolution is coupled with the kernel size and grows quadratically, which makes it inefficient to leverage the large kernel size. Unlike convolutions, the proposed NA mechanism decouples the number of parameters from the feature aggregation, which can efficiently model the long-range relations for the large kernel size.
By incorporating the proposed NA mechanism into CNNs, we propose a lightweight SISR network, named TCSR. TCSR adopts a shallow feature extraction module to extract features from an input LR image, and processes the extracted features with a feature aggregation module stacked by several NA blocks. In each NA block, we propose an enhanced feed-forward network (EFFN) following the NA module. Considering the FFN extracts the pixel-wise deep features separately and lacks the local feature aggregation, the proposed EFFN utilizes a spatial-shift operation leading to the effective local feature aggregation without extra computational cost. Finally, TCSR adopts a high-resolution reconstruction module based on $3\times 3$ convolutions and a pixelshuffle layer, resulting in the super-resolved image.

In summary, this paper propose a sliding window-based NA mechanism that sheds a new light on base model design. The proposed NA mechanism can effectively realizes large kernel sizes with much smaller number of parameters and FLOPs than standard convolutions.
Based on NA, we propose a lightweight SISR network, named TCSR. In TCSR, an EFFN with spatial-shift operations is further presented to achieve advanced feature enhancement.    
Extensive experiments demonstrate that the proposed TCSR achieves the state-of-the-art SISR performance and outperforms existing lightweight approaches, as illustrated in \cref{fig:manga109}.


%In summary, this paper proposes a sliding window-based NA mechanism and sheds a new light on base model design. The proposed NA mechanism can effectively realizes large kernel sizes, which are computationally expensive for standard convolutions.
%Meanwhile, we propose a EFFN with a spatial-shift operation to achieve advanced feature enhancement. Based on NA and EFFN, we a conv-like transformer network for lightweight image super-resolution is proposed, named TCSR. We build TCSR with different number of NA-based feature extraction blocks. Extensive experiments and analyses are presented, demonstrating the effectiveness of the proposed TCSR. As illustrated in \cref{fig:manga109}, the proposed TCSRs achieve a promise performance on Manga109 with upscale 4 and outperforms many existing lightweight approcahes.

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figure/manga109x4_psnr_param_TCSR.png}
    %\vskip-5.pt
    \caption{\textbf{PSNR} vs. \textbf{Parameters}. Comparisons with representative lightweight SISR models on Manga109 ($\times4$) test dataset.}
    %\vskip-10.pt
    \label{fig:manga109}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figure/Framework_new.pdf}
    \caption{The architecture of the proposed TCSR.}
    \label{fig:framework}
\end{figure*}




