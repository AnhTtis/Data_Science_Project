\chapter{Supporting Results}\label{chap:alg}


In this chapter we collect results which are called upon in the main body of the work, but which differ enough in character from our study of decompositions that they would distract from the central narrative of this thesis. With the potential exception of \Cref{thm:specialodds}, the results of this chapter are far from new -- we choose not to confine them to an appendix solely because we furnish proofs which could not be found in any references. There is no unified theme to this chapter, and accordingly the reader may wish to visit these sections as specific results are called upon elsewhere in the work.


\section{An Algebraic Result}


The first result we give is required in the proof of \Cref{thm:cauchylike}, and it ensures that for a given set of non-negative numbers, we can choose a set of weights whose odd powers enjoy the vanishing property of equation \eqref{eq:wowza}. This result is used to eliminate selective odd-order terms in the Taylor expansions of non-negative functions. 


\begin{thm}\label{thm:specialodds}
Let \(\ell\) be any odd integer and set \(s=\frac{\ell+1}{2}\). There exist numbers \(\eta_1,\dots,\eta_s\in\mathbb{R}\) and \(x_1,\dots,x_s\geq 0\) such that for each odd \(j\leq \ell\),
\begin{equation}\label{eq:wowza}
    \sum_{i=1}^sx_i\eta_i^j=\begin{cases}
    0 & j<\ell,\\
    1 & j=\ell.
    \end{cases}
\end{equation}
\end{thm}

\begin{proof}
It suffices to show that \(Mx=e_s\) has a non-negative solution \(x\), where \(e_s\) is the last standard basis vector in \(\mathbb{R}^s\) and the matrix \(M\) is given by
\[
    M=\begin{bmatrix}
    \eta_1&\eta_2&\cdots &\eta_s\\
    \eta_1^3&\eta_2^3&\cdots&\eta_s^{3}\\
    \vdots&\vdots&\ddots&\vdots\\
    \eta_1^\ell &\eta_2^\ell &\cdots&\eta_s^\ell 
    \end{bmatrix}.
\]
Our aim is to select \(\eta_1,\dots,\eta_s\) so that \(M\) is invertible and the entries of \(M^{-1}e_s\) are non-negative. Denoting by \(x_k\) the \(k^\mathrm{th}\) entry in column \(s\) of \(M^{-1}\), and assuming for now that \(M\) is nonsingular, we can write
\[
    x_k=\frac{(-1)^{s+k}\mathrm{det}(M_{sk})}{\mathrm{det}(M)},
\]
where \(M_{sk}\) is the matrix minor of \(M\) obtained by deleting row \(s\) and column \(k\) of \(M\). 
    

Using the properties of \(M\), we establish a formula for \(\mathrm{det}(M)\). Note that this is a homogeneous polynomial in the variables \(\eta_1,\dots,\eta_s\) of degree \(s^2\). If \(\eta_i=0\) for any \(i\) then \(M\) is singular, meaning that \(\eta_i\mid \mathrm{det}(M)\). Similarly, if \(i\neq j\) and \(\eta_i=\pm\eta_j\) then \(M\) has two linearly dependent columns, meaning once again that \(\mathrm{det}(M)=0\) and \((\eta_i^2-\eta_j^2)\mid \mathrm{det}(M)\) whenever \(i\neq j\). It follows that \(\mathrm{det}(M)=PQ\) where \(P\) and \(Q\) are polynomials and \(P\) is given by
\[
    P=\bigg(\prod_{i=1}^s\eta_i\bigg)\bigg(\prod_{i=1}^s\prod_{j=1}^{i-1}(\eta_i^2-\eta_j^2)\bigg).
\]
This polynomial has degree \(s^2\), and it follows from the Fundamental Theorem of Algebra that \(Q\) has degree zero and is constant. Moreover, the first term in the expansion of \(P\) equals the product down the diagonal of \(M\). Comparing coefficients, we see that \(Q=1\) and \(\mathrm{det}(M)=P\).


We can compute \(\mathrm{det}(M_{sk})\) with the formula above, since this minor assumes the same form as \(M\). Omitting appropriate terms from the determinant formula for \(M\), we get
\[
    \mathrm{det}(M_{sk})=\bigg(\prod_{\substack{1\leq i\leq s\\
    i\neq k}}\eta_i\bigg)\bigg(\prod_{\substack{1\leq j< i\leq s\\i,j\neq k}}(\eta_i^2-\eta_j^2)\bigg)
\]
Equipped with this identity and the formula for \(x_k\) above, we are now able to write
\[
    x_k=(-1)^{s+k}\bigg(\eta_k\prod_{j=1}^{k-1}(\eta_k^2-\eta_j^2)\prod_{i=k+1}^s(\eta_i^2-\eta_k^2)\bigg)^{-1}=\bigg(\eta_k\prod_{\substack{1\leq i\leq s\\i\neq k}}(\eta_k^2-\eta_i^2)\bigg)^{-1}
\]
To ensure that each \(x_k\) is positive, it suffices to choose \(\eta_1,\dots,\eta_s\) so that \(|\eta_1|<\cdots<|\eta_s|\) and \(\mathrm{sgn}(\eta_k)=(-1)^{s+k}\). Therefore taking \(\eta_k=(-1)^{s+k}k\) gives \eqref{eq:wowza}.
\end{proof}


Our choice of \(\eta_k\) above suffices for the purpose of proving \Cref{thm:cauchylike}, but other choices may be better suited to other applications.


As one example, the result above might be useful for understanding the coefficients of non-negative polynomials over \(\mathbb{R}\) via a similar argument to that employed in proving \Cref{thm:cauchylike}. In particular, let
\[
    P(x)=\sum_{j=0}^dc_jx^j
\]
be a non-negative polynomial on \(\mathbb{R}\) of even degree \(d\), fix \(\ell=d-1\), and choose constants \(x_1,\dots,x_s\) and \(\eta_1,\dots,\eta_s\) as in \eqref{eq:wowza}. Since \(x_iP(-\eta_ix)\geq 0\) for each \(i\) we have for all \(x\in\mathbb{R}\) that
\[
    0\leq \sum_{i=1}^sx_iP(-\eta_ix)=\sum_{i=1}^sx_i\sum_{j=0}^dc_j(-\eta_i)^jx^j=\sum_{\substack{0\leq j\leq d,\\j\textrm{even}}}^dc_jx^j\bigg(\sum_{i=1}^sx_i\eta_i^j\bigg)-c_\ell x^\ell
\]
Replacing \(x_iP(-\eta_ix)\) with \(x_iP(\eta_ix)\) and repeating this argument, we find after rearranging and taking a maximum that
\[
    |c_\ell|\leq \sum_{\substack{0\leq j\leq d,\\j\textrm{even}}}^dc_jx^{j-\ell}\bigg(\sum_{i=1}^sx_i\eta_i^j\bigg).
\]


By bounding the \(s\)-dependant terms in the inequality above and optimizing over \(x\), it is possible to obtain a bound on \(c_\ell\). Iterating this argument also affords bounds on all other odd-order coefficients of \(P\). Moreover, by using a complete Vandermonde matrix in \Cref{thm:specialodds} instead of one omitting even rows, the interested reader might also glean information about the sums in \eqref{eq:wowza} for even values of \(j\), and thereby recover explicit bounds in the preceding application and \Cref{thm:cauchylike}.


\section{Useful Estimates}


In this short section we establish some straightforward results from elementary real analysis relating to properties of the maximum, the supremum, and the triangle inequality. Their proofs are not difficult, but the results warrant justification which we include here. Our first lemma states a simple property of the maximum.


\begin{lem}\label{lem:maxproplem}
For \(m\in\mathbb{N}\) let \(a_1,\dots,a_m\) and \(b_1,\dots,b_m\) be real numbers. Then
\begin{equation}\label{eq:maxprop}
    |\max_{j\leq m}a_j-\max_{j\leq m}b_j|\leq \max_{j\leq m}|a_j-b_j|.
\end{equation}
\end{lem}


\begin{proof}
We use induction on \(m\), proving the non-trivial case \(m=2\) as our base case. Note that
\[
    \max\{a_1,b_1\}=\max\{a_1-a_2+a_2,b_1-b_2+b_2\}\leq \max\{a_1-a_2,b_1-b_2\}+\max\{a_2,b_2\}.
\]
Rearranging this, we see that \(\max\{a_1,b_1\}-\max\{a_2,b_2\}\leq \max\{a_1-a_2,b_1-b_2\}\), and the same inequality clearly holds when \(a_1\) is swapped with \(a_2\) and \(b_1\) with \(b_2\). Consequently,
\begin{align*}
    |\max\{a_1,b_1\}-\max\{a_2,b_2\}|&=\max\{\max\{a_1,b_1\}-\max\{a_2,b_2\},\max\{a_2,b_2\}-\max\{a_1,b_1\}\}\\
    &\leq \max\{\max\{a_1-a_2,b_1-b_2\},\max\{a_2-a_1,b_2-b_1\}\}\\
    &=\max\{a_1-a_2,b_1-b_2,a_2-a_1,b_2-b_1\}\\
    &=\max\{|a_1-a_2|,|b_1-b_2|\}.
\end{align*}
This establishes the base case. Suppose next that inequality \eqref{eq:maxprop} holds for \(m\) terms. Using the preceding argument we obtain the bound
\begin{align*}
    |\max_{j\leq m+1}a_j-\max_{j\leq m+1}b_j|&=|\max\{\max_{j\leq m}a_j,a_{m+1}\}-\max\{\max_{j\leq m}b_j,b_{m+1}\}|\\
    &\leq \max\{|\max_{j\leq m}a_j-\max_{j\leq m}b_j|,|a_{m+1}-b_{m+1}|\},
\end{align*}
and with this it follows from our inductive hypothesis that
\begin{align*}
    |\max_{j\leq m+1}a_j-\max_{j\leq m+1}b_j|\leq \max\{ \max_{j\leq m}|a_j-b_j|,|a_{m+1}-b_{m+1}|\}= \max_{j\leq m+1}|a_j-b_j|  .  
\end{align*}
Hence inequality \eqref{eq:maxprop} holds with \(m+1\) terms, and the claimed estimates follow by induction.
\end{proof}


Now we establish a similar property for the supremum.


\begin{lem}\label{lem:supprop}
For any non-negative function \(g:\mathbb{R}^n\times\mathbb{R}^n\rightarrow\mathbb{R}\) and for any two points \(x,y\in\mathbb{R}^n\),
\[
    |\sup_{\xi\in\mathbb{R}^n} g(x,\xi)-\sup_{\xi\in\mathbb{R}^n} g(y,\xi)|\leq \sup_{\xi\in\mathbb{R}^n} |g(x,\xi)-g(y,\xi)|.
\]
\end{lem}

\begin{proof}
For any non-negative function \(g\) we can use subadditivity of the supremum to get
\[
    \sup_{\xi\in\mathbb{R}^n} g(x,\xi)=\sup_{\xi\in\mathbb{R}^n} (g(x,\xi)-g(y,\xi)+g(y,\xi))\leq \sup_{\xi\in\mathbb{R}^n} |g(x,\xi)-g(y,\xi)|+\sup_{\xi\in\mathbb{R}^n} g(y,\xi).
\]
Therefore we can rearrange to get
\[
    \sup_{\xi\in\mathbb{R}^n} g(x,\xi)-\sup_{\xi\in\mathbb{R}^n} g(y,\xi)\leq \sup_{\xi\in\mathbb{R}^n} |g(x,\xi)-g(y,\xi)|.
\]  
An identical argument interchanging \(y\) and \(x\) shows that 
\[
    \sup_{\xi\in\mathbb{R}^n} g(y,\xi)-\sup_{\xi\in\mathbb{R}^n} g(x,\xi)\leq \sup_{\xi\in\mathbb{R}^n} |g(x,\xi)-g(y,\xi)|
\]
The claimed estimate then follows from taking a maximum.
\end{proof}


Finally, we establish a variant of the triangle inequality for products of functions.


\begin{lem}\label{lem:proddiff}
Let \(f_1,\dots,f_k\) be bounded functions on \(\mathbb{R}^n\), none of which is identically zero. Then for any two points \(x,y\in\mathbb{R}^n\),
\[
    \bigg|\prod_{j=1}^kf_j(x)-\prod_{j=1}^kf_j(y)\bigg|\leq \bigg(\prod_{j=1}^k\sup_{\mathbb{R}^n}|f_j|\bigg)\sum_{j=1}^k\frac{|f_j(x)-f_j(y)|}{\sup_{\mathbb{R}^n}|f_j|}.
\]
\end{lem}


\begin{proof}
We argue by induction on \(k\). There is nothing to show when \(k=1\), and when \(k=2\) we use the triangle inequality to write
\[
    |f_1(x)f_2(x)-f_1(y)f_2(y)|\leq |f_1(x)||f_2(x)-f_2(y)|+|f_2(y)||f_1(x)-f_1(y)|.
\]
Upon taking a supremum of \(|f_1(x)|\) and \(|f_2(y)|\), we get the desired estimate when \(k=2\). Suppose next that the estimate holds for \(k\) functions. Using the triangle inequality, we have
\begin{align*}
    \bigg|\prod_{j=1}^{k+1}f_j(x)-&\prod_{j=1}^{k+1}f_j(y)\bigg|=\bigg|f_{k+1}(x)\prod_{j=1}^{k}f_j(x)-f_{k+1}(y)\prod_{j=1}^{k}f_j(y)\bigg|\\
    &=\bigg|f_{k+1}(x)\prod_{j=1}^{k}f_j(x)-f_{k+1}(y)\prod_{j=1}^{k}f_j(x)+f_{k+1}(y)\prod_{j=1}^{k}f_j(x)-f_{k+1}(y)\prod_{j=1}^{k}f_j(y)\bigg|\\
    &\leq \bigg(\prod_{j=1}^{k}|f_j(x)|\bigg)|f_{k+1}(x)-f_{k+1}(y)|+|f_{k+1}(y)|\bigg|\prod_{j=1}^{k}f_j(x)-\prod_{j=1}^{k}f_j(y)\bigg|.
\end{align*}
Applying the inductive hypothesis, we see now that
\[
    \bigg|\prod_{j=1}^{k+1}f_j(x)-\prod_{j=1}^{k+1}f_j(y)\bigg|\leq \bigg(\prod_{j=1}^{k}|f_j(x)|\bigg)|f_{k+1}(x)-f_{k+1}(y)|+\bigg(\prod_{j=1}^{k+1}\sup_{\mathbb{R}^n}|f_j|\bigg)\sum_{j=1}^k\frac{|f_j(x)-f_j(y)|}{\sup_{\mathbb{R}^n}|f_j|}.
\]
The required estimate then follows by taking a supremum in the product on the right, completing the inductive step and showing that the claimed estimate holds for all \(k\) as claimed. 
\end{proof}

It is easy to see that the preceding results still hold if \(\mathbb{R}^n\) is replaced with any domain \(\Omega\subseteq\mathbb{R}^n\).


\section{H\"older Continuous Functions}\label{sec:holdexamples}


Our main results for H\"older spaces would not be very useful if there were no interesting functions to which they could apply. In this section we provide some concrete (and not completely trivial) examples of \(\alpha\)-H\"older continuous functions for various values of \(\alpha\). In doing so, we are also able to illustrate various techniques useful in proving H\"older continuity.


We begin with a straightforward and well-known example of an \(\alpha\)-H\"older continuous function.

\begin{clm}
If \(\alpha\in (0,1]\) then \(f(x)=|x|^\alpha\) defined for \(x\in\mathbb{R}^n\) belongs to \(C^\alpha(\mathbb{R}^n)\), and \([f]_{\alpha,\mathbb{R}^n}=1\).
\end{clm}

\begin{proof}
To verify this, we assume without loss of generality that \(x,y\in\mathbb{R}^n\) and \(|x|>|y|\), so that 
\[
    0<1-\bigg(\frac{|y|}{|x|}\bigg)^\alpha\leq 1-\frac{|y|}{|x|}\leq \bigg(1-\frac{|y|}{|x|}\bigg)^\alpha
\] 
since \(\alpha\leq 1\) and \(\frac{|y|}{|x|}<1\) by assumption. From the estimates above it follows now that
\[
    \frac{|f(x)-f(y)|}{|x-y|^\alpha}\leq  \frac{||x|^\alpha-|y|^\alpha|}{||x|-|y||^\alpha}=\frac{1-(\frac{|y|}{|x|})^\alpha}{(1-\frac{|y|}{|x|})^\alpha}\leq \frac{1-\frac{|y|}{|x|}}{1-\frac{|y|}{|x|}}=1.
\]
The same estimate clearly holds if we interchange \(x\) and \(y\). Since \(x\) and \(y\) were arbitrary, it follows from the estimate above that \([f]_{\alpha,\mathbb{R}^n}\leq 1\). Moreover this holds with equality, for if we assume that \([f]_\alpha<1\), then we would have \(|x|^\alpha=|f(x)|\leq [f]_{\alpha,\mathbb{R}^n}|x|^\alpha<|x|^\alpha\), a contradiction for \(x\neq 0\). It follows that \([f]_{\alpha,\mathbb{R}^n}=1\), as claimed.
\end{proof}


Next we move on to a more interesting example: Cantor's `Middle Thirds' function, sometimes colloquially referred to as the `Devil's Staircase.' This is famously a continuous bijection on \([0,1]\) whose derivative is zero except on a set of measure zero; as such, it serves as a pathological counterexample to the intuitive but incorrect idea that a continuous increasing function should be increasing on some interval. 


The Cantor function, which we denote by \(F\), is a fractal curve which turns out to be H\"older continuous. Additionally, it is proved in \cite[Chapter 9]{Teschl} that by taking \(f_0(x)=x\) on \([0,1]\) and defining \(f_{n+1}(x)=Tf_n(x)\), where \(T\) is the transformation
\[
    Tf(x)=\begin{cases}
    \hfil\frac{1}{2}f(3x) & 0\leq x\leq \frac{1}{3},\\
    \hfil\frac{1}{2} & \frac{1}{3}<x<\frac{2}{3},\\
    \frac{1}{2}(1+f(3x-2)) & \frac{2}{3}\leq x\leq 1,
    \end{cases}
\]
then we obtain a sequence of functions \(\{f_n\}\) which converge uniformly to \(F\) on \([0,1]\). By solving an exercise in \cite{Teschl}, we furnish a proof to the following well-known property of \(F\).


\begin{clm}
The Cantor function \(F\) belongs to \(C^{\log_32}([0,1])\) and \([F]_{\log_32,[0,1]}\leq1\).
\end{clm}

\begin{proof} Given a continuous bijection \(f\) of \([0,1]\) such that \(f(0)=0\), \(f(1)=1\) and \(f\in C^{\alpha}([0,1])\) for \(0<\alpha\leq 1\), we first examine the H\"older continuity of \(Tf\). In particular, we show that
\[
    [Tf]_\alpha\leq \frac{3^\alpha[f]_\alpha}{2}.
\]
This is done by considering six exhaustive cases which depend on the locations of \(x,y\in[0,1]\).

\noindent\underline{\textit{Case 1}}: If \(0\leq x\leq \frac{1}{3}\) and \(0\leq y\leq \frac{1}{3}\) then
\[
    |Tf(x)-Tf(y)|=\frac{1}{2}|f(3x)-f(3y)|\leq \frac{[f]_\alpha}{2}|3x-3y|^\alpha=\frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha.
\]
\noindent\underline{\textit{Case 2}}: If \(0\leq x\leq \frac{1}{3}\) and \(\frac{1}{3}< y<\frac{2}{3}\) then
\[
    |Tf(x)-Tf(y)|=\frac{1}{2}|f(3x)-1|=\frac{1}{2}|f(3x)-f(1)|\leq \frac{[f]_\alpha}{2}|3x-1|^\alpha\leq \frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha,
\]
where the last inequality holds since \(|3x-1|=3|x-\frac{1}{3}|\leq 3|x-y| \).


\noindent\underline{\textit{Case 3}}: If \(0\leq x\leq \frac{1}{3}\) and \(\frac{2}{3}\leq y\leq 1\) then 
\[
    |Tf(x)-Tf(y)|=\frac{1}{2}|f(3x)-f(1)+f(0)-f(3y-2)|\leq \frac{3^\alpha[f]_\alpha}{2}\bigg|x-\frac{1}{3}\bigg|^\alpha+\frac{3^\alpha[f]_\alpha}{2}\bigg|y-\frac{2}{3}\bigg|^\alpha.
\]
To get the required inequality, we must show that \(|x-\frac{1}{3}|^\alpha+|y-\frac{2}{3}|^\alpha\leq |x-y|^\alpha\) for \(\alpha\) sufficiently large. To this end we observe that for \(\alpha\leq 1\) the function \(Q:[0,\frac{1}{3}]\times[\frac{2}{3},1]\rightarrow\mathbb{R}\) defined by
\[
    Q(x,y)=\bigg|\frac{x-\frac{1}{3}}{x-y}\bigg|^\alpha+\bigg|\frac{y-\frac{2}{3}}{x-y}\bigg|^\alpha
\]
achieves a unique global maximum at \((0,1)\). It follows that on \([0,\frac{1}{3}]\times[\frac{2}{3},1]\) we have the bound \( Q(x,y)\leq Q(0,1)=\frac{2}{3^\alpha}\). If \(\alpha\geq\log_32\) then we see that \(Q(x,y)\leq \frac{2}{3^\alpha}\leq 1\) in the domain of interest, meaning that again,
\[
    |Tf(x)-Tf(y)|\leq \frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha.
\]
\underline{\textit{Case 4}}: If \(\frac{1}{3}<x<\frac{2}{3}\) and \(\frac{1}{3}<y<\frac{2}{3}\) then
\[
    |Tf(x)-Tf(y)|=\bigg|\frac{1}{2}-\frac{1}{2}\bigg|=0\leq \frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha.
\]
\noindent\underline{\textit{Case 5}}: If \(\frac{1}{3}<x<\frac{2}{3}\) and \(\frac{2}{3}\leq y\leq 1\) then
\[
    |Tf(x)-Tf(y)|=\frac{1}{2}|1-(1+f(3y-2))|=\frac{1}{2}|f(3y-2)-f(0)|\leq \frac{[f]_\alpha}{2}|3y-2|^\alpha\leq\frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha.
\]
\noindent\underline{\textit{Case 6}}: If \(\frac{2}{3}\leq y\leq 1\) and \(\frac{2}{3}\leq y\leq 1\) then
\[
    |Tf(x)-Tf(y)|=\frac{1}{2}|f(3x-2)-f(3y-2)|\leq \frac{[f]_\alpha}{2}|3x-3y|^\alpha=\frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha.
\]


Thus, if \(\log_32\leq\alpha\leq 1\) then for \(x,y\in[0,1]\) we have the following H\"older estimate on \(Tf\),
\[
    |Tf(x)-Tf(y)|\leq \frac{3^\alpha[f]_\alpha}{2}|x-y|^\alpha.
\]
Equipped with this property of \(T\), we now observe that if we fix \(\alpha=\log_32\) and if \([f]_\alpha\leq1\), then \(|Tf(x)-Tf(y)|\leq |x-y|^\alpha\) and \([Tf]_\alpha\leq 1\). The function \(f_0(x)=x\) satisfies \([f_0]_\alpha\leq 1\) since for \(x,y\in[0,1]\) we have \(|x-y|\leq 1\) and 
\[
    |f_0(x)-f_0(y)|=|x-y|=|x-y|^{1-\alpha}|x-y|^\alpha\leq |x-y|^\alpha.
\]
Consequently with \(\alpha=\log_32\) we have \([f_1]_\alpha=[Tf_0]_\alpha\leq 1\), and it follows by induction that \([f_n]_\alpha\leq 1\) for every \(n\). Using this estimate, we see that for every \(n\) the Cantor function \(F\) satisfies
\[
    |F(x)-F(y)|\leq |F(x)-f_n(x)|+|F(y)-f_n(y)|+|x-y|^\alpha.
\]
The left-hand side is independent of \(n\), and since the functions \(f_n\) converge to \(F\) pointwise we have that \(|F(x)-f_n(x)|\rightarrow0\) as \(n\rightarrow\infty\) for each \(x\in[0,1]\). It follows in the limit that
\[
    |F(x)-F(y)|\leq|x-y|^\alpha
\]
for every \(x,y\in[0,1]\), meaning that \(F\in C^{\log_32}([0,1])\) as we wished to show.
\end{proof}


This approach to computing H\"older continuity for fractal curves can be applied more generally. In particular, it suffices to find an appropriate map \(T\) corresponding to a given fractal curve, and to employ the technique used above to show that \([Tf]_\alpha\leq [f]_\alpha\) for some \(\alpha\). The map in question is most easily deduced from symmetries of the fractal curve in question. 

One such fractal curve is the Minkowski `Question Mark' function. This is often denoted `\(?\)' in the literature, however to avoid confusion (no pun intended) we denote this function by \(Q\). It has fixed points at \(0\) and \(1\) and it enjoys the following self-similarity relations for \(x\leq \frac{1}{2}\),
\begin{align*}
    &Q\bigg(\frac{x}{1+x}\bigg)=\frac{1}{2}Q(x),\\
    &Q(x)+Q(1-x)=1,
\end{align*}
see \cite{ALKAUSKAS_2009}. Salem shows in \cite{salem} that \(Q\in C^{\alpha}([0,1])\), with the H\"older exponent \(\alpha\) given by
\[
    \alpha=\frac{\log 2}{2\log(\frac{1+\sqrt{5}}{2})}.
\]
The proof of this result in \cite{salem} uses an altogether different technique from that employed above, but we note that owing to the symmetries listed above, \(Q\) is a fixed point of the map
\[
    Tf(x)=\begin{cases}
    \hfil\frac{1}{2}f(\frac{x}{1-x}) & x\leq\frac{1}{2}\\
    \frac{1}{2}+\frac{1}{2}f(\frac{2x-1}{x}) & x>\frac{1}{2}
    \end{cases}
\]
which preserves bijections on \([0,1]\). By taking \(f_0(x)=x\) and defining \(f_{n+1}=Tf_n\) as we did for the Cantor function, we obtain a sequence of functions which converge uniformly to \(Q\) since \(T\) is a contraction mapping -- we direct the interested reader to \cite{bezier} for a thorough investigation of this idea. Further, we close by noting that a similar approach can be used to study (and for our purposes, compute the H\"older continuity of) other fractals like the Blancmange curve.