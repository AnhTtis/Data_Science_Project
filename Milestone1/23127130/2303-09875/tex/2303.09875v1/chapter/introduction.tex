\section{Introduction}

%
Video prediction aims to predict future video frames from the current ones. The task potentially benefits the study on representation learning~\cite{oprea2020review} and downstream forecasting tasks such as human motion prediction~\cite{martinez2017human}, autonomous driving~\cite{castrejon2019improved}, and climate change~\cite{shi2015convolutional}, \etc. During the last decade, video prediction has been increasingly studied in both academia and industry community~\cite{chandra2019traphic,byeon2018contextvp}.


\begin{figure}[tb]
	\centering
	\includegraphics[width=0.47\textwidth]{image/fig1.pdf}
	\caption{
\textbf{Average MS-SSIM and GFLOPs of different video prediction methods} on Cityscapes~\cite{cityscapes}. The parameter amounts are provided in brackets. DMVFN outperforms previous methods in terms of image quality, parameter amount, and GFLOPs.
}
	\label{fig:performance}
\end{figure}

Video prediction is challenging because of the diverse and complex motion patterns in the wild, in which accurate motion estimation plays a crucial role~\cite{prednet,mcnet,dvf}.
%
Early methods~\cite{prednet,mcnet} along this direction mainly utilize recurrent neural networks~\cite{hochreiter1997long} to capture temporal motion information for video prediction. To achieve robust long-term prediction, the works of~\cite{seg2vid,vid2vid,fvs} additionally exploit the semantic or instance segmentation maps of video frames for semantically coherent motion estimation in complex scenes. However, the semantic or instance maps may not always be available in practical scenarios, which limits the application scope of these video prediction methods~\cite{seg2vid,vid2vid,fvs}. To improve the prediction capability while avoiding extra inputs, the method of OPT~\cite{wu2022optimizing} utilizes only RGB images to estimate the optical flow of video motions in an optimization manner with impressive performance. However, its inference speed is largely bogged down mainly by the computational costs of pre-trained optical flow model~\cite{raft} and frame interpolation model~\cite{rife} used in the iterative generation.

% 
The motions of different objects between two adjacent frames are usually of different scales. This is especially evident in high-resolution videos with meticulous details~\cite{xvfi}. The spatial resolution is also of huge differences in real-world video prediction applications. To this end, it is essential yet challenging to develop a single model for multi-scale motion estimation. An early attempt is to extract multi-scale motion cues in different receptive fields by employing the encoder-decoder architecture~\cite{dvf}, but in practice it is not flexible enough to deal with complex motions.

In this paper, we propose a Dynamic Multi-scale Voxel Flow Network (DMVFN) to explicitly model the complex motion cues of diverse scales between adjacent video frames by dynamic optical flow estimation. Our DMVFN is consisted of several Multi-scale Voxel Flow Blocks (MVFBs), which are stacked in a sequential manner. On top of MVFBs, a light-weight Routing Module is proposed to adaptively generate a routing vector according to the input frames, and to dynamically select a sub-network for efficient future frame prediction. We conduct experiments on four benchmark datasets, including Cityscapes~\cite{cityscapes}, KITTI~\cite{kitti}, DAVIS17~\cite{davis}, and Vimeo-Test~\cite{vimeo}, to demonstrate the comprehensive advantages of our DMVFN over representative video prediction methods in terms of visual quality, parameter amount, and computational efficiency measured by floating point operations~(FLOPs). A glimpse of comparison results by different methods is provided in Figure~\ref{fig:performance}. One can see that our DMVFN achieves much better performance in terms of accuracy and efficiency on the Cityscapes~\cite{cityscapes} dataset. Extensive ablation studies validate the effectiveness of the components in our DMVFN for video prediction.

In summary, our contributions are mainly three-fold:
\begin{itemize}
    \item We design a light-weight DMVFN to accurately predict future frames with only RGB frames as inputs. Our DMVFN is consisted of new MVFB blocks that can model different motion scales in real-world videos.
    
    \item We propose an effective Routing Module to dynamically select a suitable sub-network according to the input frames. The proposed Routing Module is end-to-end trained along with our main network DMVFN.
    
    \item Experiments on four benchmarks show that our DMVFN achieves state-of-the-art results while being an order of magnitude faster than previous methods.
\end{itemize}