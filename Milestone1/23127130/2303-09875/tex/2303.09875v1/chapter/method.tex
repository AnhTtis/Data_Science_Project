


\section{Methodology}
%
%

\subsection{Background}
\label{sec:background}

\paragraph{Video prediction.}
%
Given a sequence of past $t$ frames $\{I_{i}\in \mathbb{R}^{h\times w\times 3}|i=1,...,t\}$, video prediction aims to predict the future frames $\{ \tilde{I}_{t+1}, \tilde{I}_{t+2}, \tilde{I}_{t+3} ,\ ...\}$.
%
The input of our video prediction model only includes the two consecutive frames $I_{t-1}$ and $I_t$. 
%
We concentrate on predicting $\tilde{I}_{t+1}$, and predict future frames iteratively in a similar manner.
%
Denote the video prediction model as $G_{\theta}(I_{t-1},I_{t})$, where $\theta$ is the set of model parameters to be learned, the learning objective is to minimize the difference between $\tilde{I}_{t+1}=G_{\theta}(I_{t-1},I_{t})$ and the ``ground truth'' $I_{t+1}$.

\paragraph{Voxel flow.} Considering the local consistency in space-time, the pixels of a generated future frame come from nearby regions of the previous frames~\cite{zhou2016view,vimeo}. In video prediction task, researchers estimate optical flow $\textbf{f}_{t+1 \rightarrow t}$ from $I_{t+1}$ to $I_t$~\cite{dvf}. And the corresponding frame is obtained using the pixel-wise backward warping~\cite{backwarp}~(denoted as $\backwardwarp$). In addition, to deal with the occlusion, some methods further introduce a fusion map $\textbf{m}$~\cite{superslomo,dvf} to fuse the pixels of $I_{t}$ and $I_{t-1}$. The final predicted frame is obtained by the following formulation~(Figure~\ref{fig:predict} $(a)$):
\begin{equation}
% 
\hat{I}_{t+1\leftarrow{t-1}} = \backwardwarp( I_{t-1},   \textbf{f}_{ t+1 \rightarrow t-1} ),
\end{equation}
\begin{equation}
% 
%
     \hat{I}_{t+1 \leftarrow{t}} = \backwardwarp( I_t,  \textbf{f}_{t+1 \rightarrow t} ),
\end{equation}
\begin{equation}
%
     \tilde{I}_{t+1} = \hat{I}_{t+1\leftarrow{t-1}} \times \textbf{m} + \hat{I}_{t+1\leftarrow{t}} \times (1-\textbf{m}).
\end{equation}
Here, $\hat{I}_{t+1 \leftarrow{t}}$ and $\hat{I}_{t+1 \leftarrow{t-1}}$ are intermediate warped images. To simplify notations, we refer to the optical flows $\textbf{f}_{t+1 \rightarrow t},\textbf{f}_{t+1 \rightarrow t-1}$ and the fusion map $\textbf{m}$ collectively as the Voxel Flow $\textbf{F}_{t+1}$, same as notations in~\cite{dvf}. The above equations can be simplified to the following form:
\begin{equation}
     \tilde{I}_{t+1} = \backwardwarp( I_{t-1}, I_t,  \textbf{F}_{t+1} ).
\end{equation}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.95\linewidth]{image/comparison2.png}
	\caption{\textbf{Visual comparison of ($t+1$)-th frame predicted from $t$-th and ($t-1$)-th frames on the DAVIS17-Val~\cite{davis}}.}
	\label{fig:davis17}
 \vspace{-1em}
\end{figure*}

\subsection{Dynamic Multi-Scale Voxel Flow Network}
\label{sec:dmvfn}

\paragraph{MVFB.} 

To estimate the voxel flow, DVF~\cite{dvf} assumes that all the optical flows are locally linear and temporally symmetrical around targeted time, which may be unreasonable for large-scale motions. To address the object position changing issue~\cite{rife} in adjacent frames, OPT~\cite{wu2022optimizing} uses flow reversal layer~\cite{qvi} to convert forward flows to backward flows. We aim to estimate voxel flow end-to-end without introducing new components and unreasonable constraints.

We denote the $i$-th MVFB as $f^i_{MVFB}(\cdot)$.
Take two frames $I_{t-1}$ and $I_t$, the synthesized frame $\tilde{I}^{i-1}_{t+1}$, and the voxel flow estimated by previous blocks $\textbf{F}^{i-1}_{t+1}$ as input, MVFB learns to approximate target voxel flow $\textbf{F}^{i}_{t+1}$.

The architecture of MVFB is shown in Figure~\ref{fig:predict} $(c)$. To capture the large motion while retaining the original spatial information, we construct a two-branch network structure~\cite{yu2018bisenet}. This design inherits from pyramidal optical flow estimation~\cite{pwcnet,spynet}. In the \textbf{motion path}, the input is down-sampled by scaling factor $S^i$ to facilitate the expansion of the receptive field. Another \textbf{spatial path} operates at high resolution to complement the spatial information. We denote $\tilde{I}^{i}_{t+1} $ as the output of the $i$-th MVFB. Formally, 
%
\begin{equation}
     \tilde{I}^{i}_{t+1}, \textbf{F}^i_{t+1} = f_{\mathrm{MVFB}}^i (I_{t-1}, I_t, \tilde{I}^{i-1}_{t+1}, \textbf{F}^{i-1}_{t+1}, S^i).
\end{equation}
% 
%
The initial values of $\tilde{I}_{t+1}^{0}$ and $\textbf{F}^{0}_{t+1}$ are set to zero. As illustrated in Figure~\ref{fig:predict}~$(b)$, DMVFN contains $9$ MVFBs with $S^i$. To generate a future frame, we iteratively refine a voxel flow~\cite{dvf} and fuse the pixels of the input frame. 

Many optical flow methods estimate optical flow on a small image, and then refine the optical flow on a large image~\cite{pwcnet,gmflow}. For simplicity and intuition, we consider decreasing scaling factor sequences. Finally, the scaling factors is experimentally set to [4,4,4,2,2,2,1,1,1].

% 
\iffalse 
Formally, 
\begin{equation}
     \textbf{x}^i = Concat(I_{t-1}^{i-1}, I_t^{i-1}, \tilde{I}^{i-1}_{t+1}, \textbf{F}^{i-1}_{t+1}),
\end{equation}
\begin{equation}
     \textbf{m}^i = Convs_1(Resize(\textbf{x}^i, 1/S^i)),
\end{equation}
\begin{equation}
     \textbf{f}^i = Concat(Resize(\textbf{m}^i, 2S^i), Convs_2(\textbf{x}^i),
\end{equation}
\begin{equation}
     \textbf{F}^i_{t+1} = TransposedConv(\textbf{f}^i)+\textbf{F}^{i-1}_{t+1},
\end{equation}
\begin{equation}
     \tilde{I}^{i}_{t+1} = \backwardwarp(I_{t-1}, I_{t}, \textbf{F}_{t+1}^i),
\end{equation}
%
where we denote the two-way convolution operations as $Convs_1$ and $Convs_2$. 
\fi




\paragraph{DMVFN.} 
Different pairs of adjacent frames with diverse motion scales have different computational demands. An intuitive idea is to select dynamic architectures conditioned on each input adaptively. We then perform dynamic routing within the~\textbf{super network}~\cite{dynamicnn}~(the whole architecture), including multiple possible paths. DMVFN saves redundant computation for samples with small-scale motion and preserves the representation ability for large-scale motion.

To make DMVFN end-to-end trainable, we design a differentiable Routing Module containing a tiny neural network to estimate routing vector $v$ for each input sample. Based on this vector, DMVFN dynamically selects a sub-network to process the input data. As the figure shows, some blocks are skipped during inference.



Different from some dynamic network methods that can only continuously select the first several blocks ($n$ options)~\cite{branchynet,bolukbasi2017adaptive}, DMVFN is able to choose paths freely ($2^{n}$ options). DMVFN trains different sub-networks in the super network with various possible inference paths and uses dynamic routing inside the super network during inference to reduce redundant computation while maintaining the performance. A dynamic routing vector $v \in { \{ 0,1 \} }^{n}$ is predicted by the proposed Routing Module. Once we get the dynamic routing vector $v$, we can dynamically select the corresponding sub-network based on this $v$ during inference. For the $i$-th block $i$ of DMVFN, we denote $v_i$ as the reference of whether processing the reached voxel flow $\textbf{F}^{i-1}_{t+1}$ and the reached predicted image $\tilde{I}^{i-1}_{t+1}$. The path $f^I_\mathrm{MVFB}$ to the $i$-th block from the last block will be activated only when $v_i = 1$. Formally, 
\begin{equation}
\tilde{I}^{i}_{t+1},\textbf{F}^{i}_{t+1}=
\begin{cases}
f^i_\mathrm{MVFB}(\tilde{I}^{i-1}_{t+1},\textbf{F}^{i-1}_{t+1}), & v_i=1 \\
\\
\tilde{I}^{i-1}_{t+1},\textbf{F}^{i-1}_{t+1}, & v_i=0.
\end{cases}
\end{equation}



%

\begin{table*}[th]
\centering
\caption{
\textbf{Quantitative results of different methods on the Cityscapes~\cite{cityscapes}, and KITTI~\cite{kitti} datasets}. ``RGB", ``F", ``S" and ``I" denote the video frames, optical flow, semantic map, and instance map, respectively. We denote our DMVFN without routing as ``DMVFN~(w/o r)". FVS~\cite{fvs} integrates a segmentation model~\cite{zhu2019improving} on KITTI~\cite{kitti} to obtain the semantic maps. ``N/A'' means not available. The best results are highlighted in \textbf{bold}.}
\label{table:cityscapes}
\resizebox{0.96\linewidth}{!}{
\begin{tabular}{rccccccccccccccc}
\toprule
\multirow{3}{*}[-0.28em]{Method}& \multirow{3}{*}[-0.28em]{Inputs} &  \multicolumn{7}{c}{Cityscapes-Train$\rightarrow$Cityscapes-Test~\cite{cityscapes}} & \multicolumn{7}{c}{KITTI-Train$\rightarrow$KITTI-Test~\cite{kitti}} \\
\cmidrule(l{7pt}r{7pt}){3-9} \cmidrule(l{7pt}r{7pt}){10-16}
& & \multirow{2}{*}[-0.14em]{GFLOPs} & \multicolumn{3}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & \multicolumn{3}{c}{LPIPS ($\times10^{-2}$) $\downarrow$} &  \multirow{2}{*}[-0.14em]{GFLOPs}  & \multicolumn{3}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & 
\multicolumn{3}{c}{LPIPS ($\times10^{-2}$) $\downarrow$}\\
&  &  & t+1 & t+3 &t+5  & t+1  & t+3&t+5 & & t+1 & t+3&t+5  & t+1  & t+3&t+5  \\ 
\midrule
\midrule
Vid2vid~\cite{vid2vid} & RGB+S &   603.79    & 88.16 & 80.55&75.13 & 10.58 & 15.92& 20.14 & N/A & N/A & N/A& N/A & N/A & N/A& N/A  \\
Seg2vid~\cite{seg2vid} & RGB+S &  455.84  & 88.32 & N/A & 61.63 & 9.69 &  N/A& 25.99& N/A & N/A & N/A& N/A & N/A& N/A& N/A\\ 
FVS~\cite{fvs}   & RGB+S+I&  1891.65 & 89.10 & 81.13 & 75.68 & 8.50 & \textbf{12.98} & 16.50 & \textbf{768.96} & 79.28 & 67.65& 60.77  & 18.48 & 24.61& 30.49\\
SADM~\cite{sadm}   & RGB+S+F&  N/A & \textbf{95.99} & N/A & \textbf{83.51} & \textbf{7.67} & N/A & \textbf{14.93} & N/A & \textbf{83.06} & \textbf{72.44} & \textbf{64.72}  & \textbf{14.41} & \textbf{24.58} &  \textbf{31.16}\\

\midrule

PredNet~\cite{prednet} & RGB & 62.62 & 84.03 & 79.25& 75.21 & 25.99 & 29.99& 36.03 & 25.44 & 56.26 &  51.47& 47.56 & 55.35 & 58.66& 62.95               \\
MCNET~\cite{mcnet} & RGB & 502.80 & 89.69 & 78.07& 70.58 & 18.88 & 31.34& 37.34 & 204.26 & 75.35  & 63.52& 55.48 & 24.05  & 31.71& 37.39  \\
DVF~\cite{dvf} & RGB & 409.78 & 83.85 & 76.23& 71.11 & 17.37 & 24.05& 28.79& 166.47 & 53.93 & 46.99& 42.62  & 32.47  & 37.43& 41.59         \\
CorrWise~\cite{corrwise} & RGB & 944.29 & 92.80 & N/A & \textbf{83.90} & 8.50 & N/A & 15.00 & 383.62 & 82.00 & N/A & 66.70 & 17.20 & N/A & \textbf{25.90}\\ 
OPT~\cite{wu2022optimizing} & RGB &  313482.15  & 94.54 & 86.89 & 80.40 & 6.46 & 12.50&17.83 & 127431.71 & 82.71 & 69.50 & 61.09 & 12.34 & 20.29 & 26.35\\

DMVFN~(w/o r)  & RGB &  24.51  & 95.29 & 87.91 & 81.48 & 5.60 & 10.48 & 14.91 & 9.96 &88.06 & 76.53 & 68.29 & \textbf{10.70} & 19.28 & 26.13 \\
DMVFN  & RGB & \textbf{12.71} & \textbf{95.73} & \textbf{89.24}& 83.45 & \textbf{5.58} & \textbf{10.47}& \textbf{14.82} & \textbf{5.15} & \textbf{88.53} & \textbf{78.01} & \textbf{70.52} & 10.74 & \textbf{19.27} & 26.05    \\

\midrule
\end{tabular}
}
\end{table*}

% 
In the iterative scheme of DMVFN, each MVFB essentially refines the current voxel flow estimation to a new one. This special property allows us to skip some MVFBs.  
We will design a differentiable and efficient module for learning to trade-off every block. We propose the routing vector $v \in { \{ 0,1 \} }^{n}$ to identify the proper sub-network (0 for deactivated MVFBs, 1 for activated MVFBs). We use a small neural network~($1/6$ GFLOPs of the super network), and its detailed architecture is shown in Figure~\ref{fig:predict}~$(d)$. It learns to predict the probability $\tilde{v}$ of choosing MVFBs:
% 
\begin{equation}
     \tilde{v} =  Linear(AvgPooling( Convs(I_{t-1}, I_t) )), 
\end{equation}
\begin{equation}
     {v} = sample(\tilde{v}). 
\end{equation}

\paragraph{Differentiable Routing.} 
To train the Routing Module, we need to make this probability value have certain constraints to prevent the model from falling into trivial solutions (\eg, select all blocks). On the other hand, we allow this module to participate in the gradient calculation of the model to achieve end-to-end training. We introduce Gumbel Softmax~\cite{gumbel} and the Straight-through Estimator (STE)~\cite{bengio2013estimating} to deal with these two issues.

One popular method to make the routing probability $\tilde{v}$ learnable is the \textbf{Gumbel Softmax} technique~\cite{gumbel,channel}. Treating the trade-off of each block as a binary classification, the softened dynamic routing vector is $\tilde{v} \in \mathbb{R}^{n\times2}$. Formally,
\begin{equation}
      v_i = \frac{\exp\big (  ( \tilde{v}[i,1] + G[i,1] )/ \tau                   \big )}{ \sum^1_{j=0} \exp\big (  ( \tilde{v}[i,j] + G[i,j] )/ \tau                   \big )},
\end{equation}
where $i$ is the index of MVFB, $G \in \mathbb{R}^{n\times 2}$ is Gumbel noise following $Gumbel(0,1)$ distribution, and $\tau$ is a temperature parameter. We start at a very high temperature to ensure that all possible paths become candidates, and then the temperature is attenuated to a small value to approximate one-hot distribution. To encourage the sum of the routing vector $v$ to be small, we add the regularization term ($\frac{1}{n}\sum^{n}_{i=1}v_i$) to the final loss. However, we experimentally find that the model usually converges to an input-independent structure when temperature decreases. We conjecture that the control of the temperature parameter $\tau$ and the design of the regularization term require further study. 

Inspired by the research in low-bit width neural networks~\cite{dorefanet, hubara2017quantized}, we adopt STE for Bernoulli Sampling~(\textbf{STEBS}) to make the binary dynamic routing vector differentiable. An STE can be regarded as an operator that has arbitrary forward and backward operations. Formally,
\begin{equation}
\tilde{w}_i = \beta\times n\times\sigma(\tilde{v}_i) / \sum_i^n \sigma(\tilde{v}_i),
\label{normalize}
\end{equation}
\begin{equation}
\textbf{STE Forward}: v_i \sim Bernoulli(\tilde{w}_i),
\end{equation}
\begin{equation}
\textbf{STE Backward}: \frac{\partial o}{\partial \tilde{w}} = \frac{\partial o}{\partial v},
\end{equation}
where $\sigma$ is the Sigmoid function and we denote the objective function as $o$, $\beta$ is a hyper-parameter. We use the well-defined gradient $\frac{\partial o}{\partial v}$ as an approximation for $\frac{\partial o}{\partial \tilde{w}}$ to construct the backward pass. In formulation~(\ref{normalize}), we normalize the sample rate and use $\beta$ to control it. During training, $\beta$ is fixed at $0.5$. In the experiment section, we show that we can adjust $\beta$ to control the complexity of DMVFN.

\begin{table*}[th]
\centering
\caption{
% 
\textbf{Quantitative results on the DAVIS17-Val~\cite{davis} and Vimeo90K-Test~\cite{vimeo} benchmarks}. We denote DMVFN without routing as ``DMVFN~(w/o r)''. ``N/A'' means not available. The best results are highlighted in \textbf{bold}.}
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{rcccccccc}
\toprule
\multirow{3}{*}[-0.28em]{Method}& \multicolumn{5}{c}{UCF101-Train$\rightarrow$DAVIS17-Val} &  \multicolumn{3}{c}{UCF101-Train$\rightarrow$Vimeo90K-Test} \\
\cmidrule(l{7pt}r{7pt}){2-6} \cmidrule(l{7pt}r{7pt}){7-9}
& \multirow{2}{*}[-0.14em]{GFLOPs  $\downarrow$} & \multicolumn{2}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & 
\multicolumn{2}{c}{LPIPS ($\times10^{-2}$) $\downarrow$} &  \multirow{2}{*}[-0.14em]{GFLOPs  $\downarrow$}  & 
\multicolumn{1}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & 
\multicolumn{1}{c}{LPIPS ($\times10^{-2}$)  $\downarrow$} \\
&  &t+1   & t+3 & t+1   & t+3    &  & t+1  & t+1 \\
\midrule
\midrule
DVF~\cite{dvf}& 324.15 & 68.61 &55.47  & 23.23   & 34.22 & 89.64 & 92.11 & 7.73   \\
DYAN~\cite{dyan} & 130.12 & 78.96 &70.41 & 13.09  & 21.43 & N/A & N/A & N/A          \\
OPT~\cite{wu2022optimizing} & 165312.80 &  83.26 & 73.85& 11.40 & 18.21 & 45716.20 & 96.75 & 3.59          \\
\midrule

DMVFN~(w/o r) & 19.39 & \textbf{84.81} & \textbf{75.05} &\textbf{ 9.41} & \textbf{16.24 }& 5.36 & \textbf{97.24} & \textbf{3.30}  \\ 
DMVFN & \textbf{9.96} & 83.97 & 74.81 & 9.96& 17.28 & \textbf{2.77} & 97.01 & 3.69   \\ %298.pkl
\bottomrule
\end{tabular}
}
\label{table:davis-vimeo}
\end{table*}

\subsection{Implementation Details}
\label{sec:implement}

\noindent
\textbf{Loss function.} Our training loss ${L}_{total}$ is the sum of the reconstruction losses of outputs of each block ${I}^{i}_{t+1}$:
\begin{equation}\label{eq2}
     {L}_{total} = \sum_{i=1}^n \gamma^{n-i} d(\tilde{I}^{i}_{t+1}, I_{t+1}),
\end{equation}
\noindent where $d$ is the $l1$ loss calculated on the Laplacian pyramid representations~\cite{laplacian} construting from each pair of image. And we set $\gamma=0.8$ in our experiments following~\cite{raft}.

\begin{figure}[th]
	\centering
	\includegraphics[width=0.94\linewidth]{image/comparison.pdf}
	\caption{\textbf{Prediction comparison on KITTI.} The yellow line is aligned with the car in the ground truth. The results show that previous methods (DVF~\cite{dvf}, FVS~\cite{fvs}, and OPT~\cite{wu2022optimizing}) cannot accurately predict the car's location in the long-term prediction. The motion predicted by DMVFN is the most similar to the ground truth, while the errors of other methods grow larger with time. The fences predicted by DMVFN remain vertical when moving.}
	\label{fig:kitti}
\end{figure}

\paragraph{Training strategy.} Our DMVFN is trained on $224\times224$ image patches. The batch size is set as $64$. 
%
We employ the AdamW optimizer~\cite{adam,loshchilov2018fixing} with a weight decay of $10^{-4}$. 
%
We use a cosine annealing strategy to reduce the learning rate from $10^{-4}$ to $10^{-5}$.
%
Our model is trained on four 2080Ti GPUs for $300$ epochs.
The training process takes about $35$ hours.
%





