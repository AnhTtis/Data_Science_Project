\section{Experiments}
% 

%
\begin{figure*}[th]
	\centering
	\includegraphics[width=0.98\linewidth]{image/abl.pdf}
	\caption{\textbf{$(a)$: Average usage rate on videos with different motion magnitudes}. ``Fast'': tested on Vimeo-Fast. ``Medium'': tested on Vimeo-Medium. ``Slow'': tested on Vimeo-Slow.
    \textbf{$(b)$: Difference between ``Fast''/``Slow'' and ``Medium'' of $(a)$.}
 \textbf{$(c)$: Averaged usage rate on different time intervals between two input frames from Vimeo-Slow}. ``Int.'': time interval.}
	\label{fig:ablation1}
\end{figure*}


 \begin{figure}[th]
 	\centering
 	\includegraphics[width=0.86\linewidth]{image/comparison3.png}
 	\caption{\textbf{Visual effect comparison in the Vimeo-Test~\cite{vimeo} dataset.} our DMVFN faithfully reproduces the motion of the hand and the head with less distortion and artifacts.}
 	\label{fig:Vimeo}
  %
 \end{figure}



\subsection{Dataset and Metric}
\label{sec:dataset}
%
\paragraph{Dataset.} We use several datasets in the experiments:

\vspace{0.4em}
\noindent \textbf{Cityscapes} dataset~\cite{cityscapes} contains 3,475 driving videos with resolution of $2048 \times 1024$. We use 2,945 videos for training (Cityscapes-Train) and 500 videos in Cityscapes dataset~\cite{cityscapes} for testing (Cityscapes-Test). 

\vspace{0.4em}
\noindent \textbf{KITTI} dataset~\cite{kitti} contains 28 driving videos with resolution of $375\times 1242$. 24 videos in KITTI dataset are used for training (KITTI-Train) and the remaining four videos in KITTI dataset are used for testing (KITTI-Test). 

% 

\vspace{0.4em}
\noindent \textbf{UCF101}~\cite{ucf101} dataset contains $13,320$ videos under 101 different action categories with resolution of $240\times 320$. We only use the training subset of UCF101~\cite{ucf101}.

\vspace{0.4em}
\noindent \textbf{Vimeo90K}~\cite{vimeo} dataset has $51,312$ triplets for training, where each triplet contains three consecutive video frames with resolution of $256\times448$.
There are $3,782$ triplets in the Vimeo90K testing set. We denote the training and testing subsets as Vimeo-Train and Vimeo-Test, respectively.

\vspace{0.4em}
\noindent
\textbf{DAVIS17~\cite{davis}} has videos with resolution around $854\times 480$. We use the DAVIS17-Val set containing 30 videos.

%
\noindent
\textbf{Configurations.} We have four experimental configurations following previous works~\cite{dyan,dvf,wu2022optimizing}:

% 

\begin{itemize}
\itemsep-0.03in
\item Cityscapes-Train$\rightarrow$Cityscapes-Test
\item KITTI-Train$\rightarrow$KITTI-Test
\item UCF101$\rightarrow$DAVIS17-Val
\item UCF101$\rightarrow$Vimeo-Test
\end{itemize}
%
%
\noindent where the left and right sides of the arrow represent the training set and the test set, respectively. For a fair comparison with other methods that are not tailored for high resolution, we follow the setting in~\cite{fvs}. We resize images in Cityscapes~\cite{cityscapes} to $1024\times 512$ and images in KITTI~\cite{kitti} to $256\times 832$. During inference of Cityscapes~\cite{cityscapes} and KITTI~\cite{kitti}, we predict the next five frames. We predict the next three frames for 
DAVIS17-Val~\cite{davis} and next one frame for Vimeo-Test~\cite{vimeo}. Note that OPT~\cite{wu2022optimizing} is an optimization-based approach and uses pre-trained RAFT~\cite{raft} and RIFE~\cite{rife} models. RIFE~\cite{rife} and RAFT~\cite{raft} are trained on the Vimeo-Train~\cite{vimeo} and Flying Chairs dataset~\cite{flownet}.

\noindent
\textbf{Evaluation metric}. Following previous works~\cite{wu2022optimizing}, We use Multi-Scale Structural Similarity Index Measure (MS-SSIM)~\cite{msssim} and a perceptual metric LPIPS~\cite{lpips} for quantitative evaluation. To measure the model complexity, we calculate the GFLOPs.
%
%

\subsection{Comparison to State-of-the-Arts}
\label{sec:comparison}
We compare our DMVFN and MVFN with state-of-the-art video prediction methods. 
%
These methods fall into two categories: those that require only RGB images as input (\eg PredNet~\cite{prednet}, MCNET~\cite{mcnet}, DVF~\cite{dvf}, CorrWise~\cite{corrwise}, OPT~\cite{wu2022optimizing}) and those that require additional information as input (\eg Vid2vid~\cite{vid2vid}, Seg2vid~\cite{seg2vid}, FVS~\cite{fvs}, SADM~\cite{sadm}). 
% 

\noindent
\textbf{Quantitative results.} The quantitative results are reported in Table~\ref{table:cityscapes} and Table~\ref{table:davis-vimeo}. When calculating the GFLOPs of OPT~\cite{wu2022optimizing}, the number of iterations is set to $3000$. On MS-SSIM and LPIPS, our DMVFN achieves much better results than other methods in both short-term and long-term video prediction. The GFLOPs of our DMVFN is considerably smaller. The results show the routing strategy reduces half the GFLOPs amounts while maintaining comparable performance. Because the decrease of GFLOPs is not strictly linear with the actual latency~\cite{regnet}, we measure the running speed on TITAN 2080Ti. For predicting a 720p frame, DVF~\cite{dvf} spends $0.130$s on average. And the runtime of our DMVFN is $0.023$s.

\begin{table}[!htb]
  \vspace{-2mm}
  \centering
  \caption{Comparison between DMVFN and STRPM.%
  }
  \vspace{-1mm}
  \label{tab:ucf}
   \resizebox{0.98\linewidth}{!}{ \begin{tabular}{lcccc}
    \toprule
    \multirow{3}{*}{Method}&\multicolumn{2}{c}{UCF Sports}&\multicolumn{2}{c}{Human3.6M}\cr
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    &$t+1$&$t+6$&$t+1$&$t+4$\cr
    \cmidrule(lr){2-2} \cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}
    &PSNR$\uparrow$ / LPIPS$\downarrow$&PSNR$\uparrow$ / LPIPS$\downarrow$&PSNR$\uparrow$ / LPIPS$\downarrow$&PSNR$\uparrow$ / LPIPS$\downarrow$\cr
    \midrule
    STRPM                                                   &28.54 / 20.69  &20.59 / 41.11    &33.32 / 9.74  &29.01 / 10.44\cr
    DMVFN & \textbf{30.05} / \textbf{10.24}  &\textbf{22.67} / \textbf{22.50}  &\textbf{35.07} / \textbf{7.48}  &\textbf{29.56} / \textbf{9.74}\cr
    \bottomrule
    \end{tabular}}
    \vspace{-1.5em}
\end{table}

\noindent
\textbf{More comparison.} 
The quantitative results compared with STRPM~\cite{strpm} is reported in Table~\ref{tab:ucf}. We train DMVFN in UCFSports and Human3.6M datasets following the setting in~\cite{strpm}.
% 
We also measure the average running speed on TITAN 2080Ti. For predicting a $1024\times 1024$ frame, DMVFN is 4.06$\times$ faster than STRPM~\cite{strpm}.


\noindent
\textbf{Qualitative results.} on different datasets are shown in Figure~\ref{fig:davis17}, Figure~\ref{fig:kitti} and Figure~\ref{fig:Vimeo}.
%
As we can see, the frames predicted by our DMVFN have better temporal continuity and are more consistent with the ground truth than other methods. our DMVFN is able to predict correct motion while preserving object shape and texture information.

\subsection{Ablation Study}
\label{sec:ablation}
Here, we perform extensive ablation studies to further study the effectiveness of components in our DMVFN.
%
The experiments are performed on the Cityscapes~\cite{cityscapes} and KITTI~\cite{kitti} datasets unless otherwise specified. %


\noindent
\textbf{1) How effective is the proposed Routing Module?} To verify that our DMVFN can perceive motion scales and adaptively choose the proper sub-networks. As suggested in~\cite{xiang2020zooming,TMNet2021} We divide the Vimeo-90K~\cite{vimeo} testing set into three subsets: Vimeo-Fast, Vimeo-Medium, and Vimeo-Slow, which correspond to the motion range. We retrain our DMVFN on the Vimeo-Train~\cite{vimeo} using the same training strategy in~\S\ref{sec:implement}. We calculate the averaged usage rate for each MVFB on these three testing subsets. From Figure~\ref{fig:ablation1}~$(a)$ and Figure~\ref{fig:ablation1}~$(b)$, we observe that, for two frames with large motion, our DMVFN prefers to select blocks with large scale (\eg 4x). There are two blocks with significantly smaller probability of selection. We believe this reflects the inductive bias of our DMVFN on different combinations of scaling factors. %

To further verify that our DMVFN also perceives the size of the time interval, we test the trained our DMVFN on the same video, but the time intervals between the two input frames are different. We choose Vimeo-Slow as the test video, and the input time intervals are $1$, $3$, and $5$. The results are shown in Figure~\ref{fig:ablation1}~$(c)$. We observe that our DMVFN uses large-scale blocks when facing long-interval inputs, and prefers small-scale blocks when facing short-interval inputs. This verifies that our DMVFN can perceive temporal information and dynamically select different sub-networks to process the corresponding input frames.

To further study how the blocks are selected, we select 103 video sequences (consisting of a highly-speed moving car and a relatively static background) from the KITTI dataset, and we denote it as KITTI-A. As shown in Table~\ref{tab:ablation1_1}, on the KITTI-A dataset, DMVFN prefers to choose blocks with large scale to capture large movements. Our insight is that flow estimation for static backgrounds is straightforward, and the large motion dominates the choice of DMVFN. 

\begin{table}[!htb]
\vspace{-2mm}
  \centering
  \caption{Average usage rate ($10^{-2}$) of DMVFN.}
  \label{tab:ablation1_1}
   \resizebox{0.98\linewidth}{!}{ \begin{tabular}{lccccccccc}
    \toprule
    Scale & $4$ & $4$& $4$& $2$ & $2$ & $2$ & $1$ & $1$ & $1$ \cr
    \midrule                           
    KITTI-A & $80.95$ &$34.22$ & $26.70$ & $81.19$ & $73.91$ & $44.90$ & $55.34$ & $0.49$& $0$\cr 
    \bottomrule
    \end{tabular}}
\end{table}

\begin{table}[th]
\caption{\textbf{Routing Module based on STEBS is effective}. The evaluation metric is MS-SSIM ($\times10^{-2}$).}

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}[-0.28em]{Setting} & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{KITTI}\\
\cmidrule(l{7pt}r{7pt}){2-4} \cmidrule(l{7pt}r{7pt}){5-7} 
%
& t+1 & t+3 & t+5 & t+1  & t+3 & t+5 \\ 
\midrule
\midrule
copy & 76.95 & 68.82 & 64.45 & 58.31 & 48.99 & 44.16 \\
w/o routing  & 95.29 & 87.91& 81.48 & 88.06 & 76.53 & 68.29 \\
Random & 91.97 & 82.11 & 70.05 & 81.31 & 69.89 & 62.42 \\
Gumbel Softmax & 95.05 & 87.57 & 79.54 & 87.42 & 75.56& 65.83   \\%
STEBS & \textbf{95.73}  & \textbf{89.24} & \textbf{83.45} &  \textbf{88.53} & \textbf{78.01}& \textbf{70.52} \\ %
\bottomrule
\end{tabular}
}
%
\label{table:gumbel}
\end{table}

\noindent
\textbf{2) How to design the differentiable Routing Module?} An off-the-shelf technique is to process the routing probability $p$ with Gumbel Softmax. The comparison results between DMVFNs with different differentiable routing methods are shown in Table~\ref{table:gumbel}. DMVFN with STEBS outperforms DMVFN with Gumbel Softmax on MS-SSIM, especially for long-term prediction. The DMVFN with Gumbel Softmax usually degenerates to a fixed structure. We also compared the random selection of each block with probability $0.5$~(denoted as ``random'') and the original MVFN.


\begin{table}[th]
\caption{\textbf{Results of DMVFN with different scaling factor settings}. The evaluation metric is MS-SSIM ($\times10^{-2}$).}

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}[-0.28em]{Settings} & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{KITTI}\\
\cmidrule(l{7pt}r{7pt}){2-4} \cmidrule(l{7pt}r{7pt}){5-7} 
%
  & t+1 & t+3 & t+5  & t+1  & t+3 & t+5  \\ 
\midrule
\midrule
DMVFN [1] & 94.70 & 87.26 & 80.93 & 87.64 & 76.71 & 68.76 \\
DMVFN [2, 1] & 95.30 & 87.93 & 82.02 & 87.97 & 77.23 & 69.58 \\
DMVFN [4, 2, 1] & \textbf{95.73}  & \textbf{89.24} & \textbf{83.45} & \textbf{88.53} & \textbf{78.01} & \textbf{70.52} \\

\bottomrule
\end{tabular}
}
\label{table:blocks}
\end{table}

\noindent
\textbf{3) How to set the scaling factors in our DMVFN?} We evaluate DMVFN with different scaling factors. We experiment three descending factor sequences, including [1,1,1,1,1,1,1,1,1], [2,2,2,2,2,1,1,1,1] and [4,4,4,2,2,2,1,1,1]. They are denoted as ``[1]", ``[2,1]" and ``[4,2,1]". 
%
The results are listed in Table~\ref{table:blocks}. DMVFN~[4,2,1] performs better than DMVFN~[2,1] and DMVFN~[1]. The gap is more obvious for long-term future frames. 

\begin{table}[th]
\caption{\textbf{Spatial path is effective in DMVFN}. The evaluation metric is MS-SSIM ($\times10^{-2}$).}

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}[-0.28em]{Settings} & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{KITTI}\\

\cmidrule(l{7pt}r{7pt}){2-4} \cmidrule(l{7pt}r{7pt}){5-7} 

%
  & t+1 & t+3 & t+5  & t+1  & t+3 & t+5  \\ 
\midrule
\midrule
w/o r, w/o path & 94.99 & 87.59 & 80.98 & 87.75 & 76.22 & 67.86 \\
w/o r & 95.29 & 87.91 & 81.48 & 88.06 & 76.53 & 68.29 \\
w/o path &  95.55 & 88.89 & 83.03 & 88.29 & 77.53 & 69.86 \\%
DMVFN & \textbf{95.73}  & \textbf{89.24} & \textbf{83.45} & \textbf{88.53} & \textbf{78.01} & \textbf{70.52} \\

\bottomrule
\end{tabular}
}
\label{table:path}
%
\end{table}

\noindent
\textbf{4) How effective is the spatial path?} To verify the effectiveness of the spatial path, we ablate it for DMVFN. The results listed in Table~\ref{table:path} show the advantages of the spatial path with or without routing.

%
