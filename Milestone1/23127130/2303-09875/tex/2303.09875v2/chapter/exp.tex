\section{Experiments}
% 

%
\begin{figure*}[th]
	\centering
	\includegraphics[width=0.98\linewidth]{image/abl.pdf}
	\caption{\textbf{$(a)$: Average usage rate on videos with different motion magnitudes}. ``Fast'': tested on Vimeo-Fast. ``Medium'': tested on Vimeo-Medium. ``Slow'': tested on Vimeo-Slow.
    \textbf{$(b)$: Difference between ``Fast''/``Slow'' and ``Medium'' of $(a)$.}
 \textbf{$(c)$: Averaged usage rate on different time intervals between two input frames from Vimeo-Slow}. ``Int.'': time interval.}
	\label{fig:ablation1}
\end{figure*}


 \begin{figure}[th]
 	\centering
 	\includegraphics[width=0.86\linewidth]{image/comparison3.png}
 	\caption{\textbf{Visual effect comparison in the Vimeo-Test~\cite{vimeo} dataset.} our DMVFN faithfully reproduces the motion of the hand and the head with less distortion and artifacts.}
 	\label{fig:Vimeo}
    \vspace{-1em}
  % 
 \end{figure}



\subsection{Dataset and Metric}
\label{sec:dataset}
%
\paragraph{Dataset.} We use several datasets in the experiments:

\vspace{0.4em}
\noindent \textbf{Cityscapes} dataset~\cite{cityscapes} contains 3,475 driving videos with resolution of $2048 \times 1024$. We use 2,945 videos for training (Cityscapes-Train) and 500 videos in Cityscapes dataset~\cite{cityscapes} for testing (Cityscapes-Test).

\vspace{0.4em}
\noindent \textbf{KITTI} dataset~\cite{kitti} contains 28 driving videos with resolution of $375\times 1242$. 24 videos in KITTI dataset are used for training (KITTI-Train) and the remaining four videos in KITTI dataset are used for testing (KITTI-Test).

\vspace{0.4em}
\noindent \textbf{UCF101}~\cite{ucf101} dataset contains $13,320$ videos under 101 different action categories with resolution of $240\times 320$. We only use the training subset of UCF101~\cite{ucf101}.

\vspace{0.4em}
\noindent \textbf{Vimeo90K}~\cite{vimeo} dataset has $51,312$ triplets for training, where each triplet contains three consecutive video frames with resolution of $256\times448$.
There are $3,782$ triplets in the Vimeo90K testing set. We denote the training and testing subsets as Vimeo-Train and Vimeo-Test, respectively.

\vspace{0.4em}
\noindent
\textbf{DAVIS17~\cite{davis}} has videos with resolution around $854\times 480$. We use the DAVIS17-Val containing 30 videos as test set.

%
\noindent
\textbf{Configurations.} We have four experimental configurations following previous works~\cite{dyan,dvf,wu2022optimizing}:

% 

\begin{itemize}
\itemsep-0.03in
\item Cityscapes-Train$\rightarrow$Cityscapes-Test
\item KITTI-Train$\rightarrow$KITTI-Test
\item UCF101$\rightarrow$DAVIS17-Val
\item UCF101$\rightarrow$Vimeo-Test
\end{itemize}
Here, the left and right sides of the arrow represent the training set and the test set, respectively. For a fair comparison with other methods that are not tailored for high resolution videos, we follow the setting in~\cite{fvs} and resize the images in Cityscapes~\cite{cityscapes} to $1024\times 512$ and images in KITTI~\cite{kitti} to $256\times 832$, respectively. During inference of Cityscapes~\cite{cityscapes} and KITTI~\cite{kitti}, we predict the next five frames. We predict the next three frames for DAVIS17-Val~\cite{davis} and next one frame for Vimeo-Test~\cite{vimeo}, respectively. Note that OPT~\cite{wu2022optimizing} is an optimization-based approach and uses pre-trained RAFT~\cite{raft} and RIFE~\cite{rife} models. RIFE~\cite{rife} and RAFT~\cite{raft} are trained on the Vimeo-Train dataset~\cite{vimeo} and the Flying Chairs dataset~\cite{flownet}, respectively.

\noindent
\textbf{Evaluation metrics}. Following previous works~\cite{wu2022optimizing}, we use Multi-Scale Structural Similarity Index Measure (MS-SSIM)~\cite{msssim} and a perceptual metric LPIPS~\cite{lpips} for quantitative evaluation. To measure the model complexity, we calculate the GFLOPs.

\subsection{Comparison to State-of-the-Arts}
\label{sec:comparison}
We compare our DMVFN with state-of-the-art video prediction methods. These methods fall into two categories: the methods requiring only RGB images as input (\eg, PredNet~\cite{prednet}, MCNET~\cite{mcnet}, DVF~\cite{dvf}, CorrWise~\cite{corrwise}, OPT~\cite{wu2022optimizing}) and the methods requiring extra information as input (\eg, Vid2vid~\cite{vid2vid}, Seg2vid~\cite{seg2vid}, FVS~\cite{fvs}, SADM~\cite{sadm}). 
% 

\noindent
\textbf{Quantitative results}. The quantitative results are reported in Table~\ref{table:cityscapes} and Table~\ref{table:davis-vimeo}. When calculating the GFLOPs of OPT~\cite{wu2022optimizing}, the number of iterations is set as $3,000$. In terms of MS-SSIM and LPIPS, our DMVFN achieves much better results than the other methods in both short-term and long-term video prediction tasks. The GFLOPs of our DMVFN is considerably smaller than the comparison methods. These results show the proposed routing strategy reduces almost half the number of GFLOPs while maintaining comparable performance. Because the decrease of GFLOPs is not strictly linear with the actual latency~\cite{regnet}, we measure the running speed on TITAN 2080Ti. For predicting a 720P frame, DVF~\cite{dvf} spends $0.130$s on average, while our DMVFN only needs $0.023$s on average.

\begin{table}[!htb]
  \vspace{-2mm}
  \centering
  \caption{\textbf{Comparison between DMVFN and STRPM.%
  }}
  \vspace{-1mm}
  \label{tab:ucf}
   \resizebox{0.98\linewidth}{!}{ \begin{tabular}{lcccc}
    \toprule
    \multirow{3}{*}{Method}&\multicolumn{2}{c}{UCF Sports}&\multicolumn{2}{c}{Human3.6M}\cr
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    &$t+1$&$t+6$&$t+1$&$t+4$\cr
    \cmidrule(lr){2-2} \cmidrule(lr){3-3}\cmidrule(lr){4-4} \cmidrule(lr){5-5}
    &PSNR$\uparrow$ / LPIPS$\downarrow$&PSNR$\uparrow$ / LPIPS$\downarrow$&PSNR$\uparrow$ / LPIPS$\downarrow$&PSNR$\uparrow$ / LPIPS$\downarrow$\cr
    \midrule
    STRPM                                                   &28.54 / 20.69  &20.59 / 41.11    &33.32 / 9.74  &29.01 / 10.44\cr
    DMVFN & \textbf{30.05} / \textbf{10.24}  &\textbf{22.67} / \textbf{22.50}  &\textbf{35.07} / \textbf{7.48}  &\textbf{29.56} / \textbf{9.74}\cr
    \bottomrule
    \end{tabular}}
    \vspace{-0.2em}
\end{table}

\noindent
\textbf{More comparison}. The quantitative results compared with STRPM~\cite{strpm} are reported in Table~\ref{tab:ucf}. We train our DMVFN in UCFSports and Human3.6M datasets following the setting in~\cite{strpm}. We also measure the average running speed on TITAN 2080Ti. To predict a $1024\times 1024$ frame, our DMVFN is averagely 4.06$\times$ faster than STRPM~\cite{strpm}.


\noindent
\textbf{Qualitative results} on different datasets are shown in Figure~\ref{fig:davis17}, Figure~\ref{fig:kitti} and Figure~\ref{fig:Vimeo}. As we can see, the frames predicted by our DMVFN exhibit better temporal continuity and are more consistent with the ground truth than those by the other methods. Our DMVFN is able to predict correct motion while preserving the shape and texture of objects.

\subsection{Ablation Study}
\label{sec:ablation}
Here, we perform extensive ablation studies to further study the effectiveness of components in our DMVFN. The experiments are performed on the Cityscapes~\cite{cityscapes} and KITTI~\cite{kitti} datasets unless otherwise specified.


\noindent
\textbf{1) How effective is the proposed Routing Module}? As suggested in~\cite{xiang2020zooming,TMNet2021}, we divide the Vimeo-90K~\cite{vimeo} test set into three subsets: Vimeo-Fast, Vimeo-Medium, and Vimeo-Slow, which correspond to the motion range. To verify that our DMVFN can perceive motion scales and adaptively choose the proper sub-networks, we retrain our DMVFN on the Vimeo-Train~\cite{vimeo} using the same training strategy in~\S\ref{sec:implement}. We calculate the averaged usage rate of each MVFB on three test subsets. From Figures~\ref{fig:ablation1}~$(a)$ and \ref{fig:ablation1}~$(b)$, we observe that our DMVFN prefers to select MVFBs with large scale (\eg, 4x) for two frames with large motion. There are two MVFBs with clearly smaller selection probability. We believe this reflects the inductive bias of our DMVFN on different combinations of scaling factors.

To further verify that our DMVFN also perceives the size of the time interval, we test our DMVFN on the two frames with different time intervals (but still in the same video). We choose Vimeo-Slow as the test set, and set the time intervals as $1$, $3$, and $5$. The results are shown in Figure~\ref{fig:ablation1}~$(c)$. We observe that our DMVFN prefers large-scale blocks on long-interval inputs, and small-scale blocks on short-interval inputs. This verifies that our DMVFN can perceive temporal information and dynamically select different sub-networks to handle the input frames with different time intervals.

To further study how the MVFBs are selected, we select 103 video sequences (contain a high-speed moving car and a relatively static background) from the KITTI dataset, denoted as KITTI-A. As shown in Table~\ref{tab:ablation1_1}, on the KITTI-A dataset, our DMVFN prefers to choose MVFBs with large scaling factors to capture large movements. The flow estimation for static backgrounds is straightforward, while the large motion dominates the choice of our DMVFN.

\begin{table}[!htb]
\vspace{-2mm}
  \centering
  \caption{\textbf{Average usage rate ($10^{-2}$) of MVFBs in our DMVFN.}}
  \label{tab:ablation1_1}
   \resizebox{0.98\linewidth}{!}{ \begin{tabular}{lccccccccc}
    \toprule
    Scale & $4$ & $4$& $4$& $2$ & $2$ & $2$ & $1$ & $1$ & $1$ \cr
    \midrule                           
    KITTI-A & $80.95$ &$34.22$ & $26.70$ & $81.19$ & $73.91$ & $44.90$ & $55.34$ & $0.49$& $0$\cr 
    \bottomrule
    \end{tabular}}
\end{table}

\begin{table}[th]
\caption{\textbf{Routing Module based on STEBS is effective}. The evaluation metric is MS-SSIM ($\times10^{-2}$).}

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}[-0.28em]{Setting} & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{KITTI}\\
\cmidrule(l{7pt}r{7pt}){2-4} \cmidrule(l{7pt}r{7pt}){5-7} 
%
& $t+1$ & $t+3$ & $t+5$ & $t+1$  & $t+3$ & $t+5$ \\ 
\midrule
\midrule
Copy last frame & 76.95 & 68.82 & 64.45 & 58.31 & 48.99 & 44.16 \\
w/o routing  & 95.29 & 87.91& 81.48 & 88.06 & 76.53 & 68.29 \\
Random & 91.97 & 82.11 & 70.05 & 81.31 & 69.89 & 62.42 \\
Gumbel Softmax & 95.05 & 87.57 & 79.54 & 87.42 & 75.56& 65.83   \\%
STEBS & \textbf{95.73}  & \textbf{89.24} & \textbf{83.45} &  \textbf{88.53} & \textbf{78.01}& \textbf{70.52} \\ %
\bottomrule
\end{tabular}
}
%
\label{table:gumbel}
\end{table}

\noindent
\textbf{2) How to design the Routing Module}? A trivial solution is to process the routing probability $p$ with Gumbel Softmax. The comparison results of our DMVFNs with different differentiable routing methods are summarized in Table~\ref{table:gumbel}. Our DMVFN with STEBS outperforms the DMVFN variant with Gumbel Softmax on MS-SSIM, especially for long-term prediction. The DMVFN variant with Gumbel Softmax usually degenerates to a fixed and static structure. We also compare with the DMVFN randomly selecting each MVFB with probability $0.5$~(denoted as ``Random'') and that without routing module (denoted as ``w/o routing'').

\begin{table}[th]
\caption{\textbf{Results of our DMVFN with different scaling factor settings}. The evaluation metric is MS-SSIM ($\times10^{-2}$).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
Setting & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{KITTI}\\
\cmidrule(l{7pt}r{7pt}){2-4} \cmidrule(l{7pt}r{7pt}){5-7}
in DMVFN  & $t+1$ & $t+3$ & $t+5$  & $t+1$  & $t+3$ & $t+5$  \\ 
\midrule
\midrule
 $[1]$ & 94.70 & 87.26 & 80.93 & 87.64 & 76.71 & 68.76 \\
 $[2,1]$ & 95.30 & 87.93 & 82.02 & 87.97 & 77.23 & 69.58 \\
 $[4,2,1]$ & \textbf{95.73}  & \textbf{89.24} & \textbf{83.45} & \textbf{88.53} & \textbf{78.01} & \textbf{70.52} \\

\bottomrule
\end{tabular}
}
\label{table:blocks}
\end{table}


% descending
\noindent
\textbf{3) How to set the scaling factors}?\ We evaluate our DMVFN with different scaling factors.\ We use three non-increasing factor sequences of ``$[1,1,1,1,1,1,1,1,1]$'', ``$[2,2,2,2,2,1,1,1,1]$'' and ``$[4,4,4,2,2,2,1,1,1]$'', denoted as ``$[1]$", ``$[2,1]$" and ``$[4,2,1]$", respectively. The results are listed in Table~\ref{table:blocks}. Our DMVFN with ``$[4,2,1]$'' performs better than that with ``$[2,1]$'' and ``$[1]$''. The gap is more obvious on longer-term future frames.

\begin{table}[th]
\caption{\textbf{Spatial path is effective in our DMVFN}. The evaluation metric is MS-SSIM ($\times10^{-2}$).}

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}[-0.28em]{Setting} & \multicolumn{3}{c}{Cityscapes} & \multicolumn{3}{c}{KITTI}\\
\cmidrule(l{7pt}r{7pt}){2-4} \cmidrule(l{7pt}r{7pt}){5-7} & $t+1$ & $t+3$ & $t+5$  & $t+1$  & $t+3$ & $t+5$  \\ 
\midrule
\midrule
w/o r, w/o path & 94.99 & 87.59 & 80.98 & 87.75 & 76.22 & 67.86 \\
w/o r & 95.29 & 87.91 & 81.48 & 88.06 & 76.53 & 68.29 \\
w/o path &  95.55 & 88.89 & 83.03 & 88.29 & 77.53 & 69.86 \\%
DMVFN & \textbf{95.73}  & \textbf{89.24} & \textbf{83.45} & \textbf{88.53} & \textbf{78.01} & \textbf{70.52} \\

\bottomrule
\end{tabular}
}
\label{table:path}
%
\end{table}

\noindent
\textbf{4) How effective is the spatial path}? To verify the effectiveness of the spatial path in our DMVFN, we compare it with the DMVFN without spatial path (denoted as ``w/o path''). The results listed in Table~\ref{table:path} show our DMVFN enjoys better performance with the spatial path, no matter with or without the routing module (denoted as ``w/o r'').

%
