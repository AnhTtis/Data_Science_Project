


\section{Methodology}
%
%

\subsection{Background}
\label{sec:background}

\paragraph{Video prediction.}
%
Given a sequence of past $t$ frames $\{I_{i}\in \mathbb{R}^{h\times w\times 3}|i=1,...,t\}$, video prediction aims to predict the future frames $\{\tilde{I}_{t+1}, \tilde{I}_{t+2}, \tilde{I}_{t+3} ,\ ...\}$. The inputs of our video prediction model are only the two consecutive frames $I_{t-1}$ and $I_t$. We concentrate on predicting $\tilde{I}_{t+1}$, and iteratively predict future frames $\{\tilde{I}_{t+2}, \tilde{I}_{t+3} ,\ ...\}$ in a similar manner. Denote the video prediction model as $G_{\theta}(I_{t-1},I_{t})$, where $\theta$ is the set of model parameters to be learned, the learning objective is to minimize the difference between $\tilde{I}_{t+1}=G_{\theta}(I_{t-1},I_{t})$ and the ``ground truth'' $I_{t+1}$.

\paragraph{Voxel flow.} Considering the local consistency in space-time, the pixels of a generated future frame come from nearby regions of the previous frames~\cite{zhou2016view,vimeo}. In video prediction task, researchers estimate optical flow $\textbf{f}_{t+1 \rightarrow t}$ from $I_{t+1}$ to $I_t$~\cite{dvf}. And the corresponding frame is obtained using the pixel-wise backward warping~\cite{backwarp}~(denoted as $\backwardwarp$). In addition, to deal with the occlusion, some methods~\cite{superslomo,dvf} further introduce a fusion map $\textbf{m}$ to fuse the pixels of $I_{t}$ and $I_{t-1}$. The final predicted frame is obtained by the following formulation~(Figure~\ref{fig:predict} $(a)$):
\begin{equation}
\hat{I}_{t+1\leftarrow{t-1}} = \backwardwarp( I_{t-1},   \textbf{f}_{ t+1 \rightarrow t-1} ),
\end{equation}
\begin{equation}
     \hat{I}_{t+1 \leftarrow{t}} = \backwardwarp( I_t,  \textbf{f}_{t+1 \rightarrow t} ),
\end{equation}
\begin{equation}
     \tilde{I}_{t+1} = \hat{I}_{t+1\leftarrow{t-1}} \times \textbf{m} + \hat{I}_{t+1\leftarrow{t}} \times (1-\textbf{m}).
\end{equation}
Here, $\hat{I}_{t+1 \leftarrow{t}}$ and $\hat{I}_{t+1 \leftarrow{t-1}}$ are intermediate warped images. To simplify notations, we refer to the optical flows $\textbf{f}_{t+1 \rightarrow t},\textbf{f}_{t+1 \rightarrow t-1}$ and the fusion map $\textbf{m}$ collectively as the voxel flow $\textbf{F}_{t+1}$, similar to the notations in~\cite{dvf}. The above equations can be simplified to the following form:
\begin{equation}
     \tilde{I}_{t+1} = \backwardwarp( I_{t-1}, I_t,  \textbf{F}_{t+1} ).
\end{equation}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=0.95\linewidth]{image/comparison2.png}
	\caption{\textbf{Visual comparison of ($t+1$)-th frame predicted from $t$-th and ($t-1$)-th frames on the DAVIS17-Val~\cite{davis}}.}
	\label{fig:davis17}
 \vspace{-1em}
\end{figure*}

\subsection{Dynamic Multi-Scale Voxel Flow Network}
\label{sec:dmvfn}

\paragraph{MVFB.} To estimate the voxel flow, DVF~\cite{dvf} assumes that all optical flows are locally linear and temporally symmetric around the targeted time, which may be unreasonable for large-scale motions. To address the object position changing issue~\cite{rife} in adjacent frames, OPT~\cite{wu2022optimizing} uses flow reversal layer~\cite{qvi} to convert forward flows to backward flows. We aim to estimate voxel flow end-to-end without introducing new components and unreasonable constraints.

We denote the $i$-th MVFB as $f^i_{MVFB}(\cdot)$. It learns to approximate target voxel flow $\textbf{F}^{i}_{t+1}$ by
taking two frames $I_{t-1}$ and $I_t$, the synthesized frame $\tilde{I}^{i-1}_{t+1}$, and the voxel flow estimated by previous blocks $\textbf{F}^{i-1}_{t+1}$ as inputs.

The architecture of our MVFB is shown in Figure~\ref{fig:predict} $(c)$. To capture the large motion while retaining the original spatial information, we construct a two-branch network structure~\cite{yu2018bisenet}. This design inherits from pyramidal optical flow estimation~\cite{pwcnet,spynet}. In the \textbf{motion path}, the input is down-sampled by a scaling factor $S^i$ to facilitate the expansion of the receptive field. Another \textbf{spatial path} operates at high resolution to complement the spatial information. We denote $\tilde{I}^{i}_{t+1} $ as the output of the $i$-th MVFB. Formally,
\begin{equation}
\tilde{I}^{i}_{t+1}, \textbf{F}^i_{t+1} = f_{\mathrm{MVFB}}^i (I_{t-1}, I_t, \tilde{I}^{i-1}_{t+1}, \textbf{F}^{i-1}_{t+1}, S^i).
\end{equation}
The initial values of $\tilde{I}_{t+1}^{0}$ and $\textbf{F}^{0}_{t+1}$ are set to zero. As illustrated in Figure~\ref{fig:predict}~$(b)$, our DMVFN contains $9$ MVFBs. To generate a future frame, we iteratively refine a voxel flow~\cite{dvf} and fuse the pixels of the input frames. 

Many optical flow estimation methods predict the flow field on a small image, and then refine it on a large image~\cite{pwcnet,gmflow}. For simplicity and intuition, we consider decreasing scaling factor sequences. Finally, the scaling factors is experimentally set as $[4,4,4,2,2,2,1,1,1]$.

% 
\iffalse 
Formally, 
\begin{equation}
     \textbf{x}^i = Concat(I_{t-1}^{i-1}, I_t^{i-1}, \tilde{I}^{i-1}_{t+1}, \textbf{F}^{i-1}_{t+1}),
\end{equation}
\begin{equation}
     \textbf{m}^i = Convs_1(Resize(\textbf{x}^i, 1/S^i)),
\end{equation}
\begin{equation}
     \textbf{f}^i = Concat(Resize(\textbf{m}^i, 2S^i), Convs_2(\textbf{x}^i),
\end{equation}
\begin{equation}
     \textbf{F}^i_{t+1} = TransposedConv(\textbf{f}^i)+\textbf{F}^{i-1}_{t+1},
\end{equation}
\begin{equation}
     \tilde{I}^{i}_{t+1} = \backwardwarp(I_{t-1}, I_{t}, \textbf{F}_{t+1}^i),
\end{equation}
%
where we denote the two-way convolution operations as $Convs_1$ and $Convs_2$. 
\fi




\paragraph{DMVFN.} Different pairs of adjacent frames have diverse motion scales and different computational demands. An intuitive idea is to adaptively select dynamic architectures conditioned on each input. We then perform dynamic routing within the~\textbf{super network} (the whole architecture)~\cite{dynamicnn}, including multiple possible paths. DMVFN saves redundant computation for samples with small-scale motion and preserves the representation ability for large-scale motion.

To make our DMVFN end-to-end trainable, we design a differentiable Routing Module containing a tiny neural network to estimate routing vector $v$ for each input sample. Based on this vector, our DMVFN dynamically selects a sub-network to process the input data. As the figure shows, some blocks are skipped during inference.



Different from some dynamic network methods that can only continuously select the first several blocks ($n$ options)~\cite{branchynet,bolukbasi2017adaptive}, DMVFN is able to choose paths freely ($2^{n}$ options). DMVFN trains different sub-networks in the \textbf{super network} with various possible inference paths and uses dynamic routing inside the \textbf{super network} during inference to reduce redundant computation while maintaining the performance. A dynamic routing vector $v \in { \{ 0,1 \} }^{n}$ is predicted by the proposed Routing Module. For the $i$-th MVFN block of DMVFN, we denote $v_i$ as the reference of whether processing the reached voxel flow $\textbf{F}^{i-1}_{t+1}$ and the reached predicted frame $\tilde{I}^{i-1}_{t+1}$. The path $f^i_\mathrm{MVFB}$ to the $i$-th block from the last block will be activated only when $v_i=1$. Formally, 
\begin{equation}
\label{eqn:dualbranchselection}
\tilde{I}^{i}_{t+1},\textbf{F}^{i}_{t+1}=
\begin{cases}
f^i_\mathrm{MVFB}(\tilde{I}^{i-1}_{t+1},\textbf{F}^{i-1}_{t+1}), & v_i=1 \\
\\
\tilde{I}^{i-1}_{t+1},\textbf{F}^{i-1}_{t+1}, & v_i=0.
\end{cases}
\end{equation}
During the training phase, to enable the backpropagation of Eqn.~(\ref{eqn:dualbranchselection}), we use $v_i$ and $(1-v_i)$ as the weights of the two branches and average their outputs.

\begin{table*}[th]
\centering
\caption{
\textbf{Quantitative results of different methods on the Cityscapes~\cite{cityscapes}, and KITTI~\cite{kitti} datasets}. ``RGB", ``F", ``S" and ``I" denote the video frames, optical flow, semantic map, and instance map, respectively. We denote our DMVFN without routing module as ``DMVFN~(w/o r)". FVS~\cite{fvs} integrates a segmentation model~\cite{zhu2019improving} on KITTI~\cite{kitti} to obtain the semantic maps. ``N/A'' means not available. }
\label{table:cityscapes}
\resizebox{0.96\linewidth}{!}{
\begin{tabular}{rccccccccccccccc}
\toprule
\multirow{3}{*}[-0.28em]{Method}& \multirow{3}{*}[-0.28em]{Inputs} &  \multicolumn{7}{c}{Cityscapes-Train$\rightarrow$Cityscapes-Test~\cite{cityscapes}} & \multicolumn{7}{c}{KITTI-Train$\rightarrow$KITTI-Test~\cite{kitti}} \\
\cmidrule(l{7pt}r{7pt}){3-9} \cmidrule(l{7pt}r{7pt}){10-16}
& & \multirow{2}{*}[-0.14em]{GFLOPs} & \multicolumn{3}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & \multicolumn{3}{c}{LPIPS ($\times10^{-2}$) $\downarrow$} &  \multirow{2}{*}[-0.14em]{GFLOPs}  & \multicolumn{3}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & 
\multicolumn{3}{c}{LPIPS ($\times10^{-2}$) $\downarrow$}\\
&  &  & $t+1$ & $t+3$ & $t+5$  & $t+1$  & $t+3$ & $t+5$ & & $t+1$ & $t+3$ & $t+5$  & $t+1$  & $t+3$ & $t+5$  \\ 
\midrule
\midrule
Vid2vid~\cite{vid2vid} & RGB+S &   603.79    & 88.16 & 80.55&75.13 & 10.58 & 15.92& 20.14 & N/A & N/A & N/A& N/A & N/A & N/A& N/A  \\
Seg2vid~\cite{seg2vid} & RGB+S &  455.84  & 88.32 & N/A & 61.63 & 9.69 &  N/A& 25.99& N/A & N/A & N/A& N/A & N/A& N/A& N/A\\ 
FVS~\cite{fvs}   & RGB+S+I&  1891.65 & 89.10 & 81.13 & 75.68 & 8.50 & \textbf{12.98} & 16.50 & \textbf{768.96} & 79.28 & 67.65& 60.77  & 18.48 & 24.61& 30.49\\
SADM~\cite{sadm}   & RGB+S+F&  N/A & \textbf{95.99} & N/A & \textbf{83.51} & \textbf{7.67} & N/A & \textbf{14.93} & N/A & \textbf{83.06} & \textbf{72.44} & \textbf{64.72}  & \textbf{14.41} & \textbf{24.58} &  \textbf{31.16}\\

\midrule

PredNet~\cite{prednet} & RGB & 62.62 & 84.03 & 79.25& 75.21 & 25.99 & 29.99& 36.03 & 25.44 & 56.26 &  51.47& 47.56 & 55.35 & 58.66& 62.95               \\
MCNET~\cite{mcnet} & RGB & 502.80 & 89.69 & 78.07& 70.58 & 18.88 & 31.34& 37.34 & 204.26 & 75.35  & 63.52& 55.48 & 24.05  & 31.71& 37.39  \\
DVF~\cite{dvf} & RGB & 409.78 & 83.85 & 76.23& 71.11 & 17.37 & 24.05& 28.79& 166.47 & 53.93 & 46.99& 42.62  & 32.47  & 37.43& 41.59         \\
CorrWise~\cite{corrwise} & RGB & 944.29 & 92.80 & N/A & \textbf{83.90} & 8.50 & N/A & 15.00 & 383.62 & 82.00 & N/A & 66.70 & 17.20 & N/A & \textbf{25.90}\\ 
OPT~\cite{wu2022optimizing} & RGB &  313482.15  & 94.54 & 86.89 & 80.40 & 6.46 & 12.50&17.83 & 127431.71 & 82.71 & 69.50 & 61.09 & 12.34 & 20.29 & 26.35\\

DMVFN~(w/o r)  & RGB &  24.51  & 95.29 & 87.91 & 81.48 & 5.60 & 10.48 & 14.91 & 9.96 &88.06 & 76.53 & 68.29 & \textbf{10.70} & 19.28 & 26.13 \\
DMVFN  & RGB & \textbf{12.71} & \textbf{95.73} & \textbf{89.24}& 83.45 & \textbf{5.58} & \textbf{10.47}& \textbf{14.82} & \textbf{5.15} & \textbf{88.53} & \textbf{78.01} & \textbf{70.52} & 10.74 & \textbf{19.27} & 26.05    \\

\midrule
\end{tabular}
}
\end{table*}

% 
In the iterative scheme of our DMVFN, each MVFB essentially refines the current voxel flow estimation to a new one. This special property allows our DMVFN to skip some MVFBs for every pair of input frames. Here, we design a differentiable and efficient routing module for learning to trade-off each MVFB block. This is achieved by predicting a routing vector $v\in\{0,1\}^{n}$ to identify the proper sub-network (\eg, 0 for deactivated MVFBs, 1 for activated MVFBs). We implement the routing module by a small neural network~($\sim1/6$ GFLOPs of the \textbf{super network}), and show its architecture in Figure~\ref{fig:predict}~$(d)$. It learns to predict the probability $\tilde{v}$ of choosing MVFBs by:
\begin{equation}
\tilde{v} = \text{Linear}(\text{AvgPooling}(\text{Convs}(I_{t-1},I_t))), 
\end{equation}
\begin{equation}
v = \text{Bernoulli-Sampling}(\tilde{v}). 
\end{equation}

\paragraph{Differentiable Routing.} To train the proposed Routing Module, we need to constrain the probability values to prevent the model from falling into trivial solutions (\eg, select all blocks). On the other hand, we allow this module to participate in the gradient calculation to achieve end-to-end training. We introduce the Gumbel Softmax~\cite{gumbel} and the Straight-Through Estimator (STE)~\cite{bengio2013estimating} to tackle this issue.

One popular method to make the routing probability $\tilde{v}$ learnable is the \textbf{Gumbel Softmax} technique~\cite{gumbel,channel}. By treating the selection of each MVFB as a binary classification task, the soft dynamic routing vector $v\in\mathbb{R}^{n}$ is
%$\tilde{v}\in\mathbb{R}^{n}$ is used to compute
\begin{equation}
v_i = \frac{\exp\big (\frac{1}{\tau}(\tilde{v}_{i} + G_{i})\big )}{\exp\big (\frac{1}{\tau}( \tilde{v}_{i} + G_{i})                   \big )+\exp\big (\frac{1}{\tau}(2-\tilde{v}_{i}-G_{i})\big )},
\end{equation}
where $i=1,...,n$, $G_{i}\in\mathbb{R}$ is Gumbel noise following the Gumbel$(0,1)$ distribution, and $\tau$ is a temperature parameter. We start at a very high temperature to ensure that all possible paths become candidates, and then the temperature is attenuated to a small value to approximate one-hot distribution. To encourage the sum of the routing vectors $\{v_{i}\}_{i=1}^{n}$ to be small, we add the regularization term ($\frac{1}{n}\sum^{n}_{i=1}v_i$) to the final loss function. However, we experimentally find that our DMVFN usually converges to an input-independent structure when temperature decreases. We conjecture that the control of the temperature parameter $\tau$ and the design of the regularization term require further study.

Inspired by previous research on low-bit width neural networks~\cite{dorefanet, hubara2017quantized}, we adopt STE for Bernoulli Sampling~(\textbf{STEBS}) to make the binary dynamic routing vector differentiable. An STE can be regarded as an operator that has arbitrary forward and backward operations. Formally,
\begin{equation}
\tilde{w}_i = \min(\beta\times n\times\sigma(\tilde{v}_i) / \sum_i^n \sigma(\tilde{v}_i), 1),
\label{normalize}
\end{equation}
\begin{equation}
\textbf{STE Forward}: v_i \sim \text{Bernoulli}(\tilde{w}_i),
\end{equation}
\begin{equation}
\textbf{STE Backward}: \frac{\partial o}{\partial \tilde{w}} = \frac{\partial o}{\partial v},
\end{equation}
where $\sigma$ is the Sigmoid function and we denote the objective function as $o$. We use the well-defined gradient $\frac{\partial o}{\partial v}$ as an approximation for $\frac{\partial o}{\partial \tilde{w}}$ to construct the backward pass. In Eqn.~(\ref{normalize}), we normalize the sample rate. During training, $\beta$ is fixed at $0.5$. We can adjust the hyper-parameter $\beta$ to control the complexity in the inference phase.

\begin{table*}[th]
\centering
\caption{
% 
\textbf{Quantitative results on the DAVIS17-Val~\cite{davis} and Vimeo90K-Test~\cite{vimeo} benchmarks}. We denote DMVFN without routing as ``DMVFN~(w/o r)''. ``N/A'' means not available. }
% The best results are highlighted in \textbf{bold}.
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{rcccccccc}
\toprule
\multirow{3}{*}[-0.28em]{Method}& \multicolumn{5}{c}{UCF101-Train$\rightarrow$DAVIS17-Val} &  \multicolumn{3}{c}{UCF101-Train$\rightarrow$Vimeo90K-Test} \\
\cmidrule(l{7pt}r{7pt}){2-6} \cmidrule(l{7pt}r{7pt}){7-9}
& \multirow{2}{*}[-0.14em]{GFLOPs  $\downarrow$} & \multicolumn{2}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & 
\multicolumn{2}{c}{LPIPS ($\times10^{-2}$) $\downarrow$} &  \multirow{2}{*}[-0.14em]{GFLOPs  $\downarrow$}  & 
\multicolumn{1}{c}{MS-SSIM ($\times10^{-2}$) $\uparrow$} & 
\multicolumn{1}{c}{LPIPS ($\times10^{-2}$)  $\downarrow$} \\
&  &$t+1$   & $t+3$ & $t+1$   & $t+3$    &  & $t+1$  & $t+1$ \\
\midrule
\midrule
DVF~\cite{dvf}& 324.15 & 68.61 &55.47  & 23.23   & 34.22 & 89.64 & 92.11 & 7.73   \\
DYAN~\cite{dyan} & 130.12 & 78.96 &70.41 & 13.09  & 21.43 & N/A & N/A & N/A          \\
OPT~\cite{wu2022optimizing} & 165312.80 &  83.26 & 73.85& 11.40 & 18.21 & 45716.20 & 96.75 & 3.59          \\
\midrule

DMVFN~(w/o r) & 19.39 & \textbf{84.81} & \textbf{75.05} &\textbf{ 9.41} & \textbf{16.24 }& 5.36 & \textbf{97.24} & \textbf{3.30}  \\ 
DMVFN & \textbf{9.96} & 83.97 & 74.81 & 9.96& 17.28 & \textbf{2.77} & 97.01 & 3.69   \\ %298.pkl
\bottomrule
\end{tabular}
}
\label{table:davis-vimeo}
\end{table*}

\subsection{Implementation Details}
\label{sec:implement}

\noindent
\textbf{Loss function}. Our training loss ${L}_{total}$ is the sum of the reconstruction losses of outputs of each block ${I}^{i}_{t+1}$:
\begin{equation}\label{eq2}
     {L}_{total} = \sum_{i=1}^n \gamma^{n-i} d(\tilde{I}^{i}_{t+1}, I_{t+1}),
\end{equation}
where $d$ is the $\ell_1$ loss calculated on the Laplacian pyramid representations~\cite{laplacian} extracted from each pair of images. And we set $\gamma=0.8$ in our experiments following~\cite{raft}.

\begin{figure}[th]
	\centering
	\includegraphics[width=0.94\linewidth]{image/comparison.pdf}
	\caption{\textbf{Prediction comparison on KITTI}. The yellow line is aligned with the car in the ground truth. The results show that previous methods (DVF~\cite{dvf}, FVS~\cite{fvs}, and OPT~\cite{wu2022optimizing}) cannot accurately predict the car's location in the long-term prediction. The motion predicted by our DMVFN is the most similar to the ground truth, while the errors of other methods grow larger with time. The fences predicted by DMVFN remain vertical when moving.}
	\label{fig:kitti}
\end{figure}

\paragraph{Training strategy.} Our DMVFN is trained on $224\times224$ image patches. The batch size is set as $64$. 
%
We employ the AdamW optimizer~\cite{adam,loshchilov2018fixing} with a weight decay of $10^{-4}$.
%
We use a cosine annealing strategy to reduce the learning rate from $10^{-4}$ to $10^{-5}$.
%
Our model is trained on four 2080Ti GPUs for $300$ epochs, which takes about $35$ hours.
%





