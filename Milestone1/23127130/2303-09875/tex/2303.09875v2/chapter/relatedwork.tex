
\section{Related Work}

\subsection{Video Prediction}
Early video prediction methods~\cite{prednet,mcnet,dvf} only utilize RGB frames as inputs. For example, PredNet~\cite{prednet} learns an unsupervised neural network, with each layer making local predictions and forwarding deviations from those predictions to subsequent network layers. MCNet~\cite{mcnet} decomposes the input frames into motion and content components, which are processed by two separate encoders. DVF~\cite{dvf} is a fully-convolutional encoder-decoder network synthesizing intermediate and future frames by approximating voxel flow for motion estimation. Later, extra information is exploited by video prediction methods in pursuit of better performance. For example, the methods of Vid2vid~\cite{vid2vid}, Seg2vid~\cite{seg2vid}, HVP~\cite{hvp}, and SADM~\cite{sadm} require additional semantic maps or human pose information for better video prediction results. Additionally, Qi~\etal~\cite{qi} used extra depth maps and semantic maps to explicitly inference scene dynamics in 3D space. FVS~\cite{fvs} separates the inputs into foreground objects and background areas by semantic and instance maps, and uses a spatial transformer to predict the motion of foreground objects. In this paper, we develop a light-weight and efficient video prediction network that requires only sRGB images as the inputs.

\subsection{Optical Flow}
Optical flow estimation aims to predict the per-pixel motion between adjacent frames. Deep learning-based optical flow methods~\cite{pwcnet,raft,jonschkowski2020matters,luo2021upflow,han2022realflow} have been considerably advanced ever since Flownet~\cite{flownet}, a pioneering work to learn optical flow network from synthetic data. Flownet2.0~\cite{flownet2} improves the accuracy of optical flow estimation by stacking sub-networks for iterative refinement. A coarse-to-fine spatial pyramid network is employed in SPynet~\cite{spynet} to estimate optical flow at multiple scales. PWC-Net~\cite{pwcnet} employs feature warping operation at different resolutions and uses a cost volume layer to refine the estimated flow at each resolution. RAFT~\cite{raft} is a lightweight recurrent network sharing weights during the iterative learning process. FlowFormer~\cite{flowformer} utilizes an encoder to output latent tokens and a recurrent decoder to decode features, while refining the estimated flow iteratively. In video synthesis, optical flow for downstream tasks~\cite{vimeo,dvf,rife,qvi,zhang2023extracting} is also a hot research topic. Based on these approaches, we aim to design a flow estimation network that can adaptively operate based on each sample for the video prediction task.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.9\linewidth]{image/bigfig1.pdf}
	\caption{\textbf{Overview of the proposed Dynamic Multi-scale Voxel Flow Network (DMVFN)}.
$(a)$: To predict a future frame, we use the \textbf{voxel flow}~\cite{dvf} to guide the pixel fusion of the input frames. The voxel flow contains the prediction of object motion and occlusion. $(b)$: \textbf{DMVFN} contains several MVFBs with decreasing scaling factor $S^i$.
According to the routing vector $v$ estimated by a Routing Module, a sub-network is selected to process the input image. 
$(c)$: Each \textbf{MVFB} has a scaling factor $S^i$, which means that the motion path is performed on images whose sizes are $1/S^i$ of the original. $(d)$: Two consecutive frames are fed into several neural layers and a Differentiable Bernoulli sample to generate the hard routing vector.
}
	\label{fig:predict}
\end{figure*}

\subsection{Dynamic Network}
The design of dynamic networks is mainly divided into three categories: spatial-wise, temporal-wise, and sample-wise~\cite{dynamicnn}. Spatial-wise dynamic networks perform adaptive operations in different spatial regions to reduce computational redundancy with comparable performance~\cite{sbnet,dynamicconv,mga}. In addition to the spatial dimension, dynamic processing can also be applied in the temporal dimension. Temporal-wise dynamic networks~\cite{yeung2016end, su2016leaving, wu2019adaframe} improve the inference efficiency by performing less or no computation on unimportant sequence frames.
To handle the input in a data-driven manner, sample-wise dynamic networks adaptively adjust network structures to side-off the extra computation~\cite{skipnet,veit2018convolutional}, or adaptively change the network parameters to improve the performance~\cite{harley2017segmentation,su2019pixel,dcn,dcn2}. Designing and training a dynamic network is not trivial since it is difficult to directly enable a model with complex topology connections. We need to design a well-structured and robust model before considering its dynamic mechanism. In this paper, we propose a module to dynamically perceive the motion magnitude of input frames to select the network structure.
%
