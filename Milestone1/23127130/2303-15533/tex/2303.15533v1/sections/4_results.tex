\cutsectionup
\section{Results}
\cutsectiondown
We study the interaction between $G^{(0)}$ and $C^{(0)}$ in \cref{sec:diversity_gans}, $C^{(0)}$ and $G^{(1)}$ in \cref{sec:dcgan_fooling_detectors,sec:fooling_detectors}, and compare generators and classifiers of multiple iterations $(C^{(n)}, G^{(n)}, C^{(n+1)}, ...)$ in \cref{sec:training_over_iterations}.

\subsection{Classifiers generalize when sampling training data from multiple generator instances} \label{sec:diversity_gans}
Using our pool of generators, we can profile the number of independent generators necessary to train a classifier that can reliably generalize to samples from generators unseen during training. After the first stage of the first iteration, suppose we have a sufficiently large set of trained GAN generators. We split them into two subsets: 
$$S_0 = \{{G^{(0)}_0, ..., G^{(0)}_n}\} \hspace{8pt} \text{and} \hspace{8pt} S_1 = \{{G^{(0)}_{n+1}, ..., G^{(0)}_N}\}$$
We then train a classifier $C^{(0)}_0$ on samples drawn from generators in $S_0$ and measure its performance by testing it on samples from generators in $S_1$ (while $S_1$ can vary in size, to keep results comparable we fix $S_1$ for a given experiment). By measuring the effect of $n$ on the performance of $C^{(0)}_0$ in this way (\cref{fig:multiple_gan_instances}), we can implicitly measure the distinctiveness of artifacts produced by different generators.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/generators_diversity_dcgan.pdf}
        \caption{DCGAN classifier}
        \label{fig:generators_diversity_dcgan}
    \end{subfigure}%
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/generators_diversity_resnet.pdf}
        \caption{SG2 classifier: ResNet-50}
        \label{fig:generators_diversity_resnet}
    \end{subfigure}%
    \hspace{0.05\textwidth}
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/generators_diversity_inception.pdf}
        \caption{SG2 classifier: Inception-v3}
        \label{fig:generators_diversity_inception}
    \end{subfigure}%
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/generators_diversity_mobilenet.pdf}
        \caption{SG2 classifier: MobileNetV2}
        \label{fig:generators_diversity_mobilenet}
    \end{subfigure}%
    % \includegraphics[width=\linewidth]{images/generators_diversity.pdf}
	\caption{\textbf{Classifier generalization requires samples from several generators.} For all classifier architectures tested, the ability to distinguish between images sampled from unseen generators and real images (y-axis) depends on the number of generators used to produce training data (x-axis). This effect is present in the DCGAN setting \subref{fig:generators_diversity_dcgan} and is amplified in the SG2 setting \subref{fig:generators_diversity_resnet}, \subref{fig:generators_diversity_inception} \& \subref{fig:generators_diversity_mobilenet}. The performance of each classifier is reported as accuracy on a balanced dataset of unseen natural images and samples drawn from a pool of 5 held-out generators in the DCGAN setting, and 25 held-out generators in the SG2 setting; $0.5$ represents random classification.}
\label{fig:multiple_gan_instances}
\end{figure}

We find that a DCGAN classifier generalizes well using a single generator, and almost perfectly when trained using more than one different generators. However, in the second setting, we find that SG2 generators produce sufficient diversity between generator instances that a classifier requires samples from several generators to produce reliable generalization. With sufficient generators to sample from, however, all classifiers become extremely accurate. By contrast, when the so-called ``truncation trick'' \citecustom{karras2019style} is used to generate samples (\emph{see} Supplement), a single generator is sufficient to achieve nearly-perfect classification accuracy on unseen generators. Based on this finding, we use 3 generators in the DCGAN setting and 15 generators in the SG2 setting when training classifiers for the rest of our experiments.

\subsection{DCGAN generators fail to fool classifiers in a generalizable way}\label{sec:dcgan_fooling_detectors}
When using \cref{eq:fool_all_gan_loss} to train a DCGAN of the second iteration $i=1$, we find that, surprisingly, the GAN struggles to fool a held-out classifier. This effect is shown in \cref{fig:dcgan_fooling_accuracy} where the DCGAN learns to fool the classifier included in \cref{eq:fool_all_gan_loss} at higher values of $\phi$, but it fails to fool a held-out classifier of iteration $i=0$. As we increase the value of $\phi$ to large values, we see the output quality degrades, as shown in \cref{table:dcgan_output_quality}. Note that because we experiment with higher values of $\phi$ in this section, we normalize the coefficients as:
\begin{align}
\mathbf{\mathcal{L}^*_G} = -[\frac{1}{1+\phi}\log(D) + \frac{\phi}{1+\phi} \log(C)]
\label{eq:normalized_gan_loss}
\end{align}

\begin{figure}[h]
% \vspace{-0.1in}
    \centering
    \includegraphics[width=0.9\linewidth]{images/dcgan_fooling_accuracy.pdf}
	\caption{\textbf{DCGAN fails to fool held-out classifiers}. Here, the accuracy is reported on GAN-sampled images \emph{only}, where an accuracy of $0$ represents the GAN completely fooling the classifier. We report the mean accuracy of 5 held-out classifiers.}
\label{fig:dcgan_fooling_accuracy}
% \vspace{-0.2in}
\end{figure}
\begin{table}[h]
\vspace{-0.2in}
\centering
\begin{tabular}{cc}
\toprule
$\phi$ & DCGAN Image Samples \\
\cmidrule{1-2}
\vspace{-0.4em}
$10^{-2}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_0_01.png} \\
\vspace{-0.4em}
$10^{-1}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_0_1.png} \\
\vspace{-0.4em}
$10^{0}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_1_0.png} \\
\vspace{-0.4em}
$10^{1}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_10_0.png} \\
\vspace{-0.4em}
$10^{2}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_100_0.png} \\
\vspace{-0.4em}
$10^{3}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_1000_0.png} \\
\vspace{-0.4em}
$10^{4}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_10000_0.png} \\
\vspace{-0.4em}
$10^{5}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_100000_0.png} \\
\vspace{-0.1em}
$10^{6}$ & \includegraphics[align=c,scale=0.5]{images/dcgan_fcw_1000000_0.png} \\
\hline
\end{tabular}
\caption{\textbf{DCGAN output collapses as $\phi$ takes large values.} We show 10 random image samples from each GAN trained with a different value of $\phi$.}
\label{table:dcgan_output_quality}
% \vspace{-0.1in}
\end{table}

% \cutsubsubsectionup
\subsection{SG2 generators can be trained to fool classifiers in a generalizable way, with caveats}\label{sec:fooling_detectors}
\cutsubsubsectiondown
We observe a different behavior in the SG2 setting than in the DCGAN. In particular, very low values of $\phi$ are sufficient to cause the generator to learn to fool the classifiers. When SG2 generators are trained to fool the classifiers by using the modified loss described in \cref{eq:fool_all_gan_loss} (or \cref{eq:memoryless_gan_loss} where noted), they learn to do so early in their training, as shown in \cref{fig:fooling_curve}.

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fig_fooling_resnet.pdf}
        \caption{ResNet-50}
    \end{subfigure}%
    \hspace{0.05\textwidth}
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fig_fooling_inception.pdf}
        \caption{Inception-v3}
    \end{subfigure}%
    \centering
    \begin{subfigure}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/fig_fooling_mobilenet.pdf}
        \caption{MobileNetV2}
    \end{subfigure}%
    % \includegraphics[width=\linewidth]{images/fig_fooling.pdf}
	\caption{\textbf{SG2 generators rapidly learn to fool classifiers during training.} Accuracy of first iteration $i=0$ classifiers included in the modified loss with $\phi=0.001$ as a second iteration $i=1$ SG2 generator learns to fool them (at iteration $i=1$, the \cref{eq:fool_all_gan_loss} ``fool-all'' and  \cref{eq:memoryless_gan_loss} ``memoryless'' loss functions are equivalent). We plot 10 runs in each graph, corresponding to 10 different first-iteration classifier instances against 10 different second-iteration generators. Here, classifiers are evaluated against a balanced set of generated and real images; accuracy of $0.5$ is random classification.}
\label{fig:fooling_curve}
\end{figure}

When using ResNet-50 classifiers, SG2 generators showed a striking ability to generalize: learning to fool one classifier conferred the ability to fool any other ResNet-50 classifier (\emph{see} \cref{table:cross_architecture_eval}). The reliability of this finding, which persists over multiple iterations (\emph{see} \cref{sec:training_over_iterations}), suggests that all ResNet-50 classifier instances $C^{(i)}$ learn strongly overlapping subsets of the artifacts exhibited by the $G^{(i)}$ generators.

However, this is not the case for all classifier architectures tested. The effect was weakened slightly but statistically significantly ($p < 0.01$) in Inception-v3, though generators that fool one Inception-v3 classifier will still fool almost all others. For MobileNetV2, however, the attenuation of the effect was substantial (and statistically significant relative to ResNet-50 and Inception-v3): generators that can fool one MobileNetV2 classifier instance will be able to fool unseen MobileNetV2 classifier instances only half the time. 

We note that our ResNet-50, Inception-v3 and MobileNetV2 architectures had 23.5M, 21.8M, and 2.3M parameters, respectively, and hypothesize that this difference in fooling generalization is an effect of the capacity of the classifier the generator is learning to fool. More concretely, ResNet-50 classifiers, a high capacity model (relatively speaking), each learn the bulk of the artifacts available to them, and thus have high ``knoweldge overlap'' between instances. This overlap accounts for the observed generality of fooling ability on behalf of the generators: it's sufficient to learn to fool one ResNet-50 instance, since all the instances have learned largely the same thing. Conversely, MobileNetV2, a relatively lower capacity model, learns a smaller subset of available artifacts, reducing the probability of overlapping between instances. This reduced overlap means that a generator trained to fool a single MobileNetV2 instance is less likely to fool unseen MobileNetV2 instances when compared to the ResNet-50 case.

Interestingly, the results (\cref{table:cross_architecture_eval}, off-diagonal) imply that the sets of artifacts learned by ResNet-50 and MobileNetV2 are different. If this were not the case, we would expect a MobileNetV2 instance to learn a subset of the artifacts that ResNet-50 learns, due to its lower relative capacity, and hence expect GAN generators that fool unseen ResNet-50 classifiers to be able to readily fool unseen MobileNetV2 classifiers. This effect is even more true of Inception-v3: learning to fool Inception-v3 implies fooling unseen Inception-v3 classifiers but not unseen ResNet-50 classifiers and vice versa. Taken together, this suggests that the higher-capacity architectures learn sets of artifacts that are well-conserved within architecture but are largely distinct between architectures.

To quantify the diversity present in MobileNetV2 classifiers, we modify our ``fool-all'' $\mathbf{\mathcal{L}^\Sigma_{G^{(i)}}}$ loss function to accept multiple MobileNetV2 classifiers (each initialized differently and trained independently) from the previous iteration, rather than just one:
\begin{dmath}
\mathbf{\mathcal{L}^{\Sigma\Sigma}_{G^{(1)}}} = -[\log(D^{(1)}(G^{(1)}(w))) + \phi \sum_{j=0}^{1-1}\sum_{k=0}^{t-1}\log(C^{(j)}_k(G^{(1)}(w)))] \\
= -[\log(D^{(1)}(G^{(1)}(w))) + \phi \sum_{k=0}^{t-1}\log(C^{(0)}_k(G^{(1)}(w)))]
\label{eq:multiMobileNet_loss}
\end{dmath}

\begin{table}[h]
\centering
\begin{tabular}{r@{\hskip 0.1in}c@{\hskip 0.1in}c@{\hskip 0.1in}c@{\hskip 0.05in}c}
\toprule
 & \multicolumn{3}{c}{GAN trained to fool...} \\
 \cmidrule(r){2-4}
Classifier & ResNet-50 & Inception-v3 & MobileNetV2 \\
\midrule
ResNet-50 & \textbf{0.05$\pm$0.02} & 0.74$\pm$0.12 & 0.5$\pm$0.39 \\
Inception-v3 & 0.51$\pm$0.25 & \textbf{0.16$\pm$0.26} & 0.53$\pm$0.34 \\
MobileNetV2 & 0.31$\pm$0.17 & 0.36$\pm$0.16 & 0.41$\pm$0.38 \\
\bottomrule
\end{tabular}

\caption{\textbf{Generalization of SG2 generator classifier-fooling ability varies widely with classifier architecture.} Each entry is the accuracy (mean$\pm$std) of classifiers (row) on generated images \emph{only}, sampled from SG2 generators trained to fool other classifiers (column). Each entry is based on the performance of ten (unseen) classifiers measured against each of ten SG2 generators, where each generator is trained to fool a distinct classifier instance. ``Fooling'' is considered to occur when accuracy $\leq0.20$ (an arbitrary threshold chosen for visualization purposes), and is highlighted in \textbf{bold}. Note that, in all cases, classifiers are at least $98\%$ accurate on held-out real images.}
\label{table:cross_architecture_eval}
\end{table}
\begin{figure}[h]
% \vspace{-0.1in}
    \centering
    \includegraphics[width=0.8\linewidth]{images/multiple_mobilenet_instances.pdf}
	\caption{\textbf{Reliably fooling unseen MobileNetV2 classifiers requires learning to fool multiple MobileNetV2 classifiers.} Multiple trained instances of the low-capacity (relatively speaking) MobileNetV2 classifiers are required during generator training to achieve generalizable fooling ability. SG2 generators are trained using the loss function in \cref{eq:multiMobileNet_loss}. We report the mean accuracy of 10 held-out MobileNetV2 classifier instances against generated images from a single trained generator, and we start with fooling a classifier where it completely fails to fool held-out classifiers.}
\label{fig:multiple_mobilenet_instances}
% \vspace{-0.2in}
\end{figure}
Because we perform this experiment on iteration $i=1$, there is only one previous iteration (the first, $i=0$), so we may drop the summation-over-previous iteration terms. We add a new summation term to include $t$ classifiers from the previous iteration, rather than a single one. When additional MobileNetV2 classifiers are included, we see the fooling ability of the resulting generators begins to generalize, as shown in \cref{fig:multiple_mobilenet_instances}. 

\begin{table*}[h]
\centering
% \small
\begin{tabular}{rcccccccccc}
\toprule
\multirow{2}{*}{MobileNetV2\vspace{7pt}} & \multicolumn{10}{c}{GAN trained to fool MobileNetV2 classifier \#...} \\
\cmidrule(r){2-11}
Classifier & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
\midrule
\cline{2-7}
\rule{0pt}{2.25ex}
0 & \multicolumn{1}{|c}{\textbf{0.04}} & \textbf{0.06} & \textbf{0.05} & \textbf{0.08} & \textbf{0.07} & \multicolumn{1}{c|}{\textbf{0.06}} & 0.98 & 0.98 & 0.47 & 0.95  \\
1 & \multicolumn{1}{|c}{\textbf{0.16}} & \textbf{0.05} & \textbf{0.11} & \textbf{0.10} & \textbf{0.08} & \multicolumn{1}{c|}{\textbf{0.10}} & 0.99 & 0.99 & 0.64 & 0.97  \\
2 & \multicolumn{1}{|c}{\textbf{0.09}} & \textbf{0.08} & \textbf{0.03} & \textbf{0.10} & \textbf{0.08} & \multicolumn{1}{c|}{\textbf{0.08}} & 0.99 & 0.99 & 0.44 & 0.94  \\
3 & \multicolumn{1}{|c}{\textbf{0.09}} & \textbf{0.05} & \textbf{0.05} & \textbf{0.03} & \textbf{0.05} & \multicolumn{1}{c|}{\textbf{0.05}} & 0.99 & 0.99 & 0.53 & 0.96  \\
4 & \multicolumn{1}{|c}{\textbf{0.09}} & \textbf{0.05} & \textbf{0.09} & \textbf{0.06} & \textbf{0.04} & \multicolumn{1}{c|}{\textbf{0.10}} & 0.98 & 0.98 & 0.60 & 0.94  \\
5 & \multicolumn{1}{|c}{0.22} & \textbf{0.15} & \textbf{0.14} & \textbf{0.20} & \textbf{0.20} & \multicolumn{1}{c|}{\textbf{0.05}} & 1.00 & 1.00 & 0.70 & 0.98  \\
\cline{2-9}
\rule{0pt}{2.25ex}
6 & 0.53 & 0.45 & 0.59 & 0.37 & 0.52 & 0.35 & \multicolumn{1}{|c}{\textbf{0.01}} & \multicolumn{1}{c|}{0.21} & 0.37 & \textbf{0.03}  \\
7 & 0.28 & 0.29 & 0.45 & \textbf{0.17} & \textbf{0.17} & 0.21 & \multicolumn{1}{|c}{\textbf{0.02}} & \multicolumn{1}{c|}{\textbf{0.00}} & \textbf{0.12} & \textbf{0.03}  \\
\cline{8-9}
\rule{0pt}{2ex}
8 & \textbf{0.13} & \textbf{0.14} & \textbf{0.06} & \textbf{0.11} & \textbf{0.17} & \textbf{0.09} & 0.92 & 0.91 & \textbf{0.03} & 0.74  \\
9 & 0.44 & 0.42 & 0.37 & 0.34 & 0.40 & 0.33 & 0.60 & 0.70 & 0.28 & \textbf{0.02} \\
\bottomrule
\end{tabular}

\caption{\textbf{MobileNetV2 classifiers appear to form clusters based on the subset of artifacts learned.} Each entry $(i, j)$ is the accuracy of the classifier $i$ (row) against images sampled from SG2 generators trained to fool the classifier $j$ (column). A value of 0.0 means the classifier was completely fooled while a value of 1.0 means the classifier was never fooled. An accuracy $\leq0.20$ (an arbitrary threshold) is considered ``fooled'' and is highlighted in \textbf{bold}. Note that, in all cases, classifiers are at least $98\%$ accurate on held-out real images.}
\label{table:cross_MobileNet_eval}
% \vspace{-0.1in}
\end{table*}

Consistent with our classifier-capacity hypothesis, multiple MobileNetV2 classifiers are required to achieve generalization because each instance may only learn a subset of the artifacts available to it, and so do not overlap as often as higher-capacity architectures. To measure the between-instance overlap of MobileNetV2, we conducted a pairwise comparisons over 10 independent MobileNetV2 classifiers. To this end, we trained 10 generators to fool one of 10 trained MobileNetV2 classifiers, then we tested that generator's fooling ability against the other MobileNetV2 instances (as well as the one they were trained to fool). The results (\cref{table:cross_MobileNet_eval}) are striking: rather than randomly sampling artifacts to learn, which would result in fairly uniform off-diagonal values in the table, we see clear clusters emerging that are ``mutually-fooling'': a generator trained to fool one will fool the rest, which we take as implying the classifiers within a cluster learned a shared subset of generator artifacts. For instance, classifier instances 6 and 7 are (almost) mutually fooling, as are classifiers 0 through 5; however, a generator trained to fool 6 or 7 is totally unable to fool classifiers 0 through 5.  In the table, the two clusters mentioned are highlighted using boxes. Perhaps more surprising, the table lacks diagonal symmetry: suggesting that some classifiers learn partial subsets of the artifacts learned by others.

\cutsubsubsectionup
\subsection{SG2 generators of subsequent iterations do not change in image quality or learning behavior}\label{sec:training_over_iterations}
\cutsubsubsectiondown
\cref{sec:diversity_gans,sec:dcgan_fooling_detectors,sec:fooling_detectors} are essentially concerned with the first iteration and the first stage of the second iteration, as detailed in Figs. \ref{fig:experiment_setup_1} and \ref{fig:experiment_setup_2}. If we continue conducting iterations in the SG2 setting, new dynamics emerge. 

Regardless of the loss function used (either Eq. \ref{eq:fool_all_gan_loss} or \ref{eq:memoryless_gan_loss}), the training process does not appreciably change for the generators, nor do they require increased training time. Further, this does not result in a drop in the visual quality of the sampled images either, whether measured qualitatively by visual inspection or quantitatively by FID \citecustom{heusel2017gans}, as included in \cref{table:higher_iteration_detectors}. We do not show the images from the FFHQ dataset or image outputs from models trained on the dataset in this work.

% \begin{table}[h]
% \centering
% \caption{\textbf{Output quality does not change in 5 generator iterations.} Visual quality remains consistently high, both perceptually and as measured by FID. Each example shown here is sampled from a distinct GAN instance.}
% \label{table:multiple_iteration_images}
% % \vspace{-0.1in}
% \end{table}

\begin{table*}[h]
\centering
\begin{tabular}{rccccc}
\toprule
 & \multicolumn{5}{c}{GAN Instances} \\
\cmidrule{2-6}
Classifier & Iteration 0 & Iteration 1 & Iteration 2 & Iteration 3 & Iteration 4 \\
\midrule
Iteration 0 & \textbf{0.993$\pm$0.002} & 0.028$\pm$0.004 & 0.037$\pm$0.006 & 0.047$\pm$0.011 & 0.053$\pm$0.007 \\
Iteration 1 & 0.000$\pm$0.000 & \textbf{0.996$\pm$0.005} & 0.013$\pm$0.003 & 0.015$\pm$0.002 & 0.008$\pm$0.002 \\
Iteration 2 & 0.001$\pm$0.000 & 0.734$\pm$0.138 & \textbf{0.957$\pm$0.053} & 0.014$\pm$0.003 & 0.029$\pm$0.007 \\
Iteration 3 & 0.004$\pm$0.001 & 0.868$\pm$0.115 & 0.184$\pm$0.133 & \textbf{0.931$\pm$0.069} & 0.018$\pm$0.004 \\
Iteration 4 & 0.007$\pm$0.001 & 0.713$\pm$0.140 & 0.510$\pm$0.141 & 0.068$\pm$0.027 & \textbf{0.858$\pm$0.112} \\
\midrule
Mean FID & 36.98 & 36.62 & 36.67 & 36.39 & 36.83 \\
\bottomrule
\end{tabular}

\caption{\textbf{Classifier performance in sequential iterations.} We train $5$ iterations of SG2 GANs and their classifier counterparts. The SG2 generators of an iteration are trained with a classifier model of each of the previous iterations (using \cref{eq:fool_all_gan_loss}). $i$th iteration classifier models are trained on $15$ trained instances of $i$th iteration SG2 generators. The entry at $(i, j)$ is the accuracy mean$\pm$std of a iteration $i$ classifier evaluated on images generated with a balanced source of $10$ held-out iteration $j$ SG2 generator instances. A value of $0.0$ means the classifier is always fooled; a value of $1.0$ means it is never fooled. Note that, in all cases, classifiers are at least $95\%$ accurate on held-out real images.}
\label{table:higher_iteration_detectors}
% \vspace{-0.1in}
\end{table*}

\cref{table:higher_iteration_detectors} details five iterations, where all classifiers are ResNet-50 architecture and generators are trained with the ``fool-all'' loss $\mathbf{\mathcal{L}^\Sigma_{G^{(i)}}}$ (\cref{eq:fool_all_gan_loss}). In each iteration, generators must fool a classifier from all previous iterations, and accordingly, high-capacity classifiers from iteration $i$ are wholly unable to detect generated images from generators of subsequent iterations $k > i$ (upper-right in \cref{table:higher_iteration_detectors}). The converse is not true: higher-iteration classifiers sometimes, but not always, detect lower-iteration generators (lower-left in \cref{table:higher_iteration_detectors}). This phenomenon lacks a readily identifiable pattern but is highly replicable. For instance, across 10 independent trials, iteration 2 classifiers could detect iteration 1 generators about 73\% of the time, while iteration 4 classifiers could detect iteration 3 generators only 7\% of the time. Furthermore, classifiers of subsequent iterations completely fail to detect GANs of iteration 0 (the first column in \cref{table:higher_iteration_detectors}). The fact that this effect is iteration specific suggests underlying complexity.

Along the diagonal, we see high performance: iteration $i$ classifiers are able to learn to identify images synthesized by iteration $i$ generators, unseen by the classifier during training. We do note a potential trend of diminishing classification test accuracy in later iterations, which suggests the generators do begin to de-correlate after a number of iterations. It is not surprising that the generators still produce artifacts even after training against the classifiers. It is surprising, however, that the generalization ability of the classifiers remains high over the course of several iterations. This means that not only are the generators producing new artifacts but these artifacts are shared with the other randomly-initialized and randomly-trained generator instances of the same iteration. In other words, the generators shift consistently and largely in unison across iterations. Taken together, this suggests the existence of an ``artifact preference'' on the part of the generators. If the generation of some set of artifacts is precluded (for instance, by the need to fool classifiers that have learned them), the generators will not only begin to generate new artifacts but \emph{largely the same} set of new artifacts.

The notion that generators produce artifacts according to an orderly preference, where precluding one set of artifacts leads predictably to the generation of a new set of artifacts, is reinforced by our experiments with iterations trained with the ``memoryless'' loss function (\cref{eq:memoryless_gan_loss}). In this regime, generators must fool only a classifier from the previous iteration. Unsurprisingly, SG2 generators trained in this way fool the previous iteration's classifiers ($G^{(i)}$ fools $C^{(i-1)}$), however, they are detectable by the classifier from \emph{two} iterations ago: $G^{(i)}$ is readily detected by classifier $C^{(i-2)}$. This suggests that, fittingly, in the ``memoryless'' training regime generators oscillate between one of two clusters of artifacts depending on the parity of the iteration.

