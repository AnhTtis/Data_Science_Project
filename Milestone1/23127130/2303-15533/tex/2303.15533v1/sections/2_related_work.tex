\cutsectionup
\section{Related Work}
\cutsectiondown

Research into detection of GAN-generated media has largely tracked the increasing prominence and output quality of GANs themselves. Several studies \citecustom{mo2018fake,cozzolino2018forensictransfer,hsu2018learning,marra2018detection,tariq2018detecting,Wang_2020_CVPR,frank2020leveraging,cozzolino2021towards,gragnaniello2021gan} focus on detection of GAN-generated images using CNNs, and their robustness to data augmentation at test time. Of particular interest to us is \citecustom{Wang_2020_CVPR}, who train a ResNet-50 classifier (pre-trained using ImageNet \citecustom{russakovsky2015imagenet}) on images generated using just one modern GAN architecture, ProGAN \citecustom{karras2018progressive}. They show that the classifier generalizes to unseen GAN architectures, concluding that the task of general GAN detection is fairly straightforward, at least in the absence of image augmentations. Later studies dispute this \citecustom{gragnaniello2021gan,frank2021re}, demonstrating that the test performance of the classifier is decreased if GAN architectures used in training predate those used during test. 

Previous research \citecustom{Wang_2020_CVPR,gragnaniello2021gan} train classifiers on samples from multiple generators but train each generator on separate data domains or datasets, meaning it is not possible to discern the variation due to the generators themselves. We also note these classifiers are not robust to perturbations and can be fooled with specialized targeted attacks \citecustom{carlini2020evading,goebel2020adversarial}, as is characteristic of classifiers trained using neural networks \citecustom{szegedy2013intriguing}. We consider this out of scope for this work relative to the questions we seek to address.  

\citecustom{yu2019attributing} study GAN attribution, a related problem where the architecture of the source generator for a given sample is inferred. They show that multi-class classification works well to distinguish different GANs, where the learned latent embeddings and weights are used as the image and GAN model fingerprints respectively. \citecustom{marra2019gans} attribute fingerprints to distinguish different GAN model architectures and datasets. These studies show that GAN-generated images contain architecture- and instance-specific artifacts.

There is limited research on the behavior of GAN generators trained to fool such classifiers. \citecustom{chai2020makes} train a specialized patch-based classifier then finetune the GAN generator to fool the classifier, which results in a significant drop in the classifier's accuracy. A second classifier trained using images from the finetuned generator is able to recover in accuracy. \citecustom{zhao2021making} study a related problem of automatically eliminating generator artifacts, training a lightweight CNN generator that adds minimal perturbations to GAN-generated images, allowing them to fool even unseen classifiers. \citecustom{liu2022making} build another such ``trace removal network'' which learns to remove several types of traces left in various types of ``DeepFakes.'' For our study, we are principally interested in the consistency of these artifacts across GAN generators and how the GAN generators adjust themselves when their training loss is modified to include a classifier, to better study the space of artifacts present in GAN-generated images.

To our knowledge, there has not been any work evaluating the effect of classifier model architecture and capacity on the learned artifacts, which we have also studied in this paper.