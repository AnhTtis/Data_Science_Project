% \cutsectionup
\section{Introduction} \label{sec:introduction}
\cutsectiondown

GAN \citecustom{goodfellow2014generative} architectures like StyleGAN2 \citecustom{karras2020analyzing} generate high-resolution images that appear largely indistinguishable from real images to the untrained eye \citecustom{lago2021more,Nightingale2022AIsynthesizedFA,hulzebosch2020detecting}. While there are many positive applications, the ability to generate large amounts of realistic images is also a source of concern given its potential application in scaled abuse and misinformation. In particular, GAN-generated human faces are widely available (e.g., \href{https://thispersondoesnotexist.com}{thispersondoesnotexist.com}) and have been used for creating fake identities on the internet \citecustom{hill2020deceive}.
 
Detection of GAN-generated images is an active research area (\emph{see} \citecustom{gragnaniello2021gan} for a survey of approaches), with some using custom methods and others using generic CNN-based classifiers. Such \emph{classifiers} are distinct from the discriminator networks that are trained alongside the generator in the archetypal GAN setup. Given the adversarial nature of the training loss for GANs, the existence of the GAN-classifiers suggest consistent generator \emph{knowledge gaps} (\emph{i.e.,} artifacts present across samples that distinguish generated images from those of the underlying distribution) left by discriminators during training. Specialized classifiers \citecustom{Wang_2020_CVPR} are able to detect images sampled from held-out GAN instances and even from held-out GAN architectures. These generalization capabilities imply that the knowledge gaps are consistent not only across samples from a GAN generator but across independent GAN generator instances.
 
 In this work we modify the GAN training loss in order to fool a GAN-classifier in addition to the co-trained discriminator, and examine the effect on training dynamics and output quality. We conduct multiple rounds of training independent pools (initialized differently) of GANs followed by GAN-classifiers, and gain new insights into the GAN optimization process. We investigate two different settings: in the first setting, we choose the low-dimensional domain of handwritten digits (MNIST \citecustom{lecun2010mnist}), using a small DCGAN \citecustom{radford2015unsupervised} architecture and a vanilla GAN-classifier architecture. For the second setting, we choose a high-dimensional domain of human faces (FFHQ \citecustom{karras2019style}) with StyleGAN2 (SG2) as a SOTA GAN architecture, and three different GAN-classifier architectures (ResNet-50 \citecustom{he2016deep}, Inception-v3 \citecustom{szegedy2016rethinking}, and MobileNetV2 \citecustom{sandler2018mobilenetv2}). Our findings in this paper are as follows:
 \begin{itemize}
     \item Samples drawn from a GAN instance exhibit a space of ``artifacts'' that are exploited by the classifiers, and this space is strongly correlated with those of other GAN generator instances. This effect is present in both the DCGAN and SG2 settings.
     \item Upon introducing the need to fool held-out classifiers, the DCGAN is unable to generate high quality outputs.
     \item In the high dimensional setting, however, SG2 generators can easily fool held-out trained classifiers, and move to a new artifact space. Strikingly, we find that the artifact space is correlated among the new population of generators as it was in the original population. This correlation appears to persist in subsequent rounds as new classifiers are introduced that are adapted to the new artifact spaces.
     \item MobileNetV2 classifier instances in the SG2 setting appear unable to learn all of the artifacts available for them to exploit. Instead, MobileNetV2 instances form clusters based on the subset of artifacts learned. We hypothesize this being an effect of classifier capacity.
     \item An SG2 generator trained to reliably fool unseen classifier instances from a given architecture is not guaranteed to fool classifiers from another architecture. Therefore, the artifacts learned by a given classifier depends strongly on the classifier's architecture.
 \end{itemize}