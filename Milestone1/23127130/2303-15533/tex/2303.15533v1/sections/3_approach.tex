\cutsectionup
\section{Approach}
\cutsectiondown

We study the phenomena outlined in the introduction by creating and measuring the performance of classifiers trained to detect images sampled from \emph{unseen} generators and subsequently training new generators to fool them, in sequential rounds, forming a chain of generators and classifiers. We do this in one of two settings, first with low dimensional images (MNIST), a simplistic DCGAN, and a basic classifier architecture. In the second setting, we use higher dimensional images (FFHQ), and perform experiments using the unmodified StyleGAN2 (SG2) architecture. Seeking to  minimize sources of variance as much as possible, we limit to a single GAN architecture and a fixed dataset in both settings. We also do not use the ``truncation'' trick \citecustom{karras2019style}, a sample-time heuristic commonly used with the SG2 architecture to improve the output visual quality at the expense of diversity (\emph{see} Supplement for more discussion on this). In the SG2 setting, we test three different widely-used classifier architectures: ResNet-50, Inception-v3, and MobileNetV2. These architectures were chosen for their architectural diversity. All classifiers and generators are trained from scratch, without any pre-training. Supplement provides details about the model architectures and training parameters.

\cutsubsubsectionup
\subsection{A note on terminology} \label{sec:terminology}
\cutsubsubsectiondown

Because our procedure involves both GANs and classifiers, there is potential ambiguity in terminology as GANs themselves are trained with a subnetwork designed to distinguish generated images from natural images, which is commonly called the ``discriminator'', ``adversarial network'', or ``critic'', among others. To keep the text clear, we will refer to subnetworks co-trained with a generator which together comprise a GAN as ``\textbf{discriminators}'', denoted $D$. The networks trained on samples from multiple, independently trained generators are referred to as ``\textbf{classifiers}'', $C$. Each sequential round of training a pool of GANs followed by training classifiers is an ``\textbf{iteration}'' (detailed in Sec. \ref{sec:overview_setup}, and Figs. \ref{fig:experiment_setup_1} and \ref{fig:experiment_setup_2}) and is indexed with a superscript. Iterations are distinct from training steps: during a single iteration, GANs are fully trained, then classifiers are fully trained using those GAN generators. Broadly speaking an ``\textbf{artifact}'' is any property of a generated image that distinguishes it from a real image. By ``\textbf{knowledge gaps}'', we are referring to a specific class of artfacts that reliably occur \emph{across} samples from a generator. Since this class of artifacts is the only one studied in this work, we use artifact and knowledge gap interchangeably.

\cutsubsubsectionup
\subsection{Overview of setup and iterations} \label{sec:overview_setup}
\cutsubsubsectiondown
\begin{figure}[h]
	\centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/gan_training_zeroth.pdf}
        \caption{Stage 1 at iteration 0: GAN training with standard loss function} \label{fig:gan_training_zeroth}
    \end{subfigure}%
    \hspace{0.05\textwidth}
    % \hspace*{\fill}   % maximize separation between the subfigures
    \begin{subfigure}{\linewidth}
    \centering
        \includegraphics[width=0.7\linewidth]{images/classifier_training.pdf}
        \caption{Stage 2 at iteration $i$: Classifier training}\label{fig:classifier_training}
    \end{subfigure}%
	\caption{\textbf{Experimental setup \& training classifiers.} Generators $G$ are \textbf{\textcolor[HTML]{93C47C}{green}}, co-trained discriminators $D$ are \textbf{\textcolor[HTML]{A4C2F4}{blue}} and classifiers $C$ trained using multiple, frozen generators are \textbf{\textcolor[HTML]{C27BA0}{purple}}. Dashed borders indicate that the subnetwork is not being updated during this stage of the iteration. \subref{fig:gan_training_zeroth} Generators trained in iteration 0 are trained in the typical way. \subref{fig:classifier_training} Classifiers are trained in the second stage of all iterations, on samples drawn from subsets of the generators trained in the first stage.}
\label{fig:experiment_setup_1}
% \vspace{-0.05in}
\end{figure}

Our experiments consist of sequential rounds (``iterations''), each with two stages: first, a pool of GAN generators initialized randomly is trained, then classifiers are trained to detect samples from the generators trained in the first stage. In the first stage of the first iteration ($i = 0$), a number of GANs (DCGAN in the first setting, SG2 in the second setting) are trained independently on the training images (MNIST in the first setting, FFHQ in the second setting), as shown in \cref{fig:gan_training_zeroth}. This setup is modified slightly in later iterations (\emph{see} \cref{fig:experiment_setup_2}) as detailed below. Classifier training follows in the second stage (\cref{fig:classifier_training}) as a standard classification task where each classifier is trained on a balanced dataset of real images and images sampled from a subset of generators trained in the first stage. The second stage is the same in every iteration, always sampling images from generators trained in the first stage of the iteration.
The first stage of subsequent iterations ($i > 0$) proceeds like the first stage of the first iteration but with a modified generator loss function: generators are trained to fool not only the discriminator they are co-trained with, but also frozen classifiers from preceding iterations. To do this we modify the ``classical'' GAN generator loss function $\mathbf{\mathcal{L}}$:
\begin{dmath}
\mathbf{\mathcal{L}_{G^{(i)}}} = -\log(D^{(i)}(G^{(i)}(w)))
\label{eq:orig_gan_loss}
\end{dmath}
in one of two ways. In the first, $\mathbf{\mathcal{L}^\Sigma_{G^{(i)}}}$, generators must fool a classifier from \emph{every} preceding iteration: 
\begin{dmath}
\mathbf{\mathcal{L}^\Sigma_{G^{(i)}}} = -[\log(D^{(i)}(G^{(i)}(w))) + \phi \sum_{j=0}^{i-1}\log(C^{(j)}_0(G^{(i)}(w)))]
\label{eq:fool_all_gan_loss}
\end{dmath}
A graphical depiction of a single generator using this loss function is shown in \cref{fig:gan_training_modified}. $\phi$ is a used to weight the relative influence of classifiers. Because a classifier from each previous iteration must be fooled in order to minimize this function, we refer to it as the ``fool-all'' loss function.

The other generator loss function variation, $\mathbf{\mathcal{L}^*_{G^{(i)}}}$, relies purely on a classifier from the iteration immediately preceding the current one, rather than all preceding iterations: 
\begin{align}
\mathbf{\mathcal{L}^*_{G^{(i)}}} = -[\log(D^{(i)}(G^{(i)}(w))) + \phi \log(C^{(i-1)}_0(G^{(i)}(w)))]
\label{eq:memoryless_gan_loss}
\end{align}
This is depicted in \cref{fig:gan_training_modified_memoryless}. Because $\mathbf{\mathcal{L}^*_{G^{(i)}}}$ depends only on the current iteration and the preceding iteration, we refer to this as the ``memoryless'' loss function.

The two modifications result in markedly different training dynamics. Reported results will generally be for the ``fool-all'' $\mathcal{L}^\Sigma$ variation (\cref{fig:gan_training_modified}). When results are based on experiments using the ``memoryless'' variation $\mathcal{L}^*$ (\cref{fig:gan_training_modified_memoryless}), they will be explicitly noted as such. Classifiers are frozen (i.e., their weights are not updated) during the first stage of every iteration.

\begin{figure}[h]
	\centering
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/gan_training_modified.pdf}
        \caption{Stage 1 at iteration $i$: GAN training with ``fool-all'' modified loss function}
        \label{fig:gan_training_modified}
    \end{subfigure}%
    \hspace{0.05\textwidth}
    % \hspace*{\fill}   % maximize separation between the subfigures
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{images/gan_training_modified_memoryless.pdf}
        \caption{Stage 1 at iteration $i$: GAN training with ``memoryless'' modified loss function} \label{fig:gan_training_modified_memoryless}
    \end{subfigure}%
	\caption{\textbf{GANs trained in higher iterations.} In subsequent iterations ($i > 0$), stage 1 GAN training is modified from the first iteration ($i=0$, \emph{see} \cref{fig:gan_training_zeroth}) such that the generator $G^{(i)}_k$ learns to fool not only its co-trained discriminator $D^{(i)}_k$ but also \subref{fig:gan_training_modified} $i$ classifiers, one from each preceding iteration (\cref{eq:fool_all_gan_loss}) or \subref{fig:gan_training_modified_memoryless} a single classifier from the immediately preceding iteration (\cref{eq:memoryless_gan_loss}). At $i=1$, these two approaches are equivalent.}
\label{fig:experiment_setup_2}
\end{figure}
The classifier subscript $0$, used in Figs. \ref{fig:gan_training_modified} and \ref{fig:gan_training_modified_memoryless} (e.g., $C^{(i-1)}_0$), is purely to distinguish classifiers within the same iteration. In each iteration, multiple classifiers are trained that are initialized randomly and trained independently. When testing a GAN trained to fool the previous iteration's classifiers, classifiers used for training and testing are trained on disjoint subsets of generators, to measure generalization. For example, if $G^{(i)}_k$ is trained to fool $C^{(i-1)}_0$, and is evaluated against $C^{(i-1)}_1$, then $C^{(i-1)}_0$ and $C^{(i-1)}_1$ are trained on disjoint subsets of iteration $i-1$ generators.