\cutsectionup
\section{Conclusion} \label{sec:conclusion}
\cutsectiondown

GAN-generated images exhibit ``artifacts'' that distinguish them from real images, even if such artifacts are not apparent to the human eye. Many of these artifacts are ``knowledge gaps:'' rather than being sample-specific, they are present in most or all of the samples from a given generator. We study two settings: using DCGAN with the MNIST dataset, and StyleGAN2 with the FFHQ dataset. Our results suggest that, far from being random or just instance-specific, some artifacts are produced in a regular, repeatable way \emph{across} independent generators of the same GAN architecture (and trained on the same dataset), comprising an ``artifact space.'' This is evident from the fact that all classifiers we tested needed samples from a relatively small pool of generators in order to reliably generalize to unseen generator instances.

Between the two settings, and in stark contrast, DCGAN generators trained on MNIST were unable to fool unseen classifiers without compromising output quality. But StyleGAN2 generators trained on FFHQ learned to fool unseen classifiers of the same architecture with high reliability and often from exposure to a single classifier instance. However, this fooling ability does not generalize to classifier instances of other architectures. This suggests that the subset of the available artifacts learned by a classifier is similar within architecture but different between architecture. Further, StyleGAN2 generators' fooling ability generalizes more reliably when the classifier in question is high capacity (ResNet-50, Inception-v3). Hence, high-capacity architectures learn a large proportion of the artifacts available to them, resulting in correlated behavior. However, a classifier like MobileNetV2 only learns a portion of the artifact space available to them, and so multiple trained classifier instances are required when training a generator in order to reliably fool held-out MobileNetV2 instances. This does not imply that MobileNetV2-based classifiers learn an arbitrary subset of the artifact space: pairwise comparisons indicate that they instead tend to fall into clusters that are ``mutually-fooling.''

When iterating the process in the StyleGAN2 setting, we find that the StyleGAN2 generators continue to quickly learn to fool the classifiers. Similarly, the classifiers require samples from only a few of the newly trained generators to learn how to detect unseen generators reliably. This persistence of detector generalization suggesting that the constraint of needing to fool a classifier induces a consistent transformation on the artifact space across generators (rather than, say, inducing a random transformation specific to each generator). Thus, the StyleGAN2 generators of each new iteration produce artifact spaces that are (mostly) distinct compared to previous iteration but are largely the same \emph{within} the iteration. This suggests an induced preference or ordering over artifact spaces, and merits further study. Our results also hint that this process doesn't continue indefinitely: after a sufficient number of iterations, the StyleGAN2 generators may begin to ``decorrelate'' in terms of their artifact space.

Lastly, we discuss the limitations and societal impact of our work. While we expect our findings to be a property of GAN generators broadly, verifying this is left for future work. In line with previous work \citecustom{Wang_2020_CVPR, gragnaniello2021gan}, it would be instructive to investigate the overlap in the ``artifact spaces'' across different GAN architectures, among multiple iterations. From a misinformation mitigation perspective, we hope our findings will motivate further research into detection of GAN-generated images. However, as with all published work on detection, we cannot conclusively say that there is no potential this could benefit bad actors.