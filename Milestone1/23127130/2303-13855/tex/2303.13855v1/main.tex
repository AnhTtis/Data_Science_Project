\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{arxiv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{appendix}



\arxivfinalcopy % *** Uncomment this line for the final submission

% Include other packages here, before hyperref.
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifarxivfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Deformable Model Driven Neural
Rendering for High-fidelity 3D Reconstruction of Human Heads
Under Low-View Settings}

\author{Baixin Xu$^1$ \quad
Jiarui Zhang$^2$ \quad 
Kwan-Yee Lin $^3$ \quad
Chen Qian $^4$\quad
Ying He$^1\thanks{Corresponding author: Y. He (yhe@ntu.edu.sg).}$\\
$^1$ Nanyang Technological University \quad
$^2$ Peking University \quad\\
$^3$ The Chinese University of Hong Kong \quad
$^4$ SenseTime Research\\
}

\maketitle
% Remove page # from the first page of camera-ready.
\ifarxivfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}

We propose a robust method for learning neural implicit functions that can reconstruct 3D human heads with high-fidelity geometry from low-view inputs. We represent 3D human heads as the zero level-set of a composed signed distance field that consists of a smooth template, a non-rigid deformation, and a high-frequency displacement field. The template represents identity-independent and expression-neutral features, which is trained on multiple individuals, along with the deformation network. The displacement field encodes identity-dependent geometric details, trained for each specific individual. We train our network in two stages using a coarse-to-fine strategy without 3D supervision. Our experiments demonstrate that the geometry decomposition and two-stage training make our method robust and our model outperforms existing methods in terms of reconstruction accuracy and novel view synthesis under low-view settings. Additionally, the pre-trained template serves a good initialization for our model to adapt to unseen individuals.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Accurately modeling and rendering human heads is crucial for various digital human-related applications as the head is one of the most distinguishing features that helps us identify individuals. Neural implicit functions~\cite{oechsle2021unisurf,wang2021neus,yariv2021volume,wang2022improved,fu2022geo,zheng2022avatar,darmon2022improving} have recently emerged as a promising technique for synthesizing novel views of complex objects, including human heads, by learning a continuous function from multi-view images without being tied to a specific resolution. However, training such deep learning models often requires a large number of images as input, which can be costly and computationally inefficient. Moreover, a single implicit field may not generalize well to unseen heads, particularly under setting of low or sparse views.


This paper aims to develop a robust method for learning neural implicit functions that can accurately reconstruct 3D human heads with high-fidelity geometry and appearance from low-view inputs, thereby reducing the need of extensive data collection and annotation. To achieve this goal, we learn a signed distance field (SDF) that consists of a smooth template, a non-rigid deformation, and a high-frequency displacement map. The template represents identity-independent and expression-neutral features, while the deformation and displacement maps encode identity-dependent geometric details that are trained for each specific individual. We represent 3D human heads as the zero level-set of the composed SDF. Our training involves two stages. The first stage takes the whole set of persons as input, and learns an identity-independent and expression-neutral template head and a non-rigid deformation between each observed head and the template head. In the second stage, we learn an identity-dependent displacement map for further refining the geometry. To train the proposed SDF without any 3D supervision, we adopt a volume rendering scheme~\cite{oechsle2021unisurf,yariv2021volume,wang2021neus} to minimize the difference between the rendered colors and the ground-truth colors. We also adopt regularization terms for the SDF, the deformation, the displacement map and the latent codes. 


We evaluate our approach on both senior and young persons and demonstrate that it is robust and effective in learning SDFs for both types of inputs, resulting in 3D surfaces with high-fidelity geometry. Our experiments show that our method outperforms existing methods in terms of reconstruction accuracy and visual quality. We also demonstrate that our method can generalize to unseen identities by using the pre-trained template as a good initialization. 

\begin{figure*}[!htbp]
      \centering
      \includegraphics[height=1.6in,trim={20 0 10 0},clip]{./images/teaser/teaser_input2.pdf} \hspace{-0.1in}
      \includegraphics[height=1.6in, trim={250 0 310 0},clip]{./images/teaser/teaser_mesh.pdf}
      \includegraphics[height=1.6in,trim={200 0 120 20},clip]{./images/teaser/teaser_output_train.pdf}\hspace{-0.2in}
      \includegraphics[height=1.6in,trim={60 0 120 20},clip]{./images/teaser/teaser_output_nv.pdf}\\
     % \begin{scriptsize}
      \hspace{-0.5in}
      \makebox[1.2in]{Input}
      % \hspace{-0.1in}
      \makebox[1.2in]{3D head}
      \hspace{0.2in}
      \makebox[1.4in]{Training views}
      \makebox[2.2in]{Novel views}\\
      %\end{scriptsize}
      %\vspace{-0.1in}
      \caption{An example of a 3D head reconstructed by our method using 10 input views. Refer also to the supplementary material for a video demonstration.
      %We show the input images, the reconstructed 3D mesh, and the normal maps of two training views and three novel views. 
      %Images are rendered in high resolution, allowing close-up examination.
      }
      \label{fig:teaser}
  \end{figure*}


\section{Related Work}
\label{sec:related work}

\noindent\textbf{Multi-view 3D reconstruction} is a classical problem in computer vision, and traditional approaches can be divided into voxel-based~\cite{seitz1999photorealistic, kutulakos2000theory, de1999poxels} and point-based methods~\cite{galliani2015massively, barnes2009patchmatch,schonberger2016pixelwise,schonberger2016structure}. 
Voxel-based methods divide the 3D space into voxels and determine which ones belong to the object. However, due to the cubic space complexity, these methods are computationally expensive and may not be suitable for reconstructing complex objects. Point-based methods are memory efficient, but they lack connectivity information, which can lead to incomplete or inaccurate reconstructions.


\noindent\textbf{Neural radiance fields}~\cite{mildenhall2021nerf,zhang2020nerf++, barron2021mip} have demonstrated remarkable results in representing complex 3D scenes from only 2D images as input. However, due to the discrete nature of the radiance field, their geometric reconstructions often suffer from inaccuracies. To improve the capability of geometry reconstruction, several techniques have been proposed to replace the density-based representation by a geometry-oriented representation. For example, UNISURF~\cite{oechsle2021unisurf} uses occupancy fields, while NeuS~\cite{wang2021neus} and VolSDF~\cite{yariv2021volume} use signed distance fields. 

\noindent\textbf{Neural implicit representations} have garnered significant attention in 3D reconstruction due to their representation power and memory efficiency~\cite{park2019deepsdf, sitzmann2019scene}. DVR~\cite{niemeyer2020differentiable} and IDR~\cite{yariv2020multiview} focus on making the surface rendering pipeline differentiable. These methods often require masks to distinguish objects from the background. Recent works focus on improving the applicability and representation capability of neural implicit functions.
Geo-NeuS~\cite{fu2022geo} introduces multi-view constraints from structure from motion to encourage SDF networks to avoid geometry ambiguity under rendering loss. D2IM-Net~\cite{li2021d2im} adopts an implicit displacement field for recovering geometric details from a single input image. 

IDF~\cite{yifan2021geometry} represents complex surfaces as a smooth base surface and a displacement mapping.
%, \textcolor{red}{ trained on both SDF values and normals.} 
HF-NeuS~\cite{wang2022improved} decomposes the SDF into a base function and a displacement function with a coarse-to-fine strategy to gradually increase high-frequency details. DIF~\cite{deng2021deformed} deforms a target object to match the template shape and employs a correction field for handling topological changes.

\noindent\textbf{3D morphable face model} (3DMM)~\cite{blanz1999morphable} has been extensively used in 3D face reconstruction and animation. It is a statistical model that represents the shape and texture of a human face using a low-dimensional vector space. 

The combination of 3DMM with deep learning has proven effective in producing high-quality 3D faces~\cite{li2022implicit,zheng2022imface}, however it often requires 3D supervision to achieve good performance. Recent works also aim to improve the representation capability of 3DMM to the entire head, using multi-view images~\cite{yenamandra2021i3dmm,hong2022headnerf,zhuang2022mofanerf, wang2022morf} and monocular videos~\cite{zheng2022avatar}.
We refer the readers to a comprehensive survey of 3D morphable model and recent developments~\cite{egger20203d}.


\section{Method}
\label{sec:method}

\subsection{Overview}

\begin{table}[h]\centering
\setlength\tabcolsep{2pt}
\caption{Notation Table.}
%\vspace{-0.1in}
    \label{tab:notation}
    \begin{small}
    \begin{tabular}{lp{2.5in}}
    %\begin{tabular}{ll}
        \hline
        Symbol & Meaning\\
        \hline
        \hline 
         $I_i$ &  the input images with camera parameters\\
         \hline
         %$\pi_i$ & camera parameters \\
         $f_{\text{geo}}$ &the Geometry Network \\
         $f_\text{tem}$  &the Template Network \\
         $f_\text{def}$ &the Deformation Network \\
         $f_{\text{ren}}$ & the Rendering Network \\
         $f_{\text{dis}}$  & the Displacement Network \\
          \hline
         $\mathbf{z}_s, \mathbf{z}_c\in\mathbb{R}^{128}$ & identity-dependent latent codes for shape and color  \\
$\mathbf{F}_{\text{def}}\in\mathbb{R}^{192}$ & identity-dependent feature associated with non-rigid deformation\\
         $\mathbf{F}_{\text{tem}}\in\mathbb{R}^{64}$ & identity-independent feature associated with the template head\\
         $\mathbf{F}_{\text{dis}}\in\mathbb{R}^{64}$ & ID-dep. geometry feature associated with displacement\\
         
         $\mathbf{F}_{\text{all}}\in\mathbb{R}^{320}$ & \small{the overall feature fed into the Rendering Network in Stage 2, which is the concatenation of $\mathbf{F}_\text{def}$,  $\mathbf{F}_\text{tem}$, and $\mathbf{F}_\text{dis}$}\\
         \hline
         $\mathbf{x}\in\mathbb{R}^3$ & a query point in the observation space\\
         $\mathbf{d}\in\mathbb{R}^3$ & an offset vector indicating the deformation from an individual to the template \\
         $\mathbf{x+d}\in\mathbb{R}^3$  & a query point in the template space\\
         $s\in\mathbb{R}$ & signed distance \\
         $\mathbf{n}_b, \mathbf{n}_f\in \mathbb{R}^3$ & normal vectors of the base and final surfaces\\
         $\delta\in\mathbb{R}$ & an implicit displacement\\
         $c\in\mathbb{R}^3$ & radiance\\
         $C\in\mathbb{R}^3$ & rgb color\\
        \bottomrule
    \end{tabular}
    \end{small}
\end{table}

We have a set of RGB portrait images $\mathcal{I} = \{I_{i}\}$ for $m$ individuals. Each individual is captured from  $k$ different viewpoints covering the front, left, and right sides. Additionally, each image is  associated with camera parameters $\pi_i$. Our goal is to learn a signed distance field for the 3D geometry of the human head and a radiance field for colors for each individual. To obtain high-fidelity geometry and appearance, our method decomposes the geometric representation of 3D human heads into identity-independent and -dependent components. 

The ID-independent component is a smooth base surface that represents the common geometric characteristics of human heads and serves as a template. Since the ID-independent and expression-neutral template is the standard reference for all individuals. 
The ID-dependent components include 
a non-rigid deformation that maps each individual head to the template and a displacement field that encodes high-frequency geometric details such as wrinkles and small facial features that are particular to a given person. 

As shown in Figure~\ref{fig:pipelineOur}, our method employs a two-stage training framework for reconstructing 3D geometry and colors in a coarse-to-fine manner. The first stage is to train a geometry network for learning the template and a point-wise non-rigid deformation between an individual head and  the template head. In the second stage, we train a displacement network for learning identity-related fine geometric details to further improve the reconstruction quality. 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=6.4in]{./images/arch_pipeline_modify_noPhoto.pdf}
    %\vspace{-0.1in}
    \caption{Data flow and network architecture. Our model reconstructs 3D geometry and colors of human heads in a two-stage, coarse-to-fine manner. In the first stage, we optimize the Template Network and the Deformation Network, along with the latent code space and Rendering Network, across different identities to obtain an identity-independent and expression neutral template. In the second stage, we introduce a Displacement Network and further optimize it with a set of portrait images of a specific subject. }
    \label{fig:pipelineOur}
\end{figure*}


\subsection{Multi-Person Coarse Reconstruction}

\noindent\textbf{Geometry network.} To learn a template head from multiple individuals, we follow i3DMM~\cite{yenamandra2021i3dmm} to design the Geometry Network $f_{\text{geo}}$ with two components: a Template Network $f_{\text{tem}}$ and a Deformation Network $f_{\text{def}}$. The Template Network is designed to obtain common features across all identities, and its output is a mean face of all identities used in training. 
On the other hand, the Deformation Network aims to find correspondences from each specific subject to the mean face and learn features associated with the identity.

To model each identity separately, we introduce a shape code $\mathbf{z}_{s}$ and a color code $\mathbf{z}_{c}$, which are learned in the shape and color embedding spaces, respectively. Specifically, with the shape code $\mathbf{z}_{s}$
representing an identity, the Geometry Network $f_{\text{geo}}$ learns a global SDF for each 3D point $\mathbf{x} \in \mathbb{R}^{3}$ and generates a geometric feature $\mathbf{F}_{\text{geo}}$ for $\mathbf{x}$ that will be used to learn the displacement map and radiance information.
Then the surface normal $\mathbf{n}_b$, viewing direction $\mathbf{v}$, and geometric feature $\mathbf{F}_{\text{geo}}$ at $\mathbf{x}$ are fed into the Rendering Network $f_{\text{ren}}$, which predicts the radiance for the ray. The output of Stage 1 is an identity-independent and expression-neutral template. 

Specifically, given a query point $\mathbf{x} \in \mathbb{R}^3$ in the observation space and an identity-related latent code for geometry $\mathbf{z}_s\in\mathbb{R}^{128}$, the Deformation Network $f_\text{def}$ predicts an offset vector $\mathbf{d} \in \mathbb{R}^3 $ that maps $\bf x$ to the template space and also outputs an ID-dependent feature $\mathbf{F}_{\text{def}} \in \mathbb{R}^{192}$
\begin{equation}
     f_{\text{def}}(\mathbf{x}, \mathbf{z}_{s})=(\mathbf{d}, \mathbf{F}_{\text{def}})\label{eq:deform}.
\end{equation}    
Then the Template Network returns a signed distance value $s \in \mathbb{R}$ for the deformed position $\mathbf{x+d}$ in the template space and a template feature $\mathbf{F}_{\text{tem}} \in \mathbf{R}^{64}$ for the deformed position $\mathbf{x}+\mathbf{d}$
\begin{equation}
f_{\text{tem}}(\mathbf{x} + \mathbf{d})=(s, \mathbf{F}_{\text{tem}}).\label{eq:template}
\end{equation}
The identity-independent template feature $\mathbf{F}_{\text{def}}$ will be concatenated with the 
ID-dependent deformation feature $\mathbf{F}_{\text{def}}$ and used in the subsequent Rendering Network.
Putting it all together, we formulate the Geometry Network as follows
\begin{equation}
    f_{\text{geo}} = f_{\text{tem}} \circ f_{\text{def}}(\mathbf{x}, \mathbf{z}_{s}) \label{eq:f-geo-first-stage}.
\end{equation}


\noindent\textbf{Rendering network.} To learn the SDF $s$ from a set of 2D images $\mathcal{I}$ with camera parameters, we cast rays from the camera position $\bf o$ to each pixel of the input images. Consider a ray $\mathbf{r}(t)=\mathbf{o}+t\mathbf{v}$, where $\bf v$ is a unit vector and $t\geq 0$ is the arc-length parameter.
The Rendering Network, denoted as $f_{\text{ren}}$, 
is responsible for computing the radiance at any query point $\mathbf{x}\in \mathbf{r}(t)$. 

To consider the variety of textures among different persons in the dataset, the network also takes an ID-dependent latent code for color $\mathbf{z}_c$, the template feature $\mathbf{F}_{\text{tem}}$, the deformation feature $\mathbf{F}_{\text{def}}$, and the normal of the base surface $\mathbf{n}_b$ as input~\cite{liu2021editing} and outputs the radiance $c\in\mathbb{R}^{3}$
\begin{align}
    f_{\text{ren}}(\mathbf{z}_{c}, \mathbf{x}, \mathbf{v}, \text{concat}(\mathbf{F}_{\text{def}},\mathbf{F}_{\text{tem}}), \mathbf{n}_b)=c,\label{eq:render1}
\end{align}
where $\mathbf{n}_b=\nabla s$ is the gradient of the SDF of the base surface given an individual, representing the normal direction. 

To compute the radiance, we first transform the SDF into an S-density $\sigma$ as in~\cite{yariv2021volume}
\begin{align}
    \sigma(\mathbf{x}) = \alpha \Phi_\beta(-s(\mathbf{x})),\label{eq:s-density}
\end{align}
where $\alpha$ and $\beta$ are learnable parameters and $\Phi_\beta$ is the cumulative distribution function of the Laplace distribution with zero mean and $\beta$ scale. We query the radiance $c_i$ and the signed distance $s_i$ for a set of samples $\{\mathbf{r}(t_i)\}$ along $\bf r$. The color of the pixel $C(\mathbf{r})$ on the image is obtained by accumulating all the samples along the ray
\begin{displaymath}
    C(r) = \sum_{i} T_{i}\left(1-\exp \left(-\sigma_{i} u_i\right)\right) c_{i},
\end{displaymath}
where $c_i$ is the radiance computed by the Rendering Network, $T_{i} = \exp \left(-\sum_{j=1}^{i-1} \sigma_{j}u_j\right)$ is the transparency indicating the probability that the ray travels from $0$ to $t_i$ without hitting any other particle, and $u_i = t_{i+1}-t_i$ is the distance between adjacent samples.

\subsection{Single-Person Refinement}
% \subsection

With the pre-trained template head available, Stage 2 of our method aims to refine the SDF to learn fine geometric details for a specific individual. 
Notice that the template together with the non-rigid deformation computed in Stage 1 does not suit our purpose. The reason is two-folded. Firstly, the deformation alone does not provide sufficient degrees of freedom for modeling high-frequency details. Secondly, the template represents the mean shape of all individuals in the dataset and does not carry ID-dependent features for a specific individual. For example, some individuals have a scarf while others do not.  

To address this issue, we propose a high-frequency displacement map that captures the ID-dependent geometric details of each specific individual. To achieve this, we introduce the Displacement Network, which takes both the ID-independent template feature $\mathbf{F}_{\text{tem}}$ and the ID-dependent deformation feature $\mathbf{F}_{\text{def}}$ as inputs, along with the query position $\mathbf{x}$.

The network then outputs a displacement $\delta \in \mathbb{R}$ and a displacement feature $\mathbf{F}_{\text{dis}} \in \mathbb{R}^{64}$, which is ID-dependent
\begin{equation}
    f_{\text{dis}}(\mathbf{x}, \mathbf{F}_{\text{tem}}, \mathbf{F}_{\text{def}})=(\delta, \mathbf{F}_{\text{dis}}),\label{eq:displace}.
\end{equation}
We use the displacement to update the signed distance. Specifically, for the query point $\mathbf{x}$ in the observation space, the updated signed distance is
\begin{equation}
    \hat{s}(\mathbf{x}) = s(\mathbf{x})+\delta(\mathbf{x}) = f_{\text{tem}}(\mathbf{x}+\mathbf{d}) + \delta(\mathbf{x}).\label{eq:final-sdf}
\end{equation}
The displacement $\delta$ is a non-zero value if the query point $\bf x$ is on the region with fine geometric details, such as wrinkles; otherwise, for points $\bf x$ are on a relatively smooth region, such as cheek, $\delta$ is close to zero. Since we expect the template undergoing the ID-dependent non-rigid deformation recovers most of the shape and the displacement is to add fine details, $\delta$ should be small and smooth. We regularize $\delta$ by using both the absolute value term $|\delta|$ to control the significance and a total variation (TV) term $|\nabla\delta|$ to ensure the smoothness. 

The displacement feature $\mathbf{F}_{\text{dis}}$ is designed to further enrich the ID-dependent features of the individual and help capture more fine-grained geometric details. Therefore, the overall feature $\mathbf{F}_{\text{all}}$ is a concatenation of the deformation feature $\mathbf{F}_{\text{def}}$, the template feature $\mathbf{F}_{\text{tem}}$ and the displacement feature $\mathbf{F}_{\text{dis}}$, i.e., 
\begin{equation}\mathbf{F}_{\text{all}}=\text{concat}(\mathbf{F}_{\text{def}},\mathbf{F}_{\text{tem}},\mathbf{F}_{\text{dis}}).
\label{eqn:allfeatures}
\end{equation}

Note that the Rendering Network $f_{\text{ren}}$ is used in both stages to train the template and the final shape separately. In Stage 1, we feed the deformation and the template features $\text{concat}(\mathbf{F}_{\text{def}}, \mathbf{F}_{\text{tem}})$ into it for computing the color for the specific human head. In Stage 2 we feed the overall feature \begin{equation}
 f_{\text{ren}}(\mathbf{z}_{c}, \mathbf{x}, \mathbf{v}, \mathbf{F}_{\text{all}}, \mathbf{n}_f)=c\label{eq:render2}
 \end{equation}
 into the network to obtain the radiance for a specific individual,
 where $\mathbf{n}_f=\nabla\hat{s}$ is the normal direction of the final surface. Our ablation study confirms that the displacement map is effective in reconstructing ID-dependent details or features that are absent from the template. 


\subsection{Training}

All the components, $f_{\text{def}}$, $f_{\text{tem}}$, $f_{\text{dis}}$ and $f_{\text{ren}}$, are multi-layer perceptrons. The Deformation Network, the Template Network and the Displacement Network consist of 4, 8, and 4 layers, respectively. The Rendering Network is used in both stages and they are slightly different. In Stage 1, it has 4 layers with positional encoding of 6 and 4 frequencies for point coordinates and viewing directions, respectively. To deal with high-frequency details in Stage 2, we add 2 more layers for the Rendering Network and also increase the number of frequencies by 2 for both point locations and views. We also use a skip-connection to concatenate the input to the 3rd layer in order to strength the relationship between the input variables and the output radiance. All hidden layers have a width of 256.


Our training does not involve 3D supervision, and uses only the pixel colors to guide the training. The color loss is 
\begin{equation}
\mathcal{L}_{\text{col}} = \lambda_1\|C - C_{\text{gt}} \|_{1},\label{eq:color_loss}\\
\end{equation}
where $C_{\text{gt}}$ is the ground-truth color.

We adopt the following regularization terms. Since all human heads are of similar geometry and sizes, the deformation $\mathbf{d}$ should be smooth and not be too large. So we define the deformation loss term as
\begin{align}
     \mathcal{L}_\text{def} &= \lambda_{2}\| \mathbf{d} \|_{2} + \lambda_{3} \| \nabla \mathbf{d}\|_{2}\label{eq:deform_loss}
\end{align}
by regularizing the magnitudes of the offset vector and its gradient.
 
The template $s$ is a signed distance field, whose gradient satisfies the Eikonal equation $\|\nabla s\|_2=1$, therefore we use the Eikonal term as in ~\cite{gropp2020implicit}
\begin{equation}
     \mathcal{L}_{\text{eik}} = \lambda_{4} \left(\|\nabla s\|_{2}-1\right)^{2}\label{eq:eik} 
\end{equation}
to regularize the SDF.

To regularize the displacement map $\delta$, we consider the following situation:
let $\mathbf{p}$ be a point on the deformed template, i.e., 
$s(\mathbf{p})=0$. Denote by $\mathbf{p}'$ the corresponding point of $\bf p$ such that 1) it is along the normal direction of $\bf p$ 
\begin{displaymath}    \mathbf{p}'=\mathbf{p}+\lambda \frac{\nabla s(\mathbf{p})}{\|\nabla s(\mathbf{p})\|}, 
\end{displaymath}
where $\lambda\in\mathbb{R}$ is the signed distance between $\mathbf{p}'$ and $\bf p$, indicating how far we should move $\bf p$ along the normal; 
and 2) it is on the final surface, i.e., 
$$
s(\mathbf{p}')+\delta(\mathbf{p}')=0.
$$
Using Taylor expansion, we obtain
$$
s(\mathbf{p}')=\lambda \|\nabla s(\mathbf{p})\|,
$$
which implies that the displacement $\delta$ satisfies
$$
\delta(\mathbf{p}')=-\lambda\|\nabla s\|.
$$
We expect that the distance between $\bf p$ and $\mathbf{p}'$ is small and the displacement itself is also smooth. Therefore, we define the following displacement loss term 
\begin{equation}
 \mathcal{L}_{\text{dis}} = \lambda_{5} |\delta| + \lambda_{6} \| \nabla \delta \|_1.\label{eq:displace_loss}
\end{equation}
The first term in Equation (\ref{eq:displace_loss}) restricts the size of the displacement, while the second term, which is the total variation (TV)~\cite{osher2003image} of the displacement map $\delta$,  is to keep $\delta$ smooth while preserving fine details. 


Following DIF~\cite{deng2021deformed} and DeepSDF~\cite{park2019deepsdf}, we regularize the latent codes by assuming a Gaussian distribution $$\mathcal{L}_{\text{cod}}=\lambda_{6} \left(\|\mathbf{z}_{s}\|_{2} + \|\mathbf{z}_{c}\|_{2}\right).$$ 

Putting it all together, we define the loss function as follows:
\begin{align}
 \mathcal{L} &= \mathcal{L}_{\text{col}} +  \mathcal{L}_{\text{eik}} +  \mathcal{L}_{\text{def}} + \mathcal{L}_{\text{dis}} + \mathcal{L}_{\text{cod}}. 
\end{align}
In our implementation, we empirically set the weights as 
$\lambda_1=0.01$ and $\lambda_2=\cdots=\lambda_6=0.001$.


\subsection{Properties}
Our network offers two unique properties.

Firstly, limited viewpoints often result in missing/insufficient information, which can significantly downgrade the quality of existing methods. Our approach addresses this challenge through geometry decomposition. By training the template on individuals within the same group, who share significant geometric similarities, we obtain a smooth mean shape that represents the main geometric features of human heads. Furthermore, the randomly selected views of different individuals can complement each other to optimize the template geometry. As a result, the pre-trained template serves as a good initialization for stage 2 training, facilitating the learning of high-frequency details. 

Secondly, we have observed that learning both the smooth base surface and high-frequency details simultaneously in a low-view setting is highly unstable. For example, HF-NeuS frequently fails to accurately reconstruct geometry under the setting of only 10 views. In contrast, our method trains the network in a two-stage, coarse-to-fine manner, which effectively increases robustness. By establishing a solid foundation with the pre-trained template in stage 1, it is much easier to learn the high-frequency, ID-dependent details (such as wrinkles and scarfs) in stage 2. 

The proposed geometry decomposition and coarse-to-fine training enable our method to effectively complement missing information and produce high-quality reconstructions even in challenging low-view settings.



\subsection{Discussions}
While there are similar neural rendering models, such as DIF~\cite{deng2021deformed}, HF-NeuS~\cite{wang2022improved} and IND~\cite{li2022implicit}, our method differs from them in both design principles and application domains.

The Implicit Neural Deformation (IND) method~\cite{li2022implicit} consists of a template network, a deformation network and a rendering network. Using 3D supervision of a prior 3D face model, it can reconstruct 3D faces from sparse views, typically 2 to 4, but it is limited to only reconstructing the facial region. Since IND does not model high-frequency signals, the reconstructed facial regions do not carry additional high-frequency information beyond the template. Another significant difference is the way geometric features are fed to the rendering network. In IND, the geometric feature is the output of the template network, which cannot distinguish identities. Our method decomposes the geometric feature for each point into a template feature, which is identity independent and shared across multiple persons, a deformation feature, and a displacement feature, which are identity dependent. The feature decomposition property enables us to recover the unique geometric characteristics associated with a specific identity that are unavailable on the template, for example, scarves and wrinkles. IND is not able to do that. Furthermore, since obtaining 3D priors is often difficult in practice, our method, which does not rely on 3D supervision, is more flexible. 



The Deformed Implicit Field (DIF) method~\cite{deng2021deformed} is a shape modeling approach that utilizes a template, a non-rigid deformation and a correction field to model a group of 3D shapes with similar characteristics. Unlike our method, DIF aims at reconstructing 3D shapes from point clouds, and uses the correction field to update the topology of the template SDF and regularizes it with an $L_1$ loss term for controlling its size. In contrast, our method utilizes a displacement map for modeling high-frequency details, and regularizes it using both $L_1$ and total variation losses. Additionally, our method does reconstruction from multiview images rather than point clouds.



High-frequency NeuS~\cite{wang2022improved} improves the quality of surface reconstruction in NeuS~\cite{wang2021neus} by introducing a high-frequency displacement function. It learns frequencies at multiple scales and gradually increases the frequency content in a coarse-to-fine manner. However, unlike our method, it does not use template and trains the network in a single stage. Although it performs well for dense view inputs, we observed that HF-NeuS is unstable under low-view settings, as missing information in the input images leads to the network focusing more on improving appearance rather than learning correct geometry. In contrast, our method tackles the challenge through geometry decomposition and a two-stage training strategy, allowing us to produce high-quality 3D reconstruction with low-view inputs. 


\section{Experiments}
\noindent\textbf{Datasets.} The Portrait Relighting Dataset~\cite{wang2020single} contains 438 subjects ranging in age from 17 to 69 years old. Each subject was photographed from 30 different views around their frontal face and the images are at a resolution of 512$\times$512. The dataset includes associated camera parameters, poses and masks for easy foreground extraction. High-fidelity 3D geometry and textures were reconstructed using commercial software PhotoScan, which are considered ground-truth. For our experiments, we selected two subsets of 15 individuals each from the dataset: PR-Senior, which consists of senior individuals with rich geometric features such as wrinkles, and PR-Young, which consists of randomly selected younger individuals from the remaining dataset. We used these subsets to evaluate our proposed method.

\noindent\textbf{Implementation details.} We implemented our model using PyTorch ~\cite{Paszke2019PyTorchAI} and
used the Adam optimizer~\cite{kingma2015ba} to update the learnable parameters with an initial learning rate $5\times 10^{-4}$ in an exponential decay strategy. The latent codes for shape and color are of dimensions $\mathbf{z}_{s}$, $\mathbf{z}_{c} \in \mathbb{R}^{128}$. We follow VolSDF~\cite{yariv2021volume} to set the other hyper-parameters, such as the learnable parameters $\alpha$ and $\beta$ in Equation (\ref{eq:s-density}) and the number of samples on a ray. In the first stage, we trained the model for the 30 subjects with 10 views for 5K epochs. In the second stage, we trained the model for 300K iterations for each specific person from 10 to 20 views. 

\begin{table*}[!h]
\setlength\tabcolsep{2pt}
\scriptsize
\centering
\caption{Quantitative results for the PR-Senior and PR-Young datasets. To make fair comparison, we train all models using the same epochs. Since 3D ground truths are only available for facial regions, we crop the reconstructed 3D meshes before calculating Chamfer distances ($\downarrow$). We also apply face masks to the rendered images for calculating PSNR ($\uparrow$). The subscripts t and n denote the training and novel views, respectively. The Camber distances are measured in units of $10^{-4}$. Note that VolSDF and HF-NeuS fail to reconstruct proper geometry for 6 and 35 out of the 90 tests (30 models, 3 different view settings), respectively. We removed these models when calculating the Chamfer distances. NeuS and ours successfully reconstruct all models. See the supplementary material for more results.}
\label{tab:face_eval}
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multirow{3}{*}{Method} &\multicolumn{9}{c|}{PR-Senior} &\multicolumn{9}{c}{PR-Young}\\
\cmidrule[0.5pt](rl){2-19}
& \multicolumn{3}{c|}{10 views}  & \multicolumn{3}{c|}{15 views}&\multicolumn{3}{c|}{20 views} 
& \multicolumn{3}{c|}{10 views}  & \multicolumn{3}{c|}{15 views}&\multicolumn{3}{c}{20 views} \\
\cmidrule[0.5pt](rl){2-19}
&CD & PSNR$_\text{t}$  & PSNR$_\text{n}$ & CD  & PSNR$_\text{t}$ & PSNR$_\text{n}$  & CD & PSNR$_\text{t}$  & PSNR$_\text{n}$ &CD & PSNR$_\text{t}$  & PSNR$_\text{n}$ & CD  & PSNR$_\text{t}$ & PSNR$_\text{n}$  & CD & PSNR$_\text{t}$  & PSNR$_\text{n}$ \\
\midrule
NeuS &1.795&34.55&23.93&2.626&34.44&28.50&0.956&34.37&28.96
&1.478&35.22&23.85&1.107&35.05&26.10&1.236&34.93&26.85 \\
VolSDF &3.336&33.56&23.41  &2.483&33.46&26.07&2.279&33.41&28.09
&2.282&33.84&22.24&2.251&33.94&25.26&1.805&33.95&25.94\\
HF-NeuS &2.471&\textbf{34.81}&21.24&1.740&\textbf{34.77}&25.25&0.866&\textbf{34.70}&27.26
&2.724&\textbf{35.51}&20.29&1.885&\textbf{35.24}&21.75&0.831&\textbf{35.33}&26.50\\
Ours &\textbf{0.785}&33.78&\textbf{27.89}&\textbf{0.689}&33.82&\textbf{28.73}&\textbf{0.708}&33.55&\textbf{30.60}
&\textbf{0.930}&34.09&\textbf{26.80}&\textbf{0.784}&34.16&\textbf{27.75}&\textbf{0.818}&33.94&\textbf{30.13}\\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}
    \centering
    \rotatebox{90}{\begin{tiny}{\textbf{training view}}\end{tiny}}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/gt_571_30.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/neus_10_571_30_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/hfs_10_571_30_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/volsdf_10_571_30_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/ours_10_571_30_render.jpg}
    \\
    %\rotatebox{90}{\scriptsize train view normal}
    \rotatebox{90}{\scriptsize}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/gt_571_blank.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/neus_10_571_30_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/hfs_10_571_30_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/volsdf_10_571_30_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/ours_10_571_30_normal.jpg}
    \\
    \rotatebox{90}{\begin{scriptsize}\textbf{novel view}\end{scriptsize}}\includegraphics[width=0.18\columnwidth]{./results/template_effects/gt_571_18.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/neus_10_571_18_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/hfs_10_571_18_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/volsdf_10_571_18_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/ours_10_571_18_render.jpg}
    \\
    %\rotatebox{90}{\scriptsize novel view normal}
    \rotatebox{90}{\scriptsize}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/gt_571_blank.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/neus_10_571_18_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/hfs_10_571_18_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/volsdf_10_571_18_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/ours_10_571_18_normal.jpg}
    \\
    \makebox[0.18\columnwidth]{\scriptsize GT}
    \makebox[0.18\columnwidth]{\scriptsize NeuS}
    \makebox[0.18\columnwidth]{\scriptsize HF-NeuS}
    \makebox[0.18\columnwidth]{\scriptsize VolSDF}
    \makebox[0.18\columnwidth]{\scriptsize Ours}
    \label{fig:template_effect}
    %\vspace{-0.1in}
    \caption{Our method leverages a template in neural rendering, which makes the trained model more resilient to noise. Even when some of the training views have incomplete information about the human head, our method is able to complete the shape with reasonable geometry. Furthermore, our novel view result is closer to the ground truth than that of existing methods, demonstrating the effectiveness of using a template. Model: 571, 10 views, 4 are incomplete due to unusual viewing angles and cropping.
    }
    \end{figure}

\begin{figure}[htb] \centering
    \rotatebox{90}{\begin{tiny}\textbf{training view}\end{tiny}}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/gt_377_20.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/neus_10_377_20_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/hfs_10_377_20_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/volsdf_10_377_20_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/ours_10_377_20_render.jpg}
    \\
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/gt_571_blank.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/neus_10_377_20_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/hfs_10_377_20_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/volsdf_10_377_20_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_train_compare/ours_10_377_20_normal.jpg}
    \\
    \rotatebox{90}{\begin{scriptsize}\textbf{novel view}\end{scriptsize}}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/gt_377_12.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/neus_10_377_12_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/hfs_10_377_12_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/volsdf_10_377_12_render.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/ours_10_377_12_render.jpg}
    \\
    \includegraphics[width=0.18\columnwidth]{./results/template_effects/gt_571_blank.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/neus_10_377_12_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/hfs_10_377_12_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/volsdf_10_377_12_normal.jpg}
    \includegraphics[width=0.18\columnwidth]{./results/10_view_novel_compare/ours_10_377_12_normal.jpg}
    \\
    \makebox[0.18\columnwidth]{\scriptsize GT}
    \makebox[0.18\columnwidth]{\scriptsize NeuS}
    \makebox[0.18\columnwidth]{\scriptsize HF-NeuS}
    \makebox[0.18\columnwidth]{\scriptsize VolSDF}
    \makebox[0.18\columnwidth]{\scriptsize Ours}
    %\vspace{-0.1in}
    \caption{Our method excels in reconstructing fine details compared to other methods, thanks to the use of a displacement map for capturing high-frequency signals. We demonstrate the effectiveness of our approach in reconstructing fine details in a training view result (top) and a novel view result (bottom) here. Model: 377, 10 views.
    } 
    \label{fig:10view_detail_facial}
\end{figure}


\noindent\textbf{Baselines.} We conducted a comparative evaluation of our method with state-of-the-art neural rendering methods, including NeuS~\cite{wang2021neus}, VolSDF~\cite{yariv2021volume}, and HF-NeuS~\cite{wang2022improved}, on PR-Dataset. We used the official implementations provided by the authors for all methods, and set the same 1,024 rays in all experiments to ensure a fair comparison. 

\noindent\textbf{Quality measures.} To assess the performance of our method and other approaches, we utilized the marching cubes algorithm~\cite{lorensen1987marching} to extract the zero-level set from the computed signed distance field. We cropped both the ground-truth and predicted meshes to focus on the facial region of interest, and computed the Chamfer distance (CD) between them to evaluate geometric quality. Additionally, we applied the face masks to the rendered RGB images to calculate the PSNR for the facial regions for evaluating visual appearance. The results are reported in Tables \ref{tab:face_eval}.


\noindent\textbf{Geometry accuracy.} Both NeuS and our method were able to succesffully reconstruct the geometry of 3D heads for all testing models, while VolSDF and HF-NeuS failed to reconstruct the geometry of human heads in in a significant number of examples under the setting of 10 to 15 views. 
Therefore, we \textbf{excluded} these failed examples when calculating the Chamfer distances for VolSDF and HF-NeuS. Since their rendered images are still visually acceptable for training views, we used all results for computing the PSNR metrics. Our method consistently outperformed all other methods in terms of geometry quality on both the PR-Senior and PR-Young datasets by a large margin with 10 to 15 views as input, demonstrating our method's ability to effectively overcome the challenge of missing/insufficient information in low-view settings. With 20 views as input, the gap became less significant, but our method still performed better than other methods. 

\noindent\textbf{Visual appearance.} We also observed that although our method did not achieve the highest PSNR score for rendering results on the \textbf{training} views, it outperformed the other methods for rendering \textbf{novel} views. This is because novel view synthesis is more dependent on the accuracy of the underlying 3D geometry than training views. Our method's superior geometry quality leads to better novel view synthesis results, demonstrating the effectiveness of our approach.

\noindent\textbf{Robustness.} While HF-NeuS also adopts a displacement field, it typically achieves the best performance with a sufficient number of views. However, as the number of views decreases, the 3D reconstruction quality often degrades significantly. For instance, under the setting of 10 views, HF-NeuS failed to reconstruct 3D geometry for 19 persons out of the 30 individuals in the PR-Senior and -Young datasets, resulting in poor novel view synthesized results. Although increasing the views to 20 improved the reconstruction quality considerably, HF-NeuS still could not reconstruct the geometry accurately for Models 487, 548, and 608. In contrast, our method is robust and performs consistently well under the same low-view settings, thanks to the use of a pre-trained template and the two-stage training strategy. 


\noindent\textbf{Analysis.} 
Our method outperformed the existing methods in terms of geometry measures and novel view synthesis results in low-view settings. This can be attributed to three reasons. 1) Firstly, in the first stage of our method, we trained the template using a multi-person dataset. Since the selection of viewpoints for the subjects is random, the different views can complement each other, resulting in better facial geometry for the template. 2) Secondly, utilizing a pre-trained template increases the robustness of our model and enables it to resist a certain degree of ``noise'' caused by the limited number of viewpoints in the second stage of our method. For instance, when the images in the training set have certain disturbances to the reconstructed object, such as the cropping at the top of the head and clothing covering the neck as shown in Figure~\ref{fig:template_effect}, it can mislead other methods due to the lack of information, thereby producing incorrect geometry to complete the missing parts. To improve accuracy, the existing methods require more views to figure out the missing or occluded parts. In contrast, our method successfully modeled the top of the head and the neck of the character only 10 views, thanks to the use of the template, which provides a reasonable guide. 3) Thirdly, in the case of senior individuals with rich wrinkles, the displacement field plays a crucial role in modeling high-frequency geometry, resulting in better reconstruction of fine geometric details, as demonstrated in Figure \ref{fig:10view_detail_facial}.



\begin{figure}[htbp]
\setlength\tabcolsep{0pt}
\centering
\begin{scriptsize}
\begin{tabular}{ccccc}
    \centering
        \rotatebox{90}{\textbf{training view}} &

\includegraphics[width=0.1\textwidth]{./results/hfs_fail/gt_548_15.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/neus_10_548_15_render.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/hfs_10_548_15_render.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/ours_10_548_15_render.jpg}
    \\
  &
    \includegraphics[width=0.1\textwidth]{./results/template_effects/gt_571_blank.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/neus_10_548_15_normal.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/hfs_10_548_15_normal.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/ours_10_548_15_normal.jpg}
    \\
    \rotatebox{90}{\textbf{novel view}}&
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/gt_376_26.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/neus_10_376_26_render.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/hfs_10_376_26_render.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/ours_10_376_26_render.jpg}
    \\
    &\includegraphics[width=0.1\textwidth]{./results/template_effects/gt_571_blank.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/neus_10_376_26_normal.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/hfs_10_376_26_normal.jpg} &
    \includegraphics[width=0.1\textwidth]{./results/hfs_fail/ours_10_376_26_normal.jpg}
    \\
    & 
    (a) GT & (b) NeuS & (c) HF-NeuS & (d) Ours\\
    \end{tabular}
    \end{scriptsize}
    %\vspace{-0.1in}
    \caption{
    HF-NeuS simultaneously learns the base surface and high-frequency details, making it unstable under a low-view setting due to the insufficient information provided in the images. This instability leads to incorrect novel view results. NeuS can reconstruct the head geometry fairly well but struggles to fully recover fine details and can yield low-quality results for novel views. In contrast, our proposed method uses geometry decomposition and coarse-to-fine training, which effectively overcomes these challenges and produces accurate results in low-view settings. Models: 548 (top) and 376 (bottom).}
    \label{fig:hfs_fail}
\end{figure}


\begin{table}[htbp]
    \centering
    \begin{tabular}{c|c|c|c}
    \hline
        Model & CD ($10^{-4}$) & PSNR$_\text{t}$ & PSNR$_\text{n}$  \\
        \hline
        539 & 3.861 & 32.29 & 22.69 \\
        662 & 1.643 & 33.21 & 25.42\\
        \hline
    \end{tabular}
    \caption{Performance on two unseen identities under 10 views. See Figure~\ref{fig:unseen} for the visual result on Model 539 and the supplementary material for the other model.  }
    \label{tab:unseen_metric}
\end{table}

\noindent\textbf{Unseen identities.} Our method has the ability to adapt to new individuals as the pre-trained template serves as a good initialization. Table~\ref{tab:unseen_metric} reports its performance on two unseen identities and Figure~\ref{fig:unseen} shows a visual result. 


\noindent\textbf{Ablation studies.} To evaluate the impact of the displacement field, we conducted an ablation study by training a model without the displacement field in Stage 2 and comparing it to the proposed model with the displacement field. Both models were trained using the same hyperparameters and training data under the setting of 10 views. As shown in Table~\ref{table:ablation_disp}, the proposed model with the displacement field outperforms the model without it in terms of geometry accuracy. This indicates that the displacement field plays a crucial role in improving the accuracy of our method. Figure~\ref{fig:ablation_dis} provides visual evidence for this by showing that non-rigid deformation alone cannot capture the fine details and features that are absent in the template. Therefore, the displacement map serves as a necessary supplement to enhance the accuracy of the reconstruction. We also conducted an evaluation of the impact of the TV regularization term on the reconstruction of Model 619, a senior female with rich wrinkles, under the setting of 10 views. Figure~\ref{fig:ablation_dis} (row 3) shows that the TV regularizer $\|\nabla\delta\|_1$ can effectively reduce noise while preserving fine details in the reconstruction.
    
\begin{table}[htbp]
%\setlength\tabcolsep{1.2pt}
\centering
  \caption{Ablation study on the displacement field (10 views).}
  \begin{tabular}{c|c|c}
    \toprule
    CD ($10^{-4}$)&PR-Senior&PR-Young\\
    \midrule
    w/ dis. &0.792&0.723\\
    \midrule
    w/o dis. &1.163&1.842 \\
    \bottomrule
  \end{tabular}
  \label{table:ablation_disp}
\end{table}


\begin{figure}[htb]
\centering
    \includegraphics[width=0.125\textwidth]{./results/ablation_dis/gt_396_15.jpg}
    \includegraphics[width=0.125\textwidth]{./results/ablation_dis/wodis_10_396_15_normal.jpg}
    \includegraphics[width=0.125\textwidth]{./results/ablation_dis/ours_10_396_15_normal.jpg}
    \\
    \includegraphics[width=0.125\textwidth]{./results/ablation_dis/gt_566_15.jpg}
    \includegraphics[width=0.125\textwidth]{./results/ablation_dis/wodis_10_566_15_normal.jpg}
    \includegraphics[width=0.125\textwidth]{./results/ablation_dis/ours_10_566_15_normal.jpg}
    
    \makebox[0.125\textwidth]{\scriptsize GT}
    \makebox[0.125\textwidth]{\scriptsize w/o displacement}
    \makebox[0.125\textwidth]{\scriptsize w/ displacement}\\
    \includegraphics[width=0.125\textwidth]{./results/ablation_tv/gt_619_15.jpg}
    \includegraphics[width=0.125\textwidth]{./results/ablation_tv/wotv_10_619_15_normal.jpg}
    \includegraphics[width=0.125\textwidth]{./results/ablation_tv/wtv_10_619_15_normal.jpg}
    \\
    
    \makebox[0.125\textwidth]{\scriptsize GT}
    \makebox[0.125\textwidth]{\scriptsize w/o TV loss}
    \makebox[0.125\textwidth]{\scriptsize w/ TV loss}
    
    \caption{Ablation studies. Rows 1 \& 2: The displacement map provides an additional degree of freedom and is essential in effectively reconstructing features and fine details that are not present in the template, such as the scarf and crow's feet. Using non-rigid deformation alone is not sufficient to achieve this level of detail.  
    Row 3: The TV regularization term is effective in reducing noise while preserving sharp features and fine details. Models: 396 (top), 566 (middle) and 619 (bottom), all under a 10-view setting.}
    \label{fig:ablation_dis}
\end{figure}



\begin{figure}[htb] \centering
    \rotatebox{90}{\begin{scriptsize}\textbf{training view}\end{scriptsize}}
    \includegraphics[width=0.11\textwidth]{./results/fine_tune/gt_539_15.jpg}
    \includegraphics[width=0.11\textwidth]{./results/fine_tune/ours_10_539_15_normal.jpg}
    \includegraphics[width=0.11\textwidth]{./results/fine_tune/ours_10_539_15_render.jpg}
    \\
    \rotatebox{90}{\begin{scriptsize}\textbf{novel view}\end{scriptsize}}
    \includegraphics[width=0.11\textwidth]{./results/fine_tune/gt_539_12.jpg}
    \includegraphics[width=0.11\textwidth]{./results/fine_tune/ours_10_539_12_normal.jpg}
    \includegraphics[width=0.11\textwidth]{./results/fine_tune/ours_10_539_12_render.jpg}
    \\
    \makebox[0.11\textwidth]{\scriptsize GT}
    \makebox[0.11\textwidth]{\scriptsize Normal map}
    \makebox[0.11\textwidth]{\scriptsize Rendering result}
    %\vspace{-0.1in}
    \caption{Reconstruction for an unseen individual (Model 539) under a 10-view setting. }
    \label{fig:unseen}
\end{figure}


\begin{figure}
\centering
\setlength\tabcolsep{0.25pt}
    \begin{scriptsize}
\begin{tabular}{ccccc}
    \centering
    \includegraphics[width=0.085\textwidth]{./results/hair_geometry/gt_429_15.jpg} &
    \includegraphics[width=0.085\textwidth]{./results/hair_geometry/neus_15_429_15_normal.jpg} &
    \includegraphics[width=0.085\textwidth]{./results/hair_geometry/volsdf_15_429_15_normal.jpg} &
    \includegraphics[width=0.085\textwidth]{./results/hair_geometry/hfs_15_429_15_normal.jpg} &
    \includegraphics[width=0.085\textwidth]{./results/hair_geometry/ours_15_429_15_normal.jpg}\\
    GT & NeuS & VolSDF & HF-NeuS & Ours\\
    \end{tabular}
    \end{scriptsize}
    \caption{Our method is capable of reconstructing the geometry of the entire head, in particular providing better results in the hair region of human heads compared to other methods. This is due to the fine refinement of the SDF using the displacement field. 
    Model: 429, 15 views. }
    \label{fig:hair_geometry}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/gt_383_12.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/neus_5_383_12_render.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/hfs_5_383_12_render.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/volsdf_5_383_12_render.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/ours_5_383_12_render.jpg}\\
    \includegraphics[width=0.09\textwidth]{./results/template_effects/gt_571_blank.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/neus_5_383_12_normal.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/hfs_5_383_12_normal.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/volsdf_5_383_12_normal.jpg}
    \includegraphics[width=0.09\textwidth]{./results/sp_view_5view/ours_5_383_12_normal.jpg}\\
    \makebox[0.6in]{\scriptsize GT}
    \makebox[0.6in]{\scriptsize NeuS}
    \makebox[0.6in]{\scriptsize HF-NeuS}
    \makebox[0.6in]{\scriptsize VolSDF}
    \makebox[0.6in]{\scriptsize Ours}
    \caption{Novel view synthesis result for Model 383 with only 5 views as input.}
    \label{fig:sparse_view}
\end{figure}

%\vspace{-0.2in}
\section{Conclusion \& Future Work}
%\vspace{-0.1in}
We present a novel neural rendering model for reconstructing 3D human heads under low-view settings. By decomposing the geometry of 3D heads into an identity-independent template and two identity-dependent components (a non-rigid deformation and a displacement field), we train our network in two separate stages in a coarse-to-fine manner. Through extensive evaluation, we demonstrate that our method is robust and can accurately reconstruct 3D heads with high-quality geometry. Moreover, it outperforms state-of-the-art methods in terms of geometry accuracy and novel view synthesis with 10 to 20 views as input. The pre-trained template serves a good initialization for our model to adapt to unseen individuals.

Our approach currently models hair as a whole using implicit functions. While this is effective for capturing the overall shape of the hair (see Figure~\ref{fig:hair_geometry}), more accurate hair modeling could be achieved by using strands~\cite{rosu2022neural}. Our paper primarily focuses on improving the accuracy of 3D head reconstruction rather than runtime performance. There are promising tools such as Plenvoxels~\cite{yu2021plenoxels} and Plenoctree~\cite{yu2021plenoctrees} that have demonstrated substantial improvements in the training and inference time of neural radiance fields~\cite{mildenhall2021nerf}. Given this, it is highly desirable to develop similar tools for neural implicit functions, as this could significantly improve the scalability and practicality of our proposed approach.

 
%%%%%%%%% BODY TEXT

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\newpage
% \appendix
\setcounter{table}{0}
\setcounter{figure}{0}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}
\begin{appendices}
\section{Appendix}
In the appendix, we present 1) additional results of unseen identities and low-view inputs in
Section~\ref{subsec:sparse_view_unseen}, which demonstrate that the pre-trained template serves as a good initialization and enables our method to adapt to new identities not available in the training dataset; 2) detailed results of our method and compare them with NeuS, HF-NeuS, and VolSDF on the PR-Senior and PR-Young datasets under a 10-view setting in Section~\ref{subsec:detailed_exp_results}; and 3) an application on color transfer in 
 Figure~\ref{fig:color_trans} to demonstrate the flexibility and potential of our geometry decomposition.
\subsection{Sparse Views and Unseen Identities}
\label{subsec:sparse_view_unseen}
\input{./03_Additional_Experimental_Results_Sparse_Views_and_Unseen_Identities}
\input{./04_Additional_Experimental_Results_Color_Transfer}

\subsection{Detailed Experimental Results and Analysis}
\label{subsec:detailed_exp_results}
\input{./02_Detailed_Experimental_Results_and_Analysis}
\end{appendices}
\end{document}
