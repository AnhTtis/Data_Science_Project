@misc{Carrascosa1,
  doi = {10.48550/ARXIV.2210.07695},
  
  url = {https://arxiv.org/abs/2210.07695},
  
  author = {Carrascosa-Zamacois, Marc and Geraci, Giovanni and Galati-Giordano, Lorenzo and Jonsson, Anders and Bellalta, Boris},
  
  keywords = {Networking and Internet Architecture (cs.NI), Information Theory (cs.IT), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{Understanding Multi-link Operation in Wi-Fi 7: Performance, Anomalies, and Solutions}},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Carrascosa2,
  doi = {10.48550/ARXIV.2205.15065},
  
  url = {https://arxiv.org/abs/2205.15065},
  
  author = {Carrascosa-Zamacois, Marc and Galati-Giordano, Lorenzo and Jonsson, Anders and Geraci, Giovanni and Bellalta, Boris},
  
  keywords = {Networking and Internet Architecture (cs.NI), Information Theory (cs.IT), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{Performance and Coexistence Evaluation of IEEE 802.11be Multi-link Operation}},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@inproceedings{Lopez-Raventos2021,
abstract = {The multi-link operation (MLO) is a new feature proposed to be part of the IEEE 802.11be Extremely High Throughput (EHT) amendment. Through MLO, access points and stations will be provided with the capabilities to transmit and receive data from the same traffic flow over multiple radio interfaces. However, the question on how traffic flows should be distributed over the different interfaces to maximize the WLAN performance is still unresolved. To that end, we evaluate in this article different traffic allocation policies, under a wide variety of scenarios and traffic loads, in order to shed some light on that question. The obtained results confirm that congestion-aware policies outperform static ones. However, and more importantly, the results also reveal that traffic flows become highly vulnerable to the activity of neighboring networks when they are distributed across multiple links. As a result, the best performance is obtained when a new arriving flow is simply assigned entirely to the emptiest interface.},
archivePrefix = {arXiv},
arxivId = {2105.10199},
author = {Lopez-Raventos, Alvaro and Bellalta, Boris},
booktitle = {19th Mediterranean Communication and Computer Networking Conference (MedComNet)},
doi = {10.1109/MEDCOMNET52149.2021.9501237},
eprint = {2105.10199},
isbn = {9781665435901},
keywords = {IEEE 802.11be,Multi-link operation,Performance evaluation,WLANs,WiFi 7},
title = {{IEEE 802.11be Multi-Link Operation: When the best could be to use only a single interface}},
year = {2021}
}

@inproceedings{Lacalle2021,
abstract = {With the uprising of new fields such as robotics, IoT or Augmented Reality, one of the main objectives of industrial modernization is the incorporation of safe and secure wireless technologies in the factory floor. However, nowadays wireless protocols can not fulfill the communication requirements of industrial networks, among which reliability and bound guaranteed latency are probably the most stringent ones. The lack of guaranteed low latency and ultra-high reliability of wireless networks are mainly derived from the error-prone nature of wireless propagation. Multi-link techniques are expected to provide enhanced reliability, latency and also protection against external interference. In this work, we assess the reliability and latency enhancement of the Multi-Link technique proposed in 802.11be. The assessment is done through simulation means using realistic industrial propagation and interference models. The results shown in this paper prove that Multi-Link provides an enhancement in wireless reliability and latency.},
author = {Lacalle, Guillermo and Val, I{\~{n}}aki and Seijo, Oscar and Mendicute, Mikel and Cavalcanti, Dave and Perez-Ramirez, Javier},
booktitle = {IEEE International Conference on Industrial Informatics (INDIN)},
doi = {10.1109/INDIN45523.2021.9557495},
isbn = {9781728143958},
issn = {19354576},
keywords = {802.11,Factory automation,Industry 4.0,Low latency,Multi-Link,Reliability,Wireless communications},
title = {{Analysis of Latency and Reliability Improvement with Multi-Link Operation Over 802.11}},
year = {2021}
}

@INPROCEEDINGS{Carrascosa3,
  author={Carrascosa, Marc and Geraci, Giovanni and Knightly, Edward and Bellalta, Boris},
  booktitle={IEEE International Conference on Communications (ICC)}, 
  title={{An Experimental Study of Latency for IEEE 802.11be Multi-link Operation}}, 
  year={2022},
  volume={},
  number={},
  pages={2507-2512},
  doi={10.1109/ICC45855.2022.9838765}}


@article{Lopez-Raventos1,
  author={Lopez-Raventos, Álvaro and Bellalta, Boris},
  journal={IEEE Wirel. Commun.}, 
  title={{Multi-Link Operation in IEEE 802.11be WLANs}}, 
  year={2022},
  volume={29},
  number={4},
  pages={94-100},
  doi={10.1109/MWC.006.2100404}}
  



  
  
  @article{Yang2020,
abstract = {The IEEE 802.11ax for Wireless Local Area Network (WLAN), one of the most important wireless networks, will be released in 2020. In recent years, ultra-high definition video service and real-time applications attract increasing attention. Therefore, the next generation WLAN (beyond IEEE 802.11ax): IEEE 802.11be task group (TGbe) was formally established in 2019, which regards achieving extremely high throughput (EHT) as its core technical objective. This article investigates and analyzes the key technologies of IEEE 802.11be, and further provides our perspectives and insights on it. Specifically, this article gives a brief overview on IEEE 802.11be, including the target scenario and technical objective, key technologies overview, and the standardization process. After that, we further investigate, analyze and provide perspectives on the key technologies of IEEE 802.11be including multi-band operation, multi-AP coordination, enhanced link reliability, and latency & jitter guarantee. To the best of our knowledge, this is the first work to investigate, analyze and provide insights on IEEE 802.11be.},
author = {Yang, Mao and Li, Bo},
doi = {10.1007/s11036-020-01567-7},
issn = {15728153},
journal = {Mobile Networks and Applications},
keywords = {Extremely high throughput,IEEE 802.11be,Low Latency,WiFi,Wireless local area networks},
title = {{Survey and Perspective on Extremely High Throughput (EHT) WLAN — IEEE 802.11be}},
year = {2020}
}


@article{lopez2,author={Lopez-Raventos, Alvaro and Bellalta, Boris},journal={IEEE Wirel. Commun. Lett.}, title={{Dynamic Traffic Allocation in IEEE 802.11be Multi-Link WLANs}},year={2022},volume={11},number={7},pages={1404-1408},doi={10.1109/LWC.2022.3171442}}

@article{9685808, 
author={Zhao, Sihao and Abou-zeid, Hatem and Atawia, Ramy and Manjunath, Yoga Suhas Kuruba and Sediq, Akram Bin and Zhang, Xiao-Ping}, 
booktitle={IEEE Global Communications Conference (GLOBECOM)},   
title={{Virtual Reality Gaming on the Cloud: A Reality Check}},  
year={2021}, 
pages={1-6},
doi={10.1109/GLOBECOM46510.2021.9685808}}


@techreport{IEEEP802.112015,
author = {{IEEE P802.11}},
institution = {IEEE},
pages = {83},
title = {{TGax Simulation Scenarios}},
url = {https://mentor.ieee.org/802.11/dcn/14/11-14-0980-16-00ax-simulation-scenarios.docx},
year = {2015}
}

@inproceedings{Andrychowicz2017,
abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
archivePrefix = {arXiv},
arxivId = {1707.01495},
author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1707.01495},
issn = {10495258},
title = {{Hindsight experience replay}},
year = {2017}
}


@inproceedings{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
booktitle = {AAAI Fall Symposium - Technical Report},
eprint = {1507.06527},
isbn = {9781577357520},
title = {{Deep Recurrent Q-Learning for Partially Observable MDPs}},
year = {2015}
}

@misc{Li15,
  doi = {10.48550/ARXIV.1509.03044},
  
  url = {https://arxiv.org/abs/1509.03044},
  
  author = {Li, Xiujun and Li, Lihong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Deng, Li and He, Ji},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {{Recurrent Reinforcement Learning: A Hybrid Approach}},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Krickeberg1969,
author = {Krickeberg, K. and Feller, W.},
doi = {10.2307/1402100},
issn = {03731138},
journal = {Revue de l'Institut International de Statistique / Review of the International Statistical Institute},
title = {{An Introduction to Probability Theory and Its Applications; Vol. 1}},
year = {1969}
}


@inproceedings{Haarnoja2018,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy-that is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actorcritic formulation, our method achieves state-ofthe-art performance on a range of continuous control benchmark tasks, outperforming prior onpolicy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
booktitle = {35th International Conference on Machine Learning (ICML)},
eprint = {1801.01290},
isbn = {9781510867963},
title = {{Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor}},
year = {2018}
}
@inproceedings{Fujimoto2018,
abstract = {In value-based reinforcement learning methods such as deep Q-leaming, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit over- estimation. We draw the conncction between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
archivePrefix = {arXiv},
arxivId = {1802.09477},
author = {Fujimoto, Scott and {Van Hoof}, Herke and Meger, David},
booktitle = {35th International Conference on Machine Learning (ICML)},
eprint = {1802.09477},
isbn = {9781510867963},
title = {{Addressing Function Approximation Error in Actor-Critic Methods}},
year = {2018}
}

@misc{Christodoulou,
  doi = {10.48550/ARXIV.1910.07207},
  
  url = {https://arxiv.org/abs/1910.07207},
  
  author = {Christodoulou, Petros},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{Soft Actor-Critic for Discrete Action Settings}},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Zhou22,
  doi = {10.48550/ARXIV.2209.10081},
  
  url = {https://arxiv.org/abs/2209.10081},
  
  author = {Zhou, Haibin and Lin, Zichuan and Li, Junyou and Ye, Deheng and Fu, Qiang and Yang, Wei},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {{Revisiting Discrete Soft Actor-Critic}},
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Choi,
 
  
  url = {https://mentor.ieee.org/802.11/dcn/22/11-22-0685-00-0wng-discussion-on-next-generation-wi-fi.pptx},
  
  author = { Choi, Jinsoo},  
  title = {{Discussion on Next Generation Wi-Fi}},
  publisher = {IEEE},
  
  year = {2022},
  month = {May} 
}

@inproceedings{9700667,
  author={Iturria-Rivera, Pedro Enrique and Erol-Kantarci, Melike},
  booktitle={IEEE 19th Annual Consumer Communications \& Networking Conference (CCNC)}, 
  title={{Competitive Multi-Agent Load Balancing with Adaptive Policies in Wireless Networks}}, 
  year={2022},
  volume={},
  number={},
  pages={796-801},
  doi={10.1109/CCNC49033.2022.9700667}
  }