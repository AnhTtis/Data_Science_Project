%\section{Using the tool-kit}
%\section{Evaluation}
\section{Achieving Fairness}%\pg{better title?}}
\label{s:exp}

%\pg{Should we include both S2 and S3? I chose S2 because it provide some contrast over the cloudEx soltuin. S3 I included just because I think its a reasonable scheme.}
%Based on Section \ref{s:core}, as long as the time span between consecutive deliveries of market data (inter-delivery time) to the MPs satisfy the required constraints, we can guarantee the corresponding type of fairness in trade handling at the CES. Based on these constraints, we introduce a few fairness algorithms and later compare them against the Cloudex.

In the previous section, we derived the minimum constraints on the delivery processes for achieving different variants of fairness. In each case, we also showed that if these constraints are met, then, DBO achieves fairness. Which property/properties for the delivery process should the cloud-provider aim for and what is the best way to achieve them (with low latency and high probability) depends on many things, including the requirements for fairness of the financial exchange and the nature of latency variations in the cloud-provider. In this paper, we do not attempt to provide any verdict on this question. Instead, in this section we only aim to show that DBO coupled with controlling how delivery clocks advance can indeed provide fairness with high probability in scenarios where the network latency is highly variable. To this end, we propose two simple schemes (with very different trade-offs for fairness) for delivering market data that both use DBO for ordering trades. We also compare these schemes against CloudEx. %($D_i(x) = \max(R_i(x), G_i(x)+Th)$) and $O(i,k) =A_i(k)$).}
%where the data is delivered at the same time to all MPs ($D_i(x) = \max(R_i(x), G_i(x)+Th)$) and trades are ordered based on the submission time ($A_i(k)$).}

\begin{figure}[t]
    \centering
    \includegraphics[trim={0 0 0 0mm},clip,width=\linewidth]{hotnets-images/latency_map_alt.pdf}
    \vspace{-6mm}
    \caption{\small{\textbf{Visualizing delivery times:} x-axis shows the generation time of the market data. y-axis plots the delivery times relative to the generation time for market data points (i.e., $D_i(x)-G(x)$) for different schemes. We also include the latency from the CES to the MP for reference (dotted black line). The dashed vertical lines demarcate various regions of interest for scheme S2.}}
    %\pg{Eashan: Include the drain rate, make the colored lines thicker and use different linestyles for the three schemes..}}% \pg{Maybe label the drain rate in the figure for S1 and S2.}}
    \label{fig:latency_graph}
    \vspace{-5mm}
\end{figure}

To help understand these two schemes, we consider a simple scenario where the latency from the CES to a single MP is mostly constant except a transient spike. \Fig{latency_graph} depicts the delivery times of each of the two schemes for this particular MP.

%To help you visualize, \Fig{latency_graph} shows the delivery times for each scheme for a particular MP in a simple scenario. The latency from CES to the MP is constant except a transient spike.

\smallskip
\noindent
\textit{Scheme 1 (S1):} The inter-delivery times respect the constraints in Corollary~\ref{cor:1} and S1 provides limited fairness regardless of the variations in network latency. In S1, the CES splits the market data points into batches. %The CES includes the corresponding batch number for each market data point.
At the RB, all the market data points corresponding to the same batch are delivered simultaneously (i.e., the inter-delivery time for market data points within a batch is zero). The time interval between delivery of two batches is greater than or equal to $\delta$. \attn{Note that, as per Corollary~\ref{cor:1}, the inter-batch time at a MP can differ from other MPs. As a result, S1 can handle arbitrary latency spikes without violating the inter-delivery constraints.} %In the event of a latency spike, a RB can delay delivering the next batch arbitrarily. As a result, S1 can handle arbitrary latency spikes without violating the inter-delivery constraints.} 
%As a result, S1 can handle arbitrary latency spikes, In the event of a latency spike RB can delay the next batch arbitrarily. Thu without violating the inter-delivery constraints. } 
Formally, delivery time of a batch $b$, ($D_i(b)$), is given by,
\begin{align*}
    D_i(b) = \max(R_i(b), D_i(b-1)+\delta),
\end{align*}
where $R_i(b)$ is the time  at which the last market data point in $b$ is received by RB$_i$. The CES chooses the batch boundaries based on the generation time of the market data. Market data $x$ corresponds to batch number ($b(x)$) given by $b(x) = \lfloor\frac{G(x)}{C_{S1}\cdot\delta}\rfloor$ or equivalently $C_{S1}\cdot\delta \cdot b(x) \leq G(x) < C_{S1}\cdot\delta \cdot (b(x)+1)$, where $C_{S1} (> 1)$ is a constant. \attn{Note that, with S1 batches can be arbitrarily small and multiple batches can be outstanding within a round trip.}

% \eg{\textit{Scheme 1 (S1):} The inter-delivery times respect the constraints in Corollary~\ref{cor:1} and S1 provides limited fairness regardless of the variations in network latency. In S1, the market data points are delivered in batches, with all the market data points corresponding to the same batch delivered simultaneously . the RB waits for all points in a batch to be rceived from the CES before transmitting the complete batch to the MP batches are delivered as soon as all data points of the said batch are received  a minimum \attn{Note that, as per Corollary~\ref{cor:1}, the inter-batch time at a MP can differ from other MPs. As a result, S1 can handle arbitrary latency spikes without violating the inter-delivery constraints.} Formally, delivery time of a batch $b$, ($D_i(b)$), is given by,
% \begin{align*}
%     D_i(b) = \max(R_i(b), D_i(b-1)+\delta),
% \end{align*}
% where $R_i(b)$ is the time  at which the last market data point in $b$ is received by RB$_i$. The CES chooses the batch boundaries based on the generation time of the market data. Market data $x$ corresponds to batch number ($b(x)$) given by $b(x) = \lfloor\frac{G(x)}{C_{S1}\cdot\delta}\rfloor$ or equivalently $C_{S1}\cdot\delta \cdot b(x) \leq G(x) < C_{S1}\cdot\delta \cdot (b(x)+1)$, where $C_{S1} (> 1)$ is a constant. \attn{Note that, with S1 batches can be arbitrarily small and multiple batches can be outstanding within a round trip.}}

%\pg{Explain C?} 
%\attn{Note that ~\cite{frequent_batch_auctions} also proposes delivering market data in batches. However unlike S1, it aims for all trade orders corresponding to a batch to be processed at the CES first before the next batch is released. To this end, it uses big batch sizes ($\sim$ 100 ms) that introduces significant latency. In contrast, with S1 the batches can be arbitrarily small and multiple batches can be outstanding within a round trip.}
 
\smallskip
\noindent
\textit{Scheme 2 (S2):} For this scheme, we assume clock synchronization between RBs. The mechanism for data delivery is an extension over CloudEx based on the constraints in the previous section. Formally,
\begin{align*}
    D_i(x) = \max\left(R_i(x), G(x)+Th, D_i(x-1) + \frac{G(x)-G(x-1)}{C_{S2}}\right)
\end{align*}
where $C_{S2} (> 1)$ is a constant. Intuitively, if the network latencies for all the MPs are below $Th$ consistently (region \circled{1} and \circled{4} in \Fig{latency_graph}), then, the inter-delivery time at each MP is the same (equal to the inter-generation time, i.e., $D_i(x)-D_i(x-1)= G(x)-G(x-1), \forall i$). The delivery times thus respect the constraints in Theorem~\ref{thm:1} and S2 provides strong fairness at such times. Compared to CloudEx, the main differentiation of the scheme above is how data delivery is handled after a latency spike between the CES and a particular MP (third term in the equation). In such cases (region \circled{3}), the inter-delivery time differs from the inter-generation time by a relative factor ($C_{S2}$) and $D_i(x)-D_i(x-1)= \frac{G(x)-G(x-1)}{C_{S2}}$. At such times, if the network latency to other MPs remains consistently low, then the inter-delivery time gaps respect the constraints listed in Corollary~\ref{cor:2} (with $\epsilon = C_{S2} - 1$) and $S2$ provides approximate fairness.%, at the cost of some additional latency compared to CloudEx. 
The inter-delivery time gaps at MPs, however,  can differ significantly when latency spikes happen (region \circled{2}), hence leading to reduced fairness by S2. Despite this, since S2 uses DBO, it can still correct for late delivery of market data and provide better fairness than CloudEx in such cases. 

Note that, it is equally possible to achieve the above inter-delivery properties without clock-sync. We chose S2 specifically to show incremental benefits over CloudEx from incorporating DBO and trying to respect the constraints on inter-delivery times.


% \textit{Scheme 2 (S2):} For this scheme, we assume clock synchronization between RBs. The mechanism for data delivery is a simple extension over CloudEx based on the constraints in the previous section. Formally,
% \begin{align*}
%     D_i(x) = \max\left(R_i(x), G(x)+Th, D_i(x-1) + \frac{G(x)-G(x-1)}{C_{S2}}\right)
% \end{align*}
% where $C_{S2} (> 1)$ is a constant. Intuitively, if the network latencies for all the MPs are as expected and below $Th$ (regions \circled{1} and \circled{4} in \Fig{latency_graph}), then, the inter-delivery time at each MP is the same (equal to the inter-generation time, i.e., $D_i(x)-D_i(x-1)= G(x)-G(x-1), \forall i$). The delivery times thus respect the constraints in Theorem~\ref{thm:1} and S2 provides strong fairness at such times. In the case of a sudden latency spike between CES and a particular RB (region \circled{2}), we follow a best effort approach (first term in the equation). In such a case, the inter-delivery times across MPs would differ leading to reduced fairness. The main difference in S2 as compared to Cloudex is how it provides fairness after the spike (region \circled{3}). In such cases, the inter-delivery time differs from the inter-generation time by a relative factor $C_{S2}$ (third term of the equation) and $D_i(x)-D_i(x-1)= \frac{G(x)-G(x-1)}{C_{S2}}$. At such times, if the network latency to other MPs remains consistently low, then the inter-delivery time gaps respect the constraints listed in Corollary~\ref{cor:2} (with $\epsilon = C_{S2} - 1$) and $S2$ provides approximate fairness.


%\pg{Even in this case because of DBO S2 is better than cloudex}


\begin{figure}[t]
    \centering
    \includegraphics[trim={0 0 0 5mm},clip,width=0.9\linewidth]{hotnets-images/transmission_times.pdf}
    \vspace{-4mm}
    \caption{\small{\textbf{Variations in latency from the CES to the RBs.}}} %\pg{Eashan You can shrink the height further, in the legend use ncol=2}} %\pg{Eashan can you redeaw this figure so that the height is smaller. Increase tick label size and legend size.}
    \label{fig:transmission_times}
    \vspace{-5mm}
\end{figure}

% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}[b]{0.32\linewidth}
%     \includegraphics[width=\linewidth]{images/LRTF delivery.pdf}
%     \caption{S1}% ($C_{S1}=1.14$, $\delta=14\mu s$)}%Strategy S1 with $C_{S1}=1.14$, $\delta=14\mu s$}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.31\linewidth}
%     \includegraphics[width=\linewidth]{images/ARTF.pdf}
%     \caption{S2}% ($C_{S2}=1.1$)}%Strategy S2 with $C_{S2}=1.2$}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.36\linewidth}
%     \includegraphics[width=\linewidth]{images/clocksync.pdf}
%     \caption{Cloudex}% ($d_o=300, d_i=100$)}%we describe them already
%   \end{subfigure}
%   \vspace{-5mm}
%   \caption{\textbf{Fairness with different schemes.} Fraction of trades that were ordered fairly for different values of response time.}
%   \vspace{-3mm}
%   \label{fig:fairness-sim}
% \end{figure*}

\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.245\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{hotnets-images/clocksync.pdf}
    \vspace{-5.5mm}
    \caption{\small{Cloudex}}% ($C_{S1}=1.14$, $\delta=14\mu s$)}%Strategy S1 with $C_{S1}=1.14$, $\delta=14\mu s$}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{hotnets-images/LRTF delivery.pdf}
    \vspace{-5.5mm}
    \caption{\small{S1 (Batch delivery + DBO)}}% ($C_{S2}=1.1$)}%Strategy S2 with $C_{S2}=1.2$}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\linewidth}
    %\includegraphics[width=\linewidth]{images/ARTF-2.pdf}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{hotnets-images/ARTF.pdf}
    \vspace{-5.5mm}
    \caption{\small{S2 (CloudEx$^+$ deli + DBO)}}
  \end{subfigure}
  \begin{subfigure}[b]{0.264\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{hotnets-images/Cloudex delivery+DBO.pdf}
    \vspace{-5.5mm}
    \caption{\small{S3 (CloudEx delivery + DBO)}}% ($d_o=300, d_i=100$)}%we describe them already
  \end{subfigure}
  \vspace{-3mm}
  \caption{\small{\textbf{Fairness with different schemes.} Fraction of trades that were ordered fairly for different values of response time. 
  Closer to 1 (white) is better.}}% Except CloudEx, all other schemes used DBO.}% \pg{I wonder if we should the extend the x-axis to 64 or 128 $\mu s$. The point would be too show that all schemes can achieve reasonable performance when response times are high, it is hard when they are low.}}
  \vspace{-4.5mm}
  \label{fig:fairness-sim}
\end{figure*}


\pg{This is more fundamental move to the core section}
\smallskip
\noindent
%\textit{Buffering algorithm at the OB:} 
\textit{Handling latency variations on the reverse path:} 
The latency from the MPs to the CES could also be variable. Such variations do not affect fairness as long as the OB only forwards a trade $(i,k)$ to the ME once it has (received and) forwarded all other trades $(j,l)$ that should be ordered ahead of $(i,k)$ (i.e., $O(j,l) < O(i,k)$). This requirement means that the OB might need to delay received trades for a certain duration (buffering) before forwarding them to the ME. There are multiple ways to achieve this requirement; we describe a simple one here. Each RB sends an acknowledgement (ACK) for every market data point it delivers to the MP. We assume that ACKs and trades from each RB are delivered to the OB in-order. %using a reliable delivery transport such as TCP. 
An ACK from RB$_j$ for data $x$ thus tells the OB that it has received all trades $(j,l)$ from $MP_j$ s.t., $O(j,l) \leq \langle x, 0 \rangle$. The OB uses a priority-queue to buffer trades. The OB uses the ACK information to forward trades respecting the above requirement. In the event of RB failures, the OB could stall indefinitely waiting for ACKs from a failed RB; we can protect against such scenarios by introducing a timeout threshold. \attn{This timeout can also help reduce buffering at the OB (at the cost of fairness) when the network latency from/to certain MPs is high.}

%In the event of RB failures, the OB could stall indefinitely waiting for ACKs from a failed RB; we can protect against such scenarios by introducing a timeout threshold.

%\eg{This part about in-order and reliable communication seems to have been covered in \S\ref{s:background}:Communication model.}%A trade $(i,k)$ at the head of the queue is forwarded to the ME if the OB has 
%\pg{should we add the following note: in the event of failures, the OB might not receive ACKs from a particular RB and the OB might stall indefinitely. We propose waiting for ACKs only upto a limited time to ward off against such scenarios.}
%Each RB includes a sequence number for the 

 
%\noindent
%\textit{Setup}: Based on the properties, we have the following high level setup: CES generates data point $x$ at local time $t^{\text{CES}}_g(x)$ and multicasts it to all RBs. $\text{RB}_i$ receives $x$ at local time $t^i_r(x)$ and delivers it to $\text{MP}_i$ at $t^i_d(x)$. After delivering the latest data $x$ to the MP, $\text{RB}_i$ marks its current state, $c^i$, as $x$ and $t^i_c=t^i_d(x)$. The $\text{RB}_i$ receives a trade $k$ from $\text{MP}_i$ at local time $t^i_k$ which is transmitted to the CES along with the current state of the $\text{RB}_i$ and the time difference: $<c^i, t^i_k-t^i_c>$. Following the idea of fairness, the CES orders the trades lexicographically based on state of RB when trade was received and the time difference, $<c, t_k-t_c>$. The execution for trade can be carried out once all RBs have reached state $c+1$ which can be ensured by sending out heartbeat messages from RBs to CES or sending ACKS on delivering market data to the MPs.

%Following the above setup, a fairness algorithm can be defined by when $\text{RB}_i$ delivers the market data $x$ to the $\text{MP}_i$ i.e., its policy to determine $t^i_d(x)$. The following fairness algorithms describe the method of market data delivery from the RBs to the MPs.

% \begin{algorithm}
% \floatname{algorithm}{}
% \renewcommand{\thealgorithm}{}
% \caption{RB$_i$}
% \textbf{Receive data $x$ at local time $t^i_r$}
% \begin{algorithmic}
%     \STATE Deliver to MP at time $t^i_d$
%     \STATE Send ACK $x$ to CES
%     \STATE RB$_i$ reached state $x$. Mark current state $c=x$.
% \end{algorithmic}

% \textbf{Receive trade $P$ from MP at $t^i_P$}
% \begin{algorithmic}
%     \STATE Send to CES $(P, <c, t^i_P-t^i_c>)$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \floatname{algorithm}{}
% \renewcommand{\thealgorithm}{}
% \caption{CES}
% \textbf{On generation of data $x$ at} $t^{\text{CES}}_x$\textbf{, multicast $x$ to all RBs}\\
% \textbf{On receiving trade from RB$_i$: $(P, <x_l, t_P-t_c>)$}
% \begin{algorithmic}
%     \STATE Wait until all RBs have reached state $(x+1)$
%     \STATE Execute $P$ based on the ordering
% \end{algorithmic}

% \textbf{Receive ACK $x$ from RB$_i$}
% \begin{algorithmic}
%     \STATE Mark RB$_i$ as having reached $x$
% \end{algorithmic}
% \end{algorithm}

%\textbf{Algorithm 1} (Limited Fairness): The data is delivered in batches. RBs wait to collect a complete batch of market data points from the CES and deliver them at once to the MP. This is done while ensuring, with best effort, that the delivery time between batches is exactly $\delta$. The choice of batch size and $\delta$ is such that the queue of market data is drained eventually. Note that following Corollary \ref{cor:1}, this algorithm ensures limited fairness from Section \ref{ss:limited_fairness}.

%\textbf{Algorithm 2} (Approximate Fairness):  Following Corollary \ref{cor:2}, the data points are delivered based on $t^i_d(x)=t^i_d(x-1)+\frac{1}{1+\epsilon}\cdot(t^\text{CES}_g(x)-t^\text{CES}_g(x-1))$ for some predetermined $\epsilon$. This is again followed with best effort. For this instance, we also assume clock synchrony between all RBs and CES and add a delivery time delay, similar to Cloudex, for which the RBs wait additionally from the point of generation.



%Figure \ref{fig:latency_graph} shows how the discussed algorithms and Cloudex handle a spike in network transmission time between CES and an RB. Cloudex waits for the delay threshold time from the point of generation before delivering the data. Algorithm 1 collects a batch of data, thus has spikes on the initial entries of the batch and delivers them altogether. Algorithm 2 initially works similar to Cloudex but in case of a latency spike, it slowly adjusts back to the delay threshold set.

\subsection{Simulation Results}
\label{ss:simulation}




\if 0
\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.24\linewidth}
    \includegraphics[width=\linewidth]{images/alternate/LRTF delivery.pdf}
    \vspace{-5mm}
    \caption{S1}% ($C_{S1}=1.14$, $\delta=14\mu s$)}%Strategy S1 with $C_{S1}=1.14$, $\delta=14\mu s$}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\linewidth}
    \includegraphics[width=\linewidth]{images/alternate/ARTF.pdf}
    \vspace{-5mm}
    \caption{S2}% ($C_{S2}=1.1$)}%Strategy S2 with $C_{S2}=1.2$}
  \end{subfigure}
  \begin{subfigure}[b]{0.23\linewidth}
    \includegraphics[width=\linewidth]{images/alternate/ARTF-2.pdf}
    \vspace{-5mm}
    \caption{S3}
  \end{subfigure}
  \begin{subfigure}[b]{0.27\linewidth}
    \includegraphics[width=\linewidth]{images/alternate/clocksync.pdf}
    \vspace{-5mm}
    \caption{Cloudex}% ($d_o=300, d_i=100$)}%we describe them already
  \end{subfigure}
  \vspace{-3mm}
  \caption{\textbf{Fairness with different schemes.} Fraction of trades that were ordered fairly for different values of response time. \pg{I wonder if we should the extend the x-axis to 64 or 128 $\mu s$. The point would be too show that all schemes can achieve reasonable performance when response times are high, it is hard when they are low.} \eg{Added results with longer response time to address the concern of RTF schemes only working for HFTs or smaller horizon. We can say that it provides fairness in all scenarios. We can edit and keep the results that are more useful.}}
  \vspace{-3mm}
  \label{fig:fairness-sim-long-rt}
\end{figure*}
\fi
%\pg{Include a note on that we pick static threshold for CloudEx}

To evaluate the proposed schemes, we run a simulation experiment using two dummy MPs with \emph{highly} variable latency between the CES and each RB (generated by a random-walk-like process); see \Fig{transmission_times}. In CloudEx, latency on the reverse path can cause additional unfairness (which can be alleviated using the buffering process we describe earlier). For simplicity, the latency between each RB and the CES is assumed to be fixed (100 $\mu s$). The CES generates market data continuously every $2 \mu s$. We assume that both MPs submit a trade in response to every market data point. We do several runs of the experiment with different values of response times for each MP (fixed across data points within a run). For MP2, we vary the response time ($rt_2$) from 2 to 32 $\mu s$.\footnote{We specifically choose to evaluate over such short response times. For long response times (> 1 ms), all schemes provide close to ideal fairness.} For each value of $rt_2$, we run the experiment such that response time of MP1 ($rt_1$) is higher than from $rt_2$ by some multiplicative factor ($rt_1 = f \cdot rt_2$). We vary $f$ from 1.1 to 2. 





%\textit{Fairness}: 

To measure fairness, we calculate the fraction of MP2's trades that were executed before the corresponding ones (based on the same market data) from MP1. %Following Definition \ref{def:strong}, this fraction would be 1 for an ideal scheme. 
\Fig{fairness-sim}, shows the results for the three schemes.
%Figure \ref{fig:fairness-sim}, shows the fraction of trades that satisfy strong fairness (Definition \ref{def:strong}) for S1, S2 and CloudEx.% in each case. %We construct a heat map to contrast the differences.
We also compare the average end-to-end latency\footnote{The round trip latency between market data generation and trade handling excluding the response time.} (measured across trades) for these schemes (see Table~\ref{tab:latency}). 

%Formally, if a trade $(i,k)$ generated in response to data point $x$, is forwarded to the ME at time $t$, its end-to-end latency is given by $t-(G(x)+rt_i(k))$.  Intuitively, we are measuring the round trip latency between market data generation and trade handling excluding the response time.% of the MP.% to respond to the RB.

$Th$ governs the trade-off between fairness and latency in CloudEx, here we use a fixed $Th = 300\mu s$ s.t., the network latency from CES to the MP is below this threshold most of the time.
For S1, we use a small value of $\delta = 14\mu s$. %$C_{S1}$ governs the buffer drain rate at the RB after a spike and hence the latency.
We chose $C_{S1} \cdot \delta = 16$ s.t., the latency is similar/lower to CloudEx.  For S2, we use $C_{S2} = 1.095$ (< 1.1) to try to ensure the constraint from Corollary~\ref{cor:2} for $\epsilon = 0.095$\footnote{If this constraint is met at all times, then, S2 will achieve ideal fairness when $rt_1 > rt_2*1.095$}. 
We use the same $Th$ as CloudEx to equalize latency.% and to show improvements from DBO and controlling the inter-delivery times. %\pg{I wonder if we should show results pick a higher value of $C_{S2}$ s.t., the last row in heatmap looks bad.}

%\pg{Not sure we should include S3} We also consider another DBO based scheme (S3) here. In S3, each RB tries to maintain a constant amount of market data buffered at the RB. If the buffering goes below a certain threshold, the RB increases the inter-delivery times (by a multiplicative factor ($C_{S3}$) relative to the inter-generation time, $D_i(x) - D_i(x-1) = C_{S3}\cdot(G(x)-G(x-1))$). 
%If the buffering increases, the RB reduces the inter-delivery times ($D_i(x) - D_i(x-1) = \frac{G(x)-G(x-1)}{C_{S3}}$). This scheme ensures that inter-delivery times meet the constraints in Corollary~\ref{cor:2} (with $\epsilon=C_{S3}^{2}-1$) if all RBs have some market data buffered. However, unlike S2, this scheme does not rely on clock synchronization. We use $C_{S3}=\sqrt(1.095)$ and 50$\mu s$ of buffering threshold (to equalize latency).  






As expected, S1 achieves perfect fairness when $rt_2 < \delta$. When the response time is higher, S1 achieves the worst fairness. It is possible to provide better fairness for such trades by additionally trying to ensure that inter-batch delivery times are similar across MPs. 

 S2 provides close to ideal fairness at the cost of some additional latency.\footnote{Fairness with S2 drops when $rt_1 \leq rt_2*1.095$, but it is still comparable to CloudEx.} As explained earlier, this is because the combination of DBO and controlling how delivery clocks advance enables S2 to handle latency spikes better than CloudEx. To understand where the wins in S2 are coming from, we consider another scheme. S3 uses DBO and market data delivery is same as CloudEx. DBO on its own helps correct for differences in latency to the MPs and provides better fairness than CloudEx. By controlling the delivery clocks, S2 is further able to %mask the impact of latency spikes and 
 improve fairness.% than S3.  
 
%Note that, the aim of this experiment is to illustrate the benefits of our approach in scenarios where network latency is highly variable. The exact performance numbers do depend on the latency traces. We repeated the above experiment with other latency traces. In each case, the trend in performance was similar.
 %S2 outperforms S3 by controlling the delivery clocks.  


%\pg{I wonder if we should include an experiment with CloudEx like delivery but with DBO ordering. The point would be too show DBO in its self is useful but you combine it with controlling the delivery clock (S2) it is more useful.}
%Note that even if we increase $Th$ to $300\mu s$ for CloudEx, it still provides worse fairness than S2 (with $Th =250 \mu s$) and its latency is higher than S2.

%\pg{Not sure if we should include S3:} In S3, RBs rarely run out of market data buffering in this experiment. S3 also achieves close to ideal fairness (for similar latencies). Additionally, we can use more sophisticated schemes for managing buffers (e.g., a PID controller~\cite{somepidbook}) to achieve even better performance.


\vspace{-2mm}
\begin{table}[h!]
\small
    \centering
    \begin{tabular}{cc}
    %\hline
        \textbf{Scheme} & \textbf{End-to-end Latency ($\mu s$)} \\
    \hline
        CloudEx & 403 \\
        S1 & 382 \\
        S2 & 408 \\
        S3 & 406 \\
    %\hline
    \end{tabular}
    \vspace{-1mm}
    \caption{\small{End-to-end latency for different schemes.}}
    \label{tab:latency}
    \vspace{-5mm}
\end{table}






