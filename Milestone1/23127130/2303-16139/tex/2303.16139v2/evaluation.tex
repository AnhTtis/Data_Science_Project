\section{Evaluation}
\label{s:eval}


We evaluate the feasibility of our solution in hardware using our own hardware test bed. We use public-cloud experiments to get an understanding of overall DBO's performance in terms of latency and fairness if deployed.

%We also want to better understand the tradeoffs introduced by each mechanism proposed in our system: how does DBO alone improve direct delivery, how do heartbeats affect end-to-end latency, could a `straggler' RB  affect the system's latency etc.
%\pg{I think we don't do heartbeats. Maybe add we are most interested in understanding how to performance an latency numbers look like in public cloud. }


\subsection{Methodology}
\label{ss:eval_methodology}


 
For all of the experiments (except simulation) presented in this section we leverage our prototype CES and MP implementations. On the CES side, we generate and distribute data to all Market Participants at fixed intervals. The market data arrive at the RBs, which later on release them to the  Market Participants. The MP implementation relies on busy-polling and kernel-bypass for low-latency access to the incoming market data packets, but does not utilize a sophisticated algorithm for trading decisions; it rather busy-waits for a pre-configured response time duration before generating a trade. We set each MP's  reaction time accordingly so that we can derive the expected final ordering at the OB and evaluate fairness. 

\noindent\textbf{Fairness metric:} For any number of MPs, perfect fairness is achieved when all competing trades among all unique pairs of participants are fully ordered (from faster to slower). We define the metric of fairness as the ratio of the number of competing trade sets  that were ordered correctly to the total number of competing trade sets for all unique pairs of market participants.

%\pg{Note: I use forwarding time instead of execution time}.

\noindent\textbf{End-to-end latency} We define end-to-end latency of a trade using Equation~\ref{eq:latency_def} ($F(i,a)-G(x)-RT(i,a)$). Generation time and forwarding time are measured at the CES. For the purpose of reporting latency  and fairness (and \emph{not} for ordering trades in DBO), we assume that the trigger point is known. We use it to calculate the response time of trades at the release buffer.

%We define end-to-end latency of any trade, as the duration measured from the market data id transmission (at the CES) until a trade tagged with the same id is executed by the matching engine. This measurement is taken on the CES. 


 We  evaluate our solution on three different setups: \begin{enumerate*}[label=(\alph*)]\item on-premise, bare-metal testbed deployment, \item public-cloud-based deployment, and \item simulation \end{enumerate*}.

Evaluation schemes: We evaluate three schemes. \begin{enumerate*}\item Direct delivery: This is the baseline scheme. There is no release buffer or ordering buffer and both trades and market data points just incur the underlying network latency. 
%\item Direct w/ DC: Data is delivered directly to the participants without any batching or pacing. However, the trades are ordered using the delivery clock timestamp. This scheme is used to show the importance of ordering using delivery clocks on its own. 
\item DBO: Based on our discussion in \S\ref{ss:understanding_latency}, we use $\delta=20$, $\kappa=0.25$ and $\tau=20 \mu s$.
%We use different values for the horizon ($\delta$) and the batch size($(1+\kappa) \cdot \delta$). 
\item CloudEx: CloudEx requires fine-grained clock synchronization, which is not available in our test-bed and cloud experiments. Due to inaccuracies in clock-synchronization, in our experiments we experience frequent release buffer and ordering buffer overruns. We only report results for CloudEx in simulation where we assume perfectly synchronized clocks. %We also report the optimal latency (max network round-trip-latency Theorem~\ref{thm:latency}) for achieving perfect response time fairness
\end{enumerate*}.
%\pg{Fix comment on CloudEx not being there because of clock synschronization. Generally think about what schemes should be there, Direct+delivery clocks, DBO+clocksync, clocksync delivery + delivery clocks. Maybe include DBO+clocksync delivery + clocksync ordering. Generally think about evaluation. We need to include CloudEX in 6.2. Do we actually need so many schemes? I think all we need is DBO+Clocksync, to explain it maybe we need clock sync delivery+ delivery clock. In geral DBO+clocksync vs CloudEx comparison is complicated. 1) CloudEx gets affected by reverse path delays. Pick ordering buffer from DBO. 2) for forward path clocksync delivery + delivery clocks improve things further. 3) using batching further improve stuff }


\noindent
\textbf{Response Time:} 
%Our definition of fairness is sufficient for all trades that were generated as a response to some market data within the limited-time horizon ($\delta = 20$), but it could be inaccurate for `slow'  or `non-reactive` trades as they will likely be tagged with a later DBO id.
%$experiments. 
The response time for each trade is a random number between 5 and 20 $\mu$ s and is within the horizon ($\delta$). 
Note that our solution does not ensure fairness for speed races where the response time (of the faster participant) is greater than the horizon. We picked a horizon to accommodate majority of the speed races. But we explicitly take into account this limitation. We present latency results with longer horizons and include experiments where the response time exceeds the horizon.
%,  when presenting latency results in the rest of this paper.

\subsection{Evalution on DPU-enabled baremetal servers}
\label{ss:on-premise-eval}

Our lab setup consists of three machines: one CES server and two MP servers. The CES server is equipped with an Nvidia ConnectX-5 NIC with two 100Gbps ports. Each MP server hosts one Nvidia BlueField-2 DPU with two 100Gbps ports. The server has a dual-CPU Intel Xeon processor running at 3.1 GHz. Each BlueField-2 DPU has eight ARMv8 A72 cores. All machines are connected via a 100GbE switch. We run Linux kernel (v5.4.0) and DPDK (v21.11) for the CES, RB, MP network engines.

The CES is generating market data every $40\mu s$ ($25K$ ticks per second), and the market participant servers are generating responses within $\delta$ time horizon since the reception of the data. The RB  is executing on the BlueField-2 DPU's SoC.


Table~\ref{tbl:bluefield} shows the achieved fairness and latency of our system. Direct delivery achieve poor fairness because of differences in network latency. DBO achieves perfect fairness at the cost of latency. In particular, to achieve response time fairness, the OB waits for the slowest participant. The latency is bounded by maximum network round-trip latency. This optimal latency bound (Theorem~\ref{thm:latency}) is shown as Max-RTT in the table. The difference between the optimal and DBO is becuase of batching, pacing and heartbeats.


%Recall that the minimum latency for response time fairness is bounded by the maximum network round trip latency. Batching, pacing and heartbeats further add to this latency bound. \pg{more, add max RTT}

%Delivery clocks based ordering on top of direct delivery significantly enhances fairness, while DBO achieves perfect fairness. \pg{EDIT: Since with both these schemes at the OB we need to wait for the slowest participant, the latency is higher. As explained earlier, waiting for the slowest participant is necessary for achieving fairness.}

\if 0
\pg{We should explain these results. The next statement seems inconsitent with the results?}
Even though this is a bare-metal, on-premise lab setup with no interfering traffic the tail latency is higher than expected. Our performance analysis indicates that this overhead is due to scheduling artifacts in the BlueField ARM cores, and higher SoC-to-NIC latencies.\pg{WHAT SHOULD WE SAY ABOUT SOC TO  NIC LATENCY? how are we measuing fairness, what is the response tiem} We are currently working with NVIDIA to utilize their hardware-assisted pacing and NIC timestamping features to improve performance.%\pg{Should we say for calculating fairness ratio we use time stamps taken at the RB? What are the response times?}
\fi

\begin{table}[t]
\small
    \centering
    \begin{tabular}{c|c|cccc}
    %\hline
    & \textbf{Fairness} & \multicolumn{4}{c}{\textbf{Latency $(\mu s)$}} \\
    & \textbf{(\%)} & \textbf{avg} & \textbf{p50} & \textbf{p99} & \textbf{p999} \\
    \hline
       \textbf{Direct} & 74.62 & 9.60 & 9.52 & 16.58 & 25.25 \\
       %\textbf{Direct w/ DC} &  97.01 & 23.46 & 15.09 & 41.55 & 53.80 \\
        \textbf{Max-RTT} &  - & 10.23 & 9.94 & 18.08 & 26.18 \\
       \textbf{DBO}  & 100 &  15.92 & 12.16 & 28.82 & 46.80 \\
        %\textbf{DBO (45,60)}  & 100 & 28.18 & 20.13 & 64.86 & 94.85  \\
        %\textbf{DBO (80,120)}  & 100 & 56.05 & 52.71 & 104.31 & 120.42  \\
        %\textbf{Direct w/ DC} &  95.12 & 15.43 & 12.59 & 29.10 & 36.53 \\

        


    %\hline
    \end{tabular}
    %\vspace{-1mm}
    \caption{\small{Fairness and trade latency results on bare metal servers with BlueField-based RB implementation.}} %\pg{Something is wrong here, how can latency of direct w/DC be higher than DBO? Can we report max of RTT as well?}}
    \label{tbl:bluefield}
    \vspace{-9mm}
\end{table}

\if 0
\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/latency_cdf_bf2.png}
    % \includegraphics[width=0.95\columnwidth]{images/latency_cdfs2.pdf}
    \caption{\small{\bf CDFs of the end-to-end trade latency for various DBO configurations on bare metal servers with BlueField-based RB implementation. \pg{we need comparison with direct delivery here. And fairness numbers}}}
\end{figure}

\fi


\subsection{Cloud-hosted Testbed}
\label{ss:cloud-eval}



We wish to understand how our system performs in a real public cloud-based deployment with several market participants. As discussed in \S \ref{ss:release_buffer}, we do not have access to the cloud providers' programmable NICs to deploy the RB functionality. To work around this limitation, we have adjusted our RB implementation so that it runs as a co-located  process with the market participant's execution engine on the MP VMs. In such configuration, the RB is using a kernel-bypass network stack to take over a dedicated vNIC which it uses to receive the UDP stream of market data from the CES, and to send back any trades submitted by the MP. To facilitate fast MP-to-RB communication we rely on shared-memory-based IPC primitives. Clearly, such a solution does not provide any security guarantees as the RBs run on VMs owned by the market participants which are not part of the CES' Trusted Computing Base, and could easily tamper with the RB's market data delivery engine or the delivery clock  measurements. It allows us, however, to evaluate the real-world performance (i.e., achievable throughput and latency) of our DBO system in a public cloud deployment.



\begin{table}[h!]
\small
    \centering
    \begin{tabular}{c|c|cccc}
    %\hline
    & \textbf{Fairness} & \multicolumn{4}{c}{\textbf{Latency $(\mu s)$}} \\
    & \textbf{(\%)} & \textbf{avg} & \textbf{p50} & \textbf{p99} & \textbf{p999} \\
    \hline
       \textbf{Direct} & 57.61 & 27.9 & 27.48 & 32.5 & 44.03 \\
        % \textbf{Max-RTT}  & - & 32.94 & 32.14 & 42.63 & 105.49  \\
        \textbf{Max-RTT}  & - & 33.34 & 32.44 & 42.01 & 48.38  \\
       % \textbf{DBO}  & 100 &  55.78 & 47.09 & 59.40 & 1366.15 \\
       \textbf{DBO}  & 100 &  47.19 & 46.95 & 55.71 & 67.41 \\
       %\textbf{Direct w/ DC} &  89.84 & 47.15 & 47.10 & 57.23 & 312.92 \\
      %  \textbf{DBO (45,60)}  & 100 & 77.21 & 52.54 & 135.27 & 573.70  \\
       % \textbf{DBO (80,120)}  & 100 & 89.32 & 86.89 & 133.23 & 459.97  \\
       


    %\hline
    \end{tabular}
    %\vspace{-1mm}
    \caption{\small{Fairness and end-to-end latency for different schemes; full traces collected over a 15-minute duration. For consistency, Max-RTT is calculated using the packet timestamps from the DBO experiment trace.}}
    \label{tab:fairness_latency}
    \vspace{-7mm}
\end{table}

We set out to evaluate the fairness and end-to-end latency of different schemes. We deploy ten market participants and one CES as virtual machines (Standard\_F8s) in Microsoft Azure. We configure the  aggregate service rate to $125,000$ transactions (trades) per second (market data generation interval is fixed to $40\mu s$). Table~\ref{tab:fairness_latency} summarizes the achieved fairness and end-to-end latency results for direct delivery and DBO. %\pg{The scale of our experiments is comparable to some of the major financial exchange in our conversations.}

%\pg{Fix, show relevance of DBO, latency spikes do occur and are upto a order of magnitude higher. One way to do that is to show CDF of individual RTTs for each of the cloud participants.}


%\noindent\textbf{Fairness:} Direct delivery achieves poor fairness in our experiments. When combined with DC-based ordering at the OB  fairness is improved significantly. Finally, DBO (regardless of $\delta$) always achieve perfect fairness as all MPs are configured to (randomly) generate trades within the limited horizon. We discuss fairness for slow responders in Section~\ref{ss:simul}.

\noindent\textbf{Fairness:} Direct delivery achieves poor fairness in our experiments. Compared to our test-bed where there is no network traffic and the variability in latency across participants is lower, direct delivery performs worse in the cloud.
%When combined with DC-based ordering at the OB  fairness is improved significantly. 
DBO always achieve perfect fairness.% as all MPs are configured to (randomly) generate trades within the limited horizon. 
We discuss fairness for slow responders in \ref{ss:fairness_gt_delta}.

\noindent\textbf{Latency:}  As expected, direct delivery achieves  the lowest latency, at the cost of fairness. 
%Direct delivery with Delivery Clock-based ordering, is an enhanced version of direct delivery; as mentioned previously we maintain the delivery clock at the RB and tag each trade received with it, but there is no other delay (pacing, batching etc) introduced at the MP's end. Instead,  the OB sorts all trades based on their DC-derived timestamp and forwards them to the ME when it has heard from all RBs. In practice, this means that the  end-to-end latency for all MPs is effectively \textit{inflated} to the worst CES-to-MP network RTT experienced.
On the other hand, DBO trades off latency to achieve perfect fairness, but \emph{it still achieves sub-100us p999 tail latency in the public cloud.}
This latency is well within the requirements of many major exchanges. IEX, for example, a major exchange that prides itself on fairness had 700$\mu$ s latency~\cite{iex_cost_report}. We believe that with additional optimizations such as network traffic prioritization, in-network multicast, proximity placement groups, this number could be further brought down.The p9999 latency is much higher (\textasciitilde3.5ms);  full trace analysis shows that packet drop rate is very low but we identified a well-aligned, periodic queue buildup at the OB which we believe is due to scheduling artifacts in the VM. 

%\pg{\emph{It's interesting to note that the latency with DBO is till under 100 $\mu s$ even at the tail (p99) in the public cloud!} }
%Batching and pacing further add to this optimal latency.
%We see that increasing $\delta$, increases the latency. When the network is well behaved this increase is linear. However, we find that 



%When the network is well behaved, the end-to-end latency is similar to that of direct delivery with DC-based ordering which constitutes optimal for DBO. %The increased tail latency results presented here are due to OS scheduling artifacts and network events such as possible heartbeat drops or RTT spikes. 
%\im{I am handwaiving here and i don't like it} 

\subsubsection{Understanding DBO latency:}



\begin{figure}[t]
\centering
    \includegraphics[width=0.8\columnwidth]{images/latency_cdfs_v16.pdf}
    % \includegraphics[width=0.95\columnwidth]{images/latency_cdfs2.pdf}
    \vspace{-3mm}
    \caption{\small{\bf CDFs of the end-to-end trade  latency for various DBO configurations.}}% Max-RTT CDF corresponds to the highest network RTT among all participants.}}
    \vspace{-3mm}
    \label{fig:latency_cdfs}
\end{figure}

How do DBO parameters affect end-to-end trade latency? Figure~\ref{fig:latency_cdfs} illustrates the CDF of the latency with different DBO configurations. Here, DBO(x,y) refers to using a horizon $\delta = x$ and batch size $(1+\kappa) \cdot \delta =y$. We also include the latency bound. As expected increasing the horizon and the batch size increases the latency. When batch size is 60 $\mu$s we see one inflection point. For batch size of 120 $\mu$s we see two inflection points. These inflection points are a direct result of batching. Since market data generation rate is 40, for batch size of 60, roughly $2/3$ of the batches contain  two data points. The first point in such batches incurs 40$\mu s$ of additional delay compared to second point. This difference create the inflection point.  Similarly for batch size 120, on average there are three market data points, the first point in the batch incurs an additional delay of 80 $\mu s$ while the second point incurs an additional delay of 40 $\mu s$. For batch size of 20, which contains only one market data point, the batching delay is zero. The deviation from the optimal latency bound is primarily due to hearbeats. Recall when network latency is well behaved, pacing does not add additional delay. Since $\tau$ is 20, the difference on average is 10$\mu s$

%\im{I don't think we have space for that, and I don't understand why MaxRTT CDF does not match DBO(20,25)}

\if 0

\subsubsection{Impact of straggler RBs}

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/straggler_mitigation.pdf}
    \caption{\small{\bf Stragglers' impact to end-to-end trade  latency for all market participants. Max-RTT is the maximum RTT across all alive MPs.}}
    \label{fig:straggler_mitigation}
\end{figure}

At this point, it is important to differentiate a straggler RB from slow MP responders: the RB is part of the Trusted Computing Base and hence correct operation is critical for good end-to-end system performance. Still, failures and non-determinism stemming from the network need to be taken into account in our system design. 

A straggler RB (i.e., experiencing high latencies or packet drops) can drastically affect the end-to-end system latency and throughput. This happens because the OB needs to hear from all RBs before dequeuing trades to the matching engine, so that fairness is guaranteed. In Figure~\ref{fig:straggler_mitigation} we evaluate how latency is affected with a straggler; we emulate this by configuring one of the RBs to drop all packets (including outgoing hearbeats) as would happen in the case of a transient network outage. Such events could potentially stall to the market indefinitely; to mitigate this issue the OB maintains a timeout (in this case set to 250us) and on expiration releases trades for the current DBO id to the ME. In such cases, fairness guarantees do not hold any longer for the straggler.

A production deployment should leverage a mechanism to detect and flag stragglers quickly; upon detection, straggler RBs should be removed from the OB's shard so that the market does not unnecessarily  slow down for prolonged periods.

\fi

\subsubsection{Latency overhead with multiple participants\\}

DBO \textit{post} corrects for latency variance among participants at the OB. As the number of market participants increases, network RTT variance and asynchrony might be pronounced and can cause higher delays at the Ordering Buffer. In Figure~\ref{fig:ob_delay} we plot the time spent at the OB as a function of the number of MPs. Note, that the RB components remain unaffected as the number of participants gets higher. %\pg{A comment on plateauing?}


\begin{figure}[t]
\centering
    \includegraphics[width=0.8\columnwidth]{images/ob_delay_boxplot2.pdf}
    \vspace{-3mm}
    \caption{\small{\bf Trades' buffering delay at the Ordering Buffer.}}
    
    \label{fig:ob_delay}
    \vspace{-3mm}
\end{figure}



\subsubsection{Trades with response time > $\delta$\\}
\label{ss:fairness_gt_delta}
DBO only guarantees fairness for trades with a limited response time. Table~\ref{tab:fairness_rt} shows the fairness for such trades for different values of response time. In each experiment, the response time for the trade is derived from a range of values (shown on the top of the table). Direct delivery achieves poor fairness (similar to Table~\ref{tab:fairness_latency}). In contrast, even though the response time of trades exceeds the horizon $\delta$, DBO achieves close to ideal fairness. DBO orders such trades fairly, if the inter-delivery time for the batch that triggered the trade and the last batch corresponding to the trade is same across all participants. In the cloud experiments, even though latency differs across participants, for any particular participant the latency exhibits little variation (except a few latency spikes). As a result, the inter-delivery time for batches is similar (= $(1+\kappa) \cdot \delta$) across all participants for most of the time. DBO is thus able to correct for static differences in latency across participants and achieve fairness.% for trades that exceed the horizon.


\begin{table}[h!]
\small
    \centering
    \begin{tabular}{c|cccccc}
    %\hline
    \textbf{RT (in $\mu s$)}  & 10-15 & 15-20 & 20-25 & 25-30 & 30-35 & 35-40\\
    \hline
       \textbf{Direct}  & 0.45 & 0.46 & 0.46 & 0.46 & 0.46 & 0.46\\

       \textbf{DBO} & 1.0 & 1.0 & 0.999 & 0.999 & 0.997 & 0.985 \\
    %\hline
    \end{tabular}
    %\vspace{-1mm}
    \caption{\small{Fairness for trades with response time$> \delta = 20$.}}
    \label{tab:fairness_rt}
    \vspace{-8mm}
\end{table}


\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_fairness3.pdf}
    \vspace{-5.5mm}
    \caption{\small{Fairness}}% ($C_{S1}=1.14$, $\delta=14\mu s$)}%Strategy S1 with $C_{S1}=1.14$, $\delta=14\mu s$}
    \label{subfig:fairness}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_avg_latency2.pdf}
    \vspace{-5.5mm}
    \caption{\small{Mean Latency}}% ($C_{S2}=1.1$)}%Strategy S2 with $C_{S2}=1.2$}
    \label{subfig:mean_latency}
  \end{subfigure}
  \vspace{-4mm}
  \caption{\small{\textbf{Comparison with CloudEx.}}}% with number of participants.}.}}% Except CloudEx, all other schemes used DBO.}% \pg{I wonder if we should the extend the x-axis to 64 or 128 $\mu s$. The point would be too show that all schemes can achieve reasonable performance when response times are high, it is hard when they are low.}}
  \vspace{-5mm}
  \label{fig:sim_participant}
\end{figure}

\subsection{Simulation}
\label{ss:simul}

We use simulation to compare DBO and CloudEx with perfect clock synchronization. We generated network traces to simulate latency between from the CES to RBs and from RBs to the OB. The minimum latency is $50 \mu s$ with random spikes in latency of up to 400 $\mu s$. Figure~\ref{fig:sim_trace} (in the Appendix) illustrates one such trace. For CloudEx we use 150 $\mu s$ threshold for delivery of market data and trade ordering. Note that the exact fairness and latency numbers are depend on the nature of network latency and the numbers here are illustrative.
%We explicitly focus on scenarios which are challenging for DBO.

\noindent
\textbf{Fairness with number of participants}
Compared to DBO, the latency for a trade in CloudEx is governed solely the round-trip latency of the participant. In contrast, DBO's latency is fundamentally bounded by maximum latency across participants. %However, DBO achieves guaranteed response time fairness for trades generated within the horizon.

Figure~\ref{fig:sim_participant} shows the fairness and the average and latency for the two schemes as we scale the number of participants. The response times are between 5 to 20 $\mu s$. We also include direct delivery and max-RTT (latency bound) in the figure. CloudEx outperforms direct delivery however it incurs unfairness when latency spikes beyond the delivery threshold. As expected the fairness numbers for all the schemes are unaffected by the number of MPs. CloudEx incurs an average latency of 300 $\mu s$ of latency (sum of thresholds on the forward and the reverse path). The actual number is slightly higher due to latency spikes. The average latency does not scale with participants. In contrast, both max-RTT and DBO latency scale with participants. We find DBO achieves better latency on average than CloudEx. This is because when network latency is well behaved and close to the minimum (100 $\mu s$ round trip), the max-RTT is low and consequently the DBO latency is low. In contrast, in such scenarios CloudEx still incurs a minimum latency of 300 $\mu s$ in such cases.  However, in this experiment we find that tail latency of DBO and the latency bound (max-RTT) can both exceed the CloudEx latency (see Figure~\ref{fig:sim_tail_delta} in the Appendix). 
As explained earlier this is a fundamental cost to achieve fairness. 

%\pg{Include trades > delta for clocksync. Double check simulation with latency on the reverse path.}

\if 0

\begin{table*}[h!]
\small
    \centering
    \begin{tabular}{c|ccccccc}
    %\hline
    \textbf{Response Times $RT(i, a) (\mu s)$} & \textbf{$5<RT<10$}\\
    \textbf{Response Times $RT(i, a) (\mu s)$} & \textbf{$5<RT<10$} & \textbf{$10<RT<15$} & \textbf{$15<RT<20$} & \textbf{$20<RT<25$} & \textbf{$25<RT<30$} & \textbf{$30<RT<35$} & \textbf{$35<RT<40$} \\\hline
    \textbf{Direct} & \\
    \textbf{DBO $(\delta=20\mu s)$} & 100 & 100 & 100 & 99.98 & 99.92 & 99.78 & 98.52\\
       


    %\hline
    \end{tabular}
    %\vspace{-1mm}
    \caption{\small{Fairness for trades with response times.}}
    \label{tab:exp_rt}
    \vspace{-7mm}
\end{table*}

\fi 
% \begin{figure}[t]
% \centering
%     \includegraphics[width=0.8\columnwidth]{images/fairness_rt.pdf}
%     \label{fig:exp_rt}
%     \vspace{-2mm}
%     \caption{\small{\bf Fairness for trades with response times $\delta$}}
%     \vspace{-2mm}
% \end{figure}

\if 0
\noindent
\textbf{Beyond LRTF: Fairness for trades with response time greater than $\delta$:} To evaluate this scenario, we now repeat the above experiment with 30 participants. Figure~\ref{fig:sim_delta} shows the response time for DBO and CloudEx for different values of response time. The reponse times for each trade is derived from the buckets shown on the x-axis. Fairness with CloudEx remains unaffected with response time. However DBO's fairness drops as the response time exceed the horizon value ($\delta = 20 \mu$ s). Recall that, DBO achieves fairness for such trades if the inter-delivery gap across batches is same. More precsily if the inter-delivery time between the batch that trigger the trade and the last batch that was delivered is same across other participants then the trade is ordered fairly. As response time increases, the separation between the last batch and the batch delivered batch increases. The farther these two batches become the more unlikely it is that their inter-delivery times across participants will be similar. 

In case, we clocks across RB's are synchronized we can use a hybrid of CloudEx's and the pacing in DBO to deliver batches simultaneously across participants (when latency is below the delivery threshold) while also ensuring that the inter-delivery time for consecutive batches is greater than $\delta$. We also evaluate such a hybrid, DBO+ClockSync in ~\ref{fig:sim_delta}. Notice that DBO+ClockSync achieve perfect fairness when response times are less than $\delta$. For other trades, the fairness numbers are close to that of CloudEx. The fairness numbers are slightly worse than CloudEx because pacing elongates the duration of a latency spike (Figure~\ref{fig:latency_b+p}). This difference in Fairness due to pacing is the cost of guaranteeing fairness (LRTF).

\fi





\if 0

\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_fairness3.pdf}
    \vspace{-5.5mm}
    \caption{\small{Fairness}}% ($C_{S1}=1.14$, $\delta=14\mu s$)}%Strategy S1 with $C_{S1}=1.14$, $\delta=14\mu s$}
    \label{subfig:fairness}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_avg_latency2.pdf}
    \vspace{-5.5mm}
    \caption{\small{Mean Latency}}% ($C_{S2}=1.1$)}%Strategy S2 with $C_{S2}=1.2$}
    \label{subfig:mean_latency}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    %\includegraphics[width=\linewidth]{images/ARTF-2.pdf}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_tail_latency2.pdf}
    \vspace{-5.5mm}
    \caption{\small{Tail Latency (p99)}}
    \label{subfig:tail_latency}
  \end{subfigure}
  \vspace{-3mm}
  \caption{\small{\textbf{Fairness and Latency trends with number of participants.}.}}% Except CloudEx, all other schemes used DBO.}% \pg{I wonder if we should the extend the x-axis to 64 or 128 $\mu s$. The point would be too show that all schemes can achieve reasonable performance when response times are high, it is hard when they are low.}}
  \vspace{-4.5mm}
  \label{fig:sim_participant}
\end{figure*}

\fi
\if 0
\begin{figure}[t]
\centering
    \includegraphics[width=0.8\columnwidth]{images/sim_delta2.pdf}
    \vspace{-4mm}
    \caption{\small{\bf Fairness beyond LRTF: Using clocksync with DBO to improve fairness for trades with response time greater than $\delta$.}}
    \label{fig:sim_delta}
\end{figure}
\fi


%\textbf{Improving CloudEx using DBO}