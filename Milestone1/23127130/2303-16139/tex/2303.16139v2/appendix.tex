\section{Proof of Lemma~\ref{lemma:inter_delivery_imp}}
\label{app:lem1}

%\radhika{this is proof of Lemma 2, right?} 
The lemma states that for response time fairness the inter-delivery times should be the same across all  MPs. 
% eg{The theorem states that, for strong fairness, the intervals between consecutive market data deliveries -- inter-delivery times -- should be the same across all MPs. In other words, the delivery clocks at all RBs (at any given delivery clock time) must advance at the same rate.}

\begin{proof}
To prove that the lemma condition is necessary we will show that if this condition is not met then no ordering system exists which can achieve response time fairness for arbitrary trade orders. %\attn{We will constuct a tra}%already said this %
%The reason for this is that the OB does not know the response time of trade orders ($\sum f_i$ is unknown). 

\begin{figure}[t]
\centering
    \includegraphics[trim={0 0 0 1mm},clip,width=0.8\columnwidth]{images/delivery_times.drawio.png}
    \vspace{-3mm}
    \caption{{\small{\bf Proof of Lemma ~\ref{lemma:inter_delivery_imp}.}} }%\pg{Fix the figure. New terminology}}
    \label{fig:proof}
    \vspace{-3mm}
\end{figure}

Consider the following scenario (Figure~\ref{fig:proof}) where the lemma condition is not met. Let $D(i,x+1) - D(i,x) = c1$, $D(j,x+1) - D(j,x) = c2$. Without loss of generality we assume $c1<c2$. 

Consider hypothetical trades $(i,a)$ and $(j,b)$ s.t. $S(i,a) = D(i, x+1) + c3$ and $S(j, b) = D(j, x+1) + c4$. Further, we can pick $S(i, a)$ and $S(j, b)$ s.t. $c3>c4$ and $c1+c3<c2+c4$. %\radhika{update fig to reflect this as well}.
Now we consider two scenarios for how these trades were generated. These two scenarios are indistinguishable from the cloud provider/exchanges perspective.
%Consider a hypothetical trade order $k$ (from MP$_i$) and $l$ (from MP$_j$) s.t. $A_i(k) = D_i(x+1) + c3$ and $A_j(l) = D_j(x+1) + c4$. Further, we can pick $A_i(k)$ and $A_j(l)$ s.t. $c3>c4$ and $c1+c3<c2+c4$. Now we consider two scenarios for how these trades were generated. These two scenarios are indistinguishable at the OB.

\noindent
\text{Case 1:} $TP(i,a) = TP(j, b) = x+1$. Here,
\begin{align}
S(i,a)- D(i,x+1) = c3, S(j,b)-D(j,x+1) = c4.
\end{align}
Since $c3>c4$, condition $C1$  implies that,
%\begin{align}
$O(i,a) > O(j,b)$.
%\label{eq:theorem_1:necessary:case1}
%\end{align}

\noindent
\text{Case 2:} $TP(i,a) = TP(j, b)= x$. Here,
\begin{align}
S(i,a)- D(i,x) = c1+c3, S(j,b)-D(j,x) = c2+c4.
\end{align}
In this case, since $c1+c3<c2+c4$, for response time fairness the ordering must instead satisfy the opposite, $O(i,a) < O(j,b)$. A contradiction! \attn{Thus, no ordering system can achieve response time fairness in both these scenarios.}
%\label{eq:theorem_2:necessary:case1}
%\end{align}
\end{proof}


\section{Proof of Corollary~\ref{cor:inter_delivery_lrtf}}
\label{app:cor_inter_delivery_lrtf}

\begin{proof}
The proof is identical to that of Lemma~\ref{lemma:inter_delivery_imp}. The only difference being, we consider trades (i,a) , (j,b) and trigger point x and x+1, s.t., both $c1+c3$ and $c2+c4$ are less than $\delta$.
\end{proof}

\section{Proof of Theorem~\ref{thm:rb_to_mp_latency}}
\label{app:rb_to_mp_latency}

\begin{proof}
To the prove this theorem we will show that with DBO the ordering of trades $(i,a)$ and $(j,b)$ that meet the Theorem condition is $O(i,a) < O(j,b)$.



Consider a trade $(i,a)$ with response time less than $\delta - B_h(i)$. Let $\hat{D}(i,a)$ represent the delivery time at the RB. The observed submission time at RB ($\hat{S}(i,a)$) for such a trade will be,
\begin{align}
    \hat{S}(i,a) = \hat{D}(i,x) + RT(i,a) + RB\_MP\_L(i,x,a).
\end{align}
where $RB\_MP\_L(i,x,a)$ represents the combined network round trip latency between RB$_i$ and $MP_i$ for trigger point $x$ and trade $(i,a)$ . Because $RB\_MP\_L(i,x,a)$ is bounded by $B_h(i)$, $RT(i,a) + RB\_MP\_L(i,x,a) < \delta$ or $\hat{S}(i,a) < \hat{D}(i,x) + \delta$.

Recall, consecutive batches are atleast separated by $\delta$. This means that the trigger point ($x=TP(i,a)$) must be within the last received batch. The point $ld(i,a)$ is also the last point in this batch and $\hat{D}(i,ld(i,a)) = \hat{D}(i,x)$. The delivery clock for trade $(i,a)$ will thus be: $O(i,a) = DC(i,a) = \langle ld(i,a), RT(i,a) + RB\_MP\_L(i,x,a)\rangle$.



With batching, for participant $j$, $x$ and $ld(i,a)$ also belong to the same batch $\hat{D}(j,ld(i,a)) = \hat{D}(j,x)$.
For a competing trade $(j,b)$ with higher response time, the delivery clock at the time of submission will either read $O(j,b) = DC(j,b)) = \langle ld(i,a)), RT(j,b) + RB\_MP\_L(j,x,b)\rangle$ (if $(j,b)$ was submitted before the next batch, i.e., $\hat{S}(j,b) < \hat{D}(j,ld(i,a)+1)$) or $DC(j, b) = \langle y, \hat{S}(j,b)-\hat{D}(j,y)\rangle$ with $y>ld(i,a)$.

C3 implies that, $RT(i,a) < RT(j,b) - (B_h(i)- B_l(j))$ and $ B_l(i) \leq RB\_MP\_L(i,x,a) \leq B_h(i), B_l(j) \leq RB\_MP\_L(j,x,b) \leq B_h(j)$. As a result, $ RT(i,a) + RB\_MP\_L(i,x,a) < RT(j,b) + RB\_MP\_L(j,x,b)$

As a result, in both the cases, $O(i,a) < O(j,b)$. Hence proved.
\end{proof}

\section{Impact of Losses}
\label{app:impact_losses}
\noindent
\textbf{Impact of market data points being lost:} Like status-quo we advocate market participants requesting any dropped market data points separately. The retransmitted market data point does not update the delivery clock at the release buffer. This way trades generated using the retransmitted data points get affected. However, fairness for all other trades remains unaffected. %\pg{Include commented texted below?}
The latency of the system can get affected as the delivery clock of the participant experiencing losses lags transiently until the next data point is delivered. If data points are generated infrequently, then the delivery clock of the participant might take a large time to recover. To prevent this explicitly, we advocate CES sending periodic heartbeats. However, we believe that major exchanges already generate data at a very high frequency (a data point every 20  $\mu s$) and such heartbeats are not necessary.

%To avoid stalling of the delivery clock, ideally we want the CES to to send heartbeat market data points as well. }


\noindent
\textbf{Impact of trades being lost:} In the event a trade is lost, the participant can retransmit the trade. The retransmitted trade will be tagged by the delivery clock at the time of the retransmission. Such a retransmitted trade will incur unfairness. However, fairness of all other trades remains unaffected.

\noindent
\textbf{Impact of heartbeats being lost:} Lost hearbeats do not impact fairness. However, if a heartbeat is lost then the OB might have to wait an additional time (for the next heartbeat to arrive) before forwarding the trades to the CES increasing latency (Equation~\ref{eq:latency_def}).

\section{Thwarting front-running attacks}
\label{app:front_running}

%\pg{External participants}

%While monotonicity of delivery clocks ensure that participants are incentivized to submit trades as early as possible an delaying trades does offer any competitive advantage, there is still a potential front-running attack possible in our system. In particular, if a participant receives a market data point $x$ through some other way before RB delivers the data point $x$ to the participant then it has a competitive advantage. This scenario though unlikely is still possible.
%A simple to avoid this is to limit that participant cannot talk to anyone beyond the CES. 

%However, we would like the participant machine to use other  ``helper'' machines in the cloud to aid computation. We also want to allow the participants to be able to talk to machines outside the cloud. Participants do use external news streams and feeds from other exchanges to update trading strategies and make trading decisions. We will discuss fairness with respect to such streams shortly.  

%Allowing such communication naively can lead to attacks.
%By restricting communication, it is possible to ensure that no participant gets early access to market data 
%and thwart such front-running attacks. 

%
%\pg{Which of two alternatives is better?}
%

We impose two simple constraints on communication to preven front running. \begin{enumerate*}[label=(\arabic*)]\item A participant machine and its helper machines can communicate with each other freely but they cannot communicate with any other machines in the cloud. This restriction can be imposed easily by cloud providers today using security groups. This restriction ensures that a participant machine cannot get market data from other participant machines in the cloud directly. Next, we will ensure that a participant machine cannot get an earlier market data feed from outside the cloud. 
We will do so by restricting that a participant can only send data point x out of the cloud, when x has been delivered to all participants in the cloud. This way, market data points can only be available outside the cloud once they have been delivered to all the participants.
\item The helper machines cannot send data outside the cloud. Any data (excluding the trade orders) from a participant being sent outside the cloud is tagged by the delivery clock at the RB and buffered at a gateway. The data sent by the participant could potentially be a market data point with id less than or equal to the last point id (first tuple) of the delivery clock time stamp. The gateway thus buffers this data until it is sure that the all data points with id less than the last data point id in the delivery clock time stamp have been delivered. For this purpose, RB's periodically communicate their delivery clock to the gateway. 
%
%A simple way to achieve this is for each RB to send other RBs periodic beacons communicating the status of its delivery clock. This way each RB can maintain a lower bound on the delivery clocks at other RBs. 
\end{enumerate*}
%\pg{include this? a bit hand-wavy and not clean. There is one challenge to be solved though. If data delivery to a particular participant is straggling then the gateway buffer can get bloated. It is not necessary for the gateway to wait for such straggler if we disable the incoming data to the straggler. The gateway can identify such stragglers and then disable any data coming from outside the cloud.}


%There are other ways to thwart front-running that impose weaker restrictions on communication or are easier to implement. We chose to present this one for its simplicity.




\section{Latency for network traces used in simulation}

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/simulation_latency.pdf}
    \caption{\small{\bf Network trace used in simulation.}}
    \label{fig:sim_trace}
\end{figure}

\section{Tail Latency with number of participants}

\begin{figure}[t]
\centering
    \includegraphics[width=0.8\columnwidth]{images/sim_part_tail_latency2.pdf}
    \vspace{-4mm}
    \caption{\small{\bf Fairness and Latency trends with number of participants.}}
    \label{fig:sim_tail_delta}
\end{figure}
