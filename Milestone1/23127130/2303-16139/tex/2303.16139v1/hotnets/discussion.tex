\section{Discussion}
\label{s:disc}
%We discuss few additional challenges in moving financial exchanges to the cloud.

%\subsection{Implementation of a delivery clock system}

\subsection{DBO Cloud Architecture}
%\pg{Ilias, I wonder is this should be 5.2 instead}\pg{maybe cut short}

In a typical on-premise deployment, the CES servers and physical network are part of the trusted infrastructure of the exchange: the exchange operators have exclusive access to the physical machines, and network cables. On the other end, the MPs own the physical servers that connect to the exchange network. Migrating such components to the public cloud seems intuitive: CES servers and MPs could correspond to virtual machines owned by the different parties.

Compared to on-premise deployments one important differentiation is that our solution requires the use of Release Buffer components for correctness, hence those need to be part of the trusted infrastructure (i.e., the MPs should have no control over the RBs). As we have discussed, the RBs should be close enough to the MPs, so that the latency between them does not impact fairness. The Release Buffer components need to {\it pace} the delivery of data to MPs. The cloud operator could provide the facilities required for the RB functionality. We believe that such functionality, could potentially be embedded in many places such as the hypervisor of cloud nodes that host MPs, or better the programmable NIC of such cloud hosts (most operators already have their own programmable smartNICs~\cite{accelnet} deployed). There are several challenges that need to be considered such as performance isolation, but the cloud operator already has fine-grained control over the cloud hardware/software stack and can address this problem easily. %One other option worth exploring for the release buffer functionality, would be the use of programmable Top-of-Rack switches~\cite{tofino}.



%\subsection{Multi-machine, Multi-cloud, Multi-exchange}
%\label{ss:multi}
\subsection{Enabling a MP to use many machines}
\label{ss:multi_machine}
%\pg{Chaitanya: citation for front running attack.}
So far, we have assumed that each MP is a single machine (or a virtual machine) sitting in the cloud. However, a single machine might not be enough and a MP might want to use multiple ``helper'' machines for computation. These helper machines can be located either in the cloud or outside the cloud (e.g., a MP might want to run its own on-prem datacenter). The ``front-end'' machine of the MP (that is receiving the real-time market data from the RB) should be able to communicate with these helper machines. 

Allowing such communication naively can lead to ``front-running'' attacks. In principle, a MP (whose delivery clock is lagging behind) might be able to receive real-time market data from another MP earlier than receiving it from the CES. Since, trades are ordered based on the delivery clock, earlier access to market data gives such a MP an unfair advantage over other MPs. 
It is possible to ensure that no MP gets early access to market data %(at the cost of introducing latency in messages from the front-end to helpers outside the cloud)
and thwart such front-running attacks. Here, we give a rough sketch of the solution. 
%
%\pg{Which of two alternatives is better?}
%
There are two simple constraints. \begin{enumerate*}[label=(\arabic*)]\item Only the front-end of a MP is allowed to submit the trade orders. 
\item Any data (excluding the trade orders) from the front-end to any other machine is buffered at the corresponding RB until the RB is sure that the delivery clocks at all other RBs have advanced beyond its delivery clock when it received this data.\end{enumerate*} A simple way to achieve this is for each RB to send other RBs periodic beacons communicating the status of its delivery clock. This way each RB can maintain a lower bound on the delivery clocks at other RBs. There are other ways to thwart front-running that impose weaker restrictions on communication or are easier to implement. We chose to present this one for its simplicity.

% \eg{There are two simple constraints. \begin{enumerate*}[label=(\arabic*)]\item Only the front-end of a MP is allowed to submit the trade orders. \item Any data (excluding the trade orders) from the front-end is buffered by the cloud network to make sure that the delivery clocks at all other RBs have advanced beyond the delivery clock time when this data transmission was initiated.\end{enumerate*} A simple way to ensure this is to transmit all outgoing data through the RB and for each RB to send periodic beacons communicating the status of it's delivery clock. This way each RB can maintain a lower bound on the delivery clocks at other RBs. Note that there are other ways to thwart front-running that impose weaker restrictions on communication or are easier to implement. We chose to present this one for it's simplicity.}

\if 0
\pg{There are three simple constraints. (1) Only the front-end of a MP is allowed to submit the trade orders. (2) A helper within the cloud is only allowed to talk to it's MP's front-end and helpers in the cloud. 
%Most cloud-providers already employ proxies that can be used to enforce this restriction.  
(3) Any data (excluding the trade orders) from the front-end to other machines (excluding the MP's helpers in the cloud) is buffered at the corresponding RB until the RB is sure that the delivery clocks at all other RBs have advanced beyond its delivery clock when the it received this data. A simple way to achieve this is for each RB to send other RBs periodic beacons communicating the status of it's delivery clock. This way each RB can maintain a lower bound on the delivery clocks at other RBs.}
\fi


\if 0
Only the front-end of a MP is allowed to submit trade orders. %\pg{Add a note on why this additional latency is not bad?Chaitanya can we say that typically a MP's fast computations can fit within a VM or something?}

\noindent
\textit{Helpers in the cloud:} Such a helper is only allowed to talk to it's MP's front-end and helpers in the cloud. Most cloud-providers already employ proxies that can be used to enforce this restriction.

\noindent
\textit{Helpers outside the cloud:} Such a helper can talk to any machine outside the cloud (including helpers of other MPs). But, it is only allowed to talk to the corresponding front-end in the cloud. Any data from the front-end to outside the cloud is buffered at the corresponding RB until the RB is sure that the delivery clocks at all other RBs have advanced beyond its delivery clock when the it received this data.%\footnote{To avoid buffer overruns at the RB, the outgoing data can also be buffered at other points in the network at the cost of some additional latency.} 
A simple way to achieve this is for each RB to send other RBs periodic beacons communicating the status of its delivery clock. This way each RB can maintain a lower bound on the delivery clocks at other RBs. %The above mechanism ensures that a MP cannot have earlier access to market data from another MP.

\fi






\subsection{Enabling a MP to use a different cloud-provider}
A MP might prefer a different cloud-provider (for its front-end and helpers) than the one the financial exchange is located in. 
%or offer lower latency between the front-end and helpers that could help the MP to make trade decisions faster. 
Achieving fairness in such scenarios requires participating cloud-providers to agree on a common implementation of the RB and being truthful in tagging trades for DBO. Further, to keep end-to-end latency low, cloud-providers will likely need to enter peering agreements~\cite{sky}. %to keep the cross-cloud latency in check. 
While there are many challenges in realizing this vision, we believe our approach for fairness will prove to be even more useful here.\footnote{Achieving tight clock synchronization at RBs across cloud providers will likely be harder~\cite{huygens} and latency across clouds is likely more unpredictable.}



\subsection{Fairness with multiple exchanges}
\attn{
%To our knowledge, 
Existing financial exchanges %(and proposed solutions) 
do not optimize for ensuring fair-access to market data streams from other financial exchanges. Currently, competing financial exchanges are not colocated. MPs can go to great lengths to reduce latency for market data streams from financial exchanges that they are not colocated with~\cite{frequent_batch_auctions}.
%\footnote{\attn{For example, Spread Networks spent 300 million dollars for a private fiber optic cable to help HFT firms in New York connect to financial exchanges in Chicago and vice-versa~\cite{frequent_batch_auctions}.}}
In future, if multiple financial exchanges move to the cloud, participating cloud providers can take up the task of ensuring fairness across data streams from all such exchanges. A potential way to do this is to stitch together all such data streams into a single stream using a common serialization point. %\footnote{\attn{Creating such a single stream will likely require competing financial exchanges to agree on a fair ordering of their data in this stream.}} 
Our approach can then be used to order trades fairly based on the delivery time of the data in this super stream. Such an ecosystem offers a lot of flexibility and can enable both MPs and financial exchanges to move across cloud regions and cloud providers to optimize cost and performance.}




\if 0
\subsection{Beyond Fairness}

While the focus of this article has been on fair market access, market regulations also have additional implications.

1) Market Transparency and Coordination: Because traders trade and hedge across numerous markets, there are significant interdependencies between them. Markets must be transparent and coordinated in order to achieve efficiency and the best pricing. The National Best Bid or Offer (NBBO) rule established this obligation (Reg NMS, 1972 and 2005).

2) Market Surveillance: Because markets are so important to the economy's general stability, exchanges are essential to avoid market manipulation and fraud. This is accomplished through the use of audit trails, surveillance, and disciplinary measures. To do so, the exchanges can use cloud hosting to have access to machine learning techniques and infrastructure.

3) Market Stability and Systemic Risks: Exchanges must also have enough capacity to deal with big trading volumes and spikes.

Low latency and real-time data access are frequently required by these needs. However, cloud virtualization and sharing overheads prevent this. The "Lift and Shift" method is ineffective.


Another difficulty is replicating the "determinism" of an on-premise solution. Exchanges must, in particular, ensure that transactions are processed in a logical order based on entry time and priority. This is complicated by the cloud's inherent jitter. Kernel by-passes that give access to low-level network card controls, as well as intelligent workload placement such as avoiding workload sprawl, co-locating similar workloads in close proximity, and isolating nodes when latency-sensitive workloads are operating, can help address these issues. Each of these tasks necessitates the development of new algorithms.
\fi


%\subsection{Requirements beyond fairness}
%\pg{Chaitanya, since we are running over if we can wrap it within half a column it would be great.}

