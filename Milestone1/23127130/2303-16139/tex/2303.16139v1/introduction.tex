\section{Introduction}

Major financial exchanges such as NASDAQ, Chicago Mercantile Exchange (CME), and London Stock Exchange (LSE) have recently expressed interest in migrating their workloads to the cloud aiming to significantly reduce their capital expenditure, improve scalability and reduce operational burden. Major market participants of such  exchanges would also benefit from such migration as they are also maintaining an expensive on-premise infrastructure for data analysis, and regression modelling to formulate their trading strategies. For cloud providers such as Amazon, Google, and Microsoft, this is a big business opportunity. Migrating financial exchanges to the cloud is a mutually beneficial undertaking for all parties involved. 
%
%Financial exchanges spend a huge amount of money maintaining their own datacenters, this excessive cost is passed down to participants trading with the exchanges. For example,  NASDAQ charges its \textit{premium} participants 600,000\$ a year for its fastest market data feed and co location to their own datacenters.
%The cloud providers are hoping that the participants/traders of these financial exchanges will also move their infrastructure to the cloud bringing even more revenue.
%

 To this end, cloud providers and financial exchanges have announced long-term partnerships to facilitate such a move~\cite{nasdaq_cme_an, nasdaq_aws}. Both parties perceive that this migration will be quite challenging, especially when considering all different workloads (businesses) that are currently accommodated in the exchanges' on-premise infrastructure. 
 %\pg{Say these partnerships are long term}
 In this paper, we focus on ``speed race'' ~\cite{frequent_batch_auctions, libra} trading which is an important and highly profitable business for both the financial exchanges and the market traders. Briefly, `speed race' trading is a form of systematic electronic trading where market participants (``MPs'') use high-performance computers to execute strategies that aim to rapidly react and exploit new opportunities presented in the market (e.g., due to volatility, price discrepancies etc). Speed race traders, also known as High-Frequency Traders invest large amounts of money for hardware, systems and algorithmic development to achieve impressively low reaction times (\textmu s- or even ns- scale). This trading business is only viable if market participants can compete in a \textit{fair} playground guaranteed by the Central Exchange Server (CES) operators. Equality of opportunity -- fairness -- in such case means that all market participants must get provably simultaneous access to market data, as well as their subsequent trades must be executed in the exact order they were generated (i.e. placed in the wire). %\sadjad{i think you should drop the quotation marks for speed race, except for the first occurrence}

 With on-premise deployments financial exchanges guarantee fairness for speed race trading by guaranteeing equal bi-directional latency to the relevant market participants.  Exchanges go to a great extent to ensure fairness for their co-located MP customers; it is not uncommon, for example, to use layer-1 fan-out switches for market data stream replication and equal-length cables to all co-located MPs. On the contrary, public cloud datacenter networks do not provide such guarantees as they were originally designed for a heterogeneous, multi-tenant environment, aiming to accommodate diverse workloads. Even if the MPs are located within the same cloud region as the CES, it is hard to guarantee that the latency between CES and various MPs will be the same. Copper and fiber optics cables are not necessarily of equal length, network traffic is not evenly balanced among the different paths, multiple vendors' network elements have different performance characteristics, network oversubscription is still common, and network quality of service mechanisms for concurrent workloads are only best effort.

This problem has recently received significant attention from the academic community. Proposed solutions aim to achieve fairness by attempting to provide equal (yet inflated) bi-directional latencies in the cloud relying on tight clock synchronization and buffering for market data delivery (~\cite{cloudex}). As we explain later, such approaches are fragile because latencies in datacenter networks are not only variable, but also unbounded. Other proposals, require  intrusive modifications to existing CES implementations to work. 

In this paper, we seek to address the problem of fairness for speed race trading in cloud environments. Our key insight is that equal bi-directional latencies are not strictly required to achieve fairness. %Considering fundamental workload characteristics of speed trading, we argue that it is feasible to achieve fairness in commodity cloud datacenters. 
For speed trading, instead of ex-ante equalizing latency, we can post facto correct for any latency differences in delivery of data by ordering trades differently. We introduce logical \emph{delivery clocks} that track time at each trader relative to when market data were received. 
We present \textit{Delivery Based Ordering}, a system that uses delivery clocks to order trades and achieve guaranteed fairness in network topologies where latency is non-deterministic and unbounded. 

We implement a real DBO system, which we evaluate on a bare-metal server testbed leveraging programmable NICs. We also evaluate DBO in a public cloud deployment using standard VMs: our system achieves guaranteed fairness and sub-100us p999 latency while servicing 125K trades per second.
 
% Public clouds cannot guarantee equal bi-directional latency to Market participants
%and as a result these partnerships are long-term with cloud providers aiming to move the exchanges in the next five years. This is interesting given that the central exchange server processing the trades in a simple single threaded process \pg{cite signals and thread podcast} \radhika{grammar error in last sentence}.




%\radhika{adding a rough sketch if it's of any help. Feel free to disregard if you have better ideas on how to structure the intro}

%Para 1: Financial exchanges wanting to move to cloud. Why it's good for both parties.

%Para 2: speed trading and fairness 

%Para 3: Fairness provided by ensuring equal bi-directional latency. Not possible to do so in clouds.

%Para 4: Existing solutions: 
%can guarantee fairness only if latency is bounded (and additionally require clock synchronization [cite cloudex] or modifications to the exchange servers [cite Libra]). For instance, CloudEx strives to mimic environments with equal bi-directional latency by (i) holding on to data points generated at time $t$ for a fixed period time ($t + C_1$) at a trusted entity co-located with the market participant (called the \emph{release buffer}), before delivering them to all market participants simultaneously (assuming synchronized clocks), and (ii) holding on to trade requests generated at time $t$ for a fixed period of time ($t + C_2$) before forwarding them to the CES. 

%Para 5: Our goal: how do we achieve fairness for speed racing in cloud-hosted financial exchanges, where network latency may be variable and unbounded? 

%Para 6: Our approach is based on the simple insight that we do not need to enforce equal bi-directional latency for achieving fairness. We can instead directly order trades based on their response times (i.e. how long it takes for the market participant to generate a trade request after receiving the trigger data point). The response time can be measured locally at the release buffer (that intercepts all traffic exchanged between the market participant and the exchange server), without requiring any coordination across market participants. 

%Para 7: A key challenge in realizing this in practice is that the release buffer can only see when a trade request is generated by the market participant, and has no visibility into which data point triggered it. This makes it impossible for it to compute the response time for a given data point. 

%Para 8: We address this challenge by introducing the notion of LRTF, which guarantees that....


%Para 9: We implement a system to provide LRTF. Release buffer on the NIC. Implementing LRTF requires dealing with several ppractical challenges: how long to wait for requests with potentially lower response times to arrive before forwarding a request to the exchange server? other practical challenges...

%Para 10: 1-2 lines on how we evaluate in the real cloud cluster and what the results are. 