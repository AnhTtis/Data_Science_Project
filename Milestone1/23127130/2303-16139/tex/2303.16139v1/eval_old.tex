\section{Evaluation}
\label{s:eval}


We wish to understand how our solution affects fairness and overall latency on a financial-exchange deployment. We also want to better understand the tradeoffs introduced by each mechanism proposed in our system: how does DBO alone improve direct delivery, how do heartbeats affect end-to-end latency, could a `straggler' RB  affect the system's latency etc.
\pg{I think we don't do heartbeats. Maybe add we are most interested in understanding how to performance an latency numbers look like in public cloud. }


\subsection{Methodology}
\label{ss:eval_methodology}


 
For all of the experiments (except simulation) presented in this section we leverage our prototype CES and MP implementations. On the CES side, we generate and distribute data to all Market Participants at fixed intervals. The market data arrive at the RBs, which later on release them to the  Market Participants. The MP implementation relies on busy-polling and kernel-bypass for low-latency access to the incoming market data packets, but does not utilize a sophisticated algorithm for trading decisions; it rather busy-waits for a \eg{random} pre-configured duration (reaction time) before generating a trade. We set each MP's  reaction time accordingly so that we can derive the expected final ordering at the OB and evaluate fairness. 

\noindent\textbf{Fairness metric:} For any number of market participants, perfect fairness is achieved when all competing trades among all unique pairs of participants are fully ordered (from faster to slower). We define the metric of fairness as the ratio of the number of competing trade sets  that were ordered correctly to the total number of competing trade sets for all unique pairs of market participants.

\pg{Note: I use forwarding time instead of execution time}.

\noindent\textbf{End-to-end latency} \pg{Edit: We define end-to-end latency of a trade using Equation~\ref{eq:latency_def} ($F(i,a)-G(x)-RT(i,a)$). Generation time and forwarding time are measured at the CES. For the purpose of reporting latency  and fairness (and not for ordering trades in DBO), we assume that the trigger point is known. We use it to calculate the response time of trades at the release buffer.}

%We define end-to-end latency of any trade, as the duration measured from the market data id transmission (at the CES) until a trade tagged with the same id is executed by the matching engine. This measurement is taken on the CES. 
Our definition of fairness is sufficient for all trades that were generated as a response to some market data within the limited-time horizon, but it could be inaccurate for `slow'  or `non-reactive` trades as they will likely be tagged with a later DBO id. Our solution does not ensure fairness out of the limited horizon, but we explicitly take into account this limitation when presenting latency results in the rest of the experiments.  

 We  evaluate our solution on three different setups: \begin{enumerate*}[label=(\alph*)]\item on-premise, bare-metal testbed deployment, and \item public-cloud-based deployment, \item simulation \end{enumerate*}.

\pg{Evaluation schemes: We evaluate four schemes. \begin{enumerate*}\item Direct delivery: This is the baseline scheme. There is no release buffer or ordering buffer and both trades and market data points just incur the underlying network latency. \item Direct w/ DC: Data is delivered directly to the participants without any batching or pacing. However, the trades are ordered using the delivery clock timestamp. This scheme is used to show the importance of ordering using delivery clocks on its own. \item DBO: We use different values for the horizon ($\delta$) and the batch size($(1+\kappa) \cdot \delta$). \item CloudEx: CloudEx requires fine-grained clock synchronization, which is not available in our test-bed and cloud experiments. Due to inaccuracies in clock-synchronization, in our experiments we experience frequent release buffer and ordering buffer overruns. We only report results for CloudEx in simulation where we assume perfectly synchronized clocks. We also report the optimal latency (max network round-trip-latency Theorem~\ref{thm:latency}) for achieving perfect response time fairness\end{enumerate*}.}

\subsection{Evalution on DPU-enabled baremetal servers}
\label{ss:on-premise-eval}

Our lab setup consists of three machines: one CES server and two MP servers. The CES server is equipped with an Nvidia ConnectX-5 NIC with two 100Gbps ports. Each MP server hosts one Nvidia BlueField-2 DPU with two 100Gbps ports. The server has a dual-CPU Intel Xeon processor running at 3.1 GHz. Each BlueField-2 DPU has eight ARMv8 A72 cores. All machines are connected via a 100GbE switch. We run Linux kernel (v5.4.0) and DPDK (v21.11) for the CES, RB, MP network engines.

The CES is generating market data every $40\mu s$ ($25K$ ticks per second), and the market participant servers are generating responses within $\delta$ time horizon since the reception of the data. The Release Buffer component is executing on the Bluefield-2 DPU's SoC.


Table~\ref{tbl:bluefield} shows the achieved fairness and latency of our system. Delivery clocks based ordering on top of direct delivery significantly enhances fairness, while DBO achieves perfect fairness. \pg{EDIT: Since with both these schemes at the OB we need to wait for the slowest participant, the latency is higher. As explained earlier, waiting for the slowest participant is necessary for achieving fairness.}

\pg{We should explain these results. The next statement seems inconsitent with the results?}
Even though this is a bare-metal, on-premise lab setup with no interfering traffic the tail latency is higher than expected. Our performance analysis indicates that this overhead is due to scheduling artifacts in the Bluefield ARM cores, and higher SoC-to-NIC latencies.\pg{WHAT SHOULD WE SAY ABOUT SOC TO  NIC LATENCY? how are we measuing fairness, what is the response tiem} We are currently working with NVIDIA to utilize their hardware-assisted pacing and NIC timestamping features to improve performance.%\pg{Should we say for calculating fairness ratio we use time stamps taken at the RB? What are the response times?}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{c|c|cccc}
    %\hline
    & \textbf{Fairness} & \multicolumn{4}{c}{\textbf{Latency $(\mu s)$}} \\
    & \textbf{(\%)} & \textbf{avg} & \textbf{p50} & \textbf{p99} & \textbf{p999} \\
    \hline
       \textbf{Direct} & 85.79 & 10.15 & 9.70 & 16.40 & 26.60 \\
       \textbf{Direct w/ DC} &  97.01 & 23.46 & 15.09 & 41.55 & 53.80 \\
       \textbf{DBO (20,25)}  & 100 &  15.92 & 12.16 & 28.82 & 46.80 \\
        \textbf{DBO (45,60)}  & 100 & 28.18 & 20.13 & 64.86 & 94.85  \\
        \textbf{DBO (80,120)}  & 100 & 56.05 & 52.71 & 104.31 & 120.42  \\


    %\hline
    \end{tabular}
    %\vspace{-1mm}
    \caption{\small{Fairness and trade latency results on bare metal servers with BlueField-based RB implementation.} \pg{Something is wrong here, how can latency of direct w/DC be higher than DBO? Can we report max of RTT as well?}}
    \label{tbl:bluefield}
    \vspace{-7mm}
\end{table}




\subsection{Cloud-hosted Testbed}
\label{ss:cloud-eval}



We wish to understand how our system performs in a real public cloud-based deployment with several market participants. As discussed in \ref{ss:release_buffer}, we do not have access to the cloud providers' programmable NICs to deploy the RB functionality. To work around this limitation, we have adjusted our RB implementation so that it runs as a co-located  process with the market participant's execution engine on the MP VMs. In such configuration, the RB is using a kernel-bypass network stack to take over a dedicated vNIC which it uses to receive the UDP stream of market data from the CES, and to send back any trades submitted by the MP. To facilitate fast MP-to-RB communication we rely on shared-memory-based IPC primitives. Clearly, such a solution does not provide any security guarantees, as the RBs run on VMs owned by the market participants which are not part of the CES' Trusted Computing Base, and could easily tamper with the RB's market data delivery engine or the response time measurements. It allows us, however, to evaluate the real-world performance (i.e., achievable throughput and latency) of our DBO system in a public cloud deployment.



\begin{table}[h!]
\small
    \centering
    \begin{tabular}{c|c|cccc}
    %\hline
    & \textbf{Fairness} & \multicolumn{4}{c}{\textbf{Latency $(\mu s)$}} \\
    & \textbf{(\%)} & \textbf{avg} & \textbf{p50} & \textbf{p99} & \textbf{p999} \\
    \hline
       \textbf{Direct} & 57.61 & 27.9 & 27.48 & 32.5 & 44.03 \\
       \textbf{Direct w/ DC} &  89.84 & 47.15 & 47.10 & 57.23 & 312.92 \\
       \textbf{DBO (20,25)}  & 100 &  55.78 & 47.09 & 59.40 & 1366.15 \\
        \textbf{DBO (45,60)}  & 100 & 77.21 & 52.54 & 135.27 & 573.70  \\
        \textbf{DBO (80,120)}  & 100 & 89.32 & 86.89 & 133.23 & 459.97  \\
        \textbf{Max-RTT}  & - & 32.94 & 32.14 & 42.63 & 105.49  \\


    %\hline
    \end{tabular}
    %\vspace{-1mm}
    \caption{\small{Fairness and end-to-end latency results for different schemes.} \pg{Drop p999}}
    \label{tab:fairness_latency}
    \vspace{-7mm}
\end{table}

We set out to evaluate the fairness and end-to-end latency of different schemes. We deploy ten market participants and one Central Exchange Server as virtual machines in Microsoft Azure. We configure the  aggregate service rate to $125,000$ transactions (trades) per second (market data generation interval is fixed to $40\mu s$). Table~\ref{tab:fairness_latency} summarizes the achieved fairness and end-to-end latency results across different schemes. \pg{Can we include a citation here for number of trades?}

\noindent\textbf{Fairness:} Direct delivery achieves poor fairness in our experiments. When combined with DC-based ordering at the OB  fairness is improved significantly. Finally, DBO (regardless of $\delta$) always achieve perfect fairness as all MPs are configured to (randomly) generate trades within the limited horizon. We discuss fairness for slow responders in Section~\ref{ss:simul}.  

\noindent\textbf{Latency:}  As expected, direct delivery achieves  the lowest latency, at the cost of fairness. 
%Direct delivery with Delivery Clock-based ordering, is an enhanced version of direct delivery; as mentioned previously we maintain the delivery clock at the RB and tag each trade received with it, but there is no other delay (pacing, batching etc) introduced at the MP's end. Instead,  the OB sorts all trades based on their DC-derived timestamp and forwards them to the ME when it has heard from all RBs. In practice, this means that the  end-to-end latency for all MPs is effectively \textit{inflated} to the worst CES-to-MP network RTT experienced.
As discussed, DBO trades off latency to achieve perfect fairness. In particular, the latency is bounded by maximum network round-trip latency. 
Batching and pacing further add to this optimal latency. We see that increasing $\delta$, increases the latency. When the network is well behaved this increase is linear. However, we find that 

%When the network is well behaved, the end-to-end latency is similar to that of direct delivery with DC-based ordering which constitutes optimal for DBO. %The increased tail latency results presented here are due to OS scheduling artifacts and network events such as possible heartbeat drops or RTT spikes. 
%\im{I am handwaiving here and i don't like it} 

\subsubsection{Understanding DBO latency:}

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/latency_cdf_bf2.png}
    % \includegraphics[width=0.95\columnwidth]{images/latency_cdfs2.pdf}
    \caption{\small{\bf CDFs of the end-to-end trade latency for various DBO configurations on bare metal servers with BlueField-based RB implementation. \pg{we need comparison with direct delivery here. And fairness numbers}}}
\end{figure}

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/latency_cdfs.pdf}
    % \includegraphics[width=0.95\columnwidth]{images/latency_cdfs2.pdf}
    \caption{\small{\bf CDFs of the end-to-end trade  latency for various DBO configurations. Max-RTT CDF corresponds to the highest network RTT among all participants.}}
    \label{fig:latency_cdfs}
\end{figure}

How does batching and pacing affect end-to-end trade latency? Figure~\ref{fig:latency_cdfs} illustrates the latency with different DBO configurations. 

\im{I don't think we have space for that, and I don't understand why MaxRTT CDF does not match DBO(20,25)}



\subsubsection{Impact of straggler RBs}

\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/straggler_mitigation.pdf}
    \caption{\small{\bf Stragglers' impact to end-to-end trade  latency for all market participants. }}
    \label{fig:straggler_mitigation}
\end{figure}

At this point, it is important to differentiate a straggler RB from slow MP responders: the RB is part of the Trusted Computing Base and hence correct operation is critical for good end-to-end system performance. Still, failures and non-determinism stemming from the network need to be taken into account in our system design. 

A straggler RB (i.e., experiencing high latencies or packet drops) can drastically affect the end-to-end system latency and throughput. This happens because the OB needs to hear from all RBs before dequeuing trades to the matching engine, so that fairness is guaranteed. In Figure~\ref{fig:straggler_mitigation} we evaluate how latency is affected with a straggler; we emulate this by configuring one of the RBs to drop all packets (including outgoing hearbeats) as would happen in the case of a transient network outage. Such events could potentially stall to the market indefinitely; to mitigate this issue the OB maintains a timeout (in this case set to 250us) and on expiration releases trades for the current DBO id to the ME. In such cases, fairness guarantees do no longer hold for the straggler.

A production deployment should leverage a mechanism to detect and flag stragglers quickly; upon detection, straggler RBs should be removed from the OB's shard so that the market does not unnecessarily  slow down for prolonged periods.

\subsection{Latency overhead with multiple participants}

DBO \textit{post} corrects for latency variance at the OB. As the number of market participants increases, network RTT variance and asynchrony might be pronounced and can cause higher delays at the Ordering Buffer. In Figure~\ref{fig:ob_delay} we plot the time spent at the OB as a function of the market participants. Note, that the RB components remain unaffected as the number of participants gets higher.


\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/ob_delay_boxplot2.pdf}
    \caption{\small{\bf Trades' buffering delay at the Ordering Buffer. \im{make this figure smaller.}}}
    \label{fig:ob_delay}
\end{figure}





\subsection{Simulation}
\label{ss:simul}

We evaluate fairness and end-to-end latency of DBO as a function of the number of market participants in simulation, and we compare against Cloudex and direct delivery schemes.

Figure~\ref{subfig:fairness} shows the fairness achieved by each scheme. In our simulation, we have modelled network RTTs to follow a .... distribution and market participants' to generate trades within (0, $\delta$)us since the market data reception. DBO achieves 100\% latency regardless of the number of market participants. 



\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/simulation_latency.pdf}
    \caption{\small{\bf Network trace used in simulation.}}
    \label{fig:sim_trace}
\end{figure}


\begin{figure*}[t]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_fairness.pdf}
    \vspace{-5.5mm}
    \caption{\small{Fairness}}% ($C_{S1}=1.14$, $\delta=14\mu s$)}%Strategy S1 with $C_{S1}=1.14$, $\delta=14\mu s$}
    \label{subfig:fairness}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_avg_latency.pdf}
    \vspace{-5.5mm}
    \caption{\small{Mean Latency}}% ($C_{S2}=1.1$)}%Strategy S2 with $C_{S2}=1.2$}
    \label{subfig:mean_latency}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    %\includegraphics[width=\linewidth]{images/ARTF-2.pdf}
    \includegraphics[trim={0 0 0 1mm},clip,width=\linewidth]{images/sim_part_tail_latency.pdf}
    \vspace{-5.5mm}
    \caption{\small{Tail Latency}}
    \label{subfig:tail_latency}
  \end{subfigure}
  \vspace{-3mm}
  \caption{\small{\textbf{Fairness and Latency trends with number of participants.}.}}% Except CloudEx, all other schemes used DBO.}% \pg{I wonder if we should the extend the x-axis to 64 or 128 $\mu s$. The point would be too show that all schemes can achieve reasonable performance when response times are high, it is hard when they are low.}}
  \vspace{-4.5mm}
  \label{fig:sim_participant}
\end{figure*}



\begin{figure}[t]
\centering
    \includegraphics[width=0.95\columnwidth]{images/sim_delta.pdf}
    \caption{\small{\bf Fairness beyond LRTF: Using clocksync with DBO.}}
    \label{fig:sim_delta}
\end{figure}
