		%% Copyright 2007-2020 Elsevier Ltd
		%% This file is part of the 'Elsarticle Bundle'.
		%% ---------------------------------------------
		%% 
		%% It may be distributed under the conditions of the LaTeX Project Public
		%% License, either version 1.2 of this license or (at your option) any
		%% later version.  The latest version of this license is in
		%%    http://www.latex-project.org/lppl.txt
		%% and version 1.2 or later is part of all distributions of LaTeX
		%% version 1999/12/01 or later.
		%% 
		%% The list of all files belonging to the 'Elsarticle Bundle' is
		%% given in the file `manifest.txt'.
		%% 
		
		%% Template article for Elsevier's document class `elsarticle'
		%% with numbered style bibliographic references
		%% SP 2008/03/01
		%%
		%% 
		%%
		%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
		%%
		%%
		\documentclass[preprint,12pt]{elsarticle}
		
		%% Use the option review to obtain double line spacing
		%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}
		
		%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
		%% for a journal layout:
		%% \documentclass[final,1p,times]{elsarticle}
		%% \documentclass[final,1p,times,twocolumn]{elsarticle}
		%% \documentclass[final,3p,times]{elsarticle}
		%% \documentclass[final,3p,times,twocolumn]{elsarticle}
		%% \documentclass[final,5p,times]{elsarticle}
		%% \documentclass[final,5p,times,twocolumn]{elsarticle}
		
		%% For including figures, graphicx.sty has been loaded in
		%% elsarticle.cls. If you prefer to use the old commands
		%% please give \usepackage{epsfig}
		
		%% The amssymb package provides various useful mathematical symbols
		
		\usepackage{amsmath, amsfonts, amssymb, amsthm}
		\usepackage{geometry}
		\usepackage{booktabs}
		\usepackage{float}
		
		%% The amsthm package provides extended theorem environments
		%% \usepackage{amsthm}
		
		%% The lineno packages adds line numbers. Start line numbering with
		%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
		%% for the whole article with \linenumbers.
		%% \usepackage{lineno}
		
		\journal{journal}
		
		\begin{document}
		
		\begin{frontmatter}
		
		%% Title, authors and addresses
		
		%% use the tnoteref command within \title for footnotes;
		%% use the tnotetext command for theassociated footnote;
		%% use the fnref command within \author or \address for footnotes;
		%% use the fntext command for theassociated footnote;
		%% use the corref command within \author for corresponding author footnotes;
		%% use the cortext command for theassociated footnote;
		%% use the ead command for the email address,
		%% and the form \ead[url] for the home page:
		%% \title{Title\tnoteref{label1}}
		%% \tnotetext[label1]{}
		%% \author{Name\corref{cor1}\fnref{label2}}
		%% \ead{email address}
		%% \ead[url]{home page}
		%% \fntext[label2]{}
		%% \cortext[cor1]{}
		%% \affiliation{organization={},
		%%             addressline={},
		%%             city={},
		%%             postcode={},
		%%             state={},
		%%             country={}}
		%% \fntext[label3]{}
		
		\title{Alternate Loss Functions Can Improve the Performance of Artificial Neural Networks}
		
		%% use optional labels to link authors explicitly to addresses:
		%% \author[label1,label2]{}
		%% \affiliation[label1]{organization={},
		%%             addressline={},
		%%             city={},
		%%             postcode={},
		%%             state={},
		%%             country={}}
		%%
		%% \affiliation[label2]{organization={},
		%%             addressline={},
		%%             city={},
		%%             postcode={},
		%%             state={},
		%%             country={}}
		
		\author [label1]{Mathew Mithra Noel}
		\author [label2]{Arindam Banerjee}
		\author[label3]{Geraldine Bessie Amali D}
		
		
		
		
		\affiliation[label1]{organization={Professor, School of Electrical Engineering, Vellore Institute of Technology}, addressline={Email: mathew.m@vit.ac.in}, country={India}}
		             
		\affiliation[label2]{organization={Senior Data Scientist, Ernst and Young GDS},   addressline={Email: arindam.banerjee3@gds.ey.com}, country={India}}
		
		\affiliation[label3]{organization={Associate Professor, School of Computer Science and  Engineering, Vellore Institute of Technology}, addressline={Email: geraldine.amali@vit.ac.in}, country={India}}
		
		
		\begin{abstract}
		
		All machine learning algorithms use a loss, cost, utility or reward function to encode the learning objective and oversee the learning process. This function that supervises learning is a frequently unrecognized hyperparameter that determines how incorrect outputs are penalized and can be tuned to improve performance. This paper shows that training speed and final accuracy of neural networks can significantly depend on the loss function used to train neural networks. In particular derivative values can be significantly different with different loss functions leading to significantly different performance after gradient descent based Backpropagation (BP) training. This paper explores the effect on performance of new loss functions that are more "liberal" or "strict" compared to the popular Cross-entropy loss in penalizing incorrect outputs. Eight new loss functions are proposed and a comparison of performance with different loss functions is presented. The new loss functions presented in this paper are shown to outperform Cross-entropy loss on computer vision and NLP benchmarks.
		
		\end{abstract}
		
		
		

\begin{keyword}
		%% keywords here, in the form: keyword \sep keyword
		
		
		
		Loss Function \sep Artificial Neural Network \sep Convolutional Neural Network \sep Deep Learning
		
		%% PACS codes here, in the form: \PACS code \sep code
		
		%% MSC codes here, in the form: \MSC code \sep code
		%% or \MSC[2008] code \sep code (2000 is the default)
		
\end{keyword}
		
\end{frontmatter}
		
		%% \linenumbers
		
		%% main text
\section{Introduction}
Artificial Neural Networks (ANNs) are a class of universal function approximators that learn a parametrized approximation to the target function through a Gradient Descent (GD) based optimization process \cite{goodfellow2016deep}. Thus learning in ANNs is reduced to the problem of learning a finite set of real parameters namely the weights and biases in the ANN model. Good parameters that result in a good approximation to the target function are computed by minimizing a loss (also known as cost) function that measures the difference average error between ANN outputs and targets. An important aspect of the loss function is that it distills the performance of a ANN model over the entire dataset into a single scalar value \cite{hastie2009elements}. Thus the loss function is a single continuously differentiable real-valued function of all the parameters of the ANN model. The loss function must be continuously differentiable for GD to work. It is reasonable to choose a loss function inspired by statistical estimation theory to compute the most probable parameters given the dataset. Historically the Mean Square Error (MSE) and Cross-entropy from Maximum Likelihood Estimation theory (MLE) are almost universally used to train ANNs. MLE theory assigns to unknown parameters values that maximize the probability of observing the experimental data \cite{rossi2018mathematical}. The logarithm of the probability of observing the data is often used for convenience and results in the Cross-entropy loss. Thus MLE estimation proceed by maximizing the logarithm of the probability of the observed data as a function of the unknown parameters. 

MSE is used for regression problems where continuous real values have to be predicted and Cross-entropy is used for classification problems where the predicted outputs are discrete. Both MSE and Cross-entropy loss functions are derived from MLE theory as already described. MSE is a maximum-likelihood estimate when the model is linear and the noise is Gaussian. However it is frequently used in practice even when these assumptions cannot be justified. For classification problems both MSE and Cross-entropy losses can be used, but learning is generally faster with Cross-entropy as the gradient is larger due to the log function in Cross-entropy loss.  

\subsection{Mathematical Formalism}
Consider a dataset consisting of N training examples: $D = \{ x^i,y^i: i = 1 .. N \}$. Where $x \in R^m, y \in R^n$ in general for some natural numbers m and n. In the following we motivate and discuss possible loss functions for binary classification problems.


For binary classification problems $y \in \{0,1\}$. For the binary classification problem, the final layer consists of a single sigmoidal neuron which outputs the probability $P(y=1|x)$. This probability depends on all the weights and biases in the ANN and is hence denoted by $h_\theta(x)$. To compute the unknown parameters using maximum likelihood estimation theory the probability of the data given the parameters should be first computed.

Assuming that each training pair in the dataset is independent: \[ P(D|\theta) = \prod_{i=1}^{N} P(x^i,y^i;\theta) \].

$\prod_{i=1}^{N} P(x^i,y^i;\theta) = \prod_{i=1}^{N} P(y^i|x^i;\theta)P(x^i)$ by the definition of conditional probability.

Since $P(x^i)$ is independent of $\theta$, maximizing $P(D|\theta)$ is same as maximizing $\log(\prod_{i=1}^{N} P(y^i|x^i;\theta)) $.

Thus the log-likelihood function to be maximized is \[ \log(\prod_{i=1}^{N} P(y^i|x^i;\theta)) = \sum_{i=1}^{N} P(y^i|x^i;\theta) \]

But $P(y^i|x^i;\theta) = {h_\theta(x^i)}^{y^i}{(1-h_\theta(x^i))}^{1-y^i}$. 
 \\

Instead of maximizing the log-likelihood, the negative log-likelihood can be minimized. \\

This  negative log-likelihood after simplification is the loss \[ L(\theta) = -\sum_{i=1}^{N} (y^i\log(h_\theta(x^i)) + (1-y^i)\log(1-h_\theta(x^i))) \]. \\

Scaling with any positive number does not change the minimum, so for numerical convenience we can divide by N to obtain 

\[ L(\theta) = -\frac{1}{N}\sum_{i=1}^{N} (y^i\log(h_\theta(x^i)) + (1-y^i)\log(1-h_\theta(x^i)))) \]

\begin{equation} \label{eq:BCE}
L(\theta) = -\frac{1}{N}(\sum_{i=1}^{N} y^i\log(\hat{y}^i) + (1-y^i)\log(1-\hat{y}^i)) 
\end{equation}



For notational convenience $h_\theta(x)$ which is an approximation to the random variable y is denoted by $\hat{y}$. The above binary Cross-entropy loss for a single training pair $(x,y)$ is 

\begin{equation} \label{eq:BCE_single}
l(y,\hat{y}) = -[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]
\end{equation} \\


\begin{figure}[H]
\begin{center}
\includegraphics[width=8cm]{loss_y0.png}
\end{center}
\caption{Plot of Binary Cross-entropy loss when the target y = 0.}
\label{fig:loss0}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[width=8cm]{loss_y1.png}
\end{center}
\caption{Plot of Binary Cross-entropy loss when the target y = 1.}
\label{fig:loss1}
\end{figure}

The target assumes exactly one of two possible values namely 0 or 1, so only one term in Eq.   \ref{eq:BCE_single} is nonzero. When $y=1$, $l(y,\hat{y}) = -\log(\hat{y})$ and when  $y=0$, $l(y,\hat{y}) = -\log(1-\hat{y})$. Figures \ref{fig:loss0} and \ref{fig:loss1} show the variation of BCE loss with final ANN output $\hat{y}$ when the target y is 0 and 1 respectively. It is observed that the loss is exactly zero when target is equal to the output. Further the loss is differentiable, convex and tends to infinity as the output approaches the incorrect target value. Convexity is desirable to avoid introducing unnecessary local minima and differentiability is required for gradient based BP learning. In the following we propose new loss functions with the following desirable attributes:


\begin{itemize}
\label{loss properties}
\item $l(y,\hat{y})=0$ and $\hat{y} = y$
\item $ l(0,\hat{y}) $ and $ l(1,\hat{y})$ are convex and differentiable functions of $ \hat{y} $
\item $ l(0,0) = l(1,1) = 0 $ 
\item $ \lim_{\hat{y} \to 1} l(0,\hat{y}) = \infty $ and $ \lim_{\hat{y} \to 0} l(1,\hat{y}) = \infty $
\end{itemize}


In the above $ 0 < \hat{y} < 1 $ since $\hat{y}$ is the probability $ P(y=1|x) $. For multiclass classification problems $\hat{y}$ will be a vector with as many dimensions as classes. For C classes, the final layer will a softmax layer with C neurons and the loss function is the sum of the loss functions for each output: $ -\frac{1}{N}\sum_{j=1}^C \sum_{i=1}^{N} y_j^i\log(\hat{y_j}^i) + (1-y_j^i)\log(1-\hat{y_j}^i) $. To compare different loss functions we introduce the following definition. \\

\textbf{Definition}: Consider two loss functions $l_1$ and $l_2$ defined for a single (output,target) pair $(\hat{y},y)$, where $ \hat{y} $ and $ y $ are vectors with C components. We define $l_1$ to be stricter than $l_2$ for some set of values of $ \hat{y} $ and $ y $ if $ \nabla l_1(\hat{y},y) \geq \nabla l_2(\hat{y},y) $. In the above definition the inequality between gradient vectors is to be interpreted componentwise. \\
		
We define $l_2$ to be more lenient than $l_1$,  if $l_1$ is stricter than $l_2$. In the following we introduce  alternate loss functions that penalize network errors differently than BCE and explore the effect on performance of using stricter and lenient loss functions. The paper attempts to address a fundamental question pertaining to ML algorithms on whether stricter penalties lead to improved performance for the same dataset or whether stricter penalties beyond a certain point lead to reduced performance. \\

The main contributions of this work are:
\begin{itemize}

		    \item Eight new loss functions that provide better performance than Cross-entropy on benchmarks are proposed
		    
		    \item The effects of using "stricter" and "lenient" loss functions compared to Cross-entropy is explored
		    
		    \item The distinguishing features and advantages of different loss functions are discussed
		  		   
\end{itemize}

Hyperparameter tuning is a trick that can be used to improve the performance of ANNs and does not require extra data. However tuning hyperparameters require more computational resources. An alternative is to use state-of-the-art models that are known to provide better performance apriori. For example new activation functions that are known to perform better than popular activation functions like ReLu can be used \cite{noel2021growing}, \cite{DBLP:journals/corr/abs-2111-04020}. \\ \\


\section{New loss functions}
Next we explore alternate loss functions that satisfy properties listed in \ref{loss properties}. Considering the binary classification problem and assuming that $0 < \hat{y} < 1$ and $y \in \{0,1\}$, the following new loss functions are proposed:

\begin{itemize}
\item \textbf{M Loss:} $ M(y,\hat{y}) = y\left(\frac{1}{\hat{y}}-1\right) $ \\
\item \textbf{L Loss:} $ L(y,\hat{y}) = \frac{y}{\sqrt{1-(1-\hat{y})^2}}-1 $ \\
\item \textbf{Parametrized M Loss:} $ M_{\alpha}(y,\hat{y}) = y\left(\frac{1}{\hat{y}^\alpha}-1\right) $ \\
\item \textbf{Tan Loss:} $ T(y,\hat{y}) = y\tan(\frac{\pi (1-\hat{y})}{2})$ \\
\item \textbf{Parametrized L Loss:} $ L_{\alpha}(y,\hat{y}) = \frac{y}{\sqrt{1-(1-\hat{y})^\alpha}}-1 $ \\
\item \textbf{Two parameter L Loss:} $ L_{\alpha,\beta}(y,\hat{y}) = \frac{y}{\left({1-(1-\hat{y})^\alpha}\right)^\beta}-1 $ \\
\item \textbf{Parametrized Log Loss:} $ -y \log_{\alpha}(\hat{y}) $  \\
\end{itemize}



In the above the loss is zero if y=0 and the ANN is penalized for errors only when y=1. Modified versions of the above losses that penalize the ANN for errors even when y = 0 are given below. \\ \\ \\

\textbf{Full loss functions:}
\begin{itemize}
\item \textbf{M Loss:} $ M(y,\hat{y}) = \frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}}-1 $ \\
\item \textbf{L Loss:} $ L(y,\hat{y}) = \frac{y}{\sqrt{1-(1-\hat{y})^2}}+\frac{1-y}{\sqrt{1-\hat{y}^2}}-1 $ \\
\item \textbf{Tan Loss:} $ T(y,\hat{y}) = y\tan(\frac{\pi (1-\hat{y})}{2}) + (1-y)\tan(\frac{\pi \hat{y}}{2})$ \\
\item \textbf{Sec Loss:} $ S(y,\hat{y}) = y \csc(\frac{\pi \hat{y} }{2}) + (1-y)\sec(\frac{\pi \hat{y} }{2}) -1 $ \\

\item \textbf{Parametrized M Loss:} $ M_{\alpha}(y,\hat{y}) = \frac{y}{\hat{y}^\alpha} + \frac{1-y}{(1-\hat{y})^\alpha}-1 $ \\
\item \textbf{Parametrized L Loss:} $ L_{\alpha}(y,\hat{y}) = \frac{y}{\sqrt{1-(1-\hat{y})^\alpha}}+\frac{1-y}{\sqrt{1-\hat{y}^\alpha}}-1 $ \\
\item \textbf{Two parameter L Loss:} $ L_{\alpha,\beta}(y,\hat{y}) = \frac{y}{(1-(1-\hat{y})^\alpha)^\beta}+\frac{1-y}{(1-\hat{y}^\alpha)^\beta}-1 $ \\
\item \textbf{Parametrized Cross-entropy Loss:} $ -(y \log_{\alpha}(\hat{y}) + (1-y) \log_{\alpha}(1-\hat{y})) $ \\ \\
\end{itemize}

In the above definitions of new loss functions the constant factor of -1 can be safely ignored since it does not change the derivative of the loss. This bias of -1 has been included only to fix the loss value to be zero when the output is equal to the target and is optional.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.32]{Loss_Functions.png}
\label{fig:functions}
\end{center}
\caption{Plot of different loss functions when the target y = 1.}
\end{figure}

Figure \ref{fig:functions} shows the difference between Cross-entropy and other loss functions proposed in this paper. To apply the above loss functions to multiclass classification problems, the losses for each output are added together. In particular if $l(y_j^i,{\hat{y}_j}^i)$ is the loss for the jth output and ith training pair, the overall loss is $ \frac{1}{N} \sum_{j=1}^C \sum_{i=1}^{N} l(y_j^i,{\hat{y}_j}^i) $ where N is the size of the mini-batch for mini-batch gradient descent.

			
		
\section{Comparison of performance on benchmark datasets}
\label{section:results}

We tested both versions of the M-loss and the L-loss with deep neural networks on different benchmark datasets. We compared the results with the same experiments performed with cross-entropy loss too. We tested these losses on a VGG19 model with Imagenette and CIFAR10 datasets. The Adam optimizer with learning rate of 1e-5 was used for training the model. The model was trained on a Google Colab environment with 50 epochs and 100 steps per epoch. We compared the results among five loss functions - i) Cross Entropy loss, ii) L-loss, iii) M-loss, iv) Full L-loss, and v) Full M-loss. The training and test accuracy values for these five loss functions are shown in Table~\ref{table1}.

\begin{table}[H]
\caption{Results on image datasets}
\label{table1}
\begin{center}
\begin{tabular}{lllll}
\multicolumn{1}{c}{\bf Dataset}  &\multicolumn{1}{c}{\bf Model}  &\multicolumn{1}{c}{\bf Loss}  &\multicolumn{1}{c}{\bf Train Acc.}	&\multicolumn{1}{c}{\bf Test Acc.}
\\ \hline \\
Imagenette   &VGG19 &Cross Entropy loss &0.73 &0.74 \\
Imagenette   &VGG19 &L-loss &0.75 &\textbf{0.77} \\
Imagenette   &VGG19 &M-loss &0.74 &\textbf{0.78} \\
Imagenette   &VGG19 &Full L-loss &0.72 &\textbf{0.78} \\
Imagenette   &VGG19 &Full M-loss &0.74 &\textbf{0.76} \\
CIFAR10  &VGG19 &Cross Entropy loss &0.5806 &0.5668 \\
CIFAR10  &VGG19 &L-loss &0.5791 &\textbf{0.5684} \\
CIFAR10  &VGG19 &M-loss &0.5745 &0.5666 \\
CIFAR10  &VGG19 &Full L-loss &0.5827 &\textbf{0.571} \\
CIFAR10  &VGG19 &Full M-loss &0.5762 &\textbf{0.5668} \\
\end{tabular}
\end{center}
\end{table}

We tested the five losses on text datasets and gauged the performance of the models. We used an LSTM-based model to classify text from the Consumer Financial Protection Bureau (CFPB) dataset. The target variable has 10 possible classes. The model contains an embedding layer followed by a spatial dropout, an LSTM layer of 100 units, and a dense layer to output 10 possible classes. We also developed a sentiment analysis model using BERT to classify movie reviews as positive or negative, based on the text of the review. We used the Large Movie Review Dataset \cite{maas-EtAl:2011:ACL-HLT2011} that contains the text of 50,000 movie reviews from the Internet Movie Database (IMDB). We used the Smaller BERT model from TensorFlow Hub having 4 hidden layers (Transformer blocks), a hidden size of 512, and 8 attention heads. Adam optimizer was used with this BERT model. The test accuracy values for these five loss functions are shown in Table \ref{table2}.

\begin{table}[H]
\caption{Results on text datasets}
\label{table2}
\begin{center}
\begin{tabular}{llll}
\multicolumn{1}{c}{\bf Dataset}  &\multicolumn{1}{c}{\bf Model}  &\multicolumn{1}{c}{\bf Loss}	&\multicolumn{1}{c}{\bf Test Acc.}
\\ \hline \\
CFPB   &LSTM &Cross Entropy loss &0.822 \\
CFPB   &LSTM &L-loss &\textbf{0.843} \\
CFPB   &LSTM &M-loss &\textbf{0.827} \\
CFPB   &LSTM &Full L-loss &\textbf{0.839} \\
CFPB   &LSTM &Full M-loss &\textbf{0.825} \\
IMDB movie review  &BERT &Cross Entropy loss &0.8551 \\
IMDB movie review  &BERT &L-loss &\textbf{0.8557} \\
IMDB movie review  &BERT &M-loss &\textbf{0.8539} \\
IMDB movie review  &BERT &Full L-loss &\textbf{0.8577} \\
IMDB movie review  &BERT &Full M-loss &\textbf{0.8555} \\
\end{tabular}
\end{center}
\end{table}

The experimental results show that both the variants of the proposed L-loss and M-loss can outperform the cross-entropy loss in most of the cases.




\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=0.8\linewidth]{fig1.png}
\end{center}
\caption{Variation of accuracy with epochs for the Imagenette dataset.}
\end{figure}

\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=0.8\linewidth]{fig2.png}
\end{center}
\caption{Variation of loss with epochs for the Imagenette dataset.}
\end{figure}

\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=0.8\linewidth]{fig3.png}
\end{center}
\caption{Variation of accuracy with epochs for the CIFAR10 dataset.}
\end{figure}

\begin{figure}[H]
\begin{center}
%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\includegraphics[width=0.8\linewidth]{fig4.png}
\end{center}
\caption{Variation of loss with epochs for the CIFAR10 dataset.}
\end{figure}

		
\section{Conclusion}
This paper explored the possible advantages of using alternate loss functions for training ANNs. Eight new loss functions were proposed. Preliminary results were obtained with two of the proposed loss functions. The M and L loss functions proposed in this paper are shown to outperform cross-entropy loss in most of the scenarios for both image and text classification. The fundamental question of whether more aggressive punishments and rewards can significantly improve the performance of ML algorithms remains unanswered and will be considered in future works.



		
\bibliographystyle{elsarticle-num}
\bibliography{bibliography.bib}
		
		
		%% else use the following coding to input the bibitems directly in the
		%% TeX file.
		
\end{document}
