\section{Conclusion}
\vspace{-0.5em}
In this paper, we propose free language modeling (FLM), a new pretraining objective for accelerating vision-language pretraining. 
Different from previous language modeling methods, such as MLM and AR, FLM seamlessly disentangles the prediction rate from the tie-up with the corruption rate, meanwhile allowing a flexible corruption pattern for each prediction target. 
Experiments verify the effectiveness of the proposed FLM both in accuracy and efficiency. Our model could converge faster with a decent reduction of training time compared to MLM, while achieving comparable performance on multiple multimodal downstream tasks.

\vspace{-.3em}
\paragraph{Acknowledgement.}
This paper is partially supported by the National Key R\&D Program of China No. 2022ZD0161000, the General Research Fund of HK No.17200622, the National Natural Science Foundation of China under Grant No. 62122035 and 61972188. We thank Chengyue Wu for his technical assistance, and Jianhui Xu, Zhengxiao Du, and Zhichao Lu for their helpful comments.
