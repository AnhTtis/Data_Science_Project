
\begin{figure*}
    \centering
                \includegraphics[width=0.9 \textwidth]{figs/fig2_lms_v5.pdf}
    \vspace{-.5em}
    \caption{Dependency matrix of different language modeling methods in vision-language pretraining. $r_{\rm pred}$ represents the proportion of output tokens for reconstruction. $r_{\rm corr}$ represents the proportion of corrupted inputs for each output token. $\overline{r}_{\rm corr}$ is the mean corruption rate of all reconstruction tokens. FLM has distinct advantages compared with others: 1) Different from MLM and PrefixLM that bind  $r_{\rm pred}$ and $\overline{r}_{\rm corr}$ together by $r$, the unbound prediction rate in FLM could achieve 100\% for accelerating training as much as possible. 2) Without relying on a position-aware unidirectional corruption in AR/PrefixLM or fixed corruption across all positions in MLM (see the right-side line graph), the corrupted span in FLM for each output token could be different, and the corrupted rate is independent of the position of the output token, enabling a more flexible corruption pattern for better exploiting the bidirectional context information.} 
    \vspace{-1.0em}
    \label{fig:LMsComparison}
\end{figure*}

\section{Related Work}

\paragraph{Vision-Language Pretraining.}
Vision-language pretraining tasks can be divided into two categories: (i) \textit{discriminative} tasks, \eg, image-text contrastive (ITC), image-text matching (ITM), and (ii) \textit{generative} tasks, \eg, masked language modeling (MLM) and autoregressive language modeling (AR). \textit{Discriminative} tasks consider the image-text pairs as multi-modal views of the same semantics. Contrastive or multi-view learning is adopted for learning the alignment between multiple modalities. For example, CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and following works~\cite{yao2021filip,li2022grounded,li2021align} utilize cross-modal contrastive learning by projecting the image and language information into a joint (structured) semantic space. \textit{Generative} tasks aim to reconstruct the corrupted text (image) with the assistance of visual (text) modality. The main body of representative works~\cite{li2019visualbert,lu2019vilbert,chen2020uniter,li2020oscar,zhang2021vinvl,wang2022vlmixer,zeng2021multi,bao2022vl}, 
employ the MLM-like objectives, where input texts(image) are partially masked and then interact with visual (text) tokens to reconstruct the corrupted part. SimVLM~\cite{wang2021simvlm} introduces a single prefix language modeling (PrefixLM) objective for exploiting large-scale weak supervision in VLP. CoCa~\cite{yu2022coca} further verifies the representation ability of autoregressive language modeling (AR) in the vision-language domain. 
%
While most existing methods combine discriminative and generative tasks for better representation learning, BEiT-3~\cite{wang2022image} shows a single generative language modeling (\eg, MLM) could handle the vision-language interactions and alignments well with the mixture-of-expert transformer. Although superior performance has been attained, most existing methods based on MLM suffer from low utilization of output tokens and lead to a slow convergence rate. This paper proposes a new generative language modeling method targeting pretraining acceleration. 

\vspace{-.2em}
\paragraph{Efficient Pretraining.}
While early VLP methods~\cite{tan-bansal-2019-lxmert,li2020oscar,chen2020uniter,zhang2021vinvl} rely on time-consuming pretrained object detectors for visual representation, PiexlBERT~\cite{huang2020pixel} and ViLT~\cite{kim2021vilt} directly apply grid/patch-level visual features to reduce computation complexity of the object-level visual encoder. Beyond the design of efficient model architecture, a few research focuses on data-efficient training. Bitton et al.~\cite{bitton2021data} propose an alternative masking strategy that better focuses on visually-related physical words to improve VLP in low-resource settings. DeCLIP~\cite{li2021supervision} enhances the CLIP by exploring more supervision signals, such as self-supervision within a single modality or multi-view supervision across different modalities. The most relevant work to this paper is GRIT-VLP~\cite{byun2022grit}, which assigns a larger mask rate for MLM and performs grouped in-batch negative sampling for ITC to accelerate the convergence. However, only half of the output tokens are assigned for the reconstruction task, where the under-used output tokens impede a further speed-up of pretraining. Our method decouples the corruption and reconstruction rate, making them freely chosen for a better combination between performance and efficiency. 

\vspace{-.2em}
\paragraph{Language Modeling.}
In NLP, MLM~\cite{devlin2018bert,liu2019roberta,he2020deberta} and AR~\cite{brown2020language,chowdhery2022palm} have been the two most popular generative pretraining objectives. AR aims to estimate the probability distribution of a given text sequence using the product rule by an auto-regressive model. However, unidirectional encoding may not be suitable for language understanding tasks that prefer bidirectional context information. MLM enables bidirectional contexts for language understanding tasks but can not be directly adopted into language generation tasks. Some works~\cite{yang2019xlnet,du2022glm} unify MLM and AR for better performance on both language understanding and generation tasks. Wettig et al.~\cite{wettig2022should} study the choice of the mask ratio in MLM from the perspective of both corruption and prediction. However, among previous methods, little attention has ever been devoted to the issue of training efficiency. We target accelerating vision-language pretraining meanwhile keeping decent performances on vision-language understanding and generation tasks.




