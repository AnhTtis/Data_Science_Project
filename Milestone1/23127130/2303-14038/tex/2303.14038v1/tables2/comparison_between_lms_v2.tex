\begin{table*}[]
    \small
    \centering
    \setlength{\tabcolsep}{1.6 mm}{
    \begin{tabular}{lll|cccccccc|c}
    \toprule
    \multirow{2}{*}{ Method}  & \multirow{2}{*}{$r_{\rm corr}$} &  \multirow{2}{*}{$r_{\rm pred}$} & VQA &  \multicolumn{2}{c}{NLVR$^2$} & \multicolumn{2}{c}{Retrieval (Flickr30K)} & \multicolumn{3}{c|}{COCO Captioning} & \multirow{2}{*}{GPU Days (speed-up)}\\
    & & & test-dev & dev & test & IR@1 & TR@1 & BLEU & METER & CIDEr & \\
    \midrule
% =======================
%AR
        AR & 50\% & $100\%$ & 72.85 & 75.79 & 76.29 & 66.59 & 84.10 & \underline{35.70} & \underline{28.86} & \underline{120.6} & \ \ 9.6 (6.1\x)\\
% =======================
%PrefixLM + GLM
        PrefixLM & $25\%$ & $50\%$ & 72.64 & 75.73 & 76.17 & 66.21 & 82.70 & 35.50 & 28.79 & 119.4 & 10.0 (5.9\x)   \\
% =======================
%MLM
        MLM & $15\%$ & $15\%$ & 73.52 & 77.46 & 78.28 & 71.33 & \underline{88.40} & 34.90 & 28.50 &  117.5 &   58.7 (1\x)\\
        MLM & $40\%$ & $40\%$ & \textbf{73.95} & \underline{77.62} & \underline{78.60} & \textbf{73.41} & \textbf{89.20} & 35.50 & 28.79 & 120.3 & 58.7 (1\x) \\
% =======================
%FLM
        \rowcolor{Gray}
        FLM (Ours) & $1/L$ & $100\%$ & \underline{73.85} & \textbf{77.99} & \textbf{78.63} & \underline{72.81} & 87.40 & \textbf{36.68} & \textbf{29.17} & \textbf{123.0}  & 22.7 (2.5\x) \\
\bottomrule
    \end{tabular}
    }
    \vspace{-.5em}    
    \caption{Performance Comparison between different language modeling methods. $r_{\rm corr}$ and $r_{\rm pred}$ refer to the corruption and prediction rates. 
    All models are based on CLIP-B/32 image encoder and a text transformer initialized by RoBERTa. Note that the default $r_{\rm corr}$ of FLM is set to $1/L$ for better efficiency, while FLM's performance will be further improved by an optimal $r_{\rm corr}$, as indicated in Table~\ref{tab:ablcr}.}
    \label{tab:lmcomparison}
    \vspace{-1.5em}
\end{table*}

