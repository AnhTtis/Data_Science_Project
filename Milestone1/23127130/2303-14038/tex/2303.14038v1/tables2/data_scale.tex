\begin{table}[]
    \centering
    \small
    \setlength{\tabcolsep}{1.5 mm}{
    \begin{tabular}{l|ccccl}
    \toprule
    \multirow{2}{*}{Method} & VQA & \multicolumn{2}{c}{NLVR$^2$} & Captioning & \multirow{2}{*}{GPU Days}\\
    & test-dev & dev & test & CIDEr & \\
    \midrule
        \multicolumn{5}{l}{\it{\textbf{CLIP-B/32 on 13M data}}} \\
        AR & 73.46 & 76.60 & 77.21 & 121.5 & 21.3 (5.4\x)  \\
        MLM & 74.25 & 78.63 & 79.19 & {122.6} & 116.0 (1\x)\\
        \rowcolor{Gray}
        FLM & \textbf{74.28} & \textbf{78.73} & \textbf{79.52} & \textbf{122.6} & {\color{black}32.0 (3.6\x)} \\
        %
        \multicolumn{5}{l}{\it{\textbf{CLIP-B/16 on 4M data}}} \\
        AR & 75.05 & 77.38 & 78.79 & 126.0 & 12.3 (5.0\x) \\ 
        MLM & 75.76 & \textbf{79.93} & 79.83 & 125.4 & 61.4 (1\x) \\ 
        \rowcolor{Gray}
        FLM & \textbf{75.95} & 79.02 & \textbf{80.03} & \textbf{126.5} & 16.1 (3.8\x)\\ 
\bottomrule
    \end{tabular}
    }
    \vspace{-.5em}
    \caption{Performance comparison of different pretraining objectives with a larger data scale (from 4M to 13M) or a larger number of patches (patch size from 32 to 16). 
    For 13M data, we extend the training iteration of MLM to 200k.
    }
    \vspace{-1.5em}
    \label{tab:datascale}
\end{table}
