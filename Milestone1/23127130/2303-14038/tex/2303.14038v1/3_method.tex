


\section{Method}
In this section, we first recap the representative language modeling methods for VLP from a corruption-prediction view in Sec.~\ref{sec:lmAsCP}. Then we propose the new language modeling method FLM to decouple the prediction rate from the corruption rate in Sec.~\ref{sec:flm}. Finally, we introduce FLM into VLP and propose a novel encode-corrupt-predict framework for accelerating VLP in Sec.~\ref{sec:VLPflm}.
\subsection{Language Modeling as Corruption-Prediction}
\label{sec:lmAsCP}
%
Given an input sequence $\mathbf{x}=\{x_1,...,x_L\}$, \textbf{MLM} aims to learn a deep bidirectional representation by randomly replacing part of input tokens with a special mask token, and then maximize the probability of reconstructing those masked tokens $P(\mathbf{x_m} | \mathbf{x_{\setminus m}})$, where $\mathbf{x_{ m}}$ represents corrupted tokens. 
%
\textbf{AR} uses a left-to-right autoregressive factorization to model the density distribution of the sequence $\sum_{i=1:L} \log P(x_{i}|x_{<i})$. 
%
\textbf{PrefixLM} enables bidirectional perception between prefix tokens and left-to-right autoregressive factorization to model the density distribution of the remaining sequence $\sum_{i=L_{p}:L} \log P({x}_{i} | {x}_{[L_p, i]}, {x}_{<L_p})$ where $L_{p}$ represents the prefix length. 

%

Note that all the above methods could be interpreted as a corruption-prediction problem because each prediction token only has a partial observation of the input data, \ie, the interactions between output tokens and some input tokens are corrupted. Therefore, their pretraining objectives could be unified as maximizing the reconstruction probability:
\begin{equation}
% \vspace{-.3em}
  \begin{aligned}
\mathbb{E}_{\mathbf{M} \sim B(r)} \!\sum_{i  = 1:L}\! \mathds{1}_{m_{ii}=0} \log {P}(x_i | \{x_j| {{m}_{ij}} \!=\! 1\}), 
\end{aligned}
\label{eqn:reconst}
% \vspace{-.3em}
\end{equation}
where $\mathbf{M}=\left[ m_{ij} \right]_{1 \leq i \leq L, 1\leq j \leq L}$ represents the dependency matrix between the input and the prediction target, ${m_{ij}}=1/0$ represents that $x_j$ is visible/invisible when predicting ${x_i}$. $B(r)$ represents a distribution parameterized by $r$, which is customized by specific models. The dependency matrices $\mathbf{M}$ of different language modeling methods are as follows (also illustrated in Fig.~\ref{fig:LMsComparison}): 
\begin{itemize}
\setlength\itemsep{0em}
%
    \item \noindent For MLM, $\mathbf{m}_{1,:}\!=\!\cdots\!=\!\mathbf{m}_{L,:}\!=\!\mathbf{p} \!\sim\! \text{Binomial}(r_{\rm mask})$. The corruption for predicting all $x_i$ is the same and it is sampled from a Binomial distribution.
%
\item For LM, ${m_{ij}} \!=\! \mathds{1}_{j < i}$. The corruption for predicting $x_i$ depends on the position $i$, which gets shorter with a larger $i$.
%
\item For PrefixLM, $m_{ij}\!=\!\mathds{1}_{j < \max (i, L_{\rm p})}$, where prefix length $L_{\rm p} \!=\! (1-r_{\rm span})\cdot L$ and $r_{\rm span} \! \sim \! {\rm Uniform}(0,1)$ represents the length ratio of the corrupted span.
%
\end{itemize}



\subsection{Free Language Modeling (FLM)}
\label{sec:flm}
From the above analysis, the representative MLM or AR/PrefixLM methods have limited freedom of the dependency matrix, which is prone to the following issues: 1) the tie-up between the prediction and corruption rate in MLM may lead to a low convergence speed during training; 2) inflexible and non-customizable contexts for each prediction target result in sub-optimal context learning. For example, the suffix-like corruption in AR disables the bidirectional context modeling which is essential for downstream understanding tasks. Moreover, the autoregressive prior in AR results in uneven distribution of corruption rate. Latter tokens are always assigned with a smaller corruption rate, thus being easy-predictable compared with former ones. The position-related distribution of the corruption rate may cause a sub-optimal difficulty degree of pretraining tasks. 


The goal of FLM is to disentangle the prediction and corruption rate for fully utilizing training signals to accelerate VLP. The model after disengagement has a more flexible corruption pattern that benefits bidirectional contextual representation learning. Following the unified formulation of the corruption-prediction problem in Eqn.~\ref{eqn:reconst}, we introduce the dependency matrix of FLM:
\begin{equation}
  \begin{aligned}
  {m_{ij}} = 1 \text{ if } m_{ij} \notin {\text{span}_i}, \text{otherwise }0.
\end{aligned}
\label{eq2}
\end{equation}
where $\text{span}_i$ is random span corruption with length $L^i_{\rm span}$ that satisfies $i \in \text{span}_i$. The starting position and length of $\text{span}_i$ could be customized or randomly sampled from a distribution. In our implementation, we sample $L^i_{\rm span} \!\sim\! {\rm Bernoulli}(L, r_{\rm corr})$ for each $i$, $r_{\rm corr}$ is the hyperparameter indicating the expected corruption rate.

Note that the corrupted span in FLM could differ for different predictions, hopefully increasing the flexibility of bidirectional contextualized interactions and helping optimization. Since the choice of $\text{span}_i$ does not interfere with each other for different $i$, the prediction rate could increase to 100\% which allows all input tokens to be reconstructed. As for the corruption rate, we note that the minimal corruption rate is $1/L$, since at least one token should be corrupted to reconstruct itself to avoid information leakage.


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figs/method_v7.pdf}
    \vspace{-1em}
    \caption{Overview of the proposed VLP framework with free language modeling (FLM). First,
    the image is patchified and encoded by a vision transformer into a sequence of vision tokens. Then, the text transformer performs uni-modal text feature encoding in the bottom layers and multimodal fusion between visual and text features in the top layers. Bidirectional multimodal representations are achieved by learning forward and reverse unidirectional representations, respectively, the order of which is manipulated by (reverse) causal masks in the same text transformer. After feature encoding, we construct a set of independent corruption-prediction tasks. For each task, we inject a random span corruption into the multimodal representation and then introduce a reconstruction query that gathers informative contexts from the corrupted features for reconstructing a single target token. Benefiting from the flexibility of post-encoding corruption, 100\% text tokens could be efficiently reconstructed in parallel. 
    }
    \vspace{-1.0em}
    \label{fig:overview}
\end{figure}

\subsection{Vision-Language Pretraining with FLM}
\label{sec:VLPflm}

Built upon FLM, a new encode-corrupt-predict pretraining framework is proposed for efficient pretraining with decoupled prediction and corruption rates. Given the input sequence $\mathbf{x}=\{x_1,...,x_L\}$, we formulate the reconstruction of all input tokens as $L$  independent corruption-prediction tasks. For $i$-th task, the learning objective is to maximize ${P}(x_i | \{x_j| {{m}_{ij}} \!=\! 1\})$ by reasoning upon uncorrupted bidirectional tokens. Fig.~\ref{fig:overview} depicts the pipeline of the model.

%

\vspace{-.3em}
\paragraph{Decomposed Bidirectional Encoding.} 
Since FLM establishes a customized corruption span for each prediction task, 
%
a naive solution of repeating the MLM-style feature encoding (transformer with fully-visible attention mask) for each task becomes time-consuming. Instead, 
%
we propose to share the intermediate features of different tasks for efficient inference by decomposing the token representation into two complementary views, left-to-right, and right-to-left features. The former adopted a text transformer with a casual mask, enforcing the latter tokens attending on previous tokens. The latter adopted the same transformer with a reverse causal mask for a right-to-left context flow. 


Specifically, we first encode image features by a CLIP image transformer.
%
For the text input, the $N_{\rm bottom}$ bottom layers of the text transformer perform the decomposed bidirectional encoding with the language input only, while $N_{\rm top}$ top layers further receive image features and fuse the multimodal features by cross-attention layers. 
%
After encoding, we obtain the bidirectional representation in text transformer, denoted as $E^{n}=\{e^{n}_1,\cdots, e^{n}_{L}\}$, where $e_i=\{e^{{\rm l2r}, n}_i, e^{{\rm r2l}, n}_i\}$ is the token representation for $x_i$ comprised of features from forward and reverse flow at the $n$-th layer. 

%

\vspace{-.3em}
\paragraph{Reconstructor.} Following the description in Sec.~\ref{sec:flm}, we sample a dependency matrix $\mathbf{M}$ to construct several corruption-prediction tasks. As a consequence of the span corruption, some elements in $E^n$ that rely on corrupted inputs need to be neglected to avoid information leakage. 
To reconstruct $x_i$, we gather context from uncorrupted features in $E$ by cross-attention: 
\begin{equation}
  \begin{aligned}
    &q^{n+1}_i\!=\!\text{CrossAttention}(q^{n}_{i}, E^{n}_i) \\
    &E^{n}_i\!=\!\{e^{{\rm l2r}, n}_j | m_{ij}\!=\!1 \text{ \!,\! } j\!<\!i\} \!\cup\! \{e^{{\rm r2l},n}_j | m_{ij}\!=\!1 \text{ \!,\! } j\!>\!i\},
\end{aligned}
\label{eq2}
\end{equation}
where $E^{n}_i$ represents all uncorrupted elements in $E^{n}$ given $M$. $q_i$ is a learnable reconstruction query, which is initialized as the $i$-th positional embedding in the first layer. The selection process from $E^{n}$ to $E^{n}_i$ is implemented as a specific attention mask in cross-attention layers, as illustrated in Fig.~\ref{fig:overview}. 
%
By forwarding on stacked cross-attention layers, $q^n_{i}$ aggregates deep bidirectional contexts for effective reconstruction of $x_i$. The output features of the last layer in the reconstructor are input into an MLP for final prediction. 

Note that $q^n_i$ works independently from each other, making a flexible addition or removal of some tasks. By sharing feature encoding, all reconstruction tasks run in parallel with low computation consumption.

\vspace{-.3em}
\paragraph{Pretraining Objectives.}
The reconstruction objective is to minimize the negative log-likelihood of predicted tokens: 
\begin{equation}
  \begin{aligned}
\mathcal{L}_{\rm R} \!=\! -\mathbb{E}_{\mathbf{M} \sim B(r)} \!\sum_{i  = 1:L}\! \log {P}(x_i | \{x_j| {{m}_{ij}} \!=\! 1\}),
\end{aligned}
\label{eq2}
\end{equation}
To further enhance the model's representability, we introduce an intermediate prediction loss upon the $E^N$ that improves the local temporal dependency between words, where N represents the last layer of the text transformer. We supervised the forward/reverse sequence $e^{\rm l2r, N}_i$/$e^{\rm r2l, N}_i$ by their next/previous tokens. The intermediate loss is the summation of two unidirectional prediction problems:  $\mathcal{L}_{\text{inter}} = \mathcal{L}_{\rm l2r} + \mathcal{L}_{\rm r2l} = \sum_{i=1:L} \log P(x_{i}|x_{<i}) + \sum_{i=1:L} \log P(x_{i}|x_{>i})$. 
%
The overall pretraining objective of FLM is calculated by $\mathcal{L}_{\rm FLM} = \mathcal{L}_{\rm R} + \mathcal{L}_{\rm inter}$.

% :
% \begin{equation}
%     \mathcal{L}_{\rm FLM} = \mathcal{L}_{\rm R} + \mathcal{L}_{\rm inter}.
%     \label{eqn:sam}
% \end{equation}
% \vspace{-.5em}