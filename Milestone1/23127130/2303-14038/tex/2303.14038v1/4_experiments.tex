\input{tables2/comparison_between_lms_v2.tex}
\input{tables2/data_scale.tex}

% \vspace{-1em}

\section{Experiments}
\subsection{Experimental Setting}
\paragraph{Pretraining Data.}
Following previous work~\cite{li2021align,dou2022empirical}, the pretraining data comes from four commonly used datasets, including COCO~\cite{lin2014microsoft}, Visual Genome~\cite{krishna2017visual}, SBU Captions~\cite{ordonez2011im2text}, and Conceptual Captions 3M~\cite{sharma2018conceptual}, totally 4M images. An enlarged version of $\sim$13M images is further used to boost performance by including Conceptual Caption 12M~\cite{changpinyo2021conceptual}~\footnote{Only ~8.7M images of CC12M are accessible due to expired URLs.}.

\vspace{-.3em}
\paragraph{Downstream Tasks.}
During finetuning, we append a special $\rm [CLS]$ token into the reconstructor and use its output features as the global cross-modal representation. We follow~\cite{chen2020uniter} to adapt the pre-trained model to four downstream vision-language understanding tasks, visual question answering~(VQA)~\cite{antol2015vqa}, natural language for visual reasoning~(NLVR$^2$)~\cite{suhr2018corpus}, image-text retrieval~(TR), text-image retrieval~(IR). We also test the performance on vision-language generation tasks, \ie, image captioning~\cite{lin2014microsoft}. For image captioning, we drop the reconstructor and use the text transformer with a causal mask for sequence generation. More details can be found in supplementary materials.

\vspace{-.3em}
\paragraph{Pretraining Details.}
Following ALBEF~\cite{li2021align} and METER~\cite{dou2022empirical}, the visual transformer is initialized by CLIP-ViT~\cite{radford2021learning} pretrained on 400M noisy image-text pairs. The visual transformer with ViT-B/32 is used as our base architecture for ablation study, and the one with ViT-L/14 is for scaling up to compare with other methods. We denote models with ViT-B image encoder as {Ours} and ViT-L as {Ours}$_{\rm LARGE}$. 
%
The bottom six layers of the text transformer are initialized by the bottom six layers of RoBERTa$_{\rm BASE}$~\cite{liu2019roberta}. The reconstructor is implemented by a 12-layer ($N_{\rm bottom}=N_{\rm top}=6$) transformer decoder (remove self-attention layers and only keep cross-attention layers) with a hidden size of 768 and a head number of 12. The default corruption rate of FLM is $1/L$, \ie, in each corruption-prediction task, only a single token is corrupted and then reconstructed from their contexts. While minimal corruption achieves decent performance, we further explore the choice of corruption rates in Sec.~\ref{subsec:label}. 

We pretrain the model for a maximum of 30k steps, with a total batch size of 4096 on 16 TITAN V100 GPUs by AdamW~\cite{loshchilov2017decoupled} optimizer and gradient accumulation. Mixed-precision training is used to reduce memory consumption and accelerate training. We use a 5\% warm-up schedule with a maximum learning rate of 4e-4. Following~\cite{dou2022empirical}, we assign a lower learning rate of 8e-5 for all pretrained layers. 
The text sequence length is limited to 50 subwords. More details are in the supplementary materials.

\vspace{-.3em}
\paragraph{Baselines.}
We also pretrain some generative language modeling methods for comparison, including MLM, AR, and PrefixLM~\cite{wang2021simvlm}. Specifically, we directly input the (corrupted) text sequence into the text transformer and then build an MLP layer upon the last layer of the text transformer for reconstructing the original input. For AR and PrefixLM, we follow the same learning rate schedule as FLM. For MLM, we follow~\cite{dou2022empirical} to train the model for 100k iterations with a maximum learning rate of 5e-5 and a warm-up rate of 10\%. To compare the convergence speed of different methods, we report the GPU days when reaching the best validation performance (\ie, reconstruction accuracy on the COCO validation set).

\input{tables2/model_ablation_v2.tex}
\subsection{Comparison with Language Modeling Methods}


As shown in Table~\ref{tab:lmcomparison}, compared with MLM, the proposed FLM achieves a 2.5\x speed-up while keeping the comparable performance on VL understanding tasks and superior performance on VL generation tasks. 

AR achieves decent performance on image captioning generation but inferior performance on other VL understanding tasks, due to the lack of ability to capture bidirectional interactions among sequences. Moreover, AR has a faster convergence rate and high training efficiency. Although PrefixLM enables bidirectional interactions between partial inputs, which is beneficial for VL classification tasks, the performance of PrefixLM is similar to AR. However, the reconstruction target mainly falls on the right side of the sequence, where the uneven distribution may push the learned representation towards an unsatisfactory language prior. For MLM, we found a corruption rate of 40\% achieves the best VQA performance 
among $\{10\%, 20\%, ..., 80\%\}$, indicating an appropriate corruption rate is essential to control the task difficulty. However, the convergence rate of MLM is slow, making larger training steps necessary to achieve decent performance. Our method FLM surpasses MLM on NLVR$^2$ and image captioning, also showing an impressive 2.5\x speed-up of training time. 


In Table~\ref{tab:datascale}, we show that the superiority of the proposed FLM consistently holds with a larger data scale or with more powerful visual features. 
The reason may be that FLM learns bidirectional context patterns by encoding the text sequence once and densely predicting the 100\% input in parallel, while MLM usually needs more pretraining steps to see such diverse patterns. 
% We conjecture that enlarging the pretraining data may amplify such damage of inefficient data utilization. 
Therefore, FLM is a friendly pretext task for accelerating training under low-resource scenarios, which to some extent, enjoys the high efficiency of AR/PrefixLM and the high performance of MLM. 


As for underperformed retrieval performance compared with MLM, we conjecture that FLM with span corruptions has fewer corruption variants than MLM with random corruptions but focuses more on local semantics, which favors fine-grained tasks like VQA/captioning more than retrieval.

\subsection{Ablation Studies}
\label{subsec:label}
\paragraph{FLM Loss.}
The ablation study for the loss terms in FLM is shown in Table~\ref{tab:auxLoss}. With merely reconstruction loss $\mathcal{L}_R$, our model achieves better performance~(73.04 on VQA) compared with AR~(72.85) or PrefixLM~(72.64). When further introducing left-to-right or right-to-left intermediate caption loss, the model gains consistent improvements over two downstream tasks. Note that left-to-right loss shows non-trivial superiority to the right-to-left one, verifying the effectiveness of the causal relationships between words. By combing bidirectional caption loss, the model achieves 0.81/1.45 absolute gains over the model with merely reconstruction loss. 

\vspace{-0.5em}
\paragraph{Parameter Sharing.} During decomposed bidirectional encoding, 
parameter sharing is used in the text transformer for two unimodal encodings with different attention masks. Table~\ref{tab:sharing} shows that the shared text transformer clearly surpasses the unshared one, indicating that the two unidirectional representations could implicitly benefit each other by sharing the same feature space. 

\vspace{-0.5em}
\paragraph{Number of Reconstruction Layers.}
The reconstructor aims to construct several corrupted sequences upon the high-level representations and reconstruct the corrupted information in parallel. Table~\ref{tab:layerNum} shows that a deep structure of the reconstructor helps the downstream tasks. The multi-layer reconstructor gathers text and multimodal features from low to high levels, promising to enhance the representation ability.

\input{tables2/comparison_sota}
\input{tables2/comparison_sota_v2.tex}

\vspace{-0.5em}
\paragraph{Prediction Rate.}
We test the converged performance of the pretrained model with different prediction rates. To this end, we randomly mask a subset of output tokens from loss calculation. 
As shown in Table~\ref{tab:ablpr},  a lower prediction rate tends to achieve poor performance both on VQA and NLVR$^2$, probably suggesting that the prediction loss containing a larger number of tokens helps the optimization.

\vspace{-0.5em}
\paragraph{Corruption Rate.}
The corruption rate determines how much context should be used for predicting the corrupted tokens. It controls the difficulty of the reconstruction problem and closely affects model performance. We study the influence of corruption strategies in FLM. 
%
As shown in Fig.~\ref{tab:ablcr}, 
{First, we test the length of span corruption.} With the growth of span length, the VQA and NLVR$^2$ performance steadily reach their maximum values at the 30\%$\sim$40\% corruption rate. Our method keeps a 100\% prediction rate while allowing a customized corruption rate, which is hopeful to serve as a replacement for the widely-used MLM to improve convergence speed. 

Besides the span corruption which occurs after feature encoding, we also test the influence of pre-encoding corruption. We assign random corruptions to each token of the input sequence and then perform FLM to reconstruct all input tokens. With a 15\% corruption rate, random corruption could slightly increase the VQA score. But unfortunately, the NLVR$^2$ hurts with a larger corruption rate. We found that the optimal corruption rate may differ for different corruption methods. How to effectively fuse different types of corruption may be a promising direction to increase the diversity of contexts further.


\subsection{Comparsion with State-of-the-Arts}
The comparisons on VQA, NLVR$^2$ and image captioning are shown in Table~\ref{tab:sota}, without using complicated pretraining tasks like ITM and ITC, our method achieves competitive performance by merely using FLM as the pretraining task. Compared with prior arts, our method has appealing advantages regarding pretraining time: First, the proposed FLM helps the convergence speed by enabling 100\% token prediction. Second, we leverage FLM as the single pretraining objective, without relying on additional time-consuming pretraining objectives like ITM. 
Third, we use the patch-level image features instead of a heavy object detection used in~\cite{li2020oscar, zhang2021vinvl}. 


The performance on cross-modal retrieval is shown in Table~\ref{tab:results2}. Our FLM-trained model performs poorly if directly fine-tuned on target downstream datasets.  Note that retrieval is heavily required for cross-modal alignment learning (\eg, ITM or ITC) on large-scale datasets since negative samples are essential to learning discriminative features. Therefore, we jointly use ITM and FLM to conduct pretraining to facilitate cross-modal alignments. By doing so, we obtain considerable performance gain and reach superior performance on Flickr30K and competitive performance on COCO over prior arts, suggesting the complementarity of FLM and other alignment-oriented objectives.
