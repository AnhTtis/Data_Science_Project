% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{multicol}
\usepackage{multirow}
\usepackage{color}
% \usepackage{hyperref}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{float}
\usepackage{dsfont}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%% copied from grid-feat-vqa or slowfast paper [START]
\usepackage[font=small,labelfont=bf]{caption}  % set global caption size
\usepackage{makecell}
\usepackage{tabulary}
\definecolor{demphcolor}{RGB}{144,144,144}
\newcommand{\demph}[1]{\textcolor{demphcolor}{#1}}
\definecolor{mygray}{gray}{0.4}
\usepackage{pifont}% http://ctan.org/pkg/pifont for xmark and cmark
\newcommand{\cmark}{\color{mygray}\ding{51}}%
\newcommand{\xmark}{\color{mygray}\ding{55}}%
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand\hshline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 0.6pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\makeatletter\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}
  {.5em \@plus1ex \@minus.2ex}{-.5em}{\normalfont\normalsize\bfseries}}\makeatother
\def\x{$\times$} %% use as 2\x2 in text
%%%% from grid-feat-vqa paper [END]
\usepackage{xspace}
\def\ModelName{\textsc{FLM}\xspace}


\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}

\definecolor{baselinecolor}{gray}{.9}
\newcommand{\baseline}[1]{\cellcolor{baselinecolor}{#1}}



\begin{document}

\title{Accelerating Vision-Language Pretraining with Free Language Modeling}

\author{
Teng Wang$^{1,2\dagger}$, Yixiao Ge$^{3}$, Feng Zheng$^{1,5*}$, Ran Cheng$^1$, Ying Shan$^3$, Xiaohu Qie$^4$, Ping Luo$^{2,6}$ \\
$^1$Southern University of Science and Technology \ $^2$The University of Hong Kong \\ $^3$ARC Lab, $^4$Tencent PCG \ \ $^5$Peng Cheng Laboratory \ \ $^6$Shanghai AI Laboratory\\
{\tt\small tengwang@connect.hku.hk\ \  \{yixiaoge, yingsshan, tigerqie\}@tencent.com} \ \ \\ 
{\tt\small f.zheng@ieee.org\ \ ranchengcn@gmail.com\ \ pluo@cs.hku.hk}
}

\maketitle



%%%%%%%%% ABSTRACT
\begin{abstract}

\let\thefootnote\relax\footnotetext{$\dagger$ Work done during internship in ARC Lab, Tencent PCG. \\ {\hspace*{1.5em} $*$ Corresponding author}}



   The state of the arts in vision-language pretraining (VLP) achieves exemplary performance 
  %
  but suffers from high training costs resulting from slow convergence and long training time, especially on large-scale web datasets. 
  %
  An essential obstacle to training efficiency lies in the entangled prediction rate {\color{black}(percentage of tokens for reconstruction)} and corruption rate {\color{black}(percentage of corrupted  tokens)} in masked language modeling (MLM), 
  that is, a proper corruption rate is achieved at the cost of a large portion of output tokens being excluded from prediction loss.  
  %
To accelerate the convergence of VLP, we propose a new pretraining task, namely, free language modeling (FLM), that enables a 100\% prediction rate with arbitrary corruption rates.
FLM successfully frees the prediction rate from the tie-up with the corruption rate while allowing the corruption spans to be customized for each token to be predicted.
FLM-trained models are encouraged to learn better and faster given the same GPU time by exploiting bidirectional contexts more flexibly.
%
Extensive experiments show FLM could achieve an impressive $2.5\times$ pretraining time reduction in comparison to the MLM-based methods, while keeping competitive performance on both vision-language understanding and generation tasks. Code will be public at~\url{https://github.com/TencentARC/FLM}.

\end{abstract}


\input{1_intro}
\input{2_related_work}
\input{3_method}
\input{4_experiments}
\input{5_conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\input{6_supp.tex}

\end{document}
 