\section{Introduction}

\begin{figure}
    \centering
    \includegraphics[width=0.46 \textwidth]{figs/fig1_v14.pdf}
    \vspace{-.5em}
    \caption{(a) Large prediction rate accelerates training. 
    Given a fixed corruption rate, we vary the prediction rate by randomly selecting a subset of output tokens for prediction loss. The learning rate schedule follows METER~\cite{dou2022empirical}. (b) The proposed FLM achieves competitive performance compared with MLM meanwhile significantly accelerating the pretraining stage. The downstream performance on NLVR$^2$~\cite{suhr2018corpus} is reported. 
    We show accuracy curves before convergence for better visualization. }
    \vspace{-1.0em}
    \label{fig:intro}
\end{figure}

Vision-language pretraining (VLP) has recently demonstrated impressive performance on a handful of vision-language tasks~\cite{li2019visualbert,chen2020uniter,li2020oscar,jia2021scaling,li2021align,dou2022empirical}, \eg, visual question answering, cross-modal retrieval, and image captioning. Several factors are responsible for the success: the availability of large-scale image-text datasets collected from the web~\cite{sharma2018conceptual}, high-capacity model architectures like Transformer~\cite{vaswani2017attention}, and effective pretraining objectives for cross-modal learning. 

One of the dominant pretraining objectives is masked language modeling (MLM), which was first introduced in natural language processing~\cite{devlin2018bert} and has been applied to vision-language areas in recent years~\cite{li2019visualbert}. MLM is a generative pretraining task designed to reconstruct a few (usually 40\% for VLP) masked text tokens via reasoning among the context of the remaining texts and the paired image. While effective in capturing cross-modal interactions, MLM-based methods~\cite{chen2020uniter,li2020unimo,kim2021vilt} suffer from slow convergence and long training time, especially for large-scale models and noisy web data.


We argue that the limited {prediction rate} in MLM impedes the convergence speed of pretraining, since a large portion of tokens accompanied by corruption are excluded from prediction loss.
%
As shown in Fig.~\ref{fig:intro}~(top), under the same {corruption rate} 
, a larger prediction rate for MLM results in faster convergence of validation loss and downstream performance.
%
It is intuitive to set a prediction rate of 100\% to fully exploit text tokens.
However, a paradox emerges where a large prediction rate can only be achieved with a greater corruption rate in MLM, but an extremely large corruption rate leads to an extremely tough pretraining task that may cause training collapse.

%

Autoregressive language modeling (AR) provides a workable solution to enable a 100\% prediction rate. It predicts the next token given the observation of previous tokens. As shown in Fig.~\ref{fig:intro} (bottom), AR performs favorably in training efficiency against MLM, \ie, 6.1\x speed-up for convergence. However, the converged performance by AR is, unfortunately, much inferior to MLM.
%
It is probably caused by the sub-optimal unidirectional corruption pattern, which is insufficient for downstream understanding tasks that usually rely on bidirectional contexts.

%

A natural question arises, \textit{can we accelerate the convergence of VLP by predicting 100\% tokens like AR meanwhile achieving competitive performance with MLM?} 
Towards this end, we introduce a new pretraining task, dubbed free language modeling (FLM), for VLP, that enjoys an extreme 100\% prediction rate and flexible bidirectional contextualized representations. 
We for the first time break up the entanglement between corruption and prediction rates, making the two factors freely determined. 
Furthermore, for each output token to be predicted, we allow independent and arbitrary-length spans (from one to 100\% tokens) as corrupted connections. 
Rather than the suffix-like corruption pattern as in AR (as well as PrefixLM~\cite{wang2021simvlm}),
%
the corruption span of FLM is primarily distributed in the middle of the sequence, establishing a flexible perception of bidirectional contexts for better adaptation to VL understanding tasks. The comparison between different pretraining objectives is illustrated in Fig.~\ref{fig:LMsComparison}.

%


%
To perform VLP with FLM, we propose an encode-corrupt-predict framework, which performs feature encoding once and reconstructs several corrupted versions of the text sequence in parallel. 
In the encoding step, bidirectional representations are achieved by learning forward and reverse unidirectional representations respectively, the order of which is manipulated by (reverse) casual masks in the same text Transformer.
%
Subsequently, we ensure a 100\% prediction rate by customizing corruption-prediction tasks for predicting each input token. 
In each corruption-prediction task, a span of corruption is randomly sampled and attached to the encoded sequence, followed by a reconstructor to solve the prediction task by reasoning among the remaining contexts. 
%
Unlike previous works (\eg, MLM, AR) that adopt pre-encoding corruption, we inject corruptions after one-time feature encoding, encouraging flexible corruption patterns and efficient parallel prediction.



Our contributions are three-fold.
%
(1) A novel pretraining objective for VLP, namely, free language modeling (FLM), is proposed to free the prediction rate from the constraints of corruption rate, enabling an appealing 100\% prediction rate for accelerating convergence speed during pretraining. 
%
(2) An encode-corrupt-predict framework built upon FLM objective is proposed, allowing efficient and effective learning of a set of prediction tasks by merely conducting feature encoding once.
(3) Extensive experiments on VQA, NLVR$^2$, image captioning, and image-text retrieval demonstrate the effectiveness of our FLM, where comparable performances to MLM are achieved with less than 50\% pretraining time.

%


