\section{Supplementary Materials}

% \subsection{Pretraining Hyper-parameters }

% overall table of all ablations
\begin{table*}[t]
% \vspace{-.2em}
\centering
\small
%#################################################
% Loss term
%#################################################
\subfloat[
\textbf{4M data}
    \label{tab:hparam4m}
]{
% \centering
\begin{minipage}{0.4\linewidth}{\begin{center}
\renewcommand\arraystretch{1.0}
\setlength{\tabcolsep}{1.0 mm}{
    \begin{tabular}{lcc}
        & Ours$_{\rm BASE}$ & Ours$_{\rm LARGE}$ \\
        \midrule
        patch size & 32/16 & 14 \\
        image size & 288\x288 & 336\x336 \\
        learning rate & 4e-4 & 4e-4 \\
        learning rate (pretrained layers) & 8e-5 & 8e-5 \\
        warmup rate & 0.05 & 0.05 \\
        training steps & 30k & 30k
    \end{tabular}}
\end{center}}\end{minipage}
}
\hspace{3em}
%#################################################
% Parameter sharing
%#################################################
\subfloat[
\textbf{13M data}
        \label{tab:hparam13m}
]{
\begin{minipage}{0.4\linewidth}{\begin{center}
\renewcommand\arraystretch{1.0}
\setlength{\tabcolsep}{1.0 mm}{
    \begin{tabular}{lcc}
        & Ours$_{\rm BASE}$ & Ours$_{\rm LARGE}$ \\
        \midrule
        patch size & 32/16 & 14 \\
        image size & 288\x288 & 224\x224 \\
        learning rate & 4e-4 & 4e-4 \\
        learning rate (pretrained layers) & 8e-5 & 8e-5 \\
        warmup rate & 0.05 & 0.05 \\
        training steps & 30k & 100k \\
    \end{tabular}}
\end{center}}\end{minipage}
}
\hspace{2em}
\caption{Hyper-parameters for pretraining.}
\label{tab:hparam}
\end{table*}


\subsection{Implementation Details}

We list the hyperparameters for pretraining in Table~\ref{tab:hparam}. The implementation details for downstream tasks are described as follows.

% \vspace{-.3em}
\paragraph{Visual Question Answering (VQA).} We follow~\cite{chen2020uniter} to consider VQA as a classification problem on 3129 most frequent answers. We input a single $\rm [CLS]$ token upon the reconstructor and regard its output representation as the multimodal features, followed by an MLP classifier to obtain the final classification probability. 
Following~\cite{dou2022empirical}, during fine-tuning, the learning rates of the image encoder and bottom layers of the text transformer are 5e-6, and those for top layers of the text transformer and the reconstructor are 2.5e-5. 

% \vspace{-.3em}
\paragraph{Natural Language for Visual Reasoning for Real (NLVR$^2$).} The task aims to distinguish whether the natural language description is true given a pair of images. We follow~\cite{chen2020uniter} to consider the input triplet (a sentence and two images) as two image-text pairs. For each image-text pair, we obtain the $\rm [CLS]$ embedding from the reconstructor as the multimodal embeddings. The two embeddings are concatenated and input into an MLP for binary classification. Following~\cite{dou2022empirical}, during fine-tuning, the learning rates of the image encoder and bottom layers of the text transformer and the reconstructor are set to 1e-5, and those for top layers of the text transformer are 5e-5. 

% \vspace{-.3em}
\paragraph{Image Retrieval (IR) and Text Retrieval (TR).}  We use the image-text matching loss to finetune the pretrained model on the downstream retrieval datasets, \ie, COCO and Flickr30K. During training, we construct random negative pairs by replacing the paired images with random images sampled from the dataset. An MLP is applied on the $\rm [CLS]$ embedding of the reconstructor for binary classification. The learning rates of the image encoder and bottom layers of the text transformer are 5e-6, and those for top layers and the reconstructor of the text transformer are 2.5e-5. 

% \vspace{-.3em}
\paragraph{Image Captioning.} Since the intermediate loss of our model considers an autoregressive generation process, the finetuning performance or zero-shot performance (shown in Sec.~\ref{sec:zscaption}) on captioning datasets could be evaluated. For finetuning performance, we remove the reconstructor and finetune the model with unidirectional captioning loss. Note that we do not use beam search for simplicity. The learning rates of the image encoder and bottom layers of the text transformer are set to 3e-6, and those for top layers of the text transformer and the reconstructor are 1.5e-5. 



\subsection{Additional Analysis}
\label{sec:zscaption}

\paragraph{More Evidence of the Motivation.}
\input{tables2/prediction_corruption}
In Table~\ref{tab:vilt}, 
our key motivation -- limited prediction rate impedes convergence speed -- is justified in wider environments with different VLP structures and pretraining data, \ie, ViLT (single-encoder structure) and RoBERTa (pretrained on text-only datasets). A consistent trend is found that lower prediction rates gain higher MLM losses, verifying that such motivation is reasonable among different structures and datasets, even in text-only pretraining. 


\paragraph{MLM with Varied Masking Ratios.}
We explore how much acceleration the MLM-based methods could achieve with a larger mask ratio. As shown in Table~\ref{tab:mlmmaskrate}, when increasing the mask ratio from 0.2 to 0.8, a mask ratio of 0.6 achieves the best performance within the 30k steps. However, when the training steps grow after 50k steps, a mask rate of 0.4 achieves the best. Compared with 0.6, MLM with a 0.8 mask ratio shows slower convergence, probably caused by that larger corruption rate increasing the learning difficulty. We conclude that for MLM, the corruption rate and prediction rate are tied-up by the mask ratio, and a proper corruption rate is
achieved at the cost of a large portion of output tokens being excluded from prediction loss.


\paragraph{PrefixLM with Varied $r_{\rm pred}$ and $r_{\rm corr}$.} Similarly to MLM, the
corruption rate and prediction rate in PrefixLM are tied-up in nature. We found all experiments have a similar converge rate but much different converged performance. Table~\ref{tab:prefixLM} shows the best result is achieved with  $r_{\rm pred}$ = 2$ \cdot r_{\rm corr}\!$ = 75\%. 
Lower $r_{\rm corr}$ results in easier tasks and lower representability, while a much larger one may cause learning collapse. 


\input{tables2/supp}

\paragraph{Zero-shot Captioning Performance.} 
The proposed FLM objectives include two parts, a reconstruction loss for solving corruption-prediction tasks with bidirectional contexts, and an intermediate loss that supervises the model and focuses more on temporal relationships with unidirectional context. 
% The intermediate can  be considered as the captioning loss. 
After pretraining, we could directly test the zero-shot captioning performance without further finetuning on target datasets. The zero-shot performance of different pretraining objectives is shown in Table~\ref{tab:zscap}. While MLM-based methods can not be directly used in captioning tasks, FLM achieves reasonable zero-shot captioning performance. However, for finetuned captioning performance, FLM achieves better performance than AR/PrefixLM. We conjecture that the FLM objectives could capture more generalizable features than AR/PrefixLM for captioning after finetuning.


