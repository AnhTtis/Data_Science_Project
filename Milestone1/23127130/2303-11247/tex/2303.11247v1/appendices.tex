%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%

\appendix


%\section{Proof of Theorem \ref{th:synth}}
%	\label{sec:proofofthsynth}



%
%We recall from the theorem statement that the input $x_i$ results in $\omega_i$ active weights. 
%
%
%	Start the program with statement ``if $x_{i1}=0$'' and have an else version. inside of these statements nest ``if $x_{i2}=0$'' and so on. This results in $d$ operations. At the end, we determine which components are zero and which are non-zero. At each branching, we will also uniquely know the corresponding active nodes at layer $1$. We can determine the local fields of these active nodes using $n_0 n_1$ operations. After determining the local fields, $2^{n_1}$ branchings will follow (if local field of  first neuron $> 0$ or else, if local field of  second neuron $> 0$ or else etc), which can be implemented with $n_1$ operations, to $n_2$ active nodes in the next layer. We can determine the local fields of these active nodes using $n_1 n_2$ operations. At the final layer, the branchings will only determine the activations and we are done.
%	
%	
%%\end{proof}



\section{Proof of Theorem \ref{th:synth}}
\label{sec:syntthproof}
\input{syntthproof}

\section{Proof of Theorem \ref{th:reluach}}
\label{sec:reluachthproofer}
We explicitly construct the neural network that achieves the performance as provided in the theorem statement. Our construction relies on several steps as described in the following:

%We also provide an input of $1$ for bias purposes.

{\bf Step-1:} Let $u = [u_1\, u_2 \cdots u_q]^T$ be the input to the neural network, where $u_1 = 1$. Let $\bar{u} = [\bar{u}_1\, \bar{u}_2 \cdots \bar{u}_q]^T$ represent the output of the first layer. First, we translate the dataset vectors such that every component of the translated vectors is positive. We also provide a skip connection for the constant input $1$ at the first component. For the former purpose, given $j\in\{1\ldots,p\}$, let $x_{ij}$ denote the $j$th component of dataset pattern $x_i$. We define the constant
\begin{align}
M = 1+\max_{i\in\{1,\ldots,n\}}\max_{j\in\{1,\ldots,p\}} |x_{ij}|.
\end{align}
The input-output relationships of the first layer is then expressed as
\begin{align}
\bar{u}_1 & = u_1 = 1. \\
\bar{u}_j & = \relu\biggl( \Bigr[\begin{matrix} u_1 \\  u_j \end{matrix} \Bigl]^T\Bigr[\begin{matrix} M \\ 1 \end{matrix} \Bigl] \biggr),\,j=2,\ldots,q.
\end{align}
In particular, if $u = \bar{x}_i$ for some $i\in\{1,\ldots,n\}$, then
\begin{align}
\bar{u}_j = \relu(u_j + M) = u_j + M,\,j=2,\ldots,q.
\end{align}
The last equality follows as $u_j + M = x_{ij} + M$ is positive by the definition of $M$. Hence, for the dataset members, the output after the first layer of neurons is  an additive translation by the vector $[0\,M\cdots M]^T$. There are $p=q-1$ active neurons in the first layer. 


{\bf Step-2:} For a clearer exposition, we first describe the step through an example. We follow a divide and conquer strategy resembling a binary search, which is illustrated in Fig. \ref{fig:divideandconquerduplicate}. Suppose that the inputs are two-dimensional $p=2$ and the outputs are scalars $q=1$. In Fig. \ref{fig:divideandconqueraduplicate}, we show the second and the third components $[\bar{u}_2\,\bar{u}_3]^T$ of $7$ dataset patterns after the first layer, together with the indices of the patterns. We also show a line $w_{11}^T\bar{u} = 0$ that separates the set of points to two subsets such that one side of the line contains $\lceil \frac{7}{2} \rceil = 4$ points, and the other side of the line contains the remaining $3$ points. We will formally show later that, given $m$ points, a separation where one side of the hyperplane contains exactly $\lceil \frac{m}{2} \rceil$ of the points is always possible in general. 


\begin{figure}[h]\centerline{
		\begin{subfigure}[b]{0.45\textwidth}
			\centerline{\scalebox{0.85}{\begin{tikzpicture}
						
						\draw[->] (0,0) -- (4,0) node[anchor=north] {$\bar{u}_2$};
						\draw[->] (0,0) -- (0,4) node[anchor=east] {$\bar{u}_3$};
						\draw (1,1.5) node {$\bullet\,1$}; %label
						\draw (0.6,2.5) node {$\bullet\,7$}; %label
						\draw (1.8,0.75) node {$\bullet\,5$}; %label
						\draw (1.1,3.75) node {$\bullet\,2$}; %label
						\draw (3.4,2.9) node {$\bullet\,3$}; %label
						\draw (3,1.5) node {$\bullet\,4$}; %label
						\draw (3,3.5) node {$\bullet\,6$}; %label
						\draw[line width=1pt, red, <->] (-0.5,1.8) -- (4.5,2.8) node[anchor=north] {$w_{11}^T\bar{u} = 0$};
			\end{tikzpicture}}}
			\caption{Dividing a set of point to two equal subsets.}
			\label{fig:divideandconqueraduplicate}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\textwidth}
			\centerline{\scalebox{0.85}{\begin{tikzpicture}
						
						\draw[->] (0,0) -- (4,0) node[anchor=north] {$\bar{u}_2$};
						\draw[->] (0,0) -- (0,4) node[anchor=east] {$\bar{u}_3$};
						\draw (1,1.5) node {$\bullet\,1$}; %label
						\draw (0.6,2.5) node {$\bullet\,7$}; %label
						\draw (1.8,0.75) node {$\bullet\,5$}; %label
						\draw (1.1,3.75) node {$\bullet\,2$}; %label
						\draw (3.4,2.9) node {$\bullet\,3$}; %label
						\draw (3,1.5) node {$\bullet\,4$}; %label
						\draw (3,3.5) node {$\bullet\,6$}; %label
						\draw[line width=1pt, red, <->] (-0.5,1.8) -- (4.5,2.8) node[anchor=north] {$w_{11}^T\bar{u} = 0$};
						\draw[line width=1pt, blue, <->] (1.8,2.4) -- (2.5,4.1) node[anchor=west] {$w_{21}^T\bar{u} = 0$};
						\draw[line width=1pt, brown, <->] (2.2,2.5) -- (4.5,4) node[anchor=west] {$w_{32}^T\bar{u} = 0$};
						\draw[line width=1pt, black, <->] (1.5,2) -- (4,0.5) node[anchor=west] {$w_{22}^T\bar{u} = 0$};
						\draw[line width=1pt, magenta, <->] (1.7,1.7) -- (0.3,0.3) node[anchor=west] {$w_{33}^T\bar{u} = 0$};
						\draw[line width=1pt, green, <->] (-0.5,2.8) -- (1.8,3) node[anchor=south east] {$w_{31}^T\bar{u} = 0$};
			\end{tikzpicture}}}
			\caption{Continued divisions until reaching singletons.}
			\label{fig:divideandconquerbduplicate}
	\end{subfigure}}
	\caption{The divide and conquer strategy. This figure is a duplicate of Fig. \ref{fig:divideandconquer} in order to make the proof easier to follow.}
	\label{fig:divideandconquerduplicate}
\end{figure}

Once an even division or separation is achieved, the next step is to design what we call a ``switching network''. Switching networks are small two-layer neural networks that can be parametrized by any number of points greater than $1$. Let $0_q$ denote the $q$-dimensional all-zero column vector. Roughly speaking, for the scenario in Fig. \ref{fig:divideandconqueraduplicate}, the corresponding switching network maps the input $\bar{u}\in\mathbb{R}^3$ to $[\begin{smallmatrix} \bar{u} \\ 0_3 \end{smallmatrix}]\in\mathbb{R}^6$ if $\bar{u}$ remains ``above'' the line $w_{11}^T \bar{u} = 0$, and to $[\begin{smallmatrix} 0_3 \\ \bar{u} \end{smallmatrix}]$ if  $\bar{u}$ remains ``below'' the line.\footnote{More precisely, there will be a small margin around the separating hyperplane where the aforementioned input-output relationships may fail. It turns out that this technical complication poses no issues for the patterns that we wish to memorize, as the margin can be made arbitrarily small.} We can now feed the first $3$ components of the output of the switch to one subnetwork, and the last $3$ components to another subnetwork. The two subnetworks are disconnected except that they share different components of the same output as inputs. The first subnetwork follows the same divide and conquer strategy with a switch, but only for the four points that remain over the line $w_{11}^T \bar{u} = 0$. 
The second subnetwork similarly processes the three points that remain under the line $w_{11}^T \bar{u} = 0$. The goal of the all-zero outputs is to deactivate one subnetwork when it is no longer relevant to process a given input. Subnetworks have ``subsubnetworks'' and so on until one arrives at a singleton dataset sample at each partition, as shown in Fig. \ref{fig:divideandconquerbduplicate}. 

Before proceeding to the next step, we formalize the constructions in Step 2 via the following lemma. 
%Due to space limitations, the proof of the lemma is provided in Appendix \ref{sec:switchinglemmaproof} of the supplemental material.
\begin{lemma}
\label{lemma:switch}
For $m \geq 2$, let $a_1,\ldots,a_m\in\mathbb{R}^{q}$ be distinct input patters whose first components equal $1$.

\begin{enumerate}[label={[\roman*]}]
\item There exists $w\in\mathbb{R}^{q}$, such that 
\begin{align}
\label{eq:wcond1} |\{i:w^Ta_i < 0\}| & = \lceil \tfrac{m}{2} \rceil, \\
\label{eq:wcond2} |\{i:w^Ta_i > 0\}| & = m-\lceil \tfrac{m}{2} \rceil.
\end{align}
\item Suppose further that the components of $a_i$s are all non-negative. Let $w\in\mathbb{R}^{q}$  satisfy (\ref{eq:wcond1}) and (\ref{eq:wcond2}). Let $0_q$ represent the $q$-dimensional all-zero vector. There is a two-layer network $\mathbf{S}:\mathbb{R}^q \rightarrow \mathbb{R}^{2q}$ 
that  satisfies the input-output relationships
\begin{align}
\label{eq:saicond}
\mathbf{S}(a_i) = \left\{\begin{array}{rl} \biggl[\begin{matrix} a_i \\ 0_q  \end{matrix}\biggr],  & w^Ta_i < 0,  \\ \vphantom{\scalebox{2.1}{$\int$}} \biggl[\begin{matrix} 0_q \\ a_i \end{matrix}\biggr],  & w^Ta_i > 0. \end{array} \right.,\,i=1,\ldots,m.
\end{align}
The network has $2q+2$ neurons. Two of the $2q+2$ neurons have $q$ weights, and the remaining $2q$ neurons have $2$ weights.
\end{enumerate}
\end{lemma}
\begin{proof}
Let us first prove [i]. Let $\bar{a}_{i} = [a_{i,2},\ldots,a_{i,q}]^T$ denote the $(q-1)$-dimensional vector consisting of all but the first component of $a_i$ (which equals $1$). First, we show the existence of $\bar{w}\in\mathbb{R}^{q-1}$ such that $\bar{w}^T \bar{a}_i \neq \bar{w}^T \bar{a}_j,\,\forall i \neq j$, or equivalently 
\begin{align}
\label{desiredwproperty} \bar{w}^T(\bar{a}_i - \bar{a}_j) \neq 0,\,\forall i \neq j. 
\end{align}
Since the input patterns $a_i$s are distinct, so are $\bar{a}_i$s, and thus $\bar{a}_i - \bar{a}_j$ are non-zero vectors. It follows that any unit norm $\bar{w}$ sampled uniformly at random on $\{x\in\mathbb{R}^q:\|x\|=1\}$ satisfies (\ref{desiredwproperty}) with probability $1$. Let us now order the resulting $(\bar{w}^T\bar{a}_i)$s in ascending order as
\begin{align}
\bar{w}^T\bar{a}_{i_1} < \cdots < \bar{w}^T\bar{a}_{i_{\lceil \frac{m}{2} \rceil}} < \bar{w}^T\bar{a}_{i_{\lceil \frac{m}{2} \rceil+1}} < \cdots < \bar{w}^T\bar{a}_{i_{m}},
\end{align}
for some permutation $i_1,\ldots,i_m$ of $1,\ldots,m$. We can now tune the bias as
\begin{align}
\bar{\bar{w}} \triangleq  - \frac{1}{2}\left(\bar{w}^T\bar{a}_{i_{\lceil \frac{m}{2} \rceil}} + \bar{w}^T\bar{a}_{i_{\lceil \frac{m}{2} \rceil+1}}\right).
\end{align}
The effect of the bias is the ordering (note the zero in the middle)
\begin{align}
\bar{\bar{w}} +\bar{w}^T\bar{a}_{i_1} < \cdots < \bar{\bar{w}} +\bar{w}^T\bar{a}_{i_{\lceil \frac{m}{2} \rceil}} < 0 < \bar{\bar{w}} +\bar{w}^T\bar{a}_{i_{\lceil \frac{m}{2} \rceil+1}} < \cdots < \bar{\bar{w}} +\bar{w}^T\bar{a}_{i_{m}}.
\end{align}
Therefore, the choice $w = [\begin{smallmatrix} \bar{\bar{w}} \\ {\bar{w}} \end{smallmatrix}]$ satisfies  conditions (\ref{eq:wcond1}) and (\ref{eq:wcond2}). This concludes the proof of [i]. 

We now prove [ii]. Let 
\begin{align}
C_1 & \triangleq 1+\max_{i\in\{1,\ldots,m\}}\max_{j\in\{1,\ldots,q\}} |a_{ij}|, \\
C_2 & \triangleq \min_{i\in\{1,\ldots,m\}} |w^Ta_{i}|.
\end{align}
Let $v =[v_1 \cdots v_{q}]^T\in \mathbb{R}^q$ represent an input to the neural network $\mathbf{S}$ that we shall construct. Also, let $v^+ = [v_1^+ \cdots v_{q}^+]^T$ and $v^-=[v_1^- \cdots v_{q}^-]^T \in \mathbb{R}^{q}$ denote the first and the last $q$ components of the $2q$-dimensional output of $\mathbf{S}$. We thus have $\mathbf{S}(v) = [\begin{smallmatrix} v^+ \\ v^- \end{smallmatrix}]$, and set
\begin{align}
v_j^+ & = \relu\Biggl( \biggr[\begin{matrix} v_j \\ \vphantom{\scalebox{1.3}{$\int$}} y^+ \end{matrix} \biggl]^T\biggr[\begin{matrix} 1 \\ \vphantom{\scalebox{1.3}{$\int$}} -\!\frac{C_1}{C_2} \end{matrix} \biggl] \Biggr),\,v_j^- = \relu\Biggl( \biggr[\begin{matrix} v_j  \\ \vphantom{\scalebox{1.3}{$\int$}} y^- \end{matrix} \biggl]^T\biggr[\begin{matrix} 1 \\ \vphantom{\scalebox{1.3}{$\int$}} -\!\frac{C_1}{C_2} \end{matrix} \biggl] \Biggr),\,j\in\{1,\ldots,q\},
\end{align}
where $y^+ = \relu(w^T v)$ and $y^- = \relu(-w^T v)$. Let us now verify (\ref{eq:saicond}). Consider some index $i$ with $w^T a_i < 0$, and suppose $v = a_i$. For any $j\in\{1,\ldots,q\}$, we have 
\begin{align}
v_j^+ & = \phi(v_j - \tfrac{C_1}{C_2}y^+)  = \phi(v_j) = \phi(a_{ij}) = a_{ij}.
\end{align}
The last equality holds as the components of $a_i$ are assumed to be all non-negative. Also, 
\begin{align}
v_j^-  = \phi(v_j - \tfrac{C_1}{C_2}y^-) 
= \phi(a_{ij} - \tfrac{C_1}{C_2}|w^Ta_i|)  \leq \phi(a_{ij} - C_1)  = 0.
\end{align}
Inequality follows as $\frac{|w^Ta_i|}{C_2} \geq 1$ and $\relu$ is monotonic. Since $v_j^- \geq 0$ obviously holds, we have $v_j^- = 0$. This proves the case $w^Ta_i < 0$ in (\ref{eq:saicond}). The remaining case  $w^Ta_i > 0$ can be verified in a similar manner. This concludes the proof of [ii], and thus that of Lemma \ref{lemma:switch} as well.
\end{proof}






\begin{figure}[h]
	\centerline{\scalebox{1}{\begin{tikzpicture}[
	arrowstyle/.style={decoration={markings,mark=at position 1 with
		{\arrow[scale=1.7,>=stealth]{>}}},postaction={decorate}},
roundnode/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=2mm},
squarednode/.style={rectangle, draw=green!60, fill=yellow!5, very thick, minimum size=5mm},
squarednode2/.style={rectangle, draw=magenta!60, fill=magenta!5, very thick, minimum size=5mm},
squarednode3/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=5mm},
squarednode4/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=5mm},squarednode5/.style={rectangle, draw=orange!90, fill=orange!15, very thick, minimum size=5mm},]

\node[inner sep=0pt, minimum size=3mm](n1x) at (-0.75,-0.5){$=[\begin{smallmatrix} 1 \\ x_6 \end{smallmatrix}]$};

\node[inner sep=2pt, minimum size=3mm](n1) at (-1,0) {$u=\bar{x}_6$};
\node[squarednode](t) at (0.5,0) {$\mathbf{T}$};
\draw [arrowstyle](n1)--(t);
\node[squarednode2](s11) at (2.5,0) {$\mathbf{S}_{11}$};
\draw [arrowstyle](t)--node [below] {$\bar{u}=\bar{\bar{x}}_6$}(s11);
\node[squarednode2](s21) at (4,1.8) {$\mathbf{S}_{21}$};
\node[squarednode2](s22) at (4,-1.8) {$\mathbf{S}_{22}$};
\draw [arrowstyle](s11)--node [below] {$\,\,\bar{\bar{x}}_6$}(s21);
\draw [arrowstyle](s11)--node [below] {$0$}(s22);
\node[squarednode2](s32) at (5.5,0.9) {$\mathbf{S}_{32}$};
\node[squarednode2](s31) at (5.5,2.7) {$\mathbf{S}_{31}$};
\node[squarednode2](s33) at (5.5,-2.7) {$\mathbf{S}_{33}$};

\draw [arrowstyle](s21)--node [below] {$0$}(s31);
\draw [arrowstyle](s21)--node [below] {$\!\!\bar{\bar{x}}_6$}(s32);
\draw [arrowstyle](s22)--node [below] {$0$}(s33);

\node[squarednode3](g2) at (7,3.15) {$\gamma_2$};
\node[squarednode3](g7) at (7,2.25) {$\gamma_7$};
\node[squarednode3](g6) at (7,1.35) {$\gamma_6$};
\node[squarednode3](g3) at (7,0.45) {$\gamma_3$};
\node[squarednode3](g5) at (7,-3.15) {$\gamma_5$};
\node[squarednode3](g1) at (7,-2.25) {$\gamma_1$};
\node[squarednode3](g4) at (7,-0.9) {$\gamma_4$};

\draw [arrowstyle](s31)--node [above] {$0$}(g2);
\draw [arrowstyle](s31)--node [below] {$0$}(g7);
\draw [arrowstyle](s32)--node [above] {$\bar{\bar{x}}_6$}(g6);
\draw [arrowstyle](s32)--node [below] {$0$}(g3);
\draw [arrowstyle](s22)--node [above] {$0$}(g4);
\draw [arrowstyle](s33)--node [above] {$0$}(g1);
\draw [arrowstyle](s33)--node [below] {$0$}(g5);

%				\node[squarednode5](s32sum) at (8.5,0.9) {$\sum$};
%				\node[squarednode5](s31sum) at (8.5,2.7) {$\sum$};
%				\node[squarednode5](s33sum) at (8.5,-2.7) {$\sum$};
%				\node[squarednode5](s21sum) at (9.5,1.8) {$\sum$};
%				\node[squarednode5](s22sum) at (9.5,-1.8) {$\sum$};
\node[squarednode5](finalsum) at (9.5,0) {$\sum$};
\node[inner sep=2pt, minimum size=4mm](output) at (10.5,0) {$d_6$};
\draw [arrowstyle](finalsum)--(output); 

\draw [arrowstyle](g2)--node [above] {$0$}(finalsum);
\draw [arrowstyle](g7)--node [above] {$0$}(finalsum);
\draw [arrowstyle](g6)--node [above] {$d_6$}(finalsum);
\draw [arrowstyle](g3)--node [above] {$0$}(finalsum);
\draw [arrowstyle](g4)--node [above] {$0$}(finalsum);
\draw [arrowstyle](g1)--node [above] {$0$}(finalsum);
\draw [arrowstyle](g5)--node [above] {$0$}(finalsum);

%				\draw [arrowstyle](s33sum)--node [below] {$0$}(s22sum);
%				\draw [arrowstyle](s32sum)--node [below] {$d_6$}(s21sum);
%				\draw [arrowstyle](s31sum)--node [below] {$0$}(s21sum);
%				\draw [arrowstyle](s21sum)--node [below] {$d_6$}(finalsum);
%				\draw [arrowstyle](s22sum)--node [below] {$0$}(finalsum);
%				

	\end{tikzpicture}}}
	\caption{An example network architecture for the achievability result. The block $\mathbf{T}$ represents the transformation in Step 1. Blocks $\mathbf{S}_{ij}$ are the routing switches. Blocks $\gamma_i$ represent ReLU neurons with weights $\gamma_i$, and the $\sum$ block represents a ReLU neuron with all-one weights. This figure is a duplicate of Fig. \ref{fig:nnforachievability} in order to make the proof easier to follow.}
	\label{fig:nnforachievabilityduplicate}
\end{figure}



{\bf Step-3: } We can now proceed to describe the full network architecture, as shown in Fig. \ref{fig:nnforachievabilityduplicate} for the example in Step 2. The first layer of the network is an additive translation by the vector $[0\,M\cdots M]^T$, and is explained in Step 1 above. We use the notation $\mathbf{T}$ to denote the translation, which is followed by a sequence of switches as described in Step 2. In particular, $\mathbf{S}_{ij}$ provides the outputs of $\bar{u}$ and $0_3$ to its top and bottom branches, respectively, if its input $\bar{u}$ remains above the line defined by $w_{ij}$ in Fig. \ref{fig:divideandconquerbduplicate}.
By construction, the neurons on a given path of switches is activated for a unique dataset member. For example, the path $\mathbf{S}_{11},\mathbf{S}_{21},\mathbf{S}_{32}$ is activated only for the dataset member $x_6$. The path of switches that correspond to some $x_i$ is followed by a ReLU neuron whose weights satisfy the property that $\relu(\gamma_i^T \bar{\bar{x}}_i) = d_i$, where $\bar{\bar{x}}_i$ is the output of the first layer when the input to the network is $\bar{x}_i$. Since the first component of $\bar{\bar{x}}_i$ equals $1$ for any $i$, one can simply set the first component of $\gamma_i$ to be equal to $d_i$, and the rest of the component of $\gamma_i$ to be equal to zero. The final layers of the network simply adds all the outputs from the $\gamma_i$-neurons.


%are a cascade of ReLU neurons that simply add up all the outputs from the $\gamma_i$-neurons in a divide-and-conquer manner. We avoid summing up all the outputs of $\gamma_i$-neurons at once to achieve bounded number of weights for every neuron of the network. The topology of the summation neurons exactly mirrors the topology of switches.



In the figure, we also show the induced signals on the branches when the input is $u=\bar{x}_6$ as an example. Note that only the block $\mathbf{T}$, switches $\mathbf{S}_{11},\mathbf{S}_{21},\mathbf{S}_{32}$, the neuron with weight $\gamma_6^T$, and the a subset of the summation neurons remains active. Most of the neurons of the network are deactivated through zero signals. We also note that the desired output signal $d_6$ is obtained. 

%It is easily observed that all other desired input-output relationships are also satisfied. 

%Also, each subsequent summation unit will consist of $r$ sub-summation units operating on individual components. 

The construction generalizes to an arbitrary dataset of cardinality $n$ in the same manner. The only difference is that, in order to support  $r$-dimensional desired outputs as stated in the theorem, we need to use $r$ ReLU units in place of each $\gamma_i$ to reproduce the $r$ components of the desired output, as opposed to a single ReLU unit in the example above. Also, the final summation unit will consist of $r$ sub-summation units operating on individual components. Formally, the first layer for the general case is the translation $\mathbf{T}$ as before. Next, $\lceil \log_2 n \rceil$ layers of switches arranged on a binary tree structure act on the translated inputs, forming $n$ leaves as the output, where each leaf has the same dimension as the input. Let $h_1,\ldots,h_n$ denote the feature vectors produced at the leaf nodes after the switches. By Lemma 1, and a rearrangement of indices, we can guarantee that for every $i$, if the network input is $\bar{x}_i$, then $h_i = \bar{\bar{x}}_i$ and $h_j = 0,\,\forall j \neq i$. Here,  $\bar{\bar{x}}_i = \mathbf{T}([1\,\, x_i^T]^T)$, as defined in Fig. \ref{fig:nnforachievabilityduplicate}. Each leaf is then followed by a single-layer network that can map $\bar{\bar{x}}_i$ to its corresponding desired output vector $d_i$. Specifically, there exists $U_i$ such that $d_i = \phi(U_i \bar{\bar{x}}_i)$. Indeed, since $\sum_k u_{i,j,k}\bar{\bar{x}}_{i,k} = d_{i,j}$ has to be satisfied, one can pick $u_{i,j,k}=0,\,k\neq 1$ and $u_{i,j,1} = d_{i,j}/\bar{\bar{x}}_{i,1} = d_{i,j}$, where the last equality holds as $\bar{\bar{x}}_{i,1}=1$ by construction. The overall network output is the accumulation of the outputs of the $U_i$-neurons, and is given by $f(\overline{x}_i) = \phi(\sum_{i=1}^n \phi(U_ih_i))$. Let us show that all input-output relationships are satisfied so that the claim $f(\bar{x}_i) = d_i,\,\forall i$ in the statement of the theorem holds. If the input is $\bar{x}_i$, we have $h_i = \bar{\bar{x}}_i$ and $h_j = 0,\,j\neq i$, by construction. As a result, $f(\overline{x}_i) = \phi(\phi(U_i \bar{\bar{x}}_i)) = \phi(d_i) = d_i$, as desired. The last equality holds as the components of $d_i$ are assumed non-zero in the statement of the theorem (A ReLU network cannot provide a negative output.).

Let us now calculate the number of active neurons and weights when the input belongs to any member of the dataset. The block $\mathbf{T}$ always remains active and consists of $q-1$ neurons with weight $2$. There are at most $\lceil \log_2 n \rceil$ active switches per input. Each switch contains $2q+2$ neurons with $6q$ weights total. There is one active block $\gamma_i$ consisting of $r$ neurons with $q$ weights each. Finally, the sum unit has $r$ active weights. Hence, an upper bound on the total number of active neurons are given by 
\begin{align}
q-1  + (2q+2)\lceil \log_2 n \rceil+r+1 & = 2(q+1)\lceil \log_2 n \rceil + q+r\\
& \in O(r+q\log n),
\end{align}
and an upper bound on the number of active weights can be calculated to be 
\begin{align}
2(q-1) + 6q(2q+2)\lceil \log_2 n \rceil + rq+r & =12q(q+1)\lceil \log_2 n \rceil + (r+2)q+r-2 \\
& \in O(rq+q^2\log n).
\end{align}
 This concludes the proof of the theorem.

%q-1  + (2q+2)\lceil \log_2 n \rceil+r+1$

%2(q-1) + 6q(2q+2)\lceil \log_2 n \rceil + rq+r$

%%
%\section{Proof of	 Lemma \ref{lemma:switch}}
%\label{sec:switchinglemmaproof}


%\section{Proof of Lemma \ref{lemma:treeleafnodecounterlemma}}
%\label{sec:proofoftreeleafnodecounterlemma}
%We use the notation $a \in b$ to indicate that $a$ is a children of $b$. 
%Consider an arbitrary path $\nu_0\rightarrow \nu_1 \rightarrow \cdots \rightarrow \nu_{L}$. We have $\prod_{\ell=1}^L \sigma(\nu_{\ell}) \leq K$. Let $\chi(\nu_{L-1})$ denote the number of children of $\nu_{L-1}$. Summing up over all children of $\nu_{L-1}$, we have
%\begin{align}
%\prod_{\ell=1}^{L-1}\sigma(\nu_{\ell})  \sum_{\nu_{L} \in \nu_{L-1}} \sigma(\nu_{L}) \leq K \chi(\nu_{L-1})
%\end{align}
%Since $\chi(\nu_{L-1}) = \sigma(\nu_{L})$ for every $\nu_{L} \in \nu_{L-1}$, we obtain
%\begin{align}
%\prod_{\ell=1}^{L-1}\sigma(\nu_{\ell})  \Biggl(\sum_{\nu_{L} \in \nu_{L-1}} 1 \Biggr) \leq K 
%\end{align}
%Continuing in the same manner by summing over all children of $\nu_{\ell}$ and then simplifying by using the identity $\chi(\nu_{\ell-1}) = \sigma(\nu_{\ell})$ for $\ell=L-2,\ldots,1$, we arrive at
% \begin{align}
%\sum_{\nu_{1} \in \nu_{0}} \cdots \sum_{\nu_{L} \in \nu_{L-1}} 1 \leq K.
% \end{align}
% The left side is merely the number of leaf nodes. This concludes the proof of the lemma.

%\section{Proofs of Main Results for Threshold Networks}
%\label{sec:proofofthresholdtheorems}
%\subsection{Proof of Theorem \ref{th:thresholdach}}
%\label{sec:proofoftheoremthresholdach}
%As identified by several studies, and also by  \citet{rajput2021exponential} in particular, a key challenge in threshold networks is that the amplitude information of the input patterns is lost as soon as one passes through the first layer of the network. This is why we need a mapping that maps each input pattern to a unique binary representation in the first layer. The existence of such a mapping has been elegantly proven in \citet[Section 5, Step 1]{rajput2021exponential} by using the idea of random Gaussian weights. Their argument reveals that there exists a layer with $\smash{O(\frac{\log n}{\delta})}$ neurons that can map each input pattern to a unique binary representation. This layer becomes the first layer of our network. The rest of the layers of the network is exactly the same as in Steps 2 and 3 of the proof of Theorem \ref{th:reluach} in Appendix \ref{sec:reluachthproofer}, with the only exception being that all ReLU activation functions are replaced by threshold activation functions. Remarkably, when its inputs is a binary vector, the functionality of the switching network as constructed in the proof of Lemma \ref{lemma:switch} can easily be verified to remain the same under such a change in activation functions. Likewise, the functionality of the summation units will remain the same, given that they will only admit either two zeros or a zero and one as inputs, when the input pattern corresponds to a dataset pattern. As a result, we obtain the same bound in Theorem \ref{th:reluach} for the special case $\smash{p=\frac{\log n}{\delta}}$ in the statement of Theorem \ref{th:reluach}. This concludes the proof of Theorem \ref{th:thresholdach}.
%\subsection{Proof of Theorem \ref{th:thresholdconv}}
%\label{sec:proofoftheoremthresholdconv}
% For a threshold network, the configuration tree corresponding to the network and the dataset can be constructed in the same manner as described in the proof of Theorem \ref{th:reluconv}. The difference is that for a threshold network, the output configurations at the leaves of the tree correspond to all possible outputs that the network can provide. Since there are at most $2^{\alpha}$ leaves and the desired output vectors are all distinct, we need $n \leq 2^{\alpha}$ for successful memorization. Taking the logarithms of both sides of the inequality concludes the proof of the theorem.
%

