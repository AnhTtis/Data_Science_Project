\documentclass{article}


\usepackage{appendix}
\usepackage{algorithm,algpseudocode, stmaryrd}
\usepackage[compress]{cite}
\usepackage{afterpage}



% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage[left=1in,top=1in,bottom=1in,right=1in]{geometry}

\usepackage{wrapfig}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.markings}

\tikzset{
  treenode/.style = {shape=rectangle, rounded corners,
                     draw, align=center,
                     top color=white, bottom color=blue!30},
  treenode2/.style = {shape=rectangle, rounded corners,
                     draw, align=center,
                     top color=white, bottom color=green!30},
  env/.style      = {treenode, font=\ttfamily\normalsize},
  env2/.style      = {treenode2, font=\ttfamily\normalsize},
  dummy/.style    = {circle,draw}
}

\usepackage{caption,subcaption}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath, amssymb, amsthm, enumitem}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\newcommand{\relu}{\phi}

\title{Memorization Capacity of Neural Networks with Conditional Computation}

%On the Fundamental Limits of Conditional Computation in Neural Networks


\author{%
  Erdem Koyuncu \\
  Department of Electrical and Computer Engineering \\
  University of Illinois Chicago\\
  \texttt{ekoyuncu@uic.edu} \\
}

\newcounter{example}[section]
\newenvironment{example}[1][]{\refstepcounter{example}\par\medskip
   \noindent \textbf{Example~\theexample. #1} \rmfamily}{\medskip}



\begin{document}


\maketitle


\begin{abstract}
Many empirical studies have demonstrated the performance benefits of conditional computation in neural networks, including reduced inference time and power consumption. We study the fundamental limits of neural conditional computation from the perspective of memorization capacity. 
For  Rectified Linear Unit (ReLU) networks without conditional computation, it is known that memorizing a collection of $n$ input-output relationships can be accomplished via a neural network with $O(\sqrt{n})$ neurons. Calculating the output of this neural network can be accomplished using $O(\sqrt{n})$ elementary arithmetic operations of additions, multiplications and comparisons for each input. Using a conditional ReLU network, we show that the same task can be accomplished using only $O(\log n)$ operations per input. This represents an almost exponential improvement as compared to networks without conditional computation.\ We also show that the $\Theta(\log n)$ rate is the best possible. Our achievability result utilizes a general methodology to synthesize a conditional network out of an unconditional network in a computationally-efficient manner, bridging the gap between unconditional and conditional architectures. 
\end{abstract}

\input{intro.tex}
\input{model.tex}
\input{reluach.tex}
\input{reluconv.tex}
%\input{threshold.tex}
\input{conclusions.tex}

\newsavebox{\shortpagebox}


\subsubsection*{Acknowledgments}
This work was supported in part by National Science Foundation (NSF) under Grant CNS-2148182 and in part by Army Research Lab (ARL) under Grant W911NF-2120272.

\bibliographystyle{IEEEtran}
\bibliography{references}

\input{appendices.tex}





\end{document}