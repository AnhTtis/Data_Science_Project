
\section{Ultimate Computational Limits to Memory Recall}
\label{sec:reluconv}
%\subsection{Main Result}
In the previous section, we showed that $O(\log n)$ operations is sufficient to perfectly recall one of $n$ input-output relationships. We now analyze the necessary number of operations per input for successful recall. Our main result in this context is provided by the following theorem.

\begin{theorem} 
\label{th:reluconv}
Let the input vectors $x_1,\ldots,x_n\in\mathbb{R}^p$ and the corresponding desired output vectors $d_1,\ldots,d_n\in\mathbb{R}$ satisfy the following  property:
\begin{itemize}
%\item For every $i\in\{1,\ldots,n\}$, every component of $x_i$ is non-zero.
\item The matrix $\Bigl[\begin{matrix}x_{i_1} & \cdots & x_{i_{p+1}} \\ d_{i_1} & \cdots & d_{i_{p+1}} \end{matrix} \Bigr]$ has rank $p+1$ for any subset $\{i_1,\ldots,i_{p+1}\}$ of $\{1,\ldots,n\}$. 
\end{itemize}
Suppose that a conditional network $f$ satisfies the desired input-output relationships: For every $i$, the output of the network is $d_i$ whenever the input is $x_i$. Also, assume that the number of operations performed on each $x_i$ is bounded above by some $\alpha \geq 0$. Then, we have $\alpha \geq \log_2 \frac{n}{p} \in O(\log n)$.
%Then, for any conditional network $f$ with $A(x_i;f) \leq \alpha,\,f(x_i) = d_i,\,\forall i$, we have $\alpha \geq \log_2 \frac{n}{p}$. 
\end{theorem}
\begin{proof}
%	Suppose, contrary to the statement of the theorem, that there are $\alpha < \log_2 \frac{n}{p}$ operations for each input. This means that there are less than $\alpha < \log_2 \frac{n}{p}$ comparisons for each input (including the comparisons that have to be made to implement the neuron activation functions). 	
	Since there are at most $\alpha$ operations per input, there are at most $\alpha$ comparisons per input as well, counting the comparisons needed to implement the neuron activation functions.  We can represent the entire conditional computation graph/network as a binary tree where the distance between the root and a leaf node is at most $\alpha$. This results in tree of at most $2^{\alpha}$ leaf nodes. Each node of the tree compares real numbers to intermediate variables, which are linear functions of network inputs or other intermediate variables. Assume now the contrary to the statement of the theorem, i.e. the number of operations for every input is at most $\alpha$, all input-output relationships are satisfied, but $n > 2^\alpha p$. Since there are at most $2^\alpha$ leaf nodes, there is at least one leaf node that admits $1+p$ inputs (i.e. there are $1+p$ input patterns of the dataset such that the corresponding path over the  tree ends at the leaf node). Without loss of generality, suppose the indices for these inputs are $\{1,\ldots,1+p\}$. Writing down the input output relationship of the network for the leaf node, we obtain
	\begin{align}
		\label{rouchecond}
		[d_1 \cdots d_{1+p}] = W_0[x_1 \cdots x_{1+p}]
	\end{align}
	for some $q\times p$ matrix $W_0$. This relationship follows, as by fixing 
	a path on the tree, we obtain the unique linear transformation $W_0$ that maps the inputs to the neural network to its outputs. According to the Rouch\'{e}â€“Capelli theorem \cite[Theorem 2.38]{shafarevich2012linear}, a necessary condition for the existence of $W_0$ to solve (\ref{rouchecond}) is 
	\begin{align}
		\mathrm{rank}([x_1 \cdots x_{p+1}]) = \mathrm{rank}\Bigl(\Bigl[\begin{matrix}x_1 & \cdots & x_{p+1} \\ d_{1} & \cdots & d_{p+1} \end{matrix} \Bigr]\Bigr).
	\end{align}
	On the other hand, as a result of the rank condition stated in the theorem, the left hand side rank evaluates to $p$, while the right hand side evaluates to $1+p$. We arrive at a contradiction, which concludes the proof of the theorem.
\end{proof}

%
%$x_1,\ldots,x_d$: input variables:
%
%$y_1,\ldots,y_m$: intermediate variables. these can only be linear functions of other $y_i$s and $x_i$s.
%
%Suppose we do at most $m$ operations for every input. Then, there are at most $m$ conditionings. for every input. 

%The proof relies on constructing a tree that provides a blueprint for the operation of a given neural network for a given dataset. By construction, the number of leaves of the tree equals the number of memory patterns that the neural network can store up to constant multipliers. Moreover, the number of leaves is bounded above by $2^{\alpha}$. Combining the two statements proves the theorem. The details are provided in Section \ref{sec:proofofthreluconv}.

The  condition on the dataset and the desired outputs that appear in the statement of Theorem \ref{th:reluconv} is, up to a certain extent, necessary for the lower bound to hold. For example, if the desired outputs can simply be obtained as a linear transformation of inputs, then one only needs to perform a constant number of operations for each input to obtain the desired outputs, and the lower bound becomes invalid. In this context, the rank condition ensures that subsets of outputs cannot be obtained as linear functions of inputs. However, it should also be mentioned that the condition is not particularly restrictive in limiting the class of datasets where the theorem holds. For example, if the components of the dataset members and the corresponding desired outputs are sampled in an independent and identically distributed manner over any continuous distribution with positive support, it can be shown that the rank condition will hold with probability $1$. Hence, it can be argued that almost all datasets satisfy the rank condition and thus obey the converse result.  

Corollary \ref{toyotacorolla} has shown that $n$ patterns can be memorized using  $O(\log n)$ operations per input. The matching $\Omega(\log n)$ lower bound in Theorem \ref{th:reluconv} proves that the $\Theta(\log n)$ rate is the best possible. However, the two results do not resolve how the number of operations should scale with respect to the input and output dimensions.\footnote{Although Theorem \ref{th:reluconv} only considers scalar desired outputs, it can easily be extended to the multi-dimensional case. In fact, a successful memorization of, say, two-dimensional output vectors with $o(\log n)$ active neurons would imply the successful memorization of scalar outputs with the same number of neurons (simply by ignoring the neurons that provide the second component of the output), contradicting Theorem \ref{th:reluconv}.} This aspect of the problem is left as future work.


