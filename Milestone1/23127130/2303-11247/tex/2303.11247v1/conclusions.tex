\section{Conclusions and Discussions}
\label{sec:conclusions}
%Various studies have demonstrated the benefits of conditional computation in reducing the inference time and power consumption of neural networks. Achieving these benefits are especially critical in edge applications with resource-limited devices. While most previous work followed a numerical approach, 

We have studied the fundamental limits to the memorization capacity of neural networks with conditional computation. First, we have described a general procedure to synthesize a conditional network out of an ordinary unconditional feedforward network. According to the procedure, the number of operations required to perform inference on an input in the synthesized conditional network becomes proportional to the number of so-called ``active weights'' of the unconditional network given the same input. This reduces the problem of designing good conditional networks to the problem of designing ordinary feedforward networks with a low number of active weights or nodes. Using this idea, we have shown that for ReLU networks, $\Theta(\log n)$ operations per input is necessary and sufficient for memorizing a dataset of $n$ patterns. An unconditional network requires $\Omega(\sqrt{n})$ operations to achieve the same performance. We also described a method to synthesize a conditional network out of an unconditional network in a computationally-efficient manner.


One direction for future work is to study the memorization capacity for a sum-constraint,\ as opposed to a per-input constraint on the number of operations.\ While a per-input constraint makes sense for delay-sensitive applications, the sum-constrained scenario is also relevant for early-exit architectures, where there is a lot of variation on the size of active components of the network. Extensions of our results to different activation functions or to networks with bounded bit complexity can also be considered. In this context,  \cite{vardi2021optimal} shows that, for every $\epsilon\in[0,\frac{1}{2}]$, $\Theta(n^{\epsilon})$ weights with $\Theta(n^{1-\epsilon})$ bit complexity is optimal for memorizing $n$ patterns, up to logarithmic factors.  This result was proven under a mild separability condition, which restricts distinct dataset patterns to be $\delta$-separated in terms of Euclidean distance. The optimality of the results of \cite{vardi2021optimal} suggests that under a similar separability condition, the bit complexity of our designs can also potentially be reduced without loss of optimality. This will remain as another interesting direction for future work.

Many neural network architectures rely heavily on batch computation because matrix multiplications can be performed very efficiently on modern processors. In this context, one disadvantage of conditional architectures is their general misalignment with the idea of batching. Nevertheless, if there are not too many branches on the network, and if the branch loads are balanced, each subbranch can still receive a relatively large batch size. Fortunately, only a few gates can promise significant performance gains \cite{fedus2022switch}. More work is needed, however, to make networks with aggressive conditioning more efficient in the batch setting. In this context, our synthesis theorem can potentially enable conditional networks to be trained as if they are unconditional, enabling batch computation. We note that in techniques like soft-gating \cite{shazeer2017outrageously}, a batch also traverses the entire computation graph during training \cite{kaya2019shallow}.

The main goal of this paper has been to derive theoretical bounds on the memorization capacity. More research is clearly needed for practical methods to train neural networks that can effectively utilize conditional computation and also generalize well. We hope that the constructions and theoretical bounds provided in this paper will motivate further research in the area. 








%Also, the main goal of this paper has been to derive theoretical bounds on the memorization capacity. It is not clear how the proposed constructions would fare in practical datasets. In this context, it is known that tree-structured decision networks, which have formed the basis of our achievability results, do not generalize well to unseen examples. More research is clearly needed for practical methods to train neural networks that can effectively utilize conditional computation. We hope that the theoretical bounds provided in this paper will motivate further research in the area. 


%We note that this paper  specifically on analyzing the fundamental theoretical limits on neural conditional computation. 


%
%We note that this paper is specifically on analyzing the fundamental theoretical limits on neural conditional computation. In particular, we show that $n$ input-output relationships can be memorized using a conditional neural network that needs only $O(\log n)$ operations per input or inference step. The best unconditional architecture requires $O(\sqrt{n})$ operations for the same task. Understanding the memorization capacity is of fundamental importance as it is related to the expressive power of neural networks A related but separate problem is generalization, i.e. how the designed neural networks that can precisely recall the memory patterns generalize to unseen examples. 
%
%Also, the main goal of this paper has been to derive theoretical bounds on the memorization capacity. It is not clear how the proposed constructions would fare in practical datasets. In this context, it is known that tree-structured decision networks, which have formed the basis of our achievability results, do not generalize well to unseen examples. More research is clearly needed for practical methods to train neural networks that can effectively utilize conditional computation. We hope that the theoretical bounds provided in this paper will motivate further research in the area. 

%Another interesting extension would be to study the limits of 

% with a sum conditional neuron constraint
% other activation functions
% bit complexity
