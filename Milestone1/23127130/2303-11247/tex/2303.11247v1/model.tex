
\section{System Model}
\label{sec:model}

\subsection{Unconditional Feedforward Networks}

Consider an ordinary unconditional feedforward network of a homogeneous set of neurons, all of which have the same functionality. We also allow skip connections, which are mainly utilized for the generality of the model for converse results (The construction in our achievability results also skip layers, but at most one layer at a time). Formally, we consider an $L$-layer network with the ReLU operation $\phi(x) = \max\{0,x\}$, and the input-output relationships
\begin{align}
\label{eq:systemmodel}
y_{\ell} = \phi\left(W_{\ell} \left[y_{\ell-1}^T \cdots  y_0^T \right]^T\right),\,\ell=1,\ldots,L,
\end{align}
where $y_{0}$ is the input to the neural network, $y_{\ell}$ is the output at Layer $\ell$, and $y_L$ is the neural network output. Also, $W_{\ell}$ is the weight matrix at Layer $\ell$ of appropriate dimensions. %We consider the ReLU operation $\phi(x) = \max\{0,x\}$.
\subsection{Measuring the Cost of Computation}
\label{sec:meascost}
A recurring theme of the paper will be to calculate the output of a neural network given an arbitrary input, however with low computational complexity. In particular, in (\ref{eq:systemmodel}), given the outputs of Layer $\ell-1$, the calculation of the outputs of Layer $\ell$ can be accomplished through multiplication with matrix $W_{\ell}$, followed by the activation functions. Each ReLU activation function can be implemented via a simple comparison given local fields. Hence, calculation of the output of Layer $\ell$ can be accomplished with $O(\mathrm{dim}W_{\ell})$ operations (multiplications, additions, and comparisons), and the output $y_L$  of the entire network can be calculated using $O(\sum_{\ell}\mathrm{dim}W_{\ell})$ operations. Here, $\mathrm{dim}W_{\ell}$ represents the product of the number of rows and columns in $W_{\ell}$. In other words,  $\mathrm{dim}W_{\ell}$ is the number of dimensions in $W_{\ell}$. Hence, in unconditional architectures, the number of operations to calculate the output of the network is essentially the same (up to constant multipliers) as the number of weights in the network. With regards to our basic measure of complexity, which relies on counting the number of operations, one can argue that multiplication is more complex than addition or comparison, and hence should be assigned a larger complexity. We hasten to note that the relative difficulty of different operations will not affect our final results, which will have an asymptotic nature.

It is instructive to combine the $O(\sum_{\ell}\mathrm{dim}W_{\ell})$ complexity baseline with the memorization results described in Section \ref{sec:intro}. Let $X = \{x_1,\ldots,x_n\}\subset\mathbb{R}^p$ be a dataset of inputs. Let $d_1,\ldots,d_n\in\mathbb{R}^r$ be the corresponding desired outputs. In the memorization task, one wishes to design a network that can provide an output of $d_i$ for an input of $x_i$ for each $i\in\{1,\ldots,n\}$. It is known that, up to logarithmic factors, $O(\sqrt{n})$ weights and neurons are sufficient for memorization of $n$ patterns \cite{vardi2021optimal}. 
It follows from the discussion in the paragraph above that $O(\sqrt{n})$ operations sufficient to recall a stored memory pattern (i.e. to calculate the output of the neural network for a given $x_i$). The goal of this paper is to accomplish the same task using much fewer operations required per input. Specifically, we will show how to do perfect recall using only $O(\log n)$ operations per input. We shall later show that $\Theta(\log n)$ is, in fact, the best possible rate.



\subsection{Conditional Networks}


In order to achieve perfect recall using a subpolynomial number of operations, we use the idea of conditional computation. The conditioning model that we utilize in this work is a simple but general scenario where we allow executing different sub-neural networks depending on how an intermediate output of the network compares to some real number. Each conditioning is thus counted as one operation. Formally, we describe a conditional neural network via a rooted full binary tree where every vertex has either $0$ or $2$ children. Every vertex  is a sequence of operations of the form $\mathtt{v}_{n+1} \leftarrow \phi(\beta_{n}\mathtt{v}_{n} + \cdots + \beta_{1} \mathtt{v}_1)$, where $\beta_1,\ldots,\beta_n$ are weights, and variables $\mathtt{v}_1,\ldots,\mathtt{v}_n$ are either (i) inputs to the neural network, or (ii) defined as new variables preceding $\mathtt{v}_{n+1}$ at the same vertex or at one of the ancestor vertices. Every edge is a conditioning $\mathtt{v} \circ \beta$, where $\mathtt{v}$ should be defined at any one of the vertices that connects the edge to the root node, $\beta$ is a constant weight, and $\circ\in\{\leq , <, =, \neq, >, \geq\}$. We assume that the two edges connected to the same vertex correspond to complementary conditions; e.g. $\mathtt{v}_1 < 3$ and $\mathtt{v}_1 \geq 3$.

An example conditional network, expressed in algorithmic form, is provided in Algorithm \ref{alg:cnnexample}, where $u$ represents the input, $o$ is the output, and the subscripts are the vector components. For example, if the input is a vector $u$ with $u_1 > 3$, resulting in an intermediate feature vector with $z_3 \neq 5$, then two operations are incurred due to the conditionings in Lines 1 and 5, and $O(\mathrm{dim}(W_1)+\mathrm{dim}(W_2)+\mathrm{dim}(W_5))$ operations are accumulated due to neural computations. 





%The sequence of operations for every vertex with two children ends with a conditioning statement of the form  Edges weighted as ``Y'' or ``N'' represent the two possible paths after the conditioning that the model takes. 

Our model encompasses various neural conditional computation models in the literature. One example is early exit architectures \cite{teerapittayanon2016branchynet, kaya2019shallow, gormez2022class}. As described in Section \ref{sec:intro}, a typical scenario is where one places intermediate \begin{wrapfigure}{r}{0.3\textwidth}
	\vspace{-16pt}
	\begin{minipage}{0.3\textwidth}
		\begin{algorithm}[H]
			\caption{An example conditional neural network}\label{alg:cnnexample}
			\begin{algorithmic}[1]
				\If{$u_1 > 3$} 
				\State  $z = \phi(W_2 \phi(W_1 u))$
				\If{$z_3 = 5$} 
				\State  $o = \phi(W_4 z)$
				\Else
				\State  $o = \phi(W_5 z)$
				\EndIf 
				\Else
				\State  $o = \phi(W_3 u)$
				\EndIf 
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}
	\vspace{-10pt}
\end{wrapfigure}classifiers to a deep  neural network. If an intermediate classifier is confident enough of a decision, then an ``early exit'' is performed with the corresponding class decision. Here, one skips subsequent layers, saving computation resources. The decision to exit is often a simple threshold check, e.g. whether one of the soft probability outputs of the intermediate classifier exceeds a certain threshold. Hence, most early exit networks can be modeled via the simple if-else architecture described above. Mixture of experts (MoE) architectures \cite{shazeer2017outrageously,fedus2022switch} can also be realized under our model. In this case, there are multiple gating networks, each of which is responsible for one expert. One routes a feature vector to only a subset of experts whose gating networks have the largest outputs. The choice of experts can be accomplished through if-else statements. For example, for two gates and experts, the gate with the largest output can be found by comparing the difference between the two gates' outputs against zero. Another gating approach that can be realized as a special case of our model can be found in \cite{cho2014exponentially}. 

\section{Synthesizing a Conditional Network out of an Unconditional Network}
\label{sec:synth}
%\subsection{The Synthesis Theorem}
Consider an arbitrary unconditional network as in (\ref{eq:systemmodel}), whose implementation requires $O(\sum_{\ell}\mathrm{dim}W_{\ell})$ operations, as discussed in Section \ref{sec:meascost}. Suppose that the network is well-trained in the sense that it can already provide the output $d_i$ for a given input $x_i$, for every $i\in\{1,\ldots,n\}$. Out of such an unconditional network, we describe here a general methodology to synthesize a conditional network that requires much fewer than $O(\sum_{\ell}\mathrm{dim}W_{\ell})$ operations, while keeping the input-output relationships $x_i \mapsto d_i,\,i\in\{1,\ldots,n\}$ intact. 

We first recall some standard terminology \cite{haykin}. Consider the neuron $[x_1,\ldots,x_n] \mapsto \phi(\sum_i x_i w_i)$. We refer to $x_1,\ldots,x_n$ as the neuron inputs, and $w_1,\ldots,w_n$ as the neuron weights. We can now provide the following definition.
\begin{definition}
Suppose that the activation function satisfies $\phi(0) = 0$. Given some fixed input to a neural network, a neuron with at least one non-zero input is called an active neuron. A neuron that is not active is called an inactive or a deactivated neuron. A weight is called active if it connects an active neuron to another active neuron and is non-zero. A weight is called inactive if it is not active. 
\end{definition}


The source of the phrase ``inactive'' is the following observation: Consider an input to the neural network and the corresponding output. By definition, we will obtain the same output after removing all inactive neurons from the network for the same input. We can simply ``ignore'' inactive neurons. Likewise, we can remove any inactive weight and obtain the same output.

Our idea is to condition the computation on the set of active neurons and the corresponding active weights. Note that removing the inactive weights and neurons does not change the network output. Moreover, often the number of active weights given an input can be significantly lower than the overall number of neurons of the unconditional architecture (which determines the number of operations required to perform inference on the unconditional network), resulting in huge computational savings. The complication in this context is that the set of inactive weights depends on the network input. Fortunately, it turns out that determining the set of active weights can be accomplished with fewer operations than actually computing the local fields or outputs of the corresponding active neurons. The final result is provided by the following theorem.

\begin{theorem}
	\label{th:synth}
Consider an arbitrary dataset $X = \{x_1,\ldots,x_n\}\subset\mathbb{R}^p$ of inputs. Let $d_1,\ldots,d_n\in\mathbb{R}^r$ be the corresponding desired outputs. Suppose that the unconditional neural network defined by (\ref{eq:systemmodel}) satisfies the desired input-output relationships in the sense that for any $i$, if the input to the network is $x_i$, then the output is $d_i$.  Also suppose that the input $x_i$ results in $\omega_i$ active weights. Then, there is a conditional network that similarly satisfies all desired input output relationships, and for every $i$, performs at most $p+4\omega_i$ operations given input $x_i$.
\end{theorem}
\begin{proof} (Sketch)
By an `` input configuration,'' we mean a certain subset of neurons that are active at a certain layer of the unconditional ``base'' network. We represent input configurations by binary vectors where ``$0$'' represents an inactive neuron, while ``$1$'' represents an active neuron. As an example, the input configuration $[1\,0\,1]^T$ refers to a scenario where only the first and the third neurons at the layer is active. Analogous to input configurations, ``output configurations'' are binary vectors that represent whether neurons provide zero or non-zero outputs. For example, an output configuration of $[1\,1\,0]^T$ means that only the first and the second neurons provide a non-zero output. 

Consider first an unconditional network without skip connections. The key idea is to observe that, given the output configuration of Layer $\ell-1$, one can uniquely obtain the input configuration of Layer $\ell$. Hence, a conditional network can be designed to operate in the following manner: Given an input, we first find the output configuration $\mathtt{OC}_0$ at Layer $0$, which is nothing but the non-zero components of the input vector. This can be done by $p$ conditioning statements, where $p$ is the input dimension. The result is a unique input configuration $\mathtt{IC}_1$ at Layer $1$. Meanwhile, we can obtain the output $y_1$ of Layer $1$ by only calculating the outputs of neurons that correspond to the non-zero components of $\mathtt{IC}_1$, since other neurons at Layer $1$ are guaranteed to have all-zero inputs and thus provide zero output. This can be accomplished via $O(a_1)$ multiplications and additions, where $a_{\ell}$ represents the number of active weights in Layer $\ell$. Then, we find the output configuration $\mathtt{OC}_1$ at Layer $1$, using $|\mathtt{IC}_1|$ conditioning statements on $y_1$. Having obtained $\mathtt{OC}_1$, we can similarly find the Layer $2$ input configuration and output. The conditional network processes the remaining layers recursively in the same manner. The functionality of the unconditional network is reproduced exactly so all input-output relationships are satisfied. Given that $\sum_{\ell} a_{\ell} = \omega_i$, the total complexity is $p+O(\omega_i)$. We refer to the complete proof in Appendix \ref{sec:syntthproof} for the precise bound and generalization to networks with skip connections.
\end{proof}


The proof of the theorem in Appendix \ref{sec:syntthproof} suggests that the true complexity is closer to $2 \omega_i$ than the actual formal upper bound provided in the theorem statement. The number $2 \omega_i$  stems from the $\omega_i$ additions and $\omega_i$ multiplications that are necessary to calculate the local fields of active neurons. 

Theorem \ref{th:synth} shows that a design criterion to come up with low computational-complexity neural networks might be to put a constraint on the number of active weights given any training input to an ordinary, unconditional feedforward architecture. Using the theorem, we can synthesize a conditional neural network with the same functionality as the unconditional feedforward network. In the next section, we will apply this idea to the problem of memorization capacity. 

We conclude this section by noting that Theorem \ref{th:synth} may prove to be useful in other applications. For example, any method that results in sparse representations or weights, such as pruning, will result in many inactive neurons and weights. Sparsity is useful; however, unstructured sparsity is difficult to exploit: For example, multiplying by a matrix half of whose entries are zero at random positions will likely be as difficult as multiplying by a random matrix without sparsity constraints. The construction in Theorem \ref{th:synth} may provide computational savings for such scenarios. 

Another observation is how Theorem  \ref{th:synth} can potentially simplify the design of conditional networks. In any conditional architecture, an important question is where to place the decision gates that route information to different parts of the network. It is a combinatorially very difficult if not impossible to optimize a heterogeneous collection of "ordinary neurons" and "gates." Such an optimization is also completely unsuitable for a gradient approach. Hence, most previous works provide some empirical guidelines for gate placement, and do not optimize over the gate locations once they are placed according to these guidelines. The message of Theorem \ref{th:synth} is that there is no need to distinguish between gates and neurons. All that needs to be done is to train an unconditional network consisting only of ordinary neurons to be as ``inactive'' as possible, e.g. by increased weight sparsity through regularization \cite{louizos2018learning}.  The theorem can then construct a conditional network that can exploit the sparsity to the fullest, placing the gates automatically.

