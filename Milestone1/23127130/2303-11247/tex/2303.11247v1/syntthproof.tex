We describe the proof of Theorem \ref{th:synth} in two major steps as follows. We will repeat some of the steps in the sketch of the proof for completeness.

{\bf Step-1: } By an `` input configuration,'' we mean a certain subset of neurons that are active at a certain layer of the unconditional ``base'' network. We represent input configurations by binary vectors where ``$0$'' represents an inactive neuron, while ``$1$'' represents an active neuron. As an example, the input configuration $[1\,0\,1]^T$ refers to a scenario where only the first and the third neurons at the layer is active. We follow the convention that all components of the input vectors are ``active'' nodes.

Analogous to input configurations, ``output configurations'' are binary vectors that represent whether neurons provide zero or non-zero outputs. For example, an output configuration of $[1\,1\,0]^T$ means that only the first and the second neurons provide a non-zero output. At Layer $0$, or the input layer, we follow the convention that the output configuration represents whether the components of the input are zero or non-zero. 

For our purposes, the significance of input and output configurations is the following observation: Suppose that, at Layer $\ell$ of an unconditional network, a particular input $y_0$ provides the input and output configurations $\mathtt{IC}_{\ell}$ and  $\mathtt{OC}_{\ell}$, respectively. In particular, we have $\mathtt{OC}_0 = \mathbf{1}(y_0 \neq 0)$. According to the neural network input-output relationship in (\ref{eq:systemmodel}), and the definition of input and output configurations, we have the identity 
\begin{align}
	\label{icellidentity}
	\mathtt{IC}_{\ell} = \mathbf{1}\left(\mathbf{1}(W_{\ell} \neq 0)[\mathtt{OC}_{\ell-1}^T \cdots \mathtt{OC}_{0}^T]^T \neq 0\right),
\end{align}
where $\mathbf{1}(\cdot)$ represents the indicator function, applied component-wise. In other words, knowing the output configurations of preceding Layers $0,\ldots,\ell-1$, we can uniquely determine the input configuration of Layer $\ell$. To see why (\ref{icellidentity}) holds, note that some Neuron $j$ at Layer $\ell$ is active (and thus $\mathtt{IC}_{\ell,j}=1$) if it admits at least one non-zero input from any one of the previous layers, multiplied by a corresponding non-zero weight in $W_{\ell}$. The indicator functions encode this criterion.

Exploiting the above observation that output configurations imply unique input configurations, the first step of the proof is thus to construct what we call a ``configuration tree.'' The configuration tree is a directed rooted tree, where nodes at Depth $\ell$ correspond to all possible sequences of output configurations from neural network Layers $0,\ldots,\ell-1$, which imply a unique input configuration at Layer $\ell$. We only need to consider the sequences induced by the training set. We represent vertices of the tree by the sequence of output configurations and the unique input configuration that is induced by them (The inclusion of the induced input configuration to the vertex information is thus superfluous and mainly to make the exposition clearer.). The only exception is the root of the tree, which is represented by an all-$1$ vector of dimension $p$. This corresponds to the unique input configuration of Layer $0$ (all components of the input are considered ``active.'').  Edges between vertices represent an output configuration that maps the sequence of output configurations to an input configuration.

%that there is only one input configuration at depth $0$, corresponding to the input layer of the neural network, This unique input configuration becomes the root node of the tree. 

We construct the configuration tree by showing the inputs to the neural network one by one.  We begin with the first element $x_1$ of the dataset. The corresponding input configuration $\mathtt{IC}_0 = [1 \cdots 1]^T$ at Layer $0$ is already available on the tree as the root node. As the next step, we determine the output configuration $\mathtt{OC}_0 = \mathbf{1}(x_1 \neq 0)$ at Layer $0$. Knowing the output configuration $\mathtt{OC}_0$, we can uniquely determine the input configuration $\mathtt{IC}_1 = \mathbf{1}(\mathbf{1}(W_1 \neq 0) \mathtt{OC}_0 \neq 0)$ at Layer $1$. We add $(\mathtt{OC}_0,\mathtt{IC}_1)$ as a new vertex to Depth $1$ of the tree. We create an edge with label $\mathtt{OC}_0$ to connect the root node $\mathtt{IC}_0$ to  $(\mathtt{OC}_0,\mathtt{IC}_1)$. We continue by finding the output configuration at Layer $1$. Knowing the output configurations $\mathtt{OC}_1$ and $\mathtt{OC}_0$, we can uniquely determine the input configuration $\mathtt{IC}_2$ at Layer $2$, which we add as the vertex  $(\mathtt{OC}_1,\mathtt{OC}_0,\mathtt{IC}_2)$ to Depth $2$. We create an edge with label $\mathtt{OC}_1$ to connect $(\mathtt{OC}_0,\mathtt{IC}_1)$ to  $(\mathtt{OC}_1,\mathtt{OC}_0,\mathtt{IC}_2)$. 
We continue in the same manner for all subsequent layers, and then for all samples in the dataset. 

The formal construction of the configuration tree is provided in Algorithm \ref{alg:configtreeconstruction}.  Edges are defined as triplets, where the first two components are the endpoints, and the last component is the edge label. 

%The symbol $\mathtt{IC}$ stands for ``input configuration,'' and $\mathtt{OC}$ represents an output configuration.

\begin{algorithm}[H]
	\caption{An algorithm to construct the configuration tree}\label{alg:configtreeconstruction}
	\begin{algorithmic}[1]
		\State $\mathcal{V} \leftarrow \{[1\cdots1]^T\}$. \Comment{\parbox[t]{.5\linewidth}{Initialize the tree with a root node but no edges.}}
		\State $\mathcal{E} \leftarrow \emptyset$.
		\For{$i=1$ to $n$}  
		\State $y_0 \leftarrow x_i$ \Comment{\parbox[t]{.5\linewidth}{A dataset input to the neural network.}}
		\State $\overline{\mathtt{OC}}_{-1} = \emptyset$  \Comment{\parbox[t]{.5\linewidth}{A convention specific to this algorithm.}}
		\State $\mathtt{IC}_0 \leftarrow [1\cdots 1]^T$. \Comment{\parbox[t]{.5\linewidth}{All input components are active by convention.}}
		%		\State $\mathtt{OC}_0 \leftarrow \mathbf{1}(y_0 \neq 0)$.  \Comment{\parbox[t]{.65\linewidth}{The ``output configuration'' at Layer $0$ is the non-zero components of the ``output'' of the network at Layer $0$, which is the network input by convention.}}
		\For{$\ell=1$ to $L$} 
		\State $\overline{y}_{\ell-1} \leftarrow [y_{\ell-1}^T  \cdots  y_0^T]^T$. \Comment{\parbox[t]{.5\linewidth}{Outputs of all Layers $< \ell$ are inputs to Layer $\ell$.} }
		\State $\overline{\mathtt{OC}}_{\ell-1} \leftarrow \mathbf{1}(\overline{y}_{\ell-1}  \neq 0)$. \Comment{\parbox[t]{.5\linewidth}{Output configurations of all inputs to Layer $\ell$.} }
		\State $\mathtt{IC}_{\ell} \leftarrow \mathbf{1}(\mathbf{1}(W_{\ell} \neq 0)\overline{\mathtt{OC}}_{\ell-1}\neq 0)$.  \Comment{\parbox[t]{.5\linewidth}{Same formula as (\ref{icellidentity}).}}
		\State $\mathcal{V} \leftarrow \mathcal{V} \cup \{(\overline{\mathtt{OC}}_{\ell-1}, \mathtt{IC}_{\ell})\}$.
		\State $\mathcal{E} \leftarrow \mathcal{E} \cup \{(\overline{\mathtt{OC}}_{\ell-2}, \mathtt{IC}_{\ell-1}),(\overline{\mathtt{OC}}_{\ell-1}, \mathtt{IC}_{\ell}),\mathtt{OC}_{\ell-1})\}$.
		
		\State $y_{\ell} \leftarrow \phi(W_{\ell}\overline{y})$.
		\State $\mathtt{OC}_{\ell} \leftarrow \mathbf{1}(y_{\ell} \neq 0)$. %\Comment{\parbox[t]{.65\linewidth}{The ``output configuration'' at Layer $\ell$ is the non-zero components of the output of the network at Layer $\ell$.}}
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


\begin{figure}[h]
	\centerline{\scalebox{1.0}{\begin{tikzpicture}[
				arrowstyle/.style={decoration={markings,mark=at position 1 with
						{\arrow[scale=2,>=stealth]{>}}},postaction={decorate}},
				roundnode/.style={circle, draw=blue!60, fill=blue!5, very thick, minimum size=2mm},
				squarednode/.style={rectangle, draw=green!60, fill=yellow!5, very thick, minimum size=5mm},]
				\node[squarednode] (L01) at (0,0) {$x_i$};
				\node[roundnode] (L11) at (2,0){};
				\node[roundnode] (L12) at (2,1){};
				\node[roundnode] (L13) at (2,2){};
				\node[roundnode] (L21) at (4,0){};
				\node[roundnode] (L22) at (4,1){};
				\node[roundnode] (L23) at (4,2){};
				\node[roundnode] (L31) at (6,0){};
				\node[roundnode] (L32) at (6,1){};
				
				
				\draw [arrowstyle](L01)--(L11);
				\draw [arrowstyle](L01)--(L12);
				\draw [arrowstyle](L01)--(L13);
				\draw [arrowstyle](L13)--(L22);
				\draw [arrowstyle](L13)--(L23);
				\draw [arrowstyle](L12)--(L21);
				\draw [arrowstyle](L11)--(L22);
				
				\draw [arrowstyle](L12)--(L31);
				
				
				\draw [arrowstyle](L23)--(L32);
				\draw [arrowstyle](L22)--(L32);  
				\draw [arrowstyle](L22)--(L31);
				\draw [arrowstyle](L21)--(L31);  
	\end{tikzpicture}}}
	\caption{An example network for demonstrating the construction of the configuration tree.}
	\label{fig:neuralnetworkforconfigtreeexample}
\end{figure}

\begin{figure}\centerline{\scalebox{0.9}{
			\begin{tikzpicture}
				[
				grow                    = right,
				sibling distance        = 2.3em,
				level distance          = 10em,
				edge from parent/.style = {draw, -latex},
				every node/.style       = {font=\footnotesize},
				sloped
				]
				\node [env] {1} 
				child { node [env] {1,111} 
					child { node [env] {101,1,110} 
						child { node [env] {100,101,1,10} 
							edge from parent node [above] {100} }
						edge from parent node [above] {101} }
					child { node [env2] {001,1,010} 
						child { node [env2] {010,001,1,11} 
							edge from parent node [above] {010} }
						edge from parent node [above] {001} }
					edge from parent node [above] {1} 
				};
	\end{tikzpicture}}}
	\caption{One configuration tree corresponding to the network in Fig. \ref{fig:neuralnetworkforconfigtreeexample}.}
	\label{fig:configtreeexample}\vspace{-5pt}
\end{figure} 


\begin{example}
	As an example of the construction of the configuration tree, consider the network in Fig. \ref{fig:neuralnetworkforconfigtreeexample} with one dimensional inputs and two dimensional outputs. We consider a dataset with two samples $n=2$. Suppose that the first input $x_1 \neq 0$ provides the input configurations $[1\,1\,1]$, $[1\,1\,0]$, $[1\,0]$, and output configurations $[1\,0\,1]$, $[1\,0\,0]$, $[1\,0]$, at Layers $1$, $2$, $3$, respectively. Then, after processing $x_1$, the tree will consist of all blue/white nodes in Fig. \ref{fig:configtreeexample} and the edges between them. Note that the last components of all vertices represent the input configurations, while edges represent the output configurations. Hence, the node $\mathtt{101,1,110}$ at Depth $2$ of the tree represents the scenario where only the first and the second neurons in the second layer are active (have at least one non-zero input). Since there are $2$ active neurons, there are $4$ possible output configurations: $\mathtt{000},\mathtt{010},\mathtt{011}$, and $\mathtt{110}$. In particular, the output configuration $\mathtt{010}$ implies an input configuration of $\mathtt{11}$ at Depth $3$ (Layer $3$). This is because, Node $2$ of Layer $2$ is connected to both neurons at Layer $3$, and a non-zero output of this neuron implies a non-zero input to Layer $3$ neurons. Now, suppose the second input $x_2 \neq 0$ provides the input configurations $[1\,1\,1]$, $[0\,1\,0]$, $[1\,1]$, and output configurations $[0\,0\,1]$, $[0\,1\,0]$, $[1\,1]$, at Layers $1$, $2$, $3$, respectively. Then, the construction of the tree is completed as shown in Fig. \ref{fig:configtreeexample}.\hfill\ensuremath{\qedsymbol}
\end{example}


{\bf Step-2:} We now construct the conditional network itself out of the configuration tree. In fact, the conditional network will follow the same structure as the configuration tree, traversing from the root of the tree to one of the leaves as we calculate the output for a given input to the neural network.  As in the case of the configuration tree, we calculate the layer outputs sequentially one at a time. First, we find the output of Layer $1$. Given any input $y_{0}$ to the network, the input configuration $\mathtt{IC}_0$ at Layer $0$ is the all-one vector of dimension $p$. The conditional network first compares against all \begin{wrapfigure}{r}{0.25\textwidth}
	\vspace{-10pt}
	\begin{minipage}{0.25\textwidth}
		\begin{algorithm}[H]
			\caption{Two nested binary conditions}\label{alg:nestedconditionings}
			\begin{algorithmic}[1]
				\If{$u_1 = 0 $} 
				\If{$u_2 = 0 $} 
				\State  ...
				\Else
				\State  ...
				\EndIf 
				\Else
				\If{$u_2 = 0$} 
				\State  ...
				\Else
				\State  ...
				\EndIf 
				\EndIf 
			\end{algorithmic}
		\end{algorithm}
	\end{minipage}\vspace{-10pt}
\end{wrapfigure}possible output configurations that are found at the configuration tree at Layer $0$, which correspond to edges connected to the root node. Since the input dimension is $p$, this can be done through $p$ nested binary conditions. Moreover, $p$ operations are sufficient to reach the ``leaves'' of $p$ nested conditions. An example of this step is provided in Algorithm \ref{alg:nestedconditionings} for a two-dimensional input $[u_1 u_2]^T$. In general, at each of the $2^p$ leaves of nested conditions, we uniquely know the output configuration $\mathtt{OC}_0$ at Layer $0$, as well as the input configuration $\mathtt{IC}_1$ at Layer $1$ via the configuration tree. At Layer $1$, we only need to calculate the outputs of the neurons indicated by the input configuration $\mathtt{IC}_1$, since other neurons are guaranteed to have all-zero inputs and thus provide zero output.
This yields the output $y_1$ of the entire Layer $1$. At this stage, we have obtained the input $y_1$ to the second layer (possibly in addition to $y_0$, the skip connections from inputs, that we already know), and  the input configuration $\mathtt{IC}_1$ at Layer $1$. We proceed recursively in the same manner described above: That is to say, $|\mathtt{IC}_1|$ nested binary conditions are utilized to determine the output configuration $\mathtt{OC}_1$. Together with $\mathtt{OC}_0$, this yields the input configuration $\mathtt{IC}_2$ at Layer $2$ as well as the Layer $2$ outputs $y_2$. Outputs for all subsequent layers are determined in a similar fashion.


\begin{algorithm}[H]
	\caption{Construction of the conditional network out of the configuration tree. In the algorithm,  $x$ is the input to the neural network. Outputs are provided in the variable $y_L$.}\label{alg:condoutofconf}
	\begin{algorithmic}[1]
		\State Replace the root node with \colorbox{black!10}{$y_0 \leftarrow x$}.
		\State Replace any edge of the form $((\overline{\mathtt{OC}}_{\ell-2}, \mathtt{IC}_{\ell-1}),(\overline{\mathtt{OC}}_{\ell-1}, \mathtt{IC}_{\ell}),\mathtt{OC}_{\ell-1})$ with the conditioning statement \begin{align}\label{nestcondsalgo} \colorbox{black!10}{If $\mathbf{1}(y_{\ell-1}\llbracket \mathtt{IC}_{\ell-1} \rrbracket \neq 0) = \mathtt{OC}_{\ell-1}\llbracket \mathtt{IC}_{\ell-1} \rrbracket$, then}\end{align}
		\State Replace any vertex of the form  $(\overline{\mathtt{OC}}_{\ell-1}, \mathtt{IC}_{\ell})$ with  
		\begin{align}\label{nestcondsalgox}\colorbox{black!10}{$y_{\ell}\llbracket \mathtt{IC}_{\ell} \rrbracket = \phi(W_{\ell} \llbracket \mathtt{IC}_{\ell} , \overline{\mathtt{OC}}_{\ell-1} \rrbracket  \overline{y}_{\ell-1} \llbracket \overline{\mathtt{OC}}_{\ell-1}\rrbracket )$},\end{align} where $\overline{y}_{\ell-1} \triangleq [y_{\ell-1}^T  \cdots  y_0^T]^T$.
	\end{algorithmic}
\end{algorithm}

The construction of the conditional network is formally stated in Algorithm \ref{alg:condoutofconf}. In the description of the algorithm, for any vector $a = [a_1 \cdots a_n]$, and binary vector $b = [b_1 \cdots b_n] \in\{0,1\}^n$, we use the notation that $a \llbracket b \rrbracket$ to represent the $\|b\|_1$-dimensional vector consisting only of components $a_i$ such that $b_i = 1$. Here, $\|\cdot\|_1$ is the $1$-norm, counting the number of non-zero components for the case of a binary vector. Likewise, for a matrix $W$, the notation $W\llbracket b_1,b_2 \rrbracket$ represents the $ \|b_1\|_1 \times \|b_2\|_1$ matrix where we only consider those rows and columns of $W$ as indicated by the non-zero components of $b_1$ and $b_2$, respectively. For example, $[6,9,7]\llbracket 1,0,1 \rrbracket = [6,7]$, and $[\begin{smallmatrix} 3 & 6 & 8 \\ 1 & 4 & 7 \end{smallmatrix}]\llbracket [0,1],[0,1,1] \rrbracket = [4,7]$. Note that the conditioning statements in Line 2 are actually implemented via nested binary conditions as described in Algorithm \ref{alg:nestedconditionings}. We have presented the nested conditionings in their compact form shown in Line 2 for a clearer exposition.

% of Algorithm \ref{alg:condoutofconf}.

Let us now show that the conditional network as constructed in Algorithm  \ref{alg:condoutofconf} satisfies the statement of the theorem. In particular, it provides the same output as the unconditional network over the dataset. This can be shown by induction. Suppose the network manages to correctly calculate all outputs and input/output configurations up to and including Layer $\ell-1$ for a certain input with the exception of $\mathtt{OC}_{\ell-1}$, which is yet to be determined. At the conditional network graph, we are then ``at the end of'' vertex $(\overline{\mathtt{OC}}_{\ell-2}, \mathtt{IC}_{\ell-1})$, having just executed (\ref{nestcondsalgo}) for $\ell\leftarrow\ell-1$. A combination of (\ref{nestcondsalgo}) and (\ref{nestcondsalgox}) will now determine $\mathtt{OC}_{\ell-1}$, $\mathtt{IC}_{\ell}$, and $y_{\ell}$. Suppose that the true values for these variables are $\mathbf{O}$, $\mathbf{I}$, and $\mathbf{y}$, respectively. We will thus show that $\mathtt{OC}_{\ell-1} = \mathbf{O}$, $\mathtt{IC}_{\ell} = \mathbf{I}$ and $y_{\ell} = \mathbf{y}$. We first note that in (\ref{nestcondsalgo}), we will have an edge with $\mathtt{OC}_{\ell-1} = \mathbf{O}$. This follows as the conditional network is constructed out of the configuration tree, which itself is constructed from the dataset. Since $\mathbf{O}$ exists on the configuration tree, so it should on the conditional network tree. The conditional network then transitions to (\ref{nestcondsalgox})  with $\mathtt{OC}_{\ell-1} = \mathbf{O}$ and $\mathtt{IC}_{\ell} = \mathbf{I}$. Here, the input configuration is correctly calculated as it is unique given previous output configurations. It follows that (\ref{nestcondsalgox}) is calculated correctly so that $y_{\ell}= \mathbf{y}$.

We now evaluate the computational complexity. Note that,  Line 2 of Algorithm \ref{alg:condoutofconf} can be implemented via $|\mathtt{IC}_{\ell-1}|_1$ nested binary conditions, requiring $|\mathtt{IC}_{\ell-1}|_1$ operations. We now evaluate the cost of Line 3, where only the outputs of active nodes of Layer $\ell$ are calculated. Let $a_{\ell}$ denote the number of active nodes at Layer $\ell$. Line 3 performs at most $a_{\ell}$ multiplications and $a_{\ell}$ additions to calculate the local fields of these active neurons. Calculating the active neuron outputs through ReLU activation functions require a further $|\mathtt{IC}_{\ell}|_1$ comparisons or operations. Hence, Line $3$ requires $2a_{\ell} + |\mathtt{IC}_{\ell}|_1$ operations at most. Traversing from the root to a leaf of the computation graph, the total number of operations is at most $p+2\sum_{\ell=1}^L (a_{\ell} + |\mathtt{IC}_{\ell}|_1)$. Since, at each layer, there should be as many active neurons as there are active weights, we have $|\mathtt{IC}_{\ell}|_1 \leq a_{\ell}$. Substituting this estimate to the previous bound, we obtain the same upper bound in the statement of the theorem. This concludes the proof of Theorem \ref{th:synth}.

One aspect of the conditional network, as constructed in Algorithm \ref{alg:condoutofconf} is that certain inputs that do not belong to the dataset may end up in conditions for which no output is defined. This occurs when one of the bins or vertices induced by the nested binary conditions in Line 2 (and as exemplified in Algorithm \ref{alg:nestedconditionings}) remains empty.  Since the main focus of this work is memorization of a dataset, we are not concerned with the network operation for such inputs. Nevertheless, the network operation can easily be generalized so that the output is well-defined for any input, e.g. simply by removing the binary condition with one or two empty bins, and reconnecting any children of the removed condition to the condition's parent.
