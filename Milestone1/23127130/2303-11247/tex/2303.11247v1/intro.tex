\section{Introduction}
\label{sec:intro}
\subsection{Conditional Computation}
Conditional computation refers to utilizing only certain parts of a neural network, in an input-adaptive fashion \cite{davis2013low,bengio2013estimating,eigen2013learning}. This can be done through gating mechanisms combined with a tree-structured network, as in the case of ``conditional networks'' \cite{ioannou2016decision} or neural trees and forests \cite{tanno2019adaptive,yang2018deep,kontschieder2015deep}. Specifically, depending on the inputs or some features extracted from the inputs, a gate can choose its output sub-neural networks that will further process the gate's input features. Another family of conditional computation methods are the so-called early-exit architectures \cite{teerapittayanon2016branchynet, kaya2019shallow, gormez2022early}. In this case, one typically places classifiers at intermediate layers of a large network. This makes it possible to exit at a certain layer to reach a final verdict on classification, if the corresponding classifier is confident enough of its decision. 

Several other sub-techniques of conditional computation exist and have been well-studied, including layer skipping \cite{graves2016adaptive}, channel skipping in convolutional neural networks \cite{gao2018dynamic}, or reinforcement learning methods for input-dependent dropout policies \cite{bengio2015conditional}. Although there are many diverse methods \cite{han2021dynamic}, the general intuitions as to why conditional computation can improve the performance of neural networks remain the same: First, the computation units are chosen in an adaptive manner to process the features that are particular to the given input pattern. For example, a cat image is ideally processed by only ``neurons that are specialized to cats.'' Second, one allocates just enough computation units to a given input, avoiding a waste of resources. The end result is various benefits relative to a network without conditional computation, including reduced computation time, and power/energy consumption \cite{kim2020energy}. Achieving these benefits are especially critical in edge networks with resource-limited devices \cite{koyuncu:c26, koyuncu:c27}. Moreover, conditioning incurs minimal loss, or in some cases, no loss in learning performance. 



%Moreover, these benefits arrive with minimal loss, or in some cases, without any loss in learning performance. 

Numerous empirical studies have demonstrated the benefits of conditional computation in many different settings.\ Understanding the fundamental limits of conditional computation in neural networks is thus crucial, but has not been well-investigated in the literature. There is a wide body of work on a theoretical analysis of decision tree learning \cite{maimon2014data}, which can be considered as an instance of conditional computation. These results are, however, not directly applicable to neural networks.  
In \cite{cho2014exponentially}, a feature vector is multiplied by different weight matrices, depending on the significant bits of the feature vector, resulting in an exponential increase in the number of free parameters of the network (referred to as the capacity of the network in \cite{cho2014exponentially}). On the other hand, the potential benefits of this scheme have not been formally analyzed.


\subsection{Memorization Capacity}

In this work, we consider the problem of neural conditional computation from the perspective of memorization capacity.  Here, the capacity refers to the maximum number of input-output pairs of reasonably-general position that a neural network of a given size can learn. It is typically expressed as the minimum number of neurons or weights required for a given dataset of size, say $n$.

Early work on memorization capacity of neural networks include \cite{baum1988capabilities, mitchison1989bounds, sontag1990remarks}. In particular, \cite{baum1988capabilities} shows that, for thresholds networks, $O(n)$ neurons and weights are sufficient for memorization. This sufficiency result is later improved to $O(\sqrt{n})$ neurons and $O(n)$ weights by \cite{vershynin2020memory, rajput2021exponential}. There are also several studies on other activation functions, especially the Rectified Linear Unit (ReLU), given its practicality and wide utilization in deep learning applications. Initial works \cite{zhang2021understanding, hardt2016identity} show that $O(n)$ neurons and weights are sufficient for memorization in the case of ReLU networks. This is improved to $O(\sqrt{n})$ weights and $O(n)$ neurons in \cite{yun2019small}. In addition, \cite{park2021provable} proves the sufficiency of $\smash{O(n^{2/3})}$  weights and neurons, and finally, \cite{vardi2021optimal} shows that memorization can be achieved with only $O(\sqrt{n})$ weights and neurons, up to logarithmic factors.
For the sigmoid activation function, it is known that $O(\sqrt{n})$ neurons and $O(n)$ weights \cite{huang2003learning}, or $\smash{O(n^{2/3})}$ weights and neurons \cite{park2021provable} are sufficient. Memorization and expressivity have also been studied in the context of specific network architectures such as convolutional neural networks \cite{cohen2016convolutional,nguyen2018optimization}.

The aforementioned achievability results have also been proven to be tight in certain cases. A very useful tool in this context is the Vapnik-Chervonenkis (VC) dimension \cite{vapnik2015uniform}. In fact, applying the VC dimension theory to neural networks  \cite{anthony1999neural}, it can be shown that the number of neurons and weights should be both polynomial in the size of the dataset for successful memorization. Specifically, $\Omega(\sqrt{n})$ weights and $\Omega(n^{1/4})$ neurons are optimal for ReLU networks, up to logarithmic factors. We will justify this statement later on for completeness.




%$O(\sqrt{n})$ neurons and $O(n)$ weights are known to be the best possible for threshold networks, while 
\subsection{Scope, Main Results, and Organization}


We analyze the memorization capacity of neural networks with conditional computation. We describe our neural network and the associated computational complexity models in Section \ref{sec:model}. We describe a general method to synthesize conditional networks from unconditional networks in Section \ref{sec:synth}. We provide our main achievability and converse results for memorization capacity in Sections \ref{sec:reluach} and \ref{sec:reluconv}, respectively. We draw our main conclusions in Section \ref{sec:conclusions}. Some of the technical proofs are provided in the supplemental material. 

We note that this paper is specifically on analyzing the theoretical limits on neural conditional computation. In particular, we show that $n$ input-output relationships can be memorized using a conditional network that needs only $O(\log n)$ operations per input or inference step. The best unconditional architecture requires $O(\sqrt{n})$ operations for the same task. This suggests that conditional models can offer significant time/energy savings as compared to unconditional architectures. In general, understanding the memorization capacity of neural networks is a well-studied problem of fundamental importance and is related to the expressive power of neural networks. A related but separate problem is generalization, i.e. how to design conditional networks that can not only recall the memory patterns with reasonable accuracy but also generalize to unseen examples. The ``double-descent'' phenomenon \cite{belkin2019reconciling, nakkiran2021deep} suggests that the goals of memorization and generalization are not contradictory and that a memorizing network can potentially also generalize well. A further investigation of this phenomenon in the context of conditional networks, and the design of conditional networks for practical datasets remains beyond the scope of the present work.

%We extend our results to threshold networks in Section \ref{sec:threshold}. 

{\bf Notation: }
Unless specified otherwise, all vector variables are column vectors. We use ordinary font (such as $u$) for vectors, and the distinction between a vector and scalar will be clear from the context. The symbols $O,\Omega$, and $\Theta$ are the standard  Bachmannâ€“Landau symbols. Specifically, $f_n \in O(g_n)$ means there is a constant $C>0$ such that $f_n \leq C g_n$ for sufficiently large $n$. On the other hand, if $f_n \in \Omega(g_n)$, then there is a constant $C>0$ such that $f_n > C g_n$ for sufficiently large $n$. We write $ f_n \in \Theta(g_n)$ if $f_n \in O(g_n)$ and $f_n \in \Omega(g_n)$. The set $\mathbb{R}^p$ is the set of all $p$-dimensional real-valued column vectors. The superscript $(\cdot)^T$ is the matrix transpose. The function $\mathbf{1}(\cdot)$ is the indicator function, and $\lceil \cdot \rceil$ is the ceiling operator. A function $f(x)$ of variable $x$ is alternatively expressed as the mapping $x\mapsto f(x)$. Operator $\mathrm{rank}(\cdot)$ is the rank of a matrix. Finally, $\|\cdot\|$ is the Euclidean norm.

