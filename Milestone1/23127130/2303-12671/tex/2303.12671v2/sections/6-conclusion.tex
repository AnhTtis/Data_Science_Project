\section{CONCLUSION}
\label{conclusion}
We have used the Convolutional Sequence-to-sequence network combined with the ViT and OFA model for our proposed system in the VLSP-EVJVQA task. The final results are 0.3390 on the public test set and 0.4210 on the private test set by the F1 score. From the result, we placed the $3^{rd}$ rank in the competition. Through errors analysis, various errors have been found in the output answer, which are our limitations in this study. In summary, there are factors that have significant impact on our solution for the multilingual VQA task: the diversity of each language, the translation performance, the effects of vision and language models and the generation capability of the core Seq2Seq model.

Our future research for this task is to improve the accuracy of the model in giving the correct answer by enriching the features from images and questions. Other SOTA vision-language and image models such as BEiT, DeiT and CLIP can be applied to assess the performance on UIT-EVJVQA dataset. Besides, from the proposed system, we will implement an intelligence chat-bot application for question-answering from images. 

