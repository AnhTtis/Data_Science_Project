\section{THE PROPOSED METHOD}
\label{method}
Figure \ref{fig_method} depicts an overview of the proposed approach in this study. In general, we transform the VQA problem into a sequence-to-sequence learning task, in which we take advantage of State-of-the-art (SOTA) vision-language models to offer richer information about the question-image dependencies in the input sequence.
 The method consists of two main phases that are carried out sequentially. At the first stage, numerous hints are extracted from question-image pairs using pre-trained vision-language models. The extracted hints are then concatenated with the question and visual features to form a sequence representation as input to the proposed Seq2Seq model to generate the corresponding answers in free-form natural language. Our source code and demo for the proposed methodology are available at this link: \url{https://huggingface.co/spaces/daeron/CONVS2S-EVJVQA-DEMO}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figure/proposed_method_v6.pdf}
\caption{An overview of the proposed method for visual question answering on UIT-EVJVQA dataset}
\label{fig_method}
\end{figure}

\subsection{Hints extraction with pre-trained vision-language models}

This phase concentrates on implementing SOTA vision-language models, including OFA \citep{wang2022ofa} and ViLT \citep{pmlr-v139-kim21k} to predict the possible answers given a question and its corresponding image. Due to the diverse nature of the questions and the multilingual aspect of UIT-EVJVQA, these models are only set up to provide answers directly through zero-shot prediction, with no training or fine-tuning step on the dataset. These SOTA models, which were pre-trained and fine-tuned on various datasets (VQAv2, VG-QA, and GQA), mainly support English but do not yet support Vietnamese or Japanese. To achieve desired results, we translate the Vietnamese and Japanese questions from UIT-EVJVQA into English using Google Translate API\footnote{\url{https://cloud.google.com/translate}} before feeding them into the models to get inferences. Once the output answers are generated, they are translated back into the original languages for evaluation and experiments in the second phase. For ViLT, we choose up to five candidate answers with the highest probability for further experiments. Using more hints is feasible, but it will put more pressure on computational resources as we approach creating long sequences based on hints probability in the next phase. We concatenate each output answer from ViLT along the sequence, respectively with decreasing relevance, to assess their quality on the new dataset. The inference performance of pre-trained ViLT and OFA models on the public test set are shown in Table \ref{vqa_example}.

Under our expectations, the OFA model with unified Seq2Seq structure outperforms ViLT with F1 0.1902, while ViLT achieves the best performance with F1 0.1317 using 2 keyword answers. The evaluation results are not quite good compared to the ground truth because of the special characteristic of the dataset with long answers and, since no training has been done, the predicted answers lack sufficient vocabulary. The predicted answer may match or not match the ground truth completely but gives a similar and proper response to the question. Regardless of accuracy, these simple keyword answers provide valuable insights about question-image interactions. Due to this, we consider these answers as hints or suggestions for each question-image pair and apply them to the training of the main model in the following phase.

\begin{table}
\centering
%\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
\toprule
\textbf{Model}                & \textbf{\# hints} & \textbf{F1}      \\ 
\midrule
\multirow{5}{*}{ViLT} & 1       & 0.1303  \\
                      & 2       & 0.1317  \\
                      & 3       & 0.1315  \\
                      & 4       & 0.1290  \\
                      & 5       & 0.1252  \\ 
\midrule
OFA                 & -      & 0.1902  \\
\bottomrule
\end{tabular}
\caption{Performance of SOTA vision-language models on public test set}
\label{vqa_example}
\end{table}

\subsection{Experiment with Convolutional Sequence-to-sequence Network}
The second phase of the approach concentrates on developing and training the main model for this challenge: the Convolutional Sequence-to-sequence Network (ConvS2S) \citep{10.5555/3305381.3305510} with different combinations of textual and image features for visual question answering task. ConvS2S has significant capabilities to accelerate training progress and reduce our computational resource limitations due to its efficiency in terms of GPU hardware optimization and parallel computation. This is why the architecture is preferred over other Seq2Seq models for the competition.

In this study, each convolutional layer of ConvS2S uses many filters with a width of 3. Each filter will slide across the sequence, from beginning to end, looking at all 3 consecutive elements at a time to learn to extract a different feature from the questions, hints, visual factors and answers. With these special settings, the model has a significant capacity to extract meaningful features from the input sequence and generate free-form content. Due to its proven performance in other Seq2Seq learning tasks, such as machine translation, we anticipate the model to perform well on question-image features combination and produce good results on the visual question-answering task.


% Different from other Seq2Seq models such as bi-directional recurrent neural networks (RNN) with soft-attention mechanism \citep{bahdanau2016neural, luong-etal-2015-effective} or Transformer \citep{NIPS2017_3f5ee243} with self-attention, this Encoder-Decoder architecture based entirely on convolutional neural networks and originally used for machine translation task. ConvS2S makes use of many convolutional layers typically applied in image processing. In this study, each layer uses many filters with a width of 3. Each filter will slide across the sequence, from beginning to end, looking at all 3 consecutive tokens at a time to learn to extract a different feature from the sequence. Multi-step attention is also the key component of the architecture that allows the model to make multiple glimpses across the sequence to produce better output. With these special settings, the model has a significant capacity to extract meaningful features from the source sequence and generate free-form content. Due to its proven capabilities in other Seq2Seq learning tasks, such as machine translation, we anticipate the model to perform well on question-image features combination and produce good results on the visual question-answering task.

\subsection{Textual and visual features combination}
In the early stage, a set of various useful hints is achieved using pre-trained ViLT and OFA. In order to train the proposed Seq2Seq model with the existing materials, the textual features, including questions and hints, and image features have to be combined in the form of sequence representations as input to the Seq2Seq model.

As shown in Table \ref{vqa_example}, adding more ViLT hints to the sequence tend to reduce the F1 score performance. These simple answers, on the other hand, may passively contribute to the overall understanding of the scenario of the corresponding images. Therefore, our approach focuses on using hint probability to generate sequences with repeated keywords while avoiding noise from outliers. This method allows hints that have a higher probability to appear more frequently in the sequence. For efficiency and reducing cost, the number of times a ViLT hint occurs in the sequence is the integral part of its half probability. For experiments involving the output of two models, the hint from the OFA model is set to appear 10 times in the sequence. The newly created sequence is concatenated with the question to form the final sequence for question and hint. We then remove special characters, lowercasing, and tokenize the text contents before passing them into the encoder. For English content, we tokenize the text simply by splitting them word-by-word. For Vietnamese and Japanese content, Underthesea toolkit and Trankit \citep{nguyen2021trankit} libraries are applied for word segmentation, respectively. Figure \ref{fig:combine} illustrates an example of a question and hints combination in our approach.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{figure/combined.pdf}
\caption{An example of question and hints combination. The hint `tree' occurs 11 times in the sequence since the half of its probality is 11.19 (\%). }
\label{fig:combine}
\end{figure}



% \begin{table}[!htbp]
% \small
% \centering
% \renewcommand{\arraystretch}{1.3}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{p{\columnwidth}}
% \toprule
% \textbf{\underline{Question:}} \textcolor[HTML]{3487E1}{ what are put on the table next to the woman in red shirt?}\\ \textbf{\underline{ViLT hints:}}  \textcolor[HTML]{C40000}{\{`fruit': 0.1568\}}, \textcolor[HTML]{EB2701}{\{`bananas': 0.1338\}}, \textcolor[HTML]{F55E01}{\{`food': 0.0899\}}, \textcolor[HTML]{E9791A}{\{`berries': 0.0786\}}, \textcolor[HTML]{F19A11}{\{`carrots': 0.0655\}}\\ \textbf{\underline{OFA hints:}} \textcolor[HTML]{47B0AA}{pineapples} \\\hline%\cdashlinelr{2-3}
% \textbf{\underline{Combined sequence:}}
% \textless{}sos\textgreater{}\textcolor[HTML]{3487E1}{what are put on the table next to the woman in red shirt}\textless{}sep\textgreater{}\textcolor[HTML]{C40000}{fruit fruit fruit fruit fruit fruit fruit} \textcolor[HTML]{EB2701}{bananas bananas bananas bananas bananas bananas} \textcolor[HTML]{F55E01}{food food food food} \textcolor[HTML]{E9791A}{berries berries berries} \textcolor[HTML]{F19A11}{carrots carrots carrots}\textless{}sep\textgreater{}\textcolor[HTML]{47B0AA}{pineapples pineapples pineapples pineapples pineapples pineapples pineapples pineapples pineapples pineapples}\textless{}eos\textgreater{}
% \\\bottomrule
% \end{tabular}
% }
% \caption{An example of question and hints combination. The hint `fruit' occurs 7 times in the sequence since the half of its percentage is 7.84 (\%). }
% \label{hint_question}
% \end{table}


% \begin{table}[ht]
% \small
% \renewcommand{\arraystretch}{1.2}
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{|p{0.1\columnwidth}p{0.9\columnwidth}|}
% \hline
% Question &   \textcolor[HTML]{3487E1}{ what are put on the table next to the woman in red shirt?}\\ ViLT hints &  \textcolor[HTML]{C40000}{\{'fruit': 0.1568\}}, \textcolor[HTML]{EB2701}{\{'bananas': 0.1338\}}, \textcolor[HTML]{F55E01}{\{'food': 0.0899\}}, \textcolor[HTML]{E9791A}{\{'berries': 0.0786\}}, \textcolor[HTML]{F19A11}{\{'carrots': 0.0655\}}\\ OFA hints &  \textcolor[HTML]{47B0AA}{pineapples} \\\hline
% Combined sequence &
% \textless{}sos\textgreater{}\textcolor[HTML]{3487E1}{what are put on the table next to the woman in red shirt}\textless{}sep\textgreater{}\textcolor[HTML]{C40000}{fruit fruit fruit fruit fruit fruit fruit} \textcolor[HTML]{EB2701}{bananas bananas bananas bananas bananas bananas} \textcolor[HTML]{F55E01}{food food food food} \textcolor[HTML]{E9791A}{berries berries berries} \textcolor[HTML]{F19A11}{carrots carrots carrots}\textless{}sep\textgreater{}\textcolor[HTML]{47B0AA}{pineapples pineapples pineapples pineapples pineapples pineapples pineapples pineapples pineapples pineapples}\textless{}eos\textgreater{}
% \\\hline
% \end{tabular}}
% \caption{An example of question and hints combination. The hint 'fruit' occurs 7 times in the sequence since the half of its percentage   is 7.84 (\%). }
% \label{hint_question}
% \end{table}

% For Vietnamese content, we use Underthesea\footnote{\url{https://github.com/undertheseanlp/underthesea}}
% package for word segmentation. On Japanese, Trankit \citep{nguyen2021trankit} library, a Transformer-based toolkit for multilingual natural language processing, is used to tokenize the content.

Besides the hints from question-image pairs, we also apply the Vision Transformer (ViT) \citep{dosovitskiy2020vit} to extract visual features from the image. The input image is passed into ViT model to obtain
a sequence of patches called the patch embeddings, which then pass through a Transformer encoder with multi-head attention to output the image features with the size of 196 x 768. Once the image features are achieved, we remove the vector at [CLS] token position and concatenate these visual features with text embeddings along the sequence dimension to have the final representative embedding matrices for questions, hints, and images.