\section{INTRODUCTION}
\label{intro}
Visual Question Answering (VQA) is a trending research topic in artificial intelligence that combines natural language processing and computer fields. This task enables computers to extract meaningful information from images and answer the question by natural language text. The VQA task has various practical applications such as chat-bot systems, intelligent assistance, and recommendation system. 

The VQA can be categorized as a question-answering (QA) task. In the QA task, cross-lingual language QA has been a hot trend in recent years with the appearance of BERT \cite{devlin-etal-2019-bert} (trained on more than 100 languages), as well as plenty of multilingual datasets \cite{10.1145/3560260}. The VLSP-EVJVQA challenge \cite{vlsp2022} takes the VQA as a multilingual QA task, containing three different languages: Vietnamese, Japanese, and English. The challenge brings the first large-scale multilingual VQA dataset, UIT-EVJVQA, with approximately 5,000 images and more than 30,000 question-answer pairs. The task takes an image and a question with text form as input, and the computer must return the correct answer by text as output. The questions are written in Vietnamese, Japanese, or English, and the answers must follow the language used in the questions. To create the correct answer, the computer must understand the question content and extract the information from the corresponding image. Fig.\ref{example} illustrates several examples from the dataset provided by the organizer. According to the answer types, the VLSP-EVJVQA is a Free Form Answer QA task \cite{dzendzik-etal-2021-english}.

To solve the VLSP-EVJVQA, we propose our solution, which combines the Sequence-to-Sequence (Seq2Seq) learning with image features extraction task to generate the correct answers. We use the  ViLT \cite{pmlr-v139-kim21k} and OFA \cite{wang2022ofa} for hints extraction from the image, 
Vision Transformer \cite{dosovitskiy2020vit} for image features extraction 
and the Convolutional network for Seq2Seq learning \cite{10.5555/3305381.3305510} to generate the answer. Our results achieve the $3^{rd}$ place in the VLSP-EVJVQA. Hence, we describe our works in this paper. The paper is structured as follows. Section \ref{intro} introduces the task. Section \ref{relateds} takes a brief survey about previous works for the VQA task. Section \ref{dataset} overviews the VLSP-EVJVQA dataset. Section \ref{method} describes our proposed solution. Section \ref{results}devotes to experiments and performance analysis. Finally, Section \ref{conclusion} concludes our works and presents future studies.

\begin{figure}[ht!]
% \resizebox{\textwidth}{!}{
\centering
  \includegraphics[clip,width=0.8\textwidth]{figure/ex4.pdf}%
  \label{ex31}
% }
\caption{Multilingual samples from UIT-EVJVQA dataset. From top to bottom, left to right: English (en), Vietnamese (vi) and Japanese (ja). The dataset contains a wide variety of questions; in some cases, the image contains noises that makes it difficult for a computer to distinguish the indicated object or action, for instance, ``phones" in the English example or the action of ``the man in green shirt" in the Vietnamese case. Besides, the Japanese example provides a tough scenario, "which hand is the girl putting onto the water?" in English, that even humans find it challenging to deliver the proper response.}
\label{example}
\end{figure}

% \section{INTRODUCTION}\label{introduction}


% Object detection methods can be divided into two groups: two-stage and one-stage detectors. Two-stage detectors, especially R-CNN family methods, have been showing the outperforming result. Many previous studies surveyed and applied these two-stage methods on different data domains and achieved high results: In \cite{vo2018ensemble} surveyed detectors' performance on document data;   \cite{nguyen2020detecting} do experiments on data captured by drones. One-stage detectors are less accurate but faster than two-stage detectors so that they can satisfy the real-time requirements in real life \cite{nguyen2019real}. In this study, we focus on two-stage methods for improvement. The general feature of these two-stage detectors is two stages of processing: 1) generating proposal boxes that may contain objects; 2) refining these boxes and predicting the class of objects inside proposal boxes. In generating proposals step, a Regional Proposal Network (RPN) is trained to generate proposal boxes. Its learning samples come from predefined anchor boxes; anchor boxes are specified as negative (no object) and positive (object) with offsets which are defined as \((\delta x, \delta y, \delta w, \delta h)\). However, the way of choosing positive and negative samples is relatively unclear. Commonly, we select anchor boxes as positive samples when they have IoU values with their ground-truth higher than \(\mathrm{I_+}\), and the anchor boxes which have IoU less than \(\mathrm{I_-}\) are selected as negative samples. Two IoU thresholds \(\mathrm{I_+}\) and \(\mathrm{I_-}\) are predefined in the training configuration. In the study \cite{zhang2020dynamic}, the authors indicate that this way of selecting samples is not suitable because we will easily ignore some good samples (e.g., hard negative samples) when the selecting process depends on constant values. Therefore, it is necessary to change \(\mathrm{I_+}\) and \(\mathrm{I_-}\) appropriately based on the proposal boxes' quality statistics generated from the RPN.

% In this study, we contribute three main points: propose DLAFS Cascade R-CNN that apply Dynamic Label Assignment to adjust IoU threshold based on statistics of proposal boxes on multi-stage detector Cascade R-CNN on the first stage; do experiments of changing hyperparameters to observe the performance; compare and evaluate on three datasets: SeaShips, UIT-DODV and MS-COCO for proving the improvement of our proposed method on different domains. With the SeaShips and UIT-DODV dataset, we compare the performance with and without applying Dynamic Label Assignment on Faster R-CNN and Cascade R-CNN. With MS-COCO, we directly compare with results in \cite{zhang2020dynamic} on the test-dev set. It should be clearly noted that we are only focusing on Label Assignment to better select samples for RPN, and we do not impact the process of refining coordinates.


% The rest of the paper is structured as follows. Section~2 provides an overview of related work. Section~3 describes how we apply dynamic training technique to our research. Section~4 shows our experiments and discussions. Finally, conclusions are drawn in Section~\ref{conclusion}