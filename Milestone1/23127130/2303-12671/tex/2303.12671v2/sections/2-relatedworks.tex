\section{RELATED WORKS} 
\label{relateds}
\subsection{Existing datasets and methods for visual question answering}
In computer vision, the research purpose for VQA is to make computers understand the semantic context of images. The Microsoft COCO dataset \cite{lin2014microsoft} is one of the large-scale datasets that impact many studies in computer vision tasks, including object detection, image classification, image captioning, and visual question answering. Several VQA datasets are built on the MS-COCO in different languages, such as the VQA \cite{Antol_2015_ICCV}, VQAv2 \cite{Goyal_2017_CVPR} in English, FM-IQA \cite{10.5555/2969442.2969496} for Chinese, the Japanese VQA \cite{shimizu-etal-2018-visual} for Japanese, and the ViVQA \cite{tran-etal-2021-vivqa} for Vietnamese. There are also other two benchmark datasets for training and fine-tuning VQA methods, including 
Visual Genome (VG-QA) \cite{10.1007/s11263-016-0981-7} and GQA \cite{hudson2018gqa}. VG-QA is a VQA  dataset that contains real-world photographs. It is designed and constructed to emphasize the interactions and relationship between natural questions and particular regions on the images. The creation of VG-QA lays the groundwork for building GQA, another large VQA collection that make use of Visual Genome scene graph structures to feature compositional question answering and real world reasoning. Besides, in the natural language processing field, the SQuAD dataset \cite{rajpurkar-etal-2016-squad} has boosted many studies in question-answering and natural language understanding. Based on SQuAD, many corpora are created in different languages like DuReader \cite{he-etal-2018-dureader} for Chinese, JaQuAD \cite{so2022jaquad} for Japanese, KorQuAD \cite{lim2019korquad1} for Korean, and ViQuAD \cite{nguyen-etal-2020-vietnamese,van2022vlsp} for Vietnamese. 



Apart from creating high-quality datasets, the architecture also plays a vital role in constructing intelligence systems. Taking advantage of natural language processing, we have several robust models for sequence-to-sequence learning tasks such as Long-short Term Memory \cite{8296600}, Convolutional Neural Networks for Sequence-to-sequence \cite{10.5555/3305381.3305510}, Transformer \cite{NIPS2017_3f5ee243} and BERTology \cite{rogers-etal-2020-primer}. In computer vision, state-of-the-art models for extracting useful information from images includes YOLO \cite{redmon2016you}, VGG \cite{simonyan2014very}, Vision Transformer (ViT) \cite{dosovitskiy2020vit}. With the increasing diversity of data and the need to solve multi-modal tasks that involve both visual and textual features, recent research trends focus on developing models that combine both the vision and language modalities such as: Vision-and-Language Transformer (ViLT) \cite{pmlr-v139-kim21k}, or OFA \cite{wang2022ofa}.

\subsection{Vision-language models}
\subsubsection{Vision-and-Language Transformer (ViLT)}

Introduced at ICML 2021, Vision-and-Language Transformer or ViLT \cite{pmlr-v139-kim21k} is one of the first and considerably, the simplest architectures that unifies visual and textual modalities. The model takes advantage of transformer module to extract and process visual features without using any region features or convolutional visual embedding components, making it inherently efficient in terms of runtime and parameters. The architecture of the ViLT model is originally set up to approach the VQA problem in the direction of a classification task. Thus, the output of the model contains various keyword answers with respect to probabilities.
% ViLT pre-trained and fine-tuned on VQAv2 \cite{Goyal_2017_CVPR}

\subsubsection{The "One For All" architecture that unifies modalities (OFA)}

OFA \cite{wang2022ofa} is a unified sequence-to-sequence pre-trained model that can generate natural answers for visual question answering task. The architecture uses Transformer \cite{NIPS2017_3f5ee243} as the backbone with the Encoder-Decoder framework. The model is pre-trained on the publicly available datasets of 20M image-text pairs and achieves state-of-the-art performances in a series of vision and language downstream tasks, including image captioning, visual question answering, visual entailment, and referring expression comprehension, making it a promising component in our approach toward the VQA challenge.

\subsection{Convolutional Sequence-to-sequence Network}


Sequence-to-sequence learning (Seq2Seq) is a process of training models to map sequences from one domain to sequences in another domain. Some common Seq2Seq applications include machine translation, text summarization and free-form question answering, in which the system can generate a natural language answer given a natural language question. 
A trivial case of Seq2Seq, where the input and output sequences are in the same length, can be solved using a single Long Short Term Memory (LSTM) or Gated Recurrent Unit (GRU) layer. In a canonical Seq2Seq problem, however, the input and output sequences are of different lengths, and the entire input sequence is required to begin predicting the target. This requires a more advanced setup, in which RNN-based encoder-decoder architectures commonly used to address the problem. The typical and generic architecture of these type of models include two components: an encoder that processes input sequence $X = (x_1, x_2,..., x_m)$ and return state representation $z =  (z_1, z_2,..., z_m)$, also known as context vector, and a decoder that decodes the context vector and outputs the target sequence $y = (y_1, y_2,..., y_n)$ by generating it left-to-right consecutively, one word at a time.

Different from other Seq2Seq models such as bi-directional recurrent neural networks (Bi-RNN) with soft-attention mechanism \citep{bahdanau2016neural, luong-etal-2015-effective} or the mighty Transformer \citep{NIPS2017_3f5ee243} with self-attention, the convolutional sequence-to-sequence network (ConvS2S) has the Encoder - Decoder architecture based entirely on convolutional neural networks and originally set up for machine translation task. The model employs many convolutional layers, which are commonly used in image processing, to enable parallelization over each element in a sequence during training and thus better utilize GPU hardware and optimization compared to recurrent networks. ConvS2S applies a special activation function called the Gate Linear Unit (GLU) \cite{DBLP:journals/corr/DauphinFAG16} as non-linearity based gating mechanism over the output of the convolution layer, which has been shown to perform better in the context of language modeling. Multi-step attention is also the key component of the architecture that allows the model to make multiple glimpses across the sequence to produce better output.

% \section{RELATED WORKS}\label{relatedworks}


% \subsection{Object detection methods}

% In this section, we present an overview of object detection methods and two classical studies about object detection: Faster R - CNN and Cascade R - CNN.

% \subsubsection{Previous object detection methods}

% Most of the current object detection methods are divided into two categories: Single-stage and two-stage. 
% The main idea of the two-stage methods: 1) Extract proposed regions that may or may not contain objects; 2) Classifying these proposed regions. Two-stage methods can be mentioned such as R-CNN \cite{girshick2014rich}, Faster R-CNN \cite{ren2015faster}, Cascade R-CNN \cite{cai2019cascade}, Mask R-CNN \cite{he2017mask}, Libra R-CNN \cite{pang2019libra}. The two-stage methods have high accuracy but do not meet the real-time applicability.
% The one-stage methods make predictions with respect to anchors or a grid of possible object centres. One-stage methods include the YOLO family \cite{redmon2016you,redmon2017yolo9000,redmon2018yolov3,bochkovskiy2020yolov4}, SSD \cite{liu2016ssd}, or more recently we have seen the appearance of CornerNet \cite{law2018cornernet}, an object detection method that uses a pair of key points of left-top, right-bottom using a single CNN architecture. CenterNet \cite{duan2019centernet} is followed by CornerNet; instead of using a pair of key points, it uses a set of three points for an object. Single-stage methods often achieve real-time, practical applicability. 

% \subsubsection{Faster R - CNN}

% In 2014, Ren et al. introduced the Faster R - CNN method, specifically, the authors introduced the concept of Region proposal to create regions with the ability to contain objects based on the selective search algorithm. These regions are then scaled to the same size and further traversed through a CNN architecture. At this step, the proposed regions with threshold \( IoU \geq 0.5 \) with ground truths will be further predicted by a classifier, and the object's coordinates are predicted by the Bounding box regressor. 



% \subsubsection{Cascade R - CNN}\label{section:cascadercnn}

% Cascade R â€“ CNN is an object detection method proposed by Cai et al. to solve the problem of decreasing detection performance when the IoU threshold increases. Two main factors influence this: 1) The overmatch occurs when the positive suggested regions disappear exponentially. 2) The prediction time mismatch between the optimal IoU thresholds of the detector and the IoUs of the region proposals. 
% The authors proposed to expand R - CNN into several stages; after each stage, the results will be better selected to eliminate false-positive regions. 

% \begin{figure}[H]
%      \centering
%      \begin{subfigure}[b]{0.35\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{images/FasterRCNN2.png}
%          \label{fig:3a}
%          \caption{Faster R - CNN}
%      \end{subfigure}
%      \hspace{6mm}
%      \begin{subfigure}[b]{0.43\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{images/CascadeRCNN.png}
%          \label{fig:3b}
%          \caption{Cascade R - CNN}
%      \end{subfigure}
%      \hfill
%      \caption{A comparison between Faster R - CNN and Cascade R - CNN}
%      \label{fig:comparefastercascade}
% \end{figure}

% As can be seen in Figure \ref{fig:comparefastercascade}, Faster R - CNN includes a Region Proposal Network (denoted as the first ``Network head" in the figure) to produce object proposals. These proposals are continued to be processed and classified by an RoI detection sub-network (denoted as the second ``Network head"). Finally, each region proposal's classification score and bounding box coordinate will be assigned. But in Cascade study \cite{cai2019cascade}, the authors claim that these results are not good enough. Thereby, Cai and Vasconcelos  proposed Cascade R - CNN to handle limitations that exist in Faster R - CNN. In detail, predicted bounding boxes will be input into the network head at the previous stage and continue to go through the next stage to regress bounding boxes again. Equation \ref{equation:cascade} below shows the operation of the Cascade R-CNN method
% \begin{equation}
% f(x, \boldsymbol{b}) = f_T \circ f_{T-1} \circ \dots \circ f_1(x, \boldsymbol{b}).
% \label{equation:cascade}
% \end{equation}

% This iterative approach attempts to gradually fine-tune the bounding box to obtain a more accurate one; in this way, the region proposals are improved progressively. In addition, network heads are different at each stage in Cascade R - CNN and the IoU threshold are changed from small to large through stages. It is worth notable that cascaded regression is a resampling procedure, not a post-processing step, providing good positive samples to the next stage.


% \subsection{Proposal classification in R - CNN-based detectors}

% In R-CNN-based detectors, they use a pre-defined IoU threshold for proposal classification, and this task can be formulated as follows
% \[
%     Label = 
% \begin{cases}
%     1,& \text{if } \ maxIoU(b, G) \geq T_{+}\\
%     0,& \text{if }\  maxIoU(b, G) <  T_{-}\\
%     -1, & \text{otherwise},
% \end{cases}
% \]
% where, $b$ stands for a single proposal; $G$ presents a set of ground truths; \(T_{+}\) and \(T_{-}\) are the positive and negative thresholds for IoU; 1, 0, -1 stand for positives, negatives and ignored samples, respectively.

% The simplest idea to improve the performance is that we can increase the IoU threshold. But at the beginning of the training process, the quality of proposals is not good enough to satisfy the high IoU, so the number of positive proposals can be much low. In Cascade R-CNN, the authors try to handle this issue by increasing IoU threshold \(T_{+}\) and \(T_{-}\) through the stages with the hope that proposals in previous stages will have acceptable quality for the increased IoU threshold, which is effective but time-consuming \cite{zhang2020dynamic}.

% \subsection{Previous label assignment optimization methods}



% As discussed in Section 1, the improvement of the Label Assignment is necessary because if only one IoU threshold is applied over the entire training process, it will be easy to ignore the good samples to let the RPN learn well. There has been a lot of research related to improving this problem in many ways, including the idea of Dynamic Training. GuidedAnchoring \cite{wang2019region} proposes a method to predict the locations where the midpoints of the proposal boxes may exist, as well as the sizes and scales at different locations. ATSS \cite{zhang2020bridging} proposes an adaptive labeling method. They will initially select the anchor boxes based on the distance between the anchor boxes and the ground truth. Then they calculate the median and the IoU variance of the anchor boxes. Anchor boxes with IoU greater than the sum of the anchor boxes will be selected as positive samples. This study has been experimented with and shows improvements. However, the authors require additional layers and complex structures or only one anchor box to have a full classification score. This is not appropriate in cases where there are many high-quality and high-competition anchors. Our study is heavily inspired by Dynamic Training \cite{zhang2020dynamic}, which shows that the proposed regions generated by RPN get better with training time both in terms of classification and coordinate regression. However, the authors only performed experiments on Faster R-CNN, and the selected hyperparameters were still emotional.