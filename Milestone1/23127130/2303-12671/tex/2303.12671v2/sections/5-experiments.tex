\section{EXPERIMENTS AND ANALYSIS}
\label{results}
\subsection{Experiment Settings}
The ConvS2S model has 512 hidden units for both encoders and decoders. All embeddings, including the output produced by the decoder before the final linear layer, have a dimensionality of 768. This setup allows the encoders to concatenate with patch embeddings from ViT model. To avoid overfitting, dropout is applied on the embeddings, decoder output, and the input of the convolutional blocks with a retaining probability of 0.5. Teacher forcing with probability of 0.5 in the is also applied in the architecture to accelerate the training progress.


% We train the convolutional model using Adam optimizer with a fixed learning rate 2.50e-4.
Many experiments are carried out in order to evaluate the proposed approach toward the VLSP-EVJVQA challenge. Typically, the training and evaluation of ConvS2S model is conducted using four types of input sequence: question only, question-image, question-hint and question-hint-image. First, we initialize the baseline result of ConvS2S with only question as input sequence and no image information. This scenario is similar to the Knowledge-based question answering (KBQA) task in that the generated answers are entirely based on the question-answer dependencies learned during the training phase. The second experiment involved image features combined with question as typical VQA approach. We then add visual hints to the input sequences used in the two prior experiments and investigate their effect on overall performance.

Because of the limitation in computational resources as well as the strict timeline of the competition, we only deploy the fine-tuned ViLT-B/32 with 200K pretraining steps and pre-trained OFA$_{\mathrm{large}}$ with 472M parameters for hints inference given the question and image. For feature extraction from image, we deploy pre-trained ViT-B/16 with base-sized version.
To have the comparative result, we set up the same hyperparameters for all experiments with ConvS2S. The model is trained in 30 epochs with batch size of 128 using Adam optimizer with a fixed learning rate of 2.50e-4. After each epoch, the performance loss on the train and development sets is calculated using the Cross-Entropy Loss function.

The proposed architecture and SOTA vision and language models are implemented in PyTorch and trained on the Kaggle platform with hardware specifications: Intel(R) Xeon(R) CPU @ 2.00GHz; GPU Tesla P100 16 GB with CUDA 11.4.

\subsection{Experimental Results}

\begin{table}[!htbp]
    \centering
    \resizebox{\columnwidth}{!}{%
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcccccc}
    \toprule
        \textbf{Model} & \textbf{F1} & \textbf{BLEU-1} & \textbf{BLEU-2} & \textbf{BLEU-3} & \textbf{BLEU-4} & \textbf{BLEU}  \\ \midrule
        ConvS2S (Question only) & 0.3005 &0.2592	&0.2034	&0.1677	&0.1425& 0.1932  \\ \midrule
        ConvS2S + ViT & 0.3109 &0.2683	&0.2119	&0.1747	&0.1480 & 0.2007  \\ \midrule
        ConvS2S + ViLT & 0.3294 &0.2692	&0.2109	&0.1723	&0.1446& 0.1993  \\ 
        ConvS2S + OFA & 0.3331 &0.2858	&0.2269	&0.1876	&0.1598 & 0.2150  \\ 
        \textbf{ConvS2S + ViLT + OFA}
        % \tablefootnote{This model is not yet evaluated on the private test set \label{note1}}
        & \textbf{0.3442} &0.2797	&0.2205	&0.1808	&0.1529 &0.2085  \\ 
        \midrule
        ConvS2S + ViT + ViLT & 0.3361 &0.2833	&0.2243	&0.1845	&0.1564 & 0.2122  \\ 
        ConvS2S + ViT + OFA & 0.3390 &0.2877	&0.2276	&0.1877	&0.1593 & \textbf{0.2156}  \\
        \textbf{ConvS2S + ViT + ViLT + OFA}
        % \textsuperscript{\ref{note1}}
        & \textbf{0.3442} & 0.2747	&0.2148	&0.1747	& 0.1465& 0.2027 \\ \bottomrule
    \end{tabular}}
    \caption{Performance of ConvS2S with different combinations of pre-trained models on the public test set.}
    \label{tab:repub}
\end{table}

% \begin{table}[H]
%     \centering
%     \resizebox{\columnwidth}{!}{%
%     \setlength{\tabcolsep}{5pt}
%     \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{lcccccc}
%     \toprule
%         \textbf{Model} & \textbf{F1} & \textbf{BLEU-1} & \textbf{BLEU-2} & \textbf{BLEU-3} & \textbf{BLEU-4} & \textbf{BLEU (Avg.)}  \\ \midrule
%         ConvS2S (no image features) & 0.3005 &0.2592	&0.2034	&0.1677	&0.1425& 0.1932  \\ \midrule
%         ConvS2S + ViLT-B/32 & 0.3294 &0.2692	&0.2109	&0.1723	&0.1446& 0.1993  \\ 
%         ConvS2S + OFA$_{\mathrm{large}}$ & 0.3331 &0.2858	&0.2269	&0.1876	&0.1598 & 0.2150  \\ 
%         \textbf{ConvS2S + ViLT-B/32 + OFA$_{\mathbf{large}}$}
%         % \tablefootnote{This model is not yet evaluated on the private test set \label{note1}}
%         & \textbf{0.3442} &0.2797	&0.2205	&0.1808	&0.1529& \textbf{0.2085}  \\ 
%         \midrule
%                 ConvS2S + ViT-B/16 & 0.3109 &0.2683	&0.2119	&0.1747	&0.1480 & 0.2007  \\ %\midrule
%         ConvS2S + ViT-B/16 + ViLT-B/32 & 0.3361 &0.2833	&0.2243	&0.1845	&0.1564 & 0.2122  \\ 
%         ConvS2S + ViT-B/16 + OFA$_{\mathrm{large}}$ & 0.3390 &0.2877	&0.2276	&0.1877	&0.1593 & 0.2156  \\
%         \textbf{ConvS2S + ViT-B/16 + ViLT-B/32 + OFA$_{\mathbf{large}}$}
%         % \textsuperscript{\ref{note1}}
%         & \textbf{0.3442} & 0.2747	&0.2148	&0.1747	& 0.1465& \textbf{0.2027} \\ \bottomrule
%     \end{tabular}}
%     \caption{Performance of ConvS2S with different combinations of pre-trained models on the public test set.}
%     \label{result_public}
% \end{table}

\begin{figure}[ht]
\centering
% \subfloat[ConvS2S training loss per epoch]{%
%   \includegraphics[width=0.495\textwidth]{figure/train_loss1.pdf}%
% }
% \hspace{-0.2em}
% \subfloat[ConvS2S testing loss per epoch]{%
%   \includegraphics[width=0.495\textwidth]{figure/test_loss1.pdf}%
% }
\includegraphics[width=\textwidth]{figure/all_loss.pdf}
\caption{Training loss and public testing loss comparison of ConvS2S model with different combinations of hint and image features.}
\label{loss}
\end{figure}


The two metrics: F1 and BLEU, are used in the challenge to evaluate the results. The BLEU score is the average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4. F1 is used for ranking the final results. Table \ref{tab:repub} presents the performance of the proposed ConvS2S model with different combinations of pre-trained models on the UIT-EVJVQA public test set.

% First, with only question as input, ConvS2
According to Table \ref{tab:repub}, the original ConvS2S model using only question obtained 0.3005 by F1 and 0.1932 by BLEU. Using question-image pairs, ConvS2S achieves a marginally better performance on both metric. When visual hints are integrated into questions, the F1 score improves by at least 2.89\%, and the model achieves the best performance with 0.3442 by F1 and 0.2085 by BLEU when both ViLT and OFA hints are used. At final stage, adding image feature from ViT to question-hint sequences help improve the performance of previous models. Based on F1, these two combinations ConvS2S{\tiny~}+{\tiny~}ViLT{\tiny~}+{\tiny~}OFA and ConvS2S{\tiny~}+{\tiny~}ViT{\tiny~}+{\tiny~}ViLT{\tiny~}+{\tiny~}OFA are considered as our best methods on the public test set. 
Figure \ref{loss} depicts the gradual improvement in both training loss and testing loss as more image features and hints are added to the ConvS2S model. Knowledge-based ConvS2S (red line) does not catch the image context and thus have the highest loss. Though ConvS2S with ViT+VILT features does not obtained a competitive result on evaluation metrics, it gives the best loss among methods in the public test phase. In general, the optimal testing loss of methods is achieved between 14th and 20th epoch, then the models tend to be overfitting.


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figure/train_loss.pdf}
%     \caption{tmp}
%     \label{100score}
% \end{figure}
% \subsubsection{Qualitative analysis}
% \label{quali_analysis}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figure/test_loss.pdf}
%     \caption{tmp}
%     \label{100score}
% \end{figure}
% \subsubsection{Qualitative analysis}
% \label{quali_analysis}

We manage to deploy two ensembles of ConvS2S using features from ViT combined with hints from {\tiny~}ViLT and {\tiny~}OFA, respectively, for the final evaluation on private test set. As shown in Table \ref{result_private}, the ConvS2S{\tiny~}+{\tiny~}ViT{\tiny~}+{\tiny~}OFA model obtained the better result, which is 0.4210 by F1 and 0.3482 by BLEU, and ranked $3^{rd}$ in the challenge. Table \ref{ranking} shows the final standing at the EVLSP-EVJVQA competition, in which our best model perform poorer 1.82\% and 1.39\% by F1 compared with the first and second place solutions. In terms of methodology, our approach comes in second place after the ViT{\tiny~}+{\tiny~}mT5 method, which has a large amount of pre-trained data. Overall, there is a gap between F1 and BLEU scores.



\begin{table}[H]
    \centering
    \small
    %\resizebox{\columnwidth}{!}{%
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcc}
    \toprule
        \textbf{Model} & \textbf{F1} & \textbf{BLEU}  \\ \midrule
        ConvS2S + ViT + ViLT &0.4053  &0.3228  \\
        \textbf{ConvS2S + ViT + OFA} & \textbf{0.4210}  & \textbf{0.3482}
  \\ \bottomrule
    \end{tabular}
    \caption{Performance on the private test set.}
    \label{result_private}
\end{table}


\begin{table}[!htbp]
\renewcommand{\arraystretch}{1.2}
\newcolumntype{L}{>{\centering\arraybackslash}m{7cm}}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{clLccccc}
\toprule
\multirow{2}{*}{\textbf{No.}} & \multirow{2}{*}{\textbf{Team name}} & \multirow{2}{*}
{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Public Test}} && \multicolumn{2}{c}{\textbf{Private Test}} \\\cmidrule{4-5} \cmidrule{7-8}
                             &                                     && \textbf{F1}         & \textbf{BLEU}      && \textbf{F1}         & \textbf{BLEU}       \\\midrule
1                            & CIST AI  &  ViT + mT5                          & 0.3491              & 0.2508             && 0.4392              & 0.4009              \\\midrule
2                            & OhYeah & ViT + mT5                             & 0.5755              & 0.4866             && 0.4349              & 0.3868              \\\midrule
\textbf{3}                            & \textbf{DS\_STBFL} & \textbf{ConvS2S+ViT+OFA}                  & \textbf{0.3390}     & \textbf{0.2156}    && \textbf{0.4210}     & \textbf{0.3482}     \\\midrule
4                            & FCoin & ViT + mBERT                               & 0.3355              & 0.2437             && 0.4103              & 0.3549              \\\midrule
5                            & VL-UIT & BEiT + CLIP + Detectron-2 + mBERT + BM25 + FastText                              & 0.3053              & 0.1878             && 0.3663              & 0.2743              \\\midrule
6                            & BDboi & ViT + BEiT + SwinTransformer
+ CLIP + OFA + BLIP                              & 0.3023              & 0.2183             && 0.3164              & 0.2649              \\\midrule
7                            & UIT\_squad & VinVL+mBERT                          & 0.3224              & 0.2238             && 0.3024              & 0.1667              \\\midrule
8                            & VC\_Internship & ResNet-152 + OFA                      & 0.3017              & 0.1639             && 0.3007              & 0.1337
\\\midrule9                            & Baseline & ViT + mBERT                      & 0.2924              & 0.2183             && 0.3346              & 0.2275\\\bottomrule       
\end{tabular}}
\caption{Our performance compared with other teams at VLSP2022-EVJVQA \cite{vlsp2022}}
\label{ranking}
\end{table}




\subsection{Performance Analysis}

According to the final result in the private test phase, the generated output from ConvS2S
+ViT+OFA model are chosen for further analysis. Generally, the model manages to generate answers with correct language with the input question.
\subsubsection{Quantitative analysis}
We randomly choose 100 samples from the generated result to perform quantitative analysis. The average length, vocabulary size, and the number of POS tags in the ground truth and generated answers are calculated for each language. Table \ref{quanti} shows the statistics of the ground truth answer compared with the predicted answer by the model.

% \begin{table}[ht]
% \centering
% %\resizebox{\columnwidth}{!}{%
% \begin{tabular}{llrr}
% \toprule
% &Language&Ground Truth&Predicted\\\midrule

% \multirow{ 4}{*}{Avg. length} & English & 3.74 & 6.18 \\
% & Vietnamese & 4.42 & 5.97\\
% & Japanese & 4.67 & 8.43\\
% & All &4.26&6.78\\\midrule

% \multirow{ 4}{*}{Vocab. size} & English & 78 & 72 \\
% & Vietnamese & 97 & 101\\
% & Japanese & 77 & 83\\
% & All &252&256\\\midrule

% \multirow{ 4}{*}{\# POS tag} & English & 12 & 9 \\
% & Vietnamese &10  &9 \\
% & Japanese & 10 & 11\\
% & All &14 &14\\
% \bottomrule
% \end{tabular}
% \caption{The quantitative statistic of 100 generated samples compared with the ground truth}
% \label{quanti}
% \end{table}

\begin{table}[ht]
\centering
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrr}
\toprule
Language&Stats.&Ground Truth&Predicted\\\midrule
\multirow{ 3}{*}{English} & Avg.length  & 3.74 & 6.18 \\
& Vocab. size & 78 & 72 \\
& \# POS tag  & 12 & 9 \\\midrule

\multirow{ 3}{*}{Vietnamese} & Avg.length  & 4.42 & 5.97 \\
& Vocab. size  & 97 & 101 \\
& \# POS tag &10  &9 \\\midrule

\multirow{ 3}{*}{Japanese} & Avg.length   & 4.67 & 8.43 \\
& Vocab. size & 77 & 83 \\
& \# POS tag  & 10 & 11 \\\midrule\midrule

\multirow{ 3}{*}{All} & Avg.length  &4.26 &6.78 \\
& Vocab. size &252 &256 \\
& \# POS tag  &14 &14 \\

\bottomrule
\end{tabular}
\caption{The quantitative statistic of 100 generated samples compared with the ground truth}
\label{quanti}
\end{table}

From Table \ref{quanti}, it can be seen that although the model gave the answers longer than the ground truth answers, the semantics is not as much as the ground truth. It can be seen from Table \ref{quanti} that the predicted answers in English have an average length higher than the ground truth answers. Also, the vocabulary in the generated answers is more than the original. In contrast, the number of POS tag components in the predicted answers is lower than the ground truth. This is similar to the answers in Vietnamese. For the Japanese, the characteristics of the predicted answers in average length and vocabulary size are the same as the two remaining languages. However, the number of POS tags in the predicted answers is more than in the ground truth answers. To make it clear, we propose three types of error on our model in Section \ref{quali_analysis}.

In addition, Figure \ref{100score} illustrates the distributions of F1 and BLEU scores for each language. Generally, the histograms skewed to the right and the model  performs inconsistently across languages. The proportion of samples with F1 and BLEU scores less than 0.2 dominates the overall result across all three languages. In Vietnamese, the number of generated samples with F1 and BLEU scores greater than 0.4 is significantly higher than in other languages. Meanwhile, English and Japanese responses rarely score greater than 0.6 on both metrics, furthermore, no Japanese samples scoring greater than 0.8 in BLEU. This illustrates that our model faces numerous challenges in producing the desired responses, with specific limitations on each language.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figure/hist.pdf}
    \caption{Distributions of F1 and BLEU scores for each language from 100 generated samples}
    \label{100score}
\end{figure}

\begin{figure}[!htbp]
\centering
\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns1.pdf}%
  \label{attn1}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns2.pdf}%
  \label{attn2}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns3.pdf}%
  \label{attn3}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns4.pdf}%
  \label{attn4}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns5.pdf}%
  \label{attn5}
}
\caption{Numerous samples of attention alignment from ConvS2S and the changes in attention when adding features from ViT and OFA models. The x-axis and y-axis of each plot correspond to the words in the question and the generated answer, respectively, while each pixel illustrates the weight $w_{ij}$ of the assignment of the j-th question word for the i-th
answer word.}
\label{attn}
\end{figure}

\subsubsection{Qualitative analysis}
\label{quali_analysis}
\paragraph{Attention visualization}

Figure \ref{attn} shows several samples of attention weights between each element from the generated answer with those in the input sequence that contains no image features, OFA hints, and ViT+OFA combined features, respectively. The visualization provided an intuitive way to discover which positions in the input sequence were considered more important when generating the target answer word. The brighter a pixel's color, the more important the word in the input sequence is in producing the respect answer word. The first heatmaps illustrate the case where no image information is used during training but only question. This is similar to Knowledged-based QA task where the model gives the answer solely based on the context of question. As a result, the generated answer is just simply a guess from ConvS2S model and has poor evaluation results on both metrics. Through attention visualization, we study that OFA hint is importance feature to model's attention as it provides the near-correct insight for the question and reduces the reliance on question words when generating the answer. This reduction in attention is not completely common for all question tokens, and it still depends on the importance of other elements in the whole sequence. However, in some cases, the model focuses too much on a specific element of the hint, which may lead to bias. ViT features has shown to control the affection of OFA hint, neutralizing it with other elements from question if hint appears to be off-topic. It may enhance the attention, making the model focus stronger on specific parts of the provided hint, for instance, the hint token ``nhà hàng'' (\textit{restaurant}) in Figure \ref{attn3} is given more attention when adding ViT image features. These features can also reduce the attention in one element and distributes concentration on other parts of the sequence. Figures \ref{attn1} and \ref{attn2} depict the reduction in hint attention into question elements, while Figures \ref{attn4} and \ref{attn5} show the changes in attention weight distribution among hint tokens.

\paragraph{Error analysis}
\begin{figure}[!ht]
\centering
\subfloat[Error Case I]{%
  \includegraphics[width=\textwidth]{figure/err1.pdf}%
\label{fig:1a}}
\vspace{1em}
\subfloat[Error Case II]{%
  \includegraphics[width=\textwidth]{figure/err2.pdf}%
    \label{fig:1b}
}
\vspace{1em}
\subfloat[Error Case III]{%
  \includegraphics[width=\textwidth]{figure/err3.pdf}%
\label{fig:1c}}
\caption{Three typical error cases from generated results.}
\label{fig:1}
\end{figure}

% \begin{figure}[H]
% \centering
% \resizebox{\textwidth}{!}{
%     \begin{subfigure}[b]{.3\linewidth}
%     \centering
%     \includegraphics[width=0.99\textwidth]{figure/00000001682.jpg}
%     \raggedright
%     { \scriptsize \textbf{Question}: what hat does the narrator of the 
%     historical site wear?}\\
%     {\scriptsize \textbf{Groundtruth}: non la}\\
%     {\scriptsize \textbf{Predicted}: the boy wears a white shirt and white and white}\\
%     {\scriptsize \textbf{F1:}  0.0000}\\
%     {\scriptsize \textbf{BLEU:} 0.0000
%     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ }
%     \caption{Error Type I}
%     \label{fig:1a}
%   \end{subfigure}%
%   \hspace{0.5em}
  
%  %\hspace*{\fill}
%   \begin{subfigure}[b]{.35\linewidth}
%     \centering
%     \includegraphics[width=0.99\textwidth]{figure/00000004737.jpg}
%     \raggedright
    
%     {\scriptsize \textbf{Question}: có bao nhiêu người đứng bên phải chàng trai? (\textit{English: How many people on the right of the man?})}\\
    
%     {\scriptsize \textbf{Groundtruth}: có ba người đứng bên phải chàng trai (\textit{English: There are three people on the right of the man})}\\
    
%     {\scriptsize \textbf{Predicted}: có hai người đứng bên phải chàng trai (\textit{English: There are two people on the right of the man})}\\
    
%     {\scriptsize F1:  0.8750}\\
    
%     {\scriptsize BLEU: 0.7799}
%     \caption{Error Type II}
%     \label{fig:1b}
%   \end{subfigure}%
%   \hspace{0.5em}
%   %\hspace*{\fill}
%   \begin{subfigure}[b]{0.35\linewidth}
%      \centering
%     \includegraphics[width=0.99\textwidth]{figure/00000000111.jpg}
    
%     \raggedright {\scriptsize \textbf{Question}:}
%     {\tiny
%     \begin{CJK*}{UTF8}{min}
%     {\CJKfamily{goth}小船手は何本のオールを使っていますか? (\scriptsize \textit{English: How many paddles does the boatman use?})}
%     \end{CJK*}}\\
%     {\scriptsize \textbf{Groundtruth}: 2}\\
%     {\scriptsize \textbf{Predicted}:}
%     {\tiny
%     \begin{CJK*}{UTF8}{min}
%     {\CJKfamily{goth}2本の船を使っています (\scriptsize \textit{ English: using two boats})}
%     \end{CJK*}}\\
%     {\scriptsize \textbf{F1:} 0.0000}\\
%     {\scriptsize \textbf{BLEU:} 0.0000 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ }\\
%     \caption{Error Type III}
%     \label{fig:1c}
%   \end{subfigure}%  
% }
%   \caption{Example of generated answers that contains errors.(b) the keyword 'hai người' (two people) is given  instead of 'ba người' (three people). Coincidentally, the question and groundtruth in this case both share the same phrase "đứng bên phải chàng trai" ("on the right of the man"), }\label{fig:1}
% \end{figure}


For better understand the generation performance on the VQA task, we examine the generated answers of our best ensemble, ConvS2S
+ViT+OFA, to identify the limitations and analyze factors that may cause the model to perform poorly.
Through the error analysis process, various errors and mistakes have been pointed out in the outputs of the model. The typical examples of various types of errors are illustrated in Figure \ref{fig:1}. In summary, we divide these errors into three groups:

\begin{itemize}
    \item The generated answer does not match the question and has no correct tokens compared with the ground truth answer, as shown in Figure \ref{fig:1a}. This error case sometimes accompanied by text degeneration.
    \item The output response gives the wrong answer to the question but shares some insignificant tokens or has a similar structure with the ground truth answer, as shown in Figure \ref{fig:1b}, which significantly improves the evaluation score. This incorrect scenario exemplifies the limitation of the evaluation measures.
    \item The model managed to generate the correct key answer while also adding unnecessary information compared to the ground truth, which may lead to the response's meaning being distorted. 
    As shown in Figure \ref{fig:1c}, the model correctly predicted quantity but then added unnecessary tokens afterward, resulting in a low score on both evaluation metrics.
\end{itemize}


% \begin{figure*}[h]
% \centering
%   \begin{tabular}{@{}ccc@{}}
%     \includegraphics[width=0.3\textwidth]{figure/5.3_ex/00000000111.jpg}
%     \includegraphics[width=0.3\textwidth]{example-image-b} &
%     \includegraphics[width=0.3\textwidth]{example-image-b} \\
%   \end{tabular}
%   \caption{This is some figure side by side}
% \end{figure*}

