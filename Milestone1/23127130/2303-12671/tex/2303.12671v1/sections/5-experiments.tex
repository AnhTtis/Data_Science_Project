\section{EXPERIMENTS AND ANALYSIS}
\label{results}
\subsection{Experiment Settings}
The ConvS2S model has 512 hidden units for both encoders and decoders. All embeddings, including the output produced by the decoder before the final linear layer, have a dimensionality of 768. This setup allows the encoders to concatenate with patch embeddings from ViT model. To avoid overfitting, dropout is applied on the embeddings, decoder output, and the input of the convolutional blocks with a retaining probability of 0.5.


% We train the convolutional model using Adam optimizer with a fixed learning rate 2.50e-4.
Many experiments are carried out in order to evaluate the proposed approach toward the VLSP-EVJVQA challenge. We begin by initializing the baseline result of ConvS2S without using any image information. This mean that the generated answers are completely based on the answer-question dependencies learned by the model during the training phase. We then sequentially add hint and image features to the input sequence and study their effect on the overall performance. Because of the limitation in computational resources as well as the strict timeline of the competition, we only deploy the fine-tuned ViLT-B/32 with 200K pretraining steps and pre-trained OFA$_{\mathrm{large}}$ with 472M parameters for hints inference given the question and image.
To have the comparative result, we set up the same hyperparameters for all experiments. The models are trained in 30 epochs using Adam optimizer with a fixed learning rate of 2.50e-4 and batch size of 128. After each epoch, the performance loss on the train and development sets is calculated using the Cross-Entropy Loss function.

The proposed architecture and SOTA vision and language models are implemented in PyTorch and trained on the Kaggle platform with hardware specifications: Intel(R) Xeon(R) CPU @ 2.00GHz; GPU Tesla P100 16 GB with CUDA 11.4.

\subsection{Experimental Results}
\begin{table}[H]

    \centering
    \resizebox{\columnwidth}{!}{%
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcccccc}
    \toprule
        \textbf{Model} & \textbf{F1} & \textbf{BLEU-1} & \textbf{BLEU-2} & \textbf{BLEU-3} & \textbf{BLEU-4} & \textbf{BLEU (Avg.)}  \\ \midrule
        ConvS2S (no image features) & 0.3005 &0.2592	&0.2034	&0.1677	&0.1425& 0.1932  \\ \midrule
        ConvS2S + ViLT-B/32 & 0.3294 &0.2692	&0.2109	&0.1723	&0.1446& 0.1993  \\ 
        ConvS2S + OFA$_{\mathrm{large}}$ & 0.3331 &0.2858	&0.2269	&0.1876	&0.1598 & 0.2150  \\ 
        \textbf{ConvS2S + ViLT-B/32 + OFA$_{\mathbf{large}}$}
        % \tablefootnote{This model is not yet evaluated on the private test set \label{note1}}
        & \textbf{0.3442} &0.2797	&0.2205	&0.1808	&0.1529& \textbf{0.2085}  \\ 
        \midrule
                ConvS2S + ViT-B/16 & 0.3109 &0.2683	&0.2119	&0.1747	&0.1480 & 0.2007  \\ %\midrule
        ConvS2S + ViT-B/16 + ViLT-B/32 & 0.3361 &0.2833	&0.2243	&0.1845	&0.1564 & 0.2122  \\ 
        ConvS2S + ViT-B/16 + OFA$_{\mathrm{large}}$ & 0.3390 &0.2877	&0.2276	&0.1877	&0.1593 & 0.2156  \\
        \textbf{ConvS2S + ViT-B/16 + ViLT-B/32 + OFA$_{\mathbf{large}}$}
        % \textsuperscript{\ref{note1}}
        & \textbf{0.3442} & 0.2747	&0.2148	&0.1747	& 0.1465& \textbf{0.2027} \\ \bottomrule
    \end{tabular}}
    \caption{Performance of ConvS2S with different combinations of pre-trained models on the public test set.}
    \label{result_public}
\end{table}

\begin{figure}[ht]
\centering
% \subfloat[ConvS2S training loss per epoch]{%
%   \includegraphics[width=0.495\textwidth]{figure/train_loss1.pdf}%
% }
% \hspace{-0.2em}
% \subfloat[ConvS2S testing loss per epoch]{%
%   \includegraphics[width=0.495\textwidth]{figure/test_loss1.pdf}%
% }
\includegraphics[width=\textwidth]{figure/all_loss.pdf}
\caption{Training loss and public testing loss comparison of ConvS2S model with different combinations of hint and image features}
\label{loss}
\end{figure}


The two metrics: F1 and BLEU, are used in the challenge to evaluate the results. The BLEU score is the average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4. F1 is used for ranking the final results. Table \ref{result_public} presents the performance of the proposed ConvS2S model with different combinations of pre-trained models on the UIT-EVJVQA public test set.

% First, with only question as input, ConvS2
According to Table \ref{result_public}, the original ConvS2S model without image features but using only question obtained 0.3005 by F1 and 0.1932 by BLEU. When integrating hint features from images, the F1 score improved at least 2.89\% and achieve highest result with 0.3442 by F1 and 0.2085 by BLEU when using both ViLT and OFA hints. After adding image feature from ViT-B/16, the performance of previous models tend to improve. However the final ensemble does not surpass the ConvS2S{\tiny~}+{\tiny~}ViLT-B/32{\tiny~}+{\tiny~}OFA$_{\mathrm{large}}$ ensemble on F1 metrics and even give lower BLEU score. Based on F1, these two ensembles are considered as our best models on the public test set. 
Figure \ref{loss} depicts the gradual improvement in both training loss and testing loss as more image features are added to the ConvS2S model. Memory-based ConvS2S does not catch the image context and thus have the highest loss. Though ConvS2S with ViT+VILT features does not obtained a competitive result on F1 and BLEU score, it has the best loss among methods in the public test phase. In general, the optimal testing loss of methods is achieved between 14th and 20th epoch, then the models tend to be overfitting.


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figure/train_loss.pdf}
%     \caption{tmp}
%     \label{100score}
% \end{figure}
% \subsubsection{Qualitative analysis}
% \label{quali_analysis}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figure/test_loss.pdf}
%     \caption{tmp}
%     \label{100score}
% \end{figure}
% \subsubsection{Qualitative analysis}
% \label{quali_analysis}

We manage to deploy two ensembles of ConvS2S using features from ViT-B/16 combined with hints from {\tiny~}ViLT-B/32 and {\tiny~}OFA$_{\mathrm{large}}$, respectively, for the final evaluation on private test set. As shown in Table \ref{result_private}, the ConvS2S{\tiny~}+{\tiny~}ViT-B/16{\tiny~}+{\tiny~}OFA$_{\mathrm{large}}$ model obtained the better result, which is 0.4210 by F1 and 0.3482 by BLEU, and ranked $3^{rd}$ in the challenge. Table \ref{ranking} shows the final standing at the EVLSP-EVJVQA competition, in which our best model perform poorer 1.82\% and 1.39\% by F1 compared with the first and second place solutions. Overall, there is a gap between F1 and BLEU scores.



\begin{table}[H]
    \centering
    \small
    %\resizebox{\columnwidth}{!}{%
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcc}
    \toprule
        \textbf{Model} & \textbf{F1} & \textbf{BLEU}  \\ \midrule
        ConvS2S + ViT-B/16 + ViLT-B/32 &0.4053  &0.3228  \\
        \textbf{ConvS2S + ViT-B/16 + OFA$_{\mathbf{large}}$} & \textbf{0.4210}  & \textbf{0.3482}
  \\ \bottomrule
    \end{tabular}
    \caption{Performance on the private test set.}
    \label{result_private}
\end{table}

\begin{table}[!htbp]
\small
%\resizebox{\columnwidth}{!}{%
\centering
\begin{tabular}{clccccc}
\toprule
\multirow{2}{*}{\textbf{No.}} & \multirow{2}{*}{\textbf{Team name}} & \multicolumn{2}{c}{\textbf{Public Test}} && \multicolumn{2}{c}{\textbf{Private Test}} \\\cmidrule{3-4} \cmidrule{6-7}
                             &                                     & \textbf{F1}         & \textbf{BLEU}      && \textbf{F1}         & \textbf{BLEU}       \\\midrule
1                            & CIST AI                             & 0.3491              & 0.2508             && 0.4392              & 0.4009              \\
2                            & OhYeah                              & 0.5755              & 0.4866             && 0.4349              & 0.3868              \\
3                            & \textbf{DS\_STBFL}                  & \textbf{0.3390}     & \textbf{0.2156}    && \textbf{0.4210}     & \textbf{0.3482}     \\
4                            & FCoin                               & 0.3355              & 0.2437             && 0.4103              & 0.3549              \\
5                            & VL-UIT                              & 0.3053              & 0.1878             && 0.3663              & 0.2743              \\
6                            & BDboi                               & 0.3023              & 0.2183             && 0.3164              & 0.2649              \\
7                            & UIT\_squad                          & 0.3224              & 0.2238             && 0.3024              & 0.1667              \\
8                            & VC\_Internship                      & 0.3017              & 0.1639             && 0.3007              & 0.1337
\\\bottomrule       
\end{tabular}
\caption{Our performance compared with other teams at VLSP2022-EVJVQA}
\label{ranking}
\end{table}

\subsection{Performance Analysis}

According to the final result in the private test phase, the generated output from ConvS2S
+ViT-B/16+OFA$_{\mathrm{large}}$ model are chosen for further analysis. Generally, the model manages to generate answers with correct language with the input question.
\subsubsection{Quantitative analysis}
We randomly choose 100 samples from the generated result to perform quantitative analysis. The average length, vocabulary size, and the number of POS tags in the ground truth and generated answers are calculated for each language. Table \ref{quanti} shows the statistics of the ground truth answer compared with the predicted answer by the model.

% \begin{table}[ht]
% \centering
% %\resizebox{\columnwidth}{!}{%
% \begin{tabular}{llrr}
% \toprule
% &Language&Ground Truth&Predicted\\\midrule

% \multirow{ 4}{*}{Avg. length} & English & 3.74 & 6.18 \\
% & Vietnamese & 4.42 & 5.97\\
% & Japanese & 4.67 & 8.43\\
% & All &4.26&6.78\\\midrule

% \multirow{ 4}{*}{Vocab. size} & English & 78 & 72 \\
% & Vietnamese & 97 & 101\\
% & Japanese & 77 & 83\\
% & All &252&256\\\midrule

% \multirow{ 4}{*}{\# POS tag} & English & 12 & 9 \\
% & Vietnamese &10  &9 \\
% & Japanese & 10 & 11\\
% & All &14 &14\\
% \bottomrule
% \end{tabular}
% \caption{The quantitative statistic of 100 generated samples compared with the ground truth}
% \label{quanti}
% \end{table}

\begin{table}[ht]
\centering
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{llrr}
\toprule
Language&Stats.&Ground Truth&Predicted\\\midrule
\multirow{ 3}{*}{English} & Avg.length  & 3.74 & 6.18 \\
& Vocab. size & 78 & 72 \\
& \# POS tag  & 12 & 9 \\\midrule

\multirow{ 3}{*}{Vietnamese} & Avg.length  & 4.42 & 5.97 \\
& Vocab. size  & 97 & 101 \\
& \# POS tag &10  &9 \\\midrule

\multirow{ 3}{*}{Japanese} & Avg.length   & 4.67 & 8.43 \\
& Vocab. size & 77 & 83 \\
& \# POS tag  & 10 & 11 \\\midrule\midrule

\multirow{ 3}{*}{All} & Avg.length  &4.26 &6.78 \\
& Vocab. size &252 &256 \\
& \# POS tag  &14 &14 \\

\bottomrule
\end{tabular}
\caption{The quantitative statistic of 100 generated samples compared with the ground truth}
\label{quanti}
\end{table}

From Table \ref{quanti}, it can be seen that although the model gave the answers longer than the ground truth answers, the semantics is not as much as the ground truth. It can be seen from Table \ref{quanti} that the predicted answers in English have an average length higher than the ground truth answers. Also, the vocabulary in the generated answers is more than the original. In contrast, the number of POS tag components in the predicted answers is lower than the ground truth. This is similar to the answers in Vietnamese. For the Japanese, the characteristics of the predicted answers in average length and vocabulary size are the same as the two remaining languages. However, the number of POS tags in the predicted answers is more than in the ground truth answers. To make it clear, we propose three types of error on our model in Section \ref{quali_analysis}.

In addition, Figure \ref{100score} illustrates the distributions of F1 and BLEU scores for each language. Generally, the histograms skewed to the right and the model  performs inconsistently across languages. The proportion of samples with F1 and BLEU scores less than 0.2 dominates the overall result across all three languages. In Vietnamese, the number of generated samples with F1 and BLEU scores greater than 0.4 is significantly higher than in other languages. Meanwhile, English and Japanese responses rarely score greater than 0.6 on both metrics, furthermore, no Japanese samples scoring greater than 0.8 in BLEU. This illustrates that our model faces numerous challenges in producing the desired responses, with specific limitations on each language.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figure/hist.pdf}
    \caption{Distributions of F1 and BLEU scores for each language from 100 generated samples}
    \label{100score}
\end{figure}

\begin{figure}[!htbp]
\centering
\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns1.pdf}%
  \label{attn1}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns2.pdf}%
  \label{attn2}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns3.pdf}%
  \label{attn3}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns4.pdf}%
  \label{attn4}
}

\subfloat[]{%
  \includegraphics[width=0.8\textwidth]{figure/attns5.pdf}%
  \label{attn5}
}
\caption{Numerous samples of attention alignment from ConvS2S and the changes in attention when adding features from ViT-B/16 and OFA$_{\mathrm{large}}$. The x-axis and y-axis of each plot correspond to the words in the question and the generated answer, respectively, while each pixel illustrates the weight $w_{ij}$ of the assignment of the j-th question word for the i-th
answer word.}
\label{attn}
\end{figure}

\subsubsection{Qualitative analysis}
\label{quali_analysis}
\paragraph{Attention visualization}

Figure \ref{attn} shows several samples of attention weights between each element from the generated answer with those in the input sequence that contains no image features, OFA hints, and OFA+ViT features, respectively. The visualization provided an intuitive way to discover which positions in the input sequence were considered more important when generating the target answer word. The brighter a pixel's color, the more important the word in the input sequence is in producing the respect answer word. Through this, we study that OFA hint is importance feature to model's attention as it provide the near-correct insight for the question and reduce the reliance on question words when generating the answer. However, in some cases, the model focuses too much on a specific element of the hint, which may lead to bias. ViT features has shown to control the affection of OFA hint, neutralizing it with other elements from question if hint appears to be off-topic. It may enhance the attention, making the model focus stronger on specific parts of the provided hint, for instance, the hint token ``nhà hàng'' (\textit{restaurant}) in Figure \ref{attn3} is given more attention when adding ViT image features. These features can also reduce the attention in one element and distributes concentration on other parts of the sequence. Figures \ref{attn1} and \ref{attn2} depict the reduction in hint attention into question elements, while Figures \ref{attn4} and \ref{attn5} show the changes in attention weight distribution among hint tokens.

\paragraph{Error analysis}
\begin{figure}[!ht]
\centering
\subfloat[Error Case I]{%
  \includegraphics[width=\textwidth]{figure/err1.pdf}%
\label{fig:1a}}
\vspace{1em}
\subfloat[Error Case II]{%
  \includegraphics[width=\textwidth]{figure/err2.pdf}%
    \label{fig:1b}
}
\vspace{1em}
\subfloat[Error Case III]{%
  \includegraphics[width=\textwidth]{figure/err3.pdf}%
\label{fig:1c}}
\caption{Three typical error cases from generated results.}
\label{fig:1}
\end{figure}

% \begin{figure}[H]
% \centering
% \resizebox{\textwidth}{!}{
%     \begin{subfigure}[b]{.3\linewidth}
%     \centering
%     \includegraphics[width=0.99\textwidth]{figure/00000001682.jpg}
%     \raggedright
%     { \scriptsize \textbf{Question}: what hat does the narrator of the 
%     historical site wear?}\\
%     {\scriptsize \textbf{Groundtruth}: non la}\\
%     {\scriptsize \textbf{Predicted}: the boy wears a white shirt and white and white}\\
%     {\scriptsize \textbf{F1:}  0.0000}\\
%     {\scriptsize \textbf{BLEU:} 0.0000
%     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ }
%     \caption{Error Type I}
%     \label{fig:1a}
%   \end{subfigure}%
%   \hspace{0.5em}
  
%  %\hspace*{\fill}
%   \begin{subfigure}[b]{.35\linewidth}
%     \centering
%     \includegraphics[width=0.99\textwidth]{figure/00000004737.jpg}
%     \raggedright
    
%     {\scriptsize \textbf{Question}: có bao nhiêu người đứng bên phải chàng trai? (\textit{English: How many people on the right of the man?})}\\
    
%     {\scriptsize \textbf{Groundtruth}: có ba người đứng bên phải chàng trai (\textit{English: There are three people on the right of the man})}\\
    
%     {\scriptsize \textbf{Predicted}: có hai người đứng bên phải chàng trai (\textit{English: There are two people on the right of the man})}\\
    
%     {\scriptsize F1:  0.8750}\\
    
%     {\scriptsize BLEU: 0.7799}
%     \caption{Error Type II}
%     \label{fig:1b}
%   \end{subfigure}%
%   \hspace{0.5em}
%   %\hspace*{\fill}
%   \begin{subfigure}[b]{0.35\linewidth}
%      \centering
%     \includegraphics[width=0.99\textwidth]{figure/00000000111.jpg}
    
%     \raggedright {\scriptsize \textbf{Question}:}
%     {\tiny
%     \begin{CJK*}{UTF8}{min}
%     {\CJKfamily{goth}小船手は何本のオールを使っていますか? (\scriptsize \textit{English: How many paddles does the boatman use?})}
%     \end{CJK*}}\\
%     {\scriptsize \textbf{Groundtruth}: 2}\\
%     {\scriptsize \textbf{Predicted}:}
%     {\tiny
%     \begin{CJK*}{UTF8}{min}
%     {\CJKfamily{goth}2本の船を使っています (\scriptsize \textit{ English: using two boats})}
%     \end{CJK*}}\\
%     {\scriptsize \textbf{F1:} 0.0000}\\
%     {\scriptsize \textbf{BLEU:} 0.0000 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ }\\
%     \caption{Error Type III}
%     \label{fig:1c}
%   \end{subfigure}%  
% }
%   \caption{Example of generated answers that contains errors.(b) the keyword 'hai người' (two people) is given  instead of 'ba người' (three people). Coincidentally, the question and groundtruth in this case both share the same phrase "đứng bên phải chàng trai" ("on the right of the man"), }\label{fig:1}
% \end{figure}


For better understand the generation performance on the VQA task, we examine the generated answers of our best ensemble, ConvS2S
+ViT-B/16+OFA$_{\mathrm{large}}$, to identify the limitations and analyze factors that may cause the model to perform poorly.
Through the error analysis process, various errors and mistakes have been pointed out in the outputs of the model. The typical examples of various types of errors are illustrated in Figure \ref{fig:1}. In summary, we divide these errors into three groups:

\begin{itemize}
    \item The generated answer does not match the question and has no correct tokens compared with the ground truth answer, as shown in Figure \ref{fig:1a}. This error case sometimes accompanied by text degeneration.
    \item The generated response gives the wrong answer to the question but share some insignificant tokens with the ground truth answer, as shown in Figure \ref{fig:1b}, which significantly improves the evaluation score. This incorrect scenario exemplifies the limitation of the evaluation measures.
    \item The model managed to generate the correct key answer while also adding unnecessary information compared to the ground truth, which may lead to the response's meaning being distorted. 
    As shown in Figure \ref{fig:1c}, the model correctly predicted quantity but then added unnecessary tokens afterward, resulting in a low score on both evaluation metrics.
\end{itemize}


% \begin{figure*}[h]
% \centering
%   \begin{tabular}{@{}ccc@{}}
%     \includegraphics[width=0.3\textwidth]{figure/5.3_ex/00000000111.jpg}
%     \includegraphics[width=0.3\textwidth]{example-image-b} &
%     \includegraphics[width=0.3\textwidth]{example-image-b} \\
%   \end{tabular}
%   \caption{This is some figure side by side}
% \end{figure*}

