@InProceedings{pmlr-v139-kim21k,
  title = 	 {ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author =       {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5583--5594},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21k/kim21k.pdf},
  url = 	 {http://proceedings.mlr.press/v139/kim21k.html},
}

@article{wang2022ofa,
  author    = {Peng Wang and
               An Yang and
               Rui Men and
               Junyang Lin and
               Shuai Bai and
               Zhikang Li and
               Jianxin Ma and
               Chang Zhou and
               Jingren Zhou and
               Hongxia Yang},
  title     = {OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence
               Learning Framework},
  journal   = {CoRR},
  volume    = {abs/2202.03052},
  year      = {2022}
}

@article{hudson2018gqa,
    title={GQA: A New Dataset for Real-World Visual Reasoning 
    and Compositional Question Answering},
    author={Hudson, Drew A and Manning, Christopher D},
    journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2019}
}

@article{DBLP:journals/corr/DauphinFAG16,
  author    = {Yann N. Dauphin and
               Angela Fan and
               Michael Auli and
               David Grangier},
  title     = {Language Modeling with Gated Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1612.08083},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.08083},
  eprinttype = {arXiv},
  eprint    = {1612.08083},
  timestamp = {Mon, 13 Aug 2018 16:47:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DauphinFAG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1007/s11263-016-0981-7,
author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
title = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
year = {2017},
issue_date = {May 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {123},
number = {1},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-016-0981-7},
doi = {10.1007/s11263-016-0981-7},
abstract = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that "the person is riding a horse-drawn carriage." In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of $$35$$35 objects, $$26$$26 attributes, and $$21$$21 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.},
journal = {Int. J. Comput. Vision},
month = {may},
pages = {32–73},
numpages = {42},
keywords = {Question answering, Crowdsourcing, Knowledge, Image, Scene graph, Computer vision, Objects, Attributes, Language, Relationships, Dataset}
}

@InProceedings{Goyal_2017_CVPR,
author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
title = {Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6904–6913},
year = {2017}
}


@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@inproceedings{10.5555/3305381.3305510,
author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
title = {Convolutional Sequence to Sequence Learning},
year = {2017},
publisher = {JMLR.org},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1243–1252},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}

@inproceedings{nguyen2021trankit,
   title={Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing},
   author={Nguyen, Minh Van and Lai, Viet and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu},
   booktitle="Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
   year={2021}
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",

}

@article{10.1145/3560260,
author = {Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
title = {QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3560260},
doi = {10.1145/3560260},
journal = {ACM Comput. Surv.},
month = {sep},
keywords = {reading comprehension, natural language understanding}
}

@inproceedings{dzendzik-etal-2021-english,
    title = "{E}nglish Machine Reading Comprehension Datasets: A Survey",
    author = "Dzendzik, Daria  and
      Foster, Jennifer  and
      Vogel, Carl",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.693",
    doi = "10.18653/v1/2021.emnlp-main.693",
    pages = "8784--8804",
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{10.5555/2969442.2969496,
author = {Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
title = {Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2296–2304},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@inproceedings{shimizu-etal-2018-visual,
    title = "Visual Question Answering Dataset for Bilingual Image Understanding: A Study of Cross-Lingual Transfer Using Attention Maps",
    author = "Shimizu, Nobuyuki  and
      Rong, Na  and
      Miyazaki, Takashi",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1163",
    pages = "1918--1928",
}

@inproceedings{tran-etal-2021-vivqa,
    title = "{V}i{VQA}: {V}ietnamese Visual Question Answering",
    author = "Tran, Khanh Quoc  and
      Nguyen, An Trong  and
      Le, An Tran-Hoai  and
      Nguyen, Kiet Van",
    booktitle = "Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation",
    month = "11",
    year = "2021",
    address = "Shanghai, China",
    publisher = "Association for Computational Lingustics",
    url = "https://aclanthology.org/2021.paclic-1.72",
    pages = "683--691",
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{he-etal-2018-dureader,
    title = "{D}u{R}eader: a {C}hinese Machine Reading Comprehension Dataset from Real-world Applications",
    author = "He, Wei  and
      Liu, Kai  and
      Liu, Jing  and
      Lyu, Yajuan  and
      Zhao, Shiqi  and
      Xiao, Xinyan  and
      Liu, Yuan  and
      Wang, Yizhong  and
      Wu, Hua  and
      She, Qiaoqiao  and
      Liu, Xuan  and
      Wu, Tian  and
      Wang, Haifeng",
    booktitle = "Proceedings of the Workshop on Machine Reading for Question Answering",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2605",
    doi = "10.18653/v1/W18-2605",
    pages = "37--46",
}

@article{so2022jaquad,
  title={Jaquad: Japanese question answering dataset for machine reading comprehension},
  author={So, ByungHoon and Byun, Kyuhong and Kang, Kyungwon and Cho, Seongjin},
  journal={arXiv preprint arXiv:2202.01764},
  year={2022}
}

@article{lim2019korquad1,
  title={Korquad1. 0: Korean qa dataset for machine reading comprehension},
  author={Lim, Seungyoung and Kim, Myungji and Lee, Jooyoul},
  journal={arXiv preprint arXiv:1909.07005},
  year={2019}
}

@inproceedings{nguyen-etal-2020-vietnamese,
    title = "A {V}ietnamese Dataset for Evaluating Machine Reading Comprehension",
    author = "Nguyen, Kiet  and
      Nguyen, Vu  and
      Nguyen, Anh  and
      Nguyen, Ngan",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.233",
    doi = "10.18653/v1/2020.coling-main.233",
    pages = "2595--2605",
}

@article{van2022vlsp,
    title = {{VLSP 2021 - ViMRC Challenge: Vietnamese Machine Reading Comprehension}},
    author = "Nguyen Van Kiet and Tran Quoc Son and Nguyen Thanh Luan and Huynh Van Tin and Luu T. Son and Nguyen Luu-Thuy Ngan",
    journal={VNU Journal of Science: Computer Science and Communication Engineering},
    year={2022},
    volume={38},
    number={2}
}


@InProceedings{{VQA},
    author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
    title = {{VQA}: {V}isual {Q}uestion {A}nswering},
    booktitle = {International Conference on Computer Vision (ICCV)},
    year = {2015},
} 

@INPROCEEDINGS{8296600,  author={Chowdhury, Iqbal and Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},   title={A cascaded long short-term memory (LSTM) driven generic visual question answering (VQA)},   year={2017},  volume={},  number={},  pages={1842-1846},  doi={10.1109/ICIP.2017.8296600}}

@article{rogers-etal-2020-primer,
    title = "A Primer in {BERT}ology: What We Know About How {BERT} Works",
    author = "Rogers, Anna  and
      Kovaleva, Olga  and
      Rumshisky, Anna",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.54",
    doi = "10.1162/tacl_a_00349",
    pages = "842--866",
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}
@InProceedings{Antol_2015_ICCV,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C. Lawrence and Parikh, Devi},
title = {VQA: Visual Question Answering},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@article{vlsp2022,
  title={{VLSP 2022 - EVJVQA Challenge}: Multilingual Visual Question Answering},
  author={Ngan Luu-Thuy Nguyen and Nghia Hieu Nguyen and Duong T. D. Vo and Khanh Quoc Tran and Kiet Van Nguyen},
  journal={Journal of Computer Science and Cybernetics},
  year={2023}
}
