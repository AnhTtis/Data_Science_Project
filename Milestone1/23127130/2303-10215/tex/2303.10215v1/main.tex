\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}

\title{\LARGE Statistical inference for association studies in the presence of binary outcome misclassification}

\author[]{Kimberly A. Hochstedler}
\author[]{Martin T. Wells}
\affil[]{Department of Statistics and Data Science, Cornell University}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage[paperwidth=8.5in,paperheight=11.0in,top=1in, bottom=1in, left=1in, right=1in,lines=25]{geometry}
\usepackage[title,toc,titletoc,page]{appendix}

\usepackage{siunitx}
\usepackage{appendix}
\usepackage{mathtools}
\usepackage{threeparttable}
\usepackage{bbm}
\usepackage{caption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{url}

\usepackage{setspace}
\doublespacing
\setstretch{1.8}

%%% Macros
\DeclareMathOperator{\logit}{logit}
\usepackage{stix}

\begin{document}

\maketitle

\begin{abstract}
In biomedical and public health association studies, binary outcome variables may be subject to misclassification, resulting in substantial bias in effect estimates. The feasibility of addressing binary outcome misclassification in regression models is often hindered by model identifiability issues. In this paper, we characterize the identifiability problems in this class of models as a specific case of ``label switching'' and leverage a pattern in the resulting parameter estimates to solve the permutation invariance of the complete data log-likelihood. Our proposed algorithm in binary outcome misclassification models \textit{does not require gold standard labels} and relies only on the assumption that outcomes are correctly classified at least 50\% of the time. A label switching correction is applied within estimation methods to recover unbiased effect estimates and to estimate misclassification rates in cases with one or more sequential observed outcomes. Open source software is provided to implement the proposed methods for single- and two-stage models. We give a detailed simulation study for our proposed methodology and apply these methods to data for single-stage modeling of the Medical Expenditure Panel Survey (MEPS) from 2020 and two-stage modeling of data from the Virginia Department of Criminal Justice Services.

\textbf{Keywords: } association studies, bias correction, EM algorithm, identification, label switching, MCMC, noisy labels, sensitivity, specificity
\end{abstract}

\newpage

%%%%%

\section{Introduction}
We consider regression models where a binary outcome variable is potentially misclassified. Misclassified binary outcomes are common in biomedical and public health association studies. For example, misclassification may occur in when a diagnostic test does not have perfect sensitivity or specificity \citep{manski2021estimating, ge2023enhanced, trangucci2022identified}. Misclassification can also be present in survey data, where individuals may falsely recall disease status on a self-report item \citep{althubaiti2016information}. More recently, medical studies may rely on computer algorithms to extract patient disease status from electronic medical records, but such algorithms do not perfectly capture true disease states, even when combined with record review from subject-area experts \citep{sinnott2014improving}. It is common for analysts to ignore potential misclassification in response variables, and instead assume that the outcome is perfectly measured. Such an approach generally produces biased parameter estimates \citep{khan2020introduction}, particularly in the case of covariate-related misclassification \citep{beesley2020statistical, zhang2020genetic}. 

Despite the known impact of covariate-related misclassification, previous work on recovering unbiased association parameters is limited. \cite{neuhaus1999bias} provides general expressions for bias in the presence of covariate-related misclassification. \cite{lyles2011validation} and \cite{lyles2010sensitivity} extend this work, but require a validation sample or known sensitivity and specificity values, respectively, to identify association parameters.  \cite{beesley2020statistical} address the problem through a novel likelihood-based bias correction strategy, but make the strong assumption that the binary outcome is measured with perfect specificity. A perfect sensitivity assumption also underpins existing sensitivity analysis frameworks for evaluating the predictive bias of classifiers in the presence of outcome misclassification \citep{fogliato2020fairness}. \cite{zhang2020genetic} develop methods to correct bias in association parameters in the context of covariate-related misclassification, but they only address the problem for mixed continuous and binary bivariate outcomes. Our methods consider scenarios where validation data is not available, but the binary outcome is subject to covariate-dependent, bidirectional misclassification.

Numerous researchers instead assume that sensitivity and specificity may be considered constant \citep{magder1997logistic, daniel2003binomial, trangucci2022identified, carroll2006measurement, stamey2004parameter, xia2018bayesian, rekaya2016analysis}. In such cases, misclassification rates are often either assumed to be known \citep{magder1997logistic, trangucci2022identified} or estimated via validation data \citep{stamey2004parameter, carroll2006measurement}. In the event that an outcome measure has previously studied sensitivity and specificity rates, as is common for commercially available diagnostic tests, \cite{magder1997logistic} propose an EM algorithm to incorporate that information into the fitting of logistic regression models, resulting in unbiased estimates of odds ratios. For cases where internal validation data are present, \cite{carroll2006measurement} develop general expressions for likelihood functions that take imperfect outcome measurement into account. These methods rely on the availability of gold standard measures, which are not feasible in numerous biomedical and public health settings \citep{Faraone1994measuring, oneill2015measuring}. In contrast, fully Bayesian methods may be used to estimate constant misclassification rates in the absence of a gold standard, but such methods rely on strict and difficult prior elicitation strategies \citep{daniel2003binomial, rekaya2016analysis} or an additional assumption of equal sensitivity and specificity \citep{smith2013genome, rekaya2001threshold}.

%Other methods for correcting outcome misclassification rely on available gold standard measures \citep{stamey2004parameter} or repeated measures \citep{albert1997modeling}. For cases where internal validation data are present, \cite{carroll2006measurement} develop general expressions for likelihood functions that take imperfect outcome measurement into account. In the event that an outcome measure, like a diagnostic test, has known sensitivity and specificity rates, \cite{magder1997logistic} propose an EM algorithm to incorporate that information into the fitting of logistic regression models, resulting in unbiased estimates of odds ratios. 

%\cite{magder1997logistic} also propose using their method as a sensitivity analysis, as a way to estimate the degree to which misclassification may bias one's parameter estimates. \cite{fogliato2020fairness} use a similar sensitivity analysis framework when considering outcome misclassification in the context of fairness evaluation, but the authors assume perfect specificity in their methods. 

The impact of misclassification has also been considered for estimates of outcome prevalence. Econometricians have used partial identifiability to address misclassification and estimate population rates of SARS CoV-2 infection \citep{ziegler2020binary, manski2021estimating}, but these methods do not readily extend to association studies. Misclassification can also impact variance estimation in prevalence studies. \cite{ge2023enhanced} derive an estimable variance component that is induced by misclassification in certain testing applications, but their methods also rely on known sensitivity and specificity rates. 

In the machine learning literature, the problem of outcome misclassification, or ``noisy labels'', is typically handled with noise-robust methods or data cleaning strategies \citep{frenay2013classification}. Other methods approach misclassification from a fairness lens, where an optimization approach is proposed to ``flip'' predicted labels after estimation to alleviate systematic bias without sacrificing the value of ``merit'' within the classification scheme \citep{bandi2021price}. When imperfect classification algorithms are used to obtain disease states, \cite{sinnott2014improving} demonstrate that modeling outcome probabilities, rather than outcomes obtained via thresholding, improves power and estimation accuracy. 

In this paper, we develop new strategies to recover unbiased parameter estimates in association studies with outcome misclassification that is related to observed covariates. We propose a bias correction strategy that requires minimal external information, no gold standard labels, and no limitations on the misclassification patterns present in the data. We also extend these methods to settings with multiple stages of dependent noisy outcomes.

The feasibility of addressing outcome misclassification in association studies is often limited by model identifiability issues \cite{lyles2011validation}. \cite{xia2018bayesian} outlines cases of unidirectional outcome misclassification where key regression parameters are identifiable. In many practical settings, however, they find that some covariate terms can only be weakly identified. Numerical problems related to weak identifiability are also encountered the joint estimation methods by \cite{beesley2020statistical}. These authors address the problem by fixing model parameters at known values.

Our approach is to characterize the model identfiability and estimation issues in a binary outcome misclassification model as a specific case of ``label switching''. Label switching describes the invariance of the likelihood under relabeling of the mixture components, resulting in multimodal likelihood functions \citep{redner1984mixture}. Several remedies have been suggested to break the permutation invariance of the likelihood in both Bayesian and frequentist mixture models \citep{rodriguez2014label, yao2015label}. One common method is to impose ordering constraints on component model parameters, such as $\pi_{1} < \pi_2$ \citep{betancourt2017identifying}. Such strategies aim to remove the permutation invariance of the likelihood, but are only successful for carefully chosen constraints \citep{stephens2000dealing}. In Bayesian settings, one potential resolution of the labeling degeneracy is to use non-exchangeable prior distributions to strongly separate possible parameter sets \citep{betancourt2017identifying}. It is rare, however, that analysts have enough information to set useful ordering constraints or strongly separated prior distributions in practice \citep{rodriguez2014label}. Moreover, when prior distributions for sensitivity and specificity are misspecified, bias correction approaches tend to perform poorly \citep{ni2019comparing, daniel2003binomial}. As such, we develop a novel label switching remedy that relies only on the reasonable assumption that correct outcome classification occurs in at least $50\%$ of observations.

We use our label switching correction procedure to develop both frequentist and Bayesian estimation methods for regression parameters in an association study. This strategy also allows us to accurately estimate the average misclassification rates in the response variable and characterize the mechanism by which misclassification occurs.

In Section \ref{model}, we describe the conceptual framework for our model. Section \ref{label-switching} describes the label switching problem in greater detail and proposes a strategy to break permutation invariance of the likelihood in a binary outcome misclassification model. In Section \ref{estimation-methods}, we propose frequentist and Bayesian estimation strategies to recover the true association of interest in the presence of potentially misclassified observed outcomes. For all strategies proposed, we provide software for implementation using the R package \textit{COMBO} (COrrecting Misclassified Binary Outcomes). In Section \ref{multistage-model}, we extend our methods to a framework where multiple potentially misclassified outcomes are measured sequentially. Through simulation, we demonstrate the utility of these methods to reduce bias in parameter estimates when compared to analyses that place restrictions on or ignore outcome misclassification. Finally, we apply our proposed methods to investigate two applied examples. In a single-stage model, we study risk factors for myocardial infarction, which is known to be misdiagnosed on the basis of gender and age \citep{arber2006patient, maserejian2009disparities, mckinlay1996non}. In a two-stage model, we investigate the association between criminal history variables and the risk of recidivism, which is often predicted using fallible risk assessment tools and judge decisions \citep{desmarais2013risk, johndrow2019algorithm, fogliato2020fairness}. 

\section{Model, Notation, and Conceptual Framework} \label{model}
Let $Y = j$ denote an observation's true outcome status, taking values $j \in \{1, 2\}$. Suppose we are interested in the relationship between $Y$ and a set of predictors, $X$, that are correctly measured. This relationship constitutes the \textit{true outcome mechanism}. Let $Y^* = k$ be the observed outcome status, taking values $k \in \{1,2\}$. $Y^*$ is a potentially misclassified version of $Y$. Let $Z$ denote a set of predictors related to sensitivity and specificity. The mechanism that generates the observed outcome, $Y^*$, given the true outcome, $Y$, is called the \textit{observation mechanism}. Figure \ref{conceptual_framework_figure} displays the conceptual model. The conceptual process is mathemathically expressed as
\begin{equation}
\begin{aligned}
\label{eq:conceptual_framework_eq}
\text{True outcome mechanism: } &\; \logit\{ P(Y = j | X ; \beta) \} = \beta_{j0} + \beta_{jX} X \\
\text{Observation mechanism: } &\; \logit\{ P(Y^* = k | Y = j, Z ; \gamma) \} = \gamma_{kj0} + \gamma_{kjZ} Z.
\end{aligned}
\end{equation}

In the true outcome mechanism, we use category $Y = 2$ as the reference category, and set all corresponding $\beta$ parameters to 0. Similarly, $Y^* = 2$ is the reference category in the observation mechanism, and all corresponding $\gamma$ parameters are also set to 0. Using (\ref{eq:conceptual_framework_eq}), we can express response probabilities for individual $i$'s true outcome category and for individual $i$'s observed category, conditional on the true outcome: 
\begin{flalign}
\begin{aligned}
\label{eq:response_probabilities_eq}
P(Y_i = j | X ; \beta) = &\; \; \pi_{ij} = \frac{\text{exp}\{\beta_{j0} + \beta_{jX} X_i\}}{1 + \text{exp}\{\beta_{j0} + \beta_{jX} X_i\}} \\
P(Y^*_i = k | Y_i = j, Z ; \gamma) = &\; \pi^*_{ikj} = \frac{\text{exp}\{\gamma_{kj0} + \gamma_{kjZ} Z_i\}}{1 + \text{exp}\{\gamma_{kj0} + \gamma_{kjZ} Z_i\}}.
\end{aligned}
\end{flalign}

For $j$ and $k$ both equal to the reference category, $\pi^*_{22}$ measures the specificity in the data. When $j$ and $k$ are both $1$, $\pi^*_{11}$ measures the sensitivity. Thus, (\ref{eq:response_probabilities_eq}) allows us to model sensitivity and specificity based on a set of covariates, $Z$.
It is common for analysts to ignore potential misclassification in $Y^*$, and instead to use a naive \textit{analysis model} $Y^* | X$ and interpret the results under the \textit{true outcome model} $Y | X$. Previous work has shown that this approach produces bias in $P(Y^* = j | X)$ relative to $P(Y = j | X)$, particularly in the case of covariate-related misclassification \citep{beesley2020statistical}. 

We define the probability of observing outcome $k$ using the model structure as
\begin{equation}
\begin{aligned}
\label{eq:p_obs_Ystar}
P(Y^* = k | X, Z) = \sum_{j = 1}^2 P(Y^* = k | Y = j, Z ; \gamma) P(Y = j | X ; \beta) = \sum_{j = 1}^2 \pi^*_{kj} \pi_{j}.
\end{aligned}
\end{equation}

The contribution to the likelihood by a single subject $i$ is thus $\prod_{k = 1}^2 P(Y^*_i = k | X_i, Z_i)^{y^*_{ik}}$ where $y^*_{ik} = \mathbbm{I}\{Y^*_i = k\}$. We can estimate $(\beta, \gamma)$ using the following observed data log-likelihood for subjects $i = 1 \dots N$ as
\begin{equation}
\begin{aligned}
\label{eq:obs-log-like}
\ell_{obs}(\beta, \gamma; X, Z) = \sum_{i = 1}^N \sum_{k = 1}^2 y^*_{ik} \text{log} \{ P(Y^*_i = k | X_i, Z_i) \} = \sum_{i = 1}^N \sum_{k = 1}^2 y^*_{ik} \text{log} \{ \sum_{j = 1}^2 \pi^*_{ikj} \pi_{ij} \}.
\end{aligned}
\end{equation}

The observed data log-likelihood is difficult to use directly for estimation because jointly maximizing $\beta$ and $\gamma$ is numerically challenging, especially for large datasets. 

Viewing the true outcome value $Y$ as a latent variable, we may also construct the complete data log-likelihood based on the model structure as
\begin{equation}
    \begin{aligned}
    \label{eq:complete-log-like}
    \ell_{complete}(\beta, \gamma; X, Z) &= \sum_{i = 1}^N \Bigg[ \sum_{j = 1}^2 y_{ij} \text{log} \{ P(Y_i = j | X_i) \} + \sum_{j = 1}^2 \sum_{k = 1}^2 y_{ij} y^*_{ik} \text{log} \{ P(Y^*_i = k | Y_i = j, Z_i) \}\Bigg] & \\
    &= \sum_{i = 1}^N \Bigg[ \sum_{j = 1}^2 y_{ij} \text{log} \{ \pi_{ij} \} + \sum_{j = 1}^2 \sum_{k = 1}^2 y_{ij} y^*_{ik} \text{log} \{ \pi^*_{ikj} \}\Bigg],
    \end{aligned}
\raisetag{12pt}\end{equation}
where $y_{ij} = \mathbbm{I}\{Y_i = j\}$. Without the true outcome value, $Y$, we cannot use this likelihood form directly for maximization. It is notable, however, that (\ref{eq:complete-log-like}) can be viewed as a mixture model with latent mixture components, $y_{ij}$, and covariate-dependent mixing proportions, $\pi_{ij}$.

\section{Label Switching} \label{label-switching}
The structure of the models described in Section \ref{model} suffers from the known problem of label switching in mixture likelihoods. Mixture likelihoods are \textit{invariant under relabeling of the mixture components}, resulting in multimodal likelihood functions \citep{redner1984mixture}. Specifically, a $J$-dimensional mixture model will have $J!$ modes in the likelihood \citep{betancourt2017identifying}. Given that our proposed model in (\ref{eq:complete-log-like}) is a mixture with $J = 2$ components labeled by the true outcome $Y \in \{1,2\}$, there are $J! = 2! = 2$ peaks in the likelihood resulting in $2$ plausible parameter sets. 

\subsection{Permutation Invariance of the Complete Data Likelihood} \label{permutation-invariance-complete}
The invariance of the complete data log-likelihood under relabeling of mixture components is displayed in (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}). In (\ref{eq:permute-invariant-likelihood-1}), we label $Y$ terms in the order of appearance as either $1$ or $2$. In (\ref{eq:permute-invariant-likelihood-2}), all terms that had $Y = 1$ are replaced with that of $Y = 2$ and all terms that had $Y = 2$ are replaced with $Y = 1$. It follows that,
\begin{equation}
\label{eq:permute-invariant-likelihood-1}
    \begin{aligned}
  \ell_{complete}(\beta, \gamma; X, Z) = &\sum_{i = 1}^N \Bigl[ y_{i1} \log \{ \pi_{i1} \}  + y_{i2} \log \{ \pi_{i2} \} & \\
  &\phantom{a}+ y_{i1} y^*_{i1} \log \{ \pi^*_{i11} \}  + y_{i1} y^*_{i2} \log \{ \pi^*_{i21} \}  +
   y_{i2} y^*_{i1} \log \{ \pi^*_{i12} \}  + y_{i2} y^*_{i2} \log \{ \pi^*_{i22} \} \Bigr],
    \end{aligned}
\end{equation}
\begin{equation}
\label{eq:permute-invariant-likelihood-2}
    \begin{aligned}
  \ell_{complete}(\beta, \gamma; X, Z) = &\sum_{i = 1}^N \Bigl[y_{i2} \log \{ \pi_{i2} \} +  y_{i1} \log \{ \pi_{i1} \} \\
  &\phantom{a} + y_{i2} y^*_{i1} \log \{ \pi^*_{i12} \}  + y_{i2} y^*_{i2} \log \{ \pi^*_{i22} \}  +
   y_{i1} y^*_{i1} \log \{ \pi^*_{i11} \}  + y_{i1} y^*_{i2} \log \{ \pi^*_{i21} \} \Bigr].
    \end{aligned}
\end{equation}

Due to the additive structure of the complete data log-likelihood, the label change between (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}) does not impact the value of the function. Thus, the complete data log-likelihood in our setting is invariant under relabeling of mixture components. 

Suppose for each $y_{ij}$ we have a single predictor $x_i$ in the true outcome mechanism and a single predictor $z_i$ in the observation mechanism. Substituting the parametric form of the response probabilities from (\ref{eq:response_probabilities_eq}) into (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}), we can detect a pattern in the parameters associated with each likelihood mode, that is,
\begin{equation}
\label{eq:permute-invariant-param-1}
    \begin{aligned}
&\sum_{i = 1}^N \Bigl[ y_{i1}\beta_0 + y_{i1} x_i \beta_X - (y_{i1} + y_{i2}) \log \{ 1 + \exp \{ \beta_0 + x_i \beta_X \} \} \\
     &\qquad\phantom{a}+ y_{i1} y^*_{i1} \gamma_{110} + y_{i1} y^*_{i1} z_i \gamma_{11Z} - ( y^*_{i1} + y^*_{i2} ) y_{i1} \log \{1 +  \exp \{  \gamma_{110} + z_i \gamma_{11Z} \}  \} \\
     &\qquad\phantom{a}+ y_{i2} y^*_{i1} \gamma_{120} + y_{i2} y^*_{i1} z_i \gamma_{12Z} - (y^*_{i1} + y^*_{i2}) y_{i2}  \log \{ 1 +  \exp \{  \gamma_{120} + z_i \gamma_{12Z} \} \}  \Bigr] \\
 = &\sum_{i = 1}^N \Bigl[ y_{i2}(-\beta_0) + y_{i2} x_i (-\beta_X) - (y_{i1} + y_{i2}) \log \{ 1 + \exp \{ -\beta_0 + x_i (-\beta_X) \} \} \\
     &\qquad\phantom{a}+ y_{i2} y^*_{i1} \gamma_{120} + y_{i2} y^*_{i1} z_i \gamma_{12Z} - ( y^*_{i1} + y^*_{i2} ) y_{i2} \log \{1 +  \exp \{  \gamma_{120} + z_i \gamma_{12Z} \}  \} \\
     &\qquad\phantom{a}+ y_{i1} y^*_{i1} \gamma_{110} + y_{i1} y^*_{i1} z_i \gamma_{11Z} - (y^*_{i1} + y^*_{i2}) y_{i1}  \log \{ 1 +  \exp \{  \gamma_{110} + z_i \gamma_{11Z} \} \}  \Bigr].
   \end{aligned}
\end{equation}

Specifically, label switching generates the following two parameter sets:
(1) $\beta_0, \beta_X, \gamma_{110}, \\ \gamma_{11Z}, \gamma_{120}, \gamma_{12Z}$,
and
(2) $-\beta_0, -\beta_X, \gamma_{120}, \gamma_{12Z}, \gamma_{110}, \gamma_{11Z}$.
Suppose the true values of each parameter were equal to real numbers $a, b, c, d, e, f$, respectively, for parameter set 1. The two parameter sets corresponding to these values would be: (1)  $a, b, c, d, e, f$, and (2) $-a, -b, e, f, c, d$. That is, the $\beta$ parameters change signs while the $\gamma$ parameters change $j$ subscripts between the two parameter sets. This pattern is stable for any dimension of $x$ and $z$ vector predictors. In practice, if an analyst uses frequentist estimation methods, they will recover only one of the two parameter sets. When this model is fit via Gibbs Sampling, it is typical for different chains to center at different parameter sets. Moreover, each chain is generally unable to transition \textit{between} modes within a finite running time \citep{betancourt2017identifying}. Thus, it is possible for a final posterior sample to reflect estimates from different ``label switched'' parameter sets, making the results difficult to meaningfully summarize \citep{stephens2000dealing}.

\subsection{A Strategy to Correct Label Switching}
Given that the two likelihood modes present in the proposed model are known, we now describe a method to select the correct parameter set. After obtaining parameter estimates $\hat{\beta}$ and $\hat{\gamma}$ from an estimation method described in Section \ref{estimation-methods}, the appropriate parameter set, $\hat{\beta}_{corrected}$ and $\hat{\gamma}_{corrected}$, can be determined using the following algorithm.

\begin{algorithm}
\caption{Correcting label switching in binary outcome misclassification models}\label{alg:label-switch}
\begin{algorithmic}
\State Compute average $\pi^*_{jj}$ for all $j \in \{1,2\}$ using $\hat{\beta}$ and $\hat{\gamma}$.
\If{$ \pi^*_{jj} > 0.50$ for all $j \in \{1, 2 \}$} 
    \State $\hat{\beta}_{corrected} \gets \hat{\beta}$
    \State $\hat{\gamma}_{corrected} \gets \hat{\gamma}$
\Else
    \State $\hat{\beta}_{corrected} \gets -\hat{\beta}$
    \State $\hat{\gamma}_{corrected, k1} \gets \hat{\gamma_{k2}}$
    \State $\hat{\gamma}_{corrected, k2} \gets \hat{\gamma_{k1}}$
\EndIf 
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:label-switch} relies on the following assumption:
\newtheorem{assumption}{Assumption}
\begin{assumption}
The probability of correct classification is at least 0.50, on average across subjects $i$; that is, $\pi^*_{jj} > 0.50$. 
\end{assumption}

Assumption 1 is required because it allows for differentiation between the two possible parameter sets for this model. As displayed in (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}), each parameter set assigns a different category of the latent variable, $Y = 1$ or $Y = 2$, to each component of the mixture likelihood. This means that when we compute $\pi^*_{kj} = P(Y^* = k | Y = j)$ for a given observed outcome category, $k$, it is possible that $\pi^*_{kj}$ could refer to $\pi^*_{k1}$ or $\pi^*_{k2}$. In order to differentiate between these two latent variable assignments, we make the assumption that $\pi^*_{k = j, j} > \pi^*_{k \neq j, j}$ which implies $\pi^*_{k = j, j} > 0.50$. Thus, we are able to determine which latent category labeling is more plausible in our results. Assumption 1 is reasonable because it assumes that the observation mechanism is at least as good as a coin flip, on average.

Such assumptions are not required in other methods like \cite{beesley2020statistical} and \cite{xia2018bayesian} because unidirectional misclassification is assumed. Thus, only one latent outcome category is present in the complete data log-likelihood and so the resulting functional form is not permutation invariant. 

\section{Estimation Methods} \label{estimation-methods}
In this section, we describe two estimation methods for our proposed binary outcome misclassification model that exploit the latent variable structure of this problem. First, we propose jointly estimating $\beta$ and $\gamma$ using the expectation-maximization (EM) algorithm \citep{dempster1977maximum}. Next, we outline Bayesian methods for analyzing data from association studies in the presence of binary outcome misclassification. Both estimation strategies are available in the R package \textit{COMBO}. 

\subsection{Maximization Using an EM Algorithm} \label{em}
We use the complete data log-likelihood as the starting point for the EM algorithm. Since (\ref{eq:complete-log-like}) is linear in $y_{ij}$, we can replace $y_{ij}$ in the E-step of the EM algorithm with the quantity
\begin{equation}
\begin{aligned}
\label{eq:e-step}
w_{ij} = P(Y_i = j | Y_i^*, X, Z) = \sum_{k = 1}^2 \frac{y^*_{ik} \pi^*_{ikj} \pi_{ij}}{\sum_{\ell = 1}^2 \pi^*_{i k \ell} \pi_{i \ell}}.  
\end{aligned}
\end{equation}

In the M-step, we maximize the expected log-likelihood with respect to $\beta$ and $\gamma$
\begin{equation}
\begin{aligned}
\label{eq:m-step}
Q = \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w_{ij} \text{log} \{ \pi_{ij} \} + \sum_{j = 1}^2 \sum_{k = 1}^2 w_{ij} y^*_{ik} \text{log} \{ \pi^*_{ikj} \}\Bigr].
\end{aligned}
\end{equation}

The $Q$ function in (\ref{eq:m-step}) can be split into three separate equations for estimating $\beta$ and $\gamma$
%\begin{align}
%\label{eq:q-beta}
%Q_{\beta} &= \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w_{ij} \text{log} \{ \pi_{ij} \}\Bigr]\\
%\label{eq:q-gamma1}
%Q_{\gamma_{k1}} &= \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i1} y^*_{ik} \text{log} \{ \pi^*_{ik1} \}\Bigr]\\
%\label{eq:q-gamma2}
%Q_{\gamma_{k2}} &= \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i2} y^*_{ik} \text{log} \{ \pi^*_{ik2} \}\Bigr].
%\end{align}
\begin{align}
\label{eq:q-split}
Q_{\beta} = \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w_{ij} \text{log} \{ \pi_{ij} \}\Bigr], \: Q_{\gamma_{k1}} = \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i1} y^*_{ik} \text{log} \{ \pi^*_{ik1} \}\Bigr], \:
Q_{\gamma_{k2}} = \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i2} y^*_{ik} \text{log} \{ \pi^*_{ik2} \}\Bigr].
\end{align}
In practice, $Q_{\beta}$ in (\ref{eq:q-split}) can be fit as a logistic regression model where the outcome is replaced by weights, $w_{ij}$. $Q_{\gamma_{k1}}$ and $Q_{\gamma_{k2}}$ in (\ref{eq:q-split}) each are fit as weighted logistic regression models where the outcome is $y^*_{ik}$ \citep{agresti2003categorical}. After estimates for $\beta$ and $\gamma$ are obtained, Algorithm \ref{alg:label-switch} is required to
%address the problem of ``label switching'' and
return final parameter estimates. The covariance matrix for $\beta$ and $\gamma$ is obtained by inverting the expected information matrix.

\subsection{Bayesian Modeling}\label{mcmc}
Our proposed binary outcome misclassification model is: $Y^*_{ik} | \pi^*_{ik} \sim Multinomial (\pi^*_{ik}, J)$, where $\pi^*_{ik} = \sum_{j = 1}^2 \pi^*_{ikj} \pi_{ij}$ as in (\ref{eq:p_obs_Ystar}) and $J$ denotes the number of outcome categories. 

Assumptions and prior distributions for the parameters are based on input from subject-matter experts on the data the model is applied to. It is recommended that prior distributions are proper and relatively flat to ensure model identifiability without strongly influencing the posterior mean estimation. For example, in cases where non-informative priors are desired, a Uniform prior with a wide range may be selected. If an analyst has previous data suggesting a plausible estimate for a given parameter, a Normal prior distribution with a wide variance that is centered at the previous estimate may be used. In the R Package, \textit{COMBO}, analysts may select between Uniform, Normal, Double Exponential, or t prior distributions, with user-specified prior parameters. Before summarizing the results, Algorithm \ref{alg:label-switch} should be applied on \textit{each individual MCMC chain}, to address potential label switching within a given chain. Standard methods can be used to compute variance metrics. 

\section{A Multistage Model} \label{multistage-model}
The misclassification model first introduced in Section \ref{model} can be extended to a multistage framework. As before, we let $Y = j$ denote an observation's true outcome status, taking values $j \in \{1, 2\}$ and we are interested in the relationship between $Y$ and a set of predictors $X$. Instead of obtaining just one potentially misclassified measurement of $Y$, we now have $a$ sequential imperfect measurements of $Y$. Let $Y^{*(a)}$ denote the observed outcome from stage $a$ of the data generating process, taking values $k^{(a)} \in \{ 1, 2 \}$. Let $Z^{(a)}$ denote a set of predictors related to the misclassification of $Y^{*(a)}$. The mechanism that generates the observed outcome, $Y^{*(a)}$, given the true outcome, $Y$, and all earlier-stage observed outcomes, $Y^{*(a - 1)} \dots Y^{*(1)}$, is called the $a^{th}$\textit{-stage observation mechanism}. Figure \ref{multistage_conceptual_framework_figure} displays the conceptual model for a two-stage misclassification model. The conceptual process can be expressed mathematically as
\begin{equation}
\begin{aligned}
\label{eq:multistage_conceptual_framework_eq}
\text{True outcome mechanism: } &\; \logit\{ P(Y = j | X ; \beta) \} = \beta_{j0} + \beta_{jX} X \\
a^{th} \text{ Observation mechanism: } &\; \logit\{ P(Y^{*(a)} = k^{(a)} | \mathcal{Y}^{*(a - 1)}, Y = j, Z^{(a)} ; \gamma) \}  
\\ &\; = \gamma^{(a)}_{\mathscr{k}^{(a)} j0} + \gamma^{(a)}_{\mathscr{k}^{(a)}jZ^{(a)}} Z^{(a)}
\end{aligned}
\end{equation}
where $\mathscr{k}^{(a)} = \{ k^{(a)}, k^{(a - 1)}, \dots, k^{(1)} \}$ and $\mathcal{Y}^{*(a - 1)} = \{ Y^{*(a - 1)} = k^{(a - 1)}, \dots, Y^{*(1)} = k^{(1)} \}$. As in Section \ref{model}, category $2$ is the reference category for the true outcome and all observed outcomes. Individual $i$'s true outcome category is by denoted $\pi_{ij}$, as in (\ref{eq:response_probabilities_eq}). Using (\ref{eq:multistage_conceptual_framework_eq}), we can express probabilities for individual $i$'s $a^{th}$ observed category, conditional on the true outcome and all earlier-stage outcomes. 
\begin{flalign}
\begin{aligned}
\label{eq:multistage_response_probabilities_eq}
P(Y^{*(a)}_i = k^{(a)} | \mathcal{Y}^{*(a - 1)}, Y = j, Z^{(a)}; \gamma) = &\; \pi^{*(a)}_{i \mathscr{k}^{(a)} j} = \frac{\text{exp}\{ \gamma^{(a)}_{\mathscr{k}^{(a)} j0} + \gamma^{(a)}_{\mathscr{k}^{(a)}jZ^{(a)}} Z^{(a)}\}}{1 + \text{exp}\{ \gamma^{(a)}_{\mathscr{k}^{(a)} j0} + \gamma^{(a)}_{\mathscr{k}^{(a)}jZ^{(a)}} Z^{(a)}\}}.
\end{aligned}
\end{flalign}
We define the probability of observing outcomes $\mathscr{k}^{(a)}$ in stage $a$ of the model using the model structure
\begin{equation}
\begin{aligned}
\label{eq:multistage_p_obs_Ystar}
P(Y^{*(a)} = k^{(a)} )   &\;
= \sum_{j = 1}^2 \sum_{k^{(a-1)}, \dots, k^{(1)} = 1}^2 \Bigl( P(Y^{*(a)} = k^{(a)} | \mathcal{Y}^{*(a - 1)}, Y = j, Z^{(a)}, \dots, Z^{(1)}; \gamma) \\
&\qquad\phantom{a} \times P(Y^{*(a - 1)} = k^{(a - 1)} | \mathcal{Y}^{*(a - 2)}, Y = j, Z^{(a - 1)}, \dots, Z^{(1)}; \gamma) \times \dots \\
&\qquad\phantom{a} \times P(Y^{*(1)} = k^{(1)} | Y = j, Z^{(1)}; \gamma) \times P(Y = j | X ; \beta) \Bigr)
\\ & = \sum_{j = 1}^2 \sum_{k^{(a-1)}, \dots, k^{(1)} = 1}^2 \Bigl( \pi^{*(a)}_{\mathscr{k}^{(a)}j} \times \pi^{*(a - 1)}_{\mathscr{k}^{(a - 1)}j} \times \dots \times \pi^{*(1)}_{\mathscr{k}^{(1)}j} \times \pi_{ij} \Bigr).
\end{aligned}
\end{equation}
The contribution to the likelihood by a single subject $i$ is thus $\prod_{k^{(a)} = 1}^2 P(Y_i^{*(a)} = k^{(a)})^{y^{*(a)}_{i k^{(a)}}}$ where $y^{*(a)}_{i k^{(a)}} = \mathbb{I} \{ Y_i^{*(a)} = k^{(a)}\}$. We can estimate $(\beta, \gamma)$ using the following observed data log-likelihood for subjects $i = 1, \dots, N$, 
\begin{equation}
\begin{aligned}
\label{eq:multistage-obs-log-like}
\ell_{obs}(\beta, \gamma; X, Z^{(a)}, \dots, Z^{(1)}) &\;
 = \sum_{i = 1}^N  \log \Bigl\{ \prod_{b = 1}^a \bigl( \prod_{k^{(b)} = 1}^2 P(Y_i^{*(b)} = k^{(b)})^{y^{*(b)}_{i k^{(b)}}} \bigr)  \Bigr\} \\
 & = \sum_{i = 1}^N \Bigl[ \sum_{b = 1}^a y^{*(b)}_{i k^{(b)}} \sum_{k^{(b)} = 1}^2 \log \{ P(Y_i^{*(b)} = k^{(b)}) \} \Bigr] \\
 & = \sum_{i = 1}^N \Bigl[ \sum_{b = 1}^a y^{*(b)}_{i k^{(b)}} \sum_{k^{(b)} = 1}^2 \log \{ \sum_{j = 1}^2 \sum_{k^{(b-1)}, \dots, k^{(1)} = 1}^2 \bigl( \pi^{*(b)}_{\mathscr{k}^{(b)}j} \times \pi^{*(b - 1)}_{\mathscr{k}^{(b - 1)}j} \times \dots \times \pi^{*(1)}_{\mathscr{k}^{(1)}j} \times \pi_{ij} \bigr) \} \Bigr]
\end{aligned}
\end{equation}
where $\sum_{k^{(a)}, \dots, k^{(1)} = 1}^2$ is equivalent to $\sum_{k^{(a)} = 1}^2, \sum_{k^{(a - 1)} =1}^2, \dots, \sum_{k^{(1)} = 1}^2$. As in the basic model, the observed data log-likelihood is difficult to use directly for optimization. Instead, we can view the true outcome $Y$ as a latent variable, and construct the complete data log-likelihood for a multistage model as follows,
\begin{equation}
\begin{aligned}
\label{eq:multistage-complete-log-like}
\ell_{complete}(\beta, \gamma; X, Z^{(a)},\dots, Z^{(1)}) &\; = 
 \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 y_{ij} \text{log} \{ P(Y_i = j | X_i) \} &\; \\
  & + \sum_{b = 1}^a \Bigl( \sum_{j = 1}^2 \sum_{k^{(a)}, \dots, k^{(1)} = 1}^2 y_{ij} \bigl(\prod_{h = 1}^b y^{*(h)}_{ik^{(h)}} \bigr) \text{log} \{ P(Y^{*(b)}_i = k^{(b)} |\mathcal{Y}^{*(b - 1)}, Y = j, Z^{(b)}) \} \Bigr) \Bigr] \\
  & = \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 y_{ij} \text{log} \{ \pi_{ij} \} + 
  \sum_{b = 1}^a \Bigl( \sum_{j = 1}^2 \sum_{k^{(a)}, \dots, k^{(1)} = 1}^2 y_{ij} \bigl(\prod_{h = 1}^b y^{*(h)}_{ik^{(h)}} \bigr) \text{log} \{ \pi^{*(b)}_{i \mathscr{k}^{(b)} j} \} \Bigr) \Bigr].
\end{aligned}
\end{equation}
Since we do not observe the true outcome value $Y$, the complete data log-likelihood cannot be used for maximization. As in (\ref{eq:complete-log-like}), (\ref{eq:multistage-complete-log-like}) can be viewed as a mixture model with latent mixture components, $y_{ij}$, and covariate-dependent mixing proportions $\pi_{ij}$.

\subsection{Label Switching}
Since the complete data log-likelihood for the multistage model is a mixture likelihood, it is also invariant under relabeling of the mixture components, $y_{ij}$. Regardless of the number of stages in the model, $a$, there remains just two mixture components and therefore $J! = 2! = 2$ plausible parameter sets. In addition, the pattern that governs these parameter sets is identical to that described in Section \ref{permutation-invariance-complete}, the $\beta$ parameters change signs and the $\gamma$ parameters change $j$ subscripts. To correct label switching in a multistage misclassification model, an extension of Algorithm \ref{alg:label-switch} is provided in Algorithm \ref{alg:multistage-label-switch}.
\begin{algorithm}
\caption{Correcting label switching in multistage binary outcome misclassification models}\label{alg:multistage-label-switch}
\begin{algorithmic}
\State Compute average $\pi^{*(b)}_{\mathscr{k}^{(b)} = j, j} = \pi^{*(b)}_{k^{(b)} = j, k^{(b - 1)} = j, \dots, k^{(1)} = j, j}$ for all $j \in \{1,2\}$ and all $b \in \{1, \dots, a \}$ using $\hat{\beta}$ and $\hat{\gamma}$.
\If{$\pi^{*(b)}_{\mathscr{k}^{(b)} = j, j} > 0.50$ for all $j \in \{1, 2 \}$ and all $b \in \{1, \dots, a \}$} 
    \State $\hat{\beta}_{corrected} \gets \hat{\beta}$
    \State $\hat{\gamma}_{corrected} \gets \hat{\gamma}$
\Else
    \State $\hat{\beta}_{corrected} \gets -\hat{\beta}$
    \State $\hat{\gamma}^{(b)}_{corrected, \mathscr{k}^{(b)} 1} \gets \hat{\gamma}^{(b)}_{\mathscr{k}^{(b)} 2}$
    \State $\hat{\gamma}^{(b)}_{corrected, \mathscr{k}^{(b)} 2} \gets \hat{\gamma}^{(b)}_{\mathscr{k}^{(b)} 1}$
\EndIf 
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:multistage-label-switch} relies on the following assumption: 
\begin{assumption}
The probability of correct classification is at least 0.50, on average across subjects $i$ and across all model stages; that is, $\pi^{*(a)}_{\mathscr{k}^{(a)} = j, j} = \pi^{*(a)}_{k^{(a)} = j, k^{(a - 1)} = j, \dots, k^{(1)} = j, j} > 0.50$. 
\end{assumption}

Note that Algorithm \ref{alg:multistage-label-switch} uses average conditional response probabilities for all observation stages $b \in \{1, \dots, a\}$. Depending on the problem context, analysts may instead choose to implement Algorithm \ref{alg:multistage-label-switch} using average conditional response probabilities from only a subset of the model stages. This choice would create a less strict criterion for label switching, but would be simpler to implement than if all $a$ stage outcomes were considered. 

\subsection{Estimation Methods}
As in the basic model, both the EM algorithm and MCMC can be used to estimate $\beta$ and $\gamma$ in a multistage misclassification model. Both estimation strategies are available in the R Package \textit{COMBO} for a two-stage misclassification model.

\subsubsection{Maximization Using an EM Algorithm}
Because \ref{eq:multistage-complete-log-like} is linear in $y_{ij}$, we can replace $y_{ij}$ with the following quantity in the E-step of the algorithm,
\begin{equation}
\begin{aligned}
\label{eq:multistage-e-step}
w^{(a)}_{ij} = P(Y_i = j | Y_i^{*(a)}, \dots, Y_i^{*(1)}, X, Z) = \sum_{k^{(a)}, \dots, k^{(1)}= 1}^2 \frac{\pi_{ij} \bigl(  \prod_{b = 1}^a y^{*(b)}_{ik^{(b)}} \pi^{*(b)}_{i \mathscr{k}^{(b)} j} \bigr) }{\sum_{\ell = 1}^2  \pi_{i \ell}\bigl(  \prod_{b = 1}^a \pi^{*(b)}_{i \mathscr{k}^{(b)} \ell } \bigr) }.  
\end{aligned}
\end{equation}
In the M-step, we maximize the following expected log-likelihood with respect to $\beta$ and $\gamma$,
\begin{equation}
\begin{aligned}
\label{eq:multistage-m-step}
Q = \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w^{(a)}_{ij} \text{log} \{ \pi_{ij} \} + 
  \sum_{b = 1}^a \Bigl( \sum_{j = 1}^2 \sum_{k^{(a)}, \dots, k^{(1)} = 1}^2 w^{(a)}_{ij} \bigl(\prod_{h = 1}^b y^{*(h)}_{ik^{(h)}} \bigr) \text{log} \{ \pi^{*(b)}_{i \mathscr{k}^{(b)} j} \} \Bigr) \Bigr].
\end{aligned}
\end{equation}
After estimates for $\beta$ and $\gamma$ are obtained from this EM algorithm, Algorithm \ref{alg:multistage-label-switch} must be used to correct potential label switching and return final parameter estimates. The covariance matrix for $\beta$ and $\gamma$ is obtained by inverting the expected information matrix. 

\subsubsection{Bayesian Modeling}
Our proposed multistage misclassification model is defined for each of the $a$ stages in the model: $Y^{* (a)}_{ik^{(a)}} | \pi^{* (a)}_{ik^{(a)}} \sim Multinomial ( \pi^{* (a)}_{ik^{(a)}}, J)$. Here, $\pi^{* (a)}_{ik^{(a)}} = \sum_{j = 1}^2 \sum_{k^{(a - 1)}, \dots, k^{(1)} = 1}^{2} \pi^{* (a)}_{i \mathscr{k}^{(a)} j} \times \pi^{* (a - 1)}_{i \mathscr{k}^{(a - 1)} j} \times \dots \times \pi^{* (1)}_{i \mathscr{k}^{(1)} j} \times \pi_{ij}$. $J$ refers to the number of outcome categories. Prior distributions for the parameters should be determined using input from subject-matter experts, based on the context of the problem that the model is applied to. As in Section \ref{mcmc}, we recommend proper, relatively flat priors. In the R Package, \textit{COMBO}, users can specify prior parameters for either Uniform, Normal, Double Exponential, or t prior distributions. Before summarizing the results, Algorithm \ref{alg:multistage-label-switch} must be applied on each individual MCMC chain to correct for label switching, if it is present. Standard methods are used to compute variance metrics. 
 
\section{Simulations} \label{simulations}
We present simulations for evaluating the proposed binary outcome misclassification model in terms of bias and root mean squared error (rMSE) for both single-stage and two-stage cases. For the single-stage case, our methods are compared to the \textit{SAMBA} R package and an adapted version of \textit{SAMBA} where the \textit{observation mechanism} is assumed to have either perfect specificity or perfect sensitivity, respectively. We also compare all single-stage methods that account for misclassification to a naive logistic regression that assumes there is no measurement error in the observed outcome. The two-stage mislcassification model is compared to a naive two-stage model that assumes no measurement error in \textit{both} of the observed outcomes. 

\subsection{Single-Stage Model Simulations}
The single-stage model simulation study includes three settings: (1) small sample size and large misclassification rates, (2) large sample size and small misclassification rates, and (3) moderate sample size with perfect specificity. Details on these settings can be found in Appendix \ref{appendix-a}. 

Figures 2 and 3 present the parameter estimates for Setting 1 and Setting 2, respectively, across 500 simulated datasets. Figure \ref{fig:ps_sim_results_histogram} in Appendix \ref{appendix-b} presents parameter estimates under Setting 3. Table \ref{parameter-results-table} presents mean parameter estimates and rMSE across 500 simulated datasets for each setting and estimation method. Table \ref{probability-results-table} presents the true outcome probability, sensitivity, and specificity values measured from the generated data and estimated from the EM algorithm and MCMC results for each simulation scenario.

\textbf{Setting 1:} Across all simulated datasets, the average $P(Y = 1)$ was 64.7\% and the average $P(Y^* = 1)$ was 59.1\% (Table \ref{probability-results-table}). The average correct classification rate was 84.7\% for $Y = 1$ and 87.7\% for $Y = 2$. In Table \ref{parameter-results-table}, we see that the naive logistic regression analysis results in substantial bias the estimate of $\beta_X$. If only one type of misclassification is assumed using \textit{SAMBA} EM or ``Perfect Sensitivity EM'', bias in the estimated parameters is high. Our proposed EM Algorithm performs well for $\beta$ parameter estimates, but has wide variation in some $\gamma$ estimates. Similar results are observed for our propsed MCMC method. The rMSE in the parameter estimates is generally smaller when comparing the EM estimates to the MCMC estimates. The EM estimates also achieve lower rMSE than estimates from \textit{SAMBA} EM and Perfect Sensitivity EM. Both the EM Algorithm and MCMC methods recover the true outcome probabilities $P(Y = 1)$ and $P(Y = 2)$, but MCMC tends to consistently overestimate the sensitivity and specificity values compared to the EM Algorithm (Table \ref{probability-results-table}).

\textbf{Setting 2:} In Setting 2, the average $P(Y = 1)$ was 64.7\% and the average $P(Y^* = 1)$ was 61.8\% across all simulated datasets (Table \ref{probability-results-table}). The average correct classification rate was 92.4\% for $Y = 1$ and 94.5\% for $Y = 2$. There is substantial bias in the estimate of $\beta_X$ in the naive logistic regression and perfect sensitivity EM results in Table \ref{parameter-results-table}. If perfect specificity is assumed in \textit{SAMBA} EM, the bias in the estimate of $\beta_X$ is improved compared to the naive and perfect sensitivity cases, but is much higher than if both types of misclassification are accounted for. Our proposed EM Algorithm method performs well for $\beta$ and $\gamma$ parameter estimates. Similar results are observed for the proposed MCMC method, though bias in some $\gamma$ estimates is higher than that observed using the EM algorithm. The rMSE in the $\beta$ parameter estimates is comparable between the EM and MCMC methods. However, rMSE in the $\gamma$ parameter estimates is generally smaller for EM than MCMC. Both EM and MCMC estimates, however, achieve lower rMSE than estimates from the methods that make perfect sensitivity and/or specificity assumptions. In Table \ref{probability-results-table}, we see that both the EM and MCMC methods recover the true outcome probabilities with virtually no bias. In addition, both methods achieve low bias in the estimation of $P(Y^* = 1 | Y = 1)$ and $P(Y^* = 2 | Y = 2)$.

\textbf{Setting 3:} In Setting 3, the average $P(Y = 1)$ was 64.7\% and the average $P(Y^* = 1)$ was 54.8\% (Table \ref{probability-results-table}). Per the simulation design, there was no misclassification for $Y = 2$. The average correct classification rate was 84.6\% for $P(Y = 1)$. In this setting, the naive analysis and perfect sensitivity methods return highly biased $\beta$ estimates (Table \ref{parameter-results-table}). There is also substantial bias in the $\gamma$ estimates for the perfect sensitivity EM method. This is unsurprising, since the $\gamma_{120}$ and $\gamma_{12Z}$ terms govern the specificity portion of the \textit{observation mechanism}, which is error-free in this setting. The framework of Setting 3 matches the \textit{SAMBA} EM method developed by \cite{beesley2020statistical}, and it is unsurprising that low bias and rMSE is observed across all parameter estimates. Despite making no assumptions on misclassification direction in a setting with perfect specificity, the EM and MCMC parameter estimates have low bias for all $\beta$ terms and for $\gamma_{110}$ and $\gamma_{11Z}$. Substantial bias is observed in the EM case for $\gamma_{120}$ and $\gamma_{12Z}$. This bias is largely the result of extreme parameter estimates in some simulated datasets, as observed in Appendix \ref{appendix-b} Figure 4. Despite the extreme variation in the $\gamma_{120}$ and $\gamma_{12Z}$ estimates across simulations, the remaining parameters are estimated with low bias and with rMSE near that of the \textit{SAMBA} EM estimates (Table \ref{parameter-results-table}). In the MCMC results for $\gamma_{120}$ and $\gamma_{12Z}$, we see much less bias and much smaller rMSE than the EM estimates. This behavior is likely due to the Uniform prior distribution, which limited the posterior samples to the $[-10, 10]$ range. In Table \ref{probability-results-table}, both the EM and MCMC methods return accurate estimates of $P(Y = 1)$ and $P(Y = 2)$. The EM method slightly overestimates sensitivity and slightly underestimates specificity. The MCMC method also slightly overestimates sensitivity, but correctly estimates perfect specificity in the datasets. However, in all cases, the proposed approach is superior to that of prespecified sensitivity and specificity values.

\subsection{Two-Stage Model Simulations}
The two-stage simulation study includes four additional settings. Two settings mirror those from the single stage simulation study: (4) small sample size and large misclassification rates and (5) large sample size and small misclassification rates. The remaining two simulation studies explore how the method handles cases with some perfect measurement: (6) small sample size and perfect $Y^{*(1)}$ and (7) small sample size and perfect specificity in $Y^*{(2)}$. We present results from the EM algorithm for settings 4-7 and results from MCMC for settings 4, 6, and 7. MCMC was not performed for setting 5 due to computational time constrants (see Appendix \ref{appendix-a}). Details on these settings can be found in Appendix \ref{appendix-a}.  

Table \ref{parameter-results-table-setting4-5} and Table \ref{parameter-results-table-setting6-7} present mean parameter estimates and rMSE across 500 simulated datasets for simulation settings 4-7. For each simulatin setting, Table \ref{probability-results-table-2} presents the true outcome probability, first-stage sensitivity, first-stage specificity, second-stage sensitivity, and second-stage specificity values measured from the generated data and estimated from the EM algorithm and MCMC results.

\textbf{Setting 4:} Across all simulated datasets, the average $P(Y = 1)$ was $64.8\%$, the average $P(Y^{*(1)} = 1)$ was $61.5\%$, and the average $P(Y^{*(2)} = 1)$ was $63.3\%$. The average first-stage correct classification rate was $85.2\%$ for $Y^{*(1)} = 1$ and $82.2\%$ for $Y^{*(1)} = 2$. The average second-stage correct classification rate was $90.3\%$ for $Y^{*(2)} = 1$ and $85.3\%$ for $Y^{*(2)} = 2$ (Table \ref{probability-results-table-2}). The naive analysis results in small bias for $\beta_X$, but the rMSE is large compared to the EM Algorithm and MCMC methods that account for potential misclassification (Table \ref{parameter-results-table-setting4-5}). Our proposed EM Algorithm performs well for the $\beta$ parameters and first-stage $\gamma$ parameters. Some of the second-stage $\gamma$ parameter estimates have wider variation. While our proposed MCMC method performs well for the $\beta$ parameter estimates, the bias and rMSE for the $\gamma$ estimates are considerably higher than that of the EM Algorithm. Both the EM Algorithm and MCMC methods recover the true outcome probabilities $P(Y = 1)$ and $P(Y = 2)$ in Table \ref{probability-results-table-2}, but MCMC tends to consistently overestimate correct classification rates for both stages of model. 

\textbf{Setting 5:} In Setting 5, the average $P(Y = 1)$ was $64.8\%$ (Table \ref{probability-results-table-2}). The observed outcome response probabilities were $62.4\%$ for $Y^{*(1)}$ and $64.4\%$ for $Y^{*(2)}$. The average first-stage sensitivity and specificity were both $92.1\%$. For second-stage outcomes, the average correct classification rate was  $94.9\%$ for $Y^{*(2)} = 1$ and $92.0\%$ for $Y^{*(2)} = 2$. While the bias for naive $\beta$ estimates is not large, the naive method rMSE is higher than that of the two-stage misclassification model (Table \ref{parameter-results-table-setting4-5}). The two-stage misclassification model achieves low bias across all parameter estimates using the EM Algorithm. In addition the EM Algorithm results in near-perfect recovery of response probabilities and conditional response probabilities for $Y$, $Y^{*(1)}$, and $Y^{*(2)}$ (Table \ref{probability-results-table-2}). It should be noted that there were numerical issues in the estimation of many realizations for Setting 5. Scaling this method to large sample sizes is a topic under investigation. 

\textbf{Setting 6:} In Setting 6, the average response probabilities for each marginal outcome, $Y$, $Y^{*(1)}$, and $Y^{*(2)}$ were $64.7\%$, $64.7\%$, and $63.6\%$, respectively. Per the simulation design, $Y^{*(1)}$ was measured without error. The second-stage correct classification rate was $90.2\%$ for $Y^{*(2)} = 1$ and $85.1\%$ for $Y^{*(2)} = 2$ (Table \ref{probability-results-table-2}). In this setting, the naive model is an appropriate choice for the data. As such, we find low bias in the naive $\beta$ estimates in Table \ref{parameter-results-table-setting6-7}. Across both the EM Algorithm and MCMC estimates, we find substantial bias and large rMSE estimates for the first-stage $\gamma$ parameters. This is not concerning, since the first-stage $\gamma$ parameters govern the misclassification mechanism for $Y^{*(1)}$, and there is no first-stage misclassification in the design. Similarly, the second-stage $\gamma$ parameters associated with mismatched $Y$ and $Y^{*(1)}$ terms are estimated with considerable bias using both the EM Algorithm and MCMC. Since mismatched $Y$ and $Y^{*(1)}$ terms are not possible in this study design, it is unsurprising that the corresponding regression parameters are estimated poorly. The remaining second-stage $\gamma$ terms, however, are estimated with low bias and with rMSE well under the naive model using the EM Algorithm. MCMC also produces reasonable estimates, but the bias and rMSE are both higher than that of the EM Algorithm. In Table \ref{probability-results-table-2}, we see that both the EM and MCMC methods provide accurate estimates of $Y$ response probabilities. Importantly, both methods also correctly capture the perfect measurement of $Y^{*(1)}$, with correct classification probabilities estimated between $97.7\%$ and $99.9\%$. Second-stage correct classification rates are also estimated accurately using the EM Algorithm. These probabilities are slightly overestimated using MCMC. 

\textbf{Setting 7:} In Setting 7, the average response probability for $Y = 1$ was $64.8\%$ (Table \ref{probability-results-table-2}). The average response probabilities for the first-stage and second-stage observed outcomes were $61.5\%$ and $56.7\%$, respectively. The average first-stage correct classification rate was $85.1\%$ for $Y^{*(1)} = 1$ and $82.0\%$ for $Y^{*(1)} = 2$. The average second-stage correct classification rate was $90.3\%$ for $Y^{*(2)} = 1$. Per the simulation design, $P(Y^{*(2)} | Y^{*(1)} = 2, Y = 2) = 1$. The naive analysis yields low bias for $\beta$ terms and for $\gamma^{(2)}_{111}$ terms, but the rMSE is higher than that of the EM Algorithm estimates, in particular (Table \ref{parameter-results-table-setting6-7}). The EM algorithm performs well in terms of bias and rMSE for most terms, but estimation is problematic for $\gamma^{(2)}_{1220}$ and  $\gamma^{(2)}_{122Z^{(2)}}$. These results are not concerning because extreme parameter estimates correspond to a lack of misclassification in the specificity mechanism of the second-stage outcomes, which was appropriate given the simulation design. The MCMC estimation did not show this extreme behavior in the estimates of $\gamma^{(2)}_{1220}$ and  $\gamma^{(2)}_{122Z^{(2)}}$, presumably due to the limits of the uniform prior distribution used in the analysis. For other terms in the model, MCMC also produced reasonable estimates, though bias and rMSE were generally higher than that of the EM algorithm, especially for second-stage $\gamma^{(2)}$ parameters. Both the EM Algorithm and MCMC estimates correspond to highly accurate estimates of response and first-stage classification probabilities $P(Y = 1)$, $P(Y^{*(1)} = 1 | Y = 1)$, and $P(Y^{*(1)} = 2 | Y = 2)$ (Table \ref{probability-results-table-2}). The EM Algorithm estimates $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ without bias, while MCMC overestimates the quantity. Importantly, both methods correctly estimate near-perfect classification for $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$, as specified by the simulation design.

\section{Applied Example} \label{example}
In this section, we perform case studies using both the single-stage and the two-stage misclassification models. The single-stage study uses data from the 2020 Medical Expenditure Panel Survey (MEPS) to investigate risk factors associated with myocardial infarction (MI). The two-stage study uses pretrial data from admitted persons in Prince William County, Virginia between January 2016 and December 2019 to investigate risk factors for criminal recidivism. 

\subsection{Single-Stage Applied Example}
In this section, we perform a case study using data from the 2020 Medical Expenditure Panel Survey (MEPS), a sequence of surveys on cost and use of health care and health insurance status in the United States \citep{AHRQ}. We are primarily interested in risk factors associated with myocardial infarction (MI). This association, however, is potentially impacted by misclassification in self-reported history of MI. In particular, it is possible for the symptoms of other ailments to be attributed to MI and for the chest pain of MI to be attributed to a different diagnosis. These types of misdiagnoses are known to be more common in women than men \citep{arber2006patient, maserejian2009disparities} and among younger patients \citep{mckinlay1996non}. Thus, we expect misclassification of self-reported history of MI to be associated with patient gender and age. Our overall goal is to assess the severity of this misclassification, and to understand the impact of misclassification on the association between risk factors and MI. The MEPS 2020 survey was used for this analysis because, at the time of writing, this was the most recently collected MEPS data. In addition, the disruption of the COVID-19 pandemic in the 2020 year may have contributed additional measurement error to medical diagnoses in the dataset  \citep{AHRQ}.

Our response variable of interest is whether or not survey respondents reported any history of MI. We assessed the association of MI status with risk factors including age, smoking status, and exercise habits. The age variable was centered and scaled before use in the model. Smoking status and exercise variables were coded as binary indicators.

For the analysis, we included data only from participants age 18-85. For each family unit recorded in the dataset, we only keep the responses of the reference person. After excluding all of the records with missing values in the response and covariates, we had a total of 12731 observations on our analysis. In this dataset, 640 (5\%) reported a history of MI.

We estimate model parameters using the EM algorithm with label switching correction from Section \ref{em}. Parameter estimates and standard errors are reported in Table \ref{applied-results-table}. The $\beta$ effect estimates, particularly $\beta_{smoke}$ and $\beta_{exercise}$, in the naive analysis are all attenuated compared to the EM analysis that accounts for MI misclassification. We find that smoking, not exercising regularly, and increased age are all associated with true incidence of MI in the sample. Our results are also robust with respect to imperfect sensitivity and imperfect specificity. In the sensitivity component of the \textit{observation mechanism}, we find that, given that MI has occurred, individuals who are older and/or female are less likely to report having been diagnosed with MI. Given that MI has not occurred, we find that individuals who are female are still less likely to report an MI diagnosis, but older individuals are more likely to report having been diagnosed with MI.

We use the EM parameter estimates to estimate the sensitivity and specificity of the self-reported MI measure among males and females in the sample. Among males, the sensitivity is estimated at 76.3\% and the specificity is estimated at 94.4\%. Among females, sensitivity is much lower, at only 59.1\%, and specificity is estimated at 97.1\%. These findings are in line with previous literature which suggests that many instances of MI go misdiagnosed or undiagnosed \citep{turkbey2015prevalence}, and this problem is more pronounced for female patients in comparison to male patients \citep{arber2006patient, maserejian2009disparities}.

\subsection{Two-Stage Applied Example}
In this section, we apply our two-stage misclassifiation model to pretrial records from Prince William County, Virginia. These records were provided by the Virginia Department of Criminal Justice Services and were collected between January 1, 2016 and December 31, 2019. We are primarily interested in risk factors associated with relapse into criminal behavior after admittance and before trial date, a concept known as recidivism \citep{desmarais2013risk}. In this dataset, however, we do not have records of criminal activity after the initial arrest. Instead, we have two sequential and imperfect measures of recidivism behavior. First, risk of recidivism is measured using the Virginia Pretrial Risk Assessment Instrument (VPRAI). This instrument uses factors such as current charge, criminal history, employment status, and drug use to assess an individual's likelihood of reoffense. Based on the VPRAI recommendation, a judge will make a final decision to release or detain an individual before their trial. We can view the judge's decision as another imperfect measure of an individual's propensity for recidivism, and this outcome is based on a first-stage outcome, namely the VPRAI recommendation. Previous work in the criminal justice and fairness literature suggests that risk instrument recommendations and judge decisions may be frequently misclassified, and that these misclassifications may depend on the race of the defendant \citep{angwin2016Propublica}. Thus, we expect misclassification of recidivism to be associated with defendant race. Our overall goal is assess the severity and direction of this misclassification, while estimating the true association between risk factors and criminal recidivism. 

Our first stage response variable is the VPRAI recommendation, dichotomized into ``detain'' and ``release'' recommendation categories. Our second-stage response variable is the court's final decision to detain or release a defendant before their trial. We assessed the association of recidivism with risk factors including the number of prior failures to appear (FTA) for trial, employment status, drug abuse history, and the number of previous violent arrests. Employment and drug history variables were coded as binary indicators. 

In our analysis, we only include individuals with charge categories for which we could reconstruct the VPRAI recommendation, including non-violent misdemeanors, driving under the influence, violent misdemeanors, and firearm charges. We also only included individuals whose race was listed as either white or Black. After excluding all records with missing values in the responses and the covariates, we had a total of $1,990$ records in our dataset. Of these, 259 individuals ($13.0\%$) received a ``detain'' VPRAI recommendation, but $1,038$ defendants ($52.2\%$) were detained by the court ahead of their trial. 

We estimated model parameters using the EM algorithm and label switching correction for the multistage model, defined in Section \ref{multistage-model}. Parameter estimates and standard errors are provided in Table \ref{applied-results-table2}. With the exception of the intercept and drug history association parameters, the $\beta$ estimates in the misclassification model are all attenuated compared to the naive model. We find that increased numbers of previous FTA, unemployment, a history of drug abuse, and increased numbers of violent arrests are all associated with true incidence of recidivism in this data set. For the first-stage outcome, we find that, given true recidivism, black defendants are more likely to have a VPRAI detention recommendation than white defendants. Given no reoffense, however, black defendants are \textit{still} more likely to have a VPRAI detention recommendation than white defendants. This trend remains for court decisions. Given no reoffense and regardless of VPRAI recommendations, black individuals are more likely to be detained by the court than white individuals. Similarly, given true recidivism and a VPRAI detention recommendation, black defendants are more likely to be detained before their trial than white defendants. Black defendants are also more likely to be detained before their trial than white defendants in the case of true recidivism and a VPRAI recommendation of ``release'', though this parameter estimate has extremely high standard error due to perfect separation, or perfect prediction of the outcome by race \citep{mansournia2018separation}. In fact, conditional on true recidivism and a VPRAI ``release'' recommendation, our model estimates that $100\%$ of black defendants are detained by judges.  

We also use the EM Algorithm parameter estimates to assess VPRAI sensitivity and specificity, as well as the fairness and accuracy of judge decisions. In the sample, we estimate a recidivism rate of $17.9\%$. The VPRAI appears to have moderate sensitivity and near-perfect specificity; we estimate that the VPRAI correctly recommends detention for $67.5\%$ of defendants and correctly recommends release for $99.6\%$ of defendants. This sensitivity rate, however, differs by defendant race. Among Black defendants who are expected to reoffend, the VPRAI recommends pretrial detention $86.0\%$ of the time. Among white defendants, this rate drops to just $49.3\%$. Moving to judge decisions, we estimate that, among individuals who had a VPRAI detention recommendation, judges correctly detain defendants in $84.9\%$ of cases. However, among defendants who received a VPRAI release recommendation, we estimate that judges correctly release defendants just $54.7\%$ of the time. Again, these rates differ by defendant race. Among defendants estimated to truly reoffend and who were given a VPRAI detention recommendation, a court detention decision is received by $82.9\%$ of white defendants and $87.0\%$ of Black defendants. Among white defendants who are not expected to reoffend and who have VPRAI ``release'' recommendations, we estimate that $60.3\%$ are, in fact, released before their trial. Among black defendants, this proportion drops to just $49.0\%$. Collapsing across VPRAI recommendations, we estimate that judges appropriately detain white individuals in $76.8\%$ of cases and appropriately detain Black individuals in $88.8\%$ of cases, suggesting that white defendants may be given ``the benefit of the doubt'' more often than black defendants. Similarly, we estimate that white defendants are wrongfully detained by the court in $39.7\%$ of cases, but wrongful detentions happen in as many as $51.4\%$ of cases involving Black defendants. 

\section{Discussion}
Association studies that use misclassified outcome variables are susceptible to bias in effect estimates \citep{beesley2020statistical, khan2020introduction}. In this paper, we presented methods to recover unbiased regression parameters when a binary outcome variable is subject to misclassification and provide software in the R package \textit{COMBO}. We also extend these methods to include cases with multiple sequential misclassified outcomes. These methods are based a novel label switching correction algorithm, which is required to overcome model identifiability problems inherent in the likelihood structure. We show that our single-stage methods are able to recover parameter estimates in cases with varying sample sizes and misclassification rates, compared to methods that assume perfect sensitivity, perfect specificity, or no misclassification in the outcome measure. Similarly, we show that our two-stage methods recover parameter estimates in cases with varying misclassification rates and sample sizes, compared to methods that assume perfect measurement of both outcomes. To show the utility of our method in real-world problems, we apply our methods to data from the 2020 Medical Expenditure Panel Survey (MEPS) and estimate that sensitivity and specificity rates for MI diagnosis differ by patient gender and age. In addition, we apply our two-stage mislcassification model to pretrial data from Prince William County, Virginia and show that court bail decisions may be differentially misclassified based on defendant race. 

Our methods are attractive because they require little external information, can be implemented without repeated measures or double sampled outcomes, and do not require gold standard labels. Our findings also constitute a generalization of the work of \cite{beesley2020statistical}. Specifically, we no longer require the assumption of perfect specificity, but can still handle cases where such an assumption is valid. 

Further generalizations of our methods are still possible in future work. In particular, researchers may consider the more difficult case of a potentially misclassified outcome that can take on three category labels. In addition, other label switching correction strategies may be explored. While the assumption in Algorithm \ref{alg:label-switch} requiring at least 50\% of cases to be correctly classified may be reasonable, there are scenarios where such a restriction is not appropriate. For example, an investigation of MI prevalence found that as many as 80\% of MI cases detected via myocardial scarring went undetected by traditional clinical methods \citep{turkbey2015prevalence}. Thus, even our applied example in Section \ref{example} may not have fully captured the misclassification present in the MEPS MI data. Further research is needed to fully dismantle the permutation invariance of likelihoods in misclassification models, while relaxing assumptions on sensitivity and specificity rates. Future work can also explore how covariate-related misclassification can impact variables other than outcomes. In particular, we hope to extend existing work that accounts for misclassification in binary mediators, but does not consider scenarios where differential misclassification rates must be estimated from the data \citep{valeri2014estimation}.

\subsection*{Acknowledgements}
The authors would like to thank Sarah Riley, Karen Levy, and Solon Barocas for their insights into pretrial detention processes. Data used in the two-stage misclassification model analysis was provided by the Virginia Department of Criminal Justice Services. Funding support was provided by the LinkedIn and Cornell Ann S. Bowers College of Computing and Information Science strategic partnership PhD Award. Martin T. Wells was supported by NIH awards U19AI111143-07 and 1P01-AI159402.

\newpage

\nocite{*}
\bibliographystyle{biom}
\bibliography{references}

\newpage

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.8]{binary_obs_data_structure.png}

\caption{Diagram of the assumed data structure for a single-stage misclassification model.\label{conceptual_framework_figure}}
\end{center}
\end{figure}

\newpage

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.8]{multistage_data_structure.png}

\caption{Diagram of the assumed data structure for a two-stage misclassification model.\label{multistage_conceptual_framework_figure}}
\end{center}
\end{figure}

\newpage

\begin{figure}
\begin{center}
\includegraphics[scale=0.85]{small_n_sim_histogram.png}
\medskip
  \caption{\textit{(Caption on next page.)}}\label{fig:small_n_results_figure}
\end{center}
\end{figure}

\begin{figure*}
Figure \ref{fig:small_n_results_figure} Caption: Parameter estimates for 500 realizations of simulation Setting 1. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Solid lines represent true parameter values. Dashed lines represent mean parameter estimates for a given method.
\end{figure*}

\clearpage

\begin{figure}
\begin{center}
\includegraphics[scale=0.85]{large_n_sim_histogram.png}
\medskip
  \caption{\textit{(Caption on next page.)}}\label{fig:large_n_results_figure}
\end{center}
\end{figure}

\begin{figure*}
 Figure \ref{fig:large_n_results_figure} Caption: Parameter estimates for 500 realizations of simulation Setting 2. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Solid lines represent true parameter values. Dashed lines represent mean parameter estimates for a given method.
\end{figure*}

\newpage

\begin{table}[htbt]\footnotesize
\centering
\caption{Bias and root mean squared error (rMSE) for parameter estimates from 500 realizations of simulation Settings 1, 2, and 3. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Estimates marked with a ``-'' are not obtained by the given estimation method.} \label{parameter-results-table}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
        &       & \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{MCMC}&  
        \multicolumn{2}{c}{\textit{SAMBA} EM} & 
        \multicolumn{2}{c}{Perfect Sens. EM} & 
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-12}
Scenario &       & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} \\
\hline
    \\
      & $\beta_0$ & 0.027 & 0.256 & 0.006 & 0.300 & 0.073 & 0.211 & -0.634 & 0.640 & -0.555 & 0.561 \\
(1)   & $\beta_X$ & -0.106 & 0.397 & 0.154 & 0.530 & 0.415 & 0.454 & 0.988 & 0.992 & 1.019 & 1.023 \\
      & $\gamma_{110}$ & -0.004 & 0.267 & 0.051 & 0.355 & 0.360 & 0.455 & - & - & - & -\\
      & $\gamma_{11Z}$ & 0.037 & 0.360 & 0.297 & 0.844 & -0.050 & 0.375 & - & - &  - & -\\
      & $\gamma_{120}$ & 0.095 & 0.810 & -0.911 & 2.001 & - & - & -2.754 & 2.900 & - & -\\
      & $\gamma_{12Z}$ & -0.319 & 1.464 & -2.215 & 2.634 & - & - & 0.894 & 0.994 & - & -\\
              \\
      & $\beta_0$ & 0.005 & 0.053 & 0.006 & 0.053 & 0.038 & 0.061 & -0.373 & 0.374 & -0.350 & 0.351\\
(2)   & $\beta_X$ & -0.012 & 0.091 & 0.008 & 0.091 & 0.183 & 0.191 & 0.628 & 0.629 & 0.643 & 0.643 \\
      & $\gamma_{110}$ & 0.007 & 0.153 & 0.003 & 0.157 & 0.239 & 0.284 & - & - & - & -\\
      & $\gamma_{11Z}$ & -0.004 & 0.118 & 0.017 & 0.123 & -0.031 & 0.125 & - & - & - & -\\
      & $\gamma_{120}$ & 0.015 & 0.429 & 0.024 & 0.561 & - & - & -3.656 & 3.699 & - & -\\
      & $\gamma_{12Z}$ & -0.036 & 0.306 & -0.245 & 0.540 & - & - & 0.871 & 0.888 & - & -\\
      \\
      & $\beta_0$ & -0.005 & 0.097 & -0.008 & 0.095 & -0.006 & 0.093 & -0.781 & 0.781 & -0.757 & 0.758\\
(3)   & $\beta_X$ & -0.040 & 0.123 & -0.002 & 0.102 & 0.006 & 0.099 & 0.831 & 0.832 & 0.844 & 0.845 \\
      & $\gamma_{110}$ & -0.024 & 0.109 & -0.020 & 0.106 & -0.009 & 0.105 & - & - & - & -\\
      & $\gamma_{11Z}$ & 0.018 & 0.133 & 0.041 & 0.140 & 0.019 & 0.131 & - & - & - & -\\
      & $\gamma_{120}$ & -3.094 & 28.586 & -0.911 & 1.220 & - & - & -1.26 & 1.389 & - & -\\
      & $\gamma_{12Z}$ & 4.023 & 9.406 & 0.334 & 0.638 & - & - & 5.787 & 5.791 & - & -\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\clearpage

\begin{table}[htbt]
\centering
\caption{Estimated event probabilities from 500 realizations of simulation Settings 1, 2, and 3. ``Data'' terms refer to empirical values computed from generated datasets. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package.} \label{probability-results-table}
\begin{threeparttable}
\begin{tabular}{clrrrrr}
\hline
        Scenario & & & Data & EM & MCMC \\
\hline
    \\
(1)           && $P(Y = 1)$ & 0.647 & 0.646 & 0.652 \\
              && $P(Y = 2)$ & 0.353 & 0.354 & 0.348 \\
              && $P(Y^* = 1 | Y = 1)$ & 0.847 & 0.856 & 0.874 \\
              && $P(Y^* = 2 | Y = 2)$ & 0.877 & 0.853 & 0.940 \\
              \\
              \\
(2)           && $P(Y = 1)$ & 0.648 & 0.648 & 0.649 \\
              && $P(Y = 2)$ & 0.352 & 0.352 & 0.351 \\
              && $P(Y^* = 1 | Y = 1)$ & 0.924 & 0.931 & 0.933\\
              && $P(Y^* = 2 | Y = 2)$ & 0.945 & 0.931 & 0.942\\
              \\
              \\
(3)           && $P(Y = 1)$ & 0.647 & 0.645 & 0.646 \\
              && $P(Y = 2)$ & 0.353 & 0.355 & 0.354 \\
              && $P(Y^* = 1 | Y = 1)$ & 0.846 & 0.856 & 0.859\\
              && $P(Y^* = 2 | Y = 2)$ & 1.000 & 0.991 & 1.000\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\newpage

\begin{table}[H]\footnotesize
\centering
\caption{Bias and root mean squared error (rMSE) for parameter estimates from 500 realizations of simulation Setting 4. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``Naive Analysis'' results were obtained by a two-stage model that does not account for outcome misclassification. Estimates marked with a ``-'' are not obtained by the given estimation method.} \label{parameter-results-table-setting4-5}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
        &       & \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{MCMC}&  
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-8}
Scenario &       & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} \\
\hline
    \\
      & $\beta_0$ & 0.032 & 0.197 & -0.053 & 0.224 & 0.029 & 0.299 \\
(4)   & $\beta_X$ & -0.101 & 0.301 & 0.261 & 0.490 & -0.100 & 0.547 \\
      & $\gamma^{(1)}_{110}$ & -0.028 & 0.209 & 0.249 & 0.522 & - & - \\
      & $\gamma^{(1)}_{11Z^{(1)}}$ & 0.044 & 0.338 & 0.405 & 1.046 & - & - \\
      & $\gamma^{(1)}_{120}$ & 0.067 & 0.333 & -0.496 & 1.210 & - & - \\
      & $\gamma^{(1)}_{12Z^{(1)}}$ & -0.156 & 0.656 & -1.425 & 2.168 & - & - \\
      & $\gamma^{(2)}_{1110}$ & -0.014 & 0.249 & 1.697 & 2.385 & -0.195 & 0.364 \\
      & $\gamma^{(2)}_{111Z^{(2)}}$ & 0.047 & 0.374 & 2.674 & 2.989 & 0.114 & 1.233 \\
      & $\gamma^{(2)}_{1210}$ & -0.022 & 0.652 & -2.938 & 4.341 & - & - \\
      & $\gamma^{(2)}_{121Z^{(2)}}$ & 0.128 & 0.710 & 2.674 & 3.664 & - & - \\
      & $\gamma^{(2)}_{1120}$ & 0.017 & 1.236 & 2.387 & 3.925 & - & -\\
      & $\gamma^{(2)}_{112Z^{(2)}}$ & -0.398 & 3.091 & -0.124 & 3.091 & - & -\\
      & $\gamma^{(2)}_{1220}$ & 0.026 & 0.343 & -1.918 & 2.647 & -0.073 & 3.909 \\
      & $\gamma^{(2)}_{122Z^{(2)}}$ & -0.125 & 0.581 & -3.025 & 3.247 & -0.390 & 3.251 \\
      \\
      & $\beta_0$ & 0.003 & 0.036 &  &  & 0.006 & 0.058 \\
(5)   & $\beta_X$ & -0.005 & 0.062 &  &  & 0.026 & 0.101 \\
      & $\gamma^{(1)}_{110}$ & -0.011 & 0.108 & & & - & - \\
      & $\gamma^{(1)}_{11Z^{(1)}}$ & 0.013 & 0.096 & & & - & - \\
      & $\gamma^{(1)}_{120}$ & 0.036 & 0.188 && & - & - \\
      & $\gamma^{(1)}_{12Z^{(1)}}$ & -0.035 & 0.213 & & & - & - \\
      & $\gamma^{(2)}_{1110}$ & -0.012 & 0.111 & & & -0.053 & 0.157 \\
      & $\gamma^{(2)}_{111Z^{(2)}}$ & 0.013 & 0.108 & & & -0.128 & 0.208 \\
      & $\gamma^{(2)}_{1210}$ & 0.166 & 0.413 && & - & - \\
      & $\gamma^{(2)}_{121Z^{(2)}}$ & -0.061 & 0.211 & & & - & - \\
      & $\gamma^{(2)}_{1120}$ & -0.110 & 0.492 & & & - & -\\
      & $\gamma^{(2)}_{112Z^{(2)}}$ & 0.024 & 0.234 && & - & -\\
      & $\gamma^{(2)}_{1220}$ & -0.027 & 0.151 && & -0.092 & 0.239 \\
      & $\gamma^{(2)}_{122Z^{(2)}}$ & 0.006 & 0.133 & & & 0.234 & 0.363 \\
      \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[H]\footnotesize
\centering
\caption{Bias and root mean squared error (rMSE) for parameter estimates from 500 realizations of simulation Settings 6 and 7. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``Naive Analysis'' results were obtained by a two-stage model that does not account for outcome misclassification. Estimates marked with a ``-'' are not obtained by the given estimation method.} \label{parameter-results-table-setting6-7}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
        &       & \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{MCMC}&  
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-8}
Scenario &       & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} \\
\hline
      \\
      & $\beta_0$ & -0.018 & 0.118 & -0.001 & 0.113 & 0.016 & 0.247\\
(6)   & $\beta_X$ & -0.097 & 0.195 & 0.013 & 0.166 & -0.131 & 0.457 \\
      & $\gamma_{110}$ & 19.373 & 65.503 & 1.26 & 1.640 & - & - \\
      & $\gamma_{11Z^{(1)}}$ & -4.068 & 14.446 & -0.450 & 1.094 & - & - \\
      & $\gamma_{120}$ & -43.863 & 186.594 & -0.815 & 1.397 & - & - \\
      & $\gamma_{12Z^{(1)}}$ & -4.388 & 285.955 & 0.678 & 1.295 & - & - \\
      & $\gamma^{(2)}_{1110}$ & -0.016 & 0.194 & -0.033 & 0.327 & -0.034 & 0.325 \\
      & $\gamma^{(2)}_{111Z^{(2)}}$ & 0.064 & 0.282 & 0.527 & 0.969 & 0.239 & 0.898 \\
      & $\gamma^{(2)}_{1210}$ & 1.247 & 2.059 & -0.797 & 1.084 & - & - \\
      & $\gamma^{(2)}_{121Z^{(2)}}$ & 1.224 & 2.035 & -0.742 & 1.049 & - & - \\
      & $\gamma^{(2)}_{1120}$ & 2.191 & 2.820 & 0.872 & 1.236 & - & - \\
      & $\gamma^{(2)}_{112Z^{(2)}}$ & 1.727 & 2.242 & 0.345 & 0.883 & - & -\\
      & $\gamma^{(2)}_{1220}$ & 0.000 & 0.228 & -0.006 & 0.631 & -0.024 & 2.788 \\
      & $\gamma^{(2)}_{122Z^{(2)}}$ & -0.082 & 0.335 & -1.019 & 1.589 & -0.968 & 6.350 \\
              \\
      & $\beta_0$ & 0.016 & 0.165 & -0.049 & 0.230 & 0.006 & 0.232\\
(7)   & $\beta_X$ & -0.160 & 0.266 & 0.037 & 0.272 & -0.149 & 0.349 \\
      & $\gamma_{110}$ & -0.057 & 0.197 & 0.101 & 0.293 & - & -\\
      & $\gamma_{11Z^{(1)}}$ & 0.061 & 0.264 & 0.211 & 0.749 & - & -\\
      & $\gamma_{120}$ & 0.133 & 0.282 & 0.017 & 0.458 & - & -\\
      & $\gamma_{12Z^{(1)}}$ & -0.042 & 0.440 & -0.655 & 1.292 & - & -\\
      & $\gamma^{(2)}_{1110}$ & 0.011 & 0.252 & 1.119 & 1.823 & -0.195 & 0.349\\
      & $\gamma^{(2)}_{111Z^{(2)}}$ & 0.034 & 0.394 & 1.888 & 2.381 & 0.033 & 1.549\\
      & $\gamma^{(2)}_{1210}$ & -0.114 & 0.509 & -2.304 & 4.178 & - & -\\
      & $\gamma^{(2)}_{121Z^{(2)}}$ & 0.004 & 0.461 & 2.610 & 3.502 & - & -\\
      & $\gamma^{(2)}_{1120}$ & -0.067 & 9.275 & -0.134 & 1.197 & - & -\\
      & $\gamma^{(2)}_{112Z^{(2)}}$ & 1.251 & 8.798 & 1.370 & 2.132 & - & -\\
      & $\gamma^{(2)}_{1220}$ & -12.413 & 29.590 & -1.238 & 1.365 & -4.833 & 18.441\\
      & $\gamma^{(2)}_{122Z^{(2)}}$ & -9.996 & 24.390 & 0.413 & 0.731 & 3.482 & 10.316\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[H]
\centering
\caption{Estimated event probabilities from 500 realizations of simulation Settings 4, 5, 6, and 7. ``Data'' terms refer to empirical values computed from generated datasets. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package.} \label{probability-results-table-2}
\begin{threeparttable}
\begin{tabular}{clrrrrr}
\hline
        Scenario & & & Data & EM & MCMC \\
\hline
    \\
(4)           && $P(Y = 1)$ & 0.648 & 0.647 & 0.650 \\
              && $P(Y = 2)$ & 0.352 & 0.353 & 0.350 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ & 0.852 & 0.848 & 0.888 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ & 0.822 & 0.816 & 0.897 \\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ & 0.903 & 0.901 & 0.978 \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ & 0.853 & 0.851 & 0.973 
              \\
              \\
              \\
(5)           && $P(Y = 1)$ & 0.648 & 0.648 &  \\
              && $P(Y = 2)$ & 0.352 & 0.352 &  \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ & 0.921 & 0.920 &  \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ & 0.921 & 0.919 &  \\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ & 0.949 & 0.949 & \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ & 0.920 & 0.921 & \\
              \\
              \\
(6)           && $P(Y = 1)$ & 0.647 & 0.640 & 0.648 \\
              && $P(Y = 2)$ & 0.353 & 0.360 & 0.352 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ & 1 & 0.997 & 0.999 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ & 1 & 0.977 & 0.999 \\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ & 0.902 & 0.903 & 0.917 \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ & 0.851 & 0.855 & 0.890 \\
              \\
              \\
(7)           && $P(Y = 1)$ & 0.648 & 0.642 & 0.641 \\
              && $P(Y = 2)$ & 0.352 & 0.358 & 0.359 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ & 0.851 & 0.847 & 0.871 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ & 0.820 & 0.803 & 0.848 \\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ & 0.903 & 0.903 & 0.967 \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ & 1 & 0.994 & 0.999 \\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[H]
\centering
\caption{Parameter estimates and standard errors from the applied example using the MEPS dataset.``EM'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Estimates marked with a ``-'' are not obtained by the given estimation method. $\beta_{smoke}$ refers to the association between smoking status (reference = non-smoker) and myocardial infarction (MI). $\beta_{exercise}$ refers to the association between exercise habits (reference = regular exercise) and MI. $\beta_{age}$ refers to the association between age (centered and scaled) and MI. $\gamma_{1,j,gender}$ refers to the association between gender and observed MI, given true MI status $j$ (reference = male). $\gamma_{1,j,age}$ refers to the association between age and observed MI, given true MI status $j$. $\beta_0$, $\gamma_{110}$, and $\gamma_{120}$ are intercept terms.} \label{applied-results-table}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
           && \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{\textit{SAMBA} EM} & 
        \multicolumn{2}{c}{Perfect Sens. EM} & 
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-10}
 && \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE}\\
\hline
    \\
      $\beta_0$ && -4.374 & 0.065 & -3.184 & 0.665 & -4.159 & 101.296 & -3.576 & 0.078 \\
    $\beta_{smoke}$ && 1.544 & 0.107 & 0.643 & 0.128 & 0.763 & 3.256 & 0.635 & 0.109 \\
    $\beta_{exercise}$ && 0.303 & 0.126 & 0.273 & 0.095 & 0.428 & 1.476 & 0.184 & 0.084 \\
    $\beta_{age}$ && 0.094 & 0.010  & 0.065 & 0.009 & 0.058 & 2.688 & 0.059 & 0.003 \\
       $\gamma_{110}$ && 2.969 & 0.100 & 2.437 & 7.913 & - & - & - & -\\
       $\gamma_{11,gender}$ && -1.766 & 0.036  & -2.661 & 6.750 & - & - &  - & -\\
       $\gamma_{11,age}$ && -0.198 & 0.005  & -0.005 & 0.012 & - & - &  - & -\\
       $\gamma_{120}$ && -3.580 & 0.112 & - & - & -3.674 & 102.790 & - & -\\
       $\gamma_{12,gender}$ && -0.818 & 0.108 & - & - & -5.542 & 5.206 & - & -\\
       $\gamma_{12,age}$ && 0.084 & 0.005 & - & - & 0.063 & 2.678 & - & -\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[H]
\centering
\caption{Parameter estimates and standard errors from the applied example using the Virginia  Department of Criminal Justice Services dataset.``EM'' estimates were computed using the \textit{COMBO} R Package.  The ``Naive Analysis'' results were obtained by running a two-stage model that does not account for outcome misclassification. Estimates marked with a ``-'' are not obtained by the given estimation method. $\beta_{FTA}$ refers to the association between a defendant's number of previous failures to appear (FTA) and recidivism. $\beta_{unemployed}$ refers to the association between employment status (reference =  employed or unable to work) and recidivism. $\beta_{drug}$ refers to the association between drug abuse history (reference = no history of drug abuse) and recidivism. $\beta_{violent}$ refers to the association between a defendant's number of previous violent arrests and recidivism. $\gamma^{(1)}_{1,j,race}$ refers to the association between race and VPRAI risk score, given true recidivism status $j$. $\gamma^{(2)}_{1,k,j,race}$ refers to the association between race and the court's decision to detain a defendant, given VPRAI risk score $k$ and true recidivism status $j$. The reference category for race is white. $\beta_0$, $\gamma^{(1)}_{1j0}$, and $\gamma^{(2)}_{1kj0}$ are intercept terms for all $j$ and all $k$.} \label{applied-results-table2}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
           && \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-6}
 && \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE}\\
\hline
    \\
    $\beta_0$ && -3.512 & 0.105 & -2.382 & 1.943 \\
    $\beta_{FTA}$ && 1.224 & 0.218 & 1.344 & 1.328 \\
    $\beta_{unemployed}$ && 0.732 & 0.056 & 1.036 & 1.094 \\
    $\beta_{drug}$ && 1.968 & 0.126 & 1.586 & 1.251 \\
    $\beta_{violent}$ && 0.280 & 0.0223 & 1.028 & 1.328 \\
    
    $\gamma^{(1)}_{110}$ && -0.029 & 0.199  & - & -\\
    $\gamma^{(1)}_{11,race}$ && 1.843 & 0.376 & - & -\\
    $\gamma^{(1)}_{120}$ && -20.270 & 0.605 & - & - \\
    $\gamma^{(1)}_{12,race}$ && 15.341 & 0.603 & - & - \\

    $\gamma^{(2)}_{1110}$ && 1.576 & 0.298 & 0.666 & 0.296 \\
    $\gamma^{(2)}_{111,race}$ && 0.327 & 0.374 & 0.364 & 0.230\\
    $\gamma^{(2)}_{1210}$ && 0.892 & 0.203 & - & - \\
    $\gamma^{(2)}_{121,race}$ && 15.645 & 182.618 & - & - \\

    $\gamma^{(2)}_{1120}$ && -6.492 & 1.858 & - & -\\
    $\gamma^{(2)}_{112,race}$ && 9.822 & 1.859 & - & -\\
    $\gamma^{(2)}_{1220}$ && -0.418 & 0.018 & -0.668 & 0.304 \\
    $\gamma^{(2)}_{122,race}$ && 0.459 & 0.018 & 0.517 & 0.171 \\
    
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\newpage

\begin{appendices}

\section{Simulation Study Settings} \label{appendix-a}

\subsection{Simulation Settings}
We present simulations for evaluating the proposed binary outcome misclassification model in terms of bias and root mean squared error (rMSE) for both single stage and a two-stage cases. For a given simulation scenario, we present parameter estimates for a binary outcome misclassification model obtained from the EM-algorithm and from MCMC, under a \textit{Uniform}$(-10,10)$ prior distribution setting. For the single-stage cases, we compare these estimates to the naive \textit{analysis model} that considers only observed outcomes $Y^*$ and predictors $X$. In addition, we use the \textit{SAMBA} R package from \cite{beesley2020statistical} to find the relevant parameter estimates in the event that we assumed that our \textit{observation mechanism} had perfect specificity. We also consider the case where we incorrectly assumed that our \textit{observation mechanism} had perfect sensitivity. For the two-stage cases, we compare our estiamtes to a naive \textit{analysis model} that assumes $Y^{*(1)}$ and $Y^{*(2)}$ are measured without error. 

In all settings, we generate $500$ datasets with $P(Y = 1) \approx 65\%$. In the first simulation setting for the single-stage model, we studied an example that is expected to be highly problematic for analysts: a relatively small sample size and relatively high misclassification rate. In this setting, we generated datasets with $1000$ members and imposed outcome misclassification rates between $10\%$ and $18\%$. In the Setting 2, we show that the problem of outcome misclassification remains influential even as sample size increases and misclassification rates decrease. In this setting, we generated datasets with $10000$ members and imposed outcome misclassifiation rates between $5\%$ and $10\%$. In the third simulation setting, we consider a case with perfect specificity. The purpose of this scenario is to demonstrate that our methods recover unbiased association parameters, even in cases where we unnecessarily account for bidirectional misclassification. In this setting, we generated datasets with $5000$ members and imposed sensitivity rates around $82\%$ to $90\%$.

For a dataset with $1,000$ members, the single-stage analysis using our proposed EM algorithm took about $17$ seconds while our proposed MCMC analysis took approximately $35$ minutes. 

In the simulation settings for the two-stage model, we consider analogous cases to the single-stage simulations. First, we examine the case of a relatively small sample size and high misclassification rate, as this case would likely be highly problematic in a two-stage example. In this setting, datasets had $1000$ members and the imposed outcome misclassification rates for $Y^{*(1)}$ were between $10\%$ and $22\%$. The probability of correct measurement across all stages was set between $79\%$ and $92\%$. In Setting 5, we show that even with two sequential observed outcomes and a large sample size, even small misclassification rates can still impact parameter estimation. In this setting, we generated datasets with $10000$ members and imposed misclassifiction rates in $Y^{*(1)}$ between $4\%$ and $9\%$. In the sixth simulation setting, we evaluated the case where $Y^{*(1)}$ is measured without error, but $Y^{*(2)}$ is subject to misclassification. In Setting 7, we instead consider the case where $Y^{*(1)}$ is subject to misclassification, but $Y^{*(2)}$ has perfect specificity. These scenarios demonstrate that our multistage methods are still appropriate for cases where there is perfect measurement in at least one of the observed outcomes. In both Setting 6 and Setting 7, we generated datasets with $1000$ members and imposed misclassification rates between $10\%$ and $20\%$ in the imperfectly measured outcome.

For a dataset with $1,000$ members, the two-stage analysis using our proposed EM algorithm took about $25$ seconds. The MCMC analysis took considerably longer, at approximately $3.5$ hours. 

These settings are outlined in Table \ref{sim-setting-table1} and Table \ref{sim-setting-table2}. All analyses were conducted in \texttt{R} \citep{stats2021R}.

\begin{table}[htbt]
\centering
\caption{Number of generated datasets (N. Realizations), Sample size ($N$), $P(Y = 1)$, sensitivity ($P(Y^* = 1 | Y = 1)$), specificity ($P(Y^* = 2 | Y = 2)$), $\beta$ prior distribution, and $\gamma$ prior distribution settings for each of the the simulation Settings 1, 2, and 3.} \label{sim-setting-table1}
\begin{threeparttable}
\begin{tabular}{clrrrrr}
\hline
        Scenario & & & & Setting \\
\hline
    \\
(1)           && N. Realizations & & 500\\
              && $N$ & & 1000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^* = 1 | Y = 1)$ && 0.82 - 0.90 \\
              && $P(Y^* = 2 | Y = 2)$ && 0.82 - 0.90 \\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
              \\
(2)           && N. Realizations & & 500\\
              && $N$ & & 10000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^* = 1 | Y = 1)$ && 0.90 - 0.95 \\
              && $P(Y^* = 2 | Y = 2)$ && 0.90 - 0.95 \\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
              \\
(3)           && N. Realizations & & 500\\
              && $N$ & & 5000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^* = 1 | Y = 1)$ && 0.82 - 0.90 \\
              && $P(Y^* = 2 | Y = 2)$ && 1 \\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[htbt]
\centering
\caption{Number of generated datasets (N. Realizations), Sample size ($N$), $P(Y = 1)$, first-stage sensitivity ($P(Y^* = 1 | Y = 1)$), first-stage specificity ($P(Y^* = 2 | Y = 2)$), second-stage sensitivity ($P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$), second-stage specificity ($P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$), $\beta$ prior distribution, and $\gamma$ prior distribution settings for each of the the simulation Settings 4, 5, and 6.} \label{sim-setting-table2}
\begin{threeparttable}
\begin{tabular}{clrrrrr}
\hline
        Scenario & & & & Setting \\
\hline
    \\
(4)           && N. Realizations & & 500\\
              && $N$ & & 1000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ && 0.83 - 0.91 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ && 0.79 - 0.85\\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ && 0.85 - 0.92 \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ && 0.79 - 0.87\\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
(5)           && N. Realizations & & 500\\
              && $N$ & & 10000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ && 0.91 - 0.93 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ && 0.91 - 0.93 \\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ && 0.94 - 0.96 \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ && 0.91 - 0.93\\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
(6)           && N. Realizations & & 500\\
              && $N$ & & 1000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ && 1 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ && 1\\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ && 0.88 - 0.92\\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ && 0.82 - 0.90\\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
(7)           && N. Realizations & & 500\\
              && $N$ & & 1000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^{*(1)} = 1 | Y = 1)$ && 0.84 - 0.90 \\
              && $P(Y^{*(1)} = 2 | Y = 2)$ && 0.80 - 0.89 \\
              && $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1)$ && 0.88 - 0.92 \\
              && $P(Y^{*(2)} = 2 | Y^{*(1)} = 2, Y = 2)$ && 1\\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\subsection{Data Generation}
For each of the single-stage simulated datasets, we begin by generating the predictors $X$ and $Z$ from a multivariate Normal distribution. In Settings 1 and 3, the means were 0 and 1.5, respectively, for $X$ and $Z$. In Setting 2, the means were 0 and 2.5, respectively, for $X$ and $Z$. Covariate generation in all simulation settings used unit variances and covariance terms equal to 0.30. The absolute value of all $Z$ terms was taken. Next, we generated true outcome status using the following relationship: $P(Y = 1 | X) = 1 + (-2)X$. For Settings 1 and 2, we used the following relationships to obtain $Y^*$: $P(Y^* = 1 | Y = 1, Z) = 0.50 + (1)Z$ and $P(Y^* = 1 | Y = 2, Z) = -0.50 + (-1)Z$. The different $Z$ distribution between Settings 1 and 2 resulted in different misclassification rates. In Setting 1, misclassification rates were between $10\%$ and $18\%$. In Setting 2, misclassification rates were between $5\%$ and $10\%$. In Setting 3, we generated $Y^*$ using the following relationships: $P(Y^* = 1 | Y = 1, Z) = 0.50 + (1)Z$ and $P(Y^* = 1 | Y = 2, Z) = -5 + (-5)Z$. The choice of parameter values $(-5, -5)$ for $P(Y^* = 1 | Y = 2)$ resulted in near perfect specificity in the generated datasets. 

For each of the two-stage simulated datasets, we begin by generating the predictor $X$ from a standard Normal distribution and the predictors $Z^{(1)}$ and $Z^{(2)}$ from a Gamma distribution. The shape parameters for the Gamma distributions were 1 for both $Z^{(1)}$ and $Z^{(2)}$ in Setting 4, Setting 6, and Setting 7. In Setting 5, the shape parameters for the Gamma distributions were 2 for both $Z^{(1)}$ and $Z^{(2)}$. For Settings 4-7, we used the following relationship to generate the true outcome status: $P(Y = 1 | X) = 1 + (-2)X$. For Setting 4, Setting 5, and Setting 7, we obtained $Y^{*(1)}$ using the following relationships: $P(Y^{*(1)} = 1 | Y = 1, Z^{(1)}) = 1 + (1)Z^{(1)}$ and $P(Y^{*(1)} = 1 | Y = 2, Z^{(1)}) = -0.50 + (-1.5)Z^{(1)}$. In Setting 6, $P(Y^{*(1)} = 1 | Y = 1, Z^{(1)}) = 5 + (5)Z^{(1)}$ and $P(Y^{*(1)} = 1 | Y = 2, Z^{(1)}) = -5 + (-5)Z^{(1)}$. The choice of parameter values resulted in near perfect sensitivity and specificity for $Y^{*(1)}$ in the generated datasets. For Settings 4-6, we obtained $Y^{*(2)}$ using the following relationships: $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1, Z^{(2)}) = 1.5 + (1)Z^{(2)}$,
$P(Y^{*(2)} = 1 | Y^{*(1)} = 2, Y = 1, Z^{(2)}) = 0.50 + (0.50)Z^{(2)}$, $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 2, Z^{(2)}) = -0.50 + (0)Z^{(2)}$, and $P(Y^{*(2)} = 1 | Y^{*(1)} = 2, Y = 2, Z^{(2)}) = -1 + (-1)Z^{(2)}$. In Setting 7, the same relationships for $P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 1, Z^{(2)})$ and
$P(Y^{*(2)} = 1 | Y^{*(1)} = 2, Y = 1, Z^{(2)})$ were retained, but $P(Y^{*(2)} = 1 | Y^{*(1)} = 2, Y = 2, Z^{(2)}) = P(Y^{*(2)} = 1 | Y^{*(1)} = 1, Y = 2, Z^{(2)}) = -5 + (-5)Z^{(2)}$, ensuring perfect specificity in $Y^{*(2)}$.


\clearpage
\section{Simulation Setting 3 Results Figure} \label{appendix-b}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=.8]{ps_sim_histogram.png}
\medskip
  \caption{\textit{(Caption on next page.)}}\label{fig:ps_sim_results_histogram}
\end{center}
\end{figure}
\clearpage

\begin{figure*}
\noindent
Figure \ref{fig:ps_sim_results_histogram} Caption: Parameter estimates for 500 realizations of simulation Setting 3. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Solid lines represent true parameter values. Dashed lines represent mean parameter estimates for a given method.
\end{figure*}


\end{appendices}

\end{document}


