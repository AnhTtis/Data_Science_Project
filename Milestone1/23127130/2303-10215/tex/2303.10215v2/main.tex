\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}


\title{\LARGE Statistical inference for association studies in the presence of binary outcome misclassification}

\author[]{Kimberly A. Hochstedler Webb}
\author[]{Martin T. Wells}
\affil[]{Department of Statistics and Data Science, Cornell University \\ Ithaca, NY}
\affil[]{kah343@cornell.edu}

\usepackage[super,sort&compress]{natbib}
\setcitestyle{comma,numbers,super,open={},close={}} 
\usepackage{graphicx}
\usepackage[paperwidth=8.5in,paperheight=11.0in,top=1in, bottom=1in, left=1in, right=1in,lines=25]{geometry}
\usepackage[title,toc,titletoc,page]{appendix}

\usepackage{siunitx}
\usepackage{appendix}
\usepackage{mathtools}
\usepackage{threeparttable}
\usepackage{bbm}
\usepackage{caption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{url}
\usepackage{xcolor}

\usepackage{setspace}
\doublespacing
\setstretch{1.8}


%%% Macros
\DeclareMathOperator{\logit}{logit}
\usepackage{stix}

\begin{document}

\maketitle

\begin{abstract}
In biomedical and public health association studies, binary outcome variables may be subject to misclassification, resulting in substantial bias in effect estimates. The feasibility of addressing binary outcome misclassification in regression models is often hindered by model identifiability issues. In this paper, we characterize the identifiability problems in this class of models as a specific case of ``label switching'' and leverage a pattern in the resulting parameter estimates to solve the permutation invariance of the complete data log-likelihood. Our proposed algorithm in binary outcome misclassification models \textit{does not require gold standard labels} and relies only on the assumption that outcomes are correctly classified at least 50\% of the time. A label switching correction is applied within estimation methods to recover unbiased effect estimates and to estimate misclassification rates. Open source software is provided to implement the proposed methods. We give a detailed simulation study for our proposed methodology and apply these methods to data from the 2020 Medical Expenditure Panel Survey (MEPS).

\textbf{Keywords: } association studies, bias correction, EM algorithm, identification, label switching, MCMC
\end{abstract}

\newpage

%%%%%

\section{Introduction}
We consider regression models where a binary outcome variable is potentially misclassified. Misclassified binary outcomes are common in biomedical and public health association studies. For example, misclassification may occur in when a diagnostic test does not have perfect sensitivity or specificity \cite{manski2021estimating, ge2023enhanced, trangucci2022identified}. Misclassification can also be present in survey data, where individuals may falsely recall disease status on a self-report item \cite{althubaiti2016information}. More recently, medical studies may rely on computer algorithms to extract patient disease status from electronic medical records, but such algorithms do not perfectly capture true disease states, even when combined with record review from subject-area experts \citep{sinnott2014improving}. It is common for analysts to ignore potential misclassification in response variables, and instead assume that the outcome is perfectly measured. Such an approach generally produces biased parameter estimates \citep{khan2020introduction}, particularly in the case of covariate-related misclassification \citep{beesley2020statistical, zhang2020genetic}. 

Despite the known impact of covariate-related misclassification, previous work on recovering unbiased association parameters is limited. Neuhaus\cite{neuhaus1999bias} provides general expressions for bias in the presence of covariate-related misclassification. Lyles et al. \cite{lyles2011validation} and Lyles and Lin\cite{lyles2010sensitivity} extend this work, but require a validation sample or known sensitivity and specificity values, respectively, to identify association parameters. Beesley and Mukherjee \cite{beesley2020statistical} address the problem through a novel likelihood-based bias correction strategy, but make the strong assumption that the binary outcome is measured with perfect specificity. A perfect specificity assumption also underpins existing sensitivity analysis frameworks for evaluating the predictive bias of classifiers in the presence of outcome misclassification \citep{fogliato2020fairness}. Zhang and Yi\cite{zhang2020genetic} develop methods to correct bias in association parameters in the context of covariate-related misclassification, but they only address the problem for mixed continuous and binary bivariate outcomes. Our methods consider scenarios where validation data is not available, but the binary outcome is subject to covariate-dependent, bidirectional misclassification.

Numerous researchers instead assume that sensitivity and specificity may be considered constant \citep{magder1997logistic, daniel2003binomial, trangucci2022identified, carroll2006measurement, stamey2004parameter, xia2018bayesian, rekaya2016analysis}. In such cases, misclassification rates are often either assumed to be known \citep{magder1997logistic, trangucci2022identified} or estimated via validation data \citep{stamey2004parameter, carroll2006measurement}. In the event that an outcome measure has previously studied sensitivity and specificity rates, as is common for commercially available diagnostic tests, EM algorithm methods exist to incorporate that information into the fitting of logistic regression models, resulting in unbiased estimates of odds ratios \citep{magder1997logistic}. For cases where internal validation data are present, Carroll and colleagues \cite{carroll2006measurement} develop general expressions for likelihood functions that take imperfect outcome measurement into account. These methods rely on the availability of gold standard measures, which are not feasible in numerous biomedical and public health settings \citep{Faraone1994measuring, oneill2015measuring}. In contrast, fully Bayesian methods may be used to estimate constant misclassification rates in the absence of a gold standard, but such methods rely on strict and difficult prior elicitation strategies \citep{daniel2003binomial, rekaya2016analysis} or an additional assumption of equal sensitivity and specificity \citep{smith2013genome, rekaya2001threshold}.

%Other methods for correcting outcome misclassification rely on available gold standard measures \citep{stamey2004parameter} or repeated measures \citep{albert1997modeling}. For cases where internal validation data are present, \cite{carroll2006measurement} develop general expressions for likelihood functions that take imperfect outcome measurement into account. In the event that an outcome measure, like a diagnostic test, has known sensitivity and specificity rates, \cite{magder1997logistic} propose an EM algorithm to incorporate that information into the fitting of logistic regression models, resulting in unbiased estimates of odds ratios. 

%\cite{magder1997logistic} also propose using their method as a sensitivity analysis, as a way to estimate the degree to which misclassification may bias one's parameter estimates. \cite{fogliato2020fairness} use a similar sensitivity analysis framework when considering outcome misclassification in the context of fairness evaluation, but the authors assume perfect specificity in their methods. 

The impact of misclassification has also been considered for estimates of outcome prevalence. Econometricians have used partial identifiability to address misclassification and estimate population rates of SARS CoV-2 infection \citep{ziegler2020binary, manski2021estimating}, but these methods do not readily extend to association studies. Misclassification can also impact variance estimation in prevalence studies. Ge et al. \cite{ge2023enhanced} derive an estimable variance component that is induced by misclassification in certain testing applications, but their methods also rely on known sensitivity and specificity rates. 

In the machine learning literature, the problem of outcome misclassification, or ``noisy labels'', is typically handled with noise-robust methods or data cleaning strategies \citep{frenay2013classification}. Other methods approach misclassification from a fairness lens, where an optimization approach is proposed to ``flip'' predicted labels after estimation to alleviate systematic bias without sacrificing the value of ``merit'' within the classification scheme \citep{bandi2021price}. When imperfect classification algorithms are used to obtain disease states, Sinnott and colleagues \cite{sinnott2014improving} demonstrate that modeling outcome probabilities, rather than outcomes obtained via thresholding, improves power and estimation accuracy. 

In this paper, we develop new strategies to recover unbiased parameter estimates in association studies with outcome misclassification that is related to observed covariates. We propose a bias correction strategy that requires minimal external information, no gold standard labels, and no limitations on the misclassification patterns present in the data.

The feasibility of addressing outcome misclassification in association studies is often limited by model identifiability issues \citep{lyles2011validation, duan2021global}. One aspect of model identifiability that must be addressed for a binary outcome misclassification model is the phenomenon of ``label switching''. Label switching describes the invariance of the likelihood under relabeling of the mixture components, resulting in multimodal likelihood functions \citep{redner1984mixture}. Several remedies have been suggested to break the permutation invariance of the likelihood in both Bayesian and frequentist mixture models \citep{rodriguez2014label, yao2015label}. One common method is to impose ordering constraints on component model parameters, such as $\pi_{1} < \pi_2$ \citep{betancourt2017identifying}. Such strategies aim to remove the permutation invariance of the likelihood, but are only successful for carefully chosen constraints \citep{stephens2000dealing}. In Bayesian settings, one potential resolution of the labeling degeneracy is to use non-exchangeable prior distributions to strongly separate possible parameter sets \citep{betancourt2017identifying}. It is rare, however, that analysts have enough information to set useful ordering constraints or strongly separated prior distributions in practice \citep{rodriguez2014label}. Moreover, when prior distributions for sensitivity and specificity are misspecified, bias correction approaches tend to perform poorly \citep{ni2019comparing, daniel2003binomial}. As such, we develop a novel label switching remedy that relies only on the reasonable assumption that correct outcome classification occurs in at least $50\%$ of observations.

We use our label switching correction procedure to develop both frequentist and Bayesian estimation methods for regression parameters in an association study. This strategy also allows us to accurately estimate the average misclassification rates in the response variable and characterize the mechanism by which misclassification occurs.

In addition to label switching, several authors have considered other characteristics of misclassification models that impact identifiability. Xia and Gustafson \cite{xia2018bayesian} outlines cases of unidirectional outcome misclassification where key regression parameters are identifiable. In many practical settings, however, they find that some covariate terms can only be weakly identified. Numerical problems related to weak identifiability are also encountered the joint estimation methods by Beesley and Mukherjee \cite{beesley2020statistical}. These authors address the problem by fixing model parameters at known values. In general, however, models that include a parametric observation mechanism for misclassified outcomes are identifiable if 1) perfect sensitivity or perfect specificity is assumed and 2) at least one continuous covariate is included in the true outcome mechanism but not the observation mechanism, or vice versa \cite{beesley2020statistical, diop2011maximum}. For models that do not have covariate-dependent misclassification rates, global identifiability of univariate logistic regression with outcome misclassification is ensured if at least four values of the covariate are observed and the true regression coefficient in the model is non-zero \cite{duan2021global}. For misclassification models that fall into the larger class of finite mixtures of logistic regressions, as our proposed framework does, identifiability is guaranteed as long as there is a sufficient number of unique observed covariate combinations \cite{follmann1991identifiability}. If the mixture has two components, this sufficient number of covariates is just seven \cite{follmann1991identifiability, kelley2008zero}. Throughout this paper, we will assume that this criterion for identification is met. 

In Section \ref{model}, we describe the conceptual framework for our model. Section \ref{label-switching} describes the label switching problem in greater detail and proposes a strategy to break permutation invariance of the likelihood in a binary outcome misclassification model. In Section \ref{estimation-methods}, we propose frequentist and Bayesian estimation strategies to recover the true association of interest in the presence of potentially misclassified observed outcomes. For all strategies proposed, we provide software for implementation using the R package \textit{COMBO} (COrrecting Misclassified Binary Outcomes). Through simulation, we demonstrate the utility of these methods to reduce bias in parameter estimates when compared to analyses that place restrictions on or ignore outcome misclassification. Finally, we apply our proposed methods to investigate am applied example. In particular, we study risk factors for myocardial infarction, which is known to be misdiagnosed on the basis of gender and age \citep{arber2006patient, maserejian2009disparities, mckinlay1996non}. 

\section{Model, Notation, and Conceptual Framework} \label{model}
Let $Y = j$ denote an observation's true outcome status, taking values $j \in \{1, 2\}$. Suppose we are interested in the relationship between $Y$ and a set of predictors, $X$, that are correctly measured. This relationship constitutes the \textit{true outcome mechanism}. Let $Y^* = k$ be the observed outcome status, taking values $k \in \{1,2\}$. $Y^*$ is a potentially misclassified version of $Y$. Let $Z$ denote a set of predictors related to sensitivity and specificity. The mechanism that generates the observed outcome, $Y^*$, given the true outcome, $Y$, is called the \textit{observation mechanism}. Figure \ref{conceptual_framework_figure} displays the conceptual model. The conceptual process is mathemathically expressed as
\begin{equation}
\begin{aligned}
\label{eq:conceptual_framework_eq}
\text{True outcome mechanism: } &\; \logit\{ P(Y = j | X ; \beta) \} = \beta_{j0} + \beta_{jX} X \\
\text{Observation mechanism: } &\; \logit\{ P(Y^* = k | Y = j, Z ; \gamma) \} = \gamma_{kj0} + \gamma_{kjZ} Z.
\end{aligned}
\end{equation}

In the true outcome mechanism, we use category $Y = 2$ as the reference category, and set all corresponding $\beta$ parameters to 0. Similarly, $Y^* = 2$ is the reference category in the observation mechanism, and all corresponding $\gamma$ parameters are also set to 0. Using (\ref{eq:conceptual_framework_eq}), we can express response probabilities for individual $i$'s true outcome category and for individual $i$'s observed category, conditional on the true outcome: 
\begin{flalign}
\begin{aligned}
\label{eq:response_probabilities_eq}
P(Y_i = j | X ; \beta) = &\; \; \pi_{ij} = \frac{\text{exp}\{\beta_{j0} + \beta_{jX} X_i\}}{1 + \text{exp}\{\beta_{j0} + \beta_{jX} X_i\}} \\
P(Y^*_i = k | Y_i = j, Z ; \gamma) = &\; \pi^*_{ikj} = \frac{\text{exp}\{\gamma_{kj0} + \gamma_{kjZ} Z_i\}}{1 + \text{exp}\{\gamma_{kj0} + \gamma_{kjZ} Z_i\}}.
\end{aligned}
\end{flalign}

These quantities can be calculated for all $N$ observations in the sample. For $j$ and $k$ both equal to the reference category, $\sum_{i = 1}^N \pi^*_{i22} = \pi^*_{22}$ measures the specificity in the data. When $j$ and $k$ are both $1$, $\sum_{i = 1}^N \pi^*_{i11} = \pi^*_{11}$ measures the sensitivity. Thus, (\ref{eq:response_probabilities_eq}) allows us to model sensitivity and specificity based on a set of covariates, $Z$.
It is common for analysts to ignore potential misclassification in $Y^*$, and instead to use a naive \textit{analysis model} $Y^* | X$ and interpret the results under the \textit{true outcome model} $Y | X$. Previous work has shown that this approach produces bias in $P(Y^* = j | X)$ relative to $P(Y = j | X)$, particularly in the case of covariate-related misclassification \citep{beesley2020statistical}. 

We define the probability of observing outcome $k$ using the model structure as
\begin{equation}
\begin{aligned}
\label{eq:p_obs_Ystar}
P(Y^* = k | X, Z) = \sum_{j = 1}^2 P(Y^* = k | Y = j, Z ; \gamma) P(Y = j | X ; \beta) = \sum_{j = 1}^2 \pi^*_{kj} \pi_{j}.
\end{aligned}
\end{equation}

The contribution to the likelihood by a single subject $i$ is thus $\prod_{k = 1}^2 P(Y^*_i = k | X_i, Z_i)^{y^*_{ik}}$ where $y^*_{ik} = \mathbbm{I}(Y^*_i = k)$ and where $\mathbbm{I}(A)$ is the indicator of the set $A$. We can estimate $(\beta, \gamma)$ using the following observed data log-likelihood for subjects $i = 1 \dots N$ as
\begin{equation}
\begin{aligned}
\label{eq:obs-log-like}
\ell_{obs}(\beta, \gamma; X, Z) = \sum_{i = 1}^N \sum_{k = 1}^2 y^*_{ik} \text{log} \{ P(Y^*_i = k | X_i, Z_i) \} = \sum_{i = 1}^N \sum_{k = 1}^2 y^*_{ik} \text{log} \{ \sum_{j = 1}^2 \pi^*_{ikj} \pi_{ij} \}.
\end{aligned}
\end{equation}

The observed data log-likelihood is difficult to use directly for estimation because jointly maximizing $\beta$ and $\gamma$ is numerically challenging, especially for large datasets. 

Viewing the true outcome value $Y$ as a latent variable, we may also construct the complete data log-likelihood based on the model structure as
\begin{equation}
    \begin{aligned}
    \label{eq:complete-log-like}
    \ell_{complete}(\beta, \gamma; X, Z) &= \sum_{i = 1}^N \Bigg[ \sum_{j = 1}^2 y_{ij} \text{log} \{ P(Y_i = j | X_i) \} + \sum_{j = 1}^2 \sum_{k = 1}^2 y_{ij} y^*_{ik} \text{log} \{ P(Y^*_i = k | Y_i = j, Z_i) \}\Bigg] & \\
    &= \sum_{i = 1}^N \Bigg[ \sum_{j = 1}^2 y_{ij} \text{log} \{ \pi_{ij} \} + \sum_{j = 1}^2 \sum_{k = 1}^2 y_{ij} y^*_{ik} \text{log} \{ \pi^*_{ikj} \}\Bigg],
    \end{aligned}
\raisetag{12pt}\end{equation}
where $y_{ij} = \mathbbm{I}(Y_i = j)$. Without the true outcome value, $Y$, we cannot use this likelihood form directly for maximization. It is notable, however, that (\ref{eq:complete-log-like}) can be viewed as a mixture model with latent mixture components, $y_{ij}$, and covariate-dependent mixing proportions, $\pi_{ij}$.

\section{Label Switching} \label{label-switching}
The structure of the models described in Section \ref{model} suffers from the known problem of label switching in mixture likelihoods. Mixture likelihoods are \textit{invariant under relabeling of the mixture components}, resulting in multimodal likelihood functions \citep{redner1984mixture}. Specifically, a $J$-dimensional mixture model will have $J!$ modes in the likelihood \citep{betancourt2017identifying}. Given that our proposed model in (\ref{eq:complete-log-like}) is a mixture with $J = 2$ components labeled by the true outcome $Y \in \{1,2\}$, there are $J! = 2! = 2$ peaks in the likelihood resulting in $2$ plausible parameter sets. 

\subsection{Permutation Invariance of the Complete Data Likelihood} \label{permutation-invariance-complete}
The invariance of the complete data log-likelihood under relabeling of mixture components is displayed in (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}). In (\ref{eq:permute-invariant-likelihood-1}), we label $Y$ terms in the order of appearance as either $1$ or $2$. In (\ref{eq:permute-invariant-likelihood-2}), all terms that had $Y = 1$ are replaced with that of $Y = 2$ and all terms that had $Y = 2$ are replaced with $Y = 1$. It follows that,
\begin{equation}
\label{eq:permute-invariant-likelihood-1}
    \begin{aligned}
  \ell_{complete}(\beta, \gamma; X, Z) = &\sum_{i = 1}^N \Bigl[ y_{i1} \log \{ \pi_{i1} \}  + y_{i2} \log \{ \pi_{i2} \} & \\
  &\phantom{a}+ y_{i1} y^*_{i1} \log \{ \pi^*_{i11} \}  + y_{i1} y^*_{i2} \log \{ \pi^*_{i21} \}  +
   y_{i2} y^*_{i1} \log \{ \pi^*_{i12} \}  + y_{i2} y^*_{i2} \log \{ \pi^*_{i22} \} \Bigr],
    \end{aligned}
\end{equation}
\begin{equation}
\label{eq:permute-invariant-likelihood-2}
    \begin{aligned}
  \ell_{complete}(\beta, \gamma; X, Z) = &\sum_{i = 1}^N \Bigl[y_{i2} \log \{ \pi_{i2} \} +  y_{i1} \log \{ \pi_{i1} \} \\
  &\phantom{a} + y_{i2} y^*_{i1} \log \{ \pi^*_{i12} \}  + y_{i2} y^*_{i2} \log \{ \pi^*_{i22} \}  +
   y_{i1} y^*_{i1} \log \{ \pi^*_{i11} \}  + y_{i1} y^*_{i2} \log \{ \pi^*_{i21} \} \Bigr].
    \end{aligned}
\end{equation}

Due to the additive structure of the complete data log-likelihood, the label change between (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}) does not impact the value of the function. Thus, the complete data log-likelihood in our setting is invariant under relabeling of mixture components. 

Suppose for each $y_{ij}$ we have a single predictor $x_i$ in the true outcome mechanism and a single predictor $z_i$ in the observation mechanism. Substituting the parametric form of the response probabilities from (\ref{eq:response_probabilities_eq}) into (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}), we can detect a pattern in the parameters associated with each likelihood mode, that is,
\begin{equation}
\label{eq:permute-invariant-param-1}
    \begin{aligned}
&\sum_{i = 1}^N \Bigl[ y_{i1}\beta_0 + y_{i1} x_i \beta_X - (y_{i1} + y_{i2}) \log \{ 1 + \exp \{ \beta_0 + x_i \beta_X \} \} \\
     &\qquad\phantom{a}+ y_{i1} y^*_{i1} \gamma_{110} + y_{i1} y^*_{i1} z_i \gamma_{11Z} - ( y^*_{i1} + y^*_{i2} ) y_{i1} \log \{1 +  \exp \{  \gamma_{110} + z_i \gamma_{11Z} \}  \} \\
     &\qquad\phantom{a}+ y_{i2} y^*_{i1} \gamma_{120} + y_{i2} y^*_{i1} z_i \gamma_{12Z} - (y^*_{i1} + y^*_{i2}) y_{i2}  \log \{ 1 +  \exp \{  \gamma_{120} + z_i \gamma_{12Z} \} \}  \Bigr] \\
 = &\sum_{i = 1}^N \Bigl[ y_{i2}(-\beta_0) + y_{i2} x_i (-\beta_X) - (y_{i1} + y_{i2}) \log \{ 1 + \exp \{ -\beta_0 + x_i (-\beta_X) \} \} \\
     &\qquad\phantom{a}+ y_{i2} y^*_{i1} \gamma_{120} + y_{i2} y^*_{i1} z_i \gamma_{12Z} - ( y^*_{i1} + y^*_{i2} ) y_{i2} \log \{1 +  \exp \{  \gamma_{120} + z_i \gamma_{12Z} \}  \} \\
     &\qquad\phantom{a}+ y_{i1} y^*_{i1} \gamma_{110} + y_{i1} y^*_{i1} z_i \gamma_{11Z} - (y^*_{i1} + y^*_{i2}) y_{i1}  \log \{ 1 +  \exp \{  \gamma_{110} + z_i \gamma_{11Z} \} \}  \Bigr].
   \end{aligned}
\end{equation}

Specifically, label switching generates the following two parameter sets:
(1) $\beta_0, \beta_X, \gamma_{110},  \gamma_{11Z}, \gamma_{120}, \gamma_{12Z}$,
and
(2) $-\beta_0, -\beta_X, \gamma_{120}, \gamma_{12Z}, \gamma_{110}, \gamma_{11Z}$.
Suppose the true values of each parameter were equal to real numbers $a, b, c, d, e, f$, respectively, for parameter set 1. The two parameter sets corresponding to these values would be: (1)  $a, b, c, d, e, f$, and (2) $-a, -b, e, f, c, d$. That is, the $\beta$ parameters change signs while the $\gamma$ parameters change $j$ subscripts between the two parameter sets. This pattern is stable for any dimension of $x$ and $z$ vector predictors. In practice, if an analyst uses frequentist estimation methods, they will recover only one of the two parameter sets. When this model is fit via Gibbs Sampling, it is typical for different chains to center at different parameter sets. Moreover, each chain is generally unable to transition \textit{between} modes within a finite running time \citep{betancourt2017identifying}. Thus, it is possible for a final posterior sample to reflect estimates from different ``label switched'' parameter sets, making the results difficult to meaningfully summarize \citep{stephens2000dealing}.

\subsection{A Strategy to Correct Label Switching}
Given that the two likelihood modes present in the proposed model are known, we now describe a method to select the correct parameter set. After obtaining parameter estimates $\hat{\beta}$ and $\hat{\gamma}$ from an estimation method described in Section \ref{estimation-methods}, the appropriate parameter set, $\hat{\beta}_{corrected}$ and $\hat{\gamma}_{corrected}$, can be determined using the following algorithm.

\begin{algorithm}[H]
\caption{Correcting label switching in binary outcome misclassification models}\label{alg:label-switch}
\begin{algorithmic}
\State Compute average $\pi^*_{jj}$ for all $j \in \{1,2\}$ using $\hat{\beta}$ and $\hat{\gamma}$.
\If{$ \pi^*_{jj} > 0.50$ for all $j \in \{1, 2 \}$} 
    \State $\hat{\beta}_{corrected} \gets \hat{\beta}$
    \State $\hat{\gamma}_{corrected} \gets \hat{\gamma}$
\Else
    \State $\hat{\beta}_{corrected} \gets -\hat{\beta}$
    \State $\hat{\gamma}_{corrected, k1} \gets \hat{\gamma_{k2}}$
    \State $\hat{\gamma}_{corrected, k2} \gets \hat{\gamma_{k1}}$
\EndIf 
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:label-switch} relies on the following assumption:
\newtheorem{assumption}{Assumption}
\begin{assumption}
Within identified models, the probability of correct classification is at least 0.50, on average across subjects $i$; that is, $\pi^*_{jj} > 0.50$. 
\end{assumption}

Assumption 1 is required because it allows for differentiation between the two possible parameter sets for this model. As displayed in (\ref{eq:permute-invariant-likelihood-1}) and (\ref{eq:permute-invariant-likelihood-2}), each parameter set assigns a different category of the latent variable, $Y = 1$ or $Y = 2$, to each component of the mixture likelihood. This means that when we compute $\pi^*_{kj} = P(Y^* = k | Y = j)$ for a given observed outcome category, $k$, it is possible that $\pi^*_{kj}$ could refer to $\pi^*_{k1}$ or $\pi^*_{k2}$. In order to differentiate between these two latent variable assignments, we make the assumption that $\pi^*_{k = j, j} > \pi^*_{k \neq j, j}$ which implies $\pi^*_{k = j, j} > 0.50$. Thus, we are able to determine which latent category labeling is more plausible in our results. Assumption 1 is reasonable because it assumes that the observation mechanism is at least as good as a coin flip, on average.

Such assumptions are not required in other methods like \cite{beesley2020statistical} and \cite{xia2018bayesian} because unidirectional misclassification is assumed. Thus, only one latent outcome category is present in the complete data log-likelihood and so the resulting functional form is not permutation invariant. 

\section{Estimation Methods} \label{estimation-methods}
In this section, we describe two estimation methods for our proposed binary outcome misclassification model that exploit the latent variable structure of this problem. First, we propose jointly estimating $\beta$ and $\gamma$ using the expectation-maximization (EM) algorithm \citep{dempster1977maximum}. Next, we outline Bayesian methods for analyzing data from association studies in the presence of binary outcome misclassification. Both estimation strategies are available in the R package \textit{COMBO}. 

\subsection{Maximization Using an EM Algorithm} \label{em}
We use the complete data log-likelihood as the starting point for the EM algorithm. Since (\ref{eq:complete-log-like}) is linear in $y_{ij}$, we can replace $y_{ij}$ in the E-step of the EM algorithm with the quantity
\begin{equation}
\begin{aligned}
\label{eq:e-step}
w_{ij} = P(Y_i = j | Y_i^*, X, Z) = \sum_{k = 1}^2 \frac{y^*_{ik} \pi^*_{ikj} \pi_{ij}}{\sum_{\ell = 1}^2 \pi^*_{i k \ell} \pi_{i \ell}}.  
\end{aligned}
\end{equation}

In the M-step, we maximize the expected log-likelihood with respect to $\beta$ and $\gamma$
\begin{equation}
\begin{aligned}
\label{eq:m-step}
Q = \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w_{ij} \text{log} \{ \pi_{ij} \} + \sum_{j = 1}^2 \sum_{k = 1}^2 w_{ij} y^*_{ik} \text{log} \{ \pi^*_{ikj} \}\Bigr].
\end{aligned}
\end{equation}

The $Q$ function in (\ref{eq:m-step}) can be split into three separate equations for estimating $\beta$ and $\gamma$
%\begin{align}
%\label{eq:q-beta}
%Q_{\beta} &= \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w_{ij} \text{log} \{ \pi_{ij} \}\Bigr]\\
%\label{eq:q-gamma1}
%Q_{\gamma_{k1}} &= \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i1} y^*_{ik} \text{log} \{ \pi^*_{ik1} \}\Bigr]\\
%\label{eq:q-gamma2}
%Q_{\gamma_{k2}} &= \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i2} y^*_{ik} \text{log} \{ \pi^*_{ik2} \}\Bigr].
%\end{align}
\begin{align}
\label{eq:q-split}
Q_{\beta} = \sum_{i = 1}^N \Bigl[ \sum_{j = 1}^2 w_{ij} \text{log} \{ \pi_{ij} \}\Bigr], \: Q_{\gamma_{k1}} = \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i1} y^*_{ik} \text{log} \{ \pi^*_{ik1} \}\Bigr], \:
Q_{\gamma_{k2}} = \sum_{i = 1}^N \Bigl[\sum_{k = 1}^2 w_{i2} y^*_{ik} \text{log} \{ \pi^*_{ik2} \}\Bigr].
\end{align}
In practice, $Q_{\beta}$ in (\ref{eq:q-split}) can be fit as a logistic regression model where the outcome is replaced by weights, $w_{ij}$. $Q_{\gamma_{k1}}$ and $Q_{\gamma_{k2}}$ in (\ref{eq:q-split}) each are fit as weighted logistic regression models where the outcome is $y^*_{ik}$ \citep{agresti2003categorical}. After estimates for $\beta$ and $\gamma$ are obtained, Algorithm \ref{alg:label-switch} is required to
%address the problem of ``label switching'' and
return final parameter estimates. The covariance matrix for $\beta$ and $\gamma$ is obtained by inverting the expected information matrix.

\subsection{Bayesian Modeling}\label{mcmc}
Our proposed binary outcome misclassification model is: $Y^*_{ik} | \pi^*_{ik} \sim Multinomial (\pi^*_{ik}, J)$, where $\pi^*_{ik} = \sum_{j = 1}^2 \pi^*_{ikj} \pi_{ij}$ as in (\ref{eq:p_obs_Ystar}) and $J$ denotes the number of outcome categories (in this case, $2$). 

Assumptions and prior distributions for the parameters are based on input from subject-matter experts on the data the model is applied to. It is recommended that prior distributions are proper and relatively flat to ensure model identifiability without strongly influencing the posterior mean estimation. For example, in cases where non-informative priors are desired, a Uniform prior with a wide range may be selected. If an analyst has previous data suggesting a plausible estimate for a given parameter, a Normal prior distribution with a wide variance that is centered at the previous estimate may be used. In the R Package, \textit{COMBO}, analysts may select between Uniform, Normal, Double Exponential, or t prior distributions, with user-specified prior parameters. Before summarizing the results, Algorithm \ref{alg:label-switch} should be applied on \textit{each individual MCMC chain}, to address potential label switching within a given chain. Standard methods can be used to compute variance metrics. 

\section{Simulations} \label{simulations}
We present simulations for evaluating the proposed binary outcome misclassification model in terms of bias and root mean squared error (rMSE). Our methods are compared to the \textit{SAMBA} R package and an adapted version of \textit{SAMBA} where the \textit{observation mechanism} is assumed to have either perfect specificity or perfect sensitivity, respectively. We also compare all methods that account for misclassification to a naive logistic regression that assumes there is no measurement error in the observed outcome. 

The model simulation study includes three settings: (1) small sample size and large misclassification rates, (2) large sample size and small misclassification rates, and (3) moderate sample size with perfect specificity. Details on these settings can be found in Appendix \ref{appendix-a}. 

Figures 2 and 3 present the parameter estimates for Setting 1 and Setting 2, respectively, across 500 simulated datasets. Figure \ref{fig:ps_sim_results_histogram} presents parameter estimates under Setting 3. Table \ref{parameter-results-table} presents mean parameter estimates and rMSE across 500 simulated datasets for each setting and estimation method. Table \ref{probability-results-table} presents the true outcome probability, sensitivity, and specificity values measured from the generated data and estimated from the EM algorithm and MCMC results for each simulation scenario.

\textbf{Setting 1:} Across all simulated datasets, the average $P(Y = 1)$ was 64.7\% and the average $P(Y^* = 1)$ was 59.1\% (Table \ref{probability-results-table}). The average correct classification rate was 84.7\% for $Y = 1$ and 87.7\% for $Y = 2$. In Table \ref{parameter-results-table}, we see that the naive logistic regression analysis results in substantial bias the estimate of $\beta_X$. If only one type of misclassification is assumed using \textit{SAMBA} EM or ``Perfect Sensitivity EM'', bias in the estimated parameters is high. Our proposed EM Algorithm performs well for $\beta$ parameter estimates, but has wide variation in some $\gamma$ estimates. Similar results are observed for our propsed MCMC method. The rMSE in the parameter estimates is generally smaller when comparing the EM estimates to the MCMC estimates. The EM estimates also achieve lower rMSE than estimates from \textit{SAMBA} EM and Perfect Sensitivity EM. Both the EM Algorithm and MCMC methods recover the true outcome probabilities $P(Y = 1)$ and $P(Y = 2)$, but MCMC tends to consistently overestimate the sensitivity and specificity values compared to the EM Algorithm (Table \ref{probability-results-table}).

\textbf{Setting 2:} In Setting 2, the average $P(Y = 1)$ was 64.7\% and the average $P(Y^* = 1)$ was 61.8\% across all simulated datasets (Table \ref{probability-results-table}). The average correct classification rate was 92.4\% for $Y = 1$ and 94.5\% for $Y = 2$. There is substantial bias in the estimate of $\beta_X$ in the naive logistic regression and perfect sensitivity EM results in Table \ref{parameter-results-table}. If perfect specificity is assumed in \textit{SAMBA} EM, the bias in the estimate of $\beta_X$ is improved compared to the naive and perfect sensitivity cases, but is much higher than if both types of misclassification are accounted for. Our proposed EM Algorithm method performs well for $\beta$ and $\gamma$ parameter estimates. Similar results are observed for the proposed MCMC method, though bias in some $\gamma$ estimates is higher than that observed using the EM algorithm. The rMSE in the $\beta$ parameter estimates is comparable between the EM and MCMC methods. However, rMSE in the $\gamma$ parameter estimates is generally smaller for EM than MCMC. Both EM and MCMC estimates, however, achieve lower rMSE than estimates from the methods that make perfect sensitivity and/or specificity assumptions. In Table \ref{probability-results-table}, we see that both the EM and MCMC methods recover the true outcome probabilities with virtually no bias. In addition, both methods achieve low bias in the estimation of $P(Y^* = 1 | Y = 1)$ and $P(Y^* = 2 | Y = 2)$.

\textbf{Setting 3:} In Setting 3, the average $P(Y = 1)$ was 64.7\% and the average $P(Y^* = 1)$ was 54.8\% (Table \ref{probability-results-table}). Per the simulation design, there was no misclassification for $Y = 2$. The average correct classification rate was 84.6\% for $P(Y = 1)$. In this setting, the naive analysis and perfect sensitivity methods return highly biased $\beta$ estimates (Table \ref{parameter-results-table}). There is also substantial bias in the $\gamma$ estimates for the perfect sensitivity EM method. This is unsurprising, since the $\gamma_{120}$ and $\gamma_{12Z}$ terms govern the specificity portion of the \textit{observation mechanism}, which is error-free in this setting. The framework of Setting 3 matches the \textit{SAMBA} EM method developed by Beesley and Mukherjee\cite{beesley2020statistical}, and it is unsurprising that low bias and rMSE is observed across all  \textit{SAMBA} EM parameter estimates. Despite making no assumptions on misclassification direction in a setting with perfect specificity, the proposed EM and MCMC parameter estimates have low bias for all $\beta$ terms and for $\gamma_{110}$ and $\gamma_{11Z}$. Substantial bias is observed in the EM case for $\gamma_{120}$ and $\gamma_{12Z}$. This bias is largely the result of extreme parameter estimates in some simulated datasets, as observed in Figure \ref{fig:ps_sim_results_histogram}. Despite the extreme variation in the $\gamma_{120}$ and $\gamma_{12Z}$ estimates across simulations, the remaining parameters are estimated with low bias and with rMSE near that of the \textit{SAMBA} EM estimates (Table \ref{parameter-results-table}). In the MCMC results for $\gamma_{120}$ and $\gamma_{12Z}$, we see much less bias and much smaller rMSE than the EM estimates. This behavior is likely due to the Uniform prior distribution, which limited the posterior samples to the $[-10, 10]$ range. In Table \ref{probability-results-table}, both the EM and MCMC methods return accurate estimates of $P(Y = 1)$ and $P(Y = 2)$. The EM method slightly overestimates sensitivity and slightly underestimates specificity. The MCMC method also slightly overestimates sensitivity, but correctly estimates perfect specificity in the datasets.

\section{Applied Example} \label{example}
In this section, we perform a case study using data from the 2020 Medical Expenditure Panel Survey (MEPS), a sequence of surveys on cost and use of health care and health insurance status in the United States \citep{AHRQ}. We are primarily interested in risk factors associated with myocardial infarction (MI). This association, however, is potentially impacted by misclassification in self-reported history of MI. In particular, it is possible for the symptoms of other ailments to be attributed to MI and for the chest pain of MI to be attributed to a different diagnosis. These types of misdiagnoses are known to be more common in women than men \citep{arber2006patient, maserejian2009disparities} and among younger patients \citep{mckinlay1996non}. Thus, we expect misclassification of self-reported history of MI to be associated with patient gender and age. Our overall goal is to assess the severity of this misclassification, and to understand the impact of misclassification on the association between risk factors and MI. The MEPS 2020 survey was used for this analysis because, at the time of writing, this was the most recently collected MEPS data. In addition, the disruption of the COVID-19 pandemic in the 2020 year may have contributed additional measurement error to medical diagnoses in the dataset  \citep{AHRQ}.

Our response variable of interest is whether or not survey respondents reported any history of MI. We assessed the association of MI status with risk factors including age, smoking status, and exercise habits. The age variable was centered and scaled before use in the model. Smoking status and exercise variables were coded as binary indicators. The use of a continuous age variable ensures that we have at least seven unique covariate values in this setting. Thus, we have satisfied the conditions for identification of a finite mixture model with two mixture components.

For the analysis, we included data only from participants age 18-85. For each family unit recorded in the dataset, we only keep the responses of the reference person. After excluding all of the records with missing values in the response and covariates, we had a total of 12731 observations on our analysis. In this dataset, 640 (5\%) reported a history of MI.

We estimate model parameters using the EM algorithm with label switching correction from Section \ref{em}. Parameter estimates and standard errors are reported in Table \ref{applied-results-table}. The $\beta$ effect estimates in the naive analysis are all attenuated compared to the EM analysis that accounts for MI misclassification. We find that smoking, not exercising regularly, and increased age are all associated with true incidence of MI in the sample. Our results are also robust with respect to imperfect sensitivity and imperfect specificity. In the sensitivity component of the \textit{observation mechanism}, we find that, given that MI has occurred, individuals who are older and/or female are less likely to report having been diagnosed with MI. Given that MI has not occurred, we find that individuals who are female are still less likely to report an MI diagnosis, but older individuals are more likely to report having been diagnosed with MI.

We use the EM parameter estimates to estimate the sensitivity and specificity of the self-reported MI measure among males and females in the sample. Among males, the sensitivity is estimated at 76.3\% and the specificity is estimated at 94.4\%. Among females, sensitivity is much lower, at only 59.1\%, and specificity is estimated at 97.1\%. These findings are in line with previous literature which suggests that many instances of MI go misdiagnosed or undiagnosed \citep{turkbey2015prevalence}, and this problem is more pronounced for female patients in comparison to male patients \citep{arber2006patient, maserejian2009disparities}.

\section{Discussion}
Association studies that use misclassified outcome variables are susceptible to bias in effect estimates \citep{beesley2020statistical, khan2020introduction}. In this paper, we presented methods to recover unbiased regression parameters when a binary outcome variable is subject to misclassification and provide software in the R package \textit{COMBO}. These methods are based a novel label switching correction algorithm, which is required to overcome model identifiability problems inherent in the likelihood structure. We show that our methods are able to recover parameter estimates in cases with varying sample sizes and misclassification rates, compared to methods that assume perfect sensitivity, perfect specificity, or no misclassification in the outcome measure. To show the utility of our method in real-world problems, we apply our methods to data from the 2020 Medical Expenditure Panel Survey (MEPS) and estimate that sensitivity and specificity rates for MI diagnosis differ by patient gender and age.

Our methods are attractive because they require little external information, can be implemented without repeated measures or double sampled outcomes, and do not require gold standard labels. Our findings also constitute a generalization of the work of Beesley and Mukherjee \cite{beesley2020statistical}. Specifically, we no longer require the assumption of perfect specificity, but can still handle cases where such an assumption is valid. 

Further generalizations of our methods are still possible in future work. In particular, researchers may consider the more difficult case of a potentially misclassified outcome that can take on three category labels. In addition, other label switching correction strategies may be explored. While the assumption in Algorithm \ref{alg:label-switch} requiring at least 50\% of cases to be correctly classified may be reasonable, there are scenarios where such a restriction is not appropriate. For example, an investigation of MI prevalence found that as many as 80\% of MI cases detected via myocardial scarring went undetected by traditional clinical methods \citep{turkbey2015prevalence}. Thus, even our applied example in Section \ref{example} may not have fully captured the misclassification present in the MEPS MI data. Further research is needed to fully dismantle the permutation invariance of likelihoods in misclassification models, while relaxing assumptions on sensitivity and specificity rates. Future work can also explore how covariate-related misclassification can impact variables other than outcomes.

\subsection*{Acknowledgements}
Funding support for KHW was provided by the LinkedIn and Cornell Ann S. Bowers College of Computing and Information Science strategic partnership PhD Award. MW was supported by NIH awards U19AI111143-07 and 1P01-AI159402.

\subsection*{Data Accessibility}
The data used in the applied example are freely available for download from the Medical Expenditures Panel Survey.

\newpage

\bibliographystyle{ama}
\bibliography{references}

\newpage
\section{Tables}

\begin{table}[htbt]\footnotesize
\centering
\caption{Bias and root mean squared error (rMSE) for parameter estimates from 500 realizations of simulation Settings 1, 2, and 3. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Estimates marked with a ``-'' are not obtained by the given estimation method.} \label{parameter-results-table}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
        &       & \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{MCMC}&  
        \multicolumn{2}{c}{\textit{SAMBA} EM} & 
        \multicolumn{2}{c}{Perfect Sens. EM} & 
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-12}
Scenario &       & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} & \multicolumn{1}{c}{Bias} & \multicolumn{1}{c}{rMSE} \\
\hline
    \\
      & $\beta_0$ & 0.027 & 0.256 & 0.006 & 0.300 & 0.073 & 0.211 & -0.634 & 0.640 & -0.555 & 0.561 \\
(1)   & $\beta_X$ & -0.106 & 0.397 & 0.154 & 0.530 & 0.415 & 0.454 & 0.988 & 0.992 & 1.019 & 1.023 \\
      & $\gamma_{110}$ & -0.004 & 0.267 & 0.051 & 0.355 & 0.360 & 0.455 & - & - & - & -\\
      & $\gamma_{11Z}$ & 0.037 & 0.360 & 0.297 & 0.844 & -0.050 & 0.375 & - & - &  - & -\\
      & $\gamma_{120}$ & 0.095 & 0.810 & -0.911 & 2.001 & - & - & -2.754 & 2.900 & - & -\\
      & $\gamma_{12Z}$ & -0.319 & 1.464 & -2.215 & 2.634 & - & - & 0.894 & 0.994 & - & -\\
              \\
      & $\beta_0$ & 0.005 & 0.053 & 0.006 & 0.053 & 0.038 & 0.061 & -0.373 & 0.374 & -0.350 & 0.351\\
(2)   & $\beta_X$ & -0.012 & 0.091 & 0.008 & 0.091 & 0.183 & 0.191 & 0.628 & 0.629 & 0.643 & 0.643 \\
      & $\gamma_{110}$ & 0.007 & 0.153 & 0.003 & 0.157 & 0.239 & 0.284 & - & - & - & -\\
      & $\gamma_{11Z}$ & -0.004 & 0.118 & 0.017 & 0.123 & -0.031 & 0.125 & - & - & - & -\\
      & $\gamma_{120}$ & 0.015 & 0.429 & 0.024 & 0.561 & - & - & -3.656 & 3.699 & - & -\\
      & $\gamma_{12Z}$ & -0.036 & 0.306 & -0.245 & 0.540 & - & - & 0.871 & 0.888 & - & -\\
      \\
      & $\beta_0$ & -0.005 & 0.097 & -0.008 & 0.095 & -0.006 & 0.093 & -0.781 & 0.781 & -0.757 & 0.758\\
(3)   & $\beta_X$ & -0.040 & 0.123 & -0.002 & 0.102 & 0.006 & 0.099 & 0.831 & 0.832 & 0.844 & 0.845 \\
      & $\gamma_{110}$ & -0.024 & 0.109 & -0.020 & 0.106 & -0.009 & 0.105 & - & - & - & -\\
      & $\gamma_{11Z}$ & 0.018 & 0.133 & 0.041 & 0.140 & 0.019 & 0.131 & - & - & - & -\\
      & $\gamma_{120}$ & -3.094 & 28.586 & -0.911 & 1.220 & - & - & -1.260 & 1.389 & - & -\\
      & $\gamma_{12Z}$ & 4.023 & 9.406 & 0.334 & 0.638 & - & - & 5.787 & 5.791 & - & -\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\clearpage

\begin{table}[htbt]
\centering
\caption{Estimated event probabilities from 500 realizations of simulation Settings 1, 2, and 3. ``Data'' terms refer to empirical values computed from generated datasets. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package.} \label{probability-results-table}
\begin{threeparttable}
\begin{tabular}{clrrrrr}
\hline
        Scenario & & & Data & EM & MCMC \\
\hline
    \\
(1)           && $P(Y = 1)$ & 0.647 & 0.646 & 0.652 \\
              && $P(Y = 2)$ & 0.353 & 0.354 & 0.348 \\
              && $P(Y^* = 1 | Y = 1)$ & 0.847 & 0.856 & 0.874 \\
              && $P(Y^* = 2 | Y = 2)$ & 0.877 & 0.853 & 0.940 \\
              \\
              \\
(2)           && $P(Y = 1)$ & 0.648 & 0.648 & 0.649 \\
              && $P(Y = 2)$ & 0.352 & 0.352 & 0.351 \\
              && $P(Y^* = 1 | Y = 1)$ & 0.924 & 0.931 & 0.933\\
              && $P(Y^* = 2 | Y = 2)$ & 0.945 & 0.931 & 0.942\\
              \\
              \\
(3)           && $P(Y = 1)$ & 0.647 & 0.645 & 0.646 \\
              && $P(Y = 2)$ & 0.353 & 0.355 & 0.354 \\
              && $P(Y^* = 1 | Y = 1)$ & 0.846 & 0.856 & 0.859\\
              && $P(Y^* = 2 | Y = 2)$ & 1.000 & 0.991 & 1.000\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\newpage


\begin{table}[H]
\centering
\caption{Parameter estimates and standard errors from the applied example using the MEPS dataset.``EM'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Estimates marked with a ``-'' are not obtained by the given estimation method. $\beta_{smoke}$ refers to the association between smoking status (reference = non-smoker) and myocardial infarction (MI). $\beta_{exercise}$ refers to the association between exercise habits (reference = regular exercise) and MI. $\beta_{age}$ refers to the association between age (centered and scaled) and MI. $\gamma_{1,j,gender}$ refers to the association between gender and observed MI, given true MI status $j$ (reference = male). $\gamma_{1,j,age}$ refers to the association between age and observed MI, given true MI status $j$. $\beta_0$, $\gamma_{110}$, and $\gamma_{120}$ are intercept terms.} \label{applied-results-table}
\begin{threeparttable}
\begin{tabular}{clrrrrrrrrrrrr}
\hline
           && \multicolumn{2}{c}{EM}       &
        \multicolumn{2}{c}{\textit{SAMBA} EM} & 
        \multicolumn{2}{c}{Perfect Sens. EM} & 
        \multicolumn{2}{c}{Naive Analysis}\\
        \cline{3-10}
 && \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE} & \multicolumn{1}{c}{Est.} & \multicolumn{1}{c}{SE}\\
\hline
    \\
      $\beta_0$ && -4.374 & 0.065 & -3.184 & 0.665 & -4.159 & 101.296 & -3.576 & 0.078 \\
    $\beta_{smoke}$ && 1.544 & 0.107 & 0.643 & 0.128 & 0.763 & 3.256 & 0.635 & 0.109 \\
    $\beta_{exercise}$ && 0.303 & 0.126 & 0.273 & 0.095 & 0.428 & 1.476 & 0.184 & 0.084 \\
    $\beta_{age}$ && 0.094 & 0.010  & 0.065 & 0.009 & 0.058 & 2.688 & 0.059 & 0.003 \\
       $\gamma_{110}$ && 2.969 & 0.100 & 2.437 & 7.913 & - & - & - & -\\
       $\gamma_{11,gender}$ && -1.766 & 0.036  & -2.661 & 6.750 & - & - &  - & -\\
       $\gamma_{11,age}$ && -0.198 & 0.005  & -0.005 & 0.012 & - & - &  - & -\\
       $\gamma_{120}$ && -3.580 & 0.112 & - & - & -3.674 & 102.790 & - & -\\
       $\gamma_{12,gender}$ && -0.818 & 0.108 & - & - & -5.542 & 5.206 & - & -\\
       $\gamma_{12,age}$ && 0.084 & 0.005 & - & - & 0.063 & 2.678 & - & -\\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}

\newpage

\begin{appendices}

\section{Simulation Study Settings} \label{appendix-a}

\subsection{Simulation Settings}
We present simulations for evaluating the proposed binary outcome misclassification model in terms of bias and root mean squared error (rMSE). For a given simulation scenario, we present parameter estimates for a binary outcome misclassification model obtained from the EM-algorithm and from MCMC, under a \textit{Uniform}$(-10,10)$ prior distribution setting. We compare these estimates to the naive \textit{analysis model} that considers only observed outcomes $Y^*$ and predictors $X$. In addition, we use the \textit{SAMBA} R package from Beesley and Mukherjee\cite{beesley2020statistical} to find the relevant parameter estimates in the event that we assumed that our \textit{observation mechanism} had perfect specificity. We also consider the case where we incorrectly assumed that our \textit{observation mechanism} had perfect sensitivity.

In all settings, we generate $500$ datasets with $P(Y = 1) \approx 65\%$. In the first simulation setting, we studied an example that is expected to be highly problematic for analysts: a relatively small sample size and relatively high misclassification rate. In this setting, we generated datasets with $1000$ members and imposed outcome misclassification rates between $10\%$ and $18\%$. In the Setting 2, we show that the problem of outcome misclassification remains influential even as sample size increases and misclassification rates decrease. In this setting, we generated datasets with $10000$ members and imposed outcome misclassifiation rates between $5\%$ and $10\%$. In the third simulation setting, we consider a case with perfect specificity. The purpose of this scenario is to demonstrate that our methods recover unbiased association parameters, even in cases where we unnecessarily account for bidirectional misclassification. In this setting, we generated datasets with $5000$ members and imposed sensitivity rates around $82\%$ to $90\%$.

For a dataset with $1,000$ members, the analysis using our proposed EM algorithm took about $17$ seconds while our proposed MCMC analysis took approximately $35$ minutes. 

These settings are outlined in Table \ref{sim-setting-table1}. All analyses were conducted in \texttt{R} \citep{stats2021R}.

\begin{table}[htbt]
\centering
\caption{Number of generated datasets (N. Realizations), Sample size ($N$), $P(Y = 1)$, sensitivity ($P(Y^* = 1 | Y = 1)$), specificity ($P(Y^* = 2 | Y = 2)$), $\beta$ prior distribution, and $\gamma$ prior distribution settings for each of the the simulation Settings 1, 2, and 3.} \label{sim-setting-table1}
\begin{threeparttable}
\begin{tabular}{clrrrrr}
\hline
        Scenario & & & & Setting \\
\hline
    \\
(1)           && N. Realizations & & 500\\
              && $N$ & & 1000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^* = 1 | Y = 1)$ && 0.82 - 0.90 \\
              && $P(Y^* = 2 | Y = 2)$ && 0.82 - 0.90 \\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
              \\
(2)           && N. Realizations & & 500\\
              && $N$ & & 10000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^* = 1 | Y = 1)$ && 0.90 - 0.95 \\
              && $P(Y^* = 2 | Y = 2)$ && 0.90 - 0.95 \\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
              \\
(3)           && N. Realizations & & 500\\
              && $N$ & & 5000\\
              && $P(Y = 1)$ & & 0.65 \\
              && $P(Y^* = 1 | Y = 1)$ && 0.82 - 0.90 \\
              && $P(Y^* = 2 | Y = 2)$ && 1 \\
              && $\beta$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              && $\gamma$ prior distribution && \textit{Uniform}$(-10, 10)$ \\
              \\
\hline  % Please only put a hline at the end of the table
\end{tabular}
\end{threeparttable}
\end{table}


\subsection{Data Generation}
For each of the simulated datasets, we begin by generating the predictors $X$ and $Z$ from a multivariate Normal distribution. In Settings 1 and 3, the means were 0 and 1.5, respectively, for $X$ and $Z$. In Setting 2, the means were 0 and 2.5, respectively, for $X$ and $Z$. Covariate generation in all simulation settings used unit variances and covariance terms equal to 0.30. The absolute value of all $Z$ terms was taken. Next, we generated true outcome status using the following relationship: $P(Y = 1 | X) = 1 + (-2)X$. For Settings 1 and 2, we used the following relationships to obtain $Y^*$: $P(Y^* = 1 | Y = 1, Z) = 0.50 + (1)Z$ and $P(Y^* = 1 | Y = 2, Z) = -0.50 + (-1)Z$. The different $Z$ distribution between Settings 1 and 2 resulted in different misclassification rates. In Setting 1, misclassification rates were between $10\%$ and $18\%$. In Setting 2, misclassification rates were between $5\%$ and $10\%$. In Setting 3, we generated $Y^*$ using the following relationships: $P(Y^* = 1 | Y = 1, Z) = 0.50 + (1)Z$ and $P(Y^* = 1 | Y = 2, Z) = -5 + (-5)Z$. The choice of parameter values $(-5, -5)$ for $P(Y^* = 1 | Y = 2)$ resulted in near perfect specificity in the generated datasets. 

\end{appendices}

\newpage
\section{Figures}


\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.8]{Figures/data_structure_bw.png}

\caption{Diagram of the assumed data structure for a binary outcome misclassification model.\label{conceptual_framework_figure}}
\end{center}
\end{figure}

\newpage

\begin{figure}
\begin{center}
\includegraphics[scale=0.85]{Figures/small_n_sim_histogram_bw.png}
\medskip
  \caption{\textit{(Caption on next page.)}}\label{fig:small_n_results_figure}
\end{center}
\end{figure}

\begin{figure*}
Figure \ref{fig:small_n_results_figure} Caption: Parameter estimates for 500 realizations of simulation Setting 1. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Solid lines represent true parameter values. Dashed lines represent mean parameter estimates for a given method.
\end{figure*}

\clearpage

\begin{figure}
\begin{center}
\includegraphics[scale=0.85]{Figures/large_n_histogram_bw.png}
\medskip
  \caption{\textit{(Caption on next page.)}}\label{fig:large_n_results_figure}
\end{center}
\end{figure}

\begin{figure*}
 Figure \ref{fig:large_n_results_figure} Caption: Parameter estimates for 500 realizations of simulation Setting 2. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the \textit{SAMBA} R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Solid lines represent true parameter values. Dashed lines represent mean parameter estimates for a given method.
\end{figure*}

\newpage

\clearpage
\section*{Simulation Setting 3 Results Figure} \label{appendix-b}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=.8]{Figures/ps_sim_results_histogram_bw.png}
\medskip
  \caption{\textit{(Caption on next page.)}}\label{fig:ps_sim_results_histogram}
\end{center}
\end{figure}
\clearpage

\begin{figure*}
\noindent
Figure \ref{fig:ps_sim_results_histogram} Caption: Parameter estimates for 500 realizations of simulation Setting 3. ``EM'' and ``MCMC'' estimates were computed using the \textit{COMBO} R Package. The ``\textit{SAMBA} EM'' results assume perfect specificity and were computed using the R Package. The ``Perfect Sensitivity EM'' results were computed using an adapted function from the \textit{SAMBA} R Package  \citep{beesley2020statistical}. The ``Naive Analysis'' results were obtained by running a simple logistic regression model for $Y^* \sim X$. Solid lines represent true parameter values. Dashed lines represent mean parameter estimates for a given method.
\end{figure*}

\end{document}
