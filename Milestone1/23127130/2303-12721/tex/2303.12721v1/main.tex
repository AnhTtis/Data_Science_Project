\documentclass{article}
\usepackage{spconf,amsmath} %,graphicx
\usepackage{optidef}
\usepackage{amssymb}
\usepackage{booktabs}       % professional-quality tables
\usepackage{adjustbox}


\thispagestyle{plain}
\pagestyle{plain}

\usepackage{subcaption}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]

\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{multirow}
%multi-row
\usepackage{multirow}
\usepackage[hidelinks]{hyperref}
%\usepackage{epsfig}
%\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{graphicx,epstopdf}
%\usepackage[pdftex]{graphicx}
%\epstopdfsetup{update}
% \multirow{number rows}{width}{text}
%\setlength\parindent{0pt}


\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output
\thispagestyle{empty}

 
\newcommand{\yl}[1]{\textcolor{blue}{#1}}
\newcommand{\lx}[1]{\textcolor{red}{#1}}
\newcommand{\tz}[1]{\textcolor{green}{#1}}
\newcommand{\h}[1]{\mathbf{#1}}

\newcommand{\fft}{{\textnormal{fft}\,}}
\newcommand{\ifft}{{\textnormal{ifft}\,}}
\newcommand{\fro}{{\textnormal{F}\,}}

\title{Non-convex approaches for low-rank tensor completion\\ under tubal sampling
}
%
% Single address.
% ---------------
\name{$\textnormal{Zheng Tan}^{1}$, $\textnormal{Longxiu Huang}^{2}$, $\textnormal{HanQin Cai}^{3}$,  $\textnormal{Yifei Lou}^{4}$\thanks{The work was partially supported by   AMS Simons Travel Grant,  NSF CAREER grant, NSF DMS 1846690, and NSF DMS 2304489.} }
\address{
%  $^1$Department of Mathematics,$^2$Department of Computational Mathematics, Science and Engineering\\
%  $^3$Department of Statistics and Data Science,
%  $^3$Department of Computer Science,
% %$^3$Department of Mathematical Sciences,
$^1$University of California, Los Angeles, $^2$Michigan State University, \\ $^3$University of Central Florida, $^4$University of Texas at Dallas
%Richardson, TX 75080, USA.
}

%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%

%\if CLASSINFOpdf
%    \usepackage[pdftex]{graphicx}
%    \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
%    \usepackage[dvips,xetex]{graphicx}
%    \DeclareGraphicsExtensions{.eps}
%\fi

\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Tensor completion is an important problem in modern data analysis. In this work, we investigate a specific sampling strategy, referred to as tubal sampling. We  propose two novel  non-convex tensor completion frameworks that are easy to implement, named tensor $L_1$-$L_2$ (TL12) and tensor completion via CUR (TCCUR). We test the efficiency of both methods on synthetic data and a  color image inpainting problem. Empirical results reveal a trade-off between  the accuracy and time efficiency of these two methods in a low sampling ratio. Each of them outperforms some classical completion methods in at least one aspect. 
% In this paper, we consider the low tubal-rank tensor completion problem, using the Alternating Direction Method of Multipliers (ADMM) method and our newly proposed   tensor completion based on CUR decompostion (TCCUR) 
% %tensor Iterative Resample CUR Completion (ICURCR)
% for the tubal sampling\textbf{\cite{zhang2016exact}}. For the TCCUR method, we modified the recently proposed ICURC matrix completion method by adding a resampling procedure into it, and extended it to tensor version. We showed that under the tubal sampling, it achieved significantly higher accuracy than the ADMM method in any metric, with significantly higher time efficiency. Further experiments on the real data also show the effectiveness and efficiency of the TCCUR method, compared with the traditional ADMM method. 
\end{abstract}
%
\begin{keywords}Tensor Completion, $L_1$-$L_2$ regularization, CUR decomposition, Tubal Sampling, Image inpainting
\end{keywords}
%
%  Compared to traditional way of unfolding the multidimensional array into matrices,
% tensors often preserve more (structural) information of the underlying data. 
\section{Introduction}
\label{sec:intro}
Tensor, a multidimensional generalization of matrix, is a useful data structure that is arisen in various fields such as seismic imaging~\cite{Kilmer2015,popa2021improved}, image or video  processing \cite{zhou2017tensor,lu2019tensor,cai2021mode}, and recommendation system \cite{zheng2016topic, song2017based}.
%compared to the unfolded matrices. For example, saving a gray-scaled video with $k$ frames and each frame of size $m\times n$ as an $m\times n\times k$ tensor can keep the connections between each frames but saving it as a matrix by unfolding it from one direction will lose some conjunctional information. 
This paper considers a tensor completion problem in which the measured data has missing %or corrupted
entries. It is an ill-posed problem, thus requiring additional information to be imposed as a regularization. We focus on a low-rank structure of the desired tensor. 

The rank of a matrix is the number of nonzero singular values. As the rank minimization is NP-hard, a popular choice is to minimize the sum of its singular values, which is called \emph{nuclear norm} \cite{Candes2009}. By regarding nonnegative singular values as a vector, the sum of singular values is equivalent to the $L_1$ norm of this vector. With recent advances in tensor algebra~\cite{Kilmer2011}, tensor nuclear norm (TNN)~\cite{6909886} was proposed to enforce the low-rankness of a tensor.  Studies~\cite{yinLHX14,louYHX14} have demonstrated that a nonconvex $L_1$-$L_2$ model gives better identification of nonzero elements compared to the convex $L_1$ norm. Motivated by this empirical observation, we propose  a novel tensor $L_1$-$L_2$ model (TL12) for low-rank tensor completion.  Recently, nonconex models for tensor recovery have been investigated in \cite{wang2021generalized}, which does not include TL12. 

%Literature has demonstrated that nonconvex sparsity promoting regularizations \cite{zhang2014minimization,guo2021novel} often give better empirical results than the convex $L_1.$ Inspired by this observation, we consider the $L_1$-$L_2$ regularization https://www.overleaf.com/project/62a3d69b5fb2aa2c6004ff8a\cite{louYHX14,yinLHX14} to enforce the sparsity of the singular values, thus leading to low-rank tensor. We refer to the proposed regularization to tensor $L_1$-$L_2$ (TL12) regularization.


Other than seeking proper regularizations, one can rely on matrix decomposition techniques to enforce the low rankness. For example, CUR decomposition \cite{HH2020,drineas2008relative,chiu2013sublinear,HH_Perturbation2019} factorizes a matrix into the product of three smaller matrices  compared to the original size by {taking its column and row  subsets  to form the left and right matrices and } enforcing the middle matrix as a low-rank matrix. Recently, a tensor CUR (t-CUR) decomposition was proposed in \cite{wang2017missing,chen2022tensor,hamm2023generalized}. 
 In this work, we focus on a specific sampling strategy for 3-mode tensors (a tensor has three dimensions) that either takes all the samples along the third dimension or not at all, which is referred to as tubal sampling. With this sampling scheme, tensor completion can be reduced to a set of matrix completion problems. We thus
 propose an efficient Tensor Completion method via CUR (TCCUR) by adapting  a recent matrix completion approach \cite{cai2022matrix} to tensor completion.
 
 
 We conduct experiments to compare the proposed nonconvex methods with classical convex models on synthetic data and a color image inpainting problem. We observe that regularization-based tensor completion methods yield higher accuracy but at a cost of computational time compared to  decomposition methods, especially under the regime of low sampling ratios.
  The main contributions of this work are threefold:
\begin{enumerate}
    \itemsep0em 
    \item We propose the TL12 regularization to promote low-rankness for tensor completion. 
    \item  We  propose an accelerated  tensor low-rank decomposition method, referred to TCCUR.
    \item We compare these two nonconvex approaches (regularization and decomposition) for empirical guidance in real applications.
\end{enumerate}

 
%  However,   as caused by various unpredictable or unavoidable reasons, multidimensional datasets are commonly raw and incomplete, and thus often only a small subset of entries of tensors are available. 
 
%  Therefore, it is natural to consider the problem that estimating   the missing or damaged entries by making good use of  the observed data for improving the quality of the underlying data. The problem is termed  tensor completion (TC) problem.  TC has 
%   received  more and more attentions and achieved success in different applications e.g.,  colored image reconstruction\cite{}, seismic data analysis\cite{{}}, and video processing\cite{}. 
%To solve TC, there are XXXX(list some related works). In this work, we focus on the low-tubal-rank-based tensor completion problem based on the tubal sampling model \cite{zhang2016exact}. In summary, 
  
  

  
 
% The rest of the paper is organized as follows. Section~\ref{sect:background} provides background knowledge on tensor algebra that both regularization and decomposition methods are built upon. 




\section{Notation and preliminary} \label{sec:background}
Throughout this paper,  we denote scalars by lowercase letters, vectors by bold letters, matrices by uppercase letters, and tensors by calligraphic uppercase letters. The set of the first $n$ natural numbers is denoted by $[n]:=\{1,\cdots,n\}$.  We reserve $I$, $J$, $\mathcal{I}$, and $\mathcal{J}$ as  index sets. We use $A^\dagger$ to denote the Moore--Penrose pseudo-inverse of a matrix $A$.
%, and $\h x\circ \h y$ denotes the outer product of the vectors $\h x,\ \h y$. 


Given a 3-mode tensor $\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$, its $i$-th frontal slice is a matrix,  denoted by $[\mathcal{A}]_{:,:,i}$, and we refer $[\mathcal{A}]_{i,j,:}$ as 
its $(i,j)$-th tube. We use
\begin{align*}
    \hat{\mathcal{A}}=\fft(\mathcal{A},[~],3) \quad \mbox{and}\quad 
    \mathcal{A}=\ifft(\hat{\mathcal{A}},[~],3),
\end{align*}
to denote the Fourier transform and the inverse Fourier transform of $\mathcal{A}$ and $\hat{\mathcal{A}}$ along the third dimension, respectively. In what follows, we provide some important tensor definitions  \cite{Kilmer2011,chen2022tensor,zhang2016exact} that are relevant to  this work. 
% \begin{definition}[Tensor transpose] The conjugate transpose of a tensor $\mathcal{A} \in\mathbb{R}^{n_1\times n_2\times n_3}$ is the $n_2\times n_1\times n_3$ tensor $\mathcal{A}^\top$ obtained by conjugate transposing each of the frontal slice and then reversing the order of transposed frontal slices $2$ through $n_3$.
% \end{definition}
\begin{definition}[t-product]Given  $\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$ and $\mathcal{B}\in\mathbb{R}^{n_2\times \ell\times n_3}$, the t-product $\mathcal{C}=\mathcal{A}*\mathcal{B}$ is an $n_1\times\ell \times n_3$ tensor and its $(i,j)$-th tube is given by
\[[\mathcal{C}]_{i,j,:}=\sum_{k=1}^{n_2}[\mathcal{A}]_{i,k,:}*[\mathcal{B}]_{k,j,:},
\]
where $*$ denotes the circular convolution between two tubes (vectors) of the same length.
\end{definition}
% Based on the definition of t-product, one can define t-SVD.  Given a tensor $\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$, its $t$-SVD  is given by 
% \[\mathcal{A}=\mathcal{W}*\mathcal{S}*\mathcal{V}^\top,
% \]
% where $\mathcal{W}\in\mathbb{R}^{n_1\times n_1\times n_3}$ and $\mathcal{V}\in\mathbb{R}^{n_2\times n_2\times n_3}$ are orthogonal tensors, and $\mathcal{S}\in\mathbb{R}^{n_1\times n_2\times n_3}$ is a $f$-diagonal tensor \cite{zhang2016exact}.
% Before we introduce the definition of tubal rank, let's first introduce the definition of $f$-diagonal tensor and t-SVD.
% \begin{definition}[f-diagonal tensor \cite{zhang2016exact}]
% A 3-mode tensor $\mathcal{A}$ is called $f$-diagonal if each frontal slice $\mathcal{A}(:,:,i)$ is a diagonal matrix.
% \end{definition}

With t-product, one can extend the matrix pseudo-inverse into tensor, i.e., $\mathcal A^\dagger$ is the pseudo-inverse of a tensor $\mathcal A$ if $\mathcal{A}^{\dagger}$ satisfies $\mathcal{A}*\mathcal{A}^\dagger*\mathcal{A}=\mathcal{A}$ and $\mathcal{A}^\dagger*\mathcal{A}*\mathcal{A}^\dagger=\mathcal{A}^\dagger$.

\begin{definition}[t-SVD]
Given  $\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$, the $t$-SVD of $\mathcal{A}$ is given by 
\[\mathcal{A}=\mathcal{U}*\mathcal{S}*\mathcal{V}^\top,
\]
where $\mathcal{U}\in\mathbb{R}^{n_1\times m\times n_3}$ and $\mathcal{V}\in\mathbb{R}^{n_2\times m\times n_3}$ are orthogonal tensors,   $\mathcal S\in\mathbb{R}^{m\times m\times n_3}$ is a f-diagonal tensor (each frontal slice is a diagonal matrix), and $m=\min\{n_1,n_2\}$.
\end{definition}
%Based on t-SVD, one can define tensor tubal rank as follows.

% \begin{definition}[Tensor tubal rank]
% Given $\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$ one defines a vector $\h r\in\mathbb{R}^{n_3\times 1}$ with its $i$-th component equal to the rank of the $i$-th frontal slice of $\hat{\mathcal{A}}$. Tensor tubal rank  of $\mathcal{A}$ is thus set to be $\|\h r\|_{\infty}$.
% \end{definition}
% %\yl{do we need multirank?}

\begin{definition}[Tensor multi rank and tubal rank]
The tensor multi rank of a 3-mode tensor $\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$ is a vector $\h r\in\mathbb{R}^{n_3\times 1}$ with its $i$-th component equal to the rank of the $i$-th frontal slice of $\hat{\mathcal{A}}$. The tensor tubal rank of $\mathcal{A}$ is defined to be $r=\|\h{r}\|_{\infty}$.
\end{definition}

% \begin{theorem}[Matrix CUR Decomposition \cite{HH2020}]
% Let $C=[A]_{:,J}$ and $R=[A]_{I,:}$ be column and row submatrices of $A\in\mathbb{R}^{m\times n}$ with column and row indices $J$ and $I$, and let $U=[A]_{I,J}$ be their intersection. Then $A=CU^\dagger R$ provided that $\textnormal{rank}(U)=\textnormal{rank}(A)$.
% \end{theorem}

\begin{definition}[t-CUR]
For %a 3-mode tensor
$\mathcal{A}\in\mathbb{R}^{n_1\times n_2\times n_3}$, the t-CUR decomposition of $\mathcal{A}$ is given by
$\mathcal{C}*\mathcal{U}^\dagger*\mathcal{R}$, 
where $\mathcal{C}=[\mathcal{A}]_{:,J,:}$, $\mathcal{R}=[\mathcal{A}]_{I,:,:}$, $\mathcal{U}=[\mathcal{A}]_{I,J,:}$  with  $I\subseteq[n_1]$ and $J\subseteq[n_2]$. 
\end{definition}
When the multi ranks of $\mathcal{U}$ and $\mathcal{A}$ are the same, Chen et al.~\cite{chen2022tensor} proved that the t-CUR representation of $\mathcal{A}$ is exact, i.e.,  $\mathcal{A}=\mathcal{C}*\mathcal{U}^\dagger*\mathcal{R}$. %\yl{maybe write as a theorem as the matrix version?}

\medskip
In this work, we focus on a specific sampling strategy, referred to as tubal sampling~\cite{zhang2016exact}, under which one can  randomly select  tensor tubes to sample. Specifically for a  tensor of dimension ${n_1\times n_2\times n_3}$, one defines a  sampling operator $\mathcal{P}_{\Omega}$   by
\begin{equation}\label{eqn:sampling}
    [\mathcal{P}_{\Omega}(\mathcal{X})]_{i,j,k}:=\begin{cases}
[\mathcal{X}]_{i,j,k},& \text{if}~(i,j,k)\in\Omega\\
0,& \text{Otherwise},
\end{cases}
\end{equation} 
that corresponds to an index set
\begin{equation}\label{eq:tubalsampling}
  \Omega=\{(i,j,k):(i,j)\in  \Phi\subseteq [n_1]\times[n_2], \forall k\in[n_3]\}.  
\end{equation}
In tubal sampling \eqref{eq:tubalsampling}, each  tubal is either sampled entirely or not sampled at all. We will develop two non-convex methods for tensor completion under tubal sampling: one is by imposing regularizations (Section~\ref{sec:ADMM}) and the other is via t-CUR (Section~\ref{sec:CUR}).


% developing methods to solve the tensor completion problem proposed in , in which \cite{zhang2016exact} proposed the    the tubal sampling model. In tubal sampling model,  i.e., For a given tensor $\mathcal{X}\in\mathbb{R}^{n_1\times n_2\times n_3}$, the observed  set $\Omega$ has the form of  $\Omega=\{(i,j,k):(i,j)\in \Phi, k\in[n_3]\}$ with  $\Phi\subseteq[n_1]\times[n_2]$   and our goal is to estimate $\mathcal{X}$ from the entries with indices in $\Omega$. To solve the tensor completion problem in \cite{zhang2016exact}, we will develop two non-convex methods. Here let's introduce some related tools.


% \yl{copy and paste here, need to paraphrase}\\
%  Given a $3$-mode tensor $\mathcal{X}\in\mathbb{R}^{n_1\times n_2\times n_3}$, its 
% multidimensional Fourier transform  is denoted by $\hat{\mathcal{X}}$, where 
%   we compute the Fourier transform of $\mathcal{X}$ along its 3rd mode. In other words,  we consider \textit{tubes} or vectors oriented along the third dimension and we compute the Fourier transform of each tube.%Figure \ref{fig:tubeVis} illustrates a tube in an order-$3$ tensor.
% %For each tube we compute its Fourier transform treating the tube as a vector.

% \yl{change to 3d tensor, no need to go generally}
% The \textit{tensor product} (tProduct) \cite{Kilmer2011} is used to define multiplication between two tensors. The tProduct of $\mathcal{A} \in \mathbb{R}^{n_1\times m\times n_3\times ... \times n_p}$ and $\mathcal{B} \in \mathbb{R}^{m\times n_2\times n_3\times ... \times n_p}$ results in a tensor $\mathcal{C} = \mathcal{A}\mathcal{B} \in \mathbb{R}^{n_1\times n_2\times n_3\times ... \times n_p}$. 
% The tProduct can be computed in three steps. First, take the multidimensional Fourier transform of $\mathcal{A}$ and $\mathcal{B}$, denoted $\hat{\mathcal{A}}$ and $\hat{\mathcal{B}}$. Next, multiply the frontal slices of $\hat{\mathcal{A}}$ and $\hat{\mathcal{B}}$, storing the result of each matrix product as a frontal slice in $\hat{\mathcal{C}}$. Last, we take the inverse multidimensional Fourier transform of $\hat{\mathcal{C}}$ to obtain $\mathcal{C}$.

% Unitary tensors have the property that their conjugate transpose is their inverse, i.e. $\mathcal{U}\mathcal{U}^* = \mathcal{I}$ and $\mathcal{U}^*\mathcal{U} = \mathcal{I}$. The \textit{identity} tensor $\mathcal{I} \in \mathbb{R}^{n_1\times n_1\times n_3\times ... \times n_p}$ has the identity matrix as its first frontal slice and zeros elsewhere. Transposing a tensor is computed recursively by transposing each subtensor and reversing the order of all but the first subtensor.

% The \textit{tensor singular value decomposition} (tSVD) generalizes the matrix SVD and decomposes a tensor into a product of three tensors \cite{Martin2013},
% \begin{equation}
%     \mathcal{X} = \mathcal{U}\mathcal{S}\mathcal{V}^*.
% \end{equation}
% The tensors $\mathcal{U} \in \mathbb{R}^{n_1\times n_1\times n_3\times ... \times n_p}$ and $\mathcal{V}\in \mathbb{R}^{n_2\times n_2\times n_3\times ... \times n_p}$ are unitary.
% $\mathcal{S}$ is a diagonal tensor, where each frontal slice is a diagonal matrix. 
% The multidimensional Fourier transform of $\mathcal{S}$ over dimensions 3 to $p$ is denoted $\hat{\mathcal{S}}$.
% The diagonal of each frontal slice of $\hat{\mathcal{S}}$ is positive and descending. 
% Due to the similarity with matrix SVD, we will refer to the diagonal values in $\hat{\mathcal{S}}$ as the singular values of $\mathcal{X}$.

% \yl{need to review tensor CUR}



\input{ADMM}

\section{Tensor Completion Based on  CUR Decompositions}\label{sec:CUR}
%\yl{this section is our proposed work, i.e., iterative CUR with resampling}

In this section,  we develop a Tensor Completion method based on   CUR decompositions termed TCCUR. 
%Let   $\mathcal{T}\in\mathbb{R}^{n_1\times n_2\times n_3}$  and  let $\Omega=\{(i,j,k):(i,j)\in  \Phi\subseteq [n_1]\times[n_2], k\in[n_3]\}$ be a sampling set. 
We consider the tubal sampling operator $\mathcal{P}_{\Omega}$  defined in \eqref{eqn:sampling} and  the observed data $\mathcal Y=\mathcal{P}_{\Omega}(\mathcal X)$. By the design \eqref{eq:tubalsampling} that each tubal $[\mathcal{X}]_{i,j,:} (\forall (i,j)\in\Phi)$ is completely sampled, we have $\hat{\mathcal{Y}}=\mathcal{P}_{\Omega}(\hat{\mathcal{X}}).$ As a result, we can find an estimate of $\mathcal X$ by completing $\hat{\mathcal{Y}},$ followed by the inverse Fourier transform along the third dimension. Completing the tensor $\hat{\mathcal{Y}}$ reduces to a series of matrix completion problems, independently for each frontal slice of $\hat{\mathcal{Y}}$. 

 Due to tubal sampling \eqref{eq:tubalsampling}, the sampling set is fixed by $\Phi$ for all the frontal slices, and  we denote the sampling operator for matrix completion by $\mathcal P_\Phi$.   %whose row and column indexes are denoted by $\mathcal I, \mathcal J,$ respectively.
 To complete each frontal slice, we adopt a recently developed matrix completion method termed  iterative CUR completion (ICURC) (see~\cite{cai2022matrix}). Instead of sampling the same row/column indices during the iterations, % $\mathcal I,\mathcal J$, 
 we randomly generate row and column index sets $I, J$ at each iteration. %as $|\mathcal{I}|$ and $|\mathcal{J}|,$ respectively. 
% We sample an intermediate matrix at indexes $I, J$ for matrix completion to advance to the next solution.  
 In other words, we incorporate a resampling strategy into ICURC , hence the name ICURC with resampling (ICURC-R). 
In order to combine the completion results by t-CUR decomposition, all completed matrices only return the completed  row and column submatrices with the same row and column indices $\mathcal{I}$ and $\mathcal{J}$. %ICURC-R is summarized in Algorithm \ref{ICURC}.






% \begin{algorithm}
% \caption{Tensor Completion based on the CUR decomposition (TCCUR)}\label{alg:TCCUR}
% \DontPrintSemicolon
%     \KwInput{$\mathcal Y\in\mathbb{R}^{n_1\times n_2\times n_3}$ (observed data), $\mathcal{I}\subseteq[n_1]/ \mathcal{J}\subseteq[n_2]$ (row/column indexes based on $\Phi$),  
%     $r$ (target tensor tubal rank).}
%     %$\varepsilon:$  target precision level; \yl{why mathcal?}\lx{Not necessary but later we need to introduce another indices in ICURMC.} 
%   %  $\mathcal{I}\subseteq[n_1]$ and  $\mathcal{J}\subseteq[n_2]$ (.} %maximum iteration \textnormal{I\_max}, error tolerance \textnormal{tol}}
    
%     $\hat{\mathcal{Y}}=\fft(\mathcal Y, [~], 3)$,\\
%     \For{$i=1$ \text{to} $n_3$}{
%         $[\hat{\mathcal{C}}]_{:,:,i}, [\hat{\mathcal{U}}]_{:,:,i}, [\hat{\mathcal{R}}]_{:,:,i}=\text{ICURC-R}([\hat{\mathcal{Y}}]_{:,:,i}, \mathcal I, \mathcal J, r)$,
%     }
%     $\mathcal{C} = \ifft(\hat{\mathcal{C}}, [~], 3)$, \\ $\mathcal{U} = \ifft(\hat{\mathcal{U}}, [~], 3)$, \\$\mathcal{R} = \ifft(\hat{\mathcal{R}}, [~], 3)$,\\
%     \KwOutput{ $\widetilde{\mathcal{X}}= \mathcal{C}\ast \mathcal{U}\ast \mathcal{R}$ (estimate of $\mathcal{X}$).}
% \end{algorithm}





% We first apply the Fourier transformation on the observed data and obtain \yl{[should X inside be mathcal?]} $\widehat{\mathcal{P}_{\Omega}({X})}=\fft(\mathcal{P}_{\Omega}(\mathcal{X}),[~],3)$.  %denotes the sampling operator defined as $[\mathcal{P}_{\Omega}(\mathcal{X})]_{i,j,k}=[\mathcal{X}]_{i,j,k}$ if $(i,j,k)\in\Omega$; otherwise, $[\mathcal{P}_{\Omega}(\mathcal{X})]_{i,j,k}=0$. 
% Since the tubes $[\mathcal{X}]_{i,j,:}$ have been fully observed for all $(i,j)\in \Phi$, we thus have $\widehat{\mathcal{P}_{\Omega}({X})}=\mathcal{P}_{\Omega}(\hat{\mathcal{X}})$. Thus, we can complete $\widehat{\mathcal{P}_{\Omega}({X})}$ by applying some matrix completion method  on  each frontal slice of  $\widehat{\mathcal{P}_{\Omega}({X})}$. 

Suppose $Y$ be a matrix of $n_1\times n_2$ that corresponds to any frontal slide of $\hat{\mathcal Y}.$
  We set the initial condition as $X^{(0)}=0.$ Then at every iteration $\ell,$ suppose we have the CUR decomposition of $X^{(\ell)}=C^{(\ell)}(U^{(\ell)})^\dagger R^{(\ell)}.$ 
% Instead of using $\mathcal I,\mathcal J$ from the tubal sampling set $\Phi$ \yl{[can we have some formula for $\mathcal I,\mathcal J$ based on $\Omega$ or $\Phi$?]}, we generate the row and column indices $I, J$ that have the same cardinality as $\mathcal I, \mathcal J$.  The gradient descent update yields
 The gradient descent update yields
\begin{align*}
    [C^{(\ell+1)}]_{{I}^{c},:} &=[X^{(\ell)}]_{ {I}^c, {J}}+[Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{ {I}^c, {J}},\\
    [R^{(\ell+1)}]_{:,J^{c}}&=[X^{(\ell)}]_{I,J^c}+[Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{I,J^c}
\end{align*}
where $I^{c}=[n_1]\setminus I$ and $J^{c}=[n_2]\setminus J$.  The update of $U^{(\ell+1)}$ requires the best rank $r$ approximation, which can be achieved by truncating the largest $r$ singular values in the matrix SVD, denoted by  $\mathcal{H}_r$. In short, we have the formula,
$
    U^{(\ell+1)}=\mathcal{H}_r\left([X^{(\ell)}]_{I,J}+[Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{I,J}  \right).
$
We stop the iterations when 
\begin{equation*}
    e^{(\ell)}:= \frac{\|[Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{I,:}\|_\fro+\| [Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{:,J}\|_\fro}{\|[Y]_{I,:}\|_\fro+\| [Y]_{:,J}\|_\fro} <\varepsilon.%\frac{\langle [\mathcal{P}_{\Omega}(X-X_{k})]_{I,:},[X-X_k]_{I,:}\rangle+\langle [\mathcal{P}_{\Omega}(X-X_{k})]_{:,J},[X-X_k]_{:,J}\rangle}{\mathcal{P}_{\Omega}(X)]_{I,:},[X]_{I,:}\rangle+\langle [\mathcal{P}_{\Omega}(X)]_{:,J},[X]_{:,J}\rangle}
\end{equation*}
for a preset tolerance $\varepsilon>0$. We thus
obtain the row and  column submatrices $[C^{(\ell+1)}]_{\mathcal{I},:}$, $[R^{(\ell+1)}]_{:,\mathcal{J}}$, and $\mathcal{H}_r([X^{(\ell+1)}]_{\mathcal{I},\mathcal{J}})$. The details of ICURC-R are summarized in Algorithm~\ref{ICURC}.  




% Similar to  ,  our ICURMC \yl{[can we change the acronym to sth related to resampling]} is built upon the framework of projected gradient descent method. At each iteration of ICURMC, we need to regenerate the row and column indices $I,J$ \yl{[still don't know why $I,J$ would be better than $\mathcal I, \mathcal J$]\lx{We need to apply ifft to the results of $C,U,R$s so we need to unify the indices for each slice.}} so that we can use the gradient descent method for updating the corresponding row and column submatrices instead of updating the whole estimated matrix. Specifically, we perform a step of gradient descent on $[R_{k+1}]_{:,J^{c}}, [C_{k+1}]_{I^{c},:}, U$ with  via the following formula:
% % \[
% % [R_{k+1}]_{:,J^{c}}=[X_k]_{I,J^c}+[\mathcal{P}_{\Omega}(X-X_k)]_{I,J^c},\]

% %     \[\text{and },
% % \]
% where $\mathcal{H}_r(\cdot)$ denotes the best rank $r$ approximation to the argument. \yl{[how to do the rank approximation: SVD and then truncate?]}
% For ICURMC, we set the initial guess  as $0$ and it will return the row and  column submatrices $[X_{k}]_{\mathcal{I},:}$, $[X_k]_{:,\mathcal{J}}$ and $\mathcal{H}_r([X_{k}]_{\mathcal{I},\mathcal{J}})$, where $\mathcal{I}$ and $\mathcal{J}$ are our pre-defined row and column indices and  $X_k$ is the estimates of the underlying low-rank matrix $X$ with observations $X_{\Omega}$ such that $e_{k}<\varepsilon$ and




% \begin{equation}
% \begin{array}{rrclcl}
% \underset{\tilde{\mathcal{T}}}{\text{min}} \quad & \frac{1}{2}\langle\mathcal{P}_\Omega(\mathcal{T}-\mathcal{\tilde{T}}), \mathcal{T}-\mathcal{\tilde{T}}\rangle\\
% \textrm{subject to} &\text{rank}(\tilde{T})=r    
% \end{array}
% \end{equation}

% With these given methods, we used tubal sampling \cite{9107418} in order to generate the synthetic data. A tensor \textit{tube} is an vector extracted from the entire tensor, by fixing the first two dimensions and vary the third dimension, i.e. $\mathcal{T}(i,j,:)$ for some fixed $i$, $j$. The tubal sampling is to sample tubes from the given tensor, uniformly at random. 





%  The ICURCR method is a direct extension of the matrix version of CUR method, described in Algorithm \ref{icurcr}. In step 5, the function ICURMC is any matrix CUR completion method. Here we use the extension of the matrix cross sampling tensor CUR algorithm \cite{cai2022matrix}, shown in algorithm \ref{ICURC}. In line 8, the operator $\mathcal{H}_r(\cdot)$ denotes the best rank $r$ approximation to the argument. As mentioned in the paper, we never actually do matrix multiplications in line 11 of the algorithm \ref{ICURC}, so that the time efficiency is highly improved. \\
% Notice in the algorithm \ref{ICURC}, we further do cross-concentrated tubal sampling on the the given incomplete tensor in the Fourier domain, in order to be get higher time efficiency on the ICRUC completion. The modification of ICURC by resampling the used columns and rows for each iteration more effectively uses all entries sampled, so that both the accuracy and the time efficiency are higher than only further sampling one time, as shown in section \ref{sec:experiments}. Also notice that in the ICURC\_Resample algorithm, the row and column indices for the final time of resampling is fixed; by doing so, we ensured that the $\hat{C}(:,:,i), \hat{U}(:,:,i), \hat{R}(:,:,i)$ generated for $i=1, ..., N_3$ all match the same row and column indices, so that after inverse Fourier transformation, their tensor product will indeed generate the correct completed tensor.


%\tz{Need to make this shorter somehow}

% \begin{algorithm}
% \caption{Iterative CUR for matrix Completion with Resampling (ICURC-R) } \label{ICURC}
% \DontPrintSemicolon
%     \KwInput{Observed matrix $Y = \mathcal P_\Phi(X)\in\mathbb{R}^{n_1\times n_2}$; 
%   Predefined row and column indices $\mathcal{I},\mathcal{J}$; target rank $r$; error tolerance $\varepsilon;$ maximum iteration number $M$.}

%     $X_0=0$, $\ell=0$\\
%     \While {true}{
%         can\_stop = false\\
%         \uIf{$e_k > \varepsilon$ or $\ell\le M$}{
%             can\_stop=true\\
%             $I=\mathcal{I}$, $J=\mathcal{J}$
%         }
%         \Else{
%         Randomly sample row and column indices $I$, $J$ with $|I|=|\mathcal{I}|$ and $|J|=|\mathcal{J}|$.\\
%         }
%         Set $I^{c}=[n_1]\setminus I$, $J^{c}=[n_2]\setminus J$.\\
%         $[R_{\ell+1}]_{:,J^{c}}=[X_\ell]_{I,J^c}+ [\mathcal{P}_{\Omega}(Y-X_\ell]_{I,J^c}$\\
%         $[C_{\ell+1}]_{I^{c},:}=[X_\ell]_{I^c,J}+ [\mathcal{P}_{\Omega}(Y-X_\ell)]_{I^c,J}$\\
%         $U_{\ell+1}=\mathcal{H}_r\left([X_{\ell}]_{I,J}+ [\mathcal{P}_{\Omega}(Y-X_\ell)]_{I,J}  \right)$\\
%             $[R_{\ell+1}]_{:,J}=U_{\ell+1}$\\
%             $[C_{l+1}]_{I,:}=U_{l+1}$\\
%         $X_{l+1}$ = $C_{l+1}U_{l+1}^{\dagger}R_{l+1}$\\
%         $l = l + 1$\\
%         \uIf{can\_break}{
%             stop
%         }
%     }

%     % $\mathcal{I} = \mathcal{I}_{ccs}$, $\mathcal{J} = \mathcal{J}_{ccs}$\\
%     % $\textnormal{Do step 7 to 14}$\\
%     \KwOutput{ $[X_l]_{:,\mathcal{J}},\mathcal{H}_r([X_{l}]_{\mathcal{I},\mathcal{J}})^\dagger,[X_l]_{\mathcal{I},:}$: estimates of CUR components of $X$.}
% \end{algorithm} 

\begin{algorithm}
\caption{Iterative CUR for matrix Completion with Resampling (ICURC-R) } \label{ICURC}
\DontPrintSemicolon
    \KwInput{Observed matrix $Y = \mathcal P_\Phi(X)\in\mathbb{R}^{n_1\times n_2}$; 
  Predefined row and column indices $\mathcal{I},\mathcal{J}$; target rank $r$; error tolerance $\varepsilon;$ maximum iteration number $M$.}

    $X_0=0$, $\ell=0$\\
    \While {$e^{(\ell)}>\varepsilon$ or $\ell\le M$}{Randomly sample row and column indices $I$, $J$ with $|I|=|\mathcal{I}|$ and $|J|=|\mathcal{J}|$.\\
        % can\_stop = false\\
        % \uIf{$e_k > \varepsilon$ or $\ell\le M$}{
        %     can\_stop=true\\
        %     $I=\mathcal{I}$, $J=\mathcal{J}$
        % }
        % \Else{
        % Randomly sample row and column indices $I$, $J$ with $|I|=|\mathcal{I}|$ and $|J|=|\mathcal{J}|$.\\
        % }
        Set $I^{c}=[n_1]\setminus I$, $J^{c}=[n_2]\setminus J$.\\
        $[R^{(\ell+1)}]_{:,J^{c}}=[X^{(\ell)}]_{I,J^c}+ [Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{I,J^c}$\\
        $[C^{(\ell+1)}]_{I^{c},:}=[X^{(\ell)}]_{I^c,J}+ [Y-\mathcal{P}_{\Phi}(X^\ell)]_{I^c,J}$\\
        $U^{(\ell+1)}=\mathcal{H}_r\left([X^{(\ell)}]_{I,J}+ [Y-\mathcal{P}_{\Phi}(X^{(\ell)})]_{I,J}  \right)$\\
            $[R^{(\ell+1)}]_{:,J}=U^{(\ell+1)}$\\
            $[C^{(\ell+1)}]_{I,:}=U^{(\ell+1)}$\\
        $X^{(\ell+1)}$ = $C^{(\ell+1)}(U^{(\ell+1)})^{\dagger}R^{(\ell+1)}$\\
        $\ell = \ell + 1$\\
        % \uIf{can\_break}{
        %     stop
        % }
    }

    % $\mathcal{I} = \mathcal{I}_{ccs}$, $\mathcal{J} = \mathcal{J}_{ccs}$\\
    % $\textnormal{Do step 7 to 14}$\\
    \KwOutput{ $[X^{(\ell)}]_{:,\mathcal{J}},(\mathcal{H}_r([X^{(\ell)}]_{\mathcal{I},\mathcal{J}}))^\dagger,[X^{(\ell)}]_{\mathcal{I},:}$: estimates of CUR components of $X$.}
\end{algorithm} 

% \begin{algorithm}
% \caption{ICURC\_Resample algorithm \textbf{(Better to directly copy and paste algorithm in the paper cited)}}\label{icurc}
% \DontPrintSemicolon
%     \KwInput{An incomplete matrix $\mathcal{M}$ of size $N_1\times N_2$, sampled rows $I_{ccs}$, sampled columns $J_{ccs}$, expected tensor rank $r$, maximum iteration $MaxIter$, error tolerance $tol$}
%     \KwOutput{$C, U^{\dagger}, R$}
    
% \end{algorithm}

For every frontal slice of $[\hat{\mathcal Y}]_{:,:,k}$, 
ICURC-R returns three matrices $[\hat{\mathcal{C}}]_{:,:,k}, [\hat{\mathcal{U}}]_{:,:,k},$ and $[\hat{\mathcal{R}}]_{:,:,k}$. By combining all the matrices as frontal slices, we obtain three corresponding tensors
  $\hat{\mathcal{C}}, \hat{\mathcal{U}}$, and  $\hat{\mathcal{R}}.$ Then we take their inverse Fourier transforms, followed by  t-product, i.e., $\widetilde{\mathcal{X}}= \mathcal{C}\ast \mathcal{U}\ast \mathcal{R}$ as an estimate of $\mathcal{X}$. 

\section{Experiments on completion algorithms}
\label{sec:experiments}



We compare the performance of the two regularizations (TNN and TL12) and one decomposition method (TCCUR) on both synthetic data  and a color image inpainting problem. We generate the sampling set $\Phi\subseteq[n_1]\times[n_2]$ uniformly at random under a preset sampling ratio (without replacements) to define the index set $\Omega$ in \eqref{eq:tubalsampling}. We evaluate the performance by the relative error (RE) and peak signal-to-noise ratio (PSNR), i.e.,
\[\mbox{RE}=\frac{\|\mathcal{X}-\tilde{\mathcal{X}}\|_{\fro}}{\|\mathcal{X}\|_{\fro}} \ \mbox{and} \     \textnormal{PSNR}=10 \log_{10}\left(\frac{n_1n_2n_3\mathcal{X}^2_{\text{max}}}{ \|\tilde{\mathcal{X}}-\mathcal{X}\|^2_\fro}\right),
\]
where $\tilde{\mathcal{X}}\in\mathbb{R}^{n_1\times n_2\times n_3}$ is the recovered tensor and $\mathcal X$ is the ground truth with its maximum absolute value, denoted by $\mathcal{X}_{\text{max}}$.
All   simulations were performed on a laptop with 2.30 GHz Intel(R) Core(TM) i7-11800H processor and 16GB RAM.




\subsection{Synthetic data}
\label{sec:synthetic}
%We first perform the experiment to test the accuracy of different methods with respect to different sampling ratio(SR). In this simulation, 

\begin{figure}
\centering
 \includegraphics[width=0.49\linewidth]{Images/Rank_3_Result.pdf}  
 \includegraphics[width=0.49\linewidth]{Images/Rank_5_Result.pdf}
\caption{%This figure shows the results of different tensor completion methods on synthetic tensor completion experiments.
\small REs of completing an underlying tensor of tubal rank 3 (left) and 5 (right) versus SRs. %shows the results of different sampling ratios against $\log$ based accuracy under rank 3 synthetic tensor; the middle one shows the results when the synthetic tensor has rank 5. 
}
\label{fig:syndatacompletion}
\end{figure}

\begin{figure}
    \centering
     \includegraphics[width= 0.49\linewidth]{Images/Dimension_vs_time.pdf}
     \includegraphics[width= 0.49\linewidth]{Images/Dimension_vs_time_r3.pdf}
    \caption{\small Runtime versus the frontal slice dimension  $2^n$  of $2^n\times 2^n\times 32$ tensors with tubal rank 2 (left) and 3 (right).}
    \label{fig:time}
\end{figure}

We generate a tensor $\mathcal{X}\in\mathbb{R}^{256\times 256\times 50}$ with tubal rank $r\in\{3,5\}$. For each preset rank, Fig.~\ref{fig:syndatacompletion} shows the mean of REs over 30 random realizations with respect to sampling ratio (SR), showing that both TL12 and TCCUR achieve comparable and even better performance than  TNN. Moreover, the TL12 method has  the fastest decay of RE for smaller SRs (e.g., $10\%-20\%$).

%we generate $30$ test examples and the quality of the reconstruction result is measured by considering the relative error between   groundtruth $\mathcal{T}$ and   estimate $\widetilde{\mathcal{T}}$: 

%The performances for different methods are reported in the first two figures of . In this figure,  the averaged the relative errors over 30 tests  are reported.  One can see that  \cite{}.  

% \yl{[we only define RE for ground-truth, which is supposed to be unknown; how did you compute this RE]}

We also examine the scalability of the algorithms by reporting the runtime with respect to the tensor's dimensions. In particular, we generate a tensor $\mathcal{X}\in\mathbb{R}^{2^n\times 2^n\times 32}$ with tubal rank $2$ and $3$ for $n=6, 7, 8, 9, 10.$ We randomly select $30\%$ tubals and adopt the same stopping condition for all the algorithms, that is, the relative error on the observed portion is less than 
 $10^{-6}$. The computational time
 %(excluding the time to calculate the relative error) 
 is reported in Fig.~\ref{fig:time}, illustrating significant advantages in the efficiency of TCCUR over TNN and TL12. In addition, TL12 is comparable in speed compared to TNN, yet gives better completion accuracy. 
%. One can see our TCCUR has significant speed advantages compared to TNN and TL12 and TL12 has comparable speed compared with TNN.
% which is generated by using tensor multiplication on two randomly generated tensors of size $256\times r\times 50$ and $r\times 256\times 50$. Here the rank of the resulting matrix is guarenteed bounded by 50 by theorem \textbf{(cite the corresponding theorem)}, and by randomness it'll result in a low rank tensor with extremly high probability, since the entries in both tensors are generated uniformly at random so that they both have high probability to be full rank tensors.\\ 
% The measurement for accuracy in this section is the given by the relative error 
% \begin{equation}\label{relerr}
%     \textit{Relative Error = }\frac{||\tilde{\mathcal{T}}-\mathcal{T}||_F}{||\mathcal{T}||_F}
% \end{equation}
% where $\tilde{\mathcal{T}}$ is the completed tensor and $\mathcal{T}$ is the ground truth tensor. 
% the stopping condition  is 
% \begin{equation}\label{recerr}
%     \textit{Recovery Error = } \frac{||\mathcal{P}_{\Omega}(\tilde{\mathcal{T}})-\mathcal{P}_{\Omega}(\mathcal{T})||_F}{||\mathcal{P}_{\Omega}(\mathcal{T})||_F}<\textnormal{tol}
% \end{equation}
% where $tol$ is the pre-specified error tolerance. The $\Omega$ denotes the observed part of the given tensor.

%We recorded and compared the accuracy and time efficiency of TNN, TL12, and TCCUR methods. In this experiment setting, the completed tensor for TCCUR method is $\tilde{\mathcal{T}}=C\ast U\ast R$ where $C, U, R$ are the outputs of algorithm \ref{icurcr}. For each specific sampling ratio, 30 random tensors with rank $r$ are generated, and we calculated the average accuracy of each method. \\
%The second part of the experiments is to test the time efficiency of different methods when applying to tensors with various sizes.  The tensors generated are $\mathcal{T}\in\mathbb{R}^{2^n\times 2^n\times 32}$, where $n\in\{6, 7, 8, 9, 10\}$. The tensor rank is set to be 3 in all the cases, and the sampling ratio is 30\%. For each combination of the tensor size, 30 times of experiments were performed for each tensor completion method.\\
%The results are presented in Fig.\ref{fig:realdatacompletion}. As shown in the left and middle plots, in the rank 3 case, all three methods achieved nearly the same accuracy. For the rank 5 case, the TL12-based ADMM method quickly gets relative error in the order of $10^{-5}$ when the sampling ratio was only $15\%$, and stays around $10^{-6}$ after that; the TCCUR method, on the other hand, stays around $10^{-3}$ relative error for low sampling ratios until the sampling ratio is increased to $30\%$, and then remains comparable error as the TL12 method. It demonstrates that the TL12 method can be more reliable in accuracy when the sampling ratio is relatively low. In the right most plot, it shows the time efficiency of the three different methods; the TL12 method slightly outperformed TNN method, while the TCCUR method significantly outperformed the TNN and TL12 methods in time. 
\subsection{Application on Image Inpainting}
\label{sec:realdata}
% \begin{figure}[t]
% \centering
% % \begin{minipage}{.19\linewidth} \centering \small \hspace{2mm}  Original\end{minipage}
% % \includegraphics[width=.19\linewidth%keepaspectratio
% % ]{Images/Original iamge.jpg}
% % \\
% \begin{minipage}{.23\linewidth} \centering \small TRPCA\end{minipage}
% \begin{minipage}{.23\linewidth} \centering \small PSTNN\end{minipage}
% \begin{minipage}{.23\linewidth} \centering \small  TL12\end{minipage}
% \begin{minipage}{.23\linewidth} \centering \small  TCCUR\end{minipage}\\

% % \includegraphics[width=.19\linewidth
% % ]{Images/Sampled Image 0.05.jpg}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed facade 0.5 sampling TRPCA.eps}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed Image ADMM 0.15.jpg}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed Image TL12 0.15.jpg}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed Image CUR 0.15 Rank 30.jpg}
% \\
% % \begin{minipage}{.19\linewidth} \centering \small \hspace{2mm}  \end{minipage}
% % % \includegraphics[width=.19\linewidth
% % % ]{Images/Sampled Image 0.1.jpg}
% % \includegraphics[width=.19\linewidth]
% % {Images/Reconstructed Image CUR 0.1 Rank 30.jpg}
% % \includegraphics[width=.19\linewidth 
% % ]{Images/Reconstructed Image ADMM 0.1.jpg}
% % \includegraphics[width=.19\linewidth 
% % ]{Images/Reconstructed image TV 0.1.jpg}\\
% % \begin{minipage}{.19\linewidth} \centering \small \hspace{2mm}  \end{minipage}
% \includegraphics[width=.23\linewidth%keepaspectratio
% ]{Images/TV difference.jpg}
% \includegraphics[width=.23\linewidth 
% ]{Images/TNN difference.jpg}
% \includegraphics[width=.23\linewidth 
% ]{Images/L12 difference.jpg}
% \includegraphics[width=.23\linewidth 
% ]{Images/TCCUR difference.jpg}
% \caption{Image inpainting results (top) from $15\%$ sampled pixels and the corresponding difference maps (bottom). }\label{fig:img_inpainting}
% \end{figure}





% \begin{figure}[t]
% \centering

% \begin{minipage}{.23\linewidth} \centering \small TRPCA\end{minipage}
% \begin{minipage}{.23\linewidth} \centering \small PSTNN\end{minipage}
% \begin{minipage}{.23\linewidth} \centering \small  TL12\end{minipage}
% \begin{minipage}{.23\linewidth} \centering \small  TCCUR\end{minipage}\\


% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed door 0.5 sampling TRPCA.eps}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed door 0.5 sampling PSTNN.eps}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed door 0.5 sampling TL12.eps}
% \includegraphics[width=.23\linewidth]
% {Images/Reconstructed door 0.5 sampling CUR.eps}
% \\

% \includegraphics[width=.23\linewidth%keepaspectratio
% ]{Images/Reconstructed starfish 0.5 sampling TRPCA.eps}
% \includegraphics[width=.23\linewidth 
% ]{Images/Reconstructed starfish 0.5 sampling PSTNN.eps}
% \includegraphics[width=.23\linewidth 
% ]{Images/Reconstructed starfish 0.5 sampling TL12.eps}
% \includegraphics[width=.23\linewidth 
% ]{Images/Reconstructed starfish 0.5 sampling CUR.eps}
% \\

% \includegraphics[width=.23\linewidth%keepaspectratio
% ]{Images/Reconstructed hat 0.5 sampling TRPCA.eps}
% \includegraphics[width=.23\linewidth 
% ]{Images/Reconstructed hat 0.5 sampling PSTNN.eps}
% \includegraphics[width=.23\linewidth 
% ]{Images/Reconstructed hat 0.5 sampling TL12.eps}
% \includegraphics[width=.23\linewidth 
% ]{Images/Reconstructed hat 0.5 sampling CUR.eps}
% \caption{Image inpainting results from $50\%$ sampled pixels, compared to state of the art methods. }\label{fig:img_inpainting}
% \end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=.24\linewidth]
{Images/Reconstructed_door_5e-1_sampling_TRLRF.pdf}
\includegraphics[width=.24\linewidth]
{Images/Reconstructed_door_5e-1_sampling_PSTNN.pdf}
\includegraphics[width=.24\linewidth]
{Images/Reconstructed_door_5e-1_sampling_TL12.pdf}
\includegraphics[width=.24\linewidth]
{Images/Reconstructed_door_5e-1_sampling_CUR.pdf}
\\
\includegraphics[width=.24\linewidth
]{Images/Reconstructed_hat_5e-1_sampling_TRLRF.pdf}
\includegraphics[width=.24\linewidth 
]{Images/Reconstructed_hat_5e-1_sampling_PSTNN.pdf}
\includegraphics[width=.24\linewidth 
]{Images/Reconstructed_hat_5e-1_sampling_TL12.pdf}
\includegraphics[width=.24\linewidth 
]{Images/Reconstructed_hat_5e-1_sampling_CUR.pdf}
\\
\includegraphics[width=.24\linewidth
]{Images/Reconstructed_starfish_5e-1_sampling_TRLRF.pdf}
\includegraphics[width=.24\linewidth 
]{Images/Reconstructed_starfish_5e-1_sampling_PSTNN.pdf}
\includegraphics[width=.24\linewidth 
]{Images/Reconstructed_starfish_5e-1_sampling_TL12.pdf}
\includegraphics[width=.24\linewidth 
]{Images/Reconstructed_starfish_5e-1_sampling_CUR.pdf}
\begin{minipage}{.24\linewidth} \centering \small TRLRF \end{minipage}
\begin{minipage}{.24\linewidth} \centering \small PSTNN\end{minipage}
\begin{minipage}{.24\linewidth} \centering \small  TL12\end{minipage}
\begin{minipage}{.24\linewidth} \centering \small  TCCUR\end{minipage}
\caption{Visual comparison of color image inpainting results with two state-of-the art methods. From top to bottom, images are labeled as ``Door,'' `` Hat,'' and ``Starfish,'' all are taken from the PSTNN paper.}\label{fig:img_inpainting}
% \vspace{-0.2in}
\end{figure}

 We investigate a real application of image inpainting on three color images used in \cite{Jiang_2020}. 
 We compare the proposed methods to two state-of-the-art methods named TRLRF \cite{yuan2019tensor} and PSTNN \cite{Jiang_2020}. 
 %The testing images are the ones used in demonstrating the efficiency of PSTNN . %is of size $1669\times 2048\times 3$ in color, named ``Windows"\footnote{This dataset can be downloaded from \href{https://www.shutterstock.com/image-photo/exterior-building-closed-window-on-cement-1831174624}{https://www.shutterstock.com/image-photo/exterior-building-closed-window-on-cement-1831174624}}.%
 We randomly sample   $50\%$  tubals and compare image recovery results obtained by TRLRF, PSTNN, TL12, and TCCUR. 
 Table~\ref{tab:realdata} reports the quantitative measures of inpainting performance in terms of PSNR and computation time.  TCCUR is significantly faster than other methods, and TL12 yields the best results in all test cases both visually and in terms of SNR, though slower than other methods.
Fig.~\ref{fig:img_inpainting} shows the reconstruction results; PSTNN clearly does not achieve, while TRLRF and TCCUR produce more severe artifacts near the rim of the hat, compared to TL12.
 

\begin{table}[t]
 \label{tab:realdata}
\centering
\begin{adjustbox}{width=\columnwidth,center}
\begin{tabular}{ c|c c|c c|c c } 

\toprule

~ &  \multicolumn{2}{c|}{Door}  & \multicolumn{2}{c|}{Hat} & \multicolumn{2}{c}{Starfish} \cr

%\cline{2-7}
%\midrule
~                    & PSNR &  Time                & PSNR &  Time    & PSNR &  Time        \cr

%\hhline {|=||=|=|=|=|=|=|=|=|}
\midrule

TRLRF   &  30.01    & 5.02s    &  26.55  & 4.54s & 23.93 & 13.85s\cr

PSTNN    &  28.13   &  13.58s  & 19.38    &  2.34s & 16.08 & 4.68s \cr

TL12    &  \textbf{31.13}   & 63.60s   & \textbf{27.12} &  8.59s & \textbf{25.94} & 22.08s \cr

TCCUR    &  28.27   & \textbf{4.96s}   & 26.37    & \textbf{1.22s}& 24.14 & \textbf{4.27s} \cr

\bottomrule

\end{tabular}
\end{adjustbox}
\caption{Quantitative comparison of image inpainting from $50\%$ tubal sampling ratio.}
\label{tab:realdata}
\end{table}

 
 
% all the methods achieved decent reconstruction in the cases when sampling ratio is $10\%$ or $15\%$; the total variation method achieves comparably smaller SNR, especially for the case when sampling ratio is $5\%$. On the other hand, it is evident that the TCCUR method takes significantly less time than the TNN and TL12 method. Figure \ref{fig:img_inpainting} shows one realization of each case. We also plotted the difference image between the ground truth image and the completed image; in the difference image of TCCUR, the patterns are random, while in total variation, it clearly reflects the shape of the windows, showing that the total variation method recognized the feature in the picture as noise, while our method correctly capture the features in the image. 



% \begin{figure}
% \centering
% \captionsetup[subfigure]{labelformat=empty, font=small, justification=centering}
% \begin{subfigure}[t]{0.1\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Original iamge.jpg}
%     \caption{Origional}
%     \label{fig:11}
% \end{subfigure}
% \begin{subfigure}[t]{0.1\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Sampled Image 0.05.jpg}
%     \caption{Observed Image with $sr=5\%$}
%     \label{fig:11}
% \end{subfigure}
% \begin{subfigure}[t]{0.1\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR 0.05 Rank 30.jpg}
%     \caption{ICURCR\\SNR=29.51}
%     \label{fig:12}
% \end{subfigure}
% \begin{subfigure}[t]{0.1\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image ADMM 0.05.jpg}
%     \caption{ADMM\\SNR=29.20}
%     \label{fig:13}
% \end{subfigure}
% \begin{subfigure}[t]{0.09\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed image TV 0.05.jpg}
%     \caption{TV\\SNR=19.80}
%     \label{fig:14}
% \end{subfigure}
% % \hfill
% % \begin{subfigure}[t]{0.08\textwidth}
% %     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR Nonresample 0.05.jpg}
% %     \caption{ICURC\\SNR=19.87}
% %     \label{fig:14}
% % \end{subfigure}


% \begin{subfigure}[t]{0.09\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR 0.1 Rank 30.jpg}
%     \caption{ICURCR\\SNR=29.51}
%     \label{fig:12}
% \end{subfigure}
% \begin{subfigure}[t]{0.09\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image ADMM 0.1.jpg}
%     \caption{ADMM\\SNR=29.20}
%     \label{fig:13}
% \end{subfigure}
% \begin{subfigure}[t]{0.09\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Sampled Image 0.1.jpg}
%     \caption{Sampled 10\%}
%     \label{fig:11}
% \end{subfigure}
% \begin{subfigure}[t]{0.09\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed image TV 0.1.jpg}
%     \caption{TV\\SNR=19.80}
%     \label{fig:14}
% \end{subfigure}
% % \begin{subfigure}[t]{0.08\textwidth}
% %     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR Nonresample 0.1.jpg}
% %     \caption{ICURC\\SNR=19.87}
% %     \label{fig:14}
% % \end{subfigure}
        
% \caption{This figure shows the result of real image missing entry reconstruction results. \textbf{(To be modified; need to discuss how to arrange)}}
% \label{fig:realdatacompletion}
% \end{figure}
 

% \begin{figure}
% \centering
% \captionsetup[subfigure]{labelformat=empty, font=small, justification=centering}
% \begin{subfigure}[t]{0.1\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Original iamge.jpg}
%     \caption{Origional}
%     \label{fig:11}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Sampled Image 0.05.jpg}
%     \caption{Sampled 5\%}
%     \label{fig:11}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Sampled Image 0.1.jpg}
%     \caption{Sampled 10\%}
%     \label{fig:11}
% \end{subfigure}

% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR 0.05 Rank 30.jpg}
%     \caption{ICURCR\\SNR=29.51}
%     \label{fig:12}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image ADMM 0.05.jpg}
%     \caption{ADMM\\SNR=29.20}
%     \label{fig:13}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed image TV 0.05.jpg}
%     \caption{TV\\SNR=19.80}
%     \label{fig:14}
% \end{subfigure}
% % \hfill
% % \begin{subfigure}[t]{0.08\textwidth}
% %     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR Nonresample 0.05.jpg}
% %     \caption{ICURC\\SNR=19.87}
% %     \label{fig:14}
% % \end{subfigure}


% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR 0.1 Rank 30.jpg}
%     \caption{ICURCR\\SNR=29.51}
%     \label{fig:12}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed Image ADMM 0.1.jpg}
%     \caption{ADMM\\SNR=29.20}
%     \label{fig:13}
% \end{subfigure}
% \hfill
% \begin{subfigure}[t]{0.11\textwidth}
%     \includegraphics[width=1\textwidth]{Images/Reconstructed image TV 0.1.jpg}
%     \caption{TV\\SNR=19.80}
%     \label{fig:14}
% \end{subfigure}
% % \hfill
% % \begin{subfigure}[t]{0.08\textwidth}
% %     \includegraphics[width=1\textwidth]{Images/Reconstructed Image CUR Nonresample 0.1.jpg}
% %     \caption{ICURC\\SNR=19.87}
% %     \label{fig:14}
% % \end{subfigure}


        
% \caption{This figure shows the result of real image missing entry reconstruction results. \textbf{(To be modified; need to discuss how to arrange)}}
% \label{fig:realdatacompletion}
% \end{figure}




\section{Conclusion and discussion}
This paper proposed two novel non-convex tensor completion methods, namely TL12 and TCCUR. Simulation results   demonstrated the trade-off between accuracy and computational costs by using the two methods; the regularization-based method (TL12) achieves high accuracy in tensor completion but at a cost
of high computational complexity, while the decomposition method (TCCUR) is eicient, but its usage is limited to tubal
sampling. Additionally, we considered a real application of color image inpainting and showed the proposed methods outperform the state-of-the-art methods. 

% performed real colored image reconstruction to show that both of the methods outperformed classical TV method, and are able to correctly capture the features in the image. For future work, it would be interesting to test the accuracy and time efficiency for both methods under various sampling types.


\newpage

% \section{REFERENCES}
% \label{sec:refs}

% List and number all bibliographical references at the end of the
% paper. The references can be numbered in alphabetic order or in
% order of appearance in the document. When referring to them in
% the text, type the corresponding reference number in square
% brackets as shown at the end of this sentence \cite{C2}. An
% additional final page (the fifth page, in most cases) is
% allowed, but must contain only references to the prior
% literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\small
\bibliographystyle{IEEEbib}
\bibliography{strings,refs,sparse}

\end{document}
