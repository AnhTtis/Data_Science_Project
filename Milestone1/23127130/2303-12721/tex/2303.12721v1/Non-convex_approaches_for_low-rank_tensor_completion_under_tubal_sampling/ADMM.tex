\section{Tensor Low-rank regularization}\label{sec:ADMM}


Recovering a (complete) tensor $\mathcal X$ from its partial observations $\mathcal Y=\mathcal{P}_\Omega(\mathcal X)$ is a highly ill-posed problem. We are interested in finding a tensor with  small tubal rank by imposing a proper regularization, denoted by $h(\cdot)$. We consider a general model for low-rank tensor completion
\begin{equation}
 \min_{\mathcal X\in \mathbb{R}^{n_1\times n_2\times n_3}} h(\mathcal X) \quad s.t. \quad \mathcal{Y}=\mathcal{P}_\Omega(\mathcal X). \label{eq:4_Ch02}
\end{equation}
By introducing an auxiliary variable $\mathcal Z$, we adopt the alternating direction method of multipliers (ADMM) \cite{boyd2010distributed} that iterates as follows,
\begin{align*}
    \mathcal{X}^{(\ell+1)} &= \arg\min_\mathcal{X}\{ ||\mathcal{X} - (\mathcal{Z}^{(\ell)} - \mathcal{B}^{(\ell)})||_\text{F}^2 \ \mbox{s.t.} \ \mathcal{Y}=\mathcal{P}_\Omega(\mathcal X) \}\\ %\label{eqn:TNN-ADMM_X}\\
    \mathcal{Z}^{(\ell+1)} &= \arg\min_\mathcal{Z}\{\frac{1}{\rho}h(\mathcal Z) + \frac{1}{2}||\mathcal{Z} - (\mathcal{X}^{(\ell+1)} + \mathcal{B}^{(\ell)})||_\text{F}^2\}\\
    %, \label{eqn:TNN-ADMM_Z}\\
    \mathcal{B}^{(\ell+1)} &= \mathcal{B}^{(\ell)}+(\mathcal{X}^{(\ell)}-\mathcal{Z}^{(\ell+1)}),
\end{align*}
where $\mathcal B$ is a Lagrangian multiplier to enforce $\mathcal X=\mathcal Z$, $\rho>0$ is a weighting parameter, and $\ell$ counts the iterations. The algorithm alternates between $\mathcal X$ satisfying the data matching constraint and promoting $\mathcal Z$ to be low-rank. The closed-form solution for $\mathcal X^{(\ell+1)}$ is that it takes the values of $\mathcal Y$ on $\Omega$ and of $\mathcal Z^{(\ell)}-\mathcal B^{(\ell)}$ on the complement set of $\Omega$.

A popular choice of  $h(\cdot)$ is the tensor nuclear norm (TNN)~\cite{6909886}, defined by 
\begin{equation}
    ||\mathcal{X}||_{\text{TNN}} = \sum_{j=1}^{n_3} \sum_{i=1}^{m} [\mathcal S]_{i,i,j},
    \label{eqn:TNN}
\end{equation}
where $\mathcal X\in\mathbb{R}^{n_1\times n_2\times n_3}$ has the t-SVD of $\mathcal X=\mathcal U*\mathcal S*\mathcal V^\top$ and $m=\min(n_1,n_2)$. The algorithm that minimizes the TNN via ADMM is referred to as TNN-ADMM \cite{LiuX2020,popa2021improved}. The $\mathcal Z$-subproblem has a closed-form solution, referred to as tensor singular value thresholding \cite{lu2019tensor},
\begin{equation}
    \mathcal Z^{(\ell+1)} = \mathcal U*\mathcal S_{1/\rho}*\mathcal V^\top,
\end{equation}
where  t-SVD of $\mathcal X^{(\ell+1)}+\mathcal B^{(\ell)}$ is given by $\mathcal U*\mathcal S*\mathcal V^\top$ and $\mathcal S_{\mu}=\ifft(\max(\hat{\mathcal{S}}-\mu,0),[~],3).$ 

We propose the tensor $L_1$-$L_2$ (TL12) regularization, 
\begin{equation}
    ||\mathcal{X}||_{\text{TL12}} = \sum_{j=1}^{n_3} \left(\sum_{i=1}^m [\mathcal S]_{i,i,j}- \sqrt{\sum_{i=1}^m [\mathcal S]^2_{i,i,j}} \right),
    \label{eqn:TL12}
\end{equation}
By defining a vector $\h s_j=([\mathcal S]_{i,i,j})_{i=1,\cdots,m}$, the TL12 regularization is equivalent to the difference between the $L_1$ and $L_2$ norms of $\h s_j$, followed by summing over $j=1,\dots, n_3.$ The closed-form solution for the  $\mathcal Z$-subproblem is to replace $\max(\hat{\mathcal S}-\mu,0)$ in defining $\mathcal S_\mu$ by the proximal operator of $L_1$-$L_2$ formulated in \cite{louY18}. 

% TNN is defined based on the matrix nuclear norm, which is equivalent to the sum of the $L_1$ norm of a vector composed of all the singular values of each frontal slice after the Fourier transform. The convex $L_1$ norm can promote such vector to be sparse.  The $\mathcal Z$-subproblem corresponding to TL12 has a closed-form solution 

%To make our paper self-contained, we present its 
%ADMM alternates between satisfying the low rank requirement imposed by the TNN penalty term and satisfying the data matching constraint by splitting the variable $\mathcal{X}$ into two. Applying this method to solve equation \ref{eq:TNN-ADMM} results in the following
%iterative updates \cite{Kilmer2015},

% \yl{[agagin copy paste; need to revise]}where $k$ denotes the iteration, $\rho > 0$ is a step-length parameter, and $||\cdot||_\text{F}$ is the Frobenius norm. The indicator function is denoted by $1_{\mathcal{Y}=\mathcal{A}(\mathcal{X})}$, which takes the value 0 if the relation in the subscript is satisfied and $\infty$ otherwise. The variable $\mathcal{X}$ is required to satisfy the data matching constraint $\mathcal{Y}=\mathcal{A}(\mathcal{X})$ by the indicator function. The variable $\mathcal{Z}$ captures the TNN penalty and $\mathcal{B}$ is the dual variable.


% To perform tensor completion task, we considered two   methods: the and  tensor completion based on CUR decomposition(TCCUR). 
% Given  a 3-order tensor $\mathcal{T}$ of size $N_1\times N_2\times N_3$, with sampling set $\Omega$, and denote the sampled part of the tensor as $\mathcal{P}_\Omega(\mathcal{T})$. The TNN-based ADMM method is to solve the constrained minimization problem

% \begin{equation}
% \underset{T}{\text{min}} ||T||_* \quad\text{s.t.} \quad\mathcal{P}_\Omega(\mathcal{T})=\mathcal{X}
% \end{equation}

% where $\mathcal{X}$ is the incomplete tensor after sampling. Here the norm $||\cdot||_*$ denotes the tensor nuclear norm. By replacing the tensor nuclear norm with other non-convex norms, we can turn the optimization problem into a non-convex setting, and it would outperform the TNN-based ADMM method in accuracy. \\
% % \begin{equation}
% %     ||x||_\frac{L_1}{L_2}:=\frac{||x||_1}{||x||_2}
% % \end{equation}
% % and
% % \begin{equation}
% %     ||x||_\frac{1}{2}:=(\sum_j|x_j|^{\frac{1}{2}})^2
% % \end{equation}
% Our ICURCR method, on the other hand, extend the matrix completion CUR algorithm \cite{cai2022matrix}, which is formulated as