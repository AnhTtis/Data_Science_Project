\documentclass[11pt]{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage{amsmath,amssymb,amsthm,fullpage,mathrsfs,pgf,tikz,caption,subcaption,mathtools,mathabx}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{natbib}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{bm}
\hypersetup{colorlinks=true,citecolor=blue,linkcolor=red}
\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{thmtools}
\usepackage{float}

%%%packages from other overleaf
\usepackage[utf8]{inputenc}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\usepackage{algorithmicx}
\usepackage{comment}
\usepackage{xspace}
\usepackage{bbm}
\usepackage{ulem} \normalem
%\usepackage{paralist}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    linktocpage=true,
}




\input{macro}
\let\oldnl\nl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}
\author{%
  Mark Bun \and Marco Gaboardi \and
  Max Hopkins\thanks{Department of Computer Science and Engineering, UCSD, CA 92092. Email: \texttt{nmhopkin@eng.ucsd.edu}. Supported by NSF Award DGE-1650112.}\and Russell Impagliazzo \and Rex Lei \\ \and Toniann Pitassi
  \and Satchit Sivakumar \and Jessica Sorrell
}
\title{Connections and Separations Between Reproducibility and Privacy (Working Notes)}

\begin{document}
\maketitle
\begin{abstract}
    Stuff goes here.
\end{abstract}
\tableofcontents
\newpage


\newpage
\rexl{i'll take out the newpage when done writing this subsection}

\subsection{Pessiland Correlated Sampling}
\label{sec:owfi}

Correlated sampling algorithms give generic ways to create reproducible algorithms. 
Let $D_1, D_2$ be two distributions over $\X$ such that $\dtv(D_1,D_2)$ is small. A correlated sampling algorithm $\Bcal(D; r)$ for $\{D_1, D_2\}$ is a randomized algorithm that uses randomness $r$ to produce examples $x \in \X$ distributed according to distribution $D$. Furthermore, $\Bcal$ should produce the same example for two close distributions $D_1$ and $D_2$, when $\Bcal$ is run with the same random string $r$; i.e., $\Pr_{r} [\Bcal(D_1; r) \ne \Bcal(D_2;r)]$ should be proportional to $\dtv(D_1, D_2)$. 
More generally, a correlated sampling algorithm for distributions $\{D_i\}_i$ satisfies the second condition for all $D_j, D_k \in \{D_i\}_i$. 

\rexnote{The next paragraph is wordy and not direct enough. The point I'm trying to make is that correlated sampling can be used as a subroutine to make reproducible algorithms}
Say $\Acal(S; r)$ is a learning algorithm that takes as input a sample $S \sim D^m$ and uses internal randomness $r$ to produce an output. Let $D_1$ be the distribution of outputs of $\Acal(S_1; r)$ when sample $S_1$ is fixed, and analogously for $D_2$. 
\rexnote{Maybe more clearly, here describe why many algorithms can ensure that $D_1$ and $D_2$ are close.}
If $\Acal$ is designed such that $D_1$ and $D_2$ are close, then we can apply a correlated sampling algorithm to create a reproducible algorithm. 

In this Section, we show that the existence of one-way function inverters implies the ability to perform correlated sampling on arbitrary distributions (even with infinite support). Specifically, we show that (i) if there are no one-way functions, we can do average
case implicit correlated sampling, and (ii) if there are no non-uniform 
one-way functions, then there is polynomial time implicit correlated sampling.


\subsubsection{Definitions/Preliminaries}
\label{ssec:owfi-prelims}

\rexnote{Section currently incomplete and under revision}

\rexnote{Note to self: change Russell notation S for sampler to B (since S also is a sample of examples throughout the paper elsewhere). Change circuit D to circuit C with output distriubution $D_C$}



%We use circuits to model distributions.
\rexnote{Using circuits to model distributions? Is this ``cheating", by assuming distributions can all be approximated in this way? I guess not, since we are allowing $\nu$ slack in the approximations. 
i.e.: justify the following definition with 1-2 sentences
}

\begin{definition}[Implicit Correlated Sampling Algorithm]
\label{def:implicit-correlated-sampling-problem}
Let $m, n \in \Z^+$, and let $C: \{0,1\}^m \rightarrow \{0,1\}^n$ denote a circuit. 
Let error parameter $\nu >0$. 
$\Bcal$ is an  \emph{$(m,n,\nu)$-implicit correlated sampling algorithm} if the following conditions hold:
\begin{enumerate}
    \item \textbf{Inputs/Outputs:} $\Bcal$ takes as input a circuit $C:\{0,1\}^m \rightarrow \{0,1\}^n$, an error parameter $\nu$, and a random string $r$. $\Bcal$ outputs a string in $\{0,1\}^n$.  
    
    \item \textbf{$\nu$-distributional accuracy:} For all circuits $C: \{0,1\}^m \rightarrow \{0,1\}^n$, the distributions $D_C$ and $D_{\Bcal(C, \nu)}$ satisfy $\dtv(D_C, D_{\Bcal(C, \nu)}) \le \nu$.
    
    Here, $D_C$ denotes the distribution over $\{0,1\}^n$ induced by querying $C$ on  uniformly random inputs, i.e., probability density function $D_C(x) = \Pr_{r \sim U^m}[C(r) = x]$. 
    Similarly,
    $D_{\Bcal(C, \nu)}$ denotes the distribution over $\{0,1\}^n$ induced by querying $\Bcal(C, \nu; r)$ with uniformly random strings $r$.
%    \rexnote{The previous notational clarification sentence could be written more precisely/clearly}
    
    \item \textbf{Correlated sampling:} For all pairs of circuits $C_1, C_2: \{0,1\}^m \rightarrow \{0,1\}^n$, 
    $\Pr_{r}[\Bcal(C_1, \nu; r) \ne \Bcal(C_2, \nu; r)] \in O(\dtv(D_{C_1}, D_{C_2}) + \nu)$.
\end{enumerate}
\end{definition}

%In the implicit correlated sampling problem, the input is a circuit $D$ that uses $m$ bits of randomness to produce an output in $\{0,1\}^n$, i.e., $D: \{0,1\}^m \rightarrow \{0,1\}^n$.  Given an error parameter $\nu$, we want a sampling algorithm $\sc{S}(\nu, D, rand)$ whose distribution is within $\nu$ statistical distance of $D$, and so that  for any two such sampling circuits, $D_1,D_2$,  $Prob_{rand} [\sc{S}(\nu, D_1, rand) \neq \sc{S} (\nu , D_2 , rand)] \le O( SD(D_1,D_2)+ \nu)$.



We assume we can invert any one-way function on almost all inputs.  For now,
we will assume that there is no non-uniform one-way function family, so
that there is a uniform way of inverting any circuit computing a function via a polynomial-time inverter. 
%In other words, there is a polynomial-time  inverter $I(F, \nu, y)$ so that for any circuit $F: \{0,1\}^M\leftarrow \{0,1\}^N$ , $Prob_{R \in \{0,1\}^M} [F(I(F, \nu, F(R)))=F(R) ] \ge 1 - \nu$.  

\begin{definition}[Uniform One-Way Function Inverters]
\label{def:inverter-unif}
Let $\nu > 0$. $\Ical_{\nu}(C, y)$ is a \emph{polynomial-time uniform one-way function inverter with error $\nu$} if, for any circuit $C:\{0,1\}^m \rightarrow \{0,1\}^n$, 
$\Pr_{r \in \{0,1\}^m} [C(\Ical_{\nu}(C, C(r)))=C(r) ] \ge 1 - \nu$, and $\Ical$ runs in polynomial time.
\end{definition}



\rexnote{Notation: change circuit F to circuit C}



\subsubsection{The Construction 
\rexnote{Pick a better (more descriptive?) name for this subsection}
}
\label{ssec:owfi-construction}

\rexnote{Rewrite paragraph for intuition to completely explain how sampler works, and convince reader why owf inverters come into the picture naturally}
\rexnote{If we ever get a specific value for $k$, then explicitly say that in the beginning of each of the following algorithms (rather than just leaving it in $\theta$ notaiton)}
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{$\correlatedsampler$\\
			Input: \\
			Output: 
		}
		\label{alg:correlatedsampler}
		\begin{algorithmic}
		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$

		    \STATE $\beta \gets_r [1/2,1]$
		    \rexnote{Clarify if $\beta$ gets picked just once, or if it gets repicked each time $l$ gets repicked}

            \WHILE{1}
    
                \STATE $l \gets_r \{0,1,\dots, m\}$
    		       
    		    \STATE $H_1 \gets_r $ pairwise independent hash function from $n$ bits to $l+k$ bits 
    		        
    		    \STATE $u \gets_r \{0,1\}^{l+k}$
                
                \STATE $(x,q) \gets \elementfinder_{C, l, \beta} (H_1, u)$
                
                \IF{$(x,q) \ne \perp$}
                
                    \STATE $\coin \gets_r [0,1]$
                    \IF{$\coin \le q 2^k$}
                        \RETURN $x$
                    \ENDIF
                \ENDIF
            \ENDWHILE

		\end{algorithmic}
	\end{algorithm}
\end{figure}


\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{$\elementfinder_{C, \ell, \beta}$\\
		    (Explicit) Input: Hash function $H_1: \{0,1\}^{n} \rightarrow \{0,1\}^{l+k}$ (circuit) Strings $u \in \{0,1\}^{l+k}$\\
			(Implicit) Input: Circuit $C: \{0,1\}^m \rightarrow \{0,1\}^n$, integer $l \in \{0, 1, \dots, m\}$\\
			Output: String $x \in \{0,1\}^n$ and probability $q_x \in (0,1]$.
		}
		\label{alg:elementfinder}
		\begin{algorithmic}
		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$
		    \FOR{$i=1$ to $i = 16 (2^k) (1/\nu^2) \log (1/\nu)$}
		    
		        \STATE $H^i_2 \gets_r $ pairwise independent hash function from $m$ bits to $m-l+k$ bits 
		        
		        \STATE $v^i \gets_r \{0,1\}^{m-l+k}$

                \STATE Run $\hashchecker_{C,\ell, H_1, u} ( H_2^i, v^i)$    
		    \ENDFOR
		    
		    \STATE Let $q_x$ denote the fraction of times $x$ was returned by $\hashchecker$
		    
		    \IF{$\exists$ unique $x$ s.t. $x \ne \perp$ and $q_x \in ((\beta/2)2^{-k}, \beta 2^{-k}]$}
		    
		        \RETURN $(x, q_x)$
		    \ELSE
		        \RETURN $\perp$
		    \ENDIF
		\end{algorithmic}
	\end{algorithm}
\end{figure}





In the following algorithm, `$\mathbin\Vert$' denotes concatenation of strings. 


\begin{figure}[H]
	\begin{algorithm}[H]
	\caption{$\hashchecker_{C, l, H_1, u}(H_2,v)$\\
		    (Explicit) Input: Hash function (circuit) $H_2: \{0,1\}^{m} \rightarrow \{0,1\}^{m-l+k}$ and string $v \in \{0,1\}^{m-l+k}$\\
			(Implicit) Input: Circuit $C: \{0,1\}^m \rightarrow \{0,1\}^n$, integer $l \in \{0, 1, \dots, m\}$, hash functions $H_1: \{0,1\}^{n} \rightarrow \{0,1\}^{l+k}$ and string $u \in \{0,1\}^{l+k}$\\
			Output: String $x \in \{0,1\}^n$
		}
		\label{alg:hashchecker}
		\begin{algorithmic}
		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$

		    \STATE Define circuit $\Fcircuit (r) = H_1(C(r)) \mathbin\Vert H_2(r)$ 
		    %map strings of length $m$ to strings of length $m+2k$.
		    
		    \STATE Let $y \gets u \mathbin\Vert v$ 
	        \STATE $x \gets \Ical_{\nu} (\Fcircuit, y)$ 
	        
	        \IF{$x = \perp$}
		        \RETURN $\perp$
		        \rexnote{To do: (somewhere in this section) explain why and when the inverter can return $\perp$}
		    \ELSE
		        \RETURN $C(x)$
		    \ENDIF
		\end{algorithmic}
	\end{algorithm}
\end{figure}







At a high-level, we want to divide the distribution $D_C$ into ``levels'', where the probabilities of elements in each level are close, and use rejection sampling both to pick a level and to pick an element within that level.  We will use
the inverter to estimate the probability of an element, and test whether it is in the level we want.  
For each $x \in \{0,1\}^n$, let $p_D(x)$ be the probability that $x$ is output
using sampler $D$.
\rexnote{Maybe replace $p_D(x)$ notation with $D_C(x)$?}


We first pick $\beta \in [1/2, 1]$ uniformly, and divide the inputs into
$m$ levels $X_{\ell, \beta} = \{x | \beta 2^{- \ell-1} < p_D(x) \le \beta 
2^{-\ell}\}$.  (\rexnote{Later, check that boundary conditions when $\ell=0$ are fine})
We want to sample $\ell$ proportional to the probability that
a sample  $x$ is in $X_{\ell, \beta}$, and if we accept $\ell$, sample
each $x \in X_{\ell, \beta}$ 
proportionally to $p_D(x)$.  
The inverter $\Ical$ will be used to simultaneously  determine $x \in X_{\ell, \beta}$  and estimate
$p_D(x)$.    



We will pick a parameter $k \in \Theta( \log (m)+ \log (1/\nu))$  later.
Let $F_{C, \ell} (R, H_1, H_2) = (H_1, H_2, H_1(C(R)), H_2(R))$ where $H_1$
is a uniformly chosen pairwise independent hash function mapping $n$ bits to $\ell + k$
bits and $H_2$ is a uniformly chosen pairwise independent  hash function
mapping $m$ bits to $m- \ell +k$ bits.

Let ${A}_{C, \ell} (H_1,H_2, U, V)= C( \Ical (F_{C, {\ell}} (H_1,H_2,U, V)))$.
In the case when $\Ical$ does not find a preimage of the function,
usually because no preimage exists, we will view $\Ical$ and hence $\sc{A}_{C, \ell}$ as returning $NIL$. 

Let $\sc{B}_{C, \ell, \beta} (H_1, U)$ pick 
$B= 16 (2^k) (1/\nu^2) \log (1/\nu) $  
pairs
$H^i_2, V^i$, and run $\sc{A}_{C,\ell} (H_1, U, H_2^i, V^i)$  for  
each.     
Then it sees whether there is a unique $x \neq NIL$ in the list
of answers so that $x$ appears between $(\beta/2) 2^{-k}$ and $\beta 2^{-k}$
times in the list.  If so, it returns the pair $(x,q)$ where $q$ is the
fraction of times that $x$ was returned by $\sc{A}$. 
Otherwise, it returns NIL.

Let $\sc{S}_{C, \beta}$ repeat until a non-NIL value is
found: Pick $\ell \in_U \{0,1,..m+1\}$, pick $(H_1,U)$ and run 
$\sc{B}_{C, \ell, \beta} (H_1,U)$.  
If this returns a pair $(x,q)$, pick $a \in_U [0,1]$ and return
$x$ if $a \le q2^k$.  
Otherwise,return $NIL$.






\subsubsection{Analysis}

Fix $x \in \X_{\beta, \ell}$.  For a random $H_1, U$, there is a 
probability $2^{-(\ell +k)}$ that $U=H_1(x)$.  There are
$p_D(x)2^m$ random strings $R$  so that $C(R)=x$, and for
a random $H_2, V$, each of these satisfy $H_2(R)=V$ with probability
$2^{-(m-\ell +k)}$.   
Thus, $q$  is expected to be close to $p_D(x)2^m 2^{-(m -\ell +k)}=
p_D(x)2^{\ell}/2^k $.  

So to get a good intuition for why we expect this to work, let's
assume  that each $\sc{B}_{C,\ell,\beta}$ had this ideal behavior:  
For each $x$ in $X_{\ell, \beta}$, with probability exactly
$2^{-{\ell+k}}$, it returns the pair $(x, p_D(x)2^{\ell}/2^k)$
and otherwise it returns $NIL$. 
We will then show that the actual procedure is a close enough
approximation to this that the error introduced is small.

If the ideal situation above were true, consider any  $x$ in the
support of $D$.  $x$ is in $X_{C, \ell, \beta}$ for exactly one $\ell$.
So 
in one run of $\sc{S}_{C,\beta}$  we will pick this $\ell$ with probability
$1/(m+1)$, and if we do, $\sc{B}_{C,\ell, \beta}$ would return $(x,q)$ with probability $2^{-(\ell +k)}$.  Then we return $x$ with probability $q2^k= p(x)2^{\ell-k} 2^k= p(x)2^{\ell}$.  So the overall probability of returning $x$ in
one run is $1/(m+1) 2^{-(\ell+k)}p_D(x)2^{\ell}= p_D(x)/((m+1)2^k)$.  Thus,
the probabilities of each outcome are proportional to the probability under
$D_C$, so the conditional distribution is exactly that for $D_C$.   
Also, the overall probability of not returning $NIL$ is  $\sum_x p_D(x) /(m+1)2^k= 1/(m+1)2^k$ , so we expect to have $O(m2^k)$ runs before returning a sample.

If $x$ is in the same level $X_{C_1, \ell, \beta}$ and
$X_{C_2 , \ell, \beta}$, then under the ideal conditions,
the probability of returning $x$ in one run of $\sc{S}_{C_1, \beta}$
but not returing $x$ in the corresponding run of $\sc{S}_{C_2, \beta}$ is
the probability of returning $x$ times
the probability that $p_2(x)2^{\ell} = q_2 2^k < a < q_1 2^k=p_1(x)2^{\ell}$,
or $p_1(x)/(m+1)2^k* |p_1(x)-p_2(x)|2^{\ell}$. Since in this case, $p_1(x) \le
2^{-\ell}$, this is at most $1/(m+1)2^k |p_1(x)-p_2(x)|$.  Summing up over
all $x$, this gives a bound of $SD(D_{C_1},D_{C_2})/(m+1)2^k$  
of the answer returned by the first sampler not being returned by the second
for such $x$.
Conditioned on a non-NIL answer, this is at most a $SD(D_{C_1},D_{C_2})$ probability
of a difference. 

We now need to bound the expected 
total probability of $x$ so that $x$ is not in the same levels  $X_{C_1, \ell,
\beta}$ and $X_{C_2,\ell,\beta}$.  
Say that $p_1(x) = \gamma_1 2^{-\ell}$ with $1/2 < \gamma_1 \le 1$
and $p_2(x)= \gamma_2 2^{-\ell}$ with $\gamma_2 \le \gamma_1$. 
Then if $ \gamma_2 < \beta < \gamma_1$, they will be in different levels, 
or if $\gamma_2 < \beta/2 < \gamma_1 $. The latter can happen only if
$\gamma_2 < \gamma_1/2$, in which case $|p_1(x)-p_2(x) |2^{\ell} > 1/2 p_1(x) 2^{\ell} > 1/4$, so the probability in either case    
of being in separate levels is at most $O(|p_1(x)-p_2(x)|2^{\ell})$.  
Then the contribution to the expected probability of elements in distinct
levels is $O(|p_1(x)-p_2(x)|2^{\ell} p_1(x)) = O(|p_1(x)-p_2(x)|)$.
Thus,the total expected weight of elements in $D_1$ in distinct levels is 
$O(SD(D_{C_1},D_{C_2}))$.
Since the sampler produces the same distribution as $D_{C_1}$, the probability
that we output such an element is $O(SD(D_{C_1},D_{C_2}))$.  



\end{document}