
\begin{lemma}[Inverter Never Fails]\label{lem:inverter-failure}
The probability that the inverter fails at any point throughout $\correlatedsampler$ is at most $1-O(\nu)$.
\end{lemma}
\begin{proof}
Recall our inverter has the following guarantee
\[
\Pr_{r' \sim \{0,1\}^m} [C(\Ical_{\nu'}(C, C(r')); r)=C(r') ] \ge 1 - \nu',
\]
where $r$ stands for the internal randomness of $\Ical$ and $\nu'$ can be taken to be polynomially small in $m,n$ and $\nu$. It will be enough to take the failure rate $\nu' \leq O(\frac{\nu^2}{T_1T_2}) = \poly(\nu^{-1},m,n)$. We will bound the probability such an inverter fails on a random input $(u || v)$. 

First, observe that by Markov's inequality, most choices of internal randomness $r$ for the inverter random work for almost all $r' \in \{0,1\}^m$:
\[
\Pr_r \left[ \Pr_{r'}\left[\Ical_{\nu'}(C, C(r')); r)\neq C(r')\right] \geq O\left(\frac{\nu}{T_1T_2}\right)\right] \leq O(\nu).
\]
Assume then this event does not occur. Our algorithm only fails if the random choice of $(u || v)$ is hashed to by a string for which $\Ical_{\nu'}(C, C(r')); r)\neq C(r')$. In the worst case, these bad strings each correspond to unique $(u || v) \in \{0,1\}^{m+2k}$, in which case we have a total of $\frac{\nu}{T_1T_2}2^m$ out of $2^{m+2k}$ bad inputs. Union bounding over the $T_1T_2$ applications of the inverter in $\correlatedsampler$ gives a total failure probability of $1-O(\nu)$ as desired.
\end{proof}

Second, we will prove a number of useful conditions hold with overwhelming probability (allowing us assume them freely in the analysis).
\begin{lemma}\label{lem:corrsamp-conditions}
The following conditions hold across all steps of $\correlatedsampler$ with probability at least $1-O(\nu)$:
\begin{enumerate}
    \item The (pre)-pre-image of $u$ is small: 
    \[
    |C^{-1}H_1^{-1}(u)| \leq O\left(2^{m-\ell} \cdot \frac{mk\log(1/\nu)}{\nu}\right)
    \]
    \item For every $\ell$, there is at most one $y^* \in H_1^{-1}(u)$ s.t.
    \[
    p_D(y^*) \in [2^{-\ell+2},2^{-\ell-2}]
    \]
    \item $\elementfinder$ always returns $(y^*,\hat{q}(y^*))$ or `$\bot$'
    \item The empirical estimate of $y^{*}$ is good:
    \[
    \hat{q}(y^*) \in \left(1\pm O\left(\nu\right)\right)p_D(y^*)2^{\ell-k}
    \]\jess{I think we'll need an empirical estimate for arbitrary $x$, not just $y^*$, and in this case the bound is $\hat{q}(x) \in 2^{\ell - k}p_D(x) \pm O(\nu)2^{-\Theta(k)}$. (May need to take $T_1$ to have $2^{ck}$ dependence on $k$ to get something useful, instead of just $2^k$, not yet sure.)}
\end{enumerate}
\end{lemma}
\begin{proof}

The first condition follows from Markov's inequality. In particular, the expected amount of strings that hash to $u$ (through $C$) is $2^{m-\ell-k}$. Then by Markov the probability that $O(\frac{mk2^{m-\ell}\log(1/\nu)}{\nu})$ strings hash to $u$ is at most $O(\frac{\nu}{mk2^k\log(1/\nu)})\leq O(\nu/T_1)$. Union bounding over the $T_1$ choices of $H_1$ and $u$ gives the desired result.

To show the second condition, observe there are at most $2^{\ell+2}$ elements with measure in the range $[2^{-\ell+2},2^{-\ell-2}]$. Since our hash function is pairwise-independent, the probability of a collision between any two pairs is bounded by $\frac{2^{2\ell+4}}{2^{2\ell+2k}} = 2^{4-2k}$. Union bounding over the $T_1$ choices of $H_1$, $u$, and $\ell$, a collision still occurs with probability at most $2^{-\Omega(k)} \leq O(\nu)$ as desired.

To prove the third condition, assume the first two hold. It is enough to argue that any $x \neq y^*$ satisfies:
    \[
    \hat{q}(x) \notin [(\beta/2)2^{-k},\beta2^{-k}]
    \]
% \begin{enumerate}
%     \item $y^*$ is good:
%     \[
%     \hat{q}(y^*) \in [(\beta/2)2^{-k},\beta2^{-k}]
%     \]
%     \item 
%     \[
%     \hat{q}(x) \notin [(\beta/2)2^{-k},\beta2^{-k}]
%     \]
% \end{enumerate}
To show this, first note that the expected number of $H_2$ collisions in the pre-image of $u$ is at most:
\[
\frac{|C^{-1}H_1^{-1}(u)|^2}{2^{2m-2\ell+2k}} \leq O\left(\frac{m^2k^2\log^2(1/\nu)}{\nu^2}2^{-2k}\right).
\]
% Let $T=O(\frac{2^k}{\nu^2}\log(1/\nu))$ represent the number of $H_2$ drawn by $\elementfinder$, and observe that the above probability bound is at most $1/T$. 
Markov's inequality then implies that the probability more than 
\[
C_{col}=O\left(\frac{m^2k^2\log^2(1/\nu)}{\nu^2}2^{-2k}\right)\cdot T_2
\]
collisions occur is less than $O(\nu/T_1)$, so union bounding over all rounds of $\correlatedsampler$ this occurs with probability at most $O(\nu)$. 

Assume then that at most $C_{col}$ collisions occur. The argument now breaks into two parts. First, consider any element $y$ s.t.\ $p_D(y) \leq 2^{-\ell-2}$. In the worst case, these elements hash to at most a $\frac{2^{-k}}{4}$ fraction of range of $H_2$. \chris{might be nice to spell this out, unclear what ``these elements" are: ``Since there are at most $2^{m}2^{-l-2}$ random strings $r'$ such that $C(r') = y$, these strings $r'$ can hash to at most a $\frac{2^{-k}}{4}$ fraction of the range of $H_2$"} Since collisions only lower the empirical estimate they can be ignored, and a Chernoff+Union bound gives that all these elements have empirical estimates less than $(\beta/2)2^{-k}$ except with negligible probability. \chris{maybe spell this out a bit too?}

% with probability $2^{\Omega(m-\nu^2)} \ll O(\nu/T_1)$  

On the other hand, consider any element $y$ s.t.\ $p_D(y) \geq 2^{-\ell+2}$. In this case, we need to take collisions into account, but can assume at most $C_{col}$ collisions occur. A Chernoff bound promises that the (no-collision) empirical density of $y$ would be at least $\frac{3}{2^k}$ with probability $1-O(\nu)$. Taking into account the $C_{col}$ collisions the high probability estimate becomes at worst $\frac{\frac{3}{2^k}T_2-C_{col}}{T_2}>\frac{2}{2^k}$ as desired (since for large enough $k$ we have that $C_{col} \leq 2^{-k}T_2$).

% and adding in the potential $O(\log(m/\nu))$ collisions reduces this to at most $2\frac{1}{2^k}$ as desired.\footnote{Where we have used the fact that $\nu$ is inverse polynomial in $m$.}

Finally, we prove that $\hat{q}(y^*)$ is indeed a good empirical estimate, that is
\[ 
\hat{q}(y^*) \in \left(1\pm O\left(\nu\right)\right)p_D(y^*)2^{\ell-k}.
\]
Assuming no collisions, this is true by a Chernoff bound since the density of the pre-image of $y^*$ mapped into the range of $H_2$ is exactly $p_D(y^*)2^{\ell-k}$ by construction. Thus in this case the statement holds except with probability at least $e^{-\nu^22^{-k}T_2} \leq O(\nu/T_1)$, or $1-O(\nu)$ after union bounding over all rounds of $\correlatedsampler$. As in the previous statement, we can again assume that at most $C_{col}$ collisions occur, so it is enough to observe that, for large enough $k$, $\frac{C_{col}}{T_2} \leq O(\nu p_D(y^*)2^{\ell-k})$ \chris{why is this inequality true?}, which has no asymptotic effect on the bound. 
\end{proof}


\iffalse
%By Item 4 of Lemma~\ref{lem:corrsamp-conditions} and the fact that we have conditioned on , we have that $\hat{q_1}(x) \in (1+O(\nu))p_{D_{C_1}}(y)2^{\ell - k}$, and $\hat{q_2}(x) \in (1+O(\nu)) p_{D_{C_2}}(y)2^{\ell - k}$. By item $2$ of Lemma~\ref{lem:corrsamp-conditions}, we have that for all $\ell$, there is at most one $y^*$ such that $p_{D_{C_1}}(y^*) \in [2^{-\ell+2}, 2^{-l-2}]$, and likewise for $D_{C_2}$. Hence, there is at most one $y^*$ such that $p_{D_{C_1}}(y^*) 2^{\ell-k} \in [ 2^{-(k+2)}, 2^{-(k-2)}]$ and likewise for $p_{D_{C_2}}$. This implies that for $\beta \in (1,2)$, there is at most a single unique $y^*$ such that $p_{D_{C_1}}(y^*) 2^{\ell-k} \in [ (\beta/2) 2^{-k}, \beta 2^{-k} ]$ and likewise for $p_{D_{C_2}}$. This implies that with probability at least $1-\nu$ over the choice of $\beta$, there is at most a single unique $y^*$ such that $\hat{q}_1(y^*)  \in [ (\beta/2) 2^{-k}, \beta 2^{-k} ]$ and likewise for $\hat{q}_2$. Since one of the element finder calls returns $\bot$, we have that for one of these calls, there is no such $y^*$ and for the other there is. Without loss of generality, assume it is for the run on $D_{C_1}$. Then, the probability that the coin's value is less than $\hat{q}_1 2^k$ is at most $2^{\ell}(p_{D_{C_1}}(y^*) + O(\nu))$.



%\rexnote{If feeling especially bored, try to actually calculate the constant for bounding the correlating sampling mismatches in terms of the variation distance. }


%We can guarantee that all runs of $\correlatedsampler$ use the same portions of random string $r$ for the same subroutines. This ensures that the same parameters (e.g., $\beta, l, u, v, \coin$) and hash functions $H_1, H_2$ are chosen in each call of $\correlatedsampler$ with correlated randomness $r$. For more details, see Appendix~\ref{apps:randomness-management}.

%Let $E_1$ denote the event that at least one of $\correlatedsampler(C_1, \nu; r)$ and $\correlatedsampler(C_2, \nu; r)$ returns in a single loop iteration. Let $E_2$ denote the event that these return values are different (or one algorithm returns and the other proceeds to the next loop). 
It suffices to show that the probability that $E_1$ and $E_2$ occur in a single loop is small, relative to $E_1$. \maxh{Quantification is a bit unclear to me here. Are you fixing an iteration and analyzing each one separately?}

We begin by assuming that each loop of $\correlatedsampler$ has the following ideal behavior:
For each $x$ in $X_{\ell, \beta}$ \maxh{Not defined?}, with probability exactly
$2^{-(\ell+k)}$, $\elementfinder$ returns the pair $(x, p_D(x)2^{\ell}/2^k)$ and otherwise returns $\perp$.
Let $p_1(x)$ and $p_2(x)$ denote the probability density functions of distribution $D_{C_1}$ and $D_{C_2}$ respectively. \maxh{I suggest adding an additional outline sentence to guide reader here. Something very roughly like: for every $x$, we wish to show the probability $C_1$ returns $x$ but $C_2$ does not is at most $O(|p_1(x)-p_2(x)|)$. If this holds we are done, since BLAH. The analysis now splits into two cases, when... then maybe even use paragraph command to bold/separate the two cases.}


Say $x \in \{0,1\}^n$ is in the same level of 
$X_{C_1, \ell, \beta}$ and 
$X_{C_2 , \ell, \beta}$.
Then, under the ideal conditions,
the probability of returning $x$ in one loop of $\correlatedsampler(C_1, \nu; r)$
but not returning $x$ in the corresponding loop of $\correlatedsampler(C_2, \nu; r)$ is
the probability of returning $x$ times
the probability that the random coin is chosen between $p_2(x)2^{\ell} = q_2 2^k$ and $q_1 2^k=p_1(x)2^{\ell}$. The former probability is 
$p_1(x)/((m+2)2^k)$ and the latter is $|p_1(x)-p_2(x)|2^{\ell}$. Assuming $x$ got returned, $p_1(x) \le
2^{-\ell+1}$, this is at most $|p_1(x)-p_2(x)| \cdot 2/((m+2)2^k)$.  Summing up over
all $x$, this gives a bound of $2\dtv(D_{C_1},D_{C_2})/((m+2)2^k)$  
of the answer returned by the first sampler not being returned by the second
for such $x$.
Conditioned on a non-$\perp$ answer appearing in either run of $\correlatedsampler(C_i, \nu; r)$, this is at most a $O(\dtv(D_{C_1},D_{C_2}))$ probability
of a difference. 

Next, we turn to bounding the expected 
total probability one sampler returns $x$ when $x$ is not in the same levels  $X_{C_1, \ell,
\beta}$ and $X_{C_2,\ell,\beta}$.  
Without loss of generality, say that $p_1(x) = \gamma_1 2^{-\ell}$ with $1 < \gamma_1 \le 2$
and $p_2(x)= \gamma_2 2^{-\ell}$ with $\gamma_2 \le \gamma_1$. 
First, consider the case in which $x$ could possibly land in the same level $\ell$ (i.e., $\gamma_1/2 < \gamma_2 \le \gamma_1$). The probability that $\beta$ is chosen so that $\gamma_2 < \beta \le \gamma_1$ is $(p_1(x) - p_2(x))2^\ell$. Scaling by the probability of choosing $x$, this difference is $O(|p_1(x) - p_2(x)|)$. 
Second, consider the case in which $x$ can never land in the same level $\ell$ (i.e., $2\gamma_2 \le \gamma_1$).
Then the probability that $x$ is chosen by the sampler for $C_1$ is at most $p_1(x) 2^\ell \in |p_1(x)-p_2(x)|2^{\ell}$, since $|p_1(x)-p_2(x)| \in O(p_1(x))$.
By the same argument, $p_2(x) 2^{-\ell'} \in |p_1(x) - p_2(x)| 2^{\ell'}$, where $\ell'$ is defined so that $p_2 \in X_{C_2, \ell', \beta}$. 
In either case, the probability $x$ is chosen and is in separate levels is at most $O(|p_1(x)-p_2(x)|)$.  

Summing over all $x$'s in the support, the total probability of returning different $x$'s in round $t$ of $\correlatedsampler(C_1, \nu; r)$ and $\correlatedsampler(C_2, \nu; r)$ (under the ideal behavior)  is $O(\dtv(D_{C_1},D_{C_2}))$. 

%In each level, the total probability of returning different $x$'s under the ideal behavior
 %is $O(|p_1(x)-p_2(x)|2^{\ell} p_1(x)) = O(|p_1(x)-p_2(x)|)$.
%Thus, the total expected weight of elements in $D_1$ in distinct levels is $O(\dtv(D_{C_1},D_{C_2}))$.
%Since the sampler produces the same distribution as $D_{C_1}$, the probability
%that we output such an element is $O(\dtv(D_{C_1},D_{C_2}))$.  


%\textbf{Correlation analysis:}
%Again, we proceed by removing the assumption that each loop of $\correlatedsampler$ has the ideal behavior. 

In the non-ideal case, two additional sources of error exist.  First, our estimate $\widehat{q}(x)$ returned by $\elementfinder$ could lie in a different layer from $p_{D} (x)$  for one of the distributions.  
By the arguments in the proof of Lemma~\ref{lem:correlated-sampler-round-return-prob}, this occurs with probability at most $O(\nu)$ for a single distribution. Union bounding over both distributions gives an $O(\nu)$ bound on the likelihood of this scenario occurring in a round. 
Secondly, the $\coin$ could be randomly chosen between the ideal and actual values of $q$ for one of the two distributions.  If the estimate $\widehat{q}$ is good in the sense of
Lemma~\ref{lem:correlated-sampler-round-return-prob}, which happens with high probability by the same Lemma, there is a 
$O(\nu)$ probability the coin could fall between $\widehat{q}(x)$ and $p_D(x)$, creating an additional $O(\nu)$ chance of that the returned elements across both  runs are not correlated.
%$O\left( \frac{(1 + O(\nu))2^{\ell - k}p_D(x)}{\beta 2^{-k-1}}\right) = O(1 \pm \nu)2^{\ell}p_D(x))$
\fi 



\textbf{Distributional accuracy analysis intuition:}
%Fix $x \in \X_{C, \ell, \beta}$.  For a random $(H_1, u)$, there is a probability $2^{-(\ell +k)}$ that $u=H_1(x)$.  There are $p_D(x)2^m$ random strings $r$  so that $C(r)=x$, and for a random $(H_2, v)$, each of these satisfy $H_2(r)=v$ with probability $2^{-(m-\ell +k)}$.   
%Thus, $\widehat{q}_x$  is expected to be close to $p_D(x)2^m 2^{-(m -\ell +k)}= p_D(x)2^{\ell}/2^k $ if $H_1^{-1} (u)$ is uniquely $x$ within $X_{C, \ell, \beta}$. 
%\chris{think this sentence is a bit confusing too, something like "$x$ is the unique element in $X_{C, \ell, \beta}$ such that $H_1(x) = u$", or otherwise "$H_1$ has no collision on $u$ in $X_{C, \ell, \beta}$"}


For intuition, assume  that each loop of $\correlatedsampler$ had this ideal behavior:  
For each $x$ in $X_{\ell, \beta}$, with probability exactly
$2^{-(\ell+k)}$, $\elementfinder$ returns the pair $(x, p_D(x)2^{\ell}/2^k)$ and otherwise returns $\perp$. This would occur if the inverter never failed and the hash functions never had any problematic collisions. 
Later, we will show that the error terms avoided by this assumption are small. 

 
By definition, any $x$ in the support of $D_C$ is in $X_{C, \ell, \beta}$ for exactly one $\ell$.
In one loop of $\correlatedsampler$  we will pick this $\ell$ with probability $1/(m+2)$. 
By the above assumption, $\correlatedsampler$ would return $(x,p_D(x)2^\ell/2^k)$ with probability $2^{-(\ell +k)}$.  In the rejection sampling step, we return $x$ with probability $(p_D(x)2^l/2^k)2^k= p_D(x)2^{\ell}$.  
Combining, the overall probability of returning $x$ in
one run is 
$$ \frac{1}{m+2} 
\left( 2^{-(\ell+k)} \right) 
p_D(x)2^{\ell}
= \frac{p_D(x)}{(m+2)2^k}\text{.}
$$  
Thus, the probabilities of each outcome are proportional to the probability under $D_C$, so the conditional distribution is exactly that for $D_C$.   
Also, the overall probability of terminating in any single loop of $\correlatedsampler$ is  
$\sum_x p_D(x) /((m+2)2^k)= 1/((m+2)2^k)$, so we expect to $\correlatedsampler$ to loop $O(m2^k)$ times before returning and terminating.

\medskip

By running the inverter on random hash functions and random strings, the algorithm can sample $x$ proportional to the circuit distribution $D_C$ with high probability. 
\maxh{What are $x$ and $q_x$? How do these relate to the rest of the paragraph? At this point the reader won't have parsed the algorithm box, so this is confusing.}

First, $\correlatedsampler$ fixes a slack parameter $k \in \Theta( \log (m)+ \log (1/\nu))$ that determines the dimensions of the hash functions. \maxh{Intuition for this choice?} In each loop, it 
picks $\beta \in [1, 2]$ uniformly and divides the support into roughly $m$ levels 
\[
X_{C, \ell, \beta} = \{x\in\{0,1\}^n~|~\beta 2^{- \ell-1} < p_D(x) \le \beta 2^{-\ell}\},
\] 
where $p_D(x)$ is the probability that $x$ is output by circuit $C$. \maxh{We already explained the levels idea, but explain choice of $\beta$ here?}
% (\rexnote{Later, check that boundary conditions when $\ell=0$ are fine})
Algorithm $\correlatedsampler$ then samples a level $\ell$ uniformly and attempts to output an $x$ in $X_{C, \ell, \beta}$ proportionally to $p_D(x)$.  
The inverter $\Ical$ is used to simultaneously  determine $x \in X_{C, \ell, \beta}$  and estimate $p_D(x)$.    

Consider the circuit $\Fcircuit (r') = H_1(C(r')) \mathbin\Vert H_2(r')$, where $H_1$ is a uniformly chosen pairwise independent hash function mapping $n$ bits to $\ell + k$
bits and $H_2$ is a uniformly chosen pairwise independent  hash function mapping $m$ bits to $m- \ell +k$ bits. $F$ maps strings of length $m$ to strings of length $m+2k$. 
We run inverter $\Ical$ on random instances by randomly choosing hash functions $H_1$ and $H_2$ and randomly choosing a concatenated string $u \mathbin\Vert v$ of length $m+2k$ to invert.
If $\Ical$ does not find any valid preimage of the function $F$,
% usually because no preimage exists, 
$\Ical$ returns $\perp$. 
%\rexnote{Should we just say, we can check if the returned result by $\Ical$ is a correct inversion, by running the circuit $C$ and hash functions? Or is that too much time complexity?}

Consider the pair $(C(r),r)$. Intuitively, there should be a low probability that, for random hash functions $H_1$ and $H_2$, there exists another pair $(C(r'), r')$ such that $C(r) \ne C(r')$ but $H_1(C(r)) = H_1(C(r'))$ and $H_2(r) = H_2(r')$. Thus, for a fixed $(H_1, u)$ pair, running the inverter $\Ical$ on $F$ generated by random pairs $(H_2, v)$ should give a good estimate of the relative probability of $H_1^{-1}(u)$ in distribution $D_C$ (with high probability). \maxh{I find this a bit hard to parse standalone, diagram reference here may be helpful, or maybe refering back to informal version? Not sure}

For a fixed $H_1$ and $u$, subroutines $\elementfinderfull$ and $\hashcheckerfull$ make multiple calls to the inverter $\Ical$. 
$\elementfinder$ picks  
$\Theta(mk\nu^{-2}2^{2k} \log (1/\nu))$ pairs
of hash functions $H^i_2$ and strings $v^i$. \maxh{Having exact number here I think just clutters the paragraph and doesn't help intuition (for me at least), maybe just ``many pairs'', or ``enough to give high prob estimate''?}
For each pair, it runs $\hashchecker_{C, \nu, \ell, H_1, u} ( H_2^i, v^i)$, checking if there is a valid inversion of the induced circuit $F$. 
Then, $\elementfinder$ sees if there is a unique $x \neq \perp$ such that $x$ appears in a $((\beta/2) 2^{-l}, \beta 2^{-l})$-fraction of times in the list.  If so, it returns the pair $(x,q_x)$ where $q_x$ is the fraction of times that $x$ was returned by $\sc{A}$. Otherwise, $\elementfinder$ returns $\perp$.

\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{$\correlatedsamplerfull$\\
			Input: Circuit $C: \{0,1\}^m\rightarrow \{0,1\}^n$, distributional error parameter $\nu$, and random string $r$\\
			Output: An element $x \in \{0,1\}^n$
		}
		\label{alg:correlatedsampler}
		\begin{algorithmic}
		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$
            \STATE $T_1 \gets \Theta(mk2^{k} \log (1/\nu))$
            \FOR{$t=1$ to $t = T_1$}
    
            
		        \STATE $\beta \gets_r [1,2]$
    
    
                \STATE $\ell \gets_r \{0,1,\dots, m \}$
    		       
    		    \STATE $H_1 \gets_r $ pairwise independent hash function from $n$ bits to $\ell+k$ bits 
    		        
    		    \STATE $u \gets_r \{0,1\}^{\ell+k}$
                
%                \STATE $(x,q) \gets \elementfinder_{C, \nu, \ell, \beta} (H_1, u; r)$
               \STATE $x \gets \elementfinder_{C, \nu, \ell, \beta} (H_1, u; r)$
                
%                \IF{$(x,q) \ne \perp$}
                \IF{$x\neq \bot$}             
%                    \STATE $\coin \gets_r [0,1]$
%                    \IF{$\coin \le q 2^k$}
                        \RETURN $x$
                    %\ENDIF
                \ENDIF
            \ENDFOR
            \RETURN $\perp$
		\end{algorithmic}
	\end{algorithm}
\end{figure}


\paragraph{Case 1: }To bound the probability of event~\ref{e2}, we recall that Lemma~\ref{lem:corrsamp-conditions} bounds the estimation error $\widehat{q}(x)$ for the element $x$ output by $\elementfinder$ by $\widehat{q}(x) \in (1+O(\nu))p_D(x)2^{\ell - k}$, conditioned on $\elementfinder \neq \bot$. Then, because we have assumed that $\correlatedsampler(C_2, \nu; r)$ is the run that returns this round, and therefore $\widehat{q}_1(x) < \widehat{q}_2(x)$, we have that
\begin{align*}
\Pr[\widehat{q}_1(x) 2^k \leq \text{ coin } < \widehat{q}_2(x) 2^k]
&\leq \Pr[(1-O(\nu))p_1(x)2^{\ell} \leq \coin \leq  (1+O(\nu))p_2(x)2^{\ell}] \\
&\leq 2^{\ell}(p_2(x) - p_1(x) + O(\nu)).
\end{align*}
 Summing over all $x \in \supp(D_{C_1}) \cup \supp(D_{C_2})$, accounting for the probability of $x$ being selected by choice of $H_1$ and $u$, we have that the probability of event~\ref{e2} is
 $$ \frac{\sum_{x} 2^{\ell}(p_2(x) - p_1(x) + O(\nu))}{2^{\ell + k}} \leq d_{TV}(D_{C_1}, D_{C_2}) + O(\nu). $$

\maxh{HIGH LEVEL COMMENT: I find the exposition here hard to follow, even with the informal description in mind. I'd maybe split the description into outer and inner loops more explicitly and match the descriptions to the informal overview. Maybe also increase the focus on explaining new parts not in the  informal overview (e.g. random choice of $\beta$), and remove/shorten what we already explained. Referencing back to diagrams could help here too. It's hard to keep all the parameters in mind without a picture I think. }

\maxh{HIGH LEVEL COMMENT: Maybe it's also good to keep in mind while reading over this: what is the purpose of the exposition in this section? To me it should be to explain intuition/reasoning behind more detailed choices in the algorithm boxes than explained in the informal overview, but not too much detail that the exposition is hard to follow. For instance, algorithm box says we take "X" many of something, exposition could explain roughly why this choice is made, to prime the reader for seeing where it will appear in the eventual proof. E.g. for number of $(H_2,v_2)$ pairs, the particular number itself is not so important, what's important is that we are taking enough to get a good probability estimate.}