
\subsection{Replicability implies Approximate-DP}
In \cite{GhaziKM21}, the authors show a sample-efficient reduction from differentially private PAC learning to replicable PAC learning. In this section, we show their technique generalizes to arbitrary statistical problems.\footnote{The argument remains similar to \cite{GhaziKM21}, but requires a few changes to avoid union bounding over failure probability which can be costly in settings beyond PAC learning.}

Recall the definition of a statistical task in Definition~\ref{def:stat-task}. We will show that any statistical task with a ``good'' replicable learner can also be solved privately, without substantial blowup in runtime or sample complexity. 
\begin{theorem}[Replicability $\to$ DP]\label{thm:Rep-to-DP}
Let $\mathcal{T}$ be a statistical problem. 
For all $\beta>0$, if there is a $0.01$-replicable algorithm solving $\mathcal{T}$ using $n_R$ samples and with failure probability $\beta$, then for any $0 < \varepsilon,\delta \leq 1$ there is an $(\varepsilon,\delta)$-DP algorithm for $\mathcal{T}$ using $n_{DP}$ samples with failure probability  $O \left(\beta \log \frac{1}{\beta} \right)$, where
\begin{align*}
n_{DP}(\epsilon,\delta,\beta) \leq n_R \cdot O\left( \frac{\log\delta^{-1}\log\beta^{-1}}{\varepsilon}+\log^2\beta^{-1}\right)
\end{align*}

\end{theorem}

This conversion relies on the following private algorithm for selecting an approximate mode.

\begin{theorem}[DP Selection \cite{korolova2009releasing,bun2016simultaneous, BunDRS18}]
There exists some $c>0$ such that for every $\varepsilon,\delta>0$ and $m \in \mathbb{N}$, there is an ($\varepsilon,\delta$)-DP algorithm that on input $S \in \mathcal{X}^m$, outputs with probability $1$ an element $x \in X$ that occurs in $S$ at most $\frac{c\log \delta^{-1}}{\varepsilon}$ fewer times than the true mode of $S$. Moreover, the algorithm runs in $\text{poly}(m,\log(|\mathcal{X}|))$ time.
\end{theorem}

The idea, as in \cite{GhaziKM21}, is to use replicablity to construct a sample over the output space where some correct solution appears many times. In particular, given a replicable algorithm $\mathcal{A}$ on $n$ samples, consider the following simple procedure adapted from \cite{GhaziKM21}: partition a larger data set, run $\mathcal{A}$ on each part, and privately output a commonly repeated element.

\begin{algorithm}[H]
\KwResult{Privately ouputs solution to $(\mathcal{X},R)$}
\nonl \textbf{Input:} Statistical Problem $(\mathcal{X},R)$. Distribution $D$ over $\mathcal{X}$, Replicable algorithm $\mathcal{A}$ on $n$ samples\\
\nonl \textbf{Parameters:} 
\begin{itemize}
    \item Privacy and Correctness $\beta,\eps,\delta>0$
    \item Seed Number $k_1 = O(\log\beta^{-1})$
    \item Partition Number $k_2 = O\left(\frac{\log\delta^{-1}}{\varepsilon}+\log\beta^{-1}\right)\cdot k_1$
\end{itemize}
\nonl \textbf{Algorithm:}\\

 \begin{enumerate}
\item For every $j \in [k_1]$ and $i \in [\frac{k_2}{k_1}]$ sample $S_{i,j} \sim {\cal X}^n$\label{stp:draws}
\item Sample $k_1$ random strings $\{r_j\}$. 
\item Let $y_{i,j} = \mathcal{A}(S_{i,j}; r_{j})$. \label{stp:outputs}
\item Run ($\varepsilon,\delta$)-DP Selection on $\{y_{i,j}\}$ and denote the output by $y^*$.
\end{enumerate} 
\textbf{return} $y^*$
 \caption{DP-to-Replicability Reduction}
\label{alg:Rep-to-DP}
\end{algorithm}

% We argue that as long as $k_1,k_2$ are taken sufficiently large, it is very likely the common element output by the algorithm is a `canonical' good output of the reproducible algorithm, and therefore also a correct solution. 

Recalling the two-parameter definition of replicability (\ref{2param-defn} and \ref{claim:2parrep}), since our subroutine is $0.01$-replicable and $\beta$-correct, it is 
also $(0.1, 0.1)$-replicable and $\beta$ correct.
Therefore, the proof of \Cref{thm:Rep-to-DP} is an immediate consequence of the following proposition.
\begin{proposition}\label{prop:Rep-to-DP}
For all sufficiently small $\beta,\varepsilon,\delta >0$, if $\mathcal{A}$ is $(0.1, 0.1)$-replicable and has failure probability $\beta$, then \Cref{alg:Rep-to-DP} is $(\varepsilon,\delta)$-private and has failure probability $O(\beta \log 1/\beta)$.
\end{proposition}
\begin{proof}
Privacy is essentially immediate from DP Selection. This follows because the input to selection based on a neighboring input database $T'$ differs in at most one of the $\{y_i\}$ (as we've partitioned the sample disjointly). Thus the reduction automatically inherits $(\varepsilon,\delta)$-privacy from DP Selection.
The main interest in the reduction, then, is maintaining correctness which we argue next. The proof breaks into two parts:
\begin{enumerate}
    \item With probability $1-\beta/2$, some $y^* \in \{y_i\}$ appears at least $t_1 \coloneqq 2c\left(\frac{\log \delta^{-1}}{\varepsilon}+\log\beta^{-1}\right)$ times.
    \item With probability $1-\beta \log 1/\beta$, \textit{any} element appearing at least $t_2 \coloneqq c\left(\frac{\log \delta^{-1}}{\varepsilon}+\log\beta^{-1}\right)$ times is correct.
\end{enumerate}
The result then follows from observing that by a union bound both conditions hold with probability at least $1-O(\beta \log 1/\beta)$, and conditioned on this fact DP-Selection always outputs an element that occurs at least $t_1-c\frac{\log\delta^{-1}}{\varepsilon} \geq t_2$ times (which is then guaranteed to be correct).

It remains to prove the claims. For the first, note that since $\mathcal{A}$ is $(0.1,0.1)$-replicable, there exists some $.1$-good random string $r^* \in \{r_j\}$ with probability at least $1-\beta/4$. By a Chernoff bound, the probability that the canonical element corresponding to $r^*$ appears fewer than $t_1$ times is at most $\beta/4$, which proves the claim (for a large enough choice of $k_2$).

Finally, we argue any common element is correct. Since $\mathcal{A}$ is a $\beta$-correct algorithm, in expectation, the number of incorrect outputs is $\beta k_2$. Hence, by Markov's inequality, the probability that there are more than $b=O(\frac{k_2}{\log\beta^{-1}})$ incorrect outputs is at most $\beta \log 1/\beta$. For small enough choice of constant in the correctness of our replicable algorithm,\footnote{Note this choice can be taken universally with respect to all parameters and the statistical problem itself.} we can make $b<t_2$, so no element appearing at least $t_2$ times can be incorrect as desired.
%Recalling the two-parameter definition of reproducibility (\ref{2param-defn} and \ref{claim:2parrep}), since our subroutine is $\rho$-reproducible and $\beta/5$-correct, it is 
%also $(\beta/4, .1)$-reproducible and $\beta/5$ correct.
%Recall that our subroutine $\mathcal{A}$ is $(\beta/4,.01)$-reproducible, $\beta/4$-correct over any distribution $D$ on $\mathcal{X}$. 
% Call a random string $r$ `good' for $\mathcal{A}$ if 
% \begin{enumerate}
%     \item $r$ has a canonical element:
%     \[
%     \exists y^* \in \mathcal{Y}: \Pr_S[A(S)=y^*] \geq 0.9
%     \]
%     \item The canonical element is `correct:'
%     \[
%     R(D,y^*)=1
%     \]
% \end{enumerate}
% The proof now breaks into two claims. First, conditioning on `good' $r$, the algorithm outputs a correct solution with probability at least $1-\beta/2$. Second, a random $r$ is good with probability at least $1-\beta/2$. A union bound then completes the argument.

% We start with the former. Recall that DP selection promises the output is within $O(\frac{\log\delta^{-1}}{\varepsilon})$ of the maximally occurring element in $\{y_i\}$. If $r$ is good, we just need to show that the canonical element $y^*$ appears at least this many times more than any other element. Since each partitioned sub-sample is independent, the worst-case scenario is that some $\bar{y} \neq y^*$ occurs with probability $0.1$. For the appropriate choice of constants, a Chernoff bound then promises that $y^*$ occurs at least $\Omega\left(\frac{\log\delta^{-1}}{\varepsilon}\right)$ times more than $\bar{y}$ (additively) with probability at least $1-\beta/2$ as desired.

% It is left to prove that $r$ is good with probability at least $1-\beta/2$.
% %which is a simple observation appearing in \cite{impagliazzo2022reproducibility}. We include the proof for completeness. 
% First, being $(\beta/4,.1)$-reproducibile exactly promises that a $1-\beta/4$ fraction of random strings $r$ satisfy the first condition. If at least a $1-\beta/4$ fraction of these strings also satisfy the second (correctness), we are done. If not, there is a $\beta/4(1-\beta/4)=\beta/4-\beta^2/16$ chance that a string is drawn with an incorrect canonical element. This induces a failure probability of at least $.9\beta/4-\beta^2/16 \geq \beta/5$ for the overall algorithm (for small enough constant $\beta$), which contradicts the correctness of $\mathcal{A}$.
\end{proof}