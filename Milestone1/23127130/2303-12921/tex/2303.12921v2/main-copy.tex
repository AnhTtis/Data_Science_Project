\documentclass[11pt]{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
\usepackage{amsmath,amssymb,amsthm,fullpage,mathrsfs,pgf,tikz,caption,subcaption,mathtools,mathabx}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{natbib}
\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{bm}
\hypersetup{colorlinks=true,citecolor=blue,linkcolor=red}
\usepackage{algorithmic}
\usepackage{todonotes}
\usepackage{thmtools}
\usepackage{float}

%%%packages from other overleaf
\usepackage[utf8]{inputenc}
%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\usepackage{algorithmicx}
\usepackage{comment}
\usepackage{xspace}
\usepackage{bbm}
\usepackage{ulem} \normalem
%\usepackage{paralist}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    linktocpage=true,
}




\input{macro}
\let\oldnl\nl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}
\author{%
  Mark Bun \and Marco Gaboardi \and
  Max Hopkins\thanks{Department of Computer Science and Engineering, UCSD, CA 92092. Email: \texttt{nmhopkin@eng.ucsd.edu}. Supported by NSF Award DGE-1650112.}\and Russell Impagliazzo \and Rex Lei \\ \and Toniann Pitassi
  \and Satchit Sivakumar \and Jessica Sorrell
}
\title{Connections and Separations Between Reproducibility and Privacy }

\begin{document}
\maketitle
\begin{abstract}

The notion of replicable algorithms was introduced in~\cite{ImpLPS22} to describe randomized algorithms that are stable under resampling of their inputs. More precisely, a replicable algorithm gives the same output with high probability when its randomness is fixed and a new i.i.d. sample is drawn from the same distribution. Using replicable algorithms for data analysis can facilitate the verification of published results, by ensuring that the results of an analysis will be the same with high probability, even when that analysis is performed on a new data set.

In this work, we prove connections and separations between replicability and other established notions of stability -- differential privacy and perfect generalization. 
We show equivalences between these three stability notions for a broad class of statistical problems by giving sample-efficient transformations from approximate differential privacy to perfect generalization, perfect generalization to replicability, and replicability to approximate differential privacy. All but the transformation from perfect generalization to replicability are computationally efficient as well. 

Under reasonable cryptographic assumptions, we prove that the inefficiency of this transformation is unavoidable. We show a computational separation between privacy and replicability by demonstrating a problem that can be solved efficiently by a differentially private algorithm, but for which a private algorithm cannot be efficiently transformed into a replicable algorithm. 

The equivalence between replicability and privacy allows us to extend and quantitatively improve privacy results previously only known to hold in the PAC learning setting. We extend the results of Ghazi, Kumar, and Manurangsi~\cite{GhaziKM21}, which showed how to construct user-level private PAC learners from item-level private PAC learners, by way of replicability. By showing privacy implies replicability for statistical problems outside of the PAC learning framework, we show that user-level privacy reduces to item-level privacy for this more general class of tasks. Connections between replicability and privacy also give a privacy amplification procedure for the $\delta$ parameter of an $(\epsilon, \delta)$-differentially private algorithm. By converting a differentially private algorithm to a replicable one, applying the replicability amplifying procedure given in ~\cite{ImpLPS22}, and converting back to differential privacy, we obtain the first constructive amplification theorem for approximate differential privacy. As a final application of these connections, we answer an open question of Hopkins, Kane, Lovett, and Mahajan \cite{hopkins2021realizable} by extending their results to the differentially private setting. They give a model-independent reduction from agnostic to realizable PAC learning, which roughly preserves sample-efficiency in settings where VC-dimension may give very poor sample complexity bounds (e.g. learning over structured distributions). We give a similar reduction for replicable PAC learners, and the extension to differential privacy follows from the transformation from replicability.

\end{abstract}
\tableofcontents
\newpage

\section{Introduction}

Algorithmic stability has emerged as a central concept for ensuring both the utility and safety of modern machine learning and data analysis. Informally, an algorithm is stable if its output is insensitive to small changes to its input. Precise formulations of algorithmic stability underlie the key definitions in a number of areas:

\paragraph{Differential privacy.} A randomized algorithm is differentially private~\cite{DworkMNS06} if changing a single input record results in a small change in the distribution of the algorithm's output. When each input record corresponds to one individual's datum, differential privacy guarantees that nothing specific to any individual can be learned from the output of the algorithm. Differentially private data analysis is a mature research area with a rich algorithmic toolkit and understanding of the feasibility of fundamental statistical tasks in query estimation, classification, regression, distribution estimation, hypothesis testing, and more.

\paragraph{Generalization in adaptive data analysis.} Generalization is the ability of a learning algorithm to reflect properties of a population, rather than just properties of a specific sample drawn from that population. Techniques for provably ensuring generalization form a hallmark of theoretical machine learning. However, generalization is particularly difficult to guarantee in settings where multiple analyses are performed adaptively on the same sample. Traditional notions of generalization do not hold up to downstream misinterpretation of results. For example, a classifier that encodes detailed information about its training sample in its lower order bits may generalize well, but can be used to construct a different classifier that behaves very differently on the sample than it does on the population. Interactive processes such as exploratory data analysis or feature selection followed by classification / regression ruin the independence between the training sample and the method used to analyze it, invalidating standard generalization arguments.

Imposing stability conditions on learning algorithms leads to general-purpose solutions to these problems. A variety of such stability conditions have been studied \mb{cite a bunch of things}, each offering distinct advantages in terms of the breadth of their applicability and the quantitative parameters achievable. Two specific notions play a central role in this work. The first is \emph{perfect generalization}~\cite{CummingsLNRW16}, which ensures that whatever can be inferred from the output of a learning algorithm when run on a sample $S$ could have been learned just from the underlying population itself. The second is \emph{bounded max-information}~\cite{DworkFHPRR15} which constrains the amount of information revealed to an analyst about the training sample.

\paragraph{Reproducibility.}  Once a scientific conclusion is obtained and disseminated, it is crucial for other researchers to be able to replicate the results. The failure of adaptive learning procedures to generalize has been cited as one reason for the present ``crisis of reproducibility'' in the empirical sciences, but a host of other practices also bear the blame, such as the selective reporting of only the findings that appear most statistically significant. Impagliazzo, Lei, Pitassi, and Sorrell~\cite{ImpLPS21} recently put forth a new stability definition to capture the reproducibility of learning algorithms in and of itself. Roughly speaking, an algorithm is reproducible if with high probability over the choice of two independent samples $S, S'$ from the same distribution, it produces exactly the same output. Despite being an extremely strong form of stability, reproducibility can be achieved for many learning tasks with only a modest degradation in efficiency.

\medskip

Each stability definition described above is tailored to model a distinct desideratum. At first glance, they may all appear technically incomparable. For instance, differential privacy is stricter than the other definitions in that it holds in the worst-case over all input datasets without any assumptions on the data generating procedure. On the other hand, it is weaker in that it only requires insensitivity to changing one input record, rather than to resampling the entire input dataset as in perfect generalization and reproducibility. Meanwhile, differential privacy and perfect generalization quantify the sensitivity of the algorithm's output in a weaker way than reproducibility; the former two notions only require that the distributions on outputs are similar, whereas reproducibility demands that precisely the same output realization is obtained with high probability.

Nevertheless, the (surprising!) technical connections between these definitions have enabled substantial progress on the fundamental questions in their respective areas. For example, it was exactly the adaptive generalization guarantees of differential privacy that kickstarted the framework of adaptive data analysis from~\cite{DworkFHPRR14}; the definition of max-information was subsequently introduced~\cite{DworkFHPRR15} to unify existing analyses based on differential privacy and description length bounds. As another illustration, variants of reproducibility were introduced in~\cite{BunLM20, GhaziGKM21, GhaziKM21} for purely technical reasons, as it was observed that such algorithms could be used to construct differentially private ones. This connection was essential in proving the characterization of private PAC learnability in terms of the Littlestone dimension from online learning.

In this work, we further clarify the relationship between these disparate notions of algorithmic stability. For an extremely general class of statistical tasks, we show that the achievability of all of these definitions is equivalent. In particular, we show that any learning algorithm achieving one of $\{$differential privacy, perfect generalization, bounded max-information w.r.t. product distributions, reproducibility$\}$ can be converted to an algorithm satisfying any other with at most polynomial overhead in the number of samples required. We also give statistical and computational lower bounds illustrating the tightness of these connections. A more detailed discussion of our results follows.



\subsection{Equivalences between Stability Notions}

Our results apply to an abstract class of \emph{statistical tasks} that capture learning from i.i.d. samples from a population. An instance of such a task is a distribution $D$ from a pre-specified family of distributions. Given i.i.d. samples from $D$, the goal of a learning algorithm is to produce an outcome that is ``good'' for $D$ with high probability. See Definition~\ref{def:stat-task} for a precise definition. This formulation of a statistical task captures problems such as PAC learning, where a sample from $D$ is a pair $(x, f(x)) \in \mathcal{X} \times \{0, 1\}$ where $x$ is drawn from an arbitrary marginal distribution over $\mathcal{X}$, and $f$ is an arbitrary function from a fixed concept class. A ``good'' outcome for such a distribution $D$ is a hypothesis $h: \mathcal{X} \to \{0, 1\}$ that well-approximates $f$ on $D$. Regression, distribution parameter estimation, distribution learning, hypothesis testing, confidence interval construction, and much more can be naturally framed as statistical tasks.

The following diagram illustrates the known equivalences between the various stability notions we consider that hold for general statistical tasks.

\begin{figure} \label{fig:transformations}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=6cm,
		scale = 1,transform shape, rct/.style = {draw}]
		
		\node[rct] (rep) {$0.1$-reproducibility};
		\node[rct] (dp) [right of=rep] {$(0.1, 1/n^2)$-differential privacy};
		\node[rct] (strong-dp)  [below right of=dp]  {$(0.1/\sqrt{n}, 1/n^2)$-differential privacy};
		\node[rct, align=center] (bmi)  [below left of=strong-dp]  {$(0.1, 1/n^2)$-approximate max-info \\ w.r.t. product distributions};
		\node[rct] (pg)  [left of=bmi]  {$(0.1, 1/n^2)$-perfect generalization};
		
		\path 
		(rep) edge[dashed]   node {$n \mapsto n\log n$} (dp)
		(dp) edge[dashed]              node {$n \mapsto n^2$} (strong-dp)
		(strong-dp) edge              node {} (bmi)
		(bmi) edge              node {} (pg)
		(pg) edge[bend left=20, dotted]              node {$n \mapsto n\log n$} (rep)
		(rep) edge[bend left=20, dashed]              node {$n \mapsto n\log n$} (pg)
		(pg) edge[dashed, swap] node {$n \mapsto n \log n$} (dp);
		
	\end{tikzpicture}
	\caption{A solid arrow from $A$ to $B$ means that every algorithm satisfying $A$ also satisfies $B$. A dashed arrow means that for every statistical task, an solution satisfying $A$ can be computationally efficiently transformed into a solution satisfying $B$ with the stated blowup in sample complexity. A dotted arrow means such a transformation exists, but is not computationally efficient. \mb{Give pointers to theorem statements. Maybe use colors?}}
\end{figure}


\paragraph{Differential privacy implies bounded max-information.} A randomized algorithm $A : \mathcal{X}^n \to \mathcal{Y}$ is $(\eps, \delta)$-differentially private if for every pair of datasets $S, S' \in \mathcal{X}^n$ differing in a single index and every set of outcomes $Y \subseteq \mathcal{Y}$ we have $\Pr[A(S) \in Y] \le e^\eps \Pr[A(S') \in Y] + \delta$. When $\eps$ is a small constant, say $0.1$, and $\delta = o(1/n)$, this gives a strong guarantee of individual-level privacy for statistical data analysis. Moreover, it has been shown~\cite{DworkFHPRR14, DworkFHPRR15, BassilyNSSSU16} to ensure robust generalization guarantees. In an effort to unify differential privacy with other necessary conditions for robust generalization, \cite{DworkFHPRR15} identified a quantity called \emph{max-information} which limits the probability of bad events that result from a dependence of $A(S)$ on $S$. An algorithm $A : \mathcal{X}^n \to \mathcal{Y}$ has $(\rho, \beta)$-bounded approximate max-information with respect to product distributions (abbreviated $(\rho, \beta)$-BAMIPD)\footnote{Strictly speaking, \cite{DworkFHPRR15} gave a one-sided definition of max-information that only requires the inequality on the right. The two-sided definition we use here is more convenient for studying the implications of bounded max-information.} \mb{Terrible, terrible acronym...} if for every set of outcomes $\mathcal{O} \subseteq (\mathcal{X}^n \times \mathcal{Y})$ we have
\[e^{-\rho} (\Pr[(A(S), S') \in \mathcal{O}] - \beta) \le \Pr[(A(S), S) \in \mathcal{O}] \le e^{\rho} \Pr[(A(S), S') \in \mathcal{O}] + \beta,\]
where $S$ and $S'$ are independent samples from any product distribution over $\mathcal{X}^n$. As with differential privacy, max-information (with respect to arbitrary, not just product distributions) is robust to post-processing and degrades gracefully under adaptive composition \sstext{This is not true, right?}. \mb{Max-info w.r.t. to arbitrary distributions does...still trying to think of how to motivate the def for product distributions}%\cite{DworkFHPRR15} showed that \emph{pure} $(\eps, 0)$-differentially private algorithms are $(\rho, \beta)$-BAMIPD for every positive $\beta$, with $\rho = O(\eps^2 n + \eps\sqrt{n\log(1/\beta)})$. This was extended to $(\eps, \delta)$-differentially private algorithms by \cite{RogersRST16}.
\cite{RogersRST16} showed that if $A$ is $(\eps, \delta)$-differentially private, then it is also $(\rho, \beta)$-BAMIPD for $\rho = O(\eps^2 n + n\sqrt{\delta/\eps})$ and $\beta = O(n\sqrt{\delta/\eps})$. Note that the closeness parameters $(\rho, \beta)$ are inflated by a factor of $n$ relative to the privacy parameters $(\eps, \delta)$; this is what one should expect, as bounded max-information constrains the information revealed about an entire sample of size $n$, rather than a single individual in that dataset. To make these parameters ``small'', i.e., $\rho = O(1), \beta = o(1/n)$, \mb{Say something about why we want these to be small?} it would suffice to take $\eps = O(1/\sqrt{n})$ and $\delta = o(1/n^6)$ \mb{Check parameters} in the differentially private algorithm we began with. 

Fortunately, it is always possible to convert differentially private algorithms with weak parameters into ones with excellent parameters via random subsampling~\cite{kasiviswanathan2011can}. Namely, if $A : \mathcal{X}^n \to \mathcal{Y}$ is $(\eps, \delta)$-differentially private, then the algorithm $A' : \mathcal{X}^m \to \mathcal{Y}$ that takes a random subsample of $n$ entries from its input dataset and applies $A$ to the result is roughly $(\eps \cdot m/n, \delta \cdot m /n)$-differentially private. Putting everything together, one can efficiently convert a $(0.1, 1/n^2)$-differentially private algorithm into a $(0.1, 1/n^2)$-BAMIPD algorithm with a quadratic blowup in its sample requirement.


%\begin{itemize}
%    \item Introduce max-info
%    \item Informally state RRST conversion from ADP to max-info
%    \item Describe how to amplify DP parameters to get acceptable max-info parameters
%\end{itemize}

\paragraph{Bounded max-information implies perfect generalization.} In a similar effort to understand generalization in the face of adaptive data analysis, \cite{CummingsLNRW16} and \cite{BassilyF16} studied the following notion of \emph{perfect generalization}. An algorithm $A : \mathcal{X}^n \to \mathcal{Y}$ is $(\beta, \eps, \delta)$-perfectly generalizing if, for every product distribution $D$ over $\mathcal{X}^n$, there exists a distribution $Sim_D$ such that $A(S) \approx_{\eps, \delta} Sim_D$ with probability at least $1-\beta$ over the choice of $S \sim D$. \mb{Give simulator definition? Or two-sample definition?} That is, whatever can be learned from the output of $A(S)$, for random $S$, could have been learned given access to the distribution $D$ itself. Thus, perfect generalization gives a strong guarantee of robustness of a learning algorithm to postprocessing.

In Lemma~\ref{lem:max-info-PG}, we show that bounded max-information implies perfect generalization with similar parameters. Namely, if an algorithm $A$ is $(\rho, \beta)$-BAMIPD, then it is also $(\sqrt{\beta}, 2\rho, \sqrt{\beta})$-perfectly generalizing. The idea is to take the simulator distribution $Sim_D$ to be the distribution of $A(S')$, where the randomness is taken over both the coin tosses of $A$ \emph{and} the randomness of a sample $S' \sim D$. A similar argument is implicit in~\cite[Lemma 4.5]{BassilyF16}.

%\begin{itemize}
%    \item Describe PG
%    \item One-liner about the proof?
%\end{itemize}

\paragraph{Perfect generalization is equivalent to replicability.} \cite{ImpLPS22} initiated a formal study of replicability for learning algorithms by defining a randomized algorithm $A : \mathcal{X}^n \to \mathcal{Y}$ to be $\rho$-replicable if, for every product distribution $D$ on $\mathcal{X}^n$, we have $\Pr[A(S; r) = A(S'; r)] \ge 1-\rho$, where $S, S'$ are independent samples from $D$ and $r$ represents the random choices of $A$. That is, upon fixing the coins of $A$, it is likely to produce exactly the same output when run on two independent samples from any given population. Replicability appears to be a dramatic strengthening of perfect generalization, which only requires the distributions of $A(S)$ and $A(S')$ to be statistically close. Nevertheless, we argue that perfectly generalizing algorithms can always be converted to replicable ones whenever the output space $\mathcal{Y}$ is finite. This can be done via a primitive called \emph{correlated sampling}. A correlated sampling algorithm for a class of distributions $\mathcal{P} = \{P\}$ is a procedure $CS_P(r)$ such that 1) $CS_P(r)$ produces a sample distributed according to $P$ when provided a uniformly random input $r$, and 2) Whenever $P, Q \in \mathcal{P}$ satisfy $TV(P, Q) \le \eta$, we have $\Pr[CS_P(r) = CS_Q(r)] \ge 1 - O(\eta)$. That is, applying correlated sampling to two similar distributions results in the same output with high probability -- exactly what is needed for replicability. We give the details of this argument in Theorem~\ref{PGtoRep}.

We also give a conversion from replicable algorithms to perfectly generalizing ones. While a $\rho$-replicable algorithm is automatically also a $(\beta = O(\rho), \eps = 0, \delta = O(\rho))$-perfectly generalizing one, these parameters are too weak for applications where one wants to take $\beta, \delta$ to be inverse polynomial in the dataset size $n$. 

\begin{itemize}
    \item Describe reproducibility.
    \item Explain correlated sampling
    \item Say something about converse direction??
\end{itemize}

\paragraph{Reproducibility implies differential privacy.}

\begin{itemize}
    \item Explain construction
    \item Cite prior work that does this sort of thing
\end{itemize}

\subsection{Computationally Separating  Reproducibility and Differential Privacy}

All of the transformations appearing in Figure~\ref{fig:transformations} preserve computational efficiency, with the lone exception of the transformation from perfectly generalizing algorithms to reproducible ones. Recall that this transformation makes use of correlated sampling from the distribution of outputs of a perfectly generalizing algorithm $A$ when run on a fixed sample $S$. This step can be explicitly implemented via rejection sampling from the output space of $A$, with the rejection threshold determined by the probability mass function of $A(S)$, but in general is not computationally efficient.

We show that under cryptographic assumptions, this is inherent (Section~\ref{sec:dp-rep-separation}). Specifically, we show that under standard assumptions in public-key cryptography, there exists a statistical task that admits an efficient differentially private algorithm (hence, also an efficient perfectly generalizing one), but does not have any efficient reproducible algorithm. The task is defined in terms of a public-key encryption scheme with the following rerandomizability property: Given a ciphertext $\Enc(\pk, b)$, there is an efficient algorithm producing a uniformly random encryption of $b$. Fixing such a rerandomizable PKE, the statistical task is as follows. Given a dataset consisting of random encryptions of the form $\Enc(\pk, b)$ where $\pk$ is a fixed public key and $b \in \{0, 1\}$ is a fixed bit, output any encryption of $b$.

One can solve this problem differentially privately, essentially by choosing a random ciphertext from the input dataset and rerandomizing it. On the other hand, there is no efficient replicable algorithm for this task. If there were, then one could use the public key to produce many encryptions of $0$ and $1$ and run the replicable algorithm on the results to produce canonical ciphertexts $c_0$ and $c_1$, respectively. Then, given an unknown ciphertext, one could repeatedly rerandomize it, run the replicable algorithm on the results, and compare the answer to $c_0$ and to $c_1$ to identify the underlying plaintext.

We also show that cryptographic assumptions are necessary even to separate replicability from perfect generalization. Recalling again that the bottleneck in computationally equating the two notions is in implementing correlated sampling, we show in Section~\ref{sec:owfi} that if one-way functions do not exist, then correlated sampling is always tractable, and hence perfectly generalizing algorithms can be efficiently transformed into replicable ones.


\subsection{Statistical Lower Bounds}

\section{Preliminaries}

\subsection{Differential Privacy Preliminaries}

We start by defining differential privacy.

\begin{definition}[Differential Privacy~\cite{DworkMNS16j}]\label{def:differentially private} A randomized algorithm $\mathcal{A}: \mathcal{X}^n \rightarrow \mathcal{Y}$ is said to be {\em $(\eps, \delta)$-differentially private} if for every pair of neighboring datasets $\vec{s}, \vec{s'}\in \mathcal{X}^n$, we have that for all subsets $O \subseteq \mathcal{Y}$,
 \begin{equation*}
    \Pr[\sampler(\Datafixed) \in O] \leq e^\eps \cdot \Pr[\sampler(\Datafixed') \in O] + \delta.
 \end{equation*}
 \end{definition}

One important property of differential privacy is that it is closed under post-processing.

\begin{lemma}[Post-Processing~\cite{DworkMNS16j}]\label{prelim:postprocess} If $\mathcal{A}: \universe^n \rightarrow \mathcal{Y}$ is $(\eps, \delta)$-differentially private, and $\mathcal{B} : \mathcal{Y} \rightarrow \mathcal{Z}$ is any randomized function, then the algorithm $\mathcal{B} \circ \mathcal{A}$ is $(\eps, \delta)$-differentially private.
\end{lemma}

Standard $(\eps, \delta)$-differential privacy protects the privacy of groups of individuals.

\begin{lemma}[Group Privacy~\cite{DworkMNS16j}]\label{prelim:group_privacy}
Each $(\eps, \delta)$-differentially private algorithm $\mathcal{A}$ satisfies the following property for all datasets $\vec{s}$ and $\vec{s'}$ that differ in $k$ coordinates,
\begin{equation*}
    \Pr[\mathcal{A}(\vec{s}) \in Y] \leq e^{k\eps} \cdot \Pr[\mathcal{A}(\vec{s'}) \in Y] + \delta \cdot \frac{e^{k\eps} -1}{e^\eps-1}.
\end{equation*}
\end{lemma}

The sensitivity of a function is an important attribute that controls the amount of noise that needs to be added in many differential privacy mechanisms. 

\begin{definition}[$\ell_1$-Sensitivity] Let $f: \universe^n \rightarrow \mathbb{R}^d$ be a function. Its $\ell_1$-sensitivity is
\begin{equation*}
    \Delta_f = \max_{\substack{\vec{s}, \vec{s'} \in \universe^m \\ \vec{s}, \vec{s'} \text{neighbors}}} \|f(\vec{s}) - f(\vec{s'})\|_1.
\end{equation*}
\end{definition}

The exponential mechanism is an important algorithm that we use in some of our applications.
\begin{lemma}[Exponential Mechanism \cite{McTalwar}]\label{lem:expmech}
Let $L$ be a set of outputs and $g: L \times \mathcal{X}^m \to \mathbb{R}$ be a function that measures the quality of each output on a dataset. Assume that for every $m \in L$, the function $g(m,.)$ has $\ell_1$-sensitivity at most $\Delta$. Then, for all $\eps>0$, $n \in \mathbb{N}$ and for all datasets $\vec{s} \in \mathcal{X}^m$, there exists an $(\eps, 0)$-DP mechanism that, on input $\vec{s}d$, outputs an element $m\in L$ such that, for all $a>0$, we have
\begin{equation*}
    \Pr\left[\max_{i \in [L]} g(i,\vec{s}) -  g(m,\vec{s}) \geq 2\Delta \frac{\ln |L| + a}{\eps}\right] \leq e^{-a}. 
\end{equation*}
\end{lemma} 

Privacy amplification will be important in many of our algorithms. We recall a way to amplify the $\eps$-parameter in differential privacy.

\begin{lemma}[Secrecy of the sample] 
\label{lem:samp-wo-replace}
Let $A : \mathcal{X}^n \to \mathcal{Y}$ be an $(\eps, \delta)$-differentially private algorithm. Consider the algorithm $A' : \mathcal{X}^m \to \mathcal{Y}$ that, given a dataset of size $m$, randomly samples $n$ items without replacement and runs $A$ on the resulting subsample. Then $A'$ is $(\eps', \delta')$-differentially private for
\[\eps' = \frac{n}{m}(e^\eps - 1), \qquad \delta' = \frac{n}{m} \cdot \delta.\]
\end{lemma}



\subsection{Reproducibility Preliminaries}

It was proved in \cite{ImpLPS22} that we can amplify the reproducibility parameter at an inverse quadratic cost in the desired reproducibility parameter. 
\begin{lemma}[Amplification of Reproducibility, Theorem A.3, \cite{ImpLPS22}]\label{thm:amprep}
Let $0 < \eta, \nu, \beta < \frac{1}{2}$ and $m>0$. Let $\mathcal{A}$ be an $(\eta, \nu)$-reproducible algorithm for distribution $D$ with sample complexity $m$ and failure probability $\beta$. If $\rho > 0$, and $\nu + \rho < 0.75$, there exists a $\rho$-reproducible algorithm $\mathcal{A}'$ for $D$ with sample complexity $m ' = \tilde{O}(m(\log 1/\beta)^3 / \rho^2 (1/2-\eta)^2)$ and failure probability at most $O(\beta+\delta)$.
\end{lemma}
We also recall the $2$-parameter definition of reproducibility from \cite{ImpLPS22} (we assume that the auxiliary inputs described in their original definition are empty).

\begin{definition}[\cite{ImpLPS22}] \label{2param-defn}
Let $A(X,r)$ be an algorithm operating on samples $X$ and internal coins $r$. We say that coin $r$ is $\eta$-good for $A$ if there is a canonical output $Z_r$ such that $\Pr_X(A(X,r)=Z_r) \geq 1-\eta$. We say that $A$ is $(\eta,\nu)$ reproducible if with probability at least $1-\nu$, a uniformly random coin is $\eta$-good.
\end{definition}

\cite{ImpLPS22} observed that the two parameter and single parameter definitions are essentially equivalent.

\begin{claim}[\cite{ImpLPS22}]\label{claim:2parrep}
For every $0 \leq \rho \leq v \leq 1$,
\begin{enumerate}
    \item Every $\rho$-reproducible algorithm is also $(\rho/v, v)$-reproducible.
    \item Every $(\rho,\nu)$-reproducible algorithm is also $\rho + 2\nu$-reproducible.
\end{enumerate}
\end{claim}



\subsection{PAC-Learning Preliminaries}

We start by defining PAC Learning, which is a canonical definition of supervised learning in a paper by Leslie Valiant \cite{valiant1984theory}. We first consider the realizable setting.

\begin{definition}[Realizable PAC learning, \cite{valiant1984theory}]
A learning problem is defined by a hypothesis class $H$. For any distribution $D$ over the input space $\mathcal{X}$, consider $m$ independent draws $x_1, x_2, \cdots x_m$ from distribution $P$. A labeled sample of size $n$ is the set $\{(x_1, f(x_1)), (x_2, f(x_2)), \cdots, (x_m, f(x_m)) \}$ where $f \in H$. We say an algorithm $A$ is an $(\alpha, \beta)$-accurate PAC learner for the hypothesis class $H$ if for all functions $f \in H$ and for all distributions $D$ over the input space, $A$ on being given a labeled sample of size $m$ drawn from $D$ and labeled by $f$, outputs a hypothesis $h$ such that with probability greater than or equal to $1 - \beta$ over the randomness of the sample and the algorithm,
\begin{equation*}
    \Pr_{x \in D}[h(x) \neq f(x)] \leq \alpha.
\end{equation*}
\end{definition}

We also consider a variant called agnostic PAC learning, where the labels of the input dataset can be noisy.
\begin{definition}[Agnostic PAC learning, \cite{haussler1992decision}]
A learning problem is defined by a hypothesis class $H$. We say an algorithm $\mathcal{A}$ is an $(\alpha, \beta)$-accurate PAC learner for the hypothesis class $H$ if for all distributions $D$ over input, output pairs, $\mathcal{A}$ on being given a sample of size $m$ drawn i.i.d. from $D$ outputs a hypothesis $h$ such that with probability greater than or equal to $1 - \beta$ over the randomness of the sample and the algorithm,
\begin{equation*}
    \err_D(h) \leq \inf_{f \in H} \err_D(f)+\alpha.
\end{equation*}
where $\err_D(h) = \Pr_{(x,y) \in D} (h(x) \neq y)$.
\end{definition}
We will need uniform convergence for several of our results.

\begin{theorem}[Uniform Convergence]
\label{thm:unifconv}
Let $H$ be a binary class of functions with domain $\mathcal{X}$. Let its VC dimension be $d$. Then, for any distribution $D$ over $\mathcal{X}$, for all $m>0$,
$$\Pr_{x_1,\dots,x_m \sim D}\left[\sup_{h_z \in H}\left|\frac{1}{m}\sum_{i=1}^t \mathbbm{1}[h_z(x_i) = 1] - \Pr_{x \sim D}[h_z(x)=1] \right| \geq \gamma \right] \leq 4 (2t)^d e^{-\gamma^2 m / 8}.                                      $$
\end{theorem}

\subsection{Definitions of Correctness}

distributional-baesd correctness (``statistical tasks")

input-based correctness (``empirical tasks")















\subsection{Terminology: ``Reproducibility" and ``Replicability"}

\rexnote{Include a section explaining the terminology difference, and make change every instance of ``reproducibility" in this paper to ``replicability"}

[1] Reproducibility and Replicability in Science, National Academies of Sciences, Engineering, and Medicine, 2019. \url{https://doi.org/10.17226/25303} 
(page 46 in the report)
















\section{Equating Stability: Privacy, Perfect Generalization, and Replicability}
We connect three different notions of stablity- reproducibility \cite{ImpLPS22}, differential privacy \cite{DworkMNS06}, and perfect generalization \cite{CummingsLNRW16}. The tool we use to connect them is the notion of \textit{approximate max information} formulated in work on adaptive data analysis \cite{DworkFHPRR15, RogersRST16}.

Our approach is as follows. We first extend the known connection between differential privacy and perfect generalization to the setting of approximate differential privacy. Such connections were previously only known for pure differential privacy. That is, we prove that any approximate differentially private algorithm is perfectly generalizing with a roughly $\sqrt{n}$ blowup in $\eps$. The connection we prove has the additional advantage that the resulting $\eps$ (for perfect generalization) has no dependence on the size of the output space of the algorithm (i.e. it gives non-trivial results even for infinite output spaces, unlike prior work \cite{CummingsLNRW16}). Our approach is to prove that bounded approximate max information implies perfect generalization, and appeal to a known result \cite{RogersRST16} that approximate differentially private algorithms have bounded approximate max information.

Next, inspired by the approach in Ghazi et al. \cite{GhaziKM21}, we prove that applying correlated sampling to a perfectly generalizing algorithm (with sufficiently small parameters) gives a reproducible algorithm. Together, this proves that approximate differentially private algorithms can be compiled into reproducible ones.

For the max-information bound from differential privacy to be sufficient to get $\rho$-reproducibility, we need $\eps = \frac{\rho}{\sqrt{8 n \log(1/\rho)}}$, $\delta \leq \frac{\eps \rho^6}{n^2}$. Given a $(1,\delta)$-DP algorithm (for $\delta \leq \frac{\rho^7}{8\sqrt{\log(1/\rho) n^5}}$), we can obtain an $(\eps, \delta)$-DP algorithm with the required parameters using privacy amplification by subsampling.

This approach gives reproducible algorithms for essentially any statistical task that can be done under the constraint of differential privacy, with a roughly quadratic blowup in sample complexity.

\input{approx-dp-new.tex}

\subsection{Approximate-DP Implies Perfect Generalization}

\subsubsection{Preliminaries about Perfect Generalization}

Perfect generalization is a notion of distributional privacy. It gives one way of capturing the idea that an algorithm $A$ does not depend on its input samples too much. It requires that with high probability over a randomly drawn dataset $X$, the distribution of $A(X)$ is close to a distribution that does not depend on the specific input sample $X$.
\begin{definition}[\cite{CummingsLNRW16}]
An algorithm $A: \mathcal{Y}^n \to \mathcal{R}$ is said to be $(\beta,\eps,\delta)$-perfectly generalizing, if for every distribution $P$ over $\mathcal{Y}$, there exists a distribution $SIM_P$ such that with probability at least $1-\beta$ over the draw of an i.i.d. sample $X \sim P^n$, $A(X) \approx_{\eps, \delta} SIM_P$.
\end{definition}

We also consider the following `two-sample' version of this definition.
\begin{definition}
An algorithm $A: \mathcal{Y}^n \to \mathcal{R}$ is said to be $(\beta,\eps,\delta)$-sample perfectly generalizing, if for every distribution $P$ over $\mathcal{Y}$, with probability at least $1-\beta$ over the draw of two i.i.d. samples $X_1, X_2 \sim P^n$, $A(X_1) \approx_{\eps, \delta} A(X_2)$.
\end{definition}

Cummings et al. \cite{CummingsLNRW16} prove the following lemma about perfect generalization. \sstext{Prove converse here as well.}
\begin{lemma}[\cite{CummingsLNRW16}]\label{lem:samplePG}
If algorithm $A: \mathcal{Y}^n \to \mathcal{R}$ is $(\beta,\eps,\delta)$-perfectly generalizing, then it is also $(\beta,2\eps,3\delta)$-perfectly generalizing.
\end{lemma}


We also define a notion of `one-sided' perfect generalization that will be crucial for many of our applications in this paper.

\begin{definition}[One-way perfect generalization]
An algorithm $A: \mathcal{Y}^n \to \mathcal{R}$ is said to be $(\beta,\eps,\delta)$-one-way perfectly generalizing, if for every distribution $P$ over $\mathcal{Y}$, there exists a distribution $SIM_P$ such that with probability at least $1-\beta$ over the draw of an i.i.d. sample $X \sim P^n$, and for every output set $O \subseteq Y$, we have that
$$\Pr(A(X) \in O) \leq e^{\eps} \Pr_{SIM_P}(O) + \delta.$$
\end{definition}

Next, we prove a simple lemma concerning perfect generalization that we use in our results.

\begin{lemma}\label{lem:PGeps0}
Fix $n \in \mathbb{N}$, $\beta, \eps, \delta \in (0,1]$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be a $(\beta, \eps, \delta)$-one-way perfectly generalizating algorithm. Then, $A$ is also $(\beta, 0, 2\eps + \delta)$-perfectly generalizing.
\end{lemma}
\begin{proof}
By the definition of one-way perfect generalization, we have that for all distributions $P$ over $\mathcal{X}$, there exists a distribution $Sim_P$, such that with probability $1- \beta$ over draws of $X$, for all $O \subseteq \mathcal{Y}$,
$$ \Pr_A(A(X) \in O) \leq e^{\eps}\Pr_{Sim_P} (O) + \delta.$$
Using the fact that for $\eps \leq 1$, $e^{\eps} \leq 1 + 2\eps$, we get that
$$ \Pr_A(A(X) \in O) \leq \Pr_{Sim_P} (O)(1 + 2\eps) + \delta,$$ 
which gives us that
$$ \Pr_A(A(X) \in O) \leq \Pr_{Sim_P} (O) + 2\eps + \delta.$$ 
Now, since this works for any set $O$, consider applying it to $O^c$. Then, we get that
$$ \Pr_A(A(X) \in O^c) \leq \Pr_{Sim_P} (O^c) + 2\eps + \delta,$$ 
which implies (by writing $\Pr_A(A(X) \in O^c) = 1 - \Pr_A(A(X) \in O)$ and likewise for $SIM_P$ and doing some simple algebra) that
$$ \Pr_{Sim_P} (O) \leq \Pr_A(A(X) \in O) + 2\eps + \delta.$$ 
Hence, the lemma is proved.
\end{proof}


Cummings et al. also proved a strong relationship between $(\eps, 0)$-differential privacy and perfect generalization. Specifically, they proved the following.

\begin{theorem}[\cite{CummingsLNRW16}]
If algorithm $A: \mathcal{X}^n \to \mathcal{Y}$ is $(\eps, 0)$-differentially private, then for all $\beta > 0$, it is $(\beta, \eps \sqrt{2n \ln(2|\mathcal{Y}|/\beta},0)$-perfectly generalizing.
\end{theorem}

They left establishing similar relationships between $(\eps, \delta)$-differential privacy and perfect generalization as an open question. While we aren't able to directly prove such a result, we are able to show that any approximate differentially private algorithm is also one-way perfectly generalizing. We will later use this to prove that any approximate differentially private algorithm can be \textit{compiled} into another algorithm that is perfectly generalizing. We do this through the tool of max information, that we discuss next.

\subsubsection{Preliminaries about Max Information}
The notion of max information was formulated in work on the connection between differential privacy and adaptive data analysis. It quantitatively captures the degree of correlation between two random variables, by comparing the joint distribution of the random variables to their product measure. Intuitively, if the joint distribution and the product measure are `close', then the random variables are not as correlated with each other. %Here, we give a two-sided version of the original definition (the original definition was one-sided).
\begin{definition}[Based on \cite{DworkFHPRR15}]
The $\beta$-approximate max information between two correlated random variables $X$ and $Z$ ($I^{\beta}_{\infty}(X,Z)$) is defined as the minimum (infimum) value $k$ such that for all output sets $O$, we have that
\begin{align}
   \Pr_{(a,b) \sim (X,Z)} ((a,b) \in O) \leq 2^k \Pr_{(a,b) \sim X \otimes Z} ((a,b) \in O) + \beta
\end{align}
where $X \otimes Z$ represents the product measure of the $2$ random variables.
\end{definition}

In this paper, we will be concerned about the degree of correlation between a randomly sampled dataset, and the output of an algorithm run on that dataset. Intuitively, reproducibility requires that an algorithm's outputs not depend too much on the specific input sample it gets, so max information between these two random variables will be a useful quantity to analyze. 
\subsubsection{Approximate Differential Privacy to Bounded Max Information}
Connections between max information and differential privacy have been previously studied. Rogers, Roth, Smith and Thakkar \cite{RogersRST16} give a bound on the max information between an approximate DP algorithms' outputs and its inputs (Theorem 3.1 in their paper). In fact, they prove the following general statement that can be seen by examining their proof of Theorem 3.1.
\begin{lemma}[\cite{RogersRST16}]\label{lem:max-info-bound}
Fix $n \in \mathbb{N}$, $\eps \in (0,1/2]$ and $\delta \in [0,\eps/15)$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be an $(\eps, \delta)$-DP algorithm. Then, for any distribution $P$ over $\mathcal{X}$, if $X \sim P^n$, for all $t>0$, $I^{\beta}_{\infty}(X;A(X)) \leq n \nu + 6t \eps \sqrt{n}$, where $\beta = e^{-t^2 / 2} + cn\sqrt{\frac{\delta}{\eps}}$, and $\nu = C(\eps^2 +  \sqrt{\frac{\delta}{\eps}})$ for some sufficiently large constants $c, C$.
\end{lemma}
We instantiate this lemma with parameters that are suitable for our application.
\begin{corollary}\label{cor:max-info-concrete}
Fix $n \in \mathbb{N}$, $\rho \in (0,1]$. Let $\eps = \frac{\rho}{\sqrt{8 n \log(1/\rho)}}$, $\delta \leq \frac{\eps \rho^6}{n^2}$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be an $(\eps, \delta)$-DP algorithm. Then, for any distribution $P$ over $\mathcal{X}$, if $X \sim P^n$, $I^{\rho^3}_{\infty}(X;A(X)) \leq O(\rho)$.
\end{corollary}
\begin{proof}
Substituting the value of $\eps$, $\delta$, and setting $t = \sqrt{8\log(1/\rho)}$ in the expression for $\beta$ in Lemma~\ref{lem:max-info-bound}, we get $$\beta = e^{-4\log(1/\rho)} + cn\sqrt{\frac{\rho^6}{n^2}} = O(\rho^3).$$ Substituting in the expression for $\nu$ in Lemma~\ref{lem:max-info-bound} gives $$\nu = C(\eps^2 + \sqrt{\frac{\delta}{\eps}}) \leq C(\frac{\rho^2}{n} + \frac{\rho^3}{n}) = O(\frac{\rho^2}{n}).$$ Substituting the values of $\nu, t$, and $\eps$ in the upper bound for max information gives $$I^{\rho^3}_{\infty}(X;A(X)) \leq n \nu + 6t \eps \sqrt{n} = \rho^2 + 6\rho = O(\rho).$$
\end{proof}
\subsubsection{Bounded Max Information to One-Way Perfect Generalization}
Next, we prove a key lemma relating bounded max-information to perfect generalization. The approach we follow is similar to that used to derive relationships between pointwise $(\eps, \delta)$ indistinguishability and $(\eps, \delta)$-indistinguishability in Lemma 3.3 of \cite{KasiviswanathanS14}.
\begin{lemma}\label{lem:max-info-PG}
Fix $n \in \mathbb{N}$, $k>0$, $\beta \in (0,1)$ and $\hat{\beta} = \sqrt{\frac{\beta}{1-2^{-k}}}$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be an algorithm. Then, for any distribution $P$ over $\mathcal{X}$, $X \sim P^n$, if , $I^{\beta}_{\infty}(X;A(X)) \leq k$, then $A$ is $(\hat{\beta},2k,\hat{\beta})$-one-way perfectly generalizing.
\end{lemma}
\begin{proof}
The canonical distribution $Sim_P$ we will consider is the distribution of $A(X')$, where the randomness is over both $X' \sim P^n$, and the internal randomness of $A$. 

We start by defining a set of `bad' outputs for each fixed dataset $X$, i.e. outputs on which the probability mass of $A(X)$ is substantially larger than that of the canonical distribution $Sim_P$. Formally, for each dataset $X$, let $$B_X = \{y : y \in \mathcal{Y}, \Pr_A(A(X) = y) \geq 2^{2k} \Pr_{X' \sim P^n, A}(A(X') = y) \}.$$

Next, we define a set of ordered pairs consisting of datasets and their corresponding `bad' outputs. Formally, let $$\Theta = \{(X,y): X \in supp(P^n), y \in B_X\}.$$

Our goal will be to prove that with high probability over a draw of a dataset $X$, $A(X)$ lands in the bad set $B_X$ with small probability. This can then be used to establish one-way perfect generalization.

With this in mind, consider the expression $\mathbb{E}_X[\Pr_A(A(X) \in B_X)]$. This is clearly equal to $\mathbb{E}_X[\Pr_A((X,A(X)) \in \Theta)]$. By the law of total probability, this is equal to $\Pr_{X \sim P^n, A}((X,A(X)) \in \Theta)$. Using the definition of max-information, we get that
\begin{align}\label{eq:max-inf-relation}
    \Pr_{X \sim P^n, A}((X,A(X)) \in \Theta) \leq 2^{k}\Pr_{X,X' \sim P^n, A}((X,A(X')) \in \Theta) + \beta.
\end{align} 
Now, analyzing the term $\Pr_{X,X' \sim P^n, A}((X,A(X')) \in \Theta)$, we get 
\begin{align*}
    \Pr_{X,X' \sim P^n, A}((X,A(X')) \in \Theta) & = 
    \sum_{x \in supp(P^n)} \Pr(X=x) \Pr_{X' \sim P^n, A}(A(X') \in B_x) \\
    & \leq \sum_{x \in supp(P^n)} \Pr(X=x) 2^{-2k} \Pr_{A}(A(x) \in B_x) \\
   & = 2^{-2k} \sum_{x \in supp(P^n)} \Pr(X=x) \Pr_{A, X \sim P^n}(A(X) \in B_X \mid X=x) \\
   & = 2^{-2k} \Pr_{X \sim P^n, A}((X,A(X)) \in \Theta),
\end{align*}
where the first inequality is by the definition of $B_x$ and the last equality is by the law of total probability. Substituting the above in (\ref{eq:max-inf-relation}), we get that 
\begin{align*}
    \Pr_{X \sim P^n, A}((X,A(X)) \in \Theta) \leq 2^{-k}\Pr_{X \sim P^n, A}((X,A(X) \in \Theta) + \beta.
\end{align*} 
Rearranging, this gives us that
\begin{align}\label{eq:expec-bound}
    \mathbb{E}_X[\Pr_A(A(X) \in B_X)] = \Pr_{X \sim P^n, A}((X,A(X)) \in \Theta) \leq \frac{\beta}{1-2^{-k}} = \hat{\beta}^2.
\end{align}
Finally, by Markov's inequality and the above equation, we can write the following.
\begin{align}\label{eq:high-prob-bound}
    \Pr_X(\Pr_A(A(X) \in B_X) > \hat{\beta}) \leq
    \frac{\mathbb{E}_X[\Pr_A(A(X) \in B_X)]}{\hat{\beta}} \leq \hat{\beta}.
\end{align}
This implies that with probability $1-\hat{\beta}$ over $X \sim P^n$, $\Pr_A(A(X) \not \in B_X) > 1-\hat{\beta}$. Finally, we can write with probability $1-\hat{\beta}$ over $X \sim P^n$, for every $O \subseteq \mathcal{Y}$,
\begin{align*}
\Pr_A(A(X) \in O)  & = \Pr_A(A(X) \in O \mid A(X) \in B_X) \Pr(A(X) \in B_X) \\
& +  \Pr_A(A(X) \in O , A(X) \not\in B_X)  \\
& \leq 1 \cdot \hat{\beta} + \Pr_A(A(X) \in O , A(X) \not\in B_X) \\
& \leq \hat{\beta} + \sum_{y \in O \cap \overline{B_X}} \Pr(A(X) = y) \\
& \leq \hat{\beta} + 2^{2k}  \sum_{y \in O \cap \overline{B_X}} \Pr_{X' \sim P^n, A}(A(X') = y) \\
& \leq \hat{\beta} + 2^{2k} \Pr_{X' \sim P^n, A}(A(X') \in O , A(X') \not\in B_X) \\
& \leq \hat{\beta} + 2^{2k} \Pr_{X' \sim P^n, A}(A(X') \in O).
\end{align*}
%A similar argument (swapping the canonical distribution $Sim_P$ and the distribution of $A(X)$ for a fixed dataset in the above argument) can be used to prove that with probability $1-\hat{\beta}$ over $X \sim P^n$, for every $O \subseteq \mathcal{Y}$,
%\begin{align*}
%\Pr_{X' \sim P^n, A}(A(X') \in O) \leq \hat{\beta} + 2^{2k} \Pr_{A}(A(X) \in O)
%\end{align*}
This completes the proof.
\end{proof}
We note that the above proof sets the failure probability due to data sampling and that due to bad coins of the algorithm to be the same. Other tradeoffs between these can be obtained by using Markov's inequality with different parameters in Equation~\ref{eq:high-prob-bound}. We chose them to be equal to each other for simplicity of presentation and because that is the setting of interest in our applications.

Observe that combining Lemma~\ref{lem:max-info-bound} and Lemma~\ref{lem:max-info-PG} above gives the following connection between Differential Privacy and One-Way Perfect Generalization.
\begin{corollary}
Fix $n \in \mathbb{N}$, $\eps \in (0,1/2]$ and $\delta \in [0,\eps/15)$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be an $(\eps, \delta)$-DP algorithm. Then, for sufficiently large constants $c,C$, for all $t>0$, $A$ is $(\delta', \eps', \delta')$-one-way perfectly generalizing, where $\eps' = Cn[ \eps^2 + \sqrt{\frac{\delta}{\eps}}] + 6t \eps \sqrt{n}$, and $\delta' = \frac{\beta}{1-2^{-O(\eps')}}$, where $\beta = e^{-t^2 / 2} + cn\sqrt{\frac{\delta}{\eps}}$. 
\end{corollary}
%\sstext{Explain more about what the parameters above mean intuitively?}.
As an example of the kind of result this can give, we show what we'd get if we instantiated it with our parameters of interest (as in Corollary~\ref{cor:max-info-concrete}).
\begin{corollary}\label{cor:DPtoPG}
Fix $n \in \mathbb{N}$, sufficiently small $\rho \in (0,1]$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be an $(\eps, \delta)$-DP algorithm, where $\eps = \frac{\rho}{\sqrt{8 n \log(1/\rho)}}$, $\delta \leq \frac{\eps \rho^6}{n^2}$. Then, $A$ is $(O(\rho), O(\rho), O(\rho))$-perfectly generalizing.
\end{corollary}
\begin{proof}
From Corollary~\ref{cor:max-info-concrete}, we get that $I^{\rho^3}_{\infty}(X;A(X)) \leq O(\rho)$. Substituting $k = \rho$ and $\beta = \rho^3$ in Lemma~\ref{lem:max-info-PG}, we get that $\hat{\beta} = \sqrt{\frac{c\rho^3}{1-2^{-O(\rho)}}} \leq O(\sqrt{\rho^2}) = O(\rho)$ (where the first inequality is since $\frac{1}{1-2^{-O(\rho)}} = \frac{2^{O(\rho)}}{2^{O(\rho)}-1} \leq \frac{2}{2^{O(\rho)}-1} \leq \frac{C}{\rho}$ for some constant $C$, since $2^{c\rho} = e^{c \rho \ln 2 }$ and $e^x \geq 1+x$ for all real $x$). This gives us from Lemma~\ref{lem:max-info-PG} that $A$ is $(O(\rho), O(\rho), O(\rho))$-perfectly generalizing.
\end{proof}
\subsection{Perfect Generalization implies Replicability}

We will prove a stronger result. We will show that the larger class of one-way perfectly generalizing algorithms can be transformed to reproducible algorithms.

Let $CS(Q,\mathcal{Y},r')$ represent a correlated sampling procedure over domain $\mathcal{Y}$ sampling from a distribution $Q$ over $\mathcal{Y}$ with public randomness $r'$. %\sstext{(put in preliminaries about correlated sampling)} 
(See Section 2.1 of \cite{GhaziKM21} for an overview). We now describe our transformation.

\begin{algorithm}[H]
    \caption{Transformation from one-way perfectly generalizing algorithm to reproducible algorithm}
    \label{alg:reprodtrans}
    \hspace*{\algorithmicindent} \textbf{Input:} dataset $\Datafixed = (\datafixed_1, \ldots, \datafixed_{n})$, description of one-way perfectly generalizing algorithm $A: \mathcal{X}^n \to \mathcal{Y}$ \\
    \hspace*{\algorithmicindent} \textbf{Output:} $i\in \universe$
    \begin{algorithmic}[1] % The number tells where the line numbering should start
            \STATE Let $Q_{\Datafixed}$ represent the distribution of $A(\Datafixed)$.
            \STATE Output $CS(Q_{\Datafixed},\mathcal{Y},r')$ where $r'$ is the random string drawn in the correlated sampling algorithm (corresponding to public randomness).
    \end{algorithmic}
\end{algorithm}

The key idea is that correlated sampling converts total variation distance into collision probability, which is the notion that is used in the definition of reproducibility.

We now use this to prove the main theorem of this section.
\begin{theorem}\label{PGtoRep}
Fix $n \in \mathbb{N}$ and $\beta, \eps, \delta \in (0,1)$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be a $(\beta, \eps, \delta)$-one-way perfectly generalizing algorithm with finite output space. Then, for any distribution $P$ over $\mathcal{X}$, if $\Datafixed \sim P^n$, Algorithm~\ref{alg:reprodtrans} when run on dataset $\Datafixed$ and with access to $A$ is $2(\beta + 2\eps + \delta)$-reproducible.
\end{theorem}
\begin{proof}
By Lemma~\ref{lem:PGeps0}, we have that $A$ is also $(\beta, 0, 2\eps + \delta)$-perfectly generalizing. For any distribution $P$ over $\mathcal{X}$, let $Sim_P$ be the canonical distribution witnessing the perfect generalization property. Then, by the definition of $(0,\eps+\delta)$-indistinguishability, we have that with probability at least $1-\beta$ over a draw of a random dataset $\Datafixed \sim P^n$, 
$$d_{TV}(A(\Datafixed), Sim_P) \leq 2\eps + \delta.$$
From the guarantee of correlated sampling, we have that
$$\Pr_{r' \sim R}(CS(Q_{\Datafixed},\mathcal{Y},r') \neq CS(Sim_P, \mathcal{Y},r')) \leq 2d_{TV}(Q_{\Datafixed},Sim_P) $$
Using the bound on TV distance from perfect generalization, we get that with probability at least $1-2\beta$ over the draw of two datasets $\datafixed,\datafixed' \sim P^n$, we have that
$$\Pr_{r' \sim R}(CS(Q_{\Datafixed},\mathcal{Y},r') \neq CS(Sim_P, \mathcal{Y},r')) \leq 2(2\eps + \delta)$$ and $$\Pr_{r' \sim R}(CS(Q_{\Datafixed'},\mathcal{Y},r') \neq CS(Sim_P, \mathcal{Y},r')) \leq 2(2\eps + \delta).$$ 
Consider the event $CS(Q_{\Datafixed},\mathcal{Y},r') \neq CS(Q_{\Datafixed'},\mathcal{Y},r')$. It is clear that this implies that either $CS(Q_{\Datafixed},\mathcal{Y},r') \neq CS(A(P), \mathcal{Y},r')$ or $CS(Q_{\Datafixed'},\mathcal{Y},r') \neq CS(A(P), \mathcal{Y},r')$. Hence, we can write that with probability $1-2\beta$ over draws of $\Datafixed$ and $\Datafixed'$,
$$\Pr_{r' \sim R}(CS(Q_{\Datafixed},\mathcal{Y},r') \neq CS(Q_{\Datafixed'}, \mathcal{Y},r')) \leq 2(2\eps + \delta).$$ 
Taking expectation with respect to the draws of $\Datafixed$ and $\Datafixed'$ gives that
$$\Pr_{r' \sim R, \Datafixed, \Datafixed' \sim P^n }(CS(Q_{\Datafixed},\mathcal{Y},r') \neq CS(Q_{\Datafixed'}, \mathcal{Y},r')) \leq 2(2\eps + \delta + \beta)$$
which proves the result.
\end{proof}
Combining the above result and Corollary~\ref{cor:DPtoPG}, we get a transformation from approximate differentially private algorithms to reproducible algorithms.
\begin{corollary}\label{cor:DPtoRep}
Fix $n \in \mathbb{N}$, $\rho \in (0,1]$. Let $\eps = \frac{\rho}{\sqrt{8 n \log(1/\rho)}}$, $\delta \leq \frac{\eps \rho^6}{n^2}$. Let $A: \mathcal{X}^n \to \mathcal{Y}$ be an $(\eps, \delta)$-DP algorithm with finite output space. Fix a distribution $P$ over $\mathcal{X}$, and let $\Datafixed \sim P^n$. Then, Algorithm~\ref{alg:reprodtrans} run with inputs $\Datafixed$ and algorithm $A$ is $O(\rho)$-reproducible.
\end{corollary}
\begin{proof}
From Corollary~\ref{cor:DPtoPG}, we have that Algorithm $A$ is $(O(\rho), O(\rho), O(\rho))$-one-way perfectly generalizing. Then, applying Theorem~\ref{PGtoRep} proves that the transformation in Algorithm~\ref{alg:reprodtrans} gives a $O(\rho)$-reproducible algorithm.
\end{proof}

\subsection{Replicability implies Perfect Generalization}
In this section, we show how to convert a reproducible algorithm to a perfectly generalizing algorithm at a poly-logarithmic cost in $1/\delta$ (where $\delta$ is the additive perfect generalization parameter). Note that this could be done by converting a reproducible algorithm into a differentially private one, and then converting the differentially private algorithm into a perfectly generalizing one. However, the connection between differential privacy and perfect generalization discussed in previous sections increases $\epsilon$ by a factor of $\sqrt{m}$ (where $m$ is the size of the dataset), which will result in a quadratic blow-up in sample complexity. The direct conversion that we show in this section will show that such a quadratic blow-up can be avoided. This stronger conversion will be crucial for us to prove statistical lower bounds against reproducible algorithms.

We now give the transformation from reproducible algorithms to perfectly generalizing algorithms.

\begin{comment}
We can amplify this probability to any arbitrary $0 < \delta \leq 0.1$, as described in the following lemma. \sstext{Need to fix this.}

\begin{lemma}[Based on \cite{ImpLPS22}]
If there exists a $(\eta, v)$-reproducible algorithm for a distribution $D$ with sample complexity $m$, for all $0 < \delta \leq v$, there exists a $(\eta, \delta)$-reproducible algorithm for distribution $D$ with sample complexity $\frac{m \log (1/\delta)}{\eta^2}$.
\end{lemma}
\begin{proof}
Sample $k = 3 \log_{1/v}(1/\delta)$ values for the random coins of $A$ and call them $r_1,\dots,r_k$. Run reproducible heavy hitters for the distributions $A(;,r_1),\dots ,A(;,r_k)$ with reproducibility parameter $\eta$,  $thresh = 3/2-\eta$ and $\eps = 1/4 - \eta/2$ (chosen so that $thresh+\eps = 1-\eta$ and $thresh-\eps = \frac{1}{2}$). Output the first heavy hitter. If there are no heavy-hitters that are output, 



Call this algorithm $A'$.

If $r_i$ is $\eta$-good, then the heavy hitters algorithm will output the unique majority element with probability at least $1-\eta$. The probability that $r_i$ is not $\eta$-good is at most $v$. The probability that none of the coins $r_1,\dots,r_k$ is $\eta$-good is hence at most $v^k$, which is at most $\delta$. This is exactly the probability that the coins of the algorithm $A'$ are not $\eta$-good (since otherwise, for a fixed coin, we output a heavy hitter with probability $1-\eta$). Hence $A'$ is an $(\eta, \delta)$-reproducible algorithm. 

The sample complexity of the above procedure is $\frac{m \log(1/\delta)}{\eta^2}$. 
\end{proof}

Hence, if there exists a $(0.1,0.1)$-reproducible algorithm for $D$ with $m$ samples, there exists a $(0.1,O(\delta))$-reproducible algorithm for $D$ with $ O(m\log 1/\delta)$ samples. 
\end{comment}


We say a random coin of Algorithm $\mathcal{A}$ is `bad', if it does not have a $0.99$-heavy hitter and call it `good' otherwise. The intuition behind our transformation is as follows. It's straightforward to show that $(\delta,\delta)$-reproducibility can be used to obtain $(O(\delta),0,O(\delta))$-perfect generalization by translating from collision probability to total variation distance. However, since we typically want $\delta$ to be very small (often inverse polynomial in the number of samples $m)$, obtaining such small reproducibility parameters would come at a quadratic cost in the inverse of $\delta$ (by known lower bounds for reproducibility), which would be prohibitively large for many applications.

However, this idea still leaves hope, because it achieves $\eps = 0$. Hence, by settling for larger $\eps$, we hope to avoid this problem. 

Our approach was inspired by our initial attempt at boosting the reproducibility parameters. One natural way of doing this would be to run the reproducible algorithm on many random coins, and for each random coin, identify a heavy hitter for that coin (if one exists), by running the algorithm on many different samples with that coin used as internal randomness. Now, consider the plurality output for each coin $j$ (call this the `canonical' output for coin $j$), and let it occur $k_j$ times. Our goal is to output a canonical output that occurs lots of times (i.e. the corresponding $k_j$ value is large). The idea is that such an output is very likely the heavy hitter of a good coin- and since we drew a bunch of coins, we've simultaneously boosted the probability of seeing such a good coin. We'd unfortunately run into an obstacle here if we were attempting to boost the reproducibility parameters, since it's unclear how to reproducibly identify a single canonical output to release from this set of plurality outputs. 

However, since our goal is to obtain perfect generalization instead of reproducibility, we instead use the \textit{exponential mechanism} to identify a canonical output from the set of plurality outputs, with the score of a plurality output $z$ for coin $j$ defined as the number of samples on which the reproducible algorithm run with coin $j$ outputs $z$. We are able to prove that this idea gives a perfectly generalizing algorithm - there are a number of technical nuances that crop up when attempting to prove this that we will leave to the formal argument (for e.g., the sets of plurality outputs could differ when this algorithm is run on two i.i.d. datasets drawn from the same distribution). 

One interesting feature of this proof is that unlike standard uses of the exponential mechanism to prove privacy or perfect generalization, the \textit{accuracy} of the exponential mechanism is important to obtain perfect generalization. 

\begin{algorithm}[H]
    \caption{Transformation from reproducible algorithm $\mathcal{A}$ to perfectly generalizing algorithm $\mathcal{A}'$}
    \label{alg:perfgentrans}
    \hspace*{\algorithmicindent} \textbf{Input:} Sample access to distribution $D$, description of $(0.01, 0.01)$-reproducible algorithm $\mathcal{A}: \mathcal{X}^* \to \mathcal{Y}$, sample complexity parameter $m$, perfect generalization parameters $\epsilon, \delta, \beta$\\
    \hspace*{\algorithmicindent} \textbf{Output:} $y \in \universe$
    \begin{algorithmic}[1] % The number tells where the line numbering should start
            \STATE Let $k = O(\log 1/\delta)$, and $t = O(\log^4 1/\beta)$.
            \STATE Draw uniformly random coins $r_1,r_2,\dots,r_k$ for algorithm $\mathcal{A}$. \label{step:randcoinpg}
            \STATE Draw $k$ sets $\mathbf{\vec{s}_i}$ of $t$ samples $\vec{s}_{i,j} \sim D^m$.
            \FOR{all $j \in [k]$} \label{step:loopPG}
            \FOR{all $i \in [t]$}
            \STATE Run $\mathcal{A}$ with coins $r_j$ and sample $\vec{s}_{i,j}$ to get output $z_{i,j}$.
            \ENDFOR
            \STATE Let $Can_j = \arg \max_{z \in \mathcal{Y}} \sum_{i=1}^t 1[z_{i,j}=z]$, and let $Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)= \sum_{i=1}^t 1[z_{i,j}=Can_j]$.
            \ENDFOR
            \STATE Let $Can = \{Can_1,\dots,Can_k\}$ be a multi-set. Run the exponential mechanism on the set $Can$ with the score function $Score(.,.)$, sensitivity parameter $4\sqrt{t \log(8kt/\beta)}$, and parameter $\epsilon$, to get value $y$. \label{step:expmechPG} 
            \RETURN output $y$ of the previous step. \label{step:PGop}
    \end{algorithmic}
\end{algorithm}

\begin{theorem} \label{thm:reprodtoPG}
Fix sufficiently small $\delta, \gamma > 0$ Every $(0.01,0.01)$-reproducible algorithm $\mathcal{A}$ with $m$ samples that succeeds on a statistical task with probability at least $1-\gamma^2$ can be converted to a $(2\delta,1,2\delta)$-perfectly generalizing algorithm $\mathcal{A}'$ taking $O(m \text{ }poly\log(1/\delta))$ samples, that succeeds on the statistical task with probability at least $1-O(\delta)-\gamma \log 1/\delta$.
\end{theorem}
\begin{proof}
Fix a distribution $D$ over the input set $\mathcal{X}$. Our proof will be constructive; the corresponding algorithm is given in Algorithm~\ref{alg:perfgentrans} ($\mathcal{A}'$), and we feed it with the following inputs: a description of algorithm $\mathcal{A}$, sample complexity parameter $m$, and perfect generalization parameters $\delta, 1,\delta$ respectively (we also give it sample access to distribution $D$). We will start by proving perfect generalization of Algorithm~\ref{alg:perfgentrans}.
\begin{claim}
Algorithm $\mathcal{A}'$ (represented in Algorithm~\ref{alg:perfgentrans}) with the input parameters specified in the previous paragraph is $(2\delta,1,2\delta)$-perfectly generalizing.
\end{claim}
\begin{proof}
Consider two samples $\vec{s}$ and $\vec{s}'$ drawn independently from $D^{mkt}$. We will consider Algorithm~\ref{alg:perfgentrans} run on both of these samples and argue that their output distributions are close in the sense required by perfect generalization. 
\paragraph{Step 1: Atleast one coin is good w.h.p.} We say a random coin of Algorithm $\mathcal{A}$ is `bad', if it does not have a $0.99$-heavy hitter and call it `good' otherwise. Then, by the two parameter definition of reproducibility, a random coin is `bad' with probability at most $0.01$. Hence, the probability that all $k$ coins drawn in Step~\ref{step:randcoinpg} of Algorithm~\ref{alg:perfgentrans} are bad is at most $(0.01)^k \leq \delta^2$ for $k = O(\log 1/\delta)$. Hence, the probability that all $k$ coins are bad is at most $\delta^2$. Let $E_{coin}$ represent the event that there is at least one good coin. We will now condition on $E_{coin}$ occurring; fix any set of coins $r_1,\dots,r_k$ that has non-zero probability of occurring under this conditioning.

\paragraph{Step 2: Empirical output frequencies are close on two independent datasets} We define stage $j$ of Algorithm~\ref{alg:perfgentrans} as the process involved in generating $Can_j$ (i.e. one iteration of the outer loop in Step~\ref{step:loopPG}). We now use uniform convergence to argue that with high probability over samples, the empirical frequencies of the outputs of all stages, i.e. all the $Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)$ values are close to their expectation. 

Start by defining $H$ to be a function class consisting of point functions i.e. functions of the form $h_{\ell}(x) = 1$ if $x={\ell}$, and $h_{\ell}(x) = 0$ otherwise (where $\ell$ belongs to any discrete set. In our setting $l$ will be the set $\mathcal{Y}$). It is easy to prove that the VC dimension of $H$ is equal to $1$. Let $Q_j$ be the distribution of the output of the reproducible algorithm when run with coin $r_j$.

Then, using uniform convergence (\sstext{cite right theorem}), we get for any fixed $j$ and for every $\gamma > 0$, that
$$\Pr_{z_{1,j},\dots,z_{t,j} \sim Q_j}\left[\sup_{h_{\ell} \in H}\left|\frac{1}{t}\sum_{i=1}^t \mathbbm{1}[h_{\ell}(z_{i,j}) = 1] - \Pr_{z \sim Q_j}[h_{\ell}(z)=1] \right| \geq \gamma \right] \leq 8t e^{-\gamma^2 t / 8}.                                      $$

Observe that $\frac{1}{t}\sum_{i=1}^t \mathbbm{1}[h_{\ell}(z_{i,j}) = 1] = \frac{1}{t}\sum_{i=1}^t \mathbbm{1}[z_{i,j} = \ell]$. Similarly, $\Pr_{z \sim Q}[h_{\ell}(z)=1] = \Pr_{z \sim Q_j}[z=\ell]$. Hence, we get that 
$$\Pr_{z_{1,j},\dots,z_{t,j} \sim Q_j}\left[\sup_{\ell \in \mathcal{Y}}\left|\frac{1}{t}\sum_{i=1}^t \mathbbm{1}[z_{i,j} = \ell] -\Pr_{z \sim Q_j}[z=\ell] \right| \geq \gamma \right] \leq 8t e^{-\gamma^2 t / 8}.                                      $$

Setting $\gamma = 2\sqrt{\frac{\log(8kt/\delta)}{t}}$, we get that
$$\Pr_{z_{1,j},\dots,z_{t,j} \sim Q_j}\left[\sup_{\ell \in \mathcal{Y}}\left|\frac{1}{t}\sum_{i=1}^t \mathbbm{1}[z_{i,j} = \ell] -\Pr_{z \sim Q_j}[z=\ell] \right| \geq \sqrt{t \log(8kt/\delta)} \right] \leq \frac{\delta^2}{k}. $$

Using a union bound over all $k$ stages of the algorithm, this guarantees us that the empirical frequencies of all outputs are close to their expectations (and in particular, the values $Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)$ are all within $2\sqrt{t \log(8kt/\delta)}$ of their expectations with probability at least $1-\delta^2/2$). Note that since we conditioned on a fixed random coin, all the randomness in $z_{i,j}$ comes from the corresponding sample. 

Hence, if we consider another sample $(\mathbf{\vec{s}}'_1, \mathbf{\vec{s}}'_2, \dots, \mathbf{\vec{s}}'_k)$ drawn i.i.d from $D^{mkt}$, we can say that with probability at least $1-\delta^2$ over draws of $\vec{s}$ and $\vec{s}'$, for all $j \in [k]$, $Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)$ and $Score\left(Can_j,(\mathbf{\vec{s}}'_1, \mathbf{\vec{s}}'_2, \dots, \mathbf{\vec{s}}'_k) \right)$ (note that we consider the same output $Can_j$ above, i.e. the canonical output of the $j^{th}$ stage when Algorithm~\ref{alg:perfgentrans} is run on $\vec{s}$) are  both within $2\sqrt{t \log(8kt/\delta)}$ of their expectations, and are hence within $4\sqrt{t \log(8kt/\delta)}$ of each other. We call this event $E_{sample}$, and fix any sample pairs $(\vec{s}, \vec{s'})$ that occur with non-zero probability conditioned on this event. Note that we can use this to argue the following equation: With probability at least $1-\delta^2$, for all $j \in [k]$,
\begin{align}
\label{eq:PGcanopclose}
|Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right) - Score\left(Can'_j,(\mathbf{\vec{s}}'_1, \mathbf{\vec{s}}'_2, \dots, \mathbf{\vec{s}}'_k) \right)| \leq 4\sqrt{t \log(8kt/\delta)}.
\end{align}
This follows directly from the above argument if $Can_j = Can'_j$, but if they are not equal, it also holds since otherwise either $Can_j$ or $Can'_j$ would not be a canonical output in stage $j$ of the corresponding runs (since there would be an output that occurs more times in stage $j$). 

\paragraph{Step 3: Arguing that there is at least one canonical output $Can_j$ with high score} Conditioned on $E_{coin}$, we know that the run of Algorithm~\ref{alg:perfgentrans} on $\vec{s}$ has at least one coin with a $0.99$-heavy hitter $z$ (without loss of generality, let's say coin $r_j$ is such a coin). Next, observe that for the settings of $k$ and $t$, we get that $2\sqrt{t \log(8kt/\delta)} \leq 0.09t$. Hence, conditioned further on $E_{sample}$, we know that this heavy hitter $z$ is the canonical output of stage $j$, $Can_j$, and that $Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)$ is at least $0.9t$. Hence, there exists an output $Can_j$ with score at least $0.9t$. 

\paragraph{Step 4: Arguing that probable outputs $y$ are in both output sets $Can$ and $Can'$} By the accuracy guarantee of the exponential mechanism \sstext{add right citation}, we have that
$$\Pr\left[\max_{j \in [k]} Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)-  Score\left(y,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right) \geq 2\Delta \frac{\ln k + k}{\eps}\right] \leq e^{-k} = \delta,$$
where $\Delta = 4\sqrt{t \log(8kt/\delta)}$. Hence, for the settings of $t$ and $k$, we get that $2\Delta \frac{\ln k + k}{\eps} = O(k\sqrt{t \log(8kt/\delta)}) = o(t)$ (since $k = O(t^{1/4})$). Hence, we have that
$$\Pr\left[  Score\left(y,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right) \geq 0.9t - 2\Delta \frac{\ln k + k}{\eps}\right] \leq e^{-k} = \delta,$$
which implies that for sufficiently small $\delta$,
$$\Pr\left[Score\left(y,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right) \geq 0.8t \right] \leq e^{-k} = \delta.$$
Let's consider any such $y$ (and say without loss of generality it is $Can_j$). By the conditioning on $E_{sample}$, we have that $y$ occurs more than $0.8t -  4\sqrt{t \log (kt/\delta)}) > 0.5t$ times in the output set of stage $j$ when Algorithm~\ref{alg:perfgentrans} is run on $\mathbf{\vec{s}}'$. Hence, $Can'_j$ is also equal to $y$. 
\paragraph{Step 5: Proving that $\mathcal{A}'(\mathbf{\vec{s}},r) \approx _{\eps, \delta} \mathcal{A}'(\mathbf{\vec{s}}',r)$ w.h.p.} 
We exploit the fact that two random variables $C$ and $D$ are $(\eps, \delta)$-indistinguishable if w.p. $\geq 1-\delta$ over a draw $o$ from the distribution of $C$, $e^{-\epsilon}\Pr(D=o) \leq \Pr(C=o) \leq e^{\epsilon} \Pr(D=o)$, and likewise for a draw from the distribution of $B$. \cite[Lemma 3.3, Part 1]{KasiviswanathanS14} \\
 
We proved in Step $4$ that fixing any coins and sample pairs that have non-zero probability of occurring conditioned on $E_{coin}$ and $E_{sample}$, with probability at least $1-\delta$ from a draw of $A'(\mathbf{\vec{s}},r)$ (where the randomness is only that of the exponential mechanism), the output $y$ occurs in both the sets $Can$ and $Can'$. For all such outputs, our idea is to use the differential privacy analysis of the exponential mechanism. There are a couple of technical obstacles that need to be surmounted here.

The first obstacle is that the output sets $Can$ and $Can'$ might be different, and so the normalizing factors used in the exponential mechanism will vary accordingly. We deal with this by invoking equation~\ref{eq:PGcanopclose}, which points out that even though the output sets are different, the score functions $Score\left(Can_j,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right)$ and $Score\left(Can'_j,(\mathbf{\vec{s}}'_1, \mathbf{\vec{s}}'_2, \dots, \mathbf{\vec{s}}'_k) \right)$ can differ by at most the sensitivity specified in Step~\ref{step:expmechPG} where the exponential mechanism is invoked, and so we can surmount this obstacle.

The other obstacle is that $Can$ and $Can'$ are multisets, and so can have repeats, which could affect the analysis (for e.g. some outputs could repeat in $Can$ but not repeat in $Can'$, so their probabilities of being output would add up in the former case but not in the latter). We argue that we can assume that they are sets of size $k$ without loss of generality. The idea is the following: consider a thought experiment where instead of defining $Can = \{Can_1,\dots,Can_j\}$, we would instead define $Can$ as $\{(Can_1,1), (Can_2, 2), \dots, (Can_k, k)\}$. Then, the exponential mechanism run on this set $Can$ of size $k$ would result in some output $(Can_j, j)$. Now, we can strip off the label $j$ and simply output $Can_j$. Observe that the distribution over outputs produced in this thought experiment is identical to the distribution of outputs of Algorithm~\ref{alg:perfgentrans}. Hence, if we prove that the algorithm prior to stripping the labels is perfectly generalizing, then the stripping of the labels is simply post-processing. Since perfect generalization is robust to post-processing, this would guarantee that Algorithm~\ref{alg:perfgentrans} is perfectly generalizing with the same parameters.

Hence, exactly mimicking the differential privacy analysis of the exponential mechanism, conditioned on $E_{coin}$ and $E_{sample}$, with probability at least $1-\delta$ from a draw  $y$ of $\mathcal{A}'(\mathbf{\vec{s}},r)$ \sstext{Should I do the analysis or just cite something?}, we get that 
$$ e^{-\epsilon} \Pr( \mathcal{A}'(\mathbf{\vec{s}},r) = y) \leq \Pr( \mathcal{A}'(\mathbf{\vec{s}}',r) = y) \leq e^{\epsilon} \Pr( \mathcal{A}'(\mathbf{\vec{s}},r) = y).$$
By symmetry (since $s$ and $s'$ are both independent samples from the distribution with the same properties), conditioned on $E_{coin}$ and $E_{sample}$, we  get that with probability at least $1-\delta$ from a draw $y$ of $\mathcal{A}'(\mathbf{\vec{s}}',r)$,
$$ e^{-\epsilon} \Pr( \mathcal{A}'(\mathbf{\vec{s}},r) = y) \leq \Pr( \mathcal{A}'(\mathbf{\vec{s}}',r) = y) \leq e^{\epsilon} \Pr( \mathcal{A}'(\mathbf{\vec{s}},r) = y).$$
Hence, we have proved that conditioned on any fixed coins and sample pairs with non-zero probability of occurring conditioned on $E_{coin}$ and $E_{sample}$, $\mathcal{A}'(\mathbf{\vec{s}},r) \approx _{\eps, \delta} \mathcal{A}'(\mathbf{\vec{s}}',r)$. Now, using the law of total probability, we get that
\begin{align}
\label{eq:highprobPG}
\Pr_{r_1,\dots,r_k}\left( \Pr_{\mathbf{\vec{s}}, \mathbf{\vec{s}}' \sim D^{mkt}}\Big( \mathcal{A}'(\mathbf{\vec{s}};r_1,\dots,r_k) \approx_{\epsilon, \delta} \mathcal{A}'(\mathbf{\vec{s}}';r_1,\dots,r_k) \Big) \geq 1 - \delta^2 \right) \geq 1-\delta^2.
\end{align}
\paragraph{Step 6: Switch quantifiers to get perfect generalization:}
Now, we switch the quantifiers in equation~\ref{eq:highprobPG}. 
\begin{align*}
        & \Pr_{\mathbf{\vec{s}}, \mathbf{\vec{s}}' \sim D^{mkt}, r_1,\dots,r_k} \Big( \mathcal{A}'(\mathbf{\vec{s}};r_1,\dots,r_k)  \approx_{\epsilon, \delta }\mathcal{A}'(\mathbf{\vec{s}}';r_1,\dots,r_k)  \Big) \geq 1-2\delta^2. \implies \\
       &\mathbb{E}_{\mathbf{\vec{s}}, \mathbf{\vec{s}}' \sim D^{mkt}} \left[ \Pr_{r_1,\dots,r_k} \Big( \mathcal{A}'(\mathbf{\vec{s}};r_1,\dots,r_k)  \approx_{\epsilon, \delta } \mathcal{A}'(\mathbf{\vec{s}}';r_1,\dots,r_k) \Big)\right] \ge 1-2\delta^2 \\
       &\implies \Pr_{\mathbf{\vec{s}}, \mathbf{\vec{s}}' \sim D^{mkt}} \left[ \Pr_{r_1,\dots,r_k} \Big( \mathcal{A}'(\mathbf{\vec{s}};r_1,\dots,r_k)  \approx_{\epsilon, \delta }\mathcal{A}'(\mathbf{\vec{s}}';r_1,\dots,r_k) \Big) \geq d \right] \ge \frac{1-2\delta^2-d}{1-d} .
\end{align*}
where the last inequality is by the Reverse Markov's inequality. Setting $d = 1-\delta$, we get that
\begin{align*}
     \Pr_{\mathbf{\vec{s}}, \mathbf{\vec{s}}' \sim D^{mkt}} \left[ \Pr_{r_1,\dots,r_k} \Big( \mathcal{A}'(\mathbf{\vec{s}};r_1,\dots,r_k)  \approx_{\epsilon, \delta }\mathcal{A}'(\mathbf{\vec{s}}';r_1,\dots,r_k) \Big) \ge 1-\delta \right] \ge 1- 2\delta.
\end{align*}
Now, using the fact that if $X \approx_{\eps, \delta} Y$ and $C \approx_{\eps, \delta} D$, then $\alpha X + (1-\alpha) C \approx_{\eps, \delta} \alpha Y + (1-\alpha)D$ (i.e. $(\eps, \delta)$-indistinguishability plays well with convex combinations), we get that 
$$ \Pr_{\mathbf{\vec{s}}, \mathbf{\vec{s}}' \sim D^{mkt}} \left[ \mathcal{A}'(\mathbf{\vec{s}})  \approx_{\epsilon, 2\delta } \mathcal{A}'(\mathbf{\vec{s}}') \Big) \right] \ge 1- 2\delta.$$ \sstext{Is this much explanation enough here?}
This proves that $\mathcal{A}'$ with the specified inputs is $(2\delta, 1, 2\delta)$-perfectly generalizing, as required. Next, we deal with accuracy.
\end{proof}
\begin{claim}
If Algorithm $\mathcal{A}$ succeeds at a statistical task with probability at least $1-\gamma^2$, Algorithm $\mathcal{A}'$ succeeds at the same statistical task with probability at least $1-O(\delta)-\gamma \log 1/\delta$.
\end{claim}
\begin{proof}
Recall the definition of `succeeding' at a statistical task. The statistical task is defined by a set of distribution, set pairs. For every distribution $D$, there is an associated good set of outputs $O_D$. An algorithm succeeds at this task with probability at least $1-\gamma$ if it outputs a member of this good set with at least that probability (taken over random samples from $D$ and any internal coins of the algorithm). 

If $\mathcal{A}$ succeeds at the task with probability at least $1-\gamma^2$, by using reverse Markov's inequality as in Step 6 of the previous proof, we have that
\begin{align}
\label{eq:accuracy}
       \Pr_{r_1,\dots,r_k} \left[ \Pr_{\mathbf{\vec{s}} \sim D^{mkt}}  \Big( \mathcal{A}'(\mathbf{\vec{s}};r_1,\dots,r_k) \in O_D  \Big) \geq 1-\gamma \right] \ge 1-\gamma
\end{align}

We say a set of coins $r_1,\dots,r_k$ is `accurate' if the equation inside the outer probability is satisfied. Recall that we called a coin `good' if it had a $0.99$-heavy hitter. By the analysis in Step $1$ of the previous proof, we have that with probability at least $1-\delta^2$, there is a good coin in the set of coins $r_1,\dots, r_k$. Now, the probability that all $k$ coins are `accurate' is equal to $(1-\gamma)^{\log 1/\delta} \geq 1-\gamma \log 1/\delta$ (can see this using Bernoulli's inequality $(1+a)^k \geq 1+ak$ for all $a \geq -1$ and non-negative integers $k$). Hence, by a union bound, the probability that the set of coins both contains a good coin and that all the coins in the set are accurate is at least $1-\gamma \log 1/\delta-\delta^2$. Call this event $E_{coin-acc}$ and condition on it. Additionally, condition on $E_{sample}$ as defined in Step $2$ of the previous proof. Then, by the analysis in Step 4 of the previous proof, we have that  $$\Pr\left[Score\left(y,(\mathbf{\vec{s}}_1, \mathbf{\vec{s}}_2, \dots, \mathbf{\vec{s}}_k) \right) \geq 0.8t \right] \leq e^{-k} = \delta,$$
which implies that the exponential mechanism outputs a coin with a canonical output that occurs at least $0.8t$ times with probability at least $1-\delta$. Since we have conditioned on $E_{sample}$, we have that empirical frequencies are close to their expected values, and hence we can say that the exponential mechanism outputs the canonical output of a coin that is at least $0.25$-good with probability at least $1-\delta$. Using the law of total probability to remove the conditioning on $E_{sample}$, we get that the exponential mechanism outputs the canonical output of a coin that is at least $0.25$-good with probability at least $1-\delta-\delta^2$ (Since event $E_{sample}$ happens with probability at least $1-\delta^2$). Note that since all the drawn coins are accurate, we get that the canonical output of all such coins is in the good set $O_D$ (otherwise, the inequality inside the outer probability in equation~\ref{eq:accuracy} would not be satisfied). Hence, conditioned on $E_{coin-acc}$, $\mathcal{A}'$ outputs an element of the good set with probability at least $1-\delta-\delta^2$. Using the law of total probability, we then get that $\mathcal{A}'$ outputs an element of the good set with probability at least $1-\delta-2\delta^2 - \gamma \log 1/\delta = 1-O(\delta) - \gamma \log 1/\delta$.
\end{proof}
Hence, combining the two claims on perfect generalization and accuracy, we complete the proof of the theorem.
\end{proof}

















%===============================
%===============================
%===============================

\newpage
\section{Separating Stability: Computational Barriers}

\subsection{Cryptographic Hardness of Reproducibility}
\label{sec:dp-rep-separation}

\newcommand{\prandenc}{\Pi_{\textit{RandEnc}}}
\newcommand{\cspace}{\mathcal{C}}

In this Section, 
we define a promise problem $\prandenc$ (Definition~\ref{def:ctxt-id-problem}) for a public-key encryption scheme. $\prandenc$ is parameterized by a public-key $\pk$ for a public-key encryption scheme $\escheme$ with message space $\{0,1\}$ and ciphertext space $\cspace$. An instance of $\prandenc$ consists of a sample of $m$ elements $c_i$, drawn i.i.d. from an unknown distribution $D$ over $\cspace$.
Promised that either 
\begin{enumerate}
    \item $D$ is supported entirely on encryptions of $0$ under $\pk$ or
    \item $D$ is supported entirely on encryptions of $1$ under $\pk$,
\end{enumerate}
Problem $\prandenc$ asks the algorithm to output an encryption of $0$ under $\pk$ in the first case and an encryption of $1$ under $\pk$ in the second case. 


We show that if the public-key encryption scheme supports a strong form of rerandomization, Problem $\prandenc$ can be efficiently solved with a differentially private algorithm. At the same time, $\prandenc$ cannot be efficiently solved using a reproducible algorithm, assuming the security of the underlying encryption scheme. 
Thus, in this setting, there cannot be an efficient black-box reduction from DP algorithms to reproducible algorithms.




For our construction, we use a standard definition for public-key encryption.

\begin{definition}[Public-Key Encryption Scheme]
\label{def:encryption-scheme}
 Let $\lambda \in \N$ be a security parameter and let $\escheme = (\KeyGen, \Enc, \Dec)$ be a tuple of algorithms running in time $\poly(\lambda)$. We say $\escheme$ is a \emph{public-key encryption scheme} if it satisfies the following properties.
\begin{itemize}
\item Correctness: Let $(\sk, \pk) \gets \KeyGen(\lambda)$ and $c \gets \Enc(\pk, b)$ for $b \in \{0,1\}$. Then $\Dec(\sk, c) = b$.   

\item Security: There exists a negligible function $\varepsilon(\lambda)$, such that for all adversaries $\mathcal{A}$ running in time $\poly(\lambda)$, letting $(\sk, \pk) \gets \KeyGen(\lambda)$ we have
$$\Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 1)] - \Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 0)] < \varepsilon(\lambda) .$$

\end{itemize}
\end{definition}

We also require that a public-key encryption scheme allows for efficient, publicly computable ciphertext verification and rerandomization procedures.  
\begin{definition}[Randomizeable Encryption Scheme]\label{def:rand-enc}
Let $\escheme = (\KeyGen, \Enc, \Dec)$ be a public-key encryption scheme. We call $\escheme$ a \emph{randomizeable encryption scheme} if it supports the following additional procedures. 
\begin{itemize}
	\item \textbf{(Perfect) Verification of Ciphertexts:}
		There exists a deterministic polytime algorithm $\VVV$ such that, for an honestly generated key pair $(\sk, \pk) \gets \KeyGen(\lambda)$ and value $c$,
		\begin{itemize}
		    \item If $\Dec(\sk, c) \in \{0,1\}$, then $\VVV(\pk, c) = 1$
		    \item If $\Dec(\sk, c) = \bot$, then $\VVV(\pk, c) = 0$
		\end{itemize}

	\item \textbf{(Perfect) Randomization of Ciphertexts:}
		There exists a randomized polytime algorithm $\Ran$ such that, 
		for all honestly generated key pairs $(\sk, \pk) \gets \KeyGen(\lambda)$, and all ciphertexts $c_1, c_2$ such that $\Dec(\sk, c_1) = \Dec(\sk, c_2)$, 
		\begin{itemize}
		    \item $d_{TV}(\Ran(\pk, c_1), \Ran(\pk, c_2)) = 0$
		    \item $\Dec(\sk, \Ran(\pk, c)) = \Dec(\sk, c)$
		\end{itemize}
\end{itemize}
\end{definition}

Consider the following search problem $\prandenc$. Given a public key $\pk$ for an encryption scheme $\escheme$, and an i.i.d. sample of $m$ elements from a distribution $D$ supported on encryptions under $\pk$ of a fixed bit $b \in \{0,1\}$, output an encryption of $b$ under $\pk$. 

\begin{definition}[Ciphertext Identification Problem]
	\label{def:ctxt-id-problem}
	An instance of $\prandenc$ is defined as follows. 
	Let $\escheme = (\KeyGen, \Enc, \Dec)$ be a randomizeable encryption scheme (Definition~\ref{def:rand-enc}). 
	Let $\lambda, m \in \N$, and let $D$ be a distribution over the ciphertext space $\cspace$ of $\escheme$. 
	Given public key $\pk$, honestly generated as $(\pk, \sk) \gets \KeyGen(\lambda)$, and a sample $S \subset D^m$ drawn i.i.d. from $D$,
	output an element $c^* \in \cspace \cup \bot$ such that 
	\begin{enumerate}
	    \item If $\Dec(\sk, c) = 1$ for all $c \in \supp(D)$, $\Dec(\sk, c^*) = 1$
	    \item If $\Dec(\sk, c) = 0$ for all $c \in \supp(D)$, $\Dec(\sk, c^*) = 0$
	\end{enumerate}

\end{definition}

\newcommand{\dprandenc}{\mathtt{DPRandEnc}}
\newcommand{\cnew}{c_{\mathtt{new}}}
\newcommand{\cout}{c_{\mathtt{out}}}
\newcommand{\maj}{\texttt{Maj}}


In Subsection~\ref{ssec:dp-randenc-algo}, we give a simple algorithm $\dprandenc$ solving $\prandenc$. $\dprandenc$ is $(\eps, \delta)$-differentially private and runs in polynomial time. 
In Subsection~\ref{ssec:no-rep-randenc-algo}, we show that the existence of an efficient reproducible algorithm for $\prandenc$ would violate the security guarantee of the encryption scheme. Thus, assuming randomizable encryption schemes exists, there is no efficient black-box reduction from DP algorithms to reproducible algorithms for $\prandenc$. 
In Subsection~\ref{ssec:gm}, we recall the Goldwasser-Micali public-key encryption scheme \cite{STOC:GolMic82} and show that it is a randomizeable encryption scheme. Using the Goldwasser-Micali cryptosystem to instantiate $\prandenc$ then demonstrates that there cannot be an efficient blackbox reduction from reproducibility to differential privacy, assuming the hardness of deciding quadratic residuosity. 






\subsubsection{DP Algorithm for $\prandenc$}
\label{ssec:dp-randenc-algo}

In this Subsection, we present a differentially private algorithm $\dprandenc$ for $\prandenc$. 
Our algorithm removes from the dataset all $c_i$ for which verification fails, i.e., $\VVV(\pk, c_i) = 0$.  It then pads the remaining elements with $k$ encryptions of 0 under $\pk$ and $k$ encryptions of 1 under $\pk$. An element $c_i$ from the new dataset is then chosen uniformly at random, and the algorithm outputs $\Ran(\pk, c_i)$. 

\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{$\dprandenc$\\
			Input: Sample $S$ of $m$ elements drawn i.i.d. from $D$
			Output: a ciphertext $c \in \cspace$ 
		}
		\label{alg:dprandenc}
		\begin{algorithmic}
			\STATE For $i \in [m]$, remove $c_i$ from $S$ if $\VVV(\pk, c_i) = 0$
			\STATE Add $k$ ciphertexts $\Enc(\pk, 0)$ to the dataset
			\STATE Add $k$ ciphertexts $\Enc(\pk, 1)$ to the dataset
			\STATE Choose $c$ uniformly at random from the new dataset
			\RETURN $\Ran(\pk, c)$
		\end{algorithmic}
	\end{algorithm}
\end{figure}

\begin{lemma}\label{lem:dprandenc-dp-corr}
Let $\epsilon, \beta \in (0,1/2)$. Then for $m \in \Omega(1/(\epsilon\beta))$ and $k = 1/\epsilon$,  
$\prandenc$ (\cref{alg:dprandenc}) runs in time $\poly(\lambda, 1/\epsilon, 1/\beta)$, is $\epsilon$-DP, and correct except with probability at most $\beta$.
\end{lemma}
\begin{proof}
We begin by showing $\dprandenc$ is $\epsilon$-DP. 
Note that the last step of $\dprandenc$ calls $\Ran$ on a ciphertext $c$ that is guaranteed to be a valid encryption of a bit $b\in \{0,1\}$, since all inputs failing verification are removed from $S$ before $c$ is drawn, and only valid ciphertexts under $\pk$ are added to the input dataset. 

We will bound how much the probability that $c$ encrypts a fixed bit $b$ can differ across neighboring data sets.
Let $c$ be a random variable denoting the ciphertext chosen for rerandomization. For all $b \in \{0,1\}$ and neighboring datasets $S, S'$
\begin{align*}
\frac{\Pr[\Dec(\sk, c) = b \mid S]}{\Pr[\Dec(\sk, c) = b \mid S']} 
&= \frac{|\{c \in S \mid \Dec(\sk, c) = b\}| + k}{m - | \{c \in S \mid \VVV(\pk, c) = 0\}| + 2k }\cdot \frac{m - | \{c \in S' \mid \VVV(\pk, c) = 0\}| + 2k }{|\{c \in S' \mid \Dec(\sk, c) = b\}| + k} \\
& \leq \frac{(k+1)(2k+1)}{(2k+1)k} \\
& = \frac{k+1}{k}.
\end{align*}
Because the output distribution of $\Ran(\pk, c)$ is the same for all ciphertexts encrypting the same bit under $\pk$, it follows that
\begin{align*}
\Pr[\dprandenc(S) \in T] 
&= \Pr[\Ran(\pk, c) \in T \mid \Dec(\sk, c) = 1]\cdot \Pr[\Dec(\sk, c) = 1 \mid S] \\
& \quad \quad \quad + \Pr[\Ran(\pk, c) \in T \mid \Dec(\sk, c) = 0]\cdot \Pr[\Dec(\sk, c) = 0 \mid S]. \\
\end{align*}
Using $p_b$ to denote $\Pr[\Ran(\pk, c) \in T \mid \Dec(\sk, c) = b]$, for $b \in \{0,1\}$, we then have that
\begin{align*}
    \Pr[\dprandenc(S) \in T]
    &= p_1\cdot \Pr[\Dec(\sk, c) = 1 \mid S] + p_0\cdot \Pr[\Dec(\sk, c) = 0 \mid S] \\
    &\leq \frac{k+1}{k}\left(p_1 \cdot \Pr[\Dec(\sk, c) = 1 \mid S'] + p_0\cdot   \Pr[\Dec(\sk, c) = 0 \mid S']\right)\\
    &= \frac{k+1}{k} \cdot \Pr[\dprandenc(S') \in T] \\
    &\leq e^{\epsilon}\cdot \Pr[\dprandenc(S') \in T] 
\end{align*}
where the final inequality follows from taking $k = 1/\epsilon$ and observing $1+ x \leq e^x$.

It remains to argue correctness of $\dprandenc$ when the sample $S$ is drawn from one of the promised distributions. In this case, the input sample $S$ consists of $m$ valid encryptions of the same bit $b$ under $\pk$. Because $\Ran(\pk, c)$ is plaintext-preserving, the probability that $\prandenc$ is incorrect given $S$, i.e., outputs a ciphertext encrypting $\neg b$, is exactly the probability that one of the inserted ciphertexts encrypting $\neg b$ is chosen for rerandomization. This happens with probability $\frac{k}{m+2k}$, so taking $m > k/\beta = 1/(\epsilon\beta)$ ensures $\prandenc$ is correct except with probability $\beta$. 
\end{proof}

\subsubsection{Cryptographic Adversary from Reproducible Algorithm for $\prandenc$}\label{ssec:no-rep-randenc-algo}

In this subsection, we show that if there exists a reproducible polytime algorithm, $\mathcal{B}$, for $\prandenc$, instantiated with a randomizable encryption scheme $\escheme$, then there exists an adversary breaking the security guarantee of $\escheme$.

	\begin{algorithm}[H]
		\caption{$\mathcal{A}(\pk, c)$\\
			Input: public key $\pk$ \\ ciphertext $c$ encrypting bit $b$ \\
			Output: a bit $b'$
		}
		\label{alg:adversary}
		\begin{algorithmic}
			\STATE Draw a random string $r$
		    \STATE $c_0 \gets \Enc(\pk, 0)$
		    \STATE Generate a sample $S_0$ of $m$ ciphertexts by running $\Ran(\pk, c_0)$ $m$ times 
		    \STATE $c_0 \gets \mathcal{B}(\pk, S_0; r)$
		    \STATE Generate a sample $S$ of $m$ ciphertexts by running $\Ran(\pk, c)$ $m$ times
		    \STATE $c \gets \mathcal{B}(\pk, S; r)$
		    \IF{ $c_0 = c$}
		    \RETURN 0
            \ENDIF
            \RETURN 1
		\end{algorithmic}
	\end{algorithm}

\begin{lemma}~\label{lem:adversary}
Let $\escheme$ be a randomizeable encryption scheme, let $\prandenc^{\escheme}$ denote the instantiation of $\prandenc$ with $\escheme$. Let $\mathcal{B}$ be a $\rho$-reproducible algorithm for $\prandenc^{\escheme}$ with failure probability $\beta$, running in time $\poly(\lambda, \rho, \beta)$, and with sample complexity $m \in \poly(\lambda, \rho, \beta)$. Then there exists an adversary $\mathcal{A}$ running in time $\poly(\lambda, \rho, \beta)$ such that 
$$\Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 1)] - \Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 0)] \geq 1 - 4\beta - 2\rho.$$
\end{lemma}
\begin{proof}
The adversary $\mathcal{A}(\pk, c)$ outputs 1 whenever $c_0 \neq c$. The distribution from which $S_0$ is drawn is supported entirely on encryptions of $0$ and, conditioned on $c \gets \Enc(\pk, 1)$, the distribution from which $S$ is drawn is supported entirely on encryptions of $1$. Then $c_0 \neq c$ except when one of the two calls to $\mathcal{B}$ is incorrect, which happens with probability at most $2\beta$. Conditioned on $c \gets \Enc(\pk, 0)$, $S_0$ and $S$ comprise i.i.d. samples from the same distribution over encryptions of 0. In this case, $c_0 \neq c$ if either call to $\mathcal{B}$ is incorrect or fails to be reproducible, which happens with probability at most  $2\beta + \rho$. Therefore 
$$\Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 1)] - \Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 0)] \geq 1 - 4\beta - \rho .$$ 
\end{proof}

In particular, if there exists a $\rho$-reproducible algorithm $\mathcal{B}$ for $\prandenc^{\escheme}$ with failure probability $\beta < 1/8 - \poly(1/\lambda)$ and reproducibility parameter $\rho < 1/2 - \poly(1/\lambda)$, running in time $\poly(\lambda)$, and with sample complexity $\poly(\lambda)$, then there exists and adversary $\mathcal{A}$ running in time $\poly(\lambda)$ such that 
$$\Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 1)] - \Pr[\mathcal{A}(\pk, c) = 1 \mid c \gets \Enc(\pk, 0)] > \negl(\lambda),$$
and therefore $\mathcal{A}$ breaks the security of $\escheme$. 

\subsubsection{Instantiating $\prandenc$ with the Goldwasser-Micali Cryptosystem}
\label{ssec:gm}


\begin{definition}[Goldwasser-Micali Cryptosystem (\cite{STOC:GolMic82})]
\label{def:gol-mic}
The Goldwasser-Micali cryptosystem is a public-key cryptosystem comprising the following routines.
\begin{itemize}
    \item $\KeyGen(\lambda)$: Sample $p,q$ distinct primes of bit-length $O(\lambda)$ and let $N = pq$. Choose $x$ to be a quadratic non-residue modulo $N$. Let $\sk = (p, q)$, $\pk = (N, x)$, and output $(\sk, \pk)$.
    \item $\Enc(\pk, b)$: To encrypt a bit $b \in \{0,1\}$, sample $u \gets_{\mathcal{U}} \Z^{*}_N$ and output $u^2x^b \mod N$.
    \item $\Dec(\sk, c)$: To decrypt a ciphertext $c$, output $\bot$ if $\gcd(c,N) \neq 1$, $1$ if $c$ is not a quadratic residue modulo $N$ and 0 otherwise.  
\end{itemize}

\end{definition}

We now show that the Goldwasser-Micali cryptosystem satisfies the strong rerandomization property described above. We define the verification procedure $\VVV(\pk, c)$ to output 1 if $\gcd(c, N) = 1$ and 0 otherwise. We define $\Ran(\pk, c)$ to be the procedure that samples $u$ uniformly at random from $\Z_N^{*}$ and outputs $(\pk, u^2c \mod N)$.

\begin{lemma}
The Goldwasser-Micali cryptosystem is a rerandomizeable encryption scheme (\cref{def:rand-enc}), for the rerandomization procedure described above.
\end{lemma}

\begin{proof}
Because $\Dec(\sk, c) = \bot$ if and only if $\gcd(c, N) \neq 1$, and $\VVV(\pk, c) = 0$ if and only if $\gcd(c, N)$, $\VVV$ satisfies the requirement of \cref{def:rand-enc}. The rerandomization procedure multiplies a ciphertext $c$ by a random quadratic residue modulo $N$, and therefore preserves quadratic residuosity of $c$. This in turn preserves the plaintext message encrypted by $c$, and so $\Ran$ satisfies $\Dec(\sk, \Ran(\pk, c)) = \Dec(\sk, c)$. Finally, we observe that multiplying an invertible non-residue modulo $N$ by a quadratic residue gives a uniform distribution over non-residues. Similarly, $\Ran(\pk, c)$ for $c$ a quadratic residue modulo $N$ is uniformly distributed over residues modulo $N$. Then $d_{TV}(\Ran(\pk, c_1), \Ran(\pk, c_2)) = 0$ for all $c_1, c_2$ such that $\Dec(\sk, c_1) = \Dec(\sk, c_2)$, and $\Ran$ also satisfies the requirement of \cref{def:rand-enc}. Therefore, the Goldwasser-Micali cryptosystem satisfies \cref{def:rand-enc}.
\end{proof}

\newpage
\rexl{i'll take out the newpage when done writing this subsection}

\subsection{Pessiland Correlated Sampling}
\label{sec:owfi}

Correlated sampling algorithms give generic ways to create reproducible algorithms. 
Let $D_1, D_2$ be two distributions over $\X$ such that $\dtv(D_1,D_2)$ is small. A correlated sampling algorithm $\Bcal(D; r)$ for $\{D_1, D_2\}$ is a randomized algorithm that uses randomness $r$ to produce examples $x \in \X$ distributed according to distribution $D$. Furthermore, $\Bcal$ should produce the same example for two close distributions $D_1$ and $D_2$, when $\Bcal$ is run with the same random string $r$; i.e., $\Pr_{r} [\Bcal(D_1; r) \ne \Bcal(D_2;r)]$ should be proportional to $\dtv(D_1, D_2)$. 
More generally, a correlated sampling algorithm for distributions $\{D_i\}_i$ satisfies the second condition for all $D_j, D_k \in \{D_i\}_i$. 

\rexnote{The next paragraph is wordy and not direct enough. The point I'm trying to make is that correlated sampling can be used as a subroutine to make reproducible algorithms}
Say $\Acal(S; r)$ is a learning algorithm that takes as input a sample $S \sim D^m$ and uses internal randomness $r$ to produce an output. Let $D_1$ be the distribution of outputs of $\Acal(S_1; r)$ when sample $S_1$ is fixed, and analogously for $D_2$. 
\rexnote{Maybe more clearly, here describe why many algorithms can ensure that $D_1$ and $D_2$ are close.}
If $\Acal$ is designed such that $D_1$ and $D_2$ are close, then we can apply a correlated sampling algorithm to create a reproducible algorithm. 

In this Section, we show that the existence of one-way function inverters implies the ability to perform correlated sampling on arbitrary distributions (even with infinite support). Specifically, we show that (i) if there are no one-way functions, we can do average
case implicit correlated sampling, and (ii) if there are no non-uniform 
one-way functions, then there is polynomial time implicit correlated sampling.
\rexnote{Maybe add definitions for ``average-case implicit correlated sampling", `non-uniform owfs', and 'polynomial time implicit correlated sampling'}

\subsubsection{Motivation/Prior work on correlated sampling}


\rexnote{Somewhere (either in the paper intro or here), discuss prior attempts at achieving correlated sampling; also explain how our additional assumption leads to a better qualitative/quantitative correlated sampler}
klineberg/tardos + 2 other author groups have (independently?) wrote papers including correlated sampling over finite sets (uniform dist); in their works, they discuss rejection sampling (pick an element in $\Omega$, and find its probability; accept if a random coin is less than that probability) as a method to correlated sample. I think where that method fails in comparison to ours (with the owfi assumption) is their algorithm is ???
\rexnote{Actually, what benefit do we get over their algorithm by assuming owfi's exist?}

paper discussing correlated sampling: \url{https://theoryofcomputing.org/articles/v016a012/v016a012.pdf}

``Pessiland was introduced in ... and  refers to ....."

\subsubsection{Definitions/Preliminaries}
\label{ssec:owfi-prelims}


%We use circuits to model distributions.
\rexnote{Using circuits to model distributions? Is this ``cheating", by assuming distributions can all be approximated in this way? I guess not, since we are allowing $\nu$ slack in the approximations. 
i.e.: justify the following definition with 1-2 sentences
}

\begin{definition}[Implicit Correlated Sampling Algorithm]
\label{def:implicit-correlated-sampling-problem}
Let $m, n \in \Z^+$, and let $C: \{0,1\}^m \rightarrow \{0,1\}^n$ denote a circuit. 
Let error parameter $\nu >0$. 
$\Bcal(C, \nu; r)$ is an  \emph{$(m,n,\nu)$-implicit correlated sampling algorithm} if the following conditions hold:
\begin{enumerate}
    \item \textbf{Inputs/Outputs:} $\Bcal$ takes as input a circuit $C:\{0,1\}^m \rightarrow \{0,1\}^n$, a distributional error parameter $\nu$, and a random string $r$. $\Bcal$ outputs a string in $\{0,1\}^n$.  
    
    \item \textbf{$\nu$-distributional accuracy:} For all circuits $C: \{0,1\}^m \rightarrow \{0,1\}^n$, the distributions $D_C$ and $D_{\Bcal(C, \nu)}$ satisfy $\dtv(D_C, D_{\Bcal(C, \nu)}) \le \nu$.
    
    Here, $D_C$ denotes the distribution over $\{0,1\}^n$ induced by querying $C$ on  uniformly random inputs, i.e., probability density function $p_{D_C}(x) = \Pr_{r \sim U^m}[C(r) = x]$. 
    Similarly,
    $D_{\Bcal(C, \nu)}$ denotes the distribution over $\{0,1\}^n$ induced by querying $\Bcal(C, \nu; r)$ with uniformly random strings $r$.
%    \rexnote{The previous notational clarification sentence could be written more precisely/clearly}
    
    \item \textbf{Correlated sampling:} For all pairs of circuits $C_1, C_2: \{0,1\}^m \rightarrow \{0,1\}^n$, 
    $\Pr_{r}[\Bcal(C_1, \nu; r) \ne \Bcal(C_2, \nu; r)] \in O(\dtv(D_{C_1}, D_{C_2}) + \nu)$.
\end{enumerate}
\end{definition}

%In the implicit correlated sampling problem, the input is a circuit $D$ that uses $m$ bits of randomness to produce an output in $\{0,1\}^n$, i.e., $D: \{0,1\}^m \rightarrow \{0,1\}^n$.  Given an error parameter $\nu$, we want a sampling algorithm $\sc{S}(\nu, D, rand)$ whose distribution is within $\nu$ statistical distance of $D$, and so that  for any two such sampling circuits, $D_1,D_2$,  $Prob_{rand} [\sc{S}(\nu, D_1, rand) \neq \sc{S} (\nu , D_2 , rand)] \le O( SD(D_1,D_2)+ \nu)$.


We assume we can invert any one-way function on almost all inputs.  For now,
we will assume that there is no non-uniform one-way function family, so
that there is a uniform way of inverting any circuit computing a function via a polynomial-time inverter. 
%In other words, there is a polynomial-time  inverter $I(F, \nu, y)$ so that for any circuit $F: \{0,1\}^M\leftarrow \{0,1\}^N$ , $Prob_{R \in \{0,1\}^M} [F(I(F, \nu, F(R)))=F(R) ] \ge 1 - \nu$.  

\begin{definition}[Uniform One-Way Function Inverters]
\label{def:inverter-unif}
Let $\nu > 0$. $\Ical_{\nu}(C, y)$ is a \emph{polynomial-time uniform one-way function inverter with error $\nu$} if, for any circuit $C:\{0,1\}^m \rightarrow \{0,1\}^n$, 
$\Pr_{r \sim \{0,1\}^m} [C(\Ical_{\nu}(C, C(r)))=C(r) ] \ge 1 - \nu$, and $\Ical$ runs in polynomial time in $m$, $n$, and $1/\nu$.
\end{definition}

In this argument, we assume that $C(r)$ can be efficiently computed. Thus, we can check if and when the inverter succeeded. For notational convenience, we say that the inverter $\Ical$ returns ``$\perp$'' if does not succeed. 


Our correlated sampler uses pairwise-independent hash families in its subroutines. 

\begin{definition}[Pairwise-Independent Hash Family]
\label{def:pairwise-independent-hash-family}
A family of Boolean functions $\mathcal{H} = \{H | H: \{0,1\}^m \rightarrow \{0,1\}^n \}$ is \emph{pairwise-independent} if, for all $r_1 \ne r_2 \in \{0,1\}^m$ and $x_1, x_2 \in \{0,1\}^n$, 
$\Pr_{h \in H} [H(r_1) = x_1 \land H(r_2) = x_2 ] = 2^{-2n}$.
\end{definition}


\subsubsection{Correlated Sampling Algorithm Overview}

A correlated sampling algorithm $\Bcal$ accomplishes two goals. First, $\Bcal(C, \nu; r)$ needs to accurately sample from the distribution $D_C$. Second, $\Bcal$ convert random string $r$ into the same output when run on circuits $C_1$ and $C_2$, with high probability.  In other words, $\Bcal$ must choose a consistent way to map random strings $r$ to elements in the support of $D_C$. 

Our algorithm $\correlatedsampler$ accomplishes both these tasks as follows. $\correlatedsampler$ divides the distribution $D_C$ into levels, such that each level contains elements in the support with close probability density. 
After randomly picking a level, the algorithm uses the inverter to randomly sample an element $x \in \{0,1\}^n$ in the support of the distribution and estimate $x$'s probability density. Finally, $\correlatedsampler$ uses rejection sampling and random coin flips to determine whether to output $x$ or try again from the beginning. 

The one-way function inverter $\Ical$ enables our algorithm $\correlatedsampler$ to efficiently sample elements $x$ and estimate their probability density. We consider the circuit $F_{C, l, H_1, H_2}(r) \eqdef H_1(C(r)) \mathbin\Vert H_2(r)$, where $H_1$ and $H_2$ are hash functions randomly chosen from families of pairwise-independent hash functions, and $\mathbin\Vert$ denotes concatenation. Our algorithm asks the inverter $\Ical$ to invert multiple instances of random strings (of length equal to the output length of $F$), thereby generating samples that can be used to pick an $x \in \{0,1\}^n$ and estimate its probability. Furthermore, the pairwise-independent property of the hash functions ensures a reasonable probability of finding a unique $x$ after inverting the hash. 



\subsubsection{Algorithm Description and Pseudocode 
}
\label{ssec:owfi-construction}

Algorithm $\correlatedsampler$ repeatedly uses the inverter $\Ical$ to determine an element $x$ and a probability $q_x$. By running the inverter on random hash functions and random strings, the algorithm can sample $x$ proportional to the circuit distribution $D_C$ with high probability. Then, $\correlatedsampler$ performs rejection sampling by a random coin flip, resulting in either returning $x$ or trying again from the start. $\correlatedsampler$ calls subroutine $\elementfinder$ multiple times, which in turn calls subroutine $\hashchecker$ multiple times.

First, $\correlatedsampler$ fixes a slack parameter $k \in \Theta( \log (m)+ \log (1/\nu))$ that determines the dimensions of the hash functions. In each loop, it 
picks $\beta \in [1/2, 1]$ uniformly and divides the support into $m$ levels 
$X_{\ell, \beta} = \{x | \beta 2^{- \ell-1} < p_D(x) \le \beta 2^{-\ell}\}$, 
where $p_D(x)$ is the probability that $x \in \{0,1\}^n$ is output by circuit $C$.
 (\rexnote{Later, check that boundary conditions when $\ell=0$ are fine})
Algorithm $\correlatedsampler$ then samples a level $\ell$ uniformly, and then it attempts to output an $x$ is in $X_{\ell, \beta}$  proportionally to $p_D(x)$.  
The inverter $\Ical$ is used to simultaneously  determine $x \in X_{\ell, \beta}$  and estimate $p_D(x)$.    


\rexnote{If we ever get a specific value for $k$, then explicitly say that in the beginning of each of the following algorithms (rather than just leaving it in $\theta$ notation)}
\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{$\correlatedsamplerfull$\\
			Input: Circuit $C: \{0,1\}^m\rightarrow \{0,1\}^n$, distributional error parameter $\nu$, and random string $r$\\
			Output: An element $x \in \{0,1\}^n$
		}
		\label{alg:correlatedsampler}
		\begin{algorithmic}
		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$

            \WHILE{1}
    
            
		        \STATE $\beta \gets_r [1/2,1]$
	%	    \rexnote{Clarify if $\beta$ gets picked just once, or if it gets repicked each time $l$ gets repicked}
    %        \rexnote{How many bits of precision for $\beta$ are necessary? Add a justification argument for this in analysis section}
    
    
                \STATE $\ell \gets_r \{0,1,\dots, m\}$
    		       
    		    \STATE $H_1 \gets_r $ pairwise independent hash function from $n$ bits to $l+k$ bits 
    		        
    		    \STATE $u \gets_r \{0,1\}^{\ell+k}$
                
                \STATE $(x,q) \gets \elementfinder_{C, \nu, \ell, \beta} (H_1, u; r)$
                
                \IF{$(x,q) \ne \perp$}
                
                    \STATE $\coin \gets_r [0,1]$
                    \IF{$\coin \le q 2^k$}
                        \RETURN $x$
                    \ENDIF
                \ENDIF
            \ENDWHILE

		\end{algorithmic}
	\end{algorithm}
\end{figure}

Consider the circuit $\Fcircuit (r) = H_1(C(r)) \mathbin\Vert H_2(r)$, where $H_1$ is a uniformly chosen pairwise independent hash function mapping $n$ bits to $\ell + k$
bits and $H_2$ is a uniformly chosen pairwise independent  hash function mapping $m$ bits to $m- \ell +k$ bits. $F$ maps strings of length $m$ to strings of length $m+2k$. 
We run inverter $\Ical$ on random instances by randomly choosing hash functions $H_1$ and $H_2$ and randomly choosing a concatenated string $u \mathbin\Vert v$ of length $m+2k$ to invert.
If $\Ical$ does not find any valid preimage of the function $F$,
% usually because no preimage exists, 
$\Ical$ returns $\perp$. 
%\rexnote{Should we just say, we can check if the returned result by $\Ical$ is a correct inversion, by running the circuit $C$ and hash functions? Or is that too much time complexity?}

Consider the pair $(C(r),r)$. Intuitively, there should be a low probability that, for random hash functions $H_1$ and $H_2$, that there exists another pair $(C(r'), r')$ such that $C(r) \ne C(r')$ but $H_1(C(r)) = H_1(C(r'))$ and $H_2(r) = H_2(r')$. Thus, for a fixed $(H_1, u)$ pair, running the inverter $\Ical$ on $F$ generated by random pairs $(H_2, v)$ should give a good estimate of the relative probability of $H_1^{-1}(u)$ in distribution $D_C$ (with high probability). 

For a fixed $H_1$ and $u$, subroutines $\elementfinderfull$ and $\hashcheckerfull$ run the inverter multiple times. $\elementfinder$ picks  
$16 (2^k) (1/\nu^2) \log (1/\nu)$ pairs
of hash functions $H^i_2$ and strings $v^i$. 
For each pair, it runs $\hashchecker_{C, \nu, \ell, H_1, u} ( H_2^i, v^i)$, checking if there is a valid inversion of the induced circuit $F$. 
Then, $\elementfinder$ sees if there is a unique $x \neq \perp$ such that $x$ appears in a $((\beta/2) 2^{-l}, \beta 2^{-l})$-fraction of times in the list.  If so, it returns the pair $(x,q_x)$ where $q_x$ is the fraction of times that $x$ was returned by $\sc{A}$. Otherwise, $\elementfinder$ returns $\perp$. 





\begin{figure}[H]
	\begin{algorithm}[H]
		\caption{$\elementfinderfull$\\
		    (Explicit) Input: Hash function $H_1: \{0,1\}^{n} \rightarrow \{0,1\}^{\ell+k}$, string $u \in \{0,1\}^{\ell+k}$, and random string $r$\\
			(Implicit) Input: Circuit $C: \{0,1\}^m \rightarrow \{0,1\}^n$, distributional error parameter $\nu$, integer $\ell \in \{0, 1, \dots, m\}$, interval rescaling parameter $\beta$\\
			Output: String $x \in \{0,1\}^n$ and probability $q_x \in ((\beta/2)2^{-k}, \beta 2^{-k}]$.
		}
		\label{alg:elementfinder}
		\begin{algorithmic}
		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$
		    \FOR{$i=1$ to $i = 16 (2^k) (1/\nu^2) \log (1/\nu)$}
		    
		        \STATE $H^i_2 \gets_r $ pairwise independent hash function from $m$ bits to $m-\ell+k$ bits 
		        
		        \STATE $v^i \gets_r \{0,1\}^{m-\ell+k}$

                \STATE Run $\hashchecker_{C, \nu, \ell, H_1, u} ( H_2^i, v^i; r)$    
		    \ENDFOR
		    
		    \STATE Let $\widehat{q}_x$ denote the fraction of times $x$ was returned by $\hashchecker$
		    
		    \IF{$\exists$ unique $x$ s.t. $x \ne \perp$ and $\widehat{q}_x \in ((\beta/2)2^{-\ell}, \beta 2^{-\ell}]$}
		    
		        \RETURN $(x, \widehat{q}_x)$
		    \ELSE
		        \RETURN $\perp$
		    \ENDIF
		\end{algorithmic}
	\end{algorithm}
\end{figure}





In the following algorithm, `$\mathbin\Vert$' denotes concatenation of strings. 


\begin{figure}[H]
	\begin{algorithm}[H]
	\caption{$\hashcheckerfull$\\
		    (Explicit) Input: Hash function (circuit) $H_2: \{0,1\}^{m} \rightarrow \{0,1\}^{m-\ell+k}$, string $v \in \{0,1\}^{m-\ell+k}$, and random string $r$\\
			(Implicit) Input: Circuit $C: \{0,1\}^m \rightarrow \{0,1\}^n$, distributional error parameter $\nu$, integer $\ell \in \{0, 1, \dots, m\}$, hash functions $H_1: \{0,1\}^{n} \rightarrow \{0,1\}^{\ell+k}$ and string $u \in \{0,1\}^{\ell+k}$\\
			Output: String $x \in \{0,1\}^n$
		}
		\label{alg:hashchecker}
		\begin{algorithmic}
%		    \STATE $k \gets \Theta( \log (m)+ \log (1/\nu))$

		    \STATE Define circuit $\Fcircuit (r) = H_1(C(r)) \mathbin\Vert H_2(r)$ 

		    %map strings of length $m$ to strings of length $m+2k$.
		    
		    \STATE Let $y \gets u \mathbin\Vert v$ 
	        \STATE $x \gets \Ical_{\nu} (\Fcircuit, y; r)$ 
	        
	        \IF{$x = \perp$}
		        \RETURN $\perp$
%		        \rexnote{To do: (somewhere in this section) explain why and when the inverter can return $\perp$}
		    \ELSE
		        \RETURN $C(x)$
		    \ENDIF
		\end{algorithmic}
	\end{algorithm}
\end{figure}


In summary, $\correlatedsampler$ randomly chooses a hash function $H_1$ and substring $u$ to invert. The algorithm calls subroutine $\elementfinder$ to estimate the probability that $H_1^{-1}(u)$ appears in the distribution $D_C$, assuming the inverse is unique. Finally, given an element $x$ and relative probability estimate $\widehat{q}_x$, $\correlatedsampler$ returns with probability $\widehat{q}_x2^k$ or retries from the beginning. 







\subsubsection{Analysis}

\rexnote{This portion of the argument needs to be expanded (mostly unchanged from Russell's first draft)}

\textbf{Distributional accuracy analysis:}
Fix $x \in \X_{\beta, \ell}$.  For a random $H_1, U$, there is a 
probability $2^{-(\ell +k)}$ that $U=H_1(x)$.  There are
$p_D(x)2^m$ random strings $R$  so that $C(R)=x$, and for
a random $H_2, V$, each of these satisfy $H_2(R)=V$ with probability
$2^{-(m-\ell +k)}$.   
Thus, $q$  is expected to be close to $p_D(x)2^m 2^{-(m -\ell +k)}=
p_D(x)2^{\ell}/2^k $.  

So to get a good intuition for why we expect this to work, let's
assume  that each $\sc{B}_{C,\ell,\beta}$ had this ideal behavior:  
For each $x$ in $X_{\ell, \beta}$, with probability exactly
$2^{-(\ell+k)}$, it returns the pair $(x, p_D(x)2^{\ell}/2^k)$
and otherwise it returns $NIL$. 
We will then show that the actual procedure is a close enough
approximation to this that the error introduced is small.

If the ideal situation above were true, consider any  $x$ in the
support of $D$.  $x$ is in $X_{C, \ell, \beta}$ for exactly one $\ell$.
So 
in one run of $\sc{S}_{C,\beta}$  we will pick this $\ell$ with probability
$1/(m+1)$, and if we do, $\sc{B}_{C,\ell, \beta}$ would return $(x,q)$ with probability $2^{-(\ell +k)}$.  Then we return $x$ with probability $q2^k= p(x)2^{\ell-k} 2^k= p(x)2^{\ell}$.  So the overall probability of returning $x$ in
one run is $1/(m+1) 2^{-(\ell+k)}p_D(x)2^{\ell}= p_D(x)/((m+1)2^k)$.  Thus,
the probabilities of each outcome are proportional to the probability under
$D_C$, so the conditional distribution is exactly that for $D_C$.   
Also, the overall probability of not returning $NIL$ is  $\sum_x p_D(x) /(m+1)2^k= 1/(m+1)2^k$ , so we expect to have $O(m2^k)$ runs before returning a sample.


\textbf{Correlation analysis:}
\rexnote{Add a pointer to appendix discussing randomness management, saying that we can ensure the correlated sampler uses the proper bits of randomness for each piece of the algorithm}
If $x$ is in the same level $X_{C_1, \ell, \beta}$ and
$X_{C_2 , \ell, \beta}$, then under the ideal conditions,
the probability of returning $x$ in one run of $\sc{S}_{C_1, \beta}$
but not returing $x$ in the corresponding run of $\sc{S}_{C_2, \beta}$ is
the probability of returning $x$ times
the probability that $p_2(x)2^{\ell} = q_2 2^k < a < q_1 2^k=p_1(x)2^{\ell}$,
or $p_1(x)/(m+1)2^k* |p_1(x)-p_2(x)|2^{\ell}$. Since in this case, $p_1(x) \le
2^{-\ell}$, this is at most $1/(m+1)2^k |p_1(x)-p_2(x)|$.  Summing up over
all $x$, this gives a bound of $SD(D_{C_1},D_{C_2})/(m+1)2^k$  
of the answer returned by the first sampler not being returned by the second
for such $x$.
Conditioned on a non-NIL answer, this is at most a $SD(D_{C_1},D_{C_2})$ probability
of a difference. 

We now need to bound the expected 
total probability of $x$ so that $x$ is not in the same levels  $X_{C_1, \ell,
\beta}$ and $X_{C_2,\ell,\beta}$.  
Say that $p_1(x) = \gamma_1 2^{-\ell}$ with $1/2 < \gamma_1 \le 1$
and $p_2(x)= \gamma_2 2^{-\ell}$ with $\gamma_2 \le \gamma_1$. 
Then if $ \gamma_2 < \beta < \gamma_1$, they will be in different levels, 
or if $\gamma_2 < \beta/2 < \gamma_1 $. The latter can happen only if
$\gamma_2 < \gamma_1/2$, in which case $|p_1(x)-p_2(x) |2^{\ell} > 1/2 p_1(x) 2^{\ell} > 1/4$, so the probability in either case    
of being in separate levels is at most $O(|p_1(x)-p_2(x)|2^{\ell})$.  
Then the contribution to the expected probability of elements in distinct
levels is $O(|p_1(x)-p_2(x)|2^{\ell} p_1(x)) = O(|p_1(x)-p_2(x)|)$.
Thus,the total expected weight of elements in $D_1$ in distinct levels is 
$O(SD(D_{C_1},D_{C_2}))$.
Since the sampler produces the same distribution as $D_{C_1}$, the probability
that we output such an element is $O(SD(D_{C_1},D_{C_2}))$.  


\newpage
\rexl{i'll take out the newpage when done writing the preivoius subsection}






\section{Separating Stability: Statistical Barriers}
\subsection{Quadratic separation: One-way marginals}
We start by defining the one-way marginals problem over $d$ coordinates, which corresponds to outputting a good estimate of the expectation of a product of Rademacher distributions in $\ell_{\infty}$-distance.
\begin{definition}
Consider a product of $d$ Rademacher distributions with expectations $p = (p_1,\dots,p_d)$ respectively. A vector $v \in \mathbb{R}^d$ is said to be an $\alpha$-accurate solution to the one-way marginals problem if $\|v - p\|_{|\infty} \leq \alpha$.
\end{definition}
\begin{definition}
Fix any distribution that is the product of $d$ Rademacher distributions with biases $p = (p_1,\dots,p_d)$ respectively. We say that an algorithm $(\alpha,\beta)$-accurately solves the one-way marginals problem over this distribution if it observes samples from the distribution, and with probability at least $1-\beta$ (over the randomness of the samples and the algorithm), produces an $\alpha$-accurate solution $v \in \mathbb{R}^d$.
\end{definition}
In this section, we show that any $0.0001$-reproducible, $(0.01,0.01)$-accurate algorithm for the one-way marginals problem over $d$ coordinates requires at least $\Omega(d)$ samples.

On the other hand, under the constraint of $(1, \frac{1}{n^2})$-differential privacy, this problem can be solved using $O(\sqrt{d})$ samples, via the Gaussian mechanism. This gives a quadratic separation between differential privacy and reproducibility, and proves that our reduction is asymptotically tight (up to logarithmic factors) in some settings (since our reduction would give a $\tilde{O}(d)$-sample reproducible algorithm for this task). 
\subsubsection{Sketch of our approach}
Our techniques for proving the lower bound for reproducibility draw inspiration from those used to prove lower bounds in privacy. Specifically, tight lower bounds for the one-way marginals problem over $d$ coordinates under the constraint of differential privacy are obtained using the \textit{fingerprinting method} \cite{DworkSSUV15, BunUV18, BunSU19}. The fingerprinting method captures the idea that there is a trade-off between accuracy and correlation with the input sample. If quantifies the idea that if the algorithm obtains a sample of small size, and is also very accurate, then it must be heavily correlated with one of its input examples, which is prohibited by differential privacy. Our intuition was that since reproducibility also aims to capture the idea that an algorithm is not too correlated with its input sample, the same method might be useful in proving lower bounds for reproducibility.

More formally, given an Algorithm $\mathcal{A}$ solving the one-way marginals problem, the \textit{correlation} of coordinate $j$ of the output with the input sample $\vec{s}$ can be measured by the quantity $Z=\sum_j \mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j)$. Note that the quantity $(s^j_i - p_j)$ represents the drift between an input example coordinate and the expectation of the distribution it's drawn from. $\mathbb{E}[Z]$ is large when on average, for many $j$, $\mathcal{A}^j(\vec{s})$ is on the same side of $0$ as the drift $(s^j_i - p_j)$, implying that the algorithm's outputs are on average correlated with its input. %For sake of comparison, for a freshly drawn dataset $X'$, we consider $Z'=\sum_j \mathcal{A}^j(X)\sum_i (X'^j_i - p_j)$. We'd expect that $\mathbb{E}[Z']$ is small since $X$ and $X'$ are independent. 

We now recall the formal statement of the fingerprinting lemma.

\begin{lemma}[Fingerprinting Lemma, Lemma 3.6 in \cite{BunSU19}]
\label{lem:fingerprinting}
Let $f$ be any function from $\{-1,1\}^m \to [-1,1]$. Suppose $r$ is sampled from the uniform distribution over $[-1,1]$ and $q \in \{-1,1\}^m$ is a vector of $m$ independent Rademacher RVs each with expectation $r$. Then, if $\mu_q$ is the empirical average of $q$, we get that
$$\mathbb{E}_{r,q}[f(q)\sum_i (q_i - r) + 2|f(c) - \mu_q|] \geq \frac{1}{3}.$$
\end{lemma}

The lower bound for differential privacy proceeds by arguing that $\mathbb{E}[Z] = \mathbb{E}[\sum_j \mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j)]$ is large (via an appropriate application of the fingerprinting lemma), and hence, by an averaging argument, there is an index $i^* \in [m]$ such that $\mathbb{E}[Z_{i^*}] = \mathbb{E}[\sum_j \mathcal{A}^j(\vec{s}) (s^j_{i^*} - p_j)]$ is large. You then consider an independently drawn example $g=g^1_{i^*},\dots,g^d_{i^*}$, and consider the dataset $\vec{s'}$ obtained by replacing the single example $s_i^*$ in the original dataset with the replacement $g$. Then, considering the quantity $Z_{i^*}' = \sum_j \mathcal{A}^j(\vec{s'}) (X^j_{i^*} - p_j)$, we'd have that $\mathcal{A}^j(\vec{s'})$ is uncorrelated with $(s^j_{i^*} - p_j)$, and hence the quantity $\mathbb{E}[Z_{i^*}']$ is small. On the other hand, by differential privacy, $Z_{i^*}$ is distributionally close to $Z'_{i^*}$, and hence $\mathbb{E}[Z'_{i^*}]$ and $\mathbb{E}[Z_{i^*}]$ can't be too far from each other (and $\mathbb{E}[Z_{i^*}]$ is large). Balancing these considerations gives a lower bound on the number of samples needed for differential privacy.

For reproducibility, our idea is to obtain a stronger lower bound by following a similar logic but not applying an averaging argument. Specifically, for $Z$ defined as in the previous paragraph, we can argue that $\mathbb{E}[Z]$ is large (as we would for the differential privacy lower bound). Then, we can consider a freshly sampled dataset $\vec{s}'$ (drawn from a product distribution with the same expectation $p=(p_1,\dots,p_d)$), and consider the quantity $Z'=\sum_j \mathcal{A}^j(\vec{s'})\sum_i (s^j_i - p_j)$. We can argue that $Z'$ is a sum of uncorrelated random variables, and hence the quantity $\mathbb{E}[Z']$ is small. But, by reproducibility, $Z$ and $Z'$ are distributionally close, since they correspond to post-processing of the algorithm applied to independent datasets. Note that differential privacy would not give that $Z$ and $Z'$ are distributionally close, since datasets $\vec{s}$ and $\vec{s'}$ can differ in many entries. Now, following a similar approach to the differential privacy lower bound, we'd get a stronger lower bound for reproducibility (since we have eliminated the averaging argument).

However, this approach does not work for technical reasons. Specifically, $\rho$-reproducibility tells us that $Z$ and $Z'$ are distributionally close, but their expectations can have absolute value difference as large as $\rho dm$ (since $\mathcal{A}(\vec{s})$ and $\mathcal{A}(\vec{s'})$ could differ completely with probability $\rho$). Since we are interested in constant $\rho$, this turns out to be too large for the lower bound technique to work.

We deal with this by instead applying the fingerprinting method to prove a lower bound against $(\frac{1}{m^3},1,\frac{1}{m^3})-$\textit{perfectly generalizing} algorithms (this lower bound is interesting in its own right since it gives the first sample complexity separation between differential privacy and perfect generalization). Perfect generalization roughly asks that the algorithm's output distributions be $(1, \frac{1}{m^3})$-close on two independent datasets drawn from the product distribution. This can be used to argue that $\mathbb{E}[Z]$ is within $\frac{1}{m^2}$ in absolute value to $e \mathbb{E}[Z']$ (i.e. close in terms of absolute value difference to a constant multiple of $\mathbb{E}[Z']$, as opposed to $\mathbb{E}[Z']$ itself), which turns out to be sufficient for the lower bound technique to apply.

Finally, appealing to our method of converting a reproducible algorithm to a perfectly generalizing algorithm, we can extend this to get the lower bound for reproducible algorithms (that is tight up to logarithmic factors in the number of coordinates $d$). These logarithmic factors creep in due to the conversion of a reproducible algorithm to a perfectly generalizing one. It is an interesting problem to figure out if there is a more direct way of proving a lower bound on the number of samples needed by a reproducible algorithm solving the one-way marginals problem (without going through perfect generalization), that would hopefully avoid these logarithmic factors.
%Next, we define the notion of perfect generalization, a notion of distributional generalization defined in prior work on adaptive data analysis. \sstext{Add more exposition here.}

%\begin{definition}[\cite{CummingsLNRW16}]
%An algorithm $A: \mathcal{Y}^n \to \mathcal{R}$ is said to be $(\beta,\epsilon,\delta)$-perfectly generalizing, if for every distribution $P$ over $\mathcal{Y}$, there exists a distribution $SIM_P$ such that with probability at least $1-\beta$ over the draw of an i.i.d. sample $X \sim P^n$, $A(X) \approx_{\epsilon, \delta} SIM_P$.
%\end{definition}

%Cummings et al. \cite{CummingsLNRW16} show that perfect generalization implies the following.

%\begin{lemma}[\cite{CummingsLNRW16}]
%If algorithm $A: \mathcal{Y}^n \to \mathcal{R}$ is $(\beta,\epsilon,\delta)$-perfectly generalizing, then for every distribution $P$ over $\mathcal{Y}$, with probability $1-\beta$ over the draw of two i.i.d. samples $X_1,X_2 \sim P^n$, we have that $A(X_1) \approx_{2\epsilon,3\delta} A(X_2)$.
%\end{lemma}
\subsubsection{Formal argument}
We start by proving the lower bound for perfectly generalizing algorithms. 

\begin{theorem}\label{thm:PGmarginalsLB}
Fix any $m > 0$. Let $\mathcal{A}$ be a $(\frac{1}{m^3},1,\frac{1}{m^3})$-perfectly generalizing, $(0.01,0.01)$-accurate algorithm for the 1-way marginals problem over $d$ attributes using $m$ samples. Then, $m = \Omega(d)$. 
\end{theorem}
\begin{proof}
Assume without loss of generality that $m = \Omega(\log d)$ (We will show under this condition that $m = \Omega(d)$, which would imply that there cannot be an algorithm taking $O(\log d)$ samples, because if it was, then there would be an algorithm with number of samples between $O(\log d)$ and $O(d)$, which would give a contradiction).

Fix any distribution $D_p$ that is a product of Rademachers with expectation $p = (p_1,\dots,p_d)$. Let $\vec{s}$ be drawn from $D_p^m$. Note that we can assume without generality that the algorithm $\mathcal{A}$ outputs values between $[-1,1]$ (else, we can round its outputs, which only improves the accuracy and doesn't affect perfect generalization, which is robust to post-processing). If $\mathcal{A}$ is $(0.01,0.01)$-accurate, then we can say that with probability at least $99/100$, for all $j \in [d]$, $|\mathcal{A}^j(\vec{s}) - p_j| \leq \frac{1}{100}$. Taking expectation, we get that 
$\mathbb{E}[\sum_{j \in [d]} |\mathcal{A}^j(\vec{s}) - p_j|] \leq \frac{3d}{100}$. By a Chernoff bound, we can argue that since $m = \Omega(\log d)$, $\max_j |p_j - \mu_j| \leq 0.01$ with probability at least $0.99$, where $\mu_j$ is the empirical average of the $j^{th}$ column of the dataset. By the triangle inequality, this gives us that $\mathbb{E}[\sum_{j \in [d]} |\mathcal{A}^j(\vec{s}) - \mu_j|] \leq \frac{6d}{100}$. Since this holds for a fixed product of Rademachers, it also holds when the expectation of the Rademacher random variables are chosen at random.
%\sstext{Add in that coordinate means close to true means whp here.} 
\\\\
Next, conditioned on any set of fixed coin tosses $r$ of algorithm $\mathcal{A}$, apply the fingerprinting lemma to the function corresponding to the $j^{th}$ coordinate of the output of $\mathcal{A}$, when run on the $j^{th}$ column of the input, with the other columns set to any fixed values. Then, we get that 
$$\mathbb{E}_{p_j \sim [-1,1],\vec{s}^j \sim Rad(p_j)^m}[\mathcal{A}^j(\vec{s}; r)\sum_i (s^j_i - p_j) + 2|\mathcal{A}^j(\vec{s}; r) - \mu_j|] \geq \frac{1}{3}.$$ 
Since this is true for all fixed coins of the algorithm and fixed values of the other columns, by the law of total expectation, it is also true for random coin tosses and any distribution over the values of the other columns, and we get that 
$$\mathbb{E}_{p,\vec{s} \sim D_p^m,\mathcal{A}}[\mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j) + 2|\mathcal{A}^j(\vec{s}) - \mu_j|] \geq \frac{1}{3},$$
where we have used that $\vec{s}$ is drawn from a product distribution.
Summing over all $j \in [d]$, we get that 
$$\mathbb{E}_{p,\vec{s}, \mathcal{A}}[\sum_{j \in [d]|}\mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j) + 2|\mathcal{A}^j(\vec{s}) - \mu_j|] \geq \frac{d}{3}.$$
Using the implications of accuracy discussed in the first paragraph, we can simplify this to obtain that 
$$\mathbb{E}_{p,\vec{s},\mathcal{A}}[\sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j)] \geq \frac{d}{10}.$$
Consider a freshly sampled dataset $\vec{s'} \sim D_p^m$, and let $Z = \sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j)$ and $Z' = \sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s'^j_i - p_j)$.

First, note that $Z'$ is a sum of uncorrelated random variables with mean $0$. To see this, consider random variables $M = \mathcal{A}^j(\vec{s}) (s'^j_i - p_j) $, and $N = A^j(\vec{s}) (s'^j_{i'} - p_j)$. We claim that $\mathbb{E}[M] = \mathbb{E}[N]=0$. This is by the following sequence of inequalities (we prove this for $M$, the same argument holds for $N$).
\begin{align*}
\mathbb{E}[M]= & \mathbb{E}_p \mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}} [M \mid p] \\
& = \mathbb{E}_p \mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}} [ \mathcal{A}^j(\vec{s}) (s'^j_i - p_j) \mid p] \\
& = \mathbb{E}_p \Big[ \mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}} [ \mathcal{A}^j(\vec{s}) \mid p] \text{  }\mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}}[ (s'^j_i - p_j) \mid p] \Big] = 0,
\end{align*}
where the last equality follows because  because conditioned on the vector $p$, the expectation of the Bernoulli $s^j_{i}$ is exactly equal to $p_j$. Hence, by linearity of expectation, we get that the expectation of $Z'$ is also $0$.

Next, we show that $M$ and $N$ are uncorrelated. This is by the following argument: $\mathbb{E}_{p,\vec{s},\vec{s'},\mathcal{A}}[MN] = \mathbb{E}_p \mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}} [MN \mid p]$. Conditioned on $p$, $\vec{s}$ and $\vec{s'}$ are independent, and hence we get that $\mathbb{E}_p \mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}} [MN \mid p] = \mathbb{E}_p \Big[ \mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}} [\mathcal{A}^j(X)^2 \mid p] \text{  }\mathbb{E}_{\vec{s},\vec{s'},\mathcal{A}}[  (s'^j_i - p_j)  (s'^j_{i'} - p_j) \mid p] \Big]$. Conditioned on $p$, $(s'^j_i - p_j)$, and  $(s'^j_{i'} - p_j)$ are both independent (since $s'^j_i$ and $s'^j_{i'}$ are independent Bernoulli random variables drawn with the same bias) and hence $\mathbb{E}[  (s'^j_i - p_j)  (s'^j_{i'} - p_j) \mid p] ] = \mathbb{E}[  (s'^j_i - p_j) \mid p] \text{  }\mathbb{E}[  (s'^j_{i'} - p_j) \mid p] ] = 0$, where the equality to $0$ is because conditioned on the vector $p$, the expectation of the Bernoulli $s'^j_{i'}$ is exactly equal to $p_j$. We also have that $\mathbb{E}[M]\mathbb{E}[N]=0$ (since $\mathbb{E}[M]=0$), giving us that $M$ and $N$ are uncorrelated. A similar argument works for other pairs of random variables in the sum representing $Z'$.

We can then write that
\begin{align*}
\mathbb{E}^2[|Z'|] & = \mathbb{E}_{p,\vec{s},\vec{s'},\mathcal{A}}^2\Big[\Big| \sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s'^j_i - p_j) \Big| \Big] \\
& \leq \mathbb{E}_{p,\vec{s},\vec{s'},\mathcal{A}}\Big[\Big(\sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s'^j_i - p_j)\Big)^2\Big] = Var(Z') \\
& = \sum_j \sum_i Var(\mathcal{A}^j(\vec{s}) (s'^j_i - p_j))\\
& \leq \sum_j \sum_i \mathbb{E}[(s'^j_i - p_j)^2] \leq 4dm,
\end{align*}
where the first equality uses the fact that $\vec{s}$ and $\vec{s'}$ are identically distributed, the first inequality is by Jensen's inequality, the second equality is by the fact that the expectation of $Z'$ is $0$ and the definition of variance, the third equality is by the fact that $Z'$ is a sum of uncorrelated random variables, the second inequality is by the fact that $\mathcal{A}^j(\vec{s})^2 \leq 1$, and the final inequality is by the fact that $s'^j_i$ and $p_j$ are both between $-1$ and $1$.

Hence, we get that 
$$\mathbb{E}[|Z'|] = \mathbb{E}_{p,\vec{s},\vec{s'}, \mathcal{A}}[\sum_{j \in [d]} \mathcal{A}^j(\vec{s'})\sum_i (s^j_i - p_j)] \leq 2\sqrt{dm}.$$

If $A$ is perfectly generalizing, then by Lemma~\ref{lem:samplePG} and since perfect generalization is robust to postprocessing, $Z$ and $Z'$ are distributionally close as well, as are $|Z|$ and $|Z'|$. Let $E$ be the event that $|Z| \approx_{2,\frac{3}{m^3}} |Z'|$, where the randomness comes from the randomness of sampling $\vec{s}$ and $\vec{s'}$. 

Then, we can write the following sequence of inequalities:
\begin{align*}
    \mathbb{E}[|Z|] = \mathbb{E}_{p,\vec{s},A}[\Big| \sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j) \Big| ] & =
    \int_0^{2dm} \Pr(|Z|>z) dz \\
    & =  \int_0^{2dm} \left[\Pr(|Z|>z \mid E)\Pr(E) + \Pr(|Z|>z \mid \overline{E})\Pr(\overline{E})\right]  dz \\ & \leq
    \int_0^{2dm} (e^2 \Pr(|Z'|>z \mid E) + \frac{3}{m^3})\Pr(E) + \Pr(\overline{E})  dz \\
    & \leq
    \int_0^{2dm} e^2 \Pr(|Z'|>z \mid E)\Pr(E) dz + \int_0^{2dm} \left[\frac{3}{m^3} + \Pr(\overline{E})\right] dz \\
     & \leq
    \int_0^{2dm} e^2 \Pr(|Z'|>z ) dz + \int_0^{2dm} \left[\frac{3}{m^3} + \frac{1}{m^3}\right] dz \\
    & = e^2\mathbb{E}_{p,\vec{s},\vec{s'},\mathcal{A}}[\Big|\sum_{j \in [d]} \mathcal{A}^j(X')\sum_i (s^j_i - p_j) \Big|] + \frac{8d}{m^2},
\end{align*}
where the first equality is by the definition of expectation (and the definition of random variable $Z$), the second equality is by the law of total probability, the first inequality is using the fact that conditioned on event $E$, random variables $|Z|$ and $|Z'|$ are distributionally close, the second inequality is by the fact that $\Pr(E) \leq 1$, the third inequality is by adding a non-negative term to the first integral and using the law of total probability again and by the fact that $\overline{E}$ corresponds to the probability of failure in the definition of perfect generalization. Hence, combining the inequalities described above, we get that
$$\frac{d}{10} \leq \mathbb{E}[|Z|] \leq e^2 \mathbb{E}\Big[ |Z'| \Big] + \frac{8d}{m^2} \leq 2e^2 \sqrt{dm} + \frac{8d}{m^2}.$$
Simplifying, this gives that
$$m = \Omega(d).$$
\end{proof}
Now, we are ready to prove the lower bound for reproducible algorithms.

\begin{theorem}
Fix $d>0$. Any  $0.0001$-reproducible algorithm $\mathcal{A}$ that is $(0.01,0.001)$-accurate on the one-way marginals problem over $d$ coordinates needs $\tilde{\Omega}(d)$ samples.
\end{theorem}
\begin{proof}
By Claim~\ref{claim:2parrep}, we have that $\mathcal{A}$ is $(0.01,0.01)$-reproducible and $(0.01,0.001)$-accurate when given $m$ samples. Fix sufficiently small $\gamma > 0$. Next, applying Theorem~\ref{thm:amprep}, we get that there is a $\frac{1}{c\log(1/\gamma)}$-reproducible and $(0.01,0.008 + \frac{1}{\log(1/\gamma)}) = (0.01,0.01)$-accurate algorithm for one-way marginals over $d$ coordinates, which takes $O\left(m \log^2 (1/\gamma) \right)$ samples. 

Next, we give a way of reproducibly amplifying the failure probability to $\gamma$. We run the algorithm $\mathcal{A}$ $k=20\log 1/\gamma$ times on different samples, and take the coordinate-wise median of the outputs. Observe that for each coordinate, if more than half the values in that coordinate are within $0.01$ of the true bias, then the median is correct. Consider the probability that more than half the output values in a coordinate are not within $0.01$ of the true bias. By a Chernoff bound, we have that the number of outputs which are within $0.01$ of the true expectation in $l_{\infty}$ norm are more than $0.5k$ with probability at least $1-\gamma^2$, which guarantees that we get a $(0.01,\gamma^2)$-accurate algorithm for one-way marginals. Using composition of reproducibility, we have that the resulting algorithm is $(0.01,0.01)$-reproducible and takes $O\left(m \log^3 (1/\gamma) \right)$ samples. 

Fix $\delta > 0$. By Theorem~\ref{thm:reprodtoPG}, we have that there is a $(2\delta,1,2\delta)$-PG algorithm with failure probability at most $\delta + \gamma \log 1/\delta$ when given $m' = O(m \log^3 (1/\gamma) poly \log(1/\delta)$ samples. Setting $\gamma = \frac{0.005}{\log 1/\delta}$, we get that  that for sufficiently small $\delta > 0$, there is a $(2\delta,1,2\delta)$-PG algorithm with failure probability at most $0.01$ (i.e. $(0.01,0.01)$-accurate), when given $m' = O(m poly \log(1/\delta)$ samples. Setting $\delta = \frac{1}{2m'^3}$ and simplifying, we get that $m' = C m poly \log m$ for some constant $C$. 

Now, using the lower bound for perfect generalization in Theorem~\ref{thm:PGmarginalsLB}, we get that $m' = \Omega(d)$, which gives us that $m = \Tilde{\Omega}(d)$, completing the proof.
\end{proof}

\subsection{Quadratic separation: Agnostic Learning}
In this section, we prove a lower bound for agnostic learning under the constraint of reproducibility \sstext{Say something about it holding for perfect generalization}. Specifically, we show that any $0.1$-reproducible, $(0.01, 0.01)$-accurate agnostic learner for a hypothesis class $H$ with VC dimension $d$ needs at least $\Tilde{\Omega}(d^2)$ samples.

The key idea is that we will reduce a variant of the one-way marginals problem over $d$ coordinates to the problem of agnostically learning any hypothesis class with VC dimension $d$. The variant we consider will loosely correspond to predicting the sign of the coordinates of the expectation, and it will turn out that this can be achieved when using the agnostic learning algorithm as a subroutine. We start by defining this problem more precisely.
\subsubsection{Sign-one-way Marginals}
\begin{definition}
Consider a product of $d$ Rademacher distributions with expectations $p = (p_1,\dots,p_d)$ respectively. A vector $v \in [-1,1]^d$ is said to be an $\alpha$-accurate solution to the sign-one-way marginals problem if $\frac{1}{d}\sum_{j=1}^d v_j p_j \geq \frac{1}{d}\sum_{j=1}^d |p_j| - \alpha$.
\end{definition}
Observe that if $p_j$ was always $-1$ or $1$, this would require that $v_j$ do a very good job of predicting the signs in order to achieve small error. On the other hand, if the $p_j$ values are all $0$, then any values of $v_j$ will correspond to an $0$-accurate solution, i.e. this definition of error scales depending on how biased the expectation is to either $1$ or $-1$, and penalizes a solution more when it does a bad job of predicting in the case where the coordinates of the expectation are more biased towards $-1$ or $1$ (we'd expect such an expectation to be easier to predict, so it makes sense to penalize the solution more in such a case). Now, we are ready to define the accuracy of an algorithm for the sign-one-way marginals problem.
\begin{definition}
Fix any distribution that is the product of $d$ Rademacher distributions with biases $p = (p_1,\dots,p_d)$ respectively. We say that an algorithm $\mathcal{A}:\{\{-1,1\}^d\}^m \to [-1,1]^d$ $(\alpha, \beta)$-accurately solves the sign-one-way marginals problem over this distribution if with probability at least $1-\beta$ over the randomness of the samples and the algorithm, it outputs an $\alpha$-accurate vector $v$.
\end{definition}

\subsubsection{Solving Sign-one-way Marginals using Agnostic Learning}

\begin{algorithm}[H]
    \caption{Algorithm $\mathcal{A}$ for sign-one-way marginals}
    \label{alg:ag2marg}
    \hspace*{\algorithmicindent} \textbf{Input:} Sample access to a product distribution $D$ over $\{-1,1\}^d$, agnostic learner $\mathcal{A}_{ag}$ for hypothesis class $H$ with VC dimension $d$\\
    \hspace*{\algorithmicindent} \textbf{Output:} Biases $(v_1,\dots,v_d).$
    \begin{algorithmic}[1] % The number tells where the line numbering should start
    	    \STATE Draw $\frac{d}{\log^c d}$ i.i.d. examples from $D$ for some $c>0$. Call the corresponding sample $\vec{s}_{inp}$.
		    \STATE Let $x_1,\dots,x_d$ be a shattered set of points for $H$. Let $U_d$ be the uniform distribution over $X_1,\dots,X_d$.
		    \STATE Draw $m=\frac{d^2}{100 \log^{2c} d}$ examples $s_j$ from $U_d$. Call the sample $\vec{s}_{ag}$. If any element $x_i$ occurs more than $\frac{d}{\log^c d}$ times, then FAIL and output $(1,\dots,1)$. Else, go to the next step.
		    \STATE For each example $s_j$, label it with a new entry from coordinate $j$ of the input sample $\vec{s}_{inp}$. Call the labeled sample $\vec{s}_{ag,lab}$.
		    \STATE Run agnostic learner $\mathcal{A}_{ag}$ on the labeled sample $\vec{s}_{ag,lab}$. Let the output function be $f$.
		    \RETURN $(f(x_1), f(x_2), \dots, f(x_d))$.
    \end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm:agn2sowm}
Fix sufficiently large $d>0$. Algorithm $\mathcal{A}$, when run on a $(0.01,0.01)$-accurate, 
$0.0001$-reproducible agnostic learner for a hypothesis class $H$ with VC dimension $d$, is a $(0.02,0.02)$-accurate, $0.0005$-reproducible algorithm for the sign-one-way marginals problem over $d$ coordinates.
\end{theorem}

\begin{proof}
Let $D$ be a Rademacher distribution with expectation $(p_1, \dots,p_d)$. Let $D_{lab}$ be the following distribution over $\{x_1,\dots,x_d\} \times \{-1,1\}$. First, uniformly draw $s \in \{x_1, \dots, x_d\}$. Then, if $s=x_j$, draw $y$ from a Rademacher with expectation $p_j$. $D_{lab}$ is the distribution of the random variable $(s,y)$ obtained using this procedure. 

Firstly, we observe that by a Chernoff bound + union bound, the probability that any element $x_i$ occurs more than $\frac{d}{\log^{2c}d}$ times is exponentially small in $d$ (so less than $0.01$ for sufficiently large $d$). Call this bad event $E$.

Next, consider the following method for sampling $\frac{d}{\log^{2c}d}$ i.i.d. samples from $D_{lab}$- first draw $\ell=\frac{d}{\log^{2c}d}$ i.i.d. examples $s_i \sim U_d$ and then for each of them, if the value obtained is $x_j$, sample $y_j$ from $p_j$. Call the resultant sample $\vec{s}_{hyp}$. Notice that as long as event $E$ doesn't happen, the distributions of $\vec{s}_{ag,lab}$ and $\vec{s}_{hyp}$ are identical. Hence, we can say that the distribution of $\vec{s}_{ag,lab}$ is equal to $D_{lab}^{\ell}|_{\overline{E}}$. For any distribution $D'$ and event $\overline{E}$, a simple calculation shows $d_{TV}(D',D'|_{\overline{E}}) \leq \Pr(E)$. Hence, we get that the total variation distance between the distribution of $\vec{s}_{ag,lab}$ and $D_{lab}^{\ell}$ is at most $0.0001$ for sufficiently large $d$.

Now, we know that with probability at least $0.99$ over the coins of the algorithm and the sample, the agnostic learner produces an output that is accurate with respect to its input sample. Now, by the data-processing inequality for total variation distance, we have that $d_{TV}(\mathcal{A}_{ag}(\vec{s}_{hyp}), \mathcal{A}_{ag}(\vec{s}_{ag,lab})) \leq 0.01$. Consider the subset $O$ of $0.01$-accurate functions w.r.t. the best function in the class $H$ and the distribution $D_{lab}$. \sstext{Put this definition in preliminaries}. By the definition of total variation distance, we have that the probability that learner $\mathcal{A}_{ag}$ produces outputs in this subset $O$ on seeing $\vec{s}_{ag,lab}$ is within $0.01$ of the probability that $\mathcal{A}_{ag}$ produces outputs in this subset $O$ on seeing $\vec{s}_{ag,hyp}$. Since the latter happens with probability at least $0.99$, we have that with probability at least $0.98$, the agnostic learner is $0.01$-accurate when fed the sample $\vec{s}_{ag,lab}$. This implies by the definition of the accuracy guarantee that with probability at least $0.98$ over the randomness of the learner $\mathcal{A}_{ag}$ and sample $\vec{s}_{ag,lab}$, that
\begin{align*}
    \Pr_{(x,y) \in D_{lab}}[f(x) \neq y]] \leq inf_{h \in H} \Pr_{(x,y) \in D_{lab}}[h(x) \neq y] + 0.01,
\end{align*}
where $f$ is the output function of the agnostic learner. For $x \in \{x_1,\dots,x_d\}$, let $p_x$ be $p_j$ if $x = x_j$. Observe that the function that predicts $sign(p_x)$ achieves the infimum on the right hand side of the above equation.

Now, using the fact that expectation of an indicator gives probability of the event inside the indicator,  and that $\mathbbm{1}[a \neq b] = \frac{1-ab}{2}$ when $a$ and $b$ are in $\{-1,1\}$, we get that with probability at least $0.98$ over the randomness of the learner $\mathcal{A}_{ag}$ and sample $\vec{s}_{ag,lab}$,
\begin{align}
    & \mathbb{E}_{(x,y) \in D_{lab}}[ \mathbbm{1}[f(x) \neq y]] \leq inf_{h \in H} \mathbb{E}_{(x,y) \in D_{lab}}[\mathbbm{1}[h(x) \neq y]] + 0.01 \implies \\
    & \mathbb{E}_{(x,y) \in D_{lab}}\left[ \frac{1-f(x) y}{2}\right] \leq inf_{h \in H} \mathbb{E}_{(x,y) \in D_{lab}}\left[ \frac{1 - h(x) y}{2} \right] + 0.01 \implies \\
    & \mathbb{E}_{(x,y) \in D_{lab}}\left[ \frac{1-  f(x) y}{2}\right] \leq \mathbb{E}_{(x,y) \in D_{lab}}\left[ \frac{1 - sign(p_x) y}{2} \right] + 0.01 \implies \\
    & \mathbb{E}_{(x,y) \in D_{lab}}\left[ f(x) y \right] \geq \mathbb{E}_{(x,y) \in D_{lab}}\left[ sign(p_x) y \right] - 0.02.
\end{align}

Now, unraveling the expectations, and using the fact that the randomness of sample $\vec{s}_{ag,lab}$ is from the randomness of the algorithm as well as the randomness of $\vec{s}_{inp}$, we get that with probability at least $0.98$ over the randomness of the algorithm $\mathcal{A}$ and input sample, $\vec{s}_{inp}$, that
\begin{align}
    \frac{1}{d} \sum_{j=1}^d f(x_j) p_j  \geq \frac{1}{d} \sum_{j=1}^d sign(p_j) p_j - 0.02,
\end{align}
proving that $\mathcal{A}$ is a $(0.02,0.02)$-accurate algorithm for sign-one-way marginals over $d$ coordinates.
%Now, taking expectation over the randomness of the algorithm $\mathcal{A}$ and the input sample  $\vec{s}_{inp}$, we get that
%\begin{align}
 %   \mathbb{E}_{\mathcal{A}, \vec{s}_{inp}} \left[\frac{1}{d} \sum_{j=1}^d f(x_j) p_j \right] \geq 0.98 \left[\frac{1}{d} \sum_{j=1}^d |p_j| - 0.02 \right] - 0.02,
%\end{align}

%which implies that 
%\begin{align}
 %   \mathbb{E}_{\mathcal{A}, \vec{s}_{inp}} \left[\frac{1}{d} \sum_{j=1}^d \mathcal{A}^j(\vec{s}_{inp}) p_j \right] \geq \frac{1}{d} \sum_{j=1}^d |p_j| - 0.06,
%\end{align}
%proving that $\mathcal{A}$ is a $0.06$-accurate algorithm for sign-one-way marginals over $d$ coordinates.

Next, we prove that $\mathcal{A}$ inherits the reproducibility of the agnostic learner $\mathcal{A}_{ag}$. Consider two sets of independent samples $\vec{s}_{inp,1}$ and $\vec{s}_{inp,2}$. Consider any set of random coins $r$ drawn for algorithm $\mathcal{A}$. Note that when the coins are such that any point in the shattered set occurs too many times, the algorithm always fails. Recall that this is event $E$, we have previously shown that $E$ occurs with probability at most $0.0001$. Hence, in this case $\mathcal{A}(\vec{s}_{inp,1} ; r) = \mathcal{A}(\vec{s}_{inp,2} ; r) $ with probability $1$. Hence, it is sufficient to consider coins such that every point in the shattered set occurs fewer than $\frac{d}{\log^{2c} d}$ times in the sample $\vec{s}_{ag}$. Now, as argued previously, the total variation distance between the distribution of $\vec{s}_{ag,lab}$ (call it $D_{ag}$) and the same number of i.i.d. samples from $D_{lab}$ is at most $0.0001$ for sufficiently large $d$. Thus, we have that for some distributions $Q, Q'$ and $Q''$, $D_{ag} = 0.9999 Q + 0.0001 Q'$ and $D_{lab} = 0.9999 Q + 0.0001 Q''$. Let $r_{ag}$ be the random coins used by the agnostic learner.


Hence, we can argue from the reproducibility of the agnostic learner that for two datasets drawn from $D_{ag}$, the agnostic learner outputs the same value on the same random coin with high probability.
\begin{align*}
    & \Pr_{\vec{s}_{ag, lab}, \vec{s}'_{ag, lab} \sim D_{lab}^{m}, r_{ag}} [\mathcal{A}(\vec{s}_{ag,lab} ; r_{ag}) = \mathcal{A}(\vec{s}'_{ag,lab} ; r_{ag})] \geq 0.9999 \implies \\
    & 0.9999^2 \Pr_{\vec{s}_{ag, lab}, \vec{s}'_{ag, lab} \sim Q, r_{ag}} [\mathcal{A}(\vec{s}_{ag,lab} ; r_{ag}) = \mathcal{A}(\vec{s}'_{ag,lab} ; r_{ag})] + (0.0001)^2 + (0.9999 \times 0.0001) \geq 0.9999 \implies \\
    & \Pr_{\vec{s}_{ag, lab}, \vec{s}'_{ag, lab} \sim Q, r_{ag}} [\mathcal{A}(\vec{s}_{ag,lab} ; r_{ag}) = \mathcal{A}(\vec{s}'_{ag,lab} ; r_{ag})] \geq 0.9998 \implies \\
    & \Pr_{\vec{s}_{ag, lab}, \vec{s}'_{ag, lab} \sim D_{ag}, r_{ag}} [\mathcal{A}(\vec{s}_{ag,lab} ; r_{ag}) = \mathcal{A}(\vec{s}'_{ag,lab} ; r_{ag})] \geq \\
    & 0.9999^2 \Pr_{\vec{s}_{ag, lab}, \vec{s}'_{ag, lab} \sim Q, r_{ag}} [\mathcal{A}(\vec{s}_{ag,lab} ; r_{ag}) = \mathcal{A}(\vec{s}'_{ag,lab} ; r_{ag})] \geq 0.9996
\end{align*}

Now, we are ready to put together our logic in the previous couple of paragraphs to argue that algorithm $\mathcal{A}$ is reproducible. 
\begin{align*}
    & \Pr_{\vec{s}_{inp,1}, \vec{s}_{inp,2} \sim D^{m}, r} [\mathcal{A}(\vec{s}_{inp,1} ; r) = \mathcal{A}(\vec{s}_{inp,2} ; r) ]  \geq \\
    & \Pr_{\vec{s}_{inp,1}, \vec{s}_{inp,2} \sim D^{m}, r} [\mathcal{A}(\vec{s}_{inp,1} ; r) = \mathcal{A}(\vec{s}_{inp,2} ; r) \mid \overline{E}] \Pr[\overline{E}] \geq \\
    & 0.9999 \Pr_{\vec{s}_{inp,1}, \vec{s}_{inp,2} \sim D^{m}, r} [\mathcal{A}(\vec{s}_{inp,1} ; r) = \mathcal{A}(\vec{s}_{inp,2} ; r) \mid \overline{E}] \geq \\
    & 0.9999 \Pr_{\vec{s}_{ag, lab}, \vec{s}'_{ag, lab} \sim D_{ag}, r_{ag}} [\mathcal{A}_{ag}(\vec{s}_{ag,lab} ; r_{ag}) = \mathcal{A}_{ag}(\vec{s}'_{ag,lab} ; r_{ag})] \geq \\
    & 0.9999 \times 0.9996 \geq 0.9995
\end{align*}
Hence, we have proved that $\mathcal{A}$ is $0.0005$-reproducible.
\end{proof}

\subsubsection{Lower Bound for Sign-one-way Marginals}

In this section, we show that accurately and reproducibly solving the sign-one-way marginals problem over $d$ coordinates requires at least roughly linear in $d$ samples. We will use a variant of the \textit{fingerprinting method} used to prove the lower bound for the one-way marginals problem for perfectly generalizing algorithms, and then extend this to a lower bound for reproducible algorithms. We will require a technical lemma for this argument, that was proved as part of proving the larger fingerprinting lemma \cite{BunUV18, DworkSSUV15, BunSU19}.
\begin{lemma}[\cite{BunSU19}, Lemma A.1 and A.2]\label{lem:fingder}
Let $f$ be a function from $\{-1,1\}^m \to \mathcal{R}$. Let $p$ be a uniformly random variable between $-1$ and $1$, and $\vec{x}$ be a random vector of length $m$, consisting of i.i.d. Rademacher random variables with expectation $p$.

Then, 
\begin{align*}
    \mathbb{E}_{p, \vec{x}}[f(\vec{x}) \sum_{i=1}^m (x_i - p)] = \mathbb{E}_p [2pg(p)]
\end{align*}
where $g(p) = \mathbb{E}_{\vec{x} \sim p}[f(\vec{x})]$.
\end{lemma}




\begin{theorem}\label{thm:PGsignOWMLB}
Fix any $m > 0$. Let $\mathcal{A}$ be a $(\frac{1}{m^3},1,\frac{1}{m^3})$-perfectly generalizing, $(0.05,0.05)$-accurate algorithm for the sign-one-way marginals problem over $d$ coordinates using $m$ samples that always outputs a vector in $[-1,1]^d$. Then, $m = \Omega(d)$. 
\end{theorem}
\begin{proof}
Let $\vec{s}$ be the input dataset to the algorithm $\mathcal{A}$. By the accuracy of the algorithm, for any fixed distribution $D_p$ that is a product of Rademachers with expectation $p=(p_1,\dots,p_d)$, we have that we have that
with probability at least $0.95$ over the randomness of the algorithm $\mathcal{A}$ and its input sample,
\begin{align}
    \frac{1}{d} \sum_{j=1}^d v_j p_j  \geq \frac{1}{d} \sum_{j=1}^d sign(p_j) p_j - 0.05,
\end{align}
where $v$ is the vector output by the algorithm. 

Now, taking expectation over the randomness of the algorithm $\mathcal{A}$ and the input sample  $\vec{s}$, we get that
\begin{align}
   \mathbb{E}_{\mathcal{A}, \vec{s}  \sim D_p^m} \left[\frac{1}{d} \sum_{j=1}^d \mathcal{A}^j(\vec{s}) p_j \right] \geq 0.95 \left[\frac{1}{d} \sum_{j=1}^d |p_j| - 0.05 \right] - 0.05,
\end{align}

which implies that 
\begin{align}
    \mathbb{E}_{\mathcal{A}, \vec{s} \sim D_p^m} \left[\frac{1}{d} \sum_{j=1}^d \mathcal{A}^j(\vec{s}) p_j \right] \geq \frac{1}{d} \sum_{j=1}^d |p_j| - 0.15,
\end{align}
%proving that $\mathcal{A}$ is a $0.06$-accurate algorithm for sign-one-way marginals over $d$ coordinates.



%Taking expectation over the randomness of the algorithm $\mathcal{A}$ and the input sample  $\vec{s}_{inp}$, we get that
%\begin{align}
 %   \mathbb{E}_{\mathcal{A}, \vec{s}_{inp}} \left[\frac{1}{d} \sum_{j=1}^d f(x_j) p_j \right] \geq 0.98 \left[\frac{1}{d} \sum_{j=1}^d |p_j| - 0.02 \right] - 0.02,
%\end{align}

%By the fact that $\mathcal{A}$ is $0.05$-accurate, 
 %\begin{align*}
%         & \mathbb{E}_{\mathcal{A}, \vec{s} \sim D^m} \left[\frac{1}{d} \sum_{j=1}^d \mathcal{A}^j(\vec{s}) p_j \right] \geq \frac{1}{d} \sum_{j=1}^d |p_j| - 0.06 \implies \\
 %        & \sum_{j=1}^d \mathbb{E}_{\mathcal{A}, \vec{s} \sim D^m} \left[\mathcal{A}^j(\vec{s}) p_j \right] \geq \sum_{j=1}^d |p_j| - 0.06d.
 %\end{align*} 
Now, consider each coordinate of expectation vector $p$ drawn uniformly from $\{-1,1\}$. Then, we have that conditioned on any fixed $p$, the above equation holds. Hence, using the law of total expectation, we get that
 \begin{align}
         & \mathbb{E}_{p,\mathcal{A}, \vec{s}\sim D_p^m} \left[\frac{1}{d} \sum_{j=1}^d \mathcal{A}^j(\vec{s}) p_j \right] \geq \mathbb{E}_p\left[\sum_{j=1}^d |p_j| \right]- 0.15d \implies \\
         & \sum_{j=1}^d \mathbb{E}_{p, \mathcal{A}, \vec{s} \sim D_p^m} \left[\mathcal{A}^j(\vec{s}) p_j \right] \geq \sum_{j=1}^d \mathbb{E}_{p_j \sim [-1,1]}\left[|p_j|\right] - 0.15d = 0.35d \label{eq:corrlb},
 \end{align} 

where we have used the fact that $\mathbb{E}_{p_j \sim [-1,1]}[|p_j|] = \frac{1}{2}$. Now, fix a coordinate $j \in [d]$. For any fixed internal randomness $r$ of algorithm $\mathcal{A}$, and for any values of columns of $\vec{s}$ that are not the $j^{th}$ column, we get from Lemma~\ref{lem:fingder} applied to the function $f$ corresponding to the algorithm $\mathcal{A}$ on the complete dataset $\vec{s}$ with internal randomness $r$, that

$$\mathbb{E}_{p_j, \vec{s}^j \sim Rad(p_j)^m} \left[\mathcal{A}^j(\vec{s}; r) p_j \right] = \mathbb{E}_{p_j, \vec{s}^j \sim Rad(p_j)^m}[\mathcal{A}^j(\vec{s}; r) \sum_{i=1}^m (s^j_i - p_j)].$$

Now, since this holds for any fixed values of internal randomness $r$ and for any values of columns of $\vec{s}$ that are not the $j^{th}$ column, it holds for any distribution over the internal randomness $r$ and any distribution over values of other columns of $\vec{s}$. Hence, we get that 

$$\mathbb{E}_{p, \mathcal{A}, \vec{s} \sim D_p^m} \left[\mathcal{A}^j(\vec{s}) p_j \right] = \mathbb{E}_{p, \mathcal{A}, \vec{s} \sim D_p^m}[\mathcal{A}^j(\vec{s}) \sum_{i=1}^m (s^j_i - p_j)],$$
where we have used that $D_p$ is a product distribution. Now, summing over all coordinates $j \in [d]$, we get that 
\begin{align}\label{eq:Zlb}
\sum_{j=1}^d \mathbb{E}_{p, \mathcal{A}, \vec{s} \sim D_p^m}[\mathcal{A}^j(\vec{s}) \sum_{i=1}^m (s^j_i - p_j)] = \sum_{j=1}^d \mathbb{E}_{p, \mathcal{A}, \vec{s} \sim D_p^m} \left[\mathcal{A}^j(\vec{s}) p_j \right] \geq 0.35d,
\end{align}
where we have used equation~\ref{eq:corrlb}. 

Now, we can proceed exactly as in the proof of Theorem~\ref{thm:PGmarginalsLB} (we repeat high-level details for completeness; for more details, see that proof).

Let $\vec{s'}$ be another dataset drawn from the same distribution $D_p$. Let $Z = \sum_{j \in [d]} \mathcal{A}^j(\vec{s})\sum_i (s^j_i - p_j)$ and $Z' = \sum_{j \in [d]} \mathcal{A}^j(\vec{s'})\sum_i (s^j_i - p_j)$. 


First, note that $Z'$ is a sum of uncorrelated random variables with mean $0$ (see the proof of Theorem~\ref{thm:PGmarginalsLB} for a proof of this).

We can then prove (as in the proof of Theorem~\ref{thm:PGmarginalsLB}) that
\begin{align*}
\mathbb{E}[|Z'|] = \mathbb{E}_{p, \mathcal{A}, \vec{s},\vec{s'} \sim D_p^m}\Big[\Big| \sum_{j \in [d]} \mathcal{A}^j(\vec{s'})\sum_i (s^j_i - p_j) \Big| \Big] \leq 2\sqrt{dm},
\end{align*}

If $\mathcal{A}$ is perfectly generalizing, then since perfect generalization is robust to postprocessing, $Z$ and $Z'$ are distributionally close as well, as are $|Z|$ and $|Z'|$. Let $E$ be the event that $|Z| \approx_{2,\frac{3}{m^3}} |Z'|$, where the randomness comes from the randomness of sampling $\vec{s}$ and $\vec{s}'$. 

Then, we can prove (as in the proof of Theorem~\ref{thm:PGmarginalsLB}) that
\begin{align*}
    \mathbb{E}[ |Z| ] \leq e^2\mathbb{E}[|Z'|] + \frac{8d}{n^2},
\end{align*}
Hence, combining the inequality above with equation~\ref{eq:Zlb}, we get that
$$0.35d \leq \mathbb{E}[|Z|] \leq e^2 \mathbb{E}[|Z'|] + \frac{8d}{n^2} \leq 2e^2 \sqrt{dm} + \frac{8d}{m^2}.$$
Simplifying, this gives that
$$m = \Omega(d).$$
\end{proof}
Now, we are ready to apply our conversion from reproducibility to perfect generalization to prove a similar lower bound for reproducible algorithms.
\begin{theorem}\label{thm:sOWMreplb}
Fix $d>0$. Any  $0.0005$-reproducible algorithm $\mathcal{A}$ that is $(0.01,0.001)$-accurate on the sign-one-way marginals problem over $d$ coordinates needs $\tilde{\Omega}(d)$ samples.
\end{theorem}
\begin{proof}
By Claim~\ref{claim:2parrep}, we have that $\mathcal{A}$ is $(0.01,0.05)$-reproducible and $(0.01,0.001)$-accurate when given $m$ samples. Fix sufficiently small $\gamma > 0$. Next, applying Theorem~\ref{thm:amprep}, we get that for any constant $c>1$, there is a $\frac{1}{c\log(1/\gamma)}$-reproducible and $(0.01,0.008 + \frac{1}{\log(1/\gamma)}) = (0.01,0.01)$-accurate algorithm for sign-one-way marginals over $d$ coordinates, which takes $O\left(m \log^2 (1/\gamma) \right)$ samples. 

Next, we give a way of reproducibly amplifying the failure probability to $\gamma$. We run the algorithm $\mathcal{A}$ a number of times ($k=20\log 1/\gamma$ times) on different samples, and take the mean of the outputs. Using composition of reproducibility, we have that the resulting algorithm is $0.01$-reproducible and takes $O\left(m \log^3 (1/\gamma) \right)$ samples. Now, we analyze the failure probability of this algorithm. Let the output vectors of $\mathcal{A}$ on the $k$ runs be $v^1,\dots,v^k$. We are interested in the quantity $\frac{1}{d}\sum_{j=1}^d\left[ |p_j| - \frac{p_j}{k}\sum_{i=1}^k v^i_j \right]$. First, we analyze the expectation of this quantity. 
\begin{align*}
    \mathbb{E}\left[\frac{1}{d}\sum_{j=1}^d\left[|p_j| - p_j\sum_{i=1}^k v^i_j  \right] \right] & =  \mathbb{E}\left[\frac{1}{d}\sum_{j=1}^d p_j \left[sign(p_j) - \frac{1}{k}\sum_{i=1}^k v^i_j  \right] \right] \\
    & = \mathbb{E}\left[\frac{1}{d}\sum_{j=1}^d \frac{1}{k}\sum_{i=1}^k  p_j \left[sign(p_j) - v^i_j  \right] \right] \\
    & = \frac{1}{k}\sum_{i=1}^k  \mathbb{E}\left[ \frac{1}{d}\sum_{j=1}^d  p_j \left[sign(p_j) - v^i_j  \right] \right] \leq 0.02 
\end{align*}
where the last inequality is because the quantity inside the last expectation is less than $0.01$ with probability at least $0.99$ (and because all the $v^i$ are identically distributed). Next, observe that the quantity
$\frac{1}{k} \sum_{i=1}^k \left( \frac{1}{d}\sum_{j=1}^d  p_j \left[sign(p_j) - v^i_j \right] \right)$ is a sum of $k$ independent random variables (since the $i^{th}$ term in the sum only depends on random variable $v^i$) in the interval $[-2,2]$. Hence, using the multiplicative Hoeffding's inequality, we have that the probability that the sum is larger than $0.05$ is less than $\gamma^2$.

Now, by Theorem~\ref{thm:reprodtoPG}, we have that there is a $(2\delta,1,2\delta)$-PG algorithm with failure probability at most $\delta + \gamma \log 1/\delta$ when given $m' = O(m \log^3 (1/\gamma) poly \log(1/\delta)$ samples (where failure in this case means outputting a solution that is not $0.05$-accurate). Setting $\gamma = \frac{0.005}{\log 1/\delta}$, we get that  that for sufficiently small $\delta > 0$, there is a $(2\delta,1,2\delta)$-PG algorithm with failure probability at most $0.05$ (i.e. $(0.05,0.05)$-accurate), when given $m' = O(m poly \log(1/\delta)$ samples. Setting $\delta = \frac{1}{2m'^3}$ and simplifying, we get that $m' = C m poly \log m$ for some constant $C$. 

Now, using the lower bound for perfect generalization in Theorem~\ref{thm:PGsignOWMLB}, we get that $m' = \Omega(d)$, which gives us that $m = \Tilde{\Omega}(d)$, completing the proof.
\end{proof}

Now, we can use the reduction from sign-one-way marginals to agnostic learning to obtain a sample complexity lower bound for online learning.
\begin{corollary}
Fix sufficiently large $d>0$ and a hypothesis class $H$ with VC dimension $d$. Any $(0.01,0.01)$-accurate, $0.0001$-reproducible agnostic learning $\mathcal{A}$ for $H$ requires at least $\tilde{\Omega}(d^2)$ samples.
\end{corollary}
\begin{proof}
If there was a $(0.01,0.01)$-accurate, $0.0001$-reproducible agnostic learning algorithm that took fewer than $\frac{d^2}{\log^{2c} d}$ (where $c$ is some sufficiently large constant) samples, then by Theorem~\ref{thm:agn2sowm}, there would be a $(0.02,0.02)$-accurate, $0.0005$-reproducible algorithm for the sign-one-way marginals problem over $d$ coordinates that took $\frac{d}{\log^c d}$ samples, which contradicts Theorem~\ref{thm:sOWMreplb}. This proves the result.
\end{proof}
We note that our agnostic learning lower bound as stated only holds only for constant accuracy, and might not give the optimal dependence on the accuracy parameter $\alpha$ for general $(\alpha, \beta)$-agnostic learning. We leave it as an open problem to achieve the right dependence on $\alpha$.

\subsection{Closing the Gap: Learning Finite Classes}
Now that we've seen natural settings in which our reduction is tight (and therefore exhibited a quadratic statistical separation between privacy and reproducibility), it is reasonable to ask whether there are any settings under which the reduction is loose, or even where privacy and reproducibility might have the same statistical cost. In this section, we'll show this is indeed the case for (certain regimes of) a closely related problem: realizable PAC-learning. In particular, in this section we exhibit a reproducible algorithm for PAC-learning that not only matches the class-size lower bound for the one-way marginals problem, but also gives a quadratically improved dependence on the accuracy and confidence parameters over applying our reduction from privacy.
\begin{theorem}[Finite Classes are Reproducibly Learnable]
	\label{thm:rFinite}	
	Any class $H$ is reproducibly PAC-learnable with sample complexity: 
	\[
	m(\rho,\alpha,\beta) \leq O\left(\frac{ \log^2|H|+\log\frac{1}{\rho\beta}}{\alpha^2\rho^2}\log^3\frac{1}{\rho} \right).
	\]
In the realizable setting, the $\alpha$-dependence can be improved to linear:
	\[
	m(\rho,\alpha,\beta) \leq O\left(\frac{ \log^2|H|+\log\frac{1}{\rho\beta}}{\alpha\rho^2}\log^3\frac{1}{\rho} \right).
	\]
\end{theorem}
\Cref{thm:rFinite} gives a quadratic improvement over the sample complexity via reduction from private learning in both confidence and accuracy, and in particular has the same asymptotic dependence as in private PAC-learning (and hence avoids any statistical blowup in the setting where $\log|H|$ is thought of as small). In fact, it's worth noting the result is tight in these parameters, as even standard PAC-learning requires the same dependencies.

\subsubsection{Algorithm}
At its core, the algorithm achieving \Cref{thm:rFinite} relies on a simple random thresholding trick. In particular, the idea is roughly to estimate the risk of each concept in the class $H$ by standard uniform convergence bounds, choose a random error threshold $v \in [OPT,OPT+\eps]$, and finally output a random hypothesis with empirical error at most $v$. In practice this requires a bit more effort, and is achieved more formally by the following algorithm.

% , using the fact that $H$ is finite to get good estimations of the true risk for each concept $f \in H$. 
% Then, $\rFinite$ picks a random threshold $v < \delta$ and a random ordering of the concepts in $H$. The algorithm outputs the first concept in this random ordering that has empirical risk at most $v$. 

\begin{algorithm}[H]

\KwResult{Reproducibly outputs hypothesis with error at most $OPT+\alpha$}
\nonl \textbf{Input:} Finite Class $H$, Joint Distribution $D$ over $X \times \{0,1\}$ (Sample Access)\\
\nonl \textbf{Parameters:} 
\begin{itemize}
    \item Reproducibility, Accuracy, Confidence $\rho, \alpha, \beta>0$
    \item Sample Complexity $m=m(\rho, \alpha,\beta) \leq O\left(\frac{ \log^2|H|\log \frac{1}{\rho}+\rho^2\log\frac{1}{\beta}}{\eps^2 \rho^4} \right)$
    \item Reproducibility bucket size $\tau \leq O(\frac{\alpha \rho}{ \ln |H|})$
\end{itemize}
\nonl \textbf{Algorithm:}\\
\begin{enumerate}
    \item Draw a labeled sample $S \sim D^m$ and compute $\emprisk(f, S)$ for every $f \in H$.
	\item Reproducibly output initialization  $v_{\text{init}} \in [OPT,OPT+\eps/2]$ (see \Cref{alg:agnostic-subroutine})
	\item Select random threshold $v \gets_r \{v_{\text{init}}+\frac{3}{2}\tau,v_{\text{init}}+\frac{5}{2}\tau,\ldots,v_{\text{init}}+\eps/4-\tau/2\}$
	\item Randomly order all $f \in H$
\end{enumerate}
\textit{Return} Output the first hypothesis $f$ in the order s.t.\ $\emprisk(f, S) \le v$.
 \caption{(Intermediate) Reproducible Learner for Finite Classes}
\label{alg:finite-concept-classes}
 
\end{algorithm}
We note that Step 2, estimating OPT, follows essentially the same argument as the basic reproducible statistical query algorithm of \cite{ImpLPS22}. We give the argument in \Cref{app:OPT} for completeness.

We note that while Algorithm $\rFinite$ is a reproducible agnostic PAC learner, it is not quite sufficient to prove \Cref{thm:rFinite} due to its poor dependence on $\rho$. We'll see in the next section that the Theorem then follows from separately amplifying $\rFinite$ starting from good constant reproducibility.

% Before moving on, we remark that in the (near)-realizable setting when $OPT \leq O(\eps)$, we can improve the sample complexity by using a weaker convergence bounds (with better sample complexity).
% \begin{remark}\label{rem:rfinite-real}
% In the near-realizable setting, the sample complexity of Algorithm $\rFinite$ can be improved to:
% 	\[
% 		m(\rho,\eps,\delta) \leq O\left(\frac{ \log^2(\frac{|H|}{\delta\rho})}{\eps \rho^4} \right).
% 	\]
% \end{remark}


\subsubsection{Analysis}
% We will rely on a standard bound on the uniform convergence of finite classes, which is an immediate application of Chernofff and Union bounds.
% %a classical variant of uniform convergence in the near-realizable setting which is an immediate application of Chernoff and Union bounds.

% \begin{lemma}[Uniform Convergence]
% 	\label{lem:estimation-of-empirical-errors}
% 	There exists a constant $c>0$ such that for any joint distribution $D$ over $X \times Y$, any $\tau, \delta >0$, and any $n \geq c\frac{\log \frac{|H|}{\delta}}{\tau^2}$, the empirical and true risk of all hypotheses over samples of size $n$ are close with high probability:
% 	\begin{equation}\label{eq:uniform-ag}
% 	\Pr_{S \sim D}\left[\max_{f \in H} |\emprisk(f, S) - R(f)| < \tau \right] \ge 1- \delta.
% 	\end{equation}
% \end{lemma}

% The proof of both lemmas are standard, and are immediate applications of Chernoff and Union bounds.
We'll start by proving the following weaker bound for our intermediate learner.
\begin{theorem}[Intermediate Learnability of Finite Classes]\label{thm:intermediate-learner}
	Let $H$ be any finite concept class.
	Algorithm $\rFinite$ is a (proper) agnostic reproducible learning algorithm for $H$ with sample complexity: 
	\[
	m(\rho,\alpha,\beta) \leq O\left(\frac{ \log^2|H|\log(\frac{1}{\rho})+\rho^2\log\frac{1}{\beta}}{\alpha^2 \rho^4} \right).
	\] 
% 	Moreover, if the adversary is restricted to choosing distributions with $OPT \leq O(\eps)$, this can be improved to $O(\frac{\log (H) \log(\frac{|H|}{\delta\rho})}{\eps \rho} )$.
\end{theorem}
The main challenge lies in showing reproducibility (accuracy and failure probability are essentially immediate from standard uniform convergence arguments). To this end, note that the randomness $r$ used by $\rFinite$ is largely broken into three parts: estimating OPT, choosing a random threshold, and ordering the concepts in $H$. We'll focus first on the latter two, where the choice of $v$ restricts $H$ to two subsets $H_1$ and $H_2$ (those with empirical error at most $v$), depending on input samples $S_1$ and $S_2$. We first observe that as long as the symmetric difference of $H_1$ and $H_2$ are small, outputting the first concept from these sets (according to the random ordering) is a reproducible procedure.

\begin{observation}
	\label{obs:random-ordering-of-concepts}
	Let $O(H, r)$ be a random ordering of concept class $H$. Let $\emptyset \subset H_1, H_2 \subseteq H$, and let $f_1$ and $f_2$ be the first elements of $C_1$ and $C_2$ respectively according to $O(H, r)$. 
	Then $\Pr_{r} [f_1 \ne f_2] 
	= \frac{|H_1 \Delta H_2|}{|H_1 \cup H_2|}$, where $\Delta$ denotes the symmetric difference.
\end{observation}

% With high probability, the empirical risk of each concept $f \in C$ is estimated within error $\tau$. 

The key to proving reproducibility then reduces to observing that most choices of $v$ induce small symmetric difference between the corresponding $H_1$ and $H_2$. Namely, the idea is to observe that for any fixed joint distribution $D$, intervals
\[
I_0 = [OPT,OPT+\tau], \ldots , I_{\alpha/(2\tau)} = [OPT+\alpha/2-\tau, OPT+\alpha/2],
\]
and corresponding threshold positions $v_i=OPT+\frac{(2i+1)}{2}\tau$, the sets
\[
H^{(i)}_1 = \{h \in H: R_{\text{emp}}(h,S_1) \leq v_i\}, \quad H^{(i)}_2 = \{h \in H: R_{\text{emp}}(h,S_2) \leq v_i\}
\]
are close for most choices of $v_i$, $S_1$, and $S_2$. To adjust for the fact that we don't know the value of OPT, we will in fact prove something slightly more general that allows our starting point to range anywhere from $OPT$ to $OPT+\alpha/2$.
\begin{lemma}\label{lem:good-thresholds}
Let $v_{\text{init}} \in [OPT,OPT+\alpha/2]$ and $\tau \leq O\left(\frac{\alpha\rho^2}{\log|H|}\right)$ a parameter that divides $\alpha/4$. Define the intervals
\[
I_0 = [v_{\text{init}},v_{\text{init}}+\tau), \ I_1 = [v_{\text{init}}+\tau,v_{\text{init}}+2\tau), \ \ldots \ , \ I_{\frac{\alpha}{4\tau}} = [v_{\text{init}}+\frac{1}{4}\alpha - \tau,  v_{\text{init}}+\frac{1}{4}\alpha]
\]
and corresponding thresholds $v_i=v_{\text{init}}+\frac{(2i+1)}{2}\tau$, and let
\[
H^{(i)}_1 = \{h \in H: R_{\text{emp}}(h,S_1) \leq v_i\}, \quad H^{(i)}_2 = \{h \in H: R_{\text{emp}}(h,S_2) \leq v_i\}
\]
denote the hypotheses with empirical error at most $v_i$ across two independent samples $S_1$ and $S_2$ of size $O(\frac{\log^2|H|\log \rho^{-1}}{\alpha^2\rho^4})$. Then with probability at least $1-\rho/4$, a uniformly random choice of $i \in [\frac{\alpha}{4\tau}]$ satisfies:
\[
\frac{|H^{(i)}_1 \Delta H^{(i)}_2|}{|H^{(i)}_1 \cup H^{(i)}_2|} \leq \rho/4.
\]
\end{lemma}
\begin{proof}
For convenience of notation, let $|I_i|$ denote the number of hypotheses whose true risk lies in interval $I_i$, and $|I_{[i]}|$ the number of hypotheses in intervals up through $I_i$. We call a threshold $v_i$ `bad' if any of the following conditions hold
\begin{enumerate}
    \item The $i$th interval has too many elements:
    \[
    |I_i| > \frac{\rho}{20}|I_{[i-1]}|.
    \]
    \item The number of elements beyond $I_i$ increases too quickly: 
    \[
    \exists j \geq 1: |I_{i+j}| \geq e^{j}|I_{[i-1]}|.
    \]
%     \item The number of elements before $I_i$ increases too quickly:
% \[
%     \exists j \geq 1: |I_{i-j}| \geq 2^{j}|I_{[i-1]}|
%     \]
%     \maxh{fixing}
\end{enumerate}
and `good' otherwise. We will argue the following two claims. 
\begin{enumerate}
    \item If $v_i$ is a good threshold, then $H^{(i)}_1$ and $H^{(i)}_2$ are probably close
\[
\underset{S_1,S_2}{\Pr}\left[\frac{|H^{(i)}_1 \Delta H^{(i)}_2|}{|H^{(i)}_1 \cup H^{(i)}_2|} \leq \frac{\rho}{4}\right] \geq 1-\frac{\rho}{8}
\]
\item At most a $\frac{\rho}{8}$ fraction of thresholds are bad.
\end{enumerate}
Since we pick a threshold uniformly at random, it is good with probability at least $1-\rho/8$ and a union bound gives the desired result.

It remains to prove the claims. For the first, observe that for any fixed hypothesis $h$ with true risk $R(h,D) \in I_{i+j}$, the probability that the empirical risk of $h$ is less than $v_i$ is at most
\begin{equation}\label{eq:tail-bound}
\Pr[R_{\text{emp}}(h,S) \leq v_i] \leq e^{-\Omega(j^2\tau^2|S|)}
\end{equation}
by a Chernoff bound. Let $x_i$ denote the variable which counts the number of hypotheses with true risk beyond $I_i$ that cross the threshold $v_i$ empirically. If $v_i$ is `good,' we can bound $\mathbb{E}[x_i]$ by
\[
\mathbb{E}[x_i] \leq |I_{[i-1]}|\sum\limits_{j > 0} e^{-\Omega(j^2\tau^2|S| - j)} \leq \frac{\rho^2}{160}|I_{[i-1]}|
\]
for our choice of $|S|$. Markov's inequality then promises 
\[
\Pr\left[x_i \geq \frac{\rho}{20}|I_{[i-1]}|\right] \leq \frac{\rho}{8}|I_{[i-1]}|.
\]
Since $I_i$ itself contributes at most $\frac{\rho}{20}|I_{[i-1]}|$ hypotheses that cross the threshold in the worst case, this means that with probability at least $1-\frac{\rho}{8}$, at most $\frac{\rho}{10}|I_{[i-1]}|$ hypotheses with true error greater than $v_i$ cross the threshold. This implies that with  probability at least $1-\frac{\rho}{8}$, $|H_1 \Delta H_2|$ cannot be too big:
\[
|H_1 \Delta H_2| \leq \frac{\rho}{5}|I_{[i-1]}|.
\]
Moreover, note that the probability any hypothesis in $I_{[i-1]}$ crosses $v$ is at most $e^{-\Omega(\tau^2|S|)}$, so similarly the probability that more than a $\frac{\rho}{20}$ fraction of such hypotheses cross $v_i$ is at most $\frac{\rho}{8}$. This implies with probability at least $1-\frac{\rho}{8}$, $|H_1 \cup H_2|$ cannot be too small:
\[
|H_1 \cup H_2| \geq \left(1-\frac{\rho}{20}\right)|I_{[i-1]}|
\]
Thus altogether a union bound gives
\[
\Pr\left[\frac{|H^{(i)}_1 \Delta H^{(i)}_2|}{|H^{(i)}_1 \cup H^{(i)}_2|} \leq \frac{\rho}{4} \right] \geq 1-\frac{\rho}{4}
\]
as desired.


Finally, we need to show that almost all thresholds are good. To see this, first observe that since $v_{\text{init}} \geq OPT$, $|I_{[i]}|>0$ for all $i \geq 0$. To count the number of bad thresholds, let $i_1 \geq 1$ be the position of the first bad threshold, and $t_1$ denote the largest index such that $i_1+t_1$ fails a condition. Define $i_j$ and $t_j$ recursively as the first bad threshold beyond $i_{j-1}+t_{j-1}$ and its corresponding latest failure. Observe that by construction, any interval that does not lie in any $[i_j.i_j+t_j]$ is good, so there are at most $\sum t_j$ bad thresholds. 

Let $\ell$ denote the final index of the above greedy process. By definition of a bad interval we have
\[
|H| \geq |I_{[i_\ell+t_\ell]}| \geq \left(1+\frac{\rho}{8}\right)^{\sum\limits_{j=1}^\ell t_j}
\]
and therefore that 
\[
\sum_{j=1}^\ell t_j \leq O\left(\frac{\log(|H|)}{\rho}\right).
\]
Since we have chosen $\tau$ such that the total number of intervals is at least $\Omega\left(\frac{\log(|H|)}{\rho^2}\right)$, the appropriate choice of constant gives that at most a $\rho/8$ fraction are bad as desired.
\end{proof}
To complete the argument, it is enough to show we can find a good starting point $v_{\text{init}}$.
\begin{lemma}
There exists a $\rho$-reproducible algorithm over $O\left(\frac{\log(\frac{|H|}{\rho\beta})}{\rho^2\alpha^2}\right)$ samples that outputs a good estimate of $OPT$ with high probability:
\[
\Pr_{r,S}\big[\mathcal{A}(S) \in [OPT,OPT+\alpha/2]\big] \geq 1-\beta
\]
\end{lemma}
Proving this Lemma largely follows from prior techniques but is a bit tedious, so we leave the proof for \Cref{app:OPT}. With these tools in hand, we are finally ready to prove \Cref{thm:rFinite}.
\begin{proof}[Proof of \Cref{thm:intermediate-learner}]
We start by showing $\rFinite$ is $\rho$-reproducible. $\rFinite$ starts by running a reproducible subroutine (with parameters $\rho'=\rho/2$ an $\beta'=\beta/2$) to find an estimate for OPT. Using new (independent) randomness, it then selects a threshold $v_i$ and a random ordering over $H$, and outputs the first hypothesis in $H^{(i)} = \{h: R_{\text{emp}}(h,S) \leq v_i\}$. By \Cref{lem:good-thresholds} and \Cref{obs:random-ordering-of-concepts}, this latter process is $\rho/2$-reproducible. By composition of reproducibility, the entire algorithm is therefore $\rho$-reproducible as desired.

Correctness of $\rFinite$ follows from standard uniform convergence type arguments. 
% Recall that there exists a constant $c>0$ such that for any joint distribution $D$ over $X \times \{0,1\}$, any $\tau, \beta >0$, and any $n \geq c\frac{\log \frac{|H|}{\beta}}{\tau^2}$, the empirical and true risk of all hypotheses in $H$ over samples of size $n$ are close with high probability:
% 	\begin{equation*}\label{eq:uniform-ag}
% 	\Pr_{S \sim D}\left[\max_{f \in H} |\emprisk(f, S) - R(f)| < \tau \right] \ge 1- \beta.
% 	\end{equation*}
In particular, by our choice of $|S|$, any hypothesis with empirical risk at most $OPT+\alpha/2$ has true risk less than $OPT+\alpha$ with probability at least $1-\beta/2$. Furthermore, as long as our estimation of $OPT$ is successful (which occurs with probability at least $1-\beta/2$), we always output such a hypothesis. Thus altogether we output a hypothesis with true error at most $OPT+\alpha$ with probability at least $1-\beta$ as desired. 

Finally, we need to argue that the dependence on $\varepsilon$ can be improved to linear in the realizable setting. Note that in this case, we can simply set $v_{\text{init}}$ to 0, and ignore the estimation of OPT. The improvement then follows immediately from noting that when $OPT=0$, a standard Chernoff bound improves \Cref{eq:tail-bound} to
\[
\Pr[R_{\text{emp}}(h,S) \leq v_i] \leq e^{-\Omega(\frac{j^2\tau^2|S|}{\varepsilon})}.
\]
Similarly, only $O(\frac{\log\frac{|H|}{\beta}}{\varepsilon})$ examples are needed to ensure hypotheses with $O(\varepsilon)$ empirical risk have $O(\varepsilon)$ true risk with high probability, and the rest of the proof follows as in the agnostic case. 
\end{proof}
Finally, we amplify the above to prove \Cref{thm:rFinite}.
\begin{proof}[Proof of \Cref{thm:rFinite}]
Our amplification algorithm is a modification of the original technique introduced in \cite{ImpLPS22}, designed to take advantage of the fact that the dependence on $\beta$ (failure) and $\rho$ (reproducibility) are highly unbalanced in learning tasks. Draw $k=O(\log(1/\rho))$ random strings $r_1,\ldots,r_k$, and consider the distributions $\{D_i\}_{i=1}^k$ generated by running $\rFinite(r_i,S)$ with parameters $\rho'=.01$ and $\beta'=\beta\cdot \text{poly}(\rho)$ on a large enough sample $S$.
The idea is to argue that with good probability over the choice of random strings $r$, at least one of these distributions has an $\Omega(1)$-heavy-hitter (which is also a good hypothesis with extremely high probability). Roughly speaking, we can then use the heavy hitters algorithm of  \cite{ImpLPS22} across these distributions to $\rho/2$-reproducibly output a good hypothesis, and union bound over all applications to argue correctness of the final output.

Let's formalize this argument. First, observe since our setting of $\rFinite$ is $.01$-reproducible, at least $90\%$ of the random strings have a `canonical element,' i.e.\ one that appears across at least $90\%$ of random samples. Call such strings good, and observe that any good string $r_i$ corresponds to a distribution $\mathcal{D}_i$ with a $.9$-heavy-hitter by construction. Over a random choice of $O(\log \rho^{-1})$ such strings, the former guarantee then promises at least one of these strings is good with probability greater than $1-\rho/4$ and therefore that at least one distribution in $\{D_i\}_{i=1}^k$ has a $.9$-heavy-hitter. With this in mind, we now appeal to the heavy-hitter algorithm of \cite{ImpLPS22}, which draws $O(\frac{\log \rho^{-1}}{\rho^2})$ samples from a distribution to reproducibly output a list of all $\Omega(1)$-heavy-hitters.\footnote{We note the technique actually outputs a list of some weight close to $c$, but this is largely irrelevant in our setting where $c$ (and the shift in $c$) are constant.} To make our entire process $\rho$-reproducible, we will run the above process for $\rho'=O(\frac{\rho}{\log\rho^{-1}})$. To this end, we draw samples $S_1,\ldots, S_{t}$ for $t =O(\frac{\log^3 \rho^{-1}}{\rho^2})$ and generate $t$ corresponding samples from each $\{D_i\}_{i=1}^k$ (re-using $S_j$ between distributions), which we use to run \cite{ImpLPS22}'s heavy-hitters algorithm. Union bounding over all applications, this process is $\rho/4$-reproducible, and with probability at least $1-\rho/4$ outputs a non-empty list of hypotheses. Finally, we break in to one of two cases. If the result list is indeed non-empty, simply output a random element of the list (a fully reproducible procedure). Otherwise, run $\rFinite$ on a fresh random string and sample, and output the result.



Finally, we argue reproducibility and correctness of the above process. First, note the output list is non-empty with probability at least $1-\rho/4$, and two independent samples produce the same list (with fixed randomness) with probability at least $1-\rho/2$ by reproducibility of the repeated heavy hitter process discussed above. Therefore the entire process is $\rho$-reproducible as desired. For correctness, note that we have used at most $\poly(\rho^{-1})$ instances of $\rFinite$. Recall that we set the failure probability of $\rFinite$ to be very small, with $\beta'=\beta\cdot \text{poly}(\rho^{-1})$. Since each individual application of $\rFinite$ fails with probability less than $\beta'$, union bounding over all applications of the algorithm implies every output hypothesis is `good' (within $\alpha$ of OPT) with probability at least $1-\beta$. Since we only output hypotheses generated by this process, the probability of outputting a bad hypothesis is then less than $\beta$ as desired.

Altogether, the sample complexity of the above algorithm is given by the size of samples 
\[
t|S_i| = O\left(\frac{\log^3 \rho^{-1}}{\rho^2}\cdot \frac{ \log^2|H|+\log\frac{1}{\rho\beta}}{\alpha^2}\right),
\]
or 
\[
O\left(\frac{\log^3 \rho^{-1}}{\rho^2}\cdot\frac{ \log^2|H|+\log\frac{1}{\rho\beta}}{\alpha} \right)
\]
in the realizable case where our original samples are linear in $\varepsilon^{-1}$.
% Note that this is slightly off of the claimed complexity, where $\log\frac{1}{\rho\beta}$ is replaced with $\log\frac{1}{\beta}$. This is only an issue when $\rho \ll \beta$. In this case, one can slightly modify the above algorithm by drawing $O(\log \beta^{-1})$ random seeds instead of $O(\log \rho^{-1})$, and setting the original failure probability of $\rFinite$ to just $O(\beta)$. The only issue is that heavy hitters in the resulting distributions may not be correct. To account for this, we include an additional procedure that reproducibly tests if a hypothesis is correct. The distribution $\mathcal{D_i}$ is then defined as above composed with this procedure (i.e. outputs nothing if the test fails).

% In particular we use the following procedure. First, estimate OPT reproducibly as in \Cref{alg:OPT-estimate}, and reproducibly test the error of the given hypothesis with a statistical query (this can be done in $O(\frac{1}{\eps\rho^2})$ samples by the SQ algorithm of \cite{ImpLPS22}). The resulting heavy-hitters of composing \rFinite with this process are then always low-error hypotheses. We now compute heavy-hitters as 



% However, reproducibility beyond $\beta$ is essentially trivial. In particular, if $\rho < \beta/2$ we make the following slight adjustment. First, with probability $\beta/2$ completely forego the above procedure and output some fixed hypothesis in $H$. Note that this is a fully reproducible procedure. Now run the above amplification process with $\rho'=\beta/2$ and $\beta'=\beta/2$. The total correctness of this process is at least $1-\beta$. The total reproducibility satisfies:
% \begin{align*}
%     \Pr_{r,S_1,S_2}[A(r,S)=A(r,S)]\geq  \beta/2 + (1-\beta/2)(1-\rho) = 1- \rho + \beta\rho/2
% \end{align*}
\end{proof}

\section{Applications}
In this section we take advantage of our reductions between notions of stability to resolve (or otherwise make progress on) several open problems in the algorithmic stability literature.

\subsection{Item-level to User-level Privacy Transformation}

The original motivation of introducing the definition of pseudo-global stability in \cite{ghazi2021user} was to come up with PAC learning algorithms in the example-rich, user-level privacy setting. In this setting, there are a number of users with many samples (the regime we will be interested in is when there are a few users who have enough samples to solve the problem for themselves). The motivation behind this setting is to leverage the data of example-rich users to obtain statistical insights without compromising their privacy. Such statistical analyses could then be released and used widely, even by users who didn't have as much data. 

The technique of \cite{ghazi2021user} involves coming up with pseudo-globally stable algorithms for PAC learning, and then having each user run a pseudo-globally stable algorithm with the same coins. Then, you can privately identify a heavy hitter among the outputs of the users (such a heavy hitter exists with high probability because of the property of pseudo-global stability), and since changing an entire user's sample will affect only one output, this procedure will be user-level differentially private. One open question raised in their paper was whether their techniques could be extended beyond the PAC setting.

Our argument that pseudo-global stability and differential privacy are two sides of the same coin answers this question in the affirmative, and has the additional benefit of eliminating the need to cleverly design pseudo-globally stable (reproducible) algorithms. We showed previously that item-level differentially private algorithms can be compiled into reproducible algorithms with only a quadratic overhead in sample complexity \sstext{Add link to section}. Hence, our results allow for a general transformation from item-level to user-level privacy for statistical tasks in the example-rich setting; each user applies correlated sampling to the same item-level differentially private algorithm applied to their specific sample, and then a heavy hitter is identified via differentially private selection.
\begin{theorem}
There are universal constants $c,K >0$ such that the following holds. Let $\mathcal{T}$ be a statistical task with a finite output space. Given a $(0,1, \frac{c}{n^3})$-item level differentially private algorithm $\mathcal{A}$ that solves $\mathcal{T}$ using $n$ samples and with failure probability $0.01$, for every $\eps, \delta > 0$, there exists an $(\eps, \delta)$-user level differentially private algorithm $\mathcal{A}_u$ that solves $\mathcal{T}$ with failure probability $0.02$, when given access to the data of $O(\frac{1}{\eps} \log \frac{1}{\delta})$ users, each of whom have at least $K n^2$ examples.
\end{theorem}
\begin{proof}
Firstly, we can amplify the privacy parameters of $\mathcal{A}$ by subsampling. By Lemma~\ref{lem:samp-wo-replace}, the algorithm $\mathcal{A}'$ that, given $m = Kn^2$ samples, subsamples $n$ items without replacement and runs $\mathcal{A}'$ on the result is $(1/\sqrt{Km}, cK/m^2)$-differentially private. Moreover, when run on inputs consisting of i.i.d. samples from a distribution $D$, the output of $\mathcal{A}'$ is identically distributed to that of $\mathcal{A}$, so $\mathcal{A}'$ also solves $\mathcal{T}$ with $m$ examples and failure probability $0.01$.

For a sufficiently large constant $K$ and sufficiently small constant $c$, Corollary~\ref{cor:DPtoRep} then implies that $\mathcal{A}'$ gives rise to a $c$-reproducible algorithm solving $\mathcal{T}$ with $m$ examples and failure probability $0.01$.

Now, consider Algorithm~\ref{alg:Rep-to-DP} adapted to the user-level setting as follows: instead of partitioning a centralized sample (as done in that algorithm), each of the users applies algorithm $\mathcal{A}'$ to their own i.i.d. sample (with the same internal randomness- this can be achieved through a common random string that they share). Then, the outputs are sent to a central server, and $(\eps, \delta)$-DP selection is then applied to choose a heavy hitter (as discussed in Algorithm~\ref{alg:Rep-to-DP}). Let's call this algorithm $\mathcal{A}_u$.

The accuracy guarantees proved for Algorithm~\ref{alg:Rep-to-DP} give us that the failure probability of this Algorithm $\mathcal{A}_u$ is at most $0.02$ for sufficiently small constant $c$. Hence, we are left to argue privacy. Note that changing a single user's sample can change at most one output to which the $(\eps, \delta)$-DP selection algorithm is applied to. Hence, by the privacy of the selection algorithm, we have that $\mathcal{A}_u$ is $(\eps, \delta)$-differentially private. This completes the proof.
\end{proof}





\subsection{Amplification of $\delta$ for Differential Privacy}

The equivalence between reproducibility and differential privacy gives us the first generic amplification theorem for the $\delta$ parameter of approximate differential privacy for general statistical tasks. Prior to our work, it was known that the $\eps$ parameter could be amplified algorithmically and efficiently. That is, using random sampling (Lemma~\ref{lem:samp-wo-replace}), one can improve an $(\eps_0, \delta_0)$-differentially private algorithm to a $(p\eps_0, p\delta_0)$-differentially private one with an $O(1/p)$ blowup in the sample complexity. However, this technique is unable to improve the $\delta$ parameter of such an algorithm asymptotically as a function of the number of samples $n$, e.g., from $\delta(n) = 1/n^{10}$ to $\delta'(n) = \exp(-n^{0.99})$.

The recent characterization of private PAC learnability in terms of Littlestone dimension~\cite{AlonBLMM22, ghazi2021sample} implies that such an amplification of $\delta$ is (at least, in principle) possible for private PAC and agnostic learning and for private query release. Given a target class $\mathcal{C}$ in one of these settings, the existence of a $(\eps = 0.1, \delta = O(1/n^2
\log n))$-differentially private algorithm using a finite number of samples $n$ implies that $\mathcal{C}$ has some finite Littlestone dimension $d$. This in turn implies that, for every $\eps, \delta > 0$, there is an $(\eps, \delta)$-differentially private agnostic PAC learning algorithm for $\mathcal{C}$ using $\poly(d, 1/\eps, \log(1/\delta))$ samples and a private query release algorithm for $\mathcal{C}$ using $\poly(2^{2^d}, 1/\eps, \log(1/\delta))$ samples. Unfortunately, the first part of this argument is non-constructive, and in the worst-case, leads to a final algorithm using a number of samples that is an exponential tower of height $\Omega(n)$! Alon et al.~\cite{AlonBLMM22} posed the open question of whether such amplification could be done algorithmically, even for the special case of private PAC learning.

Our approach is to first convert a differentially private algorithm with weak parameters to a reproducible one. We may then use the fact that reproducible algorithms can be converted back to differentially private ones with excellent privacy parameters. Altogether we obtain a constructive amplification theorem that achieves only a modest blowup in sample complexity, and which applies to general statistical tasks.

\begin{theorem}
    There is a universal constant $c > 0$ such that the following holds. Let $\mathcal{T}$ be a statistical task with a finite output space. Suppose there is an $(\eps = 0.1, \delta = c/n^3)$-differentially private algorithm that solves $\mathcal{T}$ using $n$ samples and with failure probability $\beta$. Then for every $\eps, \delta > 0$, there exists an $(\eps, \delta)$-differentially private algorithm solving $\mathcal{T}$ using
    \[O\left(\frac{\log(1/\beta'\delta)}{\eps}\right) \cdot n^2\]
    samples and with failure probability $\beta + \beta'$.
\end{theorem}

\begin{proof}
Let $A$ be a $(0.1, c/n^3)$-differentially private algorithm solving $\mathcal{T}$ with $n$ samples and failure probability $\beta$. By Lemma~\ref{lem:samp-wo-replace}, the algorithm $A'$ that, given $m = Kn^2$ samples, subsamples $n$ items without replacement and runs $A$ on the result is $(1/\sqrt{Km}, cK/m^2)$-differentially private. Moreover, when run on inputs consisting of i.i.d. samples from a distribution $P$, the output of $A'$ is identically distributed to that of $A$, so $A'$ also solves $\mathcal{T}$ with $m$ samples and failure probability $\beta$.

For a sufficiently large constant $K$ and sufficiently small constant $c$, Corollary~\ref{cor:DPtoRep} implies that $A'$ gives rise to a $0.1$-reproducible algorithm solving $\mathcal{T}$ with $m$ samples and failure probability $\beta$. Applying \mb{insert name of repro-to-DP theorem and check parameters} thus results in an $(\eps, \delta)$-differentially private algorithm solving $\mathcal{T}$ with $m \cdot O(\log(1/\beta'\delta)/\eps)$ samples and failure probability $\beta + \beta'$.
\end{proof}

\subsection{A Realizable-to-Agnostic Reduction for Structured Distributions}
One of the main running examples throughout this work (and indeed a focal point in \cite{ImpLPS22,ghazi2021user} as well) is the PAC-learning paradigm. Traditionally, PAC-learning has two main settings, \textit{realizable} learning (where the adversary must choose a hypothesis in the class), and the \textit{agnostic} setting (which allows an arbitrary adversary). It is a well known fact in the study of traditional statistical learning that realizable and agnostic learning are equivalent up to polynomial blowup in sample complexity \cite{vapnik1974theory,Blumer,haussler1992decision}. Furthermore, an analog of this fact holds for most models of supervised learning, including privacy \cite{beimel2013private}. This was originally shown by Beimel, Nissim, and Stemmer \cite{beimel2013private,alon2020closure}, who gave a sample-efficient agnostic-to-realizable reduction for approximately differentially private PAC-learning. Their result has since been used extensively (see e.g. \cite{bun2016simultaneous,beimel2019private,alon2019limits,alon2020closure,bun2020equivalence}), and is often used to justify focus on the realizable setting.

While certainly impactful, Beimel, Nissim, and Stemmer's reduction (and later improvements on the same \cite{alon2020closure,bun2020equivalence}) are complicated and limited in application. Like the results that came before them (in the traditional setting), their techniques rely heavily on \textit{uniform convergence}, and therefore always incur a cost in VC dimension of the class (or analogously in $\log|H|$ in many settings we consider). Such bounds are typically only useful in the \textit{distribution-free} setting, where the adversary is free to choose arbitrary (often strange, combinatorial) distributions over the data that don't appear in practice. Outside of such cases, it is typically possible to learn in many fewer samples than VC dimension would predict (see e.g.\ \cite{nagarajan2019}), so it is reasonable to ask whether this efficiency can be generically maintained in the agnostic setting. Towards this end, Hopkins, Kane, Lovett, and Mahajan \cite{hopkins2021realizable} recently gave a more generic reduction independent of VC dimension, but their techniques do not adapt directly to the private setting, which was left as an open problem in their work.

We resolve this problem (at least in finite domains) via reduction to and from reproducibility: agnostic private learning requires only a small polynomial blowup over the realizable case that is independent of class-size, even under arbitrary distributional assumptions.
\begin{theorem}\label{thm:agn-to-real}
Let $(H,\mathscr{D})$ be a hypothesis class that is $(1,\frac{1}{poly(n)})$-privately ($\alpha$,$.01$)-PAC learnable in $n=n(\alpha)$ samples in the realizable setting. Then $(H,\mathscr{D})$ is $(1,\frac{1}{poly(m)})$-privately ($\alpha$,$\beta$)-\textbf{Agnostically} learnable in
\[
m(\alpha,\beta) \leq O\left( \frac{n^2\alpha^2+\log^3(\Pi_H(cn^2))}{\alpha^2}\log\beta^{-1}\log\left(\frac{n\log\beta^{-1}}{\alpha}\right)\right)
\]
samples for some universal constant $c>0$.
\end{theorem}
Note that since $\Pi_H(c'n^2) \leq 2^{c'n^2}$, this means agnostic learning experiences at most a polynomial blowup over the realizable setting.
\begin{corollary}
Let $(H,\mathscr{D})$ be a hypothesis class that is $(1,\frac{1}{poly(n)})$-privately ($\alpha$,$,01$)-PAC learnable in $n=n(\alpha)$ samples in the realizable setting. Then $(H,\mathscr{D})$ is $(1,\frac{1}{poly(m)})$-privately ($\alpha$,$\beta$)-\textbf{Agnostically} learnable in
\[
m(\alpha,\beta) \leq \tilde{O}\left( \frac{n^6\log\beta^{-1}}{\alpha^2}\right)
\]
samples.
\end{corollary}
At a high level, the proof of this result is (comparitively) simple. We will reduce to replicability and show how to adapt \cite{hopkins2021realizable}'s 3-line agnostic-to-realizable reduction to this setting, largely by employing techniques developed for our finite learner in the previous section.
\jess{I think this is no longer in a previous section}

\subsubsection{List Heavy-Hitters}
At the core of our reduction is a sub-routine for efficiently (and reproducibly) identifying heavy hitters from a distribution over \textit{lists} of elements. Estimating the heavy hitters of a given distribution is a core subroutine in many reproducible algorithms (used e.g.\ in \Cref{thm:rFinite}), and was studied in \cite{ImpLPS22} (and implicitly in \cite{ghazi2021user}). While outputting a heavy hitter in the list setting can be done via the original algorithm of \cite{ImpLPS22}, this comes at the cost of polynomial factors in the list size (which is typically exponential in the desired parameters). In this section, we show how the same arguments used for our finite learner can also be used to output a heavy hitter with cost only \textit{polylogarithmic} in the list size.

More formally, let $\Omega$ be a finite set, and $\mathcal{D}$ a distribution over subsets of $H$. We call $h \in H$ an $\eta$-heavy-hitter of $\mathcal{D}$ if
\[
\Pr_{S \sim \mathcal{D}}[h \in S] \geq \eta.
\]
We prove it is possible to reproducibly output a heavy hitter with complexity scaling that is only polyloggarithmic in the largest set supported by $\mathcal{D}$.
\begin{theorem}\label{thm:list-heavy-hitters}
For any finite set $\Omega$, $\rho,\eta,\beta>0$, and distribution $\mathcal{D}$ over subsets of $\Omega$ with an $\eta$-heavy-hitter, there exists a $\rho$-reproducible algorithm $\mathcal{A}$ with the following guarantees:
\begin{enumerate}
    \item $\mathcal{A}$ outputs a $\frac{\eta}{2}$-heavy-hitter with probability at least $1-\beta$
    \item $\mathcal{A}$ uses at most $O\left(\frac{\log^2\frac{|\mathcal{D}|\log\frac{1}{\rho\beta}}{\eta}\log\frac{1}{\rho}+\log \frac{1}{\rho\beta}}{\eta^2\rho^2}\log^3\frac{1}{\rho}\right)$ samples from $\mathcal{D}$,
\end{enumerate}
where $|\mathcal{D}|$ is the maximum size subset supported by $\mathcal{D}$.
\end{theorem}
We note that it is easy to modify this result to remove the assumption that $\mathcal{D}$ has a heavy hitter (the algorithm instead outputs `$\bot$' in this case, or can test for the heaviest element), but the simpler version above is sufficient for our applications. We now give the algorithm itself, which combines \cite{ImpLPS22}'s heavy hitters with our thresholding technique for finite learning.

\begin{algorithm}[H]

\KwResult{Reproducibly outputs a heavy hitter}
\nonl \textbf{Input:} Distribution $\mathcal{D}$ over subsets of universe $\Omega$ (Sample Access)\\
\nonl \textbf{Parameters:} 
\begin{itemize}
    \item Reproducibility, confidence, and heaviness $\rho,\beta, \eta > 0$
    \item Sample sizes $t_1 = O\left(\frac{\log(\frac{|\mathcal{D}|}{\rho\beta})}{\eta}\right)$, $t_2 = O\left(\frac{\log^2\frac{|\mathcal{D}|\log\frac{1}{\rho\beta}}{\eta}\log\frac{1}{\rho}+\log \frac{1}{\beta}}{\eta^2\rho^4}\right)$
    \item Threshold accuracy $\tau \leq O(\frac{\rho\eta}{\log|\mathcal{D}|})$
\end{itemize}
\nonl \textbf{Algorithm:}\\
\begin{enumerate}
    \item Sample $t_1$ subsets $C \sim \mathcal{A}$, and call their union $T$.
    \item Sample an additional $t_2$ subsets $C \sim \mathcal{A}$, and call their collection $S = \{C_i\}$.
    \item For each $t \in T$, let $\hat{p}_t$ denote its empirical measure over $S$:
    \[
    \hat{p}_t = \frac{1}{|S|}|\{ C \in S: t \in C\}|.
    \]
    \item Choose a random threshold $v \in \{\eta/4 + 2\tau, \eta/4+6\tau, \ldots 3\eta/4-2\tau\}$
    \item Randomly order $\Omega$
\end{enumerate}
\textbf{return} first $t \in T$ with respect to the order satisfying $\hat{p}_t \geq v$
 \caption{List Heavy Hitters}
 \label{alg:list-amplification}
 
\end{algorithm}
It is not hard to see this algorithm succeeds via the same analysis as for \Cref{thm:rFinite}.
\begin{proof}
By a Chernoff and Union bound, we first note that with probability at least $1-\beta\rho/4$, $T$ contains every $\eta$-heavy-hitter of $\mathcal{D}$. 

Similar to the proof of \Cref{thm:rFinite}, we consider intervals of the form
\[
I_0 = [\eta/4,\eta/4+4\tau], \ldots , I_{1/(2\tau)} = [\frac{3\eta}{4}-4\tau, \frac{3\eta}{4}],
\]
with corresponding threshold positions $v_i=\eta/4+\frac{(2i+1)}{2}\tau$, and the sets
\[
H^{(i)}_1 = \{t \in T: \hat{p}(t,S_1) \leq v_i\}, \quad H^{(i)}_2 = \{t \in T: \hat{p}(t,S_2) \leq v_i\}.
\]
In \Cref{thm:rFinite}, we argued that reproducibility followed from bounding the quantity 
\[
\frac{|H^{(i)}_1 \Delta H^{(i)}_2|}{|H^{(i)}_1 \cup H^{(i)}_2|}
\]
with high probability, as this promised that choosing the first element from a joint random ordering of $\Omega$ usually gives the same answer over $H^{(i)}_1$ and $H^{(i)}_2$. Here we need to be slightly more careful, in that we need to ensure not only that the same element is chosen, but also that it is truly an $\eta/4$-heavy-hitter. This ensures reproducibility despite the fact that our set $T$ depends on samples, because we are promised that all $\eta/4$-heavy-hitters lie in $T$ except with probability $1-\frac{\beta\rho}{4}$ (and therefore have no dependence $T$ itself).

Thankfully, this is already implicit in the proof of \Cref{lem:good-thresholds}, since it is actually proved that, with high probability, the number of elements of $T$ that cross threshold $v_i$ is at most $O(\rho|I_{[i-1}]|)$, where we recall $|I_{[i-1]}|$ denotes the number of elements with true weight in buckets $I_1,\ldots,I_{i-1}$. This followed from the fact that any element $t \in T$ whose true weight lay in the $j$th interval for $j>i$ satisfied:
\[
\Pr[\hat{p}(t,S_j) \leq v_i] \leq e^{-\Omega(j^2\tau^2|S_j|)},
\]
which remains true in this setting for our choice of $|S|$ by Chernoff. As such, our full process remains $\rho$-reproducible for the correct choice of constants as desired. Furthermore, correctness holds with probability at least $1-\beta$, since all weight estimates are correct up to $\eta/4$. Finally, to get the correct dependence on $\rho$ we simply apply the amplification technique used in the proof of \Cref{thm:rFinite}.
% \\
% \\
% \maxh{OLD}
% By a Chernoff and Union bound, we first note that with probability at least $1-\beta\rho/4$, $T$ contains every $\eta$-heavy-hitter of $\mathcal{A}$ and the empirical estimates $\hat{p}_t$ are all within $\tau$ of their true measure. The proof now essentially follows as in \Cref{thm:rFinite}, but we give the details for completeness.
% \rexl{I would not repeat the same proof if possible. In part because I'm lazy; also because, if we make edits to the first instance of this proof, we have to copy them here, and I don't want to deal with that.
% I'd just say ``the rest follows by the same arguments in Proof ..."}\maxh{The proof isn't exactly the same. Maybe we can give a shorter version here and move the below to appendix?}\maxh{Maybe now irrelevant since proof below is wrong.}

% Recall $v$ is chosen randomly from the set $\{\eta/4 + \tau, \eta/4 + 3\tau, \dots, 3\eta/4-\tau\}$. Let $k = \frac{\eta/2}{2\tau}$, and define intervals $I_1 = [\eta/4, \eta/4 +2\tau), I_2 = [\eta/4 +2\tau, \eta/4+ 4\tau), \dots, I_k = [3\eta/4-2\tau, 3\eta/4)$. Note that each choice $v_i = \eta/4+(2i-1)\tau$ lies in the center of its corresponding interval $I_i$.
	
% As in \Cref{thm:rFinite}, choosing a particular $v_i$ is only bad for reproducibility if the true measure of many elements $t \in T$ fall inside the interval $I_i$, as these are the only elements which may cross the threshold $v_i$ under the (high probability) assumption that $\hat{p}_t$ are good estimates. With this in mind, define the set $\risky_i$ to be the set of all concepts in $T$ with (true) measure in $I_i$, and $\safe_i$ to be the set of all elements with measure at least $\eta/4 + 2i\tau$, the upper bound of interval $I_{i}$. Note that $\safe_k$, the set of all elements in $T$ with risk measure at least $3\eta/4$ by the assumption that $\mathcal{A}$ is guaranteed to  have an $\eta$-heavy-hitter.
	
% 	Assume $|\risky_i|/|\safe_i \cup \risky_i| < \rho/4$. Fixing a choice of $v_i$, note that the symmetric difference between two subsets of elements $T_1$ and $T_2$ over two runs of the algorithm is at most the size of $\risky_i$.
% 	By Observation~\ref{obs:random-ordering-of-concepts} (and accounting for the failure probability of our initial assumptions), 
% 	the reproducibility error across two runs fixing $v_i$ and randomizing over $\mathcal{A}$ and the choice of ordering is at most $\rho/2$.
	
% 	Finally, as in \Cref{thm:rFinite} we argue there cannot be many intervals for which this assumption fails.
% 	If $|\risky_i|/|\safe_i \cup \risky_i| \ge \rho/4$, then $|\safe_{i-1}| \ge 4 |\safe_i| /(4-\rho)$.
% 	Since the number of $O(\eta)$-heavy-hitters is at most $O(|\mathcal{A}|/\eta)$, at most $B=O(\log_{4/(4-\rho)}|\mathcal{A}|/\eta)$ intervals can cause larger than $\rho/4$-proportion symmetric differences. 
% 	By our choice of $\tau = O(\frac{\eps \rho}{32 \ln |\mathcal{A}|/\eta})$, the total number of intervals is
% 	$(\eta/2)/(2\tau) \geq \frac{4}{\rho}B$ for the appropriate choice of constants. As a result, the probability of a bad interval is at most $\rho/4$.
	
% 	Thus assuming the empirical measures are indeed well estimated \Cref{alg:list-amplification} is $\rho/2$-reproducible, and since the assumption holds with probability at least $1-\rho/2$ is $\rho$-reproducible altogether. Optimality is immediate from the fact that any $(\eta/8)$-heavy-hitter must have score at most $s^*+\eps$, and any output of the algorithm is an $\eta/8$-heavy-hitter with probability at least $1-\beta$. The sample complexity bound is immediate from construction.
\end{proof}
We note that this result is similar to the pseudo-globally stable learner of \cite{ghazi2021user}, which also uses a method of reproducibly finding a heavy hitter from a distribution on lists. Their algorithm uses a variant of the exponential mechanism instead of random thresholding, and loses polynomial factors over our bound as a result. Both \cite{ghazi2021user} and our algorithm have the downside of only working over finite universes (or more generally in settings where correlated sampling is possible). On the other hand, the problem can be solved \textit{privately} without this assumption. This raise a natural question: does list heavy hitters give an exponential separation between privacy and reproducibility over infinite domains?\footnote{Recall the problem can be solved reproducibly with $\poly(|\mathscr{D}|)$ dependence by \cite{ImpLPS22} even in the infinite setting.}

% We note that in the finite setting of the last section, list amplification can be applied as a blackbox but loses a log factor in the number of samples due to the implementation cost of $\mathcal{A}$.
Before moving on, we note the following immediate implication of list heavy hitters for learning classes with low Littlestone dimension, giving a moderate improvement over the analogous result of \cite{ghazi2021user}.
\begin{corollary}
Let $(X,H)$ be a class with Littlestone dimension $d$. Then the sample complexity of realizably reproducibly learning $(X,H)$ is at most:
\[
n(\rho,\alpha,\beta) \leq \tilde{O}\left(\frac{d^{12}\log^3(1/\beta)}{\alpha^2\rho^2}\right)
\]
\end{corollary}
We give the proof in \Cref{app:little}.

\subsubsection{Reproducible Realizable-to-Agnostic Reduction}
We now show how to combine List Heavy-Hitters with \cite{hopkins2021realizable}'s agnostic-to-realizable technique to generalize their reduction to reproducible learning.
\begin{theorem}\label{thm:agnostic}
Let $(X,H)$ be a reproducibly learnable class with sample complexity $n(\rho,\alpha,\beta)$, and $n'=n(1/4,\alpha/4,\beta/4)$. Then $(X,H)$ is agnostically learnable in
\[
m(\rho,\alpha,\beta) \leq O\left( \frac{\log^2(\Pi_H(n')\log\frac{1}{\rho\beta})\log\frac{1}{\rho}+\log \frac{1}{\rho\beta}}{\alpha^2\rho^2}\left(\alpha^2n' + \log\frac{\Pi_H(n')}{\beta}\right)\log^3\frac{1}{\rho}\right)
% \tilde{O}\left(\frac{n^2\Pi_H(n)^2}{\rho^2(1-\eta)^4} + \frac{\log(1/\beta)}{\rho^2\alpha^2}\right)
\]
samples, where $\Pi_H(n)$ is the growth function\footnote{The growth function measures the worst-case number of labelings across a sample of size $n$, and is at most $n^{O(d)}$ for classes with VC-dim $d$.} of $(X,H)$.
\end{theorem}
Since $\Pi_H(n)\leq 2^n$, this means agnostic learning experiences only a small polynomial blow-up in sample complexity compared to the easier realizable setting. Furthermore, this bound holds even with distributional assumptions, since it does not rely on external quantities such as VC-dimension.



% In the distribution-free setting, learnable classes have finite VC dimension so we immediately get the following corollary by Sauer-Shelah-Perles.
% \begin{corollary}
% Let $(X,H)$ be a reproducibly learnable class with sample complexity $n(\rho,\alpha,\beta)$, and $n'=n(1/4,\alpha/4,\beta/4)$. Then $(X,H)$ is agnostically learnable in
% \[
% m(\rho,\alpha,\beta) \leq O\left( \frac{VC(H)^2\log(n')(VC(H)\log(n')+\log(\frac{1}{\rho\beta}))(\alpha^2n' + VC(H)\log(n')+\log(1/\beta))}{\rho^2\alpha^2}\right).
% \]
% \end{corollary}
% Note that in the distribution-free setting this is at most
% \[
% m(\eta,\nu,\alpha,\beta) \leq ???
% % \tilde{O}\left(\frac{n^{O(VC(X,H))}}{\rho^2(1-\eta)^4} + \frac{\log(1/\beta)}{\rho^2\alpha^2}\right).
% \]

The proof of \Cref{thm:agnostic} is based on the following variant of a sub-routine from \cite{hopkins2021realizable}'s agnostic-to-realizable reduction that generates a small list of good hypotheses.

\begin{algorithm}[H]

\KwResult{Outputs a list of good hypotheses}
\nonl \textbf{Input:} Reproducible learner $\mathcal{L}$ on $n=n(\rho,\alpha,\beta)$ samples, family of random strings $\{r_i\}$\\
\nonl \textbf{Parameters:} 
\begin{itemize}
    \item Accuracy and confidence parameters $\alpha,\beta > 0$
    \item Labeled sample size $t = O(\frac{\log(\Pi_H(n(1/4,\alpha/4,\beta/4))+\log(1/\beta)}{\alpha^2})$
\end{itemize}
\nonl \textbf{Algorithm:}\\
\begin{enumerate}
    \item Sample $n$ (unlabeled) samples $S_U \sim D_X^n$, and $t$ labeled samples $S_L \sim D^t$
    \item Run $\mathcal{L}$ across all strings in $r$ on all possible labelings of $S_U$ to receive:
    \[
    C_r(S_U) \coloneqq \{\mathcal{L}((S_U,h(S_U));r_i): h \in H, r_i \in \{r\} \}
    \]
    \item Prune sub-optimal hypotheses from $C_r(S_U)$:
        \[
    C^\alpha_r(S_U,S_L) \coloneqq \left\{h \in C_r(S_U,S_L): R_{S_L}(h) \leq \min_{h' \in C_r(S)}(R_{S_L}(h')) +\alpha/2 \right\}
    \]
\end{enumerate}
\textbf{return}  $C^\alpha_r(S_U,S_L)$
 \caption{List Distribution Generator}
 \label{alg:agnostic-subroutine}
 
\end{algorithm}
\Cref{alg:agnostic-subroutine} generates a sample from a distribution over families of hypotheses with near-optimal error. The accuracy of $\mathcal{A}$ promises that $C^\alpha_r(S_U,S_L)$ will be non-empty, and its reproducibility guarantees the distribution will have heavy-hitters. This means we can apply list heavy hitters to find a good hypothesis reproducibly.

\begin{proof}[Proof of \Cref{thm:agnostic}]
It is enough to prove that for any distribution $D$ over $X \times Y$, the distribution over lists defined by $C_r^\alpha(S_U,S_L)$ satisfies
\begin{enumerate}
    \item Correctness:
    \[
    \Pr[\forall h \in C_r^\alpha(S_U,S_L): R(h) \leq OPT+\alpha] \geq 1-\beta/2
    \]
    \item Heaviness:
    \[
    \exists h \in H: \Pr[h \in C_r^\alpha(S_U,S_L)] \geq 1/2
    \]
\end{enumerate}
For $\beta$ a small enough constant, any $\Omega(1)$-heavy-hitter has error at most $OPT+\alpha$, so the result then follows immediately from applying list heavy hitters.

% for most families $\{r\} = \{r_i\}_{i=1}^{O(\log(1/\beta))}$ of $O(\log(1/\beta))$ random seeds \Cref{alg:agnostic-subroutine} simulates a list oracle where the score function $s: H \to \R_{\geq 0}$ is given by the risk:
% \[
% s(h)=R(h)= \Pr_{(x,y) \sim D}[h(x) \neq y].
% \] 
% In this framework, the goal of agnostic learning is exactly to output a hypothesis $h \in H$ satisfying $s(h) \leq s^* + \alpha=OPT + \alpha$ with probability at least $1-\beta$, which is promised by the conclusion of \Cref{thm:list-amplify}. Given this fact, it is enough to show that \Cref{alg:agnostic-subroutine} is a list oracle across a $1-\beta/2$ fraction of families $\{r\}$, as the result then follows from picking a random family $\{r\}$ and running \textsc{ListAmplify} (setting the confidence parameter to $\beta/2$) with the resulting list oracle.

The first of these facts, correctness, is essentially trivial and just follows from observing that the size of $C_r(S_U,S_L)$ (the pre-pruned set) is at most $\Pi_H(n)$ by construction. Since we use empirical estimates over $t = O(\frac{\log(\Pi_H(n)/\beta)}{\alpha^2})$ samples, standard Chernoff and Union bounds imply that the empirical error of every element is estimated within $\alpha/4$ of its true value which implies the desired correctness guarantee.

% follows essentially from the analysis of \cite{hopkins2021realizable}, who observed that
% To show these facts, we first argue that over a $1-\beta/4$ fraction of families $\{r\}$, all elements in $C^\alpha_r(S_U,S_L)$ have error at most $OPT+\alpha$ with probability at least $1-\beta/4$. This follows from two observations. First, since the size of $C_r(S_U,S_L)$ (the pre-pruned set) is at most $\Pi_H(n)$ by construction and we use empirical estimates over $t = O(\frac{\log(\Pi_H(n)/\beta)}{\alpha^2})$ samples, standard Chernoff and Union bounds imply that the error of every element is estimated within $\alpha/4$ of its true value. Second, recall that 
% by the guarantee that $\mathcal{L}$ is a realizable learner, $\mathcal{L}(S_U,h_{OPT}(S_U); r)$ is within $\alpha/4$ of $h_{OPT}$ with probability at least $1-\beta/4$ over the randomness of $r$ and $S_U$. Furthermore, since 


% Considering the randomness over $r$ and $S_U$ separately, it must also then be the case that over a $1-\beta/2$ fraction of strings, $\mathcal{L}(S_U,h_{OPT}(S_U))$ is within $\alpha/4$ of $h_{OPT}$ with probability at least $1-\beta/2$ (for small enough $\beta$). Finally, since $C^r(S_U,S_L)$ contains $\mathcal{L}(S_U,h_{OPT}(S_U))$ by construction, this means that over a $1-\beta/2$ fraction of strings we have that $\min_{h' \in C_r(S)}(R_{S_L}(h')) \leq OPT + \alpha/4$ with probability at least $1-\beta/2$, and therefore that all elements with empirical error at most $\min_{h' \in C_r(S)}(R_{S_L}(h')) +\alpha/2$ have true risk at most $OPT+\alpha$ as desired.

It is left to show that $C^\alpha_r(S_U,S_L)$ has a heavy hitter. Fix some $h_{OPT} \in H$ achieving error OPT. Any $\frac{1}{4}$-reproducible learner has the property that over at least half its random strings $r$, there exists some $h_r \in H$ such that:
\[
\Pr_{S_U}[\mathcal{L}((S_U,h_{OPT}(S_U)),r) = h_r] \geq 1/2.
\]
Furthermore, since $\mathcal{L}$ is additionally a PAC-learner, it must also be the case that $h_r$ is within $\alpha/4$ of $h_{OPT}$ over a $1-\beta/4$ fraction of these `good' strings. Since our family consists of $O(\log(1/\beta))$ random strings and $\mathcal{L}((S_U,h_{OPT}(S_U)),r) \in C_r(S_U,S_L)$ by construction, this means the pre-pruned set $C_r(S_U,S_L)$ has a $1/2$-heavy-hitter with error at most $OPT+\alpha/4$ over a $1-\beta/2$ fraction of families $\{r\}$. Finally, since our empirical estimates are good with high probability, $h_r$ also appears in the pruned set $C_r^\alpha(S_U,S_L)$ with at least constant probability as desired. Finally, the sample complexity bound then follows from combining \Cref{thm:list-heavy-hitters} with the observation that generating a sample from $C_r(S_U,S_L)$ requires $O\left(\frac{\alpha^2n' + \log(\Pi_H(n'))+\log(1/\beta)}{\rho^2\alpha^2}\right)$ samples and $|C_r| \leq O(\log(1/\beta)\Pi_H(n'))$ by construction.
\end{proof}

% subroutine introduced by HKLM called \textsc{LearnToCover}. Given a realizable reproducible learner $\mathcal{A}$ on $n=n(\rho,\alpha,\beta)$ samples:
% \begin{enumerate}
%     \item Draw a random seed $r$
%     \item Draw $n$ (unlabeled) samples $S \sim D$
%     \item Run $\mathcal{A}$ on all labelings of S to receive:
%     \[
%     C_r(S) \coloneqq \{\mathcal{A}((S,h(S));r): h \in H \}
%     \]
% \end{enumerate}
% HKLM prove that the distribution over subsets induced by this algorithm is a \textit{non-uniform} $(\alpha,\beta)$-cover, meaning that:
% \[
% \forall h \in H: Pr_{r,S}[\exists h' \in C(S): d_D(h',h) \leq \alpha] \geq 1-\beta.
% \]
% The same guarantee holds in our setting, but we additionally need $C(S)$ to be reproducible. Unfortunately a priori it isn't clear that this is true. To see why, it is useful to define a dual notion of `$\eta$-goodness' for \textit{hypotheses}.
% \begin{definition}[`$\eta$-good' hypotheses]
% For a fixed string $r$ and marginal distribution $D$, we call $h \in H$ $\eta$-good if there exists a 'representative element' $h_r$ such that:
% \begin{equation*}
% Pr_{S \sim D}[A((S,h(S));r) = h_r] \geq 1-\eta.
% \end{equation*}
% \end{definition}
% The issue with using HKLM's subroutine naively is that we only have the following probabilistic guarantee on the goodness of hypotheses in $H$:
% \[
% \forall h \in H: Pr_r[h~\text{is}~\eta\text{-good}] \geq 1-\nu.
% \]
% Since $H$ is infinite, for any fixed $r$ it is possible (indeed likely) that there will be bad hypotheses! We have no control over what these bad hypotheses output, which makes it difficult to analyze the reproducibility of $C_r(S)$. The trick is now to use the heavy-hitters algorithm of ILPS as a subroutine to observe that while we cannot necessarily argue $C_r(S)$ is reproducible, we \textit{can} reproducibly find all elements that appear in $C_r(s)$ with high probability!
% \begin{proposition}\label{prop:heavy-cover}
% Let $\alpha_{\text{off}} \sim [0,(1-\eta)/2]$ be chosen uniformly at random. We can reproducibly output all $(1-\eta-\alpha_{\text{off}})$-heavy-hitters of $C_r(S)$ in:
% \[
% \tilde{O}\left(\frac{n^2\Pi_H(n)^2}{\rho^2(1-\eta)^4}\right)
% \]
% samples.
% \end{proposition}
% \begin{proof}
% This follows from a slight variant on ILPS' algorithm. First, note that there can be at most $\frac{2\Pi_H(n)}{1-\eta}$ heavy elements in our distribution. Sampling $m_1=O(\frac{\log(\frac{\Pi_H(n)}{\rho(1-\eta)})}{1-\eta})$ copies of $C_r(S)$ then ensures every heavy element appears with probability at least $1-O(\rho)$ and costs $nm_1$ total samples from $D$.

% We now have a set of at most $m_2=m_1\Pi_H(n)$ potential heavy hitters. Drawing $O(\frac{m_2^2\log(m_2/\rho)}{\rho^2(1-\eta)^2})$, we can estimate the true weight of each candidate up to $a=O(\rho(1-\eta)/m_2)$ accuracy. Since $\alpha_{\text{off}}$ is drawn uniformly at random, the probability it falls inside any interval of length $O(a)$ around each heavy hitter is at most $O(\rho)$ which gives the desired result.
% \end{proof}
% \maxh{Can we use the same ideas as in \Cref{conj:finite} to improve the above union bound? This step is the main cost.}

% Finally to get our agnostic learner, the key is to observe that there must be a heavy-hitter in our set with near-optimal error. Once this is known, we can simply reproducibly learn the heavy hitters (a finite hypothesis class) to get the desired result.

% \begin{proof}[Proof of \Cref{thm:agnostic}]
% We first show that with high probability, any hypothesis $h$ must be close to its representative $h_r$ in classification distance. Since $\mathcal{A}$ is a PAC-learner, we have that for any fixed $h \in H$ and marginal $D$:
% \[
% \Pr_{r, S \sim D}\left[\Pr_D[\mathcal{A}((S,h(S);r) \neq h] \leq \alpha\right] \geq 1-\beta.
% \]
% Since $\mathcal{A}$ is $(\eta,\nu)$-reproducible, we further know that
% \[
% \Pr_{r}\left[\Pr_{S \sim D}[\mathcal{A}((S,h(S);r) = h_r] \geq 1-\eta \right] \geq 1-\nu.
% \]
% and thus that
% \[
% \Pr_{\eta\text{-good } r}[d_D(h,h_r) \leq \alpha] \geq 1-\frac{\beta}{(1-\eta)(1-\nu)}.
% \]
% With this in mind, our agnostic learner proceeds as follows. Draw a random seed $r$, and use \Cref{prop:heavy-cover} to ($\rho/2$-reproducibly) return a list $C$ of all $(1-\eta-\alpha_{\text{off}})$-heavy hitters under the distribution induced by \textsc{CoverToLearn}. Run our reproducible learner for finite classes from \Cref{conj:finite} over this list on a fresh set of samples

% We first argue this algorithm satisfies PAC-guarantees. We know that with probability at least $(1-\nu)(1-\frac{\beta}{(1-\eta)(1-\nu)})$, $h_{OPT}$ is $\eta$-good and its representative satisfies $err_D(h_{OPT,r}) \leq OPT + \alpha/2$. Furthermore, since $h_{OPT}$ is $\eta$-good, it is a $(1-\eta)$-heavy hitter in the distribution induced by \textsc{CoverToLearn} (and therefore lies in $C$ as long as our heavy-hitter subroutine succeeds). Conditioned on the above, \Cref{conj:finite} (run with accuracy $\alpha/2$) outputs a hypothesis with error at most $OPT+\alpha$ with probability $1-\beta$. Thus in total the algorithm is a $(\alpha, (1-\nu)(1-\rho)(1-\frac{\beta}{(1-\eta)(1-\nu)})(1-\beta))$-PAC learner. Setting parameters appropriately gives the desired sample complexity bounds for a $(\alpha,\beta)$-agnostic learner.

% Finally we argue the algorithm is reproducible. For any fixed $r$, our heavy-hitter sub-routine outputs the same set $C$ with probability at least $1-O(\rho)$. Conditioned on this event, the finite learner also outputs the same element with probability $1-O(\rho)$, so the total reproducibility is $1-O(\rho)$.
% \end{proof}
\subsubsection{Private Realizable-to-Agnostic Reduction}
We are finally ready to prove our agnostic-to-realizable reduction for private learning. We restate the Theorem for ease of reading.
\begin{theorem}[\Cref{thm:agn-to-real} Restated]
Let $(H,\mathscr{D})$ be a hypothesis class that is $(1,\frac{1}{poly(n)})$-privately ($\alpha$,$.01$)-PAC learnable in $n=n(\alpha)$ samples in the realizable setting. Then $(H,\mathscr{D})$ is $(1,\frac{1}{poly(m)})$-privately ($\alpha$,$\beta$)-\textbf{Agnostically} learnable in
\[
m(\alpha,\beta) \leq O\left( \frac{n^2\alpha^2+\log^3(\Pi_H(cn^2))}{\alpha^2}\log\beta^{-1}\log\left(\frac{n\log\beta^{-1}}{\alpha}\right)\right)
\]
samples for some universal constant $c>0$.
\end{theorem}
\begin{proof}
Recall we are given a realizable $(1,\poly(n^{-1}))$-DP, $(\alpha,.01)$-accurate PAC learner $\mathcal{A}$ on $n$ samples. We will convert $\mathcal{A}$ into an agnostic learner via the following 5 step process:
\begin{enumerate}
    \item Amplify privacy to $(m^{-1/2},\poly(m^{-1}))$ by `Secrecy of the Sample' for $m \approx n^2$
    \item Convert $\mathcal{A}$ to a $.01$-reproducible realizable learner $\mathcal{R}$
    \item Convert $\mathcal{R}$ into an agnostic learner $\mathcal{R}_{agn}$
    \item Convert $\mathcal{R}_{agn}$ back into an agnostic private learner $\mathcal{A}_{agn}$
    \item Privately amplify correctness of $\mathcal{A}_{agn}$
\end{enumerate}
Let's formalize this procedure. In the first step, we simply wish to convert $\mathcal{A}$ into a $(cm^{-1/2},\poly(m^{-1}))$-DP, $(\alpha,.02)$-accurate PAC learner on $m$ samples for some small enough constant $c>0$. This can be done by the so-called `Secrecy of the Sample' method: draw $m=O(n^2)$ examples, construct a subset $S$ by independently selecting each example with probability $\frac{1}{\sqrt{m}}$, and return $\mathcal{A}(S)$. For an appropriate choice of constants this is $(cm^{-1/2},\poly(m^{-1}))$-DP, and accuracy only fails if $|S| \leq n$ (which occurs with at most say $.01$ probability). \maxh{Double check this.}

Now that we have our $(cm^{-1/2},\poly(m^{-1}))$-DP, $(\alpha,.02)$-accurate PAC learner, we invoke \Cref{cor:DPtoRep} (applying correlated sampling) to build the $.01$-replicable learner $\mathcal{R}$ on $O(n^2)$ samples that maintains $(\alpha,.02)$-correctness. Applying our agnostic-to-reproducible reduction for replicable learning, this gives an $.01$-replicable $(\alpha,.02)$-correct agnostic learner on
\[
m' \leq O\left(n^2+ \frac{\log^3(\Pi_H(c'n^2))}{\alpha^2}\right)
\]
samples for some constant $c'>0$. Finally, we move back to the private regime via \Cref{thm:Rep-to-DP}, which gives an $(\varepsilon,\delta)$-DP, $(\alpha,.1)$-correct agnostic learner on $O(\frac{m'\log \delta^{-1}}{\varepsilon})$ samples.

It is left to amplify the correctness probability $\beta$. This can be done by running the above algorithm independently $\log(1/\beta)$ times, and privately outputting the best hypothesis on the output set via the exponential mechanism, which one can check results in a ($2\varepsilon$,$\delta$)-DP $(2\alpha,\beta)$-accurate learner (see e.g.\ \cite[Theorem A.1]{sivakumar2021multiclass}).

We have now seen how to build
.a ($\varepsilon$,$\delta$)-DP ($\alpha$,$\beta$)-accurate learner on 
\[
m'' \leq O\left(\frac{m'\log\delta^{-1}\log\beta^{-1}}{\varepsilon}\right)
\]
samples. To give the form of the result in the theorem statement, it is enough to choose sample size $t$ satisfying the recurrence $t \geq \Omega(m'\log t \log \beta^{-1})$. Selecting $t=c_2m'\log \beta^{-1}\log (m'\beta^{-1})$ for large enough $c_2>0$ then completes the proof.
\end{proof}


\subsection{Reproducible Algorithms from Reduction}
\sstext{In progress.}
In this section, we show how we can use our reduction from reproducibility to differential privacy to obtain new reproducible algorithms. Our reduction preserves accuracy, because on any fixed dataset, the output distribution of the reproducible algorithm is identical to that of the differentially private algorithm (since our reduction simply applies correlated sampling to the output distribution of the differentially private algorithm on the input dataset). 

\subsubsection{PAC Learning}

We note that what we term `reproducible' PAC learning corresponds to settings where the algorithm $A$ is a PAC learner, and additionally is reproducible for all input distributions $P$. 

We show that our reduction gives the best known sample complexity bounds for reproducible realizable and agnostic PAC learning for many hypothesis classes. Prior work also had to prove sample complexity bounds separately for all of these frameworks, whereas we are able to translate bounds proved for differential privacy directly through our reduction.

\paragraph{Thresholds:} Fix any integer $d \geq 0$. We apply our framework to the hypothesis class $Thresh_d$ consisting of thresholds over the domain $\{0,1,\dots,d\}$. A threshold function $f_z$ parameterized by integer $0 \leq z \leq d$, is defined as follows.
\begin{equation}
f_z(x) = 
\begin{cases}
1 & \text{ if } x > z \\
0 & \text{ if } x \leq z
\end{cases}
\end{equation}

The paper by Impagliazzo et al. \cite{ImpLPS22} gave an PAC learner for this problem with sample complexity $O_{\alpha, \beta}(2^{\log^* d})$ (this problem is equivalent to the approximate median problem they describe, there are reductions from and to this problem from properly PAC learning thresholds \cite{BunNSV15} \sstext{Do the reductions work reproducibly?}). They left it as an open problem whether it was possible to achieve sample complexity polynomial in $log^* d$. Our reduction closes this exponential gap by using a result of \cite{KaplanLMNS20} on learning thresholds privately. \sstext{Improve the dependence on $\alpha$ by starting with constant $\alpha$ and boosting/starting with interior point problem and then converting (not sure known method works reproducibly- may need a little thought).}

We first introduce the interior point problem.
\begin{definition}
An algorithm solves the interior point problem over a totally ordered domain $\mathcal{X}$ with error probability $\beta$, if for all datasets $X \in \mathcal{X}^n$, if
$$ \Pr(\min_i X_i \leq A(X) \leq \max_i X_i ) \geq 1-\beta. $$
\end{definition}
Now we are ready to apply our reduction to obtain the improved sample complexity.
\begin{theorem}
For all sufficiently small $\rho, \alpha, \beta \in (0,1)$, there exists a $\rho$-reproducible, $(\alpha, \beta)$-accurate realizable PAC learner for the hypothesis class $Thresh_d$ with sample complexity 
\begin{align*}
    n = \tilde{O}\left( \frac{(\log^* d)^3 \log^2(1/\beta) \log^4(1/\rho)}{\alpha^2 \rho^2} \right)
\end{align*}
\end{theorem}
\begin{proof}
Let $\eps$ and $\delta$ be set as specified in Corollary~\ref{cor:DPtoRep}. The work of \cite{KaplanLMNS20} (Theorem 4.1 in their paper) give an $(\eps/2, \delta/2)$ algorithm for solving the interior point problem with sample complexity $O(\frac{1}{\eps} (\log^* d \log(1/\delta))^{1.5})$ and error probability at most $1/10$. By a result of Bun et al. \cite[Theorem 5.6, Part 1]{BunNSV15}, this gives an $(\eps, \delta)$-DP, $(\alpha, 2/10)$-proper PAC learner for $Thresh_d$ with sample complexity $O(\frac{1}{\eps \alpha} \left(\log^* d \log(1/\delta)\right)^{1.5})$. Now, by work of \cite{BunCS20} (See \cite[Theorem A.1]{SivakumarBG21} for a formal statement we use directly) this can be boosted to give an $(\eps, \delta)$-DP, $(\alpha, \beta)$-accurate proper PAC learner for $Thresh_d$ with sample complexity $O(\frac{1}{\eps \alpha} \left(\log^* d \log(1/\delta)\right)^{1.5} \log(1/\beta))$. 

First, we note that correlated sampling does not affect the accuracy guarantees since it maintains the distribution of the differentially private algorithm. Now, applying Corollary~\ref{cor:DPtoRep}, and substituting in the values of $\eps$ and $\delta$ we get that there is a $\rho$-reproducible $(\alpha, \beta)$-accurate PAC learner for $Thresh_d$, whose sample complexity is the solution to the equation
$$n = C\frac{\sqrt{n \log(1/\rho)}}{\rho \alpha} \left(\log^* d \log(n/\rho)\right)^{1.5} \log(1/\beta)$$
This gives us that 
$$n = \tilde{O}\left(\frac{\log^4(1/\rho)}{\rho^2 \alpha^2} \left(\log^* d \right)^{3} \log^2(1/\beta)\right)$$
\end{proof}
\paragraph{Finite Hypothesis Classes:} Next, we study finite hypothesis classes $H$. By applying a result of \cite{kasiviswanathan2011can} on privately (agnostically) learning these classes, we get a learner with sample complexity that's polynomial in $\log |H|$. 
\begin{theorem}
For all sufficiently small $\rho, \alpha, \beta \in (0,1)$, and for all finite hypothesis classes $H$, there exists a $\rho$-reproducible, $(\alpha, \beta)$-accurate agnostic PAC learner for $H$ with sample complexity 
\begin{align*}
    n = O\left( \frac{(\log |H| + \log(1/\beta))^2  \log (1/\rho)}{\alpha^2 \rho^2} \right)
\end{align*}
\end{theorem}
\begin{proof}
Let $\eps$ be set as specified in Corollary~\ref{cor:DPtoRep}. The work of \cite{kasiviswanathan2011can} gives an $(\eps, 0)$-DP agnostic learner for finite classes with sample complexity $n=O\left((\log |H| + \log(1/\beta))\left(\frac{1}{\alpha \eps} + \frac{1}{\alpha^2}\right)\right)$.

We note that correlated sampling does not affect the accuracy guarantees since it maintains the distribution of the differentially private algorithm. Hence,substituting the value of $\eps$ and applying Corollary~\ref{cor:DPtoRep}, we get a $\rho$-reproducible $(\alpha, \beta)$-accurate agnostic PAC learner for finite class $H$, whose sample complexity is the solution to the equation
$$n=O\left((\log |H| + \log(1/\beta))\left(\frac{\sqrt{n \log(1/\rho)}}{\alpha \rho} + \frac{1}{\alpha^2}\right)\right),$$
which gives us a quadratic in $\sqrt{n}$. Solving, we get that
\begin{align*}
    n = O\left( \frac{(\log |H| + \log(1/\beta))^2  \log (1/\rho)}{\alpha^2 \rho^2} \right)
\end{align*}
\end{proof}















\newpage


\bibliographystyle{amsalpha}  
\bibliography{references} 
\appendix
\section{Estimating OPT}\label{app:OPT}
In this section we give an algorithm for reproducibly estimating the minimum error hypothesis in a class $(X,H)$ over an arbitrary joint distribution $D$ over $X \times \{0,1\}$.

\begin{algorithm}[H]

\KwResult{Outputs $v \in [OPT, OPT+\alpha/2]$}
\nonl \textbf{Input:} Finite Class $H$, Joint Distribution $D$ over $X \times \{0,1\}$ (Sample Access)\\
\nonl \textbf{Parameters:} 
\begin{itemize}
    \item Reproducibility, Accuracy, Confidence $\rho, \alpha, \beta>0$
    \item Sample Complexity $m=m(\rho, \alpha,\beta) \leq O\left(\frac{\log(\frac{|H|}{\beta\rho})}{\alpha^2 \rho^2} \right)$
    % \item Reproducibility threshold $\tau \leq O(\frac{\alpha \rho}{ \ln |H|})$
\end{itemize}
\nonl \textbf{Algorithm:}\\
\begin{enumerate}
    \item Draw a labeled sample $S \sim D^m$ and compute $\emprisk(f, S)$ for every $f \in H$.
    \item $a \leftarrow_r [0,\alpha/16]$ 
    \item $B_i = [i\alpha+a, (i+1)\alpha+a)$
\end{enumerate}
\textbf{return} $\frac{j}{8}\alpha+a$, where $OPT_S+\alpha/4 \in B_j$
 \caption{Reproducibly estimate OPT}
\label{alg:OPT-estimate}
\end{algorithm}
\begin{lemma}
Let $\mathscr{D}$ be a joint distribution over $X \times \{0,1\}$ and $H$ a concept class over $X$. Then for any $\alpha,\beta,\rho>0$, \Cref{alg:OPT-estimate} is a $\rho$-reproducible algorithm over $O\left(\frac{\log(\frac{|H|}{\rho\beta})}{\rho^2\alpha^2}\right)$ samples that outputs a good estimate of $OPT$ with high probability:
\[
\Pr_{r,S}\big[\mathcal{A}(S) \in [OPT,OPT+\alpha/2]\big] \geq 1-\beta.
\]
\end{lemma}
\begin{proof}
The proof is similar to the randomized rounding trick introduced in \cite{ImpLPS22} for reproducible statistical queries. Assume for simplicity that $\frac{1}{8\alpha}$ is integer (the argument is essentially no different otherwise), and break the interval $[0,1]$ into $\frac{\alpha}{8}$-sized buckets:
\[
B_1 = \left[0,\frac{\alpha}{8}\right), \ \ldots \ , B_{\frac{1}{\alpha}} = \left[1-\frac{\alpha}{8},1\right].
\]
Consider the rounding scheme \textsc{Round} that maps $OPT_S$ to the upper limit of its corresponding bucket. Notice that as long as $OPT$ is not within $\frac{\rho\alpha}{64}$ of the threshold value between two buckets, uniform convergence promises that $OPT_{S_1}$ and $OPT_{S_2}$ will lie in  the same bucket with probability at least $1-\frac{\rho}{2}$. As such the problem only occurs at the boundaries, which can be fixed by randomly shifting the thresholds between each bucket by $a \in [0,\frac{\alpha}{16}]$. Then for any fixed value of $OPT$, the probability it lies within $\frac{\rho\alpha}{64}$ of a shifted boundary is at most $\frac{\rho}{2}$, which combined with the previous observation proves the algorithm $\rho$-reproducible.

Towards correctness, observe that uniform convergence of finite classes promises that the empirical optimum $OPT_S$ is within $\frac{\alpha}{16}$ of the true optimum with probability at least $1-\beta$. Furthermore, rounding shifts any value by at most $\frac{3\alpha}{16}$. Thus $\textsc{Round}(OPT_S+\alpha/4) \in [OPT,OPT+\frac{\alpha}{2}]$ with high probability as desired.
\end{proof}
\section{Learning Finite Littlestone Classes}\label{app:little}
One immediate application of list heavy-hitters is a sample-efficient reproducible algorithm for classes with finite Littlestone dimension, as in \cite{ghazi2021sample}, leading to a modest improvement in sample complexity over the best known bound of $\tilde{O}(d^{14})$.
\begin{theorem}
Let $(X,H)$ be a class with Littlestone dimension $d$. Then the sample complexity of realizably reproducibly learning $(X,H)$ is at most:
\[
n(\rho,\alpha,\beta) \leq \tilde{O}\left(\frac{d^{12}\log^3(1/\beta)}{\alpha^2\rho^2}\right)
\]
\end{theorem}
\begin{proof}
In their work on user level privacy, Ghazi, Kumar, and Manurangsi \cite{ghazi2021user} build on the work of \cite{ghazi2021sample} to show the existence of an algorithm outputting lists of hypotheses satisfying the following guarantees:
\begin{enumerate}
    \item \textit{Optimality}: With probability at least $1-\beta/2$, all hypotheses output by $\mathcal{L}$ have risk at most $\alpha/2$
    \item \textit{Heavy Hitter}: There exists $h \in H$ output with probability $\Omega(1/d)$
    \item \textit{Size}: $\mathcal{L}$ outputs at most $exp(d^2+d\log \frac{d}{\alpha\beta})$ hypotheses
    \item \textit{Sample Complexity}: $\mathcal{L}$ uses at most $\tilde{O}(\frac{d^6\log^2 \frac{1}{\beta}}{\alpha^2})$ samples.
\end{enumerate}
Note that for $\beta \leq O(1/d)$, any heavy hitter of this distribution is a good hypothesis, so it is enough to reproducibly output such a heavy hitter. Applying \Cref{thm:list-heavy-hitters}, this can be done $\rho$-reproducibly and with probability at least $1-\beta$ using
\[
O\left(\frac{\log^2\frac{|\mathcal{D}|\log\frac{1}{\rho\beta}}{\eta}\log\frac{1}{\rho}+\log \frac{1}{\rho\beta}}{\eta^2\rho^2}\log^3\frac{1}{\rho}\right)=\tilde{O}\left(\frac{d^6\log\frac{1}{\rho}+d^2\log \frac{1}{\rho\beta}}{\rho^2}\log^3\frac{1}{\rho}\right)
\]
i.i.d outputs of the list algorithm. Each output itself costs $\tilde{O}(\frac{d^6\log^2 \frac{1}{\beta}}{\alpha^2})$ samples to generate, leading to the stated sample complexity.
\end{proof}



\newpage


\section{Additional Properties of Reproducibility}

\subsection{Randomness Management}

Often, we design reproducible algorithms to use randomness for multiple purposes. What if the number of bits used for each purpose varies between runs of the algorithm? The following argument shows that we can guarantee that the same sections of the random string are used for the same purposes across both runs.

\begin{claim}
Say an algorithm $\mathcal{A}$ makes at most $k$ calls to its randomness oracle, using (at most) $b_1, \dots, b_k$ bits of randomness for each call respectively. Then there is an algorithm $\mathcal{A}'$ that reproducibly uses at most $k \cdot \max_{i \in [k]}\{b_i\}$ bits of randomness. 
\end{claim}

Here, by ``reproducibly uses" we mean that algorithm $\mathcal{A}'$ uses the same positions in the random string for every run of the algorithm. 
\begin{proof}
Have $\mathcal{A}'$ interpret its random string as follows: rather than uses randomness sequentially for each of $k$ purposes (non-reproducible if the required number of bits changes), portion the random string into $k$ pieces in a modular way. That is, assign a position to each bit of randomness, and let the bits in positions $i \mod k$ be used solely for the $i$'th call to the randomness oracle by algorithm $\mathcal{A}$. 
This canonical ordering of the randomness ensures that all runs of the algorithm uses the same subsections of the given random string. 
At most $k \cdot \max_{i \in [k]}\{b_i\}$ bits of randomness are used. 
\end{proof}

Note that the algorithm itself does not need to know how much randomness it will use a priori to use this method. 

What if the algorithm does not have a fixed number of calls to the randomness oracle? As long as the randomness calls occur sequentially, one can assign consistent subsections of the random string to each possible call. To do so, we use the same snake-path trick (the Cantor pairing function) used to equate the cardinality of the natural numbers and rational numbers. 

\begin{claim}
Say an algorithm $\mathcal{A}$ makes $k$ calls to its randomness oracle, using (at most) $b_1, \dots, b_k$ bits of randomness for each call respectively. Then there is an algorithm $\mathcal{A}'$ that reproducibly uses at most $(k + \max_{i \in [k]}\{b_i\})^2/2 + (k + \max_{i \in [k]}\{b_i\})/2$ bits of randomness. 
\end{claim}

\begin{proof}
We allocate bits from our randomness oracle to different (unknown bit-length) calls using the Cantor pairing function. The maximum overhead in bit complexity of the randomness occurs when the $k$'th randomness call uses the most bits. In this case, about half of a $(k + \max_{i \in [k]}\{b_i\})$ by $(k + \max_{i \in [k]}\{b_i\})$ grid of random bits must be drawn.
\end{proof}
Again, the algorithm itself does not need to know how much randomness it will use a priori to use this method. It also does not need to know how many different calls $k$ to the randomness oracle will be performed, so long as these calls are sequential. 


\rexl{Possibly add more info about how to juggle randomness, if you don't know if a randomness call will be made at all. Is there a better strategy than just the naive one, where we allocate space for calls that might not even happen? (Say if there's an if conditional that may or may not generate a call to the randomness oracle).}

\rexl{If we start caring about the complexity of randomness used, it could be interesting to look into other pairing functions, e.g. {\url{https://en.wikipedia.org/wiki/Pairing_function\#Other_pairing_functions}}}

\rexl{Probably a better pairing function: \url{http://szudzik.com/ElegantPairing.pdf}}




%\subsection{Reproducibility Lemmas}

\subsection{Reproducibility across Two Close Distributions}

First, we describe a basic Lemma describing how distributional shift can affect reproducibility.

\begin{lemma}[Reproducibility under Distributional Shift]
\label{lem:repr-loss-distributional-shift}
Let $D_1$ and $D_2$ be two distributions over $\X$ with total variational distance $d_{TV}(D_1, D_2) = \delta$. Let $\rho \ge 0$, and let $\Acal$ be a $\rho$-reproducible algorithm that draws a sample of size exactly $m$. 
Then
$$
\Pr_{\vec{s}_1 \sim D_1^m, \vec{s}_2 \sim D_2^m, r} [\Acal(\vec{s}_1; r) = \Acal(\vec{s}_2; r)] 
\ge (1-\delta)^{2m} \rho.$$
\end{lemma}

\begin{proof}
Since $d_{TV}(D_1, D_2) = \delta$, there exist distributions $D, D',$ and $D''$ such that 
$D_1 = (1-\delta)D + \delta D'$ and $D_2 = (1-\delta) D + \delta D''$. 
Thus,

\begin{equation*}
    \begin{split}
        \Pr_{\vec{s}_1 \sim D_1^m, \vec{s}_2 \sim D_2^m, r} [\Acal(\vec{s}_1; r) = \Acal(\vec{s}_2; r)] 
        &=
        (1-\delta)^{2m} \Pr_{\vec{s}_1 \sim D^m, \vec{s}_2 \sim D^m, r} [\Acal(\vec{s}_1; r) = \Acal(\vec{s}_2; r)] 
        \\&\qquad + \dots 
        \\&\qquad + \delta^{2m} \Pr_{\vec{s}_1 \sim D'^m, \vec{s}_2 \sim D''^m
        , r} [\Acal(\vec{s}_1; r) = \Acal(\vec{s}_2; r)] 
        \\&\ge   (1-\delta)^{2m} \Pr_{\vec{s}_1 \sim D^m, \vec{s}_2 \sim D^m, r} [\Acal(\vec{s}_1; r) = \Acal(\vec{s}_2; r)] 
        \\&= (1-\delta)^{2m} \rho
    \end{split}
\end{equation*}

\end{proof}

%This relationship is also tight. OR IS IT? I think it is, but with a very contrived reproducible algorithm

\iffalse

Note that this relationship is tight. Let $\supp(\Acal(D))$ denote the possible outputs of $\Acal$ when run on inputs in the support of $D$. If $\supp(\Acal(D')) \cap \supp(\Acal(D'')) = \supp(\Acal(D')) \cap \supp(\Acal(D)) = \supp(\Acal(D'')) \cap \supp(\Acal(D)) = \emptyset$, then $\Acal$ outputs the same result across the two runs only if $x_1$ and $x_2$ are each sampled from $D$ (and not from $D'$ or $D''$). 

%Note that a similar argument applies for the $(\eta, \nu)$ definition of reproducibility. 

A similar lemma holds for reproducible algorithms with sample complexity $m$.
\fi










%%%%%%%%%%
%%%%%%%%%%
%%%%%%%%%%
%Russell's argument; lower bound for reproducible algorithms that solve problems with different answers for two mutually bounded distributions. 

Next, we consider distributions that are not only close in total variation distance, but also close for each element in the support. If the learning problem has disjoint sets of correct answers for the two distributions, then there is a sample lower bound for any correct reproducible algorithm solving this problem.


\begin{definition}[Mutually Bounded Distributions]
\label{def:mutually-bounded-distributions}
Distributions P and Q are $\eta$-mutually bounded if $P(x) (1 -\eta)  < Q(x) < P(x) (1+ \eta)$ for all $x$ in the support of $Q$.
\rexl{Should this definition also include $x$ in the support of $P$? (i.e. does the definition need to be symmetric?)}
\end{definition}

\rexnote{Double check the following lemma and proof}

\begin{lemma}
\label{lem:distributional-shift-mutually-bounded-distributions}
Let $0 < \eta < 1/2$. If distributions $P$ and $Q$ are $\eta$-mutually bounded, and $A$ is a $\rho$-reproducible algorithm in which the correct outputs for $P$ and $Q$ are disjoint,
 then $A$ uses at least $\Omega( \log (1/\rho)/\eta^2)$ samples.
\end{lemma}


\begin{proof}
 Let $\eta(x)$ be such that $P(x)= (1+ \eta(x))Q(x)$, i.e., $\eta(x)=P(x)/Q(x)-1$.   $\eta(x)$ is bounded between $-\eta$ and $\eta$,
and when $x$ is chosen from $Q$, $\E[\eta(x)]=0$, since $\sum \eta(x) Q(x)=
\sum (P(x)-Q(x)) = \sum P(x) - \sum Q(x) = 1-1$.\rexl{For this to be true, it must be the case that $\supp(P) = \supp(Q)$, no?}

Then $\E_{x \sim Q} \log (P(x)/Q(x))= \E_{x \sim Q} \log (1+ \eta(x))=
\E_{x \sim Q} \eta(x) + O(\eta^2)= O(\eta^2)$.  
If we look at the random variable $x_1\dots x_m$ chosen from $Q$,
$\log (P(x_1)....P(x_m)/Q(x_1)....Q(x_m)) $, this is the sum of  $m$ independent copies of the random variable above, so it is $O(\eta^2 m + \eta m^{1/2} k)$ with probability all but $\exp (-k)$.   Call the set of $x$ where this is not true $Bad$.

For a fixed random string, let $S$ be the set of tuples so that $A[x_1..x_m]$ outputs the most likely string for samples sets drawn from $Q$.  The probability for these samples of landing in $S$ is $1- \rho$.  The probability of landing in $S$ but not $Bad$ is $1-\rho - exp(-k)$, and we can pick constant $k$ for which this quantity is at least $1/2$. For each such $x$, its probability under $P$ is at least $\exp (-\eta^2 m - \eta m^{1/2}) $ fraction of what it would be under $Q$. So, if  $m = o (\log (1/\rho) /\eta^2)$, this is bigger than $\omega(\rho)$,  which means there is a greater than $\rho$ chance of landing in $S$ when $x_1 \dots x_m$ are chosen from $P$.  This contradicts either correctness or reproducibility on $P$.

\begin{definition}
\label{def:ratio-bounded}
Call $P$ and $Q$ $r$-ratio-bounded if $P(x)/r < Q(x) < rP(x)$ for all $x$ in the support.
\end{definition}

\begin{lemma}
If $P$ and $Q$ are $r$-ratio-bounded and $A$ is a $\rho= o(1/r)$ reproducible algorithm and the sets of correct answers for $P$ and $Q$ are disjoint, then  $A$ uses  $\Omega ( 1/(r\rho))^2$ samples.
\end{lemma}


Let  $\gamma= o (\rho)$ so that $1/\gamma$ is an integer, and let $P_i$ be the distribution that samples from
$P$ with probability $i \gamma$ and $Q$ otherwise. 

\begin{claim}
$P_i$ and $P_{i+1}$ are $\eta$-mutually bounded for $\eta= o(r \rho)$.  
\end{claim}

\begin{equation*}
    \begin{split}
        \frac{P_{i+1} (x)}{P_{i} (x) }
        &= 
        \frac{( i+1) \gamma P(x)  + (1-(i +1)\gamma) Q(x) }{i \gamma P(x) + (1-i \gamma )Q(x)} 
        \\&= 1 + \frac{\gamma (P(x)-Q(x))}{i \gamma P(x) + (1-i \gamma )Q(x)}
        \\&\le 1 + \frac{\gamma (P(x)+Q(x))}{\min(P(x),Q(x))}
        \\&\le 1 + \gamma(r+1)
    \end{split}
\end{equation*}

The last inequality follows from the definition of $r$-ratio-boundedness.  

By the previous Lemma, with probability $1 - O(\rho)$, if $m$ is $o(1/ (r\gamma)^2)$,  the most likely outputs are the same for both $P_i$ and $P_{i+1}$. By a union bound, they are the same for $P_0=P$ and $P_{1/\gamma}= Q$ with $1-o(1)$ probability, a contradiction.


\end{proof}




\section{Glossary}
\begin{itemize}
\item $(\eps, \delta)$ - DP parameters
\item $H$ - hypothesis classes
\item $f,g,h$- target functions/functions from hypothesis class
\item $\mathcal{X}$- input spaces
\item $\mathcal{Y}$- output spaces
\item $(\alpha, \beta)$- accuracy parameters, failure probability is $\beta$.
\item $\rho$- reproducibility parameter
\item $m$ (with subscripts if necessary)- sample complexity, size of datasets
\item $D$- distributions (subscripts for multiple distribution)
\item $P$- family of distributions
\item $err$- learning error (with appropriate subscripts for sample and distribution)
\item $\vec{s}$ - input datasets. subsamples- subscript? Ask Toni.
\item $x,y$- single data point, single output.
\item $\beta, \eps, \delta$- for Perfect Generalization (but if you hvae to talk about it in context of learning, change $\beta$.
\item $r$- internal coins
\item ; - separate sample and internal randomness in algorithms
\item [] - for probabilities $\Pr$ (capital P), and expectations $\E$
\item $d$- (notions of) dimension
\item $\Delta$ - symmetric difference
\item $\dtv$-total variation distance
\item $\approx_{\eps, \delta}$- Max KL divergence
\item RETURN command in algorithms environment
\item KWresult and input in that order in beginning of algorithms environment
\item Use algorithmic environment (and not enumerate *cough* Max *cough*)
\item $p$ - Bernoulli biases (with subscripts if necessary).
\item $b$- bit used for message
\item $c$- ciphertext
\item subscript $i,j$ for dual indexing.
\item $O$- set of outputs.
\item in general use (specific) subscripts to disambiguate.
\item $\Acal, \Bcal$ - algorithms
\item use a sample if implied distribution, and dataset if no distributional assumptions. And example for a single thing in a sample.
\item $I$ - interval
\item $v$ - threshold values
\item $(\eta, \nu)$- 2 parameter definition of reproducibility
\item $\mathbin\Vert$ concatenation of strings
\end{itemize}





\end{document}