


\section{Experimental Results}\label{sec:experiments}

We presented in the previous section how to leverage watermarks for detection and identification of images generated from text prompts.
We now present more general results on robustness and image quality for different generative tasks.
We also compare Stable Signature to other watermarking algorithms applied post-generation.

\begin{SCtable*}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
        \begin{tabular}{ c l @{\hspace{2pt}} l  *{2}{c} *{4}{p{15pt}}}
        \toprule
       & & \multirow{2}{*}{}          & \multirow{2}{*}{PSNR / SSIM $\uparrow$} & \multirow{2}{*}{FID $\downarrow$} &\multicolumn{4}{c}{Bit accuracy $\uparrow$ on:} \\ 

    &                                         &                           &           &                                            &  None & Crop & Brigh. & Comb.  \\ \midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Tasks}}  
        & Text-to-Image                      & LDM~\cite{rombach2022ldm}             & $30.0$ / $0.89$   & $19.6$ \color{orange}{($-0.3$)} & $0.99$ & $0.95$ & $0.97$ & $0.92$ \\ \cmidrule{2-9}
       & Image Edition                       & DiffEdit~\cite{couairon2022diffedit}                  & $31.2$ / $0.92$ & $15.0$ \color{orange}{($-0.3$)}       & $0.99$ & $0.95$ & $0.98$ & $0.94$ \\ \cmidrule{2-9}
       & Inpainting - Full          & \multirow{2}{*}{Glide~\cite{nichol2021glide}}   & $31.1$ / $0.91$  & $16.8$ \color{orange}{($+0.6$)} & $0.99$ & $0.97$ & $0.98$ & $0.93$ \\ 
       & {\color{white}Inpa} - Mask only          &                                   & $37.8$ / $0.98$  & $9.0$~~ \color{orange}{($+0.1$)} & $0.89$ & $0.76$ & $0.84$ & $0.78$\\ \cmidrule{2-9}
       & Super-Resolution & LDM~\cite{rombach2022ldm}  & $34.0$ / $0.94$ & $11.6$ \color{orange}{($+0.0$)}      & $0.98$ & $0.93$ & $0.96$ & $0.92$ \\ 
    \midrule \rule{0pt}{8pt} \rule{0pt}{8pt} 
\multirow{7}{*}{\rotatebox[origin=c]{90}{WM Methods}} 
           & \emph{Post generation} \\
           & Dct-Dwt \cite{cox2007digital}                      & $0.14$ (s/img)      &  $39.5$ / $0.97$  & $19.5$ \color{orange}{($-0.4$)} & $0.86$ & $0.52$ & $0.51$ & $0.51$ \\ 
           & SSL Watermark \cite{fernandez2022sslwatermarking}  & $0.45$ (s/img)      &  $31.1$ / $0.86$  & $20.6$ \color{orange}{($+0.7$)} & $1.00$ & $0.73$ & $0.93$ & $0.66$ \\ 
           & FNNS \cite{kishore2021fixed}                       & $0.28$ (s/img)      &  $32.1$ / $0.90$  & $19.0$ \color{orange}{($-0.9$)} & $0.93$ & $0.93$ & $0.91$ & $0.93$ \\ 
           & HiDDeN \cite{zhu2018hidden}                        & $0.11$ (s/img)      &  $32.0$ / $0.88$  & $19.7$ \color{orange}{($-0.2$)} & $0.99$ & $0.97$ & $0.99$ & $0.98$ \\ \cmidrule{2-9}
           & \emph{Merged in generation} \\
           & Stable Signature                            & $0.00$ (s/img)             &  $30.0$ / $0.89$ & $19.6$ \color{orange}{($-0.3$)}     & $0.99$ & $0.95$ & $0.97$ & $0.92$ \\ 
        \bottomrule 
    \end{tabular}
    \caption{
        Generation quality and comparison to post-hoc watermarking on 512$\times$512 images and $48$-bit signatures.
        PSNR and SSIM are computed between generations of the original and watermarked generators.
        For FID, we show in {\color{orange} (color)} the difference with regards to original.
        Post-hoc watermarking is evaluated on text-generated images.
        (App.~\ref{sec:supp-robustness} gives results on more transformations, and App.~\ref{app:implementation-details} gives more details on the evaluations.)
        Overall, Stable Signature has minimal impact on generation quality. It has comparable robustness to post-hoc methods while being rooted in the generation itself.
    }\label{tab:quality-watermarking} \vspace*{-0.8cm}
\end{SCtable*}



\subsection{Tasks \& evaluation metrics}
Since our method only involves the LDM decoder, it makes it compatible with many generative tasks. 
We evaluate text-to-image generation and image edition on the validation set of MS-COCO~\cite{lin2014microsoft}, super-resolution and inpainting on the validation set of ImageNet~\cite{deng2009imagenet} 
(all evaluation details are available in App.~\ref{app:evaluation}).

We evaluate the image distortion with the Peak Signal-to-Noise Ratio (PSNR), which is defined as $\mathrm{PSNR}(x,x') = -10\cdot \log_{10} (\mathrm{MSE}(x,x'))$, for $x,x'\in [0,1]^{c\times h\times w}$, as well as Structural Similarity score (SSIM)~\cite{wang2004image}.
They compare images generated with and without watermark. 
On the other hand, we evaluate the diversity and quality of the generated images with the Fr\'echet Inception Distance (FID)~\cite{heusel2017gans}.
The bit accuracy -- the percentage of bits correctly decoded -- evaluates the watermarks' robustness.



\subsection{Image generation quality}
\autoref{fig:qualitative} shows qualitative examples of how the image generation is altered by the latent decoder's fine-tuning. 
The difference is very hard to perceive even for a trained eye. 
This is surprising for such a low PSNR, especially since the watermark embedding is not constrained by any Human Visual System like in professional watermarking techniques. 
Most interestingly, the LDM decoder has indeed learned to add the watermark signal only over textured areas where the human eyes are not sensitive, while the uniform backgrounds are kept intact (see the pixel-wise difference).
More visual results are available in App.~\ref{app:qualitative}.

\autoref{tab:quality-watermarking} presents a quantitative evaluation of image generation quality on the different tasks.
We report the FID, and the average PSNR and SSIM that are computed between the images generated by the fine-tuned LDM and the original one.
The results show that no matter the task, the watermarking has very small impact on the FID of the generation.

The average PSNR is around $30$~dB and SSIM around $0.9$ between images generated by the original and a  watermarked model.
They are a bit low from a watermarking perspective because we do not explicitly optimize for them.
Indeed, in a real world scenario, one would only have the watermarked version of the image. 
Therefore we don't need to be as close as possible to the original image but only want to generate artifacts-free images. Without access to the image generated by the original LDM, it is very hard to tell whether a watermark is present or not.

\subsection{Watermark robustness}\label{subsec:robustness}
We evaluate the robustness of the watermark to different image transformations.
For each task, we generate $1$k images with $10$ models fine-tuned for different messages, and report the average bit accuracy in \autoref{tab:quality-watermarking}.
The evaluated transformations are presented in Fig.~\ref{fig:transformations}. 
We provide results on additional transformations in App.~\ref{sec:supp-robustness}.

We see that the watermark is indeed robust for several tasks and across transformations.
The bit accuracy is always above $0.9$, except for inpainting, when replacing only the masked region of the image (between $1-50$\% of the image, with an average of $27\%$ across masks).
Besides, the bit accuracy is not perfect even without edition, mainly because there are images that are harder to watermark (\eg the ones that are very uniform, like the background in Fig.~\ref{fig:qualitative}) and for which the accuracy is lower.

Note that the robustness comes even without any transformation during the LDM fine-tuning phase:
it is due to the watermark extractor.
If the watermark embedding pipeline is learned to be robust against an augmentation, then the LDM will learn how to produce watermarks that are robust against it during fine-tuning.




\subsection{Comparison to post-hoc watermarking}\label{subsec:watermarking}

An alternative way to watermark generated images is to process them after the generation (post-hoc). 
This may be simpler, but less secure and efficient than Stable Signature.
We compare our method to a frequency based method, DCT-DWT~\cite{cox2007digital},
iterative approaches (SSL Watermark~\cite{fernandez2022sslwatermarking} and FNNS~\cite{kishore2021fixed}), and an encoder/decoder one like HiDDeN~\cite{zhu2018hidden}.
We choose DCT-DWT since it is employed by the original open source release of Stable Diffusion~\cite{2022stablediffusion}, and the other methods because of their performance and their ability to handle arbitrary image sizes and number of bits.
We use our implementations for each method, see details in App.~\ref{app:watermarking}.

\autoref{tab:quality-watermarking} compares the generation quality and the robustness over $5$k generated images.
Overall, Stable Signature achieves comparable results in terms of robustness. 
HiDDeN's performance is a bit higher but its output bits are not i.i.d. meaning that it cannot be used with the same guarantees as the other methods.
We also observe that post-hoc generation gives worse qualitative results, images tend to present artifacts (see Fig.~\ref{fig:supp-watermark} in the supplement).
One explanation is that Stable Signature is merged into the high-quality generation process with the LDM auto-encoder model, which is able to modify images in a more subtle way.


\vspace{0.3cm}
\subsection{Can we trade image quality for robustness?}\label{subsec:quality-tradeoff}

We can choose to maximize the image quality or the robustness of the watermark thanks to the weight $\lambda_i$ of the perceptual loss in~\eqref{eq:loss2}.
We report the average PSNR of $1$k generated images, as well as the bit accuracy obtained on the extracted message for the `Combined' editing applied before detection (qualitative results are given in App.~\ref{sec:supp-percep-loss}).
A higher $\lambda_i$ leads to an image closer to the original one, but to lower bit accuracies on the extracted message:

\begin{table}[h!]
        \centering
        \resizebox{0.9\linewidth}{!}{
        \begin{tabular}{l *{7}{c@{\hspace*{8pt}}}}
            \toprule
            $\lambda_i$ for fine-tuning     & $0.8$ & $0.4$ & $0.2$ & $0.1$ & $0.05$ & $0.025$ \\ \midrule
            \rule{0pt}{2ex}
            PSNR $\uparrow$ & $31.4$ & $30.6$ & $29.7$ & $28.5$ & $26.8$ & $24.6$ \\ 
            \rule{0pt}{2ex}
            Bit acc. $\uparrow$ on `comb.' & $0.85$ & $0.88$ & $0.90$ & $0.92$ & $0.94$ & $0.95$ \\ 
            \bottomrule \\
        \end{tabular}
        }
\end{table}



\vspace{-0.2cm}
\subsection{What makes a good watermark extractor?}\label{subsec:message-decoder}

In the following experiments, we analyze the watermark pre-training.
Ablations are conducted on a shorter schedule of $50$ epochs, on $128\times 128$ images and $16$-bits messages.

\begin{table}[t]
    \centering
    \caption{Role of the simulation layer. }\label{tab:asl}
    \vspace{0.2cm}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c@{\hspace*{4pt}} *{5}{c@{\hspace*{8pt}}}}
        \toprule
        \multirow{2}{*}{ \shortstack{ Seen at  \\ $\mathcal{W}$ training \vspace*{-4pt}} } & \multicolumn{5}{c}{Bit accuracy $\uparrow$ at test time:} \\ \cmidrule{2-6}
            & Crop $0.1$ & Rot. $90$ &JPEG $50$ & Bright. $2.0$ & Res. $0.7$  \\
        \midrule
        \xmark     & 1.00 & 0.56 & 0.50 & 0.99 & 0.48 \\
        \cmark     & 1.00 & 0.99 & 0.90 & 0.99 & 0.91 \\
        \bottomrule 
    \end{tabular} 
    }
\end{table}


\begin{table}[t]
    \centering
    \caption{
        Influence of the (discarded) watermark encoder perceptual quality. P$_{1,2}$ stands for Phase 1 or 2.
    }\label{tab:encoder-quality}\vspace{0.2cm}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{l *{5}{l}}
        \toprule
        Scaling factor $\alpha$ & $0.8$ & $0.4$ & $0.2$ & $0.1$ & $0.05$ \\ 
        \midrule
        (P$_{1}$) - PSNR $\uparrow$  & $16.1$ & $21.8$ & $27.2$ & $33.5$ & $39.3$ \\
        (P$_{2}$) - PSNR $\uparrow$  & $27.9$ & $30.5$ & \textbf{30.8} & $28.8$ & $27.8$ \\
        \midrule
        (P$_{1}$) - Bit acc. $\uparrow$ on `none'& $1.00$ & $1.00$ & $0.86$ & $0.72$ & $0.62$ \\
        (P$_{2}$) - Bit acc. $\uparrow$ on `none'& $0.98$ & \textbf{0.98} & $0.91$ & $0.90$ & $0.96$ \\
        (P$_{2}$) - Bit acc.  $\uparrow$ on `comb.'& \textbf{0.86} & $0.73$ & $0.82$ & $0.81$ & $0.69$ \\
        \bottomrule 
    \end{tabular}
    }
\end{table}

\paragraph{Attack simulation layer.}
Watermark robustness against image transformations depends solely on the watermark extractor.
We pre-train watermark extractors with or without specific transformations in the simulation layer and plug them in the LDM fine-tuning stage.
From there, we generate $1$k images from text prompts and report the bit accuracy of the extracted watermarks in \autoref{tab:asl}.
The extractor is naturally robust to some transformations, such as crops or brightness, without being trained with them, while others, like rotations or JPEG, require simulation during training for the watermark to be recovered at test time.
Empirically we observed that adding a transformation improves results for the latter, but makes training more challenging.

\paragraph{Scaling factor at pre-training.} 
The watermark encoder does not need to be perceptually good and it is beneficial to degrade image quality during pre-training.
In \autoref{tab:encoder-quality}, we train watermark encoders/extractors for different scaling factor $\alpha$ (see Sec.~\ref{subsec:pre-training}), and observe that $\alpha$ strongly affects the bit accuracy of the method.
When it is too high, the LDM needs to generate low quality images for the same performance because the distortions seen at pre-training by the extractor are too strong.
When it is too low, they are not strong enough for the watermarks to be robust: the LDM will learn how to generate watermarked images, but the extractor won't be able to extract them on edited images.
