
\section{Introduction}

\begin{figure}
    \centering 
    \vspace{-0.5cm}
    \includegraphics[width=0.99\linewidth, trim={0cm 0cm 0cm 0cm}, clip]{figs/fig1-vert.pdf}
    \caption{
        Overview. 
        The latent decoder can be fine-tuned to preemptively embed a signature into all generated images.
    }
    \label{fig:fig1} \vspace*{-0.5cm}
\end{figure}

Recent progress in generative modeling and natural language processing has made it easy to create and manipulate images in a photorealistic manner. 
For instance, DALLÂ·E~2~\cite{ramesh2022dalle2} or Stable Diffusion~\cite{rombach2022ldm} generate images from text, which are often indistinguishable from real artworks. 
They have given birth to many image edition tools like ControlNet~\cite{zhang2023adding}, Instruct-Pix2Pix~\cite{brooks2022instructpix2pix}, and others~\cite{couairon2022diffedit, gal2022image, ruiz2022dreambooth}.
They are establishing themselves as creative tools for artists, designers, and the general public.

While this is a great step forward for generative AI, it raises new ethical concerns. 
Indeed, their sophistication is such that it will soon be impossible to distinguish AI generations from real pictures. 
For example, a generated picture recently won an art competition~\cite{gault2022vice}.
Not being able to identify that images are generated by AI makes it difficult to remove them from certain platforms and to ensure their compliance with ethical standards.
The lack of traceability also opens the door to new threats such as deep fakes, impersonation or copyright usurpation~\cite{brundage2018malicious, denton2021ethical}.
    
A baseline solution to identify generated images is forensics, \ie~passive methods to detect generated/manipulated images.
On the other hand, existing watermarking methods can be added on top of image generation.
They are based on the idea of invisibly embedding a secret message into the image, which can then be extracted and used to identify the image.
This has several drawbacks. 
If the model leaks or is open-sourced, the post-generation watermarking is easy to remove.
The open source Stable Diffusion~\cite{2022stablediffusion} is a case in point, since removing the watermark amounts to commenting out a single line in the source code.

Our Stable Signature method merges watermarking into the generation process itself, without any architectural changes.
It adjusts the pre-trained generative model such that all the images it produces conceal a given watermark.
There are several advantages to this approach~\cite{lin2022cycleganwm, yu2022responsible}.
It protects both the generator and its productions. 
Besides, it does not require additional processing of the generated image, which makes the watermarking computationally lighter, straightforward, and secure.
Model providers would then be able to deploy their models to different user groups with a unique watermark, and monitor that they are used in a responsible manner.
They could also give art platforms, news outlets and other sharing platforms %
the ability to detect when an image has been generated by their AI.

We focus on Latent Diffusion Models (LDM)~\cite{rombach2022ldm} since they can perform a wide range of generative tasks.
This work shows that simply fine-tuning a small part of the generative model -- the decoder that generates images from the latent vectors -- is enough to natively embed a watermark into all generated images.
Stable Signature does not require any architectural change and does not modify the diffusion process. Hence it is  compatible with most of the LDM-based generative methods~\cite{brooks2022instructpix2pix, couairon2022diffedit, peebles2022dit, ruiz2022dreambooth, zhang2023adding}.
The fine-tuning stage is performed by back-propagating a combination of a perceptual image loss and the hidden message loss from a watermark extractor back to the LDM decoder.
We pre-train the extractor with a simplified version of the deep watermarking method HiDDeN~\cite{zhu2018hidden}.

We create an evaluation benchmark close to real world situations where images may be edited.
The tasks are: detection of AI generated images, tracing models from their generations.
For instance, we detect $90\%$ of images generated with the generative model, even if they are cropped to $10\%$ of their original size, while flagging only one false positive every $10^6$ images. 
To ensure that the model's utility is not weakened, we show that the FID~\cite{heusel2017gans} score of the generation is not affected and that the generated images are perceptually indistinguishable from the ones produced by the original model. 
This is done over several tasks involving LDM (text-to-image, inpainting, edition, etc.).


As a summary, 
(1) we efficiently merge watermarking into the generation process of LDMs, in a way that is compatible with most of the LDM-based generative methods;
(2) we demonstrate how it can be used to detect and trace generated images, through a real-world evaluation benchmark;
(3) we compare to post-hoc watermarking methods, showing that it is competitive while being more secure and efficient, and (4) evaluate robustness to intentional attacks.
