
\section{Experimental Results}\label{sec:experiments}

We presented in the previous section how to leverage watermarks for detection and identification of images generated from text prompts.
We now present more general results on robustness and image quality for different generative tasks.
We also compare Stable Signature to other watermarking algorithms applied post-generation.

\begin{SCtable*}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
        \begin{tabular}{ c l @{\hspace{2pt}} l  *{2}{c} *{4}{p{15pt}}}
        \toprule
       & & \multirow{2}{*}{}          & \multirow{2}{*}{PSNR / SSIM $\uparrow$} & \multirow{2}{*}{FID $\downarrow$} &\multicolumn{4}{c}{Bit accuracy $\uparrow$ on:} \\ 

    &                                         &                           &           &                                            &  None & Crop & Brigh. & Comb.  \\ \midrule
\multirow{6}{*}{\rotatebox[origin=c]{90}{Tasks}}  
        & Text-to-Image                      & LDM~\cite{rombach2022ldm}             & $30.0$ / $0.89$   & $19.6$ \color{orange}{($-0.3$)} & $0.99$ & $0.95$ & $0.97$ & $0.92$ \\ \cmidrule{2-9}
       & Image Edition                       & DiffEdit~\cite{couairon2022diffedit}                  & $31.2$ / $0.92$ & $15.0$ \color{orange}{($-0.3$)}       & $0.99$ & $0.95$ & $0.98$ & $0.94$ \\ \cmidrule{2-9}
       & Inpainting - Full          & \multirow{2}{*}{Glide~\cite{nichol2021glide}}   & $31.1$ / $0.91$  & $16.8$ \color{orange}{($+0.6$)} & $0.99$ & $0.97$ & $0.98$ & $0.93$ \\ 
       & {\color{white}Inpa} - Mask only          &                                   & $37.8$ / $0.98$  & $9.0$~~ \color{orange}{($+0.1$)} & $0.89$ & $0.76$ & $0.84$ & $0.78$\\ \cmidrule{2-9}
       & Super-Resolution & LDM~\cite{rombach2022ldm}  & $34.0$ / $0.94$ & $11.6$ \color{orange}{($+0.0$)}      & $0.98$ & $0.93$ & $0.96$ & $0.92$ \\ 
    \midrule \rule{0pt}{8pt} \rule{0pt}{8pt} 
\multirow{7}{*}{\rotatebox[origin=c]{90}{WM Methods}} 
           & \emph{Post generation} \\
           & Dct-Dwt \cite{cox2007digital}                      & $0.14$ (s/img)      &  $39.5$ / $0.97$  & $19.5$ \color{orange}{($-0.4$)} & $0.86$ & $0.52$ & $0.51$ & $0.51$ \\ 
           & SSL Watermark \cite{fernandez2022sslwatermarking}  & $0.45$ (s/img)      &  $31.1$ / $0.86$  & $20.6$ \color{orange}{($+0.7$)} & $1.00$ & $0.73$ & $0.93$ & $0.66$ \\ 
           & FNNS \cite{kishore2021fixed}                       & $0.28$ (s/img)      &  $32.1$ / $0.90$  & $19.0$ \color{orange}{($-0.9$)} & $0.93$ & $0.93$ & $0.91$ & $0.93$ \\ 
           & HiDDeN \cite{zhu2018hidden}                        & $0.11$ (s/img)      &  $32.0$ / $0.88$  & $19.7$ \color{orange}{($-0.2$)} & $0.99$ & $0.97$ & $0.99$ & $0.98$ \\ \cmidrule{2-9}
           & \emph{Merged in generation} \\
           & Stable Signature                            & $0.00$ (s/img)             &  $30.0$ / $0.89$ & $19.6$ \color{orange}{($-0.3$)}     & $0.99$ & $0.95$ & $0.97$ & $0.92$ \\ 
        \bottomrule \vspace*{-0.2cm}
    \end{tabular}
    \caption{
        Generation quality and comparison to post-hoc watermarking on 512$\times$512 images and $48$-bit signatures.
        PSNR and SSIM are computed between generations of the original and watermarked generators.
        For FID, we show in {\color{orange} (color)} the difference with regards to original.
        Post-hoc watermarking is evaluated on text-generated images.
        (App.~\ref{sec:supp-robustness} gives results on more transformations, and App.~\ref{app:implementation-details} gives more details on the evaluations.)
        Overall, Stable Signature has minimal impact on generation quality. It has comparable robustness to post-hoc methods while being rooted in the generation itself.
        \vspace*{-0.2cm}
    }\label{tab:quality-watermarking} 
\end{SCtable*}



\subsection{Tasks \& evaluation metrics}
Since our method only involves the LDM decoder, it makes it compatible with many generative tasks. 
We evaluate text-to-image generation and image edition on the validation set of MS-COCO~\cite{lin2014microsoft}, super-resolution and inpainting on the validation set of ImageNet~\cite{deng2009imagenet} 
(all evaluation details are available in App.~\ref{app:evaluation}).

We evaluate the image distortion with the Peak Signal-to-Noise Ratio (PSNR), which is defined as $\mathrm{PSNR}(x,x') = -10\cdot \log_{10} (\mathrm{MSE}(x,x'))$, for $x,x'\in [0,1]^{c\times h\times w}$, as well as Structural Similarity score (SSIM)~\cite{wang2004image}.
They compare images generated with and without watermark. 
On the other hand, we evaluate the diversity and quality of the generated images with the Fr\'echet Inception Distance (FID)~\cite{heusel2017gans}.
The bit accuracy -- the percentage of bits correctly decoded -- evaluates the watermarks' robustness.



\subsection{Image generation quality}
\autoref{fig:qualitative} shows qualitative examples of how the image generation is altered by the latent decoder's fine-tuning. 
The difference is very hard to perceive even for a trained eye. 
This is surprising for such a low PSNR, especially since the watermark embedding is not constrained by any Human Visual System like in professional watermarking techniques. 
Most interestingly, the LDM decoder has indeed learned to add the watermark signal only over textured areas where the human eyes are not sensitive, while the uniform backgrounds are kept intact (see the pixel-wise difference).
% More visual results are available in App.~\ref{app:qualitative}.

\autoref{tab:quality-watermarking} presents a quantitative evaluation of image generation quality on the different tasks.
We report the FID, and the average PSNR and SSIM that are computed between the images generated by the fine-tuned LDM and the original one.
The results show that no matter the task, the watermarking has very small impact on the FID of the generation.

The average PSNR is around $30$~dB and SSIM around $0.9$ between images generated by the original and a  watermarked model.
They are a bit low from a watermarking perspective because we do not explicitly optimize for them.
Indeed, in a real world scenario, one would only have the watermarked version of the image. 
Therefore we don't need to be as close as possible to the original image but only want to generate artifacts-free images. Without access to the image generated by the original LDM, it is very hard to tell whether a watermark is present or not.


\begin{figure}[b]
    \centering
    \scriptsize
    \setlength{\tabcolsep}{0pt}
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c @{\hspace{0.1cm}} c @{\hspace{0.1cm}} c}
        \toprule
        Generated with original & Generated with watermark & Pixel-wise difference ($\times 10$) \\
        \midrule
        \includegraphics[width=0.33\linewidth]{figs/qual/01_nw.png} &
        \includegraphics[width=0.33\linewidth]{figs/qual/01_w.png} &
        \includegraphics[width=0.33\linewidth]{figs/qual/01_diff.png} \\ 
        \rule{0pt}{0.2cm}
        \includegraphics[width=0.33\linewidth]{figs/qual/02_nw.png} &
        \includegraphics[width=0.33\linewidth]{figs/qual/02_w.png} &
        \includegraphics[width=0.33\linewidth]{figs/qual/02_diff.png} \\       
        \bottomrule\\
    \end{tabular}
    }
    % \captionsetup{font=small}
    \caption{
    Images generated with Stable Diffusion. 
    The PSNR is $35.4$\,dB in the first row and $28.6$\,dB in the second.
    Images generated with Stable Signature look natural because modified areas are located where the eye is not sensitive.
    More examples in App.~\ref{app:qualitative}.
    }
    \label{fig:qualitative}
\end{figure}

\begin{table}[t]
    \centering
    \caption{
        Watermark robustness on image transformations applied before decoding, details of which are available in App.~\ref{app:evaluation}.
        We report the bit accuracy, averaged over $10\times1$k images generated from COCO prompts with $10$ different keys.
        }\label{tab:robustness}
        \footnotesize
        \vspace*{0.2cm}
        \setlength{\tabcolsep}{4pt}
        \resizebox{0.96\linewidth}{!}{
            \begin{tabular}{ll|ll|ll}
                \toprule
                \bf{Attack}             & \bf{Bit acc.}     & Comb.         & $0.92$    & Sharpness $2.0$ & $0.99$ \\
                None & $0.99$                               & Bright. $2.0$  & $0.97$    & Med. Filter $k$=7  & $0.94$ \\
                Crop $0.1$ & $0.95$                         & Cont. $2.0$    & $0.98$    & Resize $0.7$  & $0.91$  \\
                JPEG $50$ & $0.88$                          & Sat. $2.0$  & $0.99$      & Text overlay    & $0.99$ \\
                \bottomrule
            \end{tabular}
        }
        \vspace*{-0.3cm}
    \end{table}

\subsection{Watermark robustness}\label{subsec:robustness}
We evaluate the robustness of the watermark to different image transformations applied before extraction.
For each task, we generate $1$k images with $10$ models fine-tuned for different messages, and report the average bit accuracy in \autoref{tab:quality-watermarking}.
Additionally, \autoref{tab:robustness} reports results on more image transformations for images generated from COCO prompts.
The main evaluated transformations are presented in Fig.~\ref{fig:transformations} (more evaluation details are available in App.~\ref{app:evaluation}).

We see that the watermark is indeed robust for several tasks and across transformations.
The bit accuracy is always above $0.9$, except for inpainting, when replacing only the masked region of the image (between $1-50$\% of the image, with an average of $27\%$ across masks).
Besides, the bit accuracy is not perfect even without edition, mainly because there are images that are harder to watermark (\eg the ones that are very uniform, like the background in Fig.~\ref{fig:qualitative}) and for which the accuracy is lower.

Note that the robustness comes even without any transformation during the LDM fine-tuning phase:
it is due to the watermark extractor.
If the watermark embedding pipeline is learned to be robust against an augmentation, then the LDM will learn how to produce watermarks that are robust against it during fine-tuning.




\subsection{Comparison to post-hoc watermarking}\label{subsec:watermarking}

An alternative way to watermark generated images is to process them after the generation (post-hoc). 
This may be simpler, but less secure and efficient than Stable Signature.
We compare our method to a frequency based method, DCT-DWT~\cite{cox2007digital},
iterative approaches (SSL Watermark~\cite{fernandez2022sslwatermarking} and FNNS~\cite{kishore2021fixed}), and an encoder/decoder one like HiDDeN~\cite{zhu2018hidden}.
We choose DCT-DWT since it is employed by the original open source release of Stable Diffusion~\cite{2022stablediffusion}, and the other methods because of their performance and their ability to handle arbitrary image sizes and number of bits.
We use our implementations (see details in App.~\ref{app:watermarking}).

\autoref{tab:quality-watermarking} compares the generation quality and the robustness over $5$k generated images.
Overall, Stable Signature achieves comparable results in terms of robustness. 
HiDDeN's performance is a bit higher but its output bits are not i.i.d. meaning that it cannot be used with the same guarantees as the other methods.
We also observe that post-hoc generation gives worse qualitative results, images tend to present artifacts (see Fig.~\ref{fig:supp-watermark} in the supplement).
One explanation is that Stable Signature is merged into the high-quality generation process with the LDM auto-encoder model, which is able to modify images in a more subtle way.


% \vspace{0.3cm}
\subsection{Can we trade image quality for robustness?}\label{subsec:quality-tradeoff}

We can choose to maximize the image quality or the robustness of the watermark thanks to the weight $\lambda_i$ of the perceptual loss in~\eqref{eq:loss2}.
We report the average PSNR of $1$k generated images, as well as the bit accuracy obtained on the extracted message for the `Combined' editing applied before detection (qualitative results are in App.~\ref{sec:supp-percep-loss}).
A higher $\lambda_i$ leads to an image closer to the original one, but to lower bit accuracies on the extracted message, see \autoref{tab:tradeoff}.

\begin{table}[t]
        \centering
        \caption{Quality-robustness trade-off during fine-tuning.}\label{tab:tradeoff}
        \resizebox{0.9\linewidth}{!}{
        \begin{tabular}{l *{7}{c@{\hspace*{8pt}}}}
            \toprule
            $\lambda_i$ for fine-tuning     & $0.8$ & $0.4$ & $0.2$ & $0.1$ & $0.05$ & $0.025$ \\ \midrule
            \rule{0pt}{2ex}
            PSNR $\uparrow$ & $31.4$ & $30.6$ & $29.7$ & $28.5$ & $26.8$ & $24.6$ \\ 
            \rule{0pt}{2ex}
            Bit acc. $\uparrow$ on `comb.' & $0.85$ & $0.88$ & $0.90$ & $0.92$ & $0.94$ & $0.95$ \\ 
            \bottomrule 
            \vspace*{-0.4cm}
        \end{tabular}
        }
\end{table}



% \vspace{-0.2cm}
\subsection{Attack simulation layer}\label{subsec:message-decoder}

\begin{table}[t]
    \centering
    \caption{Role of the attack simulation layer at pre-training.}\label{tab:asl}
    % \vspace{0.2cm}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c@{\hspace*{4pt}} *{5}{c@{\hspace*{8pt}}}}
        \toprule
        \multirow{2}{*}{ \shortstack{ Seen at  \\ $\mathcal{W}$ training \vspace*{-4pt}} } & \multicolumn{5}{c}{Bit accuracy $\uparrow$ at test time:} \\ \cmidrule{2-6}
            & Crop $0.1$ & Rot. $90$ &JPEG $50$ & Bright. $2.0$ & Res. $0.7$  \\
        \midrule
        \xmark     & 1.00 & 0.56 & 0.50 & 0.99 & 0.48 \\
        \cmark     & 1.00 & 0.99 & 0.90 & 0.99 & 0.91 \\
        \bottomrule
        \vspace*{-0.8cm} 
    \end{tabular} 
    }
\end{table}

Watermark robustness against image transformations depends solely on the watermark extractor.
here, we pre-train them with or without specific transformations in the simulation layer, on a shorter schedule of $50$ epochs, with $128\times 128$ images and $16$-bits messages.
From there, we plug them in the LDM fine-tuning stage and we generate $1$k images from text prompts.
We report the bit accuracy of the extracted watermarks in \autoref{tab:asl}.
The extractor is naturally robust to some transformations, such as crops or brightness, without being trained with them, while others, like rotations or JPEG, require simulation during training for the watermark to be recovered at test time.
Empirically we observed that adding a transformation improves results for the latter, but makes training more challenging.

