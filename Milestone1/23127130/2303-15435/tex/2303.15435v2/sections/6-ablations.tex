



\section{Attacks on Stable Signature's Watermarks}\label{sec:attacks}

We examine the watermark's resistance to intentional tampering, 
as opposed to distortions that happen without bad intentions like crops or compression (discussed in Sec.~\ref{sec:application}). 
We consider two threat models: one  is typical for many image watermarking methods~\cite{cox2007digital} and operates at the image level, and another targets the generative model level. 
For image-level attacks, we evaluate on $5$k images generated from COCO prompts.
Full details on the following experiments can be found in Appendix~\ref{app:attacks}. 

\subsection{Image-level attacks}

\begin{figure}[b]
    \centering
    \hspace{-0.4cm}
    \includegraphics[width=\linewidth, trim={0 0 0 0}, clip]{figs/attacks/purification.pdf}
    \caption{\textbf{Removal attacks.}
    $x_o$ is the image produced by the original generator, 
    $x_r$ is the version produced by the watermarked generator and then attacked.
    Bit accuracy is on the watermark extracted from $x_r$.
    Neural auto-encoders follow the \colorbox[HTML]{ebf2f8}{same trend}, except with the one used by the LDM (`KL-f8' for our LDM).
    When access to the watermark extractor is granted, adversarial attacks also remove the watermark at lower PSNR budget.
    }
    \label{fig:purification}
\end{figure}

\paragraph{Watermark removal.}
Bob alters the image to remove the watermark with deep learning techniques, like methods used for adversarial purification~\cite{shi2021online, yoon2021adversarial} or neural auto-encoders~\cite{abdelnabi2021adversarial, liu2020defending}.
Note that this kind of attacks has not been explored in the image watermarking literature to our knowledge.
\autoref{fig:purification} evaluates the robustness of the watermark against neural auto-encoders~\cite{balle2018variational, cheng2020learned, esser2021taming, rombach2022ldm} at different compression rates.
To reduce the bit accuracy closer to random (50\%), the image distortion needs to be strong (PSNR$<$26).
However, assuming the attack is \emph{informed on the generative model}, \ie the auto-encoder is the same as the one used to generate the images, the attack becomes much more  effective.
It erases the watermark while achieving high quality (PSNR$>$29).
This is because the image is modified precisely in the bandwidth where the watermark is embedded.
Note that this assumption is strong, because Alice does not need to distribute the original generator. 


\vspace*{-0.2cm}
\paragraph{Watermark removal \& embedding (white-box).}
To go further, we assume that the attack is \emph{informed on the watermark extractor} -- \eg because it has leaked.
Bob can use an adversarial attack to remove the watermark by optimizing the image under a PSNR constraint.
The objective is to minimize the $\ell_2$ distance between a random binary message sampled beforehand and the extractor's output, effectively replacing the original signature with a random one.
It makes it possible to erase the watermark with a lower distortion budget, as seen in Fig.~\ref{fig:purification}.

Instead of removing the watermark, an attacker could embed a signature into vanilla images (unauthorized embedding~\cite{cox2007digital}) to impersonate another Bob of whom they have a generated image. 
It highlights the importance of keeping the watermark extractor private.



\subsection{Network-level attacks}


\paragraph{Model purification.} 
Bob gets Alice's generative model and uses a fine-tuning process akin to Sec.~\ref{subsec:finetuning} to eliminate the watermark embedding -- that we coin \emph{model purification}. 
This involves removing the message loss $\mathcal L_m$, and shifting the focus to the perceptual loss $\mathcal L_i$ between the original image and the one reconstructed by the LDM auto-encoder.

\autoref{fig:purification-model} shows the results of this attack for the MSE loss.
The PSNR between the watermarked and purified images is plotted at various stages of fine-tuning.
Empirically, it is difficult to significantly reduce the bit accuracy without compromising the image quality: artifacts start to appear during the purification.

\begin{figure}[b]
    \centering
    \includegraphics[width=0.9\linewidth, trim={0 0 0 0}, clip]{figs/attacks/purification_model_flat.pdf}
    \caption{
        \textbf{Robustness to model purification}, \ie fine-tuning the model to remove watermarks. 
         $x_w$ is the watermarked image, $x_{r}$ is generated with the purified model at different steps of the process.
    }\label{fig:purification-model}
\end{figure}

\vspace*{-0.2cm}
\paragraph{Model collusion.}
Users may collude by aggregating their models.
For instance, Bob$^{(i)}$ and Bob$^{(j)}$ can average the weights of their models (like Model soups~\cite{wortsman2022model}) creating a new model to deceive identification.
We found that the bit at position $\ell$ output by the extractor will be $0$ (resp. $1$) when the $\ell$-th bits of Bob$^{(i)}$ and Bob$^{(j)}$ are both $0$ (resp. $1$), and that the extracted bit is random when their bits disagree.
We show the distributions of the soft bits (before thresholding) output by the watermark extractor on images generated by the average model. 
The $\ell$-th output is labeled by bits of Bob$^{(i)}$ and Bob$^{(j)}$ (\texttt{00} means both have \texttt{0} at position $\ell$):
\\[4pt]
{ \centering
    \includegraphics[width=1.0\linewidth, trim={0 0.7cm 0 0cm}, clip]{figs/attacks/model_collusion_flat.pdf}
}
\vspace{-0.1cm}

This so-called \emph{marking assumption} plays a crucial role in traitor tracing literature~\cite{furon:hal-00757152,meerwald:hal-00740964, tardos2008optimal}.
Surprisingly, it holds even though our watermarking process is not explicitly designed for it.
The study has room for improvement, such as creating user identifiers with more powerful traitor tracing codes~\cite{tardos2008optimal} and using more powerful traitor accusation algorithms~\cite{furon:hal-00757152,meerwald:hal-00740964}.
Importantly, we found the precedent remarks also hold if the colluders operate at the image level.





