\section{Related Work}\label{section:Related_Work}

\subsection{Design Trends for CPS in Space}

In recent years, \gls{scps} and onboard processors in space have been striving for more performance, fueled by more advanced mission requirements and higher expectations for onboard electronics resulting from the advancement of commercial technology~\cite{furano_ai_2020, reinhardt_high_2019}.
However, reliability and dependability~\cite{xie_resource-cost-aware_2018, bhuiyan_dependability_2018} remain key concerns in the space domain because, with high levels of cosmic rays, errors are more frequent than at ground level and occur too often for a system to perform its intended mission reliably.

To achieve higher levels of performance, the industry at large has steered towards multi-core processing architectures, leveraging multiple parallel processing cores to increase performance. Players in space computing have adopted this approach: systems such as the BAE RAD5500 architecture~\cite{berger_quad-core_2015}, the Gaisler GR740 architecture~\cite{hijorth_gr740_2015}, and the DAHLIA NG-ultra architecture~\cite{danard_ng-ultra_2022} leverage quad-core architectures built on existing, radiation-hardened processes. Other designs, like Ramon Chip's RC64~\cite{ginosar_rc64_2016}, leverage large processor arrays for DSP calculations to improve performance. However, these systems, built on PowerPC, SPARC, ARM, and a custom \gls{isa}, respectively, suffer from high costs due to specialized and limited production and lack of commercial technology and software support, further increasing their cost~\cite{di_mascio_open-source_2021}. On the other hand, the reliability and dependability of these designs built on \gls{radhard} technology increase the chances of mission success, making them the go-to choice for complex, high-cost space missions.%, being built on \gls{radhard} technology, make them the go-to choice for complex space missions, as these processors further increase the chances of mission success, which is crucial for these high-cost ventures. 

However, not all missions in the space domain require these high tolerance levels. New Space missions with smaller, more cost-effective satellites tend to rely more on \gls{cots} electronics to design the spacecraft~\cite{toorian_cubesat_2008}. These components offer significantly more performance at a lower cost, sacrificing reliability, which can be tolerated for non-critical applications, such as in CubeSats~\cite{toorian_cubesat_2008} or for non-critical machine learning workloads~\cite{furano_ai_2020}. However, even for these satellite missions, certain aspects, such as communication and control, still require some radiation tolerance in \gls{cots} solutions, often guaranteed with watchdog timers.

On the other hand, large-scale infrastructure may have satellites as one part of the pipeline but can contain many different components, from IoT sensor nodes to high-performance server infrastructure. In such cases, using many different components with different programming paradigms can lead to untenable cost overheads, where component reuse can greatly benefit the designers. In such a case, reliability may be required by the satellite but is counter-productive for the power-constrained sensor node requiring high performance and energy efficiency. Furthermore, reusing a single component both for commercial and reliability environments can greatly reduce component cost, leveraging scale to combat the cost issue plaguing the space industry.

One of the recent trends in custom \gls{soc} design is the shift to RISC-V, leveraging a modern and open-source \gls{isa} to develop custom hardware. RISC-V is also quickly impacting the space domain~\cite{di_mascio_open-source_2021, furano_european_2022}, which relies on modified custom designs to ensure proper reliability and fault tolerance. In using this new \gls{isa}, the space industry can benefit not only from designing custom, reliable hardware for their purposes but also from the commercial environment. Companies developing dedicated hardware for space have also adopted this approach: Gaisler is developing the NOEL-V architecture~\cite{andersson_development_2020}, Microchip is leveraging SiFive's processor architectures for NASA's next High Performance Space Computing project~\cite{leibson_nasa_2023}, and dedicated projects like De-RISC~\cite{wessman_-risc_2022} are pushing this development.
In this work, we continue on this trend by exploiting an open-source industrially verified RISC-V core~\cite{gautschi_near-threshold_2017, group_openhw_2023} as a building block for our system.

\subsection{Fault Tolerance Approaches}

\subsubsection{Radiation-Induced Faults in Space}
% Since the beginning of spaceflight, the radiation environment has been an object of investigation~\cite{van_allen_observation_1958}, showing to be significantly more severe than on earth, with a flux over \SI{e7}{particles\per\centi\meter\squared}/day and Linear Energy Transfer up to \SI{100}{\centi\meter\squared\per\milli\gram} for heavy ions on a bad day~\cite{bourdarie_near-earth_2008}. 
Since the beginning of spaceflight, the radiation environment has been an object of investigation~\cite{van_allen_observation_1958}, showing to be significantly more severe than on earth, with a flux over \SI{e7}{particles\per\centi\meter\squared\per day}~\cite{bourdarie_near-earth_2008}. The effect on electronics, also essential for spaceflight's development, became apparent shortly after the first space missions~\cite{ziegler_effect_1979}, where high energy ions from cosmic rays cause charge separation within semiconductors, leading to voltage spikes within transistors.
These voltage spikes can lead to inverted bits within memory cells or transient spikes at the output of logic gates. These are called ``soft errors,'' as they can be corrected simply by re-writing the logic value and do not cause permanent faults within the circuit. A further subdivision of soft errors is between \glsreset{set}\glspl{set} for transient events in combinational logic and \glsreset{seu}\glspl{seu} for events affecting state-keeping logic and memories. Both \glspl{set} sampled by a register and \glspl{seu} (directly) can cause incorrect system behavior.
While early integrated circuits seemed tolerant to these adverse effects, the hazardous space environment severely affects the more deeply integrated nodes used onboard newer satellites~\cite{wilkinson_tdrs-1_1991}, leading to a requirement for reliable electronics. For example, on a \SI{65}{\nano\meter} technology node, systems experience on the order of \SI{e-7}{errors\per bit\per day}; this error rate is increasing for smaller technology nodes and is more and more due to \glspl{set} as clock frequencies are increasing~\cite{di_mascio_open-source_2021}. The error rate is also estimated to vary with the flying orbit of the system~\cite{engel_predicting_2006}, leading to a higher probability of a \gls{set}-induced erroneous value being sampled.

\rebuttal{Fault mitigation approaches for ASICs' protection vary from using Radiation-Hardened technologies to architectural and system-level modifications intended to be adopted independently from the chosen technology to software-based approaches for protecting \gls{cots} devices. The state-of-the-art approaches are qualitatively discussed in the following and summarized in Table~\ref{tab:related_work}.}

\subsubsection{Radiation Hardened by Design}
\glsreset{radhard}\gls{radhard} technologies rely on silicon-level techniques, like transistor resizing and low-level design modifications, to increase the robustness of the technology cells towards particles striking the silicon. Industry groups like Gaisler~\cite{andersson_leon_2017, hijorth_gr740_2015}, BAE~\cite{berger_quad-core_2015}, DAHLIA~\cite{danard_ng-ultra_2022}, and Ramon Chips~\cite{ginosar_rc64_2016} mostly rely on \gls{radhard} technologies to increase the fault-resilience of space-grade multi-core processors. This increases the designs' fault tolerance capabilities by reducing the engineering effort in implementing architectural solutions for application-specific \glspl{soc}. However, one of the negative aspects of \gls{radhard} cells is that they are not readily available, being significantly more expensive than standard technological processes. Moreover, \gls{radhard} technologies, while significantly more resilient towards soft errors than commercial technologies, are not completely immune~\cite{di_mascio_open-source_2021}. To integrate the tolerance features, \gls{radhard} cells are significantly larger than those used in standard technologies due to their resilience requirements. Radiation hardening requires major investments when moving to a new technology node, both in time and money. Therefore, \gls{radhard} libraries and design kits are generally only available for older technology nodes, significantly lagging behind commercial counterparts. This leads to increased costs due to custom, low-volume designs for low-performance and low-efficiency designs.  While this is acceptable for small, mission-critical systems, it is problematic if applications require higher computing performance on a spacecraft.
% \todo{michaero extend with Trik content}
% \todo{TODO: cite DHALIA~\cite{danard_ng-ultra_2022} project.}

%The technology also scales slower than commercial technology, being a few generations behind, leading to less efficient designs and further increased costs due to the additional area required. While this is acceptable for small mission-critical systems, it is problematic if applications require deploying higher computing performance on a spacecraft.

\subsubsection{Architectural Modifications}
Rather than relying on technology-level hardening, a processor's architecture can also be used to enhance a system's reliability. This can mitigate one of the key performance and efficiency concerns, allowing modern technology nodes to be used for the design, albeit requiring more advanced architectural considerations and design. The two common approaches are \begin{enumerate*}[label=(\roman*)]
\item \gls{ecc} for information redundancy of the stored bits (for example, in registers or memories) and
\item modular redundancy, namely \glsreset{dmr}\gls{dmr} and \glsreset{tmr}\gls{tmr}, at various levels in the design, ensuring that multiple redundant copies of a critical module process the same information and produce the same outcome.
\end{enumerate*}

\Gls{ecc} is one of the most common and efficient ways to protect static data in \glspl{soc}' memories and registers. The encoding and decoding logic allows for individual data words' protection with limited additional bits, resulting in far less overhead than modular redundancy approaches. It is often used to further bolster designs protected by \gls{radhard} technologies, improving their reliability. Other examples include the Duck-Core~\cite{li_duckcore_2021}, which implements \gls{ecc} in the pipeline registers, rolling back to allow re-execution of the last instruction in case a fault occurs.

When considering modular redundancy, the size of the replicated module considerably impacts the design effort, functionality, and performance. STRV~\cite{walsemann_strv_2023} implements \gls{tmr} at a very fine granularity within a RISC-V core, replicating the circuitry and voting after each register to ensure correct processing. Similarly, \citeauthor{gkiokas_fault-tolerant_2019}~\cite{gkiokas_fault-tolerant_2019} propose a \gls{tmr} approach inside a 5-stage dual issue time predictable core triplicating the fetch, decode, and execute stage of the pipeline. The results provided by the three execution stages are propagated to a voter in charge of deciding which should continue in the pipeline to the memory and register write-back stages.

The two approaches, \gls{ecc} and modular redundancy, can also be combined well. SHAKTI-F~\cite{gupta_shakti-f_2015} uses a hybrid approach, using \gls{ecc} for registers and memory while implementing \gls{dmr} for the ALU within the processor's execution stage, ensuring the calculation is executed correctly. While these modifications can significantly increase reliability, they require severe architectural modification of the underlying component and unrecoverable area overhead. Furthermore, Duck-Core and SHAKTI-F focus only on data protection within the cores, leaving the problem of control-flow faults uncovered.

The final architectural modifications to consider are full replications of the component blocks, using modular redundancy at a far coarser granularity. Prime examples are \glsreset{dcls}\gls{dcls} and \glsreset{tcls}\gls{tcls}, replicating the entire processing core and adding checkers and voters at the boundary. These system-level solutions are closer to the approach proposed in this work and are detailed further below in \Cref{sec:system-rel}.

\subsubsection{Software Approaches}
Reliability can also be tackled in software, leveraging existing hardware \gls{cots} and adding fault tolerance in a later step. 
One option is to leverage multiple threads, duplicating~\cite{alcaide_software-only_2020} or triplicating~\cite{barbirotta_design_2022} an application on separate threads executing concurrently or sequentially. These multiple executions are matched with a final checking step, either in hardware or software, to ensure correct execution. While these approaches require little to no modification of the underlying hardware, they incur a heavy penalty in performance, either through additional time for execution or the use of multiple available threads, limiting parallelism. Performance is further penalized by the checking step, requiring either time or dedicated hardware resources to check for correctness. While this approach can properly handle faults in the data path, some faults in the processor's internal control logic may lead to an unrecoverable fault, possibly stalling a thread, thereafter requiring a watchdog timer to restart all processing properly with an extreme penalty.

Another software mitigation strategy is re-execution, which heavily uses \gls{cots} hardware and temporal reliability. Shoestring~\cite{feng_shoestring_2010} duplicates instructions to protect only those segments of code that can result in user-visible faults when subjected to a software error without first showing faulty behavior. Similarly, Inherent Time Redundancy~\cite{reddy_inherent_2007} consists in re-executing a program instruction with varying inputs. The assumption behind this approach is that certain microarchitectural events in a processor execution depend only on the instructions and not on the provided inputs. \citeauthor{jagtap_software_2020}~\cite{jagtap_software_2020} use system reset with a watchdog timer to re-execute software that experienced a fault. Contrarily to hardware redundancy, it is possible to implement software-based approaches without introducing area overhead, as the replication of hardware components that have to be checked or voted on is unnecessary. Checking or voting can be performed in software but remains vulnerable to errors. Furthermore, re-execution and software checking introduces non-negligible processing overhead, impacting the performance of the protected application.

\subsection{System-level Reliability Methods}\label{sec:system-rel}
System-level redundancy approaches make it possible to increase the reliability of digital systems without affecting the internal architecture of vulnerable components. ARM Cortex-R processors~\cite{arm_cortex-r5_2011} and other ARM offerings feature a \glsreset{dcls}\gls{dcls} option~\cite{iturbe_soft_2016}, allowing the \gls{soc} to group two cores within the design. Furthermore, this implementation allows for re-configuration of the architecture while the cores are in reset, switching from a locked operation to a split one. The architecture does not specify the behavior in case an error occurs, relying on the surrounding \gls{soc} to determine fallback behavior and to handle and repair any remaining faults, generally with a reset or relying on check-pointing.
However, critical applications on \gls{cps} with tight timing constraints may require faster repair mechanisms or even continuous operation, not tolerating any downtime due to an error that occurred. Onboard computers in spacecraft are responsible for executing critical real-time operations, which might tolerate minimum execution time variation to avoid the mission's failure~\cite{zhang_fault_2003}. Thus, ensuring that the system can satisfy strict time requirements during the recovery from incurring faults is essential.

Error recovery procedures for a \gls{dcls} implementation can be quite complex, as it is not possible to determine the correct output in case of mismatch. Therefore, \citeauthor{iturbe_arm_2019}~\cite{iturbe_arm_2019} implemented a \glsreset{tcls}\gls{tcls} design with the Cortex-R5 processor. The approach relies on an \textit{assist unit} that wraps the cores to group them in triplets so that all the grouped cores can operate in lockstep with identical inputs and outputs decided with majority voting. The \textit{assist unit} relies on \gls{radhard} technology, while the remaining components use commercial technologies and are reinforced with modular redundancy and \gls{ecc}. Any fault in a single core can be repaired during operation as the two remaining cores continue processing. A re-synchronization routine of the core ensures that any errors remaining in its state bits can be corrected. The cores store all the required internal state information in system memory, are reset by the \textit{assist unit}, and read their stored state again from memory. While not discussed in detail in~\cite{iturbe_arm_2019}, we assume the re-configuration behavior of ARM \gls{tcls} to be similar to the \gls{dcls} implementation, requiring a reset to switch between a split and a locked state.
Another common processing core for reliable systems is the AURIX TriCore~\cite{infineon_aurixtricore_2016}, built on a custom \gls{isa} for embedded applications. This processor also relies on a \gls{tcls} implementation to detect and correct faults, requiring a reset to recover from latent errors.

CEVERO~\cite{silva_cevero_2020} is an example of \gls{dcls} implementation using RISC-V-based Ibex~\cite{davide_schiavone_slow_2017} cores. Unlike ARM, it relies on a hardware-based recovery mode that consists in restoring their internal state, namely the \gls{pc} and \gls{rf}, to a known safe one. Thus, CEVERO implements a \textit{safety island} containing a backup copy of the cores' \gls{pc} and \gls{rf}, replicated every time the cores have matching outputs. In case of a mismatch, the two cores roll back to the last saved status. This is done by resetting the two cores, copying the backup copy of the \gls{pc} and \gls{rf} back into the cores, and restarting their operation. This solution introduces additional area overhead due to the backup registers. On the other hand, it requires only 40 clock cycles for a rollback to complete, significantly increasing the performance of the rollback procedure compared to other SW-based approaches. In this design, the cores in lockstep cannot be decoupled, creating a strong trade-off between area overhead and performance. In addition, the \gls{pc} and \gls{rf} alone do not represent the complete state to which a core should recover, leading to incomplete recovery. Moreover, no protection mechanism is applied to ensure the reliability of the backup registers.
%\todo{could add that this is OK if the assumption is only a single error occurs, however error accumulation may affect this.}

\citeauthor{shukla_low-overhead_2022}~\cite{shukla_low-overhead_2022} implement a re-configurable \gls{dcls} approach within a quad-core processor based on the RISC-V \gls{isa}. The simple, custom-designed core is paired with a mode control unit and a fault detection and correction unit, which ensure proper execution and correction depending on a dedicated mode selection signal. If a fault is detected, the program counter is held to repair the fault. While this may correct any error during execution, an error in the \gls{rf} might remain uncorrected. 

% LEON-3, switchable DCLS with single state linking the cores internally
\citeauthor{kempf_adaptive_2021}~\cite{kempf_adaptive_2021} propose a dual-core adaptive runtime-selectable lockstep based on a LEON processor. While normally operating in independent mode, a master core will request the slave core's assistance with a critical task requiring reliability. The slave core halts its execution and assists the master core with the critical task, requiring a pipeline flush to start proper lockstep execution. The register file is not affected, as this is assumed to be \gls{ecc} protected, and only the master core's register file is used. The LEON core is extended with a commit stage between the execute and memory stages to compare the results of the lockstepped cores' execution. If an error occurs, the checker does not commit the instruction and flushes the pipeline to re-execute the failed instruction. This implementation offers an adaptive lockstep mechanism with fast hardware-based entry and exit switching, but it does not fully protect the core, focusing only on the data flow. Furthermore, deep modifications to the LEON core's architecture are still needed, as direct access to the registers of the main core and the ALU of the slave core is required, and an additional pipeline stage is added.

SafeDE~\cite{bas_safede_2021}, SafeDM~\cite{bas_safedm_2022}, and \citeauthor{marcinek_variable_2023}~\cite{marcinek_variable_2023} propose system-level approaches that enforce the concept of diversity in the execution of redundant threads. In particular, SafeDE enforces diversity by running redundant threads on different cores but introduces limitations in terms of applicability. It cannot be used to ensure the safe execution of parallelizable applications or for applications that require IO accesses. On the other hand, SafeDM allows a wider applicability range. However, the two proposed solutions only mitigate the effect of common cause failures. \citeauthor{marcinek_variable_2023} propose a variable-delayed dual-core lockstep approach to avoid common mode failure, but with no possibility to decouple the two cores for independent execution.

The work presented in this paper belongs to the hardware redundant approaches. It proposes the implementation of system-level hardware enhancements such as independent/\gls{dcls}/\gls{tcls} split-lock, and \gls{ecc}-protected backup status registers for fast re-configuration, re-synchronization, and recovery of a multi-core computing cluster. Furthermore, as detailed in \Cref{section:Architecture}, our approach relies on using unmodified industrially verified cores, therefore guaranteeing fault protection without requiring re-verification of the protected cores from a functional viewpoint.

\input{src/tables/relwork_table}

% \begin{table}[t]
%     \centering
%     \begin{tabular}{@{}l crccc@{}}\toprule
%         System & \gls{isa} & Cores & Reliability Method & Configurable & Commercial \\ \midrule
%         BAE RAD5500~\cite{berger_quad-core_2015} & PowerPC & 4 & \gls{radhard} tech. & \xmark & \cmark \\
%         Gaisler GR740~\cite{hijorth_gr740_2015} & SPARC & 4 & \gls{radhard} tech. \& \gls{ecc} & \xmark & \cmark \\
%         Ramon Chips RC64~\cite{ginosar_rc64_2016} & CEVA X DSP & 64 & \gls{radhard} tech. & \xmark & \cmark \\
%         STRV~\cite{walsemann_strv_2023} & RISC-V & 1 & gate-level \gls{tmr} & \xmark & Research \\
%         Duck-Core~\cite{li_duckcore_2021} & RISC-V & 1 & \gls{ecc} & \xmark & Research \\
%         SHAKTI-F~\cite{gupta_shakti-f_2015} & RISC-V & 1 & block-level \gls{dmr} & \xmark & Research \\
%         % Should we tackle the SW approaches?
%         ARM \gls{dcls}~\cite{arm_cortex-r5_2011, iturbe_soft_2016} & ARM & 2 & \gls{dcls} & \tmark & \cmark \\
%         ARM \gls{tcls}~\cite{iturbe_arm_2019} & ARM & 3 & \gls{tcls} & \tmark & Research \\
%         AURIX TriCore~\cite{infineon_aurixtricore_2016} &  TriCore & 3 & \gls{tcls} & \tmark & \cmark \\
%         CEVERO~\cite{silva_cevero_2020} & RISC-V & 2 & \gls{dcls} & \xmark & Research \\
%         \citeauthor{shukla_low-overhead_2022}~\cite{shukla_low-overhead_2022} & RISC-V & 4 & \gls{dcls} & \cmark & Research \\
%         \citeauthor{kempf_adaptive_2021}~\cite{kempf_adaptive_2021} & SPARC & 2 & \gls{dcls} & \cmark & Research \\
%         \textbf{This work} & \textbf{RISC-V} & \textbf{12} & \textbf{\gls{dcls} \& \gls{tcls}} & \textbf{\cmark} & \textbf{Research} \\
%          \bottomrule
%     \end{tabular}
%     \caption{Qualitative comparison of related work with corresponding features and configuration.}
%     \label{tab:related_work}
% \end{table}

% \todo{TODO: Reviewer 3 wants us to cite \cite{bas_safede_2021} and \cite{bas_safedm_2022} and \cite{marcinek_variable_2023} and \cite{gkiokas_fault-tolerant_2019}.

%\cite{gkiokas_fault-tolerant_2019} should go under 2.2.3; \cite{bas_safedm_2022, bas_safede_2021} should go under 2.3 (and are interesting for future development)}