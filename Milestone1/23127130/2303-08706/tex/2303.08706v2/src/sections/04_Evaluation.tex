\section{Evaluation}\label{section:Evaluation}

\begin{figure}[t]
    \centering
% \begin{tabular}[c]{cc}
    % \hspace{1cm}
    \begin{subfigure}[c]{0.55\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{src/figures/LAYOUT.pdf}
        \caption{Layout of the physical implementation of the fault-tolerant \gls{pulp} cluster featuring full \gls{hmr} and \textit{rapid recovery}.}
        \label{fig:layout}
    \end{subfigure}%&\hspace{1cm}
    \hfill
    \begin{subfigure}[c]{0.4\textwidth}
        \centering
        \caption{\gls{pulp} cluster area comparison in all available configurations.}
        \begin{tabular}{@{}lrr@{}}\toprule
            \multicolumn{2}{@{}l}{PULP Cluster Area [\si{\milli\meter\squared}]} & Overhead \\ \midrule
            Baseline & 0.604 & - \\
            DMR & 0.605 & 0.3\% \\
            TMR & 0.608 & 0.7\% \\
            HMR & 0.612 & 1.3\% \\ \midrule
            \multicolumn{3}{@{}c@{}}{With Rapid Recovery} \\ \midrule
            DMR & 0.654 & 8.4\% \\
            TMR & 0.657 & 8.8\% \\
            HMR & 0.660 & 9.4\% \\
            \bottomrule
        \end{tabular}
        \label{tab:area}

    \end{subfigure}%\vspace{0.2cm}%\\
    % \hspace{1cm}
    \vspace{0.2cm}
    % \hspace{0.5cm}
    \begin{subfigure}[c]{0.60\columnwidth}
        \centering
        \begin{tikzpicture}
            % \tikzstyle{every node}=[font=\small]
            % \begin{scope}[scale=0.6]
            %     \pie[
            %         % scale=0.6,
            %         color = {
            %         % cyan!50!black,
            %         InnovusHMR,
            %         % magenta!60!pink,
            %         InnovusPink,
            %         % cyan,
            %         InnovusBlue,
            %         % yellow,
            %         InnovusYellow,
            %         % red!55!black,
            %         InnovusRed,
            %         % green!40!black,
            %         InnovusGreen,
            %         % orange!50!white,
            %         InnovusTan,
            %         % orange!75!black
            %         InnovusBrown},
            %         explode={0.8, 0, 0, 0, 0, 0, 0, 0},
            %         rotate=-5,
            %         style={lines},
            %         % text=legend
            %     ]
            %     {9/{\gls{hmr} Unit},
            %         15/{Cores (Odd IDs)}, 
            %         15/{Cores (Even IDs)},
            %         18/{\gls{tcdm}},
            %         28/{Instruction Cache},
            %         5/{\gls{dma}},
            %         2/{\gls{tcdm} Interconnect},
            %         8/{AXI Interconnect}
            %     }
            % \end{scope}
            \def\WCtest#1#2{
                \ifdim \WCpercentage pt>5 pt
                    #1
                \else
                    #2
                \fi
            }
            \wheelchart[
                % pie,
                radius={1}{2},
                start angle=-10,
                explode=\WCvarD,
                data={\WCtest{\WCvarC}{\WCvarC~(\WCperc)}},
                % inner data={\WCtest{\WCperc}{}},
                data angle shift=\WCvarE,
                % wheel data style={text=\WCvarF, shift={(90:0)}},
                % inner data style={rotate=\WCmidangle},
                % inner data sep=-1.2,
                counterclockwise,
                lines = 0.5,
                lines ext =0.1,
                lines sep =-0.3
            ]{
                9/InnovusHMR/{\textbf{\gls{hmr} Unit}}/0.6/0/white/,
                15/InnovusPink/{Cores (Odd IDs)}/0/0/black/,
                15/InnovusBlue/{Cores (Even IDs)}/0/-10/black/,
                18/InnovusYellow/{\gls{tcdm}}/0/-20/black/,
                28/InnovusRed/{Instruction Cache}/0/25/white/,
                2/InnovusTan/{\gls{tcdm} Interconnect}/0/0/black/,
                5/InnovusGreen/{\gls{dma}}/0/0/black/,
                8/InnovusBrown/{AXI Interconnect}/0/0/black/
            }
            \wheelchart[
                % pie,
                radius={1}{2},
                start angle=-10,
                explode=\WCvarD,
                data={},
                wheel data={\WCtest{\WCperc}{}},
                % data angle shift=\WCvarE,
                wheel data style={text=\WCvarF},
                wheel data pos=0.4,
                % inner data style={rotate=\WCmidangle},
                % inner data sep=-1.2,
                counterclockwise,
                slices style={fill=none}
                % lines = 0.5,
                % lines ext =0.2,
                % lines sep =-0.2
            ]{
                9/InnovusHMR/{\textbf{\gls{hmr} Unit}}/0.6/0/white/,
                15/InnovusPink/{Cores (Odd IDs)}/0/0/black/,
                15/InnovusBlue/{Cores (Even IDs)}/0/-10/black/,
                18/InnovusYellow/{\gls{tcdm}}/0/-25/black/,
                28/InnovusRed/{Instruction Cache}/0/25/white/,
                2/InnovusTan/{\gls{tcdm} Interconnect}/0/0/black/,
                5/InnovusGreen/{\gls{dma}}/0/0/black/,
                8/InnovusBrown/{AXI Interconnect}/0/0/black/
            }
        \end{tikzpicture}
        \caption{Area breakdown of the fault-tolerant \gls{pulp} cluster featuring full \gls{hmr} and \textit{rapid recovery}, with the same color as in the physical implementation layout.}
        \label{fig:pie_full}
    \end{subfigure}%&
    % \hfill
    \begin{subfigure}[c]{0.4\columnwidth}
        \centering
        \begin{tikzpicture}
            \wheelchart[
                % pie,
                data={\WCvarC~(\WCperc)},
                data angle shift=\WCvarD,
                % data style={
                % shift={(0,0.1)}
                % },
                radius={0.6}{1.2},
                start angle=-10,
                lines =0.5,
                lines ext=0.1,
                % lines ext dirsep=1,
                % lines left anchor={base east},
                % lines right anchor={base west},
                % lines ext top dir=left,
                lines sep = -0.2
            ]{
                78/PULPblue!80/{Rapid Recovery}/-80,
                7/PULPorange!80/{Control}/0,
                5/PULPgreen!80/{DMR Check}/0,
                10/PULPred!80/{TMR Vote}/0
            }
        \end{tikzpicture}
        % \vspace{1cm}
        \caption{\gls{hmr} unit area breakdown.}
        \label{fig:pie_hmr}
    \end{subfigure}%\\
    % \hspace{0.5cm}
% \end{tabular}
\hfill
    \caption{Physical implementation of the fault-tolerant \gls{pulp} cluster.}
    \label{fig:pie}
\end{figure}


\subsection{Experimental Setup}
Our evaluation setup consists of a \gls{pulp} cluster featuring 12 CV32E40P cores explained in \Cref{sec:pulp_cluster}, with an integrated \gls{hmr} unit as explained in \Cref{sec:integration}.
To evaluate the cluster's performance in different modes, the cluster was locked into the respective configuration before boot, avoiding unnecessary mode-switching overhead within the measurement. All performance evaluations were conducted in RTL simulation using QuestaSim, gathering performance data by using performance counters within the RTL or using simulation traces. We evaluated the split-lock and fault recovery performance by entering a \textit{mission-critical section} or \textit{performance section} twice to warm up the instruction caches and then collected the results corresponding to the hot cache measurement.
For implementation purposes, we target GlobalFoundries \SI{22}{\nano\meter} technology using Synopsys Design Compiler for synthesis (slow corner at $f_\mathrm{targ}=\SI{430}{\mega\hertz}$, $V_{DD}=\SI{0.72}{\volt}$, $T=\SI{-40}{\celsius},\SI{125}{\celsius}$) and Cadence Innovus for full-cluster Place \& Route in the same operating point.

\subsection{Physical Implementation}

\Cref{fig:layout} shows the post-layout implementation of our redundant \gls{pulp} cluster with all redundancy modes and \textit{rapid recovery} extension enabled, where we highlighted the cores with even and odd identifiers separately. \Cref{tab:area} shows the area occupation and the related overheads of all the available \gls{pulp} cluster configurations over the standard implementation, while \Cref{fig:pie_full} shows the cluster area breakdown. In the chosen corner, the \gls{pulp} cluster occupies \SI{0.660}{\milli\meter\squared}, where almost 27\% is the RISC-V cores, and around 29\% is the instruction cache. The \gls{hmr} unit, the wrapper in which we encapsulate all redundancy features, accounts for 9\% of the entire cluster area.

\Cref{fig:pie_hmr} shows the area breakdown of the \gls{hmr} unit, highlighting that the \textit{rapid recovery} extension accounts for 79\% of it. The \gls{tmr} voters and \gls{dmr} checkers account for 10\% and 5\% of the area occupation, respectively, while the shared control logic accounts for a 7\% area overhead. The \gls{hmr} unit is designed in a parametrized fashion, so it is possible to instantiate the unit without the hardware enhancement features for \textit{rapid recovery}, allowing a software-based recovery routine only. As shown in \Cref{tab:area}, the area overhead of the \gls{hmr} unit with only software recovery features is limited to almost $1\%$ of the \gls{pulp} cluster area, at a high cost in fault recovery performance. The fault-tolerant cluster could be synthesized at up to \SI{430}{\mega\hertz} operating frequency in the selected corner with no timing impact over the baseline implementation.
%Since the \gls{hmr} unit wraps the cores and all their input and output signals, it appears in the critical path of the \gls{pulp} cluster design but does not impact the timing.

% \begin{table}[t]
%     \centering
%     \caption{Summary of the performance provided by the proposed \gls{pulp} cluster in all the redundant available configurations (considering 1 MAC = 2 Ops).}
%     \begin{tabular}{@{}ll rrrrrrr@{}}\toprule
%                                         &  & \multicolumn{1}{c}{base} & \multicolumn{1}{c}{DMR} & \multicolumn{1}{c}{TMR} & \multicolumn{1}{c}{HMR} & \multicolumn{1}{c}{DMR-R} & \multicolumn{1}{c}{TMR-R} & \multicolumn{1}{c}{HMR-R} \\ \cmidrule{7-9}
%                                         &  &  &  &  &  & \multicolumn{3}{c}{Rapid Recovery} \\ \midrule
%         %Performance [cycles]            &  & 10203 & 19266 & 28708 & \multicolumn{1}{c}{-} & 19266 & 28708 & \multicolumn{1}{c}{-} \\
%         Performance [MOPS @ 430 MHz]            &  & 1165 & 617 & 414 & \multicolumn{1}{c}{-} & 617 & 414 & \multicolumn{1}{c}{-} \\
%         Recovery Latency [cycles]                 &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 363 & \multicolumn{1}{c}{-} & 24 & 24 & \multicolumn{1}{c}{-} \\
%         \multirow{3}{*}{Switching [cycles]} & entry & \multicolumn{1}{c}{-} & 534 & 410 & \multicolumn{1}{c}{-} & 397 & 310 & \multicolumn{1}{c}{-} \\
%                                           & exit main & \multicolumn{1}{c}{-} & 22 & 23 & \multicolumn{1}{c}{-} & 22 & 23 & \multicolumn{1}{c}{-} \\
%                                           & exit help & \multicolumn{1}{c}{-} & 147 & 165 & \multicolumn{1}{c}{-} & 184 & 182 & \multicolumn{1}{c}{-} \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:results}
% \end{table}

\begin{table}[t]
    \centering
    \caption{Summary of the performance provided by the proposed \gls{pulp} cluster in all the redundant available configurations.}
    \begin{tabular}{@{}llc rrrrr@{}}\toprule
                                        &  & & \multicolumn{1}{c}{base} & \multicolumn{1}{c}{DMR} & \multicolumn{1}{c}{TMR} & \multicolumn{1}{c}{DMR-R} & \multicolumn{1}{c@{}}{TMR-R} \\ \cmidrule{7-8}
                                        &  &  &  &  &  & \multicolumn{2}{c@{}}{Rapid Recovery} \\ \midrule
        %Performance [cycles]            &  & 10203 & 19266 & 28708 & \multicolumn{1}{c}{-} & 19266 & 28708 & \multicolumn{1}{c}{-} \\
        \multicolumn{2}{@{}l}{MatMul Performance} & [MOPS @ \SI{430}{\mega\hertz}]            & 1165 & 617 & 414  & 617 & 414 \\
        \multicolumn{2}{@{}l}{\textit{SW-based MatMul Performance}} & \textit{[MOPS @ \SI{430}{\mega\hertz}]} & \textit{1165} & \textit{576} & \textit{351} & \multicolumn{1}{c}{-} & \multicolumn{1}{c@{}}{-} \\
        \multicolumn{2}{@{}l}{\acrshort{cfft} Performance} & [MOPS @ \SI{430}{\mega\hertz}] & 989 & 531 & 385 & 531 & 385 \\\midrule
        \multicolumn{2}{@{}l}{Recovery Latency} & [cycles]                  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 363 & 24 & 24 \\
        \multirow{3}{*}{Mission-Critical} & entry & \multirow{3}{*}{[cycles]} & \multicolumn{1}{c}{-} & 534 & 410 & 397 & 310 \\
                                          & exit main & & \multicolumn{1}{c}{-} & 22 & 23 & 22 & 23 \\
                                          & exit help & & \multicolumn{1}{c}{-} & 147 & 165 & 184 & 182 \\
        \multirow{2}{*}{Performance} & entry & \multirow{2}{*}{[cycles]} & \multicolumn{1}{c}{-} & 134 & 82 & 125 & 82 \\
                                          & exit & & \multicolumn{1}{c}{-} & 373 & 311 & 183 & 94 \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
\end{table}

\subsection{Performance Evaluation}
\subsubsection{Compute Performance}
To evaluate the processing performance of the proposed fault-tolerant cluster, we initially target a highly parallel matrix-matrix multiplication benchmark. For evaluation, a size of 24$\times$24$\times$24 was chosen, meaning $2\times(24\times24\times24)=\SI{27648}{Ops}$, for optimal parallelization in all configurations. Furthermore, a highly parallel, quantized \gls{cfft} was evaluated, representing a typical workload for a \gls{scps}, e.g. used in radar processing. A length of 2048 was chosen, which amounts to $10\times \frac{n \log_2(n)}{2}=\SI{112640}{Ops}$.
\Cref{tab:results} summarizes the performance achieved by the fault-tolerant \gls{pulp} cluster, showing that at \SI{430}{MHz} our fault-tolerant cluster delivers a matrix-matrix multiplication compute performance of \SI{414}{MOPS} in \gls{tmode}, which increases to \SI{617}{MOPS} in \gls{dmode} and \SI{1165}{MOPS} in independent mode with all 12 cores available. Similarly, for the \gls{cfft}, the \gls{tmode} achieves a compute performance of \SI{385}{MOPS}, increasing to \SI{531}{MOPS} in \gls{dmode} and \SI{989} in independent mode. This performance boost of 1.8-1.9$\times$ for \gls{dmode} over the independent mode, and 2.5-2.8$\times$ for \gls{tmode} over independent mode justifies the choice to enable quick switching between different modes to balance the trade-off between performance and fault resilience. Furthermore, the hardware enhancements introduced for the \textit{rapid recovery} do not affect the compute performance of the \gls{pulp} cluster.

To compare our hardware-based solution, we implemented a software-based redundant execution of the same matrix-matrix multiplication kernel utilizing the parallel cores in the \gls{pulp} cluster for reliability. In the software-only approach, cores are similarly grouped (e.g., core 0 and core 6 in a \gls{dmr} approach), operating on the same chunk of the kernel data. At the end of the computation, the two cores check each other's results, raising an error in case of inconsistency. If the check results in no error, only one of the two cores (core 0, for example) validates the result storing the chunk in the memory. The use of software redundancy introduces significant overhead in designing the software, as it needs to be accounted for in each application manually. Further, it introduces a significant performance overhead due to memory contention (multiple cores accessing the same resources) and due to the mutual checking mechanism, resulting in 7\% performance overhead during \gls{dmr} execution and 11\% overhead in the \gls{tmr} case over the hardware lockstep approach. In addition, if the execution is corrupted in the \gls{dmr} software, the only way to recover from the fault is to repeat the computation, introducing additional non-negligible overhead that varies with the size of the executed kernel. Moreover, the overhead due to mutual checking in the software-only redundant execution depends on the size of the result.


\subsubsection{Recovery Performance}

\begin{figure}[t]
    \centering
    \pgfplotstableread{ % data 
Label             Setup Unload Reload   {HW fill}
{HW perf. exit}                 70     0      0           24
{SW perf. exit}                 22  162   127           0
{performance entry}                 82     0      0           0
{mission-critical exit}        23      0    147           0
{HW miss.-crit. entry}    86    198      0          24
{SW miss.-crit. entry}    87    195    126           0
{HW fault recovery}   0      0      0          24
{SW fault recovery}   0    247    116           0
    }\testdata
    \begin{tikzpicture}
        \begin{axis}[
            xbar stacked,   % Stacked horizontal bars
            xmin=0,         % Start x axis at 0
            ytick=data,     % Use as many tick labels as y coordinates
            %nodes near coords,
            legend style={at={(axis cs:335, 0.8)},anchor=south west, draw=none},
            yticklabels from table={\testdata}{Label},  % Get the labels from the Label column of the \datatable
            bar width=15pt,
            xmajorgrids=true,
            grid style=dashed,
            ytick pos=left,
            width=0.8\columnwidth,
            height=7.5cm,
            xlabel={Measured Cycle Count},
            axis line style={draw=none}
        ]
            \addplot [fill=PULPred!80, draw opacity=0,] table [x=Setup, meta=Label,y expr=\coordindex] {\testdata};   % "First" column against the data index
            \addplot [fill=PULPblue!80, draw opacity=0,] table [x=Unload, meta=Label,y expr=\coordindex] {\testdata};
            \addplot [fill=PULPgreen!80, draw opacity=0,] table [x=Reload, meta=Label,y expr=\coordindex] {\testdata};
            \addplot [fill=PULPorange!80, draw opacity=0,] table [x={HW fill}, meta=Label,y expr=\coordindex] {\testdata};
            \legend{Setup,Unload,Reload,{HW fill}}
            \draw[line width=0.25mm, black] [stealth-stealth, dashed] (24,6) -- (363,6) node [right] {15$\times$};
            \draw[line width=0.25mm, black] [] (363,7.5) -- (363,5.8);
            \draw [line width=0.25mm, black] [stealth-stealth, dashed] (308,4) -- (408,4) node [right] {1.3$\times$};
            \draw[line width=0.25mm, black] [] (408,5.5) -- (408,3.8);
            \draw [line width=0.25mm, black] [stealth-stealth, dashed] (94,0) -- (311,0) node [right] {3.3$\times$};
            \draw[line width=0.25mm, black] [] (311,1.5) -- (311,-0.2);
        \end{axis}
        % \draw [line width=0.25mm, black] [stealth-stealth, dashed] (0.55,3.1) -- (8.45,3.1) node [right] {15$\times$};;
        % \draw [line width=0.25mm, black] [stealth-stealth, dashed] (7.2,1.3) -- (9.50,1.3) node [right] {1.3$\times$};;
    \end{tikzpicture}
    \caption{Cycle count breakdown of the fault recovery and split-lock overheads.}
    \label{fig:bars_cycles}
\end{figure}
% We analyzed the recovery performance of the proposed cluster in \gls{dmode} and \gls{tmode} by injecting faults into the simulation directly at the boundary signals of the cores, thus triggering the recovery hardware immediately. In independent mode, the cluster requires the external host core to monitor its execution to detect whether a fault happened, with no hardware support to indicate any error. In \gls{dmode} with no \textit{rapid recovery} hardware, the cluster execution is restarted whenever a fault is detected. In these two configurations, the fault recovery is application-dependent, so no results were provided in the discussion below.

%The software recovery routine described in \Cref{sec:sw-resynch} required 363 cycles to execute a recovery in \gls{tmode} fully. 
% This section describes the performance of the software and hardware recovery mechanisms described in \Cref{sec:sw-resynch}.
The software-based recovery routine, described in \Cref{sec:sw-resynch}, required 363 cycles to execute a recovery in \gls{tmode}, as shown in \Cref{tab:results}. Figure~\ref{fig:bars_cycles} highlights that this is split between the \textit{unload} section, requiring 247 cycles, after which it executes a synchronous clear of the core. Another 116 cycles are required for the \textit{reload} phase, reloading the cores' state from memory. The \textit{rapid recovery} hardware contributes positively by introducing a $15\times$ speed-up during the fault recovery, thus reducing it to just 24 cycles. This recovery time initially requires 4 cycles to set up the \textit{rapid recovery} controller, start the recovery routine, synchronously clear the cores, and send them the debug request. Additional 4 cycles are needed to receive the debug response from the cores, and 16 cycles to restore the \gls{pc}, \gls{rf}, and \glspl{csr} in parallel, after which they will continue processing. Furthermore, the \textit{rapid recovery} hardware also allows for fast fault recovery in \gls{dmode}, requiring the same fixed 24 cycles. The cycle-by-cycle backups of the cores' status make it possible to roll back the cores' execution to the closest safe state in time, restarting the execution from the last \gls{pc} that entered the execution stage.

\subsubsection{Split-lock Performance}
We evaluate the performance of the proposed split-lock mechanism by measuring the cycles required to enter or exit a \textit{mission-critical section} for the main and helper cores as well as a \textit{performance section}, as shown in \Cref{tab:results}. The entry measurement starts from the point the main core initiates the entry into the relevant section to the point the internal code is being executed.

For the \textit{mission-critical section}, \Cref{fig:bars_cycles} shows that the main core first sets up the registers for the intended mode within 87 cycles, during which time the helper core(s) can continue processing. At this point, the interrupt is issued to all cores, launching them into the \textit{unload} stage, where the cores save their state to memory and enter a barrier, requiring 198 cycles. Finally, during the \textit{reload} phase, the cores reload the main core's state from memory in the locked configuration, requiring 126 cycles, after which the cores execute the \textit{mission-critical section} code. When exiting the \textit{mission-critical section}, the main core goes directly into the subsequent software, requiring only 23 cycles to set the configuration and return. On the other hand, the helper cores need to execute the \textit{reload} procedure, requiring 147 cycles to continue the previously interrupted thread.
% taking just 23 clock cycles. On the other hand, the helper cores have to reload their state from memory prior to continuing their execution, requiring 147 clock cycles in \gls{dmode} and 165 in \gls{tmode}.

For the \textit{performance section}, \Cref{fig:bars_cycles} shows lower entry and exit cycle requirements than the \textit{mission-critical section}. Notably, entry into the \textit{performance section} only requires 82 cycles of setup. This is due to the cores only splitting apart and, if needed, updating a \gls{sp}, not fully reloading a known state as they can keep the current execution state. Furthermore, the \textit{performance section} exit drops the state of the assisting cores, relying fully on the main core's state, thus requiring significantly fewer cycles than the \textit{mission-critical section}.

Introducing the \textit{rapid recovery} hardware helps speed up entering a \textit{mission-critical section} as the software \textit{reload} stage can be replaced with a 24-cycle hardware fill from the continuously backed-up main core, meaning $\sim25\%$ less than the software-based approach for a $1.3\times$ speedup. Similarly, exiting a \textit{performance section} also makes use of the continually replicated main core state, leading to a $3.3\times$ speedup. Exiting a \textit{mission-critical section} or entering a \textit{performance section} behaves the same way as in the software-based approach.

\subsection{Reliability Assessment}
With the \gls{hmr} implementation outlined in this work, any error within a single core propagating to its outputs will be detected when mismatching with the paired cores, and corrective action will be taken. Furthermore, with the \textit{rapid recovery} extension enabled, core-internal signals backing up the \gls{rf} are also checked, reducing the likelihood of latent errors within the cores' states. While it aims to detect and correct all possible \glspl{set} happening at the cores' interface or within the cores themselves, the proposed redundancy approach does not tackle \glspl{seu} and single hard errors that might corrupt the data and instruction memory hierarchy, which we thus assume to be reliable.

To test the behavior when a fault occurs, a single bit of the cores' interface signals was inverted using a \texttt{force} command during the execution of a test within an RTL simulation. With the cores in the locked state, this confirms that errors were detected and that the designed corrective action was initiated. Furthermore, select state bits within the cores' \gls{rf} were flipped to confirm the proper behavior of the recovery implementations. All faults injected into the system were detected and corrected or overwritten before use, thus always leading to the correct termination of the running application.

The design was further subjected to a more extensive fault injection campaign, where all registers within a core were randomly subjected to faults. Running 7717 individual RTL simulations of the matrix multiplication performance benchmark in the \gls{tmode} configuration with software recovery, a single register was picked and flipped for each simulation run. All simulations terminated successfully, with 12\% of injected errors leading to a recovery, meaning the other faults were masked.

Other research has conducted extensive fault investigation of the CV32E40P core used in our system~\cite{asciolla_characterization_2020}, showing that many injected faults do not affect the program execution, with the most critical module being the controller module.
As only 12\% of injected errors lead to corrective action, this confirms the results of \citeauthor{asciolla_characterization_2020}~\cite{asciolla_characterization_2020}, where 44\%-100\% of injected errors lead to no correction, depending on the component.

\subsection{Recovery Use-case Analysis: Satellite Onboard Image Processing}
One of the most common use cases of multi-core \gls{scps} is satellite onboard image processing. Satellites' onboard sensors generate a large amount of data that heavily loads the data link and delays processing. When images captured onboard need to be processed through complex deep learning algorithms, the satellite first transmits images to the cloud computing data center of the ground station. Then, the data center operates on the received data using deep learning models and distributes the processing result to the user~\cite{wei_application_2019}. To reduce the overhead given by raw data transmission, the data processing can be directly moved onboard a spacecraft, only sending valuable information over the communication link. This requires the onboard processing systems to offer enough performance to process data while also offering reliability with quick recovery from incurring faults.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        
        \begin{axis}[
            xmin=1000,
            xmax=1e8,
            ytick distance=0.2,
            ymin=0,
            ymax=1.25,
            samples=50,
            xmode=log,
            width=0.8\columnwidth,
            height=6.5cm,
            ylabel={Performance [GOPS]},
            xlabel={Fault Rate [\# faults/s]},
        ]
            \addplot[PULPpurple, ultra thick, dashed, domain=0.01:1e8] {27648/10203*0.43};
            \addplot[PULPred,  ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/19266*430e6},{(27648/(19266+((x)*24))*0.43)});
            \addplot[PULPblue, ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/19266*430e6},{(27648/(19266*(1+(x))))*0.43});
            \addplot[PULPorange,  ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/28708*430e6},{(27648/(28708+((x)*24/2))*0.43)});
            \addplot[PULPgreen,  ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/28708*430e6},{(27648/(28708+((x)*383/2))*0.43)});
            
            
            \legend{Baseline Performance, \gls{dcls} rapid, \gls{dcls}, \gls{tcls} rapid, \gls{tcls}}

            \draw [line width=0.25mm, cyan!70!black] [stealth-stealth, dashed] (1e6,0.03) -- (1e6,0.57) node [right] at (1.02e6,0.34) {> 6$\times$};
            \draw [line width=0.25mm, cyan!70!black] [stealth-stealth, dashed] (1e7,0.09) -- (1e7,0.31) node [right] at (1.02e7,0.2) {> 3$\times$};
        \end{axis}
    \end{tikzpicture}
    \caption{\gls{pulp} cluster performance degradation at increasing fault rate.}
    \label{fig:perfdegr}
\end{figure}

\Cref{fig:perfdegr} shows the performance degradation of our radiation-tolerant cluster when affected by multiple faults in all the available redundancy configurations. We considered a condition where the incurring faults happen during the execution time of a given application, with an increasing fault rate. In \gls{tmode}, if a fault affects one of the three grouped cores, the other two continue their operation. After some time, if another fault happens and affects another of the cores of the same group, it forces the entire group to perform a recovery. On the other hand, in \gls{dmode} without \textit{rapid recovery}, the cluster operation is restarted at each fault occurrence.

The analysis conducted shows that the worst-case scenarios are the software-based recovery procedures. In the \gls{dmode} case, a performance drop of $50\%$ happens with $\sim$\SI{2e4}{faults\per\second} injected. In the \gls{tmode}, the same performance drop happens with around \SI{2e6}{faults\per\second}. When using the \textit{rapid recovery} extension, the increased fault rate reduces the performance of the computing cluster much slower, as shown by the \gls{dcls} rapid and \gls{tcls} rapid curves in \Cref{fig:perfdegr}. The \textit{rapid recovery} feature allows for more than $6\times$ performance speedup during fault recovery over the software-based procedure at \SI{e6}{faults\per\second}, with the performance of the \gls{dcls} rapid case remaining almost constant. Similarly, \gls{tcls} rapid delivers $3\times$ speedup over the software-based recovery with \SI{e7}{faults\per\second} injected. Interestingly, the \gls{dcls} rapid performs better than \gls{tcls} rapid until the fault rate reaches around \SI{3e7}{faults\per\second}. As the fault rate increases further, the performance in \gls{tcls} rapid is the best because, with \gls{tcls}, we do not necessarily need to recover immediately from a fault. If one of the three grouped cores is faulty, the computation can continue with the other two cores, and a re-synchronization is only needed if another fault affects the same group of cores. In the \gls{dcls} rapid case, a re-synchronization is needed as soon as a fault happens. With \textit{rapid recovery} features, we could observe a $50\%$ performance degradation with a fault rate of around \SI{2e7}{faults\per\second} and \SI{4e7}{faults\per\second} for \gls{dcls} rapid and \gls{tcls} rapid, respectively. Furthermore, the conducted analysis shows that the proposed fault-tolerant \gls{pulp} cluster allows for higher computing performance depending on the criticality of the application.

In independent mode, we only have two ways to determine that a fault happened. The first case is instructions faults, meaning incoming faults compromise the executed instructions sending the victim core in an unknown state. In this case, the faulty core stalls, and an external watchdog restarts the cluster operation. The second scenario is that of data faults. Suppose a fault affects the calculation of the cluster cores. In that case, we can only identify that an error happens if the host core knows the computation results a priori and can verify the results produced by the cluster computation. Both these two scenarios imply a significant overhead, similar to or worse than the \gls{dcls} software-based recovery.

\begin{figure}[t]
    \centering
\begin{tikzpicture}
    \pgfmathsetmacro{\numEx}{1}
    \pgfmathsetmacro{\numExDiv}{1}
    \pgfmathsetmacro{\xmin}{-3.2}
    \pgfmathsetmacro{\xmax}{0.2}
    \pgfmathsetmacro{\ymin}{-4}
    \pgfmathsetmacro{\ymax}{3}
    \pgfmathsetmacro{\zmin}{-10.5}
    \pgfmathsetmacro{\zmax}{0}
    \pgfmathsetmacro{\ensingle}{1/1000}
    \pgfmathsetmacro{\scalesize}{1.0}
    \begin{axis}[
        name=dmr,
        unit vector ratio*=\scalesize 1 1,
        xmin=\xmin,
        xmax=\xmax,
        ymin=\ymin,
        ymax=\ymax,
        xtick={-6, -5, -4, -3, -2, -1, 0},
        xticklabels={$10^{-6}$, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$, $10^0$},
        ytick={-4, -2, 0, 2, 4},
        yticklabels={, $10^{-2}$, $10^0$, $10^2$, $10^4$},
        minor y tick num=1,
        ylabel={Execution Time [\si{\second}]},
        title=\gls{dcls},
        title style = {text depth=0.5ex},
        small,
        view={0}{90},
     ]
        \addplot3 [surf,
            shader=interp,
            samples=32,
            point meta min=\zmin,
            point meta max=\zmax,
        ] {ln(max(\ensingle,10^x*10^y)*(10^y/\numEx/2)/\numExDiv)/ln(10)};
    \end{axis}
    \begin{axis}[
        name=dmrr,
        at={($ (0.5cm,0cm) + (dmr.south east) $)},
        unit vector ratio*=\scalesize 1 1,
        xmin=\xmin,
        xmax=\xmax,
        ymin=\ymin,
        ymax=\ymax,
        xtick={-6, -5, -4, -3, -2, -1, 0},
        xticklabels={$10^{-6}$, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$, $10^0$},
        ytick={-4, -2, 0, 2, 4},
        yticklabels=\empty,
        minor y tick num=1,
        title=\gls{dcls} rapid,
        title style = {text depth=0.5ex},
        small,
        view={0}{90},
     ]
        \addplot3 [surf,
            shader=interp,
            samples=32,
            point meta min=\zmin,
            point meta max=\zmax,
        ] {ln((56e-9)*max(\ensingle,10^x*10^y)/\numExDiv))/ln(10)};
    \end{axis}

    \begin{axis}[
        name=tmr,
        at={($ (0.5cm,0cm) + (dmrr.south east) $)},
        unit vector ratio*=\scalesize 1 1,
        xmin=\xmin,
        xmax=\xmax,
        ymin=\ymin,
        ymax=\ymax,
        xtick={-6, -5, -4, -3, -2, -1, 0},
        xticklabels={$10^{-6}$, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$, $10^0$},
        ytick={-4, -2, 0, 2, 4},
        yticklabels=\empty,
        minor y tick num=1,
        xlabel={Error rate [faults/\si{\second}]},
        title=\gls{tcls},
        title style = {text depth=0.5ex},
        small,
        view={0}{90},
        ]
        \addplot3 [surf,
            shader=interp,
            samples=32,
            point meta min=\zmin,
            point meta max=\zmax,
        ] {ln((890e-9)*max(\ensingle,(10^x/2)*10^y)/\numExDiv)/ln(10)};
    \end{axis}

    \begin{axis}[
        name=tmrr,
        at={($ (0.5cm,0cm) + (tmr.south east) $)},
        unit vector ratio*=\scalesize 1 1,
        xmin=\xmin,
        xmax=\xmax,
        ymin=\ymin,
        ymax=\ymax,
        xtick={-6, -5, -4, -3, -2, -1, 0},
        xticklabels={$10^{-6}$, $10^{-5}$, $10^{-4}$, $10^{-3}$, $10^{-2}$, $10^{-1}$, $10^0$},
        ytick={-4, -2, 0, 2, 4},
        yticklabels=\empty,
        minor y tick num=1,
        title=\gls{tcls} rapid,
        title style = {text depth=0.5ex},
        small,view={0}{90},colorbar,
        colorbar style={
            ylabel={Runtime Overhead},
            ytick={-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, -0.2},
            yticklabels={$<100$ns, 100ns, 1us, 10us, 100us, 1ms, 10ms, 100ms, 1s, 10s, $>100$s},
            yticklabel style={
                /pgf/number format/.cd,
                fixed,
                fixed zerofill,
                precision=1,
            }
        },
    ]
        \addplot3 [surf,
            shader=interp,
            samples=32,
            point meta min=\zmin,
            point meta max=\zmax,
        ] {ln((56e-9)*max(\ensingle,(10^x/2)*10^y)/\numExDiv)/ln(10)};
    \end{axis}

\end{tikzpicture}
    \caption{\gls{pulp} cluster recovery cost from incurring faults in the available redundant modes.}
    \label{fig:rtoverhead}
\end{figure}



The analysis conducted so far helps understand the behavior of the system when exposed to varying fault rates. The fault rate corresponding to the worst-case estimation for a low-earth orbit device exposure to cosmic rays is in the order of 1.1-\SI{1.7e-3}{faults\per\second} (meaning 100/150 faults/day) and can vary with the flying orbit of the system and with the technology node~\cite{engel_predicting_2006, di_mascio_open-source_2021}. Furthermore, the probability that faults happening in sequence affect the same group of cores is low. In this scenario, the probability that the chip is corrupted by only one single fault during the entire execution of an application is high, even if the application runs for several seconds. If the fault hits one of three cores configured in \gls{tcls} with or without \textit{rapid recovery}, this fault may not affect the execution time of the application. Here, the two remaining cores can continue their operation without interruption until another fault hits the same group.

On the other hand, deeper considerations are needed if the system is configured in \gls{dcls}. With \textit{rapid recovery}, a single fault recovery has no impact on an application that runs for more than thousands of clock cycles (meaning at least \SI{2}{\micro\second} at the considered frequency). The motivation is that the \textit{rapid recovery} extension guarantees 24 clock cycles (approximately \SI{56}{\nano\second} at \SI{430}{\mega\hertz}) to complete the entire recovery procedure. Thus, if an application lasts several seconds, the \gls{dcls} rapid configuration does not introduce any significant degradation in the execution time.

The same does not hold in the case of \gls{dcls}. In this configuration, the only way to recover from a fault hitting a group of cores is to restart the application execution. The consequence is that the time to recovery varies depending on two factors: the moment the fault occurs during execution and the duration of the application. The moment the fault corrupts the executed application (if at the beginning, in the middle, or at the end) impacts the time to recovery significantly. In the worst case, the fault happens close to the end of the execution. In this situation, the time to recovery equals the time of the application itself because it must be repeated almost entirely. The second factor, the execution time of the application (hundreds of microseconds or tens of seconds), further affects the time to recovery. A critical operation may have a strict deadline that is only slightly longer than the required computation time. In this case, the unpredictability of the recovery time can severely impact the real-time capabilities of a system, potentially leading to a failure of the mission. \rebuttal{The ISO 26262 standard provides a definition for the Fault Tolerant Time Interval (FTTI), which is the duration required to identify a fault within the system, initiate a response to the fault, and return the system to a safe state prior to the occurrence of a hazardous event. FTTI is comprised of three components: the time needed for fault detection (Diagnostic Test Interval), the time taken to respond to the fault (Fault Reaction Time), and the time required to re-establish a safe operational state (Safe Tolerance Time) \cite{watzenig_functional_2017}. The precise definition of these parameters is contingent on the specific application executed by the system and should be minimized to ensure the system operates safely across various scenarios.}

In \Cref{fig:rtoverhead}, we show how the recovery cost from incurring errors affects the execution of an application when the error rate ranges between \SI{e-3}{faults\per\second} and \SI{1}{faults\per\second}. We show the recovery cost through a runtime overhead expressed in seconds, highlighted on the color bar on the right, and such overhead is computed taking into account a minimum of \SI{e-3}{faults} happening per application execution, on average.

For the \gls{dcls} configuration, we have considered that the incurring faults happen in the middle of the execution, which is the fault occurrence expected value. The consequence is that for every fault affecting the system, the overhead introduced for the recovery is half the execution time. Under these assumptions, the plots show that the \gls{pulp} cluster configured in \gls{dcls} without the \textit{rapid recovery} has a minimum runtime overhead of \SI{10}{\micro\second}. In particular, at a \SI{1e-3}{faults\per\second} rate, if the executed application requires 1-\SI{100}{\second}, the runtime overhead equals the execution time of the given application, hence being not suitable for real-time executions. Conversely, when configured in \gls{dcls} rapid, \gls{tcls}, and \gls{tcls} rapid, there is a wide range where the recovery cost is negligible. The result is that the runtime overhead due to the recovery is always lower than \SI{10}{\milli\second}, also at higher fault rates. \rebuttal{It is crucial to highlight that the rapid recovery feature enables the DCLS configuration to align with real-time constraints. This allows for a balance between computational performance and the capacity for real-time fault recovery, facilitating the reduction of the FTTI.} It is important to note that the \textit{rapid recovery} feature makes it possible for the \gls{dcls} configuration to meet real-time constraints, allowing for a trade-off between computing performance and real-time fault recovery capabilities.
