\section{Architecture}\label{section:Architecture}

The hardware template we rely on is the multi-core \gls{pulp} cluster~\cite{rossi_pulp_2015}, which can be extended with an external host subsystem featuring a controller core, a larger memory, and interfaces for external peripherals. The host subsystem is not area and performance-critical, and we assume it can be made fault-tolerant by means of high-overhead techniques.

\subsection{Background: The PULP Cluster}\label{sec:pulp_cluster}
The \gls{pulp} cluster~\cite{rossi_pulp_2015} features a parametric number of 32-bit CV32E40P cores~\cite{gautschi_near-threshold_2017}, enhanced for fast \gls{dsp} calculations and industrially verified by the OpenHW Group~\cite{group_openhw_2023}.
Each core features an instruction interface connected to a hierarchical instruction cache~\cite{chen_scalable_2023} consisting of one private bank per core, each configured to store \SI{512}{B}. Each private bank fetches instructions from a larger, shared cache of \SI{4}{KiB}, improving the performance of applications using the Single Program Multiple Data (SPMD) paradigm.

In addition, each core features a data interface that connects it to the rest of the system through dedicated demultiplexers for direct access to the system's \gls{tcdm} and all other memory-mapped peripherals. The \gls{tcdm} comprises a parametric number of \SI{32}{bit} word-interleaved memory banks featuring single-cycle access latency. A banking factor of 2 (i.e., the number of memory banks is twice the number of cores) is typically used to minimize the memory banks contention probability, guaranteeing data sharing overhead smaller than 5\%, even for highly memory-intensive workloads. If a collision occurs, round-robin arbitration guarantees fairness and avoids starvation.

Along with the connection to the \gls{tcdm}, the cores have access to both memory-mapped devices within the cluster and the host domain through a peripheral interconnect. One of the core-local peripherals is the \gls{dma} unit, capable of up to \SI{64}{bit\per cycle} data transfers in either direction, full duplex between the external larger memories in the host subsystem and the cluster's local \gls{tcdm}.

Finally, the cores directly connect to an event unit~\cite{glaser_energy-efficient_2021} responsible for synchronization barriers within the cluster. Each core attempts to read from the corresponding register within the event unit and only receives a response once all cores request the same barrier address. Furthermore, the event unit manages interrupts within the cluster, masking and forwarding incoming interrupt signals to the responsible core.

To ensure the cluster can easily access the host system, the cluster has both an input and an output AXI port connected to an AXI interconnect. Through this interconnect, a host can access the cluster's internal memory and peripherals for configuration. To ensure proper operation, the instruction cache, the \gls{dma}, and the cores through the peripherals interconnect have access to the host system's memory.

The \gls{pulp} ecosystem offers open-source software for parallel code execution on the multi-core cluster~\cite{montagna_streamlining_2021}. For various host systems, the software is provided to configure and boot the accelerating cluster and basic low-level drivers that allow for parallel software execution. The software ecosystem also provides a variety of parallel applications and benchmarks, enabling fast development of custom workloads. 

\subsection{On-Demand Redundancy Grouping}
The baseline of the work described in this paper is the \glsreset{odrg}\gls{odrg}~\cite{rogenmoser_-demand_2022}, which consists in a wrapper that allows for \gls{tcls} grouping of the cluster cores. We have extended the \gls{odrg} by also allowing \gls{dcls} grouping and implementing hardware extensions for fast fault recovery and re-synchronization of the redundant cores. Furthermore, we extended the previous work with a faster split-lock mechanism that allows for rapid runtime-selectable reconfiguration of the cluster. This allows the execution of dedicated portions of code that can be defined as critical in one of the available redundant modes while operating in individual mode, or vice versa, a non-critical portion of code while operating in a reliable mode.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{src/figures/hmr-cluster-dashed.drawio.pdf}
    \caption{Integration of the \gls{hmr} unit within the \gls{pulp} cluster.}
    \label{fig:hmr}
\end{figure}

\newpage

\subsection{System-Level Integration}\label{sec:integration}
We focus on implementing modular redundancy within the \gls{pulp} cluster to safeguard the individual processor cores' correct operation. For this purpose, our design approach and experimental evaluations assume that the modules outside the cores, particularly the \gls{tcdm} and the instruction memories, are reliable and protected by dedicated safety-critical techniques, such as \gls{ecc} encodings and memory scrubbers.
% The \gls{pulp} cluster provides an ideal foundation to implement modular redundancy, with many parallel cores and a configurable software ecosystem to support custom hardware modification. The focus of the proposed work is to exploit modular redundancy to safeguard the computing cores' operation, 
In the \gls{hmr} cluster, the cores are grouped into lockstep, either in \gls{dcls} or \gls{tcls}. All inputs and outputs to the cores pass through a \gls{hmr} unit surrounding the cluster cores, as shown in \Cref{fig:hmr}, where the \gls{hmr} unit is integrated in the \gls{pulp} cluster described in \Cref{sec:pulp_cluster}. In a \gls{pulp} cluster with 12 cores, the \gls{hmr} unit can easily divide all cores into groups of 2 and 3 for \gls{dcls} and \gls{tcls}, respectively, without remainder. Should a different number of cores be chosen, the respective mode will be unavailable for the remaining cores.

To allow access to its configuration registers, the \gls{hmr} unit exposes a peripherals memory port connected to the peripherals interconnect in the \gls{pulp} cluster. Through this interconnect, any core within the cluster and any controller outside the cluster with access to the host memory port can configure the \gls{hmr} unit. Furthermore, this memory port also allows the readout of all other memory-mapped registers within the unit, such as the current reliability state and error statistics measured within the unit.

\subsection{Hybrid Modular Redundancy Block}\label{sec:hmr_block}

Modular redundancy requires multiple hardware instances performing the same operation to ensure correct execution, with the result being compared in a \gls{dmr} case or voted in a \gls{tmr} case. For reliable calculations, all inputs of two or three cores are directly connected with each other, ensuring that the cores receive identical signals for subsequent processing. The outputs of the cores are then connected to dedicated checkers or majority voters to ensure matching calculations or detect any faults that might have occurred and then apply a recovery. When grouped, the locked cores behave within the system as a single virtual core.

While the cores are protected in case a fault occurs, the checkers and majority voters may also be vulnerable to soft errors. To mitigate this, we assume the cores implement additional protections for the bus, for example using \gls{ecc}, being checked on a protocol level within the protected core region. Thus, any error occurring within the checkers and majority voters will result in a bus error, which can then be corrected.

% \todo{explain parametrization of HMR: DCLS, TCLS, both, permanent, none.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{src/figures/hmr-DCLS.drawio.pdf}
    \caption{\acrfull{dmode} grouped cores.}
    \label{fig:dmr}
\end{figure}

\subsubsection{Dual-Core Lockstep}\label{subsection:DCLS}
When configured in \gls{dmode}, the cores in the \gls{pulp} cluster are paired, identifying one main core and one helper core, as shown in \Cref{fig:dmr}. From a system perspective, one core is disabled to no longer act on the system's interconnect, while the other continues processing based on both cores' execution.

The cores of each group are configured to receive the same input information, so they are expected to generate the same output results. The outputs of each core in a group are then propagated to a bitwise checker that detects whether the results produced by the two cores are different. If the results of the two cores match, the checker selects only the result of the main core as an effective output of the pair, propagating it to the rest of the system. On the other hand, in case of a mismatch, the checker raises an error signal, indicating that a fault affected the status of one of the two cores in the group. Since it is impossible to know which core produced the correct output in \gls{dmode}, the checkers gate their outputs toward the system to avoid fault propagation. Furthermore, the error is directly signaled to the system to start a recovery procedure.

%\begin{figure}[b]
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{src/figures/hmr-TCLS.drawio.pdf}
    \caption{\acrfull{tmode} grouped cores.}
    \label{fig:tmr}
\end{figure}

\subsubsection{Triple-Core Lockstep}\label{subsection:TCLS}
In the \gls{tmode}, three cores are grouped, identifying one main core and two helper cores, as shown in \Cref{fig:tmr}. As in \gls{dmode}, the inputs from the system are shared among the cores in the group, ensuring that they operate on identical data and control signals. However, unlike in \gls{dmode}, the outputs of the cores are connected back to the system through bitwise majority voters. Logic to disable the connection is unnecessary, as the voter properly selects the correct output. Each majority voter compares the outputs from the grouped cores and raises an error signal only if it detects a mismatch between them. Unlike \gls{dmode}, since it is possible to vote over the results of three cores in \gls{tmode}, the state of the faulty core can be restored using the state of the other two non-faulty cores. While not advisable with high error rates, letting the two non-faulty cores continue their operation in lockstep without correction can be enabled within the \gls{hmr} unit, delaying the faulty core's re-synchronization. However, if a second mismatch is detected, the grouped cores must enter a re-synchronization routine immediately.


\subsubsection{Parametrization}
While the \gls{hmr} unit offers both \gls{dmode} and \gls{tmode} configurations, the hardware can be parametrized to disable these different modes or enforce them permanently. This means that the \gls{hmr} unit can offer either only \gls{dmode}, only \gls{tmode}, or both, supporting configuration switching with the split-lock explained below. If a desired instantiation does not require split-lock functionality, \gls{dmode} or \gls{tmode} can be enforced permanently. However, these configurations were not investigated in detail.

The \gls{hmr} unit is designed to be agnostic to the cores, requiring proper configuration to adjust to the core's interface. We tested it with the CV32E40P~\cite{gautschi_near-threshold_2017} core, the default for the \gls{pulp} cluster and the configuration used here, and the Ibex~\cite{davide_schiavone_slow_2017} core.

\subsection{Split-Lock}
\begin{figure}[t]
    \centering
    \begin{subfigure}[c]{0.9\columnwidth}
         \centering
         \includegraphics[width=\textwidth]{src/figures/hmr-split-lock-full.drawio.pdf}
         \caption{Split-Lock in independent mode}
         \label{fig:split-lock-independent}
     \end{subfigure}
     % \hfill
     \begin{subfigure}[c]{0.9\columnwidth}
         \centering
         \begin{subfigure}[t]{0.45\textwidth}
             \includegraphics[width=\textwidth]{src/figures/hmr-split-lock-dmini.drawio.pdf}
             \caption{Split-Lock in \gls{dmode}}
             \label{fig:split-lock-dcls}
         \end{subfigure}
         \hfill
         \begin{subfigure}[t]{0.45\textwidth}
             \includegraphics[width=\textwidth]{src/figures/hmr-split-lock-tmini.drawio.pdf}
             \caption{Split-Lock in \gls{tmode}}
             \label{fig:split-lock-tcls}
         \end{subfigure}
     \end{subfigure}
    \caption{Split-Lock muxing example for a 6-core group configuration.}
    \label{fig:split-lock}
\end{figure}

Locking cores together into redundant configurations permanently is limiting for some application domains that might tolerate the occurrence of faults and require higher performance. For example, a satellite must satisfy tighter resilience constraints during orbital maneuvers than during image processing for the satellite's payload. Thus, the proposed \gls{pulp} cluster can be configured before startup to work in the independent mode, \gls{dmode}, or \gls{tmode}, following the grouping procedure shown in \Cref{fig:split-lock}. This allows any application running on the host subsystem to configure the cluster for its purposes and reliability requirements after a cluster reset and before its boot sequence, locking the execution for the accelerated application into the specified reliability mode.

The main differentiator of the cores in the cluster is their respective identifier (ID). It starts at zero and is incremented for each additional core, allowing for easy identification of work sections in a parallelized task. To keep this convenience when the reliability modes are enabled, the \gls{dmode} and \gls{tmode} grouping of the cores is performed in an interleaved fashion, as shown in the 6-cores example depicted in \Cref{fig:split-lock}. \Cref{fig:split-lock-dcls} (\gls{dmode}) shows that core 0 is grouped with core 3, while in \Cref{fig:split-lock-tcls} (\gls{tmode}), it is grouped with core 2 and core 4. This results in preserving the lowest core IDs when enabling all cores in the \gls{dmode} or \gls{tmode} reliability configurations, keeping a simple parallelization software scheme. Furthermore, each reliability group can be configured independently, allowing some cores to execute a reliable application without impeding the others from operating in independent mode.

Changing the reliability mode is also possible at runtime, allowing the cores to switch configurations based on the demands of the software application executed thereon. When starting in the individual configuration, the application can explicitly declare a \textit{mission-critical section} with a portion of code that must be executed reliably. Alternatively, when starting in the reliable configuration, the application can declare a \textit{performance section} with a portion of code that does not require stringent reliability guarantees. This allows a system to make use of the tradeoffs between reliability and performance on a single \gls{soc} with minimal configuration overheads.

% \todo{
% TODO: as an inverse, propose a performance section while generally in the safety mode.%
% This would be a direct way to accelerate tasks while in the reliable mode, directly leveraging the extra hardware within a preset routine. It might even make sense to have this as the first code section explained, with the safety-critical section (to be renamed) following.%
% }



\subsubsection{Mission-Critical Section}\label{sec:missioncriticalsection}
The \textit{mission-critical section} is a temporary reliable state of execution where two or three cores operate in lockstep, while otherwise remaining in the independent configuration. It is designed such that all cores assisting the main core for reliable execution only pause their own thread, returning to it after the \textit{mission-critical section} completes. As the cores' internal registers (\glsreset{pc}\glsreset{rf}\glsreset{csr}\gls{pc}, \gls{rf}, and \glspl{csr}) are used during the reliable execution, the helper cores' state is temporarily saved to the stack in the \gls{tcdm}, allowing them to retrieve it later.

\lstset{
  frame=single,
  language=C,
  basicstyle=\scriptsize,
}
\begin{figure}[b]
\vspace{-0.2cm}
\noindent\begin{minipage}{.50\textwidth}
\begin{lstlisting}[language=C, caption=Mission-Critical Section Wrapping Function, label=lst:critical]
int mission_critical_sec(int (*fn_handle)()) {
    enable_lockstep();
    // Interrupt locks cores together
    int ret = fn_handle();
    disable_lockstep();
    return ret;
}
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{.42\textwidth}
\begin{lstlisting}[language=C, caption=Mission-Critical Section Entry Interrupt Service Routine, label=lst:entry]
void synchronization_irq(void) {
    store_state_to_stack();
    store_pc_to_reg(core_id());
    barrier();
    // Cores are locked together
    // Synchronous Clear Possible
    reload_pc_and_state(core_id());
    asm volatile ("mret");
}
\end{lstlisting}
\end{minipage}
\end{figure}

To enter the \textit{mission-critical section}, the main core executes a custom software routine, shown in Listing~\ref{lst:critical}, that writes the desired state (\gls{dmode} or \gls{tmode}) into the configuration register within the \gls{hmr} unit. 
This triggers an interrupt in the cores that have to participate in the group, indicating they must start the execution of a \textit{mission-critical section}. 
To ensure the cores are awake and available to serve it, the interrupt is routed through the cluster's event unit waking up the cores even if they are asleep waiting for a barrier.
The interrupt service routine outlined in Listing~\ref{lst:entry} stores the internal state of the cores onto the stack in the \gls{tcdm}. The internal state is represented by 30 modifiable registers of the \glsreset{rf}\gls{rf}, excluding the \gls{sp}, and by specific \glspl{csr}, such as the \textit{MEPC}, which stores the last \glsreset{pc}\gls{pc} executed before that interruption.

Once the entire state is stored onto the stack, the \gls{sp} is stored in a memory-mapped register within the \gls{hmr} unit, indicating that this \textit{unload} phase is complete, and the \textit{reload} phase can start.
After storing the state, the cores enter a synchronization barrier and are locked together, ensuring they all continue as one.
Here, it is also possible to trigger a synchronous clear towards the locked cores to bring the internal flip-flops of the cores to their default value without polluting the reset network.
As the \gls{sp} storage register is linked to the core's ID, the grouped cores that share the same ID after grouping read the \gls{sp} register and load the state back from the stack in parallel. After completion, the interrupt service routine is exited into the \textit{mission-critical section} outlined by the software.

To exit the \textit{mission-critical section}, the locked cores write the new desired state (i.e., independent mode) to the \gls{hmr} unit configuration registers. This write operation frees all the cores from their locked state, so the main core can continue its execution% within 23 clock cycles, 
, as its internal state is still consistent. On the other hand, the helper cores must return to the state before the \textit{mission-critical section} entry. Therefore, the \gls{hmr} module synchronously clears the helper cores bringing them back to the boot sequence, where they use their memory-mapped \gls{sp} register to reload their state back from the \gls{tcdm}.

\subsubsection{Performance Section}

The \textit{performance section} allows an application typically requiring reliable execution to forego the stringent requirements and temporarily leverage the higher parallel performance possible with independent cores. It is designed to allow the main thread to continue processing, with the assisting cores branching from the main thread temporarily, after which the temporary threads are resolved at the end of the performance section.

To enter the \textit{performance section}, the cores split apart by a single write to a configuration register in the \gls{hmr} unit. While the main core can continue the main thread, the newly separated cores simply update their \gls{sp} to an independent region and also continue the application. Differentiated based on the core's ID, the separated cores operate on their own stack to avoid interfering with the main thread's stack. 

Once the \textit{performance section} is complete, the main core stores its internal state, and all cores enter a synchronization barrier to be re-grouped. The state of the separated cores is abandoned, as the section is only intended to be temporary to provide additional performance. Once locked together after the synchronization barrier, the cores load the main core's state in lockstep, similar to the entry of the \textit{mission-critical section} described above, and continue the application.

\subsection{Fault Recovery}
In the \gls{dmode} and \gls{tmode} outlined in \Cref{sec:hmr_block}, a fault would not immediately impact the system as the outputs of the cores are either gated in \gls{dmode} or voted in \gls{tmode}. However, further execution may not be possible, as the outputs remain gated in the \gls{dmode}, and any additional errors in another core of the group in \gls{tmode} may cause complete failure. To avoid this scenario, the possibly corrupted state within the cores must be corrected, and the locked cores must be re-synchronized. It is also essential to guarantee that the system recovery can be performed in time to avoid critical tasks from failing. On-board computers execute critical tasks that must be guaranteed to complete in very limited time intervals~\cite{zhang_fault_2003}. Consequently, if incoming faults corrupt the system's status, ensuring a minimal time to recovery to meet real-time operations' constraints is fundamental.

%\todo{Add justification for quick recovery (over ECC w/ rad-hard tech, maybe the comparison should be tackled in the results/discussion section). For real-time constraints, this is absolutely critical. Maybe we can construct an example (e.g., object avoidance burn on orbit needs precise timing). How is this typically handled? WDT with seconds of recovery time, definitely too long for our real-time constraints.}

\subsubsection{TCLS-Mode Software Re-synchronization Routine}\label{sec:sw-resynch}
When an error occurs in \gls{tmode} affecting a single core, the cluster can continue operating, as the voters overrule the invalid outputs of the erroneous core. Furthermore, in \gls{tmode}, the three cores provide redundant internal state information, allowing the system to recover to a fully functional and correct state without additional hardware. This implies storing the internal state of one of the non-faulty cores into the stack placed in TCDM during the \textit{unload} stage and then reloading the state back to all three cores during the \textit{reload} stage, similar to the \textit{mission-critical section}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\columnwidth]{src/figures/hmr-TCLS_FSM.drawio.pdf}
    \caption{\gls{fsm} of the \gls{tmode} controller.}
    \label{fig:tcls_fsm}
\end{figure}

To do this, as shown in \Cref{fig:tcls_fsm}, the \gls{tmode} \gls{fsm} of the \gls{hmr} unit enters the \textit{unload} state and sends an interrupt to the cores through the event unit, triggering an interrupt service routine. This routine stores the internal state of the non-faulty cores (i.e., \gls{pc}, \gls{rf}, and \glspl{csr}) into the stack. During the execution of the \textit{unload} routine by all three cores, the bitwise voter scans the contents of these cores and stores only the state of the non-faulty cores in memory. Then, the \gls{sp} is stored in the same memory-mapped register mentioned in the \textit{mission-critical section} which is located within the \gls{hmr} unit. This indicates to the \gls{tmode} \gls{fsm} that the \textit{unload} stage is complete, allowing it to enter the \textit{reload} state. On this transition, the \gls{tmode} \gls{fsm} can also trigger a synchronous clear toward the locked cores if configured.

To refill the cores with the correct state, the cores use the memory-mapped \gls{sp} register to reload the main core's state stored in memory back into the locked cores. Furthermore, the cores can be cleared again if another fault happens during the \textit{reload} phase, avoiding the execution of another \textit{unload} operation since a safe state was already stored in memory. To properly execute this routine even after a clear, the boot sequence of the cores directly checks the \gls{sp} register to see if the stored value is not zero. If its content is not zero, that value is used by the cores as a reload \gls{sp}. On the other hand, if the register content is zero, the cores continue with a normal boot sequence. Therefore, once the reload is complete, the software stores back a zero value to the \gls{sp} register in the \gls{hmr} unit, switching the \gls{tmode} \gls{fsm} back into a nominal \textit{run} state. The cores then execute an \texttt{mret} instruction, returning to the application running before the fault recovery interrupt was triggered.

\subsubsection{Hardware-Based Re-synchronization and Rapid Recovery Extension}
The software-based \textit{unload} and \textit{reload} procedures execution can be further improved by integrating dedicated hardware for \textit{rapid recovery} with minor modifications to the processor core's architecture. As in the software-based recovery mechanism, \textit{rapid recovery} relies on the fact that the state of each core is defined by its \gls{pc}, \gls{rf}, and \glspl{csr}. Instead of copying the state of the cores to memory and exploiting a software-based interrupt service routine, we extended the \gls{hmr} unit by introducing a hardwired recovery engine for each group of cores. \Cref{fig:rapidrecovery}a shows the cores grouped in \gls{dmode} or \gls{tmode}, connected to a bitwise checker or a majority voter, as described in \Cref{sec:hmr_block}. The \textit{rapid recovery} region, depicted in \Cref{fig:rapidrecovery}b, comprises four main modules: recovery \gls{pc}, recovery \gls{rf}, recovery \glspl{csr}, and a \textit{rapid recovery} controller.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{src/figures/RapidRecovery.pdf}
    \caption{Implementation of the \textit{rapid recovery} hardware extension, highlighting a) the grouped cores, and b) the \textit{rapid recovery} region containing the ECC-protected status registers and the finite state machine of the \textit{rapid recovery} routine.}
    \label{fig:rapidrecovery}
\end{figure}

During normal processing without any errors, a backup of the main core's \gls{pc}, \gls{rf}, and \glspl{csr} content is copied into the registers of the \textit{rapid recovery} region. This operation is done each cycle, as the core's interface was modified to expose the write ports of \gls{pc}, \gls{rf}, and \glspl{csr} to propagate them to the backup registers. Hence, the backup write operation is accomplished in a single cycle. Furthermore, the recovery \gls{pc}, recovery \gls{rf}, and recovery \glspl{csr} are protected with internal \gls{ecc} encoding/decoding, configurable through parameter selection at design time.

If the checkers or voters detect a mismatch, they raise an error signal that blocks write operations to the registers in the \textit{rapid recovery} region, ensuring their content is not corrupted by a faulty state. The same error signal triggers the \textit{rapid recovery} controller that starts the recovery routine, depicted in \Cref{fig:rapidrecovery}b. The error flag from the checker or voter sends the \gls{fsm} of the \textit{rapid recovery} routine from the \textit{Idle} state to the \textit{Clear} state. This transition raises a synchronous clear signal to the faulty group, which brings the internal flip-flops of the cores to their default value without polluting the reset network. Then, the recovery routine jumps into the \textit{Halt} state, where a debug request signal is raised towards the faulty cores, forcing them to enter the debug mode. Shortly after they receive the debug request, the cores raise the halt response signal toward the \textit{rapid recovery} controller. In debug mode, the cores are halted and allow the recovery hardware to access their internal registers without interference.

Once the faulty cores have raised the halt response signal, the recovery routine jumps into the \textit{Restore} state. In this state, the \gls{pc}, \gls{rf}, and \glspl{csr} content of the faulty cores is reloaded from the recovery \gls{pc}, recovery \gls{rf}, and  recovery \glspl{csr}. This process is executed sequentially, restoring the content of all 31 modifiable registers of the \gls{rf}, exploiting its two write ports in parallel. The \gls{pc} and the \glspl{csr} are reloaded in parallel to the \gls{rf}. 

The same hardware introduced to enable the \textit{rapid recovery} can be used to speed up the process of entering a \textit{mission-critical section} and exiting the \textit{performance section}. The recovery \gls{pc}, recovery \gls{rf}, and recovery \glspl{csr} are used to back up the state of the main core continually so that this state can be used to enter the routine safely. As the helper cores software is designed to be executed in order in the \textit{mission-critical section}, they still require their state to be saved to the stack in the TCDM. Therefore, the main core immediately enters the barrier once the interrupt occurs, while the helper cores store their state to the stack and enter the barrier afterward. After all the cores have entered the barrier and are locked together, the \textit{rapid recovery} hardware executes its recovery routine, synchronizing the state of the locked cores to continue into the \textit{mission-critical section} or return to the reliable mode after the \textit{performance section}.
