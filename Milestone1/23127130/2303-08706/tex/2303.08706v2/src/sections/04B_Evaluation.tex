\section{Evaluation}\label{section:Evaluation}

\begin{figure}[t]
    \centering
% \begin{tabular}[c]{cc}
    % \hspace{1cm}
    \begin{subfigure}[c]{0.55\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{src/figures/LAYOUT.pdf}
        \caption{Layout of the physical implementation of the fault-tolerant \gls{pulp} cluster featuring full \gls{hmr} and \textit{rapid recovery}.}
        \label{fig:layout}
    \end{subfigure}%&\hspace{1cm}
    \hfill
    \begin{subfigure}[c]{0.4\textwidth}
        \centering
        \caption{\gls{pulp} cluster area comparison in all available configurations.}
        \begin{tabular}{@{}lrr@{}}\toprule
            \multicolumn{2}{@{}l}{PULP Cluster Area [\si{\milli\meter\squared}]} & Overhead \\ \midrule
            Baseline & 0.604 & - \\
            DMR & 0.605 & 0.3\% \\
            TMR & 0.608 & 0.7\% \\
            HMR & 0.612 & 1.3\% \\ \midrule
            \multicolumn{3}{@{}c@{}}{With Rapid Recovery} \\ \midrule
            DMR & 0.654 & 8.4\% \\
            TMR & 0.657 & 8.8\% \\
            HMR & 0.660 & 9.4\% \\
            \bottomrule
        \end{tabular}
        \label{tab:area}

    \end{subfigure}%\vspace{0.2cm}%\\
    % \hspace{1cm}
    \vspace{0.2cm}
    % \hspace{0.5cm}
    \begin{subfigure}[c]{0.60\columnwidth}
        \centering
        \begin{tikzpicture}
            % \tikzstyle{every node}=[font=\small]
            % \begin{scope}[scale=0.6]
            %     \pie[
            %         % scale=0.6,
            %         color = {
            %         % cyan!50!black,
            %         InnovusHMR,
            %         % magenta!60!pink,
            %         InnovusPink,
            %         % cyan,
            %         InnovusBlue,
            %         % yellow,
            %         InnovusYellow,
            %         % red!55!black,
            %         InnovusRed,
            %         % green!40!black,
            %         InnovusGreen,
            %         % orange!50!white,
            %         InnovusTan,
            %         % orange!75!black
            %         InnovusBrown},
            %         explode={0.8, 0, 0, 0, 0, 0, 0, 0},
            %         rotate=-5,
            %         style={lines},
            %         % text=legend
            %     ]
            %     {9/{\gls{hmr} Unit},
            %         15/{Cores (Odd IDs)}, 
            %         15/{Cores (Even IDs)},
            %         18/{\gls{tcdm}},
            %         28/{Instruction Cache},
            %         5/{\gls{dma}},
            %         2/{\gls{tcdm} Interconnect},
            %         8/{AXI Interconnect}
            %     }
            % \end{scope}
            \def\WCtest#1#2{
                \ifdim \WCpercentage pt>5 pt
                    #1
                \else
                    #2
                \fi
            }
            \wheelchart[
                % pie,
                radius={1}{2},
                start angle=-10,
                explode=\WCvarD,
                data={\WCtest{\WCvarC}{\WCvarC~(\WCperc)}},
                % inner data={\WCtest{\WCperc}{}},
                data angle shift=\WCvarE,
                % wheel data style={text=\WCvarF, shift={(90:0)}},
                % inner data style={rotate=\WCmidangle},
                % inner data sep=-1.2,
                counterclockwise,
                lines = 0.5,
                lines ext =0.1,
                lines sep =-0.3
            ]{
                9/InnovusHMR/{\textbf{\gls{hmr} Unit}}/0.6/0/white/,
                15/InnovusPink/{Cores (Odd IDs)}/0/0/black/,
                15/InnovusBlue/{Cores (Even IDs)}/0/-10/black/,
                18/InnovusYellow/{\gls{tcdm}}/0/-20/black/,
                28/InnovusRed/{Instruction Cache}/0/25/white/,
                2/InnovusTan/{\gls{tcdm} Interconnect}/0/0/black/,
                5/InnovusGreen/{\gls{dma}}/0/0/black/,
                8/InnovusBrown/{AXI Interconnect}/0/0/black/
            }
            \wheelchart[
                % pie,
                radius={1}{2},
                start angle=-10,
                explode=\WCvarD,
                data={},
                wheel data={\WCtest{\WCperc}{}},
                % data angle shift=\WCvarE,
                wheel data style={text=\WCvarF},
                wheel data pos=0.4,
                % inner data style={rotate=\WCmidangle},
                % inner data sep=-1.2,
                counterclockwise,
                slices style={fill=none}
                % lines = 0.5,
                % lines ext =0.2,
                % lines sep =-0.2
            ]{
                9/InnovusHMR/{\textbf{\gls{hmr} Unit}}/0.6/0/white/,
                15/InnovusPink/{Cores (Odd IDs)}/0/0/black/,
                15/InnovusBlue/{Cores (Even IDs)}/0/-10/black/,
                18/InnovusYellow/{\gls{tcdm}}/0/-25/black/,
                28/InnovusRed/{Instruction Cache}/0/25/white/,
                2/InnovusTan/{\gls{tcdm} Interconnect}/0/0/black/,
                5/InnovusGreen/{\gls{dma}}/0/0/black/,
                8/InnovusBrown/{AXI Interconnect}/0/0/black/
            }
        \end{tikzpicture}
        \caption{Area breakdown of the fault-tolerant \gls{pulp} cluster featuring full \gls{hmr} and \textit{rapid recovery}, with the same color as in the physical implementation layout.}
        \label{fig:pie_full}
    \end{subfigure}%&
    % \hfill
    \begin{subfigure}[c]{0.4\columnwidth}
        \centering
        \begin{tikzpicture}
            \wheelchart[
                % pie,
                data={\WCvarC~(\WCperc)},
                data angle shift=\WCvarD,
                % data style={
                % shift={(0,0.1)}
                % },
                radius={0.6}{1.2},
                start angle=-10,
                lines =0.5,
                lines ext=0.1,
                % lines ext dirsep=1,
                % lines left anchor={base east},
                % lines right anchor={base west},
                % lines ext top dir=left,
                lines sep = -0.2
            ]{
                78/PULPblue!80/{Rapid Recovery}/-80,
                7/PULPorange!80/{Control}/0,
                5/PULPgreen!80/{DMR Check}/0,
                10/PULPred!80/{TMR Vote}/0
            }
        \end{tikzpicture}
        % \vspace{1cm}
        \caption{\gls{hmr} unit area breakdown.}
        \label{fig:pie_hmr}
    \end{subfigure}%\\
    % \hspace{0.5cm}
% \end{tabular}
\hfill
    \caption{Physical implementation of the fault-tolerant \gls{pulp} cluster.}
    \label{fig:pie}
\end{figure}


\subsection{Experimental Setup}
Our evaluation setup consists of a \gls{pulp} cluster featuring 12 CV32E40P cores, with an integrated \gls{hmr} unit.
To evaluate the cluster's performance in different modes, the cluster was locked into the respective configuration before boot, avoiding unnecessary mode-switching overhead within the measurement. All performance evaluations were conducted in RTL simulation using QuestaSim, by means of system-internal performance counters or simulation traces to determine cycle counts. We evaluated the split-lock and fault recovery performance by entering a \textit{\rebuttal{\st{safety}mission}-critical section}\rebuttal{ or \textit{performance section}} twice to warm up the instruction caches and then collected the results corresponding to the hot cache measurement.
For implementation purposes, we target GlobalFoundries \SI{22}{\nano\meter} technology using Synopsys Design Compiler for synthesis (slow corner at $f_\mathrm{targ}=\SI{430}{\mega\hertz}$, $V_{DD}=\SI{0.72}{\volt}$, $T=\SI{-40}{\celsius},\SI{125}{\celsius}$) and Cadence Innovus for full-cluster Place \& Route in the same operating point.

\subsection{Physical Implementation}

\Cref{fig:layout} shows the post-layout implementation of our redundant \gls{pulp} cluster with all redundancy modes and \textit{rapid recovery} extension enabled, where we highlighted the cores with even and odd identifiers separately. \Cref{tab:area} shows the area occupation and the related overheads of all the available \gls{pulp} cluster configurations over the standard implementation, while \Cref{fig:pie_full} shows the cluster area breakdown. In the chosen corner, the \gls{pulp} cluster occupies \SI{0.660}{\milli\meter\squared}, where almost 27\% is the RISC-V cores, and around 29\% is the instruction cache. The \gls{hmr} unit, the wrapper in which we encapsulate all the redundant features provided to the cluster, accounts for 9\% of the entire cluster area occupation.

\Cref{fig:pie_hmr} shows the area breakdown of the \gls{hmr} unit, highlighting that the \textit{rapid recovery} extension accounts for 79\% of it. The \gls{tmr} voters and \gls{dmr} checkers account for 10\% and 5\% of the area occupation, respectively, while the shared control logic accounts for a 7\% area overhead. The \gls{hmr} unit is designed in a parametrized fashion, so it is possible to instantiate the unit without the hardware enhancement features for \textit{rapid recovery}, allowing a software-based recovery routine only. As shown in \Cref{tab:area}, the area overhead of the \gls{hmr} unit with only software recovery features is limited to almost $1\%$ of the \gls{pulp} cluster area, at a high cost in fault recovery performance. The fault-tolerant cluster could be synthesized at up to \SI{430}{\mega\hertz} operating frequency in the selected corner with no timing impact over the baseline implementation.
%Since the \gls{hmr} unit wraps the cores and all their input and output signals, it appears in the critical path of the \gls{pulp} cluster design but does not impact the timing.

% \begin{table}[t]
%     \centering
%     \caption{Summary of the performance provided by the proposed \gls{pulp} cluster in all the redundant available configurations (considering 1 MAC = 2 Ops).}
%     \begin{tabular}{@{}ll rrrrrrr@{}}\toprule
%                                         &  & \multicolumn{1}{c}{base} & \multicolumn{1}{c}{DMR} & \multicolumn{1}{c}{TMR} & \multicolumn{1}{c}{HMR} & \multicolumn{1}{c}{DMR-R} & \multicolumn{1}{c}{TMR-R} & \multicolumn{1}{c}{HMR-R} \\ \cmidrule{7-9}
%                                         &  &  &  &  &  & \multicolumn{3}{c}{Rapid Recovery} \\ \midrule
%         %Performance [cycles]            &  & 10203 & 19266 & 28708 & \multicolumn{1}{c}{-} & 19266 & 28708 & \multicolumn{1}{c}{-} \\
%         Performance [MOPS @ 430 MHz]            &  & 1165 & 617 & 414 & \multicolumn{1}{c}{-} & 617 & 414 & \multicolumn{1}{c}{-} \\
%         Recovery Latency [cycles]                 &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 363 & \multicolumn{1}{c}{-} & 24 & 24 & \multicolumn{1}{c}{-} \\
%         \multirow{3}{*}{Switching [cycles]} & entry & \multicolumn{1}{c}{-} & 534 & 410 & \multicolumn{1}{c}{-} & 397 & 310 & \multicolumn{1}{c}{-} \\
%                                           & exit main & \multicolumn{1}{c}{-} & 22 & 23 & \multicolumn{1}{c}{-} & 22 & 23 & \multicolumn{1}{c}{-} \\
%                                           & exit help & \multicolumn{1}{c}{-} & 147 & 165 & \multicolumn{1}{c}{-} & 184 & 182 & \multicolumn{1}{c}{-} \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:results}
% \end{table}

\begin{table}[t]
    \centering
    \caption{Summary of the performance provided by the proposed \gls{pulp} cluster in all the redundant available configurations.}
    \begin{tabular}{@{}llc rrrrr@{}}\toprule
                                        &  & & \multicolumn{1}{c}{base} & \multicolumn{1}{c}{DMR} & \multicolumn{1}{c}{TMR} & \multicolumn{1}{c}{DMR-R} & \multicolumn{1}{c@{}}{TMR-R} \\ \cmidrule{7-8}
                                        &  &  &  &  &  & \multicolumn{2}{c@{}}{Rapid Recovery} \\ \midrule
        %Performance [cycles]            &  & 10203 & 19266 & 28708 & \multicolumn{1}{c}{-} & 19266 & 28708 & \multicolumn{1}{c}{-} \\
        \multicolumn{2}{@{}l}{\rebuttal{MatMul} Performance} & [MOPS @ \SI{430}{\mega\hertz}]            & 1165 & 617 & 414  & 617 & 414 \\
        \multicolumn{2}{@{}l}{\rebuttal{MatMul Perf. SW check}} & [MOPS @ \SI{430}{\mega\hertz}] & 1165 & 576 & 351 & \multicolumn{1}{c}{-} & \multicolumn{1}{c@{}}{-} \\
        \multicolumn{2}{@{}l}{\rebuttal{\acrshort{cfft} Performance}} & [MOPS @ \SI{430}{\mega\hertz}] & 989 & 531 & 385 & 531 & 385 \\\midrule
        \multicolumn{2}{@{}l}{Recovery Latency} & [cycles]                  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 363 & 24 & 24 \\
        \multirow{3}{*}{\rebuttal{\st{Switching}Mission-Critical}} & entry & \multirow{3}{*}{[cycles]} & \multicolumn{1}{c}{-} & 534 & 410 & 397 & 310 \\
                                          & exit main & & \multicolumn{1}{c}{-} & 22 & 23 & 22 & 23 \\
                                          & exit help & & \multicolumn{1}{c}{-} & 147 & 165 & 184 & 182 \\
        \multirow{2}{*}{\rebuttal{Performance}} & entry & \multirow{2}{*}{[cycles]} & \multicolumn{1}{c}{-} & 134 & 82 & 125 & 82 \\
                                          & exit & & \multicolumn{1}{c}{-} & 373 & 311 & 183 & 94 \\
        \bottomrule
    \end{tabular}
    \label{tab:results}
\end{table}

\subsection{Performance Evaluation}
\subsubsection{Compute Performance}
To evaluate the processing performance of the proposed fault-tolerant cluster, we \rebuttal{initially} target\rebuttal{\st{ed}} a highly parallel matrix-matrix multiplication benchmark\rebuttal{\st{, extended to size}. For evaluation, a size of} 24$\times$24$\times$24\rebuttal{ was chosen}, meaning $2\times(24\times24\times24)=\SI{27648}{Ops}$, for optimal parallelization in all configurations. \rebuttal{Furthermore, a highly parallel, quantized \gls{cfft} was evaluated, representing a more typical workload for a \gls{scps}, e.g., used in radar processing. A length of 2048 was chosen, which amounts to $10\times \frac{n \log_2(n)}{2}=\SI{112640}{Ops}$.}
\Cref{tab:results} summarizes the performance achieved by the fault-tolerant \gls{pulp} cluster, showing that at \SI{430}{MHz} our fault-tolerant cluster delivers a \rebuttal{matrix-matrix multiplication} compute performance of \SI{414}{MOPS} in \gls{tmode}, which increases to \SI{617}{MOPS} in \gls{dmode} and \SI{1165}{MOPS} in independent mode with all 12 cores available. \rebuttal{Similarly, for the \gls{cfft}, the \gls{tmode} achieves a compute performance of \SI{385}{MOPS}, increasing to \SI{531}{MOPS} in \gls{dmode} and \SI{989} in independent mode.} This performance boost \rebuttal{of 1.8-1.9$\times$ for \gls{dmode} over the independent mode, and 2.5-2.8$\times$ for \gls{tmode} over independent mode} justifies the choice to enable quick switching between different modes to balance the trade-off between performance and fault resilience. Furthermore, the hardware enhancements introduced for the \textit{rapid recovery} do not affect the compute performance of the \gls{pulp} cluster.

\rebuttal{We implemented a software-based redundant execution of the same kernel in \gls{dmr}/\gls{tmr} mode for direct hardware/software comparison. In the software-only approach we adopted, in the \gls{dmr} case, we have each couple of cores (for instance, core 0 and core 6) operating on the same chunk of the kernel execution. At the end of the computation, the two cores check on each other results, raising an error in case of inconsistency. If the check results in no error, only one of the two cores (core 0, for example) validates the result storing the chunk in the memory. We implemented the same software redundant execution in \gls{tmr} mode. The usage of software redundancy introduces significant performance overhead due to memory contentions (multiple cores accessing the same resources) and due to the mutual checking mechanism, resulting in 7\% performance overhead during \gls{dmr} execution and 11\% overhead in the \gls{tmr} case over the hardware lockstep approach, justifying its usage. In addition, if the execution is corrupted in the \gls{dmr} software, the only way to recover from the fault is to repeat the computation, introducing additional non-negligible overhead that varies with the size of the executed kernel. Moreover, the time overhead due to mutual checking in the software-only redundant execution depends on the kernel size.}

% \todo{State how many $\times$ better we are}

% \rebuttal{
% \todo{FFT cycle results for 32-bit quantized CFFT of length, assume $10\times \frac{n \log_2(n)}{2}$ with length 2048 for $112640 Ops$
%
% 1024: 12 cores:  26'057, 6 cores:  37'480 (1.62x), 4 cores:  50'249 (2.18x)
% 2048: 12 cores:  48'973, 6 cores:  91'217 (1.86x), 4 cores: 125'923 (2.57x) <-- parallelizes best
% 4096: 12 cores: 100'158, 6 cores: 157'346 (1.57x), 4 cores: 217'656 (2.17x)
% }
% }

% \rebuttal{\todo{Expand explanation on SW reliability approach and comparison}}
% \Cref{tab:results} summarizes the performance achieved by the fault-tolerant \gls{pulp} cluster, showing that at \SI{430}{MHz} our fault-tolerant cluster delivers a compute performance of \SI{414}{MOPS} in \gls{tmode}, requiring 28'708 cycles to conclude its operation. The performance increases to \SI{617}{MOPS} in \gls{dmode} and \SI{1165}{MOPS} in independent mode with all 12 cores available, meaning the cluster operation terminates in 19'266 and 10'203 cycles, respectively.
% Considering 1 MAC = 2 Ops, at \SI{430}{MHz} our fault-tolerant cluster delivers a compute performance of \SI{414}{MOPS} in \gls{tmode}, which increases to \SI{617}{MOPS} in \gls{dmode} and \SI{1165}{MOPS} in independent mode with all 12 cores available. This performance boost justifies the choice to enable quick switching between different modes to balance the trade-off between performance and fault resilience. Furthermore, the hardware enhancements introduced for the \textit{rapid recovery} do not affect the compute performance of the \gls{pulp} cluster.

% \begin{table}[t]
%     \centering
%     \begin{tabular}{@{}ll rrrrrrr@{}}\toprule
%                                         &  & \multicolumn{1}{c}{base} & \multicolumn{1}{c}{DMR} & \multicolumn{1}{c}{TMR} & \multicolumn{1}{c}{HMR} & \multicolumn{1}{c}{DMR-R} & \multicolumn{1}{c}{TMR-R} & \multicolumn{1}{c}{HMR-R} \\ \cmidrule{7-9}
%                                         &  &  &  &  &  & \multicolumn{3}{c}{Rapid Recovery} \\ \midrule
%         Performance [cycles]            &  & 10203 & 19266 & 28708 & \multicolumn{1}{c}{-} & 19266 & 28708 & \multicolumn{1}{c}{-} \\
%         Recovery Latency [cycles]                 &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 363 & \multicolumn{1}{c}{-} & 24 & 24 & \multicolumn{1}{c}{-} \\
%         % \multirow{2}{*}{Switching [cycles]} & entry & \multicolumn{1}{c}{-} & 427 (938) & 350 (924) &  &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
%         %                                   & exit & \multicolumn{1}{c}{-} & 23/139 (43/180) & 22/143 &  &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
%         \multirow{3}{*}{Switching [cycles]} & entry & \multicolumn{1}{c}{-} & 534 & 432 & \multicolumn{1}{c}{-} & 397 & 367 & \multicolumn{1}{c}{-} \\
%                                           & exit main & \multicolumn{1}{c}{-} & 22 & 23 & \multicolumn{1}{c}{-} & 22 & 23 & \multicolumn{1}{c}{-} \\
%                                           & exit help & \multicolumn{1}{c}{-} & 147 & 165 & \multicolumn{1}{c}{-} & 184 & 182 & \multicolumn{1}{c}{-} \\
%         Cluster Area [\si{\milli\meter\squared}]             &  & 0.598 & 0.605 & 0.608 & 0.612 & 0.654 & 0.657 & 0.660 \\
%         % Max frequency [\si{\mega\hertz}]   &  &  &  &  &  &  &  &  \\
%         % Power [\si{\milli\watt}]           &  &  &  &  &  &  &  &  \\
%         \bottomrule
%     \end{tabular}
%     \caption{Summary of the features provided by the proposed \gls{pulp} cluster in all the redundant available configurations.}
%     \label{tab:results}
% \end{table}

\subsubsection{Recovery Performance}

\begin{figure}[t]
    \centering
    \pgfplotstableread{ % data 
Label             Setup Unload Reload   {HW fill}
{\rebuttal{HW perf. exit}}                 53     0      0           41
{\rebuttal{SW perf. exit}}                 22  162   127           0
{\rebuttal{performance entry}}                 82     0      0           0
{\rebuttal{\st{safety}mission-critical} exit}        23      0    147           0
{HW \rebuttal{\st{safety}miss.-crit.} entry}    86    198      0          24
{SW \rebuttal{\st{safety}miss.-crit.} entry}    87    195    126           0
{HW fault recovery}   0      0      0          24
{SW fault recovery}   0    247    116           0
    }\testdata
    \begin{tikzpicture}
        \begin{axis}[
            xbar stacked,   % Stacked horizontal bars
            xmin=0,         % Start x axis at 0
            ytick=data,     % Use as many tick labels as y coordinates
            %nodes near coords,
            legend style={at={(axis cs:335, 0.8)},anchor=south west, draw=none},
            yticklabels from table={\testdata}{Label},  % Get the labels from the Label column of the \datatable
            bar width=15pt,
            xmajorgrids=true,
            grid style=dashed,
            ytick pos=left,
            width=0.8\columnwidth,
            height=7.5cm,
            xlabel={Measured Cycle Count},
            axis line style={draw=none}
        ]
            \addplot [fill=PULPred!80, draw opacity=0,] table [x=Setup, meta=Label,y expr=\coordindex] {\testdata};   % "First" column against the data index
            \addplot [fill=PULPblue!80, draw opacity=0,] table [x=Unload, meta=Label,y expr=\coordindex] {\testdata};
            \addplot [fill=PULPgreen!80, draw opacity=0,] table [x=Reload, meta=Label,y expr=\coordindex] {\testdata};
            \addplot [fill=PULPorange!80, draw opacity=0,] table [x={HW fill}, meta=Label,y expr=\coordindex] {\testdata};
            \legend{Setup,Unload,Reload,{HW fill}}
            \draw[line width=0.25mm, black] [stealth-stealth, dashed] (24,6) -- (363,6) node [right] {15$\times$};
            \draw[line width=0.25mm, black] [] (363,7.5) -- (363,5.8);
            \draw [line width=0.25mm, black] [stealth-stealth, dashed] (308,4) -- (408,4) node [right] {1.3$\times$};
            \draw[line width=0.25mm, black] [] (408,5.5) -- (408,3.8);
            \draw [line width=0.25mm, black] [stealth-stealth, dashed] (94,0) -- (311,0) node [right] {3.3$\times$};
            \draw[line width=0.25mm, black] [] (311,1.5) -- (311,-0.2);
        \end{axis}
        % \draw [line width=0.25mm, black] [stealth-stealth, dashed] (0.55,3.1) -- (8.45,3.1) node [right] {15$\times$};;
        % \draw [line width=0.25mm, black] [stealth-stealth, dashed] (7.2,1.3) -- (9.50,1.3) node [right] {1.3$\times$};;
    \end{tikzpicture}
    \caption{Cycle count breakdown of the fault recovery and split-lock overheads.}
    \label{fig:bars_cycles}
\end{figure}
% We analyzed the recovery performance of the proposed cluster in \gls{dmode} and \gls{tmode} by injecting faults into the simulation directly at the boundary signals of the cores, thus triggering the recovery hardware immediately. In independent mode, the cluster requires the external host core to monitor its execution to detect whether a fault happened, with no hardware support to indicate any error. In \gls{dmode} with no \textit{rapid recovery} hardware, the cluster execution is restarted whenever a fault is detected. In these two configurations, the fault recovery is application-dependent, so no results were provided in the discussion below.

%The software recovery routine described in \Cref{sec:sw-resynch} required 363 cycles to execute a recovery in \gls{tmode} fully. 
% This section describes the performance of the software and hardware recovery mechanisms described in \Cref{sec:sw-resynch}.
The software-based recovery routine, described in \Cref{sec:sw-resynch}, required 363 cycles to execute a recovery in \gls{tmode}, as shown in \Cref{tab:results}. Figure~\ref{fig:bars_cycles} highlights that this is split between the \textit{unload} section, requiring 247 cycles, after which it executes a synchronous clear of the core. Another 116 cycles are required for the \textit{reload} phase, reloading the cores' state from memory. The \textit{rapid recovery} hardware contributes positively by introducing a $15\times$ speed-up during the fault recovery, thus reducing it to just 24 cycles. This recovery time initially requires 4 cycles to set up the \textit{rapid recovery} controller, start the recovery routine, synchronously clear the cores, and send them the debug request. Additional 4 cycles are needed to receive the debug response from the cores, and 16 cycles to restore the \gls{rf}, \gls{pc}, and the \glspl{csr} in parallel, after which they will continue processing. Furthermore, the \textit{rapid recovery} hardware also allows for fast fault recovery in \gls{dmode}, requiring the same fixed 24 cycles. The cycle-by-cycle backups of the cores' status make it possible to roll back the cores' execution to the closest safe state in time, restarting the execution from the last \gls{pc} that entered the execution stage.

% \begin{figure}[t]
%     \centering
%     \pgfplotstableread{ % data 
% Label             Setup Unload Reload   {HW fill}
% {safety exit}        23      0    147           0
% {HW safety entry}    86    198      0          24
% {SW safety entry}    87    195    126           0
% {HW fault recovery}   0      0      0          24
% {SW fault recovery}   0    247    116           0
%     }\testdata
%     \begin{tikzpicture}
%         \begin{axis}[
%             xbar stacked,   % Stacked horizontal bars
%             xmin=0,         % Start x axis at 0
%             ytick=data,     % Use as many tick labels as y coordinates
%             %nodes near coords,
%             legend style={at={(axis cs:405, 2.7)},anchor=south west, draw=none},
%             yticklabels from table={\testdata}{Label},  % Get the labels from the Label column of the \datatable
%             bar width=17pt,
%             xmajorgrids=true,
%             grid style=dashed,
%             ytick pos=left,
%             width=0.8\columnwidth,
%             height=6cm,
%             xlabel={Measured Cycle Count},
%             axis line style={draw=none}
%         ]
%             \addplot [fill=PULPred!80, draw opacity=0,] table [x=Setup, meta=Label,y expr=\coordindex] {\testdata};   % "First" column against the data index
%             \addplot [fill=PULPblue!80, draw opacity=0,] table [x=Unload, meta=Label,y expr=\coordindex] {\testdata};
%             \addplot [fill=PULPgreen!80, draw opacity=0,] table [x=Reload, meta=Label,y expr=\coordindex] {\testdata};
%             \addplot [fill=PULPorange!80, draw opacity=0,] table [x={HW fill}, meta=Label,y expr=\coordindex] {\testdata};
%             \legend{Setup,Unload,Reload,{HW fill}}
%         \end{axis}
%         \draw [line width=0.25mm, black] [stealth-stealth, dashed] (0.55,3.1) -- (8.45,3.1) node [right] {15$\times$};;
%         \draw [line width=0.25mm, black] [stealth-stealth, dashed] (7.2,1.3) -- (9.50,1.3) node [right] {1.3$\times$};;
%     \end{tikzpicture}
%     \caption{Cycle count breakdown of the fault recovery and split-lock overheads.}
%     \label{fig:bars_cycles}
% \end{figure}

\subsubsection{Split-lock Performance}
% We evaluate the performance of the proposed split-lock mechanism by measuring the cycles required to enter or exit a \textit{\rebuttal{\st{safety}mission}-critical section} for the main and helper cores, executing the \textit{safety-critical section} twice to warm up the instruction cache.
%\Cref{tab:results} and 
We evaluate the performance of the proposed split-lock mechanism by measuring the cycles required to enter or exit a \textit{\rebuttal{\st{safety}mission}-critical section} for the main and helper cores\rebuttal{ as well as a \textit{performance section}}, as shown in \Cref{tab:results}. The entry measurement starts from the point the main core initiates the entry into the relevant \textit{\rebuttal{\st{safety-critical}} section} to the point the \rebuttal{\textit{\st{safety-critical section}}internal} code is being executed.

\rebuttal{For the \textit{mission-critical section}, }\Cref{fig:bars_cycles} shows that the main core first sets up the registers for the intended mode within 87 cycles, during which time the helper core(s) can continue processing. At this point, the interrupt is issued to all cores, launching them into the \textit{unload} stage, where the cores save their state to memory and enter a barrier, requiring 198 cycles. Finally, during the \textit{reload} phase the cores reload the main core's state from memory in the locked configuration, requiring 126 cycles, after which the cores execute the \textit{\rebuttal{\st{safety}mission}-critical section} code. When exiting the \textit{\rebuttal{\st{safety}mission}-critical section}, the main core goes directly into the subsequent software, requiring only 23 cycles to set the configuration and return. On the other hand, the helper cores need to execute the \textit{reload} procedure, requiring 147 cycles to continue the previously interrupted thread.
% taking just 23 clock cycles. On the other hand, the helper cores have to reload their state from memory prior to continuing their execution, requiring 147 clock cycles in \gls{dmode} and 165 in \gls{tmode}.

\rebuttal{For the \textit{performance section}, \Cref{fig:bars_cycles} shows lower entry- and exit- cycle requirements than the \textit{mission-critical section}. This is due to the additional threads spawned in the \textit{performance section} not loading a stored state and not preserving their independent state either.}

Introducing the \textit{rapid recovery} hardware helps speed up entering a \textit{\rebuttal{\st{safety}mission}-critical section} as the software \textit{reload} stage can be replaced with a 24-cycle hardware fill from the continuously backed-up main core, meaning $\sim25\%$ less than the software-based approach for a $1.3\times$ speedup. \rebuttal{Similarly, exiting a \textit{performance section} also makes use of the continually replicated main core state, leading to a $3.3\times$ speedup.} Exiting a \textit{\rebuttal{\st{safety}mission}-critical section} \rebuttal{or entering a \textit{performance section}} behaves the same way as in the software-based approach.
% Exiting a \textit{safety-critical section} using the \textit{rapid recovery} hardware requires 23 clock cycles for the main cores. This is the same as in the software-based approach since the main cores simply continue their operation.% On the other hand, the helper cores take 184 and 182 clock cycles to exit the \textit{safety-critical section}, respectively, in \gls{dmode} and \gls{tmode}, which is longer with respect to the software-based execution. The reason for this is that the \textit{rapid recovery} hardware does not contain additional backup registers for each core, so the helper cores still have to reload their previous state from the stack with colder caches with respect to the software-based case.


% \begin{table}[ht]
%     \centering
%     \begin{tabular}{@{}ll rrrrrrr@{}}\toprule
%                                         &  & \multicolumn{1}{c}{base} & \multicolumn{1}{c}{DMR} & \multicolumn{1}{c}{TMR} & \multicolumn{1}{c}{HMR} & \multicolumn{1}{c}{DMR-R} & \multicolumn{1}{c}{TMR-R} & \multicolumn{1}{c}{HMR-R} \\ \cmidrule{7-9}
%                                         &  &  &  &  &  & \multicolumn{3}{c}{Rapid Recovery} \\ \midrule
%         Performance [cycles]            &  & 10203 & 19266 & 28708 & \multicolumn{1}{c}{-} & 19266 & 28708 & \multicolumn{1}{c}{-} \\
%         Recovery [cycles]                 &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 363(383) & \multicolumn{1}{c}{-} & 24 & 24 & \multicolumn{1}{c}{-} \\
%         % \multirow{2}{*}{Switching [cycles]} & entry & \multicolumn{1}{c}{-} & 427 (938) & 350 (924) &  &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
%         %                                   & exit & \multicolumn{1}{c}{-} & 23/139 (43/180) & 22/143 &  &  & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
%         \multirow{3}{*}{Switching [cycles]} & entry & \multicolumn{1}{c}{-} & 534 (1161) & 432 (1063) & \multicolumn{1}{c}{-} & 397 (910) & 367 (817) & \multicolumn{1}{c}{-} \\
%                                           & exit main & \multicolumn{1}{c}{-} & 22 (40) & 23 (60) & \multicolumn{1}{c}{-} & 22 (41) & 23 (41) & \multicolumn{1}{c}{-} \\
%                                           & exit help & \multicolumn{1}{c}{-} & 147 (169) & 165 (209) & \multicolumn{1}{c}{-} & 184 (360) & 182 (379) & \multicolumn{1}{c}{-} \\
%         Area [\si{\milli\meter\squared}]             &  & \multicolumn{1}{c}{0.598} & \multicolumn{1}{c}{0.605} & \multicolumn{1}{c}{0.608} & \multicolumn{1}{c}{0.612} & \multicolumn{1}{c}{0.654} & \multicolumn{1}{c}{0.657} & \multicolumn{1}{c}{0.660} \\
%         % Max frequency [\si{\mega\hertz}]   &  &  &  &  &  &  &  &  \\
%         % Power [\si{\milli\watt}]           &  &  &  &  &  &  &  &  \\
%         \bottomrule
%     \end{tabular}
%     \caption{TODO: This table is optional, initially placed here to collect goals and results. If kept, explain performance benchmark (24x24 matmul), explain synth tech (GF22)}
%     \label{tab:results}
% \end{table}

% \begin{figure}[t]
% \centering
% \begin{tabular}[c]{cc}
%   \begin{subfigure}[c]{0.3\columnwidth}
%       \includegraphics[width=\columnwidth]{src/figures/PhysicalLayout.pdf}
%   \caption{\gls{pulp} cluster implemented layout.}
%   \end{subfigure}&\hspace{2cm}

%   \begin{subfigure}[c]{0.4\columnwidth}
%         \centering
%         \begin{tabular}{@{}lrr@{}}\toprule
%             \multicolumn{2}{@{}l}{PULP Cluster Area} & Overhead \\ \midrule
%             Baseline & \SI{0.604}{\micro\meter\squared} & \\
%             DMR & \SI{0.605}{\micro\meter\squared} & 0.3\% \\
%             TMR & \SI{0.608}{\micro\meter\squared} & 0.7\% \\
%             HMR & \SI{0.612}{\micro\meter\squared} & 1.3\% \\ \midrule
%             \multicolumn{3}{c}{With Rapid Recovery} \\ \midrule
%             DMR & \SI{0.654}{\micro\meter\squared} & 8.4\% \\
%             TMR & \SI{0.657}{\micro\meter\squared} & 8.8\% \\
%             HMR & \SI{0.660}{\micro\meter\squared} & 9.4\% \\
%             \bottomrule
%         \end{tabular}
%   \caption{\gls{pulp} cluster area comparison in all available configurations.}
%   \end{subfigure}\vspace{0.2cm}\\

%   \begin{subfigure}[c]{0.3\columnwidth}
%     \begin{tikzpicture}
%       \pie[scale=0.6,
%            color = {
%            cyan!50!black,
%            magenta!60!pink,
%            cyan,
%            yellow,
%            red!55!black,
%            green!40!black,
%            orange!50!white,
%            orange!75!black},
%            explode={0, 0, 0, 0, 0, 0, 0, 0},
%            rotate=-15]
%            {9/HMR Wrap,
%             15/Cores (Odd IDs), 
%             15/Cores (Even IDs),
%             18/TCDM,
%             28/Instr. Cache,
%             5/DMA,
%             2/Interconnect,
%             8/AXI XBAR}
%      \end{tikzpicture}
%   \caption{\gls{pulp} cluster area breakdown.}
%   \end{subfigure}&\hspace{1cm}
%   \begin{subfigure}[c]{0.3\columnwidth}
%     \begin{tikzpicture}
%       \pie[scale=0.5] 
%            {78/Rapid Recovery,
%             7/Control,
%             5/DMR Checkers,
%             10/TMR Voters}
%     \end{tikzpicture}
%   \caption{\gls{hmr} area breakdown.}
%   \end{subfigure}\\
% \end{tabular}

% \label{fig:pie}
% \end{figure}

\subsection{Reliability Assessment}
With the \gls{hmr} implementation outlined in this work, any error within a single core propagating to its outputs will be detected when mismatching with the paired cores, and corrective action will be taken. Furthermore, with the \textit{rapid recovery} extension enabled, core-internal signals backing up the \gls{rf} are also checked, reducing the likelihood of latent errors within the cores' states. While it aims to detect and correct all possible \glspl{set} happening at the cores' interface or within the cores themselves, the proposed redundancy approach does not tackle \glspl{seu} and single hard errors that might corrupt the data and instruction memory hierarchy, which we thus assume to be reliable.

\rebuttal{\st{Using simulation-driven fault injection,}Within an RTL simulation, a single bit of} the cores' interface signals \rebuttal{\st{were}was} inverted \rebuttal{using a \texttt{force} command} during the execution of a test within the locked state to confirm that \rebuttal{\st{all}} errors were detected and that the designed corrective action was initiated. Furthermore, select state bits within the cores' \gls{rf} were flipped to confirm the proper behavior of the recovery implementations. All faults injected into the system were detected and corrected or overwritten before use, thus always leading to the correct termination of the running application.

The design was further subjected to a more extensive fault injection campaign, where all registers within a core were randomly subjected to faults. Running 7717 \rebuttal{individual RTL} simulations of the matrix multiplication performance benchmark in the \gls{tmode} configuration with software recovery, \rebuttal{a single register was picked and flipped for each simulation run. \st{a}A}ll simulations terminated successfully\rebuttal{\st{. Furthermore, only}, with} 12\% of injected errors \rebuttal{\st{led}leading} to a recovery, meaning the other injections were masked.

Other research has conducted extensive fault investigation of the CV32E40P core used in the cluster based on fault injection~\cite{asciolla_characterization_2020}, showing that many injected faults do not affect the program execution, with the most critical module being the controller module.
As only 12\% of injected errors lead to corrective action, this confirms the results of \citeauthor{asciolla_characterization_2020}~\cite{asciolla_characterization_2020}, where 44\%-100\% of injected errors lead to no correction, depending on the component.
% Therefore further faults were injected into the controller module, with the implemented reliability scheme ensuring proper error recovery.

% \todo{Expand on fault injection approach and method!}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\columnwidth]{src/figures/PerformanceDegradation.pdf}
    
%     \caption{\gls{pulp} cluster performance degradation at increasing fault injection.}
%     \label{fig:perfdegr}

   
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \begin{tikzpicture}
%         \begin{axis}[
%             xmin=0.1,
%             xmax=3000,
%             ytick distance=0.2,
%             ymin=0,
%             ymax=1.25,
%             samples=50,
%             xmode=log,
%             width=0.8\columnwidth,
%             height=6cm,
%             ylabel={Performance [GOPS]},
%             xlabel={Fault Rate [\# faults/application execution]},
%         ]
%         % X axis: x for faults
%         % X axis: x/cycles*f for faults/s
%         % DCLS y axis: ops/(cycles * (1+x))*f
%         % DCLS-R/TCLS/TCLS-R y axis: ops/(cycles+x*recovery)*f
%             \addplot[PULPpurple, ultra thick, domain=0.01:3000] {27648/10203*0.43};
%             \addplot[PULPred,  ultra thick, domain=0.01:3000] {(27648/(19266+(x*24))*0.43)};
%             \addplot[PULPblue, ultra thick, domain=0.01:3000] {(27648/(19266*(1+x)))*0.43};
%             \addplot[PULPgreen,  ultra thick, domain=0.01:3000] {(27648/(28708+(x*383/2))*0.43)};
%             \addplot[PULPorange,  ultra thick, domain=0.01:3000] {(27648/(28708+(x*24/2))*0.43)};
%             \legend{independent, DCLS rapid, DCLS, TCLS rapid, TCLS}
%         \end{axis}
%     \end{tikzpicture}
%     \caption{\gls{pulp} cluster performance degradation at increasing fault rate}
%     \label{fig:perfdegr}
% \end{figure}
\subsection{Recovery Use-case Analysis: Satellite Onboard Image Processing}
One of the most common use cases of multi-core computing systems for \gls{scps} applications is using such devices for satellite onboard image processing. Satellites' onboard sensors generate a large amount of data that heavily delays the transmission process. When images captured onboard need to be processed through complex deep learning algorithms, the satellite first transmits images to the cloud computing data center of the ground station, where images generated by the satellite can load the transmission line for Gigabits. Then, the data center operates on the received data using deep learning models and distributes the processing result to the user~\cite{wei_application_2019}. To reduce the overhead given by raw data transmission, the data processing can be directly moved onboard a spacecraft, only sending valuable information over the communication link. This requires the onboard processing systems to be highly reliable and offer enough performance to process data while recovering from incurring faults.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        
        \begin{axis}[
            xmin=1000,
            xmax=1e8,
            ytick distance=0.2,
            ymin=0,
            ymax=1.25,
            samples=50,
            xmode=log,
            width=0.8\columnwidth,
            height=6.5cm,
            ylabel={Performance [GOPS]},
            xlabel={Fault Rate [\# faults/s]},
        ]
        % X axis: x for faults
        % X axis: x/cycles*f for faults/s
        % DCLS y axis: ops/(cycles * (1+x))*f
        % DCLS-R/TCLS/TCLS-R y axis: ops/(cycles+x*recovery)*f
            \addplot[PULPpurple, ultra thick, dashed, domain=0.01:1e8] {27648/10203*0.43};
            \addplot[PULPred,  ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/19266*430e6},{(27648/(19266+((x)*24))*0.43)});
            \addplot[PULPblue, ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/19266*430e6},{(27648/(19266*(1+(x))))*0.43});
            \addplot[PULPorange,  ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/28708*430e6},{(27648/(28708+((x)*24/2))*0.43)});
            \addplot[PULPgreen,  ultra thick, samples at={0.005138231,0.007707347,0.01156102,0.01734153,0.026012295,0.039018442,0.058527663,0.087791495,0.131687243,0.197530864,0.296296296,0.444444444,0.666666667,1,1.5,2.25,3.375,5.0625,7.59375,11.390625,17.0859375,25.62890625,38.44335938,57.66503906,86.49755859,129.7463379,194.6195068,291.9292603,437.8938904,656.8408356,985.2612534,1477.89188,2216.83782,3325.25673,4987.885095,7481.827643}] ({x/28708*430e6},{(27648/(28708+((x)*383/2))*0.43)});
            
            
            \legend{Baseline Performance, \gls{dcls} rapid, \gls{dcls}, \gls{tcls} rapid, \gls{tcls}}

            \draw [line width=0.25mm, cyan!70!black] [stealth-stealth, dashed] (1e6,0.03) -- (1e6,0.57) node [right] at (1.02e6,0.34) {> 6$\times$};
            \draw [line width=0.25mm, cyan!70!black] [stealth-stealth, dashed] (1e7,0.09) -- (1e7,0.31) node [right] at (1.02e7,0.2) {> 3$\times$};
        \end{axis}
    % \draw [line width=0.25mm, cyan!70!black] [stealth-stealth, dashed] (6.3,0.1) -- (6.3,2.0) node [right] at (6.31,1.17) {> 6$\times$};
    % \draw [line width=0.25mm, cyan!70!black] [stealth-stealth, dashed] (8.4,0.3) -- (8.4,1.1) node [right] at (8.41,0.7) {> 3$\times$};
    \end{tikzpicture}
    \caption{\gls{pulp} cluster performance degradation at increasing fault rate.}
    \label{fig:perfdegr}
\end{figure}

\Cref{fig:perfdegr} shows the performance degradation of our radiation-tolerant cluster when affected by multiple sequential faults in all the available redundant configurations. We considered an adverse condition where faults happen cycle-by-cycle over the entire execution time of an application, always affecting the same group of cores until a recovery happens. This means that in \gls{tmode}, if a fault affects one of the three grouped cores, the other two continue their operation. In the next cycle, we consider another fault affecting the remaining two cores, forcing them to perform a recovery. On the other hand, in \gls{dmode} without \textit{rapid recovery}, the cluster operation is restarted at each fault occurrence.

The analysis conducted shows that the worst-case scenarios are the software-based recovery procedures. In the \gls{dmode} case, a performance drop of $50\%$ happens with $\sim$\rebuttal{\SI{2e4}{faults\per\second}\st{$20\times10^4$ faults/s}} injected. In the \gls{tmode}, the same performance drop happens with around \rebuttal{\SI{2e6}{faults\per\second}\st{$20\times10^6$ faults/s}}. When using the \textit{rapid recovery} extension, the increased fault rate reduces the performance of the computing cluster much slower, as shown by the \gls{dcls} rapid and \gls{tcls} rapid curves in \Cref{fig:perfdegr}. The \textit{rapid recovery} feature allows for more than $6\times$ performance speedup during fault recovery over the software-based procedure for up to \SI{e6}{faults\per\second}, with the performance of the \gls{dcls} rapid case remaining almost constant. Similarly, \gls{tcls} rapid delivers $3\times$ speedup over the software-based recovery with \SI{e7}{faults\per\second} injected. Interestingly, the \gls{dcls} rapid performs better than \gls{tcls} rapid until the fault rate reaches around \rebuttal{\SI{3e7}{faults\per\second}\st{$30\times10^7$ faults/s}}. As the fault rate increases further, the performance in \gls{tcls} rapid is the best because, with \gls{tcls}, we do not necessarily need to recover immediately from a fault. If one of the three grouped cores is faulty, the computation can continue with the other two cores, and a re-synchronization is only needed if another fault affects the same group of cores. In the \gls{dcls} rapid case, a re-synchronization is needed as soon as a fault happens. With \textit{rapid recovery} features, we could observe a $50\%$ performance degradation with a fault rate of around \rebuttal{\SI{2e7}{faults\per\second}\st{$20\times10^7$ faults/s}} and \rebuttal{\SI{4e7}{faults\per\second}\st{$40\times10^7$ faults/s}} for \gls{dcls} rapid and \gls{tcls} rapid, respectively. Furthermore, the conducted analysis shows that the proposed fault-tolerant \gls{pulp} cluster allows for higher computing performance depending on the criticality of the application.

In independent mode, we only have two ways to determine that a fault happened. The first case is instructions faults, meaning incoming faults compromise the executed instructions sending the victim core in an unknown state. In this case, the faulty core stalls, and an external watchdog restarts the cluster operation. The second scenario is that of data faults. Suppose a fault affects the calculation of the cluster cores. In that case, we can only identify that an error happens if the host core knows the computation results a priori and can verify the results produced by the cluster computation. Both these two scenarios imply a significant overhead, similar to or worse than the \gls{dcls} software-based recovery.