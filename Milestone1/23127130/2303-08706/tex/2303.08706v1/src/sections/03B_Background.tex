\section{Background}\label{section:Background}

The hardware template we rely on is the multi-core \gls{pulp} cluster that can be extended with an external host subsystem featuring a controller core, a larger memory, and interfaces for external peripherals. The host subsystem is not area and performance-critical, and we assume it is fault tolerant using classic techniques such as \gls{radhard} technologies and other error correction features.

 

% \begin{figure}[t]
%     \centering
%     % \includegraphics[width=0.7\columnwidth]{src/figures/Cluster.pdf}
%     \includegraphics[width=0.7\columnwidth]{src/figures/hmr-cluster-plain.drawio.pdf}
%     \caption{\gls{pulp} cluster architecture.}
%     \label{fig:multicluster}
% \end{figure}

\subsection{PULP cluster}
The \gls{pulp} cluster~\cite{rossi_pulp_2015}%, illustrated in \Cref{fig:multicluster},
is built around a parametric number of 32-bit CV32E40P cores~\cite{gautschi_near-threshold_2017}, allowing for fast \gls{dsp} calculations. This open-source core is industrially verified by the OpenHW Group~\cite{group_openhw_2023}, ensuring the hardware is designed and operates as specified.
Following the Harvard architecture template, the cores have two main interfaces. The first is the instruction interface, connected to a hierarchical instruction cache~\cite{chen_scalable_2023}. The instruction cache consists of multiple private banks, one for each of the core, each configured to store \SI{512}{\byte}. These private banks fetch their instruction from a larger, shared cache of \SI{4}{\kibi\byte}, improving the performance of typical applications that parallelize the same instructions across the cores for different data.

The second interface is the data interface, connecting the cores to the rest of the system through dedicated demultiplexers for direct access to the system's \gls{tcdm} and all other memory-mapped peripherals. The \gls{tcdm} comprises a parametric number of \SI{32}{\bit} word-interleaved memory banks featuring single-cycle access latency. A banking factor of 2 (i.e., the number of memory banks is twice the number of cores) is typically used to minimize the memory banks contention probability, guaranteeing data sharing overhead smaller than 5\%, even for highly memory-intensive workloads. If a collision occurs, round-robin arbitration guarantees fairness and avoids starvation.

Along with the connection to the \gls{tcdm}, the cores have access to a peripheral interconnect, enabling access to both memory-mapped devices within the cluster and the host domain. One of the core-local peripherals is the \gls{dma} unit, capable of up to \SI{64}{\bit\per cycle} data transfers in either direction, full duplex between the external larger memories in the host subsystem and the cluster's local \gls{tcdm}.

Finally, the cores directly connect to an event unit~\cite{glaser_energy-efficient_2021} responsible for synchronization barriers within the cluster. Each core attempts to read from the corresponding register within the event unit and only receives a response once all cores request the same barrier address. Furthermore, the event unit manages interrupts within the cluster, masking and forwarding incoming interrupt signals to the responsible core.

The \gls{pulp} ecosystem offers open-source software for parallel code execution on the multi-core cluster~\cite{montagna_streamlining_2021}. For various host systems, the software is provided to configure and boot the accelerating cluster and basic low-level drivers that allow for parallel software execution. The software ecosystem also provides a variety of parallel applications and benchmarks, enabling fast development of custom workloads. 