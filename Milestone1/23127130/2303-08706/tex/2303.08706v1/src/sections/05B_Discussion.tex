\section{Discussion}\label{section:Discussion}
\begin{table}[t]
    \centering
        \begin{adjustbox}{width=1\textwidth,center=\textwidth}
        \begin{tabular}{@{}l ccccccc@{}}\toprule
             & Fault recovery & Split-Lock & Area overhead & Recovery Cycles & Frequency [\si{\mega\hertz}] & Area [\si{\milli\meter\squared}] & \# Cores\\ \midrule
            % \textbf{This Work (\gls{hmr})} & \textbf{HW/SW} & \textbf{Interrupt} & \textbf{9\%} & \textbf{24/-} & \textbf{430} &  & \textbf{12} \\
            \textbf{This Work (\gls{dmode})} & \textbf{HW/SW} & \textbf{Runtime} & \textbf{7.8\%/1.4\%} & \textbf{24/-} & \textbf{430} & \textbf{0.657/0.608} & \textbf{12} \\
            \textbf{This Work (\gls{tmode})} & \textbf{HW/SW} & \textbf{Runtime} & \textbf{8.3\%1.8\%} & \textbf{24/363} & \textbf{430} & \textbf{0.654/0.605} & \textbf{12} \\
            ARM DCLS~\cite{iturbe_soft_2016} & SW & At reset & - & Application dependent & - & - & 2 \\
            ARM TCLS~\cite{iturbe_arm_2019} & SW & No & 6\% & 2351 & 314 & 12.92 & 3 \\
            \citeauthor{shukla_low-overhead_2022}~\cite{shukla_low-overhead_2022} & HW & Yes & 17.9\% & 1 & 41 & 0.223 & 4 \\
            \citeauthor{kempf_adaptive_2021}~\cite{kempf_adaptive_2021} & SW & Runtime & 55,9\% (LUTs) & - & 100 & - & 2 \\
            CEVERO~\cite{silva_cevero_2020} & HW & No & - & 40 & - & - & 2 \\
            SHAKTI-F~\cite{gupta_shakti-f_2015} & HW & No & 20.5\% & 3 & 330 & 0.325 & 1 \\
            Duck Core~\cite{li_duckcore_2021} & HW & No & 0.7\% (LUTs) & 3 & 50 & - & 1 \\
            \bottomrule
        \end{tabular}
        \end{adjustbox}
    \caption{State of the Art Comparison Table}
    \label{tab:rel_results}
\end{table}

Table~\ref{tab:rel_results} resumes the comparison of the proposed paper against state of the art. We compare with ARM lockstep implementation in \gls{dmode} and \gls{tmode} as they represent the closest point of comparison with our work. ARM TCLS~\cite{iturbe_arm_2019} provides additional hardware assist unit to group three cores to operate in lockstep. While the cores are synthesised using standard cells technology, the assist unit is implemented \gls{radhard} cells. The major point of comparison between our work and ARM TCLS implementation is that the redundant grouping in the ARM case is hardwired, meaning there is no possibility to switch between the redundant mode and the independent mode. The performance reduction that results from this hardwired grouping can seriously affect the execution of compute intense tasks that require significant computing capabilities. In addition to this, the recovery routine proposed by ARM takes 2351 clock periods to conclude, meaning $6.5\times$ slower than our software-based \gls{tmode} recovery routine. ARM also proposes a DCLS approach~\cite{iturbe_soft_2016} that features split-lock functionalities. However, the operating mode of the processor can be decided only after the system reset, meaning that the cores have to be configured to statically execute the entire program in one mode or the other. The flexibility of our implementation allows to explicitly declare a \textit{safety-critical section} within the executed code, and the cluster can switch from independent to \gls{dmode} or \gls{tmode} at runtime with low switching overhead. Our fault tolerant cluster requires 534 clock cycles and 432 clock cycles to enter a \textit{safety-critical section} in \gls{dmode} and \gls{tmode} respectively with no hardware acceleration, while it takes 147 and 165 cycles to exit it. Time to enter a \textit{safety-critical section} can be further reduced of $1.35\times$ and $1.18\times$ respectively in \gls{dmode} and \gls{tmode} by the mean of the \textit{rapid recovery} hardware extension.

\citeauthor{kempf_adaptive_2021}~\cite{kempf_adaptive_2021} propose a dual core adaptive runtime-selectable lockstep based on a LEON processor with internal modifications for dual-core instruction comparison. The software recovery is software-bases and the overhead introduced for the recovery in terms of clock cycles is application dependant. Our fault tolerant \gls{pulp} cluster is cleared after a fault occurrence if only the software-based recovery is enabled. With the help of the \textit{rapid recovery} hardware extension, the performance overhead to recovery from incurring errors is fixed to 24 clock cycles. Also, the area overhead of the solution proposed by~\citeauthor{kempf_adaptive_2021} accounts for the $55.9\%$ in terms of LUTs over the regular LEON implementation, while the \gls{hmr} unit proposed in this paper accounts for just the $9\%$ of area overhead over the entire \gls{pulp} cluster.

CEVERO~\cite{silva_cevero_2020} proposes a DCLS approach based on RISC-V processors that is similar to our one with the benefit of the \textit{rapid recovery} hardware extension. However, in the CEVERO implementation the backup copies of the \gls{rf} and \gls{pc} are not protected by error correction, so if a fault occurs while copying the content of the cores' registers in the backup safe copies, there is no mechanism to guarantee that the backup state is reliable. Furthermore, CEVERO only relies on backup copies of the \gls{rf} and \gls{pc}, but these registers do not suffice in defining the state of the computing cores, for which also some specific \glspl{csr} are needed (such as the \textit{MEPC}). Also, in the CEVERO implementation the cores are locked permanently, and the recovery procedure takes $1.67\times$ the number of clock cycles when compared to our implementation since they do not exploit the two write ports available in the \gls{rf}.

\citeauthor{shukla_low-overhead_2022}~\cite{shukla_low-overhead_2022} present a quad-core RSIC-V-based re-configurable processor capable of operating both in independent and \gls{dmode} where the results generated by couples of cores are checked cycle by cycle. In case of mismatch detection, the fault-recovery procedure is based on holding the last value reached by the cores' \gls{pc} right before the mismatch detection, and restarting the processor's operation from the corresponding instruction, reducing the recovery latency to just one single clock cycle. However, since the cores run in \gls{dmode}, in case the \gls{seu} affects the \gls{pc} itself, it would be impossible to vote and decide on the right value of the \gls{pc} that must be re-executed by the grouped cores. In addition, the only \gls{pc} is insufficient to determine the state of the grouped cores right before the fault happened. The fault-tolerant processor they propose introduces up to $17.9\%$ area overhead over the base implementation, while our \gls{hmr} unit introduces just $9\%$ area overhead over a standard \gls{pulp} cluster, but adding more flexibility.

We also compare with other works such as SHATKI-F~\cite{gupta_shakti-f_2015} and Duck Core~\cite{li_duckcore_2021} that propose modifications to RISC-V cores' internal microarchitecture to perform pipeline rollback. These approaches are valuable since it requires up to just three clock cycles to perform a fault recovery. In addition, microarchitectural modifications allow for a single core to be reliable without the need of redundant grouping, that impacts performance. However, this requires significant modifications to the internal architecture of the cores, that can significantly change the behavior and compromise its formal verification.