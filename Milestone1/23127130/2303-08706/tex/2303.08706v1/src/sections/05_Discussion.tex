\section{State of the Art Comparison}\label{section:Discussion}
% \begin{table}[t]
%     \centering
%         \begin{adjustbox}{width=1\textwidth,center=\textwidth}
%         \begin{tabular}{@{}l ccccccc@{}}\toprule
%              & Fault recovery & Split-Lock & Area overhead & Recovery Cycles & Frequency [\si{\mega\hertz}] & Area [\si{\milli\meter\squared}] & \# Cores\\ \midrule
%             % \textbf{This Work (\gls{hmr})} & \textbf{HW/SW} & \textbf{Interrupt} & \textbf{9\%} & \textbf{24/-} & \textbf{430} &  & \textbf{12} \\
%             \textbf{This Work (\gls{dmode})} & \textbf{HW/SW} & \textbf{Runtime} & \textbf{7.8\%/1.4\%} & \textbf{24/-} & \textbf{430} & \textbf{0.657/0.608} & \textbf{12} \\
%             \textbf{This Work (\gls{tmode})} & \textbf{HW/SW} & \textbf{Runtime} & \textbf{8.3\%1.8\%} & \textbf{24/363} & \textbf{430} & \textbf{0.654/0.605} & \textbf{12} \\
%             ARM DCLS~\cite{iturbe_soft_2016} & SW & At reset & - & Application dependent & - & - & 2 \\
%             ARM TCLS~\cite{iturbe_arm_2019} & SW & No & 6\% & 2351 & 314 & 12.92 & 3 \\
%             \citeauthor{shukla_low-overhead_2022}~\cite{shukla_low-overhead_2022} & HW & Yes & 17.9\% & 1 & 41 & 0.223 & 4 \\
%             \citeauthor{kempf_adaptive_2021}~\cite{kempf_adaptive_2021} & SW & Runtime & 21.3\% (LUTs) & - & 100 & - & 2 \\
%             CEVERO~\cite{silva_cevero_2020} & HW & No & - & 40 & - & - & 2 \\
%             SHAKTI-F~\cite{gupta_shakti-f_2015} & HW & No & 20.5\% & 3 & 330 & 0.325 & 1 \\
%             Duck Core~\cite{li_duckcore_2021} & HW & No & 0.7\% (LUTs) & 3 & 50 & - & 1 \\
%             \bottomrule
%         \end{tabular}
%         \end{adjustbox}
%     \caption{State of the Art Comparison Table}
%     \label{tab:rel_results}
% \end{table}
\input{src/tables/soa_table}

\Cref{tab:soa_table} shows the details of our implementation alongside the State of the Art. Compared with ARM, their TCLS~\cite{iturbe_arm_2019} implementation features a split-lock mechanism that is based on resetting the system to configure it in independent or \gls{tmode}. Furthermore, the recovery routine they propose takes 2351 clock periods to conclude, meaning $6.5\times$ slower than our software-based \gls{tmode}. ARM's DCLS~\cite{iturbe_soft_2016} also features split-lock functionalities decided during system reset. Our split-lock allows for higher flexibility and performance, making the cluster capable of runtime switching between independent and \gls{dmode} or \gls{tmode} in 681 and 597 clock cycles, respectively, with further $1.17x$ and $1.08x$ reduction with the \textit{rapid recovery} hardware support. Moreover, ARM's TCLS implementation introduces $27\%$ area overhead over the single core implementation, while the \gls{hmr} unit with \textit{rapid recovery} features proposed in this work introduces just $9\%$ area overhead over the baseline implementation. 

\citeauthor{kempf_adaptive_2021}~\cite{kempf_adaptive_2021} propose a dual-core adaptive runtime-selectable lockstep processor based on a LEON core with internal modifications for instruction comparison. The fault recovery is software-based, with a time-to-recovery that is application dependent. Our \textit{rapid recovery} hardware extension on the other hand allows for fixed 24 cycles to recover from occurring faults. Also, the area overhead of the solution proposed by~\citeauthor{kempf_adaptive_2021} accounts for $21.3\%$ in terms of CLBs over the regular LEON implementation, while our full \gls{hmr} unit accounts for just $9\%$ of area overhead over a standard 12-cores \gls{pulp} cluster, with a similar dual-core configuration needing only $8.4\%$.

CEVERO~\cite{silva_cevero_2020} proposes a DCLS system with no split-lock capabilities and with a similar \textit{rapid recovery} hardware extension to ours, with a recovery procedure that takes $1.67\times$ longer than ours. Furthermore, CEVERO disregards copying the \glspl{csr}, which are fundamental in defining the complete recovery state of the core, and the backup copies of \gls{rf} and \gls{pc} are not protected by error correction, meaning there is no guarantee that the backup state is reliable.

\citeauthor{shukla_low-overhead_2022}~\cite{shukla_low-overhead_2022} present a quad-core RISC-V-based processor re-configurable for DCLS operation, introducing up to $17.9\%$ area overhead over the base implementation, while our \gls{hmr} unit offers more flexibility by introducing just $9\%$ area overhead over a standard 12-cores \gls{pulp} cluster. In addition, \citeauthor{shukla_low-overhead_2022} rely on saving just the last executed \gls{pc} for the recovery, which is insufficient to determine the entire state of the grouped cores.

In \Cref{tab:soa_table}, we also compare our design with other works, such as SHATKI-F~\cite{gupta_shakti-f_2015} and Duck Core~\cite{li_duckcore_2021}. The two works propose modifications to RISC-V cores' internal microarchitecture for pipeline rollback, showing a valuable approach that leads to just 3 clock cycles to perform a fault recovery and allowing for a single core to be reliable without the need for redundant grouping, thus saving resources. In contrast, the extensive modifications required by the internal architecture of the core can significantly change the behavior, compromising its formal verification.