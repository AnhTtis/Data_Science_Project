\section{Introduction}
Recent research has explored the exciting possibility of combining quantum information with computational hardness to enable classically infeasible cryptographic tasks. Beginning with proposals such as unforgeable money~\cite{Wiesner83}, this list has recently grown to include the possibility of provably deleting cryptographic information encoded into quantum states~\cite{Unruh2013,Broadbent_2020,hiroka2021quantum,cryptoeprint:2022/969,hiroka2021certified,Poremba22,cryptoeprint:2022/1178,BGGKMRR,AKNYY,cryptoeprint:2023/325}.


In this work, we further investigate the task of provable deletion of information via destructive measurements. 
We focus on building primitives that satisfy {\em publicly-verifiable deletion} ($\PVD$). This deletion property allows any participant in possession of a quantum encoding to publish a publicly-verifiable classical certificate proving that they deleted\footnote{In this work, we focus on \emph{information-theoretic} deletion of computationally hidden secrets, where the guarantee is that after deletion, even an unbounded adversary cannot recover the plaintext that was previously determined by their view \cite{cryptoeprint:2022/1178}.} the underlying plaintext. This is in contrast to the weaker {\em privately-verifiable deletion} property, where deletion can be verified only by parties that hold a secret verification key, and this key must remain hidden from the party holding the ciphertext.
Public verification is more desirable due to its stronger security guarantee: secret verification keys do not need to be stored in hidden locations, and security continues to hold even when the verification key is leaked. 
Furthermore, clients can outsource verification of deletion by publishing the verification key itself.

Our approach to building publicly verifiable deletion departs from templates used in prior works on deletion. While most prior works, building on~\cite{Unruh2013,Broadbent_2020}, rely on the combination of a quantum information-theoretic tool such as Wiesner encodings/BB84 states~\cite{Wiesner83,BB84} and a cryptographic object such as an encryption scheme, our work enables publicly-verifiable deletion by directly using simple cryptographic properties of many-to-one hash functions.
%, that we call {\em target-collapsing.}

%\dakshita{cite BI prominently somewhere in the intro I guess.}
%\dakshita{also discuss the information-theoretic deletion def from bk22 that we follow here.}


%\dakshita{@Alex: currently planning to write in the tech overview, when discussing collapsing hashes, that your paper had a concrete conjecture. Please feel free to suggest changes if you think your work is not credited properly.}


\paragraph{The Template, in a Nutshell.} 
When illustrating our approach to publicly-verifiable deletion, it will help to first consider enabling this for a simple cryptographic primitive: a commitment scheme. 
That is, we consider building a statistically binding non-interactive quantum bit commitment scheme where each commitment is accompanied by a classical, {\em public} verification key $\mathsf{vk}$. A receiver holding the commitment may generate a classical proof that they deleted the committed bit $b$, and this proof can be publicly verified against $\mathsf{vk}$. We would like to guarantee that as long as verification accepts, the receiver has information-theoretically removed $b$ from their view and will be unable to recover it given unbounded resources, despite previously having the bit $b$ determined by their view.

To allow verification to be a public operation, it is natural to imagine the certificate or proof of deletion to be a hard-to-find solution to a public puzzle. For instance, the public verification key could be an image $y$ of a (one-way) function, and the certificate of deletion a valid pre-image $f^{-1}(y)$ of this key. Now, the commitment itself must encode the committed bit $b$ in such a way that the ability to generate $f^{-1}(y)$ given the commitment implies information-theoretic deletion of $b$. This can be enabled by encoding $b$ in the {\em phase} of a state supported on multiple pre-images of $y$. 

Namely, given an appropriate {\em two-to-one} function $f$, 
%\dakshita{switching to encryption because purification is making it look ugly..}
a commitment\footnote{Technically, it is only an appropriate purification of the scheme described here that will satisfy binding; we ignore this detail for the purposes of this overview.} to a bit $b$ can be
\[\mathsf{Com}(b) = \left( y, \ket{0,x_0}_\sA + (-1)^b \ket{1,x_1}_\sA \right) \]
where $(0,x_0), (1,x_1)$ are the two pre-images of (a randomly sampled) image $y$.
%\dakshita{is mixed state notation better?}
%\dakshita{even this is technically not true. any ideas on how to write a simple, true claim about primitives following from one-wayness without trapdoors? or is this okay?}

Given an image $y$ and a state on register $\sA$, a valid certificate of deletion of the underlying bit could be any pre-image of $y$, which for a well-formed commitment will be obtained by measuring the $\sA$ register in the computational basis. It is easy to see that an immediate {\em honest} measurement of the $\sA$ register implies information-theoretic erasure of the phase $b$. 
But a malicious adversary holding the commitment may decide to perform arbitrary operations on this state in an attempt to find a pre-image $y$ without erasing $b$.
\iffalse{
\footnote{
This template is inspired by, and at the same time significantly generalizes, candidate encryption schemes with publicly-verifiable deletion suggested in recent work~\cite{Poremba22}. %\dakshita{we compare later}
%\iffalse{
Specifically,~\cite{Poremba22} suggested constructions for encryption with publicly-verifiable deletion assuming certain hash functions satisfy a conjectured {\em strong collapsing} property for more complex superpositions of preimages.
%{\em strong collapsing} property for {\em Gaussian superpositions of preimages}. 
In addition to proving conjectures from~\cite{Poremba22} under the (quantum) hardness of the Short Integer Solutions (SIS) problem, a key goal in our work is to base $\PVD$ on even weaker cryptographic assumptions.}
}\fi

In this work, we analyze  (minimal) requirements on the cryptographic hardness of $f$ in the template above, so that the ability to computationally find any preimage of $y$ given the commitment necessarily implies {\em information-theoretic} erasure of $b$. A useful starting point, inspired by recent conjectures in~\cite{Poremba22}, is the {\em collapsing} property of hash functions. This property was first introduced in~\cite{10.1007/978-3-662-49896-5_18} as a quantum strengthening of collision-resistance. 
%This is inspired by a recent work~\cite{Poremba22} that conjectured 

%In addition to proving conjectures in~\cite{Poremba22}, our work %}\fi
%Our key technical result is a proof that properties sufficient for deletion follow from specific standard cryptographic assumptions such as LWE, thereby proving conjectures in~\cite{Poremba22}. We also go a step further and 


%\dakshita{technically, our work has the same template as Alex's, so maybe should reword the previous sentence.}

\paragraph{Collapsing Functions.}
%{\em Collapsing} hash functions were introduced in~\cite{EC:Unruh16} as a quantum strengthening of collision-resistance. 
The notion of \emph{collapsing} considers an experiment where a computationally bounded adversary prepares an arbitrary superposition of preimages of $f$ on a register $\mathsf{A}$, after which the challenger tosses a random coin $c$. If $c = 0$, the challenger measures register $\mathsf{A}$, otherwise it measures a register containing the hash $y$ of the value on register $\mathsf{A}$, thus leaving $\mathsf{A}$ holding a superposition of preimages of $y$. The register $\mathsf{A}$ is returned to the adversary, and we say that $f$ is collapsing if the adversary cannot guess $c$ with better than negligible advantage.
%a preimage register containing a superposition of (two or more) preimages of a hash outcome $y$ cannot be computationally distinguished from a register containing a mixture of singleton preimages of $y$, 
%a computational basis measurement of the hash of a quantum superposition of messages is quantum computationally
%indistinguishable from measuring the message superposition itself
%despite both distributions being information-theoretically distinct.
Constructions of collapsing hash functions are known based on LWE~\cite{10.1007/978-3-662-53890-6_6}, low-noise LPN~\cite{crypto-2022-32202}, and more generally on special types of collision-resistant hashes. They have played a key role in the design of post-quantum protocols, especially in settings where proofs of security of these protocols rely on {\em rewinding} an adversary.

It is easy to see that 
\[\mathsf{Com}(b) = \left( y, \ket{0,x_0} + (-1)^b \ket{1,x_1} \right) \]
computationally hides the bit $b$ as long as the function $f$ used to build the commitment above is {\em collapsing}. Indeed, collapsing implies that the superposition $\ket{0,x_0} + (-1)^b \ket{1,x_1}$ is computationally indistinguishable from the result of measurement in the computational basis, and the latter perfectly erases the phase $b$. 
However, $\PVD$ requires  something stronger: we must show that any adversary that generates a valid pre-image of $y$ given the superposition $\ket{0,x_0} + (-1)^b \ket{1,x_1}$, must have {\em information-theoretically} deleted $b$ from its view, despite $b$ being information-theoretically present in the adversary's view before generating the certificate. We show via a careful proof that this is indeed the case for collapsing $f$.
Proving this turns out to be non-trivial. Indeed, a similar construction in~\cite{Poremba22} based on the Ajtai hash function \cite{DBLP:conf/stoc/Ajtai96} relied on an unproven conjecture, which we prove in this work by developing new techniques. 

In addition, we show how $f$ in the template above can be replaced with functions that satisfy weaker properties than collapsing, yielding $\PVD$ from regular variants of one-way functions. We discuss these results below.
%make many other definitional and conceptual contributions.
%, and introduce new proof techniques to obtain the results summarized below.

%\dakshita{language can be refined}
%then the bit $b$ is computationally hidden from the 

\subsection{Our Results}

We introduce new properties of (hash) functions, namely target-collapsing, generalized target-collision-resistance.
We will show that hash functions satisfying these properties (1) can be based on (regular) variants of one-way functions and (2) imply publicly-verifiable deletion in many settings. Our results also use an intermediate notion, a variant of target-collapsing that satisfies certified everlasting security. Before discussing our results, we motivate and discuss these new definitions informally below.


\subsubsection{Definitions}
%We also introduce a certified everlasting variant of target-collapsing,

\paragraph{Target-Collapsing and Generalized Target-Collision-Resistant Functions.}
Towards better understanding the computational assumptions required for $\PVD$, we observe that in the deletion experiment for the commitment above, 
%it is the receiver of the commitment that is adversarial, and 
the superposition $\ket{x_0} + (-1)^b \ket{x_1}$ is prepared by an {\em honest committer}. 
This indicates that the collapsing requirement, where  security is required to hold even for an adversarial choice of superposition over preimages, may be overkill. 


Inspired by this, 
%To this end, we show that a cryptographic property that we call {\em target-collapsing} suffices to provably instantiate the framework above. In defining this property, we move beyond two-to-one functions, and consider target-collapsing properties of arbitrary compressing functions.
we consider a natural weakening called {\em target-collapsing}, where {\em the challenger (as opposed to the adversary)} prepares a superposition of preimages of a random image $y$ of $f$ 
%according to some well-defined distribution $D_{\lambda}$ 
on register $\mathsf{A}$. After this, the challenger tosses a random coin $c$. If $c=0$, it does nothing to $\mathsf{A}$, otherwise it measures $\mathsf{A}$ in the computational basis.
%performs a {\em generalized measurement} -- i.e., measures a register containing a general hash-dependent function $M_f$ of the value on register $\mathsf{A}$, thus leaving $\mathsf{A}$ holding a superposition of preimages of $$. 
The register $\mathsf{A}$ is returned to the adversary, and we say that a hash function is target-collapsing if a computationally bounded adversary cannot guess $c$ with better than negligible advantage.

%to general superpositions and measurements.

As highlighted above, this definition weakens collapsing to allow the challenger (instead of the adversary) to prepare the preimage register. 
The weakening turns out to be significant because we show that target-collapsing functions are realizable from relatively weak cryptographic assumptions -- namely variants of one-way functions -- which are unlikely to imply (standard) collapsing or collision-resistant hash functions due to known black-box separations~\cite{10.1007/BFb0054137}.

To enable these instantiations from weaker assumptions, we first further generalize target-collapsing so that 
%the initial superposition of preimages on register $\mathsf{A}$ is sampled by the challenger according to an arbitrary distribution, and so that 
when $c=1$, the challenger applies a {\em binary-outcome measurement} $M$ %\dakshita{trying to keep it simple, hopefully this does not oversimplify} 
to $\mathsf{A}$ (as opposed to performing a computational basis measurement resulting in a singleton preimage). 
Thus, a template commitment with $\PVD$ from generalized target-collapsing hashes has the form:
\[\mathsf{Com}(b) = \left( y, \sum_{x:f(x) = y, M(x) 
= 0} \ket{x} + (-1)^b \sum_{x:f(x) = y, M(x) 
= 1} \ket{x} \right). \]
We show that this commitment satisfies $\PVD$ as long as $f$ is target-collapsing w.r.t. the measurement $M$, and satisfies an additional property of ``generalized'' target-collision-resistance (TCR), that we discuss next.




%In what follows, we summarize all our results and their implications.


Generalized target-collision-resistance is a quantum generalization of the (standard) cryptographic property of second pre-image resistance/target-collision-resistance.
Very roughly, this considers an experiment where the challenger first prepares a superposition of preimages of a random image $y$ of $f$ on register $\mathsf{A}$. After this, the challenger applies a measurement (e.g., a binary-outcome measurement) $M$ on $\mathsf{A}$ to obtain outcome $\mu$ and sends $\mathsf{A}$ to the adversary. We require that no polynomially-bounded adversary given register $\mathsf{A}$ can output {\em any} preimage $x'$ of $y$ such that $M(x') \neq M(\mu)$ (except with negligible probability)\footnote{We remark that this notion can also be seen as a generalization of ``conversion hardness'' defined in  \cite{HMY}.}.
%\dakshita{this previous sentence is not making sense, why measure $y$? I think I probably meant something like $\mathsf{A}$ shouldn't output a preimage that is not in $\mu$?}


%requires that no (polynomially-bounded) adversary, given a superposition over a set containing {\em some} (say half) of the preimages of a random image $y$, is able to find a preimage that is not contained in this set. In more detail, we define $(\cD, \cM)$ target-collision resistant functions as those where the initial distribution 
%For the purposes of this overview, we will in fact restrict ourselves to superpositions 



%\subsection{Our Results}
%\dakshita{smoother transition to the next paragraph}

\paragraph{Certified Everlasting Target-Collapsing.}
In order to show $\PVD$, instead of directly relying on target-collapsing (which only considers computationally bounded adversaries), %Instead, 
we introduce a stronger notion that we call \emph{certified everlasting} target-collapsing. This considers the following experiment: as before, the challenger prepares a superposition of preimages of a random image $y$ of $f$ on register $\mathsf{A}$. After this, the challenger tosses a random coin $c$. 
If $c = 0$, it does nothing to $\mathsf{A}$, otherwise it applies measurement $M$ to $\mathsf{A}$. The register $\mathsf{A}$ is returned to the adversary, after which the adversary is required to return a pre-image of $y$ as its ``deletion certificate''. While such a certificate can be obtained via an honest measurement of the register $\mathsf{A}$, the {\em certified everlasting target-collapsing} property requires that the following {\em everlasting} security guarantee hold. As long as the adversary is computationally bounded at the time of generating a valid deletion certificate, verification of this certificate implies that the bit $c$ is {\em information-theoretically} erased from the adversary's view, and cannot be recovered even given unbounded resources. That is, if the adversary indeed returns a valid pre-image, they will never be able to guess whether or not the challenger applied measurement $M$.
%\dakshita{motivate everlasting some more here or earlier.}

\subsubsection{New Constructions and Theorems}

\paragraph{Main Theorem.} Now, we are ready to state the main theorem of our paper.
In a nutshell, this says that any (hash) function $f$ that satisfies both target-collapsing and (generalized) target-collision resistance also satisfies {\em certified everlasting} target-collapsing.

\begin{theorem}(Informal).
    If $f$ satisfies target-collapsing and generalized target-collision-resistance with respect to measurement $M$, then $f$ satisfies {\em certified everlasting target-collapsing} with respect to the measurement $M$.
\end{theorem}

We also extend recent results from the collapsing literature \cite{cryptoeprint:2022/786,crypto-2022-32202,crypto-2022-32124} to show that for the case of binary-outcome (in fact, polynomial-outcome) measurements $M$, generalized TCR with respect to $M$ actually implies target-collapsing with respect to $M$. Thus, we obtain the following corollary.

%\begin{lemma}(Informal).
    %Any function that satisfies generalized TCR w.r.t. a {\em binary outcome} measurement $M$, also satisfies {\em target-collapsing} w.r.t. the measurement $M$.
%\end{lemma}

\begin{corollary}(Informal).
    If $f$ satisfies generalized target-collision-resistance with respect to a {\em binary-outcome} measurement $M$, then $f$ satisfies {\em certified everlasting target-collapsing} with respect to  the measurement $M$.
\end{corollary}


\paragraph{Resolving the Strong Gaussian Collapsing Conjecture~\cite{Poremba22}.} 
We now apply the main theorem and its corollary to build various cryptographic primitives with $\PVD$. First, we immediately {\bf prove} the following ``strong Gaussian-collapsing''\footnote{Here, ``Gaussian'' refers to a quantum superposition of Gaussian-weighted vectors, where the distribution assigns probability proportional to
$\rho_\sigma(\vec x) = \exp(-\pi \|\vec x \|^2/ \sigma^2)$ for vectors $\vec x \in \Z^m$ and parameter $\sigma >0$.} conjecture from \cite{Poremba22}, which essentially conjectures that the Ajtai hash function (based on the hardness of SIS) satisfies a certain form of key-leakage security after deletion.
This follows from our main theorem because the Ajtai hash function is known to be collapsing \cite{10.1007/978-3-030-26951-7_12,Poremba22} and collision-resistant (which implies that it is target-collapsing and target-collision-resistant when preimages are sampled from the Gaussian distribution).
%the notion of certified everlasting target-collapsing introduced in this work.

\begin{conjecture}[Strong Gaussian-Collapsing Conjecture,~\cite{Poremba22}] 
There exist $n,m,q \in \N$ with $m \geq 2$ and $\sigma > 0$ such that,
for every efficient quantum algorithm $\algo A$,
$$
\Big|\Pr[\mathsf{StrongGaussCollapseExp}_{\algo A,n,m,q,\sigma}(0)=1] - \Pr[\mathsf{StrongGaussCollapseExp}_{\algo A,n,m,q,\sigma}(1)=1]\Big| \leq \negl(\lambda)
$$
with respect to the experiment defined in \Cref{fig:SGC}.
\end{conjecture}

\begin{figure}[!htb]
   \begin{center} 
   \begin{tabular}{|p{14cm}|}
    \hline 
\begin{center}
\underline{$\mathsf{StrongGaussCollapseExp}_{\algo A,n,m,q,\sigma}(b)$}: 
\end{center}
\begin{enumerate}
    \item The challenger samples $ \bar{\vec A} \rand \Z_q^{n \times (m-1)}$
    and prepares the Gaussian state
    $$
    \ket{\psi}_{XY} = \sum_{\vec x \in \Z_q^m} \rho_\sigma(\vec x) \ket{\vec x}_X \otimes \ket{\vec A \cdot \vec x \Mod{q}}_Y,
    $$
    where $\vec A = [\bar{\vec A} \, \| \, \bar{\vec A} \cdot \bar{\vec x} \Mod{q}] \in \Z_q^{n \times m}$ is a matrix with $\bar{\vec x} \rand \bit^{m-1}$.
    
    
    \item The challenger measures $Y$ in the computational basis, resulting in
    $$
    \ket{\psi_{\vec y}}_{XY} = \sum_{\substack{\vec x \in \Z_q^m:\\ \vec A \vec x= \vec y \Mod{q}}} \rho_\sigma(\vec x) \ket{\vec x}_X \otimes \ket{\vec y}_Y.
    $$
\item If $b=0$, the challenger does nothing. Else, if $b=1$, the challenger measures system $X$ in the computational basis. The challenger then sends system $X$ to $\algo A$, together with the matrix $\vec A \in \Z_q^{n \times m}$ and the string $\vec y \in \Z_q^n$.

\item $\algo A$ sends a classical witness $\vec w \in \Z_q^m$ to the challenger.

\item The challenger checks if $\vec w$ satisfies $\vec A \cdot \vec w = \vec y \Mod{q}$ and $\| \vec w \| \leq \sigma \sqrt{m/2}$. If true, the challenger sends the trapdoor vector $\vec t = (\bar{\vec x},-1) \in \Z^m$ to $\algo A$, where $\vec A \cdot \vec t = \vec 0 \Mod{q}$. Else, the challenger outputs a random bit $b' \gets \{0,1\}$ and the game ends.
 
\item $\algo A$ returns a bit $b'$, which is retured as the output of the experiment.
\end{enumerate}
\ \\
\hline
\end{tabular}
    \caption{The strong Gaussian-collapsing experiment~\cite{Poremba22}.}
    \label{fig:SGC}
    \end{center}
\end{figure}

This conjecture, from~\cite{Poremba22} considers a slightly weaker notion of certified collapsing which resembles the notion of certified deletion first proposed by Broadbent and Islam~\cite{Broadbent_2020}. Here, the adversary is not computationally unbounded once a valid deletion certificate is produced; instead, the challenger simply reveals some additional secret information (in the case of the strong Gaussian-collapsing experiment, the challenger reveals a short trapdoor vector for the Ajtai hash function\footnote{In the strong Gaussian-collapsing experiment it is crucial that the trapdoor is only revealed after a valid certificate is presented; otherwise, the adversary can easily distinguish the collapsed from the non-collapsed world by applying the Fourier transform and using the trapdoor to distinguish $\LWE$ samples from uniformly random vectors~\cite{Poremba22}.}). 

Following results from \cite{Poremba22}, we obtain the following cryptosystems with $\PVD$, for the first time from standard cryptographic assumptions.

\begin{theorem}(Informal)
     Assuming the hardness of $\LWE$ and $\SIS$ with appropriate parameters, there exists public-key encryption and (leveled) fully-homomorphic encryption with $\PVD$.
\end{theorem}

Next, we ask whether one necessarily needs to rely on concrete, highly structured assumptions such as LWE in order to achieve publicly-verifiable deletion, or whether weaker generic assumptions suffice. We present a more general approach to building primitives with $\PVD$ from weaker, generic assumptions.
%We obtain the following results.

\paragraph{Commitments with $\PVD$ from Regular One-Way Functions.} 
We first formulate the notion of a \emph{balanced binary-measurement} TCR hash, which is any function that is TCR with respect to some appropriately balanced binary-outcome measurement. By balanced, we mean that the set of preimages of a random image will have significant weight on preimages that correspond to both measurement outcomes (this will roughly be required to guarantee the binding property of our commitment/correctness properties of our encryption schemes). By roughly following the template described above, we show that such hashes generically imply commitments with $\PVD$. Next, we show that such ``balanced'' functions can be based on (almost-)regular one-way functions\footnote{This is a generalization of regular one-way functions where preimage sets for different images should be polynomially related in size.} By carefully instantiating this outline, we obtain the following results.


\begin{theorem}(Informal).
    Assuming the existence of almost-regular one-way functions, there exists a balanced binary-outcome TCR hash, and consequently there exist commitments with $\PVD$.
\end{theorem}

\iffalse{
\begin{corollary} (Informal).
    Assuming the existence of almost-regular one-way functions, there exist commitments with $\PVD$.
\end{corollary}
}\fi

\paragraph{Public-Key Encryption with $\PVD$ from Regular Trapdoor Functions.}
Next, we take this framework to the public-key setting, showing that any balanced binary-outcome TCR hash with an additional ``trapdoor'' property %of ``trapdoor phase-recoverability'' 
generically implies a public-key encryption scheme with $\PVD$. The additional property roughly requires the existence of a trapdoor for $f$ that enables recovering the phase term from the quantum commitments discussed above: we call this {\em trapdoor phase-recoverability}. We show that balanced binary-outcome TCR, with trapdoor phase-recoverability, can be based on injective trapdoor one-way functions or pseudorandom group actions (the latter builds on~\cite{HMY}). 
%where the latter 
%Using this, we obtain the following results.
%, where the construction from pseudorandom group actions 
%follows immediately from a recent work of~\cite{HMY}.

\begin{theorem} (Informal).
    Assuming the existence of injective trapdoor one-way functions or pseudorandom group actions, there exists a balanced binary-outcome TCR hash with trapdoor phase-recoverability, and consequently there exists public-key encryption with $\PVD$.
\end{theorem}

We also show that injectivity requirement on the trapdoor function can be further relaxed to a notion of ``superposition-invertible'' trapdoor regular one-way function for the results above. Informally, this is a regular one-way function, where a trapdoor allows one to obtain a uniform superposition over all preimages of a given image. This is an example of a {\em generic assumption} that is not known to, and perhaps is unlikely to, imply classical public-key encryption -- but does imply PKE with quantum ciphertexts, and in fact even one that supports $\PVD$. The only other assumption in this category is the concrete assumption that pseudorandom group actions exist~\cite{HMY}.

\begin{theorem} (Informal).
    Assuming the existence of superposition-invertiable regular trapdoor functions, there exists a balanced binary-outcome TCR hash with trapdoor phase-recoverability and consequently, there exists public-key encryption with $\PVD$.
\end{theorem}

\paragraph{Advanced Encryption with $\PVD$ from Weak Assumptions}
Finally, we show that hybrid encryption gives rise to a generic compiler for encryption with $\PVD$, obtaining the following results. 

%approach for building encryption schemes preserves the publicly-verifiable deletion property. 


%Namely, given an arbitrary encryption scheme $X$ and any commitment with $\PVD$, 
%we can obtain an encryption under $X$ of bit $b$ in two parts:
%\begin{itemize}
    %\item First, generate $\mathsf{Com}(b)$ on commit and decommit registers $\mathsf{C}, \mathsf{R}$.
    %\item Output the contents of $\mathsf{C}$, along with an encryption under $X$ of the contents of $\mathsf{R}$.
%\end{itemize}


\begin{theorem}(Informal).
    Assuming the existence of injective trapdoor one-way functions or pseudorandom group actions, and $X \in \{$attribute-based encryption, quantum fully-homomorphic encryption, witness encryption, timed-release encryption$\}$,
    there exists $X$ with $\PVD$.
\end{theorem}

Prior to this work, while there existed encryption schemes with $\PVD$ from non-standard assumptions such as one-shot signatures~\cite{hiroka2021quantum}, conjectured strong collapsing~\cite{Poremba22} or post-quantum indistinguishability obfuscation~\cite{BGGKMRR}, no basic or advanced cryptosystems supporting $\PVD$ were known from standard assumptions.
%none of the basic or advanced primitives above with $\PVD$ were known from standard assumptions. 
We provide a more detailed overview of prior work below.


%A variant of our template, but with complex Gaussian-weighted phases in~\cite{Poremba22} was proven secure assuming a conjecture called {\em strong Gaussian-collapsing}.


%We prove this conjecture \dakshita{assuming the (quantum) hardness of LWE/SIS.}

%This is independently useful because the dual is a superposition over LWE ciphertexts, as pointed out in~\cite{Poremba22}. This results in a proof that the dual-Regev encryption (and corresponding fully-homomorphic encryption) scheme satisfies PVD. \dakshita{Question for Alex: did your previous paper already have a proof that dual-Regev satisfies PVD assuming the conjecture holds? I am guessing not, but just want to double check.}

%\begin{corollary}
    %Enc and FHE with PV-CD from the hardness of SIS.
%\end{corollary}


%Finally, our encryption scheme resembles a proposal of~\cite{HMY23} based on pseudorandom group actions. At a high level, describe construction. We show \dakshita{can we?} that this is an instance of a trapdoor target-collapsing function, and therefore also satisfies publicly-verifiable deletion.

%\begin{corollary}
    %Enc with PV-CD from pseudorandom group actions.
%\end{corollary}

%\paragraph{Publicly-Verifiable Deletion Beyond Public-Key Encryption}
%\dakshita{change below to use trapdoors, o/w does not give PV-CD}


%Going back to our template, at an intuitive level we (at least) require that it be hard to compute $x_1$ given $x_0$ (or vice-versa). Reminiscent of SPR - we show it indeed suffices. More generally, two sets - target-collapsing.
%Proving is difficult (see overview) - similar construction in Por relies on unproven conjecture. 





%In recent work,~\cite{Poremba22} observed that hash functions satisfying a stronger property called {\em strong Gaussian-collapsing} yield encryption schemes supporting publicly-verifiable deletion.
%The scheme itself follows a similar template to the commitment above, except that $f$ is the Ajtai hash function, and the bit $b$ is encoded into the phase of a {\em Gaussian-weighted} superposition over pre-images of $y$.
%The {\em strong Gaussian-collapsing} conjecture essentially conjectures that the above scheme satisfies publicly-verifiabstle deletion, namely, any adversary that is  


\iffalse{
Classical devices store and exchange data encoded into a string of bits. 
Once a server gains access to this string, 
it may store arbitrarily many copies of it in hidden locations and it is impossible to check if the string was ever {\em truly} deleted.
On the other hand, unknown quantum states cannot be copied, and measurements on states are inherently destructive and irreversible. 
This opens the door~\cite{EC:Unruh14,TCC:BroIsl20,AC:HMNY21,C:HMNY22,Poremba22,BK22} to the possibility of verifying that untrusted devices and servers delete private records forever -- even after they have already had access to, or computed on, suitable quantum encodings of these records. 
}\fi


\iffalse{
\james{motivations for public-verifiability: (a) improves security because we don't have to worry about a leaked verification key, (b) it makes the basic application of storage with deletion reasonable since now the client doesn't have to store a secret key that grows with the size of their original data - they can also outsource the public verification keys }

\dakshita{maybe another way to sell this (building on something Alex mentioned back at Simons) is to point out how quantum money is unlikely to follow from collapsing hashes, but here we show that ``one-sided'' verification does follow from collapsing hash functions.}

\dakshita{two templates for certified deletion so far: BB84 based~\cite{....} and subspace-coset based~\cite{....}. }\james{I think these should be considered instances of the same template}\dakshita{We prove security for a different template.

public verification of the outcome of measurement, should be something that would have been hard to compute without measurement. eg, Poremba observed that the deletion certificate could be an (I)SIS image and the certificate a solution, that is typically hard to find.. so encode bit as... conjectured that this is secure.
conjectured that ``collapsing'' implies certified everlasting security in a deletion setting.
observe that target-collapsing suffices.

basing on OWF -- 
now hardness of inverting OWF implies that one could have a pre-image of a public image be the certificate. then encode the bit in the phase between multiple certificates. no need for fourier measurements..

problem - how to prepare pre-image state? use 2-to-1 hash. turns out need balanced.}

}\fi



%\subsection{Directions for Future Work}



\subsection{Prior work}

The first notion resembling \emph{certified deletion} was introduced by
Unruh \cite{Unruh2013} who proposed a (private-key) quantum timed-release
encryption scheme that is \emph{revocable}, i.e. it allows a user to \emph{return} the ciphertext of a quantum timed-release encryption scheme, thereby losing all access to the data. Unruh's scheme uses conjugate coding~\cite{Wiesner83,BB84} and relies on the \emph{monogamy of entanglement} in order to guarantee that revocation necessarily erases information about the plaintext. 
Broadbent and Islam~\cite{Broadbent_2020} introduced the notion of \emph{certified deletion} and constructed a private-key quantum encryption scheme with the aforementioned feature which is inspired by the quantum key distribution protocol~\cite{BB84,Tomamichel2017largelyself}. 
In contrast with Unruh's~\cite{Unruh2013} notion of revocable quantum ciphertexts which are eventually returned and verified, Broadbent and Islam~\cite{Broadbent_2020} consider certificates which are entirely classical. %\dakshita{I thought certificates could be classical in Unruh's scheme also? sorry if i'm mistaken}\alex{As far as I know, Unruh's reovcation uses a projective measurement that checks if the returned state is maximally entangled with another target system . Let me check it again}. 
Moreover, the security definition requires that, once the certificate is successfully verified, the plaintext remains hidden even if the secret key is later revealed.
Inspired by the notion of quantum copy-protection~\cite{Aar09},
Ananth and La Placa~\cite{ananth2020secure} defined a form of quantum software protection called \emph{secure software leasing} whose anti-piracy notion requires that the encoded program is returned and verified.



Using a hybrid encryption scheme, Hiroka, Morimae, Nishimaki and Yamakawa~\cite{hiroka2021quantum} extended the scheme in~\cite{Broadbent_2020} to both public-key and attribute-based encryption with privately-verifiable certified deletion via \emph{receiver non-committing} encryption~\cite{10.5555/1756169.1756191,10.1145/237814.238015}. 
Hiroka, Morimae, Nishimaki and Yamakawa~\cite{hiroka2021certified} considered \emph{certified everlasting zero-knowledge proofs} for $\mathsf{QMA}$ via the notion of \emph{everlasting security} which was first formalized by M\"{u}ller-Quade and Unruh~\cite{10.1007/978-3-540-70936-7_3}. Bartusek and Khurana~\cite{cryptoeprint:2022/1178} revisited the notion of certified deletion and presented a unified approach for how to generically convert any public-key, attribute-based, fully-homomorphic, timed-release or witness encryption scheme into an equivalent quantum encryption scheme with certified deletion. In particular, they considered a stronger notion called \emph{certified everlasting security} which allows the adversary to be computationally unbounded once a valid deletion certificate is submitted. This is also the definition we consider in this work.
%\alex{Would you like to mention any other contributions from your paper which might be relevant in this work?} 
In the same spirit, Hiroka, Morimae, Nishimaki and Yamakawa~\cite{cryptoeprint:2022/969} gave a \emph{certified everlasting} functional encryption scheme which allows the receiver of the ciphertext to obtain the outcome specific function applied the plaintext, but nothing else.
In other very recent work, Ananth, Poremba and Vaikuntanathan~\cite{cryptoeprint:2023/325} used Gaussian superpositions to construct (key)-revocable cryptosystems, such as public-key encryption, fully homomorphic encryption and pseudorandom functions assuming the hardness of $\LWE$, and Agarwal et al. \cite{AKNYY} introduced a generic compiler for adding key-revocability to a variety of cryptosystems. In these systems, the cryptographic key consists of a quantum state which can later be \emph{certifiably revoked} via a quantum channel -- in contrast with the classical deletion certificates for ciphertexts considered in this work.


\paragraph{Cryptosystems with Publicly Verifiable Deletion.}
First, in addition to their results in the setting of private verification,~\cite{hiroka2021quantum} also gave a public-key encryption scheme with certified deletion which is \emph{publicly verifiable} assuming the
existence of one-shot signatures (which rely on strong black-box notions of obfucation) and extractable witness encryption. Using 
\emph{Gaussian superpositions},
Poremba~\cite{Poremba22} proposed \emph{Dual-Regev}-based public-key and fully homomorphic encryption schemes with certified deletion which are publicly verifiable and proven secure assuming the (then unproven) \emph{strong Gaussian-collapsing conjecture} --- a strengthening of the collapsing property of the Ajtai hash.
Finally, a recent work~\cite{BGGKMRR} relies on post-quantum indistinguishability obfuscation (iO) to obtain both publicly verifiable deletion and publicly verifiable key revocation. This is a strong assumption for which we have candidates, but no constructions based on standard (post-quantum) assumptions at this time. 
%On the other hand, our work relies
%than the ones we use in this work, and at this time, we only have candidate post-quantum iO schemes under plausible assumptions on leaky variants of LWE.


\section{Technical Overview}

In this overview, we begin by discussing the key ideas involved in proving our main theorem. We show how to prove publicly verifiable deletion for a toy protocol that relies on stronger assumptions than the ones that we actually rely on in our actual technical sections. 

Next, we progressively relax these assumptions to instantiate broader frameworks, including the one from~\cite{Poremba22}, obtaining public-key encryption and fully-homomorphic encryption with $\PVD$ from LWE/SIS.

Finally, we further generalize this to enable constructions from weak cryptographic assumptions -- including commitments with $\PVD$ from variants of one-way functions and PKE with $\PVD$ from trapdoored variants of the same assumption. We also discuss a hybrid approach that enables a variety of advanced encryption schemes supporting $\PVD$.

\subsection{Proving Our Main Theorem}
\label{sec:overreq}
Consider the toy commitment
\[\mathsf{Com}(b) = \left( y, \ket{0, x_0} + (-1)^b \ket{1, x_1} \right) \]
where $(0, x_0), (1, x_1) $ are preimages of $y$ under a structured two-to-one function $f$, where every image has a preimage that begins with a $0$ and another that begins with a $1$.  
We note that this commitment can be efficiently prepared by first preparing a superposition over all preimages 
\[\sum_{b \in \{0,1\}, x \in \{0,1\}^\secp} \ket{b,x}\] 
on a register $\mathsf{X}$, then writing the output of $f$ applied on $X$ to register $\mathsf{Y}$, and finally measuring the contents of register $\mathsf{Y}$ to obtain image $y$. The register $\mathsf{X}$ contains $\ket{0, x_0} + \ket{1, x_1}$, which can be converted to $\ket{0, x_0} + (-1)^b \ket{1, x_1}$ via (standard) phase kickback.

%is target collapsing and target collision-resistance w.r.t. a computational basis measurement of the pre-image register.

%We will show that this commitment satisfies publicly-verifiable deletion as long as $f$ is target collapsing and target collision-resistant w.r.t. a computational basis measurement of the pre-image register. 
To show  that the commitment satisfies publicly-verifiable deletion, 
%we must demonstrate that the advantage of 
we consider an adversary $\cA = (\cA_1, \cA_2)$ where $\cA_1$ is (quantum) polynomial time and $\cA_2$ is unbounded, participating in the following experiment.
%is negligible in following experiment.
%$c$ with probability negligibly better than $\frac{1}{2}$ in the following experiment:

\begin{itemize}
\item The challenger samples $b \leftarrow \{0,1\}$ and runs $\mathsf{Expmt}_0(b)$, described below.\\
\noindent{\underline{$\mathsf{Exmpt}_0(b):$}}
\begin{enumerate}
    \item Prepare $\left( \ket{0, x_0} + (-1)^b \ket{1, x_1}, y \right)$ on registers $\mathsf{A}, \mathsf{B}$ and send them to $\cA_1$.
    \item $\cA_1$ outputs a (classical) deletion certificate $\gamma$,\footnote{If the $\cA_1$ outputs a quantum state as their certificate, the state is measured in the computational basis to obtain a classical certificate $\gamma$.} and left-over state $\rho$.
    \item If $f(\gamma) \neq y$, output a uniformly random bit $b' \gets \{0,1\}$, otherwise output $b' = \cA_2(\rho)$.
\end{enumerate}
\item The advantage of $\cA$ is defined to be $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_0} = \big| \Pr[b' = b]  - \frac{1}{2} \big|$.
\end{itemize}

We discuss how to prove the following.
\begin{claim} (Informal).
\label{clm:overclm}
For every $\cA = (\cA_1, \cA_2)$ where $\cA_1$ is (quantum) computationally bounded, \[\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_0} = \negl(\secp), \]
as long as $f$ is target collapsing and target collision-resistant w.r.t. a computational basis measurement of the pre-image register.
\end{claim}


\iffalse{
We split the proof of this claim into two parts: (1)  show that the function $f$ is also {\em certified everlasting} target-collapsing, and (2) show that $f$ being {\em certified everlasting} target-collapsing implies that $\mathsf{Com}$ satisfies publicly verifiable deletion.

\paragraph{Part 1: $f$ is Certified Everlasting Target-Collapsing.}
}\fi

\paragraph{Overview of the Proof of Claim \ref{clm:overclm}.}
To prove this claim, we must  show that $b$ %indicating whether or not the superposition of preimages was measured, 
is information-theoretically {\em removed} from the leftover state of any $\cA_1$ that generates a valid pre-image of $y$, despite the fact that the adversary's view contains $b$ at the beginning of the experiment.

Proof techniques for this type of experiment were recently introduced in~\cite{cryptoeprint:2022/1178} in the context of {\em privately verifiable deletion} via BB84 states. Inspired by their method, our first step is to defer the dependence of the experiment on the bit $b$. 
In more detail, we will instead imagine sampling
the distribution by guessing a uniformly random $c \leftarrow \{0,1\}$, and initializing the adversary with $\left( \ket{x_0} + (-1)^c \ket{x_1}, y \right)$.
%when $c = 0$, or a uniform mixture of $\{\ket{x_0}, \ket{x_1}\}$ when $c = 1$. 
The challenger later obtains input $b$ and aborts the experiment (outputs $\bot$) if $c \neq b$.
Since $c$ was a uniformly random guess, the trace distance between the $b = 0$ and $b = 1$ outputs of this modified experiment is at least half the trace distance between the outputs of the original experiment.
%Now, the bit $b$ is only used by the experiment to determine whether or not to output $\bot$. 
Moreover, we can further delay the process of obtaining input $b$, and then abort or not until {\em after} the adversary outputs a certificate of deletion.
That is, we can consider a {\em purification} where a register $\mathsf{C}$ contains a superposition $\ket{0} + \ket{1}$ of two choices for $c$, and is later measured to determine bit $c$. This experiment is discussed in detail below.\\

\noindent 
\underline{$\mathsf{Expmt}_1(b):$}
The experiment proceeds as follows.
\begin{enumerate}
\item Prepare the $\ket{+}$ state on an ancilla register $\mathsf{C}$, and a superposition of preimages $\ket{x_0} + \ket{x_1}$ of a random $y$ on register $\mathsf{A}$.
%\dakshita{say somewhere in the beginning how to prepare this.}
\item 
Then, controlled on the contents of register $\mathsf{C}$, do the following:
if the control bit is $0$, do nothing, and otherwise flip the phase on $x_1$ (via phase kickback), changing the contents of $\mathsf{A}$ to $\ket{x_0} - \ket{x_1}$. 
This means that the overall state is
\[
\frac{1}{\sqrt{2}} \sum_{c \in \{0,1\}} \ket{c}_{\mathsf{C}} \otimes \ket{0, x_0}_{\mathsf{A}} + (-1)^c \ket{1, x_1}_{\mathsf{A}}
\]
Send $\mathsf{A}$ to $\cA_1$.  
\item Obtain from $\cA_1$ a purported certificate of deletion $\gamma$.
\item If $f(\gamma) \neq y$, abort, and otherwise measure register $\mathsf{C}$ to obtain output $c$, and abort if $c \neq b$. In the case of abort, output a uniformly random bit $b' \gets \{0,1\}$.
\item If no aborts occurred, output $b' = \cA_2(\rho)$.
\end{enumerate}
We note that the event $c = b$ occurs with probability exactly $\frac{1}{2}$, and since measurements on separate subsystems commute, we have that 
\begin{equation}
\label{eq:over1}
\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_1} \geq \frac{1}{2} \mathsf{Adv}_{\cA}^{\mathsf{Expmt}_0}.
\end{equation}
where $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_1} = \big| \Pr[\mathsf{Expmt}_1(b) = b]  - \frac{1}{2} \big|$ for $b \leftarrow \{0,1\}$.
%\dakshita{clean or define expmt 0, and the advantage notatiob.}


Once the dependence of the experiment on $b$ has been deferred, as above, we can consider another experiment (described below) where the challenger measures the contents of register $\mathsf{A}$ {\em before} sending it to $\cA_1$. Intuitively, performing this measurement {\em removes} information about $b$ from $\cA_1$'s view in a manner that is computationally undetectable by $\cA_1$ (due to the target-collapsing property of $f$). \\

\noindent \underline{$\mathsf{Expmt}_2(b):$}
The experiment proceeds as follows.
\begin{itemize}
\item Prepare the $\ket{+}$ state on an ancilla register $\mathsf{C}$, and a superposition of preimages $\ket{x_0} + \ket{x_1}$ of a random $y$ on register $\mathsf{A}$.
\emph{Next, measure register $\mathsf{A}$ in the computational basis.}

Then, controlled on the contents of register $\mathsf{C}$, do the following:
if the control bit is $0$, do nothing, and otherwise flip the phase on $x_1$. 
This means that the overall state is a uniform mixture of the states
\[\frac{1}{\sqrt{2}} \sum_{c \in \{0,1\}} \ket{c}_{\mathsf{C}} \otimes \ket{0, x_0}_{\mathsf{A}} \text{ and }
\frac{1}{\sqrt{2}} \sum_{c \in \{0,1\}} (-1)^c\ket{c}_{\mathsf{C}} \otimes \ket{1, x_1}_{\mathsf{A}}
\]
Finally, send $\mathsf{A}$ to $\cA_1$. 
%controlled on the contents of register $\mathsf{C}$, do the following:
%if the control is $0$, send $\mathsf{A}$ to the adversary, and otherwise flip the phase on the contents of $\mathsf{A}$ to obtain $\ket{x_0} - \ket{x_1}$ before sending it to $\cA_1$. 
\item Obtain from $\cA_1$ a purported certificate of deletion $\gamma$.
\item If $f(\gamma) \neq y$, abort, otherwise measure register $\mathsf{C}$ to obtain output $c$, and abort if $c \neq b$. In the case of abort, output a uniformly random bit $b' \gets \{0,1\}$.
\item If no aborts occurred, output $b' = \cA_2(\rho)$.
\end{itemize}

%\paragraph{A Failed Argument.} 
As described above, the target-collapsing property of $f$ implies that $\cA_1$ cannot (computationally) distinguish the register $\mathsf{A}$ obtained in $\mathsf{Expmt}_2(b)$ from the one obtained in $\mathsf{Expmt}_1(b)$. However, this is not immediately helpful:  information about which experiment $\cA_1$ participated in could potentially be encoded into $\cA_1$'s left-over state $\rho$, so that it remains computationally hidden from $\cA_1$ but can be extracted by (unbounded) $\cA_2$. And it is after all the output of $\cA_2$ that determines the advantage of $\cA$. 
Because of $\cA_2$ being unbounded and the experiments only being {\em computationally} indistinguishable, even if we could show that $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_2} = \negl(\secp)$, it is unclear how to use this to show our desired claim, i.e., $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_0} = \negl(\secp)$.
It may appear that the proof is stuck.

To overcome this issue, we will aim to identify an {\em efficiently computable predicate} of the challenger's system, which will {\em imply} the following (inefficient) property: when $\cA_1$ outputs a valid deletion certificate, even an unbounded $\cA_2$ cannot determine whether it participated in $\mathsf{Expmt}_1(b)$ or $\mathsf{Expmt}_2(b)$, i.e.,  $\cA_1$'s left-over state is information-theoretically independent of $b$.

\paragraph{Identifying an Efficiently Computable Predicate.} 
Observe that in $\mathsf{Expmt}_2(b)$, the ancilla register $\mathsf{C}$ is {\em unentangled} with the rest of the experiment. 
In fact, the ancilla register is exactly $\ket{+}$ when we give the adversary $\ket{0,x_0}$ on register $\mathsf{A}$, and $\ket{-}$ when we give the adversary $\ket{1,x_1}$ on register $\mathsf{A}$.
Moreover, in $\mathsf{Expmt}_2(b)$, the \underline{target-collision-resistance} of $f$ implies that the computationally-bounded $\cA_1$ given $x_0$ cannot output $x_1$ as their deletion certificate (and vice-versa).


This, along with the fact that the certificate {\em must} be a pre-image of $y$ means that the following guarantee holds   (except with negligible probability) in $\mathsf{Expmt}_2(b)$:
\begin{center}{\em When the adversary outputs a valid certificate $\gamma$, a projection of the pre-image register onto $\ket{+}$ succeeds if $\gamma = (0, x_0)$ and a projection of the pre-image register onto $\ket{-}$ succeeds if $\gamma = (1, x_1)$.}\end{center}

%In other words, given a valid deletion certificate beginning with bit $\beta$, projects register $\mathsf{C}$ onto $\ket{0} + (-1)^b \ket{1}$.
%That is, we can define an efficient project $\Pi_b$  is indeed an efficient projection that succeeds except with negligible probability in $\mathsf{Expmt}_2^b$, when the adversary generates a valid deletion certificate.

At this point, we can rely on the \underline{target-collapsing} property of $f$ to prove the following claim: the {\em efficient projection} described above also succeeds except with negligible probability in $\mathsf{Expmt}_1(b)$, when the adversary generates a valid deletion certificate. If this claim is not true, then since the experiments (including $\mathsf{\cA}_1$) run in quantum polynomial time until the point that the deletion certificate is generated, and the projection is efficient, one can build a reduction that contradicts target-collapsing of $f$. This reduction obtains a challenge (which is either a superposition when the challenger did not measure, or a mixture if the challenger did measure) on register $\mathsf{A}$, prepares ancilla $\mathsf{C}$ as in $\mathsf{Expmt}_1(b)$, then follows steps 2, 3 identically to $\mathsf{Expmt}_1(b)$. 
Next, given a deletion certificate $(\beta,x_\beta)$, the reduction projects $\mathsf{C}$ onto $\ket{0} + (-1)^\beta \ket{1}$, outputting $1$ if the projection succeeds and $0$ otherwise. 

%Having established that when the adversary outputs a valid deletion certificate $(\beta||x_\beta)$ in $\mathsf{Expmt}_1(b)$, then the projection $\Pi_\beta$ of $\mathsf{C}$ onto $\ket{0} + (-1)^\beta \ket{1}$ almost always succeeds, 
\paragraph{Introducing an Alternative Experiment.}
Having established that the projection above must succeed in $\mathsf{Expmt}_1(b)$ except with negligible probability,
we can now consider an alternative experiment $\mathsf{Expmt}_{\mathsf{alt}}(b)$. This is identical to $\mathsf{Expmt}_1(b)$, except that the challenger {\em additionally} projects register $\mathsf{C}$ onto $\ket{0} + (-1)^{\beta} \ket{1}$ when the adversary generates a valid certificate $(\beta,x_\beta)$. We established above that the projection is successful in $\mathsf{Expmt}_1(b)$ except with negligible probability, and this implies that 
\begin{equation} 
\label{eq:over2}
\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_{\mathsf{alt}}} \geq \mathsf{Adv}_{\cA}^{\mathsf{Expmt}_{1}} - \negl(\secp)
\end{equation}
where as before, $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_{\mathsf{alt}}} = \big| \Pr[\mathsf{Expmt}_{\mathsf{alt}}(b) = b]  - \frac{1}{2} \big|$ for $b \leftarrow \{0,1\}$.


Crucially, in $\mathsf{Expmt}_{\mathsf{alt}}(b)$, the bit $c$ is determined by a measurement on register $\mathsf{C}$ which is {\em unentangled} with the system and in either the $\ket{+}$ or $\ket{-}$ state (due to the projective measurement that we just applied). Thus, measuring $\mathsf{C}$ in the computational basis results in a uniformly random and independent $c$. By definition of the experiment (abort when $b \neq c$, continue otherwise) -- this implies that the bit $b$ is set in a way that is uniformly random and independent of the adversary's view, and thus  
\begin{equation}
\label{eq:over3}
\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_{\mathsf{alt}}} = 0
\end{equation}
Now, equations~(\ref{eq:over1}, \ref{eq:over2}, \ref{eq:over3}) together yield the desired claim, that is, $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_{0}} = \negl(\secp)$.

This completes a simplified overview of our key ideas, assuming the existence of a perfectly $2$-to-$1$ function $f$ where every image $y$ has preimages $\left( (0, x_0), (1, x_1) \right)$, and where $f$ satisfies both target-collapsing and target-collision-resistance.
Unfortunately, we do not know how to build functions satisfying these clean properties from simple generic assumptions. Instead, we will generalize the template above, where the first generalization will no longer require $f$ be $2$-to-$1$.

\paragraph{Generalizing the Template.}
First, note that we can replace $\ket{0, x_0}$ and $\ket{1, x_1}$ with superpositions over two disjoint sets of preimages of $y$ separated via an efficient binary-outcome measurement, namely
\[
\mathsf{Com}(b) = 
\sum_{x: f(x) = y, M(x) = 0} \ket{x} + (-1)^b \sum_{x: f(x) = y, M(x) = 1} \ket{x}
\]
We can even consider measurements $M$ that have arbitrarily many outcomes.
Proof ideas described above also generalize almost immediately to show that for any $M$, $\mathsf{Com}$ satisfies $\PVD$ as long as $f$ is target-collapsing and target-collision resistant w.r.t. $M$.
In fact, we can generalize this even further (see our main results in Section~\ref{sec:maintheorem},~\ref{sec:mainaux}) to consider arbitrary (as opposed to uniform) distributions over pre-images, as well as to account for any auxiliary information that may be sampled together with the description of the hash function.


\paragraph{Certified Everlasting Target-Collapsing.}
As discussed in the results section, our actual technical proofs proceed in two parts. (1) Show that for any $M$, a function $f$ that is target-collapsing and target-collision resistant w.r.t. $M$ is also {\em certified everlasting} target-collapsing w.r.t. $M$, and (2) show that $f$ being {\em certified everlasting} target-collapsing implies that $\mathsf{Com}$ satisfies publicly verifiable deletion.

Recall that {\em certified} everlasting target collapsing requires that an adversary that outputs a valid deletion certificate information-theoretically loses the bit $b$ determining whether they received a superposition or a mixture of preimages. Our proof of certified everlasting target-collapsing follows analogously to the proof sketched above. In short, we defer measurement of a bit $b$ which decides whether the adversary is given a superposition or a mixture, and then rely on target-collapsing and target-collision-resistance to argue that an efficient projection on the challenger's state (almost) always succeeds when the adversary outputs a valid certificate. We finally show that success of this projection implies that the adversary's state is information-theoretically independent of $b$.

The certified everlasting target-collapsing property almost immediately implies certified deletion security of $\mathsf{Com}$ via a hybrid argument: 
\begin{itemize}
\item In $\mathsf{Hyb}_0$, the adversary obtains register $\mathsf{A}$ containing
\[
\mathsf{Com}(0) = 
\sum_{x: f(x) = y, M(x) = 0} \ket{x} + \sum_{x: f(x) = y, M(x) = 1} \ket{x}
\]
\item In $\mathsf{Hyb}_1$, the measurement $M$ is applied to $\mathsf{A}$ before sending it to the adversary.
\item In $\mathsf{Hyb}_2$, the adversary obtains register $\mathsf{A}$ containing
\[
\mathsf{Com}(1) = 
\sum_{x: f(x) = y, M(x) = 0} \ket{x} - \sum_{x: f(x) = y, M(x) = 1} \ket{x}
\]
\end{itemize}
The certified everlasting hiding property of $f$ guarantees that all hybrids are statistically close when the adversary outputs a valid deletion certificate. Moreover, these experiments abort and output a random bit when the adversary does not output a valid certificate, and it is easy to show (by computational indistinguishability) that the probability of generating a valid certificate remains negligibly close between experiments.


\paragraph{TCR Implies Target-Collapsing for Polynomial-Outcome Measurements}
We also show that when $M$ has polynomially many possible outcomes, then TCR implies target-collapsing w.r.t. $M$. This follows from techniques that were recently developed in the literature on collapsing versus collision resistant hash functions~\cite{cryptoeprint:2022/786,crypto-2022-32202,crypto-2022-32124}. In a nutshell, these works showed that any distinguisher that distinguishes mixtures from superpositions over preimages for an {\em adversarially chosen} image $y$, can be used to swap between pre-images, and therefore find a collision for $y$. We observe that their technique is agnostic to whether the image $y$ is chosen randomly (in the targeted setting) or adversarially. Furthermore, it also extends to swapping superpositions over sets of pre-images to superpositions over other sets. These allow us to prove (Section~\ref{sec:tcr-implies}) that TCR w.r.t. any polynomial-outcome measurement $M$ implies target-collapsing w.r.t. $M$.


%some techniques from the literature on collapsing versus collision resistant hash functions also apply to the targeted setting. Namely, we show that techniques from~\cite{} extend to the targete
%is that a randomly sampled image $y$ (sampled by first picking uniform $x$ and then computing $y = f(x)$).
%, and otherwise the pre-image register 

%whenever $\cA_1$ output a valid deletion certificate (i.e., $\gamma$ such that $f(\gamma) = y$)

%We note that the event $c = b$ occurs with probability exactly $\frac{1}{2}$, and since measurements on independent subsystems commute, we have that $\mathsf{Adv}_{\cA}^{\mathsf{Expmt}_1} \geq \frac{1}{2} \mathsf{Adv}_{\cA}^{\mathsf{Expmt}_0}$.

%use the contents of this register instead of the bit $b'$ above. 




%We will assume that $f$ is target-collapsing and generalized target-collision resistant w.r.t. a computational basis measurement of the pre-image register. Note that the latter condition simply means that it is computationally hard to find $x_0$ given $x_{1}$ (or vice-versa).

\iffalse{

Our proof of $\PVD$ can be split into two technical components:
\begin{itemize}
    \item First, we prove that $f$ satisfies a form of {\em certified-everlasting} target-collapsing, as long as $f$ is target-collapsing and generalized target-collision resistant w.r.t. a computational basis measurement of the pre-image register. Note that for the toy example above, this latter condition simply means that it is computationally hard to find $x_0$ given $x_1$ (or vice-versa) for a randomly sampled $y$.

    This means \dakshita{that..}

    \item Second, we show that ...
    
    \end{itemize}
}\fi



\subsection{Publicly-Verifiable Deletion via Gaussian Superpositions}
In \Cref{sec:Dual-Regev}, we revisit the \emph{Dual-Regev} public-key and (leveled) fully homomorphic encryption schemes with publicly-verifiable deletion which were proposed by Poremba~\cite{Poremba22}
and were conjectured to be secure under the \emph{strong Gaussian-collapsing property}. 
By applying our main 
%result from \Cref{thm:CETC-generalization} 
theorem to the Ajtai hash function, we obtain a proof of the conjecture, which allows us to show the certified everlasting security of the aforementioned schemes assuming the hardness of the $\LWE$ assumption.

The constructions introduced in~\cite{Poremba22} exploit the the duality between $\LWE$ and $\SIS$~\cite{cryptoeprint:2009/285}, and rely on the fact that one encode Dual-Regev ciphertexts via Gaussian superpositions. Below, we give a high-level sketch of the basic public-key construction.

\begin{itemize}
\item To generate a pair of keys $(\sk,\pk)$, sample a random $\vec A \in \Z_q^{n \times (m+1)}$ together with a particular short trapdoor vector $\vec t \in \Z^{m+1}$ such that $\vec A \cdot \vec t = \vec 0 \Mod{q}$. Let $\pk = \vec A$ and $\sk = \vec t$.

\item To encrypt $b \in \bit$ using $\pk=\vec A$, generate the following for a random $\vec y \in \Z_q^n$:
$$
\vk \leftarrow (\vec A,\vec y), \quad\quad
\ket{\ct} \leftarrow \sum_{\vec s \in \Z_q^n} \sum_{\vec e \in \Z_q^{m+1}} \rho_{q/\sigma}(\vec e) \, \omega_q^{-\langle\vec s,\vec y \rangle} \ket{\vec s^\intercal \vec A + \vec e^\intercal +b \cdot (0,\dots,0, \lfloor\frac{q}{2} \rfloor)},
$$
where $\vk$ is a public verification key and $\ket{\ct}$ is the quantum ciphertext for $\sigma >0$.

\item To decrypt $\ket{\ct}$ using $\sk$, measure in the computational basis to obtain $\vec c \in \Z_q^{m+1}$, and output $0$, if $\vec c^\intercal \cdot \sk\in \Z_q$
is closer to $0$ than to $\lfloor\frac{q}{2}\rfloor$,
and output $1$, otherwise. Here $\sk = \vec t$ is chosen such that $\vec c^\intercal \cdot \sk$ yields an approximation of $b \cdot \lfloor \frac{q}{2} \rfloor$ from which we can recover $b$.
\end{itemize}
To delete the ciphertext $\ket{\ct}$, perform a measurement in the Fourier basis. Poremba~\cite{Poremba22} showed that the Fourier transform of $\ket{\ct}$ results in the \emph{dual} quantum state given by
$$
\ket{\widehat{\ct}}=\sum_{\substack{\vec x \in \Z_q^{m+1}:\\ \vec A \vec x = \vec y \Mod{q}}}\rho_{\sigma}(\vec x) \, \omega_q^{\langle \vec x,b \cdot (0,\dots,0,  \lfloor\frac{q}{2} \rfloor)\rangle} \,\ket{\vec x}.
$$
In other words, a Fourier basis measurement of $\ket{\ct}$ will necessarily erase all information about the plaintext $b \in \bit$ and results in a \emph{short} vector $\pi \in \Z_q^{m+1}$ such that $\vec A \cdot \pi = \vec y \Mod{q}$. To publicly verify a deletion certificate, simply check whether a certificate $\pi$ is a solution to the (inhomogenous) $\SIS$ problem specified by $\vk=(\vec A,\vec y)$. Due to the hardness of the $\SIS$ problem, it is computationally difficult to produce a valid deletion certificate from $(\vec A,\vec y)$ alone.

Our approach to proving certified everlasting security of the Dual-Regev public-key and fully-homomorphic encryption schemes with publicly-verifiable deletion in~\cite{Poremba22} is as follows.
First, we observe that the Ajtai hash function is both target-collapsing and target-collision-resistant with respect to the 
 discrete Gaussian distribution. Here, the former follows from $\LWE$ as a simple consequence of the \emph{Gaussian-collapsing property} previously shown by Poremba~\cite{10.1007/978-3-030-26951-7_12,Poremba22}, whereas the latter follows immediately from the quantum hardness of $\SIS$. 
 %By invoking our main result in \Cref{thm:CETC-generalization}, this 
 Thus, our main theorem implies that the Ajtai hash function is certified-everlasting target-collapsing (see \Cref{thm:ajtai-certified-everlasting}). Finally, as a simple corollary of our theorem, we obtain a proof of the \emph{strong Gaussian-collapsing conjecture} in \cite{Poremba22}, which we state in \Cref{SGC}. We also note that the aforementioned conjecture considers a weaker notion of certified collapsing which resembles the notion of certified deletion first proposed by Broadbent and Islam~\cite{Broadbent_2020}. Here, the adversary is not computationally unbounded once a valid deletion certificate is produced; instead, the challenger simply reveals additional secret information (in the case of the strong Gaussian-collapsing experiment, this is a short trapdoor vector for the Ajtai hash function). Our notion of certified everlasting target-collapsing is significantly stronger; in particular, it implies the weaker collapsing scenario considered by Poremba~\cite{Poremba22}. This follows from the fact that the security reduction can simply brute-force search for a short trapdoor solution for the Ajtai hash once it enters the phase in which it is allowed to be computationally unbounded. We exploit this fact in the proof of \Cref{SGC}.


\subsection{Weakening Assumptions for Publicly-Verifiable Deletion}
Next, we look for instantiations of the above template from {\em generic} cryptographic assumptions, as opposed to structured specific assumptions such as LWE.
Here, all of our instantiations only require us to consider functions that are target-collision-resistant and target-collapsing w.r.t. binary-outcome measurements (and as discussed above, TCR implies certified-everlasting target-collapsing in this setting).
%Techniques discussed above can already show that such functions satisfy certified everlasting target-collapsing. %However, to build commitments and other primitives such as encryption from such functions, we also need the bit $b$ to be (approximately) recoverable, with at least some inverse polynomial probability. 
In addition, for the case of commitments, in order for the commitment to satisfy binding\footnote{We  actually prove that a purification of the template commitment described above satisfies honest-binding~\cite{Yan}.
Namely, the committer generates the state above but leaves registers containing the image $y$ (and the key, if $f$ is a keyed function) unmeasured, and holds on to these registers for the opening phase. It can later either open the commitment by sending these registers to a receiver, or request deletion, by measuring them and publishing $y$ (and any keys for the function).
}, we require that there is a measurement that can distinguish 
\[\sum_{x: f(x) = y, M(x) = 0} \ket{x} + \sum_{x: f(x) = y, M(x) = 1} \ket{x}\] from \[\sum_{x: f(x) = y, M(x) = 0} \ket{x} - \sum_{x: f(x) = y, M(x) = 1} \ket{x}\] with probability $\delta$ for any constant $0 < \delta \leq 1$.
For the case of public-key encryption, we similarly require that a trapdoor be able to recover the phase with constant probability. We then resort to standard amplification techniques to boost correctness error from constant to (negligibly close to) $0$. We note that this amplification would also work if the phase was recoverable with inverse-polynomial $\delta$ (as opposed to constant); however, we focus on constant $\delta$ because of simplicity, and because it suffices for our instantiations.

In the template above, we observe that a measurement can find the phase with inverse polynomial probability whenever the sets 
\[ \sum_{x: f(x) = y, M(x) = 0} \ket{x} \text{ and } \sum_{x: f(x) = y, M(x) = 1} \ket{x} \]
are somewhat ``balanced'', i.e. for a random image $y$, for sets $S_0 = \{x: f(x) = y, M(x) = 0\}$  and $S_1 = \{x: f(x) = y, M(x) = 1\}$, we have that $\frac{|S_0|}{|S_1|}$ is a fixed constant.
We show in \cref{sec:com} and \cref{sec:pke} that commitments and PKE with $\PVD$ can be obtained from appropriate variants of TCR functions following this template.
%\dakshita{technically also possible with less balanced sets}


Now, our goal is to build such TCR functions from generic assumptions. A natural idea would be to start with any one-way function $f$ and compose it with a random two-to-one hash $h$ defined on its range\footnote{The {\em co-domain} of a function $f:\{0,1\}^n \rightarrow \{0,1\}^m$ is $\{0,1\}^m$, and we will also refer to this as the {\em range} of the function in this paper. The {\em image} is the set of all actual output values of $f$, i.e. the set $\{y: \exists x \text{ such that } f(x) = y\}$. The co-domain/range may in general be a superset of the image of a function.}.
%\footnote{By range (or codomain), we mean a set of elements in which the output of $f$ is guaranteed to fall, which may be a strict superset of its image.}
Then, any output $y$ of the composed function $(h \circ f)$ is associated with two elements $\{z_0,z_1\} = h^{-1}(y)$ in the range of $f$, and the binary-outcome measurement would measure one of $z_0$ or $z_1$. Recalling that we eventually want to prove target-collision-resistance, the hope would be that just given a superposition over the preimages of, say, $z_0$, the one-wayness of $f$ would imply the difficulty of finding a preimage of $z_1$\footnote{More concretely, a purported reduction to one-wayness when given challenge image $z_1$, can sample a random image $z_0$ with its preimages, then find $h$ s.t. $h(z_0) = h(z_1)$, thereby using a TCR adversary to find a preimage of the given challenge $z_1$.}. This could give the type of TCR property we need.

\paragraph{Technical Bottlenecks, and a Resolution.} Unfortunately, there are two issues with the approach proposed above. 
First, $f$ may be extremely unbalanced, so that the relative sizes of the sets of preimages of two random points $y_1, y_2$, i.e. $|\{x: f(x) = y_1\}|$ and $|\{x: f(x) = y_2\}|$ in its image may have very different sizes, that are not polynomially related with each other. 
%\dakshita{I had some trouble parsing the previous sentences - James, what did you mean?}
There may even be many points in the co-domain/range that have \emph{zero} preimages (for a general OWF, we cannot guarantee that its image is equal to its range). A second related issue is that the above sketched reduction to one-wayness may not work. Let's say we choose $h$ to be a two-to-one function defined by a random shift $\Delta$, i.e. $h(x) = h(x \oplus \Delta)$. Then we are essentially asking that it be hard to invert a random \emph{range} element of $f$, as opposed to $f(x)$ for a random \emph{domain} element $x$, which is the standard one-wayness assumption.

We don't know how to make this approach work from arbitrary one-way functions, which we leave as an open question. Instead, we appeal to a result of~\cite{balancedOWF}, who in the classical context of building statistically hiding commitments, show the following result. 
%Starting with an (almost)-\emph{regular} one-way function and hashing its range down with an appropriate universal hash function results in a function with exactly the properties we need: 
By appropriately combining an (almost)-\emph{regular}\footnote{An almost regular one-way function generalizes regular one-way functions to require only that for any two images $y_1, y_2$ of the function, the sizes of preimage sets of $y_1, y_2$ are polynomially related. In particular, injective functions, and (standard) regular functions also satisfy almost-regularity.} one-way function with universal hash functions, it {\em is} possible to obtain a function $f$ with exactly the required properties:
sufficiently balanced, and one-way over its \emph{range}. The former property means that an overwhelming fraction of range elements have similar-sized preimage sets, while the latter property says that an element $y$ sampled randomly from the range of the function cannot be inverted except with negligible probability. This resolves both the difficulties above.


Given such a balanced function $f$, we apply a random two-to-one hash $h$ defined by a shift $\Delta$ to the range of this $f$. We prove in Section~\ref{sec:almostreg} that this implies the flavor of target-collision-restistant hash that we need to construct commitments with $\PVD$.

%Next, we rely on prior work~\cite{balancedOWF} which shows that 
%\dakshita{@james, I was wondering if you could fill this part in}
%$\frac{(1-\delta)}{(1+\delta)} \leq \frac{|S_0|}{|S_1|} \leq \frac{(1+\delta)}{\frac(1-\delta)}$ for some constant $\delta \in [0,1)$.



\paragraph{Public-Key Encryption with $\PVD$.}
Next, we note that the construction above {\em also} yields a public-key encryption scheme, as long as there is a trapdoor that allows recovery of the phase $b$ given the state 
\[ y, \sum_{x: f(x) = y, M(x) = 0} \ket{x} + (-1)^b \sum_{x: f(x) = y, M(x) = 1} \ket{x} \]
We call this property ``trapdoor phase-recoverability''. We show that this property is achievable from generic assumptions, even those that are not known to imply classical PKE.  
\begin{itemize}
    \item Specifically, trapdoor phase-recoverability is implied by a trapdoored variant of (almost) regular one-way functions, for which a trapdoor to the function allows recovery of a uniform superposition over all preimages of any given image $y$. This then allows efficient projection onto $\sum_{x: f(x) = y, M(x) = 0} \ket{x} + (-1)^b \sum_{x: f(x) = y, M(x) = 1} \ket{x}$ for any efficient $M$. 
    We also note that this property is satisfied by any (standard) trapdoored injective function.
    But it is also satisfied by functions such as the Ajtai function that are not necessarily injective. Indeed, it is unclear how to build classical public-key encryption, or even PKE with classical ciphertexts, given a general trapdoor phase-recoverable function. Nevertheless, we formalize the above ideas in \cref{sec:pke} and \cref{sec:almostreg} to build PKE schemes with quantum ciphertexts, that also support $\PVD$.
    \item Additionally, we show in \cref{sec:hmy} that a recent public-key encryption scheme of~\cite{HMY} from pseudorandom group actions also satisfies trapdoor phase-recoverability: in fact, the decryption algorithm in~\cite{HMY} relies on recovering the phase from a similar superposition, given a trapdoor.
\end{itemize}

\paragraph{Hybrid Encryption with PVD.} Finally, we observe that we can use any encryption scheme $\Enc$ to encrypt the trapdoor $\td$ associated with the above construction, and security will still hold. That is, if $\Enc$ is semantically-secure, then our techniques extend to show that a ciphertext of the form
%
\[ y, \sum_{x: f(x) = y, M(x) = 0} \ket{x} + (-1)^b \sum_{x: f(x) = y, M(x) = 1} \ket{x} , \Enc(\td)\]
%
where $\mathsf{td}$ is the trapdoor for $f$, still supports publicly-verifiable deletion of the bit $b$. Thus, our approach can be seen as a way to \emph{upgrade} cryptographic schemes $\Enc$ with special properties to satisfy $\PVD$. In particular, we prove in \cref{sec:generic} that instantiating $\mathsf{Enc}$ appropriately with attribute-based encryption, fully-homomorphic encryption, witness encryption, or timed-release encryption gives us the same scheme supporting $\PVD$.

\subsection{Discussion and Directions for Future Work}
Our work demonstrates a strong relationship between weak security properties of (trapdoored) one-way functions and publicly-verifiable deletion. In particular, previous work~\cite{Poremba22} conjectured that collapsing functions, which are a quantum strengthening of collision-resistant hashes, lead to cryptosystems with publicly-verifiable deletion. 
%But obtaining publicly-verifiable deletion from standards assumption like LWE remained an open problem. 
Besides proving this conjecture, we also show that collapsing/collision-resistance, which are considered stronger assumptions than one-wayness, are actually not necessary for PVD. 

Indeed, weakenings that we call target-collapsing and generalized-target-collision-resistance, can be obtained from (regular) variants of one-way functions, and do suffice for publicly-verifiable deletion.
Analogously to their classical counterparts, we believe that these primitives will be of independent interest. 
Indeed, a natural question that this work leaves open is whether variants of these primitives that suffice for publicly-verifiable deletion can be based on {\em one-way functions} without the regularity constraint. It is also interesting to further understand relationships and implications between target-collision-resistance and target-collapsing, including when these properties may or may not imply each other. It may also be useful to understand if these weaker properties can suffice in place of stronger properties such as collapsing and collision-resistance in other contexts, including the design of post-quantum protocols.

Finally, note that we rely on trapdoored variants of these primitives to build public-key encryption schemes. Here too, in addition to obtaining PKE with $\PVD$ from any injective trapdoor one-way function (TDF), it becomes possible to relax assumptions to only require (almost)-regularity and trapdoor phase-recoverability -- properties that can plausibly be achieved from weaker concrete assumptions than injective TDFs.
%, such as pseudorandom group actions~\cite{HMY}. 
These are new examples of complexity assumptions that yield public-key encryption with quantum ciphertexts, but may be too weak to obtain PKE with classical ciphertexts.
It is an interesting question to further investigate the weakest complexity assumptions that may imply public-key encryption, with or without $\PVD$.
%\dakshita{out of steam for now, @james if you have energy please feel free to add, o/w I will add in the morning}


\section*{Acknowledgements}
D.K. was supported in part by NSF CAREER CNS-2238718, NSF CNS-2247727 and DARPA SIEVE. This material is based upon work supported by
the Defense Advanced Research Projects Agency through Award HR00112020024.

A.P. is partially supported by AFOSR YIP (award number FA9550-16-1-0495), the Institute for Quantum Information and Matter (an NSF Physics Frontiers Center; NSF Grant PHY-1733907) and by a grant from the Simons 
Foundation (828076, TV).
