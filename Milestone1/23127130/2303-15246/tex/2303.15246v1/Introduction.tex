\section{Introduction}
\label{sec:intro}

One of the greatest challenges in theoretical high-energy physics is
to meet the demand for increasingly precise predictions. Even leaving
aside conceptual issues, reducing theoretical uncertainties typically
requires ever more complex calculations, which incur steeply rising
computing costs. Already now Monte Carlo event generation for the LHC
constitutes a notable fraction of the experimental computing budgets.
Even with this large computing power investment event sample sizes
have to be limited to a size where the resulting uncertainty can be
non-negligible~\cite{HSFPhysicsEventGeneratorWG:2020gxw}. What is
more, event generation is only the first step in the full simulation
chain, and the already substantial computing costs of this step are
often dwarfed by the subsequent simulation of the detector
response. All these problems are expected to become even more severe
in the future, in particular with the advent of the HL-LHC. This
development is obviously at odds with general sustainability
goals. Improving the efficiency of event simulation is one of the
foremost tasks in particle physics phenomenology.

For a high-accuracy simulation a Monte Carlo generator needs to
combine real and virtual corrections, match resummations in different
limits and combine processes of different multiplicities while
avoiding double counting. Many different prescriptions exist for each
of these combination steps~\cite{Frixione:1995ms,Catani:1996vz,Catani:2002hc,Nagy:2003qn,Gehrmann-DeRidder:2005btv,Czakon:2010td,Somogyi:2005xz,Somogyi:2006da,Gaunt:2015pea,Cacciari:2015jma,Bonciani:2015sha,Magnea:2018hab,Frixione:2002ik,Frixione:2007vw,Hamilton:2012np,Hoche:2014uhw,Jadach:2015mza,Monni:2019whf,Prestel:2021vww,Catani:2001cc,Lonnblad:2001iq,Frederix:2012ps,Lonnblad:2012ix}. While they differ
substantially in the details, a common theme is the introduction of a
varying number of auxiliary events that \emph{subtract} from the
accumulated cross section instead of adding to it. If the number of
such negative-weight events becomes large enough, they can severly
impair the statistical convergence since large amounts of events are
required for a sufficient precision to allow for accurate
cancellation of the contributions with opposite signs. In fact, for a fractional
negative-weight contribution $r_-$ the number $N(r_-)$ of
required unweighted Monte Carlo events to reach a given statistics goal is (see e.g.~\cite{Frederix:2020trv})
\begin{equation}
  \label{eq:N_r_-}
  N(r_-) = \frac{N(0)}{(1 - 2r_-)^2}.
\end{equation}
Requiring a larger number of events not only increases the
computational cost in the generation stage, but especially also in the
subsequent detector simulation. A further problem is the increased disk
space usage, inducing both short- and long-term storage costs. It is
therefore highly desirable to keep the negative-weight contribution
small, $r_- \ll \frac{1}{2}$.

One avenue in this direction is to reduce the number of
negative-weight events during event generation, see
e.g.~\cite{Frederix:2020trv,Gao:2020vdv,Bothmann:2020ywa,Gao:2020zvv,Danziger:2021xvr}. A
second approach is to eliminate negative weights in the generated
sample, before detector
simulation~\cite{Andersen:2020sjs,Nachman:2020fff,Verheyen:2020bjw,Andersen:2021mvw}. In
the following, we focus on the \emph{cell resampling} approach
proposed in \cite{Andersen:2021mvw}, which in turn was inspired by
\emph{positive resampling}~\cite{Andersen:2020sjs}. Cell resampling is
independent of both the scattering process under consideration and any
later stages of the event simulation chain. It only redistributes event
weights, exactly preserving event kinematics. The effective range over
which event weights are smeared decreases with an increasing event
density. This implies that the smearing uncertainty decreases
systematically with increasing statistics without the need to change
the method. As demonstrated for the example
of the production of a W boson with two jets at next-to-leading-order
(NLO), a large fraction of negative weights can be eliminated without
discernible distortion of any predicted observable. A limitation of the original
formulation is the computational cost, which rises steeply with both the
event sample size and the number of final-state particles.

In section~\ref{sec:improvements}, we briefly review the method and
describe a number of algorithmic improvements which allow us to
overcome the original limitations through a speed-up by several orders
of magnitude. In section~\ref{sec:analyses}, we then apply cell
resampling to high-multiplicity samples with up to several billions of
events for the production of a W or Z boson in association with up to
five jets at NLO.  We conclude in section~\ref{sec:conclusions}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
