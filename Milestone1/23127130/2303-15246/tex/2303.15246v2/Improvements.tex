\section{Algorithmic Improvements}
\label{sec:improvements}

In the following, we briefly review cell resampling and describe the
main improvements that allow us to apply the method also to large
high-multiplicity Monte Carlo samples. For a detailed motivation and
description of the original algorithm see~\cite{Andersen:2021mvw}.

\subsection{Cell Resampling}
\label{sec:cellres}

At its heart, cell resampling consists of repeatedly selecting a
subset of events --- referred to as cells --- and redistributing event
weights within the selected set. The steps are as follows.
\begin{enumerate}
\item Select an event with negative weight as the first event (the
``seed'') of a new cell $\mathcal{C}$.
\item Out of all events outside the cell, add the one with the
smallest distance from the seed to the cell. Repeat for as long as the
accumulated weight of all events within the cell is negative.
\item Redistribute the weights of the events inside the cell such that
the accumulated weight is preserved and none of the final weights is
negative.\footnote{A specific method of redistributing weights in this way is discussed in~\cite{Andersen:2021mvw}.}
\item Start anew with the next cell, i.e.\ with step 1.
\end{enumerate}
Note that an event can be part of several cells, but will only be
chosen as a cell seed at most once. In practice, we usually want to
limit the maximum cell size and abort step 2 once the distance between
the cell seed and its nearest neighbour outside the cell becomes too
large. We denote the maximum cell radius, i.e. the maximum allowed
distance between the cell seed and any other event within the cell, by
$d_\text{max}$. It is a parameter of the cell resampling algorithm.
If we limit the cell size in this way, we can only achieve a partial cancellation
between negative and positive event weights in the following step
3. For practical applications this is often sufficient, since the
small remaining contribution from negative weights has a much reduced
impact on the statistical convergence, cf.~equation~(\ref{eq:N_r_-}).

The computational cost of cell resampling tends to be completely
dominated by the nearest-neighbour search in step 2. In a naive
approach, one has to calculate the distances between the cell seed and
each other event in the sample. Since the number of cells is
proportional to the sample size $N$, the total computational
complexity is $\mathcal{O}(N^2)$. This renders the naive approach
unfeasible for samples with more than a few million events. For this reason,
an alternative approximate nearest-neighbour search based on
locality-sensitive hashing (LSH)~\cite{Indyk1998,Leskovec:2020} was
considered in~\cite{Andersen:2021mvw}. While this lead to an improved
scaling behaviour, the quality of the approximate search was also
found to deteriorate with an increasing sample size. An improved
version of this algorithm, discussed in appendix~\ref{sec:LSH_search}, still appears
to suffer from the same problem. In section~\ref{sec:nearest-neighbour_search}, we introduce an
exact search algorithm that is orders of magnitude faster than the
naive search.

The problem of costly distance calculations is further exacerbated by
the fact that a direct implementation of the originally proposed
distance function suffers from poor scaling for high
multiplicities. To compute the distance between two events $e$ and
$e'$, we first cluster the outgoing particles into infrared-safe
physics objects, e.g.~jets. We collect objects of the same type $t$
into sets $s_t$ for $e$ and $s_t'$ for $s$. The distance between the
two events is then
\begin{equation}
  \label{eq:d_event}
  d(e,e') = \sum_t d(s_t, s_t'),
\end{equation}
where $d(s_t, s_t')$ is the distance between the two sets $s_t, s_t'$. It is given by
\begin{equation}
  \label{eq:d_set}
  d(s_t,s_t') = \min_{\sigma \in S_P} \sum_{i=1}^P d(p_i, q_{\sigma(i)})\,,
\end{equation}
where $p_1,\dots, p_P$ are the momenta of the objects in $s_t$ and
$q_1,\dots, q_P$ the momenta\footnote{If the number of objects in $s_t$
and $s_t'$ is different, we add auxiliary objects with vanishing
momenta as described in~\cite{Andersen:2021mvw}.} in $s_t'$. A naive
minimisation procedure considers all permutations $\sigma$ in the symmetric
group $S_P$, i.e.~$P!$ possibilities. For large multiplicities $P$ a direct
calculation quickly becomes prohibitively
expensive. In~\cite{Andersen:2021mvw}, it was therefore suggested to
use an approximate scheme in this case. In
section~\ref{sec:set-to-set-distance} we discuss how the set-to-set
distance can be calculated both exactly and efficiently.

\subsection{Nearest-Neighbour Search}
\label{sec:nearest-neighbour_search}

Our improved nearest-neighbour search is based on vantage-point
trees~\cite{UHLMANN1991175,10.5555/313559.313789}. To construct a
vantage-point tree, we choose a single event as the first vantage
point. We then compute the distance to the vantage point for each
event. The closer half of the events lie within a hypersphere with
radius given by the median distance to the vantage point. We call the
populated part of this hypersphere the \emph{inside region} and its
complement the \emph{outside region}. We then recursively construct
vantage-point trees inside each of the two regions. The construction
terminates in regions that only contain a single point.

To find the nearest neighbour for any event $e$, we start at the root
of the tree, namely the first chosen vantage point. We calculate the
distance $D$ between this vantage point and $e$. If $D$ is less than
the radius $R$ of the hypersphere defining the inside region, we first continue the search in
the inside subtree, otherwise we choose the outside subtree
first. Let us first consider the case that the inside region is
the preferred one. It will contain a nearest-neighbour \emph{candidate} with
a distance $d$ to the initial event $e$. By the triangle inequality we
deduce that the \emph{actual} nearest neighbour can have a distance of at
most $D+d$ to the current vantage point. Therefore, if $D+d < R$, the actual
nearest neighbour cannot be in the outside region. Conversely, if
we started our search in the outside region and found a
nearest-neighbour candidate with $D-d > R$, then the actual nearest
neighbour cannot lie in the inside region. In summary, if $d < |R
- D|$ only the preferred region has to be considered.

Vantage-point tree search is indeed very well suited for cell
resampling. The construction is completely agnostic to the chosen
distance function. In particular, unlike the LSH-based methods
considered in~\cite{Andersen:2021mvw} and appendix~\ref{sec:LSH_search}, it does not
require a Euclidean metric. For an event sample of size $N$, the tree
construction requires $\mathcal{O}(N \log N)$ steps and can be easily
parallelised. In the ideal case where only the preferred regions are
probed, each nearest-neighbour search requires $\log_2 N$ comparisons,
which again results in an overall asymptotic complexity of
$\mathcal{O}(N \log N)$. While this means that for sufficiently large
event samples cell resampling will eventually require more computing
time than the $\mathcal{O}(N)$ event generation, we find that this is
not the case for samples with up to several billion events. Timings
for a practical application are given in section~\ref{sec:timings}.

We further optimise the nearest-neighbour search in several
aspects. Most importantly, if we limit the maximum cell size to $d_{\text{max}}$,
we can dramatically increase the probability that only the preferred
regions have to be considered. In fact, if $|R - D| > d_{\text{max}}$ then any
suitable nearest neighbours have to lie inside the preferred
region. We can further enhance the probability through a judicious choice
of the vantage points. Since input events near the boundary between
inside and outside regions require checking both regions for
nearest neighbours, the general goal is to minimise this surface. To
this end, we choose our first vantage point at the boundary of the
populated phase space. We select a random event, calculate
the distance to all other events, and choose the event with the
largest distance as the vantage point. Then, when constructing the
subtrees for the inside and outside regions, we choose as
vantage points those events that have the largest distance to the
parent vantage point.

When constructing a cell, we have to find nearest neighbours until
either the accumulated weight becomes non-negative or the distance
exceeds the maximum cell radius. This corresponds to a so-called $k$
nearest neighbour search, where in this case the required number $k$
of nearest neighbours is a priori unknown. To speed up successive
searches, we cache the results of distance calculations, i.e.~all
values of $D$ for a given input event.

Finally, we note that the vantage-point tree can also be employed for
approximate nearest-neighbour search if one only searches the preferred
region in each step. We exploit this property by first partitioning
the input events into the inside and outside regions of a
shallow vantage point tree, aborting the construction already after
the first few steps. We then apply cell resampling to each partition
independently. This approach allows efficient parallelisation, while
yielding much better results than the independent cell resampling of
randomly chosen partial samples. The price to pay is that the quality
of the nearest-neighbour search and therefore also of the overall
resampling deteriorates to some degree. In practice this effect
appears to be minor, see also section~\ref{sec:weight_elim}.

\subsection{Set-to-Set Distance at High Multiplicities}
\label{sec:set-to-set-distance}

The distance between two events as defined in~\cite{Andersen:2021mvw}
is the sum of distances between sets of infrared-safe physics objects,
see equation~\eqref{eq:d_event}. To define the distance between two
such sets $s_t, s_t'$, we aim to find the optimal pairing between the
momenta $p_1,\dots, p_P$ of the objects in $s_t$ and the momenta $q_1,\dots, q_P$ of the objects in $s_t'$. The naive approach of
considering all possible pairings, cf.~equation~\eqref{eq:d_set},
scales very poorly with the number of objects. However, the task of
finding an optimal pairing is an instance of the well-studied
\emph{assignment problem}.

Let us introduce the matrix $\mathbb{D}$ of distances with
\begin{equation}
  \label{eq:dist_matrix}
  \mathbb{D}_{ij} \equiv d(q_i, p_j).
\end{equation}
An efficient method for minimising $\sum_{i=1}^{P}
\mathbb{D}_{i\sigma(i)}$ was first found by
Jacobi~\cite{Jacobi1,Jacobi2}. It was later rediscovered independently
and popularised under the name ``Hungarian
method''~\cite{Kuhn1,Kuhn2,Munkres,10.1145/321694.321699,https://doi.org/10.1002/net.3230010206}. The
algorithm mutates the entries of $\mathbb{D}_{ij}$ in such a way that
the optimal pairing is preserved during each step. After each step,
one marks a minimum number of rows and columns such that each
vanishing entry is part of a marked row or column. The algorithm
terminates as soon as all rows (or columns) have to be marked. The mutating steps are as follows:
\begin{enumerate}
\item Replace each $\mathbb{D}_{ij}$ by $\mathbb{D}_{ij} - \min_{k} \mathbb{D}_{ik}$.
\item Replace each $\mathbb{D}_{ij}$ by $\mathbb{D}_{ij} - \min_{k} \mathbb{D}_{kj}$.
\item Find the smallest non-vanishing entry. Subtract it from all
unmarked rows and add it to all marked columns.
\end{enumerate}
Step 3 is repeated until the termination criterion is fulfilled.  In
our code, we use the implementation in the
\texttt{pathfinding}~\cite{pathfinding} package. Like the remainder of
our implementation of cell resampling it is written in the Rust
programming language.

Using the Hungarian algorithm instead of a brute-force search improves
the scaling behaviour for sets with $P$ momenta from $\mathcal{O}(P!)$
to $\mathcal{O}(P^3)$. In practice, we find it superior for $P >
3$. The \emph{FlowAssign} algorithm proposed by Ramshaw and Tarjan
\cite{Ramshaw2012581} would scale even better, with a time complexity
of $\mathcal{O}\big(P^{5 / 2}\log(D P)\big)$. The caveat is that the
scaling also depends logarithmically on the range $D$ of distances
encountered. Since the maximum multiplicity reached in current NLO
computations is limited, an auction \cite{Bertsekas1988} (or
equivalently push-relabel \cite{Goldberg1995,Alfaro2022}) algorithm
may still perform better in practice despite formally inferior scaling
behaviour. We leave a detailed comparison to future work.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
