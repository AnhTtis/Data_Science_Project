\section{Neural DNF-based Models}

Based on the semi-symbolic layers and the neural DNF module proposed in pix2rule \cite{pix2rule}, we train two types of neural DNF-based models: a \textit{vanilla neural DNF model} for multi-label classification, and a novel \textit{neural DNF-EO model} for multi-class classification.

\vspace{0.5em}

\captionsetup{font=normalfont, labelfont=bf}

\begin{wrapfigure}{r}{.45\textwidth}
\vspace{-15pt}
\includegraphics[width=\linewidth]{figure/Vanilla_DNF.pdf}
\captionof{figure}{An example of a vanilla neural DNF model.}
\vspace{-18pt}
\label{fig:vanilla-dnf}
\end{wrapfigure}

The \textbf{vanilla neural DNF model} as shown in Figure~\ref{fig:vanilla-dnf} consists of a semi-symbolic conjunctive layer followed by a disjunctive one, sharing the same architectural design as pix2rule's neural DNF module. It is suitable for independent rule learning in both binary and multi-label classification tasks. In the latter case, the target (binary) predicates are independent of each other.

\vspace{0.5em}

\noindent \textbf{The challenge of multi-class classification} \label{chap:cross-entropy-and-multi-class-classification}

A multi-class classification task requires exactly one class to be true at a time. This can be written as a logical constraint expressed as follows: \mbox{$\leftarrow class(X), class(Y), X\neq Y$}. It states that two different classes cannot be true at the same time. This constraint is not directly satisfiable by an MLP trained with a cross-entropy loss. An MLP trained under a cross-entropy loss takes the $\arg \max$ of the output when predicting a class at inference time and does not need to enforce mutual exclusion over its outputs. Hence, it is not capable of capturing the logical constraint. Similarly, training our vanilla neural DNF model with a cross-entropy loss does not enforce mutual exclusivity on the model. At inference time, a neural DNF-based model's $\tanh$ output of a class is treated as True ($\top$) if the value is greater than 0 and otherwise False ($\bot$). Since the softmax function in the cross-entropy loss only amplifies the difference between the output logits of each class, it does not have the concept of a value being True if it goes above a threshold, which is required for us to interpret our neural DNF-based model's outputs. The vanilla neural DNF model's output logits under the cross-entropy loss are only trained to be different from each other and not to be extreme values that fall on different sides of our desired threshold value. For example, if the raw output logits of our vanilla neural DNF model are $\tilde{y} = [-2, 1.5, 3]$ with a target class of 2 (zero-indexed), then $\text{softmax}(\tilde{y}) = [0.006, 0.181, 0.813]$ which is very different between each pair, and the cross-entropy loss would be $0.207$. However, the $\tanh$ output would be $\tanh(\tilde{y}) = [-0.964, 0.905, 0.995]$, which would be interpreted as $[\bot, \top, \top]$ and violate the `exactly-one-class' constraint in the multi-class setting. Our experiments of using cross-entropy loss to train vanilla neural DNF models on multi-class classification tasks (in Appendix~\ref{app:vndnf-cub}) show this issue of multiple classes being predicted as true, by interpreting the output to be discrete True and False value at threshold value 0. This motivates our second model, which (i) can be trained with cross-entropy loss, even though it only pushes output logits to be different; (ii) but still have the property of mutual exclusivity guaranteed.

\vspace{0.5em}

\begin{wrapfigure}{r}{.55\textwidth}
\vspace{-15pt}
\includegraphics[width=\linewidth]{figure/DNF-EO.pdf}
\captionof{figure}{An example of a neural DNF-EO model. The plain neural DNF in the dotted yellow box is identical to a vanilla neural DNF model.}
\label{fig:dnf-eo}   
\vspace{-18pt}
\end{wrapfigure}

The \textbf{neural DNF-EO model} shown in Figure~\ref{fig:dnf-eo} has a constraint layer (layer C2 in the figure) that reflects the `exactly-one' constraint `$class_i \leftarrow \land_{j, j \neq i} \text{ not } class_j$' which enforces mutual exclusion on the plain neural DNF (in the dotted yellow box in the figure). If more than one output is true after the disjunctive layer C1, no class would be true after the constraint layer C2. This constraint layer C2 is initialised with all connections being -6\footnote{Having weights to be 6 or -6 saturates $\tanh$ ($\tanh(\pm 6)\approx \pm1$), and the output should be approximately 1 or -1.}, representing the logical negation `not', and its weights are not updated. During training, the constraint layer C2's output logits are pushed to be different and to match with the expected class by the cross-entropy loss, but the output of disjunctive layer C1 of the plain neural DNF is pushed to have only one value being true and all the rest being false. At inference time, we remove the constraint layer C2 and inference with the plain neural DNF that is enforced to output exactly one class true at a time, thus providing mutual exclusivity. 

\vspace{0.5em}

In order to extract rules from these neural DNF-based models, we follow and expand on the post-training processes used in pix2rule. Our post-training pipeline consists of 4 stages: pruning, finetuning, thresholding and extraction. Pruning, thresholding and extraction remain mostly the same as in pix2rule except for the metrics we use, while we add finetuning as a new step to adjust for learning and extracting rules in real-world datasets.

\textbf{Pruning} process sets the weights that do not heavily affect the performance of the neural DNF-based model to 0, where the accepted performance drop is controlled by a hyperparameter $\epsilon$. We prune the disjunctive layer first followed by the conjunctive layer. We also remove any disjunct that uses a conjunction with an empty body, i.e. the conjunction has weight 0 for all its conjuncts. For the neural DNF-EO model, we remove the constraint layer C2 and prune only the plain neural DNF.

\textbf{Finetuning} process adjusts the model's weights by re-training the neural DNF-based model on the training set without updating the pruned weights. For the neural DNF-EO model, we add back the constraint layer after the pruned plain neural DNF during the finetuning process to maintain mutual exclusivity.

\textbf{Thresholding} process goes through a range of possible threshold values, such that if the magnitude of the weight is greater than that threshold, the magnitude of the weight would be set to 6. By setting the value of weights to 6 or -6, $\tanh$ output can be saturated close to 1 and -1. We choose the threshold value that gives the best performance based on the validation set. Again, the constraint layer C2 in the neural DNF-EO model is dropped and the thresholding is done on the plain neural DNF.
    
\textbf{Symbolic rule extraction} is done by taking input attributes/conjunctions connected to weights with value $\pm 6$ as the conjunct/disjunct of their connected conjunction/disjunction. And each disjunct of the disjunction can be treated as a separate rule to classify a class or label. We express the extracted rules as an ASP program.
