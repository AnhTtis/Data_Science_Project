\section{Related Work}

Our work relates to the research of differentiable Inductive Logic Programming (differentiable ILP). Differentiable ILP methods approximate discrete logical connectives and input with continuous ones and enable continuous logic reasoning, so that the model can be trained via gradient descent. In this category, we can further split the methods into two branches: rule-template-based differentiable ILP and free-form differentiable ILP.

Rule-template-based differentiable ILP methods rely on a set of rule templates (or meta-rules). The rule templates specify the form of rules to learn, and they help with scaling by restricting the hypothesis search space. They also guide the symbolic interpretation of the model. $\delta$-ILP \cite{dnl-ilp}, Neuro-symbolic ILP with Logical Neural Networks (NS-ILP-LNN) \cite{lnn-ilp} and Hierarchical Rule Induction (HRI) \cite{hri} are examples that use rule templates as part of their model instantiation. $\delta$-ILP utilises product t-norm for fuzzy conjunction for differentiable forward chaining, but its rule templates only allow exactly 2 atoms in a rule body. NS-ILP-LNN uses Logical Neural Network \cite{lnn} to constrain the neuron weights and achieve logical behaviour, and the model is based on a tree-structured template. HRI introduces \textit{proton-rule} which extends normal meta-rule and allows flexible predicate arity, making the template set less task-specific. Its hierarchical model follows an embedding-similarity-based inference technique based on \cite{lri}. The choice of rule templates in these approaches requires careful human engineering to ensure a solution would be included in the hypothesis space, and thus the meta-rules/proton-rules set is always task-specific to some extent. Our neural DNF-based model does not rely on rule templates, nor does it restrict the form of rule body: we allow any number of atoms in the rule body and the input predicates of our conjunctive semi-symbolic layer can have any arity as long as grounded\footnote{Although all our experiments are propositional, pix2rule's experiments show the neural DNF module learning from nullary, unary and binary predicates. And to handle first-order logic, we only need to permutate all grounding for each predicate and use grounded predicates as input.}. The only hyperparameter and restriction in our model is the number of conjunctions that can be used by a disjunction.

Free-form differentiable ILP methods do not restrict the form of rules to learn, avoiding the need for human-engineered templates. Neural Logic Machine (NLM) \cite{nlm} is a multi-layer multi-group architecture that realises forward chaining. It is able to learn from nullary, unary and binary predicates, but the learned rules cannot be extracted into a symbolic form. Differentiable Logic Machine (DLM) \cite{dlm} builds upon NLM's architecture but replaces MLPs with logic modules which implement fuzzy logic, and provides symbolic rule extraction after training. Negation is separately applied on conjunction/disjunction outputs in DLM, and it requires tricks such as dropout and Gumbel noise to be trained well. The neural DNF module proposed in pix2rule \cite{pix2rule} handles the negation within a layer without special treatment, and symbolic rules in ASP encoding can be extracted after training. However, pix2rule only demonstrates that the neural DNF module performs well in synthetic binary classification datasets. Our work addresses the limitation by building upon its neural DNF module and experimenting in complex real-world multi-class and multi-label classification tasks, and our models still achieve good performance and provide interpretability by enabling rule extraction.

Another design pattern in the field of neuro-symbolic rule learning is to use separate neural and logical models, which generally does not allow full back-propagation. Two recent works in this category are $Meta_{Abd}$ \cite{metaabd} and NSIL \cite{nlm}. Both methods train a neural network for handling raw data input while their symbolic learners induce logic rules at the same time. $Meta_{Abd}$ adds an abduction procedure and learns with Metagol learning system \cite{metagol}, which is limited to definite logic (no function/constraint etc.). NSIL uses FastLAS \cite{fastlas} to learn from ASP programs with better expressiveness, as well as NeurASP \cite{neurasp} to train the network..
