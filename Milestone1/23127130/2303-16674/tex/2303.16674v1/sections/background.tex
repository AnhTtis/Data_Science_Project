\section{Background} \label{chap:background}

This chapter provides a summary of the neural Disjunctive Normal Form (neural DNF) module presented in pix2rule \cite{pix2rule}. pix2rule proposes a novel semi-symbolic layer that behaves like a neuro-symbolic disjunction or conjunction of (negated) atoms. Its disjunctive or conjunctive behaviour is controlled by a bias parameter. Given some continuous input $x_i \in [\bot, \top]$ where $\top = 1$ and $\bot = -1$, the semi-symbolic layer is defined as:

\begin{align}
    y &= f(\sum_{i} w_i x_i + \beta) \label{eq:ss-y}\\
    \beta &= \delta (\max_{i} |w_i| - \sum_i |w_i|) \label{eq:ss-bias}
\end{align}

\noindent where $w_i$ are trainable weights, $f$ is the activation function $\tanh$, and $y$ is the layer output. By setting $\delta$ to be 1 or -1, the layer can approximate conjunction or disjunction. Using this formulation instead of a t-norm fuzzy logic with a continuous input space of [0, 1] attempts to reduce the gradient vanishing problem by avoiding the multiplication of many close-to-zero values \cite{pix2rule}. The neural DNF module is created by stacking a conjunctive semi-symbolic layer followed by a disjunctive semi-symbolic layer. During training, the magnitude (i.e. absolute value) of the $\delta$ parameters in both layers, which controls the magnitude of logical biases on the layers, is increased gradually from 0.1 to 1, so that the module can learn more stably and slowly adjust to the logical biases. Symbolic rules can be extracted from a trained neural DNF module through post-training processes. The rules extracted are expressed in a logic programming language called Answer Set programming (ASP)~\cite{asp}. Although shown to be effective in learning end-to-end ASP rules from unstructured synthetic binary datasets, it is unclear whether the neural DNF module would still learn well in real-world multi-class and multi-label classification tasks.