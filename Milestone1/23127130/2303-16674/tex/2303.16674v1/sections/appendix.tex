\appendix

\section{Data Pre-processing}

\subsection{Procedure on CUB Dataset/subsets} \label{app:data-pre-processing-cub}

Algorithm~\ref{algo:data-pre-processing} shows the pseudo-code of the data pre-processing procedure used by us and also in the Concept Bottleneck Models paper \cite{cbm}. After such pre-processing, all samples of the same class would have the exact same attribute encoding. In the Concept Bottleneck Models paper, the encoding for each class is computed from the training set and also used as the encoding for the validation set and test set as well. Although it is an odd decision to change the validation set and test set using results from the training set, the data pre-processing does ensure attribute label consistency both within each class and across samples and in all 3 sets. We follow this decision in our pre-processing for a fair comparison. The $N$ values we choose for CUB-3, -10, -15, -20, -25, -50, -100, and -200 are 1, 2, 3, 3, 3, 5, 7, and 10 respectively.

\begin{algorithm}[h]
\SetKwInput{Output}{Hyperparameters}
\KwData{\\
\ \ Dataset $D$\\
\ \ Total number of classes $n_c$\\
\ \ Total number of attributes in CUB $n_a$\\
}
\SetKw{Continue}{continue}
\Output{\\
\ \ $N$ -- number of classes an attribute should be present consistently}
\BlankLine

\tcc{Count for each attribute in each class how many times it is present/not present}
$class\_attribute\_count \leftarrow n_c \times n_a \times 2 $ array of all zeros\;
\For{$d \in D$}{
    \For{Attribute $a \in d.attributes$}{
        \If{$a.is\_present == 0 \land a.certainty == 1$}{
            \tcc{Ignore attribute that is not present because not visible}
            continue\;
        }
        $class\_attribute\_count[d.class][a.attribute\_id][a.is\_present] += 1$\;
    }
}
\BlankLine
\tcc{For each class, for each attribute, which presence label (present or not present) is less frequent}
$class\_attribute\_min\_presence \leftarrow \arg\min(class\_attribute\_count, axis=2)$\;
\tcc{For each class, for each attribute, which presence label (present or not present) is more frequent}
$class\_attribute\_max\_presence \leftarrow \arg\max(class\_attribute\_count, axis=2)$\;
\BlankLine
\tcc{Find where most and least frequent, set most frequent to 1}
$equal\_count\_index \leftarrow $ indices where $class\_attribute\_min\_presence == class\_attribute\_max\_presence$\;
$class\_attribute\_max\_presence[equal\_count\_index] \leftarrow 1$\;
\BlankLine
\tcc{Count number of times each attribute is mostly present for a class}
$attribute\_class\_count \leftarrow \text{sum}(class\_attribute\_max\_presence, axis=0)$\;
\BlankLine
\tcc{Select the attributes that are present most of the time on a class level, in at least $N$ classes}
$mask \leftarrow $ indices where $attribute\_class\_count \geq N$
\BlankLine
\tcc{Get the attribute median}
$class\_attribute\_median \leftarrow class\_attribute\_max\_presence[:, mask]$\;
\caption{Data pre-processing for CUB dataset}
\label{algo:data-pre-processing}
\end{algorithm}

\subsection{Procedure on TMC Dataset/subsets} \label{app:data-pre-processing-tmc}

\newcommand{\mi}[2]{\mathrm{I}(#1;#2)}
\newcommand{\entropy}[1]{\mathrm{H}(#1)}
\newcommand{\condEntropy}[2]{\mathrm{H}(#1|#2)}

We calculate the mutual information (MI) of each attribute with respect to the output label combinations, and filter out attributes with MI value less than a threshold $t$. The $t$ values we choose for TMC-3, -5, -10, -15, and -22 are 0.01, 0.02, 0.04, 0.04 and 0.05 respectively. Below we provide the mathematical formulae to calculate the mutual information.

Mutual information of two discrete random variables $X$ and $Y$ can be defined as:

\begin{equation} \label{eq:mi}
    \mi{X}{Y} \equiv \entropy{X} - \condEntropy{X}{Y}
\end{equation}

where $\entropy{X}$ and $\entropy{Y}$ are entropies, and $\condEntropy{X}{Y}$ is conditional entropy. $\entropy{X}$ is calculated as:

\begin{equation} \label{eq:hx}
    \entropy{X} = - \sum_{x \in \mathcal{X}} p(x) \log p(x)
\end{equation}

And the conditional entropy $\condEntropy{X}{Y}$ is calculated as:

\begin{equation} \label{eq:hxy}
    \condEntropy{X}{Y} = - \sum_{y \in \mathcal{Y}} p(y) \sum_{x \in \mathcal{X}} p(x|y) \log p(x|y)
\end{equation}

For each attribute $X_i$, the mutual information of it with respect to the output label combination $Y$ is calculated as the following, using Equation \ref{eq:mi},  \ref{eq:hx} and \ref{eq:hxy}:

\begin{equation} \label{eq:mixiy-raw}
    \mi{X_i}{Y} = - \sum_{x_i \in \mathcal{X}_i} p(x_i) \log p(x_i) - \left(- \sum_{y \in \mathcal{Y}} p(y) \sum_{x_i \in \mathcal{X}_i} p(x_i|y) \log p(x_i|y) \right)
\end{equation}

Since each attribute only has two outcomes: present (1) or not present (0), Equation \ref{eq:mixiy-raw} is equivalent to:

\begin{equation}
\begin{split}
    &\mi{X_i}{Y} = - \Bigl(p(X_i = 1) \log p(X_i = 1) + p(X_i = 0) \log p(X_i = 0) \Bigr) \\
    &- \Biggl(- \sum_{y \in \mathcal{Y}} p(y) \Bigl(p(X_i = 1|y) \log p(X_i = 1|y) + p(X_i = 0|y) \log p(X_i = 0|y)\Bigr) \Biggr)
\end{split}
\end{equation}

\clearpage

\section{Experiments}

\subsection{Code} \label{app:code}

We implement the semi-symbolic layer in PyTorch\footnote{\url{https://pytorch.org/}}. The code for our CUB experiments is available at \url{https://github.com/kittykg/neural-dnf-cub}, and the code for our TMC experiments is available at \url{https://github.com/kittykg/neural-dnf-tmc}. Both repositories contain our implementation of the semi-symbolic layer and the neural DNF-based models.

\subsection{Machine Specification}

The machine specifications for our experiments are shown below in Table~\ref{tab:machine-spec}. 

\begin{table}[h]
\begin{tabular}{c|c|c|c}
Model             & CPU                                                                                                                           & GPU                                                                                        & Memory                 \\ \hline
Neural DNF-based models & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Intel\textregistered\ Xeon\textregistered\ CPU\\ E5-1620 v2 @3.70Ghz\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Nvidia GeForce GTX\\ (8GB Memory)\end{tabular}} & \multirow{4}{*}{16 GB} \\
MLP               &                                                                                                                               &                                                                                            &                        \\ \cline{1-1} \cline{3-3}
Decision Tree     &                                                                                                                               & -                                                                                          &                        \\ \cline{1-3}
FastLAS           & \begin{tabular}[c]{@{}c@{}}Intel\textregistered\ Core\texttrademark\\\ i7-7700K CPU @ 4.20GHz\end{tabular}                    & -                                                                                          &                       
\end{tabular}
\caption{Machine specifications for our experiments.}
\label{tab:machine-spec}
\end{table}


\subsection{CUB Experiments}

\subsubsection{Hyperparameters and Model Architecture}

All CUB experiments are run under a random seed of 73. We use Adam \cite{adam-opt} as the optimiser with a learning rate of 0.001 and weight decay of 0.00004. The schedule steps in every 100 epochs. The model architectures, training batch size and training epoch of our neural DNF-EO models are shown in Table~\ref{tab:cub-hyperparameters}. The untrainable constraint layer C2 is of $NUM\_CLASSES \times NUM\_CLASSES$, where relevant connections are set to -6 and rest as 0. We set the pruning threshold $\epsilon=0.005$ in the post-training pipeline.

\begin{table}[h]
\begin{tabular}{c|ccc}
Dataset & In x Conj x Out & Batch size & Epoch \\ \hline
CUB-3   & 34 * 9 * 3      & 4          & 100   \\
CUB-10  & 41 * 30 * 10    & 16         & 100   \\
CUB-15  & 40 * 45 * 15    & 32         & 100   \\
CUB-20  & 48 * 60 * 20    & 32         & 100   \\
CUB-25  & 50 * 75 * 25    & 32         & 100   \\
CUB-50  & 61 * 150 * 50   & 32         & 100   \\
CUB-100 & 82 * 300 * 100  & 32         & 150   \\
CUB-200 & 112 * 600 * 200 & 32         & 200  
\end{tabular}
\caption{Neural DNF-EO model's architecture, training batch size and training epoch for CUB experiments.}
\label{tab:cub-hyperparameters}
\end{table}

The MLP model in each experiment has the same architectural design and training settings as the neural DNF-EO model, but each layer is a normal linear feed-forward layer instead of the semi-symbolic layer.

\subsubsection{Vanilla Neural DNF Models in Multi-class Classification} \label{app:vndnf-cub}

We have also carried out experiments with the vanilla neural DNF model on CUB-3, CUB-10 and CUB-200, and the results are shown in Table~\ref{tab:vanilla-dnf-cub}. With binary cross entropy as a loss function on only CUB-3, the vanilla neural DNF model is able to learn and give a set of rules which results in an average of 1 Jaccard index score on the test set. This is expected since there are only 3 classes to predict (no need for a shortcut by predicting everything as false) and each class is pushed to be binary (true or false). Yet binary cross entropy as a loss function does not work when more classes are present, since it is easier to `cheat' by predicting every class to be false. Cross-entropy loss emphasises the difference between output but does not push the output value to be mutually exclusive, as discussed in Chapter~\ref{chap:cross-entropy-and-multi-class-classification}. Without any enforcement on exactly one class being true, the vanilla neural DNF model with cross-entropy loss is able to predict the correct class under picking the max output (average of 1 accuracy on the test set), but cannot give an output where only one class is close to 1 and everything else is -1 (less than 1 average Jaccard score). These results show the importance of the constraint layer C2 in our neural DNF-EO model.

\begin{table}[h]
\begin{tabular}{c|c|cccccc}
Model + Loss Fn. & Dataset                  & \begin{tabular}[c]{@{}c@{}}Train\\ Acc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Train\\ Jacc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Prune\\ Jacc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Finetune\\ Jacc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Threshold\\ Jacc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Rules\\ Jacc.\end{tabular} \\ \hline
Vanilla + BCE    & \multirow{2}{*}{CUB-3}   & 1.000                                                & 1.000                                                 & 1.000                                                 & 1.000                                                    & 1.000                                                     & 1.000                                                 \\
Vanilla + CE     &                          & 1.000                                                & 0.333                                                 & 0.368                                                 & 0.368                                                    & 0.368                                                     & 0.368                                                 \\ \hline
Vanilla + BCE    & \multirow{2}{*}{CUB-10}  & 0.700                                                & 0.417                                                 & 0.433                                                 & 0.433                                                    & 0.433                                                     & 0.433                                                 \\
Vanilla + CE     &                          & 1.000                                                & 0.100                                                 & 0.117                                                 & 0.117                                                    & 0.055                                                     & 0.055                                                 \\ \hline
Vanilla + BCE    & \multirow{2}{*}{CUB-200} & 0.005                                                & 0.001                                                 & -                                                     & -                                                        & -                                                         & -                                                     \\
Vanilla + CE     &                          & 1.000                                                & 0.005                                                 & -                                                     & -                                                        & -                                                         & -                                                    
\end{tabular}
\caption{\textbf{Vanilla neural DNF} trained with \textbf{binary cross entropy} and \textbf{cross entropy} in different CUB subsets/dataset, at different stages of training pipeline. All metrics are calculated by averaging across test set. Vanilla neural DNF model performs poorly when 200 classes are present, and thus we do not proceed with the post-training stage.}
\label{tab:vanilla-dnf-cub}
\end{table}

\subsection{TMC Experiments}

\subsubsection{Hyperparameters and Model Architecture}

All TMC experiments are run under a random seed of 73. We use Adam \cite{adam-opt} as the optimiser with a learning rate of 0.001 and weight decay of 0.00004. The schedule steps in every 100 epochs. Table~\ref{tab:tmc-hyperparameters} shows the model architectures, training batch size and training epoch of our vanilla neural DNF models. We set the pruning threshold $\epsilon=0.005$ in the post-training pipeline.

\begin{table}[h]
\begin{tabular}{c|ccc}
Dataset & In x Conj x Out & Batch size & Epoch \\ \hline
TMC-3   & 59 * 9 * 3      & 4          & 300   \\
TMC-5   & 60 * 15 * 5    & 16          & 300   \\
TMC-10  & 34 * 30 * 10    & 32         & 300   \\
TMC-15  & 80 * 45 * 15    & 32         & 300   \\
TMC-22  & 103 * 66 * 22    & 32        & 300 
\end{tabular}
\caption{Vanilla neural DNF model's architecture, training batch size and training epoch in TMC experiments.}
\label{tab:tmc-hyperparameters}
\end{table}

The MLP in each experiment has the same architectural design as the vanilla neural DNF and training settings, but each layer is a normal feed-forward linear layer instead of the semi-symbolic layer.

\subsection{Synthetic Datasets Experiments} \label{app:synthetic}

We generate synthetic multi-class and multi-label datasets to show that the neural DNF-based models are capable of learning rules from datasets with underlying patterns. For each synthetic dataset, we first randomly generated a set of rules that guarantee mutual exclusivity if under a multi-class setting. Then, 10,000 samples are generated based on the rule set without noise. 2,000 of the samples are randomly selected to form the test set, 1,600 for the validation set and 6,400 for the train set. Following the same training and post-training process but without the tuning stage, we evaluate a neural DNF-based model on each of the synthetic datasets. The result is shown in Table~\ref{tab:synthetic-test} below. We see that our neural DNF-based models are learning well after training in all experiments, while we still observe the problem of multi-class classification performance drop after thresholding.

\begin{table}[h]
\begin{tabular}{c|cccc}
Synthetic Dataset + Model + Metric                                                                 & After train & After prune & After threshold & Extracted Rules \\ \hline
\begin{tabular}[c]{@{}c@{}}Multi-class 3 classes +\\ Neural DNF-EO +\\ Jaccard score\end{tabular}  & 1.000       & 1.000`      & 0.981           & 0.974           \\
\begin{tabular}[c]{@{}c@{}}Multi-class 25 classes +\\ Neural DNF-EO +\\ Jaccard score\end{tabular} & 0.995       & 0.991       & 0.227           & 0.226           \\
\begin{tabular}[c]{@{}c@{}}Multi-label 3 labels +\\ Vanilla Neural DNF +\\ Macro F1\end{tabular}   & 1.000       & 0.995       & 0.995           & 0.995           \\
\begin{tabular}[c]{@{}c@{}}Multi-label 25 labels +\\ Vanilla Neural DNF +\\ Macro F1\end{tabular}  & 1.000       & 0.967       & 0.987           & 0.987          
\end{tabular}
\caption{Neural DNF-based models' performance on different synthetic datasets at different stages of the pipeline. All performance is measured on the test set.}
\label{tab:synthetic-test}
\end{table}

 These synthetic experiments can be found under the \mintinline{bash}{synthetic_test/notebooks/} folder in the two GitHub repositories we mentioned in Appendix~\ref{app:code}.
