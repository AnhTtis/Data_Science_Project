\section{Experiment Results}

We evaluate our neural DNF-based models in terms of classification performance, scalability, and interpretability. The classification task can be treated as a propositional ILP task with no background knowledge. We include a two-layer MLP as a baseline for comparison to a pure neural method, and a state-of-the-art symbolic learner FastLAS \cite{fastlas} for direct comparison between symbolic and neuro-symbolic approaches. We also compare our models against a decision tree since it is a simple classifier that also provides interpretability.

\subsection{CUB dataset/subsets}

For multi-class classification tasks, we use the Jaccard index score to measure the correct prediction of a single class at a time in case our neural DNF-EO model outputs multiple classes for one sample. If the model predicts exactly one class, the Jaccard score is of value 1; but if two or more classes are predicted, the Jaccard score will be less than 1. In each CUB experiment, we train the neural DNF-EO model with a cross-entropy loss function and let the constraint layer enforce mutual exclusivity. At inference time, we remove the constraint layer (layer C2 in Figure~\ref{fig:dnf-eo}) from the neural DNF-EO model and keep only the plain neural DNF. The plain neural DNF outputs a value in the $[-1, 1]$ range for each class, and we take the class as True if the output value is greater than 0. Table~\ref{tab:dnf-eo-cub} shows the performance of the neural DNF-EO model after training and at different stages of the post-training pipeline. The plain neural DNF performs correctly with mutual exclusion enforced on all test sets, as shown in the `\textit{Train Jacc. (no C2) column}' in Table~\ref{tab:dnf-eo-cub}. It also provides ASP rules like in Table~\ref{tab:dnf-eo-rules-cub-3} after the post-training pipeline. However, after the CUB-20 subset, the performance difference between a thresholded plain neural DNF (`\textit{Threshold Jacc. (no C2)}' column) and the original after training (`\textit{Train Jacc. (no C2)}' column) increases along with the number of classes in the subset. Upon close inspection, we believe the issue is about maintaining mutual exclusivity with an increasing number of classes using the current thresholding process.

\begin{table}[h]
\begin{tabular}{c|cccccc}
Dataset & \begin{tabular}[c]{@{}c@{}}Train\\ Acc.\end{tabular} & \begin{tabular}[c]{@{}c@{}}Train Jacc.\\ (no C2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Prune Jacc.\\ (no C2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Finetune\\ Jacc.\\ (no C2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Threshold\\ Jacc.\\ (no C2)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Rules\\ Jacc.\end{tabular} \\ \hline
CUB-3   & 1.000                                                & 1.000                                                         & 1.000                                                         & 1.000                                                              & 1.000                                                               & 1.000                                                 \\
CUB-10  & 1.000                                                & 1.000                                                         & 1.000                                                         & 1.000                                                              & 1.000                                                               & 1.000                                                 \\
CUB-15  & 1.000                                                & 1.000                                                         & 1.000                                                         & 1.000                                                              & 1.000                                                               & 1.000                                                 \\
CUB-20  & 1.000                                                & 1.000                                                         & 1.000                                                         & 1.000                                                              & 1.000                                                               & 1.000                                                 \\
CUB-25  & 1.000                                                & 1.000                                                         & 1.000                                                         & 1.000                                                              & 0.935                                                               & 0.933                                                 \\
CUB-50  & 1.000                                                & 1.000                                                         & 0.982                                                         & 0.975                                                              & 0.641                                                               & 0.636                                                 \\
CUB-100 & 1.000                                                & 1.000                                                         & 0.967                                                         & 0.978                                                              & 0.630                                                               & 0.632                                                 \\
CUB-200 & 1.000                                                &  1.000                                                        & 0.967                                                         & 0.948                                                               & 0.225                                                              & 0.225
\end{tabular}
\caption{\textbf{Neural DNF-EO model} trained with \textbf{cross-entropy loss} in different CUB subsets/dataset, at different stages of training and post-training pipeline. All metrics are calculated by averaging across the test set. (`Acc.' short for accuracy, `Jacc.' short for Jaccard index score)}
\label{tab:dnf-eo-cub}
\end{table}

\begin{table}[h]
\begin{minipage}{.4\linewidth}
\begin{minted}{text}
black_footed_albatross :-
  not has_bill_colour_black,
  not has_wing_pattern_solid.
laysan_albatross :-
  has_crown_colour_white.
sooty_albatross :-
  not has_bill_colour_buff,
  has_crown_colour_black,
  not has_crown_colour_white.
\end{minted}
\end{minipage}%
\hspace{0.05\linewidth}%
\begin{minipage}{0.55\linewidth}
\raggedleft
\begin{tabular}{c|ccc}
Dataset & MLP   & \begin{tabular}[c]{@{}c@{}}Decision Tree\\ (no pruning)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Neural DNF-EO\\ without C2\\ (after training)\end{tabular} \\ \hline
CUB-3   & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-10  & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-15  & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-20  & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-25  & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-50  & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-100 & 1.000 & 1.000                                                                & 1.000                                                                               \\
CUB-200 & 1.000 & 1.000                                                                & 1.000                                                                              
\end{tabular}
\end{minipage}
\par
\begin{minipage}[t]{.4\linewidth}
\caption{ASP rules extracted from a trained neural\\DNF-EO model on CUB-3 experiment.}
\label{tab:dnf-eo-rules-cub-3}
\end{minipage}%
\hspace{0.05\linewidth}%
\begin{minipage}[t]{0.55\linewidth}
\caption{Average test accuracy comparison among MLP, decision\\tree (no pruning) and trained neural DNF-EO model (no\\constraint layer C2, no post-training processing), across\\different CUB subsets/dataset.}
\label{tab:cub-acc-comp}
\end{minipage}
\end{table}

\noindent \textbf{Comparison to baselines}\ \ \ \ For the baseline methods, we train the two-layer MLP identically as our neural DNF-EO model but without the $\delta$ parameter adjustment. We specify FastLAS's scoring function to be on body length so that FastLAS learns the most compact rules.

\textbf{Accuracy comparison}\ \ \ \ We measure the accuracy of an MLP and a trained plain neural DNF (without the constraint layer C2) by taking the $\arg \max$. A trained decision tree gives one class as the prediction after following the decision path and reaching the leaf node. All three models perform equally perfectly across all experiments, as shown in Table~\ref{tab:cub-acc-comp}.


\textbf{Jaccard index score and scalability comparison}\ \ \ \ We interpret the $\tanh$ output of the plain neural DNF to be True if the value is greater than 0, giving the possibility of multiple class being True at the same time. Both the ASP rules learned by FastLAS and the ASP rules extracted from the plain neural DNF after the post-training pipeline can give more than 1 class in an answer set. Thus we choose the Jaccard index score as a measurement of mutual exclusivity for comparing these three approaches. We also attempt to interpret the 2-layer MLP to see if there is any mutual exclusivity at all: we expect some similarity between the MLP and our neural DNF-EO model as both models have $\tanh$ as the activation function at hidden layers and their weights are initialised the same. Thus our attempt at interpreting the MLP follows the same way as we interpret our neural DNF-EO model by thresholding at 0 after $\tanh$. However, as discussed in Chapter~\ref{chap:cross-entropy-and-multi-class-classification}, softmax only pushes the MLP to have different output logits and no need to push the logits towards a threshold such that they have symbolic meaning. Thus we expect the MLP to have poor Jaccard index scores. Table~\ref{tab:cub-jacc-comp} shows the average test Jaccard score of all three (neuro-)symbolic approaches as well as our symbolic interpretation attempt on MLP. As expected, the MLP displays little to no symbolic mutual exclusivity for tasks with 10 classes and onwards. FastLAS learns perfect mutually-exclusive rules for all subsets but fails to learn from the full dataset of CUB-200, where it runs out of memory when computing the opt-sufficient search space. Our trained plain neural DNF without any post-training processing learns perfectly in all settings regardless of the number of classes. This shows that our model provides mutual exclusion enforcement just like FastLAS, but also better scalability as it does not suffer from hypothesis search space growth. However, we cannot always extract well-performed rules from it, as there is a performance gap between the rules and the plain neural DNF without constraint layer C2 from CUB-25 and onwards.

\begin{table}[h]
\begin{tabular}{c|ccc|c}
Dataset & FastLAS & \begin{tabular}[c]{@{}c@{}}Neural DNF-EO without C2\\ (after training)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Rules Extracted from\\ Neural DNF-EO without C2\end{tabular} & \begin{tabular}[c]{@{}c@{}}MLP\\ (0-thresholded)\end{tabular} \\ \hline
CUB-3   & 1.000   & 1.000                                                                               & 1.000                                                                                   & 1.000                                                         \\
CUB-10  & 1.000   & 1.000                                                                               & 1.000                                                                                   & 0.289                                                         \\
CUB-15  & 1.000   & 1.000                                                                               & 1.000                                                                                   & 0.168                                                         \\
CUB-20  & 1.000   & 1.000                                                                               & 1.000                                                                                   & 0.114                                                         \\
CUB-25  & 1.000   & 1.000                                                                               & 0.933                                                                                   & 0.092                                                         \\
CUB-50  & 1.000   & 1.000                                                                               & 0.636                                                                                   & 0.044                                                         \\
CUB-100 & 1.000   & 1.000                                                                               & 0.632                                                                                   & 0.020                                                         \\
CUB-200 & N/A     & 1.000                                                                               & 0.225                                                                                   & 0.010                                                        
\end{tabular}
\caption{Average test Jaccard index score comparison across different CUB subsets/dataset, for FastLAS, trained neural DNF-EO model without constraint layer C2 (no post-training processing), the rules extracted from post-training processed neural DNF-EO model without layer C2, and an MLP if interpreted true and false with a threshold of 0. As we have discussed above, MLP with cross-entropy does not provide any mutual exclusivity guarantee in its output and it is expected to see MLP fail to perform when we try to interpret it under a threshold. FastLAS runs out of memory for CUB-200.}
\label{tab:cub-jacc-comp}
\end{table}

\textbf{Interpretability comparison}\ \ \ \ We use the average and maximum rule length as a measurement of interpretability. To measure the rule length of a decision tree, we treat each decision path to a leaf node as the body of a rule, and the number of steps for a decision path can be seen as the number of body atoms. Figure~\ref{fig:cub-rules-comp} shows the comparison of average and maximum rule length across FastLAS' rules, rules extracted from neural DNF-EO model (without layer C2), decision trees without pruning, and pruned decision trees with 80\% accuracy on train sets. FastLAS learns the most compact rules but has scalability issues with a large hypothesis search space, as it fails to learn with 200 classes present. While the maximum rule length for rules extracted from the plain neural DNF is the highest in most experiments, the average length increases linearly compared to decision trees' exponential growth. Rules from our neural DNF-EO are more compact than the decision tree in half of the settings, especially in experiments with large numbers of classes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figure/CUB-rule-comparison.pdf}
    \caption{Average rule length and maximum rule length comparison across rules learned from FastLAS, extracted rules from trained neural DNF-EO model and decision tree (both pruned and without pruning). The pruned decision trees still maintain an accuracy of 80\% in train sets. FastLAS runs out of memory for CUB-200.}
    \label{fig:cub-rules-comp}
\end{figure}

\FloatBarrier

\subsection{TMC dataset/subsets} \label{sec:tmc-experiments}

We use macro F1 as the metric for the performance of vanilla neural DNF models and the extracted rules. We train the vanilla neural DNF model with binary cross-entropy across different TMC subsets/dataset, and the classification performance is shown in Table~\ref{tab:dnf-tmc}. The results suggest that our vanilla neural DNF model struggles to learn when more labels are present in the subsets/dataset. However, we speculate that the lack of general patterns/regularities in the TMC dataset/subsets could be the reason for its poor performance. We further discuss this issue in the following section where we compare our model to other baselines. On the other hand, we observe that the rules extracted through the post-training process show very similar performance as the corresponding trained vanilla neural DNF model. Compared to CUB experiments, the thresholding process does not need to maintain mutual exclusion across different outputs and thus does not cause a significant performance drop when `discretising' the continuous weights.

\begin{table}[h]
\begin{tabular}{c|cccccc}
Dataset & \begin{tabular}[c]{@{}c@{}}Train\\ macro F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Prune\\ macro F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Finetune\\ macro F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Threshold\\ macro F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Rules\\ macro F1\end{tabular} \\ \hline
TMC-3   & 0.745                                                    & 0.764                                                    & 0.739                                                       & 0.754                                                        & 0.754                                                    \\
TMC-5   & 0.615                                                    & 0.649                                                    & 0.648                                                       & 0.635                                                        & 0.635                                                    \\
TMC-10  & 0.391                                                    & 0.380                                                    & 0.370                                                       & 0.362                                                        & 0.362                                                    \\
TMC-15  & 0.177                                                    & 0.143                                                    & 0.100                                                       & 0.097                                                        & 0.097                                                    \\
TMC-22  & 0.094                                                    & 0.086                                                        & 0.093                                                           & 0.094                                                            & 0.094                                                       
\end{tabular}
\caption{\textbf{Vanilla neural DNF} trained with \textbf{binary cross entropy} in different TMC subsets/dataset, at different stages of the training pipeline. All metrics are computed on the test set.}
\label{tab:dnf-tmc}
\end{table}

\noindent \textbf{Comparison to baselines} \label{sec:tmc-experiments-baseline-comparison}\ \ \ \ We report the test macro F1 score of the 2-layer MLP trained with binary cross-entropy loss and the decision tree (without limiting depth) in Table~\ref{tab:mlp-dt-tmc}. There is an increasing performance gap between our vanilla neural DNF model and the MLP as the number of labels increases. Figure~\ref{fig:tmc-dnf-loss} shows the change in macro F1 and $\delta$ magnitude in the vanilla neural DNF model's symbolic layers over 300 epochs of training on TMC-22, and we see that the model achieves a similar macro F1 (around 0.45 compared to MLP's 0.545) when the logical biases are not heavily enforced. But as the $\delta$ magnitude increases, especially over 0.5, the performance of the model drops and never recovers. This suggests that the logical biases in the semy-symbolic layers make it more difficult for the vanilla neural DNF model to learn.

\begin{table}[h]
\begin{minipage}{.43\linewidth}
\begin{tabular}{c|cc}
Dataset & \begin{tabular}[c]{@{}c@{}}MLP + BCE\\ Macro F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Decision Tree\\ (no pruning)\\ Macro F1\end{tabular} \\ \hline
TMC-3   & 0.817                                                        & 0.925                                                                           \\
TMC-5   & 0.746                                                        & 0.894                                                                           \\
TMC-10  & 0.539                                                        & 0.825                                                                           \\
TMC-15  & 0.538                                                        & 0.863                                                                           \\
TMC-22  & 0.545                                                        & 0.865                                                                          
\end{tabular}
\end{minipage}%
\begin{minipage}{.57\linewidth}
\includegraphics[width=\linewidth]{figure/TMC-22-loss-plot.pdf}
\end{minipage}
\par
\begin{minipage}[t]{.43\linewidth}
\captionof{table}{Macro F1 score of MLP trained with\\binary cross entropy and decision tree\\without limiting depth on TMC test sets.}
\label{tab:mlp-dt-tmc}
\end{minipage}%
\begin{minipage}[t]{.57\linewidth}
\captionof{figure}{Macro F1 and $\delta$ value of vanilla neural DNF on TMC-22\\over 300 epochs of training. The drop in macro F1 occurs\\around $\delta = 0.5$.}
\label{fig:tmc-dnf-loss}
\end{minipage}
\end{table}

On the other hand, the decision tree learns significantly better than both our vanilla neural DNF model and MLP in all subsets/dataset, as shown in Table~\ref{tab:mlp-dt-tmc}. This suggests that there are very few general patterns to learn in this dataset and that the decision is overfitting to the data. For the neural DNF-based model, if we allow an infinite number of conjunctions (i.e. infinite width of the conjunctive layer), we can encode every possible rule body for a label to cover all cases. But if we do so, the model will overfit: it will learn individual rules that are only `fired' for a small number of data points. The same applies to MLP: an MLP with a wide hidden layer approximates a more complex function that might overfit than an MLP with a small hidden layer. By limiting the number of conjunctions/the width of the hidden layer, the model learns to generalise among large quantities of samples. If the data have very few general patterns, meaning they cannot be encoded using features with low dimensionality, the model will need a wider hidden layer to increase the feature space's dimension and the chance of overfitting increases. The decision tree splits based on input space, and if not limiting the maximum depth of the tree, it can encode every case with nested if-else clauses and overfits the data. To see the difference in model size, we compare the number of trainable parameters of the MLP/vanilla neural DNF model and the number of non-leaf nodes (split nodes) in a decision without limiting depth in Table~\ref{tab:tmc-param-comp}. The decision tree without depth limit has more split nodes (parameters) than its corresponding MLP/vanilla neural DNF in all the experiments. And when we limit the decision tree's max depth to 15 in TMC-22, the tree has 2810 non-leaf nodes and the test macro F1 drops to 0.565 which is close to MLP's performance, confirming that the decision tree without a limit on depth is overfitting. This also shows that the dataset is too complex to learn without increasing the parameters.

The last baseline approach, FastLAS, fails in \textbf{every} TMC subsets/dataset, suffering from the same memory issue as the CUB-200 experiment. Without a mutual exclusivity constraint, the hypothesis search space is exponentially larger since every label can be fired at the same time, causing the memory issue for FastLAS.

\begin{table}[h]
\begin{tabular}{c|cc|cc}
Dataset & \begin{tabular}[c]{@{}c@{}}MLP/Vanilla Neural DNF\\ Architecture\\ (Input, Hidden, Output)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Num. Parameters\\ (Excluding bias)\end{tabular} & \begin{tabular}[c]{@{}l@{}}Decision Tree\\ Max. depth\end{tabular} & No. Non-leaf Nodes \\ \hline
TMC-3   & (59, 9, 3)                                                                                              & 558                                                                        & 54                                                                 & 1461               \\
TMC-5   & (60, 15, 5)                                                                                             & 975                                                                        & 50                                                                 & 3436               \\
TMC-10  & (34, 30, 10)                                                                                            & 1320                                                                       & 34                                                                 & 7715               \\
TMC-15  & (80, 34, 15)                                                                                            & 4275                                                                       & 67                                                                 & 9679               \\
TMC-22  & (103, 66, 22)                                                                                           & 8250                                                                       & 66                                                                 & 11878             
\end{tabular}
\caption{MLP and vanilla neural DNF model's trainable parameters count, comparing to the number of non-leaf nodes in a decision tree without limiting depth, in different TMC subsets/dataset. Non-leaf nodes of a decision tree can be treated as parameters of the decision tree since it encodes a split condition that directs the decision path.}
\label{tab:tmc-param-comp}
\end{table}

\FloatBarrier

