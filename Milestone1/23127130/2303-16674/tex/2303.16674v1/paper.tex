%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
% twocolumn,
% hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support 
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage[frozencache=true,cachedir=_minted-paper]{minted}
\usepackage{caption}
\usepackage[section]{placeins}
\usepackage{wrapfig}
\usepackage{subcaption}

%% auto break lines
\lstset{breaklines=true}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2023}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{In A. Martin, K. Hinkelmann, H.-G. Fill, A. Gerber, D. Lenat, R. Stolle, F. van Harmelen (Eds.), 
Proceedings of the AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering (AAAI-MAKE 2023), Hyatt Regency, San Francisco Airport, California, USA, March 27-29, 2023.}

%%
%% The "title" command
\title{Neuro-symbolic Rule Learning in Real-world Classification Tasks}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author[1]{Kexin Gu Baugh}[%
email=kexin.gu17@imperial.ac.uk,
orcid=0000-0003-3680-8442,
]

\author[1]{Nuri Cingillioglu}[%
email=nuric@imperial.ac.uk,
]

\author[1]{Alessandra Russo}[%
email=a.russo@imperial.ac.uk,
]

\address[1]{Imperial College London}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Neuro-symbolic rule learning has attracted lots of attention as it offers better interpretability than pure neural models and scales better than symbolic rule learning. A recent approach named pix2rule proposes a neural Disjunctive Normal Form (neural DNF) module to learn symbolic rules with feed-forward layers. Although proved to be effective in synthetic binary classification, pix2rule has not been applied to more challenging tasks such as multi-label and multi-class classifications over real-world data. In this paper, we address this limitation by extending the neural DNF module to (i) support rule learning in real-world multi-class and multi-label classification tasks, (ii) enforce the symbolic property of mutual exclusivity (i.e. predicting exactly one class) in multi-class classification, and (iii) explore its scalability over large inputs and outputs. We train a \textit{vanilla neural DNF model} similar to pix2rule's neural DNF module for multi-label classification, and we propose a novel extended model called \textit{neural DNF-EO} (Exactly One) which enforces mutual exclusivity in multi-class classification. We evaluate the classification performance, scalability and interpretability of our neural DNF-based models, and compare them against pure neural models and a state-of-the-art symbolic rule learner named FastLAS. We demonstrate that our neural DNF-based models perform similarly to neural networks, but provide better interpretability by enabling the extraction of logical rules. Our models also scale well when the rule search space grows in size, in contrast to FastLAS, which fails to learn in multi-class classification tasks with 200 classes and in all multi-label settings. \end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  neuro-symbolic rule learning \sep
  neuro-symbolic learning \sep
  neuro-symbolic AI \sep
  differentiable inductive logic programming
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\input{sections/introduction}

\input{sections/background}

\input{sections/models}

\input{sections/dataset}

\input{sections/experiments}

\input{sections/analysis}

\input{sections/related}

\input{sections/conclusion}


%%
%% The acknowledgments section is defined using the "acknowledgments" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acknowledgments}
%   Thanks to the developers of ACM consolidated LaTeX styles
%   \url{https://github.com/borisveytsman/acmart} and to the developers
%   of Elsevier updated \LaTeX{} templates
%   \url{https://www.ctan.org/tex-archive/macros/latex/contrib/els-cas-templates}.  
% \end{acknowledgments}

%%
%% Define the bibliography file to be used
\clearpage
\bibliography{sample-ceur}

%%
%% If your work has an appendix, this is the place to put it.
\clearpage
\input{sections/appendix.tex}

\end{document}

%%
%% End of file
