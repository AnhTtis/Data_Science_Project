\section{Experiments}

In this section, we first describe our experimental setup for the evaluation of our method. Then, we compare the proposed method with baselines in terms of visual quality, 3D consistency, and computational efficiency. Moreover, we provide an ablation study and a discussion on the benefits and trade-offs of the proposed method.

\subsection{Datasets}\label{sec:dataset}
Following EG3D~\cite{chan2021pi}, we evaluate our method on three different datasets:

\noindent\textbf{Flickr-Faces-HQ (FFHQ)~\cite{Karrasstyle}}: a collection of 70k high-quality images of real-world human faces, as well as corresponding approximate camera extrinsics estimated using an of-the-shell pose estimator.

\noindent\textbf{AFHQ Cats}: a sub-category of the Animal-Face-HQ (AFHQ)~\cite{choi2020stargan}, consisting of around 5k high-quality images of cat faces, as well as corresponding camera extrinsics estimated using an of-the-shell pose estimator.

\noindent\textbf{ShapeNet Cars:} a category of ShapeNet~\cite{chang2015shapenet} consisting of synthetic images of cars rendered from different viewpoints, as well as the corresponding camera extrinsics annotations.


\subsection{Baselines}\label{sec:baseline}
We consider EG3D~\cite{chan2022efficient} and The method proposed in SURF-GAN~\cite{kwak2022injecting} as our main baselines for this study. For a more complete evaluation, we also compare our method to additional relevant baselines:

\noindent\textbf{EG3D~\cite{chan2022efficient}}: the NeRF-GAN used for distilling 3D knowledge in the convolutional generator. EG3D serves as the upper bound for the 3D consistency of the proposed method.

\noindent\textbf{Pose-Conditioned StyleGAN (PC-GAN)}: a standard conditional 2D GAN, conditioned on the pose annotations without any knowledge distillation.

\noindent\textbf{SURF}: Inspired by the proposed method in SURF-GAN~\cite{kwak2022injecting}, we create a baseline called SURF, where multi-view images of EG3D are used to discover pose-control in a 2D StyleGAN (pretrained on FFHQ) based on the proposed method in~\cite{kwak2022injecting}. The details of the implementation can be found in the supplementary material\footnote{As the official implementation of SURF-GAN does not include their method for pose control in 2D GANs, we provide our best attempt in implementing their method.}.

\noindent\textbf{LiftGAN~\cite{liftgan}}: a method predating EG3D and SURF baselines, based on differentiable rendering for distilling 2D GANs to train a 3D generator.

\subsection{Implementation and Evaluation Details}\label{sec:detail}
We implement and evaluate the proposed generator using both StyleGAN2 (ST2)~\cite{Karras2020ada} and StyleGAN3 (ST3)~\cite{karras2021alias} architectures. For the pretrained NeRF-GAN, we use the official models from EG3D~\cite{chan2022efficient} (for Shapenet Cars, we re-train the model as the official model does not match the results reported by EG3D). We train our experiments using a batch size of 16. The rendering resolution and the final resolution are (128, 512) for FFHQ and AFHQ and (64, 128) for Shapenet Cars. Both training and inference experiments were conducted using NVidia RTX 3090 GPUs. In all experiments, we set all of the weights of reconstruction loss terms ($\lambda^{LR}_{smooth_l1}$, $\lambda^{LR}_{perc}$, $\lambda^{HR}_{perc}$) to the value 1 and the weight of the adversarial loss ($\lambda_{adv}$) to the value 0.1.

\subsection{Metrics}\label{sec:metric}
We evaluate our method quantitatively in terms of visual quality, and 3D consistency.

\noindent\textbf{Fr√©chet Inception Distance (FID)~\cite{heusel2017fid}}: The most common metric to assess the quality and diversity of generation.

\noindent\textbf{Kernel Inception Distance (KID)~\cite{heusel2017fid}}: An unbiased alternative to FID for smaller datasets.

\noindent\textbf{Pose Accuracy (PA)}: following previous works~\cite{chan2022efficient}, we measure the ability of the model in generating images of the query poses by calculating the mean squared error (MSE) between the query poses and the pose of the generated images, estimated using an of-the-shelf pose estimator~\cite{deng2019accurate}.

\noindent\textbf{Identity Preservation (ID)}: As a metric for 3D consistency, we measure the degree of face identity preservation between different viewpoints with respect to the canonical pose using ArcFace~\cite{deng2018arcface} cosine similarity for the FFHQ setup.

\noindent\textbf{3D Landmark Consistency}: As another 3D consistency metric, we measure the change in facial landmarks between different viewpoints in FFHQ using MSE. The 3D landmarks are estimated using an off-the-shelf estimator~\cite{deng2019accurate}.

\subsection{Quantitative Comparison}
In the following, we quantitatively compare the proposed method with the baselines described in Sec.~\ref{sec:baseline} in terms of inference efficiency, visual quality, and 3D consistency.

\subsubsection{Efficiency}
The efficiency of fully-convolutional networks compared to the rendering-based method is well-known. To better assess the practical benefit of the proposed method, we provide a comparison of inference efficiency between EG3D. Fig~\ref{fig_main} visualizes an example of the inference memory consumption and speed of the two methods using different batch sizes on a fixed GPU budget (in this case, on RTX 3090 GPU with 24G of memory). 
% To simulate a more realistic setup, where batch-wise generation is used for multi-frame video generation, we fix the objects across the batches and only change the viewpoints. We also allow volumetric rendering to reuse the tri-planes generated for the first batch for the subsequent ones for a fair comparison. 
As shown, EG3D is restricted to small batch sizes (a maximum of 14) due to its costly memory consumption, where our method can scale up to a maximum of 96 samples per batch.
As for speed, our convolutional generator achieves better frame-per-second, especially when using StyleGAN2 as its backbone.

\subsubsection{Image Quality}
To assess the trade-off brought about by our convolutional generator, we evaluate the quality of the generated images. Table~\ref{tab_fid} shows the FID and KID scores for our method and the baseliness on different datasets. Compared to the PC-GAN and SURF baselines, our method constantly achieves higher quality. This confirms that exploiting the style space of the pretrained NeRF-GAN contributes to the ability of the convolutional renderer in pose-conditioned generation. Although our method does not fully match the visual quality of EG3D, it is still able to fairly maintain high image quality and significantly reduce the compromise in the quality compared to the other convolutional counterparts.
\begin{table}[t]
\caption{Comparison of image quality on three datasets in terms of FID and KID metrics. *The value is borrowed from~\cite{liftgan}.}%
\label{tab_fid}%
\centering%
\resizebox{0.98\linewidth}{!}{%
\begin{tabular}{lllllll}
\toprule
\multicolumn{1}{l}{\bf Method} &\multicolumn{2}{c}{\bf FFHQ} &\multicolumn{2}{c}{\bf AFHQ} &\multicolumn{2}{c}{\bf ShapeNET Cars} \\ 
\multicolumn{1}{l}{} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{KID $\downarrow$} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{KID $\downarrow$} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{KID $\downarrow$} \\
\midrule
\multicolumn{1}{l}{EG3D~\cite{chan2022efficient}} &\multicolumn{1}{c}{5.0} &\multicolumn{1}{c}{0.0018} &\multicolumn{1}{c}{2.9} &\multicolumn{1}{c}{0.0003} &\multicolumn{1}{c}{3.5} &\multicolumn{1}{c}{0.0017} \\
\midrule
\multicolumn{1}{l}{PC-GAN}  &\multicolumn{1}{c}{19.3} &\multicolumn{1}{c}{0.0085} &\multicolumn{1}{c}{4.5} &\multicolumn{1}{c}{0.0009} &\multicolumn{1}{c}{6.1} &\multicolumn{1}{c}{0.0018} \\
\multicolumn{1}{l}{LiftGAN~\cite{liftgan}}  &\multicolumn{1}{c}{29.8*} &\multicolumn{1}{c}{-}  &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} \\
\multicolumn{1}{l}{SURF}  &\multicolumn{1}{c}{31.1} &\multicolumn{1}{c}{0.0153} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} \\
\multicolumn{1}{l}{Ours (ST2)}  &\multicolumn{1}{c}{6.6} &\multicolumn{1}{c}{0.0019} &\multicolumn{1}{c}{3.8} &\multicolumn{1}{c}{0.0011} &\multicolumn{1}{c}{3.1} &\multicolumn{1}{c}{0.0013} \\
\multicolumn{1}{l}{Ours (ST3)}  &\multicolumn{1}{c}{6.8} &\multicolumn{1}{c}{0.0023} &\multicolumn{1}{c}{3.2} &\multicolumn{1}{c}{0.0007} &\multicolumn{1}{c}{3.1} &\multicolumn{1}{c}{0.0012} \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\end{table}


\subsubsection{3D Consistency}
While, unlike volumetric rendering, 2D convolutions do not guarantee 3D consistency, we show that our approach achieves a good performance in this regard. We assess the 3D consistency of generated images on FFHQ by measuring the pose accuracy, 3D landmark consistency, and face identity preservation, as discussed in Sec. \ref{sec:metric}, which are provided in Table \ref{tab_3d}. Based on the results, Our method achieves comparable 3D consistency with EG3D, while PC-GAN and SURF struggle. Note that the high values for identity preservation and 3D landmark consistency in SURF are due to the limited pose variations, and hence generating similar images regardless of the input pose, as reflected by the pose accuracy (and the visual examples in Fig.~\ref{fig_vis_ffhq}). 
\begin{table}[t]
    \caption{Comparison of 3D consistency metrics on FFHQ.}
    \label{tab_3d}%
    \centering
        \resizebox{0.75\linewidth}{!}{%
        \begin{tabular}{lllll}
        \toprule
        \multicolumn{1}{l}{\bf Method} &\multicolumn{1}{c}{\bf Pose Acc. $\downarrow$} &\multicolumn{1}{c}{\bf 3D Landmark $\downarrow$} &\multicolumn{1}{c}{\bf ID $\uparrow$} \\ 
        \midrule
        \multicolumn{1}{l}{EG3D~\cite{chan2022efficient}} &\multicolumn{1}{c}{0.002} &\multicolumn{1}{c}{0.018} &\multicolumn{1}{c}{0.75}\\
        \midrule
        \multicolumn{1}{l}{PC-GAN} &\multicolumn{1}{c}{0.009} &\multicolumn{1}{c}{0.062} &\multicolumn{1}{c}{0.56}\\
        \multicolumn{1}{l}{SURF} &\multicolumn{1}{c}{0.044} &\multicolumn{1}{c}{0.014} &\multicolumn{1}{c}{0.86}\\
        \multicolumn{1}{l}{Ours (ST2)} &\multicolumn{1}{c}{0.002} &\multicolumn{1}{c}{0.023}  &\multicolumn{1}{c}{0.75}\\
        \multicolumn{1}{l}{Ours (ST3)} &\multicolumn{1}{c}{0.002} &\multicolumn{1}{c}{0.022}  &\multicolumn{1}{c}{0.75}\\
        \midrule
        \end{tabular}} 
        \vspace{-5mm}

\end{table}


\begin{figure*}
\begin{center}
\includegraphics[width=0.92\linewidth]{figures/vis_ffhq.pdf}
\end{center}
\vspace{-4mm}
\caption{Qualitative examples of variations in yaw and pitch for FFHQ. Compared to the pose-conditioned GAN and SURF baseline, our proposed method nearly matches the 3D consistency and image quality of volumetric rendering (EG3D).}
\label{fig_vis_ffhq}
\vspace{-3mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[, width=0.92\linewidth]{figures/vis_cat.pdf}
\end{center}
\vspace{-4mm}
\caption{Qualitative examples of variations in yaw and pitch for AFHQ cats. In line with our quantitative experiments, the pose-conditioned convolutional baseline (PC-GAN) fails to preserve the identity of the subject under different poses. In contrast, our method exhibits similar preservation of identity to the volume rendering approach (EG3D), despite the difference in computational resources and time.}
\label{fig_vis_cats}
\vspace{-4mm}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{figures/vis_cars.pdf}
\end{center}
\vspace{-4mm}
\caption{Qualitative examples of different camera poses in Shapenet Cars for three different car models.}
\label{fig_vis_cars}
\vspace{-4mm}
\end{figure*}

\subsection{Ablation}\label{sec_ablation}
\noindent\textbf{Ablation on loss functions:} the proposed training objective in section~\ref{sec:training} consists of different loss terms to ensure both image quality and consistency with the output of volumetric rendering. In this section, we ablate the importance of these components, by adding them one by one to form the final objective of Eq.~\ref{eq:loss}. Table~\ref{tab_ablation} shows the FID scores for the following experiments on the loss terms on AFHQ dataset:
\begin{itemize}[noitemsep]
  \item \textbf{LR}: Only the low-resolution reconstruction loss. the super-resolution network is frozen.
  \item \textbf{HR}: Only the high-resolution reconstruction loss.
  \item \textbf{LR + HR} Full reconstruction loss on low-resolution and high-resolution.
  \item \textbf{HR + Adv}: reconstruction and adversarial losses on the high-resolution images.
  \item \textbf{Full (LR + HR + ADV)} Full training objective, including the reconstruction and adversarial terms.
\end{itemize}
As shown by the ablation study, the combination of the proposed loss terms leads to the best FID scores. 


\begin{table}[b]
\vspace{-3mm}
\caption{Ablation on different loss functions for training the convolutional renderer on AFHQ Cats dataset.}%
\label{tab_ablation}%
\centering%
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{ll}
\toprule
\multicolumn{1}{l}{\bf Method} &\multicolumn{1}{c}{\bf FID $\downarrow$} \\ 
\midrule
\multicolumn{1}{l}{LR} &\multicolumn{1}{c}{30.55}   \\
\multicolumn{1}{l}{HR} &\multicolumn{1}{c}{10.39}  \\
\multicolumn{1}{l}{LR + HR} &\multicolumn{1}{c}{9.1}   \\
\multicolumn{1}{l}{HR + ADV} &\multicolumn{1}{c}{6.58}   \\
\multicolumn{1}{l}{Full (LR + HR + ADV)} &\multicolumn{1}{c}{3.2}   \\
\bottomrule
\end{tabular}}
\vspace{-2mm}
\end{table}

\begin{table}[b]
\caption{The effect of mixing real images and EG3D-rendered images as real examples for adversarial training, controlled by the parameter $\alpha$, on FFHQ dataset.}%
\label{tab_alpha}%
    \centering
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{lllll}
    \toprule
    \multicolumn{1}{l}{\bf Method} &\multicolumn{2}{c}{\bf Ours (ST2)} &\multicolumn{2}{c}{\bf Ours (ST3)}\\ 
    \multicolumn{1}{l}{} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{3D Landmark $\downarrow$} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{3D Landmark $\downarrow$} \\
    \midrule
    \multicolumn{1}{l}{$\alpha=0$}  &\multicolumn{1}{c}{5.5} &\multicolumn{1}{c}{0.027} &\multicolumn{1}{c}{5.7} &\multicolumn{1}{c}{0.027} \\
    \multicolumn{1}{l}{$\alpha=0.5$}  &\multicolumn{1}{c}{6.8} &\multicolumn{1}{c}{0.022} &\multicolumn{1}{c}{6.6} &\multicolumn{1}{c}{0.023}\\
    \bottomrule
    \end{tabular}}
\end{table}



\noindent\textbf{Single-stage v.s. two-stage training:} As mentioned in \ref{sec:training}, we find out that single-stage training by jointly optimizing for both reconstruction and adversarial losses results in subtle inconsistencies such as color shifts and geometry warps, which can be mitigated using the proposed 2-stage training in section~\ref{sec:method}. As the observed inconsistencies are difficult to capture using our quantitative 3D consistency metrics, we provide a visual comparison between the examples of single-stage and two-stage training on AFHQ in Fig. \ref{fig_stage}. To better visualize the inconsistencies, we provide a video visualization and comparison in the supplementary material.

\noindent\textbf{Mitigating Pose-Attribute Correlation:} In Table~\ref{tab_alpha}, we provide an ablation on the parameter $\alpha$ introduced in section~\ref{sec:training} for FFHQ dataset. As shown, including EG3D-generated images ($\alpha=0.5$) improves the 3D consistency at the cost of lower generation quality.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/ab_stage.pdf}
\end{center}
\vspace{-3mm}
\caption{one-stage training causes subtle color and geometry inconsistencies (first row). Such inconsistencies can be resolved using our proposed 2-stage training (second row).}
\vspace{-3mm}
\label{fig_stage}
\vspace{-1mm}
\end{figure}

\subsection{Qualitative Comparison}

In this section, We provide a visual comparison of our method with the baselines. In Fig.~\ref{fig_vis_ffhq} and ~\ref{fig_vis_cats}, we provide visual examples of variations in yaw and pitch for FFHQ and AFHQ Cats. Compared to a PC-GAN and SURF, our proposed method closely matches the 3D consistency and maintains the image quality of volumetric rendering. Fig.~\ref{fig_vis_cars} additionally provides examples of Shapenet Cars generated using our method and their corresponding images from EG3D. Similarly, our method exhibits preservation of 3D consistency and image quality, despite the difference in required computational resources. We provide more visual results in the supplementary material.

\subsection{Inversion, Interpolation, and Style Mixing}

As the proposed generator follows a StyleGAN architecture, it can easily benefit from most of the editing techniques common in the GANs' literature. Fig.~\ref{fig_edit} shows examples of inversion using Pivotal Tuning Inversion (PTI)~\cite{roich2022pivotal}, latent space interpolation, and style mixing. 

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/editing_v2.pdf}
\end{center}
\vspace{-4mm}
\caption{First row: Inversion using PTI ~\cite{roich2022pivotal}) for EG3D and our method; second row: interpolation in the latent space of our method; third row: style mixing in the latent space of our method.}
\label{fig_edit}
\vspace{-4mm}
\end{figure}

\subsection{Discussion: StyleGAN2 V.S. StyleGAN3}\label{arch}
StyleGAN2 is more computationally efficient than StyleGAN3. Based on the provided quantitative evaluations, our method reaches comparable image quality and 3D consistency with both architectures. However, StyleGAN2 is known to suffer from more texture stitching and artifacts~\cite{karras2021alias}, which we also observe in the generated images (visualizations are provided in the supplementary material).

\subsection{Correspondence between Convolutional and Volumetric Renderering}
As mentioned before, exploiting the style space of the pretrained NeRF-GAN also provides an opportunity for establishing a direct correspondence between the 3D representation of the 3D generator and the generated images using the convolutional generator. A close comparison of images generated using the convolutional and volumetric rendering in Figures~\ref{fig_vis_ffhq}, ~\ref{fig_vis_cats}, and, ~\ref{fig_vis_cars} indicates that the convolutional render is able to infer and match many attributes of the underlying 3D representation from the shared latent space and generate images similar in content to those of volumetric rendering. However, there still remains a gap in the full correspondence of the two rendering methods, as semantic and identity changes are visible between the corresponding images generated by the two methods. Investigating more explicit approaches for enforcing correspondence could be an interesting direction for improving the convolutional rendering for NeRF-GAN models. 

\label{sec:experiments}