\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\centering%
\includegraphics[width=0.94\linewidth]{figures/main.pdf}%
\vspace{-1.5mm}
\caption{Top: views of the same subject cat generated by a volumetric rendering generator (EG3D) and by our convolutional generator. Bottom: comparison of the inference memory consumption and speed (on a fixed GPU budget) for the two methods.}
\label{fig_main}%
\vspace{-4mm}
\end{figure}

Generative Adversarial Networks (GANs)~\cite{goodfellow2014gans} have undergone outstanding progress in photo-realistic image generation and manipulation in a variety of applications~\cite{brock2018large, Karras2019stylegan2, park2019SPADE, pix2pix2017, karras2018progressive, choi2020starganv2, Sauer2021ARXIV, Sauer2021ProjectedGC}.  Recently, there has been an increasing interest in extending GANs to the task of 3D-aware generation from single-view image datasets, with the goal of providing disentangled control over the content and the viewpoint of the generated images. 

Image GAN models have been historically based on convolutional architectures, enabling efficient training and generation for 2D tasks. However, pose-conditioned convolutional GANs (pcGANs) struggle with 3D-consistent image generation, due to their lack of sufficient 3D priors~\cite{HoloGAN2019}. Therefore, some studies have previously attempted to disentangle the pose from the content in pcGANs using explicit 3D supervision~\cite{rigstyle, KowalskiECCV2020Config, deng2020disco}, which, however, is not readily available for most datasets. As a result, later methods moved away from fully convolutional GANs by incorporating 3D inductive biases in the architecture and training pipeline, such as 3D neural representations and differentiable rendering methods~\cite{HoloGAN2019, BlockGAN2020, liftgan, pan2020gan2shape}.

The advent of Neural Radiance Fields (NeRFs)~\cite{mildenhall2020nerf} has recently transformed the neural 3D representation and the task of novel-view synthesis~\cite{nerftex, mipnerf, plenoxels, muller2022instant, nerfinthewild, nerfren, wu2021diver}. For this reason, NeRFs have been successfully integrated with GANs to achieve promising results in 3D-aware generation~\cite{Schwarz2020NEURIPS, chan2021pi, orel2022styleSDF, gu2021stylenerf, epigraf, niemeyer2021giraffe, schwarz2022voxgraf}. Nerf-GANs, in their general form, map a latent space to a 3D representation of objects and generate images from queried viewpoints using volumetric rendering. However, volumetric rendering is computationally demanding due to its ray-casting process, making high-resolution generation slow and memory-expensive. Recent works have proposed different approaches to improve the computational efficiency of NeRF-GANs using more efficient 3D representations~\cite{niemeyer2021giraffe, chan2021pi, schwarz2022voxgraf} and training protocols~\cite{chan2021pi, orel2022styleSDF}. Nevertheless, volumetric rendering remains an integral part of these models. 


In recent NeRF-GANs~\cite{niemeyer2021giraffe, chan2021pi, schwarz2022voxgraf, orel2022styleSDF}, convolutional networks have been reintroduced in the generator architecture as super-resolution networks or as 3D-representation generators, in order to scale up NeRF-GANs for high-resolution generation. In this study, we take a different approach to integrating NeRF-GANs and convolutional GANs for 3D-aware generation from single-view images. In particular, we investigate the capacity of convolutional generators to achieve 3D-consistent rendering with explicit pose control when learning from a pretrained NeRF-GAN without any additional explicit 3D supervision. A convolutional generator that fairly preserves the 3D consistency, image quality, and the correspondence between the generated images and the underlying 3D representation can be used for efficient multi-view inference in setups where volumetric rendering is not affordable, such as in mobile applications. However, balancing and minimizing the trade-off between efficiency and 3D consistency is a highly challenging task, which we set out to explore in this work. 

 
We propose a simple but effective method for distilling a pretrained NeRF-GAN into a pose-conditioned fully convolutional generator. The main component of our approach is based on exploiting the well-disentangled intermediate latent space of the NeRF-GAN in the convolutional generator. In particular, our convolutional generator learns to map each latent code from the 3D generator, along with the target viewpoint, to the corresponding obtained images by explicit volumetric rendering. By doing so, we aim to distill the NeRF-GAN's underlying 3D knowledge into the convolutional generator, as well as to establish a correspondence between the images of the generator and the 3D representation of the NeRF-GAN. As demonstrated in Fig.~\ref{fig_main}, our experiments on three different datasets indicate that the convolutional generator trained with our method is capable of achieving results  comparable to volumetric rendering in terms of image quality and 3D-consistency, while benefiting from the superior efficiency of convolutional networks. 
 
 Our contributions are summarized as follows:
\begin{itemize}[noitemsep]
  \item We propose a method to distill NeRF-GANs into convolutional generators for efficient 3D-aware inference.
  \item We provide a simple and effective method to condition the convolutional generator on the well-disentangled intermediate latent space of the NeRF-GAN.
  \item Through experiments on three different datasets, we show that the generator trained by our distillation method well preserves the 3D consistency, image quality, and semantics of the pretrained NeRF-GAN.
\end{itemize}




