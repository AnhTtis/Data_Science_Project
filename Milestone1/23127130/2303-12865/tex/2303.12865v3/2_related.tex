\section{Related Works}
\label{sec:related}

\noindent{\textbf{3D-aware Generation from Single-View Images.} Prior works have attempted to create 3D awareness in 2D GANs using explicit 3D supervision, such as 3D models~\cite{rigstyle, deng2020disco}, pose and landmark annotations~\cite{Shoshan_2021_gancontrol, hu2018rotation}, and synthetic data~\cite{KowalskiECCV2020Config}. In many applications, obtaining such 3D supervision is not practical. As a result, later works aimed at unsupervised methods by introducing 3D inductive biases in GANs, including 3D neural representations and differentiable rendering~\cite{HoloGAN2019, pan2020gan2shape, liftgan, BlockGAN2020} These methods, although promising, lag far behind 2D GANs in terms of image quality or struggle with high-resolution generation due to the additional computational complexity.

\noindent{\textbf{NeRF-GANs.}
NeRFS have shown outstanding potential in compactly representing 3D scenes for novel view synthesis. GRAF~\cite{Schwarz2020NEURIPS} and Pi-GAN~\cite{chan2021pi} are the first works to integrate NeRFs and GANs. While achieving highly consistent 3D-aware generation, the computational restrictions of NeRF framework make these methods impractical for high-resolution generation or environments with constrained resources. In order to extend NeRF-GANs to higher resolutions, convolutional super-resolution networks were used in later studies~\cite{niemeyer2021giraffe, gu2021stylenerf, orel2022styleSDF} at the expense of some multi-view inconsistencies. EpiGRAF~\cite{epigraf}, in contrast, adopts an efficient multi-scale patch training protocol, but still requires full high-resolution sampling rendering for inference, which makes it comparatively more computationally demanding than competitors.

Other studies aim at bringing the recent advances in the efficiency of NeRFS to NeRF-GANs. Although there exist numerous works on efficient 3D representations~\cite{mueller2022instant, plenoxels, sun2022direct, TensoRF, Wizadwongsa2021NeX, single_view_mpi} and volumetric sampling~\cite{yu2021plenoctrees, Garbin21arxiv_FastNeRF, hedman2021snerg, neff2021donerf, Hu_2022_CVPR} in NeRFs, only a subset of them~\cite{chan2022efficient, schwarz2022voxgraf, gmpi2022} have been successfully applied to NeRF-GANs. This is because they are mainly designed for the single-scene setup, making their adaptation to the generative setup not trivial. The use of sparse voxel grids in VoxGRAF~\cite{schwarz2022voxgraf} and multi-plane image representations in~\cite{gmpi2022} result in efficient and 3D-consistent generation while compromising the image quality and 3D geometry. EG3D~\cite{chan2022efficient} proposes using tri-planes to represent the geometry of the generated objects. Exploiting tri-planes, coupled with carefully designed techniques to enforce 3D consistency, allows EG3D to significantly improve both computational efficiency and image quality. Live 3D Portrait~\cite{trevithick2023} is a concurrent work based on EG3D that aims at real-time one-shot reconstruction of faces by estimating the canonical tri-planes of a pre-trained EG3D. However, Live 3D Portrait is computationally limited by the underlying volumetric rendering of EG3D.
Most similar to our study, SURF-GAN~\cite{kwak2022injecting} aims to discover directions for pose control in a pretraind 2D GAN by generating multi-view images using a pretrained NeRF-GAN. However, using NeRF-GANs only as multi-view supervision does not fully exploit their underlying 3D knowledge. Moreover, the 2D generator obtained using this method does not preserve any correspondence between the NeRF-GAN's 3D representations and the generated images. Different from SURF-GAN, we exploit the intermediate latent space of retrained NeRF-GANs to distill 3D knowledge into a 2D generator and establish correspondence between the convolutional generator and the NeRF-GAN's 3D representations.



