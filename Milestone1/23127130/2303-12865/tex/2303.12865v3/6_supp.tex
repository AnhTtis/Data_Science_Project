\clearpage

\appendix

\section*{Supplementary Material}

In what follows, we first provide an evaluation of the correspondence between the generated images using volumetric rendering and our convolutional generator in section~\ref{correspondence}. Moreover, in section~\ref{efficiency}, we extend the efficiency analysis of our method to the setup of the Shapenet Cars dataset. In section~\ref{detail}, we discuss some of the implementation details. Then, we offer a discussion on the limitations of the proposed method in Sec.~\ref{limit}. Finally, we provide more visual examples for our method in Sec.~\ref{visual}.









\section{Evaluation of Correspondence}\label{correspondence}
To provide a baseline for future research, we measure the correspondence between generated images using our method and the corresponding images rendered from EG3D using Peak Signal to Noise Ratio (PSNR), the result of which is provided in Table \ref{tab_psnr}.

\section{Analysis of Efficiency on Shapenet Cars}\label{efficiency}
In Fig.~\ref{fig_supp_compute_cars}, we provide a comparison of the computational efficiency of inference using our method and the baseline NeRF-GAN (EG3D) on Shapenet Cars setup. As mentioned in section 4.3 of the main paper, EG3D performs volumetric rendering at the resolution of 64 and super-resolves the output to the resolution of 128. As it can be seen in Fig.~\ref{fig_supp_compute_cars}, Our method with StyleGAN2 as its backbone is more efficient than EG3D in terms of both memory consumption and speed. However, the StyleGAN3 backbone is only more efficient in terms of memory consumption, but it is slower than the baseline. The reason for this is that StyleGAN3 is more computationally demanding than the tri-plane generator in EG3D, which is based on StyleGAN2. In the setup of Shapenet Cars, the computational complexity of the StyleGAN3 convolutional generator outweighs that of the lower-resolution volumetric rendering in EG3D, resulting in a slower inference (Note that the memory consumption is still lower for the StyleGAN3 convolutional generator). 

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/supp_memory_cars.pdf}
\includegraphics[width=\linewidth]{figures/supp_speed_cars.pdf}
\end{center}
\caption{Comparison of the inference memory consumption and speed (on a fixed GPU budget) for our method and EG3D on Shapenet Cars.}
\label{fig_supp_compute_cars}
\end{figure}

\begin{table}[b]
\caption{PSNR values ($\uparrow$) for measuring the correspondence between the images of EG3D and our convolutional generator on 3 datasets. The values are calculated for 1k images. ST2 and ST3 correspond to the two architectures StyleGAN2 and StyleGAN3 used as the backbone of our method}%
\label{tab_psnr}%
\centering
\resizebox{0.5\linewidth}{!}{%
\begin{tabular}{lll}
\multicolumn{1}{l}{Dataset} & \multicolumn{1}{l}{ST2} & \multicolumn{1}{l}{ST3}  \\
\toprule
\multicolumn{1}{l}{FFHQ} & \multicolumn{1}{l}{28.73} & \multicolumn{1}{l}{28.65}  \\
\multicolumn{1}{l}{AFHQ Cats} & \multicolumn{1}{l}{28.46} & \multicolumn{1}{l}{28.10}   \\
\multicolumn{1}{l}{Shapenet Cars} & \multicolumn{1}{l}{36.60} & \multicolumn{1}{l}{36.57} \\
\bottomrule
\end{tabular}}
\end{table}

\section{Implementation Details}\label{detail}

\subsection{SURF Baseline}\label{base}
In the main paper, we provided a baseline, which we referred to as ``SURF'', inspired by the method proposed in the SURF-GAN paper~\cite{kwak2022injecting} for pose/content disentanglement of a pre-trained 2D StyleGAN. As the code and the pre-trained models for such disentanglement are not made publicly available, we provided our best effort in implementing the SURF baseline inspired by their approach. 

The proposed method in~\cite{kwak2022injecting} consists of two main components: 1. a NeRF-GAN model called SURF-GAN for portrait images 2. a training recipe to add pose conditioning to a pre-trained StyleGAN2 using multi-view images generated from the 3D NeRF-GAN. Specifically, the part relevant as a baseline for our proposed method is the second component, which we tried to re-implement it for comparison with our method. To do so, for a fair comparison with our method, we use EG3D pre-trained on FFHQ for generating multi-view image triplets from 3 different viewpoints (source, canonical, and target views). Following ~\cite{kwak2022injecting}, we use pSp~\cite{richardson2021encoding}, a pretrained inversion encoder to invert the generated multi-view images in the style space of the 2D generator. Using the multi-view images and their corresponding style codes obtained using pSp, we train two mapping networks to: 
\begin{enumerate}[noitemsep]
    \item map any arbitrary style code to the style code of the canonical viewpoint, and
    \item map the canonical style code to the style code of the target viewpoint.
\end{enumerate}
  For the mapping networks, we use MLP networks with the same architecture as StyleGAN2's mapping network. For the training objective, we include reconstruction losses both on the images (MSE and Perceptual) and latent codes (MSE), as used in ~\cite{kwak2022injecting}. This architecture and training regime is inspired by the one in~\cite{kwak2022injecting}, but not exactly the same. We have adapted the method to the 3D generator network we are using (EG3D). Moreover, in the original method, the mapping from the canonical latent code to the target one is formulated as the linear combination of a set of learnable pose vectors. In our implementation, we use a more general mapping by using an MLP network instead. Nevertheless, the implemented baseline provides a comparison with a proven alternative strategy to enable pose conditioning of a StyleGAN-like convolutional architecture.

\subsection{Inversion}\label{inv}
In Sec. 4.8 of the main paper, we provided examples of inversion in our generator using Pivotal Tuning Inversion (PTI)~\cite{roich2022pivotal}. Following EG3D, inversion is performed given a target image and its corresponding camera parameters. As for the implementation, we use the adaptation to EG3D provided at~\cite{inversion}. We use 500 steps for optimizing the latent code and 350 additional steps for fine-tuning the generator according to the PTI method.

\subsection{3D Consistency Metrics}\label{3dmetric}
To calculate the 3D Landmark Consistency and Identity Preservation (ID) in section 4.5.3 of the main paper, we vary the yaw from -40 to +40 and the pitch from -30 to +30, following the evaluation setup of ~\cite{kwak2022injecting}. As an additional visualization, Fig.~\ref{fig_id} shows the comparison of Identitiy Preservation for each angle individually.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/supp_id_yaw.pdf}
\includegraphics[width=\linewidth]{figures/supp_id_pitch.pdf}
\end{center}
\caption{Comparison of identity preservation across different camera poses. Our method performs much better than the pose-conditioned GAN baseline (PC-GAN), approaching the consistency of volumetric rendering (EG3D). The higher consistency achieved by SURF is due to its limited pose variations.}
\label{fig_id}
\end{figure}


 \section{Limitations}\label{limit}
In our approach, the outputs of the volumetric rendering branch are used to supervise our convolutional renderer. Thus, the visual quality and 3D consistency of our approach is largely bound by the quality and consistency of the pretrained NeRF-GAN. 
However, our formulation is largely agnostic to the volumetric generator used. Therefore, improvements in volumetric rendering in the context of GANs will also transfer to the generated quality of our approach.

As an example, a close look at the videos provided in the supplementary video reveals small changes in the degree of the smile in different viewpoints of some of the generated faces. Such a phenomenon, which is due to the dataset bias, also exists in EG3D, as analyzed in Sec 1.1 of the supplementary material of EG3D paper~\cite{chan2022efficient} and visible in our results as well.

\section{Visual Results}\label{visual}
In Fig.~\ref{fig_supp_ffhq}, we provide more visual examples generated using our method from FFHQ dataset. Fig.~\ref{fig_supp_cats} shows examples of generated images from AFHQ Cats dataset using the proposed convolutional generator. Finally, in Fig.~\ref{fig_supp_cars}, we provide random samples from Shapenet Cars dataset generated using the volumetric rendering (EG3D) and our method. For a video comparison, please refer to the supplementary video ``\textcolor{red}{supp.mp4}''.



\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/supp_vis_ffhq.pdf}
\end{center}
\vspace{-5mm}
\caption{Visual examples of pose control in our convolutional generator and their comparison to those of EG3D on FFHQ dataset.}
\label{fig_supp_ffhq}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/supp_vis_cats.pdf}
\end{center}
\vspace{-5mm}
\caption{Visual examples of pose control in our convolutional generator and their comparison to those of EG3D on AFHQ Cats dataset.}
\label{fig_supp_cats}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{figures/supp_vis_cars.pdf}
\end{center}
\vspace{-5mm}
\caption{Visual examples of pose control in our convolutional generator and their comparison to those of EG3D on Shapenet Cars dataset.}
\label{fig_supp_cars}
\end{figure*}

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }


\end{document}