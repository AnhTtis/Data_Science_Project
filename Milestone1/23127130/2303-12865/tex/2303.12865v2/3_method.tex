
\section{Method}
\label{sec:method}

\begin{figure*}[t]
    \centering%
    \includegraphics[width=\linewidth]{figures/method.pdf}
    \vspace{-4mm}
    \caption{Using a student-teacher framework, we distill 3D consistency from a frozen volumetric rendering based Nerf-GAN (top) to a 2D convolutional renderer (bottom). A loss consisting of high- and low-resolution image reconstruction and an adversarial component allows us to retain good image quality and 3D consistency.}
    \label{fig_diagram}
\end{figure*}

In this section, we first provide a brief overview of the formulation of NeRF-GANs and then explain the proposed formulation in detail.

\subsection{Preliminaries}
\label{sec:preliminaries}
The general formulation of NeRF-GANs consists of a 3D-representation generator $G^{3D}(z)$, which maps a latent variable $z$ (usually drawn from a normal distribution) to a 3D representation of an object. Then, in order to render an image $I_{z,c}$ from the target viewpoint (camera parameters) $c \in R^{25}$, volumetric rendering is applied to the generated 3D representation. We base our method on EG3D~\cite{chan2022efficient}, as it provides a strong trade-off in image quality, 3D consistency, and efficiency, among recent NeRF-GANs.

EG3D represents 3D scenes using tri-planes, which are three axis-aligned orthogonal feature planes, each with a size of $N \times N \times C$, where $N$ is spatial resolution and $C$ is the number of channels. To represent a 3D position $x \in R^3$, $x$ is projected onto each of the three feature planes, retrieving the corresponding feature vector $(F_{xy}, F_{xz}, F_{yz})$ via bilinear interpolation, and aggregating the three feature vectors via summation. To obtain the color and density at position $x$, a lightweight MLP decodes the feature vector obtained for the queried position to a density and color value.

The tri-plane generator $G^{3D}(z, c)$ in EG3D consists of a mapping network $M^{3D}(z, c)$, which maps the input latent code and the target viewpoint to an intermediate latent variable $w$, namely the style code. The style code then is used to modulate a convolutional synthesis network $S^{3D}(w)$ to generate the tri-planes $I^{3p}_{z,c}$. In order to render an image $I_{z,c}$ from the target viewpoint $c$, hierarchical volumetric rendering is applied to the tri-planes. Since volumetric rendering at high resolutions is computationally too expensive, EG3D does so at a lower resolution and uses a convolutional super-resolution network to obtain a final image. More specifically, the low-resolution output of volumetric rendering in EG3D consists of a 32-channel feature map $I^f_{z,c}$, the first three of which represent the low-resolution RGB image $I^{LR}_{z,c}$), which is given as input to the super-resolution network. EG3D is trained in an adversarial fashion with a viewpoint-conditioned dual discriminator $D$ that ensures the photorealism of the generated images from the target viewpoints, as well as the consistency between high-resolution and low-resolution images.

\subsection{Convolutional Rendering of Pretrained NeRF-GANs} 

The aim of the proposed method is to distill a pre-trained NeRF-GAN $G^{3D}_{z,c}$ into a 2D image generator $G^{2D}_{z, c}$, such that $G^{2D}_{z, c}$ directly predicts 3D-consistent multi-view images $I'_{z, c}$, corresponding to the volumetric renderings obtained by the underlying 3D representation of $G^{3D}_{z,c}$. To this end, we propose to exploit the well-disentangled style space of $G^{3D}$ to distill the underlying 3D representation into $G^{2D}$. Sharing the style space $w$ of the pre-trained 3D generator with the convolutional renderer is the first step towards establishing a correspondence between the 3D representations and the generated images. Secondly, it allows training the convolutional generator for 3D-consistent generation without the need for generating multiple views of the same objects and enforcing multi-view consistency. 

The overall architecture of EG3D and our convolutional generator is visualized in \ref{fig_diagram}. The convolutional generator is based on StyleGAN3~\cite{karras2021alias}, consisting of a mapping network, low-resolution convolutional feature prediction, and a convolutional super-resolution network. The mapping network transforms the style code $w$ of $G^{3D}$, and the target viewpoint $c$ to the style code $w'$ of $G^{2D}$. Then the low-resolution feature predictor $S^{2D}$ estimates the EG3D features and low-resolution image obtained by the volumetric rendering. The estimated features and images are then mapped to the high-resolution output using the super-resolution network. In our setup, the super-resolution network is initialized with EG3D's super-resolution network and is jointly optimized with the feature predictor network.

\subsection{Training}
\label{sec:training}

In order to train the proposed convolutional renderer, we use a teacher-student framework, where the volumetric rendering is used to supervise $G^{2D}$ on the viewpoint-conditioned mapping of $(z, c)$ to $I'_{z,c}$.
A schematic representation of our training regime is reported in Fig.~\ref{fig_diagram}.
Specifically, for each training sample, we randomly sample $z_i$ and $c_i$ and use the pretrained NeRF-GAN to obtain: the corresponding style code $w_i$, the low-resolution image $I^{LR}_{z_i,c_i}$, feature maps $I^{f}_{z_i,c_i}$ rendered by volumetric rendering, as well as, the high-resolution image $I^{HR}_{z_i,c_i}$ generated by the super-resolution network. These together form a training sample $i$ for the proposed convolutional renderer.

We provide $z$ and $c$ to $G^{2D}$ and compute a loss function composed of three parts. We first add a reconstruction term $L^{LR}_{rec}$ between the low-resolution outputs of the volumetric and convolutional renderers. A second reconstruction loss $L^{HR}_{rec}$ is applied between the super-resolved outputs of the two renderers. Lastly, we apply an adversarial term $L_{adv}$.

In the following we drop the subscripts $z_i,c_i$ to reduce the clutter in notation. The low-resolution reconstruction loss $L^{LR}_{rec}$ consists of a pixel-wise smooth L1 loss between the two feature maps, as well as a perceptual loss between the generated and target low-resolution images,
\begin{equation}
\label{eq:loss_lr}
    \begin{split}
        L^{LR}_{rec} = &\lambda^{LR}_{\text{Smooth}L1} * \text{Smooth}L1(I'^{f}, I^{f}) + \\
        &\lambda^{LR}_{\text{perc}} * \text{PerceptualLoss}(I'^{LR}, I^{LR}).
    \end{split}
\end{equation}
Here, $\lambda^{LR}_{\text{Smooth}L1}$ and $\lambda^{LR}_{perc}$ are the weights for the low-resolution smooth L1 and perceptual loss, respectively.

The high-resolution reconstruction loss $L^{HR}_{rec}$ applied to the high-resolution images, only consists of a perceptual loss, to avoid blurry results in the high-resolution images,
\begin{equation}
\label{eq:loss_hr}
        L^{HR}_{rec} = \lambda^{HR}_{perc} * \text{PerceptualLoss}(I'^{HR}, I^{HR}),
\end{equation}
where $\lambda^{HR}_{perc}$ is the weight of the high-resolution perceptual loss.


The adversarial term $L_{adv}$ is similar to the one used in EG3D. We use the same dual-discriminator architecture $D$ as in EG3D to ensure the realism of the high-resolution images, their consistency with the low-resolution version, and the compliance of the generated image with the queried viewpoints. The total loss $L_{total}$ for training $G^{2D}$ is,
\begin{equation}
    \label{eq:loss}
    L_{total} = L^{LR}_{rec} + L^{HR}_{rec} + \lambda_{adv} *  L_{adv},
\end{equation}
where $\lambda_{adv}$ is the weight for the adversarial loss.

In practice, empirical experiments show that training the convolutional renderer using the full objective from the beginning will lead to high-quality but 3D-inconsistent images. Therefore, we instead propose a 2-stage training curriculum. In the first stage, $G^{2D}$ is only optimized by pure distillation of the volumetric rendering $G^{3D}$ using $L^{LR}_{rec}$ and $L^{HR}_{rec}$ until the renderer achieves reasonable generation quality. Then, the adversarial loss $L_{adv}$ is added to the training to further improve the performance. By applying this 2-stage curriculum, we are able to  counter the 3D inconsistency induced by the adversarial training.

