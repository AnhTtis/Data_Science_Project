\clearpage
\appendix

\section*{Appendix}

In what follows, we first provide an ablation study in Sec.~\ref{low-res} on the importance of supervision using the low-resolution image and rendered tri-plane features of EG3D in our proposed method. In Sec.~\ref{detail}, we discuss some of the implementation details. In Sec.~\ref{efficiency}, we extend the efficiency analysis of our method to all 3 experimental setups, and then we offer a discussion on the limitations of the proposed method in Sec.~\ref{limit}. Finally, we provide more visual examples for our method in Sec.~\ref{visual}.


\section{Importance of Low-Resolution Supervision}\label{low-res}
In Sec. 4.6 of the main paper, we provided an ablation on the effect of different loss terms in our proposed method. In this section, we additionally provide ablation on the importance of low-resolution loss terms by comparing our proposed objective to only supervision on high-resolution images. Tab.~\ref{tab_ablation_lr} shows the FID scores for the following setups:
\begin{itemize}[noitemsep]
  \item \textbf{HR Rec (stage 1)}: Only using the reconstruction loss on the high-resolution images.
  \item \textbf{HR Rec + Adv (stage 2)}: reconstruction and adversarial losses on the high-resolution images.
  \item \textbf{Ours}: Full training objective, including all the reconstruction and adversarial terms.
\end{itemize}
As can be seen, supervising the training of our generator using only the high-resolution images of the pretrained NeRF-GAN does not perform as well as our proposed objective, which includes reconstruction losses in both low and high resolutions. It is important to note that, the low-resolution output of EG3D (the pretrained NeRF-GAN) contains the rendered 32-dimensional features from the tri-planes, in addition to the low-resolution rendered image using volumetric rendering. Performing low-resolution distillation in our method allows for further exploitation of the intermediate features of the pretrained NeRF-GAN in addition to its intermediate latent space. This in turn results in further improvements in generation quality, as demonstrated in Tab.~\ref{tab_ablation_lr}

\begin{table}[b]
\caption{Ablation on the importance of low-resolution supervision in the proposed method. The experiments are performed on AFHQ Cats dataset.}%
\label{tab_ablation_lr}%
\centering%
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{ll}
\toprule
\multicolumn{1}{l}{\bf Method} &\multicolumn{1}{c}{\bf FID $\downarrow$} \\ 
\midrule
\multicolumn{1}{l}{HR Rec (stage 1)} &\multicolumn{1}{c}{10.39}  \\
\multicolumn{1}{l}{HR Rec + Adv (stage 2)} &\multicolumn{1}{c}{6.58}   \\
\multicolumn{1}{l}{Ours} &\multicolumn{1}{c}{3.17}   \\
\bottomrule
\end{tabular}}
\end{table}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/compute.pdf}
\end{center}
\caption{Comparison of computational efficiency between EG3D and our convolutional generator in three different setups.}
\label{fig_supp_compute}
\end{figure*}


\section{Implementation Details}\label{detail}

\subsection{SURF Baseline}\label{base}
In the main paper, we provided a baseline, which we referred to as ``SURF'', inspired by the method proposed in the SURF-GAN paper~\cite{kwak2022injecting} for pose/content disentanglement of a pre-trained 2D StyleGAN. As the code and the pre-trained models for such disentanglement are not made publicly available, we provided our best effort in implementing the SURF baseline inspired by their approach. 

The proposed method in~\cite{kwak2022injecting} consists of two main components: 1. a NeRF-GAN model called SURF-GAN for portrait images 2. a training recipe to add pose conditioning to a pre-trained StyleGAN2 using multi-view images generated from the 3D NeRF-GAN. Specifically, the part relevant as a baseline for our proposed method is the second component, which we tried to re-implement it for comparison with our method. To do so, for a fair comparison with our method, we use EG3D pre-trained on FFHQ for generating multi-view image triplets from 3 different viewpoints (source, canonical, and target views). Following ~\cite{kwak2022injecting}, we use pSp~\cite{richardson2021encoding}, a pretrained inversion encoder to invert the generated multi-view images in the style space of the 2D generator. Using the multi-view images and their corresponding style codes obtained using pSp, we train two mapping networks to: 
\begin{enumerate}[noitemsep]
    \item map any arbitrary style code to the style code of the canonical viewpoint, and
    \item map the canonical style code to the style code of the target viewpoint.
\end{enumerate}
  For the mapping networks, we use MLP networks with the same architecture as StyleGAN2's mapping network. For the training objective, we include reconstruction losses both on the images (MSE and Perceptual) and latent codes (MSE), as used in ~\cite{kwak2022injecting}. This architecture and training regime is inspired by the one in~\cite{kwak2022injecting}, but not exactly the same. We have adapted the method to the 3D generator network we are using (EG3D). Moreover, in the original method, the mapping from the canonical latent code to the target one is formulated as the linear combination of a set of learnable pose vectors. In our implementation, we use a more general mapping by using an MLP network instead. Nevertheless, the implemented baseline provides a comparison with a proven alternative strategy to enable pose conditioning of a StyleGAN-like convolutional architecture.

\subsection{Inversion}\label{inv}
In Sec. 4.8 of the main paper, we provided examples of inversion in our generator using Pivotal Tuning Inversion (PTI)~\cite{roich2022pivotal}. Following EG3D, inversion is performed given a target image and its corresponding camera parameters. As for the implementation, we use the adaptation to EG3D provided at~\cite{inversion}. We use 500 steps for optimizing the latent code and 350 additional steps for fine-tuning the generator according to the PTI method.

\section{Efficiency Analysis}\label{efficiency}
In Fig. 1 of the main paper, we have provided a comparison between our convolutional generator and volumetric rendering (EG3D) in terms of the computational efficiency for inference. Here in Fig.~\ref{fig_supp_compute}, we extend the comparison to all setups investigated in the paper: 
\begin{itemize}[noitemsep]
  \item \textbf{Rendering Resolution: 128 - Final Resolution: 512}: The setup of the pretrained NeRF-GAN (EG3D) for experiments on AFHQ Cats.
  \item \textbf{Rendering Resolution: 64 - Final Resoultion: 128}: The setup of the pretrained NeRF-GAN (EG3D) for experiments on Shapenet Cars.
  \item \textbf{Rendering Resolution: 64 - Final Resolution: 512}: The setup of the pretrained NeRF-GAN (EG3D) for experiments on FFHQ.
\end{itemize}


 As apparent in Fig.~\ref{fig_supp_compute}, for the batch size of one in FFHQ setup, EG3D has a slight computational advantage to our generator. The reason for this is that EG3D uses StyleGAN2 backbone for tri-plane generation, whereas our image generator is based on StyleGAN3, which is computationally more expensive than StyleGAN2. Nevertheless, as the batch size increases, our generator significantly improves the computational efficiency compared to EG3D.

 \section{Limitations}\label{limit}
In our approach, the outputs of the volumetric rendering branch are used to supervise our convolutional renderer. Thus, the visual quality and 3D consistency of our approach is largely bound by the quality and consistency of the pretrained NeRF-GAN. 
However, our formulation is largely agnostic to the volumetric generator used. Therefore, improvements in volumetric rendering in the context of GANs will also transfer to the generated quality of our approach.

As an example, a close look at the videos provided in the supplementary video reveals small changes in the degree of the smile in different viewpoints of some of the generated faces. Such a phenomenon, which is due to the dataset bias, also exists in EG3D, as analyzed in Sec 1.1 of the supplementary material of EG3D paper~\cite{chan2022efficient} and visible in our results as well.

\section{Visual Results}\label{visual}
In Fig.~\ref{fig_supp_ffhq}, we provide more visual examples generated using our method from FFHQ dataset. Fig.~\ref{fig_supp_cats} shows examples of generated images from AFHQ Cats dataset using the proposed convolutional generator. Finally, in Fig.~\ref{fig_supp_cars}, we provide random samples from Shapenet Cars dataset generated using the volumetric rendering (EG3D) and our method. As can be seen in the figures, our proposed convolutional generator is able to correctly reproduce the pose of the volumetric rendering and maintain the general correspondence in appearance, although differing in some details.



\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/supp_vis_ffhq.pdf}
\end{center}
\caption{More visual examples of pose control in our proposed convolutional generator and their comparison to those of EG3D on FFHQ dataset.}
\label{fig_supp_ffhq}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth]{figures/supp_vis_cats.pdf}
\end{center}
\caption{More visual examples of pose control in our proposed convolutional generator and their comparison to those of EG3D on AFHQ Cats dataset.}
\label{fig_supp_cats}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/supp_vis_cars.pdf}
\end{center}
\caption{More visual examples of pose control in our proposed convolutional generator and their comparison to those of EG3D on Shapenet Cars dataset.}
\label{fig_supp_cars}
\end{figure*}