\section{Related Works}
\label{sec:related}

\noindent{\textbf{3D-aware Generation from Single-View Images.} Prior works have attempted to create 3D awareness in 2D GANs using explicit 3D supervision, such as 3D models~\cite{rigstyle, deng2020disco}, pose and landmark annotations~\cite{Shoshan_2021_gancontrol, hu2018rotation}, and synthetic data~\cite{KowalskiECCV2020Config}. In many applications, however, obtaining such 3D supervision is not practical. As a result, later works aimed at unsupervised methods by introducing 3D inductive biases in GANs, including 3D neural representations and differentiable rendering~\cite{HoloGAN2019, pan2020gan2shape, liftgan, BlockGAN2020} These methods, although promising, lag far behind 2D GANs in terms of image quality or struggle with high-resolution generation due to the additional computational complexity compared to the convolutional generators.

\noindent{\textbf{NeRF-GANs.}
Neural Radiance Fields (NeRFS) have shown outstanding potential in compactly representing 3D scenes for novel view synthesis. GRAF~\cite{Schwarz2020NEURIPS} and Pi-GAN~\cite{chan2021pi} are the first works to integrate NeRFs and GANs. While achieving highly consistent 3D-aware generation, the computational restrictions of NeRF framework make these methods impractical for high-resolution generation or environments with constrained resources. In order to extend NeRF-GANs to higher resolutions, convolutional super-resolution networks were used in later studies~\cite{niemeyer2021giraffe, gu2021stylenerf, orel2022styleSDF} at the expense of some multi-view inconsistencies. EpiGRAF~\cite{epigraf}, in contrast, adopts an efficient multi-scale patch training protocol, but still requires full high-resolution sampling rendering for inference, which makes it comparatively slower than competitors. 

Other studies aim at bringing the recent advances in the efficiency of NeRFS to the NeRF-GAN Setup. Although there exist numerous works on efficient 3D representations~\cite{mueller2022instant, plenoxels, sun2022direct, TensoRF, Wizadwongsa2021NeX, single_view_mpi} and volumetric sampling~\cite{yu2021plenoctrees, Garbin21arxiv_FastNeRF, hedman2021snerg, neff2021donerf, Hu_2022_CVPR} in NeRFs, only a subset of them~\cite{chan2022efficient, schwarz2022voxgraf, gmpi2022} have been successfully applied to NeRF-GANs, as their adaptation to the generative setup is not trivial. The use of sparse voxel grids in VoxGRAF~\cite{schwarz2022voxgraf} and multi-plane image representations in~\cite{gmpi2022} results in efficient and 3D-consistent generation with a small compromise in image quality and 3D geometry. EG3D~\cite{chan2022efficient} proposes using tri-planes to represent the geometry of the generated objects. Exploiting tri-planes, coupled with carefully designed techniques to enforce 3D consistency, allows EG3D to significantly improve both computational efficiency and image quality. In contrast to recent methods that incorporate efficient 3D representation in NeRF-GANs, our method aims at revisiting convolutional generators for efficient rendering during inference using a NeRF-GAN as a distillation signal. Most similar to our study, SURF-GAN~\cite{kwak2022injecting} aims to discover the directions for pose control in a pretraind 2D GAN by generating multi-view images using a pretrained NeRF-GAN. However, using NeRF-GANs only as multi-view supervision does not fully exploit their underlying 3D knowledge. Moreover, the 2D generator obtained using this method does not preserve any correspondence between the NeRF-GAN's 3D representations and the generated images. Different from SURF-GAN, we aim at exploiting the intermediate latent space of retrained NeRF-GANs to distill 3D knowledge into a 2D generator and establish correspondence between the convolutional generator and the NeRF-GAN's 3D representations.



