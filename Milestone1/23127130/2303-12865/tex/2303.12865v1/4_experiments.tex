\section{Experiments}

In this section, we first describe our experimental setup for the evaluation of our method. Then, we compare the proposed method with baselines in terms of visual quality, 3D consistency, and computational efficiency. Moreover, we provide an ablation study and a discussion on the benefits and trade-offs of the proposed method.

\subsection{Datasets}\label{sec:dataset}
Following EG3D~\cite{chan2021pi}, we evaluate our method on three different datasets:

\noindent\textbf{Flickr-Faces-HQ (FFHQ)~\cite{Karrasstyle}}: a collection of 70k high-quality images of real-world human faces, as well as corresponding approximate camera extrinsics estimated using an of-the-shell pose estimator.

\noindent\textbf{AFHQ Cats}: a sub-category of the Animal-Face-HQ (AFHQ)~\cite{choi2020stargan}, consisting of around 5k high-quality images of cat faces, as well as corresponding camera extrinsics estimated using an of-the-shell pose estimator.

\noindent\textbf{ShapeNet Cars:} a category of ShapeNet~\cite{chang2015shapenet} consisting of synthetic images of cars rendered from different viewpoints, as well as the corresponding camera extrinsics annotations.


\subsection{Baselines}\label{sec:baseline}
We consider EG3D~\cite{chan2022efficient} and The method proposed in SURF-GAN~\cite{kwak2022injecting} as our main baselines for this study. For a more complete evaluation, we also compare our method to additional relevant baselines:

\noindent\textbf{EG3D~\cite{chan2022efficient}}: the NeRF-GAN used for distilling 3D knowledge in the convolutional generator. EG3D serves as the upper bound for the 3D consistency of the proposed method.

\noindent\textbf{Pose-Conditioned StyleGAN (PC-GAN)}: a standard conditional 2D GAN, conditioned on the pose annotations without any knowledge distillation.

\noindent\textbf{SURF}: Inspired by the proposed method in SURF-GAN~\cite{kwak2022injecting}, we create a baseline called SURF, where multi-view images of EG3D are used to discover pose-control in a 2D StyleGAN (pretrained on FFHQ) based on the proposed method in~\cite{kwak2022injecting}. The details of the implementation can be found in the supplementary material\footnote{As the official implementation of SURF-GAN does not include their method for pose control in 2D GANs, we provide our best attempt in implementing their method.}.

\noindent\textbf{LiftGAN~\cite{liftgan}}: a method predating EG3D and SURF baselines, based on differentiable rendering for distilling 2D GANs to train a 3D generator.

\subsection{Implementation details}\label{sec:detail}
For the pretrained NeRF-GAN, we use the official models from EG3D~\cite{chan2022efficient} (for Shapenet Cars, we re-train the model as the official model does not match the results reported by EG3D). We train our experiments using a batch size of 16. The rendering resolution and the final resolution in FFHQ, AFHQ, and Shapenet are (64, 512), (128, 512), and (64, 128), respectively (following the pretrained models). Both training and inference experiments were conducted using NVidia RTX 3090 GPUs. In all experiments, we set all of the weights of reconstruction loss terms ($\lambda^{LR}_{smooth_l1}$, $\lambda^{LR}_{perc}$, $\lambda^{HR}_{perc}$) to the value 1 and the weight of the adversarial loss ($\lambda_{adv}$) to the value 0.1.

\subsection{Metrics}\label{sec:metric}
We evaluate our method quantitatively in terms of visual quality, 3D consistency, and correspondence with the NeRF-GAN output with the following metrics:

\noindent\textbf{Fr√©chet Inception Distance (FID)~\cite{heusel2017fid}}: The most common metric to assess the quality and diversity of generation.

\noindent\textbf{Kernel Inception Distance (KID)~\cite{heusel2017fid}}: An unbiased alternative to FID for smaller datasets.

\noindent\textbf{Pose Accuracy (PA)}: following previous works~\cite{chan2022efficient}, we measure the ability of the model in generating images of the query poses by calculating the mean squared error (MSE) between the query poses and the pose of the generated images, estimated using an of-the-shelf pose estimator~\cite{deng2019accurate}.

\noindent\textbf{Identity Preservation (ID)}: As a metric for 3D consistency, we measure the degree of face identity preservation between different viewpoints with respect to the canonical pose using ArcFace~\cite{deng2018arcface} cosine similarity for the FFHQ setup.

\noindent\textbf{3D Landmark Consistency}: As another 3D consistency metric, we measure the change in facial landmarks between different viewpoints in FFHQ using MSE. The 3D landmarks are estimated using an off-the-shelf estimator~\cite{deng2019accurate}.

\noindent\textbf{Peak Signal-to-Noise Ratio (PSNR)}: We measure the correspondence between the output of the pre-traind NeRF-GAN and our convolutional generator using PSNR.

\subsection{Quantitative Comparison}
In the following, we quantitatively compare the proposed method with the baselines described in Sec.~\ref{sec:baseline} in terms of inference efficiency, visual quality, and 3D consistency.

\subsubsection{Efficiency}
The efficiency of fully-convolutional networks compared to the rendering-based method is well-known. To better assess the practical benefit of the proposed method, we provide a comparison of inference efficiency between EG3D. Fig~\ref{fig_main} visualizes an example of the generation speed and memory consumption of the two methods using different batch sizes on a fixed GPU budget (in this case, on RTX 3090 GPU with 24G of memory). To simulate a more realistic setup, where batch-wise generation is used for multi-frame video generation, we fix the objects across the batches and only change the viewpoints. We also allow volumetric rendering to reuse the tri-planes generated for the first batch for the subsequent ones for a fair comparison. As shown, EG3D is restricted to small batch sizes due to its costly memory consumption. As for speed, as the batch size increases, the convolutional generator achieves an enormous speed-up (~2 orders of magnitude) compared to volumetric rendering, permitting much higher rendering throughput.

\subsubsection{Image Quality}
To assess the trade-off brought about by our convolutional generator, we next evaluate the quality of the generated images. Table~\ref{tab_fid} shows the FID scores and KID scores for our method and the baseline methods on different datasets. Compared to the PC-GAN and SURF baselines, our method constantly achieves higher quality. This confirms that exploiting the style space of the pretrained NeRF-GAN contributes to the ability of the convolutional renderer in pose-conditioned generation. Although our method does not fully match the visual quality of EG3D, it is still able to fairly maintain high image quality and significantly reduce the compromise in the quality compared to the other convolutional counterparts.
\begin{table}[t]
\caption{Comparison of image quality on three datasets in terms of FID and KID metrics. *The value is borrowed from~\cite{liftgan}.}%
\label{tab_fid}%
\centering%
\resizebox{0.98\linewidth}{!}{%
\begin{tabular}{lllllll}
\toprule
\multicolumn{1}{l}{\bf Method} &\multicolumn{2}{c}{\bf FFHQ} &\multicolumn{2}{c}{\bf AFHQ} &\multicolumn{2}{c}{\bf ShapeNET Cars} \\ 
\multicolumn{1}{l}{} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{KID $\downarrow$} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{KID $\downarrow$} &\multicolumn{1}{c}{FID $\downarrow$} &\multicolumn{1}{c}{KID $\downarrow$} \\
\midrule
\multicolumn{1}{l}{PC-GAN}  &\multicolumn{1}{c}{19.27} &\multicolumn{1}{c}{0.0085} &\multicolumn{1}{c}{4.51} &\multicolumn{1}{c}{0.0009} &\multicolumn{1}{c}{6.08} &\multicolumn{1}{c}{0.0018} \\
\multicolumn{1}{l}{LiftGAN~\cite{liftgan}}  &\multicolumn{1}{c}{29.81*} &\multicolumn{1}{c}{-}  &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} \\
\multicolumn{1}{l}{SURF}  &\multicolumn{1}{c}{31.09} &\multicolumn{1}{c}{0.0153} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} &\multicolumn{1}{c}{-} \\
\multicolumn{1}{l}{EG3D~\cite{chan2022efficient}} &\multicolumn{1}{c}{4.7} &\multicolumn{1}{c}{0.0014} &\multicolumn{1}{c}{2.90} &\multicolumn{1}{c}{0.0003} &\multicolumn{1}{c}{3.55} &\multicolumn{1}{c}{0.0017} \\
\multicolumn{1}{l}{Ours}  &\multicolumn{1}{c}{8.3} &\multicolumn{1}{c}{0.0043} &\multicolumn{1}{c}{3.17} &\multicolumn{1}{c}{0.0007} &\multicolumn{1}{c}{3.8} &\multicolumn{1}{c}{0.0019} \\
\bottomrule
\end{tabular}}
\end{table}


\subsubsection{3D Consistency}
While, unlike volumetric rendering, 2D convolutions do not guarantee 3D consistency, we show that our approach achieves a good performance in this regard. We assess the 3D consistency of generated images on FFHQ by measuring the pose accuracy, 3D landmark consistency, and face identity preservation, as discussed in Sec. \ref{sec:metric}, which are provided in Table \ref{tab_3d}. Based on the results, our method strongly matches EG3D in all three metrics while PC-GAN and SURF struggle. Note that the high values for identity preservation and 3D landmark consistency in SURF are due to the limited pose variations, and hence generating similar images regardless of the input pose, as reflected by the pose accuracy (and the visual examples in Fig.~\ref{fig_vis_ffhq}). As an additional visualization, we provide a pose-wise comparison of the methods in terms of face identity preservation in Fig.~\ref{fig_id}. 
\begin{table}[t]
    \caption{Comparison of 3D consistency metrics on FFHQ.}
    \label{tab_3d}%
    \centering
        \resizebox{0.8\linewidth}{!}{%
        \begin{tabular}{lllll}
        \hline
        \multicolumn{1}{l}{\bf Method} &\multicolumn{1}{c}{\bf Pose Acc. $\downarrow$} &\multicolumn{1}{c}{\bf 3D Landmark $\downarrow$} &\multicolumn{1}{c}{\bf ID $\uparrow$} \\ 
        \hline
        \multicolumn{1}{l}{PC-GAN} &\multicolumn{1}{c}{0.009} &\multicolumn{1}{c}{0.062} &\multicolumn{1}{c}{0.56}\\
        \multicolumn{1}{l}{SURF} &\multicolumn{1}{c}{0.044} &\multicolumn{1}{c}{0.012} &\multicolumn{1}{c}{0.86}\\
        \multicolumn{1}{l}{EG3D~\cite{chan2022efficient}} &\multicolumn{1}{c}{0.002} &\multicolumn{1}{c}{0.018} &\multicolumn{1}{c}{0.75}\\
        \multicolumn{1}{l}{Ours} &\multicolumn{1}{c}{0.002} &\multicolumn{1}{c}{0.018}  &\multicolumn{1}{c}{0.75}\\
        \midrule
        \end{tabular}} 
\end{table}

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/vis_id.pdf}
\end{center}
\vspace{-5mm}
\caption{Comparison of identity preservation across different camera poses. Our method performs much better than the pose-conditioned GAN baseline (PC-GAN), approaching the consistency of volumetric rendering (EG3D). The higher consistency achieved by SURF is due to its limited pose variations.}
\label{fig_id}
\end{figure}


\begin{figure*}
\begin{center}
\includegraphics[width=\linewidth]{figures/vis_ffhq.pdf}
\end{center}
\vspace{-4mm}
\caption{Qualitative examples of variations in yaw and pitch for FFHQ. Compared to the pose-conditioned GAN and SURF baseline, our proposed method nearly matches the 3D consistency and image quality of volumetric rendering (EG3D).}
\label{fig_vis_ffhq}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[, width=\linewidth]{figures/vis_cat.pdf}
\end{center}
\vspace{-4mm}
\caption{Qualitative examples of variations in yaw and pitch for AFHQ cats. In line with our quantitative experiments, the pose-conditioned convolutional baseline (PC-GAN) fails to preserve the identity of the subject under different poses. In contrast, our method exhibits similar preservation of identity to the volume rendering approach (EG3D), despite the difference in computational resources and time.}
\label{fig_vis_cats}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[, width=\linewidth]{figures/vis_cars.pdf}
\end{center}
\vspace{-4mm}
\caption{Qualitative examples of different camera poses in Shapenet Cars for three different car models. Our method can faithfully approximate the result of EG3D.}
\label{fig_vis_cars}
\end{figure*}

\subsection{Ablation}
\noindent\textbf{Ablation on loss functions:} the proposed training objective in section~\ref{sec:training} consists of different loss terms to ensure both image quality and consistency with the output of volumetric rendering. In this section, we ablate the importance of these components, by adding them one by one to form the final objective of Eq.~\ref{eq:loss}. Table~\ref{tab_ablation} shows the FID scores for the following experiments on the loss terms on AFHQ dataset:
\begin{itemize}[noitemsep]
  \item \textbf{LR (L1)}: Only using low-resolution smooth L1 loss (frozen super-resolution network).
  \item \textbf{LR (L1+Perc)}: Full low-resolution reconstruction loss (frozen super-resolution network).
  \item \textbf{LR (L1+Perc) + HR (Perc)} Full reconstruction loss on low-resolution and high-resolution.
  \item \textbf{Full} Full training objective, including the reconstruction and adversarial terms.
\end{itemize}
As shown by the ablation study, the combination of the proposed loss terms leads to the best FID scores. 

\begin{table}[b]
\caption{Ablation on different loss functions for training the convolutional renderer on AFHQ Cats dataset.}%
\label{tab_ablation}%
\centering%
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{ll}
\toprule
\multicolumn{1}{l}{\bf Method} &\multicolumn{1}{c}{\bf FID $\downarrow$} \\ 
\midrule
\multicolumn{1}{l}{LR (L1)} &\multicolumn{1}{c}{56.77}  \\
\multicolumn{1}{l}{LR (L1+Perc)} &\multicolumn{1}{c}{30.55}   \\
\multicolumn{1}{l}{LR (L1+Perc) + HR (Perc)} &\multicolumn{1}{c}{11.14}   \\
\multicolumn{1}{l}{Full} &\multicolumn{1}{c}{3.17}   \\
\bottomrule
\end{tabular}}
\end{table}

\noindent\textbf{Single-stage v.s. two-stage training:} As mentioned in \ref{sec:training}, we find out that single-stage training by jointly optimizing for both reconstruction and adversarial losses results in subtle inconsistencies such as color shifts and geometry warps, which can be mitigated using the proposed 2-stage training in section~\ref{sec:method}. As the observed inconsistencies are difficult to capture using our quantitative 3D consistency metrics, we provide a visual comparison between the examples of single-stage and two-stage training on AFHQ in Fig. \ref{fig_stage}. To better visualize the inconsistencies, we provide a video visualization and comparison in the supplementary material.

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/ab_stage.pdf}
\end{center}
\vspace{-4mm}
\caption{one-stage training causes subtle color and geometry inconsistencies (first row). Such inconsistencies can be resolved using our proposed 2-stage training (second row).}
\vspace{-3mm}
\label{fig_stage}
\end{figure}

\subsection{Qualitative Comparison}

In this section, We provide a visual comparison of our method with the baselines. In Fig.~\ref{fig_vis_ffhq} and ~\ref{fig_vis_cats}, we provide visual examples of variations in yaw and pitch for FFHQ and AFHQ Cats. Compared to a PC-GAN and SURF, our proposed method closely matches the 3D consistency and maintains the image quality of volumetric rendering. Fig.~\ref{fig_vis_cars} additionally provides examples of Shapenet Cars generated using our method and their corresponding images from EG3D. Similarly, our method exhibits preservation of 3D consistency and image quality, despite the difference in required computational resources. We provide more visual results in the supplementary material.

\subsection{Inversion, Interpolation, and Style Mixing}

As the proposed generator follows a StyleGAN architecture, it can easily benefit from most of the editing techniques common in the GANs' literature. Fig.~\ref{fig_edit} shows examples of inversion using Pivotal Tuning Inversion (PTI)~\cite{roich2022pivotal}, latent space interpolation, and style mixing. 

\begin{figure}
\begin{center}
\includegraphics[width=\linewidth]{figures/editing_v2.pdf}
\end{center}
\vspace{-4mm}
\caption{First row: Inversion using PTI ~\cite{roich2022pivotal}) for EG3D and our method; second row: interpolation in the latent space of our method; third row: style mixing in the latent space of our method.}
\label{fig_edit}
\end{figure}

\begin{table}[b]
\caption{PSNR values for measuring the correspondence between the images of EG3D and our convolutional generator on 3 datasets. The values are calculated for 1k images.}%
\label{tab_psnr}%
\centering
\resizebox{0.5\linewidth}{!}{%
\begin{tabular}{ll}
\multicolumn{1}{l}{Dataset} & \multicolumn{1}{l}{PSNR $\uparrow$}  \\
\toprule
\multicolumn{1}{l}{FFHQ} & \multicolumn{1}{l}{28.49}  \\
\multicolumn{1}{l}{AFHQ Cats} & \multicolumn{1}{l}{28.10}   \\
\multicolumn{1}{l}{Shapenet Cars} & \multicolumn{1}{l}{33.02} \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Correspondence between Convolutional and Volumetric Renderering}
As shown in our experiments, exploiting the style space of the pretrained NeRF-GAN, enables our convolutional renderer to achieve an efficient 3D-consistent image generator without the need for explicit multi-view supervision. As mentioned before, conditioning the convolutional generator on the NeRF-GAN's style space also provides an opportunity for establishing a direct correspondence between the 3D representation of the 3D generator and the generated images using the convolutional generator. A close comparison of images generated using the convolutional and volumetric rendering in Figures~\ref{fig_vis_ffhq}, ~\ref{fig_vis_cats}, and, ~\ref{fig_vis_cars} indicates that the convolutional render is able to infer and match many attributes of the underlying 3D representation from the shared latent space and generate images similar in content to those of volumetric rendering. However, there still remains a gap in the full correspondence of the two rendering methods, as semantic and identity changes are visible between the corresponding images generated by the two methods. Investigating more explicit approaches for enforcing correspondence could be an interesting direction for improving the convolutional rendering for NeRF-GAN models. To provide a baseline for future research, Table \ref{tab_psnr} shows the PSNR values between generated images using our method and the corresponding images rendered from EG3D.

\label{sec:experiments}