{
    "arxiv_id": "2303.15958",
    "paper_title": "Machine learning independent conservation laws through neural deflation",
    "authors": [
        "Wei Zhu",
        "Hong-Kun Zhang",
        "P. G. Kevrekidis"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "nlin.PS"
    ],
    "abstract": "We introduce a methodology for seeking conservation laws within a Hamiltonian dynamical system, which we term ``neural deflation''. Inspired by deflation methods for steady states of dynamical systems, we propose to {iteratively} train a number of neural networks to minimize a regularized loss function accounting for the necessity of conserved quantities to be {\\it in involution} and enforcing functional independence thereof consistently in the infinite-sample limit. The method is applied to a series of integrable and non-integrable lattice differential-difference equations. In the former, the predicted number of conservation laws extensively grows with the number of degrees of freedom, while for the latter, it generically stops at a threshold related to the number of conserved quantities in the system. This data-driven tool could prove valuable in assessing a model's conserved quantities and its potential integrability.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15958v1"
    ],
    "publication_venue": "6 pages, 3 figures"
}