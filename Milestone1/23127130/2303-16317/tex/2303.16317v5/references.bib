@inproceedings{marwah2023neural,
  title={Neural Network Approximations of PDEs Beyond Linearity: A Representational Perspective},
  author={Marwah, Tanya and Lipton, Zachary Chase and Lu, Jianfeng and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  pages={24139--24172},
  year={2023},
  organization={PMLR}
}

@article{darbon2016algorithms,
  title={Algorithms for overcoming the curse of dimensionality for certain {Hamilton--Jacobi} equations arising in control theory and elsewhere},
  author={Darbon, J{\'e}r{\^o}me and Osher, Stanley},
  journal={Research in the Mathematical Sciences},
  volume={3},
  number={1},
  pages={1--26},
  year={2016},
  publisher={Springer}
}

@article{darbon2020overcoming,
  title={Overcoming the curse of dimensionality for some {Hamilton--Jacobi} partial differential equations via neural network architectures},
  author={Darbon, J{\'e}r{\^o}me and Langlois, Gabriel P and Meng, Tingwei},
  journal={Research in the Mathematical Sciences},
  volume={7},
  number={3},
  pages={1--50},
  year={2020},
  publisher={Springer}
}

@article{chow2019algorithm,
  title={Algorithm for overcoming the curse of dimensionality for state-dependent {Hamilton-Jacobi} equations},
  author={Chow, Yat Tin and Darbon, J{\'e}r{\^o}me and Osher, Stanley and Yin, Wotao},
  journal={Journal of Computational Physics},
  volume={387},
  pages={376--409},
  year={2019},
  publisher={Elsevier}
}
@article{chow2017algorithm,
  title={Algorithm for overcoming the curse of dimensionality for time-dependent non-convex {Hamilton--Jacobi} equations arising from optimal control and differential games problems},
  author={Chow, Yat Tin and Darbon, J{\'e}r{\^o}me and Osher, Stanley and Yin, Wotao},
  journal={Journal of Scientific Computing},
  volume={73},
  number={2},
  pages={617--643},
  year={2017},
  publisher={Springer}
}

@article{chen2021representation,
  title={On the representation of solutions to elliptic pdes in barron spaces},
  author={Chen, Ziang and Lu, Jianfeng and Lu, Yulong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={6454--6465},
  year={2021}
}

@article{marwah2021parametric,
  title={Parametric complexity bounds for approximating PDEs with neural networks},
  author={Marwah, Tanya and Lipton, Zachary and Risteski, Andrej},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15044--15055},
  year={2021}
}

@article{siegel2022optimal,
  title={Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev Spaces},
  author={Siegel, Jonathan W},
  journal={arXiv preprint arXiv:2211.14400},
  year={2022}
}


@book{jolliffe2002principal,
  title={Principal component analysis},
  author={Jolliffe, Ian T},
  year={2002},
  series={Springer Series in Statistics},
  publisher={Springer New York, NY}
}


@article{yarotsky2017error,
  title={Error bounds for approximations with deep ReLU networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}



@inproceedings{Maurer2021,
 author = {Maurer, Andreas and Pontil, Massimiliano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7588--7597},
 publisher = {Curran Associates, Inc.},
 title = {Concentration inequalities under sub-Gaussian and sub-exponential conditions},
 url = {https://proceedings.neurips.cc/paper/2021/file/3e33b970f21d2fc65096871ea0d2c6e4-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{milbradt2020high,
  title={High-probability bounds for the reconstruction error of {PCA}},
  author={Milbradt, Cassandra and Wahl, Martin},
  journal={Statistics \& Probability Letters},
  volume={161},
  pages={108741},
  year={2020},
  publisher={Elsevier}
}


@article{reisswahl2020,
author = {Markus Rei{\ss} and Martin Wahl},
title = {{Nonasymptotic upper bounds for the reconstruction error of PCA}},
volume = {48},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1098 -- 1123},
keywords = {Concentration inequalities, excess risk, Principal Component Analysis, reconstruction error, Spectral projectors},
year = {2020},
doi = {10.1214/19-AOS1839},
URL = {https://doi.org/10.1214/19-AOS1839}
}


@inproceedings{LMHM2022,
    abbr = "ICLR",
    title = "Nonlinear Reconstruction for Operator Learning of {PDEs} with Discontinuities",
    booktitle = "11th International Conference on Learning Representations (ICLR)",
    publisher = "OpenReview.net",
    year = "2023",
    pdf = "https://openreview.net/forum?id=CrfhZAsJDsZ",
    eprint = "2210.01074",
    author = "Lanthaler, Samuel  and Molinaro, Roberto  and Hadorn, Patrik  and Mishra, Siddhartha "
}

@article{deng2021convergence,
  title={Convergence rate of DeepONets for learning operators arising from advection-diffusion equations},
  author={Deng, Beichuan and Shin, Yeonjong and Lu, Lu and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2102.10621},
  year={2021}
}


@book{brenner2008mathematical,
  title={The mathematical theory of finite element methods},
  author={Brenner, Susanne C and Scott, L Ridgway and Scott, L Ridgway},
  volume={3},
  year={2008},
  publisher={Springer New York, NY}
}


@book{leveque2002finite,
  title={Finite volume methods for hyperbolic problems},
  author={LeVeque, Randall J},
  volume={31},
  year={2002},
  publisher={Cambridge university press}
}

@book{leveque2007fd,
  title={Finite difference methods for ordinary and partial differential equations},
  author={LeVeque, Randall J},
  year={2007},
  publisher={SIAM}
}


@book{canuto2007spectral,
  title={Spectral methods: fundamentals in single domains},
  author={Canuto, Claudio and Hussaini, M Yousuff and Quarteroni, Alfio and Zang, Thomas A},
  year={2007},
  publisher={Springer Berlin, Heidelberg}
}


@article{levy1983approximation,
  title={The approximation and extension of uniformly continuous Banach space valued mappings},
  author={Levy, Ronnie and Rice, Michael D},
  journal={Commentationes Mathematicae Universitatis Carolinae},
  volume={24},
  number={2},
  pages={251--265},
  year={1983},
  publisher={Charles University in Prague, Faculty of Mathematics and Physics}
}

@article{pca2,
title = {Non-intrusive reduced order modeling of nonlinear problems using neural networks},
journal = {Journal of Computational Physics},
volume = {363},
pages = {55-78},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118301190},
author = {J.S. Hesthaven and S. Ubbiali},
keywords = {Non-intrusive reduced basis method, Proper orthogonal decomposition, Multi-layer perceptron, Levenberg–Marquardt algorithm, Poisson equation, Driven cavity flow},
abstract = {We develop a non-intrusive reduced basis (RB) method for parametrized steady-state partial differential equations (PDEs). The method extracts a reduced basis from a collection of high-fidelity solutions via a proper orthogonal decomposition (POD) and employs artificial neural networks (ANNs), particularly multi-layer perceptrons (MLPs), to accurately approximate the coefficients of the reduced model. The search for the optimal number of neurons and the minimum amount of training samples to avoid overfitting is carried out in the offline phase through an automatic routine, relying upon a joint use of the Latin hypercube sampling (LHS) and the Levenberg–Marquardt (LM) training algorithm. This guarantees a complete offline-online decoupling, leading to an efficient RB method – referred to as POD-NN – suitable also for general nonlinear problems with a non-affine parametric dependence. Numerical studies are presented for the nonlinear Poisson equation and for driven cavity viscous flows, modeled through the steady incompressible Navier–Stokes equations. Both physical and geometrical parametrizations are considered. Several results confirm the accuracy of the POD-NN method and show the substantial speed-up enabled at the online stage as compared to a traditional RB strategy.}
}


@article{cohen2010convergence,
  title={Convergence rates of best N-term Galerkin approximations for a class of elliptic {sPDEs}},
  author={Cohen, Albert and DeVore, Ronald and Schwab, Christoph},
  journal={Foundations of Computational Mathematics},
  volume={10},
  number={6},
  pages={615--646},
  year={2010},
  publisher={Springer}
}


@Techreport{OSZ2020,
  author = {J. A. A. Opschoor and Ch. Schwab and J. Zech},
  title = {Deep learning in high dimension: {ReLU} network Expression Rates for Bayesian {PDE} inversion},
  institution = {Seminar for Applied Mathematics, ETH Z{\"u}rich},
  number = {2020-47},
  address = {Switzerland},
  year = {2020}
}

@article{OSZ2019,
  title={Exponential ReLU DNN expression of holomorphic maps in high dimension},
  author={Opschoor, Joost AA and Schwab, Ch and Zech, Jakob},
  journal={Constructive Approximation},
  volume={55},
  number={1},
  pages={537--582},
  year={2022},
  publisher={Springer}
}

@article{CDS2011,
  title={Analytic regularity and polynomial approximation of parametric and stochastic elliptic {PDE}'s},
  author={Cohen, Albert and Devore, Ronald and Schwab, Christoph},
  journal={Analysis and Applications},
  volume={9},
  number={01},
  pages={11--47},
  year={2011},
  publisher={World Scientific}
}


@article{SchwabZech2019,
  title={Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in UQ},
  author={Schwab, Christoph and Zech, Jakob},
  journal={Analysis and Applications},
  volume={17},
  number={01},
  pages={19--55},
  year={2019},
  publisher={World Scientific}
}


@article{thfno,
    title = "On Universal Approximation and Error Bounds for Fourier Neural Operators",
    journal = "Journal of Machine Learning Research",
    year = "2021",
    volume = "22",
    number = "290",
    pages = "1-76",
    url = "http://jmlr.org/papers/v22/21-0806.html",
    pdf = "https://jmlr.org/papers/volume22/21-0806/21-0806.pdf",
    eprint = "2107.07562",
    author = {Nikola  Kovachki and Samuel  Lanthaler and Siddhartha  Mishra}
}

@article{thdeeponet,
    title = "{Error estimates for DeepONets: a deep learning framework in infinite dimensions}",
    journal = "Transactions of Mathematics and Its Applications",
    volume = "6",
    number = "1",
    year = "2022",
    month = "03",
    abstract = "{DeepONets have recently been proposed as a framework for learning nonlinear operators mapping between infinite-dimensional Banach spaces. We analyze DeepONets and prove estimates on the resulting approximation and generalization errors. In particular, we extend the universal approximation property of DeepONets to include measurable mappings in non-compact spaces. By a decomposition of the error into encoding, approximation and reconstruction errors, we prove both lower and upper bounds on the total error, relating it to the spectral decay properties of the covariance operators, associated with the underlying measures. We derive almost optimal error bounds with very general affine reconstructors and with random sensor locations as well as bounds on the generalization error, using covering number arguments. We illustrate our general framework with four prototypical examples of nonlinear operators, namely those arising in a nonlinear forced ordinary differential equation, an elliptic partial differential equation (PDE) with variable coefficients and nonlinear parabolic and hyperbolic PDEs. While the approximation of arbitrary Lipschitz operators by DeepONets to accuracy \\$\\epsilon \\$ is argued to suffer from a ‘curse of dimensionality’ (requiring a neural networks of exponential size in \\$1/\\epsilon \\$), in contrast, for all the above concrete examples of interest, we rigorously prove that DeepONets can break this curse of dimensionality (achieving accuracy \\$\\epsilon \\$ with neural networks of size that can grow algebraically in \\$1/\\epsilon \\$).Thus, we demonstrate the efficient approximation of a potentially large class of operators with this machine learning framework.}",
    issn = "2398-4945",
    doi = "10.1093/imatrm/tnac001",
    url = "https://doi.org/10.1093/imatrm/tnac001",
    note = "tnac001",
    eprint = "https://academic.oup.com/imatrm/article-pdf/6/1/tnac001/42785544/tnac001.pdf",
    author = {Samuel  Lanthaler and Siddhartha  Mishra and George E Karniadakis}
}

@article{besovpriors,
title = {Besov priors for Bayesian inverse problems},
journal = {Inverse Problems \& Imaging},
volume = {6},
number = {2},
pages = {183-200},
year = {2012},
author = {Masoumeh Dashti, Stephen Harris, Andrew Stuart},
}

@article{pca,
     author = {Kaushik Bhattacharya and Bamdad Hosseini and Nikola B. Kovachki and Andrew M. Stuart},
     title = {Model {Reduction} {And} {Neural} {Networks} {For} {Parametric} {PDEs}},
     journal = {The SMAI journal of computational mathematics},
     pages = {121--157},
     publisher = {Soci\'et\'e de Math\'ematiques Appliqu\'ees et Industrielles},
     volume = {7},
     year = {2021},
     doi = {10.5802/smai-jcm.74},
     language = {en},
     url = {https://smai-jcm.centre-mersenne.org/articles/10.5802/smai-jcm.74/}
}


@article{kolmorogovwidths,
    author = {Cohen, Albert and DeVore, Ronald},
    title = "{Kolmogorov widths under holomorphic mappings}",
    journal = {IMA Journal of Numerical Analysis},
    volume = {36},
    number = {1},
    pages = {1-12},
    year = {2015},
    month = {03},
    abstract = "{ If \\$L\\$ is a bounded linear operator mapping the Banach space \\$X\\$ into the Banach space \\$Y\\$ and \\$K\\$ is a compact set in \\$X\\$ , then the Kolmogorov widths of the image \\$L(K)\\$ do not exceed those of \\$K\\$ multiplied by the norm of \\$L\\$ . We extend this result from linear maps to holomorphic mappings \\$u\\$ from \\$X\\$ to \\$Y\\$ in the following sense: when the \\$n\\$ -widths of \\$K\\$ are \\$\\mathcal \\{O\\}(n^\\{-r\\})\\$ for some \\$r\\&gt;1\\$ , then those of \\$u(K)\\$ are \\$\\mathcal \\{O\\}(n^\\{-s\\})\\$ for any \\$s\\lt r-1\\$ . We then use these results to prove various theorems about Kolmogorov widths of manifolds consisting of solutions to certain parametrized partial differential equations. Results of this type are important in the numerical analysis of reduced bases and other reduced modelling methods, since the best possible performance of such methods is governed by the rate of decay of the Kolmogorov widths of the solution manifold. }",
    issn = {0272-4979},
    doi = {10.1093/imanum/dru066},
    url = {https://doi.org/10.1093/imanum/dru066},
    eprint = {https://academic.oup.com/imajna/article-pdf/36/1/1/7921079/dru066.pdf},
}

@article{DRM,
      title={Error analysis for deep neural network approximations of parametric hyperbolic conservation laws}, 
      author={Tim De Ryck and Siddhartha Mishra},
      year={2021},
      journal={arXiv preprint arXiv:2207.07362},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}



@inproceedings{fourierop2020,
title={Fourier Neural Operator for Parametric Partial Differential Equations},
author={Zongyi Li and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=c8P9NQVtmnO}
}

@article{minsker2017some,
  title={{On some extensions of Bernstein’s inequality for self-adjoint operators}},
  author={Minsker, Stanislav},
  journal={Statistics \& Probability Letters},
  volume={127},
  pages={111--119},
  year={2017},
  publisher={Elsevier}
}

@article{li2021towards,
  title={{Towards a unified analysis of random Fourier features}},
  author={Li, Zhu and Ton, Jean-Francois and Oglic, Dino and Sejdinovic, Dino},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={108},
  year={2021},
  publisher={Journal of Machine Learning Research}
}


@article{rfm,
  title={{The random feature model for input-output maps between Banach spaces}},
  author={Nelsen, Nicholas H and Stuart, Andrew M},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3212--A3243},
  year={2021},
  publisher={SIAM}
}

@article{RR2008,
  title={{Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning}},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{BM2002,
  title={{Rademacher and Gaussian complexities: Risk bounds and structural results}},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@inproceedings{maurer2016,
  title={{A vector-contraction inequality for Rademacher complexities}},
  author={Maurer, Andreas},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={3--17},
  year={2016},
  organization={Springer}
}

@book{wainwright2019high,
	title={{High-dimensional statistics: A non-asymptotic viewpoint}},
	author={Wainwright, Martin J},
	volume={48},
	year={2019},
	publisher={Cambridge University Press}
}

@article{maurer2021concentration,
  title={{Concentration inequalities under sub-Gaussian and sub-exponential conditions}},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7588--7597},
  year={2021}
}

@article{caponnetto2007optimal,
  title={Optimal rates for the regularized least-squares algorithm},
  author={Caponnetto, Andrea and De Vito, Ernesto},
  journal={Foundations of Computational Mathematics},
  volume={7},
  number={3},
  pages={331--368},
  year={2007},
  publisher={Springer-Verlag}
}

@article{cucker2002mathematical,
  title={On the mathematical foundations of learning},
  author={Cucker, Felipe and Smale, Steve},
  journal={Bulletin of the American mathematical society},
  volume={39},
  number={1},
  pages={1--49},
  year={2002}
}

@article{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{smale2007learning,
  title={Learning theory estimates via integral operators and their approximations},
  author={Smale, Steve and Zhou, Ding-Xuan},
  journal={Constructive approximation},
  volume={26},
  number={2},
  pages={153--172},
  year={2007},
  publisher={Springer}
}

@article{smale2005shannon,
  title={{Shannon sampling II: Connections to learning theory}},
  author={Smale, Steve and Zhou, Ding-Xuan},
  journal={Applied and Computational Harmonic Analysis},
  volume={19},
  number={3},
  pages={285--302},
  year={2005},
  publisher={Elsevier}
}

@article{rudi2015less,
  title={{Less is more: Nystr{\"o}m computational regularization}},
  author={Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@incollection{de2021regularization,
  title={{Regularization: From inverse problems to large-scale machine learning}},
  author={De Vito, Ernesto and Rosasco, Lorenzo and Rudi, Alessandro},
  booktitle={Harmonic and Applied Analysis},
  pages={245--296},
  year={2021},
  publisher={Springer}
}

@article{bach2017equivalence,
  title={On the equivalence between kernel quadrature rules and random feature expansions},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={714--751},
  year={2017},
  publisher={JMLR. org}
}

@article{sun2018but,
  title={{But how does it work in theory? Linear SVM with random features}},
  author={Sun, Yitong and Gilbert, Anna and Tewari, Ambuj},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{adamczak2008tail,
  title={{A tail inequality for suprema of unbounded empirical processes with applications to Markov chains}},
  author={Adamczak, Radoslaw},
  journal={Electronic Journal of Probability},
  volume={13},
  pages={1000--1034},
  year={2008},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@inproceedings{della2021regularized,
  title={{Regularized ERM on random subspaces}},
  author={Della Vecchia, Andrea and Mourtada, Jaouad and De Vito, Ernesto and Rosasco, Lorenzo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4006--4014},
  year={2021},
  organization={PMLR}
}

@article{klochkov2020uniform,
  title={{Uniform Hanson-Wright type concentration inequalities for unbounded entries via the entropy method}},
  author={Klochkov, Yegor and Zhivotovskiy, Nikita},
  year={2020},
  publisher={Institute of Mathematical Statistics}
}

@misc{ledoux1991probability,
  title={{Probability in Banach Spaces: isoperimetry and processes}},
  author={Ledoux, Michel and Talagrand, Michel},
  year={1991},
  publisher={Springer Science \& Business Media}
}

@article{pinelis1986remarks,
  title={Remarks on inequalities for large deviation probabilities},
  author={Pinelis, Iosif F and Sakhanenko, Aleksandr Ivanovich},
  journal={Theory of Probability \& Its Applications},
  volume={30},
  number={1},
  pages={143--148},
  year={1986},
  publisher={SIAM}
}

@article{ogawa1988operator,
  title={An operator pseudo-inversion lemma},
  author={Ogawa, Hidemitsu},
  journal={SIAM Journal on Applied Mathematics},
  volume={48},
  number={6},
  pages={1527--1531},
  year={1988},
  publisher={SIAM}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}



@article{LS2023,
  title={The curse of dimensionality in operator learning},
  author={Lanthaler, Samuel and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2306.15924},
  year={2023}
}


@article{achour2022general,
  title={A general approximation lower bound in $L^p$ norm, with applications to feed-forward neural networks},
  author={Achour, El Mehdi and Foucault, Armand and Gerchinovitz, S{\'e}bastien and Malgouyres, Fran{\c{c}}ois},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22396--22408},
  year={2022}
}


%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%

@inproceedings{
ryck2022generic,
title={Generic bounds on the approximation error for physics-informed (and) operator learning},
author={Tim De Ryck and Siddhartha Mishra},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=bF4eYy3LTR9}
}

@book{anthony_bartlett_1999, 
place={Cambridge}, 
title={Neural Network Learning: Theoretical Foundations}, 
DOI={10.1017/CBO9780511624216}, 
publisher={Cambridge University Press}, 
author={Anthony, Martin and Bartlett, Peter L.}, 
year={1999}
}

@inproceedings{nonlinrec,
    abbr = "ICLR",
    title = "Nonlinear Reconstruction for Operator Learning of PDEs with Discontinuities",
    booktitle = "11th International Conference on Learning Representations (ICLR)",
    publisher = "OpenReview.net",
    year = "2023",
    pdf = "https://openreview.net/forum?id=CrfhZAsJDsZ",
    eprint = "2210.01074",
    author = "Lanthaler, Samuel  and Molinaro, Roberto  and Hadorn, Patrik  and Mishra, Siddhartha "
}


@book{stein1970singular,
  title={Singular integrals and differentiability properties of functions},
  author={Stein, Elias M},
  volume={2},
  year={1970},
  publisher={Princeton university press}
}

@article{lu2021deep,
  title={Deep network approximation for smooth functions},
  author={Lu, Jianfeng and Shen, Zuowei and Yang, Haizhao and Zhang, Shijun},
  journal={SIAM Journal on Mathematical Analysis},
  volume={53},
  number={5},
  pages={5465--5506},
  year={2021},
  publisher={SIAM}
}

@article{kohler2021rate,
  title={On the rate of convergence of fully connected deep neural network regression estimates},
  author={Kohler, Michael and Langer, Sophie},
  journal={The Annals of Statistics},
  volume={49},
  number={4},
  pages={2231--2249},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}


@inproceedings{yarotsky2020phase,
  title={Optimal approximation of continuous functions by very deep ReLU networks},
  author={Yarotsky, Dmitry},
  booktitle={Conference on learning theory},
  pages={639--649},
  year={2018},
  organization={PMLR}
}


@book{fabian2011banach,
  title={Banach Space Theory: the Basis for Linear and Nonlinear Analysis},
  author={Fabian, Mari{\'a}n and Habala, Petr and H{\'a}jek, Petr and Montesinos, Vicente and Zizler, V{\'a}clav},
  year={2011},
  publisher={Springer}
}


@article{deng2021convergence,
  title={Convergence rate of DeepONets for learning operators arising from advection-diffusion equations},
  author={Deng, Beichuan and Shin, Yeonjong and Lu, Lu and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2102.10621},
  year={2021}
}


@book{stein2009real,
  title={Real Analysis: Measure Theory, Integration, and Hilbert Spaces},
  author={Stein, Elias M and Shakarchi, Rami},
  year={2009},
  publisher={Princeton University Press}
}

@book{adams2003sobolev,
  title={Sobolev spaces},
  author={Adams, Robert A and Fournier, John JF},
  year={2003},
  publisher={Elsevier}
}

@article{anitescu2019artificial,
  title={Artificial neural network methods for the solution of second order boundary value problems},
  author={Anitescu, Cosmin and Atroshchenko, Elena and Alajlan, Naif and Rabczuk, Timon},
  journal={Computers, Materials and Continua},
  volume={59},
  number={1},
  pages={345--359},
  year={2019},
  publisher={Tech Science Press}
}

@article{tancik2020fourier,
  title={Fourier features let networks learn high frequency functions in low dimensional domains},
  author={Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7537--7547},
  year={2020}
}

@article{pascon2019large,
  title={Large deformation analysis of plane-stress hyperelastic problems via triangular membrane finite elements},
  author={Pascon, Jo{\~a}o Paulo},
  journal={International Journal of Advanced Structural Engineering},
  volume={11},
  number={3},
  pages={331--350},
  year={2019},
  publisher={Springer}
} 

@article{mezic2005spectral,
  title={Spectral properties of dynamical systems, model reduction and decompositions},
  author={Mezi{\'c}, Igor},
  journal={Nonlinear Dynamics},
  volume={41},
  number={1},
  pages={309--325},
  year={2005},
  publisher={Springer}
}

@article{swischuk2018physics,
  title = {Projection-based model reduction: Formulations for physics-based machine learning},
  author = {R. Swischuk and L. Mainini and B. Peherstorfer and K. Willcox},
  journal = {Computers and Fluids},
  pages = {704-717},
  volume = 179,
  year = {2019}
}

@article{mezic2013analysis,
  title={Analysis of fluid flows via spectral properties of the {K}oopman operator},
  author={Mezi{\'c}, I.},
  journal={Annual Review of Fluid Mechanics},
  volume={45},
  pages={357--378},
  year={2013},
  publisher={Annual Reviews}
}

@article{gosea2021data,
  title={Data-driven balancing of linear dynamical systems},
  author={Gosea, Ion Victor and Gugercin, Serkan and Beattie, Christopher},
  journal={SIAM Journal on Scientific Computing},
  volume={44},
  number={1},
  pages={A554--A582},
  year={2022},
  publisher={SIAM}
}


@inproceedings{rahimi2007random,
  title={Random Features for Large-Scale Kernel Machines.},
  author={Rahimi, Ali and Recht, Benjamin and others},
  booktitle={NIPS},
  volume={3},
  number={4},
  pages={5},
  year={2007},
  organization={Citeseer}
}


@article{bhattacharya2021model,
  title={Model Reduction And Neural Networks For Parametric PDEs},
  author={Bhattacharya, Kaushik and Hosseini, Bamdad and Kovachki, Nikola B and Stuart, Andrew M},
  journal={The SMAI Journal of Computational Mathematics},
  volume={7},
  pages={121--157},
  year={2021}
}

@article{hesthaven2018non,
  title={Non-intrusive reduced order modeling of nonlinear problems using neural networks},
  author={Hesthaven, Jan S and Ubbiali, Stefano},
  journal={Journal of Computational Physics},
  volume={363},
  pages={55--78},
  year={2018},
  publisher={Elsevier}
}


@article{khoo2019switchnet,
  title={SwitchNet: a neural network model for forward and inverse scattering problems},
  author={Khoo, Yuehaw and Ying, Lexing},
  journal={SIAM Journal on Scientific Computing},
  volume={41},
  number={5},
  pages={A3182--A3201},
  year={2019},
  publisher={SIAM}
}


@article{fang2019deep,
  title={Deep physical informed neural networks for metamaterial design},
  author={Fang, Zhiwei and Zhan, Justin},
  journal={IEEE Access},
  volume={8},
  pages={24506--24513},
  year={2019},
  publisher={IEEE}
}

@article{chevyrev2021feature,
  title={Feature Engineering with Regularity Structures},
  author={Chevyrev, Ilya and Gerasimovics, Andris and Weber, Hendrik},
  journal={arXiv preprint arXiv:2108.05879},
  year={2021}
}


@inproceedings{rizzuti2019learned,
  title={Learned iterative solvers for the Helmholtz equation},
  author={Rizzuti, Gabrio and Siahkoohi, Ali and Herrmann, Felix J},
  booktitle={81st EAGE Conference and Exhibition 2019},
  volume={2019},
  number={1},
  pages={1--5},
  year={2019},
  organization={European Association of Geoscientists \& Engineers}
}


@article{stanziola2020helmholtz,
  title={A Helmholtz equation solver using unsupervised learning: Application to transcranial ultrasound},
  author={Stanziola, Antonio and Arridge, Simon R and Cox, Ben T and Treeby, Bradley E},
  journal={arXiv preprint arXiv:2010.15761},
  year={2020}
}

@article{datchev2016iterative,
  title={Iterative reconstruction of the wave speed for the wave equation with bounded frequency boundary data},
  author={Datchev, Kiril and de Hoop, Maarten V},
  journal={Inverse Problems},
  volume={32},
  number={2},
  pages={025008},
  year={2016},
  publisher={IOP Publishing}
}

@article{devore2021neural,
  title={Neural network approximation},
  author={DeVore, Ronald and Hanin, Boris and Petrova, Guergana},
  journal={Acta Numerica},
  volume={30},
  pages={327--444},
  year={2021},
  publisher={Cambridge University Press}
}



@article{gin2020deepgreen,
  title={DeepGreen: deep learning of Green’s functions for nonlinear boundary value problems},
  author={Gin, Craig R and Shea, Daniel E and Brunton, Steven L and Kutz, J Nathan},
  journal={Scientific reports},
  volume={11},
  number={1},
  pages={21614},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{kovachki2021universal,
  title={On universal approximation and error bounds for Fourier neural operators},
  author={Kovachki, Nikola and Lanthaler, Samuel and Mishra, Siddhartha},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={13237--13312},
  year={2021},
  publisher={JMLRORG}
}

@article{lanthaler2021error,
  title={Error estimates for deeponets: A deep learning framework in infinite dimensions},
  author={Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George E},
  journal={Transactions of Mathematics and Its Applications},
  volume={6},
  number={1},
  pages={tnac001},
  year={2022},
  publisher={Oxford University Press}
}


@article{weinan2019barron,
  title={Barron spaces and the compositional function spaces for neural network models},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={arXiv preprint arXiv:1906.08039},
  year={2019}
}

@article{khoo2021solving,
  title={Solving parametric {PDE} problems with artificial neural networks},
  author={Khoo, Yuehaw and Lu, Jianfeng and Ying, Lexing},
  journal={European Journal of Applied Mathematics},
  volume={32},
  number={3},
  pages={421--435},
  year={2021},
  publisher={Cambridge University Press}
}

@article{ling2016reynolds,
  title={Reynolds averaged turbulence modelling using deep neural networks with embedded invariance},
  author={Ling, Julia and Kurzawski, Andrew and Templeton, Jeremy},
  journal={Journal of Fluid Mechanics},
  volume={807},
  pages={155--166},
  year={2016},
  publisher={Cambridge University Press}
}

@article{he2018relu,
  title={{ReLU} deep neural networks and linear finite elements},
  author={He, Juncai and Li, Lin and Xu, Jinchao and Zheng, Chunyue},
  journal={arXiv preprint arXiv:1807.03973},
  year={2018}
}

@article{weinan2017proposal,
  title={A proposal on machine learning via dynamical systems},
  author={Weinan, E},
  journal={Communications in Mathematics and Statistics},
  volume={1},
  number={5},
  pages={1--11},
  year={2017}
}

@article{lu2021learning,
  title={Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators},
  author={Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Nature Machine Intelligence},
  volume={3},
  number={3},
  pages={218--229},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{li2020fourier,
  title={Fourier Neural Operator for Parametric Partial Differential Equations},
  author={Li, Zongyi and Kovachki, Nikola Borislavov and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{NN2,
  title={Operator Learning With Neural Networks},
  author={De Hoop, Maarten and Huang, Daniel Z. and Qian, Elizabeth and Stuart, Andrew M},
  journal={ongoing work},
  year={2021}
}

@article{de2022cost,
  title={The Cost-Accuracy Trade-Off In Operator Learning With Neural Networks},
  author={De Hoop, Maarten and Huang, Daniel Zhengyu and Qian, Elizabeth and Stuart, Andrew M},
  journal={To appear, Journal of Machine Learning, arXiv:2203.13181},
  year={2022}
}


@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{huang2020learning,
  title={Learning constitutive relations from indirect observations using deep neural networks},
  author={Huang, Daniel Z and Xu, Kailai and Farhat, Charbel and Darve, Eric},
  journal={Journal of Computational Physics},
  volume={416},
  pages={109491},
  year={2020},
  publisher={Elsevier}
}


@article{xu2021learning,
  title={Learning constitutive relations using symmetric positive definite neural networks},
  author={Xu, Kailai and Huang, Daniel Z and Darve, Eric},
  journal={Journal of Computational Physics},
  volume={428},
  pages={110072},
  year={2021},
  publisher={Elsevier}
}


@article{orszag1972numerical,
  title={Numerical simulation of three-dimensional homogeneous isotropic turbulence},
  author={Orszag, Steven A and Patterson Jr, GS},
  journal={Physical Review Letters},
  volume={28},
  number={2},
  pages={76},
  year={1972},
  publisher={APS}
}


@article{de2021convergence,
  title={Convergence rates for learning linear operators from noisy data},
  author={de Hoop, Maarten V and Kovachki, Nikola B and Nelsen, Nicholas H and Stuart, Andrew M},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={11},
  number={2},
  pages={480--513},
  year={2023},
  publisher={SIAM}
}


@article{chen2021solving,
  title={Solving and learning nonlinear PDEs with Gaussian processes},
  author={Chen, Yifan and Hosseini, Bamdad and Owhadi, Houman and Stuart, Andrew M},
  journal={Journal of Computational Physics},
  volume={447},
  pages={110668},
  year={2021},
  publisher={Elsevier}
}

@article{chen2021exponential,
  title={Exponential convergence for multiscale linear elliptic pdes via adaptive edge basis functions},
  author={Chen, Yifan and Hou, Thomas Y and Wang, Yixuan},
  journal={Multiscale Modeling \& Simulation},
  volume={19},
  number={2},
  pages={980--1010},
  year={2021},
  publisher={SIAM}
}

@article{kennedy2001bayesian,
  title={Bayesian calibration of computer models},
  author={Kennedy, Marc C and O'Hagan, Anthony},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={63},
  number={3},
  pages={425--464},
  year={2001},
  publisher={Wiley Online Library}
}


@article{chen2021exponentially,
  title={Exponentially convergent multiscale methods for 2d high frequency heterogeneous Helmholtz equations},
  author={Chen, Yifan and Hou, Thomas Y and Wang, Yixuan},
  journal={Multiscale Modeling \& Simulation},
  volume={21},
  number={3},
  pages={849--883},
  year={2023},
  publisher={SIAM}
}


@article{lee2020model,
  title={Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders},
  author={Lee, Kookjin and Carlberg, Kevin T},
  journal={Journal of Computational Physics},
  volume={404},
  pages={108973},
  year={2020},
  publisher={Elsevier}
}


@inproceedings{yeung2019learning,
  title={Learning deep neural network representations for Koopman operators of nonlinear dynamical systems},
  author={Yeung, Enoch and Kundu, Soumya and Hodas, Nathan},
  booktitle={2019 American Control Conference (ACC)},
  pages={4832--4839},
  year={2019},
  organization={IEEE}
}


@article{li2017extended,
  title={Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator},
  author={Li, Qianxiao and Dietrich, Felix and Bollt, Erik M and Kevrekidis, Ioannis G},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={27},
  number={10},
  pages={103111},
  year={2017},
  publisher={AIP Publishing LLC}
}


@article{morton2018deep,
  title={Deep dynamical modeling and control of unsteady fluid flows},
  author={Morton, Jeremy and Witherden, Freddie D and Jameson, Antony and Kochenderfer, Mykel J},
  journal={arXiv preprint arXiv:1805.07472},
  year={2018}
}


@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}


@article{yang2021b,
  title={B-PINNs: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data},
  author={Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={425},
  pages={109913},
  year={2021},
  publisher={Elsevier}
}


@article{zhu2018bayesian,
  title={Bayesian deep convolutional encoder--decoder networks for surrogate modeling and uncertainty quantification},
  author={Zhu, Yinhao and Zabaras, Nicholas},
  journal={Journal of Computational Physics},
  volume={366},
  pages={415--447},
  year={2018},
  publisher={Elsevier}
}


@article{chen1995universal,
  title={Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems},
  author={Chen, Tianping and Chen, Hong},
  journal={IEEE Transactions on Neural Networks},
  volume={6},
  number={4},
  pages={911--917},
  year={1995},
  publisher={IEEE}
}


@article{peherstorfer2016data,
  title={Data-driven operator inference for nonintrusive projection-based model reduction},
  author={Peherstorfer, Benjamin and Willcox, Karen},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={306},
  pages={196--215},
  year={2016},
  publisher={Elsevier}
}





@article{wen2021u,
  title={U-FNO--an enhanced {Fourier} neural operator based-deep learning model for multiphase flow},
  author={Wen, Gege and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M},
  journal={arXiv preprint arXiv:2109.03697},
  year={2021}
}


@article{stuart2010inverse,
  title={Inverse problems: a Bayesian perspective},
  author={Stuart, Andrew M},
  journal={Acta numerica},
  volume={19},
  pages={451--559},
  year={2010},
  publisher={Cambridge University Press}
}


@article{martin2012stochastic,
  title={A stochastic Newton MCMC method for large-scale statistical inverse problems with application to seismic inversion},
  author={Martin, James and Wilcox, Lucas C and Burstedde, Carsten and Ghattas, Omar},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={3},
  pages={A1460--A1487},
  year={2012},
  publisher={SIAM}
}


@article{huang2021unscented,
  title={Unscented kalman inversion},
  author={Huang, Daniel Z and Schneider, Tapio and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2102.01580},
  year={2021}
}


@article{lu2021comprehensive,
  title={A comprehensive and fair comparison of two neural operators (with practical extensions) based on {FAIR} data},
  author={Lu, Lu and Meng, Xuhui and Cai, Shengze and Mao, Zhiping and Goswami, Somdatta and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2111.05512},
  year={2021}
}


@article{morzfeld2019localization,
  title={Localization for MCMC: sampling high-dimensional posterior distributions with local structure},
  author={Morzfeld, Matthias and Tong, Xin T and Marzouk, Youssef M},
  journal={Journal of Computational Physics},
  volume={380},
  pages={1--28},
  year={2019},
  publisher={Elsevier}
}


@article{martins2013multidisciplinary,
  title={Multidisciplinary design optimization: a survey of architectures},
  author={Martins, Joaquim RRA and Lambe, Andrew B},
  journal={AIAA journal},
  volume={51},
  number={9},
  pages={2049--2075},
  year={2013},
  publisher={American Institute of Aeronautics and Astronautics}
}


@book{bendsoe2003topology,
  title={Topology optimization: theory, methods, and applications},
  author={Bendsoe, Martin Philip and Sigmund, Ole},
  year={2003},
  publisher={Springer Science \& Business Media}
}


@article{boncoraglio2021active,
  title={Active Manifold and Model-Order Reduction to Accelerate Multidisciplinary Analysis and Optimization},
  author={Boncoraglio, Gabriele and Farhat, Charbel},
  journal={AIAA Journal},
  volume={59},
  number={11},
  pages={4739--4753},
  year={2021},
  publisher={American Institute of Aeronautics and Astronautics}
}

@article{kovachki2021multiscale,
  title={Multiscale modeling of materials: Computing, data science, uncertainty and goal-oriented optimization},
  author={Kovachki, Nikola and Liu, Burigede and Sun, Xingsheng and Zhou, Hao and Bhattacharya, Kaushik and Ortiz, Michael and Stuart, Andrew},
  journal={arXiv preprint arXiv:2104.05918},
  year={2021}
}


@article{cao2021bayesian,
  title={Bayesian Calibration for Large-Scale Fluid Structure Interaction Problems Under Embedded/Immersed Boundary Framework},
  author={Cao, Shunxiang and Huang, Daniel Zhengyu},
  journal={arXiv preprint arXiv:2105.09497},
  year={2021}
}

@article{avery2021computationally,
  title={A computationally tractable framework for nonlinear dynamic multiscale modeling of membrane woven fabrics},
  author={Avery, Philip and Huang, Daniel Z and He, Wanli and Ehlers, Johanna and Derkevorkian, Armen and Farhat, Charbel},
  journal={International Journal for Numerical Methods in Engineering},
  volume={122},
  number={10},
  pages={2598--2625},
  year={2021},
  publisher={Wiley Online Library}
}


@article{fish1997computational,
  title={Computational plasticity for composite structures based on mathematical homogenization: Theory and practice},
  author={Fish, Jacob and Shek, Kamlun and Pandheeradi, Muralidharan and Shephard, Mark S},
  journal={Computer methods in applied mechanics and engineering},
  volume={148},
  number={1-2},
  pages={53--73},
  year={1997},
  publisher={Elsevier}
}


@article{feyel2000fe2,
  title={FE2 multiscale approach for modelling the elastoviscoplastic behaviour of long fibre SiC/Ti composite materials},
  author={Feyel, Fr{\'e}d{\'e}ric and Chaboche, Jean-Louis},
  journal={Computer methods in applied mechanics and engineering},
  volume={183},
  number={3-4},
  pages={309--330},
  year={2000},
  publisher={Elsevier}
}


@article{economon2016su2,
  title={SU2: An open-source suite for multiphysics simulation and design},
  author={Economon, Thomas D and Palacios, Francisco and Copeland, Sean R and Lukaczyk, Trent W and Alonso, Juan J},
  journal={Aiaa Journal},
  volume={54},
  number={3},
  pages={828--846},
  year={2016},
  publisher={American Institute of Aeronautics and Astronautics}
}


@article{schmid2010dynamic, title={Dynamic mode decomposition of numerical and experimental data}, author={Schmid, Peter J}, journal={Journal of fluid mechanics}, volume={656}, pages={5--28}, year={2010}, publisher={Cambridge University Press} }



@article{rowley2009spectral,
  title={Spectral analysis of nonlinear flows},
  author={Rowley, Clarence W and Mezi{\'c}, Igor and Bagheri, Shervin and Schlatter, Philipp and Henningson, Dan S},
  journal={Journal of fluid mechanics},
  volume={641},
  pages={115--127},
  year={2009},
  publisher={Cambridge University Press}
}


@phdthesis{tu2013dynamic,
  title={Dynamic mode decomposition: Theory and applications},
  author={Tu, Jonathan H},
  year={2013},
  school={Princeton University}
}

@phdthesis{kovachki22,
  title={Machine Learning and Scientific Computing},
  author={Kovacvhki, N.B.},
  year={2022},
  school={California Institute of Technology}
}


@article{taira2017modal,
  title={Modal analysis of fluid flows: An overview},
  author={Taira, Kunihiko and Brunton, Steven L and Dawson, Scott TM and Rowley, Clarence W and Colonius, Tim and McKeon, Beverley J and Schmidt, Oliver T and Gordeyev, Stanislav and Theofilis, Vassilios and Ukeiley, Lawrence S},
  journal={Aiaa Journal},
  volume={55},
  number={12},
  pages={4013--4041},
  year={2017},
  publisher={American Institute of Aeronautics and Astronautics}
}

@article{qian2020lift, title={Lift \& learn: Physics-informed machine learning for large-scale nonlinear dynamical systems}, author={Qian, Elizabeth and Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen}, journal={Physica D: Nonlinear Phenomena}, volume={406}, pages={132401}, year={2020}, publisher={Elsevier} }


@article{qian2021reduced,
  title={Reduced operator inference for nonlinear partial differential equations},
  author={Qian, Elizabeth and Farcas, Ionut-Gabriel and Willcox, Karen},
  journal={SIAM Journal on Scientific Computing},
  publisher={SIAM},
  year={to appear, 2022}
}


@article{carlberg2013gnat,
  title={The GNAT method for nonlinear model reduction: effective implementation and application to computational fluid dynamics and turbulent flows},
  author={Carlberg, Kevin and Farhat, Charbel and Cortial, Julien and Amsallem, David},
  journal={Journal of Computational Physics},
  volume={242},
  pages={623--647},
  year={2013},
  publisher={Elsevier}
}


@article{grimberg2021mesh,
  title={Mesh sampling and weighting for the hyperreduction of nonlinear Petrov--Galerkin reduced-order models with local reduced-order bases},
  author={Grimberg, Sebastian and Farhat, Charbel and Tezaur, Radek and Bou-Mosleh, Charbel},
  journal={International Journal for Numerical Methods in Engineering},
  volume={122},
  number={7},
  pages={1846--1874},
  year={2021},
  publisher={Wiley Online Library}
}


@article{cleary2021calibrate,
  title={Calibrate, emulate, sample},
  author={Cleary, Emmet and Garbuno-Inigo, Alfredo and Lan, Shiwei and Schneider, Tapio and Stuart, Andrew M},
  journal={Journal of Computational Physics},
  volume={424},
  pages={109716},
  year={2021},
  publisher={Elsevier}
}

@article{zhang2018deep,
  title={Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics},
  author={Zhang, Linfeng and Han, Jiequn and Wang, Han and Car, Roberto and Weinan, E},
  journal={Physical review letters},
  volume={120},
  number={14},
  pages={143001},
  year={2018},
  publisher={APS}
}


@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{liu2022learning,
  title={A learning-based multiscale method and its application to inelastic impact problems},
  author={Liu, Burigede and Kovachki, Nikola and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Stuart, Andrew M and Bhattacharya, Kaushik},
  journal={Journal of the Mechanics and Physics of Solids},
  volume={158},
  pages={104668},
  year={2022},
  publisher={Elsevier}
}

@article{ghaboussi1998autoprogressive,
  title={Autoprogressive training of neural network constitutive models},
  author={Ghaboussi, Jamshid and Pecknold, David A and Zhang, Mingfu and Haj-Ali, Rami M},
  journal={International Journal for Numerical Methods in Engineering},
  volume={42},
  number={1},
  pages={105--126},
  year={1998},
  publisher={Wiley Online Library}
}


@article{liu2021a,
title = {A review of artificial neural networks in the constitutive modeling of composite materials},
journal = {Composites Part B: Engineering},
volume = {224},
pages = {109152},
year = {2021},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2021.109152},
url = {https://www.sciencedirect.com/science/article/pii/S1359836821005321},
author = {Xin Liu and Su Tian and Fei Tao and Wenbin Yu},
}


@article{wang2017physics,
  title = {Physics-informed machine learning approach for reconstructing Reynolds stress modeling discrepancies based on DNS data},
  author = {Wang, Jian-Xun and Wu, Jin-Long and Xiao, Heng},
  journal = {Phys. Rev. Fluids},
  volume = {2},
  issue = {3},
  pages = {034603},
  numpages = {22},
  year = {2017},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevFluids.2.034603},
  url = {https://link.aps.org/doi/10.1103/PhysRevFluids.2.034603}
}

article{ling2016reynolds, title={Reynolds averaged turbulence modelling using deep neural networks with embedded invariance}, volume={807}, DOI={10.1017/jfm.2016.615}, journal={Journal of Fluid Mechanics}, publisher={Cambridge University Press}, author={Ling, Julia and Kurzawski, Andrew and Templeton, Jeremy}, year={2016}, pages={155–166}}


@article{wu2018physics,
  title = {Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework},
  author = {Wu, Jin-Long and Xiao, Heng and Paterson, Eric},
  journal = {Phys. Rev. Fluids},
  volume = {3},
  issue = {7},
  pages = {074602},
  numpages = {28},
  year = {2018},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevFluids.3.074602},
  url = {https://link.aps.org/doi/10.1103/PhysRevFluids.3.074602}
}



@article{duraisamy2019turbulence,
author = {Duraisamy, Karthik and Iaccarino, Gianluca and Xiao, Heng},
title = {Turbulence Modeling in the Age of Data},
journal = {Annual Review of Fluid Mechanics},
volume = {51},
number = {1},
pages = {357-377},
year = {2019},
doi = {10.1146/annurev-fluid-010518-040547},

URL = { 
        https://doi.org/10.1146/annurev-fluid-010518-040547
    
},
}

@book{weinan2011principles,
  title={Principles of multiscale modeling},
  author={Weinan, E},
  year={2011},
  publisher={Cambridge University Press}
}

@book{song2016cyber,
  title={Cyber-physical systems: foundations, principles and applications},
  author={Song, Houbing and Rawat, Danda B and Jeschke, Sabina and Brecher, Christian},
  year={2016},
  publisher={Morgan Kaufmann}
}






@article{han2018solving,
  title={Solving high-dimensional partial differential equations using deep learning},
  author={Han, Jiequn and Jentzen, Arnulf and Weinan, E},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={34},
  pages={8505--8510},
  year={2018},
  publisher={National Acad Sciences}
}
@article{han2017deep, title={Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations}, author={Han, Jiequn and Jentzen, Arnulf and others}, journal={Communications in Mathematics and Statistics}, volume={5}, number={4}, pages={349--380}, year={2017}, publisher={Springer} }



@article{lu202186,
  title={86 PFLOPS Deep Potential Molecular Dynamics simulation of 100 million atoms with ab initio accuracy},
  author={Lu, Denghui and Wang, Han and Chen, Mohan and Lin, Lin and Car, Roberto and Weinan, E and Jia, Weile and Zhang, Linfeng},
  journal={Computer Physics Communications},
  volume={259},
  pages={107624},
  year={2021},
  publisher={Elsevier}
}

@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}

@article{raissi2020hidden,
  title={Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations},
  author={Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
  journal={Science},
  volume={367},
  number={6481},
  pages={1026--1030},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@article{sirignano2018dgm,
  title={DGM: A deep learning algorithm for solving partial differential equations},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Journal of computational physics},
  volume={375},
  pages={1339--1364},
  year={2018},
  publisher={Elsevier}
}

@article{hu2021extended,
  title={When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?},
  author={Hu, Zheyuan and Jagtap, Ameya D and Karniadakis, George Em and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:2109.09444},
  year={2021}
}

% this includes discussion of discontinuous functions and we might want to reference in relation to your experiments.
@article{jagtap2020adaptive,
  title={Adaptive activation functions accelerate convergence in deep and physics-informed neural networks},
  author={Jagtap, Ameya D and Kawaguchi, Kenji and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={404},
  pages={109136},
  year={2020},
  publisher={Elsevier}
}


@article{berg2018unified,
  title={A unified deep artificial neural network approach to partial differential equations in complex geometries},
  author={Berg, Jens and Nystr{\"o}m, Kaj},
  journal={Neurocomputing},
  volume={317},
  pages={28--41},
  year={2018},
  publisher={Elsevier}
}

@article{goswami2020transfer,
  title={Transfer learning enhanced physics informed neural network for phase-field modeling of fracture},
  author={Goswami, Somdatta and Anitescu, Cosmin and Chakraborty, Souvik and Rabczuk, Timon},
  journal={Theoretical and Applied Fracture Mechanics},
  volume={106},
  pages={102447},
  year={2020},
  publisher={Elsevier}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@article{sun2020physics,
  title={Physics-constrained bayesian neural network for fluid flow reconstruction with sparse and noisy data},
  author={Sun, Luning and Wang, Jian-Xun},
  journal={Theoretical and Applied Mechanics Letters},
  volume={10},
  number={3},
  pages={161--169},
  year={2020},
  publisher={Elsevier}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

% here they used PINNs for Euler 1d.

@article{mao2020physics,
  title={Physics-informed neural networks for high-speed flows},
  author={Mao, Zhiping and Jagtap, Ameya D and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={360},
  pages={112789},
  year={2020},
  publisher={Elsevier}
}

% more on adaptive activation
@article{jagtap2022deep,
  title={Deep Kronecker neural networks: A general framework for neural networks with adaptive activation functions},
  author={Jagtap, Ameya D and Shin, Yeonjong and Kawaguchi, Kenji and Karniadakis, George Em},
  journal={Neurocomputing},
  volume={468},
  pages={165--180},
  year={2022},
  publisher={Elsevier}
}


%
@article{jagtap2020locally,
  title={Locally adaptive activation functions with slope recovery for deep and physics-informed neural networks},
  author={Jagtap, Ameya D and Kawaguchi, Kenji and Em Karniadakis, George},
  journal={Proceedings of the Royal Society A},
  volume={476},
  number={2239},
  pages={20200334},
  year={2020},
  publisher={The Royal Society}
}

@book{demmel1997applied,
  title={Applied numerical linear algebra},
  author={Demmel, James W},
  year={1997},
  publisher={SIAM}
}

@book{trefethen1997numerical,
  title={Numerical linear algebra},
  author={Trefethen, Lloyd N and Bau III, David},
  volume={50},
  year={1997},
  publisher={Siam}
}

@book{jovanovic2013analysis,
  title={Analysis of Finite Difference Schemes: For Linear Partial Differential Equations with Generalized Solutions},
  author={Jovanovi{\'c}, Bo{\v{s}}ko S and S{\"u}li, Endre},
  volume={46},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@book{johnson2012numerical,
  title={Numerical solution of partial differential equations by the finite element method},
  author={Johnson, Claes},
  year={2012},
  publisher={Courier Corporation}
}

@book{canuto2012spectral,
  title={Spectral methods in fluid dynamics},
  author={Canuto, Claudio and Hussaini, M Yousuff and Quarteroni, Alfio and Thomas Jr, A and others},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{trefethen2000spectral,
  title={Spectral methods in MATLAB},
  author={Trefethen, Lloyd N},
  year={2000},
  publisher={SIAM}
}

@book{strikwerda2004finite,
  title={Finite difference schemes and partial differential equations},
  author={Strikwerda, John C},
  year={2004},
  publisher={SIAM}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International conference on machine learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@book{goodfellow2016deep,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{shin2020convergence,
  title={On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs},
  author={Shin, Yeonjong and Darbon, Jerome and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2004.01806},
  year={2020}
}

@article{giles2015multilevel,
  title={Multilevel monte carlo methods},
  author={Giles, Michael B},
  journal={Acta numerica},
  volume={24},
  pages={259--328},
  year={2015},
  publisher={Cambridge University Press}
}
@article{giles2008multilevel,
  title={Multilevel monte carlo path simulation},
  author={Giles, Michael B},
  journal={Operations research},
  volume={56},
  number={3},
  pages={607--617},
  year={2008},
  publisher={INFORMS}
}



% ROM Ron DeVore

@article{binev2017data,
  title={Data assimilation in reduced modeling},
  author={Binev, Peter and Cohen, Albert and Dahmen, Wolfgang and DeVore, Ronald and Petrova, Guergana and Wojtaszczyk, Przemyslaw},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={5},
  number={1},
  pages={1--29},
  year={2017},
  publisher={SIAM}
}

@article{chkifa2013sparse,
  title={Sparse adaptive Taylor approximation algorithms for parametric and stochastic elliptic PDEs∗},
  author={Chkifa, Abdellah and Cohen, Albert and DeVore, Ronald and Schwab, Christoph},
  journal={ESAIM: Mathematical Modelling and Numerical Analysis},
  volume={47},
  number={1},
  pages={253--280},
  year={2013},
  publisher={EDP Sciences}
}

@article{cohen2020state, title={State Estimation--The Role of Reduced Models}, author={Cohen, Albert and Dahmen, Wolfgang and DeVore, Ron}, journal={arXiv preprint arXiv:2002.00220}, year={2020} }


@article{cohen2015approximation,
  title={Approximation of high-dimensional parametric PDEs},
  author={Cohen, Albert and DeVore, Ronald},
  journal={Acta Numerica},
  volume={24},
  pages={1--159},
  year={2015},
  publisher={Cambridge University Press}
}



@article{cohen2011analytic,
  title={Analytic regularity and polynomial approximation of parametric and stochastic elliptic PDE's},
  author={Cohen, Albert and Devore, Ronald and Schwab, Christoph},
  journal={Analysis and Applications},
  volume={9},
  number={01},
  pages={11--47},
  year={2011},
  publisher={World Scientific}
}


@article{daubechies2021nonlinear,
  title={Nonlinear Approximation and (Deep)  {ReLU} Networks},
  author={Daubechies, Ingrid and DeVore, Ronald and Foucart, Simon and Hanin, Boris and Petrova, Guergana},
  journal={Constructive Approximation},
  pages={1--46},
  year={2021},
  publisher={Springer}
}


@article{devore1998nonlinear,
  title={Nonlinear approximation},
  author={DeVore, Ronald A},
  journal={Acta numerica},
  volume={7},
  pages={51--150},
  year={1998},
  publisher={Cambridge University Press}
}

@article{devore2014theoretical,
  title={The theoretical foundation of reduced basis methods},
  author={DeVore, Ronald A},
  journal={Model Reduction and approximation: Theory and Algorithms},
  pages={137--168},
  year={2014}
}


@article{herrmann2020deep,
  title={Deep {ReLU} neural network expression rates for data-to-QoI maps in {Bayesian} PDE inversion},
  author={Herrmann, Lukas and Schwab, Christoph and Zech, Jakob},
  journal={SAM Research Report},
  volume={2020},
  year={2020},
  publisher={ETH Zurich}
}


@article{schwab2019deep,
  title={Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in {UQ}},
  author={Schwab, Christoph and Zech, Jakob},
  journal={Analysis and Applications},
  volume={17},
  number={01},
  pages={19--55},
  year={2019},
  publisher={World Scientific}
}
@book{hackbusch2013multi,
  title={Multi-grid methods and applications},
  author={Hackbusch, Wolfgang},
  volume={4},
  year={2013},
  publisher={Springer Science \& Business Media}
}
@book{owhadi2019operator,
  title={Operator-Adapted Wavelets, Fast Solvers, and Numerical Homogenization: From a Game Theoretic Approach to Numerical Approximation and Algorithm Design},
  author={Owhadi, Houman and Scovel, Clint},
  volume={35},
  year={2019},
  publisher={Cambridge University Press}
}




@article{fan2020solving,
  title={Solving electrical impedance tomography with deep learning},
  author={Fan, Yuwei and Ying, Lexing},
  journal={Journal of Computational Physics},
  volume={404},
  pages={109119},
  year={2020},
  publisher={Elsevier}
}

@article{fan2019solving,
  title={Solving inverse wave scattering with deep learning},
  author={Fan, Yuwei and Ying, Lexing},
  journal={arXiv preprint arXiv:1911.13202},
  year={2019}
}
@article{nelsen2021random,
  title={The random feature model for input-output maps between banach spaces},
  author={Nelsen, Nicholas H and Stuart, Andrew M},
  journal={SIAM Journal on Scientific Computing},
  volume={43},
  number={5},
  pages={A3212--A3243},
  year={2021},
  publisher={SIAM}
}


@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@article{caflisch1998monte,
  title={Monte carlo and quasi-monte carlo methods},
  author={Caflisch, Russel E},
  journal={Acta numerica},
  volume={7},
  pages={1--49},
  year={1998},
  publisher={Cambridge University Press}
}
@article{gottlieb1997gibbs,
  title={On the Gibbs phenomenon and its resolution},
  author={Gottlieb, David and Shu, Chi-Wang},
  journal={SIAM review},
  volume={39},
  number={4},
  pages={644--668},
  year={1997},
  publisher={SIAM}
}
@article{kissas2022learning,
  title={Learning Operators with Coupled Attention},
  author={Kissas, Georgios and Seidman, Jacob and Guilhoto, Leonardo Ferreira and Preciado, Victor M and Pappas, George J and Perdikaris, Paris},
  journal={arXiv preprint arXiv:2201.01032},
  year={2022}
}

@article{COMPLEXITY,
  title={The Cost-Accuracy Trade-Off In Operator Learning With Neural Networks},
  author={De Hoop, Maarten and Huang, Daniel and Qian, Elizabeth and Stuart, Andrew M},
  journal={arXiv},
  volume={},
  pages={},
  year={},
  publisher={}
}
@article{chandler2013invariant,
  title={Invariant recurrent solutions embedded in a turbulent two-dimensional Kolmogorov flow},
  author={Chandler, Gary J and Kerswell, Rich R},
  journal={Journal of Fluid Mechanics},
  volume={722},
  pages={554--595},
  year={2013},
  publisher={Cambridge University Press}
}
@book{majda2006nonlinear,
  title={Nonlinear dynamics and statistical theories for basic geophysical flows},
  author={Majda, Andrew and Wang, Xiaoming},
  year={2006},
  publisher={Cambridge University Press}
}
@article{majda2001vorticity,
  title={Vorticity and Incompressible Flow},
  author={Majda, Andrew J and Bertozzi, Andrea L},
  journal={Vorticity and Incompressible Flow},
  pages={558},
  year={2001}
}

@article{farazmand2017variational,
  title={A variational approach to probing extreme events in turbulent dynamical systems},
  author={Farazmand, Mohammad and Sapsis, Themistoklis P},
  journal={Science advances},
  volume={3},
  number={9},
  pages={e1701533},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

@article{korolev2021two,
  title={Two-layer neural networks with values in a Banach space},
  author={Korolev, Yury},
  journal={arXiv preprint arXiv:2105.02095},
  year={2021}
}

@article{knapik2011bayesian,
  title={Bayesian inverse problems with Gaussian priors},
  author={Knapik, Bartek T and Van Der Vaart, Aad W and van Zanten, J Harry},
  journal={The Annals of Statistics},
  volume={39},
  number={5},
  pages={2626--2657},
  year={2011},
  publisher={Institute of Mathematical Statistics}
}


@article{bhattacharya2022learning,
  title={Learning Markovian Homogenized Models in Viscoelasticity},
  author={Bhattacharya, Kaushik and Liu, Burigede and Stuart, Andrew and Trautner, Margaret},
  journal={arXiv preprint arXiv:2205.14139},
  year={2022}
}

@article{boulle2022learning,
  title={Learning elliptic partial differential equations with randomized linear algebra},
  author={Boull{\'e}, Nicolas and Townsend, Alex},
  journal={Foundations of Computational Mathematics},
  pages={1--31},
  year={2022},
  publisher={Springer}
}

@article{boulle2022data,
  title={Data-driven discovery of Green’s functions with human-understandable deep learning},
  author={Boull{\'e}, Nicolas and Earls, Christopher J and Townsend, Alex},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={1--9},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{boulle2022learningb,
  title={Learning Green's functions associated with parabolic partial differential equations},
  author={Boull{\'e}, Nicolas and Kim, Seick and Shi, Tianyi and Townsend, Alex},
  journal={arXiv preprint arXiv:2204.12789},
  year={2022}
}

@article{zhang2017hamiltonian,
  title={{Hamiltonian Monte Carlo} acceleration using surrogate functions with random bases},
  author={Zhang, Cheng and Shahbaba, Babak and Zhao, Hongkai},
  journal={Statistics and computing},
  volume={27},
  number={6},
  pages={1473--1490},
  year={2017},
  publisher={Springer}
}

@article{li2020data,
  title={A data-driven approach for multiscale elliptic PDEs with random coefficients based on intrinsic dimension reduction},
  author={Li, Sijing and Zhang, Zhiwen and Zhao, Hongkai},
  journal={Multiscale Modeling \& Simulation},
  volume={18},
  number={3},
  pages={1242--1271},
  year={2020},
  publisher={SIAM}
}

@article{li2021data,
  title={A data-driven and model-based accelerated {Hamiltonian Monte Carlo} method for Bayesian elliptic inverse problems},
  author={Li, Sijing and Zhang, Cheng and Zhang, Zhiwen and Zhao, Hongkai},
  journal={arXiv preprint arXiv:2104.13070},
  year={2021}
}

@article{stuart2018posterior,
  title={Posterior consistency for Gaussian process approximations of Bayesian posterior distributions},
  author={Stuart, Andrew and Teckentrup, Aretha},
  journal={Mathematics of Computation},
  volume={87},
  number={310},
  pages={721--753},
  year={2018}
}

@article{metropolis1953equation,
  title={Equation of state calculations by fast computing machines},
  author={Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  journal={The journal of chemical physics},
  volume={21},
  number={6},
  pages={1087--1092},
  year={1953},
  publisher={American Institute of Physics}
}

@article{cotter2013mcmc,
  title={MCMC methods for functions: modifying old algorithms to make them faster},
  author={Cotter, Simon L and Roberts, Gareth O and Stuart, Andrew M and White, David},
  journal={Statistical Science},
  volume={28},
  number={3},
  pages={424--446},
  year={2013},
  publisher={Institute of Mathematical Statistics}
}

@article{hairer2014spectral,
  title={Spectral gaps for a Metropolis--Hastings algorithm in infinite dimensions},
  author={Hairer, Martin and Stuart, Andrew M and Vollmer, Sebastian J},
  journal={The Annals of Applied Probability},
  volume={24},
  number={6},
  pages={2455--2490},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@book{wendland2004scattered,
  title={Scattered Data Approximation},
  author={Wendland, Holger},
  volume={17},
  year={2004},
  publisher={Cambridge university press}
}

@book{chow2012methods,
  title={Methods of Bifurcation Theory},
  author={Chow, S-N and Hale, Jack K},
  volume={251},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{yarotsky_error_2017,
	title = {Error bounds for approximations with deep {ReLU} networks},
	volume = {94},
	journal = {Neural Networks},
	author = {Yarotsky, Dmitry},
	year = {2017},
	note = {Publisher: Elsevier},
	pages = {103--114},
}

@book{evans,
  abstract = {"This is the second edition of the now definitive text on partial differential equations (PDE). It offers a comprehensive survey of modern techniques in the theoretical study of PDE with particular emphasis on nonlinear equations. Its wide scope and clear exposition make it a great text for a graduate course in PDE. For this edition, the author has made numerous changes, including: a new chapter on nonlinear wave equations, more than 80 new exercises, several new sections, and a significantly expanded bibliography."--Publisher's description.},
  added-at = {2015-07-29T08:37:26.000+0200},
  address = {Providence, R.I.},
  author = {Evans, Lawrence C.},
  biburl = {https://www.bibsonomy.org/bibtex/2f5b120723ea78913e7e700ddd1a99301/ytyoun},
  interhash = {59982ce44cc43813ccb14c0d647a59ee},
  intrahash = {f5b120723ea78913e7e700ddd1a99301},
  isbn = {9780821849743 0821849743},
  keywords = {partial.differential.equations pde textbook},
  publisher = {American Mathematical Society},
  refid = {465190110},
  timestamp = {2015-07-29T08:37:26.000+0200},
  title = {Partial Differential Equations},
  year = 2010
}

@article{jin2020sympnets,
  title={SympNets: Intrinsic structure-preserving symplectic networks for identifying {Hamiltonian} systems},
  author={Jin, Pengzhan and Zhang, Zhen and Zhu, Aiqing and Tang, Yifa and Karniadakis, George Em},
  journal={Neural Networks},
  volume={132},
  pages={166--179},
  year={2020},
  publisher={Elsevier}
}



@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}




%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{evans2022partial,
  title={Partial differential equations},
  author={Evans, Lawrence C},
  volume={19},
  edition={2nd},
  year={2010},
  publisher={American Mathematical Society, Providence, Rhode Island}
}


@article{patel2022variationally,
      title={Variationally Mimetic Operator Networks}, 
      author={Dhruv Patel and Deep Ray and Michael R. A. Abdelmalik and Thomas J. R. Hughes and Assad A. Oberai},
      year={2022},
      journal={arXiv preprint arXiv:2209.12871},
      eprint={2209.12871},
      archivePrefix={arXiv},
      primaryClass={math.NA}
}

@book{stein1970singular,
 author = {Stein, Elias M.},
 publisher = {Princeton University Press},
 title = {Singular Integrals and Differentiability Properties of Functions},
 year = {1970}
}

@article{pinkus1999approximation,
  title={Approximation theory of the {MLP} model in neural networks},
  author={Pinkus, Allan},
  journal={Acta numerica},
  volume={8},
  pages={143--195},
  year={1999},
  publisher={Cambridge University Press}
}


@article{mionet,
author = {Jin, Pengzhan and Meng, Shuai and Lu, Lu},
title = {MIONet: learning Multiple-Input Operators via Tensor Product},
journal = {SIAM Journal on Scientific Computing},
volume = {44},
number = {6},
pages = {A3490-A3514},
year = {2022},
doi = {10.1137/22M1477751},

URL = { 
    
        https://doi.org/10.1137/22M1477751
    
    

},
eprint = { 
    
        https://doi.org/10.1137/22M1477751
    
    

}
,
    abstract = { As an emerging paradigm in scientific machine learning, neural operators aim to learn operators, via neural networks, that map between infinite-dimensional function spaces. Several neural operators have been recently developed. However, all the existing neural operators are only designed to learn operators defined on a single Banach space; i.e., the input of the operator is a single function. Here, for the first time, we study the operator regression via neural networks for multiple-input operators defined on the product of Banach spaces. We first prove a universal approximation theorem of continuous multiple-input operators. We also provide a detailed theoretical analysis including the approximation error, which provides guidance for the design of the network architecture. Based on our theory and a low-rank approximation, we propose a novel neural operator, MIONet, to learn multiple-input operators. MIONet consists of several branch nets for encoding the input functions and a trunk net for encoding the domain of the output function. We demonstrate that MIONet can learn solution operators involving systems governed by ordinary and partial differential equations. In our computational examples, we also show that we can endow MIONet with prior knowledge of the underlying system, such as linearity and periodicity, to further improve accuracy. }
}


@book{jackson1930theory,
  title={The Theory of Approximation},
  author={Jackson, Dunham},
  volume={11},
  year={1930},
  publisher={American Mathematical Soc.}
}

@article{blouza2001up,
  title={An up-to-the boundary version of {Friedrichs's} lemma and applications to the linear {Koiter} shell model},
  author={Blouza, Adel and Le Dret, Herv{\'e}},
  journal={SIAM Journal on Mathematical Analysis},
  volume={33},
  number={4},
  pages={877--895},
  year={2001},
  publisher={SIAM}
}

@article{ErnGuermond2016,
url = {https://doi.org/10.1515/cmam-2015-0034},
title = {Mollification in Strongly Lipschitz Domains with Application to Continuous and Discrete De Rham Complexes},
title = {},
author = {Alexandre Ern and Jean-Luc Guermond},
pages = {51--75},
volume = {16},
number = {1},
journal = {Computational Methods in Applied Mathematics},
doi = {doi:10.1515/cmam-2015-0034},
year = {2016},
lastchecked = {2023-02-10}
}

@inproceedings{
seidman2022nomad,
title={{NOMAD}: Nonlinear Manifold Decoders for Operator Learning},
author={Jacob H Seidman and Georgios Kissas and Paris Perdikaris and George J. Pappas},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=5OWV-sZvMl}
}

@article{hesthaven_nonintrusive_2018,
	title = {Non-intrusive reduced order modeling of nonlinear problems using neural networks},
	volume = {363},
	journal = {Journal of Computational Physics},
	author = {Hesthaven, Jan S and Ubbiali, Stefano},
	year = {2018},
	note = {Publisher: Elsevier},
	pages = {55--78},
}


@article{gupta2021multiwavelet,
  title={Multiwavelet-based operator learning for differential equations},
  author={Gupta, Gaurav and Xiao, Xiongye and Bogdan, Paul},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24048--24062},
  year={2021}
}

@article{tripura2023,
title = {Wavelet Neural Operator for solving parametric partial differential equations in computational mechanics problems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {404},
pages = {115783},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115783},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522007393},
author = {Tapas Tripura and Souvik Chakraborty},
keywords = {Nonlinear mappings, Operator learning, Wavelet, Wavelet neural operator, Scientific machine learning},
abstract = {With massive advancements in sensor technologies and Internet-of-things (IoT), we now have access to terabytes of historical data; however, there is a lack of clarity on how to best exploit the data to predict future events. One possible alternative in this context is to utilize an operator learning algorithm that directly learns the nonlinear mapping between two functional spaces; this facilitates real-time prediction of naturally arising complex evolutionary dynamics. In this work, we introduce a novel operator learning algorithm referred to as the Wavelet Neural Operator (WNO) that blends integral kernel with wavelet transformation. WNO harnesses the superiority of the wavelets in time–frequency localization of the functions and enables accurate tracking of patterns in the spatial domain and effective learning of the functional mappings. Since the wavelets are localized in both time/space and frequency, WNO can provide high spatial and frequency resolution. This offers learning of the finer details of the parametric dependencies in the solution for complex problems. The efficacy and robustness of the proposed WNO are illustrated on a wide array of problems involving Burger’s equation, Darcy flow, Navier–Stokes equation, Allen–Cahn equation, and Wave advection equation. A comparative study with respect to existing operator learning frameworks is presented. Finally, the proposed approach is used to build a digital twin capable of predicting Earth’s air temperature based on available historical data.}
}



@article{bhattacharya_model_2021,
	title = {Model Reduction And Neural Networks For Parametric PDEs},
	volume = {7},
	url = {https://smai-jcm.centre-mersenne.org/articles/10.5802/smai-jcm.74/},
	doi = {10.5802/smai-jcm.74},
	language = {en},
	journal = {The SMAI Journal of Computational Mathematics},
	author = {Bhattacharya, Kaushik and Hosseini, Bamdad and Kovachki, Nikola B. and Stuart, Andrew M.},
	year = {2021},
	note = {Publisher: Société de Mathématiques Appliquées et Industrielles},
	pages = {121--157},
}

@article{prasthofer2022variable,
  title={Variable-input deep operator networks},
  author={Prasthofer, Michael and De Ryck, Tim and Mishra, Siddhartha},
  journal={arXiv preprint arXiv:2205.11404},
  year={2022}
}


@article{kovachki2021neural,
  title={Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs.},
  author={Kovachki, Nikola B and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew M and Anandkumar, Anima},
  journal={J. Mach. Learn. Res.},
  volume={24},
  number={89},
  pages={1--97},
  year={2023}
}

@inproceedings{li_fourier_2021,
	title = {Fourier Neural Operator for Parametric Partial Differential Equations},
	url = {https://openreview.net/forum?id=c8P9NQVtmnO},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Li, Zongyi and Kovachki, Nikola Borislavov and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2021},
}


@inproceedings{li_multipole_2020,
	title = {Multipole Graph Neural Operator for Parametric Partial Differential Equations},
	volume = {33},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Li, Zongyi and Kovachki, Nikola B and Azizzadenesheli, Kamyar and Liu, Burigede and Stuart, Andrew M and Bhattacharya, Kaushik and Anandkumar, Anima},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {6755--6766},
}


@inproceedings{anandkumar_neural_2020,
	title = {Neural {Operator}: {Graph} {Kernel} {Network} for {Partial} {Differential} {Equations}},
	url = {https://openreview.net/forum?id=fg2ZFmXFO3},
	booktitle = {{ICLR} 2020 {Workshop} on {Integration} of {Deep} {Neural} {Models} and {Differential} {Equations}},
	author = {Anandkumar, Anima and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Stuart, Andrew},
	year = {2020},
}

@article{li2020multipole,
  title={Multipole graph neural operator for parametric partial differential equations},
  author={Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Stuart, Andrew and Bhattacharya, Kaushik and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6755--6766},
  year={2020}
}


@article{lanthaler_error_2021,
	title = {Error estimates for {DeepOnets}: {A} deep learning framework in infinite dimensions},
	journal = {arXiv preprint arXiv:2102.09618},
	author = {Lanthaler, Samuel and Mishra, Siddhartha and Karniadakis, George Em},
	year = {2021},
}



@book{DLbook,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press},
}



@article{di_leoni_deeponet_2021,
	title = {{DeepONet} prediction of linear instability waves in high-speed boundary layers},
	url = {https://arxiv.org/abs/2105.08697},
	journal = {arXiv preprint arXiv:2105.08697},
	author = {Di Leoni, P Clark and Lu, Lu and Meneveau, Charles and Karniadakis, George and Zaki, Tamer A},
	year = {2021},
}




@article{kovachki_universal_2021,
	title = {On Universal Approximation and Error Bounds for {Fourier} Neural Operators},
	volume = {22},
	url = {http://jmlr.org/papers/v22/21-0806.html},
	number = {290},
	journal = {Journal of Machine Learning Research},
	author = {Kovachki, Nikola and Lanthaler, Samuel and Mishra, Siddhartha},
	year = {2021},
	pages = {1--76},
}

@article{mao_deepmandmnet_2021,
	title = {{DeepMandMnet} for hypersonics: {Predicting} the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators},
	volume = {447},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121005933},
	doi = {https://doi.org/10.1016/j.jcp.2021.110698},
	journal = {Journal of Computational Physics},
	author = {Mao, Zhiping and Lu, Lu and Marxen, Olaf and Zaki, Tamer A. and Karniadakis, George Em},
	year = {2021},
	keywords = {Chemically reacting flow, Data assimilation, Deep learning, DeepONet, Hypersonics, Operator approximation},
	pages = {110698},
}



@article{cai_deepmmnet_2021,
	title = {{DeepM}\&{Mnet}: {Inferring} the electroconvection multiphysics fields based on operator approximation by neural networks},
	volume = {436},
	journal = {Journal of Computational Physics},
	author = {Cai, Shengze and Wang, Zhicheng and Lu, Lu and Zaki, Tamer A and Karniadakis, George Em},
	year = {2021},
	note = {Publisher: Elsevier},
	keywords = {Data assimilation, Deep learning, DeepONet, Operator approximation, Multiscale modeling, Mutiphysics},
	pages = {110296},
}


@article{mao_deepmmnet_2021,
	title = {{DeepM}\&{Mnet} for hypersonics: {Predicting} the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators},
	volume = {447},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999121005933},
	doi = {https://doi.org/10.1016/j.jcp.2021.110698},
	abstract = {In high-speed flow past a normal shock, the fluid temperature rises rapidly triggering downstream chemical dissociation reactions. The chemical changes lead to appreciable changes in fluid properties, and these coupled multiphysics and the resulting multiscale dynamics are challenging to resolve numerically. Using conventional computational fluid dynamics (CFD) requires excessive computing cost. Here, we propose a totally new efficient approach, assuming that some sparse measurements of the state variables are available that can be seamlessly integrated in the simulation algorithm. We employ a special neural network for approximating nonlinear operators, the DeepONet [23], which is used to predict separately each individual field, given inputs from the rest of the fields of the coupled multiphysics system. We demonstrate the effectiveness of DeepONet for a benchmark hypersonic flow involving seven field variables. Specifically we predict five species in the non-equilibrium chemistry downstream of a normal shock at high Mach numbers as well as the velocity and temperature fields. We show that upon training, DeepONets can be over five orders of magnitude faster than the CFD solver employed to generate the training data and yield good accuracy for unseen Mach numbers within the range of training. Outside this range, DeepONet can still predict accurately and fast if a few sparse measurements are available. We then propose a composite supervised neural network, DeepM\&Mnet, that uses multiple pre-trained DeepONets as building blocks and scattered measurements to infer the set of all seven fields in the entire domain of interest. Two DeepM\&Mnet architectures are tested, and we demonstrate the accuracy and capacity for efficient data assimilation. DeepM\&Mnet is simple and general: it can be employed to construct complex multiphysics and multiscale models and assimilate sparse measurements using pre-trained DeepONets in a “plug-and-play” mode.},
	journal = {Journal of Computational Physics},
	author = {Mao, Zhiping and Lu, Lu and Marxen, Olaf and Zaki, Tamer A. and Karniadakis, George Em},
	year = {2021},
	keywords = {Chemically reacting flow, Data assimilation, Deep learning, DeepONet, Hypersonics, Operator approximation},
	pages = {110698},
}


@article{BAR1,
author={A. R. Barron},
journal={IEEE Trans. Inform. Theory.},
title={Universal approximation bounds for superpositions of a sigmoidal function},
volume={39},
number={3},
pages={930-945},
year={1993},
}

@article{HOR1,
author={K. Hornik and M. Stinchcombe and H. White},
journal={Neural networks},
title={Multilayer feedforward networks are universal approximators},
volume={2},
number={5},
pages={359-366},
year={1989},
}

@article{Cy1,
author={G. Cybenko},
journal={Approximation theory and its applications},
title={Approximations by superpositions of sigmoidal functions},
volume={9},
number={3},
pages={17-28},
year={1989},
}


@article{wen2022u,
  title={{U-FNO}—An enhanced {Fourier} neural operator-based deep-learning model for multiphase flow},
  author={Wen, Gege and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M},
  journal={Advances in Water Resources},
  volume={163},
  pages={104180},
  year={2022},
  publisher={Elsevier}
}


@article{li2022fourier,
  title={Fourier neural operator with learned deformations for {PDEs} on general geometries},
  author={Li, Zongyi and Huang, Daniel Zhengyu and Liu, Burigede and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2207.05209},
  year={2022}
}

@article{you2022learning,
  title={Learning deep implicit {Fourie}r neural operators (IFNOs) with applications to heterogeneous material modeling},
  author={You, Huaiqian and Zhang, Quinn and Ross, Colton J and Lee, Chung-Hao and Yu, Yue},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={398},
  pages={115296},
  year={2022},
  publisher={Elsevier}
}


@article{pathak2022fourcastnet,
  title={Fourcastnet: A global data-driven high-resolution weather model using adaptive {Fourier} neural operators},
  author={Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and others},
  journal={arXiv preprint arXiv:2202.11214},
  year={2022}
}


@article{li2021physics,
  title={Physics-informed neural operator for learning partial differential equations},
  author={Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2111.03794},
  year={2021}
}

@inproceedings{raonic2023,
  title={Convolutional neural operators},
  author={Raonic, Bogdan and Molinaro, Roberto and Rohner, Tobias and Mishra, Siddhartha and de Bezenac, Emmanuel},
  booktitle={ICLR 2023 Workshop on Physics for Machine Learning},
  year={2023}
}

@misc{laplaceno,
  doi = {10.48550/ARXIV.2302.08166},
  url = {https://arxiv.org/abs/2302.08166},
  author = {Chen, Gengxiang and Liu, Xu and Li, Yingguang and Meng, Qinglu and Chen, Lu},
  keywords = {Numerical Analysis (math.NA), FOS: Mathematics, FOS: Mathematics},
  title = {Laplace neural operator for complex geometries},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{LLS2023,
    author={Lanthaler, Samuel and Li, Zongyi and Stuart, Andrew M.},
    title={The Nonlocal Neural Operator: Universal Approximation},
    publisher={arXiv},
    journal={(in preparation)},
    year={2023}
}

@article{Kutyniok2022,
author={Kutyniok, Gitta
and Petersen, Philipp
and Raslan, Mones
and Schneider, Reinhold},
title={A Theoretical Analysis of Deep Neural Networks and Parametric {PDEs}},
journal={Constructive Approximation},
year={2022},
month={Feb},
day={01},
volume={55},
number={1},
pages={73-125},
abstract={We derive upper bounds on the complexity of ReLU neural networks approximating the solution maps of parametric partial differential equations. In particular, without any knowledge of its concrete shape, we use the inherent low dimensionality of the solution manifold to obtain approximation rates which are significantly superior to those provided by classical neural network approximation results. Concretely, we use the existence of a small reduced basis to construct, for a large variety of parametric partial differential equations, neural networks that yield approximations of the parametric solution maps in such a way that the sizes of these networks essentially only depend on the size of the reduced basis.},
issn={1432-0940},
doi={10.1007/s00365-021-09551-4},
url={https://doi.org/10.1007/s00365-021-09551-4}
}

