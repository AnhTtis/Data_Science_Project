\section{Introduction}\label{sec:introduction}
\input{figures/ytdialogue_examples.tex}
Conversation often relies on non-verbal cues:
visual information like physical expressions, body gesture, or the surrounding environment
are used by interlocutors to shape and understand meaning.
Figure~\ref{fig:dataset_collection}:
the two conversation participants appear stressed (in the first image: one person strikes the desk with his fists; in the second, the other person is rubbing his face); but the tension is not apparent simply from the transcript, which refers mostly to financial topics.
Other visual information suggests a broader context as well: that the conversation is taking place over the phone, that one person is older, and that one person is more formally dressed are all potentially important factors for understanding the conversation's meaning, yet none are reflected in the transcript. Indeed, visual perception provides information that can help machines understand the world in ways that text alone cannot~\cite{bisk2020experience, lecun2022path, cui2021empathic}.

In this paper, we propose \modelName\footnote{\textbf{C}onvers\textbf{A}tional \textbf{M}ultimodal \textbf{P}rompted \textbf{G}e\textbf{NE}rator.}, a generative model of conversations that learns from a large-scale video corpus.
\modelNameNoEmoji takes in video frames, 
a video title, and a dialogue context as input and returns a dialogue response as output.
The model learns from videos about two \emph{conversational frames:} 
1) \textit{Social Interaction}, where the conversation is observed from a 3rd-person perspective (\textit{e.g.} movies or interviews; Figure~\ref{fig:ytdialog_examples}, (a)-left); and 
2) \textit{Visually-grounded Dialogue}, where the conversation is observed from an embodied, first-person perspective (\textit{e.g.} ego-centric videos or chit-chatting through messenger applications; Figure~\ref{fig:ytdialog_examples}, (a)-right)).

To support training \modelNameNoEmoji, we collect and release a large-scale dataset, \datasetName, which is the largest publicly available dataset for real-world conversation learning.
\datasetNameNoEmoji is constructed from 20M YouTube videos:
we use a language model to convert the noisy transcripts automatically generated by YouTube into well-formatted dialogues associated with video frames.
Human evaluation shows that \datasetNameNoEmoji covers both social interaction and visually-grounded dialogue frames in balance, and surpasses the prior resource in terms of quality and scale (see \S\ref{subsec:analysing_dataset}).


After training, we demonstrate that \modelNameNoEmoji models %
generate high-quality next-turn utterances that account for visual contexts. 
Then, we conduct fine-tuning experiments, finding that:
1) \modelNameNoEmoji exhibits strong performance on open-domain text-only conversation benchmarks (\S\ref{subsec:open_domain_conversation});
2) \modelNameNoEmoji outperforms existing SOTA models on two \emph{social interaction} understanding benchmarks: CMU-MOSEI~\cite{zadeh2018multimodal} and Visual Comet~\cite{park2020visualcomet} (\S\ref{subsec:understand_social_interactions});
3) \modelNameNoEmoji outperforms SOTA models on two \emph{visually-grounded dialogue} benchmarks: Visual Dialog~\cite{das2017visual} and Image Chat~\cite{shuster2018image} (\S\ref{subsec:visual_grounded_dialogues}); and
4) ablations confirm the importance of various components of \datasetNameNoEmoji (\S\ref{sec:ablation}), \textit{e.g.} video frames.

In summary, our main contributions are: 
\begin{enumerate}[leftmargin=*]
  \item \datasetName, a large-scale dataset that contains 18M video-based dialogue that covers real-world conversational frames derived from 20M web videos.
  \item \modelName, a generative model that learns about real-world conversations from \datasetNameNoEmoji without any manual annotation.
  \item Experiments and ablations that demonstrate learning from a large-scale video-based dialogue dataset improves model performance on various tasks related to conversation.
\end{enumerate}

We publicly release code, the \datasetName dataset, and \modelName model checkpoints to facilitate future research on understanding real-world conversations from a visually-grounded perspective.