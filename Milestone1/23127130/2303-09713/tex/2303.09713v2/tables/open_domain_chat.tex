\begin{table*}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{5.0pt} 
\begin{tabular}{lcccccccccc|cccc}
\toprule
Dataset   & \multicolumn{2}{c}{BST} & \multicolumn{2}{c}{ConvAI2} & \multicolumn{2}{c}{ED} & \multicolumn{2}{c}{WOW} & \multicolumn{2}{c|}{WOI} & \multicolumn{2}{c}{IC} & \multicolumn{2}{c}{IC First Turn}\\
\midrule
Metric (on Test Set) & PPL & Dist-3 & PPL & Dist-3 & PPL & Dist-3 & PPL & Dist-3 & PPL & Dist-3 & PPL & Dist-3 & PPL & Dist-3 \\ 
\midrule
\textit{Without Fine-tuning} & & & & & \\
\champagne\modelNameNoEmojiXL  & 34.1 & 0.417 &	34.2 & 0.371 &	32.8 & 0.544 &	31.2 & 0.705 &	34.8 & 0.641 &	50.5 & 0.213 & 69.9 & 0.135 \\
Reddit 2.7B~\cite{roller2020recipes} & 20.0 & 0.390 & 24.2 & 0.286 & 14.0 & 0.418 & 24.3 & 0.458 & 23.5 & 0.473 & 40.9 & 0.152 & 66.3 & 0.011  \\
\midrule
\textit{Fine-tuned} \\
Multimodal Blender 2.7B~\cite{shuster2020multi} & \textbf{14.6} & 0.418 & 13.8 & 0.300 & \textbf{11.4} & 0.306 & \textbf{15.6} & 0.527 &  19.8 & 0.522 & 24.0 & 0.200 & 25.9 & 0.193  \\
Unified-IO$_{PT}$ \textsc{Base} & 20.4 & 0.280 & 16.5 & 0.199 &	19.8 & 0.352 &	22.6 & 0.480 & 	26.0 & 0.449  & 29.9 & 0.106 & 29.6 & 0.113 \\
Unified-IO$_{PT}$ \textsc{Large} & 17.7 & 0.284 &	15.0 & 0.176 & 15.8 & 0.304 &	18.6 & 0.484 & 	21.3 & 0.436 & 	26.8 & 0.108 & 	26.7 & 0.096  \\
Unified-IO$_{PT}$ \textsc{XL} & 15.7 & 0.346 & 12.9 & 0.250 & 	14.9 & 0.393 & 	16.5 & 0.580 &	18.9 & 0.515 & 	20.5 & 0.182 & 22.2 & 0.163  \\
\champagne\modelNameNoEmojiBase & 19.1& 0.395 & 15.2  & 0.361 & 19.0 & 0.421 & 21.3 & 0.651 & 24.1 & 0.584 & 27.2 & 0.174 & 27.4 & 0.220  \\
\champagne\modelNameNoEmojiLarge  & 16.9	& 0.390 & 13.6 & 0.353 &	18.6 & 0.454 & 18.5 & 0.665 & 	21.7 & 0.590 & 22.9 & 0.205 & 23.6 & 0.256  \\
\champagne\modelNameNoEmojiXL   & 15.1 & \textbf{0.434}& 	\textbf{12.3} & \textbf{0.390}& 	16.2& \textbf{0.499} & 	16.1 & \textbf{0.690} & 	\textbf{18.5} & \textbf{0.624} &	\textbf{19.3} & \textbf{0.247} &	\textbf{21.2} & \textbf{0.278} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption {\small Automatic evaluation results on open-domain text conversation benchmarks and Image Chat (IC). 
For fair comparison, all perplexities (PPL, $\downarrow$) are normalized to be in the space of GPT-2 tokenizer~\cite{wolf-etal-2020-transformers}.
Dist-3 ($\uparrow$) is calculated over corpus-level (Inter Dist-3).
\textit{IC First Turn} indicates the situation in which the model generates the response given image but no textual context from IC, which is to highlight model's ability to ground on visual contexts.
Note that Multimodal Blender 2.7B is a fine-tuned version of Reddit 2.7B.
}
\vspace{-3mm}
\label{tab:open_domain_chat}
\end{table*}