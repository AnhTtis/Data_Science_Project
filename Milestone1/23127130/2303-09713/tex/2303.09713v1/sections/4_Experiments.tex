\input{figures/training_curves.tex}
\input{figures/video_prediction}
\input{tables/open_domain_chat.tex}
\section{Experiments}\label{sec:experiments}
Figure~\ref{fig:training_curves} shows the training curves for \modelNameNoEmoji models that were trained on \datasetNameNoEmoji.
We evaluate the sensibleness and specificity of the response generated by \modelNameNoEmoji models by computing perplexities~\cite{jelinek1977perplexity} on the validation set of \datasetNameNoEmoji. Perplexity (PPL) has been shown to be a reliable indicator of these qualities~\cite{adiwardana2020towards}. The training curves in Figure~\ref{fig:training_curves} demonstrate that the models are capable of learning to generate appropriate next-turn utterances given visual and dialogue contexts, and that they generalize well to the validation set when trained on \datasetNameNoEmoji. A qualitative example of this successful generalization can be seen in Figure~\ref{fig:video_prediction}.

We next conduct experiments on a range of benchmarks that assess the model's performance on real-world conversation tasks. Additional information about the benchmarks and the evaluation metrics used can be found in Appendix~\ref{sec:appendix_datasets}.

\subsection{Open-domain Text Conversation}
\label{subsec:open_domain_conversation}
To investigate whether \modelNameNoEmoji models are capable of learning chit-chat skills from \datasetNameNoEmoji, we conduct automatic evaluations on open-domain text conversation benchmarks. These benchmarks assess the model's ability to produce appropriate responses given a textual dialogue context in a conversational scenario. Specifically, we evaluate the model's performance on five text-only benchmarks: BST~\cite{smith2020can}, ConvAI2~\cite{dinan2020second}, ED~\cite{rashkin2018towards}, WOW~\cite{dinan2018wizard}, and WOI~\cite{komeili2021internet}. We employ two metrics to evaluate the models: PPL and Dist-3~\cite{li2016diversity}. Dist-3 is a metric that has been shown to reflect the diversity and interestingness of the response~\cite{Han2022MeasuringAI}. Results are shown in Table~\ref{tab:open_domain_chat} (left).



Prior to fine-tuning, both Unified-IO$_{PT}$ \textsc{XL} and Unified-IO \textsc{XL} exhibited perplexities over 100, which suggests that these models generate unintelligible sentences. In contrast, \modelNameNoEmojiXL achieved significantly lower perplexities on all benchmarks, indicating that the model has effectively learned chit-chatting capabilities from \datasetNameNoEmoji.
Next, we fine-tune both Unified-IO$_{PT}$ and \modelNameNoEmoji on a mixture of dialogue benchmarks, with the multi-task training weight set to BST: ConvAI2: ED: WOW: WOI: IC $= 1:3:3:3:3:3$, following the approach described in~\cite{roller2020recipes}. After fine-tuning, \modelNameNoEmoji achieves significantly higher Dist-3 scores and lower PPL values than the same-sized Unified-IO$_{PT}$, indicating that \modelNameNoEmoji has successfully learned about textual conversations from \datasetNameNoEmoji. Our experimental results further suggest that larger models tend to perform better overall, with \modelNameNoEmojiXL reporting Dist-3 scores that surpassed those of other baselines on all benchmarks, demonstrating its ability to generate diverse and interesting responses. However, in terms of PPL, \modelNameNoEmojiXL shows weaker performance on BST, ED, and WOW compared to the Multimodal Blender 2.7B. We conjecture that this may be due to the influence of the large text-based dialogue dataset (the Pushshift dataset~\cite{baumgartner2020pushshift}), which contains 3B Reddit comments. Indeed, the Reddit 2.7B model, the text-only model which is trained on the Pushshift dataset, performs better than \modelNameNoEmojiXL without fine-tuning, suggesting that the Pushshift dataset has a distribution that is more similar to those of these benchmarks than \datasetNameNoEmoji. 
We expect that further improving the model's performance on these benchmarks may require training \modelNameNoEmojiXL on the Pushshift dataset in addition to fine-tuning it on these benchmarks.



\subsection{Understanding Social Interactions}\label{subsec:understand_social_interactions}
\input{tables/mosei.tex}
\input{tables/visual_comet.tex}
We evaluate the performance of three variants of \modelNameNoEmoji and Unified-IO$_{PT}$ on two benchmarks that measure social interactions: the sentiment analysis task in CMU-MOSEI~\cite{zadeh2018multimodal} and Visual Comet~\cite{park2020visualcomet}.
Table~\ref{tab:mosei} and Table~\ref{tab:visualcomet} show the results on CMU-MOSEI and Visual Comet, respectively.
Results show that fine-tuning \modelNameNoEmojiXL on these benchmarks leads to the highest accuracy and F1 score among the baselines, including the state-of-the-art model (UniMSE) on CMU-MOSEI. Additionally, our model outperforms the state-of-the-art result on Visual Comet, both on the validation and test sets.
Furthermore, the models fine-tuned from \modelNameNoEmoji consistently demonstrate significant improvements compared to the models fine-tuned from same-sized Unified-IO$_{PT}$ on both CMU-MOSEI and Visual Comet. This indicates that our model effectively learns meaningful representations about social interactions from \datasetNameNoEmoji.


\subsection{Visually-grounded Dialogues}\label{subsec:visual_grounded_dialogues}
\input{figures/inference_results.tex}
\input{tables/visdial.tex}
\input{tables/open_domain_human_eval.tex}
We run experiments on two visually-grounded dialogue benchmarks: Visual Dialog~\cite{das2017visual} and Image Chat~\cite{shuster2018image}. Table~\ref{tab:visdial} presents the results of models on Visual Dialog in both zero-shot and fine-tuned settings.
Similarly, Table~\ref{tab:open_domain_chat} (right) shows the results of models fine-tuned on combination of dialogue datasets on Image Chat and Image Chat First Turn. 
Fine-tuning \modelNameNoEmoji on those benchmarks outperforms fine-tuning the same sized Unified-IO$_{PT}$ model on both tasks. This suggests that the model effectively learns from \datasetNameNoEmoji, and larger models tend to perform better. For Visual Dialog zero-shot setting, \modelNameNoEmojiXL outperforms ESPER and FROMAGe in terms of NDCG~\cite{jarvelin2002cumulated}. In the fine-tuned setting, \modelNameNoEmojiXL achieves the highest NDCG compared to Flamingo-80B and the state-of-the-art model (AlignVD). However, in the zero-shot setting, Flamingo-80B performs significantly better than \modelNameNoEmojiXL, which might be due to Flamingo's larger parameters and training data. Based on our scaling observations, we suspect that training Flamingo-80B or a similar sized model on \datasetNameNoEmoji could lead to further performance improvements. For Image Chat and Image Chat First Turn, \modelNameNoEmojiXL shows the lowest PPL and highest Dist-3 scores among the baselines.


We additionally carry out a human evaluation to compare our best model with the baselines trained on Image Chat. We randomly choose 100 examples from both Image Chat and Image Chat First Turn. For each example, we asked three workers to rate the dialogue responses generated by the model on four aspects: (1) sensibleness, (2) specificity, (3) grounding to the image, and (4) relevance to the given style. The workers were presented with positive/negative options for each aspect. For instance, for the grounding aspect, they were asked to choose between "Yes, the response is grounded to the image" or "No, the response is not grounded to the image." Our evaluation results, presented in Table~\ref{tab:open_domain_human_eval}, indicate that our model, \modelNameNoEmojiXL, outperforms the baselines in terms of specificity, grounding, and style. As for sensibleness, our model shows comparable scores to the best results. More details on the human evaluation can be found in Appendix~\ref{subsec:appendix_human_evaluation}.
Lastly, Figure~\ref{fig:inference_results} presents some sample conversations between a human and our model. The figure demonstrates that the model can engage in conversations based on images and can be prompted to respond accordingly.





\subsection{Ablations}\label{sec:ablation}
\input{tables/ablation.tex}
To investigate the impact of components in \datasetNameNoEmoji, we conduct ablation studies on CMU-MOSEI and Image Chat First Turn. Our evaluation of models on Image Chat First Turn only involves fine-tuning on Image Chat and excludes any text-only dialogue datasets.
\vspace{-13pt}

\paragraph{Number of Examples.}
The performance drop in both tasks is evident when reducing the number of examples in \datasetNameNoEmoji from 18M to 2M, as seen in Table~\ref{tab:ablation}. These findings are consistent with previous works~\cite{hoffmann2022training, brown2020language} and highlight the benefits of learning from a larger dataset. Notably, the significance of the performance drop is greater in the visually-grounded dialogue task of Image Chat, underscoring the crucial role of the number of examples in such tasks.


\vspace{-13pt}

\paragraph{Video Frames.}
 To validate whether the models learn from visual contexts, we conduct two experiments. The first experiment involves training the model without video frames, while the second experiment utilizes only a single video frame for training. Table~\ref{tab:ablation} presents the results of these experiments, which show a decrease in performance for both tasks. These findings support the idea that \datasetNameNoEmoji provides an opportunity for the model to learn visual grounding. Notably, the accuracy drop is more significant for CMU-MOSEI, where training with a single frame results in an accuracy drop from 83.2\% to 78.9\%, and training without video frames results in an even greater accuracy drop to 75.6\%. This suggests that relying on visual cues is essential for understanding sentiment in social interaction. Furthermore, in Image Chat, there is a substantial drop in Dist-3, indicating that models trained without visual contexts may generate generic responses that are irrelevant to the images.


\vspace{-13pt}

\paragraph{Dialogue Format.}
By comparing the model trained on the noisy transcript to the model trained on dialogue format, we aim to demonstrate how the latter can improve the machine's ability to learn representations of conversation. 
In Table~\ref{tab:ablation}, we observe a decrease in performance for both tasks.
Notably, on the CMU-MOSEI dataset, the accuracy drops from 83.2\% to 74.9\% without the dialogue conversion process. We speculate that the conversion process helps the dataset recover crucial speaker information that is missing from the noisy transcript. This information could prove beneficial for learning representations of social interaction.

