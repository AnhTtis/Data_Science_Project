\section{Related Work}\label{sec:related_work}

Although recent language modeling advancements have enabled machines to engage in conversation and comprehend dialogue similar to humans \cite{zhang2019dialogpt, roller2020recipes, chen2020multi, adiwardana2020towards, chen-yang-2021-simple, dziri2022faithdial, han2022understanding}, these efforts have largely been confined to textual contexts.
In our work, we consider three real-world conversational frames: social interaction, visually-grounded dialogue, and open-domain text conversation. Social interaction involves conversations that incorporate visual cues and are observed from an external viewpoint (\textit{i.e.}, third-person perspective). 
Examples of these conversations include speaker sentiment analysis in videos as proposed by CMU-MOSEI~\cite{zadeh2018multimodal}, and human-focused commonsense-based captioning as introduced by Visual Comet~\cite{park2020visualcomet}.
Visually-grounded dialogue, on the other hand, refers to conversations that involve visual contexts from the perspective of the agent, \textit{i.e.}, from the first-person point of view. 
Visual Dialog~\cite{das2017visual}, which involves answering questions about an image in a conversational setting, and Image Chat~\cite{shuster2018image}, a chit-chat grounded to the image, are examples of visually-grounded dialogue.
Finally, open-domain text conversation refers to conversations that rely solely on textual contexts, and there have been multiple endeavors to teach machines to engage in natural communication on a wide range of topics~\cite{smith2020can, dinan2020second}.


Text-only dialogue models have access to abundant training resources~\cite{kim2022soda, thoppilan2022lamda, dziri2022faithdial, shuster2022blenderbot,  baumgartner2020pushshift, dziri2019augmenting}, allowing them to generate natural responses based on conversational context. However, despite their striking conversational abilities they lack the capability to comprehend visual contexts. 
To address this shortcoming, several studies have proposed multi-modal dialogue models \cite{shuster2020multi, sun2022multimodal}.
Few large-scale datasets such as MMDialog~\cite{feng2022mmdialog} are available for conversations that are grounded on visual contexts, yet the scale of the dataset is quite smaller than those of text-only dialogue datasets, making it difficult to train such models at scale.


\vspace{-13pt}

\paragraph{Large Vision-Language Models.}
Recent studies in vision-language research have shown that scaling the size of the model in conjunction with the dataset size can significantly enhance model performance across a range of tasks \cite{radford2021learning, ramesh2021zero}. Several frameworks have been introduced, including Unified-IO \cite{lu2022unified} and OFA \cite{wang2022ofa}, which propose a unified sequence-to-sequence model for modeling vision-language tasks. These frameworks demonstrate that a single model trained on a broad range of tasks can perform well on various tasks. Additionally, BLIP \cite{li2022blip} presents a technique for pretraining models on noisy datasets for both vision-language understanding and generation tasks. The Flamingo model \cite{alayrac2022flamingo}, trained on a large amount of noisy web data, demonstrates that it can adapt to tasks with few examples. While these models provide a general vision-language model, our research aims to enhance conversation capability by focusing on specifically learning about conversations.

