\section{Details of Dataset Collection}\label{sec:appendix_dataset_collection}
\paragraph{Safety Filtering.}
We use Rewire API~\cite{rewire} to filter out unsafe contents from videos.
Rewire API identifies abusive, hateful, profane, violent, or sexually explicit content.
However, we have discovered that the API is not accurate enough to detect profanities and violent languages in video transcripts.
Thus, we only use API to detect abusive, hateful, or sexually explicit content.
We set thresholds of 0.99534, 0.83790, 0.99562 to filter out unsafe contents for abuse, hate, and sexually explicit labels, respectively.

\paragraph{Aligning Video and Dialogue.}
We use Dynamic Time Warping~\cite{muller2007dynamic} to align the dialogue (text) with the video frames.
In particular, we first calculate the distance between the noisy transcript and the converted dialogue using Levenshtein distance.
We then employ Dynamic Time Warping to align the words and minimize the distance between the transcript and the dialogue.
Following that, using the timing information associated with the transcripts, we estimate the start time of each utterances in the dialogue.
We extract the video frame using the start timing of the utterance, resulting in a video-based dialogue with video frames and the dialogue turns $(I_1, T_1, ..., I_n, T_n)$.

\section{Human Evaluation}\label{subsec:appendix_human_evaluation}
\input{tables/full_dataset_analysis}
\input{tables/full_dataset_analysis_visual}
\input{figures/dataset_evaluation.tex}
\input{figures/response_evaluation.tex}

To provide a more detailed view of the human evaluation results, in Table~\ref{tab:dataset_analysis_full} and Table~\ref{tab:dataset_analysis_visual_full}, we report the complete breakdown of human evaluation results.
These results complement the summarized results displayed in Table~\ref{tab:dataset_analysis}.
To collect the human annotations, we use Amazon Mechanical Turk (MTurk), a crowdsourcing platform, and ask human workers to annotate for the tasks.
We set the qualification tasks to recruit the qualified human workers in MTurk.
Figure~\ref{fig:dataset_evaluation} and Figure~\ref{fig:response_evaluation} show the interface used for human evaluation on MTurk.
For human evaluation, we compensate MTurk workers with an hourly wage of \$15 for their contributions.

\section{Dataset Analysis}\label{sec:appendix_data_analysis}
\input{tables/data_stats.tex}
\paragraph{Data Statistics.} Table~\ref{tab:dataset_statistics} shows the statistics about \datasetNameNoEmoji and the other conversational datasets including both text-only and visually-grounded cases.

\paragraph{Details about Visual Feature Distributions.}
\input{figures/YTD_Visualize_CLIP_cluster.tex}
To display visual feature distributions as in Figure~\ref{fig:ytdialogue_visualize}, we use $n\_neighbors=15$ and $min\_dist=0.1$ for UMAP.
In Figure~\ref{fig:ytdialogue_visualize_cluster}, we additionally show the clusters created in Figure~\ref{fig:ytdialogue_visualize} using HDBSCAN~\cite{mcinnes2017accelerated} with $min\_samples=10$ and $min\_cluster\_size=40$ for HDBSCAN, creating 11 clusters in total.

\section{Training and Fine-tuning \modelName}
\label{subsec:hparams}
\input{tables/hparams_finetuning_all}
When training \modelNameNoEmoji on \datasetNameNoEmoji, we train the model for 3 epochs with a learning rate of 3e-4, an input text sequence length of 256, a target text sequence length of 128, an input image sequence length of 576, and a batch size of 256.
For fine-tuning, we also use an input text sequence length of 256, a target text sequence length of 256, and an input image sequence length of 576.
In Table~\ref{tab:hparams_finetuning_all}, we report other important hyper-parameters when fine-tuning \modelNameNoEmoji on downstream tasks.

\section{Benchmarks and Evaluation Details}\label{sec:appendix_datasets}
\paragraph{CMU-MOSEI.} 
CMU-MOSEI~\cite{zadeh2018multimodal} is the multimodal dataset for studying sentiments and emotions in videos.
It has 16K examples in the dataset, and we use the sentiment label in our experiments.
The task uses binary classification accuracy and F1 score to measure the performance.
For the task, we use the template \texttt{"context: \{\{transcript\}\}, question: Is the person positive?"} to turn transcript to the input and the model produces the output from the given input.

\paragraph{Visual Comet.}
Visual Comet~\cite{park2020visualcomet} is the benchmark for visual commonsense reasoning where the event from a still image is given.
The dataset contains 59K examples, and the task uses generative evaluation so that the model generates five results and compares these results with the references using CIDEr-D\cite{vedantam2015cider} and BLEU-4~\cite{papineni2002bleu}.
For the task, we use the template \texttt{"Event: \{\{event\}\} Before, what the person needed to do ?"} to turn given event to the input.

\paragraph{Visual Dialog.}
Visual Dialog~\cite{das2017visual} is a visual conversational QA dataset, consisted of 150K dialogue examples.
In particular, for each example, an image, a dialogue history, and a follow-up question about the image is given, and model should answer the question.
The task reports Normalized Discounted Cumulative Gain (NDCG)~\cite{jarvelin2002cumulated} for evaluation, where each answer has 100 candidate options and four human workers annotated relevance for each candidate option.
Each given image has an caption from COCO challenge and a dialogue history, and we use the template \texttt{"<extra\_id\_1> \{\{image\_caption\}\} <extra\_id\_0> \{\{dialogue\_turn\_1\}\} <extra\_id\_0> ... \{\{dialogue\_turn\_n\}\}"} to format the given inputs.

\paragraph{Image Chat.}
Image Chat~\cite{shuster2018image} is the dataset containing 200K dialogues and each dialogue is grounded to the image.
Specifically, for each conversation, an image is given and two different styles (\textit{e.g.} "Happy", "Sad") are assigned to speakers and the speakers conduct a conversation based on the image and the styles.
For the task, we use the template \texttt{"<extra\_id\_1> Conversation with \{\{style\}\} person <extra\_id\_0> \{\{dialogue\_turn\_1\}\} <extra\_id\_0> ... \{\{dialogue\_turn\_n\}\}"} to format the given inputs.

\paragraph{Open-domain Text-only Conversations.}
We use five open-domain text-only conversation benchmarks in this study: Blended Skill Talk (BST), ConvAI2, Empathetic Dialogue (ED), Wizard Of Wikipedia (WOW), and Wizard Of Internet.
Statistics about the dataset are depicted in Table~\ref{tab:dataset_statistics}.
These benchmarks have meta information about the conversation (\textit{e.g.} for BST, the persona information for the speaker is given as a meta information), and we use the template \texttt{"<extra\_id\_1> \{\{meta\}\} <extra\_id\_0> \{\{dialogue\_turn\_1\}\} <extra\_id\_0> ... \{\{dialogue\_turn\_n\}\}"} to format the given inputs.

\section{Decoding for Model Inference}
\input{figures/inference_results_appendix}
In this section, we describe the decoding strategy for model inference in different benchmarks.
To decode the results for Visual Comet, we use beam decoding with a beam size of 10.
For Image Chat and other open-domain text-only conversation, we follow same decoding strategy from~\cite{shuster2020multi} for a fair comparison.
Specifically, we apply beam decoding with a beam size of 10, a minimum beam length of 20.
We also use a subsequence blocking of 3-grams to prevent model from generating repeated 3-grams of the input context and repeating within the generated response.
To obtain qualitative results in Figure~\ref{fig:inference_results}, we use the minimum beam length of 10 instead of 20 since large number of minimum beam length causes a degeneration, and use temperature sampling~\cite{ficler2017controlling} with $temperature=0.3$ and $topk=5$.
In Figure~\ref{fig:inference_results_appendix}, we provide additional examples of conversations between humans and \modelNameNoEmojiXL that has been fine-tuned on a mixture of dialogue benchmarks.
