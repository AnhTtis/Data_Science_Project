\begin{abstract}
Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone.
To date, however, most neural conversational models are limited to just text.
We introduce \modelName, a generative model of conversations that can account for visual contexts.
To train \modelNameNoEmoji, we collect and release \datasetName, a large-scale corpus of 18M video-based dialogues.
\datasetNameNoEmoji is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning.

Human evaluation reveals that \datasetNameNoEmoji is more sensible and specific than prior resources (MMDialog \cite{feng2022mmdialog}, 1M dialogues), while maintaining visual-groundedness.
Experiments demonstrate that 1) \modelNameNoEmoji learns to conduct conversation from \datasetNameNoEmoji; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations.
We release data, models, and code at \href{https://seungjuhan.me/champagne}{https://seungjuhan.me/champagne}.
\end{abstract}
