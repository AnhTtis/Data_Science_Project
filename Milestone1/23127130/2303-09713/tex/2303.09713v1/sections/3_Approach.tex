\section{Approach}\label{sec:approach}
\subsection{Dataset Collection: \datasetName}\label{subsec:dataset_collection}
In this section, we describe our pipeline to collect \datasetName, which is outlined in Figure~\ref{fig:dataset_collection}.

\vspace{-13pt}

\paragraph{Extracting and Filtering Videos.}
The data collection process starts with downloading public YouTube videos. Each video is associated with a user-provided title, metadata, and a transcript generated by the YouTube's internal Automatic Speech Recognition system. 
We then filter videos by applying several steps using the associated metadata, following the strategy from~\citep{zellers2021merlot, zellers2022merlot}.
In particular, we use Python \textit{cld3} library, which uses a neural network for language identification, to filter out videos whose transcripts have a probability of being English less than 80\%.
We also discard videos that do not contain visual variation (\textit{e.g.}, a video of podcast with a static thumbnail) or those lacking objects in the thumbnails according to an image classification model~\cite{sandler2018mobilenetv2}. These steps resulted in the extraction of 20M videos.

We further process these 20M videos to build video segments. First, we build a list of segments by iterating through each video with a 60-second sliding window. Second, we filter out segments which have transcripts containing less than 30 words: short transcripts are unlikely to contain multiple conversation turns. 
Finally, we filter out transcripts that contain unsafe topics to ensure that the model does not learn harmful language. We utilize Rewire API~\cite{rewire} to detect toxic content in the transcripts.
At the end of the filtering stage, we end up with 18M video segments. 

\vspace{-13pt}

\paragraph{Converting Noisy Transcripts into Dialogues.}
Although each video is equipped with its own transcript,
they are usually not suitable to train a video-based dialogue model directly.
For example, transcripts do not provide any delimitation between the interlocutors.  A naive approach to address this problem is to use speaker diarization systems to determine ``who spoke when"~\citep{tranter2003investigation}. However, these systems suffer from low accuracy~\citep{park2022review}, thus perform poorly when turning a sequence of words to a well-structured dialogue. 

Instead, %
we train a \textit{converter} model to transform noisy transcripts into structured dialogues. Motivated by the in-context learning capabilities of GPT-3 models~\citep{brown2020language}, we prompt GPT-3 with few-shot examples and ask the model to generate well-formatted dialogues given noisy transcripts. 
However, using GPT-3 to process millions of samples is computationally expensive. Therefore, we train a smaller model using denoised transcripts sampled from GPT-3.
That is, we collect 40K input-output pairs from GPT-3 where the input is a noisy transcript and the output is the converted dialogue. Our \textit{converter} is a Unified-IO~\citep{lu2022unified} Base (241M params) fine-tuned on the generated pairs. We exclude video segments that have more than 150 words as the model cannot handle excessively long inputs. The pairs are divided into train and validation sets at a ratio of 99:1. The \textit{converter} achieves a high accuracy of 90.1\% on the validation set when using teacher-forcing to predict tokens,
suggesting high-quality dialogue generation given noisy transcripts (\textit{e.g.,} in Figure~\ref{fig:dataset_collection}, Step 2). 

\vspace{-13pt}

\paragraph{Converting Videos into Video-based Dialogues.}
To ensure that each utterance in the obtained dialogue matches correctly with the corresponding video frames, we employ Dynamic Time Warping~\cite{muller2007dynamic} to align the dialogues with the original noisy transcripts.
After the alignment, we use the timing information in the original noisy transcripts to estimate the start time of each dialogue turn, which in turn helps extract the corresponding video frame and minimize alignment errors caused by the conversion process.
Figure~\ref{fig:ytdialog_examples} shows some examples from \datasetNameNoEmoji. More details about collecting dataset can be found in Appendix~\ref{sec:appendix_dataset_collection}.

\subsection{Dataset Analysis}\label{subsec:analysing_dataset}
\input{tables/dataset_analysis.tex}
To better understand our dataset, we conduct a human evaluation study comparing \datasetNameNoEmoji with MMDialog~\cite{feng2022mmdialog} --- the largest dataset for visually-grounded dialogue.
MMDialog is a dataset sourced from people's interaction in social media.
We randomly sample 500 examples from each dataset and ask three workers to assess each example for several factors.
For more information regarding the human evaluation, please refer to the Appendix~\ref{subsec:appendix_human_evaluation}.

\vspace{-13pt}
\paragraph{Dataset quality.}
To compare the dialogue quality, we ask workers to assess examples using two criteria: sensibleness and specificity~\cite{adiwardana2020towards}.
To be sensible, a dialogue should be reasonable, logical, and not confusing.
Specific dialogue is one that is not dull or generic.
Each human annotators rate the dialogue in specific aspect on a 3-point Likert scale, \textit{e.g.} for sensibleness "1" being "Not Sensible" and "3" being "Sensible".
Evaluation results are shown in Table~\ref{tab:dataset_analysis}. On average, \datasetNameNoEmoji received higher scores than MMDialog across the axes of sensibleness and specificity.
We suspect that MMDialog, which is derived from social media, may lack a natural conversation flow due to the non-consecutive nature of social media interactions.

\vspace{-13pt}
\paragraph{Social Interaction.}
Given the prevalence of third-person point of view in web videos,
we postulate that a substantial proportion of such videos feature social interactions. To investigate this claim, we enlisted the aid of workers to determine whether the conversation's interlocutors were visible in the image frames and, if so, whether their body language was present. Our findings, presented in Table~\ref{tab:dataset_analysis}, indicate that our video-based dialogue dataset has a considerably higher proportion of visible interlocutors (61.6\%) than MMDialog (11.5\%). This discrepancy can be explained by the fact that it is uncommon for interlocutors in social media interactions to reveal their identities through images. Moreover, when interlocutors are visible, workers accurately identified facial expressions in 83.6\% of cases, and tagged body posture in 64.7\% of them. These results suggest that our dataset presents a valuable resource for exploring body language in communication.






\vspace{-13pt}

\paragraph{Visual Grounding.}
Real-life conversations do not always have a direct relationship or grounding to images. Therefore, a higher degree of relevance between dialogues and images does not necessarily imply a higher quality dataset, although it offers more visual grounding opportunities for models to learn from. We ask workers to assess the degree of grounding between conversations and images on a 3-point Likert scale when interlocutors were not visible in the images. According to Table~\ref{tab:dataset_analysis}, \datasetNameNoEmoji exhibits grounding scores that are comparable to those of MMDialog, implying that models can acquire visual grounding knowledge from \datasetNameNoEmoji.


\vspace{-13pt}

\paragraph{Distribution of Visual Contexts.}
\input{figures/YTD_Visualize.tex}
In order to gain a better understanding of the visual representations encoded in \datasetNameNoEmoji from videos, we examine the distributions of visual features across three distinct datasets: Image Chat, MMDialog, and \datasetNameNoEmoji, that are grounded in dialogue. 
Similar to~\cite{liu2021visually}, we use CLIP ViT-L14~\cite{radford2021learning} trained on LAION-2B~\cite{schuhmannlaion}.\footnote{\cite{liu2021visually} conducted a similar study, but using ResNet50 ImageNet features.}%
We utilize this model to extract embeddings, and the embeddings are then projected into a 2D feature space by way of UMAP~\cite{mcinnes2018umap}. In order to conduct our analysis, we randomly sample 1K images from each of the aforementioned datasets.



Figure~\ref{fig:ytdialogue_visualize} illustrates the results, indicating that the datasets share some similarities but differ in certain aspects.  Notably, \datasetNameNoEmoji has a distinct distribution pattern from Image Chat and MMDialog, with a higher proportion of images featuring a person speaking, which is consistent with the previous discovery that \datasetNameNoEmoji  emphasizes social interactions. Additionally, \datasetNameNoEmoji  encompasses a broad range of diverse and specific topics, such as cooking or mechanics. Further information regarding the analysis of visual contexts can be found in the Appendix~\ref{sec:appendix_data_analysis}.




\vspace{-13pt}

\paragraph{Content Safety.}
We ask workers to identify whether the dialogues or images contain any potentially unsafe content, like sexually explicit material or hatespeech. As indicated in Table~\ref{tab:dataset_analysis}, our \datasetNameNoEmoji has fewer occurrences of sexually explicit content and hate speech when compared to MMDialog. We suspect that this discrepancy is due to the inclusion of safety filtering step in the data collection process for \datasetNameNoEmoji. 


\vspace{-13pt}

\paragraph{Video Title as an Additional Feature.}
We further ask the workers to rate the relevance of the video title to the dialogue on a 3-point Likert scale, ranging from 1 (Not Related) to 3 (Related). The results
 show that 65\% of the videos have a title that is relevant to the dialogues, and 21\% have titles that were somewhat related. This led us to utilize the video titles as prompts when training our model. Our qualitative findings (\S\ref{subsec:open_domain_conversation}) suggest that \modelNameNoEmoji can be conditioned effectively by such prompts. 


\subsection{Model: \modelName}\label{subsec:model}
\input{figures/model_training.tex}
\modelNameNoEmoji is a multimodal conversational agent that takes image frames, a prompt, and a dialogue context as an input and generates a response. 
The overview of training process is described in Figure~\ref{fig:model_training}.
The collected YouTube titles for each video serve as prompts and are denoted as $P$. 
Finally, the vision-based dialogue is denoted as $D = (P, I_1, T_1, ..., I_n, T_n)$, where $I_i$ and $T_i$ denote an image frame and the dialogue turn, respectively. 

\vspace{-13pt}

\paragraph{Architecture.}
The architecture used in \modelNameNoEmoji is based on the Unified-IO model proposed by \cite{lu2022unified}. This model is designed for vision-and-language tasks and operates as a sequence-to-sequence model. Although it can handle image and text inputs together, it is unable to handle multiple images. To address this limitation, we introduce \textit{video position embeddings} that can be learned and incorporated into \modelNameNoEmoji. Specifically, \modelNameNoEmoji  converts each frame of the video into a sequence of patch encodings using a visual encoder. It then adds video position embeddings to the patch encodings to capture temporal information of the video frames. The patch encodings from multiple image frames are averaged through mean pooling and fed into a Transformer encoder. Our experiments use three image frames per dialogue to train \modelNameNoEmoji.


\vspace{-13pt}

\paragraph{Training.}
We initialize \modelNameNoEmoji training with the pretrained weight from Unified-IO model that is pretrained on collection of C4~\cite{raffel2020exploring}, Wikipedia, ImageNet21K~\cite{ridnik2021imagenet}, and YFCC15M~\cite{radford2021learning} with a denoising objective.
Unified-IO model is trained in two stages, pre-training and the multi-task stage, and the weights from the pre-training stage, referred to as Unified-IO$_{PT}$\footnote{We run a pilot study and find that the model initialized from Unified-IO$_{PT}$ weights perform better on downstream tasks compared to the model initialized from Unified-IO.}, are used to initialize the \modelNameNoEmoji model.
We train the model using a next token prediction objective, which aims to maximize the likelihood of the target response $T_k$ when taking multiple images $I_{i\leq k}$, dialogue context $T_{i < k}$, and the video title $P$ as an input, \textit{i.e.}, $p_\theta(T_k|I_1, T_1, ... , I_{k-1}, T_{k-1}, T_k, P)$.

We present three versions of the model, \textsc{Base} (241M), \textsc{Large} (776M), and \textsc{XL} (2.9B).
We train models for 3 epochs on \datasetNameNoEmoji, and training \modelNameNoEmojiXL takes approximately 3 days on TPU v3-256 on Google Cloud Virtual Machines with T5X framework~\cite{roberts2022scaling}.
More details about hyperparameters are in Appendix~\ref{subsec:hparams}.
