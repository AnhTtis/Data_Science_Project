\documentclass{midl} % Include author names
%\documentclass[anon]{midl} % Anonymized submission

% The following packages will be automatically loaded:
% jmlr, amsmath, amssymb, natbib, graphicx, url, algorithm2e
% ifoddpage, relsize and probably more
% make sure they are installed with your latex distribution

\usepackage{mwe} % to get dummy images
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{adjustbox}
\usepackage{xcolor}
%\jmlrvolume{-- Under Review}
\jmlryear{2023}
\jmlrworkshop{Full Paper -- MIDL 2023 submission}
%\editors{Under Review for MIDL 2023}

\title[Short Title]{Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation}

 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
  \midlauthor{\Name{Md Mostafijur Rahman} \Email{mostafijur.rahman@utexas.edu}\and
  \\ \Name{Radu Marculescu} \Email{radum@utexas.edu}\\
   \addr The University of Texas at Austin}

 % Three or more authors with the same address:
 % \midlauthor{\Name{Author Name1} \Email{an1@sample.edu}\\
 %  \Name{Author Name2} \Email{an2@sample.edu}\\
 %  \Name{Author Name3} \Email{an3@sample.edu}\\
 %  \addr Address}


% Authors with different addresses:
% \midlauthor{\Name{Author Name1} \Email{abc@sample.edu}\\
% \addr Address 1
% \AND
% \Name{Author Name2} \Email{xyz@sample.edu}\\
% \addr Address 2
% }

%\footnotetext[1]{Contributed equally}

% More complicate cases, e.g. with dual affiliations and joint authorship
%\midlauthor{\Name{Author Name1\midljointauthortext{Contributed equally}\nametag{$^{1,2}$}} \Email{abc@sample.edu}\\
%\addr $^{1}$ Address 1 \\
%\addr $^{2}$ Address 2 \AND
%\Name{Author Name2\midlotherjointauthor\nametag{$^{1}$}} \Email{xyz@sample.edu}\\
%\Name{Author Name3\nametag{$^{2}$}} \Email{alphabeta@example.edu}\\
%\Name{Author Name4\midljointauthortext{Contributed equally}\nametag{$^{3}$}} \Email{uvw@foo.ac.uk}\\
%\addr $^{3}$ Address 3 \AND
%\Name{Author Name5\midlotherjointauthor\nametag{$^{4}$}} \Email{fgh@bar.com}\\
%\addr $^{4}$ Address 4
%}

\begin{document}

\maketitle

\begin{abstract}
%Multi-scalE hieRarchical vIsion Transformer (MERIT) 
Transformers have shown great success in medical image segmentation. However, transformers may exhibit a limited generalization ability due to the underlying single-scale self-attention (SA) mechanism. In this paper, we address this issue by introducing a Multi-scale hiERarchical vIsion Transformer (MERIT) backbone network, which improves the generalizability of the model by computing SA at multiple scales. We also incorporate an attention-based decoder, namely Cascaded Attention Decoding (CASCADE), for further refinement of multi-stage features generated by MERIT. Finally, we introduce \textcolor{black}{an effective multi-stage feature mixing loss aggregation (MUTATION) method for better model training via implicit ensembling}. Our experiments on two widely used medical image segmentation benchmarks (i.e., Synapse Multi-organ, ACDC) demonstrate the superior performance of MERIT over state-of-the-art methods. Our MERIT architecture and MUTATION loss aggregation can be used with downstream medical image and semantic segmentation tasks.  
\end{abstract}

\begin{keywords}
Medical image segmentation, Vision transformer, Multi-scale transformer, Feature-mixing augmentation, Self-attention.
\end{keywords}

\section{Introduction}
\label{introduction}
Automatic medical image segmentation has become an important step in disease diagnosis nowadays. Since the emergence of UNet \cite{ronneberger2015u}, U-shaped convolutional neural networks (CNNs) \cite{oktay2018attention, huang2020unet, zhou2018unet++, fan2020pranet} have become de facto methods for medical image segmentation. By producing high-resolution segmentation maps through aggregating multi-stage features via skip connections, UNet variants, such as UNet++ \cite{zhou2018unet++} and UNet3Plus \cite{huang2020unet}, have shown good performance in medical image segmentation. However, the spatial context of the convolution operation limits the CNN-based methods ability to learn the long-range relations among pixels \cite{cao2021swin}. Some works \cite{chen2018reverse, oktay2018attention, fan2020pranet} try to address this issue by embedding attention mechanisms in the encoder or decoder. Despite the significant efforts made in this direction, the CNN-based methods still have insufficient ability to capture long-range dependencies.

%Transformers rely on an attention-based network architecture; they were first introduced for sequence-to-sequence prediction in natural language processing (NLP) \cite{vaswani2017attention}
With the emergence of Vision transformers \cite{dosovitskiy2020image}, many works \cite{cao2021swin, chen2021transunet, dong2021polyp, wang2022stepwise} try to address the above problem using a transformer encoder, specifically for medical image segmentation. Transformers capture long-range dependencies by learning correlations among all the input patches using self-attention (SA). Recently, hierarchical vision transformers, such as pyramid vision transformer (PVT) \cite{wang2021pyramid} with spatial reduction attention, Swin transformer \cite{liu2021swin} with window-based attention, and MaxViT \cite{tu2022maxvit} with multi-axis attention have been introduced to improve performance. Indeed, these hierarchical vision transformers are very effective for medical image segmentation tasks \cite{cao2021swin, dong2021polyp, wang2022stepwise}. However, these transformer-based architectures have two limitations: 1) self-attention is performed with a single attention window (scale) which has limited feature processing ability, and 2) the self-attention modules used in transformers have limited ability to learn spatial relations among pixels \cite{chu2021conditional}. %multi-scale
 
%Following the success of transformers in NLP, the vision transformer \cite{dosovitskiy2020image} divides an image into non-overlapping patches which are fed into the transformer module with positional embeddings. 
 %i) have limited discrimination ability due to embedding convolution layer directly between fully-connected layers of the feed-forward network, and ii) 

More recently, PVTv2 \cite{wang2022pvt} embeds convolution layers in transformer encoders, while CASCADE \cite{rahman2023medical} introduces an attention-based decoder to address the limitation of learning spatial relations among pixels. Although these methods enable learning, the local (spatial) relations among pixels, they still have limited ability to capture features of multi-scale (e.g., small, large) organs/lesions/objects due to computing self-attention in a single-scale attention window. To address this limitation, we introduce a novel \textit{multi-scale hierarchical} vision transformer (MERIT) backbone which computes \textcolor{black}{self-attention} across \textit{multiple attention windows} to improve the generalizability of the model. We also incorporate multiple CASCADE decoders to produce better high-resolution segmentation maps by effectively aggregating and enhancing multi-scale hierarchical features. Finally, we introduce a novel effective multi-stage (hierarchical) feature-mixing loss aggregation (MUTATION) strategy \textcolor{black}{for implicit ensembling/augmentation} which produces new synthetic predictions by mixing hierarchical prediction maps from the decoder. The aggregated loss from these synthetic predictions improves the performance of medical image segmentation. Our contributions are as follows:
\vspace{-0.3cm} 

\begin{itemize}
  \item \textbf{Novel Network Architecture:} We propose a novel multi-scale hierarchical vision transformer (MERIT) for 2D medical image segmentation which captures both multi-scale and multi-resolution features. Besides, we incorporate a cascaded attention-based decoder for better hierarchical multi-scale feature aggregation and refinement.
\vspace{-0.5cm} \item \textbf{Multi-stage Feature-mixing Loss Aggregation:} We propose a new simple, yet effective way, namely MUTATION, to create synthetic predictions by mixing features during loss calculation; this improves the medical image segmentation performance.
\vspace{-0.1cm} \item \textbf{New State-of-the-art Results:} 
We perform rigorous experiments and ablation studies on two medical image segmentation benchmarks, namely Synapse multi-organ and ACDC cardiac diagnosis. Our implementation of MERIT using two instances (with different windows for SA) of MaxViT \cite{tu2022maxvit} backbone with CASCADE decoder and MUTATION loss aggregation strategy produces new state-of-the-art (SOTA) results on Synapse multi-organ and ACDC segmentation benchmarks.
%\vspace{-0.6cm} 

\end{itemize}

\section{Related Work}
\label{sec:related_work}
%We divide the related work into two parts, such as vision transformers, and medical image segmentation; these are described next. 

\subsection{Vision transformers}

%%where they feed non-overlapping image patches with positional encoding in the transformer block. ViT 

Dosovitskiy et al. \cite{dosovitskiy2020image} build the first vision transformer (ViT), which can learn long-range (global) relations among the pixels through SA. Recent works focus on improving ViT in different ways, such as designing new SA blocks \cite{liu2021swin, tu2022maxvit}, incorporating CNNs \cite{wang2022pvt, tu2022maxvit}, or introducing new architectural designs \cite{wang2021pyramid, xie2021segformer}. Liu et al. \cite{liu2021swin} introduce a sliding window attention mechanism in the hierarchical Swin transformer. In DeiT \cite{touvron2021training}, authors explore data-efficient training strategies to minimize the computational cost for ViT. 
SegFormer \cite{xie2021segformer} proposes a positional-encoding-free hierarchical transformer using Mix-FFN blocks. In PVT, authors \cite{wang2021pyramid} develop a pyramid vision transformer using a spatial reduction attention mechanism. The authors extend the PVT to PVTv2 \cite{wang2022pvt} by embedding an overlapping patch embedding, a linear complexity attention layer, and a convolutional feed-forward network. Recently, in MaxViT \cite{tu2022maxvit}, authors propose a multi-axis self-attention mechanism to build a hierarchical hybrid CNN transformer.

Although vision transformers have shown excellent promise, they have limited spatial information processing ability; also, there is little effort in designing multi-scale transformer backbones \cite{lin2022ds}. In this paper, we address these very limitations by introducing a multi-scale hierarchical vision transformer with attention-based decoding.

%Although vision transformers have shown excellent promise, their performance is limited when trained on small datasets. This limitation makes the transformers difficult to train for applications like medical image segmentation with small amounts of data. We try to overcome this limitation by using pretrained transformer backbones in large datasets (like ImageNet); indeed, previous studies \cite{dong2021polyp, wang2022stepwise} have found that pretrained transformer weights on other non-medical large datasets boost the performance of medical image segmentation tasks.


%\subsection{Attention mechanisms}
%%Different types of attention mechanisms are commonly incorporated in CNNs for learning better feature representations, especially on datasets having high variability. 
%%We are the first to use the AG module for decoding transformer features. 
%Oktay et al. \cite{oktay2018attention} introduce a low-cost attention gate module for U-shaped architectures to fuse features with skip-connections; this helps the model focus on the relevant information in the image. Chen et al. \cite{chen2018reverse} propose a reverse attention module to explore the missing detail information which results in high resolution and accurate outputs. Hu et al. \cite{hu2018squeeze} introduce a squeeze-and-excitation block using global average-pooled features to compute channel attention; this identifies the important feature maps for learning and then enhances them. Although channel attention can identify which feature map to focus on, it lacks the ability to identify where to focus. To supplement the channel attention block, Chen et al. \cite{chen2017sca} propose a spatial attention block to better focus on a feature map. Woo et al. \cite{woo2018cbam} introduce a convolutional block attention module (CBAM) utilizing both channel and spatial attention to capture where and on which feature to focus in a feature map. %They suggest using both average-pooled and max-pooled features instead of only average-pooled features. 
%Their experiments show that channel attention followed by spatial attention produces the best results. 

%Due to the additive advantage of CBAM with negligible overhead, we incorporate channel attention followed by spatial attention in our CAM. The CAM differs from CBAM in the design of the block itself and in how the blocks are used. Firstly, our CAM consists of channel attention, spatial attention, and a convolutional block, while CBAM consists of only channel attention and spatial attention. Secondly, CBAM is placed in each convolutional block of both encoder and decoder, while the CAM module appears only in the decoder.

%%In general, most CNNs and neural networks can benefit by adding attention modules. However, even with these attention mechanisms, CNNs are limited by the locality of convolution operations. This limitation makes them difficult to model natural long-range spatial dependencies between input segments.

%%PraNet [23] uses the Reverse Attention module [30], which enforces focus on the boundary between a polyp and its surroundings. 
%\vspace{-0.5cm} 
\subsection{Medical image segmentation}
Medical image segmentation can be formulated as a dense prediction task of classifying the pixels of lesions or organs in endoscopy, CT, MRI, etc. \cite{dong2021polyp, chen2021transunet}. U-shaped architectures \cite{ronneberger2015u, oktay2018attention, zhou2018unet++, huang2020unet, lou2021dc} are commonly used in medical image segmentation because of their sophisticated encoder-decoder architecture. Ronneberger et al. \cite{ronneberger2015u} introduce UNet, an encoder-decoder architecture that aggregates features from multiple stages through skip connections. In UNet++ \cite{zhou2018unet++}, authors use nested encoder-decoder sub-networks that are linked using dense skip connections. Besides, UNet3Plus \cite{huang2020unet} explores the full-scale skip connections having intra-connections among the decoder blocks.


Transformers are nowadays widely used in medical image segmentation \cite{cao2021swin, chen2021transunet, dong2021polyp}. In TransUNet \cite{chen2021transunet}, authors propose a hybrid CNN transformer architecture to learn both local and global relations among pixels. Swin-Unet \cite{cao2021swin} introduces a pure U-shaped transformer using Swin transformer \cite{liu2021swin} blocks. %Due to using Swin transformer blocks both in the encoder and decoder, Swin-Unet shows little improvement in performance. 
Recently, in CASTFormer \cite{you2022class}, authors introduce a class-aware transformer with adversarial training.

Some studies explore attention mechanisms with CNN \cite{oktay2018attention, fan2020pranet} and transformer-based architectures \cite{dong2021polyp} for medical image segmentation. In PraNet \cite{fan2020pranet}, authors utilize the reverse attention \cite{chen2018reverse}. PolypPVT \cite{dong2021polyp} uses PVTv2 \cite{wang2022pvt} as the encoder and adopts a CBAM \cite{woo2018cbam} attention block in the decoder with other modules. In CASCADE \cite{rahman2023medical}, authors propose a cascaded decoder using attention modules for feature refinement. %They extract features from four stages of the transformer encoder and use CASCADE to create high-resolution segmentation maps after cascaded refinement of features. 
Due to its remarkable performance in medical image segmentation, we incorporate the CASCADE decoder with our architecture.

%\vspace{-0.5cm} 
\section{Method}
\label{sec:method}
In this section, we first introduce our proposed multi-scale hierarchical vision transformer (MERIT) backbone and decoder. We then describe an overall architecture combining our MERIT (i.e., MaxViT \cite{tu2022maxvit}) with the decoder (i.e., CASCADE \cite{rahman2023medical}). Finally, we introduce a new hierarchical feature-mixing loss aggregation method. 
%\subsection{Hierarchical Vision Transformer}
%To ensure enough generalization and multi-scale feature processing abilities for medical image segmentation, we use the pyramid transformer, as well as the hybrid CNNtransformer (instead of only CNN) as the encoder. Specifically, we adopt the encoder design of PVTv2 [32] (Figure 1(a)) and TransUNet [3]. PVTv2 uses the convolution operation instead of the patch embedding module of the traditional transformer to consistently capture the spatial information. TransUNet utilizes a transformer on top of CNN to capture both global and spatial relationships among features. Our proposed decoder is flexible and easy to adopt with other hierarchical backbone networks.

%\vspace{-0.5cm} 
\subsection{Multi-scale hierarchical vision transformer (MERIT)}
To improve the generalizability of the model across small and large objects in an image, we propose two designs based on the MERIT backbone network, i.e., Cascaded and Parallel. %we introduce MERIT which uses multiple backbone networks with different windows for SA. 

\begin{figure}[t]%[htbp]
% % Caption and label go in the first argument and the figure contents
% % go in the second argument
\floatconts
  {fig:cascaded_architecture}
  {\vspace{-0.7cm} 
\caption{Cascaded MERIT architecture. (a) cascaded MERIT backbone, (b) decoders with cascaded skip connections from the decoder 1, (c) prediction maps aggregation of two decoders. p1, p2, p3, and p4 are the aggregated multi-stage prediction maps.}}
  {\includegraphics[width=0.9\linewidth]{images/cascaded_merit_architecture.png}}
  \vspace{-0.5cm}
\end{figure}


\subsubsection{Cascaded MERIT}
\label{ssec:cascaded_merit}

In the cascaded design of our MERIT, we add (i.e., cascade) feedback from a backbone to the next backbone. We extract the hierarchical features from four different stages of the backbone network. Then, we cascade these features with the features from the previous backbone and pass them to the skip connections and bottleneck modules of respective decoders, except the first decoder. We also pass feedback from the decoder of one backbone (except the last) to the next backbone. This design captures the multi-scale, as well as multi-resolution features due to using multiple attention windows and hierarchical features. It also refines the features well due to adding feedback from the decoder of a backbone to the next backbone and using cascaded skip connections. Fig. \ref{fig:cascaded_architecture}(a) presents the Cascaded MERIT architecture with two backbone networks. For each backbone network, the images with resolution (H, W) are first put into a Stem layer (TB1 Stem, TB2 Stem in Fig. \ref{fig:cascaded_architecture}(a)) which reduces the feature resolution to (H/4, W/4). Afterward, these features are passed through four stages of transformer backbones (this reduces feature resolution by 2 times at each stage except the fourth). The features from the last stage of the first decoder are combined with the input image to cascade it (feature) with the second backbone in Fig. \ref{fig:cascaded_architecture}(a). To do this, we reduce the number of channels to one and produce logits by applying a 1x1 convolution followed by Sigmoid activation. \textcolor{black}{We also resize the feature map to the input resolution (i.e., $224\times224$ in our implementation) of Backbone 2.}

\subsubsection{Parallel MERIT}
\label{ssec:parallel_merit}

Unlike Cascaded MERIT, in the parallel design of our MERIT backbone, we pass input images of multiple resolutions/scales in parallel into separate hierarchical transformer backbone encoders with different attention windows. Similar to Cascaded MERIT, we extract the hierarchical features from four different stages of the backbone networks and pass those features to the respective parallel decoders. This design also captures multi-scale features due to using hierarchical backbones with multiple attention windows. Fig. \ref{fig:parallel_architecture}(a) in Appendix \ref{asec:parallel_architecture} presents a design for the Parallel MERIT with two backbone networks. The input images are passed through similar steps in the backbone networks as in Cascaded MERIT. However, the Parallel MERIT shares information among the backbone networks only at the very end during the feature aggregation step (Fig. \ref{fig:parallel_architecture}(c) in Appendix \ref{asec:parallel_architecture}).  
%At last, we aggregate the multi-stage prediction maps generated by the decoders \textit{at the end} to generate the final multi-stage segmentation maps.

\subsection{Decoder}
We propose using a separate decoder for each transformer backbone. As shown in Fig.  \ref{fig:cascaded_architecture}(b), we use cascaded skip connections in the decoder of our cascaded MERIT architecture. Here, we add the skip connections from the first backbone to the skip connections of the second backbone network. In this case, we share information across backbones in three phases, i.e., during backbone cascading, skip connections cascading, and aggregating prediction maps. This sharing of information helps to capture richer information than the single-resolution backbone, as well as our Parallel MERIT.

Unlike Fig. \ref{fig:cascaded_architecture}(b), in Fig. \ref{fig:parallel_architecture}(b) in Appendix \ref{asec:parallel_architecture}, we have two parallel decoders for our parallel backbones. Each decoder has four stages that correspond to four stages of the transformer backbone. We only aggregate the multi-stage prediction maps produced by the decoders in Fig. \ref{fig:parallel_architecture}(b) at the aggregation step shown in Fig. \ref{fig:parallel_architecture}(c). 
%\subsubsection{CASCADE using Catenation Aggregation}
%\subsubsection{CASCADE using Additive Aggregation}

\subsection{Overall Architecture}
In our experiments, we use one of the most recent SOTA transformers, MaxViT \cite{tu2022maxvit}. We use two instances of MaxViT-S (standard) backbone with $8\times8$ and $7\times7$ attention windows to create our MERIT backbone. Each MaxViT backbone has two Stem blocks followed by four stages that consist of multiple (i.e., 2, 2, 5, 2) MaxViT blocks. Each MaxViT block is built with a Mobile Convolution Block (MBConv), a Block Attention having Block Self-Attention (SA) followed by a Feed Forward Network (FFN), a Grid Attention having a Grid SA followed by an FFN. We note that although we use the MaxViT backbone in our experiments, other transformer backbones can easily be used with our MERIT. %with multi-scale attention windows 

Pure transformers have limited (spatial) contextual information processing ability among pixels. As a result, the transformer-based models face difficulties in locating discriminative local features. To address this issue, we adopt a recent attention-based cascaded decoder, CASCADE \cite{rahman2023medical}, for multi-stage feature refinement and aggregation. CASCADE \textcolor{black}{decoder} uses the attention gate (AG) \cite{oktay2018attention} for cascaded feature aggregation and the convolutional attention module (CAM) for robust feature map enhancement. CASCADE \textcolor{black}{decoder} has four CAM blocks for the four stages of hierarchical features from the transformer backbone and three AGs for three skip connections. CASCADE \textcolor{black}{decoder} aggregates the multi-resolution features by combining the upsampled features from the previous stage of the decoder with the features from the skip connections using AG. Then, CASCADE \textcolor{black}{decoder} processes the aggregated features using the CAM module (consists of channel attention \cite{hu2018squeeze} followed by spatial attention \cite{chen2017sca}) which groups pixels together and suppresses background information. Lastly, CASCADE \textcolor{black}{decoder} sends the output from the CAM block of each stage to a prediction head to produce prediction maps. %Although we propose to use CASCADE with our MERIT backbone for the experiments, other hierarchical decoders having skip connections can be used with our architectural design.

We produce four prediction maps from the four stages of the CASCADE \textcolor{black}{decoder}. As shown in Fig. \ref{fig:cascaded_architecture}(c) and Fig. \ref{fig:parallel_architecture}(c) in Appendix \ref{asec:parallel_architecture}, we aggregate (additive) the prediction maps for each stage of our two decoders. We generate the final prediction map, $\hat{y}$, using Equation \ref{eq:output_aggregation}:
\begin{equation}
    \hat{y} = \alpha \times p1 + \beta \times p2 + \gamma \times p3 + \psi \times p4
    \label{eq:output_aggregation}
\end{equation}
where $p1$, $p2$, $p3$, and $p4$ represent the prediction maps, and $\alpha$, $\beta$, $\gamma$, and $\psi$ are the weights of each prediction heads. We use the value of 1.0 for $\alpha$, $\beta$, $\gamma$, and $\psi$. Finally, we apply Softmax activation on $\hat{y}$ to get the multi-class segmentation output.

\subsection{Multi-stage feature-mixing loss aggregation (MUTATION)}
We now introduce a simple, yet effective multi-stage feature mixing loss aggregation strategy for image segmentation, which enables better model training. Our intention is to create new prediction maps combining the available prediction maps. So, we take all the prediction maps from different stages of a network as input and aggregate the losses of prediction maps generated using $2^n-1$ non-empty subsets of $n$ prediction maps. For example, if a network produces 4 prediction maps, our multi-stage feature-mixing loss aggregation produces a total of $2^4-1 = 15$ prediction maps including 4 original maps. This mixing strategy is simple, it does not require additional parameters to calculate, and it does not introduce inference overheads. Due to its potential benefits, this strategy can be used with \textit{any} multi-stage image segmentation or dense prediction networks. Algorithm \ref{alg:msfmla} presents the steps to produce new prediction maps and loss aggregation.   

\begin{algorithm2e}
\caption{Multi-stage Feature-Mixing Loss Aggregation}
\label{alg:msfmla}
\LinesNumbered
\KwIn{$y; $ the ground truth mask$\newline$ A list $[P_i]$; $i=0, 1, \cdots, n-1$, where each element is a prediction map}
\KwOut{$loss$; the aggregated loss}
$loss\leftarrow 0.0$\;
$\mathcal S \leftarrow$ find all non-empty subsets of prediction map indices, $\{0, \ldots, n-1\}$;
\tcp{$\mathcal S$ is the set of non-empty subsets of $\{0, \ldots, n-1\}$}
\ForEach {$s \in \mathcal S $}{

  $\hat{y} \leftarrow 0.0$;  \tcp{$\hat{y}$ is a new prediction map}
  \ForEach {$i \in s$}{    
    $\hat{y} \leftarrow \hat{y} + P_i$\;
  }
  $loss \leftarrow loss\_function(y, \hat{y})$; \textcolor{black}{\tcp{$loss\_function(.)$ is any loss function (e.g., CrossEntropy, DICE)}}
}
%\vspace{-0.5cm}
\end{algorithm2e}

\begin{table*}[t]
%\begin{center}
\centering
\floatconts
{tab:multi_organ_results}
{\caption{Results on Synapse multi-organ dataset. DICE scores (\%) are reported for individual organs. The results of UNet, AttnUNet, PolypPVT, and SSFormerPVT are taken from CASCADE \cite{rahman2023medical}. MERIT results are averaged over five runs for MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION. $\uparrow$ denotes higher the better, $\downarrow$ denotes lower the better. The best results are in bold.}
\vspace{-0.5cm}
} % We report the average results of five runs for MERIT. 
    {%\small{
\begin{adjustbox}{width=1\textwidth}
{\begin{tabular}{lrrrrrrrrrr}
\toprule
\multirow{2}{*}{Architectures} & \multicolumn{2}{c}{Average}                                                                                     & \multicolumn{1}{l}{\multirow{2}{*}{Aorta}} & \multicolumn{1}{l}{\multirow{2}{*}{GB$^b$}} & \multicolumn{1}{l}{\multirow{2}{*}{KL$^b$}} & \multicolumn{1}{l}{\multirow{2}{*}{KR$^b$}} & \multicolumn{1}{l}{\multirow{2}{*}{Liver}} & \multicolumn{1}{l}{\multirow{2}{*}{PC$^b$}} & \multicolumn{1}{l}{\multirow{2}{*}{SP$^b$}} & \multicolumn{1}{l}{\multirow{2}{*}{SM$^b$}} \\
                               & \multicolumn{1}{l}{DICE$\uparrow$} & \multicolumn{1}{l}{HD95$^a$$\downarrow$} &  \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                         & \multicolumn{1}{l}{}                         & \multicolumn{1}{l}{}                       & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                    & \multicolumn{1}{l}{}                    \\
\midrule
%   
UNet \cite{ronneberger2015u}     &      70.11      &    44.69 
    &   84.00   &      56.70   &       72.41  &       62.64  &      86.98   &     48.73   &      81.48   &     67.96
    \\
AttnUNet \cite{oktay2018attention}    &  71.70     &  34.47 
     &  82.61       &  61.94        &  76.07   &  70.42  
    &  87.54        &  46.70   &  80.67      &  67.66

\\
%UNet \cite{ronneberger2015u}                  & 76.85                    & 39.70                        & 89.07                                      & 69.72                                   & 77.77                                        & 68.60                                        & 93.43                                      & 53.98                                   & 86.67                                   & 75.58           %          \\
%AttnUNet \cite{oktay2018attention}                    & 77.77                    & 36.02                   & \textbf{89.55}                                     & 71.11                                   & 77.98                                        & 70.42                                        & 93.57                                      & 58.04                                   & 87.30                                   &  75.75                                   
%\\
% 
R50+UNet \cite{chen2021transunet}                  & 74.68                    & 36.87                    & 84.18                                      & 62.84                                   & 79.19                                        & 71.29                                        & 93.35                                      & 48.23                                   & 84.41                                   & 73.92                                   
\\
%
R50+AttnUNet \cite{chen2021transunet}                   & 75.57                    & 36.97                    & 55.92                                      & 63.91                                   & 79.20                                        & 72.71                                        & 93.56                                      & 49.37                                   & 87.19                                   & 74.95                                   
\\
%
SSFormerPVT \cite{wang2022stepwise}                   & 78.01                    & 25.72                    & 82.78                                      & 63.74                                   & 80.72                                        & 78.11                                        & 93.53                                      & 61.53                                   & 87.07                                   & 76.61                                   \\ %
PolypPVT \cite{dong2021polyp}                       & 78.08                    & 25.61                    & 82.34                                      & 66.14                                   & 81.21                                        & 73.78                                        & 94.37                                      & 59.34                                   & 88.05                                   & 79.4                                    \\ %  
TransUNet \cite{chen2021transunet}                    & 77.48                    & 31.69                     & 87.23                                      &  63.13                                   & 81.87                                       & 77.02                                        & 94.08                                     & 55.86                                   & 85.08                                   & 75.62                                      \\ %         
SwinUNet \cite{cao2021swin}                      & 79.13                    & 21.55                    & 85.47                                      & 66.53                                  &  83.28                                        & 79.61                                        & 94.29                                      & 56.58                                   & 90.66                                   & 76.60                                  \\ 
MT-UNet \cite{wang2022mixed}                      & 78.59                    & 26.59                    & 87.92                                      & 64.99                                   & 81.47                                        & 77.29                                        & 93.06                                     & 59.46                                  & 87.75                                   & 76.81                                  \\
MISSFormer \cite{huang2021missformer}                      & 81.96                    & 18.20                    & 86.99                                      &  68.65                                   &  85.21                                        & 82.00                                        & 94.41                                      & 65.67                                   & 91.92                                   & 80.81                                   \\        
CASTformer \cite{you2022class}                      & 82.55                    & 22.73                    & \textbf{89.05}                                      &  67.48                                   & 86.05                                       & 82.17                                        & \textbf{95.61}                                      & 67.49                                   & 91.00                                   & 81.55                                   \\
%
PVT-CASCADE \cite{rahman2023medical}                  & 81.06                    & 20.23                   & 83.01                                      & 70.59                                   & 82.23                                        & 80.37                                        & 94.08                                      & 64.43                                   & 90.1                                    & 83.69                                   \\ %
TransCASCADE \cite{rahman2023medical}                 & 82.68                    & 17.34                    & 86.63                                      & 68.48                                   & 87.66                                        & 84.56                                        & 94.43                                      & 65.33                                   & 90.79                                   & 83.52                                   \\
%MaxViT \cite{tu2022maxvit}                 & 78.53                    & 22.03                    & 85.07                                      & 65.59                                   & 81.57                                        & 77.94                                        & 94.05                                     & 57.81                                   & 89.07                                  & 77.15                                   \\
\midrule
Parallel MERIT (Ours)                   & 84.22                    & 16.51                    & 88.38                                      & 73.48                                  & 87.21                                        & 84.31                                        & 95.06                                      & 69.97                                   & 91.21                                    & 84.15                                   \\
Cascaded MERIT (Ours)                   & \textbf{84.90}                    & \textbf{13.22}                    & 87.71                                      & \textbf{74.40}                                   & \textbf{87.79}                                       & \textbf{84.85}                                        & 95.26                                      & \textbf{71.81}                                   &  \textbf{92.01}                                    & \textbf{85.38}                                   \\
%\midrule
%Improve MaxViT              & 5.07                     & 9.56                     & 0.07                                       & 8.05                                    & 7.12                                         & 6.03                                         & 0.1                                        & 6.86                                    & 3.73                                    & 8.52        \\
\bottomrule %\\
\end{tabular}}
\end{adjustbox}
}%}
\footnotesize{$^a$ More details in Appendix \ref{assec:eval_metrics}, $^b$ more details in Appendix \ref{assec:datasets}}\\
\vspace{-0.3cm}
\end{table*}

\section{Experiments}
In this section, we demonstrate the superiority of our proposed MERIT architectures by comparing the results with SOTA methods. We introduce datasets, evaluation metrics, and implementation details in \textbf{Appendix \ref{asec:experimental_setup}}. %We compare our proposed architectures (i.e., Parallel MERIT and Cascaded MERIT) with SOTA CNN and transformer-based segmentation methods
%, including UNet \cite{ronneberger2015u}, AttnUNet \cite{oktay2018attention}, R50+UNet \cite{ronneberger2015u}, R50+AttnUNet \cite{oktay2018attention}, TransUNet \cite{chen2021transunet}, SwinUnet \cite{cao2021swin} 
%on Synapse multi-organ and ACDC segmentation datasets. 
More experiments and ablation studies to answer questions related to our architectures are given in \textbf{Appendix \ref{assec:baseline_compare}-\ref{assec:loss_weight}}.

% R50+AttnUNet and R50+UNet use a pre-trained ResNet50 backbone. 


%\subsection{Results}
%\label{ssec:results}

%The results for answering intrinsic questions related to our proposed architectures are reported in the Appendix section. 
%, and Polyp (i.e., Endoscene \cite{vazquez2017benchmark}, CVC-ClinicDB \cite{bernal2015wm}, Kvasir \cite{jha2020kvasir}, ColonDB \cite{tajbakhsh2015automated}, ETIS-LaribDB \cite{silva2014toward}) datasets. More results are available in the supplementary materials.

\subsection{Results on Synapse multi-organ segmentation}
Table \ref{tab:multi_organ_results} presents the results of Synapse multi-organ segmentation; it can be seen that both variants of our MERIT significantly outperform all the SOTA CNN- and transformer-based 2D medical image segmentation methods. Among all the methods, our Cascaded MERIT achieves the best average DICE score (84.90\%). Cascaded MERIT outperforms two popular methods on this dataset, such as TransUNet and SwinUNet by 7.42\% and 5.57\%, respectively, when compared to their original reported DICE scores. Cascaded MERIT achieves 2.22\% better DICE than the existing best method, TransCASCADE (82.68\% DICE), on this dataset. When we compare the HD95 distance of all the methods, we find that both variants of our MERIT achieve a lower HD95 distance. Cascaded MERIT has the lowest HD95 distance (13.22) which is 18.47 lower than TransUNet (HD95 of 31.69) and 4.12 lower than the best SOTA method, TransCASCADE (HD95 of 17.34).


If we look into the DICE score of individual organs, we observe that proposed MERIT variants significantly outperform SOTA methods on six out of eight organs. We also can conclude that Cascaded MERIT performs better both in large and small organs, though it exhibits greater improvement for small organs. We believe that both MERIT variants demonstrate better performance due to using the multi-scale hierarchical transformer encoder with cascaded attention-based decoding and MUTATION loss aggregation. 
%We show the effect of each component empirically by conducting some ablation studies which are reported in the Appendix \ref{asec:ablation_studies}. 

\begin{table*}[t]
\centering
\floatconts
{tab:acdc_results}
{\caption{Results on the ACDC dataset. DICE scores (\%) are reported for individual organs. We present the results of MERIT averaging over five runs with the setting MERIT+CASCADE \textcolor{black}{decoder(Additive)}+MUTATION. The best results are in bold.}
\vspace{-0.5cm}
} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {\small{
    %\vspace{-0.5}
%\begin{adjustbox}{width=1\textwidth}
{\begin{tabular}{lrrrr}
\toprule
Architectures     & \multicolumn{1}{l}{Avg DICE} & \multicolumn{1}{l}{RV$^a$} & \multicolumn{1}{l}{Myo$^a$} & \multicolumn{1}{l}{LV$^a$} \\
\midrule
R50+UNet   \cite{chen2021transunet}       & 87.55                        & 87.10                  & 80.63                   & 94.92                  \\
R50+AttnUNet  \cite{chen2021transunet}  & 86.75                        & 87.58                  & 79.20                   & 93.47                  \\
ViT+CUP \cite{chen2021transunet}   & 81.45                        & 81.46                  & 70.71                   & 92.18                 \\
R50+ViT+CUP \cite{chen2021transunet} & 87.57                        & 86.07                  & 81.88                   & 94.75                  \\
TransUNet  \cite{chen2021transunet}       & 89.71                        & 88.86                  &  84.53                   & 95.73                  \\   
SwinUNet \cite{cao2021swin}         & 90.00                        & 88.55                  & 85.62                   & 95.83                  \\
MT-UNet \cite{wang2022mixed}         & 90.43                        & 86.64                  & 89.04                   & 95.62                  \\
MISSFormer \cite{huang2021missformer}         & 90.86                        & 89.55                  & 88.04                   & 94.99                  \\
PVT-CASCADE \cite{rahman2023medical}      & 91.46                        & 88.9                   & 89.97                   & 95.50                   \\
TransCASCADE \cite{rahman2023medical}    & 91.63                        & 89.14                 & \textbf{90.25}                   & 95.50 \\
%MaxViT \cite{tu2022maxvit}    & 90.98                        & 88.71                  & 89.02                   & 95.21 \\
\midrule
Parallel MERIT (Ours)      & \textbf{92.32}                        & \textbf{90.87}                   & 90.00                   & \textbf{96.08}                   \\
Cascaded MERIT (Ours)    & 91.85                      & 90.23                 & 89.53                   & 95.80 \\
\bottomrule %\\
\end{tabular}}
%\end{adjustbox}
}}
\footnotesize{$^a$ More details in Appendix \ref{assec:datasets}}\\
\vspace{-0.3cm}
\end{table*}

\subsection{Results on ACDC cardiac organ segmentation}
Table \ref{tab:acdc_results} reports three cardiac organ segmentation results of different methods on the ACDC dataset for MRI data modality. Both our Parallel and Cascaded MERIT have better DICE scores than all other SOTA methods. Our Parallel MERIT achieves the best average DICE score (92.32\%) which outperforms TransUNet and SwinUNet by 2.61\% and 2.32\%, respectively. Parallel MERIT also shows the best DICE scores in RV$^ {\ref{assec:datasets}}$ (90.87\%) and LV$^{\ref{assec:datasets}}$ (96.08\%) segmentation. We can conclude from these results that our method performs the best across different medical imaging data modalities. 

\section{Conclusion}
\label{sec:conclution}
In this paper, we have introduced a novel multi-scale hierarchical transformer architecture (MERIT) that can capture both the multi-scale and multi-resolution features necessary for medical image segmentation. We have also incorporated an attention-based cascaded decoder to further refine features. Moreover, we have proposed a novel multi-stage feature mixing loss aggregation (MUTATION) strategy \textcolor{black}{for implicit ensembling/augmentation} which ensures better model training and boosts the performance without introducing additional hyper-parameters and inference overhead. Our experimental results on two well-known multi-class medical image segmentation benchmarks demonstrate the superiority of our proposed method over all SOTA approaches. Finally, we believe that our proposed MERIT architectures and MUTATION loss aggregation strategy will improve other downstream medical image segmentation and semantic segmentation tasks.   %Our MERIT can easily be adopted with other vision transformers. Besides, we believe that our MUTATION loss aggregation strategy can be used to boost the performance of other multi-stage architectures. 

%% Acknowledgments---Will not appear in anonymized version
%\midlacknowledgments{We thank a bunch of people.}


\bibliography{midl-samplebibliography}


\newpage

\appendix
\vspace{-0.5cm}
\section{Parallel MERIT Architecture}
\label{asec:parallel_architecture}
Due to the page limitation, our parallel MERIT architecture is given in Fig. \ref{fig:parallel_architecture}. This architecture is described in Section \ref{ssec:parallel_merit} of the main text.

\begin{figure}[t]%[htbp]
% % Caption and label go in the first argument and the figure contents
% % go in the second argument
\floatconts
  {fig:parallel_architecture}
  {\vspace{-0.6cm}\caption{Parallel MERIT architecture. (a) parallel MERIT backbone, (b) parallel decoders, (c) prediction maps aggregation from two decoders. p1, p2, p3, and p4 are the aggregated multi-stage prediction maps.}}
  {\includegraphics[width=0.9\linewidth]{images/parallel_merit_architecture.png}}\vspace{-0.5cm}
\end{figure}

%\vspace{-0.3cm}
\section{Experimental Setup}
\label{asec:experimental_setup}
This section first describes datasets, then introduces evaluation metrics, and finally provides the implementation details of our proposed architecture and experiments. 

\subsection{Datasets}
\label{assec:datasets}

\textbf{Synapse multi-organ dataset.} There are 30 abdominal CT scans with 3779 axial contrast-enhanced abdominal CT images in the Synapse multi-organ dataset\footnote{\href{https://www.synapse.org/\#!Synapse:syn3193805/wiki/217789}{https://www.synapse.org/\#!Synapse:syn3193805/wiki/217789 }}. Each CT scan has 85-198 slices of resolution $512 \times 512$ pixels, having a voxel spatial resolution of ([0:54-0:54] $\times$ [0:98-0:98]$\times$[2:5-5:0])$mm^3$. We extract 2D slices from the CT scans and segment 8 abdominal organs, such as the aorta, gallbladder (GB), left kidney (KL), right kidney (KR), liver, pancreas (PC), spleen (SP), and stomach (SM). Following the experimental protocol of TransUNet \cite{chen2021transunet}, we split the dataset into 18 scans (2211 axial slices) for training, and 12 for validation.  %, and Average Symmetric Surface Distance (ASD) 

\textbf{ACDC dataset.} The ACDC dataset\footnote{\href{https://www.creatis.insa-lyon.fr/Challenge/acdc/}{https://www.creatis.insa-lyon.fr/Challenge/acdc/}} contains 100 cardiac MRI scans collected from different patients. We extract 2D slices from each MRI scan and segment three organs, such as the right ventricle (RV), left ventricle (LV), and myocardium (Myo). Following MT-UNet \cite{wang2022mixed}, we split the dataset into 70 (1304 axial slices), 10 (182 axial slices), and 20 cases for training, validation, and testing, respectively.
% and MT-UNet \cite{wang2022mixed}

%\textbf{Polyp datasets.} CVC-ClinicDB \cite{bernal2015wm} contains 612 images, which are extracted from 31 colonoscopy videos. Kvasir includes 1,000 polyp images, which are collected from the polyp class in the Kvasir-SEG dataset \cite{jha2020kvasir}. Following the settings in PraNet \cite{fan2020pranet}, we adopt the same 900 and 548 images from CVC-ClinicDB and Kvasir datasets as the training set, and the remaining 64 and 100 images are employed as the respective testsets. To evaluate the generalization performance, we test the model on three unseen datasets, namely EndoScene \cite{vazquez2017benchmark}, ColonDB \cite{tajbakhsh2015automated}, and ETIS-LaribDB \cite{silva2014toward}. These three testsets are collected from different medical centers. In other words, the data from these three sources are not used to train our model. EndoScene, ColonDB, and ETIS-LaribDB contain 60, 380, and 196 images, respectively. 

\subsection{Evaluation metrics.} 
\label{assec:eval_metrics}
In our experiments on the Synapse Multi-organ dataset, we use DICE and 95\% Hausdorff Distance (95\%HD) as the evaluation metrics. However, we use only DICE scores as an evaluation metric for the ACDC dataset. The DICE similarity scores $DSC(Y, \hat{Y})$ and 95\%HD distance $D_H(Y, \hat{Y})$ (95th percentile of the distances between boundary points in $Y$ and $\hat{Y}$) are calculated using Equations \ref{eq:dice} and \ref{eq:95hd}, respectively. %For the experiments on polyp segmentation, we use DICE and mIoU as the evaluation metrics.

\begin{equation}\label{eq:dice}
DSC(Y, \hat{Y}) = \frac{2 \times \lvert Y \cap \hat{Y} \rvert}{\lvert Y \rvert + \lvert \hat{Y} \rvert}\times100
\end{equation}

\begin{equation}\label{eq:95hd}
D_H(Y, \hat{Y}) = \max\{d_{Y\hat{Y}},d_{\hat{Y}Y}\} = \max \{\max_{y \in Y} \min_{\hat{y} \in \hat{Y}}d(y, \hat{y}), \{\max_{\hat{y} \in \hat{Y}} \min_{y \in Y}d(y, \hat{y})\}
%\frac{2 \times \lvert Y \cap \hat{Y} \rvert}{\lvert Y \rvert + \lvert \hat{Y} \rvert}*100
\end{equation}
where $Y$ and $\hat{Y}$ are the ground truth mask and predicted segmentation map, respectively. 

\subsection{Implementation details}
\label{assec:impl_details}
We use PyTorch 1.12.0 with CUDA 11.6 in all of our experiments. Besides, we use a single NVIDIA RTX A6000 GPU with 48GB of memory to train all the models. We utilize the Pytorch pre-trained weights on ImageNet from timm library \cite{rw2019timm} for MaxViT backbone networks. We use the input resolutions and attention windows of $\{(224 \times 224), (256 \times 256)\}$ and $\{(8 \times 8), (7 \times 7)\}$, respectively, in our (dual-scale) MERIT. We augment data using only random rotation and flipping. We train our model using AdamW \cite{loshchilov2017decoupled} optimizer with a weight decay and learning rate of 0.0001. We optimize the combined DICE and Cross-Entropy (CE) loss $\mathcal{L}$ in Equation \ref{eq:loss} with $\lambda_1 = 0.7$ and $\lambda_2 = (1 - \lambda_1) = 0.3$ (weights are selected empirically in Appendix \ref{assec:loss_weight}) in all our experiments:

\begin{equation}\label{eq:loss}
\mathcal{L} = \lambda_1\mathcal{L}_{DICE} + \lambda_2\mathcal{L}_{CE}
\end{equation} 
where $\lambda_1$ and $\lambda_2$ are the weight for the DICE ($\mathcal{L}_{DICE}$) and CE ($\mathcal{L}_{CE}$) losses, respectively.

We train each model a maximum of 300 epochs with a batch size of 24 for Synapse multi-organ segmentation. For ACDC cardiac organ segmentation, we use a batch size of 12 and train each model for a maximum of 400 epochs.
%\textbf{Synapse Multi-organ Segmentation.}


%\textbf{ACDC Cardiac Organ Segmentation.}
 %We set the input resolution and patch size P as $224 \times 224$ and 16, respectively. Random flipping and rotation are applied for data augmentation.

%\textbf{Polyp Segmentation.}
%Following Polyp-PVT \cite{dong2021polyp}, we use a batch size of 16 and train each model maximum of 100 epochs. We resize the image to $352 \times 352$ and use a similar multi-scale \{0.75, 1.0, 1.25\} training strategy with a gradient clip limit of 0.5 as Polyp-PVT. We use the combined weighted IoU and weighted BCE loss function.

\begin{table*}[t]
\centering
\floatconts
{tab:compare_baseline_results}
{\caption{Comparison with the baseline method on Synapse multi-organ and ACDC datasets. We report the results of our MERIT with the setting MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION. We report the average inference time (ms) over 5000 samples. All reported \textcolor{black}{DICE scores (\%) in columns Synapse Multi-organ and ACDC} are averaged over five runs. The best results are in bold.}} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {%\small{
\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm} \begin{tabular}{lrrrrr}
\toprule
Architectures    &  Input Resolutions   &    \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Params (M)/\\FLOPS (G)  \end{tabular}} &   
  \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Inference \\Time (ms)  \end{tabular}} &   \begin{tabular}[c]{@{}c@{}}Synapse \\Multi-organ  \end{tabular} & ACDC \\
\midrule
MaxViT            &  $224\times224$        &  \textcolor{black}{65.25/10.43}	&  \textcolor{black}{19.79}  &  77.11  & 90.56 \\
MaxViT            &  $256\times256$        &  \textcolor{black}{65.25/14.19}	  & \textcolor{black}{20.58}  & 78.53  & 90.98  \\
\midrule
\textcolor{black}{MaxViT with CASCADE decoder}           &  \textcolor{black}{$224\times224$}       &  \textcolor{black}{82.62/14.2}	&  \textcolor{black}{21.84}  &  \textcolor{black}{79.83}  & \textcolor{black}{90.87} \\
\textcolor{black}{MaxViT with CASCADE decoder}            &  \textcolor{black}{$256\times256$}        &  \textcolor{black}{82.62/19.11}	  & \textcolor{black}{23.07}  & \textcolor{black}{80.20}  & \textcolor{black}{91.15}  \\
\midrule
Parallel MERIT \textcolor{black}{(\textbf{our})}          & $256\times256$, $224\times224$           &  \textcolor{black}{147.86/33.31}	&  \textcolor{black}{37.01}  & 84.22   & \textbf{92.32} \\
Cascaded MERIT \textcolor{black}{(\textbf{our})}          & $256\times256$, $224\times224$          & \textcolor{black}{147.86/33.31}	&  \textcolor{black}{37.06}  & \textbf{84.90}   & 91.85 \\
\bottomrule %\\
\end{tabular}}
\end{adjustbox}
}%}
\end{table*}


\section{Ablation Studies}
\label{asec:ablation_studies}
In this section, we present a wide range of ablation studies to answer different intrinsic questions related to our proposed architectures, loss aggregation, and experiments; these are described in the following subsections.

\subsection{Comparison with the baseline method}
\label{assec:baseline_compare}
%MERIT vs Double Same Scale Backbone
We compare our proposed methods with baseline hierarchical MaxViT architecture. In the case of MaxViT, we do the same multi-stage prediction for a fair comparison. We also use a similar experimental setting except using MUTATION with our architectures. Table \ref{tab:compare_baseline_results} presents the results of these experiments. We can see from Table \ref{tab:compare_baseline_results} that our proposed architectures with MUTATION loss \textcolor{black}{(see "our" entries in Table \ref{tab:compare_baseline_results})} improve the baseline \textcolor{black}{hierarchical $256\times256$} resolution MaxViT \textcolor{black}{(see $2^{nd}$ row entries in Table \ref{tab:compare_baseline_results})} by 6.37\% and 1.34\% DICE scores \textcolor{black}{(with $1.62\times$ more FLOPS and $1.8\times$ longer inference time)} in Synapse multi-organ and ACDC datasets, respectively. \textcolor{black}{We can also see that our MERIT architecture has 147.86M parameters which is $1.8\times$ larger than $256\times256$ resolution MaxViT with CASCADE decoder (see $4^{th}$ row entries in Table \ref{tab:compare_baseline_results}), but with a 4.7\% better DICE score in Synapse multi-organ. We think that this increase of parameters/FLOPS/inference time worth it given the improvement in performance.}

\begin{table*}[]
\centering
\floatconts
{tab:multi_scale_results}
{\caption{Effect of multi-scale backbone on Synapse multi-organ dataset. We report the results of the backbone with CASCADE \textcolor{black}{decoder} (no MUTATION) to clarify the effect of multi-scale backbones. All reported results are averaged over five runs. The best results are in bold.}} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {%\small{
\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm} \begin{tabular}{lrrrrrr}
\toprule
Architectures    &  Input Resolutions   & \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Attention\\Windows\end{tabular}}	 & \textcolor{black}{\begin{tabular}[c]{@{}c@{}}Params (M)/\\FLOPS (G)  \end{tabular}} & \begin{tabular}[c]{@{}c@{}}Avg \\DICE (\%) \end{tabular} \\
\midrule
\textcolor{black}{(single)} MaxViT            &  $224\times224$       & \textcolor{black}{$7\times7$}	& \textcolor{black}{82.62/14.2} &  79.83 \\
\textcolor{black}{(single)} MaxViT            &  $256\times256$       &  \textcolor{black}{$8\times8$}	 & \textcolor{black}{82.62/19.11}  & 80.20  \\
\midrule
Parallel Double MaxViT      & $224\times224$, $224\times224$     & \textcolor{black}{$7\times7$, $7\times7$}	& \textcolor{black}{147.86/28.4} & 80.81   \\
Parallel Double MaxViT      & $256\times256$, $256\times256$      &  \textcolor{black}{$8\times8$, $8\times8$}	& \textcolor{black}{147.86/38.22}  & 82.15  \\
Cascaded Double MaxViT      & $224\times224$, $224\times224$      &  \textcolor{black}{$7\times7$, $7\times7$}	& \textcolor{black}{147.86/28.4}  &  81.06 \\
Cascaded Double MaxViT      & $256\times256$, $256\times256$      &  \textcolor{black}{$8\times8$, $8\times8$}	& \textcolor{black}{147.86/38.22}  &  83.02 \\
\midrule
Parallel MERIT \textcolor{black}{(\textbf{our})}           & $256\times256$, $224\times224$           &  \textcolor{black}{$8\times8$, $7\times7$}    & \textcolor{black}{147.86/33.31}  & \textbf{82.91}   \\
Cascaded MERIT \textcolor{black}{(\textbf{our})}          & $256\times256$, $224\times224$         & \textcolor{black}{$8\times8$, $7\times7$}  & \textcolor{black}{147.86/33.31}  & \textbf{83.35}    \\
\bottomrule %\\
\end{tabular}}
\end{adjustbox}
}%}
\end{table*}


\begin{table*}[t]
\centering
\floatconts
{tab:tiny_vs_small_results}
{\caption{\textcolor{black}{Comparison of Tiny MERIT vs. Small MaxViT architectures on Synapse multi-organ dataset. We report the results of the backbone with CASCADE \textcolor{black}{decoder} (no MUTATION) to clarify the effect of multi-scale backbones. All reported results are averaged over five runs. The best results are in bold.}}} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {%\small{
\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm}\begin{tabular}{lrrrr}
\toprule
\textcolor{black}{Architectures}              & \textcolor{black}{Input Resolution} & \textcolor{black}{\begin{tabular}[c]{@{}l@{}}Attention \\ Windows\end{tabular}} & \textcolor{black}{\begin{tabular}[c]{@{}l@{}}Params (M)/\\ FLOPS (G)\end{tabular}} & \textcolor{black}{Avg DICE (\%)} \\
\midrule
\textcolor{black}{MaxViT-Tiny}                & \textcolor{black}{224$\times$224}          & \textcolor{black}{7$\times$7}                                                          & \textcolor{black}{36.86/6.57}                                                      & \textcolor{black}{77.84}     \\
\textcolor{black}{MaxViT-Tiny}                & \textcolor{black}{256$\times$256}          & \textcolor{black}{8$\times$8}                                                          & \textcolor{black}{36.86/8.61}                                                      & \textcolor{black}{78.43}     \\
\midrule
\textcolor{black}{Parallel MERIT-Tiny (ours)} & \textcolor{black}{256$\times$256, 224$\times$224} & \textcolor{black}{8$\times$8, 7$\times$7}                                                     & \textcolor{black}{65.41/15.18}                                                     & \textcolor{black}{\textbf{81.34}}     \\
\textcolor{black}{Cascaded MERIT-Tiny (ours)} & \textcolor{black}{256$\times$256, 224$\times$224} & \textcolor{black}{8$\times$8, 7$\times$7}                                                     & \textcolor{black}{65.41/15.18}                                                     & \textcolor{black}{\textbf{81.82}}     \\
\midrule
\textcolor{black}{MaxViT-Small}               & \textcolor{black}{224$\times$224}          & \textcolor{black}{7$\times$7}                                                          & \textcolor{black}{82.62/14.2}                                                      & \textcolor{black}{79.83}     \\
\textcolor{black}{MaxViT-Small}               & \textcolor{black}{256$\times$256}          & \textcolor{black}{8$\times$8}                                                          & \textcolor{black}{82.62/19.11}                                                     & \textcolor{black}{80.20} \\
\bottomrule
\end{tabular}}
\end{adjustbox}
}
\end{table*}

\subsection{Effect of multi-scale backbone}
\label{assec:effect_multi_scale}
%MERIT vs Double Same Scale Backbone
We have conducted experiments on the Synapse multi-organ dataset to show the effect of our multi-scale backbone on medical image segmentation. In Table \ref{tab:multi_scale_results}, we present the results of all the methods with the CASCADE decoder (no MUTATION) to make a fair comparison of our proposed architecture. It can be seen from Table \ref{tab:multi_scale_results} that the input resolution has an impact on DICE score improvement. More precisely, $256 \times 256$ resolution backbones have better DICE scores than the $224 \times 224$ resolution backbones. \textcolor{black}{As shown, our Cascaded MERIT achieves the best DICE score (83.35\%) which improves the baseline $256\times256$ resolution MaxViT (see $2^{nd}$ row entries in Table \ref{tab:multi_scale_results}) by 3.15\%. When comparing with the double backbone architectures with the same input scale (attention window), we can see that our multi-scale (attention window) double backbone architectures achieve better DICE scores due to their additive advantage of multi-scale feature extraction. We note that our Parallel/Cascaded MERIT (33.31G) has a significantly lower computational complexity/FLOPS than the $256\times256$ resolution Parallel/Cascaded Double MaxViT (38.22G) due to using one $256\times256$ and another $224\times224$ resolution inputs. Despite that, our Parallel and Cascaded MERIT outperform the Double MaxViT by 0.76\% and 0.33\%, respectively. These improvements in the DICE score support the claim regarding the benefit of calculating SA in multiple scale attention windows.}

 \textcolor{black}{We have conducted an additional set of experiments by implementing a tiny version of MERIT using the tiny MaxViT backbones, to clarify that performance improvement is due to the effect of multi-scale SA, not because of using a model with more parameters. As shown in Table \ref{tab:tiny_vs_small_results}, when comparing against the Small MaxViT backbone which has more model parameters, both of our Tiny MERIT backbones perform better. Our Tiny Cascaded MERIT backbone outperforms the Small MaxViT backbone (see $4^{th}$ row entries in Table \ref{tab:tiny_vs_small_results}) by up to 1.62\% DICE score for a 256$\times$256 input resolution, while having $1.26\times$ smaller model parameters and $1.26\times$ fewer FLOPS. Therefore, again, we can conclude from the empirical evaluation that our multi-scale SA calculation improves the performance of medical image segmentation.}

%Our Cascaded MERIT achieves the best DICE score (83.35\%) which improves the baseline $256\times 256$ resolution MaxViT (with CASCADE) by 3.15\%. When comparing the dual backbone architectures with the same input scale, we can see that our multi-scale dual backbone architectures achieve better DICE scores due to their additive advantage of multi-scale feature extraction. We note that the $256\times256$ resolution Cascaded Double MaxViT has significantly more computational complexity than Cascaded MERIT due to using one $256\times256$ and another $224 \times 224$ resolution input. Despite that, our Parallel and Cascaded MERIT outperform respective Double MaxViT by 0.76\% and 0.33\%.
\begin{table*}[t]
\centering
\floatconts
{tab:cascade_mutation_effects_results}
{\caption{Effect of CASCADE \textcolor{black}{decoder} and MUTATION loss aggregation in MERIT on Synapse multi-organ dataset. We present the results of MERIT averaging over five runs. The best results are in bold.}} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {%\small{
%\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm}\begin{tabular}{lrrrr}
\toprule
Architectures    &  CASCADE \textcolor{black}{decoder}   &  MUTATION     & \multicolumn{1}{l}{Avg DICE (\%)} \\
\midrule
Parallel MERIT            &  No         & No    & 80.44  \\
Parallel MERIT           & No         & Yes                   & 81.06   \\
Parallel MERIT            &  Yes         & No            & 82.91  \\
Parallel MERIT \textcolor{black}{(\textbf{our})}          & Yes       & Yes                     & 84.22   \\
\midrule
Cascaded MERIT            &  No         & No    & 80.76  \\
Cascaded MERIT           & No         & Yes                   & 82.03   \\
Cascaded MERIT            &  Yes         & No            & 83.35  \\
Cascaded MERIT \textcolor{black}{(\textbf{our})}           & Yes       & Yes                     & \textbf{84.90}    \\
\bottomrule %\\
\end{tabular}}
%\end{adjustbox}
}%}
\end{table*}

\begin{figure}[t]%[htbp]
% % Caption and label go in the first argument and the figure contents
% % go in the second argument
\floatconts
  {fig:qualitative_results}
  {\caption{\textcolor{black}{Qualitative results on Synapse multi-organ dataset. (a) Ground Truth (GT), (b) MaxViT (input resolution 224$\times$224), (c) MaxViT (input resolution 256$\times$256), (d) Parallel MERIT, (e) Cascaded MERIT. We produce all the segmentation maps with the CASCADE decoder and overlay on top of original image/slice.}}}
  {\includegraphics[width=1\linewidth]{images/qualitative_results.png}}
\end{figure}

\subsection{Effect of CASCADE \textcolor{black}{decoder} and MUTATION loss aggregation in MERIT}
\label{assec:effect_cascade_mutation}
We have conducted some experiments on Synapse multi-organ dataset to demonstrate the effect of CASCADE \textcolor{black}{decoder} and MUTATION loss aggregation strategy on our MERIT architectures. Table \ref{tab:cascade_mutation_effects_results} presents the results of our Parallel MERIT with or without CASCADE \textcolor{black}{decoder} and MUTATION. We can see from Table \ref{tab:cascade_mutation_effects_results} that Parallel and Cascaded MERIT without both CASCADE \textcolor{black}{decoder} and MUTATION have the lowest DICE scores. CASCADE \textcolor{black}{decoder} significantly increases the DICE scores (2.47-2.59\%) \textcolor{black}{due to capturing the spatial (contextual) relations among pixels (usually limited in vision transformer)}, while MUTATION alone marginally improves the DICE (0.62-1.27\%). However, when MUTATION is used with the outputs from CASCADE \textcolor{black}{decoder}, it achieves the best DICE scores (84.22\%, 84.90\%) improving CASCADE \textcolor{black}{decoder} by 1.31-1.55\%. We believe the reason behind this is that MUTATION works well with the refined features of the CASCADE \textcolor{black}{decoder}. \textcolor{black}{Therefore, we can conclude that the synthesized prediction maps generated via combinatory aggregation (MUTATION) help us improve the performance of the model; this is why we prefer combinatory loss aggregation over linear aggregation. We believe that our combinatory loss aggregation (MUTATION) can be used as a beneficial ensembling/augmentation method in other downstream semantic and medical image segmentation tasks.}

\begin{table*}[t]
\centering
\floatconts
{tab:cascade_aggregation_results}
{\caption{Comparison of different aggregations in CASCADE \textcolor{black}{decoder} on Synapse multi-organ dataset. We present the results of MERIT averaging over five runs with the setting MERIT+CASCADE \textcolor{black}{decoder}+MUTATION. The best results are in bold.}} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {%\small{
%\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm} \begin{tabular}{lrrrr}
\toprule
Architectures    &  Aggregation in CASCADE \textcolor{black}{decoder}   &   \multicolumn{1}{l}{Avg DICE (\%)} \\
\midrule
Parallel MERIT            & Concatenation            & 84.18  \\
Parallel MERIT           & Concatenation                            & 84.22   \\
\midrule 
Cascaded MERIT         & Additive                    & 84.88      \\
Cascaded MERIT        & Additive                     & \textbf{84.90}   \\
\bottomrule %\\
\end{tabular}}
%\end{adjustbox}
}%}
\end{table*}

\subsection{\textcolor{black}{Qualitative results on Synapse Multi-organ Segmentation}}
\label{assec:qualitative_results}
\textcolor{black}{Fig. \ref{fig:qualitative_results} shows the qualitative results of the baseline hierarchical MaxViT and our proposed MERIT architectures. As shown in the figure, our MERIT architecture can segment the small organs (see the red rectangular box) well. In contrast, the single scale MaxViT architecture with both $224\times224$ and $256\times256$ input resolutions fail to segment that small organ. Our MERIT architecture also segments the larger organ much better than the single scale MaxViT. We believe the reason behind this better segmentation of both small and large organs by our MERIT architectures is the use of multi-scale SA. 
}


\subsection{Effect of different aggregations in CASCADE \textcolor{black}{decoder}} 
\label{assec:effect_aggregation_cascade}
%Concatenation vs Additive

Table \ref{tab:cascade_aggregation_results} presents the results of concatenation and additive aggregations in CASCADE \textcolor{black}{decoder} on Synapse multi-organ dataset. We can see from Table \ref{tab:cascade_aggregation_results} that our MERIT architectures with additive aggregation in CASCADE \textcolor{black}{decoder} are marginally better (0.04-0.02\%) than the concatenation. Therefore, we can conclude from these results that the aggregation techniques do not have much impact on the CASCADE decoder of our architectures while using MUTATION. However, concatenation aggregation-based methods usually have additional computational overheads due to increasing the number of channels after the aggregation, while additive aggregation keeps the number of channels the same. Consequently, we recommend using additive aggregation in our MERIT architectures due to its computational benefits.



\begin{table*}[t]
\centering
\floatconts
{tab:interpolations_results}
{\caption{Comparison of different interpolations in MERIT on Synapse multi-organ dataset. We present the results of MERIT averaging over five runs with the setting MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION. The best results are in bold.}} % R50+UNet and R50+AttnUNet adopted a pre-trained ResNet50 backbone network.  
    {%\small{
%\begin{adjustbox}{width=1\textwidth}
{\vspace{-0.4cm}\begin{tabular}{lrrrr}
\toprule
Architectures    &  Interpolations    &   \multicolumn{1}{l}{Avg DICE (\%)} \\
\midrule
Parallel MERIT            &  nearest-exact            & 81.67  \\
Parallel MERIT           & area                            & 81.76   \\
Parallel MERIT            &  bicubic            & 83.58  \\
Parallel MERIT           & bilinear                            & 84.22   \\
\midrule
Cascaded MERIT         & nearest-exact                    & 82.27      \\
Cascaded MERIT        & area                     & 82.38   \\
Cascaded MERIT         & bicubic                    & 84.05      \\
Cascaded MERIT        & bilinear                     & \textbf{84.90}   \\
\bottomrule %\\
\end{tabular}}
%\end{adjustbox}
}%}
\end{table*}

\begin{figure}[t]%[htbp]
% % Caption and label go in the first argument and the figure contents
% % go in the second argument
\floatconts
  {fig:loss_curve}
  {\vspace{-0.7cm} \caption{Loss weight vs. DICE curve on Synapse multi-organ dataset. We report the results of our Cascaded MERIT+CASCADE \textcolor{black}{decoder}(Additive)+MUTATION with different weights for DICE and CE losses. X-axis presents the weights of DICE loss, $\lambda_1$, while the weight for CE loss is $\lambda_2=1-\lambda_1$. The value of 0.0 on the X-axis represents weights for DICE and CE losses of 0.0 and 1.0, respectively (i.e., only CE loss is used). While the value of 1.0 on the X-axis represents weights for DICE and CE losses of 1.0 and 0.0, respectively (i.e., only DICE loss is used)}}
  {\includegraphics[width=0.7\linewidth]{images/loss_curve.png}}
\end{figure}


\subsection{Effect of different interpolations in MERIT}
\label{assec:effect_interpolations}
%('nearest' | 'bilinear' | 'bicubic' | 'area' | 'nearest-exact')
 We have conducted some experiments on Synapse multi-organ dataset to choose the best interpolations methods for our proposed MERIT architectures. Table \ref{tab:interpolations_results} presents the results of Parallel and Cascaded MERIT using \textit{nearest-exact} (nearest neighbor), \textit{area}, \textit{bicubic}, and \textit{bilinear} interpolation methods from Pytorch. The \textit{nearest-exact} interpolation shows the lowest DICE scores while \textit{bilinear} and \textit{bicubic} interpolation achieve the best and second best DICE scores, respectively. Therefore, we recommend using \textit{bilinear} interpolation in our proposed MERIT architectures to re-scale the features and prediction maps. 

\subsection{Choosing weight for DICE and CE losses}
\label{assec:loss_weight}
We optimize the combined DICE and CE loss during the training of our models. Here, we have conducted some experiments to choose the best weight pairs to combine these two losses. Fig. \ref{fig:loss_curve} presents the DICE scores for different weight pairs for losses. We can see in the graph that the model shows the worst DICE score when using only the CE loss. We get the best DICE score for the weights pair ($\lambda_1$, $\lambda_2$) = (0.7, 0.3) which we have used in all of our experiments.   

%\section{Additional Results}
%\subsection{Polyp segmentation}

\section{Supplementary Materials}
We make our source code publicly available at $https://github.com/SLDGroup/MERIT$.
%and trained weights
% with CASCADE vs W/O CASCADE


%This is a boring technical proof of
%\begin{equation}\label{eq:example}
%\cos^2\theta + \sin^2\theta \equiv 1.
%\end{equation}

%\section{Proof of Theorem 2}

%This is a complete version of a proof sketched in the main text.

\end{document}
