%-------------------------------------------------------------------------
\section{Experiments}
\textbf{Datasets:}
RAF-DB \cite{li2019reliable}
is one of the most widely used large-scale real-world FER database.
The single-label subset (six basic expression and neutral), consisting of 12,771 training images and 3,068 test images, is used in the experiments.
The overall accuracy is reported on the test set.
FERPlus \cite{BarsoumICMI2016}
extends from FER 2013 \cite{goodfellow2013challenges} and contains eight expression categories (six basic expressions, plus neutral and contempt).
It consists of 28,709 training, 3,589 validation, and 3,589 test images.
The overall accuracy is reported on the test set.
SFEW \cite{dhall2011static}
dataset is created by selecting static frames from the AFEW database.
SFEW 2.0 is the most commonly used version and contains 958 training samples, 436 validation samples, and 372 testing samples.
Each image is assigned to one of seven expression categories, including neutral and the six basic expressions.
The results on validation sets is reported because the labels of the testing set are not released.

\textbf{Implementation details:}
For RAF-DB and SFEW 2.0 datasets, original images are used in all experiments.
For FERPlus dataset, we use MTCNN \cite{zhang2016joint} to detect, align, and crop images.
During training, data augmentation is employed on-the-fly and includes random grayscale, random horizontal flip, and random erasing to avoid over-fitting.
At test time, no data augmentation is used.
All images are resized to $112 \times 112$ pixels before they are fed into the network.
Our model is optimized via the Stochastic Gradient Descent (SGD) optimizer with a mini-batch size of 16.
The learning rate is initialized to 0.0001 and is decayed by a factor of 10 every 30 epochs.
Our experiment is conducted on a GeForce RTX2060 GPU, with PyTorch 1.7.1, Python 3.8.13, and Windows 10.

For RAF-DB and FERPlus datasets, the pre-trained IR-50 \cite{deng2019arcface} on Ms-Celeb-1M \cite{guo2016ms} is adopted as a feature extractor, which is consistent with TranFER \cite{xue2021transfer}.
For SFEW 2.0 dataset, we pre-train FER-former on RAF-DB dataset and then fine-tune it on SFEW 2.0 dataset, which is consistent with IPD-FER \cite{jiang2022disentangling}.
Regarding our scratch-trained Transformer encoder, the depth is 16, the embedding dimension is 256, the number of heads is 4, and the mlp ratio is 4.

\subsection{Comparison with state-of-the-art methods}
We compare our results to several state-of-the-art methods on RAF-DB, FERPlus, and SFEW 2.0 datasets.

\textbf{Comparison on RAF-DB dataset}  between our result and several state-of-the-art methods is shown in Table \ref{tab1}.
RAF-DB dataset is one of the most widely used large-scale real-world FER datasets because it facilitates fair comparisons, in which all images are cropped and do not require any additional preprocessing.
Results show that our FaceFormer achieves state-of-the-art performance compared to all other methods, including FER with unconstrained variations (RAN \cite{wang2020region}, MA-Net \cite{zhao2021learning}, IPD-FER \cite{jiang2022disentangling}), and FER with annotation ambiguity (SCN \cite{wang2020suppressing}, DMUE \cite{she2021dive}, KTN \cite{li2021adaptively}, EfficientFace \cite{zhao2021robust}, SPLDL\cite{shao2022self}, EASE\cite{wang2022ease}).
In particular, when compared to TransFER \cite{xue2021transfer}, the previous best achieved by combining CNN and ViT, FER-former lowers the error rate from 9.09\% to 8.7\%, a 4.3\% improvement.
\begin{table}[h]
	\setlength{\belowcaptionskip}{0.2cm}
	\renewcommand\arraystretch{0.8}
	\caption{Performance comparison with the state-of-the-art methods on RAF-DB dataset.}
	\label{tab1}
	\centering
    \setlength{\tabcolsep}{5.5mm}{
	\begin{tabular}{ccc}
		\toprule
		Methods     &  Years &  Acc.(\%)   \\
		\midrule
		RAN \cite{wang2020region} &  2020  &  86.90  \\
		SCN \cite{wang2020suppressing}&   2020 &  88.14 \\
		DLN \cite{zhang2021learning}&  2021 &  86.40  \\
		%DACL \cite{farzaneh2021facial}& 2021 &  87.78 \\
		KTN \cite{li2021adaptively}&  2021 &  88.07 \\
		MA-Net \cite{zhao2021learning}&  2021 &  88.40 \\
		DMUE \cite{she2021dive}&  2021 &  89.42 \\
		EfficientFace \cite{zhao2021robust}& 2021 & 88.36 \\
		TransFER \cite{xue2021transfer}&  2021  & 90.91 \\
		%HRL \cite{han2022devil}&  2022 & 87.77 \\
		IPD-FER \cite{jiang2022disentangling}&  2022 & 88.89 \\
		CRS-CONT \cite{li2022crs} &  2022 & 88.07 \\
		SPLDL\cite{shao2022self} & 2022 & 89.08 \\
		EASE\cite{wang2022ease} & 2022 & 89.56 \\
		FER-former (Ours)&   2023     &    \textbf{91.30}     \\
		\bottomrule
	\end{tabular}}
\end{table}

\textbf{Comparison on FERPlus dataset}  is shown in Table \ref{tab2}.
It can been seen that our FER-former achieved best FER performance compared to other methods, including FER with unconstrained variations (RAN \cite{wang2020region}, IPD-FER \cite{jiang2022disentangling}), and FER with annotation ambiguity (SCN \cite{wang2020suppressing}, DMUE \cite{she2021dive}, KTN \cite{li2021adaptively}, EASE \cite{wang2022ease}).
In particular, FER-former still shows superiority over previous best results reported in TransFER \cite{xue2021transfer}.
\begin{table}[h]
	\setlength{\belowcaptionskip}{0.2cm}
	\renewcommand\arraystretch{0.8}
	\caption{Performance comparison with the state-of-the-art methods on FERPlus dataset.}
	\label{tab2}
	\centering
    \setlength{\tabcolsep}{5.5mm}{
	\begin{tabular}{ccc}
		\toprule
		Methods     & Years &  Acc.(\%) \\
		\midrule
		RAN \cite{wang2020region}& 2020  &  88.55  \\
		SCN \cite{wang2020suppressing}& 2020  & 89.35 \\
		DMUE \cite{she2021dive}&  2021   & 89.51 \\
		KTN \cite{li2021adaptively}&2021 & 90.49   \\
		TransFER \cite{xue2021transfer}& 2021 & 90.83 \\
		IPD-FER \cite{jiang2022disentangling}& 2022 & 88.42 \\
		EASE \cite{wang2022ease}& 2022 & 90.26 \\
		FER-former (Ours) & 2023 & \textbf{90.96} \\
		\bottomrule
	\end{tabular}}
\end{table}

\textbf{Comparison on SFEW 2.0 dataset} is shown in Table \ref{tab3}.
Due to the limited images, we pre-trained FER-former on RAF-DB dataset and fine-tune it on SFEW 2.0 dataset because they have the same expression categories, which is in consistent with IPD-FER \cite{jiang2022disentangling}.
It can be observed that CRS-CONT \cite{li2022crs}, EASE \cite{wang2022ease} and FER-former are the only three models to achieve accuracy over 60\% on this dataset.
FER-former improves FER accuracy by 2.06\% compared to EASE \cite{wang2022ease}, achieving state-of-the-art performance.
\begin{table}[h]
	\setlength{\belowcaptionskip}{0.2cm}
	\renewcommand\arraystretch{0.8}
	\caption{Performance comparison with the state-of-the-art methods on SFEW 2.0 dataset.}
	\label{tab3}
	\centering
    \setlength{\tabcolsep}{5.5mm}{
	\begin{tabular}{ccc}
		\toprule
		Methods    & Years  & Acc.(\%)  \\
		\midrule
		Incept-ResV1 \cite{acharya2018covariance} & 2018 & 51.90 \\
		Island loss \cite{cai2018island} & 2018 & 52.52 \\
		LTNet \cite{zeng2018facial} & 2018 & 58.29 \\
		RAN \cite{wang2020region}     & 2020 & 56.40 \\
		MA-Net \cite{zhao2021learning}     &  2021  & 59.40 \\
		DMUE \cite{she2021dive}     &  2021   & 58.34 \\
		IPD-FER \cite{jiang2022disentangling} & 2022 & 58.43 \\
		CRS-CONT \cite{li2022crs} & 2022 & 60.09 \\
		EASE \cite{wang2022ease}& 2022 & 60.12 \\
		FER-former (Ours)     &   2023  &  \textbf{62.18} \\
		\bottomrule
	\end{tabular}}
	\label{tab:table}
\end{table}


\subsection{Confusion matrices}
The confusion matrices of our FER-former on the three datasets are shown in Fig \ref{fig3}.
It can be observed that the FER accuracy of ``fear", ``disgust" and ``contempt" is noticeably lower than that of other expressions.
In particular, ``fear" is likely to confused with ``surprise" and ``sad" with ``neutral" on all three datasets, indicating the proximity of these expressions in the semantic space.
The accuracy of ``happy" and ``neutral" are higher than that of other expressions, suggesting that they are less ambiguous and easier to distinguish from other expressions.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.1465\textwidth]{./RAF-DB_1.pdf}
    \includegraphics[width=0.1465\textwidth]{./FERPlus_1.pdf}
    \includegraphics[width=0.175\textwidth]{./SFEW.pdf}
	\caption{Confusion matrices of FER-former on RAF-DB, FERPlus, and SFEW 2.0 datasets, where y-axis and x-axis represent ground-truth and predicted labels, respectively (Su: surprise, Fe: fear, Di: disgust, Ha: happy, Sa: sad, An: anger, Ne: neutral, Co: contempt).}
	\label{fig3}
\end{figure}

To analyze when FER-former mislabels impressions, selected images from different confusion categories are shown in Fig \ref{fig5}.
It can be observed that ambiguous samples are widespread in all expression categories. %, including ambiguous facial expressions and low-quality facial images.
Close inspection on the highlighted mislabelled samples shows that the labels produced by FER-former are sometime arguably more convincing than the ground truth.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{./wrong_samples.png}
	\caption{Some mislabelled examples on RAF-DB test set, where y-axis and x-axis represent ground-truth and predicted labels, respectively. The empty spaces indicate that there is no mislabelled sample in the corresponding categories. The samples highlighted by red boxes are those that we consider to be highly ambiguous.}
	\label{fig5}
\end{figure}

\subsection{Ablation study}
\textbf{Evaluation of MGEI, HDSS on RAF-DB and FERPlus datasets.}
To validate the effectiveness of MGEI and HDSS modules in our FER-former, an ablation study was conducted on RAF-DB and FERPlus datasets, as shown in Table \ref{tab:table4}.
The baseline in the first row does not use MGEI nor HDSS.
The extracted features from stem CNN are sliced with local size $2 \times 2 $ along spatial dimension.
Compared to the baseline, the accuracy is improved by 0.65\% and 0.32\% on the two datasets when adding MGEI module and by 1.01\% and 0.64\% when adding HDSS. The combination of MGEI and HDSS can noticeably boost the performance of FER in the wild and achieves the best results.
\begin{table}[h]
	\setlength{\belowcaptionskip}{0.2cm}
	\renewcommand\arraystretch{0.8}
	\caption{Evaluation (\%) of MGEI, HDSS on RAF-DB and FERPlus datasets.}
	\centering
    \setlength{\tabcolsep}{4.5mm}{
	\begin{tabular}{cc|cc}
		\toprule
		MGEI    &  HDSS  &  RAF-DB  & FERPlus \\
		\midrule
		&    &  89.80   &  90.07    \\
		$\surd$ &    &  90.45  &   90.39    \\
		& $\surd$  &  90.81  &  90.71   \\
		$\surd$ & $\surd$  &  \textbf{91.30}  &  \textbf{90.96}   \\
		\bottomrule
	\end{tabular}}
	\label{tab:table4}
\end{table}

To more intuitively illustrate the effectiveness of MGEI and HDSS modules, the high dimensional image features corresponding to the steering token from the last encoder is visualized by t-SNE \cite{van2008visualizing}.
As shown in Fig \ref{fig4}, both MGEI and HDSS are effective in maximizing the margins between expression classes.
With both, FER-former can effectively separate ``angry" and ``disgust" from other expressions.
In addition, we can observe that ``fear" is always close to ``surprise" and ``sad" is always close to ``neutral" in high dimension space, which aligns well with the above observations.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{./tsne.png}
	\caption{Visualization (t-SNE \cite{van2008visualizing}) of high dimension features generated under different settings on the RAF-DB dataset. Each colour-coded dot represents a test image with the corresponding labelled expression.} %It can be observed that FER-former is capable of increasing inter-class distance, as well as decreasing inner-class distance.}
	\label{fig4}
\end{figure}

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.45\textwidth]{./img/anotation_ambiguous.png}
%	\caption{Annotation ambiguous-robust FER. Example results are shown on the RAF-DB test set. The labels above each expression indicate the prediction results of FER-former (red) and MGEI (with text supervision removed) , respectively.}
%	\label{fig4}
%\end{figure}
\textbf{Evaluation of different text contents on RAF-DB and FERPlus datasets.}
Text content is introduced to mitigate annotation ambiguity by calculating cosine similarity between image features and text features, which plays an important role in our proposed FER-former.
In order to evaluate the robustness of FER-former to different text contents, comparative experiments of five cases were conducted, including case 1: without using text, case 2: using expression name as a single word, case 3: using a phrase in the format of ``a face image of \{\}'', case 4: using a full active sentence ``this is a face image of \{\}'', and case 5: using a passive sentence ``a/an \{\} expression is shown in the image''.
The detailed results are reported in Table \ref{tab:table5}.

\begin{table}[h]
	\caption{Evaluation of different text contents on RAF-DB and FERPlus datasets.}
	\centering
	\setlength{\belowcaptionskip}{0.2cm}
	\renewcommand\arraystretch{0.8}
    \setlength{\tabcolsep}{5.5mm}{
		\begin{tabular}{clc}
			\toprule
			Datasets  & Texts contents  &  Acc.(\%) \\
			\midrule
			\multirow{5}*{RAFDB} & without text   &  90.45   \\
			~& single word   &   90.81   \\
			~& phrase   &   \textbf{91.30}   \\
			~& active sentence   &    91.13  \\
			~& passive sentence   &    90.97  \\
			\midrule
			\multirow{5}*{FERPlus} & without text   &  90.39   \\
			~& single word   &  90.61    \\
			~& phrase   &   90.90   \\
			~& active sentence   &    \textbf{90.96}  \\
			~& passive sentence   &  90.87   \\
			\bottomrule
	\end{tabular}}
	\label{tab:table5}
\end{table}
On RAF-DB dataset, the accuracy of four cases with text supervision surpasses that of without text, which shows the positive effects of text contents.
In addition, single word performs worst among the several text cases, and active sentence outperforms passive sentence.
Our hypothesis is that single expression words lack context whereas the passive sentences are too complicated.
The results indicate that contextual semantics play an important role in the text encoder.
More importantly, the performance of the phrase achieves state-of-the-art results with a 0.85\% improvement compared to case 1, which shows the superiority of our FER-former.
For FERPlus dataset, most of the conclusions are consistent with the above.
It is worth emphasizing that the active sentence rather than the phrase performs best on FERPlus dataset.
We consider that both active sentence and phrase are capable of achieving state-of-the-art results, and the difference in results depends on different model initialization parameters.

\textbf{Evaluation of patch size on RAF-DB and FERPlus datasets.}
To enable our ER-former to overcome the issues of occlusion and pose variances, the extracted features are split into sequences of patches with varied resolutions along spatial dimension.
The above results have shown that MGEI has a great impact on the performance of FER.
Here we aim to investigate the impact of local patches of different resolutions, as shown in Table \ref{tab:table6}.
It can be seen that each single division method behaves differently on different datastes.
For example, $4*4$ performs better on RAF-DB dataset and $2*2$ performs better on FERPlus dataset.
We believe this is due to the huge differences between the original images of different datasets.
Therefore, we propose to integrate multi-granularity embeddings to achieve resolution-robust feature division and occlusion and pose-robust feature learning.
The current results show that the integration of (2,4,6,12) can perform best on the both datasets, further indicating the positive effect of MGEI.

\begin{table}[h]
	\setlength{\belowcaptionskip}{0.2cm}
	\renewcommand\arraystretch{0.8}
	\caption{Evaluation of patch resolution on RAF-DB and FERPlus datasets.}
	\centering
	\setlength{\tabcolsep}{1mm}{
		\begin{tabular}{ccccccc}
			\toprule
			Datasets & 2*2 & 3*3 & 4*4 & 6*6 & (2,4,6) & (2,4,6,12)\\
			\midrule
			RAFDB &  90.81  &  90.38  &  90.91   &  90.48   &   90.81    &   \textbf{91.30}  \\
			FERPlus &  90.71  &  90.39  &  90.10   &  90.42  &   90.45   &   \textbf{90.96}  \\
			\bottomrule
	\end{tabular}}
	\label{tab:table6}
\end{table}
