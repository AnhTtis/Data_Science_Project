%-------------------------------------------------------------------------
\section{Methods}

\begin{figure*}
	\centering
	\includegraphics[width=1\textwidth]{./System_v1.png}
	\caption{Pipeline of the proposed model. It consists of multi-granularity embedding integration (MGEI), hybrid self-attention, and heterogeneous domains-steering supervision (HDSS). The diverse global receptive fields with multi-modal semantic cues are captured by integrating text semantics into multi-head self-attention (MHSA). For simplicity, the convolutional position encodings are omitted.}
	\label{fig1}
\end{figure*}

\subsection{Overview}
An overview of the model is depicted in Fig \ref{fig1}. The proposed FER-former consists of multi-granularity embedding integration (MGEI), hybrid self-attention, and heterogeneous domains-steering supervision (HDSS) modules.
Same as the previous study in \cite{xue2021transfer}, the IR-50 \cite{deng2019arcface} pre-trained on Ms-Celeb-1M \cite{guo2016ms}) is adopted as a feature extractor due to its good generalization.
The pre-trained text encoder in CLIP \cite{radford2021learning} is fine-tuned to perceive the rich relationship among different text labels.

A scheme of downsampling the crude features from pre-trained CNN stem is devised to disentangle diverse spatial cues, yielding one global patch and 49 local patches of different scales. Then, all the patches are flattened and projected as multi-scale tokens.
%Unlike standard transformer, the position embedding is not necessary in our FER-former.
A novel FER-specific transformer encoder block containing a hybrid self-attention scheme is devised to characterize heterogeneous supervision signals.
More importantly, the extracted text features from the text encoder is delicately used to realize heterogeneous domains-steering
supervision by calculating image-text similarity.
More details are illustrated as follows.

\subsection{Multi-granularity embedding integration (MGEI)}
As described earlier, ViT does not generalize well when trained on insufficient amounts of data.
Therefore, to make full use of the merits of CNNs and ViT, a pre-trained stem network is utilized as a feature extractor, obtaining deep features $ X \in \mathbb{R} ^{C\times H\times W}$  where $C$, $H$ and $W$ refer to the number of channels, height, and width.
To facilitate splitting the extracted features into sequences of patches with various resolutions, the extracted features $X$ are pooled into $ X^{'} \in \mathbb{R} ^{C\times 12\times 12}$.
The pooled features are then downsampled to obtain diverse spatial cues that enable the model to overcome the issues of occlusion and pose variances.
Let $ X_{p_{i}} \in \mathbb{R} ^{N_{i}\times C \times P_{i}^{2}}, i\in (1,4)$ denote the local patches of different resolutions, where $N_{i}$ represents the patch numbers of each resolution, $(P_{i},P_{i})$ is the resolution of each feature patch.
To satisfy standard Transformer Encoder that receives a $1D$ sequence of token embeddings as input, all the patches are flattened and projected to uniform embeddings, which is denoted as $ X_{p} \in \mathbb{R} ^{N \times D}$, where $N = \sum_{i=1}^{4} N_{i}$ is the number of all patches and $D$ is the length of each feature embedding.

\subsection{Hybrid self-attention}
The standard transformer encoder consists of a stack of $M$ encoder blocks, and each block contains alternating layers of multiheaded self-attention and multi-layer perception.
As a key to ViT's success, the self-attention makes it possible to embed information globally across the whole image.
As shown in Fig \ref{fig2}, we propose a novel hybrid self-attention that can characterize heterogeneous hard one-hot label-focusing and text-oriented classification tokens, where the text semantic information is passed to class token and other patch tokens by steering token in each encoder block.

Given a sequence of $1D$ feature embeddings $X_{p} \in \mathbb{R} ^{N \times D}$ and a class token $X_{c} \in \mathbb{R} ^{1 \times D}$, the routine input of an encoder block can be denoted as $Z^{'} = [X_{c}; X_{p}^{1}; X_{p}^{2}; \ldots; X_{p}^{N}]$.
To integrate image features with text features, a hybrid self-attention mechanism is designed.
Specifically, a steering token $X_{s} \in \mathbb{R} ^{1 \times D}$ is defined, yielding a new input for our FER-specific transformer encoder, which we denote as $Z = [X_{c}; X_{s}; X_{p}^{1}; X_{p}^{2}; \ldots; X_{p}^{N}]$.
For simplicity, we rename it as $Z = [a^{1}; a^{2}; \ldots; a^{n}]$, where $n = N + 2$. The steering token $a^{2}$ is connected with all other embeddings by calculating self-attention in each encoder block.
Firstly, the input $Z$ is linearly transformed to queries $q$, keys $k$, and values $v$ as follows:
\begin{equation}
	[q,k,v] = Z[W_{q},W_{k},W_{v}]
\end{equation}
where $W_{q},W_{k}\in \mathbb{R}^{D\times d_{q}},W_{v}\in \mathbb{R}^{D\times d_{v}}$. Then, the output of each encoder block corresponding to the steering token is calculated by:

\begin{equation}
	b^{2} = \sum_{i=1}^{n}a_{2,i}^{'}v^{i}
\end{equation}
where the attention weights are updated by softmax operation, $a_{2,i}^{'} = softmax(q^{2}\bullet k^{i}/\sqrt{d_{q}})$, with $\bullet$ referring dot product operation.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{./Steering_attention}
	\caption{Flowchart of hybrid self-attention scheme. The last layer's class token is directly utilized for ground truth supervision, while the newly defined steering token is utilized for the integration of image and text information by calculating their similarity.}
	\label{fig2}
\end{figure}

\subsection{Heterogeneous domains-steering supervision (HDSS)}
The hard one-hot label leads to homogenous supervisory signals that hinder the further improvement of FER performance.
Recently, the progress in CLIP \cite{radford2021learning} allows the thriving development of multi-modal models (image and text) in many fields \cite{conde2021clip, wang2022clip} and inspires our work.
Our main motivation is to make image features also have text-space semantic correlations by supervising the similarity between image features and text features, thereby easing the issue of annotation ambiguity.
In this section, we describe in detail how to realize heterogeneous domains-steering supervision by calculating image-text similarity.
The pipeline of HDSS is shown in Fig \ref{fig1}.

Given a dataset contains $M$ expressions, a vector of text label can be generated from one-hot labels, denoted as $t = \{t_{1}; t_{2}; \ldots; t_{M}\}$.
For example, the text label for RAF-DB dataset can be denoted as: "this is a face image of \{expression\}", where \{expression\} belongs to [surprise; fear; disgust; happy; sad; angry; neutral].
The image feature corresponding to steering token is denoted as $I_{S} \in \mathbb{R}^{1\times D}$, and the text features from text encoder are denoted as $T = \{T_{1}; T_{2}; \ldots; T_{M}\}$, $T \in \mathbb{R}^{M\times D}$.
Then, cosine similarity between an image feature $I_{S}$ and a text features $T$ is calculated by $S_{i-t} = I_{S}\times T^{\mathrm{T}}$, where $S_{i-t} \in \mathbb{R}^{1\times M}$. The result after softmax operation is directly used for calculating cross-entropy loss.
\begin{equation}
	P_{S_{i-t}^{i}} = \frac{\exp(S_{i-t}^{i})}{\sum_{j}\exp(S_{i-t}^{j})},
	\mathcal{L}_{text} = - \sum_{i}y_{i}log(P_{S_{i-t}^{i}})
\end{equation}

In addition, the image feature corresponding to class token is denoted as $I_{C} \in \mathbb{R}^{1\times D}$, which is firstly linearly projected to $S_{i-l} \in \mathbb{R}^{1\times M}$ by a fully-connected layer: $S_{i-l} = proj(I_{C})$. The result after softmax operation is also used for calculating another cross-entropy loss.
\begin{equation}
	P_{S_{i-l}^{i}} = \frac{\exp(S_{i-l}^{i})}{\sum_{j}\exp(S_{i-l}^{j})},
	\mathcal{L}_{image} = - \sum_{i}y_{i}log(P_{S_{i-l}^{i}})
\end{equation}
where $y_{i}$ is the ground truth.

Finally, the joint loss function is defined as $\mathcal{L} = \mathcal{L}_{text} + \mathcal{L}_{image} \label{equat1}$.
