\section{Introduction}
Recently, Facial Expression Recognition (FER) has carved a promising research path in computer vision toward fine cognition for human facial expressions as they are one of most natural and effective carriers to express emotions.
Impressive performance \cite{li2022dual,yang2020novel} has been attained on strictly lab-controlled datasets (e.g., MMI \cite{valstar2010induced}, CK+ \cite{lucey2010extended}, and Oulu-CASIA \cite{zhao2011facial}), where all samples were produced and annotated in a heuristically-defined manner with low annotation error. However, the transferability of these models to scenes in the wild have large room for improvements.
Compared to FER in controlled environments, FER in the wild is more challenging due to the issues of limited samples, annotation ambiguity, and unconstrained variations (occlusion, pose, illumination, etc.).
Yet, the FER's capability of adapting for natural scenarios is of paramount importance for the prospective generalization/deployment of FER models in practical applications.

Convolutional Neural Networks (CNNs) have achieved roaring success for various vision problems such as semantic segmentation \cite{zhou2022contextual}, object detection \cite{wang2021end}, and image classification \cite{sun2021supervised}.
Since Kahou et al. \cite{kahou2013combining} and Tang et al. \cite{tang2013deep} unprecedentedly adopt CNNs into FER and win the EmotiW 2013 and FER 2013 emotion recognition competitions, CNNs-based approaches become the dominating techniques and a pool of pioneering methods have been developed.
For example, to ameliorate the occlusion problem, several current studies \cite{wang2020region,LiZSC19,zhao2021learning} attempt to weaken the negative impacts of occlusion regions in raw images by delving into more sophisticated CNNs architectures.
Albeit the promising achievements and insights, these CNNs-based methods only focus on the merits of CNNs (e.g., pyramidal representations and well-established pretrained models.), but overlook the limitations of narrow receptive fields. Lacking global contextual semantics easily impels the model to produce error-prone classification results and hinders the generalization of FER algorithms. To enlarge the receptive fields of features, several existing works have strived to deepen the networks or improve convolution path \cite{han2019enhanced,gu2017enlarging}.
Nevertheless, these approaches still follow the learning paradigm of CNNs and representations with global semantics are neglected.

In recent couple of years, the learning paradigm of transformer, with self-attention mechanisms, has demonstrated its advantages among learning-based models in Natural Language Processing (NLP) \cite{vaswani2017attention, devlin2018bert}. It has also been widely applied in computer vision to model global relationships among pixels over 2D feature maps. In particular, Vision Transformer (ViT) \cite{dosovitskiy2020image} is a pioneering work and becomes favoured baseline stem for various vision tasks, e.g., semantic segmentation \cite{gu2022multi}, image classification \cite{sun2022spectral}, object detection \cite{han2022few}, and image processing \cite{tu2022maxim}.
Although global receptive fields can be modelled via self-attention condensers, the insufficiency of training data \cite{dosovitskiy2020image} impedes the training of models from scratch, because the pure transformer block often requires a larger amount of training data. Besides, patch-based tokens learned by transformer loss the pyramidal structure of features, which damages the extraction of local spatial information. To circumvent these challenges, TransFER \cite{xue2021transfer} is proposed to combine both pre-trained CNNs-based IR-50 and Transformer-based encoder, resulting intriguing FER performance. However, the collaboration procedure is coarse in exiting hybrid approaches and they somewhat ignore the fine-grained extraction/integration of multi-scale features, thereby reducing the robustness and scale invariance of representations. Hence, there is still large room for further improvements of FER algorithms to absorb the merits of hybrid CNNs and Transformer paradigms in one pass.

Another common nuisance in the wild FER is that only manually-annotated one-hot ground truths (hard) are provided in most of existing prevailing datasets, which easily incurs annotation ambiguity due to the subjectiveness of annotators, ambiguous facial expressions, or low-quality facial images. For example, Chen et al. \cite{chen2021understanding} demonstrate the existence of annotation biases between \emph{genders} in many expression datasets, whereas the work \cite{mao2021label} suggests that inter-correlations among emotions are inherently disparate, consequently degrading the performance of FER.

Lots of efforts have been made to address the issue of annotation ambiguity and can be roughly categorized as two mainstream directions: Learning of Soft Label Distribution and Learning from Hard Samples via delicately-designed mechanisms (e.g., attention mechanism or self-supervised strategy). Specifically, \emph{i)} Instead of hard one-hot supervisory signals, a set of probability distributions with auxiliary priors are assigned to facial images with the goal of driving the deep learning-based models to extract representations with fuzzy boundaries \cite{zhao2021robust,chen2020label,she2021dive,shao2022self}; \emph{ii)} Hard samples are selected as outliers and then imposed on extra special processes to suppress their negative impacts on the holistic models' learning \cite{shao2022self,wang2020suppressing,Li2022towards,wang2022ease}. Even though a series of attempts have been conducted to ease annotation ambiguity, the required auxiliary supervisory signals are still derived from raw one-hot labels and the ambiguity is prone to be inherited. Hence, homogenous supervision with intrinsic ambiguity is considered by existing endeavours and the merits of heterogeneous supervisory signals from multi-modal annotations are grossly underestimated or ignored. It is a remarkable fact that the recent resounding success of CLIP \cite{radford2021learning} allows the thriving development of various multi-modal learning (involving image and text) in many fields \cite{conde2021clip, wang2022clip}, and therefore provides a strong inspiration for our research.

Motivated by the above observations, in this paper, we propose a novel \emph{Multifarious Supervision-steering Transformer} for FER in the wild, dubbed as FER-former, which features multi-granularity embedding integration, hybrid self-attention scheme and  heterogeneous supervisory signals (one-hot and self-defined textual labels).
To promote effective collaboration between features from prevailing CNNs (pyramidal property) and Transformer (global receptive fields), a hybrid structure is designed for possessing both learning paradigms in one pass.
To mitigate the issues of uncontrolled occlusion and pose variations, a downsampling scheme imposed on the crude features from pre-trained CNN stem is devised to disentangle diverse spatial cues, while increasing the scale invariance of tokens.
%Besides,
%a multi-scale strategy is also proposed to increase the scale invariance of tokens.
%In specific, raw features are split into sequences of patches with varied resolutions and projected as multi-scale tokens.
To ease the annotation ambiguity and boost the diversity of supervisory signals, we introduce textual annotations using natural language for each emotion, and then treat them as the auxiliary heterogeneous supervision to work together with the conventional hard one-hot labels.
On top of these two types of signals, a FER-specific transformer block containing a hybrid self-attention scheme (\emph{e.g. one-hot and text-focusing heads}) is delicately devised in our FER-Former to produce the ambiguity-robust tokens.
In summery, the main contributions are fourfold in this paper.

\begin{itemize}
	\item A novel heterogeneous supervision-steering Transformer for FER in the wild is proposed, namely FER-former, which features multi-granularity embedding integration, hybrid self-attention scheme and heterogeneous domain-steering supervision.
	\item A FER-specific transformer block that involves a hybrid self-attention scheme is delicately devised to characterize conventional one-hot and text-oriented tokens for heterogeneous classifications, where text semantic information is passed to class token and other patch tokens by steering token in each encoder block.
    \item To ease the issue of annotation ambiguity, a heterogeneous domains-steering supervision module is proposed to make image features also have text-space semantic correlations by supervising the similarity between image features and text features.
	\item Extensive experiments on popular benchmarks demonstrate the superiority of the proposed FER-former over the existing state-of-the-art approaches.
\end{itemize}

