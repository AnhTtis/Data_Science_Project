%------------------------------------------------------------------------
\section{Related work}
In this section, we review related studies which can be roughly summarized into three categories: FER in the wild, hybrid leaning paradigms, and vision-language pretraining.

\subsection{FER in the wild}
\paragraph{FER with unconstrained variations}
To handle FER under weaker input constraints, researchers have mainly focused on two challenges: occlusion and non-frontal head pose. To suppress their negative impacts on FER models' performance, remarkable efforts have been made.
Li et al. \cite{LiZSC19} split intermediate feature maps into 24 sub-patches, and then utilize self-attention mechanism to recalibrate the patch-based information for occlusion-aware FER. %on the basis of facial landmarks
Methods in \cite{pan2019occluded, xia2020occluded} propose to leverage non-occluded facial images as auxiliary information to further fine-tune the occluded network, thereby producing occlusion-robust representations for FER.
To increase the pose invariance of FER models, Zhang et al. \cite{zhang2020geometry} try to disentangle expression and head pose from a given image by using geometry information.
In addition, several works address the challenges of occlusion and pose variations simultaneously. For example,
Wang et al. \cite{wang2020region} crop five fixed regions from a raw image and then feed them into a region attention network (RAN) for modelling inner-relationships among these facial regions. Zhao et al. \cite{zhao2021learning} propose a global multi-scale and local attention network (MA-Net) with different kernel sizes and attention-based recalibration weights.
Even though various attempts have been exerted to widen the receptive fields, they all based on the CNNs learning paradigm and do not make full use of the global semantic features of facial images.

\paragraph{FER with annotation ambiguity}
Annotation ambiguity is inevitable in wild expression datasets due to the subjectiveness of annotators, ambiguous facial expressions, and low-quality facial images \cite{wang2020suppressing}.
Label distribution learning is an intuitive and favoured scheme to reduce the ambiguity.
Zhao et al. \cite{zhao2021robust} treat the output of an auxiliary ResNet-50 as probability distribution to guide the learning of the other backbone network.
Shao et al. \cite{shao2022self} further adopt an auxiliary network as a label distribution generator to generate label distributions for guiding the backbone network training and selecting easy samples.
Chen et al. \cite{chen2020label} put forward an approach to learn label distribution by calculating the confidence probability of its nearest neighbours. Mao et al. \cite{mao2021label} try to incorporate the correlations among expressions into the label distribution in the semantic space.
Besides, DMUE \cite{she2021dive} makes use of auxiliary multi-branching framework to mine the latent distribution while estimating the uncertainty of the annotation ambiguity.
%In addition, many other excellent approaches\cite{wang2020suppressing, Li2022towards, wang2022ease} try to alleviate the annotation ambiguity by absorbing extra informative clues from hard samples.

Many excellent approaches try to alleviate the annotation ambiguity by absorbing extra informative clues from hard samples. Wherein, Wang et al. \cite{wang2020suppressing} propose the combination of attention mechanism, ranking regularization and relabelling low-ranked samples to evade the influence of uncertain samples on the final recognition results. Ada-CM \cite{Li2022towards} is proposed to learn adaptive confidence margin for each expression and exploit the low-confidence samples. In \cite{wang2022ease}, Wang et al. divide all training samples into three groups: clean, noisy, and conflict, and then focus on learning with ambiguous samples to enhance the robustness of models.
Despite the impressive results have been achieved by existing methods through reconstructing label distribution or emphasizing on hard samples, they still derive the additional supervisory signals from raw one-hot labels and the ambiguity is inclined to be inherited.
In this paper, to mitigate the issues of annotation ambiguity, a FER-specific transformer block is well devised to characterize heterogeneous hard one-hot label-focusing and text-oriented classification tokens.


\subsection{Hybrid leaning paradigms}
Recently, ViT \cite{dosovitskiy2020image} has become a dominating component for various computer vision models thanks to its capability of modelling long-range dependencies among a set of sub-patches. Even though transformer has higher ability of capturing global receptive fields, the intrinsic merits of CNNs (e.g., pyramidal feature maps) are beneficial for enhancing model capacity. Hence, a series of hybrid architectures are designed to inherit the advantages of both CNNs and Transformer-based learning paradigms. For example,
CMT \cite{guo2022cmt} and CeiT \cite{yuan2021incorporating} combine depth-wise convolution with a feed-forward network in encoder blocks to compensate for the limitations of pure transformers.
The proposed convolutional token embedding and convolutional projection in CVT \cite{wu2021cvt} allow the model to inherit the merits of CNNs while enlarging the receptive fields via transformer. Moreover, LeViT \cite{graham2021levit} devise a fabric-like neural network for fast inference image classification by incorporating some components that were proven useful for convolutional architectures (replace the uniform structure of a Transformer by a pyramid with pooling). BoTNet \cite{srinivas2021bottleneck} achieves promising performance on image classification, object detection, and instance segmentation by replacing the spatial convolutions with global self-attention mechanism in the final three bottleneck blocks of a ResNet.

When handling FER tasks, the priori knowledge of pre-trained models are necessary for good performance due to limited datasets. Inspired by hybrid structures in other vision tasks, TransFER \cite{xue2021transfer} combines a pre-trained IR50 and a pre-trained Transformer encoder to accomplish a state-of-the-art FER performance. However, the collaboration procedure is coarse and somewhat ignores the extraction/integration of multi-scale features, thereby reducing the robustness and scale invariance of representations. Hence, there is still large room for fully exploiting the merits of hybrid CNNs and Transformer paradigms in one pass.

\subsection{Vision-language pretraining}
Vision-language pretraining (VLP) aims to learn strong representations from large-scale image-text pairs, which has achieved impressive performance on a variety of downstream vision-language tasks \cite{li2021align,radford2021learning}.
Most current approaches in VLP can be divided into two categories: fusion encoder(s) and dual encoders \cite{singh2022flava}.
The first category aims to model the shared self-attention between image features and text features with complex multimodal encoders, such as ALBEF \cite{li2021align}, FLAVA \cite{singh2022flava}, SIMVLM \cite{wang2021simvlm}, UniT \cite{hu2021unit}, ViLT \cite{kim2021vilt} and VinVL \cite{zhang2021vinvl}.
These methods are capable of handing complex tasks such as object detection, natural language understanding, and multimodal reasoning, but they mostly require pre-trained object detectors and high-resolution input images, and their small corpus limits their ability to generalize to specific fine-grained classification tasks.
In contrast, the methods in the second category such as ALIGN \cite{jia2021scaling} and CLIP \cite{radford2021learning} focus on learning separate unimodal encoders for image and text, which are simple and good at retrieval tasks.
In particular, CLIP \cite{radford2021learning} was trained from scratch on a large dataset of 400 million (image, text) pairs collected from the internet, which supports a wide range of corpus requirements for fine-grained classification tasks.
It shows the superiority of text supervision on multiple vision tasks \cite{conde2021clip, wang2022clip}, thus provides a strong inspiration for our research.
