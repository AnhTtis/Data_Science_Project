\subsection{Datasets}
\label{subsec:datasets}

\paragraph{Introspect~\cite{selvaraju2020squinting}:}Contains perception questions (or sub-questions) created by annotators for a subset of reasoning question (or main questions) of the VQA v1.0 and VQA v2.0 datasets~\cite{antol2015vqa,goyal2017making}. It contains 27,441 reasoning questions with 79,905 sub-questions in its training set and 15,448 reasoning questions with 52,573 sub-questions for validation. For images that have the same sub-question repeated multiple times, we remove duplicates in the sub-questions for every image in both the train and validation sets.

\paragraph{DME Dataset~\cite{tascon2022consistency}:}This dataset consists of retinal fundus images, for the task of Diabetic Macular Edema (DME) staging. It contains 9,779 QA pairs for training, 2,380 QA pairs for validation and 1,311 QA pairs for testing. There are three types of questions in the dataset: main, sub, and independent questions. Main questions ask information about the diagnosis (\ie the stage of the disease) and sub-questions ask about presence and location of biomarkers. Sub-questions are further sub-divided into grade questions, questions about the whole images, questions about a region of the eye called macula, and questions about random regions in the image. To deal with questions about image regions, we follow the procedure described in~\cite{tascon2022consistency}, whereby only the relevant region is shown to the model.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Baseline methods and base models} We consider 3 different consistency enhancement baseline methods. To ensure fair comparisons, all methods use the same VQA base models and only differ in the consistency method used. These consist in: 
\begin{itemize}
    \item None: Indicating that no consistency preserving method is used with the VQA model. This corresponds to the case where $\lambda=0$.%, and enforce that related pairs are in the same mini-batch.
    \item SQuINT~\cite{selvaraju2020squinting}: Optimizes consistency by maximizing the similarity between the attention maps of pairs of questions. As such, it requires a VQA model that uses guided attention. 
    \item CP-VQA~\cite{tascon2022consistency}: Assumes entailment relations and uses a regularizer to improve consistency. 
\end{itemize}

%(1) The case in which no consistency method is used, which corresponds to the case in which $\lambda=0$ in \cref{eq:total_loss}. So that the comparison is fair, for this baseline we also apply the pair sampling method described in \cref{sec:method} (\ie related pairs are in the same mini-batch). 
%(2) SQuINT~\cite{selvaraju2020squinting}, which is a method that aims at optimizing consistency by maximizing the similarity between the attention maps of pairs of questions. (3) Consistency-preserving VQA (C.P. VQA)~\cite{tascon2022consistency}, which assumes entailment relations and uses a regularizer to improve consistency. 

\paragraph{VQA architectures:}
We show experiments using three VQA models depending on the dataset used. For experiments on Introspect, we make use of the BAN model~\cite{kim2018bilinear}, as its structure with guided attention allows the use of SQuINT. In addition, we evaluate the vision-language architecture LXMERT~\cite{tan2019lxmert} on this dataset to see how our approach can help improve state-of-the-art, transformer-based, VQA models too. For experiments on the DME dataset, we use the base model described in~\cite{tascon2022consistency}, which we denote by MVQA. 


\subsection{Implementation details}
\label{subsec:imp_details}
\paragraph{LI-Model} We first pre-train BERT on SNLI for 5 epochs until it reaches a maximum accuracy of 84.32\% on that dataset. For this pre-training stage, we initialize BERT with the \textit{bert-base-uncased} weights and use a batch size of~16. We use a weight decay rate of 0.01 and the AdamW optimizer with learning rate $2\cdot10^{-5}$ without bias correction. The same setup was kept to finetune the model on a subset of 2'000 pairs of propositions from Introspect which were manually annotated (distribution of labels being: $\leftarrow 60\%, \leftrightarrow 17\%, - 12\%, \rightarrow  11\%$), and an additional 500 pairs were annotated for validation. Notice that LI-MOD is only necessary for the Introspect dataset, since for the DME dataset the implications annotations are available.

\paragraph{VQA models:}
For our base models, we use the official and publicly available implementations (BAN~\cite{jackroos2019}, LXMERT~\cite{tan2019lxmert} and MVQA~\cite{tascon2022consistency}) with default configurations. We re-implemented SQuINT~\cite{selvaraju2020squinting} and used the provided implementation of CP-VQA~\cite{tascon2022consistency}, reporting the best results which were obtained with $\lambda=0.1, \gamma=0.5$ for BAN and $\lambda=0.5, \gamma=1$ for MVQA. These parameters refer to the corresponding symbols of the original implementations. For SQuINT, we set the gain of the attention map similarity term to 0.5 for BAN and to 1.0 for MVQA. For Introspect, we train 5 models with different seeds for each parameter set and for DME we train 10 models with different seeds. To train LXMERT, BAN and MVQA, we use batch sizes of 32, 64 and 128, respectively. Regarding the VQA cross-entropy loss, we follow the original implementations and use soft scores for the answers in LXMERT, and categorical answers for BAN and MVQA.
