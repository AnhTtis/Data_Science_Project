\section{Introduction}
\label{sec:intro}

Visual Questioning Answering (VQA) models have drawn recent interest in the computer vision community as they allow text queries to question image content. This has given way to a number of novel applications in the space of model reasoning~\cite{wang2015explicit,cadene2019murel,wu2021multi,jing2022maintaining}, medical diagnosis~\cite{nguyen2019overcoming,vu2020question,gupta2021hierarchical,zhan2020medical} and counter factual learning~\cite{agarwal2020towards,chen2020counterfactual,abbasnejad2020counterfactual}. With ability to combine language and image information in a common model, it is unsurprising to see a growing use of VQA methods.

Despite this recent progress however, a number of important challenges remain when making VQAs more proficient. For one, it remains extremely challenging to build VQA datasets that are void of bias. Yet this is critical to ensure subsequent models are not learning spurious correlations or shortcuts~\cite{teney2020unshuffling}. This is particularly daunting in applications where domain knowledge plays an important role (\eg,  medicine~\cite{he2020pathvqa,lau2018dataset,do2021multiple}). Alternatively, ensuring that responses of a VQA are coherent, or {\it consistent}, is paramount as well. That is, VQA models that answer differently about similar content in a given image, imply inconsistencies in how the model interprets the inputs. A number of recent methods have attempted to address this using logic-based approaches~\cite{gokhale2020vqa}, rephrashing~\cite{shah2019cycle}, question generation~\cite{ribeiro2019red,ray2019sunny, goel2021iq} and regularizing using consistency constraints~\cite{tascon2022consistency}. In this work, we follow this line of research and look to yield more reliable VQA models.

\begin{figure}[!t]
\centering
\includegraphics[width=0.4\textwidth]{images/image_intro.pdf}
\caption{Top: Conventional VQA models tend to produce inconsistent answers as a consequence of not considering the relations between question and answer pairs. Bottom: Our method learns the logical relation between question and answers pairs to improve consistency.
} 
\label{fig:image_intro}
\end{figure}

Specifically, we wish to ensure that VQA models are consistent in their ability to answer questions about images. This implies that if one poses multiple questions about the same image, then the model's answers should not contradict themselves. For instance, if one question about the image in Fig.~\ref{fig:image_intro} asks ``Is there snow on the ground?", then the answer inferred should be consistent with that of the question ``Is it the middle of summer?" As noted in~\cite{selvaraju2020squinting}, such question pairs involve reasoning and perception, and consequentially lead the authors to define inconsistency when the reasoning and perception questions are answered correctly and incorrectly, respectively. Along this line,~\cite{tascon2022consistency} use a similar definition of inconsistency to regularize a VQA model meant to answer medical diagnosis questions that are hierarchical in nature. What is critical in both cases however is that the consistency of the VQA model depends explicitly on its answers, as well as the question and true answer. This hinges on the assumption that perception questions are sufficient to answer reasoning question. Yet, for any question pair, this may not be the case. As such, the current definition of consistency (or inconsistency) has been highly limited and does not truly reflect how VQAs should behave. 

To address the need to have VQA models that are self-consistent, we propose a novel training strategy that relies on logical relations. To do so, we re-frame question-answer (QA) pairs as propositions and consider the relational construct between pairs of propositions. This construct allows us to properly categorise pairs of propositions in terms of their logical relations. From this, we introduce a novel loss function that explicitly leverages the logical relations between pairs of questions and answers in order to enforce that VQA models be self-consistent.
Unfortunately however, datasets typically do not contain relational information about pairs of QA and collecting this would be extremely laborious and difficult to achieve. To overcome this, we propose to train a dedicated language model capable of inferring logical relations between propositions.
By doing so, we show in our experiments that not only are we able to effectively infer logical relations from propositions, but that these can be explicitly used in our loss function to train VQA models that improve state-of-the-art methods via consistency. We show this over two different VQA datasets, against different consistency methods and with different VQA model architectures. Our code and data are available at \url{https://github.com/sergiotasconmorales/imp_vqa}.
