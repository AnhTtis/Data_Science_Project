@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


%% About VQA in general
@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}




%% About previous works on consistency in VQA
@inproceedings{ribeiro2019red,
  title={Are red roses red? evaluating consistency of question-answering models},
  author={Ribeiro, Marco Tulio and Guestrin, Carlos and Singh, Sameer},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6174--6184},
  year={2019}
}

@article{teney2019incorporating,
  title={On incorporating semantic prior knowledge in deep learning through embedding-space constraints},
  author={Teney, Damien and Abbasnejad, Ehsan and Hengel, Anton van den},
  journal={arXiv preprint arXiv:1909.13471},
  year={2019}
}

@inproceedings{shah2019cycle,
  title={Cycle-consistency for robust visual question answering},
  author={Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6649--6658},
  year={2019}
}

@inproceedings{gokhale2020vqa,
  title={{VQA-LOL}: Visual question answering under the lens of logic},
  author={Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
  booktitle={European conference on computer vision},
  pages={379--396},
  year={2020},
  organization={Springer}
}

@inproceedings{goel2021iq,
  title={{IQ-VQA}: Intelligent Visual Question Answering},
  author={Goel, Vatsal and Chandak, Mohit and Anand, Ashish and Guha, Prithwijit},
  booktitle={International Conference on Pattern Recognition},
  pages={357--370},
  year={2021},
  organization={Springer}
}

@inproceedings{selvaraju2020squinting,
  title={SQuINTing at VQA Models: Introspecting VQA Models With Sub-Questions},
  author={Selvaraju, Ramprasaath R and Tendulkar, Purva and Parikh, Devi and Horvitz, Eric and Ribeiro, Marco Tulio and Nushi, Besmira and Kamar, Ece},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10003--10011},
  year={2020}
}

@inproceedings{jing2022maintaining,
  title={Maintaining Reasoning Consistency in Compositional Visual Question Answering},
  author={Jing, Chenchen and Jia, Yunde and Wu, Yuwei and Liu, Xinyu and Wu, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5099--5108},
  year={2022}
}

@article{ray2019sunny,
  title={Sunny and dark outside?! improving answer consistency in vqa through entailed question generation},
  author={Ray, Arijit and Sikka, Karan and Divakaran, Ajay and Lee, Stefan and Burachas, Giedrius},
  journal={arXiv preprint arXiv:1909.04696},
  year={2019}
}


@inproceedings{yuan2021perception,
  title={Perception Matters: Detecting Perception Failures of VQA Models Using Metamorphic Testing},
  author={Yuan, Yuanyuan and Wang, Shuai and Jiang, Mingyue and Chen, Tsong Yueh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16908--16917},
  year={2021}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{tascon2022consistency,
  title={Consistency-Preserving Visual Question Answering in Medical Imaging},
  author={Tascon-Morales, Sergio and M{\'a}rquez-Neila, Pablo and Sznitman, Raphael},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={386--395},
  year={2022},
  organization={Springer}
}

@book{bradley1979possible,
  title={Possible Worlds: An Introduction to Logic and Its Philosophy},
  author={Bradley, R. and Swartz, N.},
  isbn={9780631161400},
  lccn={lc85672970},
  url={https://books.google.ch/books?id=x353QgAACAAJ},
  year={1979},
  publisher={B. Blackwell}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={67--78},
  year={2014},
  publisher={MIT Press}
}

@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}

@inproceedings{nam2017dual,
  title={Dual attention networks for multimodal reasoning and matching},
  author={Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={299--307},
  year={2017}
}

@inproceedings{cadene2019murel,
  title={Murel: Multimodal relational reasoning for visual question answering},
  author={Cadene, Remi and Ben-Younes, Hedi and Cord, Matthieu and Thome, Nicolas},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1989--1998},
  year={2019}
}

@article{kim2018bilinear,
  title={Bilinear attention networks},
  author={Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{gurari2019vizwiz,
  title={Vizwiz-priv: A dataset for recognizing the presence and purpose of private visual information in images taken by blind people},
  author={Gurari, Danna and Li, Qing and Lin, Chi and Zhao, Yinan and Guo, Anhong and Stangl, Abigale and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={939--948},
  year={2019}
}

@article{fukui2016multimodal,
  title={Multimodal compact bilinear pooling for visual question answering and visual grounding},
  author={Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1606.01847},
  year={2016}
}

@article{kim2016hadamard,
  title={Hadamard product for low-rank bilinear pooling},
  author={Kim, Jin-Hwa and On, Kyoung-Woon and Lim, Woosang and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:1610.04325},
  year={2016}
}

@inproceedings{ben2017mutan,
  title={Mutan: Multimodal tucker fusion for visual question answering},
  author={Ben-Younes, Hedi and Cadene, R{\'e}mi and Cord, Matthieu and Thome, Nicolas},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2612--2620},
  year={2017}
}


@inproceedings{agrawal2018don,
  title={Don't just assume; look and answer: Overcoming priors for visual question answering},
  author={Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4971--4980},
  year={2018}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@inproceedings{nguyen2019overcoming,
  title={Overcoming data limitation in medical visual question answering},
  author={Nguyen, Binh D and Do, Thanh-Toan and Nguyen, Binh X and Do, Tuong and Tjiputra, Erman and Tran, Quang D},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={522--530},
  year={2019},
  organization={Springer}
}

@article{gupta2021hierarchical,
  title={Hierarchical deep multi-modal network for medical visual question answering},
  author={Gupta, Deepak and Suman, Swati and Ekbal, Asif},
  journal={Expert Systems with Applications},
  volume={164},
  pages={113993},
  year={2021},
  publisher={Elsevier}
}

@article{vu2020question,
  title={A question-centric model for visual question answering in medical imaging},
  author={Vu, Minh H and L{\"o}fstedt, Tommy and Nyholm, Tufve and Sznitman, Raphael},
  journal={IEEE transactions on medical imaging},
  volume={39},
  number={9},
  pages={2856--2868},
  year={2020},
  publisher={IEEE}
}

@inproceedings{chen2022grounding,
  title={Grounding Answers for Visual Questions Asked by Visually Impaired People},
  author={Chen, Chongyan and Anjum, Samreen and Gurari, Danna},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19098--19107},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{han2021greedy,
  title={Greedy gradient ensemble for robust visual question answering},
  author={Han, Xinzhe and Wang, Shuhui and Su, Chi and Huang, Qingming and Tian, Qi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1584--1593},
  year={2021}
}

@inproceedings{banerjee2021weakly,
  title={Weakly Supervised Relative Spatial Reasoning for Visual Question Answering},
  author={Banerjee, Pratyay and Gokhale, Tejas and Yang, Yezhou and Baral, Chitta},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1908--1918},
  year={2021}
}
@inproceedings{cao2021linguistically,
  title={Linguistically routing capsule network for out-of-distribution visual question answering},
  author={Cao, Qingxing and Wan, Wentao and Wang, Keze and Liang, Xiaodan and Lin, Liang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1614--1623},
  year={2021}
}

@inproceedings{dancette2021beyond,
  title={Beyond question-based biases: Assessing multimodal shortcut learning in visual question answering},
  author={Dancette, Corentin and Cadene, Remi and Teney, Damien and Cord, Matthieu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1574--1583},
  year={2021}
}

@article{teney2020unshuffling,
  title={Unshuffling data for improved generalization},
  author={Teney, Damien and Abbasnejad, Ehsan and Hengel, Anton van den},
  journal={arXiv preprint arXiv:2002.11894},
  year={2020}
}

@inproceedings{yang2021auto,
  title={Auto-parsing network for image captioning and visual question answering},
  author={Yang, Xu and Gao, Chongyang and Zhang, Hanwang and Cai, Jianfei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2197--2207},
  year={2021}
}

@inproceedings{zhou2021trar,
  title={Trar: Routing the attention spans in transformer for visual question answering},
  author={Zhou, Yiyi and Ren, Tianhe and Zhu, Chaoyang and Sun, Xiaoshuai and Liu, Jianzhuang and Ding, Xinghao and Xu, Mingliang and Ji, Rongrong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2074--2084},
  year={2021}
}

@inproceedings{ding2022mukea,
  title={MuKEA: Multimodal Knowledge Extraction and Accumulation for Knowledge-based Visual Question Answering},
  author={Ding, Yang and Yu, Jing and Liu, Bang and Hu, Yue and Cui, Mingxin and Wu, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5089--5098},
  year={2022}
}

@inproceedings{gao2022transform,
  title={Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering},
  author={Gao, Feng and Ping, Qing and Thattai, Govind and Reganti, Aishwarya and Wu, Ying Nian and Natarajan, Prem},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5067--5077},
  year={2022}
}

@inproceedings{gupta2022swapmix,
  title={Swapmix: Diagnosing and regularizing the over-reliance on visual context in visual question answering},
  author={Gupta, Vipul and Li, Zhuowan and Kortylewski, Adam and Zhang, Chenyu and Li, Yingwei and Yuille, Alan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5078--5088},
  year={2022}
}

@inproceedings{walmer2022dual,
  title={Dual-Key Multimodal Backdoors for Visual Question Answering},
  author={Walmer, Matthew and Sikka, Karan and Sur, Indranil and Shrivastava, Abhinav and Jha, Susmit},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15375--15385},
  year={2022}
}

@inproceedings{agarwal2020towards,
  title={Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing},
  author={Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9690--9698},
  year={2020}
}

@inproceedings{chen2020counterfactual,
  title={Counterfactual samples synthesizing for robust visual question answering},
  author={Chen, Long and Yan, Xin and Xiao, Jun and Zhang, Hanwang and Pu, Shiliang and Zhuang, Yueting},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10800--10809},
  year={2020}
}

@inproceedings{abbasnejad2020counterfactual,
  title={Counterfactual vision and language learning},
  author={Abbasnejad, Ehsan and Teney, Damien and Parvaneh, Amin and Shi, Javen and Hengel, Anton van den},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10044--10054},
  year={2020}
}
@inproceedings{zhan2020medical,
  title={Medical visual question answering via conditional reasoning},
  author={Zhan, Li-Ming and Liu, Bo and Fan, Lu and Chen, Jiaxin and Wu, Xiao-Ming},
  booktitle={Proceedings of the 28th ACM International Conference on Multimedia},
  pages={2345--2354},
  year={2020}
}

@article{wang2015explicit,
  title={Explicit knowledge-based reasoning for visual question answering},
  author={Wang, Peng and Wu, Qi and Shen, Chunhua and Hengel, Anton van den and Dick, Anthony},
  journal={arXiv preprint arXiv:1511.02570},
  year={2015}
}

@article{wu2021multi,
  title={Multi-scale relation reasoning for multi-modal Visual Question Answering},
  author={Wu, Yirui and Ma, Yuntao and Wan, Shaohua},
  journal={Signal Processing: Image Communication},
  volume={96},
  pages={116319},
  year={2021},
  publisher={Elsevier}
}

@article{he2020pathvqa,
  title={Pathvqa: 30000+ questions for medical visual question answering},
  author={He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  journal={arXiv preprint arXiv:2003.10286},
  year={2020}
}

@article{lau2018dataset,
  title={A dataset of clinically generated visual questions and answers about radiology images},
  author={Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina},
  journal={Scientific data},
  volume={5},
  number={1},
  pages={1--10},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{do2021multiple,
  title={Multiple meta-model quantifying for medical visual question answering},
  author={Do, Tuong and Nguyen, Binh X and Tjiputra, Erman and Tran, Minh and Tran, Quang D and Nguyen, Anh},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={64--74},
  year={2021},
  organization={Springer}
}

@inproceedings{maccartney2008modeling,
  title={Modeling semantic containment and exclusion in natural language inference},
  author={MacCartney, Bill and Manning, Christopher D},
  booktitle={Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
  pages={521--528},
  year={2008}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{sathe2020automated,
  title={Automated fact-checking of claims from wikipedia},
  author={Sathe, Aalok and Ather, Salar and Le, Tuan Manh and Perry, Nathan and Park, Joonsuk},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={6874--6882},
  year={2020}
}

@article{nie2019adversarial,
  title={Adversarial NLI: A new benchmark for natural language understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  journal={arXiv preprint arXiv:1910.14599},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}

@article{petrov2011universal,
  title={A universal part-of-speech tagset},
  author={Petrov, Slav and Das, Dipanjan and McDonald, Ryan},
  journal={arXiv preprint arXiv:1104.2086},
  year={2011}
}

@misc{jackroos2019,
  author = {Weijie Su},
  title = {Pythia},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jackroos/pythia}},
  commit = {12f67cd4f67499814bb0b3665ff14dd635800f63}
}

@inproceedings{zhang2023practice,
  title={How to Practice VQA on a Resource-limited Target Domain},
  author={Zhang, Mingda and Hwa, Rebecca and Kovashka, Adriana},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4451--4460},
  year={2023}
}

@article{xu2023question,
  title={A question-guided multi-hop reasoning graph network for visual question answering},
  author={Xu, Zhaoyang and Gu, Jinguang and Liu, Maofu and Zhou, Guangyou and Fu, Haidong and Qiu, Chen},
  journal={Information Processing \& Management},
  volume={60},
  number={2},
  pages={103207},
  year={2023},
  publisher={Elsevier}
}
