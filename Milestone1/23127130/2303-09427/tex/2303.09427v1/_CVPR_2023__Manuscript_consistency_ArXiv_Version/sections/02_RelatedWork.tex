\section{Related work}
\label{sec:relatedwork}

Since its initial presentation in Antol \etal~\cite{antol2015vqa}, VQA has thoroughly advanced. Initial developments focused on multimodal fusion modules, which combine visual and text embeddings~\cite{nam2017dual,cadene2019murel}. From basic concatenation and summation~\cite{antol2015vqa}, to more complex fusion mechanisms that benefit from projecting the embeddings to different spaces, numerous approaches have been proposed~\cite{fukui2016multimodal,kim2016hadamard,ben2017mutan}. The addition of attention mechanisms~\cite{kim2018bilinear, nam2017dual,cadene2019murel} and subsequently transformer architectures~\cite{vaswani2017attention} has also contributed to the creation of transformer-based vision-language models, such as LXMERT, which have shown state-of-the-art performances~\cite{tan2019lxmert}. 

More recently, methods have proposed to improve other aspects of VQA, including avoiding shortcut learning and biases~\cite{dancette2021beyond,han2021greedy}, improving 3D spatial reasoning~\cite{banerjee2021weakly}, Out-Of-Distribution (OOD) generalization~\cite{cao2021linguistically,teney2020unshuffling}, improving transformer-based vision-language models~\cite{yang2021auto,zhou2021trar}, external knowledge integration~\cite{ding2022mukea,gao2022transform} and model evaluation with visual and/or textual perturbations~\cite{gupta2022swapmix,walmer2022dual}. With the awareness of bias in VQA training data some works have also addressed building better datasets (\eg, v2.0~\cite{goyal2017making}, VQA-CP~\cite{agrawal2018don}, CLEVR~\cite{johnson2017clevr} and GCP~\cite{hudson2019gqa}).

Furthermore, these developments have now given rise to VQA methods in specific domains. For instance, the VizWiz challenge~\cite{gurari2018vizwiz,gurari2019vizwiz,chen2022grounding} aims at creating VQA models that can help visually impaired persons with routine daily tasks, while there is a growing number of medical VQA works with direct medicine applications~\cite{nguyen2019overcoming,gupta2021hierarchical,vu2020question,zhan2020medical}. 

\paragraph{Consistency in VQA}
Consistency in VQA can be defined as the ability of a model to produce answers that are not contradictory. This is, given a pair of questions about an image, the answers predicted by a VQA model should not be contrary (\eg answering ``Yes" to ``Is it the middle of summer?" and ``Winter" to ``What season is it?"). Due to its significance in reasoning, consistency in VQA has become a focus of study in recent years~\cite{ribeiro2019red,shah2019cycle,gokhale2020vqa,selvaraju2020squinting,jing2022maintaining}. Some of the first approaches for consistency enhancement focused on creating re-phrasings of questions, either by dataset design or at training time~\cite{shah2019cycle}. Along this line, entailed questions were proposed~\cite{ribeiro2019red,gokhale2020vqa}, such that a question generation module was integrated into a VQA model~\cite{ray2019sunny,goel2021iq}, used as a benchmarking method to evaluate consistency~\cite{yuan2021perception} or as a rule-based data-augmentation technique~\cite{ribeiro2019red}. Other approaches tried to shape the embedding space by imposing constraints in the learned representations~\cite{teney2019incorporating} and by imposing similarities between the attention maps of pairs of questions~\cite{selvaraju2020squinting}. Another work~\cite{tascon2022consistency} assumed entailment relations between pairs of questions to regularize training. A more recent approach attempts to improve consistency by using graph neural networks to simulate a dialog in the learning process~\cite{jing2022maintaining}. 

While these approaches show benefits in some cases, they typically only consider that a subset of logical relationships exists between pairs of question-answers or assume that a single relation holds for all QA pairs. Though true in the case of re-phrasings, other question generation approaches cannot guarantee that the produced questions preserve unique relations or that grammatical structure remains valid. Consequently, these methods often rely on metrics that either over or under estimate consistency by relying on these assumptions. In the present work, we propose a strategy to alleviate these limitation by considering all logical relations between pairs of questions and answers. 

\paragraph{Entailment prediction} Natural Language Inference (NLI), or Recognizing Textual Entailment (RTE), is the task of predicting how two input sentences (namely \textit{premise} and \textit{hypothesis}) are related, according to three pre-established categories: entailment, contradiction and neutrality~\cite{maccartney2008modeling}. For example, if the premise is ``A soccer game with multiple males playing" and the hypothesis is ``Some men are playing a sport," then the predicted relation should be an entailment, because the hypothesis logically follows from the premise. Several benchmarking datasets (\eg, SNLI~\cite{young2014image}, MultiNLI~\cite{williams2017broad}, SuperGLUE~\cite{wang2019superglue}, WIKI-FACTCHECK~\cite{sathe2020automated} and ANLI~\cite{nie2019adversarial}) have contributed to the adaption of general-purpose transformer-based models like BERT~\cite{devlin2018bert}, RoBERTa~\cite{liu2019roberta} and DeBERTa~\cite{he2020deberta} for this task. In this work, we will leverage these recent developments to build a model capable of inferring relations between propositions. 

 




