%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference,twoside]{IEEEtran}

\usepackage{cite}
% \usepackage[sort&compress, numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bbm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{lineno,hyperref}
\usepackage{caption}
\usepackage{subcaption}
\DeclareMathOperator*{\argmax}{arg\,max} % The * in \DeclareMathOperator* places the underscored argument underneath the word rather than to the bottom right of it.
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{filecontents}

%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{CLIDiM: Contrastive Learning for Image Denoising in Microscopy}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Felix Fuentes-Hurtado, Jean-Baptiste Sibarita and Virgile Viasnoff% <-this % stops a space
\thanks{F. Fuentes-Hurtado is with the KNODIS Research Group, Universidad Politécnica de Madrid, Madrid, Spain, and also with the Departamento de Sistemas Informaticos, Universidad Politécnica de Madrid, Madrid, Spain.\\ 0000-0002-4320-245X}% <-this % stops a space
\thanks{J. Sibarita is with the Centre national de la recherche scientifique (CNRS), Interdisciplinary Institute for Neuroscience (IINS), University of Bordeaux, France \\
0000-0002-9920-7700.}% <-this % stops a space
\thanks{V. Viasnoff is with CNRS@CREATE, Mechanobiology Institute, Department of Biological Sciences, Singapore, and also with the National University of Singapore, Singapore\\
0000-0003-3949-2244.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
% \markboth{IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS}%
% {Fuentes-Hurtado \MakeLowercase{\textit{et al.}}: CLIDiM: Contrastive Learning for Image Denoising in Microscopy}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Microscopy images often suffer from high levels of noise, which can hinder further analysis and interpretation. Content-aware image restoration (CARE) methods have been proposed to address this issue, but they often require large amounts of training data and suffer from over-fitting. To overcome these challenges, we propose a novel framework for few-shot microscopy image denoising. Our approach combines a generative adversarial network (GAN) trained via \textit{contrastive learning} (CL) with two structure preserving loss terms – Structural Similarity Index and Total Variation loss – to further improve the quality of the denoised images using little data. We demonstrate the effectiveness of our method on three well-known microscopy imaging datasets, and show that we can drastically reduce the amount of training data while retaining the quality of the denoising, thus alleviating the burden of acquiring paired data and enabling few-shot learning. The proposed framework can be easily extended to other image restoration tasks and has the potential to significantly advance the field of microscopy image analysis.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
denoising, generative methods, contrastive learning, few-shot learning, microscopy data
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
\label{sec:intro}

\IEEEPARstart{I}{mage} denoising has become a popular and crucial topic in computer vision problems due to the inherent presence of various types of noise in acquired images, which can significantly reduce their visual quality. Thus, removing noise from images is essential in numerous computer vision and image processing tasks \cite{liu2020connecting, fan2019brief, gu2019brief, goyal2020image}.

One of the key applications where image denoising is vital is microscopy imaging. Advanced microscopy techniques have enabled the investigation of biological processes at sub-cellular resolution, yet they still suffer from high noise levels during the acquisition process, which make the obtained images difficult to analyze and process. Therefore, image denoising is an essential step for accurate image analysis and interpretation in microscopy imaging.

The removal of unwanted noise and distortions from acquired images, known as \textit{image denoising}, is an important and challenging task in the field of computer vision. Mathematically, the image degradation process can be described as $x=y+n$, where $x$ represents the degraded form of the original image $y$ with added noise $n$. Although Gaussian noise (AWGN) is often used to model image noise \cite{chen2018image, krull2020probabilistic}, in reality, noise in images can be caused by various mechanisms, such as low-light noise, shot noise, readout noise and non-structured noise. Therefore, the development of denoising methods that can generalize to different types of noise is crucial.

Recent research has shown that Convolutional Neural Networks (CNNs) can effectively construct powerful and efficient content-aware image restoration pipelines, both supervised \cite{weigert2017isotropic, weigert2018content, zhang2017beyond, zhang2019poisson} and self-supervised \cite{batson2019noise2self, krull2019noise2void, krull2020probabilistic, lehtinen2018noise2noise, laine2019high}. Supervised approaches require corresponding pairs of noisy and clean images to learn a mapping between the two quality levels. Conversely, self-supervised methods perform this task without paired data by making assumptions about the characteristics of noise and signals.

Each method has advantages and disadvantages. Supervised methods learn directly from corresponding pairs of clean and noisy images to map the noisy images to their clean counterparts. This enables them to remove structured noise (artefacts) in addition to pixel-noise, and find the best possible solution for denoising. However, unsupervised or self-supervised methods are not dependent on clean images and can work with noisy images alone. While they cannot guarantee the best possible solution, they are still able to effectively remove pixel-noise, making them a preferred option in situations where obtaining clean counterparts is challenging.

Moreover, despite the benefits of unsupervised methods, most of them do not surpass the performance of supervised methods. At the time of writing this work, only two unsupervised methods have achieved better results than supervised methods \cite{prakash2020fully, prakash2021removing}. Interestingly, both methods employ variational auto-encoders (VAEs) and average several predictions, which results in blurrier images compared to those obtained by a single prediction made by supervised methods. Moreover, the sampling process employed by these networks makes them slower than other methods. Thus, the development of a generative model based on VAEs that produces sharp images remains an open and important question that requires further attention.

In our work, we aim to address the limitations of VAE-based methods in image restoration by exploring the potential of Generative Adversarial Networks (GANs)\cite{goodfellow2020generative}. GANs have shown great promise in generating sharp and high-quality images. We propose to use this learning framework to develop an efficient and effective method for image denoising. The GAN architecture consists of two networks - the generative network and the discriminative network. The generative network purpose is to generate samples that are indistinguishable from real data, while the discriminative network is trained to distinguish between real and generated samples. By leveraging GANs, we aim to produce high-quality denoised images that are visually appealing and closer to the ground truth.

Formally, GANs are generative models that learn a mapping from a random noise vector $z$ to an output image $y$, $G:z\rightarrow y$ \cite{goodfellow2020generative}.

One particular use of GANs is the image-to-image task, which consists on transforming an image defined in a given domain to its counterpart in a different domain \cite{isola2017image}. This method is suitable for paired data and can provide high-resolution, high-quality transformations, therefore deeming it suitable for the denoising task previously introduced. For this work, however, we are not feeding our generator with a random noise vector, but with a noisy image that we want to denoise instead. This type of problem is typically addressed with \textit{conditional} GANs (cGANs), which learn a mapping from an observed image $x$ and a random noise vector $z$, to a ``transformed'' image $y$, $G:{x,z} \rightarrow y$.

In this work, we present a novel approach to few-shot microscopy image denoising combining both supervised and unsupervised learning techniques. Concretely, our method is based upon a conditional Generative Adversarial Network built upon the well-known U-Net architecture trained via contrastive learning \cite{khosla2020supervised}. Moreover, we add two loss terms to increase the quality of the denoised images, often outperforming the current state of the art in image denoising. Finally, we demonstrate that it is possible to significantly reduce the number of training samples while maintaining high performance. To the best of our knowledge, our work is the first to use contrastive learning for microscopy image restoration. 

Our contributions are two-fold: (1) we present a conditional GAN based framework for microscopy image denoising based upon the U-Net architecture trained via contrastive learning with total variation loss and structural similarity index loss that complement each other and produce the highest quality; and (2) we demonstrate the robustness of our method, showing that it can effectively operate with limited data, specifically $\sim$10\% of the initial data, while still retaining a significant portion of the initial performance or even surpassing it.

\section{Related work}
\label{sec:related_work}

\subsection{Classical methods.} In the past few decades, a variety of filtering approaches has been used to address the denoising problem. Together with classical methods such as bilateral filtering \cite{tomasi1998bilateral} or Wiener filtering \cite{ghael1997improved}, the most prominent ones are Non-Local Means \cite{buades2005non}, which uses information from similar regions in the image to compute estimates of the underlying clean pixel values, and BM3D\cite{dabov2007image}, that leverages a two stage non-local collaborative filtering. The reader can find a detailed discussion and comprehensive survey of classical denoising methods in \cite{milanfar2012tour}.

\subsection{Deep Learning (DL) based methods.} Recent years have proven deep learning methods more effective in the image denoising task. DL based methods work by directly learning a mapping from a noisy image to its clean counterpart. This task can be performed in two ways: supervised or unsupervised. As discussed in Section \ref{sec:intro}, each approach has its advantages and disadvantages.

Some of the best-known supervised methods include the work of \cite{zhang2017beyond}, where a Deep CNN is used to perform blind Gaussian denoising by learning the residual between the clean and the noisy images; or that of \cite{weigert2018content}, that implements a simple U-Net architecture to learn the mapping between clean and noisy images and establishes a new baseline in microscopy image denoising. GANs have also been recently used for denoising tasks with success. For example, \cite{su2018generative} implement a GAN as a tool to recover structural information from cryo-electron microscopy data; \cite{gu2021robust} that apply a $\beta$-GAN combining GANs and auto-encoders to achieve a robust estimate of certain distributional parameters under Huber contamination model with statistical optimality; or \cite{ouyang2020research}, which also use a GAN-based method with a modified discriminator that performs regression and helps to stabilize the training process. Denoising Optical Coherence Tomography is also a task where GANs have been widely used. For instance, \cite{huang2019simultaneous} leverage a generative adversarial network combined with a pixel loss and a content loss in order to simultaneously denoise and augment the resolution of OCT images. \cite{huang2020noise} implement a GAN model using a set of encoders and decoders to disentangle the content of OCT images from the speckle noise. \cite{fuentes2022mid3a} employ a conditional GAN combined with differentiable data augmentation and two structure preserving losses to denoise microscopy images with few data.

On the unsupervised side, most methods address pixel-noise removal exclusively. Some of them need to be trained for each input image separately, such as \cite{jo2021rethinking} or \cite{quan2020self2self}. Most methods, however, can be trained on a corpus of noisy images and used on new, never-seen images without retraining the model, such as \cite{batson2019noise2self, krull2019noise2void, krull2020probabilistic, laine2019high, xie2020noise2same}. Recently, the works of Broadus \textit{et al.} \cite{broaddus2020removing} and Prakash \textit{et al.}\cite{prakash2021removing} showed that it is possible to successfully remove structured noise to some extent. There are a variety of works using GANs in an unsupervised manner, such as \cite{chen2018image}, who employ the GAN learning framework to generate pairs of noisy and clean images and apply employ supervised denoising networks; or \cite{manakov2019noise}, who use a cycleGAN to learn a mapping between noisy and clean images without paired images, even though they still need clean and noisy images. Or the ``cycle-free'' cycleGAN defined by \cite{kwon2021cycle}, which uses an invertible generator to be able to remove the need for paired data. 

\section{Method}
\label{sec:method}

The proposed method is built upon a conditional Generative Adversarial Network (i.e. pix-2-pix) modified to include two additional structured loss functions (Structural Similarity Index and Total Variation losses) to help denoise the images and trained via contrastive learning, to help the model learn the characteristics of negative and positive examples even when dealing with few data.

\subsection{Pix-2-pix architecture} 
\label{ssec:pix2pix}

We base our method in the pix-2-pix architecture, a kind of conditional GAN designed for Image-to-Image tasks. This architecture consists of a generator built upon the basic ``U-Net'' \cite{ronneberger2015u} architecture and a discriminator implementing a ``PatchGAN'' architecture \cite{isola2017image}. The ``U-Net'' is an encoder-decoder architecture with skip connections between mirrored layers in the encoder and decoder stacks. The choice of ``PatchGAN'' by \cite{isola2017image} as the architecture for the discriminator is based on the well known fact that the L2 and L1 losses produce blurry results on image generation problems \cite{larsen2016autoencoding}. These losses generally capture low frequencies, but fail at the high frequencies. Thus, the ``PatchGAN'' architecture is engineered so that it only penalizes structure at the scale of patches, thus focusing in high-frequency structure and relying on an L1 term to force low-frequency correctness. Hence, the image is modeled as a Markov random field, in which pixels separated by more than a patch diameter are assumed independent. Therefore, the original pix-2-pix architecture combines the adversarial loss and an L1 term to produce sharper images.

Our choice of this architecture for our work is motivated by the fact that we wanted an as-simple-as-possible backbone. In this manner, we intend to show that a simple approach combined with the right loss and regularization terms can perform just as well (if not better) than more complicated approaches.

\subsection{Adversarial loss}
\label{ssec:adv_loss}

The adversarial term of the conditional GAN objective of the pix-2-pix architecture can be expressed as:

\begin{equation}
    \label{eq:gan_objective}
    \begin{split}
    \mathcal{L}_{GAN}(G,D) & = \mathbb{E}_{x,y}[\log D(x,y)] \\
    & + \mathbb{E}_{x,z}[\log (1-D(x,G(x,z)))]
    \end{split}
\end{equation}
% $$\mathcal{L}_{GAN}(G,D) = \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{x,z}[\log (1-D(x,G(x,z)))]$$

where $G$ tries to minimize this objective against an adversarial $D$ that tries to maximize it. After training this model, the trained generator model would be $G*=\argmin_{G} \max_{D} \mathcal{L}_{cGAN}$. 

However, for the pix-2-pix architecture, the authors chose to remove the random noise input since they found that the network was ignoring it. Instead, they add some stochasticity by including dropout on several layers of the generator both on training and testing time, but succeed only to a certain level. However, this is not a critical point for our purpose, since we only need a plausible mapping between noisy and clean images. The original pix-2-pix equation then becomes

\begin{equation}
    \label{eq:pix2pix_objective}
    \begin{split}
    \mathcal{L}_{GAN}(G,D) & = \mathbb{E}_{x,y}[\log D(x,y)]
    \\ & + \mathbb{E}_{x}[\log (1-D(x,G(x)))].
    \end{split}
\end{equation}
% $$\mathcal{L}_{GAN}(G,D) = \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{x}[\log (1-D(x,G(x)))].$$


\subsection{Pixel-level loss}
\label{ssec:l1_loss}

Pixel-level losses are widely used in image-to-image tasks to reduce the pixel-to-pixel difference between the initial and the generated images. In this case, we chose the L1-norm loss, which calculates the L1 norm of the difference between the denoised image $y$ and the original, noisy image $x$:

\begin{equation}
    \label{eq:l1_loss}
    \mathcal{L}_{L1}(x, y) = || y - x ||_1,
\end{equation}

where $x$ denotes the original, noisy image, and $y$ the synthesized, denoised counterpart.

\subsection{Structural Similarity loss}
\label{ssec:ssi_loss}

The previous L1 loss is not enough to produce sharp, denoised images. L1 loss assumes that pixels are independent of each other, whereas images are highly structured (i.e. ordering of the pixels carry important information about the content of an image). By making this assumption, it is possible to obtain the same L1 loss irrespective of the correlation between the original image and the generated one, even though this correlation can have a strong impact on perceptual similarity \cite{wang2009mean}.

To solve this issue, we chose to add a term based on the Structural Similarity (SSIM) index \cite{wang2004image} to our loss function. Contrary to L1 loss, the SSIM index provides a measure of the similarity by comparing two images based on luminance, contrast and structural similarity information.

Let define $l(x,y)=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2 + C_1}$ as the luminance similarity, where $\mu_x=\frac{1}{N}\sum^N_{i=1}x_i$ and $C_1$ is a constant; $c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2 + C_2}$ as the constrast similarity, where $\sigma_x=(\frac{1}{N-1}\sum^N_{i=1}(x_i - \mu_x)^2)^\frac{1}{2}$ and $C_2$ is a constant; and $s(x, y)=\frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3}$ as the structural information, where $\sigma_x=\frac{1}{N-1}\sum^N_{i=1}(x_i - \mu_x)(y_i - \mu_y)$ and $C_3$ is a constant. It is possible then to define the SSIM index as $SSIM(x,y)=l(x,y)\cdot c(x,y) \cdot s(x,y)$.

For convenience, we used the Structural dissimilarity for our optimization objective as defined in \cite{loza2006structural}:

\begin{equation}
    \label{eq:ssim_loss}
    \mathcal{L}_{SSIM}(x, y) = \frac{1-SSIM(x,y)}{2}.
\end{equation}

where the resulting value can vary between 0 (both images are the same) and 1 (images are very different).

\subsection{Total Variation Loss}
\label{ssec:tv_loss}

We already defined losses to take care of the structure of the image, but our problem is that these losses might neglect the existance of noise. For this purpose, we introduce a new loss term with the aim of reducing the variation of neighbouring pixels in the denoised image, which would most likely be caused by the presence of noise. 

The Total variation (TV) measure has been used for denoising during decades \cite{rudin1992nonlinear}. In the continuous domain, for a 1D function, TV computes an integral over the difference among neighboring values, leading to smoother outputs. Formally, the total variation of a differentiable function $f$, defined on an interval $[a,b] \subset \mathbb{R}$ is defined as $V^b_a=\int^b_a ||f'(x)dx$ if $f'$ is Riemann-integrable. For 1D discrete signals ($y=[Y_1, ..., y_N]$) can be defined as $TV(y)=\sum^{N-1}_{n=1} |y_{n+1}-y_n|$. 

For our purposes, since we are dealing with images, we employ the 2D TV as a term in out optimization objective:

\begin{equation}
    \label{eq:tv_loss}
    \mathcal{L}_{TV}(y) = \sum_{i,j}|y_{i+1,j}-y_{i,j}| + |y_{i,j+1}-y_{i,j}|.
\end{equation}

To simplify our optimization objective, we use the anisotropic version of TV, defined as the sum of horizontal and vertical gradients at each pixel.

\subsection{Contrastive Learning} 
\label{ssec:cl}

Contrastive learning \cite{chen2020simple} aims to learn a representation function $f(x)$ that maps input images $x$ to a latent feature space $\mathbb{R}^d$. Given a pair of augmented images $x_1$ and $x_2$, contrastive learning aims to bring their corresponding feature vectors $f(x_1)$ and $f(x_2)$ closer if they share the same identity, while pushing them further apart otherwise. This is achieved by minimizing a contrastive loss function that encourages the distance between the feature vectors to be small for positive pairs (images with the same identity) and large for negative pairs (images with different identities). 

Formally, let $x$ be an input image and $f(x)$ be its corresponding feature vector in $\mathbb{R}^d$. We can define the contrastive loss for a positive pair of examples $(i, j)$ as:

$$\ell_{i,j} = -\log\frac{\exp({sim(f(x_1), f(x_2))/\tau})}{\sum_{j=1}^{N}\mathbbm{1}_{[j\neq i]} \exp({sim(f(x_i),f(x_j))/\tau})}$$

where $sim(f(x_1), f(x_2))$ denotes the similarity between the feature vectors of $x_1$ and $x_2$, $\mathbbm{1}_{[j\neq i]}$ is an indicator function that is 1 if $j \neq i$ and 0 otherwise, $\tau$ is a temperature parameter that controls the smoothness of the distribution, and $N$ is the batch size, formed by $N/2$ examples and their augmented versions. The contrastive loss encourages the model to learn representations that group similar noisy images together and separate dissimilar noisy images apart. In the context of image denoising, this leads to an embedding space where similar noisy images (e.g., with the same type of noise) are close to each other, while dissimilar noisy images (e.g., with different types of noise) are far apart. By training a denoiser on top of the learned embeddings, the model can effectively denoise images even in scenarios where only a small amount of data is available.

\subsection{Full optimization objective}
\label{ssec:full_obj}

The full objective function of our denoising framework is finally defined as a weighted sum of all the losses from (\ref{eq:pix2pix_objective}) to (\ref{eq:tv_loss}):

\begin{equation}
    \label{eq:full_obj}
    \mathcal{L}=\lambda_{GAN}\mathcal{L}_{GAN} + \lambda_{L1}\mathcal{L}_{L1} + \lambda_{SSIM}\mathcal{L}_{SSIM} + \lambda_{TV}\mathcal{L}_{TV}
\end{equation}

where the weights of each loss was empirically set to balance their importance.

\section{Data, Experiments and Results}

\subsection{Implementation details}
\label{ssec:implementation}

We define a ``Unet-256'' consisting of 7 downsampling and 7 upsampling blocks. 
The input size is 256x256 pixels. The batch size for all the experiments is 32, except for the generalization experiments, when different number of batch size are used as specified. The optimizer used is Adam with learning rate set to $2e^{-4}$, and $\beta_1=0.5$ and $\beta_2=0.999$. The temperature parameter for contrastive learning is set to $\tau=0.1$. We trained the model for a total of 1000 epochs, with a linear decay scheduler for the last 500 epochs. Finally, the coefficients of the different loss terms (when used) are $\lambda_{GAN}=1$, $\lambda_{L1}=1$, $\lambda_{SSIM}=10$ and $\lambda_{TV}=1e^{-4}$. This combination of coefficients achieves the overall best performance. The code is publicly available at \href{https://github.com/ffuhu/clidim-microscopy-image-denoising}{https://github.com/ffuhu/clidim-microscopy-image-denoising}.

\subsection{Datasets and evaluation metrics}
\label{ssec:data}

\subsubsection{Datasets.} We employed the datasets acquired by Prakash \textit{et al.} \cite{prakash2020fully} publicly available. There are three datasets: $(i)$ \textit{Convallaria} data, consisting of 100 noisy images of the same \textit{Convallaria} section; $(ii)$ mouse \textit{skull nuclei} dataset, consisting of 200 noisy acquisitions of the same static mouse skull nuclei; and $(iii)$ mouse \textit{actin} data, consisting of 100 noisy realizations of the same static actin sample. In order to obtain the corresponding ground truth (i.e. clean counter-parts) we simply average all the samples.

\subsubsection{Evaluation metrics.} We employ three different measures in order to assess the quality of the denoising process: $(i)$ the Peak Signal-to-Noise Ratio (PSNR), $(ii)$ the Structural Similarity Index (SSIM) and $(iii)$ the Normalised Root Mean Squared Error (NRMSE).

\subsection{Experiments}
\label{ssec:experiments}

We design a set of experiments to test the effect of each feature included in our proposed method. The first experiment performed is considered as our method baseline, since we simply use the pix-2-pix architecture to denoise the images without any improvement. We then experiment adding TV loss, SSIM loss, contrastive learning, and the combination of all of them. 
The ablation study carried out show that training our model via contrastive learning is the adition that produces the highest increase in quality and helps with the generalization of the method. Finally, the TV and SSIM losses help to further increase the quality of the denoised images. Table \ref{tab:results_ns} shows the results for each dataset.


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table*}[!h]
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lllllllllll}
\hline
 &  & \multicolumn{3}{l}{\textit{\textbf{Convallaria}}} & \multicolumn{3}{l}{\textbf{Mouse \textit{actin}}} & \multicolumn{3}{l}{\textbf{Mouse \textit{skull nuclei}}} \\
\textbf{ns} & \textbf{experiment} & \textbf{PSNR} & \textbf{SSIM} & \textbf{NRMSE} & \textbf{PSNR} & \textbf{SSIM} & \textbf{NRMSE} & \textbf{PSNR} & \textbf{SSIM} & \textbf{NRMSE} \\ \hline
\multirow{5}{*}{\textbf{all}} & \textbf{baseline} & 35.59 & .9409 & .0023 & 32.11 & .8474 & .0015 & 40.70 & .9427 & .0057 \\
 & \textbf{TV} & 35.59 & .9390 & .0019 & 32.75 & .8583 & .0013 & 40.65 & .9425 & .0028 \\
 & \textbf{SSIM} & 35.70 & .9393 & .0014 & 32.76 & .8566 & .0009 & 40.71 & .9458 & .0049 \\ 
 & \textbf{CL} & 36.95 & .9556 & .0012 & \textbf{33.46} & \textbf{.8708} & \textbf{.0008} & 40.97 & .9521 & .0023 \\
 & \textbf{CL + TV + SSIM} & \underline{\textbf{37.04}} & \underline{\textbf{.9567}} & \underline{\textbf{.0010}} & 33.42 & .8734 & .0009 & \underline{\textbf{41.04}} & \underline{\textbf{.9532}} & \underline{\textbf{.0058}} \\ \hline
\multirow{3}{*}{\textbf{32}} & \textbf{baseline} & 35.69 & .9420 & .0023 & 32.55 & .8454 & .0009 & 40.81 & .9476 & .0030 \\
 & \textbf{CL} & 36.61 & .9531 & .0018 & 33.45 & .8718 & .0009 & 41.01 & .9528 & .0046 \\
 & \textbf{CL + TV + SSIM} & \textbf{36.92} & \textbf{.9564} & \textbf{.0016} & \textbf{33.45} & \textbf{.8733} & \textbf{.0007} & \textbf{41.02} & \textbf{.9533} & \textbf{.0057} \\ \hline
\multirow{3}{*}{\textbf{16}} & \textbf{baseline} & 36.60 & .9527 & .0014 & 32.54 & .8453 & .0007 & 40.69 & .9447 & .0017 \\
 & \textbf{CL} & 36.77 & .9554 & .0018 & 33.41 & .8710 & .0011 & 41.00 & .9526 & .0065 \\
 & \textbf{CL + TV + SSIM} & \textbf{36.96} & \textbf{.9566} & \textbf{.0013} & \underline{\textbf{33.49}} & \underline{\textbf{.8744}} & \underline{\textbf{.0010}} & 41.01 & .9529 & .0048 \\ \hline
\end{tabular}%
}
\caption{\textbf{Quantitative denoising results for our ablation test}. We compare the contribution of each loss to the performance of the model and how the amount of training data affects. Best values are highlighted in \textbf{bold} for experiments done with each amount of data (all, 32 and 16 samples). Best overall values are \underline{underlined}.}
\label{tab:results_ns}
\end{table*}


We also include some samples to qualitatively evaluate the performance of the method (Figure \ref{fig:qualitative_results}).



\begin{figure*}
     \centering % Convallaria
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_A.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_CARE_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Mouse actin
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_A.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_CARE_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Mouse skull nuclei
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_A.png}
        \caption{Noisy input}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_A_crop_72_200_32_200.png}
         \caption{Input crop}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_CARE_B_crop_72_200_32_200.png}
        \caption{CARE}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_72_200_32_200.png}
        \caption{Our method}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_B_crop_72_200_32_200.png}
        \caption{Ground truth}
        %  \label{fig:conv_ssim}
     \end{subfigure}
    %  \newline
     
    \caption{\textbf{Qualitative denoising results.} We compare the performance of our method against the current state-of-the-art method (CARE). Best viewed on a computer display.}
    \label{fig:qualitative_results}
\end{figure*}

Table \ref{tab:results_sota} shows our proposed method's performance in terms of PSNR compared to some of the current SOTA methods.
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table*}[!h]
\centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\hline
 & \multicolumn{4}{l}{\textbf{Unsupervised}} & \multicolumn{3}{l}{\textbf{Supervised}} \\
\textbf{Dataset} & \textbf{N2V} & \textbf{Vanilla VAE} & \textbf{DivNoising} & \textbf{PN2V} & \textbf{CARE} & \textbf{MID3A} & \textbf{Our method} \\ \hline
\textbf{\textit{Convallaria}} & 35.73 & 36.57 & 36.78 & 36.70 & 36.71 & \textbf{37.07} & 37.04 \\
\textbf{Mouse \textit{actin}} & 33.39 & 33.46 & 33.82 & 33.86 & \textbf{34.20} & 33.45 & 33.46 \\
\textbf{Mouse \textit{skull nuclei}} & 35.84 & 35.84 & 36.05 & 36.35 & 36.58 & 37.04 & \textbf{41.04} \\ \hline
\end{tabular}%
}
\caption{\textbf{Quantitative comparison with state-of-the-art methods in terms of PSNR measure.} Best values are highlighted in bold for each configuration and dataset.}
\label{tab:results_sota}
\end{table*}

In addition, we perform additional experiments to find out how well our method can generalise with few samples. To test it, we successively reduce the number of traininig examples to 32 and 16 samples, as shown in Table \ref{tab:results_ns}. The original total amount of training examples are 100 for \textit{Convallaria}, 100 for mouse actin and 200 for mouse skull nuclei, as described in Section \ref{ssec:data}. Figures \ref{fig:qualitative_results_ns_conv}, \ref{fig:qualitative_results_ns_ma}, and \ref{fig:qualitative_results_ns_msn} show a qualitative comparison of the denoising method with the three best configuration and varying number of training instances for \textit{Convallaria}, mouse actin and mouse skull nuclei datasets, respectively.


% FIGURA COMPARANDO DENOISING CON ns variando - Convallaria
\begin{figure*}
     \centering % Convallaria - baseline
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_ns32_baseline_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_ns16_baseline_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Convallaria - CL
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_CLreg0_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_ns32_CLreg0_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_ns16_CLreg0_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Convallaria - CL TV SSIM
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        \caption{Input crop}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
         \caption{All training samples}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_ns32_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
        \caption{32 training samples}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_ns16_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
        \caption{16 training samples}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Convallaria_diaphragm_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        \caption{Ground truth}
        %  \label{fig:conv_ssim}
     \end{subfigure}
    %  \newline
     
    \caption{\textbf{Generalization study for the \textit{Convallaria} dataset}. First row shows the baseline configuration of our method, second row the version with contrastive learning, and the third row the complete proposed method. Best viewed on a computer display.}
    \label{fig:qualitative_results_ns_conv}
\end{figure*}

% FIGURA COMPARANDO DENOISING CON ns variando - Mouse actin
\begin{figure*}
     \centering % Mouse actin - baseline
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_ns32_baseline_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_ns16_baseline_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Mouse actin - CL
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_CLreg0_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_ns32_CLreg0_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_ns16_CLreg0_seed0_0_0_fake_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Mouse actin - CL TV SSIM
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_A_crop_200_328_160_328.png}
        \caption{Input crop}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
         \caption{All training samples}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_ns32_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
        \caption{32 training samples}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_ns16_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_200_328_160_328.png}
        \caption{16 training samples}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_actin_baseline_seed0_0_0_real_B_crop_200_328_160_328.png}
        \caption{Ground truth}
        %  \label{fig:conv_ssim}
     \end{subfigure}
    %  \newline
     
    \caption{\textbf{Generalization study for the mouse \textit{actin} dataset}. First row shows the baseline configuration of our method, second row the version with contrastive learning, and the third row the complete proposed method. Best viewed on a computer display.}
    \label{fig:qualitative_results_ns_ma}
\end{figure*}

% FIGURA COMPARANDO DENOISING CON ns variando - Mouse skull nuclei
\begin{figure*}
     \centering % Mouse skull nuclei - baseline
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_A_crop_72_200_32_200.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_fake_B_crop_72_200_32_200.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_ns32_baseline_seed0_0_0_fake_B_crop_72_200_32_200.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_ns16_baseline_seed0_0_0_fake_B_crop_72_200_32_200.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_B_crop_72_200_32_200.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Mouse skull nuclei - CL
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_A_crop_72_200_32_200.png}
        %  \caption{$y=x$}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_CLreg0_seed0_0_0_fake_B_crop_72_200_32_200.png}
        %  \caption{$y=3sinx$}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_ns32_CLreg0_seed0_0_0_fake_B_crop_72_200_32_200.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_ns16_CLreg0_seed0_0_0_fake_B_crop_72_200_32_200.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_B_crop_72_200_32_200.png}
        %  \caption{$y=5/x$}
        %  \label{fig:conv_ssim}
     \end{subfigure}
     
     \par \smallskip
     
     \centering % Mouse skull nuclei - CL TV SSIM
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_A_crop_72_200_32_200.png}
        \caption{Input crop}
        %  \label{fig:conv_in_full}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_72_200_32_200.png}
         \caption{All training samples}
        %  \label{fig:conv_bl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_ns32_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_72_200_32_200.png}
        \caption{32 training samples}
        %  \label{fig:conv_cl}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_ns16_CLreg0_TV1e-4_SSIM10_seed0_0_0_fake_B_crop_72_200_32_200.png}
        \caption{16 training samples}
        %  \label{fig:conv_tv}
     \end{subfigure}
    %  \hfill
     \begin{subfigure}[b]{0.19\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/Mouse_skull_nuclei_baseline_seed0_0_0_real_B_crop_72_200_32_200.png}
        \caption{Ground truth}
        %  \label{fig:conv_ssim}
     \end{subfigure}
    %  \newline
     
    \caption{\textbf{Generalization study for the mouse \textit{skull nuclei} dataset}. First row shows the baseline configuration of our method, second row the version with contrastive learning, and the third row the complete proposed method. Best viewed on a computer display.}
    \label{fig:qualitative_results_ns_msn}
\end{figure*}

\section{Conclusion}
\label{sec:discussion}

We have introduced a highly competitive generative method for few-shot microscopy image denoising with a great ability to generalize thanks to the use of contrastive learning (see Table \ref{tab:results_sota}).

Our approach combines two well-known methods in image restoration, namely pix2pix and contrastive learning \cite{chen2020simple}. Our experiments demonstrate that this combination leads to significant improvements in the quality of denoised microscopy images even when working with limited amounts of data. In addition, we introduce two loss functions that further enhance the structural appearance of the denoised images, resulting in even better performance. The use of contrastive learning is particularly valuable for scenarios with limited data, where it allows the method to adapt effectively (see Table \ref{tab:results_ns}).

Our ultimate goal is not only to produce high-quality denoised images, but also to enable downstream processing, such as cell counting or segmentation, which is critical for many applications in the biomedical field. Although we focus on per-pixel noise in this work, we believe that our method could potentially handle more complex image degradation models, such as structured noise, blur, or compression artifacts. However, this remains a topic for future research.

Despite its promising results, our method does not improve the state-of-the-art performance on the mouse actin dataset. We hypothesize that this may be due to the high-frequency content of the images, which may not be well-reproduced by the pix2pix architecture. We also acknowledge that newer methods, such as SPADE \cite{park2019semantic}, CC-FPSE \cite{liu2019learning}, and LC-GAN \cite{tang2020local}, outperform pix2pix and may be more suitable for this task. Other methods based in diffusion models or transformers will also be explored in future work, since they have shown remarkable performance in image restoration tasks \cite{wang2022uformer, ali2023vision, zamir2022restormer}.

Finally, while our method requires only a limited amount of paired data, it is still supervised. We aim to make it fully unsupervised in the future or at least remove the need for paired data without diminishing its performance.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.

\section*{Conflict of interest}

The Author(s) declare(s) that there is no conflict of interest.

% use section* for acknowledgment
\section*{Acknowledgment}

This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme, France BioImaging infrastructure (ANR-10-INSB-04), the ''programme investissement d’avenir`` (ANR-10-IDEX-03-02) and the CNRS. Computational resources and infrastructure used in present publication were provided by the Bordeaux Bioinformatics Center (CBiB).

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
% \bibliography{IEEEabrv, mybib}
\bibliography{IEEEabrv, mybibfile}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiographynophoto}{Felix Fuentes-Hurtado}
% Biography text here.
% \end{IEEEbiographynophoto}

% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{Jean-Baptiste Sibarita}
% Biography text here.
% \end{IEEEbiographynophoto}

% % insert where needed to balance the two columns on the last page with
% % biographies
% %\newpage

% \begin{IEEEbiographynophoto}{Virgile Viasnoff}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


