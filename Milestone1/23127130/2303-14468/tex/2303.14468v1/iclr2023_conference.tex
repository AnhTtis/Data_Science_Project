\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{thm-restate}

\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{caption}
\usepackage{tikz,pgfplots}
\usepackage{tikz-3dplot}
\usepackage{afterpage}
\usepackage{placeins}
\usepackage{multicol}
\usepackage{lipsum}
\usetikzlibrary{shapes,calc,positioning}
\usetikzlibrary{shapes.callouts}
\pgfplotsset{compat=1.10}
\captionsetup[figure]{font={stretch=1.0,small}}
\captionsetup[table]{font={stretch=1.0,small}}

% Remove the extra spacing introduced by `\paragraph`.
\renewcommand{\paragraph}[1]{\textbf{#1.}}
\newcommand{\shortminus}{\scalebox{0.7}[1]{$-$}}

\title{Autoregressive Conditional Neural \\ Processes}

\author{\hspace{0.025\linewidth}\begin{minipage}{.94\linewidth}
\vspace{12pt}  % Make spacing look balanced.
\centering\normalfont
\textbf{Wessel P.\ Bruinsma}${}^{*12}$,
\textbf{Stratis Markou}${}^{*2}$,
\textbf{James Requiema}${}^{*2}$,
\textbf{Andrew Y.\ K.\ Foong}${}^{*1}$,
\textbf{Tom R. Andersson}${}^{3}$,
\textbf{Anna Vaughan}${}^{2}$,
\textbf{Anthony Buonomo}${}^{2}$,
\textbf{J.\ Scott Hosking}${}^{34}$, \\
\textbf{Richard E.\ Turner}${}^{12}$ \\[.5em]
${}^*$Equal contribution \\[.5em]
${}^1$Microsoft Research AI4Science,
${}^2$University of Cambridge, \\
${}^3$British Antarctic Survey,
${}^4$The Alan Turing Institute
\\[.5em]
\texttt{\{wbruinsma,andrewfoong\}@microsoft.com} \\
\texttt{\{em626,jrr41,av555,ab2707,ret26\}@cam.ac.uk} \\
\texttt{\{tomand,jask\}@bas.ac.uk}
\end{minipage}\vspace{-15pt}}


\input{preamble}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}

\maketitle

\begin{abstract}
    Conditional neural processes \citep[CNPs;][]{Garnelo:2018:Conditional_Neural_Processes} are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure.
    %
    Although CNPs have many advantages, they are unable to model dependencies in their predictions.
    %
    Various works propose solutions to this, but these come at the cost of either requiring approximations or being limited to Gaussian predictions.
    %
    In this work, we instead propose to change how CNPs are deployed at test time, \emph{without any modifications to the model or training procedure}.
    %
    Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature.
    %
    We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions.
    %
    Perhaps surprisingly, in an extensive range of tasks with synthetic and real data, we show that CNPs in autoregressive (AR) mode not only significantly outperform non-AR CNPs, but are also competitive with more sophisticated models that are significantly more expensive and challenging to train.
    %
    This performance is remarkable since AR CNPs are not trained to model joint dependencies.
    %
    Our work provides an example of how ideas from neural distribution estimation can benefit neural processes, motivating research into the AR deployment of other neural process models.
\end{abstract}


\vspace{-0.3cm}
\section{Introduction} \label{sec:intro}
\vspace{-0.2cm}

\begin{figure}[b!]
    \centering
    \vspace{-0.6cm}
    \includegraphics[width=0.49\linewidth]{figures/sawtooth-noisy-samples-last.pdf}
    \includegraphics[width=0.49\linewidth]{figures/sawtooth-ar-samples-last.pdf}
    \caption{A ConvCNP trained on random sawtooth functions and applied in standard mode (\textit{left}) and in our proposed autoregressive (AR) mode (\textit{right}).
    The black crosses denote observed data points, the blue lines show model samples, and the bottom plots show the marginal predictive distributions at the locations marked by the dashed vertical lines.
    In standard mode, the CNP models each output with an independent Gaussian (\textit{left}).
    However, when run in AR mode, the \emph{same} CNP can produce coherent samples and model multimodality (\textit{right}).}
    \label{fig:mf-vs-ar}
\end{figure}

Conditional neural processes \cite[CNPs;][]{Garnelo:2018:Conditional_Neural_Processes} are a family of meta-learning models which combine the flexibility of deep learning with the uncertainty awareness of probabilistic models.
%
They are trained to produce well-calibrated predictions via a simple maximum-likelihood procedure, and naturally handle off-the-grid and missing data, making them ideally suited for tasks in climate science and healthcare.
%
Since their introduction, attentive \citep[ACNP;][]{Kim:2019:Attentive_Neural_Processes} and convolutional \citep[ConvCNP;][]{Gordon:2020:Convolutional_Conditional_Neural_Processes} variants have also been proposed.
%
Unfortunately, existing CNPs do not model statistical dependencies (\cref{fig:mf-vs-ar}; left).
%
This harms their predictive performance and makes it impossible to draw coherent function samples, which are necessary in downstream estimation tasks \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}.
%
Various approaches have been proposed to address this.
%
\citet{Garnelo:2018:Neural_Processes} introduced the latent neural process (LNP), which uses a latent variable to induce dependencies and model non-Gaussianity.
% 
However, this renders the likelihood intractable, necessitating approximate inference.
%
Another approach
is the fully convolutional Gaussian neural process \citep[FullConvGNP;][]{Bruinsma:2021:The_Gaussian_Neural_Process},
which maintains tractability at the cost of only allowing Gaussian predictions.
%
It uses a neural network to define the mean and covariance function of a predictive Gaussian process (GP) that models dependencies.
%
However, it uses a much more complex architecture and is only practically applicable to problems with one-dimensional inputs, limiting its adoption compared to the more lightweight CNP.
%
Recently, \citet{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable} proposed the Gaussian neural process (GNP), which is considerably simpler but sacrifices performance relative to the FullConvGNP.

\begin{table}
    \vspace{-0.4cm}
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        Class & Consistent & Dependencies & Non-Gaussian & Exact Training \\ \midrule
        AR CNPs (this work) & \bad & \good & \good & \good \\
        CNPs \citep{Garnelo:2018:Conditional_Neural_Processes} & \good & \bad & \good & \good \\
        GNPs \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable} & \good & \good & \bad & \good \\
        LNPs \citep{Garnelo:2018:Neural_Processes} & \good & \good & \good & \bad \\
        \bottomrule
    \end{tabular}
    \vspace{-0.1cm}
    \caption[
        Comparison of various classes of neural processes
    ]{
        Comparison of various classes of neural processes. 
        Shows whether a class
        produces consistent predictions,
        models dependencies,
        can produce non-Gaussian predictions, and
        can be trained without approximating the objective function.
        For CNPs, even though the presentation by \citet{Garnelo:2018:Conditional_Neural_Processes} assumes Gaussian predictions, it is simple to relax this Gaussianity assumption;
        this is not the case for GNPs.
    } \label{tab:comparison_of_neural_process_approaches}
    \vspace{-0.8cm}
\end{table}

\vspace{-0.1cm}
In this paper we propose a much simpler method for modelling dependencies with neural processes that has been largely overlooked: autoregressive (AR) sampling.
%
AR sampling requires \emph{no changes} to the architecture \emph{or} training procedure. % of CNPs.
%
Instead, we \emph{change how the CNP is deployed at test time}, extracting predictive information that would ordinarily be ignored.
%
Instead of making predictions at all target points simultaneously, we autoregressively feed samples back into the model.
%
AR CNPs trade the fundamental property of \emph{consistency under marginalisation and permutation}, which is foundational to many neural process models, for non-Gaussian and correlated predictions.
% 
In \cref{tab:comparison_of_neural_process_approaches} we place AR CNPs within the framework of other neural process models.
Our key contributions are:
\vspace{-0.2cm}
\begin{itemize}
    \item We show that CNPs used in AR mode capture rich, non-Gaussian predictive distributions and produce coherent samples (\cref{fig:mf-vs-ar}). 
    %
    This is remarkable, since these CNPs have Gaussian likelihoods, are not trained to model joint dependencies or non-Gaussianity, and are significantly cheaper to train than LNPs and FullConvGNPs (\cref{fig:timing}).
    \vspace{0.08cm}
    \item
    We prove that, given sufficient data and model capacity, the performance of AR CNPs is at least as good as that of GNPs, which explicitly model correlations in their predictions.
    \vspace{0.08cm}
    \item 
    Viewing AR CNPs as a type of neural density estimator \citep{Uria:2016:Neural_Autoregressive_Distribution_Estimation}, we highlight their connections to a range of existing methods in the deep generative modelling literature.
    \vspace{0.08cm}
    \item
    In an extensive range of Gaussian and non-Gaussian regression tasks, we show that AR CNPs are consistently competitive with, and often significantly outperform, all other neural process models in terms of predictive log-likelihood.
    \vspace{0.08cm}
    \item
    We deploy AR CNPs on a range of tasks involving real-world climate data.
    To handle the high-resolution data in a computationally tractable manner, we introduce a novel multi-scale architecture for ConvCNPs.
    We also combine AR ConvCNPs with a beta-categorical mixture likelihood, producing strong results compared to other neural processes.
\end{itemize}
\vspace{-0.2cm}
Our work represents a promising first application of this procedure to the simplest class of neural processes, and motivates future work on applications of AR sampling to other neural process models.

\vspace{-0.3cm}
\section{Autoregressive Conditional Neural Processes} \label{sec:ARCNP}
\vspace{-0.3cm}
\paragraph{Meta-learning}
We first define the problem setup.
Let $\X$ be a compact input space and let $\Y$ be the output space.
Let $\D_N = (\X \times \Y)^N$ be the collection of all sets of $N$ input--output pairs,
and let $\D = \smash{\union_{N=0}^\infty} \D_N$.
We call elements $D \in \D$ \textit{data sets} and
denote $D = (\vx, \vy)$ where $\vx \in \X^N$, $\vy \in \Y^N$ are the inputs and outputs respectively.
In meta-learning we are given a collection of data sets $(D_m)_{m=1}^M$, called a \emph{meta--data set}, with the individual data sets $D_m$ called \emph{tasks} \citep{Vinyals:2016:Matching_Networks_for_One_Shot}.
Every task $D_m$ is split up $D_m = D_m\us{(c)} \cup D_m\us{(t)}$ into a
\emph{context set} $D_m\us{(c)} = (\vx_m\us{(c)}, \vy_m\us{(c)})$ and a \emph{target set} $D_m\us{(t)} = (\vx_m\us{(t)}, \vy_m\us{(t)})$. \index{context set}\index{target set}
Here $\vx\us{(c)}_m$ are called the \emph{context inputs},
$\vy\us{(c)}_m$ the \emph{context outputs},
$\vx\us{(t)}_m$ the \emph{target inputs}, and
$\vy\us{(t)}_m$ the \emph{target outputs}.
Our goal is to devise an algorithm which takes in a context set $D_m\us{(c)}$ and produces the best possible prediction for the target outputs $\vy\us{(t)}_m$ given target inputs $\vx\us{(t)}_m$.

\paragraph{Neural processes}
Let $\Pc$ be the set of all $\Y$-valued stochastic processes on $\X$.
Neural processes (NPs) directly and flexibly parametrise a map $\pi_\theta\colon \D \to \Qc$ where $\Qc \sub \Pc$ and where $\theta$ are learnable parameters.
CNPs set $\Qc$ to be the collection of GPs $f$ such that $\cov(f(x), f(y))=0$ for $x \neq y$.
%
GNPs
let $\Qc$ be the collection of continuous GPs.
%
{Latent NPs} \citep[LNPs;][]{Garnelo:2018:Neural_Processes} let $\Qc$ be a collection of non-Gaussian processes by making use of a latent variable.
%
Let $\smash{P_{\vx_m\us{(t)}} \pi(D_m\us{(c)})}$ denote the finite-dimensional distribution of the process $\pi(D_m\us{(c)})$ evaluated at inputs $\vx_m\us{(t)}$, and denote its density by $q_\theta(\vardot \cond \vx\us{(t)}_m, D\us{(c)}_m)$.
%
To learn the parameters $\theta$, NPs seek to maximise
\begin{equation} \textstyle \label{eq:neural-process-objective}
    \mathcal{L}_M(\pi) = \frac1M \sum_{m=1}^M \log q_\theta(\vy\us{(t)}_m \cond \vx\us{(t)}_m, D\us{(c)}_m).
\end{equation}
%
For CNPs and GNPs, $\mathcal{L}_M$ can be computed exactly, since $q_\theta$ is Gaussian.\footnote{
Unless otherwise specified, we assume CNPs use Gaussian likelihoods, as in \citet{Garnelo:2018:Conditional_Neural_Processes}.
However, it is straightforward to modify them to use non-Gaussian likelihoods, as we do in \cref{sec:environmental-modelling}.
}.
However, for LNPs, $\mathcal{L}_M$ must be approximated \citep{Garnelo:2018:Neural_Processes,Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction}, typically impacting performance.

\paragraph{Autoregressive CNPs}
    Our proposal is to take an existing CNP and run it in an autoregressive fashion, feeding predictions for earlier outputs back into the model.
    Inspired by the product rule, we define the joint predictive as a product of conditionals, modelling each conditional with a CNP.
    For example, in the case of three target points, 
    \smash{$
        q_\theta(y_3\us{(t)} \cond \vy_{1:2}\us{(t)}, D\us{(c)}_m)
        q_\theta(y_2\us{(t)} \cond y_1\us{(t)}, D\us{(c)}_m)
        q_\theta(y_1\us{(t)} \cond D\us{(c)}_m)
    $}.
To enable a theoretical analysis of this procedure, we now  proceed to set up more formal notation.
Suppose that $\pi_\theta \colon \D \to \Qc$ is an NP, and we wish to predict at some target inputs $\vx\us{(t)}$ given a context set $D\us{(c)}$.
Standard NPs would output the predictive $P_{\vx\us{(t)}} \pi_\theta(D\us{(c)})$ which, for CNPs, would be a factorised Gaussian.
We propose to instead roll out the NP autoregressively, as described in Proc.\ \ref{proc:ar}.

\begin{figure}[t]
    \vspace{-0.4cm}
    \begin{minipage}[t]{.47\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/timing.pdf}
        \vspace{-0.5cm}
        \caption{Negative log-likelihoods on non-Gaussian sawtooth data. %regression task.
        Deploying the ConvCNP in AR mode dramatically improves performance, and outperforms state-of-the-art NPs with Gaussian (FullConvGNP) and non-Gaussian (ConvLNP) predictive distributions, at a fraction of the training cost.}
        \label{fig:timing}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.51\textwidth}
        \centering
        \raisebox{1pt}{\includegraphics[width=0.94\linewidth]{figures/mm_pred_map2.pdf}}
        \vspace{-0.1cm}
        \caption{
            \textit{Top:}
            generative process: mixture model of three deterministic functions with additive Gaussian noise.
            \textit{Bottom:} at the four target locations indicated by dashed lines, the panes show the \textcolor{tab:blue}{true
            distribution} and predictions by the \textcolor{tab:purple}{ideal CNP} 
            and the \textcolor{tab:orange}{ideal CNP applied autoregressively} at the targets from left to right.
            See \cref{app:analysis-ar-cnps-gaussian,app:details-fig-mm}.
        }
        \label{fig:mm_pred}
    \end{minipage}
    \vspace{-7mm}
\end{figure}

\newcommand{\AR}{\operatorname{AR}}
\begin{procedure}[Autoregressive application of neural process]
    \label{proc:ar}
    For a neural process $\pi_\theta$, context set $D\us{(c)}=(\vx\us{(c)}, \vy\us{(c)})$, and target inputs $\vx\us{(t)}$,
    let $\AR_{\vx\us{(t)}}(\pi_\theta, D\us{(c)})$ be the distribution defined as follows:
    \vspace{-1mm}
    \begin{equation} \textstyle
        \text{for $i = 1, \ldots, N$,}\quad
        y\us{(t)}_i \sim P_{x\us{(t)}_i} \pi_\theta(\vx\us{(c)} \oplus \vx\us{(t)}_{1:(i-1)},\,\vy\us{(c)} \oplus \vy\us{(t)}_{1:(i-1)}),
        \label{eq:ar}
        \vspace{-1mm}
    \end{equation}
    where $\va \oplus \vb$ concatenates two vectors $\va$ and $\vb$.
        See \cref{fig:ar-illustration} in \cref{app:ar-illustration}
        for an illustration.
\end{procedure}
\vspace{-.5\baselineskip}  % Spacing is weird.
%
Since earlier samples $y\us{(t)}_i$ feed back into later applications of $\pi_\theta$, the whole sample $\vy\us{(t)}$ is correlated, \emph{even if $\pi_\theta$ does not model dependencies between target outputs}, as with CNPs.
%
At test time, when evaluating the corresponding the density $\smash{q_\theta^{\mathrm{(AR)}}}$ of $\AR_{\vx\us{(t)}}(\pi_\theta, D\us{(c)})$ at $\vy\us{(t)}$, we use the formula
%
\vspace{-1mm}
\begin{equation} \textstyle
    \log q_\theta^{\mathrm{(AR)}} (\vy\us{(t)} \cond \vx\us{(t)}, D\us{(c)}) = \sum_{i=1}^N \log q_\theta (y\us{(t)}_i \cond  x\us{(t)}_i, D\us{(c)} \oplus (\vx\us{(t)}_{1:(i-1)},\vy\us{(t)}_{1:(i-1)})).
\end{equation}
%
Whilst any NP can be used in AR, we focus on CNPs as they are the computationally cheapest class.

\paragraph{Understanding the infinite data limit}
To better understand why AR CNPs successfully model dependencies, we analyse the idealised case of infinite data and model capacity.
%
Let $p(f)$ be the law of the data-generating stochastic process, and let $p(\varepsilon)$ be the law of a stochastic process representing observation noise, defined by letting $\varepsilon(\vx)$ be a vector of i.i.d.\ noise variables for all $\vx$.
%
We assume
\vspace{-1mm}
\begin{equation} \label{eq:generative-model}
    \vy_m\us{(c)}
    = y_m(\vx\us{(c)}_m) \quad \text{and} \quad
    \vy_m\us{(t)}
    = y_m(\vx\us{(t)}_m) \quad \text{where} \quad
    y_m(\vardot) = f_m(\vardot) + \varepsilon_m(\vardot),
\end{equation}
$(f_m)_{m=1}^M$ are i.i.d.\ draws from $p(f)$, and $(\varepsilon_m)_{m=1}^M$ are i.i.d.\ draws from $p(\varepsilon)$.
%
Define the \emph{prediction map} $\pi_y\colon \D \to \Pc$ as the mapping from a data set to the posterior over $y$, $\pi_y(D) = p(y \cond D)$.
Then $\mathcal{L}_M$ is a Monte Carlo approximation of the following infinite-sample objective \citep{Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction}:
\vspace{-1mm}
\begin{equation}
    \L_\infty(\pi)
    = -\E_{p(D\us{(c)}) p(\vx\us{(t)})}[\KL(P_{\vx\us{(t)}}\pi_y(D\us{(c)}), P_{\vx\us{(t)}} \pi(D\us{(c)}))] + \mathrm{const.}
\end{equation}
\newpage~\\[-2em]
Under appropriate regularity assumptions, $\L_\infty(\pi)$ is maximised over all $\pi$ when the expected KL divergence term is zero, which occurs if and only if $\pi = \pi_y$.
%
In practice, NPs do not maximise $\L_\infty(\pi)$ over all $\pi$, but (i) use a finite-sized meta--data set and (ii) restrict $\Qc \sub \Pc$:
%
\begin{align} \label{eq:ideal-np-approx}
    \overbracket[1pt]{\pi_M \in \argmax_{\mathclap{\text{all }\pi \colon \D \to \Qc}}\,\mathcal{L}_M (\pi)}^{\textup{what we compute in practice}} \,\, \xrightarrow[M \to \infty]{\text{(i)}} \,\, \overbracket[1pt]{\pi_\infty \in \argmax_{\mathclap{\text{all }\pi \colon \D \to \Qc}} \,\mathcal{L}_\infty (\pi)}^{\textup{ideal NP}} \,\, \xrightarrow[\Qc \to \Pc]{\text{(ii)}} \,\, \overbracket[1pt]{\pi_y = \argmax_{\mathclap{\text{all }\pi \colon \D \to \Pc}} \,\L_\infty(\pi)}^{\textup{exact prediction map}}
\end{align}
Here $\pi_M$ is an NP trained on the practical objective \eqref{eq:neural-process-objective}, which, in the limit of infinite data,
approximates the so-called \emph{ideal NP} $\pi_\infty$.
The ideal NP depends on the choice of $\Qc$, \textit{i.e.}~the class of NPs under
consideration, and, in turn, approximates $\pi_y$.
For CNPs and GNPs, using the fact that minimising $\KL(p, q)$ over $q$ matches moments \citep{Minka:2001:Expectation_Propagation_for_Approximate_Bayesian}, we can readily determine and even practically compute the ideal NP for these two classes of NPs.
%
The \emph{ideal CNP} predicts a diagonal-covariance GP whose mean function and \emph{marginal variance function} match $\pi_y$: $\pi_\infty(D) = \GP(m,k)$ where $m(x) = \E[ y(x) \cond D ]$, and $k(x, x’) = \Var[y(x) \cond D]$ if $x = x’$ and $k(x, x’) = 0$ otherwise.
%
On the other hand, the \emph{ideal GNP} predicts a GP whose mean function and \emph{full covariance function} match $\pi_y$:
$\pi_\infty(D) = \GP(m,k)$ where
$m(x) = \E[ y(x) \cond D ]$, $ k(x, x') = \cov( y(x), y(x') \cond D) $.
%
The main result of this subsection is that the ideal CNP, despite not modelling correlations, becomes superior to the ideal GNP \emph{when deployed in AR mode}:
%
\begin{restatable}[Advantage of AR CNPs over GNPs]{proposition}{propadvantage} \label{prop:arcnp-advantage}
    Assume appropriate regularity conditions on $y$.
    Let $\pi\ss{C}$ be the ideal CNP
    and let $\pi\ss{G}$ be the ideal GNP.
    Then, for all inputs $\vx$ and data sets $D \in \D$,
    \begin{equation}
        \KL(P_\vx \pi_y(D), \operatorname{AR}_\vx(\pi\ss{C}, D))
            \le \KL(P_\vx \pi_y(D), P_\vx \pi\ss{G}(D)).
    \end{equation}
\end{restatable}
\vspace{-0.5em}
We provide a proof in \cref{app:proof-advantage}.
%
Intuitively, the advantage of AR CNPs comes from their ability to model non-Gaussian dependencies.
%
\cref{prop:arcnp-advantage} shows that to outperform the GNP, it suffices to train a CNP to model the \emph{marginals} of $\pi_y$, and rely on the AR procedure to induce dependencies. A visualisation of the ideal CNP and the ideal CNP applied autoregressively can be seen in \cref{fig:mm_pred}.

\paragraph{Consistency and the AR design space}
%
As shown in \cref{tab:comparison_of_neural_process_approaches}, AR CNPs give up the fundamental property of \emph{consistency}, since the distributions $\set{\AR_{\vx}(\pi_\theta, D\us{(c)}_m) : \vx \in \X^N, \, N \in \N}$ are not consistent under permutation or marginalisation: permuting $\vx$ and introducing or marginalising target points can change the distribution.
%
This violates the conditions of the Kolmogorov extension theorem \citep{oksendal2013stochastic}, preventing the distributions from defining a consistent stochastic process.
%
There is thus a large design space involved when deploying AR CNPs, where choices that have no effect on the predictions of other NPs can now significantly affect performance.

One such choice is how many points to sample at a time.
%
Sampling one at a time induces dependencies between all points, but requires $N$ forward passes.
%
Alternatively, 
we could divide the $N$ inputs in $\vx\us{(t)}$ into blocks of $K$ points each, and sample each block with a single CNP forward pass.
%
This requires $N / K$ forward passes, with points in the same block conditionally independent.
%
If $K = N$, this is the standard CNP prediction; and if $K=1$, we recover \cref{proc:ar}.
%
This provides a knob for practitioners to trade off between faster, consistent, but less expressive standard CNP predictions; and slower, less consistent, but more expressive AR predictions.
%
In this paper, we use full AR mode with $K=1$, and leave an investigation of block AR sampling to future work.

\paragraph{Obtaining smooth samples}
Due to the lack of consistency in AR mode, the spacing chosen between target points can significantly affect performance.
%
For example, care must be taken so the number of target points is not much greater than the size of the context sets seen during train time, to avoid confronting the model with an out-of-distribution context set size at test time.
%
This raises the question of how to sample functions on a very fine grid.
%
Furthermore, since CNPs do not differentiate between epistemic and aleatoric uncertainty, it is not clear how to obtain smooth, noiseless samples, that is, samples for $f$ uncorrupted by the i.i.d.~noise $\varepsilon$ in \eqref{eq:generative-model}.
%
The following proposition shows that, for a smooth sample corrupted by additive noise,
the smooth component can be approximated with the predictive mean conditioned on noisy observations:

\begin{restatable}[Recovery of smooth samples]{proposition}{propsmooth} \label{prop:smooth-samples}
    Let $\X \sub \R$ be compact,
    and let $f$ be a stochastic process with surely continuous sample paths and $\sup_{x \in \X} \norm{f(x)}_{L^2} < \infty$.
    Let $(\e_n)_{n \ge 0}$ be i.i.d.\ (potentially non-Gaussian) random variables such that $\E[\e_0] = 0$ and $\V[\e_0] < \infty$.
    Consider any sequence $(x_n)_{n\ge1} \sub \X$, and let $x^* \in \X$ be a limit point of $(x_n)_{n\ge 1}$.
    If $y(x^*) = f(x^*) + \e_0$ and $y_n = f(x_n) + \e_n$ are noisy observations of $f$, then
    \begin{equation} \label{eq:smooth-samples}
        \lim_{n \to \infty}
        \E[y(x^*) \cond y_1, \ldots, y_n] = f(x^*)
        \quad\text{almost surely}.
    \end{equation} 
\end{restatable}
\vspace{-0.6em}
We provide a proof in \cref{app:proof-smooth}.
%
\cref{eq:smooth-samples} suggests the following two-step procedure for obtaining smooth samples from AR CNPs.
\textbf{Step 1:}
Let $\vx_{1:n}$ be a number of target inputs that does not exceed the number of points seen during training.
%
Sample $\vy_{1:n} \sim \AR_{\vx_{1:n}}(\pi_\theta, D\us{(c)}_m)$.
This sample includes observation noise.
\textbf{Step 2:}
Remove the noise from the sample by passing it through the model once more:
$\Normal(\vmu_{1:n}, \mathbf{D}) = P_{\vx_{1:n}}\pi_\theta(D\us{(c)}_m \oplus (\vx_{1:n}, \vy_{1:n}))$.
Here the predictive mean $\vmu_{1:n}$ forms the noiseless sample.
To produce a sample at arbitrarily many inputs, one may also evaluate 
$\Normal(\vmu'_{1:n}, \mathbf{D}) = P_{\vx'_{1:n}}\pi_\theta(D\us{(c)}_m \oplus (\vx_{1:n}, \vy_{1:n}))$
where $\vx'_{1:n}$ is arbitrary.
%
This result of this procedure is illustrated in \cref{fig:noiseless_and_noisy_samples}, and was used to generate the noiseless samples shown in \cref{fig:mf-vs-ar} (right).
\Cref{fig:ar-illustration} in \cref{app:ar-illustration} also illustrates this two-step procedure in a pictorial step-by-step fashion.

\begin{figure}[t]
    \vspace{-0.4cm}
    \small
    \centering
    \includegraphics[width=\linewidth]{figures/noisy_noiseless.pdf}
    \caption{
        Comparison of noiseless (\textit{left}) and
        noisy (\textit{right})
        samples from an AR ConvCNP trained on data sampled from a GP with an exponentiated-quadratic kernel, and the
        ground truth GP.
        The noiseless AR samples were generated from the noisy AR samples using the procedure suggested by \cref{prop:smooth-samples}.
        % The ConvCNP is taken from \cref{sec:exp-synthetic}.
    }
    \vspace{-0.8cm}
    \label{fig:noiseless_and_noisy_samples}
\end{figure}

\vspace{-0.3cm}
\section{Connections to Other Neural Distribution Estimators}
\vspace{-0.3cm}

\tdplotsetmaincoords{70}{20}

\begin{figure}[b!]
    \centering
    \vspace{-1.3cm}
    \begin{tikzpicture}[scale=.95, every node/.style={transform shape}] %[tdplot_main_coords]

        \pgfmathsetmacro{\d}{2.2}
        \pgfmathsetmacro{\s}{1.5}

        \draw[dotted, thick] (0, {-1*\s}, {-1*\s}) -- ({\d*5}, {-1*\s}, {-1*\s});
            
        \foreach \y in {-1,...,1}
        {
            \draw[thick, gray] (0, {\y*\s}, {-1*\s}) -- (0, {\y*\s}, \s);
        }
            
        \foreach \z in {-1,...,1}
        {
            \draw[thick, gray] (0, {-1*\s}, {\z*\s}) -- (0, \s, {\z*\s});
        }
        
        \draw[draw=gray, fill=white, opacity=0.5] ({0*\d}, {-1*\s}, {-1*\s}) -- ({0*\d}, {-1*\s}, \s) -- ({0*\d}, \s, \s) -- ({0*\d}, \s, {-1*\s}) -- cycle;
        
        \foreach \x in {1,...,5}
        {
            
            \foreach \y in {-1,...,1}
            {
                \draw[thick, gray] ({\x*\d}, {\y*\s}, {-1*\s}) -- ({\x*\d}, {\y*\s}, \s);
            }
            
            \foreach \z in {-1,...,1}
            {
                \draw[thick, gray] ({\x*\d}, {-1*\s}, {\z*\s}) -- ({\x*\d}, \s, {\z*\s});
            }
            
            \draw[draw=gray, fill=white, opacity=0.5] ({\x*\d}, {-1*\s}, {-1*\s}) -- ({\x*\d}, {-1*\s}, \s) -- ({\x*\d}, \s, \s) -- ({\x*\d}, \s, {-1*\s}) -- cycle;
            
        }
        
        \path (\d, \s, \s) -- (\d, \s, {-1*\s}) node [midway, above, sloped] (NDE) {\tiny \begin{tabular}{c} Neural processes \end{tabular}};
        
        \path (2*\d, \s, \s) -- (2*\d, \s, {-1*\s}) node [midway, above, sloped] (NF) {\tiny Normalising flows};
        
        \path (3*\d, \s, \s) -- (3*\d, \s, {-1*\s}) node [midway, above, sloped] (GAN) {\tiny \begin{tabular}{c} Generative \\ adversarial networks \end{tabular}};
        
        \path (4*\d, \s, \s) -- (4*\d, \s, {-1*\s}) node [midway, above, sloped] (GAN) {\tiny Variational autoencoders};
        
        \path (5*\d, \s, \s) -- (5*\d, \s, {-1*\s}) node [midway, above, sloped] (NDE) {\tiny \begin{tabular}{c} Autoregressive models\end{tabular}};
        
        \node at (\d, \s, \s) (nun) {};
        \node[rectangle callout,fill=mygreen!20,draw,inner sep = 4pt,callout absolute pointer={(nun)},anchor=west, draw=none] at ($(nun) + (0.1, 0, -0.1)$) {\tiny 1,2}; 
        
        \node at (\d, \s, 0) (nun) {};
        \node[rectangle callout,fill=mygreen!20,draw,inner sep = 4pt,callout absolute pointer={(nun)},anchor=west, draw=none] at ($(nun) + (0.1, 0, -0.1)$) {\tiny 3}; 
        
        \node at (\d, \s, -\s) (cun) {};
        \node[rectangle callout,fill=mygreen!20,draw,inner sep = 4pt,callout absolute pointer={(cun)},anchor=west, draw=none] at ($(cun) + (0.1, 0, -0.1)$) {\tiny 4-6};
        
        \node at (2*\d, -\s, \s) (afn) {};
        \node[rectangle callout,fill=blue!20,draw,inner sep = 4pt,callout absolute pointer={(afn)},anchor=west, draw=none] at ($(afn) + (0.1, 0, -0.1)$) {\tiny 7}; 
        
        \node at (2*\d, -\s, 0) (afn) {};
        \node[rectangle callout,fill=blue!20,draw,inner sep = 4pt,callout absolute pointer={(afn)},anchor=west, draw=none] at ($(afn) + (0.1, 0, -0.1)$) {\tiny 8}; 
        
        \node at (2*\d, -\s, -\s) (cfn) {};
        \node[rectangle callout,fill=blue!20,inner sep=4.0pt, draw,callout absolute pointer={(cfn)},anchor=south, draw=none] at ($(cfn) + (0, 0.2, 0)$) {\tiny 9--15}; 
        
        \node at (2*\d, \s, \s) (afn) {};
        \node[rectangle callout,fill=blue!20,draw,inner sep = 4pt,callout absolute pointer={(afn)},anchor=west, draw=none] at ($(afn) + (0.1, 0, -0.1)$) {\tiny 16}; 
        
        
        \node at (2*\d, \s, -\s) (afn) {};
        \node[rectangle callout,fill=mygreen!20,draw,inner sep = 4pt,callout absolute pointer={(afn)},anchor=west, draw=none] at ($(afn) + (0.1, 0, -0.1)$) {\tiny 17, 18}; 
        
        \node at (2.25*\d, -\s, -\s) (cfn2) {};
        \node[rectangle callout,fill=blue!20, draw,callout absolute pointer={(cfn2)},inner sep = 4pt,anchor=north, draw=none] at ($(cfn2) + (0, -0.2, 0)$) {\tiny 19}; 
        
        \node at (3*\d, -\s, \s) (nfg) {};
        \node[rectangle callout,fill=blue!20, draw,callout absolute pointer={(nfg)},inner sep = 4pt,anchor=east, draw=none] at ($(nfg) + (-0.2, 0, 0)$) {\tiny 20};
        
        \node at (3*\d, -\s, -0.4*\s) (nfg2) {};
        \node[rectangle callout,fill=blue!20, draw,callout absolute pointer={(nfg2)},inner sep = 4pt, anchor=south, draw=none] at ($(nfg2) + (0, 0.2, 0)$) {\tiny 21};
        
        \node at (3*\d, -\s, -\s) (cfg) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4pt, draw,callout absolute pointer={(cfg)},anchor=north, draw=none] at ($(cfg) + (0.15, -0.35, 0)$) {\tiny 22--27};
        
        \node at (3*\d, 0, -\s) (ccg) {};
        \node[rectangle callout,fill=blue!20, draw,inner sep = 4pt,callout absolute pointer={(ccg)},anchor=south, draw=none] at ($(ccg) + (0, 0.2, 0)$) {\tiny 28, 29};
        
        \node at (3.2*\d, -\s, -\s) (cfg2) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4pt, draw,callout absolute pointer={(cfg2)},anchor=south, draw=none] at ($(cfg2) + (0, 0.2, 0)$) {\tiny 30};
        
        \node at (4*\d, -\s, \s) (nfv) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4pt, draw,callout absolute pointer={(nfv)},anchor=west, draw=none] at ($(nfv) + (0.2, 0, 0)$) {\tiny 31, 32};
        
        \node at (4*\d, -\s, -\s) (ccv) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4pt, draw,callout absolute pointer={(ccv)},anchor=north, draw=none] at ($(ccv) + (0, -0.2, 0)$) {\tiny 33, 34};

        \node at (4*\d + 0.8, \s, \s) (nfv) {};
        \node[rectangle callout,fill=mygreen!20, inner sep = 4.5pt, draw,callout absolute pointer={(nfv)},anchor=west, draw=none] at ($(nfv) + (0.2, 0, 0)$) {\tiny 37};

        \node at (4*\d, \s, \s) (nfv) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4pt, draw,callout absolute pointer={(nfv)},anchor=west, draw=none] at ($(nfv) + (0.2, 0, 0)$) {\tiny 35, 36,};
        
        \node at (4*\d, \s, 0) (nfv) {};
        \node[rectangle callout,fill=mygreen!20, inner sep = 4pt, draw,callout absolute pointer={(nfv)},anchor=west, draw=none] at ($(nfv) + (0.2, 0, 0)$) {\tiny 38};
        
        \node at (4*\d, \s, -\s) (nfv) {};
        \node[rectangle callout,fill=mygreen!20, inner sep = 4pt, draw,callout absolute pointer={(nfv)},anchor=west, draw=none] at ($(nfv) + (0.2, 0, 0)$) {\tiny 39};
        
        \node at (4.2*\d, -\s, -\s) (ccv2) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4.0pt, draw,callout absolute pointer={(ccv2)},anchor=south, draw=none] at ($(ccv2) + (0, 0.2, 0)$) {\tiny 40--42};
        
        \node at (5*\d, -\s, \s) (nfa) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4.0pt, draw,callout absolute pointer={(nfa)},anchor=west, draw=none] at ($(nfa) + (0.2, -0.1, 0)$) {\tiny 43--47};
        
        \node at (5*\d, -\s, 0) (afa) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4.0pt, draw,callout absolute pointer={(afa)},anchor=west, draw=none] at ($(afa) + (0.2, 0, 0)$) {\tiny 48--51};
        
        \node at (5*\d, -\s, -\s) (cfa) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4.0pt, draw,callout absolute pointer={(cfa)},anchor=west, draw=none] at ($(cfa) + (0.2, 0, 0)$) {\tiny 52--57};
        
        \node at (5*\d, 0, -\s) (cca) {};
        \node[rectangle callout,fill=blue!20, inner sep = 4.0pt, draw,callout absolute pointer={(cca)},anchor=west, draw=none] at ($(cca) + (0.2, 0, 0)$) {\tiny 58};
        
        \node at (5*\d, \s, \s) (nua) {};
        \node[rectangle callout,fill=mygreen!20, inner sep = 4.0pt, draw,callout absolute pointer={(nua)},anchor=west, draw=none] at ($(nua) + (0.2, 0, 0)$) {\tiny 59};
        
        \node at (5*\d, \s, 0) (nua) {};
        \node[rectangle callout,fill=mygreen!20, inner sep = 4.0pt, draw,callout absolute pointer={(nua)},anchor=west, draw=none] at ($(nua) + (0.2, 0, 0)$) {\tiny 60, 61};
        
        \node at (5*\d, \s, -\s) (cua) {};
        \node[rectangle callout,fill=mygreen!20, inner sep = 4.0pt, draw,callout absolute pointer={(cua)},anchor=west, draw=none] at ($(cua) + (0.2, 0, 0)$) {\tiny 62};
        
        \node at (0, -\s, \s) (n0) {};
        \node[anchor=west] at ($(n0) + (0, 0, 0)$) {\tiny MLP};
        
        \node at (0, -\s, 0) (a0) {};
        \node[anchor=west] at ($(a0) + (0, 0, 0)$) {\tiny Attention};
        
        \node at (0, -\s, -\s) (cv0) {};
        \node[anchor=south west] at ($(cv0) + (0, 0, 0)$) {\tiny Conv.};
        
        \node at (0, -\s, \s) (f0) {};
        \node[anchor=east] at ($(f0) + (0, 0, 0)$) {\tiny Finite};
        
        \node at (0, 0, \s) (c0) {};
        \node[anchor=east] at ($(c0) + (0, 0, 0)$) {\tiny Countable};
        
        \node at (0, \s, \s) (u0) {};
        \node[anchor=east] at ($(u0) + (0, 0, 0)$) {\tiny Uncountable};
        
        \node at (2.5*\d, -2.6, 0.0) (u0) {};

        \node[anchor=north] at ($(u0) + (0, 0, 0)$) {
            \begin{minipage}{\textwidth}
            \tiny
            \begin{multicols}{4}
                \hspace{-0.1cm} \textbf{Neural processes} 
                \begin{enumerate}[label={\arabic*.},leftmargin=3mm]
                    \item \textcolor{mygreen}{CNP} \citep{Garnelo:2018:Conditional_Neural_Processes}
                    \item \textcolor{mygreen}{GNP} \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}
                    \item \textcolor{mygreen}{ACNP} \citep{Kim:2019:Attentive_Neural_Processes}
                    \item \textcolor{mygreen}{ConvCNP} \cite{Gordon:2020:Convolutional_Conditional_Neural_Processes}
                    \item \textcolor{mygreen}{ConvGNP} \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}
                    \item \textcolor{mygreen}{FullConvGNP} \citep{Bruinsma:2021:The_Gaussian_Neural_Process}
                \end{enumerate}
                \hspace{-0.1cm} \textbf{Normalising flows}
                \begin{enumerate}[resume,label={\arabic*.},leftmargin=3mm]
                    \item NICE \citep{dihn2014nice}
                    \item Flow++ \citep{ho2019flow++}
                    \item RealNVP \citep{Dinh:2016:Density_Estimation_Using_Real_NVP}
                    \item FFJORD \citep{grathwohl2018ffjord}
                    \item Glow \citep{kingma2018glow}
                    \item i-ResNets \citep{Behrmann:2018:Invertible_Residual_Networks}
                    \item Res.~Flows \citep{chen2019residual}
                    \item IAF \citep{Kingma:2016:Improving_VI_IAF}
                    \item NAF \citep{huang2018neural}
                    \item TGP \citep{maronas2021transforming} % (uncountable, MLP)
                    \item \textcolor{mygreen}{BRUNO} \citep{korshunova2020conditional}
                    \item \textcolor{mygreen}{Copula GNP} \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}
                    \item FlowGAN \citep{grover2018flow}
                \end{enumerate}
                \hspace{-.2cm} \textbf{GANs}
                \begin{enumerate}[resume,label={\arabic*.},leftmargin=3mm]
                    \item GAN \citep{Goodfellow:2014:Generative_Adversarial_Networks}
                    \item SAGAN \citep{zhang2019self}
                    \item DCGAN \citep{radford2015unsupervised}
                    \item WGAN  \citep{Arjovsky:2017:Wasserstein_Generative_Adversarial_Networks}
                    \item InfoGAN \citep{Chen:2016:InfoGAN_Interpretable_Representation_Learning_by}
                    \item SNGAN \citep{miyato2018spectral}
                    \item BigGAN \citep{brock2018large}
                    \item StyleGAN \citep{karras2019style}
                    \item Spatial GAN \citep{jetchev2016texture}
                    \item $\infty$-GAN \citep{lu2020infinitygan}
                    \item PresGAN \citep{dieng2019prescribed}
                \end{enumerate}
                \hspace{-.2cm} \textbf{Variational autoencoders}
                \begin{enumerate}[resume,label={\arabic*.},leftmargin=3mm]
                    \item VAEs \citep{Kingma:2013:Auto-Encoding_VB}
                    \item IWAE \citep{Burda:2015:Importance_Weighted_Autoencoders}
                    \item Conv. VAE \citep{salimans2015markov}
                    \item VDVAE \citep{child2020very}
                    \item GP-VAE \citep{fortuin2020gp}
                    \item SGP-VAE \citep{ashman2020sparse} % Ashman et al 2020 Sparse GP VAEs (uncountable, MLP)
                    \item \textcolor{mygreen}{LNP} \citep{Garnelo:2018:Neural_Processes}   
                    \item \textcolor{mygreen}{ALNP} \citep{Kim:2019:Attentive_Neural_Processes}
                    \item \textcolor{mygreen}{ConvLNP} \citep{Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction}
                    \item VQ-VAE \citep{van2017neural}
                    \item PixelVAE \citep{gulrajani2016pixelvae}
                    \item VLAE \citep{Chen:2016:Lossy_VAE}
                \end{enumerate}
                \hspace{-.2cm} \textbf{Autoregressive models}
                \begin{enumerate}[resume,label={\arabic*.},leftmargin=3mm]
                    \item RNADE \citep{uria2013rnade}
                    \item NADE \citep{Uria:2016:Neural_Autoregressive_Distribution_Estimation}
                    \item DeepNADE \citep{uria2014deep}
                    \item EoNADE \citep{uria2014deep}
                    \item MADE \citep{germain2015made}
                    \item PixelSNAIL \citep{chen2018pixelsnail}
                    \item Sparse Transformer \citep{child2019generating}
                    \item DEformer \citep{alcorn2021deformer}
                    \item XLNet \citep{yang2019xlnet}
                    \item ConvNADE \citep{Uria:2016:Neural_Autoregressive_Distribution_Estimation}
                    \item WaveNet \citep{oord2016wavenet}
                    \item PixelCNN \citep{van2016conditional}
                    \item PixelCNN++ \citep{salimans2017pixelcnn++}
                    \item Fast PixelCNN++ \citep{Ramachandran:2017:Fast_Generation_for_Convolutional_Autoregressive_Model_Tuning}
                    \item Scalable Pixel Net \citep{menick2018generating}
                    \item ARDMs \citep{hoogeboom2021autoregressive}
                    \item \textcolor{mygreen}{AR CNP} (this work)
                    \item \textcolor{mygreen}{AR ACNP} (this work)
                    \item \textcolor{mygreen}{Transformer NP} \citep{Nguyen:2022:Transformer_Neural_Processes}
                    \item \textcolor{mygreen}{AR ConvCNP} (this work)
                \end{enumerate}
            \end{multicols}
            \end{minipage}
        };
        
    \end{tikzpicture}
    \caption{Conceptual diagram showing the relationships between AR CNPs and various neural distribution estimators. The vertical axis denotes whether the model learns a distribution over a finite number of random variables, a countably infinite number, or an uncountably infinite number. The axis into the page denotes whether the architecture is MLP-based, or uses attention or convolutions. From left to right, we show different modelling paradigms. Fruitful exchanges occur when NPs (highlighted in \textcolor{mygreen}{green}) are introduced into other modelling paradigms. Our proposed AR CNPs can be viewed as introducing NPs to the AR modelling paradigm.}
    \label{fig:litlandscape}
    \vspace{-0.2cm}
\end{figure}


Various paradigms have been developed for neural distribution estimators (NDEs): normalising flows \citep{dihn2014nice}, generative adversarial networks \cite[GANs;][]{Goodfellow:2014:Generative_Adversarial_Networks}, variational
autoencoders \cite[VAEs;][]{Kingma:2013:Auto-Encoding_VB}, autoregressive models  \citep{Uria:2016:Neural_Autoregressive_Distribution_Estimation}, and diffusion models \citep{sohl2015deep, ho2020denoising}.
\cref{fig:litlandscape} visualises the landscape of NDEs.
%
We argue that NPs and AR CNPs should be viewed as neural distribution estimators (NDEs) and be placed in this landscape.
%
AR CNPs inherit the strengths of AR models, such as the ability to model complex dependencies with a tractable likelihood, but also some of their weaknesses, most
notably slow test-time sampling.
%
Slow sampling is the main drawback of AR CNPs, 
though
\newpage~\\[-2em]
it may be possible to adapt techniques for speeding up AR models
\citep{Ramachandran:2017:Fast_Generation_for_Convolutional_Autoregressive_Model_Tuning}.
%
One major difference between AR CNPs and existing AR models is that AR CNPs decompose the joint distribution of an \emph{uncountably} infinite set of variables, allowing querying at arbitrary input locations (\cref{sec:ARCNP}). 
%
Like
DEformer \citep{alcorn2021deformer}, EoNADE \citep{uria2014deep}, and
%
XLnet \citep{yang2019xlnet},
AR CNPs are trained to not prefer any particular order of decomposing the joint distribution into conditionals.
%
To achieve this goal,
the AR CNP shares design choices with other AR models: (i) a shared architecture is used to produce each conditional distribution, similar to WaveNet \citep{oord2016wavenet} and PixelCNN \citep{van2016conditional}; (ii) the data point index is given as input to the network as in the DEformer model \citep{alcorn2021deformer}; and (iii)  training maximises a log-likelihood including all decompositions of the joint distribution, similar to EoNADE \citep{uria2014deep} and XLnet \citep{yang2019xlnet}.

\vspace{-0.1cm}
\cref{fig:litlandscape} also shows the connections between NPs, VAEs and normalising flows (NFs).
%
Like VAEs, LNPs use decoders that parametrise a factorised distribution and rely on the latent variable to induce dependencies.
%
Again, the key difference is that LNPs model a distribution over an \emph{uncountable} set of variables.
%
Models like conditional BRUNO \citep{korshunova2020conditional} and copula GNPs \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable} combine ideas from NPs and NFs, transforming a stochastic process with an invertible transformation.
%
Finally, GAN models such as Spatial GAN \citep{jetchev2016texture} and $\infty-$GAN \citep{lu2020infinitygan} model countable numbers of variables, such as images of arbitrary size.
%
Inspecting \cref{fig:litlandscape}, we see that GANs are the only class of models depicted that do not currently have an NP version: a version that models an uncountable number of variables.
%
This suggests adversarial training of NPs as an interesting avenue for future investigation.

\vspace{-0.1cm}
In recent work, \citet{Nguyen:2022:Transformer_Neural_Processes} proposed the Transformer NP (TNPs), which uses a causally-masked transformer architecture with an autoregressive likelihood.
%
    In contrast, rather than proposing a new AR architecture, our work focuses on running \emph{existing CNPs} in AR mode to obtain coherent samples and improved likelihoods, without modifying the architecture or training procedure.
%
In prior work, \citet{Volpp:2021:Bayesian_Context_Aggregation_for_Neural_Processes} used AR sampling in order to visualise samples from CNPs.
%
However, their work focuses on proposing a novel context aggregation mechanism for NPs, and they do not evaluate the likelihood of CNPs autoregressively or investigate any performance gains.

\vspace{-0.3cm}
\section{The Performance of Autoregressive Neural Processes}
\vspace{-0.3cm}

In this section we investigate the performance of AR CNPs on synthetic and real data.
%
Across a wide range of tasks, the AR CNP is competitive with much more sophisticated approaches.
Throughout, we train LNPs with both the ELBO \citep{Garnelo:2018:Neural_Processes} and ML objective \citep{Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction}.
Code for implementations of NPs and reproducing our experiments can be found at {\small \url{https://github.com/wesselb/neuralprocesses}}.
For all experiments, we use a random ordering of the target points in Proc.\ \ref{proc:ar}; see App.\ \ref{app:order} for a justification.

\vspace{-0.25cm}
\subsection{Synthetically Generated Gaussian and Non-Gaussian Data}
\label{sec:exp-synthetic}
\vspace{-0.2cm}

\paragraph{Synthetic experiment setup}
We evaluate an extensive collection of NP models on a wide range of Gaussian and non-Gaussian synthetic regression tasks.
%
We consider tasks with functions drawn from (i) different GPs; (ii) a non-Gaussian sawtooth process (as in \cref{fig:mf-vs-ar}); (iii) a non-Gaussian mixture task, where, with equal probability, we sample the function from one of three possible GPs or the sawtooth process.
%
We also consider various versions of the tasks for different input and output dimension $d_x, d_y$, with dependencies across the output channels.
%
To ensure a fair comparison, we configure the architectures to make the parameter counts comparable between all models.
%

\paragraph{Results}
\cref{tab:synthetic-main} highlights the best performing models on some representative tasks; for further results across all twenty synthetic tasks and further experimental details, see \cref{app:results-synthetic}.
%
The AR procedure dramatically improves the performance of the ConvCNP, with the AR ConvCNP being the best performing model for most tasks, except on the Gaussian EQ task where it performs marginally worse than the FullConvGNP. 
%
In particular, the AR ConvCNP outperforms the FullConvGNP and ConvGNP on non-Gaussian tasks, in agreement with \cref{prop:arcnp-advantage}, while having a faster training time than the other convolutional models (\cref{fig:timing}).
%
For the sawtooth task, \Cref{fig:synthetic_non-Gaussian_sawtooth_collape} in \cref{app:sawtooth-collapse} illustrates that predictions by the AR ConvCNP can be multi-modal and non-Gaussian, even when using a Gaussian likelihood.
%
Finally, we note that in tasks with $d_x = 2$, where the FullConvGNP cannot be used (as discussed in \cref{sec:intro}), the AR ConvCNP far outperforms all competing approaches.
% 

\vspace{-0.25cm}
\subsection{Sim-to-Real Transfer with the Lotka--Volterra Equations} \label{sec:predprey}
\vspace{-0.2cm}

\index{sim-to-real transfer}
\paragraph{Predator-prey data}
We next investigate sim-to-real transfer, where the models are trained on simulated data and tested on real data.
%
NPs are well-suited to this setting, since a large meta-data set can be easily generated to train them. 
%
We consider the hare--lynx data set, which is a population time series of Snowshoe hares and Canadian lynx \citep{MacLulich:1937:Fluctuations_in_the_Numbers_of_the_Varying_Hare}.
%
To generate simulated data, we use a stochastic version of the Lotka--Volterra equations
\citep{Lotka:1910:Contribution_to_the_Theory_of,Volterra:1926:Variazioni_e_Fluttuazioni_del_Bumero}:
\begin{equation}
    \label{eq:stoch_Lotka-Volterra_predator_prey}
    \sd X_t = \alpha X_t \isd t - \beta Y_t X_t \isd t + \sigma X^\nu_t \isd W^{(1)}_t,\hspace{3mm}
    \sd Y_t = -\gamma X_t \isd t + \delta Y_t X_t \isd t + \sigma Y^\nu_t \isd W^{(2)}_t.
\end{equation}
Under these equations, the
prey population $X_t$ grows exponentially with rate $\alpha$, the
predator population $Y_t$ decays exponentially with rate $\gamma$, and the predators hunt the prey.
%
$W^{(1)}$ and $W^{(2)}$ are independent Brownian motions introducing noisy behaviour.
%
These equations generate non-Gaussian data with both within-channel as well as cross-channel dependencies.
%
We simulate the Lotka-Volterra equations on a dense grid, and use them to generate meta--data sets in three different ways.
%
\emph{Interpolation}: we randomly divide the data into context and target sets.
%
\emph{Forecasting}: we choose a random time, before which all data are contexts, and all future data are targets.
%
\emph{Reconstruction:} we randomly choose between the $X_t$ or $Y_t$, split the chosen series as in forecasting, and append the other series to the context.
% 
In training, for every batch, we choose one of these tasks uniformly at random.

\vspace{-0.1cm}
\paragraph{Results}
\Cref{tab:predprey_results} shows the results of the best performing models.
%
The AR ConvCNP performs best both on the simulated as well as the real data, again demonstrating that running CNPs in AR mode improves performance and can even outperform strong GNP and LNP baselines.
%
For full experimental details and additional results see \cref{app:predprey}.

\begin{table}[t!]
    \vspace{-0.4cm}
    \centering
\newcommand{\markmissing}{}
\resizebox{\textwidth}{!}{\begin{tabular}[t]{lccccccc} 
% Show nothing for missing numbers. The dashes were misaligned.
\toprule 
 & \multicolumn{2}{c}{EQ} & \multicolumn{2}{c}{Sawtooth} & \multicolumn{2}{c}{Mixture} \\ 
 & \multicolumn{2}{c}{\footnotesize Norm.\ KL to truth ($\downarrow$ better)} & \multicolumn{2}{c}{\footnotesize Norm.\ log-lik.\ ($\uparrow$ better)} & \multicolumn{2}{c}{\footnotesize Norm.\ log-lik.\ ($\uparrow$ better)} \\
 & $d_x,d_y\!=\!1$ & $d_x,d_y\!=\!2$ & $d_x,d_y\!=\!1$ & $d_x,d_y\!=\!2$ & $d_x,d_y\!=\!1$ & $d_x,d_y\!=\!2$ \\ 
 \midrule 
ConvCNP & $0.41\,{ \scriptstyle \pm  0.01 }$ & $0.41\,{ \scriptstyle \pm  0.00 }$ & \hphantom{\scalebox{0.7}[1]{$-$}}$    2.38 {\scriptstyle \pm   0.04}$ & $\hphantom{\scalebox{0.7}[1]{$-$}}0.12\,{ \scriptstyle \pm  0.01 }$ & \scalebox{0.7}[1]{$-$}$    0.23 {\scriptstyle \pm   0.04}$ & $\scalebox{0.7}[1]{$-$}0.85\,{ \scriptstyle \pm  0.01 }$ \\
ConvCNP (AR) & $0.01\,{ \scriptstyle \pm  0.00 }$ & $\mathbf{0.03}\,{ \scriptstyle \pm  0.00 }$ & \hphantom{\scalebox{0.7}[1]{$-$}}$    \mathbf{3.60} {\scriptstyle \pm   0.01}$ & $\hphantom{\scalebox{0.7}[1]{$-$}}\mathbf{0.38}\,{ \scriptstyle \pm  0.00 }$ &  \hphantom{\scalebox{0.7}[1]{$-$}}$    \mathbf{0.45} {\scriptstyle \pm   0.04}$ & $\mathbf{\scalebox{0.7}[1]{$-$}0.62}\,{ \scriptstyle \pm  0.01 }$ \\ 
ConvGNP & $0.01\,{ \scriptstyle \pm  0.00 }$ & $0.19\,{ \scriptstyle \pm  0.00 }$ & \hphantom{\scalebox{0.7}[1]{$-$}}$    2.62 {\scriptstyle \pm   0.05}$ & $\hphantom{\scalebox{0.7}[1]{$-$}}0.26\,{ \scriptstyle \pm  0.01 }$ & \scalebox{0.7}[1]{$-$}$    0.24 {\scriptstyle \pm   0.02}$ & $\scalebox{0.7}[1]{$-$}0.74\,{ \scriptstyle \pm  0.01 }$ \\ 
FullConvGNP & $\mathbf{0.00}\,{ \scriptstyle \pm  0.00 }$ & \markmissing & \hphantom{\scalebox{0.7}[1]{$-$}}$    2.16 {\scriptstyle \pm   0.04}$ & \markmissing & \scalebox{0.7}[1]{$-$}$    0.05 {\scriptstyle \pm   0.03}$  & \markmissing \\ 
ConvLNP (ML) & $0.25\,{ \scriptstyle \pm  0.01 }$ & $0.39\,{ \scriptstyle \pm  0.00 }$ & \hphantom{\scalebox{0.7}[1]{$-$}}$    3.06 {\scriptstyle \pm   0.04}$ & $\hphantom{\scalebox{0.7}[1]{$-$}}0.31\,{ \scriptstyle \pm  0.01 }$ & \scalebox{0.7}[1]{$-$}$    0.06 {\scriptstyle \pm   0.03}$ & $\scalebox{0.7}[1]{$-$}0.78\,{ \scriptstyle \pm  0.02 }$ \\
ConvLNP (ELBO) & $0.06\,{ \scriptstyle \pm  0.00 }$ & $0.79\,{ \scriptstyle \pm  0.00 }$ & \hphantom{\scalebox{0.7}[1]{$-$}}$    3.51 {\scriptstyle \pm   0.02}$ & $\hphantom{\scalebox{0.7}[1]{$-$}}0.04\,{ \scriptstyle \pm  0.00 }$ & \hphantom{\scalebox{0.7}[1]{$-$}}$    0.12 {\scriptstyle \pm   0.04}$ & $\scalebox{0.7}[1]{$-$}0.92\,{ \scriptstyle \pm  0.01 }$ \\ 
{\normalshape \textit{Diagonal GP}} & $0.40\,{ \scriptstyle \pm  0.01 }$ & $0.40\,{ \scriptstyle \pm  0.00 }$ & \markmissing & \markmissing & \markmissing & \markmissing \\ 
{\normalshape \textit{Trivial}} & $1.19\,{ \scriptstyle \pm  0.00 }$ & $0.79\,{ \scriptstyle \pm  0.00 }$ & \scalebox{0.7}[1]{$-$}$    0.18 {\scriptstyle \pm   0.00}$ & $\scalebox{0.7}[1]{$-$}0.32\,{ \scriptstyle \pm  0.00 }$ & \scalebox{0.7}[1]{$-$}$    1.32 {\scriptstyle \pm   0.00}$ & $\scalebox{0.7}[1]{$-$}1.46\,{ \scriptstyle \pm  0.00 }$ \\ 
\bottomrule \\
\end{tabular}}
\vspace{-0.5cm}
\caption{
Performance of NPs training on the GP EQ task, sawtooth task, and mixture task.
\emph{Diagonal GP} denotes the exact GP predictive, but with correlations removed.
\emph{Trivial} denotes a model that predicts a Gaussian distribution with the empirical mean and standard deviation of the context outputs.
Significantly best models in bold.
Note that the FullConvGNP cannot be run on tasks where $d_x > 1$.
}
    \label{tab:synthetic-main}
    \vspace{-0.5cm}
\end{table}

\begin{table}[t]
    \centering
    \vspace{-0.3cm}
    \small
    \setlength{\tabcolsep}{1.5pt}
    \subimport{results/}{predprey_results.tex}
    \vspace{-0.5cm}
    \caption{
    Normalised log-likelihoods in the predator--prey experiments, showing interpolation (int.), forecasting (for.), and reconstruction (rec.) on simulated (S) and real (R) data.
    Significantly best results in bold.
    }
    \label{tab:predprey_results}
    \vspace{-0.8cm}
\end{table}

\vspace{-0.25cm}
\subsection{Electroencephalogram experiments}
\vspace{-0.2cm}
\label{sec:exp-eeg}

\paragraph{Electroencephalogram data}
We next trained various NPs on real time series data consisting of electroencephalogram (EEG) measurements \citep{Zhang:1995:Event_Related_Potentials_During_Object}, following \cite{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}.
Each time series consists of 256 regularly spaced measurements across 7 correlated channels.
%
For each channel, we randomly select a number of the 256 points uniformly at random to be target points, and use the remaining ones as context points, independently across the channels.
%

\vspace{-0.1cm}
\paragraph{Results}
After training, we test the models on this interpolation task and also on a reconstruction task, where we set a part of a channel as target and the remainder as context.
%
In \cref{tab:eeg-results}, we observe that the AR ConvCNP is competitive with the FullConvGNP, despite having significantly shorter training times and fewer parameters.
Both the AR ConvCNP and the FullConvGNP outperform the ConvCNP and the ConvLNP.
%
Full experimental detail are in \cref{app:eeg}.

\begin{table}[t!]
 \small
 \centering
\vspace{-0.4cm}
\newcommand{\colwidth}{2.13cm}
\newcommand{\smallcolwidth}{1.83cm}
\newcommand{\tinycolwidth}{1.43cm}
\resizebox{\textwidth}{!}{\begin{tabular}{
    lcccccc
}
 \toprule
 & {\small ConvCNP}
 & {\small ConvCNP (AR)}
 & {\small ConvGNP}
 & {FullConvGNP}
 & {\small ConvLNP (ML)}
 & {\small ConvLNP (ELBO)} \\ \midrule
 {Int.}
    & $\scalebox{0.7}[1]{$-$}1.02 {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}\mathbf{0.34} {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}0.93 {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}\mathbf{0.35} {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}1.04 {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}1.20 {\scriptstyle \pm 0.01}$ \\
 {Rec.}
    & $\scalebox{0.7}[1]{$-$}2.07 {\scriptstyle \pm 0.03}$
    & $\scalebox{0.7}[1]{$-$}0.63 {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}1.45 {\scriptstyle \pm 0.03}$
    & $\scalebox{0.7}[1]{$-$}\mathbf{0.57} {\scriptstyle \pm 0.01}$
    & $\scalebox{0.7}[1]{$-$}1.53 {\scriptstyle \pm 0.02}$
    & $\scalebox{0.7}[1]{$-$}2.00 {\scriptstyle \pm 0.06}$ \\
 \bottomrule
\end{tabular}}
\vspace{-0.3cm}
\caption{Normalised log-likelihoods on the EEG experiments. Significantly best results in bold.}
\label{tab:eeg-results}
\vspace{-0.7cm}
\end{table}

\vspace{-0.25cm}
\subsection{Environmental Modelling}
\label{sec:environmental-modelling}
\vspace{-0.25cm}

Environmental datasets bring a range of modelling challenges.
%
One example is fusing spatio-temporal data from disparate sources \citep{chang2018multisensor,lahat2015multimodal}, which arises in diverse environmental sciences applications from climate monitoring to hydrology \citep{gettelman2022future, ferrer2020multisensor, robinson2021global, lu2010population, hosseini2017data}.
%
Another challenge involves estimating the probability of events of interest, such as the compound risk of \emph{both} low wind speeds at an offshore wind farm \emph{and} high cloud cover over a solar panel farm.
%
To obtain robust uncertainty estimates for such events, it is essential to model correlations as well as non-Gaussian marginals (such as cloud cover).
%
Current GAN-based approaches (e.g. \citealt{ravuri_skilful_2021}) can capture both joint and non-Gaussian statistics, but they cannot perform data fusion or predict at arbitrary off-grid locations.
%
The AR ConvCNP can fuse data of on-grid and off-grid modalities and make predictions at arbitrary locations while modelling arbitrary non-Gaussian likelihoods and capturing statistical dependencies, thus achieving all the desiderata and filling a gap in the environmental modelling toolbox.
%
Here, we assess the AR ConvCNP on two common environmental modelling tasks, namely \emph{data assimilation} and \emph{statistical downscaling}.

\vspace{-0.1cm}
\paragraph{Data assimilation}
Data assimilation is the task of combining observations of the Earth system to produce predictions on a regular grid, called a \textit{reanalysis}.
%
Reanalyses are typically generated by fitting the trajectories of physics-based climate models to observations \citep{hersbach_era5_2020, gettelman2022future}, but the potential for improving data assimilation with ML has drawn increasing attention in recent years \citep{geer_learning_2021}.
%
To explore the AR ConvCNP's data assimilation abilities for a non-Gaussian variable, we train convolutional NP models to predict simulated daily-average cloud cover fraction over Antarctica.
%
We use reanalysis data from ECMWF ERA5 \citep{hersbach_era5_2020} as ground truth.
%
Cloud cover takes values in the interval $[0, 1]$, with observations frequently taking values of $0$ or $1$ (\cref{fig:clt_hist}).
% 
We evaluate the performance of NPs using either a Gaussian likelihood or a beta-categorical mixture model with three components: two discrete delta components for values of exactly 0 or 1, and a beta distribution capturing continuous values in $(0, 1)$.
%
This provides a robust way of handling $0$ and $1$ values, unlike the existing copula GNP model \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable} which can have its output constrained in $(0, 1)$ but places zero density at the endpoints.

\vspace{-0.1cm}
\paragraph{Data assimilation results}
In \cref{tab:antarctica-results} we see that the AR ConvCNP significantly outperforms competing NPs for both the Gaussian and beta-categorical likelihoods.
%
\Cref{fig:antarctic_main} shows samples drawn from the models, after observing context points on half of the space.
%
The AR ConvCNP displays remarkable ability to extrapolate rich, non-stationary, multi-scale structure, such as sudden changes in cloud cover over the Ross Ice Shelf coastline at the bottom of the figure.
%
By comparison, the ConvLNP and ConvGNP produce blurry, lower frequency samples.
%
Unlike GPs, convolutional NP models have a fixed receptive field induced by the CNN architecture used for the encoder, which is computationally expensive to increase.
Away from the context points on the left, samples from the non-AR models will be independent of the observations, reverting to some mean representation of the data (Fig. \ref{fig:antarctic_main}c-e).
This highlights a further benefit of AR CNPs: successive AR applications increase the receptive field, enabling rich, conditional sample structure to extrapolate far away from observed data.
See \Cref{app:antarctic} for further commentary, sample figures, and details.

\begin{table}[t!]
    \centering
    \vspace{-0.3cm}
    \resizebox{\columnwidth}{!}{
    \small
    \begin{tabular}{@{}lccccccc@{}}
    \toprule
        & \multicolumn{4}{c}{{\scshape Gaussian}} & \multicolumn{3}{c}{{\scshape Beta-Categorical}} \\
        & ConvGNP & ConvLNP (ML) & ConvCNP & ConvCNP  (AR) & ConvLNP (ML) & ConvCNP & ConvCNP (AR) \\
        \midrule
        Log-lik. & $\phantom{1}0.60 {\scriptstyle \pm 0.02}$ & $\phantom{1}0.62 {\scriptstyle \pm 0.02}$ & $\phantom{1}0.58 {\scriptstyle \pm 0.02}$ & \hspace{1mm}$\mathbf{0.88} {\scriptstyle \pm 0.02}$ & $\phantom{1}1.06 {\scriptstyle \pm 0.02}$ & $\phantom{1}1.03 {\scriptstyle \pm 0.02}$ & $ \hspace{1mm}\mathbf{1.27} {\scriptstyle \pm 0.02}$ \\
        MAE (\%) & $13.05 {\scriptstyle \pm 0.17}$ & $12.98 {\scriptstyle \pm 0.16}$ & $13.01 {\scriptstyle \pm 0.16}$ & $13.01 {\scriptstyle \pm 0.16}$ & $12.99 {\scriptstyle \pm 0.16}$ & $13.13 {\scriptstyle \pm 0.16}$ & $13.13 {\scriptstyle \pm 0.16}$ \\
        \bottomrule
    \end{tabular}
    }
    \small
    \vspace{-0.3cm}
    \caption{Normalised log-likelihoods and mean absolute errors (MAE, in units of cloud cover \%), over the 2019-2019 test period for the cloud cover task. Note that log-likelihoods cannot be compared directly across the Gaussian and beta-categorical models. Errors indicate standard errors. Significantly best results in bold.}
    \vspace{-0.9cm}
    \label{tab:antarctica-results}
\end{table}

%
\vspace{-0.1cm}
\paragraph{Environmental downscaling}
The spatial resolutions of physics-based reanalyses are limited by their vast computational demands, making them unsuitable for capturing local and extreme events \citep{Stocker:2013:Climate_Change_2013,Maraun:2017:Towards_Process-Informed_Bias_Correction_of}.
%
\emph{Statistical downscaling} addresses this issue by leveraging additional information to produce fine-grained predictions
\citep{Maraun:2018:Statistical_Downscaling_and_Bias_Correction}.
%
Recently, NPs have been shown to outperform a large ensemble of existing climate downscaling approaches \citep{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for}.
%
We compare the AR ConvCNP to the MLP ConvCNP of \citeauthor{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for} and the MLP ConvGNP of \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable} in a temperature downscaling task over Germany.
%
In this task, the context data consist of low-resolution ERA-Interim reanalysis data and high-resolution topography, and the target data consist of weather station observations from the ECA\&D dataset.
%
We also consider a second setup where we reveal some station observations to aid the downscaling process.
%
As \cref{app:downscaling_arch} explains, the MLP ConvCNP and MLP ConvGNP cannot be extended to include these station observations.
We therefore introduce a novel multiscale architecture, which we use to run the ConvCNP in AR mode. 
%
%
See \Cref{app:downscaling} for full experimental details.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/antarctic_cloud_sample_comparison.png}
    \vspace{-0.4cm}
    \small
    \caption{(a) Ground truth simulated cloud cover fraction on 25/06/2018. (b-e), Sample draws from the AR ConvCNP, ConvCNP, ConvLNP and ConvGNP with context points denoted by red dots. Context points were removed from the right hand side of the 2D space to test the models' abilities to extrapolate coherent function samples far away from observations. The ConvCNP and ConvLNP models used a beta-categorical likelihood while the ConvGNP uses a low-rank Gaussian likelihood.}
    \label{fig:antarctic_main}
    \vspace{-0.7cm}
\end{figure}

\vspace{-0.1cm}
\paragraph{Environmental downscaling results} In \cref{tab:climate_fusion_results} we observe that the AR ConvCNP matches the performance of the ConvGNP, which is remarkable as the latter has been previously demonstrated to outperform a range of state-of-the-art downscaling approaches \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable,Vaughan:2022:Convolutional_Conditional_Neural_Processes_for}.
When additional observations from weather stations are revealed, the AR ConvCNP significantly outperforms the MLP ConvGNP in both metrics.

\begin{table}[h]
    \centering
    \vspace{-0.7cm}
    \small
    \begin{tabular}{lcccc}
        \toprule
        {Downscaling}
            & {Norm.\ log-lik.}
            & {MAE ($^\circ$C)} \\
        \midrule
        ConvCNP (MLP)
            & $\shortminus 1.55 \, {\scriptstyle \pm 0.01}$
            & $\mathbf{0.94} \, {\scriptstyle \pm 0.03}$ \\
        ConvGNP (MLP)
            & $\mathbf{\shortminus 1.36} \, {\scriptstyle \pm 0.01}$
            & $1.09 \, {\scriptstyle \pm 0.09}$ \\
        ConvCNP (AR)
            & $\mathbf{\shortminus 1.36} \, {\scriptstyle \pm 0.01}$
            & $1.04 \, {\scriptstyle \pm 0.04}$ \\
        \bottomrule
    \end{tabular}
    \begin{tabular}{lcccc}
        \toprule
        {Down. + stations}
            & {Norm.\ log-lik.}
            & {MAE ($^\circ$C)} \\
        \midrule
        ConvCNP${}^*$ (MLP)
            & $\shortminus 1.55 \, {\scriptstyle \pm 0.01}$ 
            & $0.94 \, {\scriptstyle \pm 0.03}$ \\
        ConvGNP${}^*$ (MLP)
            & $\shortminus 1.38 \, {\scriptstyle \pm 0.01}$
            & $1.09 \, {\scriptstyle \pm 0.09}$ \\
        ConvCNP (AR)
            & $\mathbf{\shortminus 1.31} \, {\scriptstyle \pm 0.01}$
            & $\mathbf{0.85} \, {\scriptstyle \pm 0.05}$ \\
        \bottomrule
    \end{tabular}
    \vspace{-0.1cm}
    \caption{
       Normalised log-likelihoods and mean absolute errors (MAEs) in the downscaling experiments, without (\textit{left}) and with (\textit{right}) assisting weather station observations.
        Significantly best results in bold.
        ${}^*$Cannot use extra weather station observations.
    }
    \label{tab:climate_fusion_results}
    \vspace{-1cm}
\end{table}

\vspace{-0.15cm}
\section{Limitations and Conclusion}
\vspace{-0.25cm}

We have shown that the AR procedure can be readily applied to improve the performance of CNPs, producing coherent samples and dramatically improved likelihoods.
%
Surprisingly, in an extensive range of experiments, this simple approach often outperforms more complicated methods which rely on latent variables or which explicitly model correlations.
%
We demonstrate the effectiveness of our approach on data sets of real-world interest 
by applying AR CNPs on climate data fusion tasks, modelling $[0, 1]$-constrained data with a beta-categorical likelihood and introducing a novel multiscale architecture.
%
Notably, AR CNPs fill a gap in the climate modelling toolbox by enabling joint, non-Gaussian predictives, which could be used to better estimate the magnitude of compound risks.
%
We also position AR CNPs within the larger neural density estimator literature, showing the fruitfulness of combining NPs with other modelling paradigms.

\vspace{-0.1cm}
More generally, AR CNPs equip the NP framework with a new knob where modelling complexity and computational expense at training time can be traded for computational expense at test time.
%
In particular, the higher quality samples and better likelihoods obtained by applying NPs autoregressively come with the additional cost of performing a forward pass for every element in the target set.
%
This can be prohibitively expensive for large target sets, and constitutes the primary practical drawback of using AR CNPs.
%
In addition, since AR CNPs do not define a consistent stochastic process, design choices for the AR procedure may affect the quality of the results.
%
Thus practitioners need to avoid choosing target sets that lead to pathological behaviour, such as when the spatial density of the target inputs is too high.
%
However, the flexibility of this design space also presents an opportunity: as an example, in \cref{sec:AR_auxiliary} we show that auxiliary target points can be used to further improve predictions.
%
Finally, promising avenues for future work include applying the AR procedure to other NPs besides CNPs, and investigating the efficacy of the block sampling scheme presented in \cref{sec:ARCNP}.
\vspace{-2em}


\newpage
\section{Reproducibility Statement}

All our experiments are carried out using either synthetic or publicly available datasets.
The EEG data set is available through the UCI database,\footnote{\url{https://kdd.ics.uci.edu/databases/eeg/eeg.data.html}.} and the environmental data are also publicly available through the European Climate Data Service.\footnote{\url{https://cds.climate.copernicus.eu/\#!/home}.}

We make publicly available all code necessary to reproduce our experiments\footnote{\small \url{https://github.com/wesselb/neuralprocesses}.} as well as instructions for downloading, preprocessing, and modelling the Antarctic cloud cover data\footnote{\small \url{https://github.com/tom-andersson/iclr2023-antarctic-arconvcnp}.}.
Proofs for \cref{prop:arcnp-advantage,prop:smooth-samples} are given in \cref{app:proof-advantage} and \cref{app:proof-smooth} respectively.
Details on the model architectures and the experimental setup can be found in \cref{app:description-models,app:protocols,app:results-synthetic} for the synthetic datasets, \cref{app:predprey} the sim-to-real transfer experiments, \cref{app:eeg} for the EEG experiments, \cref{app:antarctic} for the data assimilation experiment, and \cref{app:downscaling} for the downscaling experiment.

\section{Ethics Statement}

Training CNPs autoregressively improves their performance dramatically, but we do not foresee adverse societal impacts as a result of this work.
That being said, the problem of capturing the statistical trends present in a dataset must be performed with care, especially in safety critical applications, where the stakes of making incorrect and confident predictions can have severe consequences.
We view the AR procedure as a useful tool, rather than a panacea, for capturing such behaviours, and hope this work encourages further research into building effective but reliable models to this end.

We also note that while training CNPs is computationally cheaper than alternative NP models, AR sampling itself incurs a substantial computational cost, and thus energy cost, at test time.
Running AR sampling on a large scale could lead to greater power demands for these models, resulting in larger carbon footprints which are undesirable.
However, we believe the potential benefits for environmental modelling could outweigh this cost, while leveraging methods to make AR CNPs more computationally efficient should help alleviate this issue.

\section{Acknowledgements}
This research was conducted while WPB and AYKF were students at the University of Cambridge.
During that time, WPB was supported by the Engineering and Physical Research Council (studentship number 10436152),
and AYKF was supported by the Trinity Hall Studentship and the George and Lilian Schiff Foundation.
SM acknowledges funding from the Vice Chancellor's \& George and Marie Vergottis scholarship and the Qualcomm Innovation Fellowship.
TRA and JSH are supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/W006022/1, particularly the AI for Science theme within that grant \& The Alan Turing Institute.
RET is supported by Google, Amazon, ARM, Improbable and EPSRC grant EP/T005386/1.

\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}


\clearpage
\appendix

\section{Proof of Proposition \ref{prop:arcnp-advantage}}
\label{app:proof-advantage}

\paragraph{Additional notation}
If $\vy_1 \oplus \vy_2 \sim P_{\vx_1 \oplus \vx_2} \pi(D)$, then denote the distribution of $\vy_1 \cond \vy_2$ by $P_{\vx_1 \cond \vx_2} \pi(D)$.
Note that $P_{\vx_1 \cond \vx_2} \pi(D)$ depends on $\vy_2$, because it is the distribution of $\vy_1 \cond \vy_2$, even though the notation does not make this dependence explicit.

\paragraph{The ``appropriate regularity conditions''}
Let $\Pc^N_{\lambda}$ be the collection of distributions on $\R^N$ that (a) have a density with respect to the Lebesgue measure and (b) have a covariance matrix which is strictly positive definite.
Let $\Pc^N\ss{$\lambda$,G} \sub \Pc^N_{\lambda}$ be the subcollection of distributions which are Gaussian.
Then, by Corollary B.1 by \citet{Bruinsma:2021:The_Gaussian_Neural_Process}, for all $\mu \in \Pc^N_{\lambda}$ such that $\inf_{\nu \in \Pc^N\ss{$\lambda$,G}} \,\KL(\mu, \nu) < \infty$,
\begin{equation} \label{eq:moment-matching} \textstyle
    \argmin_{\nu \in \Pc^N\ss{$\lambda$,G}} \KL(\mu, \nu) = \Normal(\mu)
\end{equation}
where $\Normal(\mu)$ denotes the Gaussian distribution with mean vector and covariance matrix equal to those of $\mu$.

In the proposition, by appropriate regularity conditions on $y$, we mean the assumption that, for all inputs $\vx$ and $D \in \mathcal{D}$, $P_{\vx} \pi_y(D)$ is in $\smash{\Pc^{\abs{\vx}}_\lambda}$ and such that $\inf_{\nu \in \Pc^{\abs{\vx}}\ss{$\lambda$,G}} \,\KL(P_\vx \pi_y(D), \nu) < \infty$.

Assume the appropriate regularity conditions on $y$.
We now list three technical observations.
\vspace{-0.5em}
\begin{enumerate}
    \item 
        Note that $P_{\vx_1 \cond \vx_2} \pi_y(D)$ is the distribution of $y(\vx_1) \cond D, (\vx_2, \vy_2)$,
        so we have the identity $P_{\vx_1 \cond \vx_2} \pi_y(D) = P_{\vx_1} \pi_y(D \oplus (\vx_2, \vy_2))$.
        Therefore, for all inputs $\vx_1$, inputs $\vx_2$, and $D \in \mathcal{D}$, $P_{\vx_1 \cond \vx_2} \pi_y(D)$ is in $\smash{\Pc^{\abs{\vx_1}}_\lambda}$ and such that $
            \inf_{
                % Leave this implicit.
                \nu %\in \Pc^{\abs{\vx_1}}_{\lambda,\text{G}}
            }
         \,\KL(P_{\vx_1 \cond \vx_2} \pi_y(D), \nu) < \infty$. \\[-0.5em]
    \item
        The ideal CNP $\pi\ss{C}$ matches the means and marginal variances of the true posterior predictives (\cref{sec:ARCNP}).
        Hence, for all $x \in \mathcal{X}$ and $D \in \mathcal{D}$, $P_{x} \pi\ss{C}(D)$ is in $\Pc^1_{\lambda,\text{G}}$. \\[-0.5em]
    \item
        The ideal GNP $\pi\ss{G}$ matches the mean vectors and covariance matrices of the true posterior predictives (\cref{sec:ARCNP}). 
        Hence, for all inputs $\vx$ and $D \in \mathcal{D}$, $P_{\vx} \pi\ss{G}(D)$ is in $\smash{\Pc^{\abs{\vx}}_{\lambda,\text{G}}}$; which
        means that, for all $x_1 \in \mathcal{X}$, inputs $\vx_2$, and $D \in \mathcal{D}$, $P_{x_1 \cond \vx_2} \pi\ss{G}(D)$ is in $\Pc^{1}_{\lambda,\text{G}}$.
\end{enumerate}
\vspace{-0.75em}
In the proof, to apply and use \eqref{eq:moment-matching}, we implicitly use these observations.
\vspace{0.5em}

\propadvantage*
\begin{proof}[Proof of Proposition \ref{prop:arcnp-advantage}]
    Let $\vx$ be some inputs and let $D \in \mathcal{D}$ be some data set.
    We will argue that, for all $n = 1, \ldots, \abs{\vx}$,
    \begin{align}
        &\KL(
            P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D),
            P_{x_n} \pi\ss{C}(D \oplus (\vx_{1:(n-1)}, \vy_{1:(n - 1)}))
        ) \nonumber \\
        &\qquad\le \KL(
            P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D),
            P_{x_n \cond \vx_{1:(n-1)}} \pi\ss{G}(D)
        ).
    \end{align}
    Assuming this inequality, the result follows directly from the chain rule for the KL divergence in combination with the definition of $\AR_\vx$ (\cref{proc:ar}):
    \begin{align}
        &\KL(
            P_\vx \pi_y(D),
            \operatorname{AR}_\vx(\pi\ss{C}, D)
        ) \nonumber \\
        &\qquad=\textstyle\sum_{n=1}^{\abs{\vx}}
        \E_{\vy_{1:(n-1)}}[
        \KL(
            P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D),
            P_{x_n} \pi\ss{C}(D \oplus (\vx_{1:(n-1)}, \vy_{1:(n - 1)}))
        )
        ]\\
        &\qquad\le\textstyle\sum_{n=1}^{\abs{\vx}}
        \E_{\vy_{1:(n-1)}}[
        \KL(
            P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D),
            P_{x_n \cond \vx_{1:(n-1)}} \pi\ss{G}(D)
        )
        ] \\
        &\qquad\vphantom{\textstyle\sum_n^n}=
       \KL(
            P_\vx \pi_y(D),
            P_\vx \pi\ss{G}(D)
        ) 
    \end{align}
    where the expectations are over $\vy_{1:(n-1)} \sim P_{\vx_{1:(n-1)}} \pi_y(D)$.
    To prove the inequality, note that, conditional on $\vy_{1:(n-1)}$, using \eqref{eq:moment-matching},
    \begin{equation}
        \textstyle
        \argmin_{\nu \in \Pc\ss{$\lambda$,G}^1}\,
        \KL(
            P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D),
            \nu
        )
        = \Normal(P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D)).
    \end{equation}
    By the property of $\pi\ss{C}$ that it matches the mean and marginal variance of the true posterior (\cref{sec:ARCNP}),
    \begin{align}
        \Normal(P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D))
        &= \Normal(P_{x_n} \pi_y(D \oplus (\vx_{1:(n-1)}, \vy_{1:(n-1)}))) \\
        &= P_{x_n} \pi\ss{C}(D \oplus (\vx_{1:(n-1)}, \vy_{1:(n-1)})).
    \end{align}
    Therefore,
    \begin{equation} \label{eq:advantage_ar:argmin}
        \argmin_{\nu \in \Pc\ss{$\lambda$,G}^1}\,
        \KL(
            P_{x_n \cond \vx_{1:(n-1)}} \pi_y(D),
            \nu
        )
        = P_{x_n} \pi\ss{C}(D \oplus (\vx_{1:(n-1)}, \vy_{1:(n-1)})).
    \end{equation}
    Noting that  $P_{x_n \cond \vx_{1:(n-1)}} \pi\ss{G}(D) \in \Pc\ss{$\lambda$,G}^1$, we obtain
    the desired inequality.
\end{proof}

\section{Proof of Proposition \ref{prop:smooth-samples}}
\label{app:proof-smooth}

\propsmooth*
\begin{proof}[Proof of Proposition \ref{prop:smooth-samples}]
    Consider the increasing filtration $\mathcal{F}_n = \sigma(y_1, \ldots, y_n)$ with limit $\mathcal{F}_\infty = \sigma(\smash{\union_{n=1}^\infty} \mathcal{F}_n)$.
    Also let $\Tc_n = \sigma(\e_{n+1}, \e_{n+2}, \ldots)$ and consider the tail $\sigma$-algebra $\Tc = \intersection_{n=1}^\infty \Tc_n$.
    Let $(x_{n_i})_{i=1}^\infty$ be a subsequence of $(x_n)_{n=1}^\infty$ such that $x_{n_i} \to x^*$.
    Let $g_n = \frac{1}{n} \sum_{i=1}^n y_i$.
    Since $g_n$ is a function of $y_1, \ldots, y_n$, it is $\F_n$--measurable and therefore $\mathcal{F}_\infty$--measurable.
    Note that
    \begin{equation}
        g_n = \frac{1}{n} \sum_{i=1}^n f(x_{n_i}) + \frac{1}{n} \sum_{i=1}^n \e_i.
    \end{equation}
    By sure continuity of $f$, the first term converges to $f(x^*)$ surely.
    By the strong law of large numbers \citep[Example 5.6.1;][]{Durrett:2010:Probability_Theory_and_Examples}, % and the conditions on $(\e_n)_{n \ge 1}$,
    the second term converges to zero on a tail event $A \in \Tc$ of probability one.
    We conclude that $\ind_{A} f(x^*)$ is $\sigma(\F_\infty, \Tc)$--measurable.
    Therefore, by almost sure convergence of $L^2$--bounded martingales \citep[Theorem 5.4.5;][]{Durrett:2010:Probability_Theory_and_Examples}, 
    \begin{align}
        \lim_{n \to \infty} \E[y(x^*) \cond y_1, \ldots, y_n]
        &= \lim_{n \to \infty} \E[f(x^*) \cond y_1, \ldots, y_n] && \text{($\E[\e_0] = 0$)} \\
        &= \lim_{n \to \infty} \E[f(x^*) \cond \F_n]  && \text{(definition of $\F_n$)} \\
        &= \lim_{n \to \infty} \E[f(x^*) \cond \F_n, \Tc] && \text{($\sigma(f(x^*), \F_n) \perp \Tc$)} \\
        &= \lim_{n \to \infty} \E[\ind_A f(x^*) \cond \F_n, \Tc] && \text{($\P(A) = 1$)} \\
        &= \vphantom{\lim_n} \E[\ind_A f(x^*) \cond \F_\infty, \Tc] && \text{($L^2$--mart.\ convergence)}\\
        &= \vphantom{\lim_n} \ind_A f(x^*) && \text{($\ind_A f(x^*) \in \sigma(\F_\infty, \Tc)$)} \\
        &= f(x^*), && \text{($\P(A) = 1$)}
    \end{align}
    where all equalities hold almost surely.  
\end{proof}

\newpage
\section{Illustration of the AR procedure}
\label{app:ar-illustration}
\cref{fig:ar-illustration} depicts the AR sampling procedure (\cref{proc:ar}) and procedure to produce smooth samples (\cref{prop:smooth-samples}) using the ConvCNP trained on the EQ data process from \cref{sec:exp-synthetic}.
\begin{figure}[h!]
    \vspace{-0.7cm}
    \centering
    \textbf{Model fit}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-first.pdf}
    \textbf{Step 1: Draw noisy samples using AR sampling (\cref{proc:ar})}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-0.pdf}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-1.pdf}
    \begin{center}
        \vspace{-3pt}
        $\LARGE \vdots \nonumber$
    \end{center}
    \vspace{0.1cm}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-197.pdf}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-198.pdf}
    \textbf{Step 2: Denoise sample by passing it through the model (\cref{prop:smooth-samples})}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-single-sample.pdf}
    \textbf{Multiple samples}
    \includegraphics[width=\linewidth,height=0.145\linewidth]{figures/ar-procedure/ar-eq-multiple-samples.pdf}
    \vspace{-0.5cm}
    \caption{Illustration of the AR procedure with a random AR ordering and the de-noising step (\cref{proc:ar} and \cref{prop:smooth-samples}), to produce smooth samples. Given a context set (\emph{black crosses}), we can use the CNP to get marginal predictions at arbitrary input locations (\emph{first figure}). We choose a randomly sampled input location, draw a corresponding output sample from the model's predictive (\emph{blue dots in the second plot}), append this to the context set, and pass the augmented context set through the model again.
    We repeat this step a number of times (\emph{third and fourth figures}), until all function (epistemic) uncertainty has been removed and all that remains is irreducible noise (aleatoric) uncertainty (\emph{fifth figure}).
    This procedure yields noisy function samples (\emph{blue dots in the sixth plot}), which we pass one last time through the model to obtained a denoised sample, treating the mean prediction as an approximate noiseless sample (\emph{seventh figure}).
    Repeating this procedure yields high-quality samples from the model predictive (\emph{eighth figure}).
    }
    \label{fig:ar-illustration}
    \vspace{-0.3cm}
\end{figure}

\newpage
\section{Number and Order of Target Points}
\label{app:order}
When deploying a conditional neural process (CNP) autoregressively (AR; \cref{proc:ar}), the number and ordering of the target points matters.
In this appendix, we describe our observations of the effects of the number and ordering of the target points on the quality of the predictions.
In short, our recommendation is to choose a different random ordering for every sample, and to not let the number/density of target points exceed that at training time.

\subsection{Effects of the Number of Target Points}
During the AR sampling procedure, the AR CNP is evaluated at context sets of increasing size.
Our experience is that, as long as the sizes of these context sets do not exceed the sizes seen at training time, the predictions should not be significantly affected by changes in the number of target points.
However, if the AR sampling procedure evaluates the model at context sets of larger sizes than seen during training time, then that presents the model with an out-of-distribution situation.
What happens then comes down to how well the neural networks generalise.
Our experience is that the predictions quickly start to break down.

A notable exception of this rule of thumb are convolutional-deep-set--based models, such as the Convolutional Conditional Neural Process \citep[ConvCNP;][]{Gordon:2020:Convolutional_Conditional_Neural_Processes}.
For these models, the \emph{magnitude of the density channel} is what determines whether the models generalises or not.
This means that it is not the total number of points that matters, but rather the \emph{density} of the points.
Therefore, the AR ConvCNP can be evaluated at arbitrarily many target points, as long as the density of these points does not significantly exceed the density of context points seen at training time.
Once the density exceeds the density of the training data, the model is presented with an out-of-distribution situation, and what happens then again comes down to how well neural networks generalise.

Figure \cref{fig:density} illustrates this observation.
When the density target points does not exceed the training data (50 and 100 points), the predictions look calibrated.
However, once the density of target points comes close or exceed the training data (200, 500, and 1000 points), bias starts to creep into the predictions.

Although the number/density of points in the AR sampling procedure should not exceed that at training time, AR CNPs can still produce high-quality samples at arbitrarily many target points by following the trick outlined at the end of the two-step procedure below \cref{prop:smooth-samples}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/density.pdf}
    \label{fig:density}
    \caption{
        Samples and predictions for an AR ConvCNP with various numbers of target points ordered randomly (\emph{left column}) and ordered left to right (\emph{right column}).
        When the density of the target points does not exceed the training data (\emph{50 and 100 points}), ordering the target points randomly or left to right does not matter.
        When the density of the target points comes close to the training data or exceeds it (\emph{200, 500, and 1000 points}), bias creeps into the predictions.
        The random ordering appears to perform more robustly than left to right.
        The data is sampled from the EQ data process from the synthetic experiments (\cref{sec:exp-synthetic}),
        and the trained model is also taken from the synthetic experiments. 
        The predictions by the model are shown in solid blue and the marginals by the ground-truth EQ process are shown in dot-dashed purple.
    }
\end{figure}

\subsection{Effects of the Ordering of Target Points}
Our experience is that, as long as the number of target points (or density) does not exceed that at training time, the ordering of the target point does not really matter.
\Cref{fig:density} also demonstrates this.
When the density of the target points does not exceed the training data (50 and 100 points), sampling randomly or left to right does not really matter.
However, once the density of the target points comes close to or exceeds the training data (200, 500, and 1000 points), we observe a difference in performance between sampling randomly and sampling left to right.
Across all numbers of target points, a random ordering seems to perform most robustly.
Our recommendation is therefore to choose a different random ordering of the target points for every sample.

\subsection{Analysis of AR CNPs for CNPs with Gaussian Marginals}
\label{app:analysis-ar-cnps-gaussian} 
In this subsection, we argue that, for CNPs with Gaussian marginals, predictions in the first few AR steps might be poor, but predictions in later AR steps tend to be more accurate.
Choosing a different random ordering for every sample therefore ``averages out'' the effects from these first few AR steps.

When evaluating a CNP with Gaussian marginals in AR mode, every conditional prediction in the AR process is Gaussian.
Let us consider the process of producing an AR sample.
For the first target input $x_1$, we run the CNP forward to obtain a distribution for the corresponding target output $y_1$.
In reality, the true posterior most likely is non-Gaussian, which means that the prediction for the first target point may be poor.
Nevertheless, we sample this Gaussian, append the sample $(x_1, y_1)$ to the context set, and run the CNP forward again.
Because we now feed the earlier sample $y_1$ through the non-linear network, the marginal predictive for the next target output $y_2$ (having integrated out $y_1$) is non-Gaussian.
As we perform more AR steps, the marginal predictions of later points become increasingly non-Gaussian, increasing the model’s flexibility.

We see that, for a given ordering of the target inputs, the prediction for the first target input is likely poor (because it is Gaussian), and (in the best case) the predictions become more and more accurate as we take more AR steps (because they become more and more non-Gaussian).
This is exactly what is happening in \cref{fig:mm_pred}: the left prediction is Gaussian and therefore a poor approximation, and, as we go to the right and take more and more AR steps, the prediction becomes more and more non-Gaussian and therefore more accurate.
If we were to feed the target inputs in right to left, then the same phenomenon would happen. The right prediction would be a Gaussian and a very poor approximation, and, as we go to the left and take more AR steps, the prediction would become more non-Gaussian and therefore more accurate.

More generally, for a given ordering of the target points, the ordering will produce high quality predictions if the conditional distributions of the AR factorisation match the corresponding conditional distributions of the true posterior.
Since the conditionals of the AR CNP are typically Gaussian by design, this means that the ordering is “good” if the corresponding conditionals of the true posterior are close to Gaussian.

So when is a conditional of the posterior close to Gaussian?
Let us assume that the true underlying process is a sum of a non-Gaussian process (constituting epistemic uncertainty) and independent Gaussian noise (constituting aleatoric uncertainty).
Generally, a conditional will have both epistemic and aleatoric uncertainty, so a Gaussian will be a bad fit.
\emph{However}, as we condition the conditionals of the true generative process on more and more data, the underlying function will be pinned down more and more accurately, meaning that the conditional will consist mostly of aleatoric uncertainty, which is Gaussian.
Therefore, as we condition on more and more data, we expect the conditionals to become more and more Gaussian.
This again suggests that the samples in the first few AR steps might be a poor fit (because the corresponding conditionals of the true posterior are not yet Gaussian),
but that samples in later AR steps should be a better fit (because the corresponding conditionals are then close to Gaussian).

To summarise, an ordering of the target points is ``good'' if the corresponding conditionals of the true posterior are also close to Gaussian.
Under the assumption that the ground-truth process is a non-Gaussian process with additive Gaussian noise, conditionals tend to be close to Gaussian if they are conditioned on many data points. As a consequence, the earlier conditionals in the AR factorisation tend to be poor fits to the ground-truth posterior, whereas later conditionals tend to produce better fits.
Choosing a different random ordering for every sample therefore ``averages out'' the effects from the first few AR steps.

\subsection{Effect of the random ordering on the spread of the log-likelihood}

We have thus far argued for the benefit of using random ordering in AR, due to the robustness it provides.
However, one issue with random orderings is that, since different random orderings do not in general give rise to the same predictive distribution, we may obtain different predictive log-likelihoods in practice, depending on the exact random ordering that we sample.
Ideally, we would like not only the mean predictive log-likelihood (averaged out over orderings) to be high, but also the standard deviation of the log-likelihood (due to, again, different random orderings) to be small.
In other words, we would like the model to perform well regardless of the random ordering which we happen to sample.

At this point, note that if the true underlying process is Gaussian, then a sufficiently well-trained AR CNP with Gaussian marginals would have a small such spread in the log-likelihood, because all conditional predictions of the model will be close to the ground truth conditional predictions.
Consequently the order with which we make predictions will have a small effect on the log-likelihood, resulting in a small spread of predictive log-likelihood values.
Consider for example the case where the conditionals of the CNP exactly match the conditionals of the true process.
In this case, there will be zero variance in the predictive log-likelihood of the process under different orderings.
However, the situation is different when the ground truth is non-Gaussian.
In this case, as we explained in the previous section, the conditionals of the first few target points may be highly non-Gaussian under the true process, while those of the AR CNP are Gaussian.
In this case, we may get different log-likelihoods depending on the random order that we happen to sample.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/sawtooth-ar-random-comparison.pdf}
    \caption{Plot of the standard deviation, due to different random orderings, of the per-data-point predictive log-likelihood (in nats) of an AR ConvCNP on one-dimensional sawtooth data, as a function of the number of target set size.
    For each point in the plot, we have used $2^{10}$ randomly sampled and fixed tasks, on each of which we apply the AR ConvCNP with 100 different randomly sampled orderings.}
    \label{fig:my_label}
\end{figure}

\Cref{fig:my_label} provides a quantitative illustration of the above point.
In this figure, we show the standard deviation in the per datapoint predictive log-likelihood of an AR CNP (due to different random orderings) on two variants of a task with sawtooth data.
On the first variant, we always pass an empty context set to the model (blue), and on the other task, we pass non-empty context sets with randomly sampled number of context points, uniformly distributed between 0 and 100 (red).
We observe that for empty contexts (blue), we get a relatively large standard deviation in predictive log-likelihood for the first few target points.
This likely happens because, initially, the model may randomly pick a target input where the conditional of the true process is highly non-Gaussian (making a poor prediction), or it might pick a target input where the true conditional is Gaussian (making a good prediction).
This results in a larger variance in performance for the first few target points.
However, as more target points are introduced, the standard deviation shrinks.
This is because the conditionals of the true process become increasingly Gaussian, which means that no matter which target input is picked next, the model will approximate the true conditional accurately using a Gaussian, thereby reducing the impact of the ordering of subsequent points on the variance of the log likelihood.
Further, introducing a relatively modest number of initial context points (red) in a second variant of the task, substantially reduces the spread in the predictive log-likelihoods.
This is again because conditioning on a context set means that the conditionals of the true process are better approximated by Gaussians, reducing the impact that different random orderings have on the spread of the log-likelihood.
In practice, in our experiments, we have found the variance in the log-likelihood to be near-zero for Gaussian or Gaussian-like ground truth processes, and larger, but acceptable, for non-Gaussian tasks.

\newpage
\section{Details for Figure \ref{fig:mm_pred}} \label{app:details-fig-mm}

The generative process visualised in the top panel of figure \ref{fig:mm_pred} is defined by the following mixture distribution:
\begin{equation}
    p_{\text{true}}(y \cond x) = a_1 \Normal (f_1(x), 1) + a_2\Normal (f_2(x), 1) + a_3 \Normal (f_3(x), 1). \label{eq:mixture}
\end{equation}
Given this mixture distribution, the (Gaussian) ideal CNP can be computed in closed form by computing the first two moments of $p_{\text{true}}$:
\begin{equation}
    p_{\text{CNP}}(y \cond x) =  \Normal (\mu(x), \sigma^2(x)) 
\end{equation}
where 
\begin{align}
    \mu(x) &= \sum_{i=1}^3 a_i f_i(x), \\
    \sigma^2(x) &= \sum_{i=1}^3 a_i \left(1 + f_i(x)^2 \right) - \left( \sum_{i=1}^3 a_i f_i(x) \right)^2.
\end{align}
The updated mixture weights for the posterior distribution $p_{\text{true}}(y \cond x, D\us{(c)})$ given a context set $D\us{(c)}$ can be computed via Bayes rule and $p_{\text{CNP}}(y \cond x, D\us{(c)})$ can be computed given the updated mixture weights. Note that in \cref{fig:mm_pred} the prior mixture weights are  $a_1=a_3=0.25$ and $a_2=0.5$, means are given by
\begin{align}
    f_1(x) &= x^2 + 1, \\
    f_2(x) &= x, \\
    f_3(x) &= -x, 
\end{align}
and the target locations are $x=1, 2, 4,$ and  $6$. The bottom four panels of \cref{fig:mm_pred} show kernel density estimates (Gaussian kernel) of $30\,000$ samples drawn from the generative distribution $p_{\text{true}}(y_1, y_2, y_4, y_6)$, the ideal CNP $p_{\text{CNP}}(y_1, y_2, y_4, y_6)$, and the ideal CNP applied in AR mode from left to right:
\begin{equation}
    p_{\text{AR CNP}}(y_1, y_2, y_4, y_6) = p_{\text{CNP}}(y_1)   p_{\text{CNP}}(y_2 \cond y_1 )  p_{\text{CNP}}(y_4 \cond y_1, y_2 ) p_{\text{CNP}}(y_6 \cond y_1, y_2, y_4 ).
\end{equation}
    
\newpage
\section{Description of Models} \label{app:description-models}

The architectures follow the descriptions from the respective papers they are introduced.
Although these descriptions are for one-dimensional inputs and outputs, the architectures are readily generalised to multidimensional inputs and outputs;
we will explicitly mention
wherever that generalisation requires extra care.
All architectures use ReLU activation functions.
All GNPs,
in addition to a covariance matrix over the target points,
also output heterogeneous observation noise along the marginal means;
the total covariance over the target points is thus the sum of the covariance by the model and a diagonal matrix formed from these observation noises.

\paragraph{Conditional neural process \citep[CNP;][]{Garnelo:2018:Conditional_Neural_Processes}}
% The architecture follows \cref{\xrprefix{mod:cnp}}.
Set the dimensionality of the encoding to $K = 256$.
Parametrise the encoder with a three-hidden-layer multi-layer perceptron (MLP) of width $256$;
and parametrise the decoder with a six-hidden-layer MLP of width $256$.
For multidimensional outputs, let the decoder have width $512$.
For multidimensional outputs where outputs can have context points at different inputs, produce a separate encoding for every output and concatenate these into one big encoding.
These encoders may or may not share parameters.
In our experiments, for two-dimensional outputs, parametrise separate encoders;
for higher-dimensional outputs, apply the same encoder.

\paragraph{Gaussian neural process \citep[GNP;][]{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}}
Use the same choices for $K$, the encoder, and the decoder as the CNP.
Set the rank of the kernel map to $R = 64$.
As mentioned in the introduction, let the decoder produce one extra dimension which forms heterogeneous observation noise.
For multidimensional outputs, the same caveats as for the CNP apply.

\paragraph{Latent neural process \citep[LNP;][]{Garnelo:2018:Neural_Processes}}
The LNP builds off the CNP.
Call the existing encoder the \emph{deterministic encoder}.
The NP adds one more encoder called the \emph{stochastic encoder}.
The stochastic encoder mimics the deterministic encoder, but outputs a $K$-dimensional vector of means and a $K$-dimensional vector of marginal variances.
These are used to sample a $K$-dimensional Gaussian latent variable (the \emph{stochastic encoding}).
The decoder now additionally takes in the stochastic encoding.
For multidimensional outputs, the same caveats as for the CNP apply.

\paragraph{Attentive conditional neural process \citep[ACNP;][]{Kim:2019:Attentive_Neural_Processes}}
The ACNP builds off the CNP.
It replaces the deterministic encoder $\enc_\theta \colon \D \to \R^K$ with an 
eight-head attentive encoder $\enc_\theta\us{(att)}\colon \D \times \X \to \R^K$
\citep{Vaswani:2017:Attention_Is_All_You_Need}.
Unlike the original deterministic encoder $\enc_\theta$, the new attentive encoder $\enc_\theta\us{(att)}$ also takes in the target input.
Let $D\us{(c)} = (\vx\us{(c)}, \vy\us{(c)}) \in \D$ be a context set of size $N$ and let $x\us{(t)} \in \X$ be a target input.
We now descibe the computation of $\enc_\theta\us{(att)}(D\us{(c)}, x\us{(t)})$.
Parametrise $\phi_x\colon \X \to (\R^{32})^8$ and $\phi_{xy}\colon \X \times \Y \to (\R^{32})^8$ both with three-hidden-layer MLPs of width $256$.
Compute
\begin{align}
    \text{the \emph{keys}:} & \quad (\vk_{h,n})_{h=1}^8 = \phi_{x}(x\us{(c)}_n) \quad\hspace{22pt} \text{for $n = 1, \ldots, N$}, \\
    \text{the \emph{values}:} & \quad (\vv_{h,n})_{h=1}^8 = \phi_{xy}(x\us{(c)}_n, y\us{(c)}_n) \quad \text{for $n = 1, \ldots, N$}, \\
    \text{the \emph{query}:} & \quad (\vq_h)_{h=1}^8 = \phi_{x}(x\us{(t)}).
\end{align}
Then compute
\begin{equation}
    \vv\us{(q)}_h = \sum_{n=1}^N  \frac{e^{\lra{\vq_h, \vk_{h, n}}}}{\sum_{n'=1}^Ne^{\lra{\vq_h, \vk_{h, n'}}}} \vv_{h,n} \in \R^{256}
\end{equation}
Concatenate $\vv\us{(q)} = (\vv_1\us{(q)}, \ldots, \vv_8\us{(q)}) \in \R^{256} $ and $\vq = (\vq_1, \ldots, \vq_8) \in \R^{256}$.
Let $\mL \colon \R^{256} \to \R^{256}$ be a linear layer;
let $\phi\us{(res)}\colon \R^{256} \to \R^{256}$ be a one-hidden-layer MLP of width $256$;
and let $\operatorname{norm}_1$ and $\operatorname{norm}_2$ be two layer normalisation layers with learned pointwise transformations \citep{Ba:2016:Layer_Normalization}.
Then
\begin{equation}
    \enc_\theta\us{(att)}(D\us{(c)}, x\us{(t)})
    = \operatorname{norm}_2(\vz + \phi\us{(res)}(\vz))
    \quad\text{where}\quad
    \vz = \operatorname{norm}_1(\vv\us{(q)} + \mL \vq).
\end{equation}
For multidimensional outputs, the same caveats as for the CNP apply.

\paragraph{Attentive Gaussian neural process (AGNP)}
The AGNP build off the GNP.
It replaces the deterministic encoder with the same eight-head attentive deterministic encoder of the ACNP.

\paragraph{Attentive neural process \citep[ALNP;][]{Kim:2019:Attentive_Neural_Processes}}
The ALNP build off the LNP.
It replaces the deterministic encoder with the same eight-head attentive deterministic encoder of the ACNP.

\paragraph{Convolutional Conditional Neural Process \citep[ConvCNP;][]{Gordon:2020:Convolutional_Conditional_Neural_Processes}}
% The architecture follows \cref{\xrprefix{mod:convcnp}} and performs the discretisation approach outlined by \eqref{\xrprefix{proc:discretisation}}.
Set the discretisation to an evenly spaced grid at a certain density (the \emph{points per unit}) spanning a bit more (the \emph{margin}) than the most extremal context and target inputs.
The points per unit and margin are specified separately for every experiment.
Initialise the length scales of all Gaussian kernels to twice the interpoint spacing of the discretisation.
Divide the data channel by the density channel.
Parametrise $\dec_\theta$ with a 
U-Net \citep{Ronneberger:2015:U-Net_Convolutional_Networks_for_Biomedical}.
Before the U-turn, let the U-Net have six convolutional layers with kernel size five, stride two, and 64 output channels;
and six more such layers, but using transposed convolutions, after the U-turn.
The layers after the U-turn additionally take in the outputs of the layers before the U-turn in reversed order;
this is the U-net structure \citep[Figure 1;][]{Ronneberger:2015:U-Net_Convolutional_Networks_for_Biomedical}.
For multidimensional outputs where outputs can have context points at different inputs, produce a separate data and density channel for every output and concatenate these into one big encoding;
use separate length scales for every application of $\enc_\theta$.

\paragraph{Convolutional Gaussian neural process \citep[ConvGNP;][]{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}}
Use the same choices for the discretisation, length scales, and CNN architecture as for the ConvCNP.
Set the rank of the kernel map to $R = 64$.
As mentioned in the introduction, let the decoder produce one extra channel which forms heterogeneous observation noise.
For multidimensional outputs, the same caveat as for the ConvCNP applies.

\paragraph{Fully convolutional Gaussian neural process \citep[FullConvGNP;][]{Bruinsma:2021:The_Gaussian_Neural_Process}}
For the mean architecture and the kernel architecture, use the same choices for the discretisation, length scales, and CNN architecture as for the ConvCNP.
Implement the source channel with the identity matrix
and apply the matrix transform $\mZ \mapsto \mZ \mZ^\T$ to ensure positive definiteness.
Let the decoder produce one extra channel which forms heterogeneous observation noise.
% the total covariance over the target points is then the sum of $k_\theta(D)$ and a diagonal matrix formed from these observation noises.
For multidimensional outputs, in addition to the caveat for the ConvCNP, two additional caveats apply.
First, for $D\ss{o}$-dimensional outputs, let the decoder produce $D\ss{o}^2$ channels rather than just one. 
These channels should be interpreted as all covariance and cross-covariance matrices between all outputs.
Second, when applying the matrix transform $\mZ \mapsto \mZ \mZ^\T$, these channels should first be assembled into one total covariance matrix.

\paragraph{Convolutional latent neural process \citep[ConvLNP;][]{Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction}}
The ConvLNP builds off the ConvCNP.
The ConvLNP replaces the CNN architecture by two copies of this architecture placed in sequence.
In between the two architectures, there is a sampling step:
the first architecture outputs 32 channels, comprising 16 means and 16 marginal variances, which are used to sample a 16-dimensional Gaussian latent variable;
and the second architecture then takes in this sample.

\paragraph{Autoregressive Conditional Neural Processes (AR CNPs)}
The AR CNP, AR ACNP, and AR ConvCNP use the architectures described above.
Rolling out an AR CNP according to \cref{proc:ar} requires an ordering of the target points.
In all experiments, we choose a random ordering of the target points.

\newpage
\section{Training, Cross-Validation, and Evaluation Protocols}
\label{app:protocols}

The following description applies to the synthetic experiments (\cref{sec:exp-synthetic}),
the predator--prey experiments (\cref{sec:predprey}),
the EEG experiments (\cref{sec:exp-eeg}),
and the environmental downscaling experiments (\cref{sec:environmental-modelling}).
For the environmental data assimilation experiments, a different protocol was used; 
we refer the reader to \cref{app:antarctic} for full details of the environmental data assimilation experiments.

A \emph{task} consists of a context set and target set.
How precisely the context and target sets are generated is specific to an experiment.
To train a model, we consider batches of 16 tasks at a time, compute an objective function value, and update the model parameters using ADAM \citep{Kingma:2014:Adam_A_Method_for_Stochastic}.
The learning rate is specified separately for every experiment.
We define an epoch to consist of $2^{14} \approx$ \SI{16}{k} tasks.
We typically train a model for between 100 and 1000 epochs.

For an experiment, we split up the meta--data set into a \emph{training set}, a \emph{cross-validation set}, and an \emph{evaluation set}.
The model is trained on the training set.
During training, after every epoch, the model is cross-validated on the cross-validation set.
Cross-validation uses $2^{12}$ fixed tasks.
These $2^{12}$ are fixed, which means that cross-validation always happens with exactly the same data.
The cross-validation objective is a confidence bound computed from the model objective.
Suppose that model objective over all $2^{12}$ cross-validation tasks has empirical mean $\hat\mu$ and empirical variance $\hat\sigma^2$.
If a higher model objective is better, then the cross-validation objective is given by $\hat\mu - 1.96 \cdot \hat\sigma / \sqrt{2^{12}}$.
The model with the best cross-validation objective is selected and used for evaluation.
Evaluation is performed with the evaluation set and also uses $2^{12}$ tasks.

Conditional neural processes and Gaussian neural processes are trained, cross-validated, and evaluated with the neural process ELBO objective proposed by \citet{Garnelo:2018:Conditional_Neural_Processes}.
We normalise the terms in the neural process objective by the target set sizes.
Latent-variable neural processes (LNPs) are trained, cross-validated, and evaluated with the ELBO objective proposed by \citet{Garnelo:2018:Neural_Processes} using five samples, also normalised by the target set size.
When training LNPs with the ELBO objective, but not when cross-validating and evaluating, the context set is subsumed in the target set.
Additionally, LNPs are trained, cross-validated, and evaluated with the ML objective proposed by \citet{Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction}, again normalised by the target set size.
When training and cross-validating LNPs with the ML objective, we use twenty samples;
and when evaluating, we use 512 samples.
For completeness, LNPs trained with the ELBO objective are also evaluated with the ML objective using 512 samples.

To stabilise the numerics for GNPs, we increase the regularisation of covariance matrices for one epoch.
To encourage LNPs to fit, we fix the variance of the observation noise of the decoder to $10^{-4}$ for the first three epochs.


\newpage
\section{Details of Synthetic Experiments} \label{app:results-synthetic}

\subsection{Description of Experiments}
We synthetically generate data sets by randomly sampling from five different choices for the ground-truth stochastic process $f$.
Let the inputs be $d_x$-dimensional.
Then define the following stochastic processes:
\begin{itemize}[leftmargin=8em]
    \item[EQ:]
        a Gaussian process with an exponentiated quadratic (EQ) kernel:
        \begin{equation}
           f \sim \GP(0, \exp(-\tfrac1{2\ell^2}\norm{\vx - \vx'}_2^2)) 
        \end{equation}
        where $\ell > 0$ is a length scale;
    \item[Mat\'ern--$\tfrac52$:]
        a Gaussian process with a Mat\'ern--$\tfrac52$ kernel:
        \begin{equation}
           f \sim \GP(0, k(\tfrac1\ell\norm{\vx - \vx'}_2))
        \end{equation}
        where $k(r) = (1 + \sqrt{5}r + \tfrac53 r^2)e^{-r}$
        and $\ell > 0$ is a length scale;
    \item[weakly periodic:]
        a Gaussian process with a weakly periodic kernel:
        \begin{equation}
            f \sim \GP(0, \exp(-\tfrac1{2\ell\ss{d}^2}\norm{\vx - \vx'}_2^2 - \tfrac{2}{\ell\ss{p}^2}\norm{\sin(\tfrac{\pi}{p} (\vx - \vx'))}_2^2))  
        \end{equation}
        where $\ell\ss{d} > 0$ is a length scale specifying how quickly the periodic pattern changes, $\ell\ss{p} > 0$ a length scale of the periodic pattern, and $p > 0$ the period;
        and where the application of $\sin$ is elementwise;
    \item[sawtooth:]
        a sawtooth process with a random frequency, direction, and phase:
        \begin{equation}
             f = \omega \lra{\vx, \vu}_2 + \phi \mod 1
        \end{equation}
        where $\omega \sim \Unif(\Omega)$ is the frequency of the sawtooth wave, $\vu \sim \Unif(\set{\vx \in \R^{d_x} : \norm{\vx}_2 = 1})$ the direction, and $\phi \sim \Unif([0, 1])$ the phase;
    \item[mixture:]
        with equal probability, sample $f$ from the EQ process, Mat\'ern--$\tfrac52$ process, weakly periodic process, or sawtooth process.
\end{itemize}
We will call these stochastic processes the \emph{data processes}.
The data processes are stochastic processes with $d_x$-dimensional inputs and one-dimensional outputs.
We will turn them into processes with $d_y$-dimensional outputs according to the following procedure:
sample from the one-dimensional-output prior $d_y$ times;
and, for these $d_y$ samples, take $d_y$ different linear combinations.

We choose the parameters of the data processes based on the input dimensionality $d_x$:
\begin{align}
     % d_x&=1\colon\quad&
    \ell &= c \cdot\tfrac14, &
    \ell\ss{d} &= c \cdot\tfrac12, &
    \ell\ss{s} &= c, &
    p &= c \cdot\tfrac14, &
    \Omega &= [c^{-1} \cdot 2, c^{-1} \cdot 4]
\end{align}
with $c = \sqrt{d_x}$.
Scaling with the input dimensionality aims to roughly ensure that data with one-dimensional inputs and data with two-dimensional inputs are equally difficult.
\Cref{fig:synthetic_sawtooth} illustrates the sawtooth data process in all four configurations.

\begin{figure}[t]
    \centering
    \small
    \hfill
    \begin{subfigure}{0.36\linewidth}
        \includegraphics[height=3.9cm]{figures/sawtooth_x1y1.pdf}\hfill~
        \caption{$d_x = 1$, $d_y = 1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.63\linewidth}
        \hspace{5pt}\includegraphics[height=3.9cm]{figures/sawtooth_x1y2.pdf}\hfill~
        \caption{$d_x = 1$, $d_y = 2$}
    \end{subfigure}\\[1em]
    \begin{subfigure}{0.36\linewidth}
        ~\hfill\includegraphics[height=3.9cm]{figures/sawtooth_x2y1.pdf}
        \caption{$d_x = 2$, $d_y = 1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.63\linewidth}
        ~\hfill\includegraphics[height=3.9cm]{figures/sawtooth_x2y2.pdf}
        \caption{$d_x = 2$, $d_y = 2$}
    \end{subfigure}
    \hfill
    \caption[
        Samples from the sawtooth data process
    ]{
        Samples from the sawtooth data process with one and two-dimensional inputs ($d_x = 1$ and $d_x = 2$) and one and two-dimensional outputs ($d_y = 1$ and $d_y = 2$)
    }
    \label{fig:synthetic_sawtooth}
\end{figure}

We will construct data sets by sampling inputs uniformly at random from
$\X = [-2, 2]^{d_x}$
and then sampling outputs from one of the data processes.
We will colloquially call $\X$ the \emph{training range}.
For the EQ, Mat\'ern--$\frac52$, and weakly periodic process, but not for the sawtooth process\footnote{
    The sawtooth process is already challenging enough.
},
we also add independent Gaussian noise with variance $0.05$.
The numbers of context and target points are as follows.
For the EQ, Mat\'ern--$\frac52$, and weakly periodic process,
the number of context points is chosen uniformly at random from $\set{0, \ldots, 30 \cdot d_x}$ and
the number of targets points is fixed to $50 \cdot d_x$.
For the sawtooth and mixture process,
the number of context points is chosen uniformly at random from $\set{0, \ldots, 30}$ if $d_x = 1$ and $\set{0, \ldots, 75 \cdot d_x}$ otherwise; and
the number of targets points is fixed to $100 \cdot d_x$.
In the case of a multidimensional-output data process, we separately sample the number and positions of the context and target inputs for every output dimension.

For every data process and each of the four configurations, we evaluate every model in three different ways.
First, we evaluate the model on data generated exactly like the training data.
This task is called \emph{interpolation} and abbreviated ``int.'' in the tables of results.
The interpolation task measures how well a model fits the data
and is the primary measure of performance.
Second, we evaluate the model on data with inputs sampled from $[2, 6]^{d_x}$.
This task is called \emph{out-of-input-distribution (OOID) interpolation} and abbreviated ``OOID'' in the tables of results.
OOID interpolation measures how well a model generalises to data sampled from other regions of the input space.
Third, we evaluate the model on data with context inputs sampled from $[-2, 2]^{d_x}$ and target inputs sampled from $[2, 6]^{d_x}$.
This task is called \emph{extrapolation} and abbreviated ``ext.'' in the tables of results.
The extrapolation task measures how well predictions based on data in the training range generalise to other regions of the input space.

For this experiment,
the learning rate is $3\cdot10^{-4}$,
the margin is $0.1$,
and the points per unit is $64$.
We trained the models for 100 epochs.
Due to an error in the cross-validation procedure, we did not use cross-validation, but used the model at epoch 100.

For the kernel architecture of the FullConvGNP, we reduce the points per unit and the number of channels in the U-Net by a factor two.
For the ConvLNP with two-dimensional inputs, we reduce the number of outputs channels in the U-Net by a factor $\sqrt{2}$; and, for training and cross-validation, we reduce the number of samples of the ELBO objective to one and the number of samples for the ML objective to five.

\subsection{Multi-Modality of Predictions by AR ConvCNP}
\label{app:sawtooth-collapse}

\Cref{fig:synthetic_non-Gaussian_sawtooth_collape} demonstrates multi-modality of predictions by the AR ConvCNP trained on the sawtooth process.
Note that the prediction is bimodal for one and two observations, and collapses to a single mode upon observing the third observation.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sawtooth_collapse.pdf}
    \caption[
        Multi-modality of predictions by the AR ConvCNP
    ]{
        Multi-modality of predictions by the AR ConvCNP.
        Shows four observations sampled from the sawtooth process.
        In the four rows, these four observations are revealed one data point at a time.
        Every row also shows a kernel density estimate of the prediction at $x = 0$.
        Filled regions are central 95\%-credible regions.

    }
    \label{fig:synthetic_non-Gaussian_sawtooth_collape}
\end{figure}



\subsection{Full Results}
We the show the full results for all data sets and tasks in \cref{tab:synthetic_weakly-periodic-2,tab:synthetic_weakly-periodic-1,tab:synthetic_mixture-2,tab:synthetic_mixture-1,tab:synthetic_sawtooth-2,tab:synthetic_sawtooth-1,tab:synthetic_matern-2,tab:synthetic_matern-1,tab:synthetic_eq-2,tab:synthetic_eq-1,tab:synthetic_non-Gaussian,tab:synthetic_Gaussian}. 
The AR ConvCNP consistently shows very strong performance compared to other NP models.
%
Note that the FullConvGNP takes much longer to train than the ConvCNP (\cref{fig:timing}), and cannot be applied to tasks with 2-dimensional input spaces.



\newcommand{\onedtwoddescription}{
    Shows for one-dimensional inputs (1D; $d_x=1$) and two-dimensional inputs (2D; $d_y=2$) the performance for
    interpolation within the range $[-2, 2]^{d_x}$ where the models were trained  (``Int.'');
    interpolation within the range $[2, 6]^{d_x}$ which the models have never seen before (``OOID'');
    and extrapolation from the range $[-2, 2]^{d_x}$ to the range $[2, 6]^{d_x}$ (``Ext.'').
    Models are ordered by interpolation performance for one-dimensional inputs.
}
\newcommand{\oneddescription}{
    Shows for one-dimensional outputs ($d_y=1$) and two-dimensional outputs ($d_y=2$) the performance for
    interpolation within the range $[-2, 2]$ where the models where trained  (``Int.'');
    interpolation within the range $[2, 6]$ which the models have never seen before (``OOID'');
    and extrapolation from the range $[-2, 2]$ to the range $[2, 6]$ (``Ext.'').
    Models are ordered by interpolation performance.
}
\newcommand{\twoddescription}{
    Shows for one-dimensional outputs ($d_y=1$) and two-dimensional outputs ($d_y=2$) the performance for
    interpolation within the range $[-2, 2]^2$ where the models where trained  (``Int.'');
    interpolation within the range $[2, 6]^2$ which the models have never seen before (``OOID'');
    and extrapolation from the range $[-2, 2]^2$ to the range $[2, 6]^2$ (``Ext.'').
    Models are ordered by interpolation performance.
}
\newcommand{\latentvariabledescription}{
    The latent variable models are trained and evaluated with the ELBO objective (E);
    trained and evaluated with the ML objective (M);
    and trained with the ELBO objective and evaluated with the ML objective (E--M).
}
\newcommand{\convabbrevdescription}{
    ``Conv'' is abbreviated with ``Cv''.
}
\newcommand{\numbersdescription}{
    Errors indicate the central 95\%-confidence interval.
    Numbers which are significantly best ($p < 0.05$) are boldfaced.
    Numbers which are very large are marked as failed with ``F''.
    Numbers which are missing could not be run.
}
\newcommand{\diagonaldescription}{
    Diagonal GP refers to predictions by the ground-truth Gaussian processes without correlations.
}
\newcommand{\trivialdescription}{
    Trivial refers to predicting the empirical means and standard deviation of the test data.
}

\FloatBarrier

\begin{table}[t]
    \centering
    \caption
    [
        Summary of results for the Gaussian synthetic experiments
    ]
    {
        For the Gaussian experiments, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \onedtwoddescription
        \latentvariabledescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_Gaussian}
    \small
    \setlength{\tabcolsep}{3pt}
    \subimport{results/}{synthetic_gaussian_average.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Summary of results for the non-Gaussian synthetic experiments
    ]
    {
        For the non-Gaussian experiments, average log-likelihoods normalised by the number of target points.
        \onedtwoddescription
        \latentvariabledescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_non-Gaussian}
    \small
    \setlength{\tabcolsep}{3pt}
    \subimport{results/}{synthetic_non-gaussian_average.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the EQ synthetic experiments with 1D inputs
    ]
    {
        For the EQ synthetic experiments with one-dimensional inputs, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \oneddescription
        \latentvariabledescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_eq-1}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \subimport{results/}{synthetic_eq1-1.tex}
    \subimport{results/}{synthetic_eq1-2.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the EQ synthetic experiments with 2D inputs
    ]
    {
        For the EQ synthetic experiments with two-dimensional inputs, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \twoddescription
        \latentvariabledescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_eq-2}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \subimport{results/}{synthetic_eq2-1.tex}
    \subimport{results/}{synthetic_eq2-2.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the Mat\'ern--$\tfrac52$ synthetic experiments with 1D inputs
    ]
    {
        For the Mat\'ern--$\frac52$ synthetic experiments with one-dimensional inputs, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \oneddescription
        \latentvariabledescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_matern-1}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \subimport{results/}{synthetic_matern1-1.tex}
    \subimport{results/}{synthetic_matern1-2.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the Mat\'ern--$\tfrac52$ synthetic experiments with 2D inputs
    ]
    {
        For the Mat\'ern--$\frac52$ synthetic experiments with two-dimensional inputs, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \twoddescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_matern-2}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \subimport{results/}{synthetic_matern2-1.tex}
    \subimport{results/}{synthetic_matern2-2.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the weakly periodic synthetic experiments with 1D inputs
    ]
    {
        For the weakly periodic synthetic experiments with one-dimensional inputs, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \oneddescription
        \latentvariabledescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_weakly-periodic-1}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \subimport{results/}{synthetic_weakly-periodic1-1.tex}
    \subimport{results/}{synthetic_weakly-periodic1-2.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the weakly periodic synthetic experiments with 2D inputs
    ]
    {
        For the weakly periodic synthetic experiments with two-dimensional inputs, average Kullback--Leibler divergences of the posterior prediction map $\pi_y$ with respect to the model normalised by the number of target points.
        \twoddescription
        \latentvariabledescription
        \diagonaldescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_weakly-periodic-2}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    \subimport{results/}{synthetic_weakly-periodic2-1.tex}
    \subimport{results/}{synthetic_weakly-periodic2-2.tex}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the sawtooth synthetic experiments with 1D inputs
    ]
    {
        For the sawtooth synthetic experiments with one-dimensional inputs, average log-likelihoods normalised by the number of target points.
        \oneddescription
        \latentvariabledescription
        % \convabbrevdescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_sawtooth-1}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    % One should never use `\centerline`, but time is running out...
    \centerline{\subimport{results/}{synthetic_sawtooth1-1.tex}%
    \subimport{results/}{synthetic_sawtooth1-2.tex}%
    \hspace*{-4pt}}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the sawtooth synthetic experiments with 2D inputs
    ]
    {
        For the sawtooth synthetic experiments with two-dimensional inputs, average log-likelihoods normalised by the number of target points.
        \twoddescription
        \latentvariabledescription
        % \convabbrevdescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_sawtooth-2}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    % One should never use `\centerline`, but time is running out...
    \centerline{\subimport{results/}{synthetic_sawtooth2-1.tex}%
    \subimport{results/}{synthetic_sawtooth2-2.tex}%
    \hspace*{-4pt}}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the mixture synthetic experiments with 1D inputs
    ]
    {
        For the mixture synthetic experiments with one-dimensional inputs, average log-likelihoods normalised by the number of target points.
        \oneddescription
        \latentvariabledescription
        % \convabbrevdescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_mixture-1}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    % One should never use `\centerline`, but time is running out...
    \centerline{\subimport{results/}{synthetic_mixture1-1.tex}%
    \subimport{results/}{synthetic_mixture1-2.tex}%
    \hspace*{-4pt}}
\end{table}

\begin{table}[t]
    \centering
    \caption
    [
        Results for the mixture synthetic experiments with 2D inputs
    ]
    {
        For the mixture synthetic experiments with two-dimensional inputs, average log-likelihoods normalised by the number of target points.
        \twoddescription
        \latentvariabledescription
        % \convabbrevdescription
        \trivialdescription
        \numbersdescription
    }
    \label{tab:synthetic_mixture-2}
    \footnotesize
    \setlength{\tabcolsep}{1pt}
    % One should never use `\centerline`, but time is running out...
    \centerline{\subimport{results/}{synthetic_mixture2-1.tex}%
    \subimport{results/}{synthetic_mixture2-2.tex}%
    \hspace*{-4pt}}
\end{table}


\FloatBarrier

\newpage
\section{Details of Sim-to-Real Transfer Experiments} \label{app:predprey}

\subsection{Description of Experiment}

Our goal will be to make predictions for the famous hare--lynx data set.
The hare--lynx data set is a time series from 1845 to 1935 recording yearly population counts of a population of Snowshoe hares and a population of Canadian lynx \citep{MacLulich:1937:Fluctuations_in_the_Numbers_of_the_Varying_Hare}.
A digital version extracted from the original graph by \citet{MacLulich:1937:Fluctuations_in_the_Numbers_of_the_Varying_Hare}
is available by \cite{Hundley:2022:Introduction_to_Mathematical_Modelling}.\footnote{See \url{http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt}.}
\citet{Hundley:2022:Introduction_to_Mathematical_Modelling},
the author of this digital source,
says that other authors caution that the hare--lynx data is actually a composition of multiple time series, and presents the data with that caution.
We, therefore, also present the data with this caution.
\Cref{fig:hare-lyxn_data} visualises the hare--lynx data set.

\begin{figure}[t]
    \centering
    \small
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/harelynx_data.pdf}
        \caption{
            Illustration of the hare--lynx data set
        }
        \label{fig:hare-lyxn_data}
    \end{subfigure} \\[1em]
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/harelynx_simulation.pdf}
        \caption{
            Four samples from the proposed stochastic version of the Lotka--Volterra equations \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app}.
            The parameters of \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app} are sampled according to
             \cref{tab:stoch_Lotka-Volterra_parameters}.
        }
        \label{fig:samples_pred-prey_simulator}
    \end{subfigure}
    \caption{
        Hare--lynx data set and proposed stochastic simulator
    }
\end{figure}

To make predictions for the hare--lynx data set, we use the Lotka--Volterra equations
\citep{Lotka:1910:Contribution_to_the_Theory_of,Volterra:1926:Variazioni_e_Fluttuazioni_del_Bumero},
also called the predator--prey equations.
The Lotka--Volterra equations are an idealised mathematical model for the population counts of a prey population and a predator population:\index{Lotka--Volterra equations}
\begin{align}
    \hspace{-4cm}\text{prey population:}\qquad
    & x'(t) = \hspace{6.5pt}\alpha x(t) - \beta x(t) y(t), \\
    \hspace{-4cm}\text{predator population:}\qquad
    & y'(t) = -\delta y(t) + \gamma x(t) y(t). 
\end{align}
These differential equations say that the prey population naturally grows exponentially with rate $\alpha$,
and
that the predator population naturally decays exponentially with rate $\delta$.
In addition, the predators hunt the prey.
The resulting additional growth in the predator population
and
the resulting additional decrease in the prey population
are both proportional to the product of the densities.
In this idealised mathematical form, the population counts converge to a smooth, noiseless limit cycle and then perfectly track this limit cycle ever after.
This is unlike real-world predator--prey population counts, which exhibit noisy behaviour and imperfect cycles.
We therefore consider a stochastic version of the Lotka--Volterra equations, given by the following two coupled stochastic differential equations:
\begin{align}
    \label{eq:stoch_Lotka-Volterra_prey_app}
    \sd X_t &= \hspace{8pt}\alpha X_t \isd t - \beta Y_t X_t \isd t + \sigma X^\nu_t \isd W^{(1)}_t, \\
    \label{eq:stoch_Lotka-Volterra_predator_app}
    \sd Y_t &= -\gamma X_t \isd t + \delta Y_t X_t \isd t + \sigma Y^\nu_t \isd W^{(2)}_t
\end{align}
where $W^{(1)}$ and $W^{(2)}$ are two independent Brownian motions.
Compared to the Lotka--Volterra equations, \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app} have two additional terms, $\sigma X^\nu_t \isd W^{(1)}_t$ and $\sigma Y^\nu_t \isd W^{(2)}_t$, which introduce noisy behaviour.
In these terms, multiplying by $X^\nu_t$ and $Y^\nu_t$ makes the noise go to zero when $X_t$ and $Y_t$ become small, ensuring that $X_t$ and $Y_t$ remain positive.
In addition,
we multiply by a parameter $\sigma > 0$ to control the magnitude of the noise,
and we raise $X_t$ and $Y_t$ to a power $\nu > 0$ to control how quickly the noise grows as $X_t$ and $Y_t$ grow.
Namely, $X_t$ naturally grows exponentially, so, by adding noise of magnitude proportional to $X_t$, we risk large spikes in the prey population.
To moderate this behaviour, we choose $\nu$ to be strictly less than one.
Finally, to obtain a variety of magnitudes of population counts, we multiply the realisation with a scale $\eta$.

After simulating from \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app} a few times, we settle on $\nu = \tfrac16$. 
For the remainder of the parameters, we simply manually play around with
\eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app}, settle on parameter ranges that look reasonable,
and randomly sample parameters from those intervals.
\Cref{tab:stoch_Lotka-Volterra_parameters} summarises the sampling distributions for all parameters of \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app}. 
\Cref{fig:samples_pred-prey_simulator} shows four samples from the proposed stochastic model.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{ll}
        \toprule
        Parameter & Distribution \\ \midrule
        Initial condition $X_{-10}$ & $\Unif([5, 100])$ \\ 
        Initial condition $Y_{-10}$ & $\Unif([5, 100])$ \\
        $\alpha$ & $\Unif([0.2, 0.8])$ \\
        $\beta$ & $\Unif([0.04, 0.08])$ \\
        $\gamma$ & $\Unif([0.8, 1.2])$ \\
        $\delta$ & $\Unif([0.04, 0.08])$ \\
        $\nu$ & Fixed to $1/6$ \\
        $\sigma$ & $\Unif([0.5, 10])$ \\
        $\eta$ & $\Unif([1, 5])$ \\
        \bottomrule
    \end{tabular}
    \caption[
        Parameters of the stochastic Lotka--Volterra equations
    ]
    {
        Sampling distributions for the parameters of the stochastic version of the Lotka--Volterra equations \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app}.
        These equations are simulated on a dense grid spanning $[-10, 100]$.
        The table also shows the distribution for the initial conditions at $t = -10$.
        To not depend too heavily on these initial conditions, the simulation results on $[-10, 0]$ are discarded.
    }
    \label{tab:stoch_Lotka-Volterra_parameters}
\end{table}

To generate a meta--data set,
we simulate \eqref{eq:stoch_Lotka-Volterra_prey_app} and \eqref{eq:stoch_Lotka-Volterra_predator_app} on a dense grid spanning 110 years, throw away the first 10 years, and retain between 150 and 250 data points for $X_t$ and $Y_t$.
The numbers of data points and the locations of the data points are sampled separately for $X_t$ and $Y_t$.
Hence, whereas the hare--lynx data is regularly spaced and the populations are always simultaneously observed, our simulator generates data at arbitrary and nonsimultaneous points in time.
We split these data sets into context and target sets in three different ways.
To train the models, for every batch, we randomly choose one of the interpolation, forecasting or reconstruction tasks by rolling a three-sided die.
We will also perform these tasks on the real hare--lynx data;
in that case, for interpolation, we let the number of target points per output be between one and fifteen.
The tasks on simulated and real data are similar, but slightly differ in the number of context and target points.

To deal with the positivity of population counts, we transform the marginals of all models to distributions on $(0, \infty)$ by pushing the marginals through $x \mapsto \log(1 + x)$.

For this experiment,
the learning rate is $1\cdot10^{-4}$,
the margin is $1$,
and the points per unit is $4$.
We trained the models for 200 epochs.

The convolutional models use a U-Net architecture with seven layers instead of six
where, in the first layer, the stride is one instead of two.
For the kernel architecture of the FullConvGNP, we reduce the points per unit and the number of channels in the U-Net by a factor two.

\subsection{Full Results}

In \cref{tab:predprey_results_full}, we present the full results for the sim-to-real experiments.

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{1.5pt}
    \subimport{results/}{predprey_results_full.tex}
    \vspace{-1em}
    \caption[
        Results for the predator--prey experiments
    ]{
        Normalised log-likelihoods in the predator--prey experiments.
        Shows the performance for
        interpolation (``Int.''),
        forecasting (``For.''),
        and reconstruction (``Rec.'')
        on simulated (``S'')
        and real (``R'') data.
        Models are ordered by interpolation performance on simulated data.
        \latentvariabledescription
        \numbersdescription
    }
    \label{tab:predprey_results_full}
\end{table}
\FloatBarrier

\newpage
\section{Details of Electroencephalography Experiments} \label{app:eeg}

\begin{figure}[t]
    \centering
    \small
    \includegraphics[width=\linewidth]{figures/eeg_example.pdf}
    \caption[
        Example of trial in the EEG data set
    ]{
        Example of trial in the EEG data set.
        Note that the signals for all electrodes appear correlated, but are subtly different. 
    }
    \label{fig:eeg_example}
\end{figure}

We explore an electroencephalography data set collected from 122 subjects
\citep{Begleiter:2022:EEG_Database_Data_Set}.
There are two groups of subjects: alcoholic and control.
Every subject was subjected to a single stimulus or two stimuli,
and their response was measured with 64 electrodes placed on a subject's scalp.
These measurements are in terms of \emph{trials}, where a trial consists of 256 samples of the electrodes spanning one second.
The data sets contains up to 120 trials for each subject.
The data is available at \url{https://archive.ics.uci.edu/ml/datasets/eeg+database}
and the collection is described in detail by \citet{Zhang:1995:Event_Related_Potentials_During_Object}.
In this experiment, we focus only on seven frontal electrodes: FZ, F1, F2, F3, F4, F5, and F6.
\Cref{fig:eeg_example} illustrates a trial of a subject, showing the samples for these seven electrodes.

We randomly split all subjects into three sets:
an evaluation set consisting of ten subjects,
a cross-validation set consisting of ten other subjects,
and a training set consisting of all remaining subjects.
For each of these sets, we create a meta--data set by aggregating the trials for all subjects.
We split every trial into a context and target set in the same three ways as for the predator--prey experiment.
First, for all seven electrodes separately, randomly designate between 50 and 256 points to be the target points and let the remainder (between 0 and 206) be the context points.
This task is called \emph{interpolation} and is the primary measure of performance.
Additionally, randomly choose one of the seven electrodes and, for that choice, split the data in two exactly like for forecasting.
For all other electrodes, append all data to the context set.
This task is called \emph{reconstruction} and measures the model's ability to infer a signal for one electrode from the others.
We train all models on the interpolation task,
and evaluate the models on the interpolation and reconstruction task.

For this experiment,
the learning rate is $5\cdot10^{-5}$,
the margin is $0.1$,
and the points per unit is $256$.
We trained the models for 1000 epochs.
For the FullConvGNP, the learning rate is $2\cdot 10^{-4}$.
The training run for the FullConvGNP was terminated after 84 hours, reaching epoch 127.

The convolutional models use a U-Net architecture where, in the first layer, the stride is one instead of two.
In addition, the number of channels are adjusted as follows:
the ConvCNP and ConvGNP use 128 channels, the ConvLNP uses 96 channels, and the FullConvGNP uses 64 channels.
The length scales of the Gaussian kernels of the convolutional model is initialised to $0.77 / 256$.
To scale to seven outputs, the deep set--based and attentive models reuse the same encoder for every output dimension.


\FloatBarrier

\newpage
\section{Details of Environmental Data Assimilation Experiment}\label{app:antarctic}

In this section we provide further details on the Antarctic cloud cover data assimilation experiment described in \cref{sec:environmental-modelling}.

\subsection{Data considerations}\label{apx.data_considerations}

\paragraph{Data sources} Daily-averaged cloud cover reanalysis data was obtained from ERA5 \citep{hersbach_era5_2020}.
An Antarctic land mask and elevation field was obtained from the BedMachine dataset \citep{Morlighem}.

\Cref{fig:clt_hist} shows an empircal density of the ERA5 cloud cover values calculated over the models' training period of 2000-2013.
The spikes at 0 and 1 correspond largely to values of exactly 0 and exactly 1.
This motivates the beta-categorical likelihood described in \cref{sec:environmental-modelling}.

\begin{figure}[h]
    \small
    \centering
    \includegraphics[]{figures/clt_hist.png}
    \caption{
        Empirical density of ERA5 cloud cover fraction computed over the period 2000-2013.
    }
    \label{fig:clt_hist}
\end{figure}

\paragraph{Data preprocessing} The cloud cover data and land/elevation auxiliary data were regridded from lat/lon to a Southern Hemisphere Equal Area Scalable Earth 2 (EASE2) grid at 25 km resolution and cropping to a size of 280 $\times$ 280.
This centres the data on the South Pole.

\paragraph{Data normalisation} We normalised the data before passing it to the convolutional NP models.
The cloud cover and land mask data already took appropriate normalised values in $[ 0, 1 ]$.
The elevation field was normalised from metres to values in $[0, 1]$.

The input coordinates $x$ were normalised from metres to take values in $[-1, 1]$.


\subsection{Model considerations}
Here we provide details on the training procedure and architectures for each of the convolutional NP models in the Antarctic data assimilation experiment.

\paragraph{Generating the training, validation, and test tasks} Following meta-learning principles, we collect data from day $\tau$ into a task $D_\tau$.
Each task $D_\tau$ was generated by first drawing the integer number of simulated cloud cover context points $N\us{(c)} \sim \text{Unif} \{1, 2, \dots 500\}$.
Letting $N\us{(c)}$ vary encourages the model to learn to deal with both data-sparse and data-rich scenarios.
The number of target points $N\us{(t)}$ was fixed to a value of 2,000.

Next, the input locations $\vx_\tau\us{(c)}$ and $\vx_\tau\us{(t)}$ were sampled uniformly at random across the entire $280\times280$ input space and the corresponding $\vy_\tau$ values were sampled without observation noise.

For the training dates, the random seed used for generating $D_\tau$ is updated every epoch, allowing for an infinitely growing simulated training data set.
In contrast, for the validation and test dates, the random seeds were held fixed so that metrics computed over the validation and test sets are not stochastic.

\paragraph{Training procedure} Each model was trained for 150 epochs on 14 years of data from 2000--2013.
An Adam optimiser was used with a learning rate of $5 \times 10^{-5}$ and a batch size of 2.
For the loss functions we use a negative log-likelihood loss function for the ConvCNP and ConvGNP. For the ConvLNP we use the ELBO objective and fix the variance of the observation noise to $0.01$ for the first four epochs.
Validation data from 2014--2017 was used for checkpointing the model weights using the per-datapoint predictive log-likelihood.
The two year period of 2018--2019 data was reserved for the test set.

The time taken to train each model on a Tesla V100 GPU is as follows:
\begin{itemize}
    \item ConvCNP: $25.0$ hours,
    \item ConvGNP: $27.5$ hours,
    \item ConvLNP: $43.6$ hours.
\end{itemize}

\paragraph{Architectures} For each model, the U-Net component of the encoder uses $5\times5$ convolutional kernels with the following sequence of channel numbers (d.s.\ = $2\times2$ downsample layer, u.s.\ = $2\times2$ upsample layer): $\smash{16 \xrightarrow{\text{d.s.}} 32 \xrightarrow{\text{d.s.}} 64 \xrightarrow{\text{d.s.}} 128 \xrightarrow{\text{u.s.}} 64 \xrightarrow{\text{u.s.}} 32 \xrightarrow{\text{u.s.}} 16}$.
We use bilinear resize operators for the upsampling layers to fix checkerboard artifacts that we encountered when using standard zero-padding upsampling \citep{odena_deconvolution_2016}.
We use a margin of $0.1$ and $150$ points per unit for the encoder's internal discretisation.
The length scales of the Gaussian kernels for both the encoder and decoder SetConv layers are set to $1 / 150$ and held fixed during training.
These architecture choices result in a receptive field of $0.433$
in normalised input space, or roughly \SI{1,500}{km} in raw input space, spanning around 20\% of the region in \cref{fig:antarctic_main} in either dimension. 

For the ConvGNP we use 128 basis functions for the low-rank covariance parameterisation described in \citealt{Markou:2021:Efficient_Gaussian_Neural_Processes_for}. 

For the ConvLNP we use an 8-dimensional latent variable and evaluate the ML objective \citep{Foong:2020:Meta-Learning_Stationary_Stochastic_Process_Prediction} using 8 latent samples.

The number of learnable parameters for each model is as follows:
\begin{itemize}
    \item ConvCNP: \SI{618}{k},
    \item ConvGNP: \SI{621}{k},
    \item ConvLNP: \SI{1,234}{k} (increase due to second UNet architecture after the latent variable).
\end{itemize}
The difference in parameters from switching to a beta-categorical likelihood from a Gaussian likelihood is negligible.

\paragraph{Input data} Each model receives two context sets as input. The first contains observations of the simulated ERA5 daily-average cloud cover. The second contains a set of 6 gridded auxiliary variables. These are elevation, land mask, $\cos(2\pi\times\text{day of year}/365)$, $\sin(2\pi\times\text{day of year}/365)$, $x_1$, and $x_2$.
The elevation and land mask auxiliary fields allow the models to predict spatial non-stationarity.
For example, the convolutional filters of the model's encoder could learn how cloud cover around the Antarctic coastline behaves differently to the centre of the continent. 
The $\cos$ and $\sin$ variables inform the model at what time of year $D_\tau$ corresponds to, helping with learning seasonal variations in the data.
The $x_1$ and $x_2$ inputs again help with breaking translation equivariance in the convolutional filters by informing the model where in input space the data corresponds to.

\subsection{Antarctic cloud cover model samples}
\label{app:antarctic_samples}

\Cref{fig:antarctic_samples_all} gives a detailed breakdown of sample extrapolation ability, showing seven samples from the four Antarctic cloud cover models.
The AR ConvCNP samples display remarkable structure and variation while still closely interpolating the context points.
The samples also provide interesting scenarios in the gaps between the context points on the left hand side.

In contrast, the ConvCNP samples are incoherent, underestimating the probability of joint events.

The ConvLNP samples were generated by sampling from the latent variable and then computing the mean of the marginal distributions.
As is visible in \Cref{fig:antarctic_samples_all}, the ConvLNP displays low sample variance with respect to the latent variable.
However, to the best of our knowledge we used a faithful reproduction of the original ConvLNP model, so we leave a more rigorous treatment of this undesirable behaviour to future work.

For all the non-AR models, the limited receptive field size leads to samples on the right hand side becoming independent of the context observations on the left hand side after roughly \SI{750}{km} of distance from them.
This results in the models defaulting to some mean representation of the data.
It is interesting to see that all the non-AR models display similar marginal mean structure, with greater cloud cover towards the centre of the continent and lower cloud cover towards the coastline, followed by increased cloud levels over the Southern Ocean.

The AR samples were drawn on a sparse $70$x$70$ grid spanning the entire input space to save compute time.
The ConvCNP model was then conditioned on these AR samples and the predictive mean was computed over the dense $280$x$280$ target space.
It took 14 minutes to generate these AR ConvCNP samples on a Tesla V100 GPU.

\begin{figure}[h]
    \small
    \centering
    \vspace{-0.5cm}
    \includegraphics[trim=1cm 0cm 1cm 0cm,clip, scale=1.1]{figures/antarctic_samples_all.png}
    \caption{
        Seven samples from each model in the Antarctic cloud cover sample extrapolation task for 25/06/2018.
    }
    \label{fig:antarctic_samples_all}
\end{figure}


\FloatBarrier

\newpage
\section{Details of Climate Downscaling Experiments}
\label{app:downscaling}

\subsection{Description of Experiment}

The MLP ConvGNP \citep{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable} can be used to successfully model dependencies between outputs in a statistical downscaling task, improving log-likelihoods over the MLP ConvCNP \citep{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for} and enabling coherent samples.
In this experiment, we demonstrate that the AR ConvCNP can also be used for this purpose.

\begin{figure}[t]
    \small
    \centering
    \includegraphics[height=4.5cm]{figures/germany_locations.png}
    \caption{
        Locations of the 589 weather stations around Germany in the downscaling experiments.
    }
    \label{fig:locations_germany}
\end{figure}

\begin{table}[]
    \centering
    \begin{tabular}[t]{lcccc}

\toprule
Variable & Level & Units \\
\midrule
\multicolumn{3}{c}{Surface}\\
\midrule
Maximum temperature & 2 m & degrees Celsius \\
Mean temperature & 2 m & degrees Celsius \\
Northward wind & 10 m & knots \\
Eastward wind & 10 m & knots \\
\midrule
\multicolumn{3}{c}{Upper atmosphere}\\
\midrule
Specific humidity & 850, 700, and 500 hPa & g/kg \\
Mean temperature & 850, 700, and 500 hPa & degrees Celsius \\
Northward wind & 850, 700, and 500 hPa & knots \\
Eastward wind & 850, 700, and 500 hPa & knots \\
\midrule
\multicolumn{3}{c}{Invariant}\\
\midrule
Angle of sub-grid-scale orography & surface &  \\
Anisotropy of sub-grid-scale orography & surface &  \\
Standard deviation of filtered sub-grid-scale orography & surface &  \\
Standard deviation of orography & surface & \\
Geopotential & surface & J/kg \\
Longitude & surface & degrees \\
Latitude & surface & degrees \\
\midrule
\multicolumn{3}{c}{Temporal}\\
\midrule
\multicolumn{3}{l}{Fractional position in the year $t$ transformed with $t \mapsto (\cos(2\pi t), \sin(2 \pi t))$}  \\
\bottomrule
\end{tabular}
    \caption{ERA-Interim reanalysis predictors.}
    \label{tab:era_variables}
\end{table}


The goal of this experiment is to estimate the maximum daily temperature at 589 weather stations around Germany.
To estimate these temperatures, we follow \citet{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for} and use 25 coarse-grained ERA-Interim reanalysis variables \citep{Dee:2011:The_ERA-Interim_Reanalysis_Configuration_and} in combination with \SI{1}{km}--resolution elevation data \citep{EROSC:1997:GPTOTO30}.
We also consider a second setup where we reveal some of the weather station observations.
These revealed weather station observations can be used by the models to aid downscaling performance.

The ERA-Interim reanalysis variables considered are tabulated in \cref{tab:era_variables}.
%
In contrast to previous downscaling work, which degrade reanalysis data to between 2$^\circ$ and 2.5$^\circ$, we opt to use the ERA-Reanalysis data at the native 0.75$^\circ$ resolution, consistent with the latest high-resolution climate models with horizontal resolution ranging from 0.5$^\circ$ to 1.0$^\circ$.
All variables are spatially subset to between 6$^\circ$ to 16$^\circ$ longitude and 47$^\circ$ to 55$^\circ$ latitude, covering Germany. 
The weather station data are a subselection from of the European Climate Assessment \& Dataset \citep{Klein_Tank:2002:Daily_Dataset_of_20th-Century_Surface} and are available at \url{https://www.ecad.eu};
we use the blended data.
Like for the ERA-Reanaysis variables, we take the weather stations located within the aforementioned square.
The locations of the weather stations around Germany are visualised in \cref{fig:locations_germany}.
The \SI{1}{km}--resolution elevation data is taken from the United States Geological Survey GTOPO30 elevation data set available at
\url{https://doi.org/10.5066/F7DF6PQS}.
This provides global elevation data at 30-arc second resolution, which is approximately \SI{1}{km}.


Following the VALUE framework \citep{Maraun:2015:VALUE_A_Framework_to_Validate}, we consider all days of the years 1979--2008 and split these years into five folds.
We use the first four folds (spanning 1979--2003) for training, holding out the last 1000 days for cross-validation;
and use the fifth fold (spanning 2003--2008) for evaluation.

\subsection{Multiscale Convolutional Architecture}
\label{app:downscaling_arch}

Deploying the AR ConvCNP in this downscaling experiment comes with a significant challenge.
Because the elevation data has a fine resolution of \SI{1}{km}, we expect that predictions by the AR ConvCNP will vary roughly also on this length scale.
In the autoregressive sampling procedure (\cref{proc:ar}), samples from the model will be fed back into the model.
Therefore, the AR ConvCNP must handle context data which varies on a \SI{1}{km} spatial scale, which means that the discretisation of the AR ConvCNP must roughly be a \SI{1}{km}--resolution grid.
Unfortunately, making the discretisation this fine is prohibitively expensive and imposes prohibitive memory requirements.
It is this limitation that prevents us from extending the \citet{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for}'s MLP ConvCNP and \citet{Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}'s MLP ConvGNP to include additional weather station observations. 
We must therefore innovate on the AR ConvCNP design to come up with a convolutional architecture that can handle such a fine discretisation at reasonable computational expense.

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[remember picture]
        \node [align=center] (hr) at (0,0) {
            $\displaystyle
                \vz\ss{hr}(\vardot)
                = \mathsf{CNN}\ss{hr}\parens*{
                    \begin{bmatrix}
                        \tikzmarknode{zmr}{\vz\ss{mr}(\vardot)} \\
                        \mathsf{data}(D\ss{hr}) \\
                        \mathsf{density}(D\ss{hr})
                    \end{bmatrix}
                }
            $
        };
        \node [align=center, anchor=west] (mr) at ($(zmr.west) + (0, 2)$) {
            $\displaystyle
                \hspace{-4pt}\tikzmarknode{zmrstart}{\vz\ss{mr}(\vardot)}
                = \mathsf{CNN}\ss{mr}\parens*{
                    \begin{bmatrix}
                        \tikzmarknode{zlr}{\vz\ss{lr}(\vardot)} \\
                        \mathsf{data}(D\ss{mr}) \\
                        \mathsf{density}(D\ss{mr})
                    \end{bmatrix}
                }
            $
        };
        \draw [line, ->] ([yshift=-2pt]zmrstart.south) -- ([yshift=2pt]zmr.north);
        \node [align=center, anchor=west] (lr) at ($(zlr.west) + (0, 2)$) {
            $\displaystyle
                \hspace{-4pt}\tikzmarknode{zlrstart}{\vz\ss{lr}(\vardot)}
                = \mathsf{CNN}\ss{lr}\parens*{
                    \begin{bmatrix}
                        \mathsf{data}(D\ss{lr}) \\
                        \mathsf{density}(D\ss{lr})
                    \end{bmatrix}
                }
            $
        };
        \draw [line, ->] ([yshift=-2pt]zlrstart.south) -- ([yshift=2pt]zlr.north);
        \node [yshift=-5pt] (zmrmidway) at ($(zmrstart)!0.5!(zmr)$) {};
        \draw [thick, dashed]
            (current bounding box.west |- zmrmidway)
            --
            (current bounding box.east |- zmrmidway)
            node [pos=1, anchor=north east] {\small $0.01^\circ$ (\good), local to target points (\bad)}
        ;
        \node [yshift=-2pt] (zlrmidway) at ($(zlrstart)!0.5!(zlr)$) {};
        \draw [thick, dashed]
            (current bounding box.west |- zlrmidway)
            --
            (current bounding box.east |- zlrmidway)
            node [pos=0, anchor=south west, yshift=45pt] {\small $\tikzmarknode{res}{0.75^\circ}$ (\bad), \tikzmarknode{where}{covering all of Germany} (\good)}
            node [pos=0, anchor=north west] {\small $0.1^\circ$ (\mediumgood), covering a medium-sized square (\mediumgood)}
        ;
    \draw [line, line width=0.5pt, <-] ([yshift=-2pt]res.south) |- ++(0.35, -0.5) node [anchor=west] {\scriptsize resolution of discretisation};
    \draw [line, line width=0.5pt, <-] (where.north) |- ++(0, 0.3) node [anchor=south] {\scriptsize positioning of discretisation};
    \end{tikzpicture}
    \captionsetup{font=footnotesize}
    \caption[
        Multiscale architecture for the AR ConvCNP
    ]{
        \footnotesize
        Multiscale architecture for the AR ConvCNP.
        A cascade of three convolutional deep sets \citep{Gordon:2020:Convolutional_Conditional_Neural_Processes}
        representing a low-resolution, medium-resolution, and high-resolution component.
        Shows the resolution and positioning of the internal discretisation for every convolutional deep set.
        The context set $D = D\ss{lr} \cup D\ss{mr} \cup D\ss{hr}$ is also divided into
        a low-resolution $D\ss{lr}$, medium-resolution $D\ss{mr}$,
        and high-resolution component $D\ss{hr}$.
        The low-resolution context data $D\ss{lr}$ consists of the 25 coarse-grained ERA-Interim reanalysis variables.
        The medium-resolution $D\ss{mr}$ and high-resolution context data $D\ss{hr}$ both consist of the station observations and the \SI{1}{km}--resolution elevation data.
        The functions $\mathsf{data}(D)$ and $\mathsf{density}(D)$ produce respectively the data channel
        and density channel for context data $D$;
        see \citet{Gordon:2020:Convolutional_Conditional_Neural_Processes}.
        The variables $\vz\ss{lr}(\vardot)$, $\vz\ss{mr}(\vardot)$, and $\vz\ss{hr}(\vardot)$ represent intermediate representations as continuous functions,
        and the maps $\mathsf{CNN}\ss{lr}$, $\mathsf{CNN}\ss{mr}$, and $\mathsf{CNN}\ss{hr}$
        are translation-equivariant maps between functions on $\X$.
        Following the construction of the ConvCNP \citep{Gordon:2020:Convolutional_Conditional_Neural_Processes}, these maps are all implemented with convolutional neural networks (CNN)
        using a discretisation. 
        For $\mathsf{CNN}\ss{lr}$, the internal discretisation is the $0.75^\circ$-resolution grid
        corresponding to the 25 coarse-grained ERA-Interim reanalysis variables.
        For $\mathsf{CNN}\ss{mr}$, the internal discretisation is a $0.1^\circ$-resolution grid
        spanning $5^\circ$ more than the most extremal target inputs;
        the discretisation does \emph{not} depend on the context set.
        For $\mathsf{CNN}\ss{hr}$, the internal discretisation is a $0.01^\circ$-resolution grid
        spanning $0.25^\circ$ more than the most extremal target inputs;
        the discretisation also does \emph{not} depend on the context set.
    }
    \label{fig:multires_arch}
\end{figure}

The architecture that we propose is a \emph{multiscale architecture} operating on multiple spatial length scales.
Let us divide the context set $D = D\ss{lr} \cup D\ss{mr} \cup D\ss{hr}$ into
a low-resolution component $D\ss{lr}$, a medium-resolution component $D\ss{mr}$, and a high-resolution component $D\ss{hr}$.
Let the low-resolution component $D\ss{lr}$ consist of features of the context set that vary on a \emph{long} spatial length scale,
the medium-resolution component $D\ss{mr}$ of features that vary on a \emph{medium-range} spatial length scale,
and the high-resolution component $D\ss{hr}$ of features that vary on a \emph{short} spatial length scale. 
The central assumption of the architecture is that predictions for target points depend on precise short-length-scale details $D\ss{hr}$ nearby, but that this dependence weakens as we move away from the target point, starting to depend more on broad-stroke long-length-scale components $D\ss{lr}$.
For example, 
predictions might depend on detailed orographic information nearby,
but more on general orographic shapes farther away.

\Cref{fig:multires_arch} depicts the multiscale architecture.
The architecture is a cascade of three convolutional deep sets, parametrised by three CNNs;
please see the caption.
The low-resolution CNN handles the context data $D\ss{lr}$ with a long spatial length scale.
Because these features have a long spatial length scale, the CNN can get away with a low-resolution discretisation.
The output of the low-resolution CNN then feeds into a medium-resolution CNN.
The medium-resolution CNN handles the context data $D\ss{mr}$ with a medium spatial length scale and has a medium-resolution discretisation.
Finally, the output of the medium-resolution CNN feeds into a high-resolution CNN.
This CNN handles the context data $D\ss{hr}$ with a short spatial length scale and has a high-resolution discretisation.

The key to the computational efficiency of this architecture is that we construct the high-resolution discretisation
only locally to the target points: a small square covering $0.25^\circ$ more than the most extremal target points.
If the target points are confined to a small region, then the high-resolution grid will also be small, covering only $0.25^\circ$ more than that region.
Crucially, the high-resolution grid will not be constructed over all of Germany, like it would if we were to more naively apply the ConvCNP with a high-resolution discretisation, incurring prohitive computational cost.
Even though the high-resolution grid is only constructed locally to the target points, the model can still capture long-range dependencies via the medium-resolution and low-resolution grids.
Namely, the medium-resolution grid is a square covering $5^\circ$ more than the most extremal target points, and the low-resolution grid covers all of Germany; see \cref{fig:multires_arch}.
To utilise this computational gain, the target points must be confined to a small region.
This perfectly synergises with the autoregressive sampling procedure (\cref{proc:ar}),
because this procedure evaluates the model one target point at a time.
The training procedure, however, must be adapted.
During training, we subsample the target points to ensure that the target set is always confined to a small square, which is described in \cref{app:exp-downscaling-training-details}.

During the autoregressive sampling procedure, the AR ConvCNP takes in earlier AR samples from the model.
In the architectures of the MLP ConvCNP and MLP ConvGNP, these is no natural context data to which these samples can be appended.
Therefore, in addition to the ERA-Interim reanalysis variables and the elevation data, we also let the AR ConvCNP take in observed weather stations as context data.
We will append the earlier AR samples to these weather station context data.
To have the model make appropriate use of the weather station context set,
we must randomly divide the weather stations observations over the context and target set, which we describe in \cref{app:exp-downscaling-training-details}.
We let the low-resolution context data $D\ss{lr}$ consist of the 25 coarse-grained ERA-Interim reanalysis variables,
and let the medium-resolution $D\ss{mr}$ and high-resolution context data $D\ss{hr}$ both consist of the weather station observations (and earlier AR samples) and the \SI{1}{km}--resolution elevation data.
When the \SI{1}{km}--resolution data is fed to the medium-resolution CNN, the data loses some detail, because the internal discretisation of the medium-resolution CNN is coarser than the data;
however, when it is fed to the high-resolution CNN, the data retains its detail.
The same holds for the weather station observations (and earlier AR samples).


\subsection{Architectures}

\paragraph{MLP ConvCNP and MLP ConvGNP \citep{Vaughan:2022:Convolutional_Conditional_Neural_Processes_for,Markou:2022:Practical_Conditional_Neural_Processes_for_Tractable}}
%
The MLP ConvCNP and MLP ConvGNP are a respectively a ConvCNP and ConvGNP where
the decoder $\dec_\theta = \mathsf{fuse}_\theta \comp \dec'_\theta$ is decomposed into a convolutional architecture $\dec'_\theta$ followed by a pointwise MLP $\mathsf{fuse}_\theta$:
\begin{equation} \label{eq:fuse}
    \mathsf{fuse}_\theta(\vz(\vardot)) = \mathsf{MLP}_\theta(\vz(\vardot), \mathsf{elevation}(\vardot)).
\end{equation}
In this architecture, the ERA-Interim variables are incorporated via the convolutional architecture, producing the encoding $\vz(\vardot)$.
On the other hand, as \eqref{eq:fuse} shows,
the \SI{1}{km}--resolution elevation data is included via the pointwise MLP $\mathsf{fuse}_\theta$.

Parametrise $\dec'_\theta$ with a seven-layer residual convolutional neural network \citep{He:2016:Deep_Residual_Learning_for_Image}.
Every residual layer involves one depthwise-separable convolutional filter \citep{Chollet:2017:Xception_Deep_Learning_With_Depthwise} with kernel size three followed by a pointwise MLP.
Every layer has 128 channels, and the network also outputs 128 channels.
The discretisation for $\dec'_\theta$ is the grid of the ERA-Interim reanalysis variables.
Parametrise $\mathsf{fuse}_\theta$ with a three-hidden-layer MLP of width 128.

\paragraph{AR ConvCNP}
The AR ConvCNP does not use the pointwise MLP $\mathsf{fuse}_\theta$ to incorporate the \SI{1}{km}--resolution elevation data.
Instead, it is a normal ConvCNP where the convolutional architecture is implemented by the multi-scale architecture described in \cref{fig:multires_arch}.

Parametrise $\mathsf{CNN}\ss{lr}$ with a depthwise-separable residual convolutional neural network like in the MLP ConvCNP and MLP ConvGNP, but use six layers instead of seven.
Let $\mathsf{CNN}\ss{lr}$ output 64 channels.
The discretisation for $\mathsf{CNN}\ss{lr}$ is the grid of the ERA-Interim reanalysis variables.
Parametrise $\mathsf{CNN}\ss{mr}$ with a U-Net \citep{Ronneberger:2015:U-Net_Convolutional_Networks_for_Biomedical} using an architecture similar to what we have been using.
Before the U-turn, let the U-Net have five convolutional layers with kernel size five, stride one for the first layer and stride two afterwards, 64 output channels for the first three layers and 128 output channels afterwards.
After the U-turn, instead of using transposed convolutions, use regular convolutions combined with an upsampling layer using bilinear interpolation.
Let $\mathsf{CNN}\ss{mr}$ output 64 channels.
The receptive field of $\mathsf{CNN}\ss{mr}$ is approximately $10^\circ$.
The discretisation for $\mathsf{CNN}\ss{mr}$ is centred around the target points with margin $5^\circ$.
Parametrise $\mathsf{CNN}\ss{hr}$ with a U-Net like for $\mathsf{CNN}\ss{hr}$, but with four convolutional layers before the U-turn.
The receptive field of $\mathsf{CNN}\ss{hr}$ is approximately $0.5^\circ$.
The discretisation for $\mathsf{CNN}\ss{hr}$ is centred around the target points with margin $0.25^\circ$.


\subsection{Training Details}
\label{app:exp-downscaling-training-details}

\paragraph{MLP ConvCNP and MLP ConvGNP}
The MLP ConvCNP and MLP ConvGNP are trained with learning rate $2.5 \cdot 10^{-5}$ for 500 epochs.
For the MLP ConvGNP, to encourage the covariance to fit, we fix the variance of the decoder to $10^{-4} \mI$ for the first ten epochs.


\paragraph{AR ConvCNP}
The AR ConvCNP is trained with learning rate $1 \cdot 10^{-5}$ for 500 epochs.
During training and cross-validation, the target points are subsampled to lie in a $3^\circ\times3^\circ$ square.
For training, the number of target points is ensured to be at least ten;
and for cross-validation, at least one.
The size of the cross-validation set is increased ten fold.

\paragraph{Sampling of data}
For the MLP ConvCNP and MLP ConvGNP, since these architectures cannot take in weather station observations, all weather stations are used as context data.
For the AR ConvCNP,
a data set is split into a context and target set by randomly selecting $n$ points as context points and letting the remainder be target points.
Specifically, the number of context points $n$ is sampled from $p(n) \propto e^{-0.01n}$.
This splitting is done after subsampling the $3^\circ\times3^\circ$ square.


\newpage
\section{Alternate AR Procedure with Auxiliary Data} \label{sec:AR_auxiliary}

We propose an additional procedure which uses autoregressive sampling with auxiliary data to generate more expressive marginal predictives.
%
The input points of the auxiliary data are chosen randomly, and then sampled autoregressively before sampling the target points.
%
Finally, we discard the sampled values for the auxiliary data, but retain the samples for the target points.
%
Adding auxiliary points in this way allows the model to roll out autoregressively with more steps, even if the target set is small (or just a single point).
%
We describe the procedure below:

\begin{procedure}[Autoregressive application of neural process with auxiliary data]
    \label{proc:ar_marginal}
    For a neural process $\pi_\theta$, context set $D\us{(c)}=(\vx\us{(c)}, \vy\us{(c)})$,
    a target input $x\us{(t)}$,
    a distribution $r$ over $\X$,
    a number of auxiliary data points $R \in \N$, and
    a number of trajectories $M \in \N$,
    let $\mathrm{AuxAR}_{\vx\us{(t)}}(\pi_\theta, D\us{(c)}, r, R, M)$ be the distribution defined as follows.
    We first autoregressively sample the auxiliary data trajectories at random locations sampled from $r$:
    \begin{align}
        &\text{for $j = 1,\ldots,M$ and $\ell=1,\ldots,R$}\hspace{0pt},
        &x\us{(aux,$j$)}_\ell & \sim r, \\
        &\text{for $j = 1,\ldots,M$},
        &\vy\us{(aux,$j$)} &\sim \operatorname{AR}_{\vx\us{(aux,$j$)}}(\pi_\theta, D\us{(c)}).%, \\
    \end{align}
    Next, conditioned on the auxiliary data, we sample the target point of interest to make predictions. We then marginalise out the auxiliary data by averaging over the $M$ trajectories:
    \begin{align}
        y\us{(t)} &\sim \textstyle \frac1M \sum_{j=1}^M P_{x\us{(t)}}(
            \vx\us{(c)} \oplus \vx\us{(aux,$j$)},\,\vy\us{(c)} \oplus \vy\us{(aux,$j$)}
        ).
    \end{align}
\end{procedure}

This procedure introduces three hyperparameters:
the distribution from which to draw inputs $r$,
the length of trajectories $R$, and
the number of trajectories to sample $M$.

In the following experiments, we set the distribution $r$ to be uniform over the training domain with no dependence on the context set or target point of interest: $r = \mathrm{Uniform}([b, h])$, where $b$ and $h$ are the lower and upper bounds of the training domain, respectively. One could experiment with other choices for the distribution $r$. The trajectory length $R$ is chosen between $0$\footnote{A trajectory length of $0$ is equivalent to the standard test-time procedure.} and $8$, and the number of trajectories $M$ is chosen between $1$ and $128$.

\subsection{Generated Data}

We create three data generating processes for our experiments: a mixture of functions, random sawtooth functions, and random audio-like functions. The first two experiments have multi-modal true marginals, whereas the last has heavy-tailed marginals.

\paragraph{Function mixture}
The function mixture data are generated by choosing one of the following three functions, the first with a probability of $\tfrac14$, the second with a probability of $\tfrac12$, and the third with a probability of $\tfrac14$:
\begin{align}
\hspace{3cm}
y &= x^2 + \epsilon,& \epsilon &\sim  \mathcal{N}(0,0.25), \hspace{3cm}
\\
y &= x + \epsilon,& \epsilon &\sim  \mathcal{N}(0,0.0625),
\\
y &= -x + \epsilon,& \epsilon &\sim  \mathcal{N}(0,0.25).
\end{align}

\paragraph{Sawtooth} 
The sawtooth data are generated from the following function:
\begin{equation}
    \label{eq:sawtooth_gen}
    y(x) = \left[\omega (dx - \phi)\right]\, \mathrm{mod} \,1 
\end{equation}
We sample the frequency $\omega \sim \mathrm{Unif}([3, 5])$, the direction $d$ as either $-1$ or $1$ with equal probability, and the shift as $\phi \sim \mathrm{Unif}([\tfrac1\omega, 1])$. 

\paragraph{Synthetic Audio}
Synthetic audio data are generated by convolving a Dirac comb\footnote{The Dirac comb is defined as $\mathrm{Comb}_{T}(t) := \sum_{k=-\infty}^{\infty}\delta(t - kT)$ for given period $P$.} with a truncated decaying sum of sinusoids:
\begin{align}
    s(t) &= 
    \begin{cases}
        e^{-\frac{t}{\tau}}\left[\sin(\omega_{1}t) + \sin(\omega_{2}t)\right] & \text{for } 0\le t < T, \\    
        0 &\text{otherwise}  , 
    \end{cases}
    \\
    f(x) &= \mathrm{Comb}_{T}(x) \ast s(x),
    \\
    y &= f(x) + \epsilon \quad \text{where}\quad \epsilon \sim \mathcal{N}(0, 0.001)
\end{align}
where
\begin{equation}
    \omega_1, \omega_2 \sim \mathrm{Unif}([50, 70]), \quad
    T \sim \mathrm{Unif}([0.75, 1.25]), \quad
    \tau \sim \mathrm{Unif}([0.1, 0.3]).
\end{equation}
We truncate the waves up to the period length, because otherwise the convolution with the Dirac comb would lead to increasing amplitude, resulting in a non-stationary process.

\subsection{Training}
For the function mixture experiment, no training is required because we use the analytically derived ideal CNP $\pi_\infty$ as our model.
See \cref{sec:ARCNP}.

For the sawtooth and synthetic audio experiments, we train ConvCNP models. We train each model for 100 epochs using 1024 batches per epoch with a batch size of 16. We discretise the encoder by evaluating 64 points per unit. We use a margin of 0.1, and a stride length of 2 for each of the 6 layers of the U-Net. Each layer has 64 channels. The receptive field size from this combination of parameters is 6.953.

During training, we sample a number of context points between uniformly at random from \{0,\ldots,75\}, and we sample exactly 100 target points. The context points and target points are sampled uniformly from $[-2, 2]$.
We use the Adam optimizer with a learning rate of $3 \times 10^{-4}$.

\subsection{Results}

\begin{table}[]
    \centering
    \small
    \vspace{-1em}
    \begin{tabular}{lcc}
        \toprule
         Experiment & ~ConvCNP & ConvCNP (AuxAR) \\
         \midrule
         Function Mixture & $-0.63 \pm 0.20$ & $\mathbf{-0.46} \pm 0.15$ \\
         Sawtooth & $\phantom{-}1.46 \pm 0.30$ & $\phantom{-}1.64 \pm 0.28$ \\
         Synthetic Audio & $\phantom{-}0.12 \pm 0.14$ & $\phantom{-}\mathbf{0.55} \pm 0.10$ \\
         \bottomrule
    \end{tabular}
    \caption{Using autoregressive sampling for marginal approximation improves held-out log-likelihoods on all experiments. Values are normalized by the number of target points. Values which are significantly best ($p < 0.05$) are shown in bold.}
    \label{tab:ar_marginal_results}
\end{table}

\begin{table}[!ht]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}
    \vspace{-1em}
    \begin{tabular}{lcccccc}
        \toprule
Model&0&1&2&4&8&16 \\
\midrule
ConvCNP&$\scalebox{0.7}[1]{$-$}0.47{\scriptstyle \pm 0.07}$&$\scalebox{0.7}[1]{$-$}0.45{\scriptstyle \pm 0.18}$&$\scalebox{0.7}[1]{$-$}0.24{\scriptstyle \pm 0.12}$&$\scalebox{0.7}[1]{$-$}0.12{\scriptstyle \pm 0.12}$&$0.36{\scriptstyle \pm 0.15}$&$1.07{\scriptstyle \pm 0.18}$ \\
ConvCNP (AuxAR)
    &$\hphantom{\shortminus}\mathbf{0.02}{\scriptstyle \pm 0.11}$
    &$\hphantom{\shortminus}\mathbf{0.19}{\scriptstyle \pm 0.11}$
    &$\hphantom{\shortminus}\mathbf{0.24}{\scriptstyle \pm 0.12}$
    &$\hphantom{\shortminus}\mathbf{0.31}{\scriptstyle \pm 0.11}$
    &$\mathbf{0.74}{\scriptstyle \pm 0.10}$
    &$1.27{\scriptstyle \pm 0.12}$ \\
\bottomrule
    \end{tabular}
    \caption{Log likelihood values for varying context sizes using ConvCNP and ConvCNP (AuxAR) using the function mixture data generator. Column headers indicate the context set size. Log-likelihoods are shown in bold when they are significantly best ($p < 0.05$). Column headers are context sizes. Errors indicate central 95\% confidence interval.}
    \label{tab:auxar_synthetic_audio}
\end{table}

\begin{table}[!ht]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}
    \vspace{-1em}
    \begin{tabular}{lccccccc}
        \toprule
Model&0&1&2&4&8&16&32 \\
\midrule
ConvCNP&$\mathbf{\scalebox{0.7}[1]{$-$}0.18}{\scriptstyle \pm 0.00}$&$\scalebox{0.7}[1]{$-$}0.14{\scriptstyle \pm 0.02}$&$\scalebox{0.7}[1]{$-$}0.02{\scriptstyle \pm 0.05}$&$0.42{\scriptstyle \pm 0.19}$&$1.89{\scriptstyle \pm 0.23}$&$3.06{\scriptstyle \pm 0.13}$&$3.54{\scriptstyle \pm 0.08}$ \\
ConvCNP (AuxAR)&$\scalebox{0.7}[1]{$-$}0.21{\scriptstyle \pm 0.02}$&$\scalebox{0.7}[1]{$-$}0.09{\scriptstyle \pm 0.04}$&$\hphantom{\shortminus}0.08{\scriptstyle \pm 0.08}$&$\mathbf{0.95}{\scriptstyle \pm 0.16}$&$\mathbf{2.37}{\scriptstyle \pm 0.15}$&$3.03{\scriptstyle \pm 0.13}$&$3.51{\scriptstyle \pm 0.08}$ \\
\bottomrule
    \end{tabular}
    \caption{Log likelihood values for varying context sizes using ConvCNP and ConvCNP (AuxAR) using the sawtooth data generator. Column headers indicate the context set size.. Log-likelihoods are shown in bold when they are significantly best ($p < 0.05$). Column headers are context sizes. Errors indicate central 95\% confidence interval.}
    \label{tab:auxar_sawtooth}
\end{table}


\begin{table}[]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}
    \vspace{-1em}
    \begin{tabular}{lccc}
        \toprule
Model&0&1&2 \\
\midrule
ConvCNP&$\scalebox{0.7}[1]{$-$}1.40{\scriptstyle \pm 0.11}$&$\scalebox{0.7}[1]{$-$}0.91{\scriptstyle \pm 0.28}$&$\scalebox{0.7}[1]{$-$}0.36{\scriptstyle \pm 0.20}$ \\
ConvCNP (AuxAR)&$\mathbf{\scalebox{0.7}[1]{$-$}1.05}{\scriptstyle \pm 0.11}$&$\scalebox{0.7}[1]{$-$}0.63{\scriptstyle \pm 0.23}$&$\scalebox{0.7}[1]{$-$}0.29{\scriptstyle \pm 0.17}$ \\
\bottomrule
    \end{tabular}
    \caption{ Log likelihood values for varying context sizes using ConvCNP and ConvCNP (AuxAR) using the function mixture data generator. Column headers indicate the context set size. Log-likelihoods are shown in bold when they are significantly best ($p < 0.05$). Column headers are context sizes. Errors indicate central 95\% confidence interval.}
    \label{tab:auxar_mixture}
\end{table}

In Table~\ref{tab:ar_marginal_results}, we see that using this procedure improves the held-out log-likelihoods for all of the experiments. We can better understand the utility of this method by observing the performance for different context sizes in Tables~\ref{tab:auxar_sawtooth} and \ref{tab:auxar_synthetic_audio}. For example, the sawtooth data results in Table~\ref{tab:auxar_sawtooth} show that, for context set sizes 16, the AR method provides no benefits. The increased flexibility of this AR method is not needed in this case --- a Gaussian predictive models the marginal sufficiently well. For context set sizes of 4 and 8, on the other hand, we see significant improvements using the AR method. Similarly, for the synthetic audio data results in Table~\ref{tab:auxar_synthetic_audio}, we see improvements using the ConvCNP (AuxAR) for all context set sizes except 16.

The autoregressive sampling with auxiliary data method shown here shows promise for improving modeling of processes with multi-modal and heavy-tailed marginal distributions --- all with no changes to the training procedure. The scenarios where this method are most useful are highly contingent upon the context set size, because of its impact how well the marginals are modeled by Gaussians.

\end{document}
