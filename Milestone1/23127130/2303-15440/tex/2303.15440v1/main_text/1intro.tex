\vspace{-1.5em}
\section{Introduction}
\vspace{\vspaceaftersection}
\label{sec:intro}

Learning how to decompose 3D scenes into object instances is a fundamental problem in visual perception systems.
Past developments in 3D computer vision have made huge strides on this problem by training neural networks on 3D scene datasets with segmentation masks~\cite{schult2022mask3d,vu2022softgroup,wu20223d}. However, these works heavily rely on large labeled datasets~\cite{dai2017scannet,Matterport3D} that require laborious 3D annotation based on special expertise.
Few recent papers alleviate this problem by reducing the need to either sparse point labeling~\cite{hou2021exploring,tang2022learning} or bounding boxes~\cite{chibane2022box2mask}.

In this work, we follow an object-centric approach inspired by the Gestalt school of perception that captures an object as a whole shape \cite{koffka35,palmer99} invariant to its pose and scale \cite{kendall1989survey}. 
A holistic approach builds up a prior for each object category, that then enables object recognition in different complex scenes with varying configurations. Directly learning object-centric priors instead of analyzing each 3D scene inspires a more efficient way of learning instance segmentation: both a mug on the table and a mug in the dishwasher are mugs, and one does not have to learn to segment out a mug in all possible environmental contexts if we have a unified shape concept for mugs.
Such object-centric recognition facilitates a robust scene analysis for autonomous systems in many interactive real-world environments with a diversity of object configurations: Imagine a scenario where a robot is doing the dishes in the kitchen. Dirty bowls are piled in the sink and the robot is cleaning them and placing them into a cabinet. Objects of the same category appear in the scene repeatedly under different configurations (piles, neat lines in the cabinet). What is even more challenging is that even within this one single task (doing dishes) the scene configuration can drastically change when objects are moved.
We show that such scenarios cannot be addressed by the state-of-the-art strongly or weakly supervised methods that struggle under such scene configuration variations.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{./figures/teaser2_comp.png}
    \caption{We present EFEM, an unsupervised 3D object segmentation method applicable to real-world scenes (results on the right) by only training on ShapeNet single object reconstruction.}
    \label{fig:ser}
\end{figure}

In this paper, we introduce a method that can segment 3D object instances from 3D static scenes by learning priors of single object shapes (ShapeNet~\cite{chang2015shapenet}) without using any scene-level labels.
Two main challenges arise when we remove the scene-level annotation. First, objects in the scene can have a different position, rotation, and scale than the canonical poses where the single object shapes were trained. Second, the shape encoder which is trained on object-level input cannot be directly applied to the scene observations unless the object masks are known.
We address the first challenge by introducing equivariance to this problem. By learning a shape prior that is equivariant to the similitude group SIM(3), the composition of a rotation, a translation, and a uniform scaling in 3D (Sec.~\ref{sec:shape_prior}), we address the complexity induced by the SIM(3) composition of objects. For the second challenge, we introduce a simple and effective iterative algorithm, Equivariant neural Field Expectation Maximization (\textbf{EFEM}), that refines the object segmentation mask, by alternately iterating between mask updating and shape reconstruction (Sec.~\ref{sec:single_alg}).
The above two steps enable us to directly exploit the learned single instance shape prior to perform segmentation in real-world scenes. We collected and annotated a novel real-world test set (240 scenes) (Sec.~\ref{sec:exp_real}) that contains diverse object configurations and novel scenes to evaluate the generalizability and robustness to novel object instances and object configuration changes. Experiments on both synthetic data (Sec.~\ref{sec:exp_syn}) and our novel real dataset (Sec.~\ref{sec:exp_real}) give us an insight to the effectiveness of the method. 
Compared to weakly supervised methods, 
when the testing scene setup is similar to the training setup, our method has a small performance gap to the (weakly) supervised baselines. However, when the testing scenes are drawn from novel object configurations, our method consistently outperforms the (weakly) supervised baselines.

Our paper makes the following novel contributions to the 3D scene segmentation problem:
(1) a simple and effective iterative EM algorithm that can segment objects from the scenes using only single object shape priors. 
(2) addressing the diversity of object composition in 3D scenes by combining representations equivariant to rotation, translation, and scaling of the objects.
(3) an unsupervised pipeline for 3D instance segmentation that works in real-world data and can generalize to novel setups. 
(4) a novel real-world test set \textbf{Chairs and Mugs} that contains diverse object configurations and scenes.





















