


\vspace{\vspacebeforesection}
\section{Experiments}
\vspace{\vspaceaftersection}





\begin{figure*}[t!]
\centering
    \vspace{-.2em}
   \includegraphics[width=1.0\textwidth]{./figures/syn_results2_comp.png}
   \\\vspace{-.7em}
    \caption{\textbf{Synthetic scenes:} \textbf{Top}: the mesh reconstruction as input, note how our data has a realistic simulated depth pattern; \textbf{Middle}: visualization of the estimated shapes, poses and bounding boxes as the byproducts of our method. \textbf{Bottom}: Our segmentation prediction.}
    \label{fig:syn_results}
\end{figure*}

\begin{figure*}[t!]
\centering
    \vspace{-.2em}
   \includegraphics[width=1.0\textwidth]{./figures/real_results2_comp.png}
   \\\vspace{-.7em}
    \caption{\textbf{Chairs and Mugs} real testset: the three rows are in the same format as Fig.~\ref{fig:syn_results}.
    }
    \label{fig:real_results}
\end{figure*}






We focus our experiments on answering four main questions: First, can our method successfully segment the objects of interest from the scene?
Second, how does our method compare to existing baselines when different training data distributions are accessible?
Third, how does each approach generalize to the different testing environments, including the real-world scenes and the out-of-distribution configurations?
And fourth, how do the different components of our model contribute to its performance?
To answer these questions, we perform a series of baseline comparisons as well as ablation studies both on synthetic and real-world scenes.

\vspace{\vspacebeforesection}
\subsection{Baselines}
\vspace{\vspaceaftersection}
To the best of the authors' knowledge, fully unsupervised instance segmentation (not foreground-background segmentation) in static scenes remains unexplored in the deep learning literature. Therefore, we compare our method with supervised and weakly supervised methods. 
SoftGroup (CVPR22)~\cite{vu2022softgroup} is the current SoTA for 3D instance segmentation methods that are trained with ground truth instance masks.
Box2Mask (ECCV22)~\cite{chibane2022box2mask} is a recent weakly supervised method, which only needs ground truth instance bounding boxes as supervision while retaining competitive performance.
The closest weakly supervised method to ours is ContrastiveSceneContext (CVPR21)~\cite{hou2021exploring}, which can be trained with only a few point labels.
To conduct fair comparisons and reduce the sim2real gap, all baselines are fully trained on the corresponding training set with positions and normals as input while not using colors.
\input{tables/compact_mugs.tex}
\input{tables/compact_chairs_kit.tex}
\input{tables/compact_real_chairs.tex}

\vspace{\vspacebeforesection}
\subsection{Experimental Setup}
\label{sec:exp_setup}
\vspace{\vspaceaftersection}
We focus our experiments on scenes that contain  objects humans or robots can interact with.  These objects are more interesting targets for segmentation as their configurations can change dramatically in real-world applications, such as robotics or AR/VR.
We experiment with three object categories that frequently appear in the presence of clutter and in diverse configurations in real-world data: \textbf{Mugs}, \textbf{Kitchen containers}, and \textbf{Chairs}.
Since there is no available dataset that contains these objects with rich configuration changes (not just sitting upright on a flat surface), and all baselines need to be trained with ground truth, we gather simulated data to train the baselines, then evaluate on simulated and real test scenes containing completely unseen object instances.

Three types of scenes are used for training baselines on each of the object classes. Take the mugs scene as an example (Fig.~\ref{fig:syn_results} Left top): 
In the \textbf{Z} scenes, all instances are upright and not in contact with each other.
In \textbf{SO(3)}, the objects can have a random orientation but are still not in contact with each other. 
\textbf{Pile} is a much more challenging setting where the objects can touch each other, can take any orientation, and can form piles.
We simulate 500 scenes for training, 50 scenes for validation and 100 scenes for testing in each of the setups.
For each baseline we train three models, one on the data from each of the three scene types.

We train our equivariant shape prior (Sec.~\ref{sec:shape_prior}) on the corresponding categories from ShapeNet~\cite{chang2015shapenet} and freeze their weights once trained. Note that all the following results are generated from the same trained shape prior for each category, and all the objects in our testing scenes never appear in the training set of the shape prior. 
We evaluate each model on all of the scenes and report results in Tab.~\ref{tab:compact_mugs}, Tab.~\ref{tab:compact_syn_chair_kit} and Tab.~\ref{tab:compact_real_chairs}.
The gray cells indicate results from models that were trained on less difficult datasets than they were evaluated on, with the underlined number indicating the best performing inside gray cells. The metric  for evaluating the segmentation is the commonly used mAP~\cite{dai2017scannet}.



\vspace{\vspacebeforesection}
\subsection{Results on synthetic data}
\label{sec:exp_syn}
\vspace{\vspaceaftersection}

We first evaluate the performance on simulated data, and we found that existing baselines work very well inside the training distribution if trained with enough supervisory signals. 
However, the weakly supervised method CSC~\cite{hou2021exploring} has a significant performance drop when the number of supervision points decreases from 200 to 100.
This performance drop becomes increasingly severe when the training set distribution becomes more complex, demonstrating the difficulty of unsupervised segmentation.
With zero scene-level supervision, our method performs well with a small gap in performance to the (weakly) supervised methods.

When the baselines are trained on \textbf{Z} but tested on \textbf{SO3}, the baselines do not show a large drop in performance, potentially because both \textbf{Z} and \textbf{SO3} scenes have no objects in contact with each other.
However, when tested on \textbf{Pile}, we see that baselines trained on \textbf{Z} or \textbf{SO3} perform differently, and are both worse than the ones trained on \textbf{Pile}, indicating a failure to generalize to clutter. Our method outperforms all the baselines that are not trained on \textbf{Pile} configurations.

We additionally generate 50 scenes each from three new and more difficult scene setups for testing on \textbf{Mugs} as shown in Fig.~\ref{fig:syn_results}.
In the \textbf{Tree} scenes mugs are hanging on a holder tree and distributed vertically.
The \textbf{Box} scenes include cubes to simulate objects that have not been seen during training.
In the \textbf{Shelf} scenes, the mugs are put on a shelf that is only visible from one side.
In all three of these scene setups, our method is able to outperform all baselines as the baselines are unable to generalize to configurations of mugs that are significantly outside of the training distribution.












\subsection{Results on real data}
\label{sec:exp_real}
\vspace{\vspaceaftersection}
We additionally evaluate the performance of our model on real data.
To the best of the authors' knowledge, there is no existing real dataset that contains interactable objects in diverse configurations for 3D instance segmentation. 
Therefore we collect a test set \textbf{Chairs and Mugs} that contains the reconstruction of 240 real scenes with object instance mask annotations to test our method in the real world. 

\paragraph{RealMugs:} As shown in Fig.~\ref{fig:real_results} we replicate the \textbf{Z, SO3, Pile} and \textbf{Tree} setups in the real world. 
The scene is captured by 4 calibrated realsense D455 cameras mounted on the corners of the table.
We further introduce two new setups that cover hard-to-simulate scenes. 
The \textbf{Others} setup contains random objects that a manipulator may encounter, including cloth, toys, paper bags, wires and tools. 
These objects are added into the tabletop scene, contacting and occluding the mugs.
The \textbf{Wild} setup contains crops of real-world indoor scans from labs, kitchens, and teaching buildings. Unlike~\cite{xu2022scene}, our scenes are not restricted to only showing the table and upright mugs, but include diverse configurations, backgrounds, and distractors. 
These scans are captured by an iPad with a lidar scanner. Since the \textbf{Z, SO3} and \textbf{Pile} setups are easy to simulate realistically, we only collect 10 scenes per setup. We collect 50 scenes for each of the \textbf{Tree}, \textbf{Others} and \textbf{Wild} setups.

We test all the baselines trained on the synthetic dataset and our method trained on ShapeNet directly on these real scenes.  Quantitative results are shown in~Tab.~\ref{tab:compact_mugs}. 
Since our simulator includes advanced techniques of active light simulation in the physical engine~\cite{xiang2020sapien} and we do not use the colours as input, the sim2real gap for the baseline methods is minimal in these controlled setups. 
We can draw similar conclusions for \textbf{Z}, \textbf{SO3}, \textbf{Pile} and \textbf{Tree} setups as with the synthetic experiments. 
Our method retains reasonable performance on \textbf{Others} scenes due to its awareness of the object shape. When tested on the \textbf{Wild} real-world scenes, our method also performs the best, which demonstrates the strong generalizability of our method.


\paragraph{RealChairs:} We also collect data of chairs in the real world following the \textbf{Z}, \textbf{SO3} and \textbf{Pile} setups. Although we have plenty of real-world scan datasets like~\cite{dai2017scannet}, 
 none of them include diverse configuration changes of chairs. 
 Therefore we collected and annotated a small test set with 20 scenes per setup as shown in Fig.~\ref{fig:real_results}. 
 We train the baselines in simulation and test them in the real world. 
 Additionally, we take all the baselines' official model weights from being trained on the real world ScanNet dataset~\cite{dai2017scannet} to evaluate on our test set. The results are shown in Tab.~\ref{tab:compact_real_chairs}. 
 There is a larger sim2real gap for the baselines with the chairs data than with the mugs data because of less realistic depth simulation and difficulties aligning the scale between Shapenet chairs and real-world chairs.
In contrast, the baselines trained on ScanNet work very well on the \textbf{Z} setup, which aligns best with the ScanNet dataset. However, they have a significant drop in performance when the testing distribution shifts to the \textbf{SO3} and \textbf{Pile} setups.
 In contrast, our method retains reasonable performance on real world chairs across different setups.
 We will release our Chairs and Mugs dataset to provide more opportunities to study robustness, generalizability and equivariance for scene and object understanding in the real world.



\begin{figure}[t!]
\centering
   \includegraphics[width=1.0\linewidth]{./figures/EFEM_scannet_comp.png}
    \\\vspace{-.7em}
    \caption{ScanNet qualitative results, the same format as Fig.~\ref{fig:syn_results}}.
    \label{fig:scannet}
\end{figure}


We also show our effectiveness on ScanNet~\cite{dai2017scannet}, where the indoor scene scan can span the whole room. 
Qualitative results are shown in Fig.~\ref{fig:scannet} and the AP metrics for the chairs category of our method on the validation/test set are $AP=24.6/20.2$, $AP50=50.8/39.0$ and $AP25=61.3/48.3$ where in comparison the weakly supervised method CSC~\cite{hou2021exploring} trained on 200 points labels achieves $AP50=62.9/61.1$ (See Suppl. for a table). One main reason for our performance drop on ScanNet is that ScanNet has many  partially observed chairs, which are hard to be recognized via shape.
We leave future explorations to fill this gap between our unsupervised method and the (weakly) supervised ones.


\vspace{-.5em}
\subsection{Ablations}
\vspace{-.5em}
\input{tables/ablation.tex}

We verify the effectiveness of our design by removing the phase-2 joint iterations and removing the usage of the normals. 
When not using phase-2 (Sec.~\ref{sec:full_alg}), we let the phase-1 independent iterations run more steps to keep the total number of iterations constant. 
When removing the normals, all error computing, assignment weight updating, and confidence scoring will only take the distance error term in to account while ignoring the normal term. 
We compare our full model with the ablated models on SynKit \textbf{Pile} setups. The quantitative results are shown in Tab.~\ref{tab:abl}, which illustrates that both components contribute to our model's overall performance. More ablation studies can be found in our supplementary.
