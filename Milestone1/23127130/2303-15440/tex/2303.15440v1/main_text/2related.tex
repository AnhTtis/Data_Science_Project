\vspace{\vspacebeforesection}
\section{Related Work}
\label{sec:related}
\vspace{\vspaceaftersection}

\paragraph{3D Instance segmentation}

Point cloud instance segmentation has been a long-existing challenge in 3D vision even before the existence of deep learning. 
Early interests have been focusing on 3D object retrieval in scenes where RANSAC and generalized Hough voting were the most prominent paradigms~\cite{schnabel2007efficient}.
In recent years, with the proliferation of large synthetic \cite{wu2018building, fu20213d} or real \cite{hua2016scenenn, Matterport3D, dai2017scannet, hou2019sis} datasets with rich annotations, learning-based methods have shown great success on this task.

\paragraph{Supervised methods}
Most supervised methods fall into two categories: top-down methods that locate objects first and then predict the refined segmentation mask \cite{yang2019learning, hou20193d}, and bottom-up methods learning to group points into object proposals.
Regarding the grouping approach, a variety of algorithms have been explored, such as point-pair similarity matrices \cite{wang2018sgpn, zhang2021point}, mean-shift clustering \cite{lahoud20193d}, graph-based grouping \cite{han2020occuseg}, cluster growing \cite{jiang2020pointgroup}, hierarchical aggregation \cite{chen2021hierarchical, liang2021instance}, or adversarial methods \cite{yi2019gspn}. Later works \cite{vu2022softgroup, vu2022softgroup++} combine top-down and bottom-up approaches and achieve impressive results. Recently, transformers and attention mechanisms have also been introduced to this problem~\cite{schult2022mask3d}.
\cite{nie2021rfd} also incorporates neural implicit representations and simultaneously performs segmentation and shape reconstruction.
Despite their requirements for laborious data annotation, we will also show that supervised methods heavily rely on the correlation between training and test scenes, and even the state-of-the-art supervised methods struggle to generalize to novel scene configurations or to changes in background patterns.

\paragraph{Unsupervised or weakly supervised methods}
Many attempts have been made to learn instance segmentation with limited annotations.
A number of works leverage pre-extracted features from scenes via representation learning such as graph attentions \cite{lee2022gaia} or contrastive learning \cite{xie2020pointcontrast, hou2021exploring, yang2021unsupervised,chu2022twist} to facilitate weakly-supervised training with fewer point labels.
Other approaches directly propagate sparse annotations to dense point labels by learning point affinity graphs \cite{tang2022learning} or by bounding box voting \cite{chibane2022box2mask}.
These methods are usually the most scalable to large scenes, but their point features are scene-dependent, and we will show that similarly to fully-supervised learning methods, weakly-supervised methods have difficulties in generalizing to scenes with different configurations.
When confronting dynamic scenes, one can exploit temporal self-consistency  \cite{song2022ogc} or scene pair constraints \cite{huang2021multibodysync, yang2021unsupervised} but such constraints  introduce additional assumptions about the scenes.
Most related to our work are the retrieval-based methods leveraging object priors \cite{li2015database, xie2022improved}. Traditional retrieval methods are either limited to one given object template \cite{xie2022improved} or need to solve a discrete combinatorial optimization problem searching for the target template in the object category \cite{li2015database}.
We resolve these issues by learning an implicit shape prior which can be optimized continuously in the feature space.





\paragraph{Implicit object priors}

First introduced in \cite{deepsdf, imnet, onet}, neural implicit representations (also known as neural fields~\cite{srinathsurvey}) parameterize 3D shapes as level sets of neural networks. They not only show strong capabilities of capturing geometric details within limited capacity \cite{sitzmann2020implicit, tancik2020fourfeat, peng2020convolutional, jiang2020local} but also avoid shape discretization that introduces sampling noises.
When representing a collection of objects with a shared implicit network, its bottleneck layer naturally forms a latent embedding of the objects, which can serve as a shape prior for many downstream tasks such as shape generation \cite{imnet, ibing20213d, liu2022towards}, reconstruction \cite{lin2020sdfsrn}, completion \cite{deepsdf, onet, chibane2020implicit}, computing correspondences \cite{kohli2020semantic, simeonov2022neural, liu2020learning, lei2022cadex}, and part decomposition \cite{chen2019bae}. We refer the readers to~\cite{srinathsurvey} for a comprehensive review.
Our method takes advantage of the recent work in neural fields to learn a deep shape prior.





\paragraph{Equivariant point cloud networks}
Equivariant networks are designed to preserve transformation coherence between the input and latent representations. With well-developed theories~\cite{cohen2018general,kondor2018generalization,  weiler2021coordinate,aronsson2022homogeneous,xu2022unified}, equivariant networks have a variety of designs on pointclouds~\cite{deng2021vector,thomas2018tensor, fuchs2020se,poulenard2021functional,assaad2022vn,katzir2022shape}, which benefits many downstream tasks such as robotic applications~\cite{simeonov2022neural,weng2022neural,higuera2022neural,xue2022useek,fu2022robust,ryu2022equivariant}, 3D reconstruction~\cite{chen2021equivariant,chatzipantazis2022se}, and object pose estimation/canonicalization~\cite{zhu2022correspondence,pan2022so,li2021leveraging,lin2022coarse,sajnani2022_condor}.
Unlike these object-level works, we focus on leveraging equivariance object features in scene understandings.
Most related to us is \cite{yu2022rotationally} which also applies object equivariance to scenes. But they perform supervised 3D object bounding box detection while we predict dense instance masks.
In this work, we employ the vector neurons~\cite{deng2021vector,chen2021equivariant,assaad2022vn} to build our equivariant shape prior.
