We propose a new framework \textbf{PanoVPR}, based on sliding window to solve the problem of visual place recognition from perspective to equirectangular images. 
Our framework obtains the perspective query image's geographical location by calculating and matching the descriptors generated from the sliding window. 
The benefit of our framework lies in its ability to avoid hard cropping and allow direct transferring of the perspective-to-perspective visual place recognition backbones without any modification, supporting not only CNNs but also Transformers.
To tackle the problem of mismatched descriptor lengths between query and database, which prevents calculating triplet loss, we propose a window-based triplet loss function to train the model more effectively. 
We also derive a new large-scale dataset \emph{Pitts250k-P2E}, with geographical location tags for the P2E task. Furthermore, to better simulate real-world scenarios, we collected another P2E dataset \emph{YQ360}, in which the perspective query images and panoramic database images have non-overlapping FoV. 
Experimental results show that our proposed PanoVPR framework with a lightweight backbone achieves higher recall than our baseline and STATE-OF-THEART approaches.