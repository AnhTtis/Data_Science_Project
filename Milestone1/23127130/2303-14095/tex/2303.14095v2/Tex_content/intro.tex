%

Visual place recognition refers to the process of identifying pre-stored scene locations based on image data captured by visual sensors such as cameras, which can help to obtain more accurate geographical coordinates~\cite{garg2021your}.
Related applications are widely deployed on unmanned vehicles and mobile robots~\cite{liu2018mobile,zhang2021visual}.
Generally, the visual place recognition task is redefined as a fine-grained image retrieval task that applies the image retrieval pipeline~\cite{masone_survey_2021}. 
%

Most image-based visual place recognition methods can be divided into two categories: \textit{perspective to perspective} (P2P)~\cite{arandjelovic2016netvlad, hausler2021patch, wang2022transvpr} or \textit{equirectangular to equirectangular}~(E2E)~\cite{fang2020cfvl,cheng2019panoramic,wang2018omnidirectional} fashion. 
However, both of the aforementioned tasks have their own limitations.
P2P methods have limitations with regard to the field of view (FoV) of the pinhole camera and the potential for inaccuracies due to different shooting orientations. The non-overlapping hard cropping method utilized in panoramic database images can also lead to mismatches for query images whose FoV stride over the cropping boundary~\cite{orhan2021efficient}. On the other hand, E2E methods are relatively complex in optical design~\cite{gao2022review} and expensive to integrate into consumer-grade hardware~\cite{broxton2019low}.

Hence, a natural and straightforward idea is to employ perspective images for retrieving panoramic database images. These perspective images can be captured by users using low-cost consumer-grade pinhole cameras in a convenient manner. Furthermore, panoramic images with location tags can be obtained from map providers, whose data are abundant and easily accessible~\cite{orhan2021efficient}.
In this way, users can obtain relatively reliable geographical coordinates in GPS-unreliable outdoor scenes through P2E (perspective-to-equirectangular) algorithms.

%
\begin{figure}[!t]
\renewcommand{\thefigure}{2}    %
   \centering
   \vspace{0.5em}
   \includegraphics[width=1\linewidth]{Pics/performance_comparison_update3.png}
   \vspace{-1em}
   \caption{\emph{\textbf{Performance comparison} against} NetVLAD~\cite{arandjelovic2016netvlad}, Berton~\etal~\cite{berton2022deep} \emph{and} Orhan~\etal~\cite{orhan2021efficient} \emph{with PanoVPRs on pitts250K-P2E dataset for the P2E-VPR task.}}
   \label{fig:performance_comparison}
  \vspace{-1.0em}
\end{figure}

However, the difficulty of designing a unified P2E framework is the asymmetrical data capacity and informational content between perspective and panoramic images, making it unreasonable to apply P2P or E2E frameworks directly.
%
In the field of visual place recognition, using a pipeline that employs consistent encoding behavior for database and query images is of vital importance. 
This is because an offline database must be constructed and online query features must be ensured. Thus, it is necessary for the \emph{query} pinhole images and the \emph{database} panoramic images to be encoded by the same encoder. 
Therefore, the utilization of a transformer-based backbone is limited in the P2E-VPR task by natural structural design, such as positional embedding~\cite{vaswani2017attention}.

To tackle these issues and enable the direct transfer of backbone networks from P2P methods without modifications, we slide a window over the equidistant cylindrical projection of the panoramic image, calculating and comparing the similarity of descriptors within the window.
Additionally, during training, we only select the window in the whole panoramic image with the closest descriptor distance in feature space relative to the query image, and then extract the descriptors of that window patch.
Window-based descriptors with the same dimension are applied to calculate the triplet loss function~\cite{balntas2016learning}, avoiding label misclassification caused by dissimilar windows in the same panoramic image.
The experiment shows that our proposed sliding window method achieves higher retrieval accuracy than resizing the panoramic image and applying the P2P framework directly.
Moreover, we integrate the sliding window strategy and proposed a unified end-to-end P2E visual place recognition framework, \textbf{PanoVPR}.
To train the proposed network on large-scale datasets, we derive a dataset by panoramic stitching database images specifically for our P2E task, called Pitts250k-P2E, based on the Pitts250k dataset~\cite{arandjelovic2016netvlad,torii2013visual}.

Further, to validate the framework's performance in real-world environments where the FoV of query images does not completely overlap with that of the panoramic database images, we collect the first P2E dataset in real-world via a mobile robot with a Panoramic Annular Lens (PAL) camera, which we refer to as YQ360.
Extensive experiments on the two datasets verify that
the sliding window strategy on panoramic images is consistently effective for various backbone networks, and a small step size with overlapping windows can achieve higher accuracy.
This conclusion holds true for different types of backbone networks used in the framework, whether a Transformer or a CNN (Fig.~\ref{fig:performance_comparison}).

In summary, our work has the following contributions:
\begin{compactitem}
    \item Proposing a method of cyclically overlapping sliding windows on panoramic images (Fig.~\ref{fig:sliding_window}), which solves the problem of completed objects being cut apart due to hard cropping of panoramic images, leading to insufficient features during retrieval. Further, our method leverages the cyclically invariant characteristics of panoramic images to solve the problem of discontinuous boundaries in unfolded panoramas (Sec.~\ref{sec:slidig_window}).

    \item Designing a sliding window-based framework named \textbf{PanoVPR} for visual place recognition from perspective to panoramic images (Fig.~\ref{fig:framework}). This unified framework can directly transfer most perspective-to-perspective methods without any modification. The feature encoding strategy using sliding windows across raw panoramic images enables compatibility with transformer-based backbones. Moreover, a window-based triplet loss function is proposed, facilitating easy training and fast convergence of the model (Sec.~\ref{sec:framework}).

    \item Proposing two datasets for the P2E task, one is the large-scale dataset derived from the Pitts250k dataset by stitching panoramas in the database Pitts250k-P2E. The other is a real-world dataset YQ360, which was collected with query images and panoramic database images having partially overlapped FoVs (Sec.~\ref{sec:dataset}).

\end{compactitem}