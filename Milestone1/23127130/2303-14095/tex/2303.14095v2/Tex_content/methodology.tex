%
\begin{figure*}[!t]
\renewcommand{\thefigure}{3}    %
   \centering
   % \vspace{-3.5em}
   \includegraphics[width=1.0\linewidth]{Pics/sw-2.pdf}
   \vspace{-3em}
   \caption{\textbf{\emph{Illustrations of the proposed sliding window strategy.}} We slide a window over the panoramic image and encode the image in each window. The overlapping sliding window (\blue{blue} shadow in the figure) can eliminate the problem of separating entire objects due to hard image clipping. The cyclic sliding window (\orange{orange}) fully uses the cyclic invariance of the panoramic image. The \green{green} window is the one that should be retrieved correctly during the sliding search. Both the perspective and panoramic images utilize the P2P backbone to extract feature descriptors.}
   \label{fig:sliding_window}
  \vspace{-2em}
\end{figure*}

We propose a \textbf{V}isual \textbf{P}lace \textbf{R}ecognition framework for retrieving \textbf{Pano}ramic database images using perspective query images, dubbed \textbf{PanoVPR}.

Our proposed framework employs a sliding window approach and utilizes an image retrieval pipeline to provide global feature descriptors for both panoramic and perspective images.
In Sec.~\ref{sec:slidig_window}, we describe our panoramic sliding window approach, including the overlap sliding window design that eliminates the effect of seams and the cyclic sliding window strategy that utilizes the cyclic information of panoramas.
Furthermore, in Sec.~\ref{sec:framework}, we detail our unified P2E visual place recognition framework based on the sliding window approach and elaborate the workflow of PanoVPR in the training and interface process. 
Besides, we also explain the window-based modification to the loss function, which makes the training process more reasonable and stable. At the end of the section, we provided a plain and simple explanation for the evaluation metric of the framework.

\subsection{Sliding Window}
\label{sec:slidig_window}

As there are discrepancies in input sizes between perspective and panoramic images, using the transformer-based P2P backbone directly is not feasible due to the need for homogeneous encoding behavior like positional embedding.
To address this challenge and create a unified framework that can leverage the P2P visual place recognition backbone directly, we employ a sliding window approach on the panoramic database images. 
This approach narrows the model's observation range of the large field of view panoramas and transforms the problem into a comparison-and-retrieval process within the window.

%
 
As shown in Fig.~\ref{fig:sliding_window},  we utilize a sliding window approach on panoramic images, dividing them into multiple sub-images. 
Each sub-image between the same panoramic image is independent and processed by the same encoder to extract features.
It could be intuitively understood that the \emph{window} refers to the area of interest during the sliding retrieval phase, and the sliding operation represents the process of horizontal interested region change in the scene captured by this panoramic image.
%

%
Specifically, the \blue{blue} region depicted in Fig.~\ref{fig:sliding_window} corresponds to an overlapping sliding window applied to the panoramic image. The shaded portion within the blue region represents the overlapping segment of the sliding window. 
To emphasize the crucial elements, we only explicitly illustrate part of all blue windows.
The blue window accompanied by the rightward arrow, concealed behind the panorama, stands for the dynamic process of the overlapping sliding window as it traverses the panorama from left to right.
The \orange{orange} window in the figure represents the cyclic sliding window. 
Since the image of the panorama has continuous content at both the left and right borders, the section beyond the right border of the panorama is complemented by the left border, thereby leveraging the cyclical invariance property of the panoramic image.
The \green{green} window is the one that should be retrieved correctly during the sliding search.
It can be observed that the contents of the green window are similar in appearance to the first perspective query image by zooming in.

%
There are two strategies for sliding window search based on different strides: overlapping sliding windows and non-overlapping sliding windows.
In contrast to the latter, the former contains a section of images from the previous sliding window within each sliding window, which mitigates the risk of continuous feature loss at the seam. 
This, in turn, leads to enhanced accuracy in feature extraction and matching, detailed in Tab.~\ref{tab:panoVPR_pitts}. 
Meanwhile, overlapping sliding windows can also increase the number of samples and improve the robustness of the model.
Furthermore, the model's metric accuracy can be further enhanced by applying a cyclic sliding window approach, and the comparative experiment results are also available in Tab.~\ref{tab:panoVPR_pitts}.

%
It should be emphasized that the length of each sliding window on the panoramic image is equal to the horizontal size of the perspective query image, which is set uniformly.
This enables the utilization of the same encoder for extracting descriptors from both the query and the windowed database image, potentially satisfying the requirement for consistent query-database encoding behavior in VPR.

%
The entire panoramic descriptor is obtained by concatenating the sub-window vectors.  
To obtain the correct window on panoramas, we slide the query descriptor over the panoramic descriptor with a step equal to the query descriptor's length.
As shown in Fig.~\ref{fig:sliding_window}, the window with the highest similarity score between the query and sub-window database descriptor is the correct window in the panoramic image that matches the query image (window and descriptor colored in \green{green}).
Then the panorama to which it belongs provides the geographic location for the query image.

%
\begin{figure*}[!t]
\renewcommand{\thefigure}{4}    %
   \centering
   \vspace{-3.5em}
   \includegraphics[width=1.0\linewidth]{Pics/framework_update.pdf}
   \vspace{-3em}
   \caption{\textbf{\emph{Illustrations of the proposed PanoVPR framework.}} During training, perspective query images and panoramic database images are fed into a shared encoder to extract features, which are then used to obtain triplet image pairs using hard positive and negative sample mining. The triplet image pairs are encoded and the feature distances are computed using a window-based triplet loss. During testing, the feature descriptors of the panoramic database images are first extracted offline, and then the perspective query images are encoded one after another. The query descriptors are compared with those of the panoramic database descriptors to obtain the Top-N predictions with the window similarity.}
   \label{fig:framework}
  \vspace{-1.0em}
\end{figure*}

\subsection{Framework}
\label{sec:framework}

%
\noindent\textbf{PanoVPR Framework}. 
As shown in Fig.\ref{fig:framework}, our PanoVPR framework mainly follows an image retrieval pipeline~\cite{lowry2016visual}.
In this pipeline, the training stage is completed online, where both the query images and the database images are fed into the network synchronously. While the testing stage is completed offline, where the model first extracts features from the database images and stores them, then processes the query images in streaming, with the two operations being asynchronous.
The inconsistency between the training and testing logic requires that the query images and the database images share the same feature encoding network.

%
During training, the model first infers all query and database images to obtain feature descriptors for all images. Then the hard positive and hard negative samples mining is conducted on the database images with the aid of GPS ground truth.
Positive samples are defined as database images that are similar to the query image both in terms of geographical coordinates and feature space, while negative samples are database images that are only similar in feature space but are far away in geographical coordinates. 
After the hard mining process, we obtain many triplets image pairs containing a query image, a hard positive sample, and multiple hard negative samples:
\begin{equation}
    (q,db) \longrightarrow (q, p, *n),    
\end{equation}
where $q$ and $db$ refer to perspective query images and panoramic database images respectively, $p$ refers to a hard positive panoramic image, and $*n$ denotes a collection of hard negative panoramic images.
%
The obtained triplets are fed into the backbone for subsequent training. 

%
During the testing phase, we first encode the panoramic database images using the backbone and store them as descriptors. 
Next, for each perspective query image, we encode it using the same backbone to generate a query feature descriptor. We slide this descriptor on all the database descriptors and calculate the similarity score for each local window. 
The panoramic database image with the highest similarity score within the searched window is selected as the top-$1$ predicted image, and this process is repeated for all descriptor windows. 
Using this search method, we reorder all database images according to their local window similarity. Finally, we select the top-$N$ images to obtain the top-$N$ predictions for each perspective query image.

%
\noindent\textbf{Window-based Triplet Loss}. 
Since the PanoVPR framework extracts perspective query images and panoramic database images with different descriptor lengths, we design window-based triplet loss based on the original triplet loss~\cite{balntas2016learning}.
The original triplet loss function is defined as:
\begin{equation}
    loss(q,p,*n)=\sum_{i}^{}  max\{d(q,p)-d(q,n_i)+m, 0\},
\end{equation}
where $m$ is the margin parameter, and $d(x, y)$ is the distance between the feature vectors $x$ and $y$, defined as follows:
\begin{equation}
    d(x, y)=||x-y||_p.
\end{equation}

The distance mentioned above can serve as a measure of the similarity score between query and database descriptors in feature space. 
The smaller the distance value, the higher the similarity score.
We only calculate the windowed database descriptor that exhibits the highest similarity to the query descriptor in the feature space.
Therefore, the above equation is modified below:
\begin{equation}
    d(x, y)=||x-y_w||_p,
\end{equation}
%
where $y_w$ represents the sliding window component of vector $y$ that has the most similar to $x$, and it can be mathematically represented as:
\begin{equation}
    y_w=argmin_{win}(||x-y[win]||_p).
\end{equation}

%
\noindent\textbf{Evaluation Metric.}
Following ~\cite{lowry2016visual}, we use the recall metric to estimate the model's performance. The recall metric is defined as follows: given a query image, we search for the database images and predict many retrieval candidates. If the ground truth is included in the top-$N$ predictions, then the prediction for this query image is correct. The percentage of correct predictions for all query images is calculated and denoted as Recall@N.