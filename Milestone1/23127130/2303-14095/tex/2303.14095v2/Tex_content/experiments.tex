\subsection{Datasets}
\label{sec:dataset}
%
Currently, there are few readily available datasets for the P2E place recognition task.
To conduct experiments and facilitate future research, we propose two datasets specifically for the P2E task, including \emph{Pitts250K-P2E} and \emph{YQ360} datasets, respectively.
The scale and division of the two datasets are shown in Table \ref{tab:dataset}.

\noindent\textbf{Pitts250k-P2E.}
Pitts250k-P2E is derived from the original Pitts250k dataset~\cite{arandjelovic2016netvlad,torii2013visual}.
In the original dataset, each location's panoramas were captured in $2$ yaw directions, we select $12$ images with lower yaw angles for each location as they contained more street-view information instead of irrelevant background elements such as the sky and less distortion. 
These images are then stitched together to create one panoramic image~\cite{brown2007automatic} for each location.
As for the query image, we select $12$ images with lower yaw angles that have overlapping fields of view (FoV) with the panoramic image but are captured at different times.

%
\begin{figure}[!t]
\renewcommand{\thefigure}{5}    %
   \centering
   \includegraphics[width=0.95\linewidth]{Pics/device_routine-v4.pdf}
   \vspace{-0.5em}
   \caption{\textbf{\emph{YQ360 Datasets}}. (a) The data collection devices, include front and rear pinhole cameras, panoramic annular cameras, and GNSS. (b) The visualization of the dataset.}
   \label{fig:YQ360}
  \vspace{-1.5em}
\end{figure}

\noindent\textbf{YQ360.}
The YQ360 dataset is collected using a mobile vehicle in Fig.~\ref{fig:YQ360}(a), which
is equipped with a Panoramic Annular Lens (PAL) camera on the top.
Meanwhile, a network camera and a laptop's built-in camera are mounted on the front and rear of the car respectively, serving as the front and rear pinhole cameras.
The GNSS sensor is also placed on the vehicle to obtain the GPS coordinates of the acquisition location.
We capture a panoramic image and two narrow-field images (one from the front and another from the rear) approximately every $5$ meters, tagged with the GNSS output of the location. 
To simulate a more realistic testing scenario, the vertical fields of view of the perspective and panoramic images are not completely overlapped, which is also an important characteristic of YQ360. 
The collection routine covers the same path twice as shown in Fig.~\ref{fig:YQ360}(b).
It includes a boulevard with heavily repeated textures, as well as open street views that have distinctive features.
%
The scale of the YQ360 dataset is currently constrained owing to the discrete data acquisition process from various sensors in different outdoor locations, which is labor-intensive.
%

\subsection{Implementation Details}
Our framework is implemented using PyTorch and the experiments are conducted on a machine with $4$ NVIDIA 3090 graphics cards, 128 GB RAM, and 88 CPU cores.
During training, each panoramic image is transformed into multiple sub-images using our sliding window method. 
GPU memory consumption increases with the number of sub-images, the training batch size is set to $2$ considering that.
We also set the margin parameters $m$ in triplet loss to $0.1$. 
Moreover, when testing on the YQ360 dataset, we consider the variations in the overlap of the field of view between the query and database images from different datasets.
Specifically, we fine-tune our model, pre-trained on pitts250k-P2E, using the training and validation sets of the YQ360 dataset. The fine-tuned model is then evaluated on the test set.

As conventional practice in ~\cite{arandjelovic2016netvlad}, during the mining period, $1$ hard positive sample and $10$ hard negative samples are mined for each query image.
The geographical coordinate labels are clustered using KNN~\cite{taunk2019brief} method, with a clustering threshold of $10$ meters during training and $25$ meters during testing.
In~\cite{berton2022deep}, it has been shown that the \emph{partial} mining method can achieve accuracy similar to the \emph{full} mining approach while reducing memory usage. 
Therefore, we use the \emph{partial} mining method in the following experiments. 
Moreover, the back-end of the framework utilizes the GeM pooling layer~\cite{radenovic2018fine}. 
%

\subsection{Results}
%
\noindent\textbf{Sliding Window Validation.}
To verify the effectiveness of our proposed sliding window method on panoramic images, we conduct the following experiments on the Pitts250k-P2E dataset and show the results in Table \ref{tab:panoVPR_pitts}. 
We select the lightweight Swin-Tiny~\cite{liu2021swin} as our backbone and directly resize panoramas to meet the requirements for input image size as our baseline.
$\times N$ indicates that the panoramic image is divided into $N$ equal parts, and the sliding window's stride selects the length value of one of them ($1/N$ of the width of the panorama). 
As the sliding window size is predetermined (discussed in Sec.~\ref{sec:slidig_window}), larger values of $N$ correspond to smaller strides and more refined sliding windows.

%
\begin{table}[!t]
\renewcommand{\thetable}{1}
    \begin{center}
        \caption{The training, validation, and testing splits.}
        \label{tab:dataset}
        % \vspace{-1.0em}
        \input{Tables/dataset}
        \vspace{-1em}
    \end{center}
\end{table}

%
\begin{table}[!t]
\renewcommand{\thetable}{2}
    \begin{center}
        \caption{Effectiveness of PanoVPR on Pitts250k-P2E.}
        \label{tab:panoVPR_pitts}
        % \vspace{-1.0em}
        \input{Tables/PanoSwin}
        \vspace{-1em}
    \end{center}
\end{table}

%
\begin{table}[!t]
\renewcommand{\thetable}{3}
    \begin{center}
        \caption{Quantitative comparison on the YQ360 dataset.}
        \label{tab:panoVPR_yq}
        % \vspace{-1.0em}
        \input{Tables/YQ360_compare}
        \vspace{-2em}
    \end{center}
\end{table}

%
\begin{figure*}[!t]
\renewcommand{\thefigure}{6}    %
   \centering
   \includegraphics[width=1.0\linewidth]{Pics/vis-v4.pdf}
   \vspace{-2em}
   %
  \caption{\emph{\textbf{Visualization results on the Pitts250k-P2E and YQ360 test set using the proposed PanoVPR (SwinT) framework.} We highlight the predicted key windows in the Top3 matched database panoramic images, where the most important Top1 has achieved precise matching and positioning. PanoVPR can accurately predict the perspective query's geolocalization and focus on relevant regions of the panoramas.}}
   \label{fig:quality_pitts250k}
  \vspace{-2em}
\end{figure*}

%
In our experimental setup, we configured the panoramic image width to be 8 times the length of the window. Therefore, the symbol $\times 8$ indicates that the sliding window is non-overlapping, non-cycling, and equal to the stride.
We experiment with whether use circular sliding windows while maintaining the same window overlap. A comparison of the results between the third and fourth rows of Tab.~\ref{tab:panoVPR_pitts} reveals that utilizing overlapping sliding windows can lead to a $1.2\%$ increase in R@1, as well as a general improvement in other metrics.
Furthermore, we experiment with various sliding window strides \{$\times8$, $\times16$, $\times24$, $\times32$\} and find that as the stride decreases, the model's recall precision improves (from R@1 $22.0$ to $41.4$ when striding from $\times8$ to $\times32$). 

Comparisons between our method PanoVPR (SwinT) and baseline show that directly resizing panoramic images to the appropriate size for the encoder greatly reduces the model's accuracy, only reaching $10.1$ R@1 on the Pitts250k-P2E dataset, which is $31.3$ points lower than our method \emph{PanoVPR (SwinT)$_{{\times}24}$}.
We hypothesize that the reason for this is that resizing panoramic images causes a significant loss of information, resulting in distorted global features that are no longer accurate or reliable after encoding. 
However, with our proposed approach, the encoder receives panoramic images through sliding image windows, which preserves their original integrity and avoids distortion before encoding.
Therefore, our proposed method significantly improves the accuracy compared to the baseline.

%
\noindent\textbf{Comparison against State-of-the-Art.}
%
We conduct the quantitative comparison on two datasets and show the results in Tab.~\ref{tab:panoVPR_yq} and Tab.~\ref{tab:abl_pitts250k}, respectively.
In NetVLAD~\cite{arandjelovic2016netvlad}, the backbone is selected as ResNet50, and other settings remain unchanged.
In~\cite{berton2022deep}, the backbone is selected as ViT, and the clustering back-end uses GeM~\cite{radenovic2018fine}. The best result is \textbf{bolded} and the second best result is \underline{underlined}.
%

As shown in Tab.~\ref{tab:panoVPR_yq}, when employing Swin-Tiny as PanoVPR's backbone, it achieves a $37.5\%$ increase in R@1 accuracy compared to the popular CNN method~\cite{arandjelovic2016netvlad} ($48.4$ \textit{vs.} $35.2$). Besides, PanoVPR improves R@1 by $19.8\%$ ($48.4$ \textit{vs.} $40.4$) compared to the ViT method~\cite{berton2022deep} while reducing the model parameter amount by $67.4\%$ ($28.29M$ \textit{vs.} $86.86M$).
%
Furthermore, PanoVPR (ConvNeXtS) can achieve $7.98\%$ ($51.4$ \textit{vs.} $47.6$) improvement over the previous best P2E method~\cite{orhan2021efficient} in R@1 on the newly proposed YQ360 dataset, verifying our proposed method in real scenes. 
%
As shown in Tab.~\ref{tab:abl_pitts250k}, PanoVPR also achieves 48.8 R@1 on the derived pitts250k-P2E dataset, surpassing the previous best method by $3.8\%$ ($48.8$ \textit{vs.} $47.0$) while reducing $63.2\%$ parameters ($50.22M$ \textit{vs.} $136.62M$).
We found that PanoVPR does not require a complex model with a large number of parameters. 
In summary,
our proposed PanoVPR framework is robust and powerful that demonstrates excellent compatibility with the backbone, as well as high accuracy in localization.
From the analysis above, it is evident that our proposed sliding window approach leads to a higher recall when perspective query images retrieve panoramic images with incomplete overlapping FoV in real scenarios.

Qualitative results are shown in Fig.~\ref{fig:quality_pitts250k}, which confirm PanoVPR continues to provide accurate retrieval and localization capabilities in response to illumination, viewpoint changes, and image distortion, while also providing stable generalization across different datasets.

%
\begin{table}[!t]
\renewcommand{\thetable}{4}
    \begin{center}
        \caption{Quantitative comparison on Pitts250k-P2E.}
        \label{tab:abl_pitts250k}
        % \vspace{-1.0em}
        \input{Tables/backbone}
        \vspace{-2em}
    \end{center}
\end{table}

\noindent\textbf{Ablation Studies.}
%
To investigate the impact of different backbone networks and the sliding window stride on our framework, we conduct the following ablations.

%
We select a CNN backbone ConvNeXt~\cite{liu2022convnet}, and a Transformer backbone Swin Transformer~\cite{liu2021swin}. The results are shown in Tab.~\ref{tab:abl_pitts250k}.
The result demonstrates that $\times24$ sliding window stride achieves at least $24.3\%$ R@1 metric improvement over the corresponding baseline model ($34.0$ \textit{vs.} $9.7$). We also found that compared to SwinS, SwinT generalizes better on both datasets.
The conclusion that a small stride overlapping sliding window can achieve higher recall also holds true when the PanoVPR backbone changes, whether CNN or ViT architecture. Thus, PanoVPR can easily benefit from progress in classical P2P VPR methods.