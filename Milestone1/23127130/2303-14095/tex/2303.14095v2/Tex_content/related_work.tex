\noindent\textbf{Perspective Visual Place Recognition.} 
Visual place recognition from perspective to perspective involves narrow field-of-view images for both query and database.
The image data is directly acquired from a pinhole camera, such as Norland~\cite{sunderhauf2013we}, RobotCar Seasons~\cite{maddern20171}, and MSLS~\cite{warburg2020mapillary}, or by unwrapping and cropping panoramic images provided by a map provider, such as Pitts250k~\cite{arandjelovic2016netvlad,torii2013visual}, SF-XL~\cite{Berton_CVPR_2022_CosPlace}, \textit{etc.}

Initially, researchers used hand-crafted methods to detect key points in images and extract compressed representations as feature descriptors, which serve as the representation of the key points information in images, such as SIFT~\cite{ng2003sift}, SURF~\cite{bay2006surf}, and VLAD~\cite{jegou2010aggregating}.
With the popularity of deep learning, researchers found that using end-to-end neural networks, supplemented by keypoint clustering methods to extract image feature descriptors, have higher robustness than hand-crafted methods and achieve higher recall accuracy in retrieving when it comes to complex scenarios~\cite{masone2021survey, chen2022deep}.

NetVLAD~\cite{arandjelovic2016netvlad} employs a CNN backbone to extract local features from images and designs a NetVLAD layer to obtain a global feature descriptor for the image. 
Patch-NetVLAD~\cite{hausler2021patch} divides an image into patches, extracts their features using NetVLAD, and combines them with the global image descriptor to obtain a robust descriptor that considers both global and local information.
SuperPoint~\cite{detone2018superpoint} combines the interest point detection decoder and descriptor decoder in parallel at the back end of a shared CNN encoder, making it faster to infer and simpler to train. 
TransVPR~\cite{wang2022transvpr} uses a vision transformer as the backbone, to obtain the global features through multi-layer attention aggregation.

More recently, some works, such as CosPlace~\cite{Berton_CVPR_2022_CosPlace}, have converted the image retrieval problem into an image classification problem to solve the problem without expensive negative sample mining costs.
Unlike these works, we address panoramic visual place recognition which can provide a wider FoV for the image retrieval paradigm.

\noindent\textbf{Panoramic Visual Place Recognition.}
This visual place recognition method uses panoramic unwrapped images with a large field of view for both query and database images.
Wang~\etal~\cite{wang2018omnidirectional} utilize CNNs to extract features from indoor scenes and design circular padding and roll branching techniques to improve the model's performance, considering the rotational invariance characteristics of panoramic images.
Cheng~\etal~\cite{cheng2019panoramic} propose a visual localization method for challenging outdoor scenes, unwrapping panoramas into cylindrical projection as database images.
Marta~\etal~\cite{ballesta2021cnn} use CNNs to extract robust scene descriptors from omnidirectional images directly for performing hierarchical localization of a mobile robot in indoor environments.
Fang~\etal~\cite{fang2020cfvl} propose a two-stage place recognition method, which roughly filters panoramic cylindrical unwrapped database images in the first stage and re-ranks the candidate images using Geodesc~\cite{luo2018geodesc} in stage two.

Recent work on efficient retrieval of perspective images from panoramic images involves sliding windows on \emph{feature maps} generated by \emph{CNNs}~\cite{orhan2021efficient}.
CNN can handle input images of arbitrary sizes, so it is possible to encode entire panoramic images. 
However, if we use visual transformers~\cite{liu2021swin,dosovitskiy2020image} as the backbone, encoding the entire panoramic image directly would require a huge amount of GPU memory which is impractical.
Moreover, directly resizing the entire raw panorama to an appropriate size for the transformer would significantly reduce the model's accuracy, as shown in Sec.~\ref{sec:experiments}.

To increase the diversity of backbone network types in the framework and address the issue of transformer-based backbone being unable to encode entire panoramic images directly, we slide windows over the panorama and only feed the windowed portion of the panorama into the backbone.
Therefore, the backbone in the framework only computes for the windowed portion, solving the problem of encoding large-sized panoramic database images.
