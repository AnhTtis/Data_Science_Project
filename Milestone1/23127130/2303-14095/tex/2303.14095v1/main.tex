% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mathtools}
% \usepackage{arydshln}

\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{bbm}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\usepackage{float}

\usepackage{lipsum}
\usepackage{stfloats}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bm}
\usepackage{etoolbox}
\usepackage{icomma} % for comma
\usepackage{array}
\usepackage{tabulary}
\usepackage[table]{xcolor} % for rowcolor
\usepackage{paralist}
\usepackage{booktabs}
\usepackage{adjustbox}
% \usepackage[caption=false,font=footnotesize]{subfig}

% \usepackage[sort,nocompress]{cite} %sort inline

% === for table caption
\usepackage{caption}
\captionsetup[table]{format=plain,labelformat=simple,labelsep=period}%
% \usepackage{float}  % table follow text by [H]
% === for figure subcaption
\usepackage{subcaption}

% ======= ADDED commands

\usepackage{arydshln}
% \usepackage{tabulary}
% \usepackage[table]{xcolor}
\newcommand\ver[1]{\rotatebox[origin=c]{90}{#1}}
\newcommand{\fl}[1]{\multicolumn{1}{c}{#1}}
\definecolor{gray}{rgb}{0.3,0.3,0.3}
\definecolor{blue}{rgb}{0,0.5,1}
\definecolor{mask_red}{rgb}{1,0,0.8}
\definecolor{green}{rgb}{0.2,1,0.2}
\definecolor{rblue}{rgb}{0,0,1}
\definecolor{lightblue}{HTML}{6495ed}
\definecolor{lightred}{HTML}{F19C99}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor[RGB]{96,177,87}{#1}}
\newcommand{\lightblue}[1]{\textcolor{lightblue}{#1}}
\newcommand{\fn}[1]{\footnotesize{#1}}
\newcommand{\gbf}[1]{\green{\bf{\fn{(#1)}}}}
\newcommand{\bbf}[1]{\lightblue{\bf{\fn{(#1)}}}}
\newcommand{\rbf}[1]{\gray{\bf{\fn{(#1)}}}}
\newcommand{\obf}[1]{\textcolor{orange}{\bf{\fn{(#1)}}}}
\definecolor{graytablerow}{gray}{0.6}
\newcommand{\grow}[1]{\textcolor{graytablerow}{#1}}

\definecolor{revised_color}{HTML}{0066CC}
\definecolor{revised_color_PKY}{HTML}{FF2E82}
\definecolor{revised_color_SH}{HTML}{007FFF}
\newcommand{\YKL}[1]{\textcolor{red}{#1}}
\newcommand{\SZ}[1]{\textcolor{orange}{#1}}
\newcommand{\SH}[1]{\textcolor{revised_color_SH}{#1}}
\newcommand{\PKY}[1]{\textcolor{revised_color_PKY}{#1}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\hypersetup{colorlinks, citecolor=blue}%teal
%\hypersetup{colorlinks, citebordercolor=black}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\definecolor{blue}{rgb}{0,0,1}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}

\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or *\or \dagger\or \ddagger\or
    \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
    \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{PanoVPR: Towards Unified Perspective-to-Equirectangular Visual Place Recognition via Sliding Windows across the Panoramic View}

\author{Ze Shi$^{1}$\thanks{The first two authors contribute equally to this work.}\, ,
~~Hao Shi$^{1,3,*}$,
~~Kailun Yang$^{2,}$\thanks{Corresponding author (e-mail: {\tt kailun.yang@hnu.edu.cn, wangkaiwei@zju.edu.cn}).}\, ,
~~Zhe Yin$^{1}$,
~~Yining Lin$^{3}$,
~~Kaiwei Wang$^{1,\dagger}$\\
\normalsize
$^{1}$Zhejiang University
~~$^{2}$Hunan University
~~$^{3}$Supremind
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
\input{Tex_content/abstract}

\end{abstract}

%%%%%%%%% BODY TEXT

\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
   \centering
   % \vspace{-3.5em}
   \includegraphics[width=1.0\linewidth]{Pics/performance_comparison_update.png}
   \vspace{-2em}
   \caption{\emph{Performance comparison against} NetVLAD~\cite{arandjelovic2016netvlad} \emph{and} Berton~\etal~\cite{berton2022deep} \emph{on YQ360 for the P2E-VPR task.}}
   \label{fig:performance_comparison}
  \vspace{-1.0em}
\end{figure}

With the popularization of network technology and the advancement of mobile devices, people can now achieve location services using low-cost mobile devices, and related applications are widely deployed on unmanned vehicles and mobile robots~\cite{wang2017survey,liu2018mobile, xu2020low}.
However, most current positioning solutions rely on GPS signals provided by satellites, which can result in delayed location updates and significant errors in indoor or densely-obstructed outdoor environments~\cite{xia2021cross}.
To improve the accuracy of locating and compensate for the reliability of GPS, researchers have introduced visual sensors such as cameras, which use localization algorithms to obtain more accurate geographical coordinates in GPS-denied or weak-signal scenarios~\cite{xia2021cross}.

Visual place recognition refers to the process of identifying pre-stored scene locations based solely on image data captured by visual sensors such as cameras which can help to obtain more accurate geographical coordinates~\cite{garg2021your}.
Generally, the visual place recognition task is redefined as a fine-grained image retrieval task that applies the image retrieval pipeline. 
Given a query image, the system retrieves the most similar images from a pre-built image database with geographical coordinate tags. By obtaining the geographical coordinate tags of the retrieved database images, the system achieves the localization for the query image.

However, this task is notoriously challenging to tackle.
On the one hand, stored database images may differ from actual query images due to lighting, viewing angle, and occlusion. Additionally, changes in image content over time can increase the difficulty of retrieval.~\cite{sheng2021nyu}.
On the other hand, the increase in the scale of stored database images also leads to a larger number of challenging samples that are similar to the query image, thereby requiring a more accurate visual recognition algorithm.

Most image-based visual place recognition methods can be divided into two categories: perspective-to-perspective (P2P)~\cite{arandjelovic2016netvlad, hausler2021patch, wang2022transvpr} or equirectangular-to-equirectangular (E2E) ~\cite{fang2020cfvl,cheng2019panoramic,wang2018omnidirectional} visual place recognition. P2P methods capture images using a pinhole camera or multiple non-overlapping panoramic images, while E2E methods use expensive panoramic cameras that are not easily integrated~\cite{lin2019design}.
However, both of the aforementioned tasks have their own limitations.
P2P methods have limitations with regards to the field of view (FoV) of the pinhole camera and potential for inaccuracies due to different shooting directions. The non-overlapping hard cropping method used in panoramic images can also cause mis-matches for query images that span the cutting seam. On the other hand, E2E methods are relatively complex in optical design and expensive for consumers to use~\cite{park2016design}.

Therefore, a natural and direct idea is to use perspective images to retrieve panoramic database images, where the perspective images are captured by users via low-cost consumer-grade pinhole cameras conveniently and the panoramas with location tags from map providers, whose data are abundant and easy to obtain.
In this way, users can obtain relatively reliable geographical coordinates in GPS-unreliable outdoor scenes through P2E (perspective-to-equirectangular) algorithms~\cite{orhan2021efficient}.

However, the difficulty of designing a P2E unified framework is that the data capacity and informational content in perspective and panorama images are different, making it unreasonable to directly apply P2P or E2E frameworks.
One the other hand, the requirement that the consistent encoding behavior of the database and query images leads to the Transformer-based backbone being limited due to the natural issues such as positional embedding.
To address these issues and enable the direct transfer of backbone networks from P2P methods without modifications, we slide a window over the equidistant cylindrical projection of the panorama image, calculating and comparing the similarity of vector features within the window.
Additionally, during training, we only calculate the triplet loss~\cite{rahman2020triplet} for the patch in the entire panorama image that is closest to the query image's feature descriptor.
The experiment shows that our proposed sliding window method achieves higher retrieval accuracy than directly resizing the panorama image and applying the P2P framework.
Futhermore, we integrate the sliding window strategy and proposed an unified end-to-end P2E visual place recognition framework, \textbf{PanoVPR}.
To train the proposed network on large-scale datasets, we derive a dataset by panoramic stitching database images specifically for our P2E task, called Pitts250k-P2E, based on the Pitts250k dataset~\cite{torii2013visual}.
To validate the performance of framework in real-world environments where the field of view of query images does not completely overlap with that of the panoramic database images, we collect a P2E dataset called YQ360.
Extensive experiments verify that the sliding window strategy on panoramic images is consistently effective for various backbone networks, and a small step size with overlapping windows can achieve higher accuracy.

In summary, our work has the following contributions:
\begin{compactitem}
    \item Rethinking the scenario of the VPR task and proposing a new task that is more in line with reality: perspective-to-equirectangular visual place recognition.

    \item Proposing a method of cyclically overlapping sliding windows on panoramic images, which solves the problem of key objects being cut apart due to hard cropping of panoramic images, leading to insufficient features during querying. Additionally, this method also utilizes the cyclically invariant characteristics of panoramic images to solve the issue of discontinuous image boundaries in panoramic unfolded images.
    
    \item Designing a sliding window-based framework named \textbf{PanoVPR} for visual place recognition from perspective to panorama. This unified framework can directly transfer most perspective-to-perspective methods without any modification. Additionally, the feature encoding strategy using sliding windows on panorama images enables compatibility with transformer-based backbone networks. Moreover, a window-based triplet loss function is proposed, which facilitates easy training and fast convergence of the model.

    \item Proposing two datasets for P2E task, one is the large-scale dataset derived from Pitts250k dataset by stitching panorama images in the database, named as Pitts250k-P2E. The other is a real-world dataset named YQ360, which was collected with query images and panoramic database images having partially overlapped fields of view.

\end{compactitem}

\begin{figure*}[!t]
   \centering
   % \vspace{-3.5em}
   \includegraphics[width=1.0\linewidth]{Pics/sw.pdf}
   \vspace{-2em}
   \caption{\textbf{\emph{Illustrations of the proposed sliding window strategy.}} We slide a window over the panoramic image and encode the image in each window. The overlapping sliding window (blue shadow in the figure) can eliminate the problem of key objects in the image being separated due to hard clipping. The cyclic sliding window (green section on the far right in the figure) makes full use of the cyclic invariance of the panoramic image. The perspective and panoramic images both extract feature descriptors through the P2P backbone.}
   \label{fig:sliding_window}
  \vspace{-1.0em}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related_work}
\input{Tex_content/related_work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\label{sec:methodology}

We propose a \textbf{V}isual \textbf{P}lace \textbf{R}ecognition framework for retrieving \textbf{Pano}ramic database images using perspective query images, dubbed \textbf{PanoVPR}.
Due to the different input size between perspective images and panoramic images, the Transformer-based P2P backbone can not be directly used considering that the encoding behavior must be homogeneous such as positional embedding.
To tackle this issue and build a unified framework that can directly exploit P2P visual place recognition backbone, we adopt sliding window approach on panoramic database images to narrow the model's observation range of the large field of view panoramas, transforming the problem into a comparison-and-retrieval process within the window.
In Sec.~\ref{sec:slidig_window}, we describe our panoramic sliding window approach, including the overlap sliding window design that eliminates the effect of seams and the cyclic sliding window strategy that utilizes the cyclic information of panoramic images.
In Sec.~\ref{sec:framework}, we detail our unified P2E visual place recognition framework PanoVPR in training and inference, and explain the window-based triplet loss function modification to make training process more reasonable and easier.

\begin{figure*}[!t]
   \centering
   \vspace{-3.5em}
   \includegraphics[width=1.0\linewidth]{Pics/framework_update.pdf}
   \vspace{-3em}
   \caption{\textbf{\emph{Illustrations of the proposed PanoVPR framework.}} During training, perspective query images and panoramic database images are fed into a shared encoder to extract features, which are then used to obtain triplet image pairs using hard positive and negative sample mining. The triplet image pairs are encoded and the feature distances are computed using a window-based triplet loss. During testing, the feature descriptors of the panoramic database images are first extracted offline, and then the perspective query images are encoded. The query descriptors are compared with those of the panoramic database images to obtain the Top-N predictions with the window similarity.}
   \label{fig:framework}
  \vspace{-1.0em}
\end{figure*}

\subsection{Sliding Window}
\label{sec:slidig_window}

Visual place recognition tasks using image retrieval paradigm require encoding both query images and database images during training in the exactly same behavior. However, due to the different size and FoV of perspective and panoramic images in P2E task, the same encoder cannot be directly used to encode both images simultaneously.
Therefore, it is one of the difficulties in the design of the P2E framework to utilize all the information of the large-scale panoramic database images without distortion.
If hard cropping is used, it will destroy the image features on both sides of the cut seam.

Based on the above observations, we slide the window on panoramic images (shown in Fig.~\ref{fig:sliding_window}) and split them into multiple sub-images, each sub-image between the same panoramic image is independent of each other and sent to the same encoder to extract features. 
The window represents the region of interest during retrieval period, and the sliding operation simulates the process of horizontal interested region change in the scene captured by this panoramic image.
The sliding window approach allows the feature encoding process of panoramic images to be done within the window, so PanoVPR can migrate all the backbone of P2P networks without any modification.

There are two strategies based on different sliding window strides: overlapping sliding windows and non-overlapping sliding windows.
Compared with the latter, the former strategy contains part of the images of the previous sliding window in each sliding window, which avoids the continuous feature destruction caused by the separation of the left and right images at the seam, thus improving the accuracy of feature extraction and matching. 
Meanwhile, overlapping sliding windows can also increase the number of samples and improve the robustness of the model.

To make full use of the cyclical invariance property of the panoramic image, the sliding window follows a cyclic manner, which means that when the window falls short of the right boundary, it is supplemented with the left boundary, thereby cyclically wrapping the left and right boundaries.

The overall panorama image descriptor is obtained by concatenating the sub-window vectors, and the query image descriptor is compared by sliding window on it. In addition, the query image is resized to match the window size on the panorama image to ensure that query feature vectors after encoding have the same length with the panorama image window part. 
As shown in Fig.~\ref{fig:sliding_window}, the window with the highest similarity between query and sub-window database descriptor is the key window in the panoramic image that matches the query image, and the panoramic image to which it belongs provides the geographic location of the query image.

\subsection{Framework}
\label{sec:framework}

\noindent\textbf{PanoVPR Framework Introduction}. 
As shown in Fig.\ref{fig:framework}, our PanoVPR framework mainly follows an image retrieval pipeline~\cite{lowry2016visual}.
In this pipeline, the training stage is completed online, where both the query images and the database images are fed into the network to obtain feature descriptors. While the testing stage is completed offline, where the model first extracts features from the database images and stores them, then processes the query images in the same way, with the two operations being asynchronous.
The inconsistency between the training and testing logic requires that the query images and the database images share the same feature encoding network.

During training, the model first infers all query and database images to obtain feature descriptors for all images. Then the hard positive and hard negative samples mining is conducted on the database images with the aid of GPS ground truth.
Positive samples are defined as database images that are similar to the query image both in terms of geographical coordinates and feature space, while negative samples are database images that are only similar in feature space but are far away in geographical coordinates. 
As for panorama image, what we aforementioned database images refer to the slide window part. 
After the mining process, we get many triplets image pairs contained a query image, a hard positive sample, and multiple hard negative samples:
\begin{equation}
    (q,db) \longrightarrow (q, p, *n),    
\end{equation}
where $q$, $db$ refer to perspective query images and panoramic database images respectively, $p$ refers to hard positive panoramic image, and $*n$ are multiple hard negative panoramic images.
The obtained triplets are fed into the backbone for training. The window-based triplet loss function is calculated, and the loss is backpropagated to update the model parameters.

During the testing phase, the panoramic database images are encoded by backbone firstly and then stored in the form of descriptors. After then, the perspective query image is coming one by one, and the same backbone encode each frame into a query feature descriptor which will be slid on all database descriptors.
The panoramic database image with the highest similarity score within the searched window will be the top 1 predicted image and so on.
Using this search method, all database images will be reordered according to their local window similarity and the top $N$ images will be selected to obtain the Top-$N$ predictions for each perspective query image.

\noindent\textbf{Window-based Triplet Loss}. 
Since the PanoVPR framework extracts perspective query images and panoramic database images with different descriptor lengths, we design window-based triplet loss based on the original triplet loss~\cite{Schroff2015Learning} for training model more reasonably and conveniently.
The original triplet loss function is defined as:
\begin{equation}
    loss(q,p,*n)=\sum_{i}^{}  max\{d(q,p)-d(q,n_i)+m, 0\},
\end{equation}
where $m$ is the margin parameter, which is a regularization term, and $d(x, y)$ is the distance between the feature vectors $x$ and $y$, defined as follows:
\begin{equation}
    d(x, y)=||x-y||_p.
\end{equation}

The distance above can be considered as a measure of the similarity score between the perspective query image descriptors and the perspective database image descriptors, the smaller the distance value, the higher the similarity score.
In order to adapt the loss function to the panoramic database descriptor, we only compare the similarity of features for the window in the panoramic image that is most similar to the query image. 
This solves the problem of different descriptor lengths between the perspective query image and panoramic database image, making it possible to train the model in an end-to-end manner.
Therefore, the above equation is modified as follows:
\begin{equation}
    d(x, y)=||x-y_w||_p,
\end{equation}
where $y_w$ is the part of vector $y$ with the smallest distance to $x$ when sliding a window on vector $y$, which can be expressed mathematically as:
\begin{equation}
    y_w=argmin_{win}(||x-y[win]||_p).
\end{equation}

\noindent\textbf{Evaluation Metric.}
Following the evaluation method for visual place recognition~\cite{lowry2016visual}, we use recall metric to estimate the performance of the model. Recall metric is defined as follows: given a query image, we search for the database images and predict many retrial candidates. If the ground truth is included in the top-$N$ predictions, then the prediction for this query image is correct. The percentage of correct predictions for all query images is calculated and denoted as Recall@N.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
Currently, there are few readily available datasets for the P2E place recognition task.
To conduct experiments and facilitate future research, we propose two datasets specifically for the P2E task, they are \emph{Pitts250K-P2E} and \emph{YQ360} datasets.
The scale and division of the two datasets are shown in Table \ref{tab:dataset}.

\noindent\textbf{Pitts250k-P2E.}
Pitts250k-P2E is derived from the original Pitts250k dataset~\cite{arandjelovic2016netvlad}.
In the original dataset, the panoramic image acquired by each spot is divided into $2$ yaw directions, but the large angle yaw direction contains more irrelevant background elements such as the sky.
We only select $12$ images with lower yaw angles for each location (these images contain more street view information), and stitch them together to form a panoramic image~\cite{brown2007automatic}, while discarding the $12$ images with larger yaw angles.
For the query image part, we only select images that have overlapping fields of view with the panoramic image but with time gaps, that is, $12$ images with lower yaw angles.

\begin{figure}[!t]
   \centering
   \includegraphics[width=0.95\linewidth]{Pics/device_routine-v4.pdf}
   \vspace{-1em}
   \caption{\textbf{\emph{YQ360 Datasets}}. (a) The data collection devices of the dataset, including front and rear pinhole cameras, panoramic annular cameras, and GNSS. (b) The visualization of the dataset.}
   \label{fig:YQ360}
  \vspace{-1.5em}
\end{figure}

\noindent\textbf{YQ360.}
The YQ360 dataset is collected using a mobile vehicle in Fig.~\ref{fig:YQ360}(a), which
is equipped with a Panoramic Annular Lens (PAL) camera on the top.
Meanwhile, a network camera and a laptop's built-in camera mounted on the front and rear of the car respectively, serving as the front and rear pinhole cameras.
The GNSS sensor is also placed on the vehicle to obtain the GPS coordinates of the acquisition location.
We capture a panoramic image and two narrow-field images (one from the front and another from the rear) approximately every $5$ meters, tagged with the GNSS output of the location. 
To simulate a more realistic testing scenario, the vertical fields of view of the perspective and panoramic images are not completely overlapped. 
The collection routine covers the same path twice as shown in Fig.~\ref{fig:YQ360}(b). It includes a boulevard with heavily repeated textures, as well as open street views that have distinctive features.

\subsection{Implementation Details}
As conventional practice in NetVLAD~\cite{arandjelovic2016netvlad}, during the mining period, one hard positive sample and ten hard negative samples are mined for each query image.
The geographical coordinate labels are clustered using KNN~\cite{liu2000gabor} method, with a clustering threshold of $10$ meters during training and $25$ meters during testing.
In~\cite{berton2022deep}, it has been shown that using the partial mining method can achieve accuracy similar to full mining approach while reducing memory usage. Therefore, we use the \emph{partial} mining method in following experiments.

Moreover, the back-end of the framework utilizes the GeM pooling~\cite{rahman2020triplet} to aggregate output tensor of the encoder. This method globally averages the tensor either in the token number dimension or in the feature map dimension. This process ultimately generates the feature descriptors for the input image.

During training, the query images and database images are simultaneously used as inputs, and each panoramic image is transformed into multiple sub-images using our sliding window method. 
Therefore, sending a group of ($q$, $p$, $*n$) triplets to the encoder requires a large GPU memory.
For this reason train-batch-size is set to $2$.
Instead, the feature extraction process for query and database images is independent during testing, so infer-batch-size is set to $16$. 
We also set the margin parameters $m$ in triplet loss to $0.1$.

Furthermore, when conducting testing experiments on YQ360 dataset, we account for variations in the overlapping field of view range between query and database images across different datasets. 
Specifically, we fine-tune our model on the Trip1 subset of the YQ360 dataset and evaluate its performance on the Trip2 subset.

Our framework is implemented using PyTorch and the experiments are conducted on a machine with $4$ NVIDIA 3090 graphics cards, 128 GB RAM, and 88 CPU cores.

\subsection{Results}
\noindent\textbf{Sliding Window Validation.}
To verify the effectiveness of our proposed method of sliding windows on panoramic images, we conduct the following experiments on Pitts250k-P2E dataset and show the results in Table \ref{tab:dataset}.

\begin{table}[!t]
\renewcommand{\thetable}{1}
    \begin{center}
        \caption{\emph{The training, validation and testing sets split.}}
        \label{tab:dataset}
        \vspace{-1.0em}
        \input{Tables/dataset}
        \vspace{-1em}
    \end{center}
\end{table}

\begin{table}[!t]
\renewcommand{\thetable}{2}
    \begin{center}
        \caption{\emph{Effectiveness of PanoVPR on Pitts250k-P2E dataset.}}
        \label{tab:panoVPR_pitts}
        \vspace{-1.0em}
        \input{Tables/PanoSwin}
        \vspace{-1em}
    \end{center}
\end{table}

\begin{table}[!t]
\renewcommand{\thetable}{3}
    \begin{center}
        \caption{\emph{Quantitative comparison on the YQ360 dataset.}}
        \label{tab:panoVPR_yq}
        \vspace{-1.0em}
        \input{Tables/YQ360_compare}
        \vspace{-2em}
    \end{center}
\end{table}

We select the lightweight Swin-Transformer-Tiny~\cite{liu2021swin} as our backbone, and directly resize panoramic images to meet the requirements for input image size as our baseline.
 
Comparisons between our method PanoVPR and baseline show that directly resizing panoramic images to the appropriate size for the encoder greatly reduces the model's accuracy, only can get 10.1 R@1 in Pitts250k-P2E dataset, which is 31.3 points lower than our method \emph{PanoVPR(SwinT)$_{{\times}24}$}.
We think the reason for this is that resizing panoramic images causes a significant loss of information, resulting in distorted global features that are no longer accurate or reliable after encoding. 
However, with our proposed approach, the encoder receives panoramic images in the form of sliding image windows, which preserves their original integrity and avoids distortion before encoding.
Therefore, our proposed method significantly improves the accuracy compared to the baseline.

In addition, we experiment with different sliding window strides \{$\times8$, $\times16$, $\times24$, $\times32$\} and find that as the stride decreases and the sliding window becomes finer, the model's recall precision improves(from R@1 22.0 to 41.4 when stride from $\times8$ to $\times32$). 
It's important to note that the aforementioned notation $\times N$ indicates that the panoramic image is divided into $N$ equal parts, and stride selects the length value of one of them. Therefore the larger the value of $N$, the smaller the stride and the finer the sliding window.

\noindent\textbf{Comparison against State-of-the-Art.}
To verify our proposed PanoVPR in real scenes (query images' FoV may not completely overlap database images) and to compare it with current mainstream methods, we conduct the comparison experiments and show the results in Table \ref{tab:panoVPR_yq}.

In the NetVLAD~\cite{arandjelovic2016netvlad} method, the backbone is selected as ResNet50, and other settings remain unchanged. In \cite{berton2022deep}, the backbone is selected as ViT, and the clustering back-end uses GeM. From the comparison, it can be found that PanoVPR does not require a complex model with a large number of parameters. Only simply sliding a window over the panoramic image, a significant improvement in recall metric can be achieved. 
PanoVPR employs Swin-Tiny as its backbone, and by using $\times24$ sliding window stride, it achieves a 37.5\% increase in R@1 accuracy compared to the CNN method. Besides, PanoVPR improves R@1 accuracy by 19.8\% compared to the ViT method while reducing the model parameter count by 67\%.

From the analysis above, it's evident that our proposed sliding window approach leads to higher recall metric, when perspective query images retrieve panoramic images with incomplete overlapping fields of view on real dataset.
Some qualitative presentations are shown in the Fig.~\ref{fig:quality_pitts250k}.

\begin{figure*}[!t]
   \centering
   \includegraphics[width=1.0\linewidth]{Pics/vis-v4.pdf}
   \vspace{-2em}
  \caption{\emph{\textbf{Visualization results on the Pitts250k-P2E and YQ360 test set using the proposed PanoVPR (SwinT) framework.} We highlight the predicted key windows in the Top3 matched panorama database images, where the most important Top1 has achieved precise matching and positioning. PanoVPR can accurately predict the perspective query's geolocalization and focus on relevant regions of the panoramas.}}
   \label{fig:quality_pitts250k}
  \vspace{-1em}
\end{figure*}

\subsection{Ablation Studies}

\begin{table}[!t]
\renewcommand{\thetable}{4}
    \begin{center}
        \caption{\emph{Ablation of different backbones.}}
        \label{tab:abl_backbone}
        \vspace{-1.0em}
        \input{Tables/backbone}
        \vspace{-2em}
    \end{center}
\end{table}

Our proposed PanoVPR framework can directly use the backbone in the P2P task without any modification. To investigate the impact of different backbone networks on our framework, we conducted ablation experiments by changing the backbone network and the sliding window stride.

In ablation experiments, we select a CNN backbone ConvNeXt-Tiny, ConvNeXt-Small and a Transformer backbone Swin-Small. The results are shown in the Table \ref{tab:abl_backbone}.
It can be seen that using PanoVPR with any backbone in our ablation experiment, $\times24$ sliding window stride achieves at least 25\% R@1 metric improvement over the corresponding baseline model.
In other words, our proposed PanoVPR framework is a strong robust model with high compatibility to backbone.
The conclusion that a small stride overlapping sliding window can achieve higher recall metric also holds true when PanoVPR backbone changed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\input{Tex_content/conclusion}

%------------------------------------------------------------------------

%%%%%%%%% REFERENCES
\clearpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
