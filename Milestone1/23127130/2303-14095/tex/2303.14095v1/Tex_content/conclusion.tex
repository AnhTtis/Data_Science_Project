We propose a new framework \textbf{PanoVPR}, based on sliding window to solve the problem of visual place recognition from perspective to equirectangular images. The framework determines the shooting position of the perspective image indirectly by calculating and matching the description vectors generated from the images within the sliding window. This framework avoids hard cropping that destroys image continuity and allows direct transfer of the backbone network from perspective to perspective visual place recognition tasks without any modification. Additionally, we propose a window-based triplet loss function to train the model more effectively. We also derive a new large-scale dataset  \emph{Pitts250k-P2E}, with geographical location tags for the P2E task. Furthermore, to better simulate real-world scenarios, we collected another P2E datasets \emph{YQ360}, which perspective query images and panoramic database images with non-overlapping FOV. The experiments show that our proposed PanoVPR framework achieves higher recall metric while using a lightweight backbone compared with our baseline and other mainstream approach.