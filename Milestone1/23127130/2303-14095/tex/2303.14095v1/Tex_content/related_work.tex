\noindent\textbf{Perspective Visual Place Recognition.} 
Visual position recognition from perspective to perspective involves narrow field-of-view images for both query and database images.
The image data is directly acquired from a pinhole camera, such as Norland~\cite{sunderhauf2013we}, RobotCar Seasons~\cite{maddern20171}, and MSLS~\cite{warburg2020mapillary}, or by unwrapping and cropping panoramic images provided by a map provider, such as Pitts250k~\cite{torii2013visual}, SF-XL~\cite{Berton_CVPR_2022_CosPlace}, \etc.

Initially, researchers used hand-crafted methods to detect keypoints in images and extract compressed representations as feature descriptors, which serve as the representation of the key information in images, such as SIFT~\cite{lowe1999object}, SURF~\cite{bay2006surf}, and VLAD~\cite{jegou2010aggregating}.
With the popularity of deep learning methods, researchers found that using end-to-end neural networks, supplemented by keypoint clustering methods to extract image feature descriptors, have higher robustness than hand-crafted methods and achieve higher recall accuracy in retrieving images in complex and difficult scenarios~\cite{masone2021survey, chen2022deep}.

NetVLAD~\cite{arandjelovic2016netvlad} employs a CNN backbone to extract local features from images and originally designs the NetVLAD layer, which convert the previously non-differentiable clustering operation into an end-to-end manner, thus obtaining a global feature descriptor for the image. 
Patch-NetVLAD~\cite{hausler2021patch} is based on the above work and divides the image into multiple patches, extracts features for each patch image using NetVLAD, and combines the descriptor features of multiple scales of patches with the entire image's descriptor feature to obtain a robust descriptor that considers both global and local information. 
SuperPoint~\cite{detone2018superpoint} combines the interest point detection decoder and descriptor decoder in parallel at the back end of a shared CNN encoder, making it faster to inference and simpler to train. 
TransVPR~\cite{wang2022transvpr} uses vision transformer as the backbone, to obtain the global representation of the image through multi-layer attention aggregation.

More recently, some works, such as CosPlace~\cite{Berton_CVPR_2022_CosPlace}, have converted the image retrieval problem into an image classification problem to solve the problem without expensive negative sample mining costs when training on large-scale datasets.
Unlike these works, we address panoramic visual place recognition which can provide a wider FoV for image retrieval paradigm.

\noindent\textbf{Panoramic Visual Place Recognition.}
This visual place recognition method uses panoramic unwrapped images with a large field of view for both query and database images.
Wang~\etal~\cite{wang2018omnidirectional} utilized convolutional neural networks to extract features from indoor scenes and designed circular padding and roll branching techniques to improve the model's performance, taking into account the rotational invariance characteristics of panoramic images.
Cheng~\etal~\cite{cheng2019panoramic} proposed a visual localization method for challenging outdoor scenes. The method involves unwrapping a panoramic image into a cylindrical projection and feeding the results into a NetVLAD network to generate active deep descriptors.
They then generate localization results using multiple image sequence matching.
Marta ~\etal~\cite{rostkowskaembedded} used CNN networks directly on an omnidirectional image without unfolding and extracted original omnidirectional image features, using this approach to solve indoor place recognition.
Fang~\etal~\cite{fang2020cfvl} proposed a two-stage place recognition method. In the first stage, a NetVLAD model is used to roughly filter database images (panoramic cylindrical unwrapped images) to obtain candidate images. In the second stage, a learnable descriptor Geodesc~\cite{luo2018geodesc} is used to re-rank the candidate images to achieve higher recall.

A recent work on efficient retrieval of perspective images from panoramic images involves sliding windows on feature maps generated by CNNs~\cite{orhan2021efficient}.
CNN can handle input images of arbitrary sizes, so it is possible to encode entire panoramic images. 
However, if we use visual transformer~\cite{liu2021swin,dosovitskiy2020image} as the backbone network, encoding the entire panoramic image directly would require a huge amount of GPU memory which is impractical.
On the other hand, directly resizing the entire panoramic image to an appropriate size for the transformer would greatly reduce the model's accuracy, as demonstrated in Sec.~\ref{sec:experiments}.

To increase the diversity of backbone network types in the framework and address the issue of transformer-based backbone networks being unable to directly encode entire panoramic images, we slide windows over the panoramic image and only feed the windowed portion of the panoramic image into the backbone network.
Therefore, the backbone network in the framework only computes for the windowed portion, solving the problem of encoding large-sized panoramic database images.
