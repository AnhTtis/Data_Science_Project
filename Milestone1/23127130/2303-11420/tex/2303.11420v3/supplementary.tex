% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[final]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16513} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2024}


\begin{document}

%%%%%%%%% TITLE
\title{Supplementary for ``ADCNet: Learning from RAW Radar Data via Distillation''}



\maketitle

\section{More experiment details}
For training the ADCNet (both training from scratch and fine-turing from a pre-trained checkpoint), we use a single machine with 4  NVIDIA A10G GPUs. We use the Adam optimizer \cite{kingma2014adam}, with initial learning rate of $4\times 10^{-4}$, and a batch size of 4 per GPU. We use the sum of validation F1 score (for object detection) and validation mIOU score (for freespace segmentation) to pick the best checkpoint.

For pre-training, we use 4 nodes each equipped with 4  NVIDIA A10G GPUs. The learing rate $16 \times 10^{-4}$, and the batch size is 12. We train the model for a total of 60 epochs, and pick the model with smallest validation loss for the subsequent fine-tuning step.

\section{Learnable DSP training}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/range-window.png}
    \caption{The first learnable window function of ADCNet before and after training}
    \label{fig:range_window}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/doppler-window.png}
    \caption{The second learnable window function of ADCNet before and after training}
    \label{fig:doppler_window}
\end{figure}

\subsection{Implementation of the learnable SP module}
One difficulty of implementing the learnable SP module is the fact that the DFT operation is in complex domain. To avoid using complex operations in the neural network, we split the complex tensors (ADC array and DFT matrix) into real and imaginary parts, and perform the multiplications separately, as shown in Figure~\ref{fig:learnable_dft}. These are standard operations and can be easily implemented in a typical deep learning framework such as Pytorch. The learnable window module involves only one parameter vector, and it is multiplied to the inputs in the forward pass.

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/learnableDFT.pdf}
    \caption{Implementing learnable DFT using real paramater matrices}
    \label{fig:learnable_dft}
\end{figure}

\subsection{Effects of training on the windowing functions}
The effect of end-to-end training on window functions are shown in Figure~\ref{fig:range_window} and Figure~\ref{fig:doppler_window}. It can be seen that in both the functions, the top is reduced and sides are raised. In signal processing, choice of window functions (see \eg \cite{window_funcs}) are usually made by a human expert, while the proposed approach amounts to data-driven window design -- a process that could be incorporated in radar signal processing implementation.


\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/exact_dft_diff.png}
    \caption{The real weights of the DFT matrix before and after training when \textit{Exact-DFT} initialization is used. x-axis: DFT index; y-axis: real part of the DFT coefficient.}
    \label{fig:exact_dft_training}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=\linewidth]{figs/perturbed_dft_diff.png}
    \caption{The real weights of the DFT matrix before and after training when \textit{Perturbed-DFT} ($\gamma = 0.1$) initialization is used. x-axis: DFT index; y-axis: real part of the DFT coefficient.}
    \label{fig:perturbed_dft_training}
\end{figure}


\subsection{Effects of perturbation on the learnable DFT}
When training with the \textit{Exact-DFT} initialization, we found that the DFT weights almost do not change before and after training, as shown in Figure~\ref{fig:exact_dft_training}. Figure~\ref{fig:exact_dft_training} shows the real part of the first 5 rows of the second DFT matrix in ADCNet. This confirms our suspicion that without perturbation, the learnable DFT layer can get stuck in the local optimum of the DFT matrix. For a comparison, the same visualization for ADCNet with \textit{Perturbed-DFT} initialization is shown in Figure~\ref{fig:perturbed_dft_training}.

\begin{table}[!htp]\centering
\caption{The difference between initialization and trained DFT weights, using either \textit{Exact-DFT} or \textit{Perturbed-DFT} initialization}\label{tab:init_diff}
\scriptsize
\begin{tabular}{lrrrrr}\toprule
&\multicolumn{2}{c}{The first learnable DFT} &\multicolumn{2}{c}{The second learnable DFT} \\\cmidrule{2-5}
&real &imaginary &real &imaginary \\\midrule
Exact-DFT &0.039 &0.039 &0.039 &0.039 \\
Perturbed-DFT &0.120 &0.120 &0.121 &0.121 \\
\bottomrule
\end{tabular}
\end{table}

To get a quantitative measure of the difference between the initialization and trained DFT weights, we compute the average absolute difference between the two set of weights. The real and imaginary part is computed separately, and the results are shown in Table~\ref{tab:init_diff}. As can be seen from Table~\ref{tab:init_diff}, with Exact-DFT initialization, the DFT matrix indeed does not change much compared with the one with Perturbed-DFT, confirming the necessity to add perturbation to the DFT matrix for a strong end-to-end model.

\section{Additional description on the ADC Unet and Conv3d+FFTRadNet baselines}
The ADC Unet and Conv3d+ FFTRadNet are designed such that they have similar number of learnable parameters as the ADCNet.

For ADC Unet, we use a standard Unet backbone to replace the backbone the in the ADCNet. Importantly, we also removed the dilated convolution layer that is supposed to exploit the radar signal structure. Removal of the dilated convolution layer is motivated by the fact that these special convolution layer runs much slower than the standard 3x3 convoutional layers. As a result, we get a baseline model that, albeit yields worse accuracy, runs much faster. The proposed pre-training technique improves this baseline quite notably (5\% absolute improvement on F1 score).

For the Conv3d+FFTRadNet baseline, we removed the learnable SP module from the ADCNet and replaced with a stack of Conv3d layers. The motivation is to test if such a generic model can handle the intricate radar signal purely from end2end training: the results from our experiment (in the main paper) is negative, highlighting the necessity of incorporating signal processing knowledge and techniques into the end2end learning regime. 
{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}


\end{document}