\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{times}
\usepackage{epsfig}
% https://www.overleaf.com/project/63d164e7c7c9de845b2082e6
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage[letterpaper, left=1in, right=1in, bottom=1in, top=0.75in]{geometry}

\usepackage{xcolor}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\etal}{\textit{et al}., }
% \newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}.\ }


\begin{document}
\title{ADCNet: End-to-end perception with raw radar ADC data}
\author{Bo Yang \hspace{20pt} Ishan Khatri \hspace{20pt} Michael Happold \hspace{20pt} Chulong Chen\\
Motional\\
{\tt \small \{bo.yang, ishan.khatri, michael.happold, chulong.chen\}@motional.com}
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle


%%%%%%%%% ABSTRACT
\begin{abstract}
   There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\begin{figure}[!ht]
    \centering
    % \fbox{\rule{0pt}{3in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{figs/1793.png}
    \includegraphics[width=\linewidth]{figs/1769.png}
    \includegraphics[width=\linewidth]{figs/1824.png}
    \includegraphics[width=\linewidth]{figs/3702.png}
    \caption{ADCNet prediction samples. The camera images are provided for reference only. The Range-Azimuth map is generated by the RADIal SDK. The small red squares in the RA image denote ground truth, while the blue squares denote network predictions }
    % {\color{blue}TODO larger boxes}
    \label{fig:adcnet_samples}
\end{figure}

Perception systems for autonomous driving have been undergoing fast progress over the last decade. This is well-motivated, as the perception system provides input for subsequent systems such as motion prediction and planning. Accurate perception of the surroundings is thus paramount for the operation of an autonomous vehicle (AV).

Compared to LiDAR, radar has been under-investigated for AV perception tasks. Indeed, much of perception system's progress is built upon LiDAR sensors. LiDAR sensors provide accurate measurements in common driving scenarios, particularly those with sunny and dry weather. 
However, the shortcomings of LiDAR cannot be overlooked. Among other things, LiDAR sensors are less physically robust and much more costly than radars, and LiDARs are usually not robust against adverse weather, such as rain and fog \cite{kutila2018automotive}. The hefty cost of LiDARs undermines the cost-effectiveness of AVs, while non-robustness poses safety risks.


While radars have long been applied in automotive applications such as blind spot monitoring and adaptive cruise control, their application in assisting semantic understanding of the scene have been limited. This is due to the fact that traditional automotive radars mostly work at 24GHz, with a limited usable frequency bandwidth of 200MHz \cite{karthik2019moving}, and even this usable band is expiring \cite{frequency_choice}.  This small frequency band limits the attainable range resolution, and the low carrier frequency limits attainable angle resolution. Together these factors render traditional radars unsuitable for AVs, as they usually require high resolution sensing of the environment.



In the past few years, imaging radars for autonomous driving started to emerge. Compared to traditional automotive radars, imaging radars usually operate at around 77GHz, with roughly 4GHz available bandwidth -- a much larger band than that of traditional automotive radars. In addition, these imaging radars are often equipped with multiple transmit and receiving antenna. Utilizing Multiple-Input-Multiple-Output (MIMO) technology \cite{rao2018mimo}, these new radars achieve much improved range and angle resolution. 

Capitalizing on this improved sensing capability of imaging radar, several works \cite{meyer2021graph, hwang2022cramnet,  wang2021rethinking, li2022modality, qian2021robust, li2022exploiting, palffy2020cnn} have attempted building perception models on these radars. One common theme of these works is that perception is done with low-level radar data, instead of the radar detection data as in traditional automotive radars. Low-level radar data refers to any radar sensor data that have not gone through the peak detection \& thresholding step of the signal processing chain (see Figure~\ref{fig:sp_radar}). There are several low-level radar data representations: range-doppler (RD) radar cube, range-azimuth-doppler (RAD) cube,  etc., that have gone through various stages of signal processing, but not the final peak detection step that leads to very sparse radar detections.

In this work, we explore the possibility of end-to-end perception from raw radar data. That is, the perception model is built upon the ADC data, without any pre-defined signal processing steps. Although signal processing steps, such as applying discrete Fourier transform (DFT) and windowing functions, are well-motivated, they could also introduce signal model mismatch that hurts final perception performance. On the other hand, radar ADC data is quite intricate, where waveform design (e.g. chirp firing sequence, time-division, code-division, and Doppler-division multiplexing, etc.) choices could make unraveling the data challenging. In other words, it remains an open question whether a system learned directly on ADC data alone can produce competitive or even state-of-the-art results.

To the best of our knowledge, this work provides the first affirmative answer. In order to circumvent the intricacy of radar ADC data, we develop several techniques, specifically in model architecture and training method. At a high level, our contributions are 
\begin{itemize}
    \item We showcase that better perception performance can be obtained using radar ADC data;
    \item We design a model architecture that effectively incorporates \textit{trainable} signal processing modules into the neural network such that end-to-end learning starts from a good starting point;
    \item We design a signal processing (SP) guided pre-training method that assists in learning a powerful representation with unlabelled data. {To the best of our knowledge, this is the first pre-training method design for low-level radar data.}
\end{itemize}
To clarify, while we devise an end-to-end learning-based approach on raw ADC data, signal processing techniques are exploited on various aspects of the model architecture and training method. We view this as a happy marriage of a traditional, theory-based paradigm with a data-driven, learning-based paradigm. We show that superior performance can be achieved when wisdom from both worlds is judiciously employed.

\section{Background on radar signal processing}
This work is mainly concerned with a type of automotive radar termed Frequency Modulated Continuous Wave (FMCW) radar, see \eg \cite{brooker2005understanding}. At each measuring cycle, these radars send out a series of rapid ``chirps'' -- short waves with increasing frequency. At the receiving antenna, the reflections are captured, and sampled by an ADC device.
This digitized signal is then passed to other software modules in the radar sensor for signal processing.  A typical radar signal processing chain is shown in Figure~\ref{fig:sp_radar}. The readers are referred to \cite{richards2010principles} for a more in-depth coverage of radar basics.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/sp-radar-crop.pdf}
    \caption{A simplified radar signal processing chain}
    \label{fig:sp_radar}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/adc-rd-crop.pdf}
    \caption{Illustration of ADC and RD arrays. Left: ADC, Right: RD}
    \label{fig:adc_rd_array}
\end{figure}
 The ADC data is usually organized as arrays as shown in Figure~\ref{fig:adc_rd_array}. Each frontal slab has dimension (number of samples per chirp, number of chirps), and holds all the samples for one receiving antenna. Signal processing operations (mainly DFTs) transform these two dimensions into range and Doppler dimensions, forming the RD array, as shown in the right part of Figure~\ref{fig:adc_rd_array}.


\section{Related works}

\textbf{Radars for AV perception.} {There has been a considerable amount of work} utilizing radar detections for AV perception \cite{yang2020radarnet, shah2020liranet, nabati2021centerfusion}. In these papers, the radar detections are usually fused with another sensor modality, such as camera and LiDAR. The main reason is that radar detections are very sparse, and reliable semantic understanding of the driving scene is hard to achieve with radar alone.
In \cite{yang2020radarnet}, the authors propose a fusion scheme that combines radar detections and LiDAR point clouds, and obtain improved performance for object detection and velocity estimation. In \cite{shah2020liranet}, the authors combine radar detections and LiDAR point clouds and achieve improved performance for trajectory prediction, compared to using LiDAR alone.

There is a growing body of work attacking the AV perception problem by means of low-level radar data \cite{meyer2021graph, hwang2022cramnet,  wang2021rethinking, li2022modality, qian2021robust, li2022exploiting, palffy2020cnn}. CramNet \cite{hwang2022cramnet} proposes a method for fusing radar Range-Azimuth (RA) images with camera images using the attention mechanism. In \cite{meyer2021graph}, a graph convolution network is developed to work with radar RA data. In \cite{qian2021robust}, a radar-LiDAR fusion method is developed to improve perception robustness against adverse weather. In \cite{li2022modality}, the authors propose a method to cope with the missing modality problem for a radar-LiDAR fusion system.

It is also worth highlighting the low-level radar datasets that have been facilitating the research in this direction. These include RADIATE \cite{sheeny2021radiate}, RADIal \cite{rebut2022raw}, ORR \cite{RadarRobotCarDatasetICRA2020}, CRUW \cite{Wang_2021_WACV}, while new ones (\eg K-Radar\cite{paek2022k}) continue to emerge. Different datasets often use different radar sensors, so their radar data representations tend to be different. For example, RADIATE provides radar radio-frequency (RF) images, ORR provides radar intensity maps, CRUW provies RA maps, while RADIal provides both ADC and Range-Doppler (RD) data. Since we are interested in exploring ADC data, we adopt the RADIal dataset to conduct our experiments, as among the currently ready-to-use radar datasets, RADIal is the only one that provides radar ADC data.

\textbf{Pre-training} Pre-training has been proven quite successful in several important domains, such as computer vision \cite{oord2018representation,he2020momentum,pmlr-v119-chen20j}, natural language understanding \cite{devlin2018bert, brown2020language}, speech and audio processing \cite{chung2020generative, li2021contrastive, zhang2022bigssl}, etc. These methods usually entail designing a surrogate learning task, such as predicting masked part of the input signal, so that the neural network learns intrinsic structure of the data. While deceptively simplistic, many works in this genre have shown that powerful representations, that support impressive performance on downstream tasks (such as image classification, question answering with natural language, automatic speech recognition), can be learned from a large collection of data.

\textbf{End-to-end learning.} End-to-end learning from raw sensor data has been gaining traction in several fields. In these works, traditional feature representations, which are usually designed with SP techniques, or handcrafted with human intuition, are replaced with carefully designed neural networks.
In audio and speech processing, deep learning from the raw audio waveform has shown impressive performance for several important tasks, such as music analysis \cite{dieleman2014end},  phone classification and speaker recognition \cite{ oord2018representation}, speech recognition \cite{schneider2019wav2vec} etc. In computer vision, learning from raw pixel values has been the mainstream paradigm, after \cite{krizhevsky2017imagenet} showed impressive performance on the difficult ImageNet dataset \cite{deng2009imagenet}.





\begin{figure}[htp]
\begin{center}
\includegraphics[width=\linewidth]{figs/radial_cam_rd_ra.png}
\end{center}
   \caption{Two data samples from the RADIal dataset. Top row: camera images, Middle row: radar Range-Azimuth map, Bottom row: radar Range-Doppler map.}
\label{fig:radial_example}
\end{figure}

\begin{figure*}[ht]
\begin{center}
% \fbox{\rule{0pt}{2in} \rule{0.9\textwidth}{0pt}}
\includegraphics[width=\linewidth]{figs/adcnet-crop.pdf}
\end{center}
   \caption{The ADCNet model. A learnable signal processing module, as shown in the dashed box, is introduced within the network to enable end-to-end learning. The backbone and head networks are adopted from \cite{rebut2022raw} for comparison, while other types of backbone and head networks can be easily incorporated.}
\label{fig:adcnet}
\end{figure*}

\section{The RADIal dataset}\label{sec:radial}
To assist discussion, we provide a brief introduction to the RADIal \cite{rebut2022raw} dataset, and readers are referred to the original publication for more details.

The RADIal dataset provides synchronized radar, LiDAR and camera images for about 2 hours of driving. Labels for vehicle detection and freespace segmentation are also provided. In this work, we are interested in the radar data. For this, the dataset provides ADC data and RD data. The RADIal SDK \footnote{https://github.com/valeoai/RADIal} also enables generating RAD maps from the ADC data. Figure ~\ref{fig:radial_example} shows two samples from the dataset.

The radar sensor used in the RADIal dataset is an imaging radar with 12 transmit antennas and 16 receiving antennas. Doppler division multipliexing (see \eg \cite{xu2021transmit}) is employed in the radar waveform design -- this can be seen from Figure~\ref{fig:radial_example}, where a single target produces multiple equal-spaced bright spots in the RD map. The radar is mounted at the front of the vehicle, and has an approximate field-of-view (FOV) of 180 degrees, and a range of around 100 meters.

The RADIal dataset contains a relatively balanced distribution of driving scenes in city, country side, and highway. For the object detection task and the freespace segmentation task, the labels are generated by models running on images and LiDAR, followed by human verification. Only vehicles are labeled for object detection.

Note that, out of the total 25,000 synced data frames, only about 8300 frames have objects and segmentation label. In Section~\ref{sec:pre_training} we will propose a method to utilize the unlabeled dataset for pre-training and learn a meaningful representation.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/radial-dsp-crop.pdf}
    \caption{The signal processing steps for generating RD map in the RADIal dataset}
    \label{fig:radial_dsp}
\end{figure}

\section{The ADCNet}
In this section, we present our novel techniques that enable learning from radar ADC data.


\begin{figure}[ht]
    \centering
    % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{figs/learnable-dft-crop.pdf}
    \caption{A learnable DFT layer. The dashed box is shared. The two modules inside the dashed box represent parameter matrices of the neural network, and are initialized as (perturbed) real and imaginary part of the DFT matrix.}
    \label{fig:learnable_dsp}
\end{figure}
\subsection{A trainable DSP module}\label{sec:trainable_dsp}
In \cite{rebut2022raw}, the authors presented a model termed FFT-RadNet. This model learns to perform object detection and freespace segmentation, using radar RD data. For the RADIal dataset, the RD data is generated from ADC data by applying DFT and windowing functions, as commonly done in radar signal processing. The specific signal processing steps for the RADIal dataset are summarized in Figure~\ref{fig:radial_dsp}.

We hypothesize that end-to-end learning from radar ADC data needs to start from signal processing. That is, we need to include layers that mimics signal processing steps in the early part of the neural network, while these SP-like layers will be trained end-to-end together with other layers. In Section~\ref{sec:init_exp}, we detail experiments to examine this hypothesis. 

Following this intuition, we design an SP module inside the proposed ADCNet, as shown in Figure~\ref{fig:adcnet}. The complex DFT operations are implemented with real weight matrices as shown in Figure~\ref{fig:learnable_dsp}.  At initialization, this module mimics the signal processing operations of RADIal as shown in Figure~\ref{fig:radial_dsp}.  If we initialize the module to be the exact signal processing chain used in FFT-RadNet, then the ADCNet is the same as FFT-RadNet at initialization, while the SP module is left trainable during the training process. 


However, we suspect that initializing the SP module exactly as the signal processing chain of FFT-RadNet can hurt performance, since the training process may fail to drive the weights away from the local minima to which they are initialized.  To alleviate the issue, we developed a perturbed-DFT initialization strategy: 
\begin{align}
    \mathbf{M} = \mathbf{M}_{\text{DFT}} + \mathcal{N}(0, \gamma)
    \label{eq:perturb}
\end{align}
In Eq.~\ref{eq:perturb}, $\mathbf{M}_{\text{DFT}}$ denotes the DFT matrix, and $\mathcal{N}(0, \gamma)$ denotes a random Gaussion matrix (with the same shape as the DFT matrix), with i.i.d. elements each has mean 0, and variance $\gamma$.  Intuitively, this $\gamma$ parameter should be small, such that the final matrix $\mathbf{M}$ used for initializing the SP module is perturbed, but still resemble the DFT matrix. In our experiments, the $\gamma$ is set to 0.1. The effect of this perturbation is examined in Section~\ref{sec:init_exp}

\subsection{Supervised model training}\label{sec:supervised}
To facilitate comparison with FFT-RadNet, we adopt the same multi-task learning setup. Specifically, the ADCNet is trained to perform both object detection and freespace segmentation at the same time.

For model training, the object detection part of the loss is
\begin{align}
    \mathcal{L}_{\text{det}} = \text{focal}(\mathbf{y}_{\text{cls}}, \hat{\mathbf{y}}_{\text{cls}}) + \alpha \text{smooth-L1}(\mathbf{y}_{\text{reg}}, \hat{\mathbf{y}}_{\text{reg}}),
\end{align}
where $\mathbf{y}_{\text{cls}}$ denotes the ground-truth classification labels on the feature map, with $1$ indicates presence of object while $0$ otherwise. The shape of  $\mathbf{y}_{\text{cls}}$ is $N_{\text{range-bins}} \times N_{\text{azimuth-bins}}$.The $\mathbf{y}_{\text{reg}}$ denotes the regression targets. Following \cite{rebut2022raw}, the network predicts a range and an azimuth value. The regression targets are thus the remainders of range and azimuth modulo range and azimuth bin sizes. The shape of $\mathbf{y}_{\text{reg}}$ is $N_{\text{range-bins}} \times N_{\text{azimuth-bins}} \times 2$, where the last dimension corresponds to range and azimuth. The $\text{focal}()$ function is from \cite{lin2017focal}, and the $\text{smooth-L1}()$ function is also known as Huber loss \cite{huber1992robust}. The $\alpha$ is a hyper-parameter balancing the two parts.

The freespace segmentation part of the loss is
\begin{align}
    \mathcal{L}_{\text{seg}} = \sum_{r, a} \text{BCE}(\mathbf{y}_{\text{seg}}(r, a), \hat{\mathbf{y}}_{\text{seg}}(r, a)),
\end{align}
where $\mathbf{y}_{\text{seg}}(r, a)$ denotes the ground truth at location $(r, a)$, with $1$ denotes object presence and $0$ otherwise. The $\text{BCE}()$ term denotes the binary cross entropy loss.

Combining the two parts, the loss function for training is thus
\begin{align}
    \mathcal{L} = \mathcal{L}_{\text{det}} + \beta\mathcal{L}_{\text{seg}},
\end{align}
where $\beta$ is a hyper-parameter controlling the weights on the freespace segmentation task.

\subsection{A signal processing-guided pre-training scheme}\label{sec:pre_training}
As discussed in Section~\ref{sec:radial}, only a small part of all the frames from RADIal are labeled. This is likely to hold true for other (future) radar datasets as well, since collection of data samples is relatively easy while obtaining high-quality labels is difficult and costly. This is especially true with raw radar data as shown in \cite{paek2022k} since radar data tends to be less human interpretable and requires sensor fusion techniques for accurate 3D labels. We envision that leveraging a large collection of unlabeled data will help with end-to-end learning. This is because learning from ADC data directly means disregarding important prior information (\eg  the fact that fast-time frequency correspond to target range, and slow-time frequency correspond to relative radial velocity), in order to leave larger parameter space for data-driven learning.

While exposing the model to a larger dataset is a good motivation, the proposed pre-training method is further designed to exploit advanced techniques in SP. In particular, we would like to pre-train the network such that it learns to estimate angle-of-arrival (AoA) of targets. Together with estimated range and Doppler information, we would like to get the Range-Azimuth-Doppler (RAD) cube from the radar ADC data. This is because, as spatial information, range and azimuth help localize the object in the scene, while Doppler information is helpful distinguishing moving and static objects. Such a data representation should be quite useful for downstream perception tasks.

Learning from the time-tested SP algorithms is well motivated. A considerable amount of effort has been devoted to study the angle estimation problem (core step in generating the RAD map) in the SP community, see \eg \cite{yardibi2010source, roberts2010iterative, qian2016enhanced}. These algorithms are designed to achieve some desirable properties, such as noise robustness, outlier rejection, and interference mitigation capability. Intuitively, many of these algorithms can be a good fit to generate RAD, and serve as the training signal to train the trunk of the neural network.

As such, the proposed pre-training method consists of two main steps:
\begin{enumerate}
    \item Using an SP algorithm, generating RAD from all the ADC data;
    \item Using the collected (ADC, RAD) pairs as input and (pseudo) labels to train the trunk network in a supervised learning fashion.
\end{enumerate}
Unlike unsupervised pre-training techniques, which usually ask the neural network to predict missing parts of the signal (\eg predictive coding \cite{oord2018representation} in the audio domain), or to differentiate ``real'' and ``fake'' samples as in the contrastive learning regime,
the proposed method is designed to learn from an \textit{algorithm}. That is, we task a neural network to learn the mapping between ADC and RAD, where the RAD is generated by a classical SP algorithm.

Specifically, using the RADIal SDK, we generate RAD map for all the available ADC data. As such, we get a collection of (ADC, RAD) pairs. We use these pairs to pre-train the SP module and backbone network, with the following loss
\begin{align}
    \mathcal{L} = \text{smooth-L1}(\mathbf{Y}_{\text{RAD}} -  \hat{\mathbf{Y}}_{\text{RAD}}),
\end{align}
where $\mathbf{Y}_{\text{RAD}}$ denotes the generated RAD tensor and $\hat{\mathbf{Y}}_{\text{RAD}}$ denotes the prediction of the neural network. From the RADIal SDK, the generated RAD tensor is of shape $512 \times 751 \times 256$. We downsample the tensor to have shape $128 \times 248 \times 256$ for easier training. 
After pre-training the trunk of the ADCNet, we proceed to fine-tune the network with the multi-task setting as describe in Section~\ref{sec:supervised}.



\begin{table*}[!htp]\centering
\caption{Comparing ADCNet with baselines. AP: Average Precision, AR: Average Recall, RE: Range Error on detected objects, AE: Azimuth Error on detected objects. ADCNet-PTFT: ADCNet with pre-training and fine-tuning. The full testset is broken down into an Easy and a Hard subset for detailed comparison. *: results cited from \cite{rebut2022raw}.}\label{tab:main_res}
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrr|rrrrr|rrrrrr}\toprule
\textbf{} &\textbf{} &\multicolumn{5}{c|}{\textbf{All}} &\multicolumn{5}{c|}{\textbf{Easy}} &\multicolumn{5}{c}{\textbf{Hard}} \\\cmidrule{3-17}
\textbf{} &\textbf{Radar Input} &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE (m)} &\textbf{AE (°)} &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE (m)} &\textbf{AE (°)} &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE (m)} &\textbf{AE (°)} \\\midrule
FFT-RadNet * &RD &0.89 &0.97 &0.82 &0.11 &0.17 &0.95 &0.98 &0.92 &0.10 &0.13 &0.76 &0.93 &0.65 &0.13 &0.26 \\
Pixor - PC * &Point cloud &0.48 &0.96 &0.32 &0.17 &0.25 &0.45 &0.99 &0.29 &0.15 &0.19 &0.55 &0.93 &0.39 &0.19 &0.33 \\
Pixor - RA * &RA &0.89 &0.97 &0.82 &0.10 &0.20 &0.92 &0.97 &0.88 &0.09 &0.16 &\textbf{0.81} &0.96 &0.70 &0.12 &0.27 \\
FFT-RadNet-optimized &RD &0.90 &0.92 &0.89 &0.15 &0.11 &0.96 &0.95 &0.97 &0.14 &0.10 &0.80 &0.87 &0.74 &0.17 &0.13 \\
ADCNet &ADC &0.91 &0.96 &0.87 &0.12 &0.10 &\textbf{0.97} &0.98 &0.97 &0.11 &0.10 &0.78 &0.91 &0.68 &0.16 &0.12 \\
ADCNet-PTFT &ADC &\textbf{0.92} &0.95 &0.89 &0.13 &0.11 &\textbf{0.97} &0.96 &0.98 &0.12 &0.11 &\textbf{0.81} &0.91 &0.73 &0.16 &0.12 \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table}[!htp]\centering
\caption{Freespace segmentation comparison. The performance is measured by mIOU, the higher the value the better. *: results cited from \cite{rebut2022raw}.}\label{tab:freespace}
\scriptsize
\begin{tabular}{lrrrr}\toprule
&All &Easy &Hard \\\midrule
FFT-RadNet * &74.00\% &74.60\% &72.30\% \\
PolarNet \cite{nowruzi2021polarnet} * & 60.6\%& 61.9\% & 57.4\% \\
FFT-RadNet-optimized &76.74\% &77.83\% &73.96\% \\
ADCNet &74.45\% &75.85\% &70.89\% \\
ADCNet-PTFT &\textbf{78.59}\% &\textbf{79.63}\% &\textbf{75.90}\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Experiment results}
In this section, we detail experiment results to verify the effectiveness of the proposed techniques. We present an overall comparison in Section~\ref{sec:exp_overall}, while design and present ablation studies in the following subsections. 
\subsection{Baseline methods}
For the proposed method, the most relevant baseline to compare against is FFT-RadNet. This is because the FFT-RadNet method is specifically designed for the RADIal dataset: a MIMO pre-encoder module (a dilated convolution layer) is designed to fully exploit the Doppler division multiplexing (DDM) scheme used on this radar, while other methods such as those in \cite{meyer2021graph, hwang2022cramnet,  wang2021rethinking, li2022modality, qian2021robust, li2022exploiting, palffy2020cnn} are designed for other types of radar data, such as radar intensity map, radar radio frequency image, etc., so they are unlikely to be competitive on the RADIal dataset without major changes.
Futhermore, to highlight the effects of our novel techniques, the proposed ADCNet is following FFT-RadNet, with a novel learnable SP module incorporated into the neural network. We emphasize that the proposed techniques can be easily combined with other type of backbone and head networks, while the ADCNet present in this work is intentionally kept simple and similar to FFT-RadNet, for the purpose of comparison.

The Pixor \cite{yang2018pixor} method is also included for comparison as the detection head used in Section~\ref{sec:supervised} is inspired from it. For this method, two different radar data representations are used: point cloud and RA map. For the point cloud-based method, the point cloud are voxelized into  a 3D volume before feeding into the network; for the RA-based method, the RA input is formed by collapsing (summing over) the Doppler dimension of the RAD tensor.

In order to ensure a fair comparison, we first optimize the hyper-parameters for FFT-RadNet. Surprisingly, we get much improved performance for FFT-RadNet with a different batch-size and learning-rate than the original paper (see Table~\ref{tab:main_res}). We thus use our own tuned FFT-RadNet as the baseline.

\subsection{Experiment setup}
We use the same data partitioning as that of \cite{rebut2022raw}. The whole labelled dataset is split into train, val, and test set by \textit{sequence} (driving session). That is, data samples from a sequence can only appear in one of the splits. This is to avoid allocating closely-resembling data samples (\eg two consecutive data samples in the same driving session) to the same split, which can lead to inflated performance metrics.

For evaluation, we also follow the setup as in \cite{rebut2022raw}. Object detection performance is measured by average-precision, average-recall and average-f1 scores, where average is done across a range of detection thresholds. The freespace segmentation results are evaluated with mIOU metric: the average IOU (intersection over union) score across test samples.

For the pre-training experiments in Section~\ref{sec:pretraining_exp}, it is important to note that we make sure any samples from the labeled test set sequences are not included in the pre-training stage. This avoids giving unfair advantages to the ADCNet-PTFT, so that it is not allowed to see test samples even in the pre-training stage.

\subsection{Comparing ADCNet to baselines}\label{sec:exp_overall}

In this section we compare the ADCNet to baselines. We consider two variant of ADCNet -- with and without pre-training. To differentiate these, we denote the method with pre-training as ADCNet-PTFT, where PTFT stands for pre-training \& fine-tuning.

The object detection performance results are shown in Table~\ref{tab:main_res}, and the freespace segmentation results are presented in Table~\ref{tab:freespace}. Overall, we can see that the ADCNet-PTFT method out-performs FFT-RadNet for both tasks. In particular, the ADCNet-PTFT method achieves about 2 percentage higher freespace segmentation mIOU, while at the same time outperforming the well tuned FFT-RadNet-optimized method on the detection task.
The benefit of pre-training is apparent when comparing ADCNet and ADCNet-PTFT. For example, compared with ADCNet, ADCNet-PTFT achieves about 4 points improvement on the freespace segmentation task on the labeled test set.



We present several model prediction samples in Figure~\ref{fig:adcnet_samples}. It can be seen from the samples that the network is able to correctly recognize the cars while rejecting other bright spots in the RA images. In the second sample, the network misses one car, possibly due to the road sign on the right, which causes a bright spot in the RA map, at a similar range of the missed car.

\subsection{RAD prediction accuracy in pre-training}\label{sec:pretraining_exp}
In Table~\ref{tab:main_res} and Table~\ref{tab:freespace}, the effect of pre-training is evident by comparing ADCNet and ADCNet-PTFT. In this section, we are interested in examining the prediction accuracy of RAD tensor in the pre-training stage. This is a valid question, as both the ADC tensor and the RAD are large: ADC is of shape $512 \times 256 \times 8$ while RAD is $128\times 248 \times 256$ for the RADIal dataset. It is unclear whether learning a mapping between these two high-dimensional vectors is feasible.

While the results from Table~\ref{tab:main_res} and Table~\ref{tab:freespace} hint that pre-training is successful, here we observed that the lowest validation loss (calculated on 2014 samples) during training is about 1.6. This is a very small error, as the ground truth (power spectrum) values of RAD is distributed around 60.

Furthermore, we visualize several RAD prediction samples in Figure~\ref{fig:rad}. To visualize this 3D RAD tensor, we summed over the third dimension, and get a 2D image of dimension range and azimuth.
To get a quantitative error measure, we compute the Relative Absolute Error (RAE) for each RAD entry as 
\begin{align}
    \text{RAE}(i, j, k) = \frac{|\mathbf{Y}_{\text{RAD}}(i, j, k)  - \hat{\mathbf{Y}}_{\text{RAD}}(i, j, k) |}{|\mathbf{Y}_{\text{RAD}}(i, j, k) |}.
\end{align}
As such, we get a relative error measure for each entry, and we report the maximum and mean of these errors in Figure~\ref{fig:rad}.


\begin{figure}[bt]
    \centering
    % \fbox{\rule{0pt}{4in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=\linewidth]{figs/adc_rad_pred_err_4.png}
    % \includegraphics{}
    \caption{RAD prediction examples. The mean and max of Relative Absolute Error (RAE) for each sample are reported. As can be seen these examples, after pre-training, the network is able to predict RAD tensors to a very high accuracy.}
    \label{fig:rad}
\end{figure}

\begin{table*}[ht]\centering
\caption{Comparing Exact-DFT and Perturbed-DFT initialization method for the object detection task}\label{tab:perturbation_detection}
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrrr|rrrrr|rrrrrr}\toprule
\textbf{} &\textbf{} &\multicolumn{5}{c|}{\textbf{All}} &\multicolumn{5}{c|}{\textbf{Easy}} &\multicolumn{5}{c}{\textbf{Hard}} \\\cmidrule{3-17}
\textbf{} &\textbf{Initialization} &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE (m)} &\textbf{AE (°)} &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE (m)} &\textbf{AE (°)} &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE (m)} &\textbf{AE (°)} \\\midrule
ADCNet &Exact-DFT &0.90 &0.96 &0.84 &0.13 &\textbf{0.10} &0.95 &0.98 &0.93 &0.12 &\textbf{0.09} &0.77 &0.91 &0.67 &\textbf{0.15} &0.12 \\
ADCNet &Perturbed-DFT &0.91 &0.96 &0.87 &\textbf{0.12} &\textbf{0.10} &\textbf{0.97} &0.98 &0.97 &\textbf{0.11} &0.10 &0.78 &0.91 &0.68 &0.16 &0.12 \\
ADCNet-PTFT &Exact-DFT &0.89 &0.93 &0.86 &0.13 &\textbf{0.10} &0.96 &0.95 &0.97 &0.12 &0.10 &0.75 &0.86 &0.67 &\textbf{0.15} &0.12 \\
ADCNet-PTFT &Perturbed-DFT &\textbf{0.92} &0.95 &0.89 &0.13 &0.11 &\textbf{0.97} &0.96 &0.98 &0.12 &0.11 &\textbf{0.81} &0.91 &0.73 &0.16 &0.12 \\
\bottomrule
\end{tabular}
}
\end{table*}

It can be seen from Figure~\ref{fig:rad} that the network can predict the RAD tensor to a very high accuracy: the ground truth and predicted images are visually close, and the RAE measures confirm this observation.

\subsection{Different methods for SP module initialization}\label{sec:init_exp}
\begin{table}[!htp]\centering
\caption{Comparing different initialization methods for ADCNet. The numbers represent the performance on the full test set.}\label{tab:init}
\scriptsize
\begin{tabular}{lrrrrrrr}\toprule
&\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE(m)} &\textbf{AE (°)} &\textbf{mIOU} \\\midrule
Exact-DFT &0.87 &0.91 &0.84 &\textbf{0.12} &0.10 &\textbf{75.60\%} \\
Random-Doppler &0.82 &0.91 &0.74 &0.16 &0.10 &74.43\% \\
Perturbed-DFT &\textbf{0.91} &0.96 &0.87 &\textbf{0.12} &0.10 &74.50\% \\
Random &\multicolumn{6}{c}{Failed to converge} \\
\bottomrule
\end{tabular}
\end{table}
In this section, we present an ablation study regarding the initialization method for the learnable SP module. In particular, we compare these methods:
\begin{itemize}
    \item Perturb-DFT initialization: perturbed DFT matrices as shown in Equation~\ref{eq:perturb} are used to initialize both the range and Doppler DFT module in Figure~\ref{fig:adcnet}
    \item Exact-DFT initialization: the exact DFT matrices are used to initialize both range and Doppler DFT modules
    \item Random-Doppler initialization: the exact DFT matrix is used to initialize the range DFT, while a randomly generated matrix is used for Doppler 
    \item Random: randomly generated matrices are used for both range and Doppler modules
\end{itemize}
These methods employ a various amount of SP knowledge: the {Exact-DFT} method relies fully on SP, and the {Random} method almost completely disregard SP, while {Perturbed-DFT }and {Random-Doppler} lie in between.

The results are shown in Table~\ref{tab:init}. As can be seen, the {Perturbed-DFT } yields the best results. As we suspected, using the {Exact-DFT} initialization method can lead to sub-optimal performance. We hypothesize that this is because the network gets stuck in a local minima when initialized exactly. The {Random-Doppler} method yields considerably worse performance, while the {Random} method failed to converge to any meaningful model.



Since the Exact-DFT and Perburbed-DFT initialization methods get better performance than other choices, we provide a more in-depth comparison between the two. Moreover, we implement these initialization methods in the pre-training stage, then fine-tune to get the final detection and segmentation results. For this experiment, the multi-task learning setup as described in Section~\ref{sec:supervised} is performed, with and without pre-training. All the hyper-parameters are kept the same, and only the initialization method is varied.

The results are reported in Table~\ref{tab:perturbation_detection} and Table~\ref{tab:perburbed_freespace}. As can be seen from these tables, with and without pre-training, the Perturbed-DFT initialization method always yields better performance than Exact-DFT. The improvement by using Perturbed-DFT is especially visible for ADCNet, confirming our intuition that a larger dataset is needed for end-to-end learning, as limited training data may not be able to nudge the network too far from the SP-based initialization.

\begin{table}[ht]\centering
\caption{Comparing Exact-DFT and Perturbed-DFT initialization method for the freespace segmentation task}\label{tab:perburbed_freespace}
\scriptsize
\begin{tabular}{lrrrrr}\toprule
& Initialization &\textbf{All} &\textbf{Easy} &\textbf{Hard} \\\midrule
ADCNet & Exact-DFT &70.39\% &70.81\% &69.31\% \\
ADCNet & Perburbed-DFT &74.45\% &75.85\% &70.89\% \\
\midrule
ADCNet-PTFT & Exact-DFT &77.28\% &78.37\% &74.50\% \\
ADCNet-PTFT & Perturbed-DFT & 78.59\% &79.63\% &75.90\% \\
\bottomrule
\end{tabular}
\end{table}

As discussed in Section~\ref{sec:trainable_dsp}, the $\gamma$ parameter in \ref{eq:perturb} should be chosen with care: a large $\gamma$ can destroy the DFT structure, while a too small $\gamma$ can leave the SP module in a non-optimal local minimum. Here we present an experiment, where different $\gamma$ is applied in the Perturb-DFT initialization scheme, and an ADCNet is trained (without pre-training) on the labeled RADIal dataset using the supervised multi-task learning setup in Section~\ref{sec:supervised}.
Table~\ref{tab:gamma} shows the results:  a large $\gamma$ (\eg $\gamma=2$) brings significant degradation to both object detection and freespace segmentation, while a small $\gamma$ like 0.1 achieves better performance than if no perturbation ($\gamma=0$) is applied.

\begin{table}[!htp]\centering
\caption{The effect of the $\gamma$ parameter in Perturb-DFT initialization}\label{tab:gamma}
\scriptsize
\begin{tabular}{rrrrrrrr}\toprule
$\gamma$ &\textbf{F1} &\textbf{AP} &\textbf{AR} &\textbf{RE(m)} &\textbf{AE (°)} &\textbf{mIOU} \\\midrule
0 &0.90 &0.96 &0.84 &0.13 &\textbf{0.1} &70.39\% \\
0.1 &\textbf{0.91} &0.96 &0.87 &\textbf{0.12} &\textbf{0.1} &\textbf{74.45\%} \\
0.5 &0.84 &0.88 &0.81 &0.16 &0.13 &70.03\% \\
2 &0.65 &0.75 &0.57 &0.18 &0.12 &69.47\% \\
\bottomrule
\end{tabular}
\end{table}


\section{Conclusion}
We presented an end-to-end learning based model on raw radar ADC data. To overcome the difficulty associated with learning from the ADC data, we designed a learnable signal processing module with an improved initialization scheme as well as an effective pre-training strategy. The combination of these techniques results in improved object detection and freespace segmentation results on the RADIal dataset within a unified multi-task model.

The designed learnable SP sub-network is modular, such that it can be used with other backbone or head networks to tackle other AV perception tasks. In the same vein, the pre-training technique is generic and can be adapted for other radar-based perception tasks as well as other RAD estimation techniques. We showcased the merit of the proposed techniques, and we expect these techniques to be more powerful in more challenging environments (\eg dense urban roads), or when using more unlabeled data for pre-training. We leave these explorations for future works. 
%------------------------------------------------------------------------


% When placing figures in \LaTeX, it's almost always best to use
% \verb+\includegraphics+, and to specify the  figure width as a multiple of
% the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.eps}
% \end{verbatim}
% }

%-------------------------------------------------------------------------


%------------------------------------------------------------------------

\clearpage
\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
