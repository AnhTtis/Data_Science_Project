\documentclass{article}

\usepackage{arxiv}
\usepackage{xcolor}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[english]{babel}

\usepackage{amsthm, amsmath}


\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
%\usepackage{natbib}

\usepackage{comment}
%\usepackage[dvipsnames]{xcolor}
%\usepackage{subcaption}
\usepackage{float}
\usepackage{url}

% call the following before enumitem with ieee latex
\let\labelindent\relax
\usepackage{enumitem}



% Custom Commands
\newcommand{\xt}{\tilde{x}}
\newcommand{\zh}{\hat{z}}
\newcommand{\zb}{\bar{z}}
\newcommand{\zht}[1]{\hat{z}_{#1}}
\newcommand{\zbt}[1]{\bar{z}_{#1}}
\newcommand{\At}{\tilde{A}}
\newcommand{\Bt}{\tilde{B}}
\newcommand{\Qt}{\tilde{Q}}
\newcommand{\xtp}[2]{x_{#1}^{\pi_{#2}}}
\newcommand{\utp}[2]{u_{#1}^{\pi_{#2}}}
\newcommand{\xht}[1]{\hat{x}_{#1}}
\newcommand{\yht}[1]{\hat{y}_{#1}}
\newcommand{\uogdt}[1]{u^{\mathrm{ogd}}_{#1}}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}{Assumption}


\usepackage{doi}
\usepackage{amsmath, amssymb}
\usepackage{mathtools}
\usepackage{commath}
\newcommand{\tas}[1]{{\color{cyan}{#1}}}
\title{Online Linear Quadratic Tracking with Regret Guarantees
\thanks{
This work has been supported by the Swiss National Science Foundation under NCCR Automation (grant agreement $51\text{NF}40\_180545$), and by the  European Research Council under the ERC Advanced grant agreement  $787845$ (OCAL).\\
Diego Bolliger and Aren Karapetyan have equal contribution.
} 
}
%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it
\author{
  Aren Karapetyan, Diego Bolliger, Anastasios Tsiamis, Efe C. Balta, John Lygeros \\ \\
  Automatic Control Laboratory, \\
  ETH Z\"urich, \\
  Z\"urich, Switzerland
}
%\notice{This work has been supported by the Swiss National Science Foundation under NCCR Automation (grant agreement $51\text{NF}40\_180545$), and by the  European Research Council under the ERC Advanced grant agreement  $787845$ (OCAL).}
%\notice{The authors are with the Automatic Control Laboratory, Department of Information Technology and Electrical Engineering, ETH Z{\"u}rich, 8092 Z{\"u}rich, Switzerland (e-mail: \{akarapetyan, atsiamis, ebalta, jlygeros\}@control.ee.ethz.ch, diegobo@student.ethz.ch). Diego Bolliger and Aren Karapetyan have equal contribution.}

% Uncomment to remove the date
\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
%\hypersetup{
%pdftitle={A template for the arxiv style},
%pdfsubject={q-bio.NC, q-bio.QM},
%pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
%pdfkeywords={First keyword, Second keyword, More},
%}

\begin{document}
\maketitle

\begin{abstract}
    Online learning algorithms for dynamical systems provide finite time guarantees for control in the presence of sequentially revealed cost functions. We pose the classical linear quadratic tracking problem in the framework of online optimization where the time-varying reference state is unknown \textit{a priori} and is revealed after the applied control input. We show the equivalence of this problem to the control of linear systems subject to adversarial disturbances and propose a novel online gradient descent based algorithm to achieve efficient tracking in finite time. We provide a dynamic regret upper bound scaling linearly with the path length of the reference trajectory and a numerical example to corroborate the theoretical guarantees. 
\end{abstract}
% keywords can be removed
\keywords{Optimal Tracking \and Online Control}

\section{Introduction}
\label{sec:introduction}

Linear quadratic tracking (LQT) is the natural generalization of the optimal linear quadratic regulator (LQR) for the setting where the goal is not to drive the state to the origin but to a certain reference. The reference trajectory need not be necessarily time-invariant and in the classic formulation of the problem is known in advance. This is a reasonable assumption in many practical applications, such as aircraft tracking of a predetermined trajectory or precision control in industrial process engineering. However, in other scenarios, for example, in tracking the output of a secondary agent whose dynamics are unknown and/or the measurements are imperfect, the prediction of the next reference point is non-trivial. In these cases the reference trajectory is only revealed sequentially, after the action has been taken, suggesting the need for an online or adaptive algorithm that will learn or adapt to the dynamics of the reference generating agent.

In this letter, we study the LQT problem with an unknown reference trajectory. We pose the problem in the framework of online convex optimization (OCO) subject to the dynamics constraint of the system \cite{hazan2020nonstochastic} . In particular, the tracking problem is recast into an equivalent regulation problem with a redefined state that evolves with linear dynamics subject to additive adversarial disturbances. In the spirit of OCO, we show why classical online gradient descent (OGD) may fail to achieve optimal tracking and propose a modified algorithm, called SS-OGD (steady state OGD) that is guaranteed to achieve the goal under mild conditions. Given the online nature of the algorithm, its performance is quantified through the means of dynamic regret that compares the accumulated finite time cost of a given algorithm to that of an optimal benchmark that solves the LQT problem with an \emph{a priori} knowledge of the reference trajectory. We provide dynamic regret bounds in terms of the path length of the reference trajectory.

The LQT problem for sequentially revealed adversarial reference states is studied in \cite{abbasi2014tracking}  with policy regret bounds. In cases where a window of predictions is available, a receding horizon gradient descent algorithm is suggested in \cite{li2019online} with a dynamic regret analysis. In a more recent line of work \cite{nonhoff2020online}, the authors introduce a memory based, gradient descent algorithm and in \cite{nonhoff2022online}, tackle the constrained tracking problem with policy regret guarantees. In\cite{balta2022regret}, the authors analyze the output tracking scheme of an iterative learning controller and provide dynamic and static regret bounds. A lower bound on the regret of the online LQT problem is given in \cite{li2019online} with the reference path length as the complexity term. We pose the tracking problem in the framework of nonstochastic control subject to adversarial disturbances, studied in \cite{hazan2020nonstochastic}, with logarithmic policy regret bounds achieved in \cite{foster2020logarithmic}.

In Section \ref{sec:problem_statement}, we formally define the LQT problem and recast it into a regulation problem for linear systems subject to adversarial disturbances. In Section \ref{sec:ss_ogd_algorithm}, we present the SS-OGD algorithm and provide dynamic regret bounds in Section \ref{sec:regret_analysis}. The performance of SS-OGD is compared to that of a certainty equivalence (CE) controller that assumes a constant reference, by applying both to the tracking control problem of a quadrotor model in Section \ref{sec:numerical_examples}.

\textit{Notation}: The set of positive real numbers is denoted by $\mathbb{R}_+$  and  that of non-negative integers by $\mathbb{N}$.  For a matrix $W$ the spectral radius and the spectral norm are denoted by $\rho(W)$, and  $\|W\|$, respectively, and $\lambda_{min}(W)$ denotes its lowest eigenvalue. We define $\lambda_W := \frac{1+\rho(W)}{2}$; one can show that if $\rho(W)<1$, there exists a $c_W \in \mathbb{R}_+$ such that for all $k\geq 1$ $\|W^k\|\leq c_W{\lambda_W}^k$. For a given vector $x$, its  Euclidean norm is denoted by $\|x\|$, and the one weighted by some matrix $Q$  by $\|x\|_Q = \sqrt{x^{\top}Qx}$.  
\section{Problem Statement}
\label{sec:problem_statement}

Consider the discrete-time linear time-invariant (LTI) dynamical system, given by
\begin{equation}
    \label{eq:lti_system}
    x_{t+1} = Ax_t + Bu_t, \quad \forall t \in \mathbb{N}, 
\end{equation}
where $x_t \in \mathbb{R}^n$ and $u_t \in \mathbb{R}^m$ are the state and input vectors respectively, and $A\in \mathbb{R}^{n\times n}, B \in \mathbb{R}^{n\times m}$ are \emph{known} system matrices. The goal of the optimal LQT problem is the tracking of a time-varying signal $r_t \in \mathbb{R}^n$, such that the cost
\begin{equation*}
    \|x_T-r_T\|_{P}^2+\sum_{t=0}^{T-1} \|x_t-r_t\|_Q^2 + \|u_t\|_R^2
\end{equation*}
is minimized for some weighting matrices $Q \in \mathbb{R}^{n\times n}$ and $R\in \mathbb{R}^{m\times m}$, and where  $P\in \mathbb{R}^{n\times n}$ is the solution of the discrete algebraic Riccati equation (DARE)\footnote{Given that the dynamics information and the cost matrices are known \textit{a priori} we consider the final cost matrix to be $P$ for notational simplicity. For other values of the terminal cost matrix the results still hold, thanks to the fact of exponentially converging state feedback matrices.}
\begin{equation}
     P = Q + A^{\top}PA - A^{\top}PB(R+B^{\top}P B)^{-1}B^{\top}PA.
     \label{eq:riccati}
 \end{equation}


The LQT problem can also be recast into an equivalent LQR formulation \cite{karapetyan2022implications} by considering instead the dynamics
\begin{equation}
    \label{eq:lti_system_noise}
    e_{t+1} = A e_t +B u_t + w_t, \quad \forall t \in \mathbb{N},
\end{equation}
with $e_t := x_t - r_t$ and $w_t := Ar_t - r_{t+1}$ for all $t\in\mathbb{N}$, and the corresponding cost function
\begin{equation}
    \label{eq:cost}
J(e_0,u) := \|e_T\|_{P}^2+\sum_{t=0}^{T-1} \|e_t \|_Q^2 + \|u_t\|_R^2.
\end{equation}
When the reference trajectory $r_t$, $t\in\mathbb{N}$ is known at the initial time a closed form solution for the optimal controller that solves the following optimization problem can be obtained
\begin{subequations}
\label{eq:optimal_offline_benchmark}
\begin{align}
   u^{\star}= &\argmin_u \quad J(e_0,u)\\
     &\text{subject to} \quad \eqref{eq:lti_system_noise} \quad \forall~ 0\leq t<T.
\end{align}
\end{subequations}
This controller, often referred to as the optimal offline noncausal controller, can be represented as  a linear feedback on the current state and the future reference \cite{lewis2012optimal, goel2022power}.

Departing from the classical formulation of tracking control, we assume that the reference signal is \emph{unknown} and is only revealed sequentially after the control input has been applied, similar to the adversarial tracking framework in \cite{abbasi2014tracking}. In particular, for each time step $0\leq t<T$:
\begin{enumerate}[label=\arabic*:]
    \item The state $x_t$ and the reference state $r_t$ are observed,
    \item The agent decides on an input $u_t$,
    \item The environment decides on the next reference $r_{t+1}$, which, in turn, determines $w_t$. The error state then evolves according to \eqref{eq:lti_system_noise}, incurring the following cost for the agent
    \begin{equation}
    \label{eq:online_objective}
    c_t(e_t,u_t):= \|Ae_t + Bu_t +w_t\|_Q^2 + \|u_t\|^2_R.
\end{equation}
\end{enumerate}

Note that the online cost~\eqref{eq:online_objective}, depends on the current input $u_t$ and the unknown disturbance $w_t$, and is therefore unknown to the decision maker at timestep $t$; it is revealed only at time $t+1$, after the input $u_t$ has been applied to the system. From this point of view, our problem formulation fits the online learning framework, with the extra challenge of inherent dynamics. The goal of the controller is, then, to minimize the online cumulative cost \footnote{For consistency we require $c_{T-1}(e_{T-1},u_{T-1}):= \|Ae_{T-1} + Bu_{T-1} +w_{T-1}\|_{P}^2 + \|u_{T-1}\|^2_R$. To forego unnecessary cluttering of the notation, the separate treatment of the last timestep is implied implicitly.}
\begin{equation*}
    \sum_{t=0}^{T-1} c_t(e_t, u_t)=J(e_0,u)-\|e_0\|^2_Q.
\end{equation*}
This is the same as the LQR cost without the initial state, implying that the minimizers of both problems coincide.
 
We quantify the finite-time performance of the algorithm through the means of dynamic regret. Consider a policy $\pi:\mathcal{I}\rightarrow\mathbb{R}^m$, mapping from the available information set, $\mathcal{I}$, to the control input space. Its  dynamic regret, given a disturbance signal $w$,  is defined as
\begin{equation}
\label{eq:dynamic_regret}
    \mathcal{R}^{\pi}(w,e_0) = J(e_0,u^{\pi}) - J(e_0,u^{\star}),
\end{equation}
where $u^{\pi}$ is the input generated by $\pi$ and $u^{\star}$ is given by \eqref{eq:optimal_offline_benchmark}.

We allow the trajectory $r_t, \; t\in\mathbb{N}$ to be arbitrary, as long as it remains bounded. \begin{assumption}[Bounded trajectory]
    \label{ass:boundedness}
     There exist $\bar{R} \in \mathbb{R}_+$, such that  $\|r_t\|\leq \bar{R}$ for all $t\in \mathbb{N}$.
\end{assumption}

The more abruptly a trajectory changes, the harder it is to achieve good tracking performance, especially if the trajectory is unknown beforehand. To capture  this inherent complexity of the problem, we use the well established notion of path length \cite{li2019online}. 
\begin{definition}[Path Length]
The path length of a reference trajectory $r_{0:T}\in \mathbb{R}^{n(T+1)}$ is 
\begin{equation*}
    L(T) = \sum_{t=0}^{T-1}\|\Delta r_t\|,
\end{equation*}
where $\Delta r_t = r_{t+1}-r_t$.
\end{definition}
For more random and abrupt changes in the trajectory, the path length is higher, and one expects the  performance of an online algorithm to deteriorate. Likewise, an efficient algorithm should improve as path length decreases. This is  captured quantitatively by showing at least a linear dependence of the algorithm's regret on the path length.

Finally, we make the following standard assumptions for the LQR problem to be well-posed.
\begin{assumption}[LQR is well-posed]
\label{ass:standard}
The system $(A,B)$ is stabilisable, the pair $(Q^{\frac{1}{2}},A)$ is detectable and $R\succ 0$. 
\end{assumption}


\section{The SS-OGD Algorithm}
\label{sec:ss_ogd_algorithm}
We consider a control law of the following form
\begin{equation}
    \label{eq:ogd_input}
    u_t = -Ke_t + v_t, \quad \forall 0\leq t<T,
\end{equation}
where $K = (R+B^{\top}PB)^{-1}B^{\top}PA$ is fixed to the optimal LQR gain, and $v_t$ is a correction term that should account for the unknown disturbances; we will employ online learning techniques to update the latter term. 

We investigate the performance of online gradient descent  based algorithms. Consider the following ``naive" update
\begin{equation}
\label{eq:ogd_update}
    v_{t} = v_{t-1} - \alpha \nabla_{v}c_{t-1}(e_{t-1}, u_{t-1}), 
\end{equation}

where  $v_t$ is updated in the opposite direction of the gradient of the most recent cost. Here $\alpha\in R_+$ is the step size and the recursion starts from some $v_0 \in \mathbb{R}^m$. As the online objective is quadratic, the gradient is available in a closed form and the update can be represented as
$v_{t} = v_{t-1} - 2 \alpha (Ru_{t-1} + B^\top Q e_t)$. For the case of a constant reference signal and an underactuated system, the algorithm can converge to a point that is not necessarily the optimal one with respect to infinite horizon cost minimization. This is due to the greedy behavior of the update that does not take into account future dynamics. In this section, we propose a simple modification to this myopic OGD update \eqref{eq:ogd_update}, called SS-OGD that accounts for this shortcoming.



To motivate the SS-OGD update, we consider the steady state solution of \eqref{eq:lti_system_noise} in closed loop with the affine control law \eqref{eq:ogd_input} when we fix $v_t = \bar{v}$ and $r_t = \bar{r}$ for all $i\geq t$. Defining $S := (I-A+BK)^{-1} B$, a closed form solution for the steady state and input is given by~\footnote{Note that $\bar{x}$ and $\bar{u}$ are both defined for a given $\bar{v}$ and $\bar{r}$. The dependence is left for simplicity}
\begin{align*}
     \bar{x} &= S\bar{v}+SK\bar{r} \\
	\bar{u} &= (I-KS)(\bar{v}+K\bar{r}).
\end{align*}
One can then find the $\Bar{v}$ which will recover the optimal steady state solution by minimizing the time-averaged infinite horizon steady state cost. This is equivalent to minimizing  
\begin{equation}
    \label{eq:steady_state_program}
    \argmin_{\bar{v}}  \{c(\Bar{x}-\bar{r},\Bar{u}) = \|\bar{x}-\bar{r}\|_Q^2 + \|\bar{u}\|_R^2\},
\end{equation}
whose gradient is given by 
\begin{equation}
   \nabla_{\bar{v}} c(\Bar{x}\!-\bar{r},\Bar{u})\!=\! 2 \left((I-KS)^\top R \Bar{u}\! +\! S^\top Q (\Bar{x} \!- \bar{r}) \right).
    \label{eq:ss_OGD_ss_solution}
\end{equation}

\begin{comment}
\eb{No need for this equation probably.}
\begin{equation}
\begin{split}
	&(S^T Q S + (I-KS)^T R (I-KS)) \bar{v} \\
 &= \left( S^T Q - S^T Q S K - (I-K S)^T R (I-K S) K \right)  r
	\label{eq:ss_OGD_ss_solution}
\end{split}
\end{equation}
\end{comment}

\begin{lemma}
\label{lem:strong_convexity}
Under Assumption \ref{ass:standard}, \eqref{eq:steady_state_program} is strictly convex for any $K\in \mathbb{R}^{m\times n}$, for which $\rho(A-BK)<1$.
\end{lemma}

\begin{proof}
If the matrix $I-KS$ is singular, there exists a $v\in\mathbb{R}^n$, such that $v=KSv$. Then, for $x = Sv$, at steady state $x = Ax +B(KSv-Kx) = Ax$. Given the detectability condition of the pair $(Q^{\frac{1}{2}},A)$, for any unstable, or marginally stable mode of $A$, the matrix $Q\succ 0$. This ensures that the matrix $S^\top QS+(I-KS)^\top R(I-KS)$ is positive definite, which is equivalent to the strong convexity of \eqref{eq:steady_state_program} .  
\end{proof}
\begin{comment}
\begin{remark}
    It can be shown that when $K$ is the optimal Kalman gain, then $S^T Q S + (I-KS)^T R (I-KS) = R+B^{\top}PB$
\end{remark}
\end{comment}

Since $r$ is, in general, not constant, we suggest a new OGD-like update rule on the bias term $v_t$ that is a modified version of the gradient in~\eqref{eq:ss_OGD_ss_solution}. Specifically, the feedback on the steady state error, $\bar{x}-\bar{r}$, is replaced with the measured error, $x_t-r_t$, and  the steady state input, $\Bar{u}$, with the latest applied input, $u_{t-1}$. This results in the following update, named SS-OGD
\begin{equation}
    \label{eq:ss_ogd_update}
    v_t = v_{t-1} - 2\alpha\left((I-KS)^{\top} R u_{t-1}+ S^\top Qe_t\right).
\end{equation}

 The modifications from the standard OGD can be interpreted as incorporating the dynamics information in the update rule. As we show in the following, this ensures that in the limit, if the algorithm is stable and the reference signal is constant, the SS-OGD converges to the same point as the solution of the LQR problem minimizing \eqref{eq:cost}. Moreover, through the feedback on the state $e_t$ and input $u_{t-1}$, the update rule \eqref{eq:ss_ogd_update} incorporates a proportional integral (\emph{PI}) control on the measured state. This is demonstrated on a quadrotor control example in Section \ref{sec:numerical_examples}, where, with the inherent integrator dynamics of the quadrotor, the SS-OGD achieves a zero steady state error in tracking a position reference signal with a constant rate of change.
 
 To study the SS-OGD update rule, we introduce the following  evolution of the combined system optimizer dynamics
 \begin{equation}
    \label{eq:combined_system_dynamics}
    z_{t+1} = \Tilde{A}z_t + \Tilde{B}w_t, 
\end{equation}
 where $z_t := [v_t^{\top}~e_t^{\top}]^{\top}$, the matrices $\Tilde{A}\in\mathbb{R}^{p \times p}$ and $\Tilde{B}\in \mathbb{R}^{p \times n}$ are defined in Appendix A and $p:=m+n$. 

 
 \begin{assumption}\label{ass:alpha}
    The step size  $\alpha>0$ is such that $\rho(\Tilde{A}) < 1$ .
 \end{assumption}

Since all the variables in $\Tilde{A}$ are known \emph{a priori}, we show that there always exists an $\alpha$ satisfying this assumption and provide a sufficient condition in Appendix A.
 
The following theorem shows that, for a constant  $w_t = \bar{w}$ for all $0\leq t<T$, SS-OGD update \eqref{eq:ss_ogd_update} converges to the solution of 
    \begin{subequations}
        \label{eq:step_wise_ss_min_prob}
        \begin{align}
            (\hat{e}_t, \hat{v}_t) =&~\argmin_{(e,v)}\quad \|e\|_Q^2 + \left\|-Ke+v\right\|_R^2\\
		      &~\text{subject to} \quad e = (A-BK)e + Bv + w_t,
        \end{align}
    \end{subequations}
with $\textstyle{r_{T+1}:=r_T}$. The solution \eqref{eq:step_wise_ss_min_prob} can be interpreted as the steady state and input that minimize the infinite horizon time-averaged cost \eqref{eq:cost}.

\begin{theorem}\label{thm:ss_ogd_convergence}

   Under Assumptions~\ref{ass:standard} and ~\ref{ass:alpha},  if $w_t = \bar{w}$ for all $t\in\mathbb{N}$, the steady state  of \eqref{eq:combined_system_dynamics} coincides with the solution of \eqref{eq:step_wise_ss_min_prob}.
\end{theorem}

The proof of the theorem is provided in Appendix B.  As a corollary,  for a constant signal $r_t=\bar{r}$, the update converges to the solution of \eqref{eq:steady_state_program}. Note that this is not always true for the naive OGD update \eqref{eq:ss_ogd_update}, as its fixed point for a fixed disturbance is not necessarily the same as \eqref{eq:step_wise_ss_min_prob}. 



\section{Regret Analysis}
\label{sec:regret_analysis}
To characterize the effectiveness of the algorithm for time-varying signals and to provide finite time guarantees, we analyze its dynamic regret and show that it scales with the path length. The next theorem summarizes this main result.

\begin{theorem}
\label{thm:optimal_offline_regret}
Under Assumptions \ref{ass:boundedness}, \ref{ass:standard} and \ref{ass:alpha}, the dynamic regret of the SS-OGD algorithm scales with the path length
\begin{equation*}
    \mathcal{R}^{\mathrm{SS-OGD}}(w,e_0) \leq \mathcal{O}\left(1+L(T)\right).
\end{equation*}
\end{theorem}

The proof of the theorem is provided in Section \ref{sec:regret_proof} after some auxiliary results.

\begin{lemma}
\label{lem:cost_difference}
(Cost Difference Lemma \cite{kakade2002approximately}) For any two policies $\pi_1, \pi_2$ mapping states to inputs, the difference of their  accumulated costs over a $T$ long horizon is given by
\begin{equation*}
    J(e_0,u^{\pi_2}) - J(e_0,u^{\pi_1}) = \sum_{t=0}^{T-1}\mathcal{Q}_t^{\pi_1}(e_t^{\pi_2},u_t^{\pi_2}) - J_t(e_t^{\pi_2},u^{\pi_1}),
\end{equation*}
where $u_t^{\pi_2}$ is the input generated by the policy $\pi_2$ at time t, $e_t^{\pi_2}$ is the state at time $t$ generated by applying the policy $\pi_2$, and ${Q}_t^{\pi_1}(e,u) = \|e\|_Q^2 + \|u\|_R^2 + J_{t+1}(Ae+Bu+w_t,u^{\pi_1})$ is the Q-function for policy $\pi_1$ and $J_i(e_i,u)$
is the cost-to-go at time step $i$, with initial state $e_i$ and control signal $u$.
\end{lemma}
The proof is omitted, as it is identical to the one for Markov decision processes \cite{kakade2002approximately}. The following result for a general policy $\pi$ akin to the result in \cite{zhang2021regret} follows. 

\begin{lemma}
\label{lem:regret_formulation}
    Given the system dynamics \eqref{eq:lti_system_noise} and cost function \eqref{eq:cost}, the dynamic regret of any policy $\pi$ is given by
    \begin{equation*}
      \mathcal{R}^\pi(w, e_0)  = \sum_{t = 0}^{T-1} (\utp{t}{} - u_t^*)^\top (R + B^T P B) (\utp{t}{} - u_t^*),
    \end{equation*}
    where $P$ is the solution of the DARE \eqref{eq:riccati}.
\end{lemma}

\begin{proof}
    Let $ Q^{\star}_t (e,u)$ be the optimal Q-function, associated with the optimal control law $u^{\star}$. Then, using Lemma \ref{lem:cost_difference} the dynamic regret of the policy $\pi$ is given by
    \begin{equation}
    \label{eq:regret_cost_diff_lemma}
    \textstyle{
        \mathcal{R}^\pi(w, e_0) =\sum_{t = 0}^{T-1} \mathcal{Q}_t^{*}(e_t^{\pi},u_t^{\pi}) - \min_{u}Q_t^{*}(e_t^{\pi},u).}
    \end{equation}
    
    As $J_t(x,u^{\star})$ can be represented as an extended quadratic function of $x$, there exist $f\in\mathbb{R}^m$ and $g\in \mathbb{R}$ \cite{karapetyan2022regret} such that
    \begin{align*}
        Q^{\star}_t (e,u)= \|e\|_Q^2 + \|u\|_R^2 + J_{t+1}(Ae+Bu+w_t,u^{\star})= u^{\top}(R+B^{\top}PB)u + f^{\top}u+g,
    \end{align*}
Regret \eqref{eq:regret_cost_diff_lemma} is then a difference of two extended quadratic functions of the input, completing the proof.
\end{proof}
For future references, we also recall the Cauchy Product inequality defined for two finite series $\{a_i\}_{i=1}^T$ and $\{b_i\}_{i=1}^T$:
    \begin{equation}
        \label{eq:cauchy_product}\textstyle{
        \sum_{i=0}^T\left|\sum_{j=0}^{i}a_jb_{i-j}\right| \leq  \left(\sum_{i=0}^T|a_i|\right) \left(\sum_{j=0}^T|b_j|\right)}.
    \end{equation}
\subsection{Proof of Theorem \ref{thm:optimal_offline_regret}}
\label{sec:regret_proof}
 As Lemma \ref{lem:regret_formulation} suggests, the dynamic regret depends on the stepwise control input difference,
    \begin{align*}
        		\left\| \uogdt{t} - u_t^*\right\| = \left\| -K e_t + v_t + K e_t + \sum_{i=t}^{T-1} K_w^{i,t} w_i \right\|\leq \underbrace{\left\| v_t +\sum_{i=t}^{\infty} K_w^{i,t} w_t \right\|}_{s_{1,t}} + \underbrace{\left\|\sum_{i=t}^{T-1}K_w^{i,t}\Delta w_{i,t}\right\|}_{s_{2,t}} + \underbrace{\left\|\sum_{i=T}^{\infty}K_w^{i,t}w_t\right\|}_{s_{3,t}},
    \end{align*}
    where $\Delta w_{i,t} = w_i - w_t$ and for all $0\leq t \leq i <T $,
    \begin{equation*}
        K_w^{i,t} = (R+B^{\top}PB)^{-1}B^{\top}\left((A-BK)^{\top}\right)^{i-t}P.
    \end{equation*}
    We proceed by bounding each of the above terms separately.
    
\textbf{Term $\boldsymbol{s_{2,t}}$:}  This captures the deviation of the artificial disturbance term from the one fixed at timestep $t$. By noting that $\Delta w_{i,t}$ can be represented as a telescopic sum,
    \begin{align*}
        s_{2,t} \leq c_F d \sum_{i=t}^{T-1}\lambda_F^{i-t}\sum_{j=t}^{i-1}\|w_{j+1} - w_j\|
        %= c_F\sum_{j=t}^{T-2}\|w_{j+1}-w_j\|\sum_{i=j+1}^{T-1}\lambda_F^{i-t}
         \leq \frac{c_Fd}{1-\lambda_F}\sum_{j=t}^{T-2}\|w_{j+1}-w_j\| \lambda_F^{j-t},
    \end{align*}
    where $F:=A-BK$ and  $d = \| (R+B^{\top}PB)^{-1}B^{\top}\|\cdot\|P\|$. Using \eqref{eq:cauchy_product} 
    \begin{align*}
        \sum_{t=0}^{T-1}s_{2,t} &\leq \frac{c_Fd}{1-\lambda_F}\sum_{t=1}^{T-1}\sum_{j=t}^{T-1}\|w_{j}-w_{j-1}\|\lambda_F^{j-t}\leq \frac{c_Fd}{1-\lambda_F}\sum_{j=1}^{T-1}\sum_{t=1}^{j}\|w_{j}-w_{j-1}\|\lambda_F^{j-t} \\ 
        &\leq \frac{c_F d}{(1-\lambda_F)^2}\sum_{j=1}^{T-1}\|w_{j}-w_{j-1}\|
        \leq \frac{c_F d\left(\|A\|+1\right)}{(1-\lambda_F)^2}\cdot L(T)
    \end{align*}
    \textbf{Term $\boldsymbol{s_{3,t}}$:} This captures the effect of truncating the infinite horizon problem to a finite one
    \begin{align*}
        s_{3,t} \leq c_Fh\bar{R}\sum_{i=T}^{\infty}\lambda_F^{i-t}\leq \frac{c_Fh\bar{R}\lambda_F^{T-t}}{1-\lambda_F}\sum_{t=0}^{T-1}s_{3,t} \leq  \frac{c_Fh\Bar{R}\left(1-\lambda_F^T\right)}{(1-\lambda_F)^2},
    \end{align*}
where $h = \left\|\left(I - \At\right)^{-1}\Tilde{B}\right\|\left(\|A\|+1\right)$.

 \textbf{Term $\boldsymbol{s_{1,t}}$:} This captures the cost of performing a gradient step in the direction of the steady state solution instead of the full solution, for a fixed $w_t$. Note that $-\sum_{i=t}^{\infty} K_w^{i,t} w_t$ is the solution of the following infinite horizon optimization problem and is independent of the initial state \cite{goel2022power, yu2020power}
    \begin{equation*}
        	\begin{split}
		\hat{v}_t &=\argmin_{v}\lim_{T\rightarrow \infty} \frac{1}{T}\sum_{i=0}^{T}\|e\|_Q^2 + \|Ke+v\|_R^2\\
  &\begin{split}
		\text{subject to} \quad &e = (A-BK)e +Bv + w_t,
		\label{eq:infinite_horizon_problem}
  \end{split}
	\end{split}
    \end{equation*}
    which is equivalent to \eqref{eq:step_wise_ss_min_prob}. Hence, by Theorem \ref{thm:ss_ogd_convergence}
    \begin{equation*}
        \sum_{i=t}^{\infty} K_w^{i,t} w_t = -\left[I \; 0\right](I-\Tilde{A})^{-1}\Tilde{B}w_t = -\left[I \; 0\right]\hat{z}_t,
    \end{equation*}
where $\hat{z}_t = [ \hat{v}^{\top}_t~\hat{e}^{\top}_t]^{\top}:= (I - \Tilde{A})^{-1}\Tilde{B}w_t$ is the steady state of the SS-OGD dynamics \eqref{eq:combined_system_dynamics} for a given $w_t$. This term captures the difference between the SS-OGD update term $v_t$ and the steady state value $\hat{v}_t$ for that timestep. We look at the evolution of the augmented state difference; for all $0< t\leq T$

\begin{equation}
     \label{eq:epsilon_evolution}
    \varepsilon_{t} = \Tilde{A}z_{t-1} + \Tilde{B}w_{t-1} - \hat{z}_{t} = \Tilde{A}z_{t-1} + (I - \Tilde{A})\hat{z}_{t-1} - \hat{z}_{t} =\Tilde{A}\varepsilon_{t-1} + \hat{z}_{t-1} -\hat{z}_{t}.
\end{equation}

Thus, at a given time step $0\leq t \leq T$
    \begin{equation*}
    \textstyle{
        \varepsilon_t = \At^t\varepsilon_0 + \sum_{i=0}^{t-1}\At^i\left(\hat{z}_{t-i-1}-\hat{z}_{t-i}\right).}
    \end{equation*}
 Then, under Assumption~\ref{ass:alpha},
    \begin{equation*}
    \begin{split}
        \|\varepsilon_t\| &\leq c_{\At}\lambda_{\At}^t \left(\|e_0\| + \|v_0-\hat{z}_0\|\right) + \left\|\left(I - \At\right)^{-1}\Tilde{B}\right\| \cdot \sum_{i=0}^{t-1}\lambda_{\Tilde{A}}^i\left(\|A\|\|\Delta r_{t-i-1}\| + \|\Delta r_{t-i}\|\right)\\
        &\leq c_{\Tilde{A}}b\lambda_{\Tilde{A}}^t + c_{\Tilde{A}}h\sum_{i=0}^{t-1}\lambda_{\Tilde{A}}^i\left(\|\Delta r_{t-i}\| + \|\Delta r_{t-1-i}\|\right).
        \end{split}
    \end{equation*}
    Defining $b = (h+1)\bar{R} + \|x_0\| +\|v_0\|$, and $\Bar{\varepsilon} = c_{\Tilde{A}}\left(b+\frac{2\Bar{R}h}{1-\lambda_{\At}}\right)$, the following time dependent and uniform bounds apply
    \begin{equation}
    \label{eq:epsilon_bound}
         \|\varepsilon_t\| \leq c_{\Tilde{A}}b\lambda_{\Tilde{A}}^t + c_{\Tilde{A}}h\sum_{i=0}^{t-1}\lambda_{\Tilde{A}}^i\left(\|\Delta r_{t-i}\| + \|\Delta r_{t-1-i}\|\right) <\Bar{\varepsilon},
    \end{equation}
Using the above bound,
    \begin{align*}
        s_{1,t} &\leq \|\varepsilon_t\| \leq c_{\Tilde{A}}b\lambda_{\Tilde{A}}^t + c_{\Tilde{A}}h\sum_{i=0}^{t}\lambda_{\Tilde{A}}^i\|\Delta r_{t-i}\|,\\
        \sum_{t=0}^{T-1}s_{1,t}&\leq\sum_{t=0}^{T-1}\|\varepsilon_t\| \leq \frac{c_{\Tilde{A}}}{1-\lambda_{\At}}\left(b+hL(T)\right).
    \end{align*}

    
    Note that there exist $s_2,s_3\in\mathbb{R}_+$, such that $s_{2,t}\leq s_2, \; s_{3,t}\leq s_3$ and from  \eqref{eq:epsilon_bound} $s_{1,t}\leq \Bar{\varepsilon}$ for all $t\in\mathbb{N}$. Using Lemma \ref{lem:regret_formulation} and denoting $\Bar{P} = 4\|R+B^{\top}PB\|$
    \begin{align*}
        \mathcal{R}(w,e_0) < \Bar{P}\sum_{t=0}^{T-1}\left(s_{1,t}^2+s_{2,t}^2+s_{3,t}^2\right)\leq \mathcal{O}(1+L(T)).
    \end{align*}


\subsection{Steady-State Benchmark}
Given Theorem \ref{thm:ss_ogd_convergence}, one can also compare  SS-OGD  to the steady state optimal solution for each timestep. Consider the steady state control law 
\begin{equation}
 \label{eq:steady_state_controller}
 \hat{u}_t = -K\hat{e}_t+\hat{v}_t   
\end{equation} 
for all $0\leq t< T$, where $\hat{e}_t$ and $\hat{v}_t$ solve \eqref{eq:step_wise_ss_min_prob}. This steady state controller can be interpreted as an optimal benchmark that is decoupled from the system dynamics, has access to the current cost $c_t$ and solves for its optimal, steady state solution.
The following Lemma provides a side result on the regret of the SS-OGD algorithm with respect to the steady state controller \eqref{eq:steady_state_controller},  $\mathcal{R}_{\mathrm{SS}}^{\mathrm{SS-OGD}}(w,e_0) := J(e_0,u^\mathrm{SS-OGD}) - J(e_0,\hat{u})$.
\begin{lemma}
Under Assumptions \ref{ass:boundedness}, \ref{ass:standard}, and \ref{ass:alpha}, the regret of the SS-OGD algorithm with respect to the steady state benchmark \eqref{eq:steady_state_controller} scales with the reference path length,
\begin{equation*}
    \mathcal{R}_{\mathrm{SS}}^{\mathrm{SS-OGD}}(w,e_0) \leq \mathcal{O}\left(1+L(T)\right).
\end{equation*}
\end{lemma}
\begin{proof}
    The regret can be expressed as a function of the combined error state $\varepsilon$ evolving according to \eqref{eq:epsilon_evolution}. Defining $\Qt_i:=\Qt$ for all $0\leq i<T$ and $\Qt_T$ as in \eqref{eq:Q_tilde} in Appendix A
    \begin{equation*}
             \mathcal{R}_{\mathrm{SS}}^{\mathrm{SS-OGD}}(w,e_0) \leq \|\Qt\|\left(2h\Bar{R}+\Bar{\varepsilon}\right)\sum_{t=0}^T\|\varepsilon_t\|,
    \end{equation*}
    where we used the uniform bound in \eqref{eq:epsilon_bound} and the fact $\|\hat{z}_t\|\leq h\Bar{R}$ for all $t$. 
    Using the bound in \eqref{eq:epsilon_bound} again, together with the Cauchy Product inequality \eqref{eq:cauchy_product} completes the proof
    \begin{align*}
        \mathcal{R}(w,e_0)&\leq \frac{c_{\Tilde{A}}\|\Qt\|\left(2h\Bar{r}+\Bar{\varepsilon}\right)}{1-\lambda_{\At}}\left(b+hL(T)\right).
    \end{align*}
\end{proof}

\section{Numerical Example}
\label{sec:numerical_examples}

The SS-OGD algorithm is implemented on a linearized model of  CrazyFlie quadrotors \cite{beuchat2019n}  in closed loop with a \emph{PI} velocity controller \cite{Karapetyan_Distributed_Control_of}, to track a reference trajectory in two dimensions.  In particular, we consider the following model 
\begin{align*}
    &\resizebox{!}{1.25cm}{$A = \begin{bmatrix}
		1.000 & 0 & 0.096 & 0 & 0 & 0.040\\
		0 & 1.000 & 0 & 0.096 & -0.040 & 0\\
        0 & 0 & 0.894 & 0 & 0 & 0.703\\
        0 & 0 & 0 & 0.894 & -0.703 & 0\\
        0 & 0 & 0 & 0.193 & 0.452 & 0\\
        0 & 0 & -0.193 & 0 & 0 & 0.452
	\end{bmatrix}$} , 
	&\resizebox{!}{0.55cm}{$B = \begin{bmatrix}
	0.004 & 0 & 0.106 & 0 & 0 & 0.193\\
     0 & 0.004 & 0 & 0.106 & -0.193 & 0
	\end{bmatrix}^{\top}$},
\label{eq:naive_ogd_exp_sysmat}
\end{align*}
where the state $x := \begin{bmatrix}p_x&p_y&v_x&v_y&\beta&\rho\end{bmatrix}^\top$ contains the horizontal position and velocity, as well as the roll and pitch angles of the quadrotor, and the input $u: = \begin{bmatrix}v_x^{t}&v_y^{t}\end{bmatrix}^\top$ sets the target horizontal velocities.The cost matrices are taken to be $Q =\mathop{\rm diag}\left(100,100,1,1,0,0\right)$ and $R=0.1\cdot I$. 
In the first experiment, the drone tracks the shape of the letters \textbf{IFA} for an \textit{a priori} unknown reference with a fixed $\Delta r_t$ for all timesteps. SS-OGD's performance is compared to that of the CE controller \eqref{eq:step_wise_ss_min_prob} that assumes a constant reference, fixing $\textstyle{r_{i+1} = r_i}$ for all $t\leq i<T$.  
\begin{figure*}
     \begin{center}
 \includegraphics[width=0.9\textwidth]{figures/combo_v3.eps}    
\caption{Tracking a 2-D shape with a linearized quadrotor model. The horizontal position plot (left panel) suggests that the CE controller tracks the reference signal better. However, the time plot (top right panel) shows the visible time lag of the CE controller; by contrast SS-OGD quickly converges to the reference. This leads to a much lower rate of regret for SS-OGD (bottom right panel).}
 \label{fig:ifa}
 \end{center}
 \end{figure*}
The results are shown in Figure \ref{fig:ifa}. Even though the CE controller appears to be tracking the reference better in the $(p_x, p_y)$ plot, the plot of position as a function of time reveals that in fact it lags behind the reference trajectory, resulting in around $3$ times higher regret, compared to SS-OGD. As the reference signal has a constant rate of change, the double integrator dynamics of the open loop transfer function from the error to the state, allows SS-OGD to achieve perfect position tracking. Even though, for references that have a varying rate of change, this is no longer the case, SS-OGD still performs better than the CE controller. 

In a second experiment we calculate the empirical worst case regret as a function $T$. In particular, for each $T$, $60$ random reference signals are simulated and the highest value of regret is noted. The references are generated such that $\|\Delta r_t\|$ decreases with a constant factor of $0.99$. This ensures a finite path length in the limit, as visualized in Figure \ref{fig:regret}, in agreement with  the  upper bound  in Theorem \ref{thm:optimal_offline_regret}.
\begin{figure}%[H]
\begin{center}
\includegraphics[width=0.5\columnwidth]{figures/regret_v4.eps}   
\caption{Empirical regret of SS-OGD for an exponentially decreasing path length.} 
\label{fig:regret}
\end{center}
\end{figure}
\section{Conclusion}
In this letter, we reformulate the LQT problem as an online control problem for linear systems subject to adversarial disturbances. Within this framework we propose a novel online gradient descent-based algorithm, called SS-OGD, that incorporates the known dynamics of the system into the online control update. We analyze the dynamic regret of the proposed scheme and show that it scales with the path length of the reference signal. Numerical examples are presented to validate the results on a linearised quadrotor model.


\section*{Appendix A - System-Optimizer Dynamics}
\label{sec:appendix_matrices}
The combined system-optimizer dynamics matrices are 
\begin{equation*}
\Tilde{A} = \begin{bmatrix}
			I-\alpha M & -\alpha H \\
			B & A-BK
\end{bmatrix}, \qquad
\Tilde{B} = \begin{bmatrix}
			-2 \alpha S^T Q \\ I
	\end{bmatrix},
\end{equation*}
where $M: = 2 \left(S^T Q B - (I-KS)^\top R\right)$, and $H:=2(S^T Q (A-BK)-(I-KS)^\top R K)$. The objective function in \eqref{eq:step_wise_ss_min_prob} can be equivalently written as $z_t^{\top}\Tilde{Q}z_t$ for $0<t<T$ and as  $z_T^{\top}\Tilde{Q}_Tz_T$, where
\begin{equation}
\label{eq:Q_tilde}
    \Tilde{Q} \!=\! \begin{bmatrix}
		R & -RK \\ -K^{\top}R & Q\!+\!K^{\top}RK
 	\end{bmatrix}\!,~
      \Tilde{Q}_T \!=\! \begin{bmatrix}
		\boldsymbol{0}_{m \times m} & \boldsymbol{0}_{m \times n} \\ \boldsymbol{0}_{n \times m} & P
 	\end{bmatrix}\!.
\end{equation}

Consider the  coordinate transformation  $\Tilde{A}_V := V\Tilde{A}V^{-1}$
\begin{equation*}
    V = \begin{bmatrix}
        I & \boldsymbol{0}_{m \times n}\\
        -S & I
    \end{bmatrix}, \Tilde{A}_V = \begin{bmatrix}
        I -\alpha\overline{M} & -\alpha H\\
        \alpha S\overline{M} & \alpha SH + (A-BK)
    \end{bmatrix},
\end{equation*}
with $\overline{M} := M+HS = 2 \left(S^{\top}QS + (I-KS)^{\top}R(I-KS)\right)$  positive definite, as shown in Lemma \ref{lem:strong_convexity}. Using the small gain theorem for interconnected systems \cite{green2012linear}, the following, along with $\alpha<2/\lambda_{max}(\overline{M})$ is a sufficient condition for the stability of $\Tilde{A}_V$ and therefore of the  dynamics \eqref{eq:combined_system_dynamics}
\begin{equation*}
    \alpha\cdot\frac{\|S\overline{M}\|\|H\|}{\lambda_{min}(\overline{M})}\cdot \max_{w\in\mathbb{R}}\big\|\big[e^{jw}I - \alpha SH - (A-BK)\big]^{-1}\big\|<1.
\end{equation*}
Since $A-BK$ is stable, there always exists an arbitrarily small $\alpha>0$  such that the above is fulfilled.

\section*{Appendix B - Proof of Theorem~\ref{thm:ss_ogd_convergence}}
\label{sec:appendix_steady_state_proof}
\begin{proof}
Given a disturbance vector $w_t$ and a bias input $v$, the steady state  of the dynamics \eqref{eq:lti_system_noise} with the control law \eqref{eq:ogd_input} is given by $e = Sv + \hat{S}w_t$, where $\hat{S} := (I -A+BK)^{-1}$.
Substituting this in the objective function of \eqref{eq:step_wise_ss_min_prob}, one can confirm that the $v$ that minimizes that cost is the unique (as shown in Lemma \ref{lem:strong_convexity}) solution of 
\begin{equation*}
    \left(S^{\top} QS + \|I-KS\|_R^2\right) v = \left( (I-KS)^{\top} R K \hat{S} - S^{\top} Q \hat{S}\right) w_t.
\end{equation*}
Using the definition of $\Tilde{A}$, the steady state $\hat{v}$ of the SS-OGD update~\eqref{eq:ss_ogd_update} for a constant $w_t$, is characterized by $ \hat{v}= M\hat{v} -H(S\hat{v}+\hat{S}w_t)-2\alpha S^{\top}Qw_t$. Then,
\begin{align*}
    0 = S^T Q (+(A-BK)\hat{S})B \hat{v} +(I-KS)^T R(I-KS) \hat{v}   + S^T Q (I+(A-BK) \hat{S}) w_t-(I-KS)^T R K
\end{align*}
Since $I+(A-BK) \hat{S} = \hat{S}$ , the two equations coincide, leading to the unique steady state solution $\hat{v}$.
\end{proof}



\bibliographystyle{ieeetr}
\bibliography{bibliography.bib}

\end{document}