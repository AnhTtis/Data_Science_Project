\section{Appendix}








\subsection{Hyperparameters}
\label{ssec:hyperparams}

\Cref{table:otil-hyperparams-locomotion} lists the hyperparameters used by OTR and IQL on the locomotion datasets. For Antmaze and Adroit, unless otherwise specified by~\cref{table:otil-hyperparams-antmaze} or~\cref{table:otil-hyperparams-adroit}, the hyperparameters follows from those used in the locomotion datasets.

The IQL hyperparameters are kept the same as those used in~\citep{kostrikov2022IQL}. Note that IQL rescales the rewards in the dataset so that the same set of hyperparameters can be used for datasets of different qualities.
Since OTR computes rewards offline, we also apply reward scaling as in IQL. For the locomotion datasets, the rewards are rescaled by $\frac{1000}{\text{max\_return} - \text{min\_return}}$ while for antmaze we subtract $2$ to the rewards computed by OTR. The reward processing in antmaze is different from the one used by the original IQL paper (which subtracts $1$) since the rewards computed by OTR have a different range.

The squashing function used by OTR is based on the one used in~\citep{dadashi2022PWIL}. The antmaze squashing differs slightly from the one used in locomotion and adroit due to use of an earlier configuration. In practice, this should have minimal effect on the performance.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
     & Hyperparameter & Value \\
    \midrule
    & Discount & $0.99$ \\
    \midrule
    \multirow{2}{*}{Network Architectures}
    & Hidden layers & $(256, 256)$ \\
    & Dropout & none \\
    & Network initialization & orthogonal \\
    \midrule
    \multirow{2}{*}{IQL}
    & Optimizer & Adam \\
    & Policy learning rate & $3e^{-4}$, cosine decay to $0$ \\
    & Critic learning rate & $3e^{-4}$ \\
    & Value learning rate & $3e^{-4}$ \\
    & Target network update rate & $5e^{-3}$ \\
    & Temperature & 3.0 \\
    & Expectile & 0.7 \\
    \midrule
    \multirow{3}{*}{OTR}
    & Episode length $T$ & $1000$ \\
    & Cost function & cosine \\
    & Squashing function & $s(r) = 5.0 \cdot \exp (5.0 \cdot T \cdot r / |\mathcal{A}|) $ \\
    \bottomrule
    \end{tabular}
    \caption{OTR hyperparameters for D4RL Locomotion.}
    \label{table:otil-hyperparams-locomotion}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{llr}
     & Hyperparameter & Value \\
    \midrule
    \multirow{2}{*}{IQL}
    & Temperature & 10.0 \\
    & Expectile & 0.9 \\
    \midrule
    \multirow{1}{*}{OTR}
    & Squashing function & $s(r) = 5.0 \cdot \exp (T \cdot r) $ \\
    \bottomrule
    \end{tabular}
    \caption{OTR hyperparameters for D4RL Antmaze.}
    \label{table:otil-hyperparams-antmaze}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{llr}
     & Hyperparameter & Value \\
    \midrule
    \multirow{1}{*}{Network Architectures}
    & Dropout & 0.1 \\
    \midrule
    \multirow{2}{*}{IQL}
    & Temperature & 0.5 \\
    & Expectile & 0.7 \\
    \bottomrule
    \end{tabular}
    \caption{OTR hyperparameters for D4RL Adroit.}
    \label{table:otil-hyperparams-adroit}
\end{table}

\subsection{Additional Experimental Results on Adroit and Antmaze}
\label{ssec:experiment-results}
We evaluate OTR on additional datasets from the antmaze and adroit domains with varying number of expert demonstrations. The results are presented in~\cref{table:otil-iql-adroit-results} and~\cref{table:otil-iql-antmaze-results}. OTR consistently recovers the performance of IQL with ground-truth rewards on these datasets, largely independent of the number $K$ of expert demonstrations provided.

\begin{table}[h]
    \centering
    \input{results/otil_iql_adroit}
    \caption{OTR+IQL Results on Adroit. The standard deviations for IQL (oracle) are not available from~\citep{kostrikov2022IQL}.}
    \label{table:otil-iql-adroit-results}
\end{table}

\begin{table}[h]
    \centering
    \input{results/otil_iql_antmaze}
    \caption{OTR+IQL Results on Antmaze.}
    \label{table:otil-iql-antmaze-results}
\end{table}

\newpage
\subsection{Combining OTR with Different Offline RL Algorithms}
In the main experiments, we evaluated OTR by pairing it with the IQL algorithm. In this section, we investigate if OTR can recover the performance of a different offline RL algorithm (TD3-BC)~\citep{fujimoto2021TD3-BC} using ground-truth rewards. We observe that (i) the performance from OTR+TD3-BC mostly matches those using the ground-truth rewards; (ii) the performance is fairly robust to the choice of the number of expert trajectories ($K=1$ and $K=10$ many expert demonstrations provide comparable performance). However, There are more variances on some datasets (e.g., \texttt{halfcheetah-medium-expert-v2}). Nevertheless, the differences are smaller compared to the baselines and
OTR+TD3-BC still performs better than the baselines presented in~\cref{sec:experiments} in terms of the aggregate performance.
\begin{table}[h]
    \centering
    \input{results/otil_td3_bc_mujoco}
    \caption{OTR+TD3-BC Results on MuJoCo.}
    \label{table:otil-td3-bc-mujoco-results}
\end{table}

\newpage
\subsection{Importance of Using the Optimal Transport Plan}
In the main experiments, we compute the rewards based on the optimal coupling computed by the Sinkhorn solver. The optimal transport plan is sparse and transports most of the probability masses to only a few expert samples. In this section, we investigate what happens if we use a suboptimal transport plan where each sample from the policy's trajectory is transported equally to each sample in the expert's trajectory. In this case, the reward function essentially boils down to computing the average costs with respect to all of the states in the expert's trajectory.

\begin{table}[h]
    \centering
    \input{results/otil_iql_uniform_transport.tex}
    \caption{OTR with Uniform Transport Plan}
    \label{table:otil-uniform-transport}
\end{table}

\Cref{table:otil-uniform-transport} compares the performance of OTR+IQL using the optimal transport plan and uniform transport plan. We find that for many datasets, using the suboptimal uniform transport plan is sufficient for reaching good performance. This indicates that using a reward function based on the similarity of states from the policy and the expert can be a simple and effective method for reward labeling. However, note that the uniform transport plan can still underperform compared to using the optimal transport plan (e.g., \texttt{hopper-medium-replay-v2}). This shows that the optimal transport formulation enables better and more consistent performance.

\subsection{Comparison to PWIL}
In this section, we investigate if the online imitation learning algorithm PWIL~\citep{dadashi2022PWIL} can be used in the offline setting with a change from using an online RL algorithm to an offline RL algorithm. We ran PWIL with IQL similar to what we did for OTR in the main paper. We use the PWIL implementation from Acme~\citep{hoffman2022Acme}\footnote{\url{https://github.com/deepmind/acme/tree/master/acme/agents/jax/pwil}}.

Note that although OTR is similar to PWIL in using the Wasserstein distance to construct RL reward signals, OTR differs from PWIL in the choices of OT solver, the cost function as well as the approach used for aggregating results multiple expert demonstrations. Also note that for all experiments in the paper we consider learning only from expert state instead of state-action pairs. This is both a more general and challenging setting. It was found in ~\citep{dadashi2022PWIL} that PWIL sometimes perform badly without expert actions.
We ran OTR and PWIL using only expert observations (denoted as OTR-state and PWIL-state) and OTR and PWIL using state-action pairs (denoted as OTR-action and PWIL-action). The results are illustrated in~\cref{table:otr-pwil}. We found that we are unable to get good results when running PWIL using only expert state sequences. This is possibly due to difference choices of OT solver and cost functions. PWIL can perform well when combined with IQL to learn in the offline setting although sometimes performance is significantly worse compared to IQL oracle or OTR (e.g., \texttt{hopper-medium-expert-v2}).
\begin{table}[h]
    \centering
    \input{results/pwil_iql.tex}
    \caption{Comparison between OTR and PWIL with IQL as offline RL backbone.}
    \label{table:otr-pwil}
\end{table}

In addition, ~\citet{dadashi2022PWIL} found that PWIL's performance may deteriorate when learning from demonstrations consisting of only expert observations (i.e., no actions are present in the expert demonstrations).

\newpage
\subsection{Hyper-parameter Sensitivity}
\begin{table}[h]
    \centering
    \input{results/otr_a1b1.tex}
    \caption{Effect of $\alpha$ and $\beta$ in the squashing function.}
    \label{table:otr-a1b1}
\end{table}

For the main results, the hyper-parameters for the squashing function ($\alpha$ and $\beta$) was chosen to be consistent with those used in~\citep{dadashi2022PWIL}.
In this section we compare the differences in the choices of these hyper-parameters by running OTR with $\alpha = \beta = 1$. This reduces to simply applying an exponential transformation to the optimal transport costs. The results are illustrated in~\cref{table:otr-a1b1}. We find that OTR still performs well, demonstrating that it is not sensitive to the choices of these hyper-parameters.



