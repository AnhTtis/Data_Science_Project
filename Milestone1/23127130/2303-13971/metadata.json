{
    "arxiv_id": "2303.13971",
    "paper_title": "Optimal Transport for Offline Imitation Learning",
    "authors": [
        "Yicheng Luo",
        "Zhengyao Jiang",
        "Samuel Cohen",
        "Edward Grefenstette",
        "Marc Peter Deisenroth"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG"
    ],
    "abstract": "With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the performance of offline RL with ground-truth rewards.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13971v1"
    ],
    "publication_venue": "Published in ICLR 2023"
}