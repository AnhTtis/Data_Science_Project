In this section, we evaluate the performance of adaptation protocols across several additional safety objectives \citep{Hendrycks21_UnsolvedProblems}, as practical transfer learning applications require both strong and safe generalization. Through this expanded evaluation, we find that no single protocol is optimal across all safety objectives. Indeed, the inability of \lpft~to induce safe adaptation indicates that a complementary perspective to feature distortion, namely simplicity bias, is necessary when designing generalizable and safe protocols (see Sec. \ref{sec:simplicity}). We further argue that by constraining models around the \lp~initialization during \ft, \lpft~may inadvertently harm safety performance by hampering models' abilities to learn complex, task-specific features needed for robust generalization. While we expand upon the role of \lp~initialization in Secs. \ref{sec:simplicity} and \ref{sec:hardness}, we begin, here, by introducing the expanded evaluation and experimental setup.

\paragraph{Experimental Setup.} Three downstream adaptation tasks (and their respective OOD distributions) are considered: CIFAR-10 (ID)  $\rightarrow$ \{STL10, CIFAR10.1\} (OOD), Domainnet-Sketch $\rightarrow$ \{Domainnet-ClipArt, Domainnet-Painting, Domainnet-Real\} and Living17 (Source) $\rightarrow$ Living17 (Target). These datasets are selected as they correspond to two different types of distribution shifts (standard domain adaptation and subpopulation) and three levels of distortion (low, medium, high). A MoCo-V2 ResNet-50~\citep{He20_MoCo} pretrained on ImageNet-1K is used as the base-feature extractor for CIFAR10 and Living17 experiments, and the CLIP ResNet-50 image encoder pretrained on 400 million (image,text) pairs is used for Domainnet-Sketch. These models are selected as they provide sufficiently high-quality representations capable of generalizing to both ID and OOD downstream data \citep{Kumar22_FinetuningDistorts}. We perform grid-search to find the best hyper-parameters, and average over $3$ seeds. See Sup. \ref{sup:experimentaldetails} for additional details. 

\paragraph{Expanded Evaluation.} In addition to OOD accuracy on the aforementioned distribution shifts, we report performance on the following metrics in order to evaluate adapted models on key problems in machine learning safety~\citep{Hendrycks21_UnsolvedProblems}. Our evaluation setup is inspired by \citet{Hendrycks21_PixMix}:
\begin{itemize}[leftmargin=*]
    \item\textit{Mean Corruption Accuracy (mCA/m$\Bar{C}$A):} We consider two sets of corruptions: the $15$ naturalistic corruptions $(Corr)$~\citep{Hendrycks19_CIFAR10C}, and 10 perceptually dissimilar corruptions $(\overline{Corr})$~\citep{mintun21_cbar}. Corruptions are applied to the ID test dataset and the average accuracy over each set is reported.
    \item\textit{Calibration Error (RMSE)}: It is important that models are well-calibrated so that practitioners may trust the provided predictions in high-risk applications \cite{Guo17_Calibration}. We measure the root mean square error of calibration as follows: {\tiny$\sqrt{\mathbb{E}_{\mathrm{C}}\left[(\mathbb{P}(Y=\hat{Y} \mid \mathrm{C}=\mathrm{c})-\mathrm{c})^{2}\right]}$}, where {\small$\mathrm{C}$} indicates the confidence scores, while {\small$\hat{Y}$} and {\small$Y$} denote the model's predictions and ground-truth labels, respectively.
    \item\textit{Anomaly Detection Performance (AUROC):} Recognizing when samples are anomalous allows models to abstain from making uninformed and inapplicable predictions. We consider samples from Blobs, Gaussian, LSUN, Places69, Rademacher, Textures, and SVHN datasets as anomalies and report the AUROC (area under the ROC curve) of the binary classification problem of detecting such samples as anomalies. 
    \item\textit{Adversarial Accuracy:} DNNs are well-known to be fooled by imperceptible distortions \citep{ilyas19_bugs}. We use a 2/225, 10-step PGD \citep{Madry18_PGD} attack to measure the robustness of models to such perturbations.
\end{itemize}
 
We make the following observations regarding the behavior of different protocols using this expanded evaluation. In brief, we find that no single protocol is effective across all datasets in jointly obtaining strong and safe adaptation, and that, on low distortion adaptation tasks, the quality of the \lp~initialization is critical as pre-trained feature extractor is not substantially updated during \lpft. 

\subsection*{Observation 1: Mitigating Feature Distortion may not induce safe adaptation.}
\begin{wrapfigure}{rt}{0.34\textwidth}
\vspace{-0.2in}
\begin{center}
\includegraphics[width=0.34\textwidth]{FIGS_v3/sample_heatmap-10-vert.pdf}
\end{center}
\caption{\textbf{Disparate Performance of Protocols.} We plot the average rank of each protocol for different safety and generalization metrics. We see no single protocol achieves top rank across all metrics.}
\vspace{-0.65in}
\label{fig:ranking}
\end{wrapfigure}

Here, we ask how protocols perform when we consider both safety and generalization objectives to better understand the feature distortion perspective. In particular, if \lpft~is able to outperform \lp~and \ft~in this expanded evaluation, then it suggests that solely mitigating feature distortion during \ft~may be sufficient to induce robust adaptation. To test this claim, we rank protocol performance for each safety metric, where ranks are first computed for each dataset separately, and then averaged. Results are shown in Fig. \ref{fig:ranking}. Smaller ranks correspond to better performance.

\textit{Results.} \lpft~obtains the best rank for ID and OOD accuracy as expected, as well as $Corr$ and $\overline{Corr}$ accuracy. However, we also see that \ft~is better ranked for Adversarial Accuracy and OOD calibration, while \lp~is better ranked for ID calibration and $\overline{Corr}$ calibration. However, given that \lpft~trails behind protocols that are not explicitly designed to limit distortion on some safety metrics, it is clear that a complementary perspective is needed to better understand protocol behavior. Indeed, \lpft~has the best \textit{average} rank, indicating that it is a good starting point to improve upon. 

The above results are aggregated across different types of distribution shifts; we extend this analysis next by considering the interplay between individual datasets and protocol performance. These detailed results are presented in Table \ref{tab:teaser_table}.

\subsection*{Observation 2: Linear Probing Solutions Matter.} 

Naturally, the amount of distortion required to effectively adapt a pretrained model to a downstream task will vary in accordance to the similarity of the downstream and pretraining data. Here, we seek to understand how protocols behave under different levels of distortion. In particular, we hypothesize that the \lp~initialization becomes more influential for \lpft~in low distortion settings, as subsequent \ft~remains in the vicinity of initialization. To this end, we compute the batched centered kernel alignment (CKA) score~\citep{nguyen2020wide} with respect to the adapted and pretrained models, and take a closer look at performance across metrics. We note that while CKA is better suited for measuring distortion than the L2 norm as used by \cite{Kumar22_FinetuningDistorts}, other neural representation metrics can also be used \citep{Ding21_RepSimStatTest,Davari23_ReliabilityCKA}. 

\textit{Results.} As shown in Fig. \ref{fig:cka-bars}, we see that minimal distortion (CKA $\ge$ 0.9) is required to obtain competitive \lpft~performance on DomainNet and Living17. However, on CIFAR10, which requires the most distortion as evidenced by lower CKA scores, \ft~is the most effective protocol for safety measures and is very comparable on generalization performance (see Table \ref{tab:teaser_table}).

The effectiveness of \lp~and \lpft~on Living17 in improving OOD generalization over \ft~is hardly surprising, as Living17 is a subset of ImageNet, on which the base feature-encoder was already trained. 
\begin{wrapfigure}{l}{0.30\textwidth}
\begin{center}
% \vspace{-0.2in}
\includegraphics[width=0.30\textwidth]{FIGS_v3/sample_cka-7.pdf}
\end{center}
\caption{\textbf{Dataset Distortion.} We plot the CKA similarity between adapted and pretrained models. DomainNet and Living17 require low distortion, as seen by performance of \lpft~across metrics with high CKA ($>0.9$).} 
\label{fig:cka-bars}
\vspace{-0.2in}
\end{wrapfigure}
In contrast, on DomainNet, the difficulty of \ft~in matching the ID \textit{test} task performance, despite achieving high training accuracy, suggests \ft~may learn a solution that relies upon shortcuts (or simple features) that do not generalize. 
We emphasize that \lpft~greatly benefits from strong \lp~initializations on these low-distortion datasets as corresponding CKA scores show that very limited updates are made during \ft. While \lpft~does induce meaningful improvements over \lp~on Living17 and performs comparably to \lp~on DomainNet, we stress the model must be kept close to the \lp~initialization during \ft. Indeed, to obtain acceptable \lpft~performance, small learning rates (3e-7,1e-5) and frozen batch-norm parameters during \ft~are necessary.

\textit{Summary.} Taken jointly, these results suggest that while solely mitigating feature distortion may not be sufficient to ensure that adapted models perform well on safety metrics across different levels of shift, improving the \lp~initialization may be a viable solution to obtaining strong and safe generalization. Indeed, the effectiveness of \lpft~on low distortion datasets and its high average ranking indicates that it is a promising protocol to build upon. To understand how to build better protocols, we next introduce \textit{simplicity bias} as a complementary perspective to feature distortion. 

\input{TABLES_v3/teaser_table.tex}