Here, we discuss the most relevant work on adaptation protocols and simplicity bias; we discuss additional related work in Sup. \ref{sup:relatedwork}.

\paragraph{Adaptation Protocols.}
For a comprehensive overview of transfer learning, please see the surveys of~\citet{zhuang21_transferlearningsurvey} and~\citet{Pan10_TransferSurvey}. Here, we discuss the works that are most relevant to our own. \citet{Kirichenko22_LastLayerRetrain} recently demonstrated that models are able to learn both core features and spurious features. However, classifiers can rely upon spurious features, harming performance on minority groups. To reduce the reliance on spurious features, they propose to retrain the classifier on a small amount of ``re-weighting" data, which allows the model to leverage the core features instead of the spurious features. Other modifications and heuristics have also been proposed to improve \ft's performance, including side-tuning~\citep{Zhang19_Sidetuning}, which tunes a small secondary network that is then combined with the original model, using larger/smaller learning rates for the classifier, as well as regularization-based methods \citep{Jiang20_SMART}. In this work, we focus on two popular and effective protocols, \lp~and \ft. We additionally study the \lpft~protocol as it is theoretically-grounded, does not require re-weighting data, is designed to exploit high-quality pre-trained representations and achieves SOTA OOD performance during adaptation. 

\paragraph{Simplicity Bias.}
It is well-known that DNNs demonstrate a bias toward simple, potentially less expressive features~\citep{brutzkus18_sgd,Soudry18_ImplicitBias,Gunasekar18,geirhos18_texturebias,Hermann20_OriginsTexture,Lubana23_ModeConnectivity}, such as textures and backgrounds, and that this bias can lead to shortcuts that limit the generalization of DNNs. Indeed, recently \citet{Shah20_SimplicityBias} formalized this intuition by more precisely defining simplicity bias, based on the number of linear components to define a decision boundary, and showed that SB leads to non-robust decision boundaries that affects a model's sensitivity to distribution shifts and adversarial perturbations. In brief, by learning simple features first, models become invariant to complex features, potentially leading to narrow decision boundaries which can fail to generalize under data shifts. Notably, DNNs exhibit this bias even when complex features are more expressive and necessary for fitting the distribution. While various techniques have recently been proposed to mitigate simplicity bias when training from scratch or in the context of pretraining \citep{Teney21_EvadingSB}, we are, to the best of our knowledge, the first to rigorously study the role of simplicity in the context of model adaptation. 
