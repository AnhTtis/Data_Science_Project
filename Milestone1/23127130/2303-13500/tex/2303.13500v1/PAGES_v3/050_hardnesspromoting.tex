Given the effectiveness of incorporating hardness promoting (hp) augmentations with the family of \lpft~protocols (hp-\lpft) in mitigating simplicity bias in a synthetic setting, we further evaluate the modified protocols on the three real-world datasets (Living17, DomainNet, and CIFAR10) with respect to the generalization and safety metrics introduced in Sec. \ref{sec:tradeoffs}. We present our results in Tables \ref{tab:living17},\ref{tab:domainnet}, and \ref{tab:cifar10}); our observations are summarized below. Any method-specific hyperparameters (e.g., epsilon) are tuned using ID validation data and all results are reported over three seeds. We provide additional results in Supp. \ref{sup:additional_results}.

\textit{Results.} As discussed in Sec. \ref{sec:tradeoffs}, these three datasets represent scenarios where different levels of distortion are necessary when adapting the pretrained model. On Living17, a setting which requires minimal distortion during adaptation, we see that vanilla \lpft~is quite effective with respect to both generalization and safety metrics and is a difficult baseline to surpass. Indeed, while hp-\lpft~variants do not lead to significant benefits, they generally perform comparably to vanilla \lpft. On DomainNet, a setting where fairly low distortion is required for \lpft~but \ft~struggles to find a good solution, we see that hp-\lpft~variants induce some slight benefits with respect to ID/OOD generalization and robustness, though vanilla \lp~and hp-\lp~have better calibration performance. In contrast on CIFAR10, which requires more distortion to obtain an acceptable solution, we see that hp-\lpft~ variants lead to improved generalization and a noticeable boost in corruption robustness. \vat  + \ft~ and \vat~are particularly effective in this regard. Lastly, across all datasets, we observe that hp-\lpft~protocols lead to similar distortion to vanilla \lpft, which suggests that any additional benefits of hp-\lpft~should not be attributed to only reducing feature distortion. 

\input{TABLES_v3/living17.tex}
\input{TABLES_v3/domainnet.tex}
\textit{Discussion.} We find that while vanilla \lpft~is already an effective protocol, especially in settings where low distortion is required, hp-\lpft~can provide some benefits and performs competitively. We suspect that the performance of these modified protocols can further be improved if more sophisticated simplicity bias mitigation strategies are used. Indeed, our central claim, that adaptation protocols should mitigate feature distortion and simplicity, is not dependent on a specific strategy. We additionally note that while such mitigation strategies may optionally \textit{also} be used during \ft, they cannot \textit{solely} be used in \ft. Indeed, in the case of extreme simplicity, if the \lp~classifier relies upon simple features to find a low-loss solution, during the subsequent \ft~step, gradients may not be back propagated in directions that contain complex features. This entails that the decision boundary continues to rely upon simple features and is at risk of reduced safety performance. We provide further discussion in Supp.\ref{sup:ablation}. To this end, we recommend incorporating hardness-promoting augmentations during \lp~as a potential safe-guard to simplicity bias. 

\input{TABLES_v3/cifar10.tex}
