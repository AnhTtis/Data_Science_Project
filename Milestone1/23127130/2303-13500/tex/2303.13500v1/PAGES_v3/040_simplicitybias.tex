As discussed in Sec.~\ref{sec:related_work}, simplicity bias (SB) underlies various safety issues in machine learning as models may learn to rely upon simple features that often do not generalize under distribution shifts, such as corruptions or adversaries~\citep{Shah20_SimplicityBias}. Therefore, we argue that mitigating feature distortion in a way that minimizes this bias can be a valid mechanism to improve both safety and generalization performance. 
Correspondingly, in this section, we first measure the propensity of different protocols to simplicity bias in a controlled setting. In particular, given our previous observation that \lpft~models remain in close vicinity of the \lp~solution after \ft, we focus on improving the performance of this initial \lp~initialization so that we may capitalize upon \lpft~strong OOD performance, while simultaneously improving safety. To this end, we propose three light-weight \lpft~variants that are able to both reduce distortion and SB. We begin by introducing our synthetic dataset and experimental setup. 

\paragraph{Dataset.} As shown in Fig. \ref{fig:cifar_mnist}, we create ``dominoes" of complex and simple features by pairing each class \citep{Shah20_SimplicityBias} from CIFAR10 (complex) with the corresponding ``digit" class in MNIST (simple), e.g., ``bird" samples are paired with digit ``2" samples, where the label for each domino is determined by the complex, CIFAR10 sample. Datasets with three levels of correlation (95\%, 99\%, 100\%) between the simple and complex features are constructed for training. While 100\% correlation allows models to only learn the simple feature for perfect generalization, the more realistic lower correlation settings require models learn at least some aspect of the complex features.

\begin{wrapfigure}{r}{0.3\textwidth}
\vspace{-0.1in}
\begin{center}
\includegraphics[width=0.3\textwidth]{FIGS_v3/cifar_mnist_tight.pdf}
\end{center}
\caption{\textbf{Synthetic Data with Simple and Complex Features.} Using a synthetic dominoes dataset \citep{Shah20_SimplicityBias}, we study the effect of simplicity bias on safety and OOD generalization.}
\label{fig:cifar_mnist}
\vspace{-0.2in}
\end{wrapfigure}

\paragraph{Experimental Setup.} For evaluation, we also construct a randomized (10\% correlation) variant, where simple features are randomly paired with complex features. We give two examples in  panels 3 and 4 of Fig. \ref{fig:cifar_mnist}. To assess OOD generalization, we create a variant where complex features are sampled from STL10, instead of CIFAR10, e.g., panels 1 and 2 in Fig. \ref{fig:cifar_mnist}.

\textit{Metrics.} We assess the reliance on simple features using the following metrics: (1) \textit{Randomized Accuracy}: the accuracy on the variant where samples contain random pairings between simple and complex features; (2) \textit{Correlated Accuracy}: accuracy when pairings between simple and complex features remain correlated. 
Models that \textit{are} susceptible to simplicity bias will have \textit{high} Correlated Accuracy and \textit{low} Randomized Accuracy. Likewise, models that are \textit{not} susceptible to simplicity bias will have relatively \textit{lower} correlated accuracy and \textit{higher} randomized accuracy.

\textit{Training Details.} A MoCo-V2 ResNet-50~\citep{He20_MoCo} pretrained on ImageNet-1K is the base-feature extractor.
See Supp. \ref{sup:experimentaldetails} for additional details. We performed grid-search to find the best parameters. Results are over $3$ seeds and 3 correlation strengths. 

\input{TABLES_v3/dominoes_vanilla_protocols.tex}

\textit{Results.} Given the above experimental setup, we report the performance of different adaptation protocols in Table. \ref{tab:dominoes_table}. 
Across all correlation strengths, \ft~has the lowest \textit{Rand.} OOD accuracy and high \textit{Corr.} OOD accuracy. This clearly indicates that \ft~has learned to rely upon simple features, effectively disregarding the expressive features of the pretrained model, and is easily susceptible to simplicity bias. In contrast, by preserving the expressive features of the underlying feature encoder, \lp~best mitigates simplicity bias in high correlation (0.99,1.0) settings as evidenced by the highest \textit{Rand} OOD accuracy (though \textit{Corr.} ID/OOD accuracy does slightly suffer). However, in moderate correlation (0.95), \lpft~improves upon \lp~to achieve better \textit{Corr.} OOD accuracy than \lp~and the best \textit{Rand.} OOD accuracy across protocols. This suggests that when the correlation is not extreme, that moderate distortion, given a suitable \lp, is in fact beneficial to mitigating simplicity bias. At higher correlation strengths (0.99,1.0), however, \lpft~has lower \textit{Rand} OOD accuracy, while improving the \textit{Corr.} OOD accuracy relative to \lp, indicating in such extreme settings the distortion incurred by subsequent \ft~ is not beneficial and has increased reliance upon simple features. 

\subsection{Improved Linear Probes for Mitigating Simplicity Bias}\label{sec:improved_protocols}

As discussed earlier, adaptation protocols have varying susceptibility to simplicity bias, and mitigating this susceptibility can help improve generalization and safety. 
\begin{wrapfigure}{r}{0.50\textwidth}
\begin{center}
\vspace{-0.15in}
\includegraphics[width=0.48\textwidth]{FIGS_v3/bigplot-4.pdf}
\end{center}
\caption{\textbf{Hardness Promoting Augmentations help Mitigate Simplicity Bias.} We evaluate the modified \lpft~protocols on the dominoes dataset, and find they improve the Rand. OOD Accuracy over vanilla \ft~and \lpft. This suggests that modified protocols can rely less upon shortcuts or simple features.}
\label{fig:dominoes}
\vspace{-0.2in}
\end{wrapfigure}
In particular, we observe that \lpft~and \lp~are effective protocols for reducing reliance upon simple features on both synthetic datasets, and on low-distortion real datasets (Sec. \ref{sec:tradeoffs}). However, as some level of distortion is typically required when adapting to downstream tasks to obtain sufficient ID task performance, we propose new variants of the \lpft~ protocol that attempt to enable the subsequent \ft~step to distort features without compromising generalization or increasing simplicity bias.

We note that, while it is possible to modify the \ft~step as well, modifications to \lp~are inexpensive as the feature-encoder is not updated. Moreover, as discussed in Sec. \ref{sec:tradeoffs}, fine-tuned solutions remain in close vicinity of initial \lp~initializations, further motivating strong starting solutions. To this end, we introduce the following modifications to the \lp~step of \lpft~below, where $h$ are the hidden representations, $\theta$ model parameters, $y$ labels, $\hat{y}$ predicted classes, $C$ the number of classes, $g$ the classifier, and $\delta$ the perturbation. See Supp. \ref{sup:motivation} for additional discussion on the choice of these mitigation strategies and Supp. \ref{sup:ablation} for discussion on the importance of applying mitigations during \lp. 

\begin{itemize}[leftmargin=*]
    \item \textbf{\vat}: Virtual adversarial training (VAT)~\citep{Miyato17_Vat} enforces local distribution smoothness by minimizing the KL-divergence between the predictions of perturbed pairs of examples. 
    Since we are using expressive pretrained models, such perturbations may be meaningful in the inverted latent space as well, and resulting classifiers become robust in some $\epsilon$-neighborhood around each latent-space input. 
    Formally, let $\epsilon$ be some perturbation budget, and $\alpha$ a hyper-parameter weighting distributional label smoothness, we minimize the following loss:
    $\min_\theta \mathcal{L}_{\text{CE}}(g_\theta(h),y) -\alpha \text{KL}\left[p\left(y \mid g_\theta(h) \right), p\left(y \mid g_\theta(h+\delta) \right)\right]
    \text{where }\delta :=\underset{\|\delta\|_{2} \leq \epsilon}{\arg\max } \quad \text{KL}\left[p\left(y \mid g_\theta(h)\right), p\left(y \mid g_\theta(h +\delta) \right)\right].$
    \item \textbf{\udp}: Instead of maximizing the loss, uncertainty-driven perturbations (UDP)~\citep{pagliardini22_udp} adversarially maximize a model's estimated uncertainty. UDPs have been shown to be effective in decreasing simplicity bias and improving generalization in non-adaptation settings. Formally, they can be defined as: $ \delta_u=\underset{\|\delta\|_{2} \leq \epsilon}{\arg \max } \mathcal{H}(g_\theta(h)+\delta), \text{where }\mathcal{H}(g_\theta(h))=-\sum_{c \in C} \hat{y}_c \log \hat{y}_c,$ (e.g., entropy of predictions).
    \item \textbf{\soup}: Inspired by \citet{wortsman22_modelsoup}, we train multiple, sparse, linear probes jointly and then take the average of their weights (aka soup) as the learned \lp~for subsequent \ft. While soups of large models improve generalization by combining models from the same low-error basin, we consider sparse classifiers soups as an alternative strategy which seeks to average diverse decision rules, to avoid relying upon a single set of simple features. Formally, given $k$ classifiers, we minimize $\min_{\theta_{1\dots k}} \frac{1}{k}\sum_{i=1}^k \mathcal{L}_{\text{CE}}(g_{\theta_i}(h),y)$ and let $g_{\bar{\theta}} = \frac{1}{k}\sum_i^k \theta_i$ for the \ft~step.
    
\end{itemize}

\paragraph{\textbf{Empirical Evaluation of Hardness Promoting Augmentations.}} We evaluate the effectiveness of the above \lp~variants, which we collectively refer to as ``hardness-promoting'', in mitigating the simplicity of bias of \lpft. We make the following observations (see Fig.  \ref{fig:dominoes}). 

Across all correlation strengths, we find that using the modified hardness-promoting \lp s during \lpft~(aka hp-\lpft) improves the Rand. OOD Accuracy over vanilla \lpft ($\ge$ 2\%) and \ft($>20\%$). This clearly indicates that hp-\lpft~is indeed effective in decreasing reliance on simple features, potentially also leading to improved safety. Furthermore, with the exception of \soup+\ft, hp-\lpft~also performs better than vanilla \lpft~on Corr. OOD accuracy. Vanilla \ft~does outperform all \lpft~protocols in this setting, but this is due to reliance upon simple features. Lastly, we observe that with respect to Corr. ID Accuracy that hp-\lpft~improves performance at low correlation strength, but slightly loses performance at higher correlation strengths. This is not entirely unexpected as \ft's reliance upon simple features will be useful in the correlated setting. Given that hp-\lpft~is able to reduce reliance upon simple features in this controlled setting, we next evaluate whether these modified protocols are beneficial in improving the performance of \lpft~on real datasets.  