\begin{table}[!t]
\setlength\tabcolsep{5pt}
\renewcommand{\arraystretch}{1.3}
\small
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c | ccccccccccc } 
\multicolumn{1}{c}{}& \multicolumn{2}{c}{\hlb{\normalsize Generalization}} & \multicolumn{3}{c}{\hlb{\normalsize Robustness}} & \multicolumn{4}{c}{\hlb{\normalsize Calibration}} & \multicolumn{1}{c}{\hlb{\normalsize Anomaly Det.}} & \multicolumn{1}{c}{\hlb{\normalsize Rep. Similarity}} \\ \cmidrule(lr){1-1} \cmidrule(lr){2-3} \cmidrule(lr){4-6}  \cmidrule(lr){7-10}  \cmidrule(lr){11-11} \cmidrule(lr){12-12} 

\multirow{2}*{Protocol} &  ID & OOD & C & $\overline{\text{C}}$ & Adv. & ID & C & $\overline{\text{C}}$ & OOD. & Out-of-Class & ID \\
& \textcolor{gray}{Acc.} & \textcolor{gray}{Acc.} & \textcolor{gray}{Acc.} & \textcolor{gray}{Acc.} & \textcolor{gray}{Acc.} & \textcolor{gray}{1-RMS} &  \textcolor{gray}{1-RMS} & \textcolor{gray}{1-RMS} & \textcolor{gray}{1-RMS} & \textcolor{gray}{AUROC}  & \textcolor{gray}{CKA}  \\ 
\hline
\lp              & 0.9297 & 0.9083 & 0.8532 & 0.7491 & 0.7077 & 0.9794 & 0.9006 & 0.9007 & 0.9301 & 0.9623 & 0.0668 \\
\midrule
\lp + soup-5        & 0.9220 & 0.9151 & 0.8315 & 0.7432 & 0.7050 & 0.9598 & 0.9232 & 0.9279 & 0.9623 & 0.9665 & 0.1399 \\
\lp + soup-10      & 0.9156 & 0.9135 & 0.8183 & 0.7344 & 0.6985 & 0.9476 & 0.9221 & 0.9271 & 0.9732 & 0.9602 & 0.1778 \\
\lp + soup-20      & 0.9069 & 0.9064 & 0.8065 & 0.7216 & 0.6885 & 0.9279 & 0.9129 & 0.9191 & 0.9714 & 0.9484 & 0.2617 \\
\midrule
\lp + udp-0.005    & 0.9299 & 0.9092 & 0.8533 & 0.7494 & 0.7079 & 0.9794 & 0.9009 & 0.9003 & 0.9312 & 0.9614 & 0.0822 \\
\lp + udp-0.01    & 0.9298 & 0.9097 & 0.8535 & 0.7495 & 0.7083 & 0.9795 & 0.9007 & 0.9006 & 0.9316 & 0.9616 & 0.0880 \\
\lp + udp-0.02     & 0.9294 & 0.9108 & 0.8538 & 0.7497 & 0.7088 & 0.9789 & 0.9012 & 0.9014 & 0.9335 & 0.9631 & 0.1017 \\
\lp + udp-0.1      & 0.9238 & 0.9218 & 0.8377 & 0.7488 & 0.7111 & 0.9801 & 0.9154 & 0.9216 & 0.9517 & 0.9645 & 0.1478 \\
\midrule
\lp + vat-0.001    & 0.9298 & 0.9091 & 0.8533 & 0.7493 & 0.7078 & 0.9801 & 0.9014 & 0.9012 & 0.9325 & 0.9614 & 0.0784 \\
\lp + vat-0.01     & 0.9295 & 0.9094 & 0.8531 & 0.7494 & 0.7080 & 0.9800 & 0.9039 & 0.9040 & 0.9342 & 0.9632 & 0.0837 \\
\lp + vat-0.1      & 0.9275 & 0.9106 & 0.8493 & 0.7481 & 0.7087 & 0.9581 & 0.9191 & 0.9246 & 0.9589 & 0.9598 & 0.1528 \\
\midrule
\ft             & 0.9724 & 0.8761 & 0.9218 & 0.8131 & 0.8074 & 0.9577 & 0.8429 & 0.8418 & 0.8855 & 0.9138 & 0.9317 \\
\midrule
\lp+\ft         & 0.9692 & 0.9387 & 0.9195 & 0.8106 & 0.7736 & 0.9451 & 0.8034 & 0.7743 & 0.9026 & 0.8949 & 0.5349 \\
\midrule
(\lp+soup-5) +\ft    & 0.9685 & 0.9417 & 0.9210 & 0.8136 & 0.7787 & 0.9385 & 0.8079 & 0.7765 & 0.9102 & 0.8974 & 0.5315 \\
(\lp+soup-10) +\ft   & 0.9681 & 0.9411 & 0.9220 & 0.8178 & 0.7824 & 0.9382 & 0.8119 & 0.7796 & 0.9072 & 0.8933 & 0.5521 \\
(\lp+soup-20) +\ft  & 0.9677 & 0.9395 & 0.9213 & 0.8164 & 0.7837 & 0.9385 & 0.8107 & 0.7817 & 0.9070 & 0.8964 & 0.5411 \\
\midrule
(\lp+udp-0.005) +\ft  & 0.9677 & 0.9297 & 0.9142 & 0.8104 & 0.7710 & 0.9422 & 0.8024 & 0.7718 & 0.8942 & 0.8916 & 0.6428 \\
(\lp+udp-0.01) +\ft   & 0.9677 & 0.9359 & 0.9195 & 0.8098 & 0.7721 & 0.9417 & 0.8029 & 0.7732 & 0.9019 & 0.8999 & 0.4239 \\
(\lp+udp-0.02) +\ft  & 0.9687 & 0.9349 & 0.9195 & 0.8136 & 0.7724 & 0.9437 & 0.8067 & 0.7736 & 0.8994 & 0.8981 & 0.5015 \\
(\lp+udp-0.1) +\ft    & 0.9688 & 0.9423 & 0.9242 & 0.8174 & 0.7811 & 0.9408 & 0.8130 & 0.7815 & 0.9072 & 0.9064 & 0.4496 \\
\midrule
(\lp+vat-0.001)+\ft & 0.9681 & 0.9366 & 0.9180 & 0.8111 & 0.7727 & 0.9422 & 0.8033 & 0.7732 & 0.9013 & 0.8962 & 0.5904 \\
(\lp+vat-0.01)+\ft  & 0.9689 & 0.9366 & 0.9168 & 0.8121 & 0.7766 & 0.9455 & 0.8062 & 0.7791 & 0.9013 & 0.8918 & 0.5687 \\
(\lp+vat-0.1)+\ft  & 0.9692 & 0.9402 & 0.9207 & 0.8127 & 0.7743 & 0.9420 & 0.8068 & 0.7734 & 0.9083 & 0.8978 & 0.4398 \\
\bottomrule
\end{tabular}%
}
\caption{\textbf{CIFAR10 with Resnet101/SimCLR Pretrained Model.} We see that with a larger model, and different pretraining method, our proposed variants still have some benefits. We note that the baseline performance is also improved as a result of a more larger pretrained model.}
% \vspace{-10pt}
\label{tab:rn101-cifar10}
\end{table}
