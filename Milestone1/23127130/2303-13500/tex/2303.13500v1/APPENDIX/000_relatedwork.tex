For a comprehensive overview of transfer learning, please see the surveys of~\citeauthor{zhuang21_transferlearningsurvey} and~\citeauthor{Pan10_TransferSurvey}. Here, we discuss a few directly works directly relevant to our own.

Recently, \citeauthor{Kumar22_FinetuningDistorts} demonstrated that learning probing prior to fine-tuning (e.g., \lpft) can improve both in-distribution and out-of-distribution performance when transferring to a downstream task given a highly expressive, pretrained model. They demonstrated that \ft~only modifies features in the ID representation subspace and not in other directions, which can lead higher OOD error as direction outside the ID subspace are necessary for OOD generalization. However, by initializing \ft with a trained linear probe, feature distortion can be decreased since this initialization is closer to optimal model, and thus requires less distortion in ID subspace, preserving the expressiveness of the original model. Concurrently, \citeauthor{Kirichenko22_LastLayerRetrain} demonstrated that models are able to learn both core features and spurious features. However, classifiers can rely upon spurious features, harming performance on minority groups. To reduce the reliance on spurious features, they propose to retrain the classifier on a small amount of ``re-weighting" data, that allows the model to leverage the core features instead of the spurious features. 

Other modifications and heuristics have also been proposed to improve fine-tuning, including side-tuning~\citep{Zhang19_Sidetuning}, which tunes a small secondary network that is then combined with the original model, using larger/smaller learning rates for the classifier, as well as regularization-based methods \citep{Jiang20_SMART}. We focus on the \lpft~protocol, as it is principled and achieves strong OOD performance. 

Additionally, several works have studied properties of the model that influence the effectiveness of transfer learning~\citep{Azizpour16_FactorsTransfer,Huh16_ImageNetGoodTransfer,Kornblith19_BetterImageNetTransferBetter,Lee23_surgicalFT,evci22_Head2Toe,Lee23_DivDis,Izmailov22_FeatLearningSpurious,Lubana23_ModeConnectivity, Rame22_DiWA,Trivedi22_GraphSSL}, including the robustness of pretrained features ~\citep{Salman20,Utrera21_AdvTrainedImageNets}. While the connection between adversarial training and improved feature representations~\citep{Zhu21_FeaturePurification,Kaur19_PerceptuallyAligned} has been studied, we use virtual adversarial training during \lp~to learn a better classifier that is less reliant upon simple features, and we do not use an adversarially trained feature extractor. 
Finally, we note that while we are, to the best of our knowledge, the first to consider this holistic evaluation of safety and generalization in the context of transfer learning with highly expressive pretrained models, \citeauthor{Hendrycks21_PixMix} have considered the trade-offs induced by different data augmentation strategies~\citep{Yun19_CutMix,Devries17_cutout,Hendrycks20_AugMix,Cubuk18_AutoAugment,Cubuk20_RandAug} on safety metrics in supervised learning. We emphasize that while our evaluation is similar, that our work focuses on a different context and contains an additional layer of complexity as we consider the interaction between adaptation protocols, generalization behavior and safety performance.