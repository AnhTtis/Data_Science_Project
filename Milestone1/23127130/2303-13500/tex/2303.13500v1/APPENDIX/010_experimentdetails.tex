Please see the \href{https://github.com/pujacomputes/23-ICLR-Adaptation.git}{https://github.com/pujacomputes/23-ICLR-Adaptation.git} for training details. In brief, we performed grid-search to find the best parameters, which are as follows. 
For CIFAR-10 and CIFAR-100, we train only the classifier for 200 epochs with LR=30 during \lp. For \ft, the entire model is trained for 20 epochs with LR=1e-5. For \lpft, the model's classifier is initialized with the solution found by \lp, and then it is fine-tuned for 20 epochs. A grid-search was conducted to determine the LR for \lp~and \ft. For Domain-Net Experiments, we use 200 epochs with LR=30 during \lp. For \ft, the entire model is trained for 20 epochs with LR=3e-4. For \lpft, the model's classifier is initialized with the solution found by \lp, and then it is fine-tuned for 20 epochs, using LR=3e-7. Furthermore, following \citeauthor{Kumar22_FinetuningDistorts}, we freeze the batchnorm layers during \lpft. A  CLIP~\citep{Radford21_Clip} pretrained ResNet-50 is used for the DomainNet experiments, while a MoCoV2~\citep{He20_MoCo} is used for all CIFAR experiments. We use augmentation functions from timm~\citep{rw2019timm} and compute CKA scores using the packaged provided by  \href{https://github.com/AntixK/PyTorch-Model-Compare}{torch-cka}. When using augmented protocols, the same LRs are used. Note, all results were obtained by averaging over 3 seeds. We consider model soups of sizes 5,10,20, tune $\epsilon$ in 0.005, 0.01, 0.02 and 0.1 for UDP, and $\alpha$ in 0.001, 0.01, 0.1 for VAT. For CIFAR-MNIST results, LP is done for 100 epochs, and FT is done for 20 epochs.

\subsection{Motivation for Hardness-Promoting Variants}\label{sup:motivation} 
We selected UDP~\citep{pagliardini22_udp}, VAT~\citep{Miyato17_Vat}, and model-soups~\citep{wortsman22_modelsoup} as simplicity bias mitigation strategies due to their effectiveness and ease of use. We emphasize, however, that our findings are not specific to the choice of a given mitigation strategy and we expect that advancements in such strategies will further improve the effectiveness of our proposed \lpft variants. At present, the selected strategies are strong, representative mitigations that we have confirmed are effective at mitigating simplicity bias in the adaptation context using the synthetic dominoes dataset in Sec. \ref{sec:simplicity}.

We conceptually justify each strategy here: 
\begin{itemize}
    \item UDP is designed to help mitigate simplicity bias by learning by a large margin classifier, opposed to a narrow margin classifier that relies upon simple features. As noted by \citet{Shah20_SimplicityBias}, such narrow margin classifiers are sensitive to small perturbations and the simple features supporting the decision boundary may not be discriminative under distribution shifts. By maximizing uncertainty (instead of loss) to create adversarial perturbations, UDP is able to learn a maximum-margin classifier that is better able to handle such shifts. Notably, to create such a maximum-margin classifier, the model will necessarily learn more complex features;
    \item We use virtual adversarial training (VAT) to help avoid reliance upon simple features, as VAT enforces distribution smoothness so that classifiers become robust in some epsilon neighborhood around the input. We note that we are performing this training in the hidden representation space, so perturbations correspond may be altering high-level semantics. To maintain strong performance under such high-level perturbations, the model should learn to rely upon more complex features, and learn a better margin classifier;
    \item We use model-soups so that we may learn a set of classifiers that rely upon disjoint sets of features. By learning a set of diverse classifiers, we are able to average classifiers that have learned to rely upon different features, instead of becoming overly reliant upon a single simple feature. In future work, we intend to build a theoretical framework that helps us better justify these interventions and create new ones. 
\end{itemize}

\subsection{Applying Simplicity Bias Mitigation Strategies to Fine-Tuning Step.}\label{sup:ablation}
\begin{wrapfigure}{r}{0.5\textwidth}
\begin{center}
\includegraphics[width=0.5\textwidth]{FIGS_v3/Appendix-Variants.pdf}
\end{center}
\caption{\textbf{Applying Mitigation Strategies to \ft.} We create \ft~variants of our \lp~mitigation strategies and evaluate them on the synthetic dominoes dataset. We see that \ft~variants lose performance with respect to \lp~variants, indicating that interventions must be undertaken during the \lp~step as originally proposed.}
\label{sup:variants}
\vspace{-.2in}
\end{wrapfigure}
To demonstrate that simplicity bias mitigation strategies must be applied during the \lp~step of \ft~for maximum effectiveness, we conduct the following additional experiment.

\textit{Setup.} We evaluate two additional protocols where \texttt{VAT} and \texttt{UDP} are applied only during the \ft~step, ($\lp \texttt{+}\ft(\texttt{VAT})$, and $\lp \texttt{+} \ft(\texttt{UDP})$), on the synthetic dominoes dataset. We plot the results for Randomized OOD Accuracy in Fig. \ref{sup:variants}. 

\textit{Results.} Here, we see that, across three different correlation ratios, \ft~variants lose performance with respect to the \lp~mitigation variants. Notably, \lp \texttt{+} \ft~(\texttt{UDP})~loses up to 4\% performance with respect to \lp (\texttt{UDP})\texttt{+} \ft. While performance drops are not as large for \texttt{VAT}, we nonetheless see that \lp \texttt{+} \ft (\texttt{VAT})~loses performance with respect to \lp (\texttt{VAT})\texttt{+} \ft.

Our results in Fig. \ref{sup:variants} support our conceptual argument that mitigation strategies must be undertaken during the \lp~step to ensure that subsequent \ft~is in a direction that preserves complex features; applying mitigation strategies during \ft~may be too late to avoid simplicity bias. We note that applying mitigation strategies during \ft, in addition to \lp, may further improve performance, and we will add these variants in the final version. We did not include a \ft~soup variant as it would be prohibitively expensive to train and average large soups of entire models (instead of classifiers). This highlights the computational efficiency of implementing mitigation strategies in the \lp~step itself. 