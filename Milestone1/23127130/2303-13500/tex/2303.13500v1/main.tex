
\documentclass{article} % For LaTeX2e
\usepackage{iclr2023_conference,times}
  
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm,multirow,xcolor}
\usepackage{wrapfig,soul} %subfig
\usepackage{adjustbox}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
% \usepackage{colortbl}
% \usepackage[table]{xcolor}
% \usepackage{nicematrix}
\usepackage{booktabs}

\usepackage{enumitem}
\colorlet{sb}{cyan!30}
\DeclareRobustCommand{\hlb}[1]{{\sethlcolor{sb}\hl{#1}}}
\DeclareRobustCommand{\hlb}[1]{{\textit{\textbf{#1}}}}
% \title{Exploring the Design of Adaptation Protocols for Improved Generalization and Machine Learning Safety}

\title{A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias} 
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Puja Trivedi \\ 
CSE Department \\
University of Michigan\\
\texttt{pujat@umich.edu} \\
\And
Danai Koutra \\ 
CSE Department \\
University of Michigan\\
\texttt{dkoutra@umich.edu} \\
\And
Jayaraman J. Thiagarajan\\
Center of Applied Scientific Computing \\
Lawrence Livermore Natl. Laboratory \\
\texttt{jjayaram@llnl.gov} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\ft}{\texttt{FT}}
\newcommand{\lp}{\texttt{LP}}
\newcommand{\udp}{\texttt{LP(UDP)}}
\newcommand{\vat}{\texttt{LP(VAT)}}
\newcommand{\soup}{\texttt{LP(Soup)}}
%\newcommand{\lpft}{\texttt{LP+FT}}
\newcommand{\pt}[1]{\textcolor{blue}{#1}}
% \newcommand{\lpft}{\lp\texttt{+}~\ft}
\newcommand{\lpft}{\texttt{LP+FT}}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\begin{abstract}
Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe \textit{and} effective transfer learning.
Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). 
In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed.
However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.\let\thefootnote\relax\footnote{Correspondence to \texttt{pujat@umich.edu}.} 
\end{abstract}

\section{Introduction}
\input{PAGES_v3/010_introduction.tex}

\section{Related Work and Background}\label{sec:related_work}
\input{PAGES_v3/020_relatedwork.tex}

\section{Joint Analysis of Protocol Safety and Generalization}
\input{PAGES_v3/030_experimentalprotocol.tex}\label{sec:tradeoffs}

\section{Mitigating Simplicity Bias \& Feature Distortion for Safe Adaptation}\label{sec:simplicity}
\input{PAGES_v3/040_simplicitybias.tex}

\section{Evaluating Generalization and Safety of the \lpft~Family}\label{sec:hardness}
\input{PAGES_v3/050_hardnesspromoting.tex}

\section{Conclusion}
\input{PAGES_v3/060_conclusion.tex}

\newpage
\subsubsection*{Acknowledgments}
We thank Ekdeep Singh Lubana for several helpful discussions during the course of this project. This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344, Lawrence Livermore National Security, LLC.and was supported by the LLNL-LDRD Program under Project No. 21-ERD-012. It was also partially supported by the National Science Foundation under CAREER Grant No.~IIS 1845491. PT began this work as an intern at Lawrence Livermore National Laboratory.
\bibliography{main}
\bibliographystyle{iclr2023_conference}
\newpage
\appendix
\section*{Appendix}
\section{Additional Related Work}\label{sup:relatedwork}
\input{APPENDIX/000_relatedwork}
\section{Experimental Details}
\input{APPENDIX/010_experimentdetails}\label{sup:experimentaldetails}
\section{Additional Results}\label{sup:additional_results}
Below, we include results corresponding to different hyperparameters (number of souped classifiers, $\alpha$ for vat, and $\delta$ for udp). 

\input{APPENDIX/020_tables}
\end{document}
