\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Customized by Haoxuan
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{makecell}
\usepackage{color, colortbl}
\usepackage{tabu}
\usepackage{xcolor}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

% Customized by Haoxuan
\newcommand{\cocadraw}{CoBIT}
\newcommand{\cocadrawbase}{CoBIT-Base}
\newcommand{\cocadrawlarge}{CoBIT-Large}
\newcommand{\bfcocadraw}{\textbf{CoBIT}}
\newcommand{\todohy}[1]{\textcolor{blue}{\small{\bf TODO(Haoxuan): #1}}}
% \newcommand{\red}[1]{\textcolor{red}{ #1}}
% \newcommand{\redbf}[1]{\textcolor{red}{\underline{ #1}}}
\newcommand{\red}[1]{\underline{#1}}
\newcommand{\redbf}[1]{\textbf{\underline{#1}}}
\definecolor{lg}{gray}{0.9}
\definecolor{lb}{gray}{0.35}
\definecolor{llb}{gray}{0.6}
\newcommand{\lb}[1]{\textcolor{lb}{ #1}}
\newcommand{\llb}[1]{\textcolor{llb}{ #1}}
\newcommand\blfootnote[1]{%
\begingroup
\renewcommand\thefootnote{}\footnote{#1}%
\addtocounter{footnote}{-1}%
\endgroup
}

\title{\cocadraw: A Contrastive Bi-directional Image-Text Generation Model}
\author{{\fontsize{11}{10} \selectfont Haoxuan You$^{1*}$,  Mandy Guo$^2$, Zhecan Wang$^1$, Kai-Wei Chang$^3$, Jason Baldridge$^2$, Jiahui Yu$^2$}   \\
{\fontsize{11}{10} \selectfont $^1$Columbia University}\\
{\fontsize{11}{10} \selectfont $^2$Google Research}\\
{\fontsize{11}{10} \selectfont $^3$UCLA}\\
{\tt\small haoxuan.you@cs.columbia.edu, jiahuiyu@google.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\begin{document}
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=1.03\linewidth]{./figs/cocadraw_fig1_v13.pdf}
    \vspace{-6mm}
    \captionof{figure}{We propose \cocadraw{}, a unicoder-decoder architecture, pre-trained jointly on contrastive loss, image-to-text generation loss and text-to-image generation loss. \cocadraw{}  can address a variety of vision and vision-language tasks 
    % across text-to-image generation, multimodal retrieval, multimodal understanding, image understanding 
    in the manner of both zero-shot and fine-tuning. The right-hand side displays the zero-shot generated images by \cocadraw{} given novel prompts, and the zero-shot generated captions by \cocadraw{} given the previously generated images as input. }
    \label{fig:intro}
\end{center}%
}]

% \begin{figure*}[t]
%   \includegraphics[width=1\linewidth]{./figs/cocadraw_fig1_v4.pdf}
%   \caption{\todohy{A rough version, will refine later}}
%   \label{fig:intro}
% \end{figure*}

% \begin{document}
% %%%%%%%%% TITLE
% \title{\cocadraw: Contrastive Bi-directional Image-Text Generator}
% % CoLI: Contrastive Bi-directional Language-Image Generator 
% % CULI: Contrastive Dual-directional Language-Image Generator 
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\blfootnote{$^*$ This work was done when Haoxuan was an intern at Google. }

%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-4mm}
  The field of vision and language has witnessed a proliferation of pre-trained foundation models. Most existing methods  are independently pre-trained with contrastive objective like CLIP, image-to-text generative objective like PaLI, or text-to-image generative objective like Parti. However, the three objectives can be pre-trained on the same data, image-text pairs, and intuitively they complement each other as contrasting provides global alignment capacity and generation grants fine-grained understanding. In this work, we present a \textbf{Co}ntrastive \textbf{B}i-directional \textbf{I}mage-\textbf{T}ext generation model (\bfcocadraw), which attempts to unify the three pre-training objectives in one framework.   Specifically, \cocadraw{} employs a novel unicoder-decoder structure, consisting of an image unicoder, a text unicoder and a cross-modal decoder. The image/text unicoders can switch between encoding and decoding in different tasks, enabling flexibility and shared knowledge that benefits both image-to-text and text-to-image generations. \cocadraw{} achieves superior performance in image understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE) and text-based content creation, particularly in zero-shot scenarios. For instance, 82.7\% in zero-shot ImageNet classification, 9.37 FID score in zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.
  

%   In this work, we present \textbf{Co}ntrastive Bi-directional \textbf{I}mage-\textbf{T}ext Generator (\bfcocadraw), a simple and unified model family, for learning generic representations transferable to various vision-language and vision-only tasks.  The proposed \cocadraw{} is pre-trained with a joint of three loss functions: contrastive loss, image-to-text generation loss and text-to-image generation loss. More specifically, unlike conventional encoder-decoder model, \cocadraw{} employs a novel unicoder-decoder structure, consisting of an image unicoder, a text unicoder and a cross-modal decoder. Image/text unicoders can switch between encoding and decoding, with reusing the same Transformer parameters. The introduced unicoders enable flexibility and the  common knowledge shared in encoding/decoding can facilitate both image-to-text and text-to-image generations. In experiments, pre-trained on image-text datasets,  \cocadraw{} achieves superior performance in image understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE) and text-based content creation, particularly in zero-shot scenarios. For instance, 82.7\% in zero-shot ImageNet classification, 9.37 FID score in zero-shot text-to-image generation and 44.8 CIDEr in zero-shot captioning.
   
\end{abstract}



%%%%%%%%% BODY TEXT
\section{Introduction}

% General-purpose foundation models pre-trained with large-scale data are now commonplace in the field of deep learning; examples include T5~\cite{raffel2020exploring}, BERT~\cite{devlin2018bert}, GPT~\cite{radford2019language,brown2020language} in natural language processing and CLIP ~\cite{radford2021learning}, GPV ~\cite{gupta2022towards}, BEiT ~\cite{bao2021beit,wang2022image} in computer vision.

For vision-and-language learning, foundation model development has been primarily dedicated to the following directions. (1)  Guiding visual representation pre-training using correlated textual descriptions, using contrastive losses ~\cite{radford2021learning, yao2021filip, mu2022slip, you2022learning}. In this line of work, two unimodal encoders encode image and text, respectively. Such pre-trained \textit{dual-encoder models} can support cross-modal matching downstream tasks, including zero-shot image classification, image-text retrieval, and more.  Also, the visual encoders exhibit strong representational capacity for core image processing tasks, especially classification. (2) Pre-training \textit{image-encoder \& text-decoder models} with Image-to-Text  (I2T) token generation loss \cite{wang2022git, chen2022pali, wang2021simvlm, alayrac2022flamingo}. Pre-trained generative models of this form have demonstrated strong capabilities in vision-and-language tasks such as Visual Question Answering (citation needed--Agrawal et al, IIRC) and image captioning. (3) Pre-training \textit{text-encoder \& image-decoder models} with Text-to-Image (T2I) (visual) token generation loss \cite{ramesh2021zero, yu2022scaling, chang2022maskgit, chang2023muse}. In such work, raw images are tokenized/quantized into a sequence of image tokens by VQ-VAE/GAN models \cite{NIPS2017_7a98af17, yu2021vector}, such that the text-to-image generation problem can be formulated as a standard sequence-to-sequence task. 
% During inference, the predicted image tokens are fed into the decoder of VQ-VAE model to reconstruct original image in pixel space. 
The resulting models have shown strong results in downstream text-based image generation tasks.

Most of above-mentioned models are trained independently with only one of these three objectives. Only a few are trained with a combination of two objectives; e.g. CoCa ~\cite{yu2022coca} combines contrastive learning and image-to-text generation. OFA \cite{wang2022ofa} and UnifiedIO \cite{lu2022unified} ensemble image-to-text generation and text-to-image generation. However, it's worth noting that the three objectives share the same pre-training data source: image-text pairs. Intuitively, the knowledge they learn should complement each other: for example, contrastive learning drives high-level multimodal matching, whereas image/text generation requires more fine-grained image-text representations. Last but not least, joint pre-training enables partially sharing of the computational graphs, and thus can be optimized and deployed more efficiently. 


In this work, we aim to unify the three learning objectives: cross-modal contrastive learning, image-to-text generation and text-to-image generation, and thus consolidate the strengths of them in one framework.
% consolidate the generalizability brought by them in one foundation framework. 
We present a simple and unified \textbf{Co}ntrastive \textbf{B}i-directional \textbf{I}mage-\textbf{T}ext generation model (\bfcocadraw), which consists of an \textit{image unicoder} and a \textit{text unicoder}, as well as a cross-attention decoder. The proposed image/text unicoder structurally is Transformer, but they can switch in-between two modes: unimodal image/text encoding and decoding, which reuse the same set of Transformer parameters and only differ in input embeddings and attention masks. As shown in Fig. \ref{fig:diagram}, when optimizing contrastive objective, image unicoder and text unicoder work as two encoders. When optimizing text/image generation loss, image/text unicoder extracts features in encoding mode and text/image unicoder works in autoregressive decoding mode, then the cross-attention decoder will let autoregressive text/image features cross-attend to encoded image/text feature, serving as a fuser and generator. Each unicoder efficiently shares the knowledge between encoding and decoding, and therefore can jointly improve both T2I and I2T generation. In such a way, all three pre-training paradigms are unified in our framework. 


\begin{figure*}[t]
  \includegraphics[width=\linewidth]{./figs/cocadraw_diagram.pdf}
  \vspace{-5mm}
  \caption{(a): Overview of \cocadraw{} pre-training pipeline; (b): When optimizing contrastive objective, image unicoder and text unicoder work as two encoders; (c) and (d): When optimizing image/text generation loss, text/image unicoder extracts features in encoding mode and image/text unicoder works in autoregressive decoding mode, then the cross-attention decoder will let autoregressive image/text features cross-attend to encoded text/image feature.
%   Figure (b)(c)(d) provide the details of  contrastive objective, image-to-text objective and text-to-image objective, respectively. 
}
  \label{fig:diagram}
  \vspace{-5mm}
\end{figure*}

\cocadraw{} is trained on both large-scale noisy web-crawled image-text data and image annotation data by treating labels as text. The pre-trained \cocadraw{}  subsumes strong zero-shot and transferable capacities of unimodal visual understanding, image-text matching, image-text understanding and text-to-image generation. For example, \cocadraw{} achieves 82.7\% accuracy in zero-shot ImageNet classification, 9.37 FID in zero-shot text-to-image generation, 44.8 CIDEr score in zero-shot image-to-text captioning. After fine-tuning, \cocadraw{} further achieves 86.44\% linear probing accuracy on ImageNet, 4.62 FID on text-to-image generation, and 78.3 VQA score.

\section{Related Work}
\noindent\textbf{Learning Visual Representation from Text.} Recent works studied pre-training a visual backbone with supervision from paired text data. CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} are two prominent large-scale models that used contrastive learning to pull together paired image-text embeddings while repelling unpaired ones. The resulting models exhibit strong visual zero-shot capacity as well as transferable visual representations. Florence \cite{yuan2021florence}, BASIC \cite{pham2021combined} and LiT \cite{zhai2022lit} further scale both datasets and models. FILIP \cite{yao2021filip} proposes to employ local token features from image and text for fine-grained contrastive learning. MS-CLIP \cite{you2022learning} and CLIPPO \cite{tschannen2022image} study sharing the model parameters between vision and text.  


\noindent\textbf{Vision-Language Pre-training. } Another line of research focuses on learning a strong joint multimodal embedding of vision and language through pre-training. Some pre-train with mask-reconstruction loss \cite{li2019visualbert, wang2022image, li2022blip, chen2019uniter, shen2021much, li2021align}, \ie, mask partial image and text tokens in input and require the model to predict the masked tokens. Others pre-train models by generating text autoregressively  \cite{wang2021simvlm, chen2022pali, wang2022git,alayrac2022flamingo}. Both exhibit strong performance in downstream vision-language understanding tasks, such as VQA \cite{antol2015vqa} and captioning.

\noindent\textbf{Text-to-Image Generation.} Text-guided image creation is a challenging problem that has attracted intense interest in the past two years. Two families of methods are widely studied: diffusion-based and token-based. Diffusion-based models \cite{rombach2022high, saharia2022photorealistic, ramesh2022hierarchical} are based on a process that iteratively adds noise to images and then learns to reverse the noising process, while conditioning on textual descriptions of the image. With token-based methods, raw image are quantized into image tokens by an image tokenizer; then, given text input, Transformer models are used to predict image tokens autoregressively in a manner similar to machine translation \cite{ramesh2021zero, yu2022scaling} or by iteratively predicting image tokens in parallel\cite{chang2022maskgit, chang2023muse}.  

As these three broad lines of research have demonstrated great transferable ability to various downstream tasks, there have been many efforts to unify some of them \cite{yu2022coca, wang2022ofa, lu2022unified, zhang2021ernie, kim2022verse, huang2021unifying}. 
% For example,  CoCa ~\cite{yu2022coca} unifies contrastive learning and image-to-text generation. OFA \cite{wang2022ofa}, UnifiedIO \cite{lu2022unified} and L-Verse \cite{kim2022verse} unify image-to-text generation and text-to-image generation. 
%Nevertheless, unifying all three capacities remains unexplored. 
Our work, \cocadraw, serves as the first effort to integrate contrastive loss, image-to-text generation and text-to-image loss under one unified pre-training framework. 

\section{The \cocadraw{} Model}
We begin with describing input processing and then present the model architecture, which includes proposed unicoder module that shares the merit of both unimodal encoding and decoding. Finally, we provide a detailed explanation of the pre-training.

\subsection{Input}
To cover various tasks,
% including text generation, pixel-level image generation, and image understanding,
our model supports three types of input: text tokens, discrete image tokens, and raw images.

\noindent\textbf{Text Tokens.} Following the default process in past work ~\cite{raffel2020exploring, jia2021scaling, yu2022coca}, we tokenize text inputs using a SentencePiece model with a 64k vocabulary trained on the sampled pre-training datasets. The maximum text token length is 64. 

\noindent\textbf{Discrete Image Tokens.} \cocadraw{} generates images in an autoregressive manner, which requires tokenizing 2D images into a sequence of image tokens \cite{ramesh2021zero, ding2021cogview, ding2022cogview2, gafni2022make, yu2022scaling}. Following Parti~\cite{yu2022scaling}, we employ a pre-trained and frozen ViT-VQGAN \cite{yu2021vector} as the tokenizer. Specifically, each 256$\times$256 image is tokenized into a 32$\times$32 grid of image tokens, with 8192 image token classes in the codebook. We append the codebook to the text vocabulary as additional tokens. In inference, to generate images, we decode the image tokens one-by-one and feed them into the decoder in ViT-VQGAN to reconstruct the raw image. 
\label{sec:model_input}

\noindent\textbf{Raw Image.} For the purpose of image understanding and image-text understanding tasks, we also input raw images because it preserves the original information in pixels. Then each image is divided into non-overlapped patches following the de facto process in ViTs. In default, unless specified, the image resolution is 288x288 and the patch size is 18x18.

\subsection{Architecture}
\label{sec:arch}
 As shown in Fig. \ref{fig:diagram}, \cocadraw{} is composed of one image unicoder, one text unicoder and one cross-attention decoder. We term them unicoders because they can act as either encoders or decoders, depending on role they play for each task. The incorporation of text/image unicoder is inspired by ~\cite{dong2019unified, bao2020unilmv2, zhou2020unified}, which demonstrated that one Transformer model can perform both bidirectional encoding for understanding tasks and autoregressive decoding for generation tasks. In our scenario, compared with plain image/text encoders, unicoders in decoding mode can take advantage of the common knowledge shared with encoding to produce unimodal autoregressive features as a decent prior for cross-modal generative objective. Experimental ablation also validates that unicoders boost both T2I generation and multimodal understanding. 

\begin{table*}[t]
\small
\setlength\tabcolsep{10pt} % 
\renewcommand{\arraystretch}{0.9} % 
\centering
\begin{tabular}{lccccccc}
\toprule 
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Image Unicoder} & \multicolumn{2}{c}{Text Unicoder} & \multicolumn{2}{c}{Cross-modal Decoder} &  \multirow{2}{*}{Total Params}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
 & Layers & Dims & Layers & Dims & Layers & Dims & \\
 \midrule
 \cocadrawbase & 12 & 768 & 12 & 768 & 18 & 1024 & 626M  \\
% 495M and 943M if only counting Transformer weight.
% \midrule
 \cocadrawlarge & 20 & 1024 & 12 & 1024 & 30 & 1024 & 1091M  \\
    \bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Size variants of \cocadraw. }
\label{tab:model_scale}
\vspace{-6mm}
\end{table*}

\noindent\textbf{Image Unicoder.}  Recently, Vision Transformers (ViT) \cite{dosovitskiy2020image, touvron2021training, liu2021swin} has been established as the strongest approach for image feature 
\textit{encoding}. As \textit{decoders}, Transformers are used in autoregressive image token generation \cite{ramesh2021zero, gafni2022make, yu2022scaling}. We combine these two functionalities into a single image unicoder.  The image unicoder has two working modes: (1) In the encoding mode, following ViT, each 2D patch in the raw image is projected into  a feature vector by a trainable linear projection layer. Then, the sequence of projected features is input into cascaded Transformer layers to obtain the encoded image features, where the attention mask is bi-directional. (2) In the decoding mode, firstly, the input processing is different. As described in Sec. \ref{sec:model_input}, we tokenize the raw image into image tokens and initialize an embedding layer where token embeddings are indexed. Then, the same Transformer layers in encoding mode are reused in decoding mode to process the features; however, to guarantee the causal decoding ability, we use causal conv-shaped attention mask \cite{ramesh2021zero, yu2022scaling, child2019generating} instead. Overall, the two modes share the Transformer layers' parameters, and only differ in input processing and attention masks. Our assumption is that, compared with the design of plain image encoders as in previous work \cite{yu2022coca, wang2022git}, the additional decoding mode can exploit the common knowledge learned in image encoding to generate image autoregressive features, which we hypothesize should boost the (text-to-)image generation capacity. 


\noindent\textbf{Text Unicoder.} Similar to image unicoder mentioned above, the text unicoder also has both encoding and decoding modes, which reuse the Transformer parameters. In both modes, the same tokenizer and embedding layer are utilized to obtain token features, given that they share the same input formats.
% Differently, for two modes, since the input formats are the same, we use the same tokenizer and embedding layer to obtain the token features. 
A causal attention mask is applied in decoding mode. During encoding of text, there are two options in previous work: bi-directional mask \cite{devlin2018bert, raffel2020exploring, yu2022scaling}, or causal mask \cite{brown2020language, radford2021learning, yao2021filip}. We empirically found that two masks make no difference in performance, and use causal masking as the default in the reported experiments. 



\noindent\textbf{Cross-modal Decoder} The cross-modal decoder performs as a fusion-and-generation module, which structure-wise follows the cross-attention decoder \cite{vaswani2017attention, yu2022coca}. When generating text, the input is the text autoregressive feature from the text unicoder in decoding mode; encoded image features will be treated as cross-attention information, \ie, key and value in cross-attention layers. When generating the image, symmetrically, the image token autoregressive feature from the image unicoder in decoding mode is input and cross-attends to encoded text features. Also,  different from text generation where plain causal (autoregressive) mask is used in cross-modal decoder, image generation employs a conv-shaped masked sparse attention \cite{ramesh2021zero, yu2022scaling, child2019generating}, which can save memory and computation brought by long sequences of image tokens.

\subsection{Pre-training}
\label{sec:pretrain}
The pre-training of \cocadraw{} subsumes three fundamental objectives: image-text contrastive loss, I2T generation loss, T2I generation loss. Here, we provide details on the losses and also clarify the scaling and initialization strategy. 


\noindent\textbf{Contrastive Loss.} We input raw image and text into the image unicoder and the text unicoder, respectively (both in encoding mode) to get encoded image and text features. For text, as with CLIP \cite{radford2019language} and ALIGN \cite{jia2021scaling}, we take the feature vector of the CLS token appended at the end of input sequence as the global representation. For images, however, the unicoder outputs a sequence of features. To aggregate them, following \cite{yu2022coca}, we apply an attention pooler, which is a single multi-head
attention layer with one learnable query and unicoder output features as key and value. After obtaining two global features of image and text, a contrastive loss is applied to optimize the paired image-text against others in the same batch:

\begin{footnotesize}
\begin{equation}
    \mathcal{L}_{\text{Con}}=- \frac{1}{N} (\sum_{i}^{N} \text{log} \frac{\text{exp}(x_i^T y_i / \tau)}{\sum_{j=1}^{N} \text{exp}(x_i^T y_j/ \tau)} +  \sum_{i}^{N} \text{log} \frac{\text{exp}(y_i^T x_i / \tau)}{\sum_{j=1}^{N} \text{exp}(y_i^T x_j/ \tau)}),
\end{equation}
\end{footnotesize}
where $x_i$ and $y_j$ denote the normalized global embeddings of $i$-th image and $j$-th text. $\tau$ is a learnable temperature for adjusting the scale of the loss.


\begin{table*}[t]
\small
% \vspace{-4mm}
\setlength\tabcolsep{4pt} % 
\renewcommand{\arraystretch}{1.0} % 
\centering
\begin{tabular}{lccccccccccc}
\toprule
% \multirow{3}{*}{ViT-B/32}
\multirow{3}{*}{Model} & Image Understd. & \multicolumn{9}{c}{Image-Text Understd.} & Content Creation \\
\cmidrule(lr){2-2} \cmidrule(lr){3-11} \cmidrule(lr){12-12}
&\multirow{2}{*}{\makecell[c]{ImageNet \\ Classification}} &\multicolumn{4}{c}{Flickr Retrieval} & \multicolumn{4}{c}{MS-COCO Retrieval} & \multirow{2}{*}{\makecell[c]{MS-COCO \\ Captioning}}   & \multirow{2}{*}{\makecell[c]{MS-COCO \\ T2I Generation}}\\
\cmidrule(lr){3-6} \cmidrule(lr){7-10}
&  &\multicolumn{2}{c}{Image$\rightarrow$Text}&\multicolumn{2}{c}{Text$\rightarrow$Image}&\multicolumn{2}{c}{Image$\rightarrow$Text}&\multicolumn{2}{c}{Text$\rightarrow$Image}& &  \\
\cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}  \cmidrule(lr){9-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12}
& Acc(\%) & R@1 &R@5 &R@1 &R@5 & R@1 &R@5 &R@1 &R@5 & CIDEr & FID ($\downarrow$) \\
\midrule
CLIP \cite{radford2021learning} & 76.2 & 88.0 & 98.7 & 68.7 & 90.6 & 58.4 & 81.5 & 37.8 & 62.4 & - & - \\
ALIGN \cite{jia2021scaling} & 76.4  &  88.6 & 98.7 & 75.7 & 93.8 & 58.6 & 83.0 & 45.6 & 69.8 & - & - \\
FILIP \cite{yao2021filip} & 78.3 & 89.8 &  99.2 & 75.0 &  93.4  & 61.3 &  84.3  & 45.9 &  70.6 & - & - \\
Florence \cite{yuan2021florence} & \red{83.7} & 90.9 & 99.1 & 76.7 & 93.6 & 64.7 & \redbf{85.9} & 47.2 & 71.4 & - & - \\
CoCa-Large \cite{yu2022coca} & \redbf{84.8} & \red{91.4} & \redbf{99.2} &  \red{79.0} & \red{95.1} & \redbf{65.4} & \red{85.6} & \red{50.1} & \red{73.8} & - & - \\
ZeroCap \cite{tewel2021zero} & -  & - & - & - & - & - & - & - & - & 14.6 & - \\
SimVLM \cite{wang2021simvlm} & -  & - & - & - & - & - & - & - & - & 32.2 & - \\
VLKD \cite{dai2022enabling} & -  & - & - & - & - & - & - & - & - & \lb{58.3\textsuperscript{\textdagger}} & - \\
Parti-350M \cite{yu2022scaling} & -  & - & - & - & - & - & - & - & - & - & 14.10 \\ 
Parti-750M \cite{yu2022scaling} & -  & - & - & - & - & - & - & - & - & - & 10.71 \\ 
LDM-1.4B \cite{rombach2022high} & -  & - & - & - & - & - & - & - & - & - & 12.63 \\
\midrule
\rowcolor{lg} 
\lb{Coca-2B \cite{yu2022coca}} & \lb{86.3} & \lb{92.5} & \lb{99.5} & \lb{80.4} & \lb{95.7} & \lb{66.3} & \lb{86.2} & \lb{51.2} & \lb{74.2} & - & - \\ 
\rowcolor{lg}
\lb{Flamingo-3B \cite{alayrac2022flamingo}} & -  & - & - & - & - & - & - & - & - & \lb{73.0\textsuperscript{\textdagger}}  & -  \\
% \rowcolor{lg}
% Make-A-Scene \cite{gafni2022make} & -  & - & - & - & - & - & - & - & - & - & 11.84 \\
\rowcolor{lg}
\lb{DALL-E 2 \cite{ramesh2022hierarchical}} & -  & - & - & - & - & - & - & - & - & - & \lb{10.39} \\
\rowcolor{lg}
\lb{Parti-20B \cite{yu2022scaling}} & -  & - & - & - & - & - & - & - & - & - & \lb{7.23} \\
\midrule
\cocadrawbase{} & 79.4 & 89.5 & 98.4 & 76.5 & 94.3 & 62.1 & 83.5 & 47.3 & 72.3 & \red{43.0} & \red{10.35} \\
\cocadrawlarge{} & 82.7 & \redbf{91.5} & \red{99.1} & \redbf{79.9} & \redbf{95.3} & \red{65.1} & 85.5 & \redbf{50.3} & \redbf{74.2} & \redbf{44.8} & \redbf{9.37} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{\textbf{Zeroshot Evaluation} of \cocadraw{} against previous image-text models.  The models in \colorbox{lg}{gray} background have $\gg$ 1B parameters while others in white background have $\lesssim$ 1B parameters. For models with $\lesssim$ 1B parameters , we highlight the best score in \redbf{bold+underline} and the second-best score in \red{underline}. Understd. is the abbreviation of Understanding. }
\label{tab:exp_zs}
\vspace{-5mm}
\end{table*}


\noindent\textbf{I2T and T2I Generation Loss. } We formulate two generation tasks as token generation problems. As shown in Fig. \ref{fig:diagram}, by cascading image unicoder, text unicoder and cross-modal decoder, we can perform two tasks seamlessly by only switching the working modes of unicoders. A cross-entropy loss is applied on top of cross-modal decoder to maximize the conditional likelihood of the ground-truth token under the forward autoregressive factorization.
\begin{equation}
    \mathcal{L}_{\text{I2T}} = -\sum_{t=1}^{T} \text{log} P_\theta (y_t|y_{<t}, I), \\
\end{equation}
\begin{equation}
    \mathcal{L}_{\text{T2I}} = -\sum_{t=1}^{T} \text{log} P_\theta (x_t|x_{<t}, T), \\
\end{equation}
where $y$ and $x$ denote text and image tokens respectively. 

\noindent\textbf{Classifier-Free Guidance for T2I. } Following \cite{yu2022scaling, chang2023muse, ramesh2021zero}, we employ classifier-free guidance (CFG) \cite{ho2022classifier} in text-to-image generation. To be more specific, in training, we randomly mask conditioning vectors, \ie, input text tokens, by certain possibility (10\% in our implementation). In inference, we compute two predictions: conditional one $I(z, T)$ and unconditional one $I(z)$, which only differ in text input: conditional prediction  $I(z, T)$ has original text tokens as input while the input text of unconditional prediction $I(z)$ is fully masked. Then we linearly interpolate the $I(z,c)$ and $I(z)$ to obtain the final generated image:
\begin{equation}
    I = I(z) + \alpha (I(z,T) - I(z)),
\end{equation}
where $\alpha$ is a hyperparameter for adjusting the scale of classifier-free guidance, and we set $\alpha$=2.0 in default.

\noindent\textbf{Final Loss. } In the end, we simply add those three losses up to optimize the model end-to-end.
\begin{equation}
\mathcal{L}_{\text{\cocadraw}} = \lambda_{\text{Con}} \mathcal{L}_{\text{Con}} + \lambda_{\text{I2T}} \mathcal{L}_{\text{I2T}} + \lambda_{\text{T2I}} \mathcal{L}_{\text{T2I}}
\end{equation}
where $\lambda_{\text{Con}}$, $ \lambda_{\text{I2T}}$, $ \lambda_{\text{T2I}}$ denote corresponding scalar coefficients for contrastive, I2T and T2I loss. In default, we set $\lambda_{\text{T2I}}$ : $ \lambda_{\text{I2T}}$ : $ \lambda_{\text{Con}}$ = 1 : 0.2 : 0.1. 

\noindent\textbf{Scaling.}  As shown in Tab.\ref{tab:model_scale}, we start from \cocadrawbase{}, and scale it up, w.r.t. both number of layers and model dimension, to obtain \cocadrawlarge{} with around 1B parameters. 

\noindent\textbf{Initialization.}  In previous text-to-image generation models \cite{yu2022scaling, rombach2022high, saharia2022photorealistic}, the text feature extractor is usually initialized by a pre-trained text model. Correspondingly, in \cocadraw{}, we also initialize text unicoder with pre-trained text unimodal decoder from CoCa \cite{yu2022coca}, while leaving the image unicoder and cross-modal decoder trained from scratch. In Sec. \ref{sec:exp_ablation}, we compare it with training all from scratch, and find the initialization indeed helps with a small margin.  

% \subsection{Comparison with Other Works}


\section{Experiments}
In this section, we first describe the pre-training data and optimization process (Sec. \ref{sec:exp_setup}). Sec. \ref{sec:exp_zs} and Sec. \ref{sec:exp_ft} detail the primary results of zero-shot evaluation and fine-tuning evaluation, respectively.  Both evaluations examine three capacities: (1) visual understanding, (2) image captioning and multimodal understanding, (3) text-to-image content creation. In Sec. \ref{sec:exp_ablation}, different components of \cocadraw{} are ablated to justify our design.


\begin{table*}[t]
\small
% \vspace{-4mm}
\setlength\tabcolsep{4pt} % 
\centering
\begin{tabular}{lcccccccc}
\toprule
% \multirow{3}{*}{ViT-B/32}
\multirow{3}{*}{Model} & \multirow{3}{*}{\makecell[c]{Visual \\ Backbone}} & Image Understd. & \multicolumn{5}{c}{Image-Text Understd.} & Content Creation \\
\cmidrule(lr){3-3} \cmidrule(lr){4-8} \cmidrule(lr){9-9}
& & \multirow{2}{*}{\makecell[c]{ImageNet  \\ Linear Probing }} &\multicolumn{2}{c}{VQA} & \multicolumn{2}{c}{SNLI-VE} & \multirow{2}{*}{\makecell[c]{MS-COCO \\ Captioning \scriptsize{(CIDEr)}}}   & \multirow{2}{*}{\makecell[c]{MS-COCO T2I \\  Generation \scriptsize{(FID$\downarrow$)}}}\\
\cmidrule(lr){4-5} \cmidrule(lr){6-7}
&  & &test-dev&test-std&dev&test& &  \\
% & Acc(\%) & R@1 &R@5 &R@1 &R@5 & R@1 &R@5 &R@1 &R@5 & CIDEr & FID ($\downarrow$) \\
\midrule
CLIP \cite{radford2021learning}  & Scratch & 85.4 &  -   & -  & -   & -  & - & - \\
ALIGN \cite{jia2021scaling}  & Scratch & \red{85.5} &  -   & -  & -   & -  & - & - \\
UNITER \cite{chen2019uniter}  & Faster-RCNN & - &  73.8  & 74.0 & 79.4  & 79.4 & - & - \\
VinVL \cite{zhang2021vinvl} & Faster-RCNN & - &  76.5 &76.6 & - & - & 130.8 & - \\
CLIP-ViL \cite{shen2021much} & CLIP & - &  76.5   &  76.7 & 80.6 & 80.2 & 134.2 & - \\
ALBEF \cite{li2021align} & PT. ViT & - &  75.8  & 76.0 & 80.8  & 80.9  & - & - \\
BLIP \cite{li2022blip} & PT. ViT & - &  78.3  & 78.3  & -   & -  & 136.7 & - \\
SimVLM \cite{wang2021simvlm} & PT. ResNet  & - &   \red{80.0} & \red{80.3}  & \redbf{86.2} & \redbf{86.3} &  \red{143.3} & - \\
OFA \cite{wang2022ofa} & PT. ResNet  & - %85.6 (ft)%
&   \redbf{82.0} & \redbf{82.0}  & \llb{91.0} \textsuperscript{\textdagger}  & \llb{91.2}\textsuperscript{\textdagger} &  \redbf{145.3} & 10.5 \\
X-LXMERT \cite{huang2021unifying} & Faster-RCNN  & - &   - & -  & - & - &  122.6 &  29.9 \\
\midrule
\rowcolor{lg} 
\lb{CoCa-2.1B \cite{yu2022coca}} &  \lb{Scratch}  & - &   \lb{80.0} & \lb{80.3}  & \lb{87.0} & \lb{87.1} &  \lb{143.3} & - \\
\rowcolor{lg} 
\lb{BEIT3-1.9B \cite{wang2022image}} & \lb{Scratch}  & - % 89.6 (ft)
&   \lb{84.2} & \lb{84.0}  & -  & - &  \lb{147.6} & - \\
\rowcolor{lg} 
\lb{PALI-17B \cite{chen2022pali}} & \lb{PT. ViT}  & - % 89.6 (ft)
&   \lb{84.3}  & \lb{84.3}  & -  & - &  \lb{149.1} & - \\
\rowcolor{lg}
\lb{Parti-20B \cite{yu2022scaling}} & -  & - & - & - & - & - & - & \lb{3.22} \\
\midrule
\cocadrawbase{} & Scratch  & 83.48  & 76.3 & 76.6  & 85.4  & 85.4 &  135.4 & \red{5.06} \\
\cocadrawlarge{} & Scratch  & \redbf{86.44}  & 77.9 & 78.3  & \redbf{86.2}  & \red{86.0} &  139.5 & \redbf{4.62} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{\textbf{Fine-tuning Evaluation} of \cocadraw{} against previous image-text models. PT. denotes pre-trained and Scratch denotes trained from scratch. {}\textsuperscript{\textdagger}OFA incorporates both images and text in its input while others only use image one. }
\label{tab:exp_ft}
\vspace{-5mm}
\end{table*}


\subsection{Pre-training Details}
\label{sec:exp_setup}
\noindent\textbf{Data.} \cocadraw{} is designed to be pre-trained with image-text data.  For contrastive loss and I2T loss, we use a mixture of ALIGN dataset \cite{jia2021scaling}, and JFT-4B dataset \cite{zhai2022scaling} where category names are transformed into texts by prompts as in \cite{pham2021combined}. Differently, for T2I generation, we found that the short text in JFT-4B is less informative for generating the image as extensive descriptions of visual details are important. Instead, we replace JFT with WebLI dataset \cite{chen2022pali}, and mix it with ALIGN for T2I generation loss. 
%It's noted that for ALIGN and WebLI, we filter out image-text pairs that are either not accessible by public crawlers, or have low-resolution images. 
We further perform de-duplication, as in \cite{jia2021scaling, zhai2022lit}, to remove the examples close to downstream tasks. In the end, we obtain 1.1B pairs from ALIGN dataset, 162M pairs from WebLI dataset and 4B pairs from JFT-4B dataset. 


\noindent\textbf{Optimization. } Our \cocadraw{} models are implemented using Pax \cite{Pax}, a Jax-based framework. Within each batch, for optimizing T2I loss, we sample 1,024 image-text pairs from a mixture of ALIGN and WebLI datasets, and for optimizing contrastive and I2T losses, we sample 30,720 image-text pairs from a mixture of ALIGN and JFT datasets. In total, the batch size is 31,744. We use the Adafactor \cite{shazeer2018adafactor} optimizer with $\beta_1$ = 0.9, $\beta_2$ = 0.96 and a weight decay of 0.045. As for the learning rate schedule, we warm it up to 4.5e-5 in first 5,000 steps and then use an exponential decay starting from the step of 85,000. In total, models are pre-trained for 1M steps and \cocadrawbase/\cocadrawlarge{} takes around 12 days on 256/512 CloudTPUv4 chips. Then, following \cite{radford2021learning, jia2021scaling, yuan2021florence, yu2022coca}, we further pre-train our models for 50k steps with 576x576 high-resolution raw images as input in image encoding. The image input to ViT-VQGAN, \ie, image for decoding, is kept at 256x256 resolution.     


\subsection{Zero-shot Evaluation on Downstream Tasks}
\label{sec:exp_zs}
\cocadraw{} is capable of versatile zero-shot abilities. By evaluating on 5 representative tasks, \cocadraw{} stands out as the first work able to perform image understanding, image-text understanding and text-guided content creation in the zero-shot manner, and achieves superior performance on the majority of evaluation metrics. 

\noindent\textbf{Zero-shot Image Classification. }  Following \cite{radford2021learning, jia2021scaling, yuan2021florence}, we apply the same set of prompts to transfer labels into sentences, such as ``a photo of \{class\}''. The same as the way of computing contrastive loss in Sec. \ref{sec:pretrain}, we input raw image/text into image/text unicoders in encoding mode to  obtain the global features of image and text.  Then we compute their similarity to match images and labels. As shown in Tab. \ref{tab:exp_zs}, compared to models with similar scales, in ImageNet \cite{russakovsky2015imagenet}, \cocadrawlarge{} can achieve 82.7 \%, outperforming strong baselines such as CLIP \cite{radford2021learning}, ALIGN \cite{jia2021scaling}. We can see there is still a 2\% gap between \cocadrawlarge{} and CoCa-Large \cite{yu2022coca}, which may come from batch size difference: CoCa's batch size is 64k while ours is 30k only.

\noindent\textbf{Zero-shot Image-Text Retrieval. } The image and text feature extraction process is the same as zero-shot image classification. Flick \cite{plummer2015flickr30k} and MS-COCO \cite{lin2014microsoft} are used for evaluation. In Tab. \ref{tab:exp_zs}, within comparable scales, \cocadrawlarge{} can outperform the previous best model CoCa-Large in 5 out of 8 metrics and is ranked the second best in another two metrics, which shows a superior vision-language understanding capacity. 


\noindent\textbf{Zero-shot Image Captioning. } Since \cocadraw{} is already pre-trained with image-to-text generation loss on noisy image-text data, it's natural to directly evaluate it on zero-shot image captioning in the same way. As shown in Tab. \ref{tab:exp_zs}, in MS-COCO, \cocadrawbase/\cocadrawlarge{} can achieve 43.0/44.8 CIDEr score, surpassing SimVLM by 10.8/12.6. It's noted that the models with \textsuperscript{\textdagger}, \eg, Flamingo, VLKD, have much higher scores than others, because they reuse a pre-trained large language model as a decoder that inherits strong text generation ability.   


\noindent\textbf{Zero-shot Text-to-Image Generation. } Similar to computing text-to-image generation in pre-training, we can also evaluate \cocadraw{} in zero-shot text-to-image generation. In decoding, following \cite{yu2022scaling, ramesh2021zero}, we employ Top-K sampling to sample 16 images for each text and use a reranker to select the best image for evaluation. Following the de facto process, we compute FID score \cite{heusel2017gans} on MS-COCO 30k data \cite{ramesh2021zero, saharia2022photorealistic} (lower FID is better). As we can see in Tab.\ref{tab:exp_zs}, \cocadraw s can beat specialized models with comparable scales, and \cocadrawlarge{} can achieve an impressive FID of 9.37 which outperforms some models with larger scale by a significant margin, \eg, DALL-E 2 with 3.5B parameters, Make-A-Scene (FID=11.84) \cite{gafni2022make} with 4B parameters. 

\begin{table}[t]
\centering
% \setlength\tabcolsep{10pt} % 
\renewcommand{\arraystretch}{0.9} % 
\begin{tabular}{cccccc}
\toprule
 \multicolumn{3}{c}{Objectives} & \multicolumn{3}{c}{Evaluation} \\
 \cmidrule(lr){1-3}  \cmidrule(lr){4-6}
 Con. & T2I & I2T & ZS IN. & VQA. & ZS IG. ($\downarrow$) \\
 \midrule
 $\checkmark$ & - & - & 70.8 & - & -\\
 - &  $\checkmark$& - & - &  - & 12.6 \\
 - & - & $\checkmark$ & - & 68 & -\\
 - & $\checkmark$ & $\checkmark$ & - & 65.4 & 13.2\\
 $\checkmark$ & $\checkmark$ & $\checkmark$ & 71.1 & 66.9 & 13.3\\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Ablation on three objectives. Con. denotes contrastive loss.}
\label{tab:exp_ablate_obj}
\vspace{-2mm}
\end{table}


\subsection{Fine-tuning on Downstream Tasks}
\label{sec:exp_ft}
To demonstrate the transferability of \cocadraw{} on image understanding, image-text understanding and text-guided content creation, we further conduct linear probing or fine-tuning on multiple downstream tasks.

\noindent\textbf{Linear Probing on ImageNet. } Following \cite{radford2021learning, jia2021scaling}, we linear probe \cocadraw{} by fixing all parameters of image unicoder and only training a linear classifier on top for image recognition. \cocadrawlarge{} can outperform CLIP and ALIGN by around 1\%.

\noindent\textbf{Image-Text Understanding. } We categorize VQA \cite{antol2015vqa}, SNLI-VE \cite{xie2019visual} and image captioning into tasks requires image-text understanding. We fine-tune all parameters of \cocadraw{} and evaluate it on val/test set. 

\begin{table}[t]
\centering
\renewcommand{\arraystretch}{0.9} % 
\begin{tabular}{ccccc}
\toprule
 \multicolumn{2}{c}{Module} & \multicolumn{3}{c}{Evaluation} \\
 \cmidrule(lr){1-2}  \cmidrule(lr){3-5}
 Image & Text & VQA & ZS Cap. & ZS IG. ($\downarrow$) \\
\midrule
Encoder & Encoder & 65.9 & 32.9  &  13.8\\
Unicoder & Encoder & 66.5 & 36.9 &  13.38\\
Encoder & Unicoder & 67.8 & 35.0  &  13.67\\
Unicoder & Unicoder & 66.9 & 37.9  & 13.31\\
\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{Ablation on unicoder vs. encoder.}
\label{tab:exp_ablate_unicoder}
\vspace{-4mm}
\end{table}

\begin{table}[t]
\centering
\begin{tabular}{cccc}
\toprule
 \multirow{2}{*}{Model} & \multicolumn{3}{c}{Evaluation} \\
 \cmidrule(lr){2-4} 
 & ZS IN. & VQA & ZS IG. ($\downarrow$) \\
 \midrule
 \multirow{2}{*}{\makecell[c]{Init. Text Unicoder\\ from CoCa}} &  \multirow{2}{*}{75.35} &  \multirow{2}{*}{68.48} &  \multirow{2}{*}{11.42} \\
 &  &  &  \\ 
 \hline
 Train from Scratch &  75.02 & 68.55 & 11.63\\
\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Ablation on initialization. }
\label{tab:exp_ablate_init}
\vspace{-5mm}
\end{table}


\noindent\textbf{\textit{Captioning. }} In fine-tuning, \cocadraw{} computes caption predictions in the same way as zero-shot image captioning in Sec. \ref{sec:exp_zs}. In Tab. \ref{tab:exp_ft}, we can see the \cocadraw{} can achieve a competitive CIDEr score against other models. It's noted that some works \cite{wang2022ofa} additionally apply task-specific tricks such as CIDEr optimization, but for a fair comparison, we only present their results with plain cross-entropy loss . 

\noindent\textbf{\textit{VQA. }}   Following prior works \cite{wang2021simvlm, yu2022coca}, we use the VQA v2 and the task is formulated as a classification problem over 3,129 most frequent answers in the training set. To accomplish this, the raw image is fed into the image unicoder using encoding mode, while the question is processed by the text unicoder in decoding mode. Subsequently, the cross-modal decoder utilizes the text decoding features as input and cross-attends to the encoded image features. The final token output feature of the cross-modal decoder is considered the fused global feature. To predict the answer, a linear classifier is trained on top of this feature. As shown in Tab. \ref{tab:exp_ft}, \cocadraw{} can achieve satisfactory performance compared with other VLP models.

\noindent\textbf{\textit{SNLI-VE. }} Similar to fine-tuning VQA, we extract the final token output feature of cross-modal decoder and apply a linear classifier on top to predict the three relations. As shown in Tab. \ref{tab:exp_ft}, \cocadraw{} can outperform strong VLP models and achieve superior performance. Note that other models including \cocadraw{} only use image premise as inputs, but OFA incorporates both image and text premises in its input.

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{./figs/cocadraw_vis1_v3.pdf}
  \vspace{-6mm}
  \caption{Qualitative results of zero-shot text-to-image generation from \cocadrawlarge{} with both good and failed cases. }
  \label{fig:vis}
\vspace{-5mm}
\end{figure*}


\noindent\textbf{Text-to-Image Generation. } Following \cite{yu2022scaling, saharia2022photorealistic}, we fine-tune \cocadraw{} on MS-COCO training set and evaluate the FID score on sampled 30k test set. Compared with zero-shot performance, fine-tuning on \cocadrawbase/\cocadrawlarge{} further reduces the FID from 10.35/9.37 to 5.06/4.62, outperforming models of comparable scales. 



\subsection{Ablation}
\label{sec:exp_ablation}
In this section, we comprehensively ablate the design choices in \cocadraw{}. Most ablation experiments are conducted on \cocadrawbase{} with a reduced batch size and a shrunken training schedule. Specifically, total batch size is 4,352, containing 4,096 for contrastive and I2T loss, and 256 for T2I loss, and the total training step is 200k, without high-resolution pre-training.  We select following representative tasks: zero-shot ImageNet Classification (ZS IN.) for image understanding, fine-tuned VQA or MS-COCO zero-shot Captioning (ZS Cap.) for multimodal understanding, MS-COCO zero-shot text-to-image generation (ZS IG.) for image generation. ZS Cap. result is measured by CIDEr and ZS IG. result is measured in FID (lower FID is better).



\noindent\textbf{Training Objectives. } We ablate the existence of three training objectives: contrastive loss, I2T loss and T2I loss, and study how they affect each other. The result is shown in Tab. \ref{tab:exp_ablate_obj}. We can obtain several interesting observations: (1). By comparing the first and last rows, it's found \textit{cross-modal generation objectives can improve image understanding a bit on top of contrastive loss}. The zero-shot ImageNet accuracy is improved by 0.3\%. (2) Comparing the second, third and fourth rows, we see \textit{two generations losses, \ie, I2T loss and T2I loss, contradict each other a little bit}. After adding T2I loss, VQA accuracy drops by 2.6, and after adding I2T loss, zero-shot image generation FID score rises by 0.6. (3) From the fourth row and fifth row, we can see \textit{contrastive loss improves vision-language understanding while it doesn't influence image generation}. Overall, we demonstrate the feasibility of unifying three fundamental objectives in one framework relatively harmoniously. 


\noindent\textbf{Loss Weight. } Given three objectives, we ablate different weights for them and select the best one as the default configuration for all experiments. Please see supplementary. 


\noindent\textbf{Unicoder vs. Encoder. } In previous Vision-Language works \cite{wang2021simvlm, yu2022coca, chen2022pali}, encoder-decoder has been a de facto pipeline, where encoder encodes image/text features and cross-modal decoder fuses them and perform generation. Differently, we propose unicoder to replace encoder, which can both encode and decode unimodal representations with shared parameters. Here, we ablate image and text unicoders against image and text encoders. It's noted that unicoder doesn't add extra parameters to encoders because encoding and decoding in unicoder reuse same set of parameters. We put a diagram in supplementary to illustrate how the compared encoder-only models work.  As shown in Tab. \ref{tab:exp_ablate_unicoder}, either image unicoder or text unicoder can improve over encoders, and applying them together brings best trade-off for both image generation and multimodal understanding.  
% TODO(haoxuanyou): Probably need a figure to describe the encoder-decoder pipeline in supplementary?

\noindent\textbf{Train From Scratch. } As mentioned in Sec. \ref{sec:pretrain}, we initialize text unicoder with a pre-trained unimodal text decoder from CoCa. Here we also attempt to train all from scratch. In this comparison, all models are trained with non-shrunken batch size to mitigate the possible gap due to much larger batch size of CoCa. In Tab. \ref{tab:exp_ablate_init}, loading pre-trained weight from CoCa improves zero-shot Imagenet recognition and text-to-image generation by 0.3\% and 0.2, which is a small margin. Also, it doesn't even improve VQA. This comparison verifies the do-ability of training \cocadraw{} all from scratch without hurting much performance.    


\subsection{Visualization}
We visualized good and failed generated images of \cocadrawlarge{} using the prompts from PartiPrompt \cite{yu2022scaling}. As in Fig. \ref{fig:vis}, \cocadraw{} can generate high-quality, broadly capable, open-domain images based on text. As for failed cases, we can see \cocadraw{} misunderstands ``A car made out of Sushi'' as ``A car with Sushi on top'', also \cocadraw{} fails to generate the reflection of mountains in the bowl of soup. More visualization and analysis are in supplementary.

% \todohy{Working on adding SR module upon CoBIT, will add this part later.}

\section{Conclusion}
We present a vision-language foundation model, CoBIT, which unifies three objectives: cross-modal contrastive learning, image-to-text generation, and text-to-image generation. CoBIT consists of an image unicoder, a text unicoder, and a cross-attention decoder. The unicoders can switch between two modes: unimodal image/text encoding and decoding. The model is trained on large-scale noisy web-crawled image-text data and image annotation data. CoBIT achieves strong zero-shot and transferable capacities of unimodal visual understanding, image-text matching, image-text understanding, and text-to-image content creation. 

\vspace{-2mm}
\section*{Limitations \& Broader Impact}
\vspace{-1mm}
\noindent\textbf{Limitations. } Although \cocadraw{} unifies contrastive loss, text-to-image generation (T2I) loss, and image-to-text (I2T) generation loss, from ablation experiments, we nevertheless find that the T2I and I2T objectives contradict each other somewhat. We hypothesize that it is because the two different types of generation require fine-grained knowledge that is specific to each modality.

\noindent\textbf{Broader Impact. } Models such as Stable Diffusion, DALL-E, Parti, and \cocadraw{} are trained on large and noisy  image-text datasets that include many biases due to the distribution of images contained in them (which is not representative of real world, generally speaking) and the way the images are described (which includes human biases in descriptions of different subjects). Such models also create risks with respect to misinformation (e.g. deepfakes) and they introduce both challenges and opportunities for creativity and art. See the Parti \cite{yu2022scaling} and Imagen \cite{saharia2022photorealistic} broader impacts sections for extensive discussion and references on these topics. Furthermore, the task setting of creating images from descriptions itself involves inherent ambiguities (especially due to underspecification) that play an important role -- beyond the training data -- in the behavior and risks inherent in such models \cite{hutchinson-etal-2022-underspecification}.

\vspace{-3mm}
\section*{Acknowledgement}
\vspace{-2mm}
We would like to thank Prof. Shih-Fu Chang, Luowei Zhou, Long Chen for constructive discussion, Zirui Wang for help with downstream fine-tuning, Huiwen Chang for proofreading, and Laurent El Shafey for infra support.

% \noindent\textbf{Zero-shot Transfer. } After pre-training, we can directly deploy \cocadraw to zero-shot downstream tasks. By extracting and pooling image/text features with image/text unicoders in encoding modes, we can compute their cosine similarity and perform zero-shot image classification and zero-shot image-text retrieval. We can also do text-to-image generation in a zero-shot manner.   

% \noindent\textbf{Fine-tuning. } \cocadraw can cover following tasks in a fine-tuning manner. (1) Visual understanding: we take the pre-trained image unicoder in a encoding mode and fine-tune it in various visual-only downstream tasks, such as image classification. (2) Image-text alignment: image and text unicoders are both fine-tuned in a encoding mode for retrieval task. (3) Image-text understanding and captioning: 



% which are trained with the ensemble of vision-language contrastive loss, image-to-text generative loss and text-to-image generative loss. In terms of model structure, we introduce \textbf{image}/\textbf{text unicoder} that can work in two modes: unimodal image/text encoding and decoding. Inside each unicoder, encoding and decoding utilize same set of Transformer parameters, while they differs in input embedding and attention masks. 

%-------------------------------------------------------------------------


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

% \vspace{22mm}
% \clearpage


\begin{table*}[h]
\small
\setlength\tabcolsep{10pt} % 
\renewcommand{\arraystretch}{1.2} % 
\centering
        \begin{tabular}{l|c|c|c|c|c}
        % \begin{tabular}{lccccc}
        \toprule
         & \multirow{2}{*}{\makecell[c]{ImageNet \\ Linear Probe}} & \multirow{2}{*}{VQA} & \multirow{2}{*}{SNLI-VE} & \multirow{2}{*}{\makecell[c]{MS-COCO \\ Captioning}}   & \multirow{2}{*}{\makecell[c]{MS-COCO  \\  T2I Generation}} \\
          &  &  &  &  & \\
        % \cmidrule(lr){2-2}  \cmidrule(lr){3-3}  \cmidrule(lr){4-4}  \cmidrule(lr){5-5}  \cmidrule(lr){6-6}
         \hline
         Optimizer &  SGD & \multicolumn{4}{c}{Adafacter}\\
         \hline
         Gradient Clip &  \multicolumn{5}{c}{1.0}\\
         \hline
         LR decay  schedule &  Cosine Schedule to zero & \multicolumn{3}{c|}{Linear Schedule to zero}  & Exponential Schedule to zero\\
         \hline
         RandAugment & 2, 5 & \multicolumn{3}{c|}{1, 10}  &  None\\
         \hline
         Training Step &  225k & 100k & 50k & 15k & 100k\\
         \hline
         Warm-Up Step &  0 & 1000 &  1000 & 500 & 1000\\
         \hline
         Batch Size &  512 & 32 & 128 & 128 & 256\\
         \hline
         Learning Rate &  3.2 & 1e-5 & 5e-5 & 5e-6 & 1e-5\\
         \hline
         Weight Decay &  0.0 & 0.1 & 0.1 & 0.01 & 0.045\\
        \bottomrule
        \end{tabular}
\caption{Hyper-parameters used in the multimodal experiments.}
\label{tab:hps}
\end{table*}

\section{Appendix}

\subsection{Ablation on Loss Weight} 




\begin{figure*}[t]
\centering
  \includegraphics[width=1\linewidth]{./figs/cocadraw_unicoder_encoder_supp.pdf}
%   \vspace{-5mm}
  \caption{Diagram of three compared models in the ablation of \textbf{unicoder vs. encoder}. \textbf{Top}: Replacing both image unicoder and text unicoder with image encoder and text encoder respectively.  \textbf{Middle}: Replacing text unicoder with text encoder while keeping image unicoder. \textbf{Bottom}: Replacing image unicoder with image encoder while keeping text unicoder. }
  \label{fig:diagram_supp}
%   \vspace{-5mm}
\end{figure*}


Given three objectives, we ablate different weights for them and select the best one as the default configuration for all experiments.  We start with T2I and I2T first: given the weight of T2I fixed to 1, we ablate the loss of I2T. Then given T2I and I2T loss both fixed, the weight of contrastive loss is ablated. As we can see in Tab. \ref{tab:exp_ablate_weight}, a high weight of I2T such as 1 will hurt the image generation heavily but also improve VQA. On the other hand, a high weight of contrastive loss like 0.4 will not essentially improve image recognition and hurts both VQA and image generation. Overall, we chose Con.:T2I:I2T = 0.1:0.2:1 as our default setting, as it achieves a good trade-off between three losses. 

\begin{table}[h]
\caption{Ablation on weights of three losses. Con. denotes contrastive loss. T2I denotes text-to-image generation loss. I2T denotes image-to-text generation loss. ZS IN. denotes zero-shot ImageNet classification. ZS IG. denotes zero-shot text-to-image generation on MS-COCO, which is evaluated by FID and lower FID is better. }
\label{tab:exp_ablate_weight}
\begin{center}
\begin{tabular}{cccccc}
\toprule
 \multicolumn{3}{c}{Weights} & \multicolumn{3}{c}{Evaluation} \\
 \cmidrule(lr){1-3}  \cmidrule(lr){4-6}
 Con. & T2I & I2T & ZS IN. & VQA & ZS IG. ($\downarrow$) \\
 \midrule
 - &  0.1 & 1  & - & 63.2 & 13.17 \\
 - & 0.2 & 1  & - & 65.4 & 13.24\\
 - & 1 & 1 & - & 67.8 & 16.33 \\
\midrule{}
 0.1 &  0.2 & 1  & 71.1 & 66.9 & 13.31 \\
 0.4 & 0.2 & 1 & 71.2 & 66.5 & 13.92 \\
\midrule
\bottomrule
\end{tabular}
\end{center}
\end{table}


\subsection{Hyperparameters in Fine-tuning}
In Tab. \ref{tab:hps}, we present the hyperparameters we used in fine-tuning/linear probing of \cocadraw{}.


\begin{figure*}[t]
 \centering
  \includegraphics[width=0.88\linewidth]{./figs/cocadraw_vis_supp.pdf}
  \caption{More qualitative results of zero-shot text-to-image generation from \cocadrawlarge{} with both good and failed cases. }
  \label{fig:vis_supp}
% \vspace{-5mm}
\end{figure*}

\subsection{Illustration of Replacing Unicoders with Encoders in \cocadraw} 
In Sec.4.4, we ablate \textbf{Unicoder  vs.   Encoder} and demonstrate the effectiveness of proposed unicoders. In Fig. \ref{fig:diagram_supp} we show the diagram of using image and text encoders, image encoder+text unicoder, and image unicoder+text encoder.  As we can see, encoders can only encode visual or textual features while unicoders can perform both encoding and decoding, which shares the knowledge and boosts the generation result as shown in previous ablation. It's noted that, replacing uncoders with encoders doesn't save the parameters, because inside unicoders, encoding and decoding share the same Transformer parameters. So in terms of parameter efficiency, unicoder and encoder have no difference. 

\subsection{More visualization} 
In Fig. \ref{fig:vis_supp}, We attach more visualization of \cocadrawlarge{} on zero-shot text-to-image generation with novel prompts in PartiPrompts \cite{yu2022scaling}. For better visualization when zoom-in, we employ \cite{sahak2023denoising} as the super-resolution module to upsample generated 256x256 images to 1024x1024 images. It's noted that when computing FID, we still use 256x256 images and the high-resolution ones are only used for visualization. In failed cases, we find that: (1) \cocadraw{} sometimes messes up the size attributes of two objects. For example, in the last example, yellow sphere ought to be smaller. (2) \cocadraw{} sometimes couldn't render the details of words in text very well. In the second last example, ``DRAWIT'' is rendered as ``DRAWMI?''. (3) \cocadraw{} occasionally misunderstands the text. In the third last example, we expect a geico that looks like a cat whereas \cocadraw{} first renders ``GEICO THAT LOOK'' then generates a cat. It's indeed a new way to interpret the text but not the desired way of humans.





\end{document}
