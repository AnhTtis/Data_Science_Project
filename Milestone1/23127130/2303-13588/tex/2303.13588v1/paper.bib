@InProceedings{reluplex,
author="Katz, Guy
and Barrett, Clark
and Dill, David L.
and Julian, Kyle
and Kochenderfer, Mykel J.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="97--117",
abstract="Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
isbn="978-3-319-63387-9"
}

@article{IUA,
author = {Wang, Zi and Albarghouthi, Aws and Prakriya, Gautam and Jha, Somesh},
title = {Interval Universal Approximation for Neural Networks},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {POPL},
url = {https://doi.org/10.1145/3498675},
doi = {10.1145/3498675},
abstract = {To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function f (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of f (the result of applying f to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al. from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we can always construct provably robust neural networks under ℓ∞-norm using almost any practical activation function. Second, we study the computational complexity of constructing neural networks that are amenable to precise interval analysis. This is a crucial question, as our constructive proof of IUA is exponential in the size of the approximation domain. We boil this question down to the problem of approximating the range of a neural network with squashable activation functions. We show that the range approximation problem (RA) is a Δ2-intermediate problem, which is strictly harder than NP-complete problems, assuming coNP⊄NP. As a result, IUA is an inherently hard problem: No matter what abstract domain or computational tools we consider to achieve interval approximation, there is no efficient construction of such a universal approximator. This implies that it is hard to construct a provably robust network, even if we have a robust network to start with.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {14},
numpages = {29},
keywords = {Universal Approximation, Abstract Interpretation}
}

@book{hdp, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={High-Dimensional Probability: An Introduction with Applications in Data Science}, DOI={10.1017/9781108231596}, publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@book{spectra, series={Universitext}, title={Spectra of Graphs}, DOI={10.1007/978-1-4614-1939-6}, publisher={Springer New York, NY}, author={Brouwer, Andries E. and Haemers, Willem H.}, year={2011}}

@inproceedings{
geolip,
title={A Quantitative Geometric Approach to Neural-Network Smoothness},
author={Zi Wang and Gautam Prakriya and Somesh Jha},
booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
year={2022},
url={https://openreview.net/forum?id=ZQcpYaE1z1r}
}

@inproceedings{lipsdp,
 author = {Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and Morari, Manfred and Pappas, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2019/file/95e1533eb1b20a97777749fb94fdb944-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{sdp_rob_local,
author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
title = {Semidefinite Relaxations for Certifying Robustness to Adversarial Examples},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite their impressive performance on diverse tasks, neural networks fail catas-trophically in the presence of adversarial inputs—imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different foreign networks whose training objectives are agnostic to our proposed relaxation.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {10900–10910},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@book{modern_co,
author = {Ben-Tal, Aharon and Nemirovski, Arkadi},
title = {Lectures on Modern Convex Optimization},
publisher = {Society for Industrial and Applied Mathematics},
year = {2001},
doi = {10.1137/1.9780898718829},
address = {},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718829},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718829}
}

@article{flag, title={Flag algebras}, volume={72}, DOI={10.2178/jsl/1203350785}, number={4}, journal={The Journal of Symbolic Logic}, publisher={Cambridge University Press}, author={Razborov, Alexander A.}, year={2007}, pages={1239–1282}}

@inproceedings{nlp_2013,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed Representations of Words and Phrases and Their Compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}


@article{Krizhevsky_imagenetclassification,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {60},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3065386},
doi = {10.1145/3065386},
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
journal = {Commun. ACM},
month = {may},
pages = {84–90},
numpages = {7}
}

@inproceedings{szegedy2014intriguing,
  author    = {Christian Szegedy and
               Wojciech Zaremba and
               Ilya Sutskever and
               Joan Bruna and
               Dumitru Erhan and
               Ian J. Goodfellow and
               Rob Fergus},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Intriguing properties of neural networks},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6199},
  timestamp = {Thu, 25 Jul 2019 14:35:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{goodfellow2015explaining,
  author    = {Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Explaining and Harnessing Adversarial Examples},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6572},
  timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoodfellowSS14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{adv-rnn,
  author={Papernot, Nicolas and McDaniel, Patrick and Swami, Ananthram and Harang, Richard},
  booktitle={MILCOM 2016 - 2016 IEEE Military Communications Conference}, 
  title={Crafting adversarial input sequences for recurrent neural networks}, 
  year={2016},
  volume={},
  number={},
  pages={49-54},
  doi={10.1109/MILCOM.2016.7795300}}

@misc{adv-metric,
  doi = {10.48550/ARXIV.2102.07265},
  
  url = {https://arxiv.org/abs/2102.07265},
  
  author = {Panum, Thomas Kobber and Wang, Zi and Kan, Pengyu and Fernandes, Earlence and Jha, Somesh},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exploring Adversarial Robustness of Deep Metric Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{adv-transformer,
    author    = {Mahmood, Kaleel and Mahmood, Rigel and van Dijk, Marten},
    title     = {On the Robustness of Vision Transformers to Adversarial Examples},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {7838-7847}
}

@INPROCEEDINGS {CW,
author = {N. Carlini and D. Wagner},
booktitle = {2017 IEEE Symposium on Security and Privacy (SP)},
title = {Towards Evaluating the Robustness of Neural Networks},
year = {2017},
volume = {},
issn = {2375-1207},
pages = {39-57},
abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x&#x27; that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks&#x27; ability to find adversarial examples from 95% to 0.5%. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
keywords = {neural networks;robustness;measurement;speech recognition;security;malware;resists},
doi = {10.1109/SP.2017.49},
url = {https://doi.ieeecomputersociety.org/10.1109/SP.2017.49},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {may}
}
}

@INPROCEEDINGS{papernot2015limitations,
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
  booktitle={2016 IEEE European Symposium on Security and Privacy (EuroS\&P)}, 
  title={The Limitations of Deep Learning in Adversarial Settings}, 
  year={2016},
  volume={},
  number={},
  pages={372-387},
  doi={10.1109/EuroSP.2016.36}}

@misc{albarghouthi2021introduction,
      title={Introduction to Neural Network Verification}, 
      author={Aws Albarghouthi},
      year={2021},
      eprint={2109.10317},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
certified_def,
title={Certified Defenses against Adversarial Examples},
author={Aditi Raghunathan and Jacob Steinhardt and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Bys4ob-Rb},
}


@inproceedings{cav-safety,
	abstract = {Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer. Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations. If found, adversarial examples can be shown to human testers and/or used to fine-tune the network. We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks. We also compare against existing techniques to search for adversarial examples and estimate network robustness.},
	address = {Cham},
	author = {Huang, Xiaowei and Kwiatkowska, Marta and Wang, Sen and Wu, Min},
	booktitle = {Computer Aided Verification},
	editor = {Majumdar, Rupak and Kun{\v{c}}ak, Viktor},
	isbn = {978-3-319-63387-9},
	pages = {3--29},
	publisher = {Springer International Publishing},
	title = {Safety Verification of Deep Neural Networks},
	year = {2017}}

@inproceedings{singh2018fast,
title={Fast and effective robustness certification},
author={Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
booktitle={Advances in Neural Information Processing Systems},
pages={10802--10813},
year={2018}
}

@article{opt_inf2,
 author = {Bri{\"e}t, Jop and Regev, Oded and Saket, Rishi},
 title = {Tight Hardness of the Non-Commutative Grothendieck Problem},
 year = {2017},
 pages = {1--24},
 doi = {10.4086/toc.2017.v013a015},
 publisher = {Theory of Computing},
 journal = {Theory of Computing},
 volume = {13},
 number = {15},
 URL = {https://theoryofcomputing.org/articles/v013a015},
}

@article{shor_SDP,
 author = {Shor, Naum Zuselevich},
 title = {Quadratic Optimization Problems},
 year ={1987},
 journal = {Soviet Journal of Computer and Systems Sciences},
 volume = {25},
 number = {1-11},
}

@inproceedings{UGC_inf1_opt,
author = {Raghavendra, Prasad and Steurer, David},
title = {Towards Computing the Grothendieck Constant},
year = {2009},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The Grothendieck constant KG is the smallest constant such that for every d ∈ N and every matrix A = (aij),[EQUATION]where B(d) is the unit ball in Rd. Despite several efforts [15, 23], the value of the constant KG remains unknown. The Grothendieck constant KG is precisely the integrality gap of a natural SDP relaxation for the KM, N-Quadratic Programming problem. The input to this problem is a matrix A = (aij) and the objective is to maximize the quadratic form Σij aijxiyj over xiyj ∈ [−1, 1].In this work, we apply techniques from [22] to the KM, N-Quadratic Programming problem. Using some standard but non-trivial modifications, the reduction in [22] yields the following hardness result: Assuming the Unique Games Conjecture [9], it is NP-hard to approximate the KM, N-Quadratic Programming problem to any factor better than the Grothendieck constant KG.By adapting a "bootstrapping" argument used in a proof of Grothendieck inequality [5], we are able to perform a tighter analysis than [22]. Through this careful analysis, we obtain the following new results:• An approximation algorithm for KM, N-Quadratic Programming that is guaranteed to achieve an approximation ratio arbitrarily close to the Grothendieck constant KG (optimal approximation ratio assuming the Unique Games Conjecture).• We show that the Grothendieck constant KG can be computed within an error η, in time depending only on η. Specifically, for each η, we formulate an explicit finite linear program, whose optimum is η-close to the Grothendieck constant.We also exhibit a simple family of operators on the Gaussian Hilbert space that is guaranteed to contain tight examples for the Grothendieck inequality.},
booktitle = {Proceedings of the Twentieth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {525–534},
numpages = {10},
location = {New York, New York},
series = {SODA '09}
}

@article{smt,
author = {Nieuwenhuis, Robert and Oliveras, Albert and Tinelli, Cesare},
title = {Solving SAT and SAT Modulo Theories: From an Abstract Davis--Putnam--Logemann--Loveland Procedure to DPLL(T)},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0004-5411},
url = {https://doi.org/10.1145/1217856.1217859},
doi = {10.1145/1217856.1217859},
abstract = {We first introduce Abstract DPLL, a rule-based formulation of the Davis--Putnam--Logemann--Loveland (DPLL) procedure for propositional satisfiability. This abstract framework allows one to cleanly express practical DPLL algorithms and to formally reason about them in a simple way. Its properties, such as soundness, completeness or termination, immediately carry over to the modern DPLL implementations with features such as backjumping or clause learning.We then extend the framework to Satisfiability Modulo background Theories (SMT) and use it to model several variants of the so-called lazy approach for SMT. In particular, we use it to introduce a few variants of a new, efficient and modular approach for SMT based on a general DPLL(X) engine, whose parameter X can be instantiated with a specialized solver SolverT for a given theory T, thus producing a DPLL(T) system. We describe the high-level design of DPLL(X) and its cooperation with SolverT, discuss the role of theory propagation, and describe different DPLL(T) strategies for some theories arising in industrial applications.Our extensive experimental evidence, summarized in this article, shows that DPLL(T) systems can significantly outperform the other state-of-the-art tools, frequently even in orders of magnitude, and have better scaling properties.},
journal = {J. ACM},
month = {nov},
pages = {937–977},
numpages = {41},
keywords = {Satisfiability Modulo Theories, SAT solvers}
}

@inproceedings{chen2020semialgebraic,
 author = {Chen, Tong and Lasserre, Jean B and Magron, Victor and Pauwels, Edouard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {19189--19200},
 publisher = {Curran Associates, Inc.},
 title = {Semialgebraic Optimization for Lipschitz Constants of ReLU Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{abstract_int,
author = {Cousot, Patrick and Cousot, Radhia},
title = {Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints},
year = {1977},
isbn = {9781450373500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512950.512973},
doi = {10.1145/512950.512973},
abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
pages = {238–252},
numpages = {15},
location = {Los Angeles, California},
series = {POPL '77}
}

@INPROCEEDINGS{ai2,
  author={Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)}, 
  title={AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation}, 
  year={2018},
  volume={},
  number={},
  pages={3-18},
  doi={10.1109/SP.2018.00058}}

@inproceedings{symb_int,
author = {Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and Yang, Junfeng and Jana, Suman},
title = {Formal Security Analysis of Neural Networks Using Symbolic Intervals},
year = {2018},
isbn = {9781931971461},
publisher = {USENIX Association},
address = {USA},
abstract = {Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world security-critical domains including autonomous vehicles and collision avoidance systems, formally checking security properties of DNNs, especially under different attacker capabilities, is becoming crucial. Most existing security testing techniques for DNNs try to find adversarial examples without providing any formal security guarantees about the nonexistence of such adversarial examples. Recently, several projects have used different types of Satisfiability Modulo Theory (SMT) solvers to formally check security properties of DNNs. However, all of these approaches are limited by the high overhead caused by the solver.In this paper, we present a new direction for formally checking security properties of DNNs without using SMT solvers. Instead, we leverage interval arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike existing solver-based approaches, is easily parallelizable. We further present symbolic interval analysis along with several other optimizations to minimize overestimations of output bounds.We design, implement, and evaluate our approach as part of ReluVal, a system for formally checking security properties of Relu-based DNNs. Our extensive empirical results show that ReluVal outperforms Reluplex, a state-of-the-art solver-based system, by 200 times on average. On a single 8-core machine without GPUs, within 4 hours, ReluVal is able to verify a security property that Reluplex deemed inconclusive due to timeout after running for more than 5 days. Our experiments demonstrate that symbolic interval analysis is a promising new direction towards rigorously analyzing different security properties of DNNs.},
booktitle = {Proceedings of the 27th USENIX Conference on Security Symposium},
pages = {1599–1614},
numpages = {16},
location = {Baltimore, MD, USA},
series = {SEC'18}
}

@article{zonotope,
author = {Singh, Gagandeep and Gehr, Timon and P\"{u}schel, Markus and Vechev, Martin},
title = {An Abstract Domain for Certifying Neural Networks},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {POPL},
url = {https://doi.org/10.1145/3290354},
doi = {10.1145/3290354},
abstract = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {41},
numpages = {30},
keywords = {Adversarial attacks, Abstract Interpretation, Deep Learning}
}

@article{maxcut,
author = {Goemans, Michel X. and Williamson, David P.},
title = {Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming},
year = {1995},
issue_date = {Nov. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0004-5411},
url = {https://doi.org/10.1145/227683.227684},
doi = {10.1145/227683.227684},
journal = {J. ACM},
month = {nov},
pages = {1115–1145},
numpages = {31},
keywords = {satisfiability, randomized algorithms, convex optimization, Approximation algorithms}
}

@INPROCEEDINGS{facenet,
  author={Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={FaceNet: A unified embedding for face recognition and clustering}, 
  year={2015},
  volume={},
  number={},
  pages={815-823},
  doi={10.1109/CVPR.2015.7298682}}

@inproceedings{phishNet,
author = {Abdelnabi, Sahar and Krombholz, Katharina and Fritz, Mario},
title = {VisualPhishNet: Zero-Day Phishing Website Detection by Visual Similarity},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3417233},
doi = {10.1145/3372297.3417233},
abstract = {Phishing websites are still a major threat in today's Internet ecosystem. Despite numerous previous efforts, similarity-based detection methods do not offer sufficient protection for the trusted websites, in particular against unseen phishing pages. This paper contributes VisualPhishNet, a new similarity-based phishing detection framework, based on a triplet Convolutional Neural Network (CNN). VisualPhishNet learns profiles for websites in order to detect phishing websites by a similarity metric that can generalize to pages with new visual appearances. We furthermore present VisualPhish, the largest dataset to date that facilitates visual phishing detection in an ecologically valid manner. We show that our method outperforms previous visual similarity phishing detection approaches by a large margin while being robust against a range of evasion attacks.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1681–1698},
numpages = {18},
keywords = {phishing detection, visual similarity, triplet networks},
location = {Virtual Event, USA},
series = {CCS '20}
}

@InProceedings{Wu_2017_ICCV,
author = {Wu, Chao-Yuan and Manmatha, R. and Smola, Alexander J. and Krahenbuhl, Philipp},
title = {Sampling Matters in Deep Embedding Learning},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@Misc{GroIneq,
 Author = {A. {Grothendieck}},
 Title = {{R\'esum\'e de la th\'eorie m\'etrique des produits tensoriels topologiques}},
 Year = {1956},
 Language = {French},
 HowPublished = {{Bol. Soc. Mat. S\~ao Paulo 8, 1-79 (1956).}},
 MSC2010 = {46M05 46A99},
 Zbl = {0074.32303}
}

@inproceedings{cut-norm,
author = {Alon, Noga and Naor, Assaf},
title = {Approximating the Cut-Norm via Grothendieck's Inequality},
year = {2004},
isbn = {1581138520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007352.1007371},
doi = {10.1145/1007352.1007371},
booktitle = {Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing},
pages = {72–80},
numpages = {9},
keywords = {cut-norm, Grothendieck's inequaity, rounding techniques},
location = {Chicago, IL, USA},
series = {STOC '04}
}

@article{nest,
author = { Yu   Nesterov },
title = {Semidefinite relaxation and nonconvex quadratic optimization},
journal = {Optimization Methods and Software},
volume = {9},
number = {1-3},
pages = {141-160},
year  = {1998},
publisher = {Taylor & Francis},
doi = {10.1080/10556789808805690},
URL = { 
        https://doi.org/10.1080/10556789808805690
},
eprint = { 
        https://doi.org/10.1080/10556789808805690
}
}

@inproceedings{
madry2018towards,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJzIBfZAb},
}

@inproceedings{UGC,
author = {Khot, Subhash},
title = {On the Power of Unique 2-Prover 1-Round Games},
year = {2002},
isbn = {1581134959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/509907.510017},
doi = {10.1145/509907.510017},
abstract = {A 2-prover game is called unique if the answer of one prover uniquely determines the answer of the second prover and vice versa (we implicitly assume games to be one round games). The value of a 2-prover game is the maximum acceptance probability of the verifier over all the prover strategies. We make the following conjecture regarding the power of unique 2-prover games, which we call the Unique Games Conjecture:(MATH) The Unique Games Conjecture: For arbitrarily small constants $ zeta, delta > 0$, there exists a constant $k = k(zeta,delta)$ such that it is NP-hard to determine whether a unique 2-prover game with answers from a domain of size $k$ has value at least $1-zeta$ or at most $delta$. medskip.(MATH) We show that a positive resolution of this conjecture would imply the following hardness results:For any $frac{1}{2} < t < 1$, for all sufficiently small constants $epsilon > 0$, it is NP-hard to distinguish between the instances of the problem 2-Linear-Equations mod 2 where either there exists an assignment that satisfies $1-epsilon$ fraction of equations or no assignment can satisfy more than $1-epsilon^t$ fraction of equations. As a corollary of the above result, it is NP-hard to approximate the Min-2CNF-deletion problem within any constant factor.For the constraint satisfaction problem where every constraint is the predicate Not-all-equal($a,b,c$), $ a, b, c in GF(3) $, it is NP-hard to distinguish between the instances where either there exists an assignment that satisfies $1-epsilon$ fraction of the constraints or no assignment satisfies more than $frac{8}{9}+epsilon$ fraction of the constraints for an arbitrarily small constant $epsilon > 0$. We also get a hardness result for a slight variation of approximate coloring of 3-uniform hypergraphs.(MATH) We also show that a variation of the Unique Games Conjecture implies that for arbitrarily small constant $delta > 0$ it is hard to find an independent set of size $delta n$ in a graph that is guaranteed to have an independent set of size $Omega(n)$.The main idea in all the above results is to use the 2-prover game given by the Unique Games Conjecture as an "outer verifier" and build new probabilistically checkable proof systems (PCPs) on top of it. The uniqueness property plays a crucial role in the analysis of these PCPs.(MATH) In light of such interesting consequences, we think it is an important open problem to prove (or disprove) the Unique Games Conjecture. We also present a semi-definite programming based algorithm for finding reasonable prover strategies for a unique 2-prover game. Given a unique 2-prover game with value $1-zeta$ and answers from a domain of size $k$, this algorithm finds prover strategies that make the verifier accept with probability $1-O(k^2 zeta^{1/5} sqrt{log (frac{1}{zeta})})$. This result shows that the domain size $k = k(zeta, delta)$ must be sufficiently large if the Unique Games Conjecture is true.},
booktitle = {Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing},
pages = {767–775},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {STOC '02}
}


@inproceedings{deq_model,
	author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {5238--5250},
	publisher = {Curran Associates, Inc.},
	title = {Multiscale Deep Equilibrium Models},
	url = {https://proceedings.neurips.cc/paper/2020/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf}}

@inproceedings{multi-deq,
author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
title = {Multiscale Deep Equilibrium Models},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a new class of implicit networks, the multiscale deep equilibrium model (MDEQ), suited to large-scale and highly hierarchical pattern recognition domains. An MDEQ directly solves for and backpropagates through the equilibrium points of multiple feature resolutions simultaneously, using implicit differentiation to avoid storing intermediate states (and thus requiring only O (1) memory consumption). These simultaneously-learned multi-resolution features allow us to train a single model on a diverse set of tasks and loss functions, such as using a single MDEQ to perform both image classification and semantic segmentation. We illustrate the effectiveness of this approach on two large-scale vision tasks: ImageNet classification and semantic segmentation on high-resolution images from the Cityscapes dataset. In both settings, MDEQs are able to match or exceed the performance of recent competitive computer vision models: the first time such performance and scale have been achieved by an implicit deep learning approach.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {440},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@manual{mosek,
   author = "MOSEK ApS",
   title = "The MOSEK optimization toolbox for MATLAB manual. Version 9.0.",
   year = 2019,
   url = "http://docs.mosek.com/9.0/toolbox/index.html"
}

@misc{cvx,
  author       = {CVX Research, Inc.},
  title        = {{CVX}: Software for Disciplined Convex Programming, Version 2.2, Build 1148 },
  howpublished = {\url{http://cvxr.com/cvx}},
  month        = Jan,
  year         = 2020
}

@book{MATLAB,
year = {2021},
author = {MATLAB},
title = {9.11.0.1837725 (R2021b) Update 2},
publisher = {The MathWorks Inc.},
address = {Natick, Massachusetts}
}

@inproceedings{bcp,
author = {Lee, Sungyoon and Lee, Jaewook and Park, Saerom},
title = {Lipschitz-Certifiable Training with a Tight Outer Bound},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Verifiable training is a promising research direction for training a robust network. However, most verifiable training methods are slow or lack scalability. In this study, we propose a fast and scalable certifiable training algorithm based on Lipschitz analysis and interval arithmetic. Our certifiable training algorithm provides a tight propagated outer bound by introducing the box constraint propagation (BCP), and it efficiently computes the worst logit over the outer bound. In the experiments, we show that BCP achieves a tighter outer bound than the global Lipschitz-based outer bound. Moreover, our certifiable training algorithm is over 12 times faster than the state-of-the-art dual relaxation-based method; however, it achieves comparable or better verification performance, improving natural accuracy. Our fast certifiable training algorithm with the tight outer bound can scale to Tiny ImageNet with verification accuracy of 20.1\% (ℓ2-perturbation of ε = 36/255).},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1417},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{
Baader2020Universal,
title={Universal Approximation with Certified Networks},
author={Maximilian Baader and Matthew Mirman and Martin Vechev},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1gX8kBtPr}
}


@inproceedings{sdp_quality,
	author = {Zhang, Richard},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {3808--3820},
	publisher = {Curran Associates, Inc.},
	title = {On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples},
	url = {https://proceedings.neurips.cc/paper/2020/file/27b587bbe83aecf9a98c8fe6ab48cacc-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/file/27b587bbe83aecf9a98c8fe6ab48cacc-Paper.pdf}}


@InProceedings{chordal_sparsity,
  title = 	 {Exploiting Sparsity for Neural Network Verification},
  author =       {Newton, Matthew and Papachristodoulou, Antonis},
  booktitle = 	 {Proceedings of the 3rd Conference on Learning for Dynamics and Control},
  pages = 	 {715--727},
  year = 	 {2021},
  editor = 	 {Jadbabaie, Ali and Lygeros, John and Pappas, George J. and Parrilo, Pablo and Recht, Benjamin and Tomlin, Claire J. and Zeilinger, Melanie N.},
  
  volume = 	 {144},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {07 -- 08 June},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v144/newton21a/newton21a.pdf},
  url = 	 {https://proceedings.mlr.press/v144/newton21a.html},
  abstract = 	 {The problem of verifying the properties of a neural network has never been more important. This task is often done by bounding the activation functions in the network. Some approaches are more conservative than others and in general there is a trade-off between complexity and conservativeness. There has been significant progress to improve the efficiency and the accuracy of these methods. We investigate the sparsity that arises in a recently proposed semi-definite programming framework to verify a fully connected feed-forward neural network. We show that due to the intrinsic cascading structure of the neural network the constraint matrices in the semi-definite program form a block-arrow pattern and satisfy conditions for chordal sparsity. We reformulate and implement the optimisation problem, showing a significant speed-up in computation, without sacrificing solution accuracy.}
}



@INPROCEEDINGS{explore_chordal,
  author={Xue, Anton and Lindemann, Lars and Robey, Alexander and Hassani, Hamed and Pappas, George J. and Alur, Rajeev},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)}, 
  title={Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks}, 
  year={2022},
  volume={},
  number={},
  pages={3389-3396},
  doi={10.1109/CDC51059.2022.9993136}}


@inproceedings{efficient_sdp,
	author = {Dathathri, Sumanth and Dvijotham, Krishnamurthy and Kurakin, Alexey and Raghunathan, Aditi and Uesato, Jonathan and Bunel, Rudy R and Shankar, Shreya and Steinhardt, Jacob and Goodfellow, Ian and Liang, Percy S and Kohli, Pushmeet},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {5318--5331},
	publisher = {Curran Associates, Inc.},
	title = {Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming},
	url = {https://proceedings.neurips.cc/paper/2020/file/397d6b4c83c91021fe928a8c4220386b-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/file/397d6b4c83c91021fe928a8c4220386b-Paper.pdf}}

@inproceedings{
lipopt,
title={Lipschitz constant estimation of Neural Networks via sparse polynomial optimization},
author={Fabian Latorre and Paul Rolland and Volkan Cevher},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJe4_xSFDB}
}


@inproceedings{sdp-relax-operator,
	abstract = {The paper describes a class of mathematical problems at an intersection of operator theory and combinatorics, and discusses their application in complex system analysis. The main object of study is duality gap bounds in quadratic programming which deals with problems of maximizing quadratic functionals subject to quadratic constraints Such optimization is known to be universal, in the sense that many computationally hard questions can be reduced to quadratic programming On the other hand, it is conjectured that an efficient algorithm of solving general non-convex quadratic programs exactly does not exist.},
	address = {Basel},
	author = {Megretski, Alexandre},
	booktitle = {Systems, Approximation, Singular Integral Operators, and Related Topics},
	editor = {Borichev, Alexander A. and Nikolski, Nikolai K.},
	isbn = {978-3-0348-8362-7},
	pages = {365--392},
	publisher = {Birkh{\"a}user Basel},
	title = {Relaxations of Quadratic Programs in Operator Theory and System Analysis},
	year = {2001}}


@article{ugc_cut,
	author = {Khot, Subhash and Kindler, Guy and Mossel, Elchanan and O'Donnell, Ryan},
	doi = {10.1137/S0097539705447372},
	eprint = {https://doi.org/10.1137/S0097539705447372},
	journal = {SIAM Journal on Computing},
	number = {1},
	pages = {319-357},
	title = {Optimal Inapproximability Results for MAX‐CUT and Other 2‐Variable CSPs?},
	url = {https://doi.org/10.1137/S0097539705447372},
	volume = {37},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1137/S0097539705447372}}

 @ARTICLE{anti_lipSDP,
  author={Pauli, Patricia and Koch, Anne and Berberich, Julian and Kohler, Paul and Allgöwer, Frank},

  journal={IEEE Control Systems Letters}, 

  title={Training Robust Neural Networks Using Lipschitz Bounds}, 

  year={2022},

  volume={6},

  number={},

  pages={121-126},

  doi={10.1109/LCSYS.2021.3050444}}


@inproceedings{semialgebraic-verify,
	abstract = {In order to verify semialgebraic programs, we automatize the Floyd/Naur/Hoare proof method. The main task is to automatically infer valid invariants and rank functions.},
	address = {Berlin, Heidelberg},
	author = {Cousot, Patrick},
	booktitle = {Verification, Model Checking, and Abstract Interpretation},
	editor = {Cousot, Radhia},
	isbn = {978-3-540-30579-8},
	pages = {1--24},
	publisher = {Springer Berlin Heidelberg},
	title = {Proving Program Invariance and Termination by Parametric Abstraction, Lagrangian Relaxation and Semidefinite Programming},
	year = {2005}}

 
@inproceedings{sdp-prob,
	abstract = {We consider nondeterministic probabilistic programs with the most basic liveness property of termination. We present efficient methods for termination analysis of nondeterministic probabilistic programs with polynomial guards and assignments. Our approach is through synthesis of polynomial ranking supermartingales, that on one hand significantly generalizes linear ranking supermartingales and on the other hand is a counterpart of polynomial ranking-functions for proving termination of nonprobabilistic programs. The approach synthesizes polynomial ranking-supermartingales through Positivstellensatz's, yielding an efficient method which is not only sound, but also semi-complete over a large subclass of programs. We show experimental results to demonstrate that our approach can handle several classical programs with complex polynomial guards and assignments, and can synthesize efficient quadratic ranking-supermartingales when a linear one does not exist even for simple affine programs.},
	address = {Cham},
	author = {Chatterjee, Krishnendu and Fu, Hongfei and Goharshady, Amir Kafshdar},
	booktitle = {Computer Aided Verification},
	editor = {Chaudhuri, Swarat and Farzan, Azadeh},
	isbn = {978-3-319-41528-4},
	pages = {3--22},
	publisher = {Springer International Publishing},
	title = {Termination Analysis of Probabilistic Programs Through Positivstellensatz's},
	year = {2016}}


@InProceedings{linear-prog,
  title = 	 {Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope},
  author =       {Wong, Eric and Kolter, Zico},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5286--5295},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/wong18a/wong18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/wong18a.html},
  abstract = 	 {We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\epsilon = 0.1$).}
}
@inproceedings{crown,
author = {Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
title = {Efficient Neural Network Robustness Certification with General Activation Functions},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4944–4953},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{int-lp,
author = {Chan, Siu On and Lee, James R. and Raghavendra, Prasad and Steurer, David},
title = {Approximate Constraint Satisfaction Requires Large LP Relaxations},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/2811255},
doi = {10.1145/2811255},
abstract = {We prove super-polynomial lower bounds on the size of linear programming relaxations for approximation versions of constraint satisfaction problems. We show that for these problems, polynomial-sized linear programs are no more powerful than programs arising from a constant number of rounds of the Sherali--Adams hierarchy. In particular, any polynomial-sized linear program for Max Cut has an integrality gap of ½ and any such linear program for Max 3-Sat has an integrality gap of ⅞.},
journal = {J. ACM},
month = {oct},
articleno = {34},
numpages = {22},
keywords = {approximation complexity, Linear programming, constraint satisfaction problems, LP hierarchies, lower bounds, extended formulations}
}

@article{lap_eigen,
author = {Delorme, C. and Poljak, S.},
title = {Laplacian Eigenvalues and the Maximum Cut Problem},
year = {1993},
issue_date = {February  1993},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {62},
number = {1–3},
issn = {0025-5610},
abstract = {We introduce and study an eigenvalue upper bound\'{z}(G) on the maximum cut mc (G) of a weighted graph. The function\'{z}(G) has several interesting properties that resemble the behaviour of mc (G). The following results are presented.We show that\'{z} is subadditive with respect to amalgam, and additive with respect to disjoint sum and 1-sum. We prove that\'{z}(G) is never worse that 1.131 mc(G) for a planar, or more generally, a weakly bipartite graph with nonnegative edge weights. We give a dual characterization of\'{z}(G), and show that\'{z}(G) is computable in polynomial time with an arbitrary precision.},
journal = {Math. Program.},
month = {feb},
pages = {557–574},
numpages = {18},
keywords = {eigenvalues, Max-cut, algorithms}
}

@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010
}

@article{symbex,
author = {King, James C.},
title = {Symbolic Execution and Program Testing},
year = {1976},
issue_date = {July 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/360248.360252},
doi = {10.1145/360248.360252},
abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
journal = {Commun. ACM},
month = {jul},
pages = {385–394},
numpages = {10},
keywords = {program proving, program verification, symbolic interpretation, program debugging, program testing, symbolic execution}
}

@inproceedings{jordan2021exactly,
 author = {Jordan, Matt and Dimakis, Alexandros G},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7344--7353},
 publisher = {Curran Associates, Inc.},
 title = {Exactly Computing the Local Lipschitz Constant of ReLU Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/5227fa9a19dce7ba113f50a405dcaf09-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{convex_barrier,
	author = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks},
	url = {https://proceedings.neurips.cc/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf}}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@INPROCEEDINGS{gloro,
    title = {Globally-Robust Neural Networks},
    author = {Klas Leino and Zifan Wang and Matt Fredrikson},
    booktitle = {International Conference on Machine Learning (ICML)},
    year = {2021},
}

@inproceedings{
relu-express,
title={Understanding Deep Neural Networks with Rectified Linear Units},
author={Raman Arora and Amitabh Basu and Poorya Mianjy and Anirbit Mukherjee},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1J_rgWRW},
}

@inproceedings{
huang2021local,
title={Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds},
author={Yujia Huang and Huan Zhang and Yuanyuan Shi and J Zico Kolter and Anima Anandkumar},
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=FTt28RYj5Pc}
}

@inproceedings{
alg-lip,
title={A Unified Algebraic Perspective on Lipschitz Neural Networks},
author={Alexandre Araujo and Aaron J Havens and Blaise Delattre and Alexandre Allauzen and Bin Hu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=k71IGLC8cfc}
}