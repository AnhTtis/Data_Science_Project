\section{Evaluation}\label{sec:eva}
We want to understand the quality of the framework proposed in the work. Since this is a framework, it can be applied to different tasks. We choose $\ell_2$-data-dependent analysis for evaluation because there are no tools amenable to our framework but there are other benchmark tools that we can compare with. This can help us evaluate the empirical strength and weakness of our framework. 

Specifically, we want to certify the robustness of data when $\ell_2$-perturbations of a specific radius are allowed. The SDP for this task is described in~\cref{eq:l2-cert-sdp}. On this certification task, we aim to empirically answer the following research questions:
\begin{tcolorbox}
\begin{enumerate}[start=1,label={\bfseries RQ\arabic*:}]
\item How precise is the framework to certify inputs subject to $\ell_2$ adversarial attacks?
\item How efficient is the framework to certify data points?
\end{enumerate}
\end{tcolorbox}
Notice that RQ1 is usually problem specific, independent of the computing environment and implementation if unlimited computing resources are assumed, but RQ2 heavily relies on implementation and computing environment. On average, the framework is $60\%$ better than the benchmark tool, and the result is summarized in~\cref{tab:result}. However, because the framework relies on off-the-shelf SDP solvers, in our implementation and experiment, the framework is considerably slower than the benchmark, as summarized in~\cref{tab:time-result}. However, our implementation is still able to handle practical-sized networks, compared to exponential-time verifier, for example, Reluplex~\cite{reluplex}.

\paragraph{Server specification}All the experiments are run on a workstation with forty-eight Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Silver 4214 CPUs running at 2.20GHz, and 258 GB of memory, and eight Nvidia GeForce RTX 2080 Ti GPUs. Each GPU has 4352 CUDA cores and 11 GB of GDDR6 memory.

\paragraph{Tools}Our framework transforms the verification tasks into SDPs, and to finish the analysis, we need to rely on off-the-shelf SDP solvers. We implement our framework for $\ell_2$-local analysis using the MATLAB CVX and MOSEK solver~\cite{MATLAB,cvx,mosek}, and name the tool \emph{ShenBao}, which resembles ``symbol'', to emphasize the symbolic reasoning essence.

We use BCP~\cite{bcp} as the benchmark to certify the robustness of inputs. BCP uses both the $\ell_2$-Lipschitz constant and the interval domain to refine the abstract interpretation of the input execution induced from $\ell_2$-balls.

Additionally, we also use the PGD attack~\cite{madry2018towards} to evaluate the model. PGD adversarial examples are considered standard $\ell_2$ and $\ell_\infty$-attacks, and are also used in adversarial training. The result from PGD attacks can also serve as a sanity check for certifiable correctness because certifiable correctness by definition is no higher than adversarial accuracy for any attacks.

\paragraph{Attacks} The \emph{strength} of attack is measured by the attack radius. For robust and adversarial training, we train the model with one strength and test the model using a weaker, the same, and a stronger attacks to measure the performance. We use $\epsilon$ to denote the attack strength during training. 

\paragraph{Neural network models}We use two models for evaluation. One is a \emph{small} DNN, consisting of a single hidden layer with $64$ $\relu$ nodes; and the other one is a \emph{medium} DNN with two hidden layers: the first layer has $128$ nodes and the second one has $64$ nodes. 

We train our network on the MNIST dataset~\cite{mnist}. The models are trained under three different modes. The first one is the standard natural training. The second one is the BCP training as in~\cite{bcp}. The third one is PGD training: the model is fed with PGD adversarial examples of certain attack strengths during training.

For all small models, we train the model for $60$ epochs; and for all medium models, we train the model for $100$ epochs.

\paragraph{Measurement}We fix $200$ test data points that are inaccessible to the model during training. In the test phase, we measure the following quantities:
\begin{enumerate}
    \item \emph{Accuracy} denotes how many inputs can be correctly classified when no attacks are applied;
    \item \emph{PGD} measures how many points can still be correctly classified when PGD attacks are applied.
    \item \emph{ShenBao} denotes how many inputs can be certifiably classified correctly by ShenBao when $\ell_2$-attacks are allowed.
    \item \emph{BCP} measures how many points can be certifiably classified correctly by BCP when $\ell_2$-attacks are allowed.
\end{enumerate}

\paragraph{Precision of certification} The result of certification precision is summarized in~\cref{tab:result}. 

\begin{table}
\begin{center}
\begin{tabular}{p{3cm}p{2cm}p{1.8cm}p{1.5cm}p{1.8cm}p{1.5cm}}
\toprule
\multicolumn{6}{c}{\centering Number of Correctly Classified Inputs} \\
\midrule
\centering Model & \hfil Accuracy &\hfil Strength & \hfil PGD & \hfil ShenBao & \hfil BCP \\
\midrule
 \centering Small & \hfil  & \hfil 0.3  & \hfil 181& \hfil 126 & \hfil 58 \\
\centering Natural Training & \hfil 200 & \hfil 0.5 & \hfil 138& \hfil 70 & \hfil 5 \\
\centering$\epsilon=0$ & \hfil  & \hfil 0.7  & \hfil 90& \hfil 20 & \hfil 0 \\
\midrule
\centering Small &\hfil   & \hfil 0.3 & \hfil 192& \hfil 186  & \hfil 188  \\
\centering BCP Training & \hfil 198 & \hfil 0.5 & \hfil 190& \hfil 177 & \hfil 176 \\
\centering $\epsilon=0.5$ & \hfil  & \hfil 0.7 & \hfil 182& \hfil 156 & \hfil 158 \\
\midrule
\centering Small & \hfil  & \hfil 0.3 & \hfil 196& \hfil 193 & \hfil 159  \\
\centering PGD Training & \hfil 198 & \hfil 0.5 & \hfil 191& \hfil 178 & \hfil 72 \\
\centering $\epsilon=0.5$ & \hfil & \hfil 0.7 & \hfil 182& \hfil 161 & \hfil 12 \\
\midrule
\centering Medium & \hfil  & \hfil 0.7& \hfil 182& \hfil  151& \hfil 158  \\
\centering BCP Training & \hfil 193 & \hfil 1.0 & \hfil 166& \hfil 126 & \hfil 128 \\
\centering $\epsilon=1.0$ & \hfil & \hfil 1.3 & \hfil 146& \hfil 100 & \hfil 105 \\
\midrule
\centering Medium &\hfil  & \hfil 0.7& \hfil 195& \hfil 177 & \hfil 0  \\
\centering PGD Training & \hfil 199  & \hfil 1.0& \hfil 179& \hfil 142 & \hfil 0 \\
\centering $\epsilon=1.0$ & \hfil & \hfil 1.3& \hfil 167& \hfil 90 & \hfil 0 \\
\bottomrule
\end{tabular}
\newline\newline 
\caption{The result of experiments to certify the robustness of $200$ test inputs. The column PGD denotes how many points remains accurate with PGD attack. ShenBao denotes how many points are certified accurate by ShenBao. BCP denotes how many points are certified accurate by BCP. We can see that ShenBao performs consistently better than BCP except for a few cases when the network is trained with the BCP-induced bound. It is noteworthy that on the medium PGD-trained model, it is already quite certifiably robust from the measurement of ShenBao while BCP cannot certify the robustness at all. }\label{tab:result}
\end{center}
\end{table}

Additionally, we also profile the time used by ShenBao and BCP to certify the inputs.

\paragraph{Speed of certification}
We measure the certification speed of ShenBao and BCP. We ran our experiments on the MNIST dataset, which has $10$ classes. For ShenBao to certify an input, we need to solve $(10-1=9)$ SDP programs. These $9$ SDP programs are independent, so can be solved in parallel. We record the average running time to solve one SDP for both the small and the medium models.

On the other hand, BCP is implemented to run on GPUs very efficiently. We record the total time to certify all $200$ data. The certification time is summarized in~\cref{tab:time-result}.

\begin{table}
\begin{center}
\begin{tabular}{p{4cm}p{3cm}p{3cm}}
\toprule
\multicolumn{3}{c}{\centering Running Time (in seconds) of ShenBao and BCP} \\
\midrule
\centering Model & \hfil ShenBao &\hfil BCP \\
\midrule
\centering Small & \hfil 102.4 &\hfil  2.1\\
\midrule
\centering Medium & \hfil 426.2 &\hfil  1.8\\
\bottomrule
\end{tabular}
\newline\newline 
\caption{The running time of certification. On average, ShenBao needs $102.4$ seconds to solve the SDP from a small model, and $426.2$ seconds to solve the SDP from a medium model; while BCP only needs about $2$ seconds to certify all $200$ inputs.}\label{tab:time-result}
\end{center}
\end{table}
