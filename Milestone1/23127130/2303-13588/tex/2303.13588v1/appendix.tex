\section{Quadratic Encodings for $\ell_p$-norm Constraints}\label{sec:ell-p}
In the canonical finite-dimensional vector space $\R^n$, the most commonly considered $\ell_p$-norms in practice are the $\ell_2$ and $\ell_\infty$-norms. $\ell_1$-attacks are also considered in some literatures~\cite{jordan2021exactly}. We have shown how to encode $\ell_2$ and $\ell_\infty$-norm constraints in the main text. 

The $\ell_1$-norm constraint can be encoded via Hölder's inequality:
\[\norm{x}_1\leq \epsilon \Leftrightarrow \inprod{x, y}\leq \epsilon , \norm{y}_\infty\leq 1.\]
Therefore, one can introduce a new variable $y$ to encode the $\ell_1$-norm constraint.

For more general $\ell_p$-norms, where $p\geq 1$ and is rational, we can use the following encoding, as described in~\cite{modern_co}.

First, let $q$ be the Hölder conjugate of $p$, i.e., $\frac{1}{p} + \frac{1}{q} = 1$. For example, the dual norm of $\ell_2$-norm is $\ell_2$ because $\frac{1}{2} + \frac{1}{2} = 1$, and also $1$ and $\infty$ are dual to each other. One can verify that $\norm{x}_p\leq \epsilon$ is equivalent to the following conditions:
\begin{equation}\label{eq:ell-p}
    y_i\geq 0, |x_i|\leq \epsilon^{1/q}y_i^{1/p}, \sum_{i=1}^n y_i\leq \epsilon.
\end{equation}
This is because by definition, $\norm{x}_p\leq \epsilon$ means that $\sum_{i=1}^n |x_i|^p\leq \epsilon^p$. One can then think of $y_i$ as $|x_i|^p\epsilon^{1-p}$.

The linear equations can be easily encoded by quadratic relations. It remains to encode the inequality $x_i\leq \epsilon^{1/q}y_i^{1/p}$ in quadratic relations. This is easy because $\epsilon^{1/q}$ is a number, so we only need to consider how to express $x_i\leq y_i^{q}$.

Because $q$ is a rational number, the above inequality is equivalent to $x_i^{n}\leq y_i^m$, for integers $n$ and $m$. If $n, m \geq 3$, we can introduce additional variables and inequalities to decrease the degree. For example, $x^3\leq y$ can be encoded as $xz\leq y$ and $z=x^2$.

\section{Quadratic encodings for other activations}\label{sec:other-act}
Let $\act$ be the activation function, and $z=\act(x)$.

If we consider the data-independent analysis and the slope-restricted interpretation of the activation, we only need to know the derivative bounds of the activation function. Most activation functions such as $\relu$, sigmoid functions are slope-restricted, and the bounds can be easily found. Suppose the lower and upper bounds are $a$ and $b$, then we can use $(\Delta z - a\Delta x)(\Delta z - b\Delta x)\leq 0$ to capture this interpretation.

The complicated case is the exact encoding of the activation functions. One can easily see that quadratic relations cannot exactly encode non-algebraic functions. If the activation contains $e^x$ in its irreducible representation, for example, the hyperbolic tangent function $\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$, we can only approximate them. One can consider segment the input space into several regions and then use two linear piecewise functions to upper and lower bound the non-algebraic function. Because piecewise linear functions can approximate any continuous function within any precision on a compact space, we can bound any continuous activation function up to arbitrary precision. It remains to approximate the piecewise linear function. 

In~\cite{relu-express}, the authors showed that $\relu$ network can exactly encode any piecewise linear functions. Therefore, one can use the $\relu$ quadratic encoding as the block to construct any piecewise linear function. To illustrate this process, we use $\relu_\theta$ as an example. $\relu_\theta$ is used in~\cite{huang2021local} to train locally Lipschitz network. $\relu_\theta$ is defined as
\[ \relu_\theta(x) = \left\{ \begin{array}{ll}
            \theta, & \mbox{$x \geq \theta$}\\
            x, & \mbox{$0\leq x < \theta$}\\
            0, & \mbox{$x < 0$}\end{array} \right.
        \]
where $\theta>0$.

We can use a similar squashable function gadget in~\cite{IUA} to construct $\relu_\theta$ from $\relu$. One can easily check that $\relu_\theta(x) = \relu(\theta-\relu(\theta-x))$.

Because $z = \relu_\theta(x)$, we want to constrain $z$ and $x$ with quadratic relations. We can introduce another variable $y = \relu(\theta-x)$. From $\relu$'s quadratic encoding, we have 
$y\geq \theta-x$, $y\geq 0$ and $(y-(\theta-x))y\leq 0$.

Similarly, we have $z\geq \theta-y$, $z\geq 0$ and $(z-(\theta-y))z\leq 0$.

With these six quadratic inequalities, we can encode the computation $z = \relu_\theta(x)$.

\section{Shor's Relaxation Scheme}\label{sec:shor}
We provide a detailed introduction of Shor's relaxation scheme as in~\cite{modern_co}. The quadratic program is:
\begin{align*}
\begin{split}
    \min \;\;\;&f_0(x) = x^TA_0x+2b_0^Tx+c_0\\ 
    s.t.\;\;\;\;  & f_i(x) = x^TA_ix+2b_i^Tx+c_i \leq 0,\;\forall i\in[m]
\end{split}
\end{align*}

\paragraph{Dual form SDP relaxation}One can introduce $m$ nonnegative variables $\lambda_i\geq 0$ as ``weights'' for the constraints. These non-negative variables are known as the Lagrangian dual variables. One can add the weighted constraint to the objective, and obtain:
\[f_\lambda(x) = f_0(x)+\sum_{i=1}^m\lambda_if_i(x)=x^TA(\lambda)x+2b^T(\lambda)x+c(\lambda),\]
where
\begin{align*}
    A(\lambda) &= A_0 + \sum_{i=1}^m \lambda_iA_i,\\ 
    b(\lambda) &= b_0 + \sum_{i=1}^m \lambda_ib_i,\\ 
    c(\lambda) &= c_0 + \sum_{i=1}^m \lambda_ic_i.
\end{align*}
Because $f_i(x)\leq 0$ and $\lambda_i\geq 0$, by construction, $\inf_{\lambda}f_\lambda(x)$ lower bounds the optimum of~\cref{eq:quad-prog}, and thus a relaxation. The interesting part of this weighted optimization problem is that this can be represented as an SDP. The minimization of of $f_\lambda(x)$ is to maximize $\obju$ such that $f_\lambda(x) - \obju\geq 0$
. This problem has an SDP representation:
\[
\max_{\obju,\dualv}\Big\{\obju:\begin{pmatrix}
c(\lambda)-\obju & \;\;\;\;b(\lambda)^T \\
b(\lambda) & \;\;\;\;A(\lambda)
\end{pmatrix}\succeq 0, \dualv_i\geq 0\Big\}.
\]
This is Shor's semidefinite relaxation of the quadratic program in the dual form.

\paragraph{Primal form SDP relaxation}We have introduced the primal form SDP relaxation in~\cref{sec:overview}. Now we can discuss why the primal SDP relaxation can be viewed as the natural continuous relaxation for some combinatorial problems. Essentially this is because the discrete constraint quantified by a quadratic relation is relaxed to a continuous constraint by adding more dimensions.  We can use the MAXCUT problem as an example.

The MAXCUT problem is defined as $\max_{x\in\{-1,1\}^n} x^TLx$, where $L\in\R^{n\times n}$ is the Laplacian matrix of a graph.

The SDP relaxation for this problem is
\begin{align*}
    &\inprod{L,X}_F\\
    s.t.\;\;&X\succeq 0, X_{ii} = 1.
\end{align*}
Because $X$ is PSD, $X = MM^T$ for some matrix $M\in\R^{n\times d}$, where $d\in\Z_+$. Let $M_i$ be the $i$-th row vector of $M$. $X_{ij} = \inprod{M_i,M_j}$, and $X_{ii}=1$ means $\inprod{M_i, M_i} = 1$. As a result, $\inprod{L,X}_F = \sum_{i,j} L_{ij}X_{ij} = \sum_{i,j} L_{ij}\inprod{M_i, M_j}$.
In contrast, in the quadratic program, the variable to $L_{ij}$ is $x_ix_j$, the product of two scalars. 
If $d=1$ in the SDP relaxation, $M$ is a column vector, and $X$ is a rank-1 matrix. In this case, the SDP coincides with the combinatorial problem, because the inner product degenerates to the multiplication of two scalars. Hence, the SDP relaxation can be viewed as a continuous relaxation of a discrete problem.

\section{An Example for Recursion Reasoning}\label{sec:rec-prob}
Let us consider a classical geometric distribution example. Given a fair coin, suppose we want to flip the coin until we see heads for the first time, what is the expected number of trials?

There are two approaches to this problem. We can define a series and then find the limit: We need $1$ trial with probability $1/2$; $2$ trials with probability $(1/2)^2$, etc. So the expected number of trials is 
\[
\sum_{i=1}^\infty \frac{i}{2^i}.
\]
The limit of this series is $2$.

We can build a recursion: If we flip the coin, we either get a head with probability $1/2$, or get a tail. In the tail case, we are essentially the same as in the beginning. This actually forms a Markov chain or a probabilistic finite state machine. Therefore, $T = 1+0*(1/2)+T*(1/2)$. Solving this formula gives $T = 2$. Similar reasoning is used for calculating the absorbing probability of Markov chains, which is a recurrent system.

For the second approach, we reason the recursion directly by assigning symbols to the state and constructing a symbolic formula. Therefore, we can reason the limiting behavior directly. We use this example to demonstrate how universal and powerful symbolic reasoning is.

%\section{Adapting $\ell_\infty$-techniques}\label{sec:adapt}
%We use two examples to show how we can adapt the $\ell_\infty$-techniques to the $\ell_2$-case. A common ingredient to transfer the techniques is to express the perturbation using $l$ and $u$. As we discussed in~\cref{sec:linf2l2}, we view $l$ and $u$ as symbols now. We can express the $\ell_p$-ball centered at $a$ with radius $\epsilon$ with $l$ and $u$.

%Suppose $a$, $l$, $u$ are vectors in $\R^n$, we can use $u_i\geq l_i$ to ensure $u_i$ is the upper bound. $u_i+l_i = 2a_i$ expresses that $a_i$ is the center. $\norm{ \frac{u-l}{2}}_p\leq \epsilon$ encode that $l$ and $u$ are the bounds of a $\ell_p$-ball with radius $\epsilon$. If we also want to incorporate $x$ as the input as before, we can add $l_i\leq x_i\leq u_i$.

%The first $\ell_\infty$ technique we can transfer is~\cite{linear-prog}. This can be viewed as a symbolic reasoning work with linear programming relaxation. To enable the linear programming,~\cite{linear-prog} over-approximates $\relu$'s computation with a convex polytope. This polytope is defined with symbols $x$ $l$ and $u$. Therefore, we can directly apply their approximation for $\relu$, and the only modification is to express the $\ell_2$-ball with $l$ $u$ and $x$ for each input node.


\section{Proof of~\cref{thm:weighted-avg}}\label{sec:proof}
\begin{proof}
From Rayleigh quotient, for any symmetric matrix $M$, $\eig_{\max}(M) = \max x^T M x$, where $x^Tx = 1$. This is a quadratic program, and its Shor's relaxation is:
\begin{align*}
\begin{split}
   &\max  \inprod{M,X}_F\\
    s.t.\; & X \succeq 0, \trace(IX)=1.
\end{split}
\end{align*}
The dual program is:
\begin{align}\label{eq:max-eigval}
\begin{split}
   &\min  t\\
    s.t.\; & tI-M\succeq 0.
\end{split}
\end{align}
Notice that by S-lemma, this relaxation is exact.

Let us consider the following semidefinite program:
\begin{align*}
\begin{split}
   &\max  \inprod{M,X}_F\\
    s.t.\; & X \succeq 0, X_{ii}=1, i\in [n];
\end{split}
\end{align*}
where $M$ is a symmetric matrix. From Slater's condition, the strong duality holds, and this program has the same value as its dual formulation:
\begin{align*}
\begin{split}
   &\min  \sum_{i=1}^{n} y_i\\
    s.t.&\;\;  \ACTL(y)-M\succeq 0;
\end{split}
\end{align*}
for $y_i\geq 0$. Because we want to use the $\lambda_{\max}$-SDP, we cannot translate $X_{ii}=1$ to $\trace(IX)=1$ directly. However, we can consider a \emph{corrected average} approach to do this translation. 

Let $t = \frac{\sum y_i}{n}$. Therefore, $y = te_{n} - h$ for some $\sum_{i=1}^{n} h_i = 0$, in which we can consider $h$ as a correction term for the average. Then the objective becomes $\sum y_i = n t$, and $\ACTL(y) = \ACTL(te_{n}-h) = tI - \ACTL(u)$. So the dual program can be rewritten as
\begin{align*}
\begin{split}
   &\min\;  nt\\
    s.t.\;\;  tI &- (M+\ACTL(h))\succeq 0;
\end{split}
\end{align*}
where $\sum h_i = 0$. From~\cref{eq:max-eigval}, this is equivalent to $n\min_h \eig_{\max}(M+\ACTL(h))$. Solving the SDP amounts to finding the optimal \emph{correcting vector} $u$ that minimizes the maximum eigenvalue of $M+\ACTL(h)$. %This technique can handle any SDP with constraints of the form $X_{ii}=1$, without any assumption on $M$.

This concludes the proof of~\cref{thm:weighted-avg}.
\end{proof}

\cite{certified_def} used a similar result to train the network with the SDP constraint because $\eig_{\max}$ is a differentiable function and can provide an estimation of the FGL immediately. Their bound is
\[\min_{c\in\R_+^{n}}(\sum_{i=1}^{n} c_i + n\eig_{\max}^+(M-\ACTL(c)),\] 
where $\eig_{\max}^+(x) = \max(\eig_{\max}(x),0)$.

However, our bound is \[\min_h n\eig_{\max}(M+\ACTL(h)),\] where $\sum h_i=0$, and for any symmetric $M$.

Our bound is tighter. Consider
$M = \begin{pmatrix}
1 & 0\\
0& -1
\end{pmatrix}$. We can choose the weighted average vector $h = (-1,1)$, so $M+\ACTL(h)$ is the zero matrix, and $\eig_{\max}(M+\ACTL(h)) = 0$. While for any $c\in \R_+^2$, $\sum_{i=1}^{n} c_i + 2\eig_{\max}^+(M-\ACTL(c)) > 0$.

%\paragraph{Proof of~\cref{thm:upper-bound}}
%\begin{proof}
%Let $M= \begin{pmatrix}
%\obju I & -W^T T\\
%-T W & 2T - u^T u
%\end{pmatrix}$. We want to find an assignment of $\obju$ and $\dualv$ such that $M\succeq 0$. From Schur complement lemma, a sufficient condition for $M$ to be PSD is that $\obju > 0$, and 
%$C = 2T-u^T u - \frac{1}{\obju} T W W^T T \succeq 0$.

%Let $T=\norm{u}_2^2I$, and $\obju = \norm{W}_2^2\norm{u}_2^2$. Then
%\[
%C = 2 \norm{u}_2^2I - u^Tu-\frac{1}{\norm{W}_2^2\norm{u}_2^2}\norm{u}_2^4 W W^T\succeq (2 \norm{u}_2^2-\norm{u}_2^2-\norm{u}_2^2) I.
%\]
%Thus, $C$ is PSD and so is $M$. $\obju = %\norm{W}_2^2\norm{u}_2^2$. A solution to the SDP is $\norm{W}_2\norm{u}_2$, a trivial $\ell_2$-bound for the neural network.
%\end{proof}

