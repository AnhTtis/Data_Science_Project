\section{Discussion}\label{sec:dis}
We provide some discussion on the empirical result and the theoretical aspects of the framework.
\subsection{Empirical Discussion}
\paragraph{RQ1}
From~\cref{tab:result}, we can conclude that ShenBao performs consistently better than BCP, except for a few cases when the model is BCP-trained. BCP-training will regularize the model to have a tighter BCP bound, and thus benefits the BCP certification. Even on these models, ShenBao still achieves comparable performance. This demonstrates the good precision of our framework.

One interesting observation is that networks not trained for certifiably robust purposes are also certifiably robust to some extent from the measurement of ShenBao. We would not observe this with only the measurement from BCP. This is particularly true for the PGD-trained medium model. The certifiable robustness of the model is about $88\%$ with an attack radius of $0.7$, while BCP is unable to certify the robustness at all. One surprising observation is that PGD training was not considered certifiably robust because the PGD attacks are weaker than the certifiably robust bound. However, with ShenBao's measurement, PGD-trained models can achieve similar or even better certifiable robustness compared to BCP-trained models.

\paragraph{RQ2}
The bottleneck for using our framework is solving the SDPs generated from the verification tasks. In our implementation, we use the MOSEK solver, a generic SDP solver. BCP is implemented to use GPUs, and can run very fast, while the MOSEK solver that we use for the evaluation can only run on CPUs.

Because all the SDPs are independent, one can solve all the verification SDPs in parallel when more CPUs are available. There are a few other works on improving the SDP solving for DNN certification. \cite{efficient_sdp} devised a first-order algorithm to solve the SDP that is implementable on GPUs, and successfully scaled the SDP verification to CIFAR-size networks. \cite{chordal_sparsity,explore_chordal} explored the chordal sparsity manifested in the DNN-induced SDPs. The chordal sparsity enables decomposing the large PSD constraint into a few smaller ones, so solving the SDP is much faster. However, solving the SDPs induced by the verification tasks is beyond the scope of this work. Our work demonstrates the power of SDPs in verifying DNNs, and can motivate more future work on specified SDP-solving algorithms for DNN verification.

\paragraph{Practical implication}
Unlike theoretical exponential-time verification tools, e.g., Reluplex~\cite{reluplex}, our framework is theoretically polynomial-time and can handle reasonable real-world size networks like the MNIST network. Moreover, it is powerful enough to reason about various network models and properties beyond the linear-constrained properties on feed-forward networks. 

Our framework complements the current verification methodology. For example, with symbolic reasoning, we can conclude that certifying metric learning models is no different from the feed-forward model. One can then choose techniques from the feed-forward models to verify the metric learning models.

On the other hand, for verification tasks or models without other techniques available, one can try to use quadratic relations to represent the verification task and then relax the program to an SDP. On small models, our framework can provide useful verification measurements. When more efficient and scalable methods are available, our method can also serve as a benchmark that provides information about the more efficient methods. 

\subsection{Theoretical Discussion}\label{sec:theory}
\paragraph{Other relaxations}We broadly categorize any reasoning method, which results in algebraic expressions, to the symbolic reasoning paradigm. The decision or numerical results are usually not immediately available from these expressions so they need further analysis. Therefore, one can view any DNN verification works that require solvers to post-process the algebraic expressions as examples of symbolic reasoning. In this work, we used quadratic encodings and Shor's relaxation to achieve efficiency. One can also consider other symbols and relational representations to express and relax the verification problem.~\cite{reluplex} used Reluplex to exactly verify neural networks.~\cite{linear-prog} used linear programming (LP) to relax the certification problem. Because many verification problems are intrinsically non-linear, one has to relax the problem to enable efficient verification. 

\cite{int-lp} demonstrated that there is an intrinsic precision gap between polynomial-sized LPs and SDPs on some natural optimization problems. This implies that SDP is strictly more powerful than LP in solving intractable problems within polynomial time. It is interesting to understand whether a similar gap exists for DNN verification problems. However, without the QP representations, we would not be able to ask this question.

\paragraph{Quadratic encoding}
Quadratic encoding is fairly expressive. For example, $x(x-1)=0$ can express $x=0$ or $x=1$. This enables the quadratic program to encode many intractable combinatorial problems. See~\cite{modern_co} for more information on the expressiveness of quadratic relations. In the meantime, the quadratic encoding of a problem is not necessarily unique. For example, we presented two exact quadratic encodings for $\relu$ (see~\cref{rm:unique}). This is important when we consider its SDP relaxation because equivalent quadratic programs can have different relaxations, which result in different solutions~\cite{sdp-relax-operator}. Exploring which encoding gives better relaxation is an interesting question but it is beyond the scope of this work. 

\paragraph{SDP relaxation}
Using SDP relaxation to solve intractable problems is pioneered by the seminal Goemans-Williamson algorithm for the MAXCUT problem~\cite{maxcut}. It is an approximation algorithm with a tight bound, and is the optimal polynomial-time algorithm assuming the unique games conjecture~\cite{UGC,ugc_cut}.

As such, an important theoretical problem for the SDP framework is how precise the relaxation is. \cite{sdp_quality} studied cases when the SDP relaxation is precise, i.e., has no precision loss for certifying the robustness of inputs.

\cite{geolip} proved that on two-layer networks, the SDP relaxations have approximation guarantees of $K_G$ (known as the Grothendieck constant) for $\ell_\infty$-FGL and $\sqrt{\pi/2}$ for $\ell_2$-FGL on two-layer networks.
These guarantees rely on the results from the mixed-norm problem. The $p\rightarrow q$ mixed-norm of a matrix $A$ is defined as $\max_{\norm{x}_p = 1}\norm{Ax}_q$.

\cite{cut-norm} proved that the SDP for the $\infty\rightarrow 1$ mixed-norm problem induces an $K_G$-approximation guarantee. The $\infty\rightarrow 2$ mixed-norm problem has an $\sqrt{\frac{\pi}{2}}$ approximation guarantee from its SDP. $\frac{\pi}{2}$ comes from Grothendieck's original paper~\cite{GroIneq} and was rediscovered by~\cite{nest} later. \cite{geolip} built reductions from the FGL estimation to the mixed-norm problem, so the FGL estimation admits the same approximation ratios as the mixed-norm problem.
Moreover, \cite{opt_inf2} showed that assuming $\p\neq \NP$, the $\sqrt{\frac{\pi}{2}}$-approximation guarantee is optimal for the $\infty\rightarrow 2$ mixed-norm problem; and \cite{UGC_inf1_opt} showed that assuming the unique games conjecture, the $K_G$-approximation guarantee is optimal for the $\infty\rightarrow 1$ mixed-norm problem. 

However, there are no approximation guarantees for multi-layer DNN FGL-estimation SDPs, and the authors of~\cite{geolip} posed this problem as an open question.

\paragraph{SDP representation of verification problems}
One perspective of this framework is that the SDP provides a new representation of the verification task. SDP is intrinsically connected to many subjects, so the new representation also establishes relations between the verification problem and other mathematical tools.
As an analogy, in spectral graph theory~\cite{spectra}, one can study the matrix representation of a graph and derive graph properties from these matrices. 

Here we prove an analytical result about the SDPs for FGL estimations on two-layer networks. This exemplifies how the new representation brings new techniques to study the verification problem.

For a two-layer network, the primal SDPs for FGL estimation are of the following form~\cite{geolip}:
\begin{align}\label{eq:fgl-sdp}
\begin{split}
   \max\;\; &\inprod{M,X}_F\\
    s.t.\; & X \succeq 0, X_{ii}=1, i\in [n],
\end{split}
\end{align}
where $M\in \R^{n\times n}$ is a symmetric matrix for some $n\in\Z_+$.

Let $h\in \R^{n}$ be a vector whose entries sum to $0$, i.e., $\sum_{i=1}^{n} h_i = 0$. If we use $O$ to denote the optimal value of~\cref{eq:fgl-sdp}, we have the following result:

\begin{theorem}\label{thm:weighted-avg}
$O = \min_{h}n\eig_{\max}(M+\ACTL(h))$.
\end{theorem}
That is to say, to estimate a precise FGL of the network, one only needs to check the largest eigenvalue of the $M+\ACTL(h)$. This result is motivated by~\cite{lap_eigen}. \cite{certified_def} used a similar technique to train robust networks subject to the SDP constraint. Our result can be viewed as an improvement of theirs. See~\cref{sec:proof} for more discussion.

As we can see from this result, the SDPs contain very rich information about the verification task, especially from the spectrum of the matrices. Therefore, the SDP programs provide new representations of the verification problems, and exploiting the SDP structures brings more understanding of these problems.

Meanwhile, the framework is expressive and can encode many network properties. The resulting SDPs can provide estimations of these properties. It is appealing to train networks with the constraints from the SDPs because this can regularize the network to achieve desired properties. For this purpose, it is promising to analyze the SDPs and discover surrogates amenable to the first-order method.  %Here we prove two analytical results on the SDPs for FGL estimations, which can be used to train the network with the SDPs.
%These results exemplify that the SDPs contain rich information about the verification problems.

%The results are on the primal and dual SDPs in~\cite{geolip} for two-layer network $\ell_2$-FGL estimations.
%For a two-layer network (see~\cref{eq:2-network}), the primal SDP is:
%\begin{align}\label{eq:l2-psdp}
%\begin{split}
%   \max \frac{1}{2}&\sqrt{\trace(\begin{pmatrix}
%A^TA &\;\;\;  A^TAe_n\\
%e_n^TA^TA &\;\;\;  e_n^TA^TAe_n
%\end{pmatrix} X)}\\
%    s.t.\; & X \succeq 0, X_{ii}=1, i\in [n+1],
%\end{split}
%\end{align}
%where $A = W^T \ACTL(v)$.

%The dual SDP is:
%\begin{equation}\label{eq:l2-dsdp}
%\min_{\obju,\dualv}\Big\{\sqrt{\obju}: \begin{pmatrix}
%- \obju I & W^T T\\
%T W & -2T + v^T v
%\end{pmatrix}\preceq 0, \dualv_i\geq 0\Big\},
%\end{equation}
%where $\dualv$ is the vector of the Lagrangian dual variables, and $T=\ACTL(\dualv)$.
%Notice that this dual SDP is the same as LipSDP-Neuron in~\cite{lipsdp}.

%Let $M = \begin{pmatrix}
%A^TA &\;\;\;\; A^TAe_n\\
%e_n^TA^TA  &\;\;\;\; e_n^TA^TAe_n
%\end{pmatrix}$ and $O_p$ be the optimal value of~\cref{eq:l2-psdp}. %and $O_d$ be the optimal value of~\cref{eq:l2-dsdp}. 
%Define $u$ be an $(n+1)$ vector such that $\sum_{i=1}^{n+1}u_i = 0$.


%This result allows us to estimate the FGL from only checking the largest eigenvalue of $M+\ACTL(u)$.

%\begin{theorem}\label{thm:upper-bound}
%$\dualv_i=\norm{v}_2$ and $\obju = (\norm{W}_2\norm{v}_2)^2$ are a feasible solution~\cref{eq:l2-dsdp}.
%\end{theorem}
%Notice that because $\obju = (\norm{W}_2\norm{v}_2)^2$ is a feasible solution, this also recovers the naive upper bound of $K_f$. This upper bound is not surprising, though deriving this result is not as straightforward as using the submultiplicity of norms, i.e., for a network in~\cref{eq:2-network},
%\[K_f\leq \norm{W}_p\norm{\ACTL(\relu)}_p\norm{v}_p.\] The interesting part is to find a feasible solution of $\dualv$. This provides an initialization for the training when the PSD constraint is imposed. We also provide how this can be used when training a neural network~\cref{sec:proof}.

\paragraph{Symbolic framework}
One way of viewing our work is to distill a framework from a few existing examples and generalize this framework to other verification tasks. We argue that this distillation is important and necessary. For example, in~\cref{sec:indept}, we mentioned that~\cite{geolip} showed that the SDPs in~\cite{lipsdp} and~\cite{certified_def} are just Shor's relaxations of~\cref{eq:formal} in the primal and dual forms. \cite{lipsdp} works for multi-layer network $\ell_2$-FGL estimation, and~\cite{certified_def} is designed for two-layer networks when $p=\infty$. However, this duality is not clear without demonstrating the framework. For example, \cite{lipopt} argued that~\cite{lipsdp} does not work when $p=\infty$, and~\cite{sdp_rob_local} believed that~\cite{certified_def} only works for two-layer networks. Nevertheless, the SDPs are general enough, and this is straightforward from our framework. Similarly, we implemented a tool for $\ell_2$-robustness certification, which is natural from our framework. However, even though it has been a while since the SDP for $\ell_\infty$-robustness certification was proposed~\cite{sdp_rob_local}, we are unaware of any existing SDP tools to certify $\ell_2$-robustness. The $\ell_2$-robustness certification is an active topic~\cite{bcp,gloro}.%\zw{move to intro}

\cite{convex_barrier} tried to unify efficient robustness certification works, which the authors claimed is a convex relaxation barrier. However, they only studied LP relaxation methods and were unable to include the SDP certification method~\cite{sdp_rob_local}. As we have discussed, SDP is strictly more powerful than LP. Our framework can be viewed as complementing~\cite{convex_barrier} towards a tighter convex relaxation barrier.
