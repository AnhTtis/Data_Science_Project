\section{Introduction}

Deep neural networks (DNN) have achieved unprecedented success in many complex tasks and has been integrated into modern software systems~\cite{Krizhevsky_imagenetclassification,nlp_2013}. This brings new challenges and opportunities to the program-verification community to analyze these programs. Like traditional software, DNNs were also shown vulnerable to various attacks. Among them, the most unique and notable are adversarial attacks. 

Adversarial examples were first identified in~\cite{szegedy2014intriguing,goodfellow2015explaining}. Later works found that adversarial attacks are common for different neural-network architectures and tasks~\cite{adv-rnn,adv-metric,adv-transformer}. Adversarial attacks are small perturbations that can cause a prediction to change when they are applied to the inputs, and these perturbations are unnoticeable to humans. This causes serious concerns for DNN safety and reliability because a malicious user can exploit these attacks. Researchers invented various defense mechanisms but they were later shown still vulnerable to those attacks~\cite{CW,papernot2015limitations,madry2018towards}. More recently, the community focuses on certifiable robustness against adversarial attacks~\cite{albarghouthi2021introduction,cav-safety,reluplex,certified_def}.

\paragraph{Neural-network verification} To certify a prediction, we need to estimate the change of the output given an input perturbation. Unfortunately, exact verification has been shown $\NP$/$\coNP$-hard~\cite{jordan2021exactly,reluplex,IUA}. To enable efficient DNN verification, we have to over-approximate the computation of DNNs. As a result, the key challenge is to balance efficiency and precision. 

A large body of verification works is based on the classical abstract interpretation~\cite{abstract_int}. Verification against adversarial attacks requires capturing all possible executions. Abstract interpretation starts by defining an abstract domain that can over-approximate all the inputs, and re-interpret the network execution in terms of the abstract domain~\cite{ai2,symb_int,zonotope}. Because this re-interpretation is sound, and the execution is compositional, abstract interpretation enables a sound verification of the network. However, it remains challenging to verify DNNs with non-linear perturbations and unconventional architectures because the abstract domain is usually defined in terms of linear relations, and reasoning non-linear computations involves many ad-hoc techniques.

\paragraph{Symbolic framework}In this work, we propose a new program-reasoning framework for DNN verification. We refer to this framework as symbolic reasoning~\footnote{Our symbolic reasoning framework can also be viewed as an abstract interpretation example with the symbolic domain.}. %We first want to emphasize that this framework is not entirely new. Symbolic reasoning is a fundamental reason paradigm and not limited to NN verification. Many past verification works can be categorized as examples of symbolic reasoning. 
Our framework contains two distinguished components: symbolic domains and quadratic relations. Symbolic domains have flexible semantics, allowing us to reason tasks that appear challenging for non-symbolic domains. We will elaborate on this paradigm and demonstrate its power for DNN verification. Similar to abstract interpretation, this program reasoning scheme is also compositional, and can be easily adapted to different architectures.

We use quadratic relations to encode the computation and attack constraints rather than the commonly used linear relations. Quadratic encoding is quite expressive. For example, many intractable combinatorial optimization problems can be written as quadratic programs~\cite{maxcut}. As we will demonstrate, it can precisely encode various DNN verification tasks as algebraic formulas. The verification problem is then translated into a quadratic program (QP). This allows us to use mathematical tools to accurately analyze those tasks. Meanwhile, this precise representation also enables us to ask important theoretical questions about the verification problems. We will provide quadratic encodings for various tasks, such as different attacks and activations. Constructing the QPs for different verification tasks amounts to assembling these components together.

Because quadratic encoding is fairly expressive, the resulting QPs are in general hard to solve. To enable efficiency, we consider the semidefinite relaxation of quadratic programs. Semidefinite programming (SDP) is a unique topic where many subjects meet together such as theoretical computer science, optimization, control theory, combinatorics and functional analysis. For example, \cite{flag} introduced flag algebras, which uses SDP to derive bounds for extremal combinatorics problems. In particular, some SDP-induced algorithms are optimal within polynomial time assuming some complexity-theoretical conjectures~\cite{opt_inf2,UGC_inf1_opt}. We believe that formulating the verification problems as semidefinite programs introduces new representations and perspectives for the verification tasks, and brings more theoretical and practical tools to address those problems. 

\paragraph{Relevance} There have been a few works on using SDP to verify DNNs~\cite{lipsdp,certified_def,sdp_rob_local,geolip}. These works appear designed for specific verification tasks and are less accessible to researchers, as some works~\cite{lipopt,sdp_rob_local} claimed that one technique cannot transfer to another setting (see more discussion in~\cref{sec:theory}). We will demonstrate the reasoning framework behind these works to popularize them. Moreover, we show that the paradigm is powerful and can be applied beyond the application scope of these works. In the meantime, there are a few other works aiming to understand the quality and improve the empirical performance of these works~\cite{efficient_sdp,chordal_sparsity,explore_chordal,sdp_quality}. Our paper implies that these works can be more impactful beyond their original scope. We also present a comprehensive discussion of this framework, which sheds light on future directions.

\paragraph{Contributions}To summarize, our paper makes the following contributions:
\begin{enumerate}
    \item We provide a novel systematic neural-network reasoning paradigm and it provides new representations for many verification tasks, i.e., as QPs and SDPs. These representations can bring new theoretical tools, such as matrix analysis and approximation theory, to those tasks (\cref{sec:method,sec:dis});
    \item We present encodings for various mathematical components, which allows us to verify different properties and network structures, especially those that appear challenging for linear-relational non-symbolic domains (\cref{sec:power});
    \item We empirically examine our framework on a specific DNN verification task: $\ell_2$-robustness certification. The evaluation shows that the result from our framework is consistently precise ($60\%$ improvement on average compared to the benchmark), and it can handle practical-size DNNs (\cref{sec:eva}).
\end{enumerate}