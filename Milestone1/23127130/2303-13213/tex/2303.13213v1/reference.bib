@ARTICLE{MP,
  author={González, David and Pérez, Joshué and Milanés, Vicente and Nashashibi, Fawzi},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={A Review of Motion Planning Techniques for Automated Vehicles}, 
  year={2016},
  volume={17},
  number={4},
  pages={1135-1145},
  doi={10.1109/TITS.2015.2498841}}


@Article{obstacle,
AUTHOR = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph},
TITLE = {Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {6},
ARTICLE-NUMBER = {2140},
PubMedID = {33803889},
ISSN = {1424-8220},
DOI = {10.3390/s21062140}
}


@article{Vision,
author = {O'Malley, R. and Glavin, Martin and Jones, Edward},
year = {2011},
month = {Mar.},
pages = {1 - 10},
title = {Vision-based detection and tracking of vehicles to the rear with perspective correction in low-light conditions},
volume = {5},
journal = {Intelligent Transport Systems, IET},
doi = {10.1049/iet-its.2010.0032}
}

@ARTICLE{AutoDriving,
  author={Paden, Brian and Čáp, Michal and Yong, Sze Zheng and Yershov, Dmitry and Frazzoli, Emilio},
  journal={IEEE Transactions on Intelligent Vehicles}, 
  title={A Survey of Motion Planning and Control Techniques for Self-Driving Urban Vehicles}, 
  year={2016},
  volume={1},
  number={1},
  pages={33-55},
  doi={10.1109/TIV.2016.2578706}}


@article{Ast,
author = {Maxim Likhachev and Dave Ferguson},
title ={Planning Long Dynamically Feasible Maneuvers for Autonomous Vehicles},
journal = {The International Journal of Robotics Research},
volume = {28},
number = {8},
pages = {933-945},
year = {2009},
doi = {10.1177/0278364909340445}
}

@ARTICLE{RRT,
  author={Kuwata, Yoshiaki and Teo, Justin and Fiore, Gaston and Karaman, Sertac and Frazzoli, Emilio and How, Jonathan P.},
  journal={IEEE Transactions on Control Systems Technology}, 
  title={Real-Time Motion Planning With Applications to Autonomous Urban Driving}, 
  year={2009},
  volume={17},
  number={5},
  pages={1105-1118},
  doi={10.1109/TCST.2008.2012116}}

@ARTICLE{APT,
  author={Huang, Yanjun and Ding, Haitao and Zhang, Yubiao and Wang, Hong and Cao, Dongpu and Xu, Nan and Hu, Chuan},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={A Motion Planning and Tracking Framework for Autonomous Vehicles Based on Artificial Potential Field Elaborated Resistance Network Approach}, 
  year={2020},
  volume={67},
  number={2},
  pages={1376-1386},
  doi={10.1109/TIE.2019.2898599}}

@ARTICLE{MPC,
  author={Shen, Chao and Shi, Yang and Buckham, Brad},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={Trajectory Tracking Control of an Autonomous Underwater Vehicle Using Lyapunov-Based Model Predictive Control}, 
  year={2018},
  volume={65},
  number={7},
  pages={5796-5805},
  doi={10.1109/TIE.2017.2779442}}

@ARTICLE{V2X,
  author={Liu, Jizheng and Wang, Zhenpo and Zhang, Lei},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Integrated Vehicle-Following Control for Four-Wheel-Independent-Drive Electric Vehicles Against Non-Ideal V2X Communication}, 
  year={2022},
  volume={71},
  number={4},
  pages={3648-3659},
  doi={10.1109/TVT.2022.3141732}}

@ARTICLE{Signal,
  author={Li, Li and Lv, Yisheng and Wang, Fei-Yue},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Traffic signal timing via deep reinforcement learning}, 
  year={2016},
  volume={3},
  number={3},
  pages={247-254},
  doi={10.1109/JAS.2016.7508798}}

@Article{Fleet,
AUTHOR = {Soni, Aakash and Hu, Huosheng},
TITLE = {Formation Control for a Fleet of Autonomous Ground Vehicles: A Survey},
JOURNAL = {Robotics},
VOLUME = {7},
YEAR = {2018},
NUMBER = {4},
ARTICLE-NUMBER = {67},
ISSN = {2218-6581},
DOI = {10.3390/robotics7040067}
}

@Article{Survey,
AUTHOR = {Pendleton, Scott Drew and Andersen, Hans and Du, Xinxin and Shen, Xiaotong and Meghjani, Malika and Eng, You Hong and Rus, Daniela and Ang, Marcelo H.},
TITLE = {Perception, Planning, Control, and Coordination for Autonomous Vehicles},
JOURNAL = {Machines},
VOLUME = {5},
YEAR = {2017},
NUMBER = {1},
ARTICLE-NUMBER = {6},
ISSN = {2075-1702},
DOI = {10.3390/machines5010006}
}

@ARTICLE{NA,
  author={Nguyen, Khoi Khac and Duong, Trung Q. and Vien, Ngo Anh and Le-Khac, Nhien-An and Nguyen, Minh-Nghia},
  journal={IEEE Access}, 
  title={Non-Cooperative Energy Efficient Power Allocation Game in D2D Communication: A Multi-Agent Deep Reinforcement Learning Approach}, 
  year={2019},
  volume={7},
  number={},
  pages={100480-100490},
  doi={10.1109/ACCESS.2019.2930115}}

@ARTICLE{UAV,
  author={Cui, Jingjing and Liu, Yuanwei and Nallanathan, Arumugam},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Multi-Agent Reinforcement Learning-Based Resource Allocation for UAV Networks}, 
  year={2020},
  volume={19},
  number={2},
  pages={729-743},
  doi={10.1109/TWC.2019.2935201}}

@ARTICLE{TC,
  author={Yang, Jiachen and Zhang, Jipeng and Wang, Huihui},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Urban Traffic Control in Software Defined Internet of Things via a Multi-Agent Deep Reinforcement Learning Approach}, 
  year={2021},
  volume={22},
  number={6},
  pages={3742-3754},
  doi={10.1109/TITS.2020.3023788}}

@article{DRL,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@INPROCEEDINGS{CNN-RNN,
  author={Huang, Chih-Wei and Chiang, Chiu-Ti and Li, Qiuhui},
  booktitle={2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)}, 
  title={A study of deep learning networks on mobile traffic forecasting}, 
  year={2017},
  month={Oct.},
  volume={},
  address ={Montreal, Quebec, Canada},
  pages={1-6},
  doi={10.1109/PIMRC.2017.8292737}}

@ARTICLE{Imitation,
  author={Yu, Shuai and Gong, Xiaowen and Shi, Qian and Wang, Xiaofei and Chen, Xu},
  journal={IEEE Internet of Things Journal}, 
  title={EC-SAGINs: Edge-Computing-Enhanced Space–Air–Ground-Integrated Networks for Internet of Vehicles}, 
  year={2022},
  volume={9},
  number={8},
  pages={5742-5754},
  doi={10.1109/JIOT.2021.3052542}}

@inproceedings{COMA,
author = {Foerster, Jakob N. and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
title = {Counterfactual Multi-Agent Policy Gradients},
year = {2018},
isbn = {978-1-57735-800-8},
abstract = {Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence},
month={Feb.},
address = {New Orleans, Louisiana, USA},
}

@inproceedings{RIAL,
 author = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},

 title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
 volume = {29},
 address = {Barcelona, Spain},
 month = {Dec.},
 year = {2016}
}

@inproceedings{ATOC,
 author = {Jiang, Jiechuan and Lu, Zongqing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 address = {Montreal, Canada},
 month = {Dec.},
 title = {Learning Attentional Communication for Multi-Agent Cooperation},
 volume = {31},
 year = {2018}
}


@InProceedings{Mean_Field,
  title = 	 {Mean Field Multi-Agent Reinforcement Learning},
  author =       {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5571--5580},
  year = 	 {2018},

  volume = 	 {80},
  address = {Stockholm, Sverige},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {Jul.},
}


@inproceedings{IQL,
author = {Tan, Ming},
title = {Multi-Agent Reinforcement Learning: Independent versus Cooperative Agents},
year = {1993},
month = {Jun.},
booktitle = {Proceedings of the Tenth International Conference on International Conference on Machine Learning},
address = {Amherst, MA, USA},
}

@inproceedings{MADDPG,
author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
year = {2017},
isbn = {9781510860964},
abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
address = {Long Beach, California, USA},
}

@InProceedings{QMIX,
  title = 	 {{QMIX}: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  author =       {Rashid, Tabish and Samvelyan, Mikayel and Schroeder, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year = 	 {2018},
  address = {Stockholm, Sverige},
  month = 	 {Jul.},
  abstract = 	 {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.}
}

@inproceedings{
DOP,
title={DOP: Off-Policy Multi-Agent Decomposed Policy Gradients},
author={Yihan Wang and Beining Han and Tonghan Wang and Heng Dong and Chongjie Zhang},
booktitle={International Conference on Learning Representations},
address = {Virtual Edition},
month = {May},
year={2021},
}

@inproceedings{VMAC, title={Value-Decomposition Multi-Agent Actor-Critics}, volume={35}, DOI={10.1609/aaai.v35i13.17353}, abstractNote={The exploitation of extra state information has been an active research area in multi-agent reinforcement learning (MARL). QMIX represents the joint action-value using a non-negative function approximator and achieves the best performance on the StarCraft II micromanagement testbed, a common MARL benchmark. However, our experiments demonstrate that, in some cases, QMIX performs sub-optimally with the A2C framework, a training paradigm that promotes algorithm training efficiency. To obtain a reasonable trade-off between training efficiency and algorithm performance, we extend value-decomposition to actor-critic methods that are compatible with A2C and propose a novel actor-critic framework, value-decomposition actor-critic (VDAC). We evaluate VDAC on the StarCraft II micromanagement task and demonstrate that the proposed framework improves median performance over other actor-critic methods. Furthermore, we use a set of ablation experiments to identify the key factors that contribute to the performance of VDAC.}, number={13}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence }, author={Su, Jianyu and Adams, Stephen and Beling, Peter}, address = {Virtual Edition}, year={2021}, month={May},}

@inproceedings{Neigh, title={Neighborhood Cognition Consistent Multi-Agent Reinforcement Learning}, volume={34}, DOI={10.1609/aaai.v34i05.6212}, abstractNote={&lt;p&gt;Social psychology and real experiences show that cognitive consistency plays an important role to keep human society in order: if people have a more consistent cognition about their environments, they are more likely to achieve better cooperation. Meanwhile, only cognitive consistency within a neighborhood matters because humans only interact directly with their neighbors. Inspired by these observations, we take the first step to introduce &lt;em&gt;neighborhood cognitive consistency&lt;/em&gt; (NCC) into multi-agent reinforcement learning (MARL). Our NCC design is quite general and can be easily combined with existing MARL methods. As examples, we propose neighborhood cognition consistent deep Q-learning and Actor-Critic to facilitate large-scale multi-agent cooperations. Extensive experiments on several challenging tasks (i.e., packet routing, wifi configuration and Google football player control) justify the superior performance of our methods compared with state-of-the-art MARL approaches.&lt;/p&gt;}, number={05}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Mao, Hangyu and Liu, Wulong and Hao, Jianye and Luo, Jun and Li, Dong and Zhang, Zhengchao and Wang, Jun and Xiao, Zhen}, address={New York Hilton Midtown, New York, USA}, year={2020}, month={Apr.}, }

@inproceedings{MGAN,
author = {Xu, Zhiwei and Zhang, Bin and Bai, Yunpeng and Li, Dapeng and Fan, Guoliang},
title = {Learning to Coordinate via Multiple Graph Neural Networks},
year = {2021},
month = {Dec.},
address = {Berlin, Heidelberg},
booktitle = {Neural Information Processing: 28th International Conference},
}

@ARTICLE{IPPO,
  author={Yang, Jiachen and Zhang, Jipeng and Wang, Huihui},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Urban Traffic Control in Software Defined Internet of Things via a Multi-Agent Deep Reinforcement Learning Approach}, 
  year={2021},
  volume={22},
  number={6},
  pages={3742-3754},
  doi={10.1109/TITS.2020.3023788}}

@INPROCEEDINGS{Games,
  author={Palanisamy, Praveen},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multi-Agent Connected Autonomous Driving using Deep Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  month={Jul.},
  address={Glasgow, UK},
  pages={1-7},
  doi={10.1109/IJCNN48605.2020.9207663}}

@InProceedings{DIAL,
author="Bhalla, Sushrut
and Ganapathi Subramanian, Sriram
and Crowley, Mark",
editor="Goutte, Cyril
and Zhu, Xiaodan",
title="Deep Multi Agent Reinforcement Learning for Autonomous Driving",
booktitle="Advances in Artificial Intelligence",
year="2020",
month="May",
address="Cham",

}

@article{GCQ,
author = {Chen, Sikai and Dong, Jiqian and Ha, Paul (Young Joun) and Li, Yujie and Labi, Samuel},
title = {Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles},
journal = {Computer-Aided Civil and Infrastructure Engineering},
volume = {36},
number = {7},
pages = {838-857},
year = {2021}
}

@inproceedings{Dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning},
  year={2016},
  month={Jun.},
  address={New York, USA}
}

@ARTICLE{GNN,
  author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Comprehensive Survey on Graph Neural Networks}, 
  year={2021},
  volume={32},
  number={1},
  pages={4-24},
  doi={10.1109/TNNLS.2020.2978386}}

@inproceedings{GCN,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
month={Apr.},
address={Toulon, France}
}

@article{PPO,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year={2017},  
  journal={arXiv:1707.06347}

}


@ARTICLE{SGNN,
  author={Gao, Zhan and Isufi, Elvin and Ribeiro, Alejandro},
  journal={IEEE Transactions on Signal Processing}, 
  title={Stochastic Graph Neural Networks}, 
  year={2021},
  volume={69},
  number={},
  pages={4428-4443},
  doi={10.1109/TSP.2021.3092336}}

@book{DecPOMDP,
  title={A concise introduction to decentralized POMDPs},
  author={Oliehoek, Frans A and Amato, Christopher},
  year={2016},
  publisher={Springer},
  volume={1}
}

@ARTICLE{RES,
  author={Isufi, Elvin and Loukas, Andreas and Simonetto, Andrea and Leus, Geert},
  journal={IEEE Transactions on Signal Processing}, 
  title={Filtering Random Graph Processes Over Random Time-Varying Graphs}, 
  year={2017},
  volume={65},
  number={16},
  pages={4406-4421},
  doi={10.1109/TSP.2017.2706186}}

@ARTICLE{Flow,
  author={Wu, Cathy and Kreidieh, Abdul Rahman and Parvate, Kanaad and Vinitsky, Eugene and Bayen, Alexandre M.},
  journal={IEEE Transactions on Robotics}, 
  title={Flow: A Modular Learning Framework for Mixed Autonomy Traffic}, 
  year={2022},
  volume={38},
  number={2},
  pages={1270-1286},
  doi={10.1109/TRO.2021.3087314}}


@InProceedings{Benchmarks,
  title = 	 {Benchmarks for reinforcement learning in mixed-autonomy traffic},
  author =       {Vinitsky, Eugene and Kreidieh, Aboudy and Flem, Luc Le and Kheterpal, Nishant and Jang, Kathy and Wu, Cathy and Wu, Fangyu and Liaw, Richard and Liang, Eric and Bayen, Alexandre M.},
  booktitle = 	 {Proceedings of The 2nd Conference on Robot Learning},
  pages = 	 {399--409},
  year = 	 {2018},
  volume = 	 {87},
  month = 	 {Oct.},
  address = {Zurich, Switzerland},
}

@INPROCEEDINGS{FIRL,
  author={Xu, Xing and Li, Rongpeng and Zhao, Zhifeng and Zhang, Honggang},
  booktitle={IEEE International Conference on Communications}, 
  title={Communication-Efficient Consensus Mechanism for Federated Reinforcement Learning}, 
  year={2022},
  month={May},
  address = {Virtual Edition}
 }

@INPROCEEDINGS{SVMIX,
  author={Xiao, Baidi and Li, Rongpeng and Wang, Fei and Peng, Chenghui and Wu, Jianjun and Zhao, Zhifeng and Zhang, Honggang},
  booktitle={IEEE 97th Vehicular Technology Conference (VTC-Spring)}, 
  title={Stochastic Graph Neural Network-based Value Decomposition for Multi-Agent Reinforcement Learning in Urban Traffic Control}, 
  year={2023},
  month={Jun.},
  address = {Florence, Italy}
 }


 @article{eigen,
title = {Laplacian matrices of graphs: a survey},
journal = {Linear Algebra and its Applications},
volume = {197-198},
pages = {143-176},
year = {1994},
issn = {0024-3795},
author = {Russell Merris},
abstract = {Let G be a graph on n vertices. Its Laplacian matrix is the n-by-n matrix L(G)D(G)−A(G), where A(G) is the familiar (0,1) adjacency matrix, and D(G) is the diagonal matrix of vertex degrees. This is primarily an expository article surveying some of the many results known for Laplacian matrices. Its six sections are: Introduction, The Spectrum, The Algebraic Connectivity, Congruence and Equivalence, Chemical Applications, and Immanants.}
}

@article{IDM,
  title={Congested traffic states in empirical observations and microscopic simulations},
  author={Treiber, Martin and Hennecke, Ansgar and Helbing, Dirk},
  journal={Physical review E},
  volume={62},
  number={2},
  pages={1805},
  year={2000},
  publisher={APS}
}

@INPROCEEDINGS{CACC,
  author={Schakel, Wouter J. and van Arem, Bart and Netten, Bart D.},
  booktitle={13th International IEEE Conference on Intelligent Transportation Systems}, 
  title={Effects of Cooperative Adaptive Cruise Control on traffic flow stability}, 
  year={2010},
  month={Sep.},
  address={Madeira Island, Portugal}
}

@article{Frequency-domain,
title = {Stabilizing mixed vehicular platoons with connected automated vehicles: An H-infinity approach},
journal = {Transportation Research Part B: Methodological},
volume = {132},
pages = {152-170},
year = {2020},
author = {Yang Zhou and Soyoung Ahn and Meng Wang and Serge Hoogendoorn},
}

@article{Physics-based,
title = {A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {125},
pages = {103008},
year = {2021},
author = {Xuan Di and Rongye Shi}
}

@article{rd,
  title={Multi-Agent Collaboration via Reward Attribution Decomposition},
  author={Zhang, Tianjun and Xu, Huazhe and Wang, Xiaolong and Wu, Yi and Keutzer, Kurt and Gonzalez, Joseph E and Tian, Yuandong},
   year={2020},
   journal={arXiv:2010.08531}

}