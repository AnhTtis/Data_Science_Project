% !TEX root =  paper.tex
\section{Introduction}\label{sec:introduction}

An interesting problem that currently is a hot topic in the security and software engineering research communities~\cite{dullien2005graph, khoo2013rendezvous, alrabaee2015sigma}, is the {\em binary similarity problem}. That is to determine if two functions in assembly code are compiled from the same source code~\cite{massarelli2021function}. In this case, the two functions are {\em similar}. This problem is far from trivial: it is well-known that different compilers and optimization levels radically change the shape of the generated assembly code.

Binary similarity has many applications, including plagiarism detection, malware detection and classification, and vulnerability detection~\cite{DBLP:conf/pldi/DavidPY16, DBLP:conf/pldi/DavidPY17, egele2014blanket}. It can also be a valid aid for a reverse engineer as it helps with the identification of functions taken from well-known libraries or open-source software. Recent research~\cite{massarelli2021function} shows that techniques for binary similarity generalize, as they are are able to find similarities between semantically similar functions.

We can distinguish binary similarity solutions between the ones that use deep neural networks (DNNs), like~\cite{ding2019asm2vec, xu2017neural, massarelli2021function}, and the ones that do not, like~\cite{dullien2005graph, DBLP:conf/acsac/PewnySBHR14, DBLP:conf/pldi/DavidY14}.
Nearly all of the most recent works rely on DNNs, which offer in practice state-of-the-art performance while being computationally inexpensive. This aspect is particularly apparent when compared with solutions that build on symbolic execution or other computationally intensive techniques.

However, a drawback of DNN-based solutions is their sensitivity to adversarial attacks~\cite{yuan2019adversarial} where an adversary crafts an innocuously looking instance with the purpose of misleading the target neural network model. Successful adversarial attacks have been well-documented for DNNs that process, for example, images~\cite{DBLP:journals/corr/SzegedyZSBEGF13, goodfellow2014explaining, carlini2017towards}, audio and video samples~\cite{DBLP:conf/nips/0001QLSKM19}, and text~\cite{DBLP:conf/emnlp/JiaL17}.

Binary similarity systems are an attractive target for an adversary. For example, an attacker could: (1) hide a malicious function inside a firmware by making it similar to a benign white-listed function; (2) make a plagiarized function dissimilar to the original one; or (3) replace a function with one containing a known vulnerability and made dissimilar from the publicly known instance.

In this paper, we say an attack is {\bf targeted} when the goal is to make a rogue function be the most similar to a target, as with example (1). An attack is {\bf untargeted} when the goal is to make a rogue function the most dissimilar from its original self, as with examples (2) and (3). In both contexts, the adversarial instance has to preserve the semantics of the rogue function as in its original form: that is, it must have the same execution behavior.

Surprisingly, in spite of the wealth of works identifying similar functions with ever improving accuracy, we found that the resilience of (DNN-based) binary similarity solutions against adversarial attacks is yet to be investigated.

In this paper, we aim to close this gap by proposing and evaluating techniques for targeted and untargeted attacks using both 
 \textit{black-box} (where adversaries have access to the similarity model without knowing its internals) and \textit{white-box} (where they know also its internals) methods.

For the black-box scenario, we study a greedy approach that modifies a function by adding a single assembly instruction to its body at each optimization step. Where applicable, we also consider an enhanced gray-box~\cite{pierazzi2020intriguing} variant that, leveraging limited knowledge of the model, chooses only between instructions that the model treats as distinct. We then propose a novel black-box technique, which we call {\em Spatial Greedy}, where we transform the discrete space of assembly instructions into a continuous space using a technique based on instruction embeddings~\cite{DBLP:conf/nips/MikolovSCCD13}. This technique is on par or outperforms the gray-box greedy attack without requiring any knowledge of the model.
For the white-box scenario, we repurpose a method for adversarial attacks on images that relies on gradient descent~\cite{madry2017towards} and use it to drive instruction insertion decisions.

We test our techniques against state-of-the-art binary similarity systems---Gemini~\cite{xu2017neural}, GMN~\cite{DBLP:conf/icml/LiGDVK19}, and SAFE~\cite{massarelli2021function}---showing that they are vulnerable to targeted and untargeted attacks performed with our methods. For the best attack technique, the tested instances that could mislead the target model were 36.6\% for Gemini, 59.68\% for GMN, and 83.43\% for SAFE in the \textbf{targeted} scenario, while in the \textbf{untargeted} one they were, respectively, 53.89\%, 93.81\%, and 90.62\%. All three models appear inherently weaker in the face of an attacker seeking to stage untargeted attacks: with only a handful of added instructions, the attacker may evade state-of-the-art binary similarity models with high probability.

\subsection{Contributions}
This paper makes the following contributions:
\begin{itemize}
	\item we propose to study the problem of adversarial attacks against binary similarity systems, identifying targeted and untargeted attack opportunities;
	\item	we investigate black-box attacks against DNN-based binary similarity systems, exploring a greedy approach based on instruction insertion. Where applicable, we enhance it with partial knowledge of the model sensitivity to instruction types for efficiency;
	\item we propose Spatial Greedy, a fully black-box technique that matches or outperforms gray-box greedy by using embeddings to guide instruction selection;
	\item we investigate white-box attacks against DNN-based binary similarity systems, exploring a gradient-guided search strategy for inserting instructions;
	\item we conduct an extensive experimental evaluation of our techniques for targeted and untargeted attack scenarios against three highly performant~\cite{marcelli2022machine} systems backed by largely different models.
\end{itemize}
