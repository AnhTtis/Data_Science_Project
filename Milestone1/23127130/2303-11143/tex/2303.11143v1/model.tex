% !TEX root =  paper.tex

\section{Threat Model and Problem Definition}
In this section, we define our threat model together with the problem of attacking binary similarity models.

\subsection{Threat Model}
Following the description provided in Section \ref{sec:NNThreats}, we consider two different attack scenarios: respectively, a black-box and a white-box one. In the first case, the adversary has no knowledge of the target binary similarity model; nevertheless, they can perform an unlimited number of queries to observe the output produced by the model. In the second case, we assume that the attacker has perfect knowledge of the target binary similarity model. 

\subsection{Problem Definition}
Let $sim$ be a similarity function that takes as input two functions, $f_1$ and $f_2$, and returns a real number, the {\em similarity score} between them, in $[0,1]$.

We define two binary functions to be \textit{semantically equivalent} if they are two implementations of the same abstract functionality. We assume that there exists an adversary that wants to attack the similarity function. The adversary can mount two different kind of attacks:
\begin{itemize}
	\item \textbf{targeted attack}. Given two binary functions, $f_1$ (identified as \textit{source}) and $f_2$ (identified as \textit{target}), the adversary wants to find a binary function $f_{adv}$  semantically equivalent to $f_1$ such that:
	\begin{center}
		$sim(f_{adv}, f_{2}) \ge \mT$.
	\end{center}
	Where $\mT$ is a success threshold\footnotemark{} chosen by the attacker depending on the victim at hand.
	\item \textbf{untargeted attack}. Given a binary function $f_1$, the adversary goal consists of finding a binary function $f_{adv}$ semantically equivalent to $f_1$ such that:
	\begin{center}
		$sim(f_{1}, f_{adv}) \le \mU$.
	\end{center}
	The threshold $\mU$ is the analogous of the previous case for the untargeted attack scenario.
\end{itemize}
Loosely speaking, in the first case, the adversarial sample has to be similar to a target, as in the example scenario (1) presented in Section \ref{sec:introduction}. In the second one, the adversarial sample has to be dissimilar from its original version, as in the example scenarios (2) and (3) also from Section \ref{sec:introduction}. 

\begin{figure*}[t!]
	\centering
	\includegraphics[width=7in]{schema3.pdf}
	\caption{Overall workflow of the \textit{black-box} $\varepsilon$-greedy action-selection strategy in the \textit{targeted} scenario.}
	\label{img:advWork}
\end{figure*}

\subsection{Perturbation Selection}\label{ss:perturbation-selection}
Given a binary function $f_1$, our attack consists in applying to it perturbations that do not alter its semantics.

To study the feasibility of our approach, we choose \textit{dead branch addition} (DBA) among the suitable perturbations outlined in Section \ref{sec:SemPres}. We find DBA adequate for this study for two reasons: it is sufficiently expressive so as to affect heterogeneous models (which may not hold for others\footnotemark{}) and its implementation complexity for an attacker is fairly limited. Nonetheless, other choices remain possible, as we will further discuss in Section~\ref{se:limitations}.

\iffalse
\CONO{Perhaps we can discuss here why other transformations are left to future work and can only made the approach stronger (but we showed already that the attacks are feasible). Let's see if we find a better place later, though.}
\GADL{I agree that a few lines would help.}
\GC{We restrict our choice to this perturbation as is it the most generic with respect to the attacked model. Differently from \textbf{IR} and \textbf{RR}, which are ineffective against models counting classes of instructions inside basic blocks (i.e., Gemini~\cite{xu2017neural} and GMN~\cite{DBLP:conf/icml/LiGDVK19}), \textbf{DBA} adds both basic blocks and instructions in the target function, resulting in a perturbation that can easily trigger the attacked model.}
\fi

At each application, our embodiment of DBA inserts in the binary code of $f_1$ one or more instructions in a new or existing basic block guarded by a branch that is never taken at runtime (i.e., we use an always-false branch predicate).

Such a perturbation can be done at compilation time or on an existing binary function instance. For our study, we apply DBA during compilation by adding placeholder blocks as inline assembly, which eases the generation of many adversarial samples from a single attacker-controlled code. State-of-the-art binary rewriting techniques would work analogously over already-compiled source functions.

\footnotetext[1]{Although $f_{adv}$ and $f_2$ are similar for the model, they are not semantically equivalent: this is precisely the purpose of an attack that wants to fool the model to consider them as such, while they are not.}

\footnotetext[2]{For example, basic block-local transformations such as IR and RR would have limited efficacy on models that study an individual block for its instruction types and counts or other coarse-grained abstractions. This is the case with Gemini and GMN that we attack in this paper.}

We currently do not attempt to conceal the nature of our branch predicates for preprocessing robustness, which~\cite{pierazzi2020intriguing} discusses as something that attackers should be wary of to mount stronger attacks. We believe off-the-shelf obfuscations (e.g., opaque predicates, mixed boolean-arithmetic expressions) or more complex perturbation choices may improve our approach in this respect. Nevertheless, our main goal was to investigate its feasibility in the first place.
