% !TEX root =  paper.tex

\section{Background}
	
In this section, we provide background knowledge for adversarial attacks against models for code analysis. Then, we introduce a categorization of semantics-preserving perturbations for binary functions.

\subsection{Threats to Neural Networks}\label{sec:NNThreats}

As first pointed out in \cite{DBLP:conf/pkdd/BiggioCMNSLGR13} and \cite{DBLP:journals/corr/SzegedyZSBEGF13}, deep learning models suffer from different kinds of threats. Specifically, we can distinguish between attacks performed at training time (such as data poisoning) and the ones performed at the testing stage (i.e., adversarial examples generation), which can be considered evasive techniques. In this paper, we are interested in this second alternative, so as to force already-trained models into making wrong predictions. 

In this context, we can define the threat model according to adversarial \textit{knowledge} and \textit{specificity}.

\subsubsection{Adversarial Knowledge}
We can describe a deep learning model through different aspects: training data, layers architecture, loss function, and weights parameters. Having complete or partial knowledge about such elements can facilitate the attack from a computational point of view. According to seminal works in the area~\cite{BIGGIO2018317, pierazzi2020intriguing}, we can distinguish between:
\begin{itemize}
	\item \textbf{white-box} attacks, where the attacker has \textit{perfect knowledge} of the target model, including all the dimensions mentioned before. This type of attack is realistic when the adversary has direct access to the model (e.g., an open-source malware classifier);
	\item \textbf{gray-box} attacks, where the attacker has \textit{partial knowledge} of the target model. For example, they have knowledge about feature representation (e.g., categories of features relevant for feature extraction);
	\item \textbf{black-box} attacks: the attacker has \textit{zero knowledge} of the target model. Specifically, the attacker is only aware of the task the model was designed for and has a rough idea of what potential perturbations to apply to cause some feature changes~\cite{BIGGIO2018317}.
\end{itemize}

Different attack types may suit different scenarios best. A white-box attack appears realistic when the adversary has direct access to the model, as it is the case, for example, with an open-source malware classifier. Conversely, a black-box attack would suit also a model hosted on a remote server to interrogate, as with a commercial cloud-based antivirus.

In the study of models for binary similarity from this paper, our focus will be general black-box and white-box attack methods for them.

\subsubsection{Adversarial Specificity}
This aspect of the threat model defines what goal the attacker wants to reach. We can distinguish between:
\begin{itemize}
	\item \textbf{targeted} attacks: the attacker crafts adversarial samples to have the model classify them into a specific target class. This can be achieved by maximizing the probability that the adversarial sample belongs to the chosen target class;
	\item \textbf{untargeted} attacks: the attacker crafts adversarial samples to have the model classify them in any class except the correct one. This can be achieved by minimizing the probability that the adversarial sample belongs to the correct class.	
\end{itemize}

\noindent
We provided examples for both attacks in Section~\ref{sec:introduction}.


\begin{figure}[t]
	\centering
	\includegraphics[width=3.5in]{featureMap2.pdf}
	\caption{Problem-space objects mapped into feature vectors using a feature mapping function. The two binary functions shown in \textcolor{green}{green} boxes implement similar functionalities, so they are mapped to two close points in the feature space.} 
	\label{img:featureMapping}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=5in]{transformations.pdf}
	\caption{Taxonomy of semantics-preserving perturbations suitable for the proposed attacks. Acronyms are spelled out in the main text (Section~\ref{sec:SemPres}).} 
	\label{img:transformations}
\end{figure*}

\subsection{Inverse Feature Mapping Problem}\label{sec:invMapping}

In the following, we refer to the input domain as \textit{problem space} and to all its instances as \textit{problem-space} objects.

Deep learning models can manipulate only continuous {problem-space} objects. For this reason, when inputs have a discrete representation, a first phase must map them into continuous instances. Such a phase, as shown in Figure \ref{img:featureMapping}, usually relies on a \textit{feature mapping function} whose outputs are \textit{feature vectors}. The set of all possible {feature vectors} defines the \textit{feature space}.

Traditional white-box attacks against deep learning models consist in solving an optimization problem in the {feature space} by minimizing (or maximizing) an objective function in the direction that follows its negative (positive, resp.) gradient~\cite{madry2017towards}. At the end of the optimization process, we obtain a {feature vector} that corresponds to a {problem-space} object representing the generated adversarial sample.

Unfortunately, given a {feature vector}, it is not always possible to obtain its corresponding {problem-space} representation. This issue is known as \textbf{inverse feature mapping} problem \cite{pierazzi2020intriguing}. We can distinguish two scenarios for it:
\begin{itemize}
	\item the feature mapping function is not \textit{invertible} but is \textit{differentiable}. In this case, one can understand how to modify the original input to follow the negative gradient of the objective function. This is a typical behavior in attacks against image classification models and, more in general, when the original {problem-space} objects have a continuous representation;
	\item the feature mapping function is neither \textit{invertible} nor \textit{differentiable}. In this case, one cannot understand how to modify an original {problem-space} object to obtain the given \textit{feature vector}.
\end{itemize}

For \textit{code models}, the feature mapping function is neither invertible nor differentiable. This is the main reason for the low number of white-box attacks against such models \cite{pierazzi2020intriguing}. In particular, the attacker has to resort to approximation techniques that create a feasible {problem-space} object from a feature vector. 
Ultimately, carrying out an attack requires a manipulation of a {problem-space} object via perturbations guided either by gradient-space attacks (as in the white-box case above) or by ``gradient-free'' optimization techniques (as with black-box attacks). We discuss perturbations for our specific context in the next section. 

\begin{figure*}[t]
	\centering
	\includegraphics[width=5in]{noCfg_example.pdf}
	\caption{Examples of semantics-preserving perturbations that do not alter the binary CFG layout. In \textbf{(a)} we have a snippet of assembly code that we then modify using: \textbf{(b)} Instruction Reordering, \textbf{(c)} Semantics-Preserving Rewriting, and \textbf{(d)} Register Renaming. The altered instructions are in \textcolor{red}{\textbf{red}}.}
	\label{img:noCFGtransformations}
\end{figure*}

\subsection{Semantics-Preserving Perturbations of Problem-Space Objects}\label{sec:SemPres}

In this section we will discuss how to manipulate problem-space objects in the specific case of binary code models working on functions. To this end, we review and extend perturbations from prior works~\cite{pierazzi2020intriguing,DBLP:conf/asiaccs/SongLAGKY22,lucas2021malware}, identifying those suitable for adversarial manipulation of functions.

For our purpose, an original binary function $f$ has to be transformed into an adversarial binary sample $f_{adv}$ that preserves its semantics; this intuitively restricts the set of available perturbations for the adversary. We report a taxonomy of possible \textit{semantics-preserving} perturbations in Figure \ref{img:transformations}, dividing them according to how they affect the binary layout of the function control-flow graph (CFG). 

Among CFG-preserving perturbations, we identify:
\begin{itemize}
	\item \textbf{(IR) Instruction Reordering}: reorder independent instructions in the function;
	\item \textbf{(SPR) Semantics-Preserving Rewriting}: substitute a sequence of instructions with a semantically equivalent sequence;
	\item \textbf{(DSL) Modify the Data-Section Layout}: modify the memory layout of the \texttt{.data} section and update all the global memory offsets referenced by instructions;	
	\item \textbf{(RR) Register Renaming}: change all the occurrences of a register as instruction operand with a register currently not in use or swap the use of two registers.
\end{itemize}

Figure~\ref{img:noCFGtransformations} shows examples of their application.
As for perturbations that affect the (binary-level) CFG layout, we can identify the ones that involve adding or deleting nodes:
\begin{itemize}
\item  \textbf{(DBA) Dead Branch Addition}: add dead code in a basic block guarded by an always-false branch;
\item  \textbf{(NS) Node Split}: split a basic block without altering the semantics of its instructions (e.g., the original block will jump to the one introduced with the split);
\item   \textbf{(NM) Node Merge}: merge two basic blocks when semantics can be preserved. For example, by using predicated execution to linearize branch-dependent assignments as conditional {\tt mov} instructions~\cite{constantine}. 
\end{itemize}

And the ones that leave the graph structure unaltered:
\begin{itemize}
\item  \textbf{(CP) Complement Predicates}: change the predicate of a conditional branch and the branch instruction with their negated version;
\item  \textbf{(IBR) Independent Blocks Reordering}: change the order in which independent basic blocks appear in the binary representation of the function.
\end{itemize}