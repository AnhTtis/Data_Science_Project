% !TEX root = paper.tex

\section{Related Works}
In this section, we first discuss loosely related approaches for attacking image classifiers and natural language processing (NLP) models; then, we describe attacks against source code models. Finally, we discuss prominent attacks against models for binary analysis and malware classifiers.

\subsection{Attacks to Image Classifiers and NLP Models}
Historically, the first adversarial samples targeted image classifiers. The crucial point for these attacks is to introduce inside a clean image instance a perturbation that should not be visible to the human eye while being able to fool the target model, as first pointed out by~\cite{DBLP:journals/corr/SzegedyZSBEGF13} and~\cite{goodfellow2014explaining}.

Most of the attacks modify the original instances using gradient-guided methods. In particular, when computing an adversarial sample, they keep the weights constant while altering the starting input in the direction of the gradient that maximizes (or minimizes, depending on whether the attack is targeted or untargeted) the loss function of the attacked model. The FGSM attack~\cite{goodfellow2014explaining} explicitly implements this technique. Other attacks, such as the Carlini-Wagner~\cite{carlini2017towards} one, generate a noise that is subject to $L_p$-norm constraints to preserve similarity to original objects.

As observed in Section~\ref{sec:invMapping}, adversarial sample generation is possibly easier in the image domain than in the textual one, due to the continuous representation of the original objects.
In the NLP domain, the inputs are discrete objects, a fact that prevents any direct application of gradient-guided methods for adversarial sample generation. Ideally, perturbations to fool deep models for language analysis should be grammatically correct and semantically coherent with the original instance.

One of the earliest methodologies for attacking NLP models is presented in~\cite{DBLP:conf/emnlp/JiaL17}. The authors propose attacks to mislead deep learning-based reading comprehension systems by introducing perturbations in the form of new sentences inside a paragraph, so as to confuse the target model while maintaining intact the original correct answer. The attacks proposed in~\cite{DBLP:conf/naacl/MrksicSTGRSVWY16} and~\cite{DBLP:conf/acl/RenDHC19} focus on finding replacement strategies for words composing the input sequence. Intuitively, valid substitutes should be searched through synonyms; however, this strategy could fall short in considering the context surrounding the word to substitute. Works like~\cite{DBLP:conf/naacl/LiZPCBSD21} and~\cite{DBLP:conf/emnlp/LiMGXQ20} further investigate this idea using BERT-based models for identifying accurate word replacements.

\subsection{Attacks against Models for Source Code Analysis}\label{sec:sc_attacks}
This section covers some prominent attacks against models that work on source code. The techniques from these and other works have limited applicability to binary similarity, as their perturbations may not survive compilation (e.g., variable renaming) or result in marginal differences in compiled code (e.g., turning a while-loop into a for-loop).

The general white-box attack of~\cite{yefet2020adversarial} iteratively substitutes a target variable name in all of its occurrences with an alternative name until a misclassification occurs.
The attack against plagiarism detection from~\cite{DBLP:journals/pacmpl/Devore-McDonald20} uses genetic programming to augment a program with code lines picked from a pool and validated for program equivalence by checking that an optimizer compiler removes them.
The attack against clone detection from~\cite{zhang2023challenging} combines several semantics-preserving transformations of source code using different optimization heuristic strategies.

\subsection{Attacks against Malware Classifiers}\label{sec:related}
We complete our review of related works by covering prominent research on evading ML-based malware classifiers. These works solve a binary classification problem. 

Attacks such as~\cite{kolosnjaji2018adversarial,k2018adversarial} to malware detectors based on convolutional neural networks add perturbations in a new non-executable section appended to a Windows PE binary. Both use gradient-guided methods for choosing single-byte perturbations to mislead the model in classifying the whole binary. However, they are ineffective when only actual code is analyzed (e.g., once non-executable sections are stripped).

Pierazzi et al.~\cite{pierazzi2020intriguing} explore transplanting binary code gadgets into a malicious Android program to avoid detection. The attack follows a gradient-guided search strategy based on a greedy optimization. In the initialization phase, they mine from benign binaries code gadgets that modify features that the classifier uses to compute its classification score. In the attack phase, they pick the gadgets that can mostly contribute to the (mis)classification of the currently analyzed malware sample; they insert gadgets in order of decreasing negative contribution, repeating the procedure until misclassification occurs. To preserve program semantics, gadgets are injected into never-executed code portions.

Lucas et al.~\cite{lucas2021malware} target malware classifiers analyzing raw bytes. They propose a functionality-preserving iterative procedure viable for both black-box and white-box attackers. At every iteration, the attack determines a set of applicable transformations for every function in the binary and applies a randomly selected one (following a hill-climbing approach in the black-box scenario or using the gradient in the white-box one). Done via binary rewriting, the transformations are local and include instruction reordering, register renaming, and replacing instructions with equivalent ones of identical length. The results show that these transformations can be effective even against (ML-based) commercial antivirus products, leading the authors to advocate for augmenting such systems with provisions that do not rely on ML. In the context of binary similarity, though, we note that these transformations would have limited efficacy if done on a specific pair of functions: for example, both instruction reordering and register renaming would go completely unnoticed by Gemini and GMN (Section~\ref{sec:gemini} and~\ref{sec:gmn}).

MAB-Malware~\cite{DBLP:conf/asiaccs/SongLAGKY22} is a reinforcement learning-based approach for generating adversarial samples against PE malware classifiers in a black-box context. Adversarial samples are generated through a multi-armed bandit (MAB) model that has to keep the sample in a single, non-evasive state when selecting actions while learning reward probabilities. The goal of the optimization strategy is to maximize the total reward. The set of applicable actions are standard PE manipulation techniques from prior works: header manipulation, section insertion and manipulation (e.g., adding trailing byte), and in-place randomization of an instruction sequence (i.e., replacing it with a semantically equivalent one). Each action is associated with a specific content---a payload---added to the malware when the action is selected. An extensive evaluation is conducted on two popular ML-based classifiers and three commercial antivirus products.

\section{Limitations and Future Works} \label{se:limitations}
In this paper, we have seen how adding dead code is a natural and effective way to realize appreciable perturbations for a selection of heterogeneous binary similarity systems.

In Section~\ref{ss:perturbation-selection}, we acknowledged how, in the face of defenders that pre-process code with static analysis, our implementation would be limited from having the inserted dead blocks guarded by non-obfuscated branch predicates.

Our experiments suggest that, depending on the characteristics of a given model and pair of functions, the success of an attack may be affected by factors like the initial difference in code size and CFG topology, among others. In this respect, it could be interesting to explore how to alternate our dead-branch addition perturbation, for example, with the insertion of dead fragments within existing blocks.

We believe both limitations could be addressed in future work with implementation effort, whereas the main goal of this paper was to show that adversarial attacks against binary similarity systems are a concrete possibility. For defensive attempts, we could rely on off-the-shelf obfuscation methods (Section~\ref{ss:perturbation-selection}). To enhance our attacks, we could explore more complex patching implementation strategies based on binary rewriting or a modified compiler back-end.

Alongside looking at pre-processing techniques~\cite{pierazzi2020intriguing}, we find that an interesting direction for defensive research could be to increase the resilience of code models by designing more robust feature mapping processes.

\section{Conclusions}\label{se:conclusions}
We presented the first study on the resilience of code models for binary similarity to black-box and white-box adversarial attacks, covering targeted and untargeted scenarios. Our tests highlight that current state-of-the-art solutions in the field (Gemini, GMN, and SAFE) are not robust to adversarial attacks crafted for misleading binary similarity models. Furthermore, their resilience against untargeted attacks appears significantly lower in our tests. Our black-box Spatial Greedy technique also shows that an instruction-selection strategy guided by a dynamic exploration of the entire ISA is more effective than using a fixed set of instructions. We hope to encourage follow-up studies by the community to improve the robustness and performance of these systems.