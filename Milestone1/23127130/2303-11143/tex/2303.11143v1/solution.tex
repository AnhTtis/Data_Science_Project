% !TEX root = paper.tex

\section{Black-Box attack: Solution Overview}\label{sec:blackbox}
In this section we describe our black-box attacks. We first introduce our baseline solution (named \textbf{Greedy}), highlighting its limitations. We then move to our main contribution in the black-box scenario (named \textbf{Spatial Greedy}). Figure \ref{img:advWork} depicts a general overview of our black-box approaches.

\subsection{Greedy}\label{sec:greedy}
The baseline black-box approach we devise for attacking binary function similarity models consists of an iterative action-selection rule that follows a greedy optimization strategy. Starting from the original sample $f_{1}$, we iteratively apply perturbations $T_1, T_2, \ldots, T_k$ selected from a set of available actions, generating a series of samples $f_{adv_1}, f_{adv_2}, \ldots, f_{adv_k}$. This procedure ends upon generating a sample $f_{adv}$ meeting the desired similarity threshold, otherwise the attack fails after $\bar{\delta}$ completed iterations.

For instantiating Greedy using DBA perturbations, we reason on a set of positions $\texttt{BLK}$ for inserting dead branches in function $f_{1}$ and a set of instructions $\texttt{CAND}$, which we call the \textit{set of candidates}. Each action consists of a $\langle \texttt{bl},\texttt{in} \rangle$ pair made of  the branch $\texttt{bl} \in \texttt{BLK}$ and an instruction $\texttt{in} \in \texttt{CAND}$ to insert in the dead code block guarded by $\texttt{bl}$.

The naive action-selection rule (i.e., greedy) at each step selects the action (i.e., the perturbation) that locally maximizes (or minimizes in case of untargeted attack) the relative increase (or decrease) of the objective function.

This approach, however, may be prone to finding local optima. To avoid this problem, for our Greedy baseline we adopt an $\varepsilon$-greedy action-selection rule. Here, we select with a small probability $\varepsilon$ a suboptimal action instead of the one that the standard greedy strategy picks, and with probability $1-\varepsilon$ the one representing the local optimum.
 
The objective function is the similarity between $f_{adv}$ and the target function $f_{2}$ in case of a targeted attack (formally, $sim(f_{adv}, f_2)$) or the negative of the similarity between $f_{adv}$ and the original function in case of untargeted attack (formally, $-sim(f_{1}, f_{adv})$). In the following, we only discuss the maximization strategy followed by targeted attacks; mutatis mutandis, the same rationale holds for untargeted attacks.

\subsubsection{Limitations of the Complete Enumeration Strategy}
At each step, Greedy enumerates all the applicable transformations computing the marginal increase of the objective function, thus resulting in selecting an instruction $\texttt{in}$ by enumerating all the possible instructions of the considered set of candidates $\texttt{CAND}$ for each position $\texttt{bl} \in \texttt{BLK}$.

Unfortunately, the instruction set architecture (ISA) of a modern CPU may consist of a large number of instructions. To give an example, consider the x86-64 ISA: according to~\cite{DBLP:conf/pldi/HeuleS0A16}, it has 981 unique mnemonics and a total of 3,684 instruction variants (without counting register operand choices for them). 
Therefore, it would be unfeasible to have a $\texttt{CAND}$ set that covers all possible instructions of an ISA.

This means that the size of $\texttt{CAND}$ must be limited. One possibility is to use hand-picked instructions. However, this approach has two problems. Such a set could not cover all the possible behaviors of the ISA, missing fundamental aspects (for example, leaving vector instructions uncovered); furthermore, this effort has to be redone for a new ISA. There is also a more subtle pitfall: a set of candidates fixed in advance could include instructions that the specific binary similarity model under attack deems as not significant.

On specific models, it may still be possible to use a small set of candidates profitably, enabling a \textbf{gray-box} attack strategy for Greedy. In particular, one can restrict the set of instructions to the ones that effectively impact the features extracted by the attacked model (which obviously requires knowledge of the features it uses; hence, the gray-box characterization). In such cases, this strategy is equivalent to the black-box Greedy attack that picks from all the instructions in the ISA, but computationally much more efficient.

\subsection{Spatial Greedy}\label{sec:spatial}

In this section, we extend our baseline solution introducing a fully black-box approach named Spatial Greedy. This approach overcomes all the limitations discussed for Greedy using an adaptive procedure that dynamically updates the set of candidates according to a feedback from the model under attack \textit{without requiring any knowledge} of it. 

In Spatial Greedy, we extend the $\varepsilon$-greedy action-selection strategy by adaptively modifying the set of candidates that we use at each iteration. In particular, using instructions embedding techniques, we transform each instruction $\texttt{in} \in \texttt{CAND}$ into a vector of real values. This creates vectors that partially preserve the semantics of the original instructions. Chua et al.~\cite{DBLP:conf/uss/ChuaSSL17} first showed that such vectors may be grouped by instruction semantics, creating a notion of proximity between instructions: for example, vectors representing arithmetic instructions are in a cluster, vectors representing branches in another, and so on.

Here, at each step, we populate a portion of the set of candidates by selecting the instructions that are close, in the embedding metric space, to instructions that have shown a good impact on the objective function. The remaining portion of the set is composed of random instructions. We discuss our choices for instruction embedding techniques and dynamic candidates selection in the following.

In the experimental section, for the black-box realm, we will compare Spatial Greedy against the Greedy approach, opting for the computationally efficient gray-box flavor of the latter when allowed by the specific model under study.

\begin{figure}[t!]
	\centering
	\includegraphics[width=3.5in]{spatial.pdf}
	\caption{Dynamic update of the set of candidates. The $\mathtt{mov}$ instruction is the greedy action for the current iteration and is mapped to the \textcolor{blue}{\textbf{blue}} point in the instruction embedding space. The set of candidates is updated selecting $c/k$ neighbours of the considered \textit{top-k} action (represented in \textcolor{red}{\textbf{red}}), $c-c/k$ instructions among the closest neighbours of the remaining \textit{top-k} greedy actions, and $rN$ random instructions.}
	\label{img:spatial}
\end{figure}


\subsubsection{Instruction Embedding Space}\label{sec:proxy}
We embed assembly instructions into numeric vectors using an instruction embedding model~\cite{DBLP:conf/nips/MikolovSCCD13}. Given such a model $M$ and a set $I$ of assembly instructions, we map each $i \in I$ to a vector of real values $\vec{i} \in \mathbb{R}^n$, using $M$. The model is such that, for two instructions having similar semantics, the embeddings it produces will be close in the metric space.

\subsubsection{Dynamic Selection of the Set of Candidates}\label{sec:spatialgreedy:selection}
The process for selecting the set of candidates for each iteration of the $\varepsilon$-greedy action-selection procedure represents the focal point of Spatial Greedy.

Let $N$ be the size of the set of candidates $\texttt{CAND}$. Initially, we fill it with N random instructions. Then, at each iteration of the $\varepsilon$-greedy procedure, we update $\texttt{CAND}$ by inserting $rN$ random instructions, where $r \in [0,1)$, and $c$ instructions we select among the closest neighbors of the instructions composing the {top-$k$ greedy actions} of the last iteration.

The {\em top-$k$ greedy actions} are the $k$ actions that, at the end of the last iteration, achieved the highest increase of the objective function. To keep the size of the set stable at value $N$, we take the closest $c/k$ neighbors of each top-$k$ action\footnote{In this description, we omit for brevity the rounding that we perform to have integer numbers to work on.}.

The rationale of having $r$ random and $c$ selected instructions is seeking a balance between {\em exploration} and {\em exploitation}. With the random instructions, we randomly sample the solution space to escape from a possibly local optimum found for the objective function. With the selected instructions, we exploit the part of the space that in the past has brought the best solutions. Figure \ref{img:spatial} provides a pictorial representation of the update procedure.

We present the complete description of Spatial Greedy in Algorithm \ref{spatGreedyPseudocode}. The first step consists in identifying the positions $\texttt{BLK}$ where to introduce dead branches and initializing the set of candidates $\texttt{CAND}$ with $N$ random instructions (lines \ref{alg:getPositions} and \ref{alg:initCandidates}). Then, during the iterative procedure (lines \ref{alg:startIters}-\ref{alg:endIters}), we first enumerate all the possible actions (lines \ref{alg:startActs}-\ref{alg:endActs}), then apply the action-selection rule according to the value of $\varepsilon$ (lines \ref{alg:startSel}-\ref{alg:endSel}). Finally, we get the \textit{top-k greedy actions} (line \ref{alg:getTopK}) and update the set of candidates (line \ref{alg:updateCandidates}).

\begin{algorithm}[t!]
\begin{small}
	\caption{Spatial Greedy procedure (\textbf{targeted} case)}
	\label{spatGreedyPseudocode}
	\textbf{Input}: source function $f_1$, target function $f_2$, similarity threshold \T, max number of dead branches $B$, max number of instructions to be inserted $\bar{\delta}$, max number of instructions to be tested $N$, max number of random instructions $r$, max number of neighbours $c$, probability of selecting random action $\varepsilon$.\\
	\textbf{Output}: adversarial sample $f_{adv}$.
	\begin{algorithmic}[1]
		\State $f_{adv} \leftarrow f_1$
		\State $instr \leftarrow 0$
		\State $\texttt{BLK} \leftarrow \textrm{getPositions}(f_1, B)$ \label{alg:getPositions}
		\State $\texttt{CAND} \leftarrow \textrm{getRandomInstructions}(N)$ \label{alg:initCandidates}
		\State $sim \leftarrow \textrm{sim}(f_{adv}, f_{2})$
		\While{$sim \le $ \T AND $instr < \bar{\delta}$} \label{alg:startIters}
		\State $iterSim \leftarrow sim$
		\State $iterBlock \leftarrow \langle \rangle$
		\State $testedActions \leftarrow [\:]$
		\For{$\langle \texttt{bl},\texttt{in} \rangle \in \texttt{BLK} \times \texttt{CAND}$} \label{alg:startActs}
		\State $\overline{f}_{adv} \leftarrow f_{adv}+ \langle \texttt{bl},\texttt{in} \rangle$
		\State $currSim \leftarrow \textrm{sim}(\overline{f}_{adv}, f_2)$
		\State $testedActions.\textrm{append}(\langle \langle \texttt{bl},\texttt{in} \rangle, currSim \rangle)$
		\State $prob \leftarrow \textrm{uniform}(0,1)$
		\EndFor \label{alg:endActs}
		\If{$\textrm{prob} < \varepsilon$} \label{alg:startSel}
		\State $iterSim, iterBlock \leftarrow \textrm{selectGreedy}()$
		\Else
		\State $iterSim, iterBlock \leftarrow \textrm{selectRandom}()$
		\EndIf \label{alg:endSel}
		\State $elected \leftarrow \textrm{getTopK}(testedActions, K)$ \label{alg:getTopK}
		\State $\texttt{CAND} \leftarrow \textrm{updateInstructions}(elected, r, c)$ \label{alg:updateCandidates}
		\State $sim \leftarrow iterSim$
		\State $f_{adv} \leftarrow f_{adv} + iterBlock$
		\State $instr\leftarrow instr + 1$
		\EndWhile \label{alg:endIters}
		\State \Return $f_{1}'$
	\end{algorithmic}
\end{small}
\end{algorithm}

\section{White-Box attack: Solution Overview} \label{sec:whitebox}
As pointed out in Section \ref{sec:NNThreats}, in a white-box scenario the attacker has a perfect knowledge of the target deep learning model, including its loss function and gradients. We discuss next how we can build on them to mount an attack.

\subsection{Gradient-guided Code Addition Method}\label{sec:GCAM}
White-box adversarial attacks have been largely investigated against image classifiers by the literature, resulting in valuable effectiveness~\cite{goodfellow2014explaining}. Our attack strategy for binary similarity derives from the design pattern of the PGD attack~\cite{madry2017towards}, which iteratively targets image classifiers.

We call our proposed white-box attack \textit{Gradient-guided Code Addition Method} (\textbf{GCAM}). It consists in applying a set of transformations using a gradient-guided strategy. In particular, we try to minimize (or maximize, depending on whether the attack is targeted or untargeted) the loss function of the attacked model on the given input while keeping the size of the perturbation small (and respecting the semantics-preserving constraint): specifically, we use for this purpose $L_p$-norm as soft constraint.

Because of the \textit{inverse feature mapping} problem, gradient optimization-based approaches cannot be directly applied in our context (Section~\ref{sec:invMapping}). We need a further (hard) constraint that acts on the feature-space representation of the input binary function. This constraint strictly depends on the target model: we will further investigate its definition in Section \ref{chap:targetSystems}. For the following, we focus on the loss minimization strategy argued for targeted attacks. As before, we can easily adapt the same concepts to the untargeted case.

We can describe a DNN-based model for binary similarity as the concatenation of the two functions $\lambda$ and $sim_v$. In particular, $\lambda$ is the function that maps a problem-space object to a feature vector (i.e., the feature mapping function discussed in Section \ref{sec:invMapping}), while $sim_v$ is the neural network computing the similarity given the feature vectors.

Given two binary functions $f_1$ and $f_2$, we aim to find a perturbation $\delta$ that maximizes $sim_v(\lambda(f_1))+\delta, \lambda(f_2))$. To do so, we use an iterative strategy where, during each iteration, we solve the following optimization problem:
\begin{equation} \label{eq:whiteAttack}
	\begin{aligned}
		& \text{min}
		& & \mathcal{L}(sim_v(\lambda(f_1)+\delta, \lambda(f_2)),\theta)  + \epsilon ||\delta||_{p},\\
	\end{aligned}
\end{equation}
where $\mathcal{L}$ is the loss function, $\theta$ are the weights of the target model, and $\epsilon$ is a coefficient in $[0,\infty)$.  

We randomly initialize the perturbation $\delta$ and then update it at each iteration by a quantity given by the negative gradient of the loss function $\mathcal{L}$. The vector $\delta$ has several components equal to zero and it is crafted so that it modifies only the (dead) instructions in the added blocks. The exact procedure depends on the target model: we return to this aspect in Section \ref{chap:targetSystems}.
 
Notice that the procedure above allows us to find a perturbation in the feature space, while our final goal is to find a problem-space perturbation to modify the function $f_1$. Therefore, we derive from the perturbation $\delta$ a problem-space perturbation $\delta_p$. The exact technique is specific to the model we are attacking, as we further discuss in Section \ref{chap:targetSystems}.

The common idea behind all technique instances is to find the problem-space perturbation $\delta_p$ whose representation in the feature space is the {\em closest} to $\delta$. Essentially, we use a {\em rounding-based inverse} strategy to solve the inverse feature mapping problem that accounts to {\em rounding} the feature space vector to the closest vector that corresponds to an object in the problem space.
The generated adversarial function is $f_{adv} = f_1 + \delta_p$. As for the black-box scenario, the process ends whenever we reach a maximum number of iterations or the desired threshold value for the function $sim$.
