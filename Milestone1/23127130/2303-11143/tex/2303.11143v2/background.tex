% !TEX root =  paper.tex

\section{Background}

In this section, we provide background knowledge for adversarial attacks against models for code analysis. Then, we introduce a categorization of semantics-preserving perturbations for binary functions.

\subsection{Adversarial Knowledge}\label{sec:NNThreats}
We can describe a deep learning model through different aspects: training data, layers architecture, loss function, and weights parameters. Having complete or partial knowledge about such elements can facilitate an attack from a computational point of view. According to seminal works in the area~\cite{BIGGIO2018317, pierazzi2020intriguing}, we can distinguish between:
\begin{itemize}
	\item \textbf{white-box} attacks, where the attacker has \textit{perfect knowledge} of the target model, including all the dimensions mentioned before. These type of attacks are realistic when the adversary has direct access to the model (e.g., an open-source malware classifier);
	\item \textbf{gray-box} attacks, where the attacker has \textit{partial knowledge} of the target model. For example, they have knowledge about feature representation (e.g., categories of features relevant for feature extraction);
	\item \textbf{black-box} attacks: the attacker has \textit{zero knowledge} of the target model. Specifically, the attacker is only aware of the task the model was designed for and has a rough idea of what potential perturbations to apply to cause some feature changes~\cite{BIGGIO2018317}.
\end{itemize}

Different attack types may suit different scenarios best. A white-box attack, for example, could be attempted on an open-source malware classifier. Conversely, a black-box attack would suit also a model hosted on a remote server to interrogate, as with a commercial cloud-based antivirus.


\begin{figure}[t!]
	\centering
	\includegraphics[width=2.95in,trim = 0cm 1cm 0cm 0cm]{featureMap2.pdf}
	\caption{A feature mapping function maps problem-space objects into feature vectors. The two boxed binary functions implement similar functionalities, so they are mapped to two close points in the feature space.} 
	\label{img:featureMapping}
\end{figure}



\subsection{Inverse Feature Mapping Problem}\label{sec:invMapping}

In the following, we refer to the input domain as \textit{problem space} and to all its instances as \textit{problem-space} objects.

Deep learning models can manipulate only continuous {problem-space} objects. When inputs have a discrete representation, a first phase must map them into continuous instances. The phase usually relies on a \textit{feature mapping function} (Figure \ref{img:featureMapping}) whose outputs are \textit{feature vectors}. The set of all possible {feature vectors} is known as the \textit{feature space}.

Traditional white-box attacks against deep learning models solve an optimization problem in the {feature space} by minimizing an objective function in the direction following its negative gradient~\cite{madry2017towards}. When optimization ends, they obtain a {feature vector} that corresponds to a {problem-space} object representing the generated adversarial sample.

Unfortunately, given a {feature vector}, it is not always possible to obtain its {problem-space} representation. This issue is called the \textbf{inverse feature mapping} problem \cite{pierazzi2020intriguing}.

For {code models}, the feature mapping function is neither invertible nor differentiable. Therefore, one cannot understand how to modify an original {problem-space} object to obtain the given {feature vector}.
In particular, the attacker has to employ approximation techniques that create a feasible {problem-space} object from a feature vector. 
Ultimately, mounting an attack requires a manipulation of a {problem-space} object via perturbations guided by either  gradient-space attacks (as in the white-box case above) or ``gradient-free'' optimization techniques (as with black-box attacks). We discuss perturbations specific to our context next.

\subsection{Semantics-Preserving Perturbations of Problem-Space Objects}\label{sec:SemPres}

In this section, we discuss how to manipulate problem-space objects in the specific case of binary code models working on functions. To this end, we review and extend perturbations from prior works~\cite{pierazzi2020intriguing,DBLP:conf/asiaccs/SongLAGKY22,lucas2021malware}, identifying those suitable for adversarial manipulation of functions.

For our purpose, we seek to transform an original binary function $f$ into an adversarial binary sample $f_{adv}$ that preserves the semantics of $f$; intuitively, this restricts the set of available perturbations for the adversary. We report a taxonomy of possible \textit{semantics-preserving} perturbations in Figure \ref{img:transformations}, dividing them according to how they affect the binary layout of the function's control-flow graph (CFG). 

\begin{figure}[h]
	\centering
	\includegraphics[width=3.5in, trim = 0cm 0.5cm 0cm 0cm]{transformations2.pdf}
	\caption{Taxonomy of semantics-preserving perturbations suitable for the proposed attacks. Acronyms are spelled out in the body of the paper.}
	\label{img:transformations}
\end{figure}

Among CFG-preserving perturbations, we identify:
\begin{itemize}
	\item \textbf{(IR) Instruction Reordering}: reorder independent instructions in the function;
	\item \textbf{(SPR) Semantics-Preserving Rewriting}: substitute a sequence of instructions with a semantically equivalent sequence;
	\item \textbf{(DSL) Modify the Data-Section Layout}: modify the memory layout of the \texttt{.data} section and update all the global memory offsets referenced by instructions;
	\item \textbf{(RR) Register Renaming}: change all the occurrences of a register as instruction operand with a register currently not in use or swap the use of two registers.
\end{itemize}

Figure~\ref{img:noCFGtransformations} shows examples of their application.
As for perturbations that affect the (binary-level) CFG layout, we can identify the ones that involve adding or deleting nodes:
\begin{itemize}
	\item  \textbf{(DBA) Dead Branch Addition}: add dead code in a basic block guarded by an always-false branch;
	\item  \textbf{(NS) Node Split}: split a basic block without altering the semantics of its instructions (e.g., the original block will jump to the one introduced with the split);
	\item   \textbf{(NM) Node Merge}: merge two basic blocks when semantics can be preserved. For example, by using predicated execution to linearize branch-dependent assignments as conditional {\tt mov} instructions~\cite{constantine}. 
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=2.7in, trim = 0cm 0.5cm 0cm 0cm]{noCfg_example2.pdf}
	\caption{Examples of semantics-preserving perturbations that do not alter the binary CFG layout. We modify the assembly snippet in \textbf{(a)} by applying, in turn, \textbf{(b)} Instruction Reordering, \textbf{(c)} Semantics-Preserving Rewriting, and \textbf{(d)} Register Renaming. Altered instructions are in \textcolor{red}{\textbf{red}}.}
	\label{img:noCFGtransformations}
\end{figure}

And the ones that leave the graph structure unaltered:
\begin{itemize}
	\item  \textbf{(CP) Complement Predicates}: change the predicate of a conditional branch and the branch instruction with their negated version;
	\item  \textbf{(IBR) Independent Blocks Reordering}: change the order in which independent basic blocks appear in the binary representation of the function.
\end{itemize}