% !TEX root =  paper.tex

\section{Threat Model and Problem Definition}
In this section, we define our threat model together with the problem of attacking binary similarity models.

\subsection{Threat Model}
\label{ss:threat-model}
The focus of this work is to create adversarial instances that attack a model at inference time (i.e., we do not investigate attacks at training time). Following the description provided in Section \ref{sec:NNThreats}, we consider two different attack scenarios: respectively, a black-box and a white-box one. In the first case, the adversary has no knowledge of the target binary similarity model; nevertheless, they can perform an unlimited number of queries to observe the output produced by the model. In the second case, we assume that the attacker has perfect knowledge of the target binary similarity model. 

\subsection{Problem Definition}
\label{ss:problem-definition}
Let $sim$ be a similarity function that takes as input two functions, $f_1$ and $f_2$, and returns a real number, the {\em similarity score} between them, in $[0,1]$. 

\footnotetext[1]{Although $f_{adv}$ and $f_2$ are similar for the model, they are not semantically equivalent: this is precisely the purpose of an attack that wants to fool the model to consider them as such, while they are not.}

We define two binary functions to be \textit{semantically equivalent} if they are two implementations of the same abstract functionality. We assume that there exists an adversary that wants to attack the similarity function. The adversary can mount two different kind of attacks:
\begin{itemize}[noitemsep,topsep=2pt]
	\item \textbf{Targeted attack}. Given two binary functions, $f_1$ (identified as \textit{source}) and $f_2$ (identified as \textit{target}), the adversary wants to find a binary function $f_{adv}$  semantically equivalent to $f_1$ such that: $sim(f_{adv}, f_{2}) \ge \mT$, where $\mT$ is a success threshold\footnotemark{} chosen by the attacker depending on the victim at hand.% We will fix a precise value to such a threshold in our experimental section.
	% CONO: I commented out the last part as it may draw unnecessary emphasis
	\item \textbf{Untargeted attack}. Given a binary function $f_1$, the adversary goal consists of finding a binary function $f_{adv}$ semantically equivalent to $f_1$ such that: $sim(f_{1}, f_{adv}) \le \mU$. The threshold $\mU$ is the analogous of the previous case for the untargeted attack scenario.
\end{itemize}
Loosely speaking, in the first case, the adversarial sample has to be similar to a target, as in the example scenario (1) presented in Section \ref{sec:introduction}. In the second one, the adversarial sample has to be dissimilar from its original version, as in the example scenarios (2) and (3) also from Section \ref{sec:introduction}. 

\begin{figure*}[t!]
	\centering
	\includegraphics[width=7in, trim = 0cm 1.2cm 0cm 0cm]{schema3.pdf}
	\caption{Overall workflow of the \textit{black-box} $\varepsilon$-greedy action-selection strategy in the \textit{targeted} scenario.}
	\label{img:advWork}
\end{figure*}

\subsection{Perturbation Selection}\label{ss:perturbation-selection}
Given a binary function $f_1$, our attack consists in applying to it perturbations that do not alter its semantics.

To study the feasibility of our approach, we choose \textit{dead branch addition} (DBA) among the suitable perturbations outlined in Section \ref{sec:SemPres}. We find DBA adequate for this study for two reasons: it is sufficiently expressive so as to affect heterogeneous models (which may not hold for others\footnotemark{}) and its implementation complexity for an attacker is fairly limited. Nonetheless, other choices remain possible, as we will further discuss in Section~\ref{se:limitations}.

At each application, our embodiment of DBA inserts in the binary code of $f_1$ one or more instructions in a new or existing basic block guarded by a branch that is never taken at runtime (i.e., we use an always-false branch predicate).

Such a perturbation can be done at compilation time or on an existing binary function instance. For our study, we apply DBA during compilation by adding placeholder blocks as inline assembly, which eases the generation of many adversarial samples from a single attacker-controlled code. State-of-the-art binary rewriting techniques would work analogously over already-compiled source functions.


We currently do not attempt to conceal the nature of our branch predicates for preprocessing robustness, which~\cite{pierazzi2020intriguing} discusses as something that attackers should be wary of to mount stronger attacks. We believe off-the-shelf obfuscations (e.g., opaque predicates, mixed boolean-arithmetic expressions) or more complex perturbation choices may improve our approach in this respect. Nevertheless, our main goal was to investigate its feasibility in the first place.
