% !TEX root = paper.tex

\section{Datasets and Implementation}
In this section, we discuss the evaluation datasets and the corpus for training the embedding model of Spatial Greedy.

\subsection{Attack Dataset}\label{sec:dataset}

We test our approaches by considering pairs of binary functions randomly extracted from 6 open-source projects written in C language: binutils, curl, gsl, libconfig, libhttp, and openssl.
We compile the programs for an \texttt{x86-64} architecture using the \texttt{gcc} 9.2.1 compiler with \texttt{-O0} optimization level on Ubuntu 20.04. We filter out all functions with less than six instructions.
As a result, we obtain a dataset of code representative of real-world software, with source programs used in the evaluation or training of binary similarity solutions (e.g.,~\cite{marcelli2022machine,massarelli2021function,ding2019asm2vec,xu2017neural}), and that could be potential targets for the exemplary scenarios outlined in Section~\ref{sec:introduction}.


To evaluate the robustness of the three target models against our proposed approaches, we used two datasets for the \textbf{targeted} scenario, each made of 500 pairs of binary functions sampled from the general dataset. For both datasets, the same function cannot be considered twice as source function, but it can appear as target more than once. The first dataset, which we call \textsf{Random}, consists of pairs of random functions: functions in a pair differ at most by 1345 instructions, and on average by 135.27. The second dataset, which we call \textsf{Balanced}, includes pairs of functions having similar length. In particular, they differ at most by 10 instructions, and on average by 5.47. We make no attempt to balance either dataset by considering the number of CFG nodes: for them, we measure an average difference of 17.8 nodes in \textsf{Random} and 7.5 in \textsf{Balanced}.

In the \textbf{untargeted} scenario, source and target functions have to coincide. For these attacks, we use the dataset \textsf{Untarg} composed by the 500 functions used as source in the \textsf{Random} dataset. Being pairs made of identical functions, they are trivially balanced for instructions and CFG nodes.

\subsection{Dataset used for Spatial Greedy}\label{sec:SG_dataset}
As described in Section~\ref{sec:proxy}, in Spatial Greedy we use an instruction embedding model to induce a metric space over assembly instructions. We opt for word2vec~\cite{DBLP:conf/nips/MikolovSCCD13}, being it currently the state-of-the-art solution in the field\footnotemark{}. For each of the considered models, we use the following parameters during training: embedding size 100, window size 8, word frequency 8 and learning rate 0.05.
We train these models using assembly instructions as tokens. We use as training set a corpus of {23,181,478} assembly instructions, extracted from {291,688} binary functions collected by compiling various system libraries with the same setup of the previous section.


One aspect worth emphasizing is that Spatial Greedy uses embeddings unrelated to the binary similarity model being targeted. We trained the Spatial Greedy embedding model using distinct dataset and parameters compared to SAFE, whereas neither GMN nor Gemini incorporate a layer that converts a single instruction into a feature vector. Spatial Greedy relies on embeddings to enhance instruction selection during the attack by clustering the instruction space, independently of the underlying model being attacked.


\subsection{Implementation details}
We implement our attacks in Python in about 3100 LOC.

An aspect that is worth mentioning for the greedy attacks involves the application of the action $\langle \texttt{bl}, \texttt{in} \rangle$ chosen at each iteration. We note that modifying the binary representation of the function every time incurs costs (recompilation in our case; binary rewriting in alternative implementations) that we may avoid through a simulation.

In particular, we directly modify the data structures that the target models use for feature mapping when parsing the binary, simulating the presence of newly inserted instructions. These models have been implemented by their authors in tensorflow or pytorch, which allows us to keep our modifications rather limited. In preliminary experiments, we have verified that the similarity values from our simulation are comparable with those we would have obtained by recompiling the modified functions output by our attacks. Hence, we will use it for the experiments of Section~\ref{sec:Eval}.

\footnotetext{The attentive reader may wonder whether this choice may unfairly favor Spatial Greedy when attacking SAFE, as the model also uses word2vec in its initial instruction embedding stage. We conducted additional experiments for SAFE using two other models, GloVe~\cite{pennington2014glove} and fastText~\cite{bojanowski2016enriching}. The three models perform almost identically in targeted attacks, while in untargeted ones fastText occasionally outperforms the others by a small margin. For the sake of generality, in the paper evaluation we will report and discuss results for word2vec only.}

\begin{table*}[t!]
	\footnotesize
	\ra{1.5}
	\caption{Evaluation metrics with $\mT = 0.80$ relative to the \textbf{black-box} attacks against the three target models in the \textbf{targeted} scenario. Spatial Greedy (SG) is evaluated using parameters $\varepsilon = 0.1$ and $r = 0.75$. Greedy (G) is evaluated using $\varepsilon = 0.1$. \textcolor{blue}{\textbf{G*}} is the gray-box version of Greedy: when such a version is available (Section~\ref{chap:targetSystems}), we show it instead of G. When examining G against SAFE, a set of candidates of size 400 is considered.}
	\label{tab:BBTargeted}
	\centering
	\resizebox{\linewidth}{!}{%
		\begin{tabular}{c|c|c|cccc|cccc|cccc|cccc} 
			\toprule
			\multicolumn{1}{l|}{\multirow{2}{*}{}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Target}}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Attack}}}                        & \multicolumn{4}{c|}{\textbf{A-rate (\%) ($\mT = 0.80$)}} & \multicolumn{4}{c|}{\textbf{M-size ($\mT = 0.80$)}}  & \multicolumn{4}{c|}{\textbf{A-sim ($\mT = 0.80$)}}   & \multicolumn{4}{c}{\textbf{N-inc ($\mT = 0.80$)}}     \\ 
			\cline{4-19}
			\multicolumn{1}{l|}{}                  & \multicolumn{1}{l|}{}                                 & \multicolumn{1}{l|}{}                                                        & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4}     & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4}  \\ 
			\toprule\toprule
			&                                                & \textcolor{blue}{\textbf{G*}}                                                                       & 15.36       & 21.96       & 24.55       & \textcolor{red}{\textbf{27.94}}           & 11.91       & 24.16       & 35.40       & 46.23       & 0.84        & 0.85        & 0.86        & 0.86        & 0.18        & 0.25        & 0.26        & 0.26         \\
			\cline{3-19}
			& \multirow{-2}{*}{Gemini}                                                & SG                                                                       & 15.77       & 22.55       & 26.55       & 27.54           & 12.20       & 23.83       & 35.47       & 44.02       & 0.84        & 0.85        & 0.86        & 0.86        & 0.18        & 0.24        & 0.26        & 0.27         \\ 
			\cline{2-19}
			&                                                    & \textcolor{blue}{\textbf{G*}}                                                                       & 26.40       & 43.31       & 51.29       & 59.08           & 8.26        & 15.95       & 23.48       & 29.50       & 0.92        & 0.92        & 0.92        & 0.93        & 0.74        & 0.76        & 0.77        & 0.78         \\
			\cline{3-19}
			& \multirow{-2}{*}{GMN}                                                   & SG                                                                       & 31.13       & 45.77       & 54.71       & \textcolor{red}{\textbf{59.68}}           & 3.78        & 15.67       & 22.8       & 28.13       & 0.92        & 0.92        & 0.92        & 0.93        & 0.75        & 0.77        & 0.78        & 0.79         \\  
			\cline{2-19}
			&                                  & G                                                                       & 34.33       & 48.70       & 54.49       & 56.89           & 10.35       & 16.54       & 20.49       & 23.85       & 0.88        & 0.91        & 0.91        & 0.92        & 0.46        & 0.52        & 0.53        & 0.53         \\ 
			\cline{3-19}
			\multirow{-6}{*}{\rotatebox[origin=c]{90}{\textsf{\textbf{Random}}}} &  \multirow{-2}{*}{SAFE} & SG & 37.13       & 51.89       & 58.08       & \textcolor{red}{\textbf{60.68}}           & 10.53       & 16.48       & 21.54       & 25.22       & 0.89        & 0.92        & 0.92        & 0.92        & 0.47        & 0.52        & 0.54        & 0.55         \\ 
			\hline\hline
			&                                                 & \textcolor{blue}{\textbf{G*}}                                                                       & 15.77       & 21.16       & 20.96       & 21.36           & 16.40       & 35.60       & 48.88       & 65.18       & 0.86        & 0.85        & 0.85        & 0.86        & 0.16        & 0.17        & 0.18        & 0.19         \\ 
			\cline{3-19}
			& \multirow{-2}{*}{Gemini}                                                & SG                                                                       & 15.17       & 18.56       & 19.36       & \textcolor{red}{\textbf{21.96}}           & 11.17       & 24.48       & 36.67       & 47.56       & 0.86        & 0.86        & 0.86        & 0.85        & 0.16        & 0.18        & 0.18        & 0.20         \\ 
			\cline{2-19}
			&                                                   & \textcolor{blue}{\textbf{G*}}                                                                       & 32.93       & 47.31       & 53.29       & \textcolor{red}{\textbf{53.29}}           & 6.82        & 16.50       & 22.81       & 26.68       & 0.92        & 0.92        & 0.93        & 0.94        & 0.59        & 0.65        & 0.68        & 0.70         \\
			\cline{3-19}
			& \multirow{-2}{*}{GMN}                                                   & SG                                                                       & 36.47       & 47.21       & 51.2       & 51.68           & 7.33        & 14.65       & 20.59       & 24.10       & 0.93        & 0.93        & 0.93        & 0.94        & 0.60        & 0.64        & 0.68        & 0.68         \\
			\cline{2-19}
			&                                  & G                                                                       & 60.28       & 77.05       & 80.84       & 83.43           & 20.36       & 32.90       & 47.90       & 55.36       & 0.90        & 0.91        & 0.92        & 0.92        & 0.38        & 0.43        & 0.45        & 0.46         \\ 
			\cline{3-19}
			\multirow{-6}{*}{\rotatebox[origin=c]{90}{\textsf{\textbf{Balanced}}}} & \multirow{-2}{*}{SAFE} & SG                      & 61.01       & 75.85       & 82.83       & \textcolor{red}{\textbf{83.43}}           & 9.49        & 14.24       & 18.02       & 21.28       & 0.90        & 0.92        & 0.93        & 0.93        & 0.39        & 0.44        & 0.46        & 0.46         \\
			\bottomrule
	\end{tabular}}
\end{table*}

\section{Evaluation}\label{sec:Eval}
In this section we evaluate our attacks and investigate the following research questions: 
\begin{mybox}
	\textbf{RQ1}: \textit{Are the three target models more robust against targeted or untargeted attacks?}

\textbf{RQ2}: \textit{Are the three target models more robust against black-box or white-box approaches?}

\textbf{RQ3}: \textit{Are models that consider CFG topology more robust against our attacks if compared to models that do not?}

\textbf{RQ4}: \textit{Does a different number of instructions for the source and target functions affect the success of the attack?}
\end{mybox}

\paragraph*{\textbf{Performance metrics}}
Our main evaluation metric is the \textbf{Attack success rate (A-rate)}, that is the percentage of adversarial samples that successfully mislead the target model.
We complement our investigation by collecting a set of support metrics to gain qualitative and quantitative insights into the attacking process: 
\begin{itemize}
\item \textbf{Modification size (M-size)}: number of inserted instructions;
\item \textbf{Average Similarity (A-sim)}: obtained final similarity values;
\item \textbf{Normalized Increment (N-inc)}: similarity increments normalized with respect to the initial value; only used for targeted attacks;
\item \textbf{Normalized Decrement (N-dec)}: similarity decrements normalized with respect to the initial value; only used for untargeted attacks.
\end{itemize}

Support metrics are computed over the set of samples that successfully mislead the model.

As an example, let us consider a targeted attack against three pairs of functions with initial similarities 0.40, 0.50, and 0.60. After the attack we reach final similarities that are 0.75, 0.88, and 0.94. We deem an attack as successful if the final similarity is above $0.8$ (the reason will be clear in the next section). In this example, we have an \textbf{A-rate} of 66.66\%, an \textbf{A-sim} of 0.91 and a \textbf{N-inc} of 0.81.

The \textbf{N-inc} is the average of the formula below over the samples that successfully mislead the model:

\begin{equation}\label{eq:N_inc}
\begin{aligned}
\frac{final \; similarity - initial \; similarity}{1 - initial \; similarity}
\end{aligned}
\end{equation}

The denominator for the fraction above is the maximum possible increment for the analyzed pair: we use it to normalize the obtained increment. Intuitively, the value of this metric is related with the initial similarities of the successfully attacked pair. Consider a targeted attack where a pair exhibits a final similarity of 0.80. When the normalized increment is 0.7, their initial similarity is 0.33 (from Equation~\ref{eq:N_inc}); when the normalized increment is 0.3, we have a much higher 0.7 initial similarity.

The comparison between  \textbf{A-sim} and the success threshold gives us insights on the ability of the attack to reach high similarity values. In the aforementioned example, the  \textbf{A-sim} value of 0.91 shows that when the attack is able to exceed the success threshold, it has actually an easy time to bring the similarity around the value of 0.91. 

\paragraph*{\textbf{Evaluation outline}}
We test our black-box and white-bock attacks against each target model in both scenarios. As discussed in Section~\ref{sec:dataset}, we use datasets \textsf{Random} and \textsf{Balanced} for the former and dataset \textsf{Untarg} for the latter.


\subsection{Setup}
In this section, we describe the attack parameters selected for our experimental evaluation.

\paragraph*{\textbf{Successful Attacks}} \label{sec:tau}

%\GADL{
An attack is successful depending on the similarity value between the adversarial sample and the target function. For a targeted attack, the similarity score has to be increased during the attack until it trespasses a success threshold \T. For an untargeted attack, this score, which is initially 1, has to decrease until it is below a success threshold \U. Operatively, the values of such thresholds are determined by the way the similarity score is used in practice. In our experimental evaluation, we choose the thresholds as follows. We compute the similarity scores that our attacked systems give over a set of similar pairs and over a set of dissimilar pairs. For the first set, the average score is 0.79 with a standard deviation of 0.15. For the second set, these values are respectively 0.37 and 0.17. We thus opted for a success threshold $\mU=0.5$ for untargeted attacks and $\mT=0.8$ for targeted ones. Both \U and \T are over one standard deviation distant from the average similarity value measured for the relevant set for the attack. For the charts, we plot \U $\in[0.46, 0.62]$ and \T $\in[0.74, 0.88]$.

To fully understand the performance of the attacks, we also measure the amount of function pairs in a dataset already meeting a given threshold. For the targeted scenario, we plot it as a line labeled \textbf{C0}. As our readers can see (Figure~\ref{img:arates_BBTargeted} and~\ref{img:arates_WBTargeted}), their contribution is marginal: hence, we do not discuss them in the remainder of the evaluation. For the untargeted scenario, no such pair can exist by construction.  


\paragraph*{\textbf{Black-box Attacks}}\label{sec:blacksetup}
To evaluate the effectiveness of Spatial Greedy against the black-box baseline Greedy, we select a maximum perturbation size $\bar{\delta}$ and a number of dead branches $B$ in four settings: \textbf{C1} ($\bar{\delta}=15$, $B=5$), \textbf{C2} ($\bar{\delta}=30$, $B=10$), \textbf{C3} ($\bar{\delta}=45$, $B=15$), and \textbf{C4} ($\bar{\delta}=60$, $B=20$). 


We set $\varepsilon = 0.1$ in all greedy attacks. For Spatial Greedy and black-box Greedy, we test two sizes for the set of candidates: $110$ and $400$. For Greedy, we pick $110$ instructions manually and then randomly add others for a total of $400$; for Spatial Greedy, we recall that the selection is dynamic (Section~\ref{sec:spatialgreedy:selection}). The larger size brought consistently better results in both attacks, hence we present results only for it. Finally, for Spatial Greedy, we use $c = 10$ and $r \in \{0.25, 0.50, 0.75\}$, with $r = 0.75$ being the most effective choice in our tests (thus, the only one presented next). For the gray-box Greedy embodiments for Gemini and GMN, we refer to Section~\ref{sec:black:gemini} and~\ref{sec:black:gmn}, respectively.

\paragraph*{\textbf{White-box Attack}}\label{sec:whitesetup}
We evaluate GCAM considering four different values for the number $B$ of inserted dead branches: \textbf{C1} ($B=5$), \textbf{C2} ($B=10$), \textbf{C3} ($B=15$), and \textbf{C4} ($B=20$). For each model, we use the number of iterations that brings the attack to convergence. 

\begin{table*}[t!]
\ra{1.5}
\footnotesize
\centering
\caption{Evaluation metrics with $\mU=0.50$ relative to the \textbf{black-box} attacks against the three target models in the \textbf{untargeted} scenario. Spatial Greedy (SG) is evaluated using parameters $\varepsilon = 0.1$ and $r = 0.75$. Greedy (G) is evaluated using $\varepsilon = 0.1$. Similarly to Table~\ref{tab:BBTargeted}, \textcolor{blue}{\textbf{G*}} is the gray-box version of Greedy where applicable. When examining G against SAFE, a set of candidates of size 400 is considered.}
\label{tab:BBUntargeted}
\resizebox{\linewidth}{!}{%
	\begin{tabular}{c|c|c|cccc|cccc|cccc|cccc} 
		\toprule
		\multicolumn{1}{l|}{\multirow{2}{*}{}} & \multirow{2}{*}{\textbf{Target}} & \multirow{2}{*}{\textbf{Attack}}                        & \multicolumn{4}{c|}{\textbf{A-rate (\%) ($\mU=0.50$)}} & \multicolumn{4}{c|}{\textbf{M-size ($\mU=0.50$)}}  & \multicolumn{4}{c|}{\textbf{A-sim ($\mU=0.50$)}}   & \multicolumn{4}{c}{\textbf{N-dec ($\mU=0.50$)}}     \\ 
		\cline{4-19}
		\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4}     & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4} & \textbf{C1} & \textbf{C2} & \textbf{C3} & \textbf{C4}  \\ 
		\toprule\toprule
		
		& & \textcolor{blue}{\textbf{G*}}                            & 23.60        & 37.40        & 47.0        & 50.60            & 2.73        & 5.24        & 9.16        & 11.78       & 0.47        & 0.48        & 0.48        & 0.48        & 0.53        & 0.52        & 0.52        & 0.52         \\ 
		\cline{3-19}
		& \multirow{-2}{*}{Gemini}      & SG                             & 22.95        & 40.32        & 48.10        & \textcolor{red}{\textbf{53.89}}            & 3.32        & 5.80        & 9.09        & 11.35       & 0.48        & 0.48        & 0.48        & 0.48        & 0.52        & 0.52        & 0.52        & 0.52         \\ 
		\cline{2-19}
		& & \textcolor{blue}{\textbf{G*}}                             & 68.40        & 86.20        & 91.02       & \textcolor{red}{\textbf{93.81}}           & 2.55        & 3.77        & 3.82        & 3.71        & 0.27        & 0.25        & 0.22        & 0.21        & 0.73        & 0.75        & 0.77        & 0.79         \\
		\cline{3-19}
		& \multirow{-2}{*}{GMN}         & SG                             & 65.87        & 83.23        & 88.13       & 91.62           & 2.75        & 3.21        & 4.12        & 4.14        & 0.27        & 0.24        & 0.24        & 0.23        & 0.73        & 0.76        & 0.76        & 0.77         \\
		\cline{2-19}
		& & G                             & 44.71       & 74.65       & 82.83       & 88.22           & 6.59        & 8.28        & 8.77        & 9.04        & 0.40        & 0.42        & 0.42        & 0.42        & 0.40        & 0.44        & 0.45        & 0.45         \\ 
		\cline{3-19}
		\multirow{-6}{*}{\rotatebox[origin=c]{90}{\textsf{\textbf{Untarg}}}} & \multirow{-2}{*}{SAFE} & SG & 56.49       & 80.83       & 87.42       & \textcolor{red}{\textbf{90.62}}           & 6.67        & 7.74        & 7.77        & 7.64        & 0.39        & 0.41        & 0.42        & 0.42        & 0.60        & 0.59        & 0.58        & 0.59         \\
		\bottomrule
\end{tabular}}
\end{table*}

\subsection{Complete Attack Results}\label{sec:complete-results}
This section provides complete results for our black-box and white-box attacks on the three target models. For brevity, we focus only on Spatial Greedy when discussing black-box targeted and untargeted attacks, leaving out the results for the baseline Greedy. The two will see a detailed comparison later, with Spatial Greedy emerging as generally superior.

\subsubsection{Black-box Targeted Attack}\label{sec:BB_Targeted}
Considering an attacker with {black-box} knowledge in a {targeted} scenario, the three target models show a similar behavior against Spatial Greedy.

\subsubsection*{\textsf{Random} Dataset}
The attack success rate \textbf{A-rate} is positively correlated with the number $B$ of dead branches and the maximum number $\bar{\delta}$ of instructions introduced in the adversarial example. Fixing at $\mT=0.80$ the success threshold for the attack, we have an \textbf{A-rate} that on Gemini goes from 15.77\% (setting \textbf{C1}) up to 27.54\% (setting \textbf{C4}). The other target models follow this behavior, as the \textbf{A-rate} for GMN goes from 31.13\% up to 59.68\%, and from 37.13\% up to 60.68\% for SAFE. This trend holds for other success thresholds as visible in Figure~\ref{img:arates_BBTargeted}. From these results, it is evident that the higher the values of the two parameters, the lower the robustness of the attacked models. Table~\ref{tab:BBTargeted} presents a complete overview of the results.

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{BB_targeted_C0.pdf}
\caption{\textbf{Black-box targeted} attack with Spatial Greedy against the three target models on the \textsf{Random} \textbf{(a)} and \textsf{Balanced} \textbf{(b)} datasets, while varying the success threshold \T $\in[0.74, 0.88]$. We use a set of candidates of 400 instructions, $\varepsilon = 0.1$, and $r = 0.75$.} 
\label{img:arates_BBTargeted}
\end{figure}

The other metrics confirm the relationship between the parameters $B$ and $\bar{\delta}$ and the effectiveness of our attack. In particular, when increasing the perturbation size, as highlighted by the modification size \textbf{M-size} metric, both \textbf{A-sim} and the normalized increment \textbf{N-inc} increase, suggesting that incrementing the perturbation size is always beneficial. 

\subsubsection*{\textsf{Balanced Dataset}}
We see similar trends also when considering pairs of functions having similar initial length. When fixing $\mT =0.80$, the \textbf{A-rate} for GMN is as low as 36.47\% in setting  \textbf{C1} and as high as 51.68\% in setting \textbf{C4}; for SAFE, it is as low as 61.01\% in setting \textbf{C1} and as high as 83.43\% in setting \textbf{C4}. Contrarily, Gemini shows higher resilience to the attacks, as the \textbf{A-rate} referring to $\mT= 0.80$ is as low as 15.17\% in \textbf{C1} and as high as 21.96\% in setting \textbf{C4}. Figure~\ref{img:arates_BBTargeted} and Table~\ref{tab:BBTargeted} report a complete overview of the results.

The \textbf{A-sim} and the \textbf{N-inc} metrics further highlight the correlation between the two parameters and the efficacy of our approaches. When increasing the perturbation size, as pointed out by the \textbf{M-size} metric, it is easier to obtain higher final similarity values regardless of the initial ones.


\subsubsection{Black-box Untargeted Attack}\label{sec:BB_Untargeted}
Considering an attacker with {black-box} knowledge in a {untargeted} scenario, all the three target models are vulnerable to Spatial Greedy, with different robustness.

The observations highlighted in Section~\ref{sec:BB_Targeted} also hold in this scenario. Incrementing  $B$ and $\bar{\delta}$ is beneficial for the attacker. As visible in Figure~\ref{img:BB_WB_Untargeted} and in Table~\ref{tab:BBUntargeted}, the attack success rate \textbf{A-rate} for $\mU=0.50$ in setting \textbf{C1} is 22.95\% for Gemini, 65.87\% for GMN, and 56.49\% for SAFE. The metric increases across settings, peaking at 53.89\% for Gemini, 91.62\% for GMN, and 90.62\% for SAFE in setting \textbf{C4}.

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{BB_WB_untargeted.pdf}
\caption{\textbf{(a) Black-box untargeted} attack with Spatial Greedy against the three target models while varying the success threshold \U $\in[0.46, 0.62]$, and the settings \textbf{C1}, \textbf{C2}, \textbf{C3}, and \textbf{C4}. We use a set of candidates of 400 instructions, $\varepsilon = 0.1$, and $r = 0.75$. \newline \textbf{(b) White-box} \textbf{untargeted} attack against the three target models while varying the success threshold \U $\in[0.46, 0.62]$, and the settings \textbf{C1}, \textbf{C2}, \textbf{C3}, and \textbf{C4}. \textit{Left}: GCAM attack with 40k iterations against GEMINI. \textit{Center}: GCAM attack with 1k iterations against GMN. \textit{Right}: GCAM attack with 1k iterations against SAFE.}
\label{img:BB_WB_Untargeted}
\end{figure}

Table~\ref{tab:BBUntargeted} also reports the results for modification size metric \textbf{M-size}. In this case, we can see the effectiveness of Spatial Greedy as a small number of added instructions is needed against each of the considered target models. Indeed, considering setting \textbf{C4}, which is the one that modifies the function most, the \textbf{M-size} at $\mU=0.50$ is 11.35 for Gemini, 4.14 for GMN, and 7.64 for SAFE.

\subsubsection{White-box Targeted Attack}\label{sec:WB_Targeted}

With an attacker with {white-box} knowledge in a {targeted} scenario, the three target models show different behaviors. Table~\ref{tab:WBTargeted} presents a complete overview of the results.

\begin{table*}
\centering
\footnotesize
\ra{1.7}
\caption{Evaluation metrics with $\mT=0.80$ for the \textbf{white-box targeted} attack against the three target models. The GCAM attack is executed up to 20k iterations for Gemini and up to 1k for GMN and SAFE.}
\label{tab:WBTargeted}
\resizebox{\linewidth}{!}{%
	\begin{tabular}{c|c|cccc|cccc|cccc|cccc} 
		\toprule
		\multicolumn{1}{l|}{\multirow{2}{*}{}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Target}}} & \multicolumn{4}{c|}{\textbf{A-rate (\%) ($\mT=0.80$)}}                                                        & \multicolumn{4}{c|}{\textbf{M-size ($\mT=0.80$)}}                       & \multicolumn{4}{c|}{\textbf{A-sim ($\mT=0.80$)}}                                                            & \multicolumn{4}{c}{\textbf{N-inc ($\mT=0.80$)}}                                           \\ 
		\cline{3-18}
		\multicolumn{1}{l|}{}                  & \multicolumn{1}{l|}{}                                 & \textbf{C1}                    & \textbf{C2}                     & \textbf{C3} & \textbf{C4}                     & \textbf{C1}                      & \textbf{C2} & \textbf{C3} & \textbf{C4} & \textbf{C1}                    & \textbf{C2}                    & \textbf{C3} & \textbf{C4}                    & \textbf{C1} & \textbf{C2} & \textbf{C3}                    & \textbf{C4}                     \\ 
		\toprule\toprule
		& Gemini                                                & 24.35                          & 29.94                           & 30.94       & \textcolor{red}{\textbf{31.60}}        & 53.67  & 86.53       & 111.83      & 133.84      & 0.85                           & 0.86                           & 0.86        & 0.86      & 0.54        & 0.60        & 0.63       & 0.62                            \\ 
		\cline{2-18}
		& GMN                                                   & 34.73            & \textcolor{red}{\textbf{38.32}}             & 34.93       & 35.33      & 212.79      & 350.5       & 439.03      & 461.08      & 0.85                           & 0.84                           & 0.84        & 0.84         & 0.78        & 0.78        & 0.77          & 0.79  \\
		\cline{2-18}
		\multirow{-3}{*}{\rotatebox[origin=c]{90}{\textsf{\textbf{Random}}}}    & SAFE                           & 11.57                          & 18.96                           & 20.36       & \textcolor{red}{\textbf{21.76}}                           & 18.62                            & 31.87       & 38.27       & 38.90       & 0.84                           & 0.85                           & 0.85        & 0.85         & 0.67        & 0.71        & 0.70                           & 0.71  \\ 
		\hline\hline
		& Gemini                                 & \textcolor{red}{\textbf{36.60}}        & 34.93     & 33.13       & 29.94      & 95.86            & 101.83      & 134.51      & 161.63      & 0.86       & 0.85          & 0.86        & 0.86           & 0.53        & 0.51        & 0.52       & 0.54                            \\ 
		\cline{2-18}
		& GMN                                                   & 44.71                          & \textcolor{red}{\textbf{55.89}}    & 49.30       & 45.31        & 474.73            & 688.89      & 838.48      & 847.01      & 0.85       & 0.85   & 0.85        & 0.85      & 0.70        & 0.72        & 0.71        & 0.71                            \\
		\cline{2-18}
		\multirow{-3}{*}{\rotatebox[origin=c]{90}{\textsf{\textbf{Balanced}}}}                & SAFE                                                  & 16.60                           & 26.34                           & 25.35       & \textcolor{red}{\textbf{27.35}} & 16.39  & 28.30       & 33.67       & 36.99       & 0.85                           & 0.84                           & 0.85        & 0.85      & 0.47     & 0.56        & 0.57                           & 0.58                            \\
		\bottomrule
\end{tabular}}
\end{table*}

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{WB_targeted_C0.pdf}
\caption{\textbf{White-box} \textbf{targeted} attack against the three target models on the \textsf{Random} \textbf{(a)} and \textsf{Balanced} \textbf{(b)} datasets while varying the success threshold \T $\in[0.74, 0.88]$ and settings \textbf{C0} to \textbf{C4}. \textit{Left}: GCAM attack with 20k iterations against GEMINI. \textit{Center}: GCAM attack with 1k iterations against GMN. \textit{Right}: GCAM attack with 1k iterations against SAFE.} 
\label{img:arates_WBTargeted}
\end{figure}

\subsubsection*{\textsf{Random Dataset}}
Both Gemini and SAFE show a higher robustness to our GCAM attack if compared to GMN.

As visible in Figure~\ref{img:arates_WBTargeted}, when attacking Gemini and SAFE, there is a positive correlation between the number $B$ of locations (i.e., dead branches) where to insert perturbations and the attack success rate \textbf{A-rate}. When considering setting \textbf{C1}, the \textbf{A-rate} for $\mT=0.80$ is 24.35\% for Gemini, and 11.57\% for SAFE; moving to \textbf{C4}, it increases up to 31.60\% for Gemini, and 21.76\% for SAFE. On the contrary, GMN does not show a monotonic \textbf{A-rate} increase for an increasing $B$ value, as the peak \textbf{A-rate} is 38.32\% in setting \textbf{C2}.


We now discuss the modification size \textbf{M-size} metric: fixing $\mT=0.80$ and considering the setting where \textbf{A-rate} peaks, we measure an \textbf{M-size} value of 38.90 for SAFE (\textbf{C4}), 133.84 for Gemini (\textbf{C4}), and 350.50 for GMN (\textbf{C2}): SAFE is the model that sees the insertion of fewer instructions. This is not surprising: due to the feature-space representation of SAFE,  the embeddings we alter in the attack for it (Section~\ref{sec:white:safe}) refer to a number of instructions that is fixed.


\begin{table*}[t!]
\centering
\footnotesize
\ra{1.5}
\caption{Evaluation metrics with $\mU=0.50$ for the \textbf{white-box untargeted} attack against the three target models. The GCAM attack is executed up to 20k iterations for Gemini and up to 1k for GMN and SAFE.}
\label{tab:WBUntargeted}
\resizebox{\linewidth}{!}{%
	\begin{tabular}{c|c|cccc|cccc|cccc|cccc} 
		\toprule
		\multicolumn{1}{l|}{\multirow{2}{*}{}} & \multicolumn{1}{l|}{\multirow{2}{*}{\textbf{Target}}} & \multicolumn{4}{c|}{\textbf{A-rate (\%) ($\mT=0.50$)}}                                                        & \multicolumn{4}{c|}{\textbf{M-size ($\mT=0.50$)}}                       & \multicolumn{4}{c|}{\textbf{A-sim ($\mT=0.50$)}}                                                            & \multicolumn{4}{c}{\textbf{N-dec ($\mT=0.50$)}}                                           \\ 
		\cline{3-18}
		\multicolumn{1}{l|}{}                  & \multicolumn{1}{l|}{}                                 & \textbf{C1}                    & \textbf{C2}                     & \textbf{C3} & \textbf{C4}                     & \textbf{C1}                      & \textbf{C2} & \textbf{C3} & \textbf{C4} & \textbf{C1}                    & \textbf{C2}                    & \textbf{C3} & \textbf{C4}                    & \textbf{C1} & \textbf{C2} & \textbf{C3}                    & \textbf{C4}                     \\ 
		\toprule\toprule
		
		& Gemini                                                & 16.37       & 28.94       & 36.32                           & \textcolor{red}{\textbf{39.52}}       & 46.63       & 79.76       & 103.86          & 117.21      & 0.48        & 0.48        & 0.48        & 0.47        & 0.51        & 0.52        & 0.52        & 0.53         \\ 
		\cline{2-18}
		& GMN                                                   & 68.06       & 80.24       & 83.63                           & \textcolor{red}{\textbf{84.63}}       & 298.34      & 541.32      & 718.57       & 859.53      & 0.23        & 0.19        & 0.18        & 0.18        & 0.78        & 0.81        & 0.82        & 0.82         \\
		\cline{2-18}
		\multirow{-3}{*}{\rotatebox[origin=c]{90}{\textsf{\textbf{Untarg}}}} & SAFE                                                  & 68.46       & 83.43       & \textcolor{red}{\textbf{88.42}}      &  87.42       &Â 17.52       & 30.01       & 36.0       & 38.49       & 0.39        & 0.38        & 0.39        & 0.40        & 0.56        & 0.58        & 0.58        & 0.58         \\
		\bottomrule
\end{tabular}}
\end{table*}

\subsubsection*{\textsf{Balanced Dataset}}
On pairs of functions having similar initial length, the GCAM attack can generate adversarial examples that can deceive all the three target models, even though we find no direct relationship between the parameter $B$ and the attack success rate \textbf{A-rate}. As visible in Figure~\ref{img:arates_WBTargeted} and reported in Table~\ref{tab:WBTargeted}, SAFE manifests the highest robustness with a peak \textbf{A-rate} of 27.35\% in setting \textbf{C4} at $\mT=0.80$. For the same threshold choice, the peak \textbf{A-rate} is higher for both Gemini (i.e., 36.60\% in \textbf{C1}) and GMN (i.e., 55.89\% in \textbf{C2}).

Consistently with the \textsf{Random} results, GCAM resorts to fewer instructions when targeting the SAFE model: the modification size \textbf{M-size} at $\mT=0.80$ is 36.99 in the \textbf{C4} setting. On the contrary, the \textbf{M-size} at $\mT=0.80$ is 95.86 in setting \textbf{C1} for Gemini, and 688.89 in setting \textbf{C2} for GMN.

\subsubsection{White-box untargeted attack}\label{sec:WB_Untargeted}
Figure~\ref{img:BB_WB_Untargeted} and Table~\ref{tab:WBUntargeted} report the results for our attacks with {white-box} knowledge in the {untargeted} scenario.

Gemini looks more robust than the other models: for example, fixing $\mU=0.50$, we measure the highest attack success rate \textbf{A-rate} as 39.52\% in the \textbf{C4} setting. On the contrary, for the same \U, the highest \textbf{A-rate} for SAFE is 88.42\% (setting \textbf{C3}) and 84.63\% for GMN (setting \textbf{C4}).

The general trend of having a positive correlation of $B$ and the \textbf{A-rate} is still observable (with a sharp increase of the \textbf{A-rate} from setting \textbf{C1} to \textbf{C2}). The \textbf{M-size} shows that SAFE is the most fragile model in terms of instructions to add, as they are much fewer than with the other two models. 

\subsubsection{Greedy vs. Spatial Greedy}
We now compare the performance of Spatial Greedy against the Greedy baseline, until now left out of our discussions for brevity. Figure~\ref{img:greedySpatial_UB_BBTargeted} shows the results for a targeted attack on the \textsf{Random}. Additional data points are available in Table~\ref{tab:BBTargeted}. 


We discuss Gemini and GMN first. We recall that we could exploit their feature extraction process to reduce the size of the set of candidates, devising a gray-box Greedy procedure. Spatial Greedy is instead always black-box.

\begin{figure}[h!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{greedySpatial_Targeted.pdf}
\caption{\textbf{Greedy} and \textbf{Spatial Greedy} targeted attacks against the three models while varying the success threshold \T $\in[0.74, 0.88]$, using the \textsf{Random} dataset and setting \textbf{C4}. For both, we consider $\varepsilon = 0.1$ and $|\texttt{CAND}| = 400$. For Spatial Greedy, we also set $r = 0.75$.}
\label{img:greedySpatial_UB_BBTargeted}
\end{figure}

Considering the attack success rate \textbf{A-rate} at $\mT=0.80$, Spatial Greedy always outperform the gray-box baseline, except for setting \textbf{C4} on Gemini (although the two perform similarly: 27.94\% for Greedy and 27.56\% for Spatial Greedy).  Looking at the other metrics, we can see that our black-box approach based on instructions embeddings is on par or improves on the results provided by the gray-box baseline. There are only few cases in which gray-box Greedy outperforms Spatial Greedy: i.e., the modification size \textbf{M-size} at $\mT=0.80$ on Gemini for settings \textbf{C1} and \textbf{C3}.

Moving to SAFE, we recall that only a black-box Greedy is feasible. Considering the \textbf{A-rate}, we can notice that increasing both $\bar{\delta}$ and $B$ produces a more noticeable difference between the baseline technique and Spatial Greedy. In the \textbf{C1} setting, the \textbf{A-rate} at $\mT=0.80$ is 34.33\% for Greedy and 37.13\% for Spatial Greedy; then, it increases up to 56.89\% for Greedy and 60.68\% for Spatial Greedy when considering the \textbf{C4} scenario.

The other metrics confirm this behavior. Considering the average similarity \textbf{A-sim}, regardless of the chosen $\bar{\delta}$ and $B$ from the setting, we can observe that adversarial pairs generated through Spatial Greedy present a final average similarity that is higher than the one relative to the pairs generated using the baseline solution. The effectiveness of Spatial Greedy is finally confirmed by the normalized increment \textbf{N-inc} metric; at a comparison of the results, the impact of the candidates selected using Spatial Greedy is more consistent if compared to the one of the candidates selected using the baseline approach.

Comparing Spatial Greedy with Greedy, we measure on the \textsf{Random} dataset an average A-rate increase of $2.26$ and a decreased M-size by $0.5$ instructions across all configurations and models. On the \textsf{Balanced} dataset, the two attacks are essentially on par in terms of success ($0.25$ A-rate increase for Spatial Greedy), but Spatial Greedy uses appreciably fewer instructions (M-size decrease of $12$). 

We omit a detailed discussion for the untargeted scenario for brevity. For average results across all models and configurations, Spatial Greedy sees an A-rate increase of $1.75$, whereas the M-size is smaller by $0.16$ instructions.

\begin{mybox2}{\bf Take away:}
We conclude that \textbf{Spatial Greedy} is typically superior (and always at least comparable) to a Greedy attack even when an efficient gray-box Greedy variant is possible. The results suggest that our dynamic update of the set of candidates, done at each iteration of the optimization procedure, can lead to the identification of new portions of the instruction space (and consequently a new subset of the ISA) that can positively influence the attack results.
\end{mybox2}

\subsection{RQ1: Targeted vs. Untargeted Attacks}
From the previous sections, the attentive reader may have noticed that all our approaches are much more effective in an \textbf{untargeted} scenario for all models and proposed metrics. The analysis provided below takes, for the targeted attacks, the values obtained on the \textsf{Random} dataset, but analogous trends can be observed if picking the \textsf{Balanced} one instead.

When looking at attack success rate \textbf{A-rate} for all thresholds of similarities, the three target models are less robust against untargeted attacks (rather than targeted ones) regardless of the adversarial knowledge. For the best attack among black-box and white-box configurations, in the targeted scenario, the peak \textbf{A-rate} at $\mT=0.80$ is 27.54\% for Gemini, 59.68\% for GMN, and 60.68\% for SAFE. For the untargeted scenario, the peak \textbf{A-rate} at $\mU=0.50$ is 53.89\% for Gemini, 91.62\% for GMN, and 90.62\% for SAFE.

The number of instructions \textbf{M-size} needed for generating valid adversarial examples further confirms the weak resilience of the target models to untargeted attacks. When considering the worst setting according to \textbf{M-size} (i.e., \textbf{C4}), while we need only few instructions for untargeted attacks at $\mU=0.50$ (i.e., 11.35 for Gemini, 4.14 for GMN, and 7.64 for SAFE), we need a significantly higher number of added instructions for targeted attacks (i.e., 44.02 for Gemini, 28.13 for GMN, and 25.22 for SAFE) at $\mT=0.80$.

\begin{mybox2}{\bf Take away:}
On all the attacked models, both targeted and untargeted attacks are feasible, especially using Spatial Greedy (see also RQ2). Their resilience against untargeted attacks is significantly lower.
\end{mybox2} 

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{blackWhite_Targeted.pdf}
\caption{\textbf{Black-box} and \textbf{white-box targeted} attacks against the three models while varying the success threshold \T $\in[0.74, 0.88]$ and considering the \textsf{Random} dataset. In the {black-box} scenario, all the results refer to the Spatial Greedy approach ($\varepsilon = 0.1$, $r = 0.75$, and $|\texttt{CAND}|=400$). In the {white-box} scenario, the results for Gemini are for a GCAM attack with 20k iterations while the ones for SAFE and GMN are for a GCAM attack with 1k iterations. We consider all approaches in their most effective parameter choice, being it always setting \textbf{C4} except for the GCAM attack against GMN, for which we consider setting \textbf{C2}.} 
\label{img:blackWhite_Targeted}
\end{figure}

\subsection{RQ2: Black-box vs. White-box Attacks}\label{ssec:rq2}
An interesting finding from our tests is that the white-box strategy does not always outperform the black-box one.

Figure~\ref{img:blackWhite_Targeted} depicts a comparison in the targeted scenario between Spatial Greedy and GCAM for the attack success rate \textbf{A-rate}, average similarity \textbf{A-sim}, and normalized increment \textbf{N-inc} metrics. The comparison considers the \textsf{Random} dataset; we will cover the \textsf{Balanced} dataset separately with a brief discussion. The figure shows how different values of the success attack threshold \T can influence the considered metrics. On GMN and SAFE, Spatial Greedy is more effective than GCAM, resulting in significantly higher \text{A-rate} values, while the two perform similarly on Gemini.

Interestingly, in contrast with the evaluation based on the \textbf{A-rate} metric, both the \textbf{A-sim} and \textbf{N-inc} values highlight a coherent behavior among the three target models. Generally, adversarial examples generated using Spatial Greedy exhibit a higher \textbf{A-sim} value than the white-box ones (considering $\mT=0.80$, we have 0.86 vs. 0.86 for Gemini, 0.93 vs. 0.84 for GMN, and 0.92 vs. 0.85 for SAFE). Looking at \textbf{N-inc}, we face a completely reversed situation; the metric is better in the adversarial samples generated using GCAM (0.62 for Gemini, 0.79 for GMN, and 0.71 for SAFE) compared to those from Spatial Greedy (0.27 for Gemini, 0.79 for GMN, and 0.55 for SAFE). These two observations lead us to the hypothesis that the black-box attack is more effective against pairs of binary functions that exhibit high initial similarity values and can potentially reach a high final similarity. On the other side, GCAM is particularly effective against pairs that are very dissimilar at the beginning.

For the \textsf{Balanced} dataset, we observe trends analogous to those discussed above for the effects of black-box and white-box targeted attacks. The only difference worth mentioning involves the \textbf{A-rate} for GMN, with the GCAM attack now prevailing on Spatial Greedy (55.89\% vs 51.68\%).

For the untargeted scenario, our results (Tables~\ref{tab:BBUntargeted} and~\ref{tab:WBUntargeted}) for the \textbf{A-rate} metric considering $\mU=0.50$ show that Spatial Greedy has a slight advantage on GCAM. For Spatial Greedy, we have best-setting values of 53.89\% for Gemini, 91.62\% for GMN, and 90.62\% for SAFE; for GCAM, we have 39.52\% for Gemini, 84.63\% for GMN, and 88.42\% for SAFE.

In our experiments, GCAM performed worse than the black-box strategy, which may look puzzling since theoretically a white-box attack should be more potent than a black-box one. We explored if this could be due to the obfuscated gradient phenomenon~\cite{athalye2018obfuscated} or the inverse feature mapping problem (Section~\ref{sec:invMapping}). Our hypothesis leans towards the latter.
To this end, we conducted a GCAM attack exclusively in the feature space by eliminating all constraints needed to identify a valid potential sample in the problem space (i.e., non-negativity of coefficients for Gemini and GMN, rounding to genuine instruction embeddings for SAFE). As a result, GCAM achieved a success rate between 86.03\% and 99.81\% in targeted scenarios and between 97.01\% and 100\% in untargeted ones.
As indicated in~\cite{athalye2018obfuscated}, high performance once constraints are removed strongly suggests the absence of obfuscated gradient phenomena.


\begin{mybox2}
{\bf Take away:} Our tests show that the Spatial Greedy black-box strategy is on par or beats our white-box GCAM attack based on a rounding inverse strategy. Further investigation is needed to confirm if this result will hold for more refined inverse feature mapping techniques and when attacking other models. 
\end{mybox2}


\subsection{RQ3: Role of CFG-related Features}\label{sec:rq-cfg}
Among the target models under study, we can distinguish between the ones considering the CFG during the feature extraction process (Gemini and GMN) and the one that does not (SAFE). We want to investigate whether this aspect can influence the robustness of a model against our attacks. We focus on the \textsf{Random} dataset as its pairs of functions have an average CFG node count difference that is higher than in the \textsf{Balanced} dataset (17.8 vs 7.5, Section~\ref{sec:dataset}). This metric acts as a proxy for having (more) different CFG-related features.

In the {targeted} black-box scenario (Figure~\ref{img:arates_BBTargeted} and Table~\ref{tab:BBTargeted}), SAFE is the weakest among the three considered models, as the peak attack success rate \textbf{A-rate} at $\mT=0.80$ is 60.68\% for SAFE, 59.68\% for GMN, and 27.54\% for Gemini (\textbf{C4} setting). We thus conducted a manual analysis of the CFGs of each tested pair of functions: the majority of the successful attacks on all three models are from pairs where the initial CFGs have a similar number of nodes.

In the targeted {white-box} scenario (Figure~\ref{img:arates_WBTargeted} and Table~\ref{tab:WBTargeted}), the results slightly change. In particular, GCAM is particularly effective against GMN (the \textbf{A-rate} for it at $\mT=0.80$ peaks at 38.32\%) while both SAFE and Gemini show more robustness (the peak \textbf{A-rate} $\mT=0.80$ is 21.75\% for SAFE and 31.60\% for Gemini). We believe that these results are due to the difficulty in inverting the feature mapping which, as pointed out in Section~\ref{sec:invMapping}, prevents the application of classical white-box approaches against code models.

As for untargeted attacks, we recall the initial CFGs in each pair are trivially identical. We find that the choice whether to rely on CFG features in the model does not impact the final result. In the black-box scenario, for example, in the peak setting \textbf{C4} the \textbf{A-rate} at $\mU=0.50$ is 53.89\% for Gemini, 91.62\% for GMN, and 90.62\% for SAFE (Table~\ref{tab:BBUntargeted} and Figure~\ref{img:BB_WB_Untargeted}). Similarly, in the white-box scenario, the \textbf{A-rate} at $\mU=0.50$ in the best setting is 39.52\% for Gemini, 84.63\% for GMN, and 88.42\% for SAFE (Table~\ref{tab:WBUntargeted} and Figure~\ref{img:BB_WB_Untargeted}).

\begin{mybox2}{\bf Take away:} The presence of CFG-based features in the attacked model plays an important role when manipulating pairs of functions that are unbalanced in the number of CFG nodes, requiring a greater effort by the attacker. Untargeted attacks are unaffected. \end {mybox2}

\begin{figure}[t!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{BalUnbal.pdf}
\caption{Resilience of the three models to \textbf{targeted} \textbf{black-box} and \textbf{white-box} attackers on the \textsf{Random} and \textsf{Balanced} datasets for a success threshold \T $\in[0.74, 0.88]$. The best setting for each attack is shown.\newline 
\textbf{(a) Black-box} attacker. We test the Spatial Greedy approach against the target models with $\varepsilon = 0.1$, $r = 0.75$, $|\texttt{CAND}| = 400$, and setting \textbf{C4}.\newline
\textbf{(b) White-box} attacker. \textit{Left}: GCAM attack with 20k iterations against GEMINI in setting \textbf{C4} for the \textsf{Random} dataset and \textbf{C1} for the \textsf{Balanced} dataset. \textit{Center}: GCAM attack with 1k iterations against GMN, considering setting \textbf{C2} for both datasets. \textit{Right}: GCAM attack with 1k iterations against SAFE, considering setting \textbf{C4} for both datasets.} 
\label{img:balUnbal}
\end{figure}

\subsection{RQ4: Role of Initial Function Size Difference}\label{sec:rq-size}
In this section, we review our targeted-attack results by studying if the three target models are more robust against adversarial samples generated from pairs of either random functions (\textsf{Random} dataset) or functions initially balanced in length ({\textsf{Balanced} dataset}). We evaluate this research question by considering the attack success rate \textbf{A-rate}.

Figure~\ref{img:balUnbal} and Table~\ref{tab:BBTargeted} show the results in the case of a {black-box} attacker. SAFE is less resilient against adversarial samples generated from the \textsf{Balanced} dataset. For example, considering the \textbf{C4} setting, the \textbf{A-rate} at $\mT=0.80$ is 60.68\% on the \textsf{Random} dataset and it increases to 83.43\% on the \textsf{Balanced} dataset.  The other two models exhibit different behaviors: in the same settings as above, the \textbf{A-rate} is 27.54\% vs 21.96\% for Gemini and 59.68\% vs 51.68\% for GMN for pairs from \textsf{Random} and \textsf{Balanced}, respectively.

Figure~\ref{img:balUnbal} and Table~\ref{tab:WBTargeted} show the results in the case of {white-box} attacker. Interestingly, the three target models behave similarly in this scenario, showing better robustness against adversarial samples crafted from the \textsf{Random} dataset rather than from the \textsf{Balanced} one. Actually, the \textbf{A-rate} at $\mT=0.80$ is 31.60\% vs 36.6\% for Gemini, 38.32\% vs 55.89\% for GMN, and 21.76\% vs 27.35\% for SAFE.

\begin{mybox2}{\bf Take away:} Mounting our targeted attacks using a source function similar in length to the target one may or not be helpful depending on the model. The resilience of SAFE considerably drops in our tests, showing that an adversary has an easy time to create an adversarial sample. Conversely, the attacks we mount on pairs of functions with similar initial length are not more effective against Gemini and GMN than those on pairs of functions unbalanced in length.
\end{mybox2}

\section{Mirai Case Study}\label{sec:rq-mirai}
We complement our evaluation with a case study examining our attacks in the context of disguising functions from malware, analogously to exemplary scenario (1) from Section~\ref{sec:introduction}. 

We consider the code base from a famous leak of the Mirai malware, compiling it \texttt{gcc} 9.2.1 with  \texttt{-O0} optimization level on Ubuntu 20.04. After filtering out all functions with less than six instructions, we obtain a set of 63 functions. We build distinct datasets for the targeted and untargeted case. For the former, we pair malicious Mirai functions with benign ones from the \textsf{Random} dataset from the main evaluation, without making any attempt to balance lengths. For the latter, each of the 63 functions is paired with itself.

Figure \ref{img:miraiTar} reports on our targeted attacks, comparing Greedy, Spatial Greedy, and the white-box GCAM for the metrics of \textbf{A-rate}, \textbf{A-sim} and \textbf{N-inc}. For brevity, we focus on the performant \textbf{C4} configuration from the main evaluation.

For the \textbf{A-rate}, when attacking GMN and SAFE, Spatial Greedy has an edge on both Greedy and GCAM, with the latter performing markedly worse than the two black-box ones. With Gemini, Spatial Greedy and Greedy perform similarly, with both resulting below GCAM. This behaviour is consistent with the main evaluation results (cf. Figure~\ref{img:blackWhite_Targeted}).

In more detail, with GMN, the average increase of {A-rate} for Spatial Greedy over Greedy is $3.73$ (max. of $6.27$ at $\mT=0.74$, min. of $2.27$ at $\mT=0.88$).
With SAFE, this increase is $3.81$ (max. of $6.35$ at $\mT=0.74$; min. of zero at $\mT=0.8$). With Gemini, GCAM is the best attack with an average $7.94$ increase over Spatial Greedy (max. of $9.52\%$ at $\mT=0.74$; min. of $6.35\%$ at $\mT=0.88$).
SAFE remains the easiest model to attack also on this dataset.

Regarding \textbf{A-sim} and \textbf{N-inc}, Spatial Greedy and Greedy perform similarly on GMN and SAFE, whereas on Gemini Spatial Greedy is slightly worse than Greedy for \textbf{A-sim} at lower thresholds. The relative performance of GCAM vs. the black-box attacks resembles the trends discussed in the main evaluation (cf. Figure \ref{img:blackWhite_Targeted}).

\begin{figure}[h!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{mirai_BB_WB_targeted.pdf}
\caption{Experiments on the three models subject of \textbf{black-box} and \textbf{white-box} attackers in the \textbf{targeted} scenario, on the \textsf{Mirai} dataset for a different success threshold $\mT \in (0.74,0.88)$ in setting \textbf{C4}. In case of \textbf{black-box} attacker, we test the Spatial Greedy approach against the target models with $\varepsilon = 0.1$, $r = 0.75$, $|\texttt{CAND}| = 400$. In case of \textbf{white-box} attacker, we test GCAM attack with 20k iterations against GEMINI, with 1k iterations against GMN, and with 1k iterations against SAFE.} \label{img:miraiTar}
\end{figure}

Figure~\ref{img:miraiUntar} reports on the experiments we conducted for the untargeted scenario. We note that Spatial Greedy outperforms the other attacks on SAFE (with the exception of GCAM when \U=$0.46$) and performs analogously to them on the other two models. Compared to the main evaluation results, targeted attacks have worse performance than untargeted ones also on this dataset. Moreover, successful targeted attacks continue to require fewer instructions: in particular, across all models, a successful black-box targeted attack needs on average $29.77$ instructions, whereas the untargeted one adds on average $5.27$ instructions.


\begin{figure}[h!]
\centering
\includegraphics[width=3.5in, trim = 0cm 0.6cm 0cm 0cm]{mirai_C4_untargeted.pdf}
\caption{Resilience of the three models to \textbf{black-box} and \textbf{white-box} attackers in the \textbf{untargeted} scenario, on the \textsf{Mirai} dataset for a different success threshold \U $\in[0.46, 0.62]$, considering the setting \textbf{C4}. In case of \textbf{black-box} attacker, we test the Spatial Greedy approach against the target models with $\varepsilon = 0.1$, $r = 0.75$, $|\texttt{CAND}| = 400$. In case of \textbf{white-box} attacker, we test GCAM attack with 20k iterations against GEMINI, with 1k iterations against GMN, and with 1k iterations against SAFE.} 
\label{img:miraiUntar}
\end{figure}



