{
    "arxiv_id": "2303.11143",
    "paper_title": "Adversarial Attacks against Binary Similarity Systems",
    "authors": [
        "Gianluca Capozzi",
        "Daniele Cono D'Elia",
        "Giuseppe Antonio Di Luna",
        "Leonardo Querzoni"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CR",
        "cs.LG"
    ],
    "abstract": "In recent years, binary analysis gained traction as a fundamental approach to inspect software and guarantee its security. Due to the exponential increase of devices running software, much research is now moving towards new autonomous solutions based on deep learning models, as they have been showing state-of-the-art performances in solving binary analysis problems. One of the hot topics in this context is binary similarity, which consists in determining if two functions in assembly code are compiled from the same source code. However, it is unclear how deep learning models for binary similarity behave in an adversarial context. In this paper, we study the resilience of binary similarity models against adversarial examples, showing that they are susceptible to both targeted and untargeted attacks (w.r.t. similarity goals) performed by black-box and white-box attackers. In more detail, we extensively test three current state-of-the-art solutions for binary similarity against two black-box greedy attacks, including a new technique that we call Spatial Greedy, and one white-box attack in which we repurpose a gradient-guided strategy used in attacks to image classifiers.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11143v1"
    ],
    "publication_venue": null
}