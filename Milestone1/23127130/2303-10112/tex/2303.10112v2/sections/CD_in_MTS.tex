
\section{Causal Discovery from Multivariate Time Series}\label{sec:mts}


In this section, we review causal discovery methods for multivariate time-series data, including constraint-based approaches, score-based approaches, functional causal model-based approaches, Granger causality, and others.
The representative algorithms combined with the characteristics are summarized in table \ref{tab:ts_category_overview}.




\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\begin{table}[t]
    \centering
    \caption{Characteristics of causal discovery algorithm reviewed for time-series data, arranged by category.}
    \label{tab:ts_category_overview}
    \tiny %\footnotesize
    \resizebox{\textwidth}{!}{
    % \scalebox{0.95}{
    \begin{tabular}{c|c|cccccccc}
    \toprule
    Section & Method & Causal Graph & Nonlinear & Instantaneous effects & Hidden confounders & Sufficiency Asm. & Markov Asm. & Faithfulness Asm. & Minimality Asm.  \\
    \midrule
    \multirow{9}{*}{Constraint-based}
    & oCSE (2015)~\cite{MTS/CB/oCSE/siamads/0007TB15} &  Summary & Yes & No & No & Yes & Yes & Yes &  \\
    & PCGCE (2022)~\cite{MTS/others/information_criterion/uai/AssaadDG22} &  Extended & Yes & Yes & No & Yes & Yes & Yes & \\
    & PCMCI (2019)~\cite{MTS/CB/PCMCI_runge2019detecting}  &  Window & Yes & No & No  & Yes & Yes & Yes &\\
    & PCMCI$^+$ (2020)~\cite{MTS/CB/insPCMCI_Runge20}  &  Window & Yes & Yes & No & Yes & Yes & Yes &\\
    & ANLTSM (2008)~\cite{MTS/CB/jmlr_ChuG08} &  Window & Yes & Yes & Yes & No & Yes & Yes &\\
    & tsFCI (2010)~\cite{MTS/CB/FCI_tsFCI_entner2010causal} &  Window & Yes & No & Yes & No & Yes & Yes &\\
    & SVAR-FCI (2018)~\cite{MTS/CB_FCI_SVAR_FCI_MalinskyS18} &  Window & No & Yes & Yes & No & Yes & Yes &  \\
    & FCIGCE (2022)~\cite{MTS/others/information_criterion/uai/AssaadDG22} &  Extended & Yes & Yes & Yes & No & Yes & Yes &\\
    & LPCMCI (2020)~\cite{MTS/CB/LPCMCI_GerhardusR20} &  Window & Yes & Yes & Yes & No & Yes & Yes &\\
    \midrule
    \multirow{3}{*}{Score-based}  % 只保留*-NOTEARS与DYNOTEARS
    % & Structural EM(*) (1998)~\cite{MTS/SB/learnDBN/uai/FriedmanMR98} &  * & * & * & * & * & * & * & * \\
    % & CV-BD/CV-BIC(*)\cite{MTS/SB/CV_DBN_prl/PenaBT05} &  * & * & * & * & * & * & * & * \\
    % & JMLR11(*)\cite{MTS/SB/learnDBN/jmlr/CamposJ11} &  * & * & * & * & * & * & * & * \\
    & DYNOTEARS (2020)~\cite{MTS/SB/Dynotears_aistats_PamfilSDPGBA20} &  Window & No & Yes & No & Yes & Yes & No & No\\
    & NTS-NOTEARS (2021)~\cite{MTS/SB/NTS_NOTEARS} &  Window & Yes & Yes & No & Yes & Yes & No & No\\
    & IDYNO (2022)~\cite{MTS/SB/Dynotears_interventionalDATA/icml/GaoBNLY22} &  Window & Yes & Yes & No & Yes & Yes & No & No\\
    \midrule
    \multirow{4}{*}{FCM-Based} 
    &VAR-LiNGAM (2008)~\cite{MTS/FCM/VAR_LINGAM_icml_HyvarinenSH08} &  Window & No & Yes & No & Yes & Yes & No & Yes\\
    &NCDH (2022)~\cite{MTS/FCM/cikm_WuWWLC22} &  Summary & Yes & No & No & Yes & Yes & No & Yes \\
    &TiMINo (2013)~\cite{MTS/FCM/nips_PetersJS13} &  Summary & Yes & Yes & No & Yes & Yes & No & Yes\\
    &NBCB (2021)~\cite{MTS/FCM_maybe/pkdd/AssaadDGA21} &  Summary & Yes & Yes & No & Yes & Yes & Yes\footnotemark[3] & Yes\\
    \midrule
    \multirow{11}{*}{Granger Causality} 
    &HSIC-Lasso-GC (2020)~\cite{MTS/Granger/Kernel_ren2020novel} &  Summary & Yes & No & No & No & No & No & No\\
    &(R)NN-GC (2015,2018)~\cite{MTS/Granger/NN_GC_MontaltoSFTPM15, MTS/Granger/RNN_GC_WangLQLFWP18} &  Summary & Yes & Yes & No & No & No & No & No\\
    &MPIR (2019)~\cite{MTS/Granger/MPIR_ICML19TS_workshop} &  Summary & Yes & No & No  & No & No & No & No\\
    &NGC (2022)~\cite{MTS/Granger/pamiNGC22} &  Summary & Yes & No & No & No & No & No & No \\
    &eSRU (2020)~\cite{MTS/Granger/iclr20_esru} &  Summary & Yes & No & No & No & No & No & No\\
    &SCGL (2019)~\cite{MTS/Granger/SCGL_CIKM_XuHY19} &  Summary & Yes & No & No & No & No & No & No\\
    &GVAR (2021)~\cite{MTS/Granger/iclr21_GVAR_MarcinkevicsV} &  Summary & Yes & Yes & No & No & No & No & No\\
    &TCDF (2019)~\cite{MTS/Attention/TCDF_NautaBS19} &  Window & Yes & Yes & Yes  & No & No & No & No\\
    &CR-VAE (2023)~\cite{MTS/Granger/CR_VAE2023} &  Summary & Yes & Yes & No & No & No & No & No\\ 
    &InGRA (2020)~\cite{MTS/Attention/icdm_InGRA_ChuWMJZY20} &  Summary & Yes & No & No & No & No & No & No\\
    &ACD (2022)~\cite{Discussion/NewForm/ACD_LoweMSW22} &  Summary & Yes & No & Yes  & No & No & No & No\\
    \midrule
    \multirow{4}{*}{Others} 
    &DBCL (2010)~\cite{MTS/others/difference_based/uai/VoortmanDD10} &  Summary & Yes & Yes & Yes & No & Yes & Yes &  \\
    &NGM (2022)~\cite{MTS/Others/NGM_neuralode_iclr_BellotBS22} &  Summary & Yes & Yes & No & No & No & No & No\\
    &CCM (2012)~\cite{MTS/CCM/work2_main_science_sugihara2012detecting} &  Summary & Yes & No & No & No & No & No & No\\
    &PCTL(c) (2009,2011)~\cite{MTS/logic/uai/KleinbergM09,MTS/logic/ijcai/Kleinberg11} &  Summary & Yes & No & No  & No & No & No & No\\
    \bottomrule
    \end{tabular}}
    \vspace{-4ex}

    % \begin{tablenotes} %\tnote{}
    %     \tiny %\footnotesize
    %     \item[1] A lighter version of the faithfulness assumption, termed adjacency faithfulness, is needed.
    %   \end{tablenotes}

\end{table}





% \begin{table}[t]
%     \centering
%     \caption{Characteristics of causal discovery algorithm reviewed for time-series data, arranged by category.}
%     \label{tab:ts_category_overview}
%     \tiny %\footnotesize
%     \scalebox{0.95}{
%     \begin{tabular}{c|c|cccccccc}
%     \toprule
%     Section & Method & Causal Graph & Nonlinear & Instantaneous effects & Hidden confounders  \\
%     \midrule
%     \multirow{9}{*}{Constraint-based}
%     & oCSE (2015)~\cite{MTS/CB/oCSE/siamads/0007TB15} &  Summary & Yes & No & No \\
%     & PCGCE (2022)~\cite{MTS/others/information_criterion/uai/AssaadDG22} &  Extended & Yes & Yes & No \\
%     & PCMCI (2019)~\cite{MTS/CB/PCMCI_runge2019detecting}  &  Window & Yes & No & No \\
%     & PCMCI$^+$ (2020)~\cite{MTS/CB/insPCMCI_Runge20}  &  Window & Yes & Yes & No \\
%     & ANLTSM (2008)~\cite{MTS/CB/jmlr_ChuG08} &  Window & Yes & Yes & Yes\\
%     & tsFCI (2010)~\cite{MTS/CB/FCI_tsFCI_entner2010causal} &  Window & Yes & No & Yes \\
%     & SVAR-FCI (2018)~\cite{MTS/CB_FCI_SVAR_FCI_MalinskyS18} &  Window & No & Yes & Yes  \\
%     & FCIGCE (2022)~\cite{MTS/others/information_criterion/uai/AssaadDG22} &  Extended & Yes & Yes & Yes \\
%     & LPCMCI (2020)~\cite{MTS/CB/LPCMCI_GerhardusR20} &  Window & Yes & Yes & Yes \\
%     \midrule
%     \multirow{3}{*}{Score-based}  % 只保留*-NOTEARS与DYNOTEARS
%     % & Structural EM(*) (1998)~\cite{MTS/SB/learnDBN/uai/FriedmanMR98} &  * & * & * & * & * & * & * & * \\
%     % & CV-BD/CV-BIC(*)\cite{MTS/SB/CV_DBN_prl/PenaBT05} &  * & * & * & * & * & * & * & * \\
%     % & JMLR11(*)\cite{MTS/SB/learnDBN/jmlr/CamposJ11} &  * & * & * & * & * & * & * & * \\
%     & DYNOTEARS (2020)~\cite{MTS/SB/Dynotears_aistats_PamfilSDPGBA20} &  Window & No & Yes & No \\
%     & NTS-NOTEARS (2021)~\cite{MTS/SB/NTS_NOTEARS} &  Window & Yes & Yes & No \\
%     & IDYNO (2022)~\cite{MTS/SB/Dynotears_interventionalDATA/icml/GaoBNLY22} &  Window & Yes & Yes & No \\
%     \midrule
%     \multirow{4}{*}{FCM-Based} 
%     &VAR-LiNGAM (2008)~\cite{MTS/FCM/VAR_LINGAM_icml_HyvarinenSH08} &  Window & No & Yes & No \\
%     &NCDH (2022)~\cite{MTS/FCM/cikm_WuWWLC22} &  Summary & Yes & No & No  \\
%     &TiMINo (2013)~\cite{MTS/FCM/nips_PetersJS13} &  Summary & Yes & Yes & No\\
%     &NBCB (2021)~\cite{MTS/FCM_maybe/pkdd/AssaadDGA21} &  Summary & Yes & Yes & No \\
%     \midrule
%     \multirow{11}{*}{Granger Causality} 
%     &HSIC-Lasso-GC (2020)~\cite{MTS/Granger/Kernel_ren2020novel} &  Summary & Yes & No & No \\
%     &(R)NN-GC (2015,2018)~\cite{MTS/Granger/NN_GC_MontaltoSFTPM15, MTS/Granger/RNN_GC_WangLQLFWP18} &  Summary & Yes & Yes & No \\
%     &MPIR (2019)~\cite{MTS/Granger/MPIR_ICML19TS_workshop} &  Summary & Yes & No & No  \\
%     &NGC (2022)~\cite{MTS/Granger/pamiNGC22} &  Summary & Yes & No & No  \\
%     &eSRU (2020)~\cite{MTS/Granger/iclr20_esru} &  Summary & Yes & No & No\\
%     &SCGL (2019)~\cite{MTS/Granger/SCGL_CIKM_XuHY19} &  Summary & Yes & No & No\\
%     &GVAR (2021)~\cite{MTS/Granger/iclr21_GVAR_MarcinkevicsV} &  Summary & Yes & Yes & No \\
%     &TCDF (2019)~\cite{MTS/Attention/TCDF_NautaBS19} &  Window & Yes & Yes & Yes  \\
%     &CR-VAE (2023)~\cite{MTS/Granger/CR_VAE2023} &  Summary & Yes & Yes & No \\ 
%     &InGRA (2020)~\cite{MTS/Attention/icdm_InGRA_ChuWMJZY20} &  Summary & Yes & No & No \\
%     &ACD (2022)~\cite{Discussion/NewForm/ACD_LoweMSW22} &  Summary & Yes & No & Yes  \\
%     \midrule
%     \multirow{4}{*}{Others} 
%     &DBCL (2010)~\cite{MTS/others/difference_based/uai/VoortmanDD10} &  Summary & Yes & Yes & Yes  \\
%     &NGM (2022)~\cite{MTS/Others/NGM_neuralode_iclr_BellotBS22} &  Summary & Yes & Yes & No\\
%     &CCM (2012)~\cite{MTS/CCM/work2_main_science_sugihara2012detecting} &  Summary & Yes & No & No \\
%     &PCTL(c) (2009,2011)~\cite{MTS/logic/uai/KleinbergM09,MTS/logic/ijcai/Kleinberg11} &  Summary & Yes & No & No  \\
%     \bottomrule
%     \end{tabular}}
%     \vspace{-4ex}

%     % \begin{tablenotes} %\tnote{}
%     %     \tiny %\footnotesize
%     %     \item[1] The ANLTSM method restricts contemporaneous interactions to be linear, and latent confounders to be linear and contemporaneous.
%     %   \end{tablenotes}

% \end{table}




% \begin{table}[t]
%     \centering
%     \caption{Characteristics of causal discovery algorithm reviewed for time-series data, arranged by category.}
%     \label{tab:ts_category_overview}
%     \tiny %\footnotesize
%     \scalebox{0.95}{
%     \begin{tabular}{c|c|cccccccc}
%     \toprule
%     Section & Method & Causal Graph & Nonlinear & Instantaneous effects & Hidden confounders & Nonstationary & Interventional  & (xx char.) \\
%     \midrule
%     \multirow{9}{*}{Constraint-based}
%     & oCSE (2015)~\cite{MTS/CB/oCSE/siamads/0007TB15} &  Summary & Yes & No & No & * & * & * & * \\
%     & PCGCE (2022)~\cite{MTS/others/information_criterion/uai/AssaadDG22} &  Extended & Yes & Yes & No & * & * & * & * \\
%     & PCMCI (2019)~\cite{MTS/CB/PCMCI_runge2019detecting}  &  Window & Yes & No & No & * & * & * & * \\
%     & PCMCI$^+$ (2020)~\cite{MTS/CB/insPCMCI_Runge20}  &  Window & Yes & Yes & No & * & * & * & * \\
%     & ANLTSM (2008)~\cite{MTS/CB/jmlr_ChuG08} &  Window & Yes & Yes & Yes & * & * & * & * \\
%     & tsFCI (2010)~\cite{MTS/CB/FCI_tsFCI_entner2010causal} &  Window & Yes & No & Yes & * & * & * & * \\
%     & SVAR-FCI (2018)~\cite{MTS/CB_FCI_SVAR_FCI_MalinskyS18} &  Window & No & Yes & Yes & * & * & * & * \\
%     & FCIGCE (2022)~\cite{MTS/others/information_criterion/uai/AssaadDG22} &  Extended & Yes & Yes & Yes & * & * & * & * \\
%     & LPCMCI (2020)~\cite{MTS/CB/LPCMCI_GerhardusR20} &  Window & Yes & Yes & Yes & * & * & * & * \\
%     \midrule
%     \multirow{3}{*}{Score-based}  % 只保留*-NOTEARS与DYNOTEARS
%     % & Structural EM(*) (1998)~\cite{MTS/SB/learnDBN/uai/FriedmanMR98} &  * & * & * & * & * & * & * & * \\
%     % & CV-BD/CV-BIC(*)\cite{MTS/SB/CV_DBN_prl/PenaBT05} &  * & * & * & * & * & * & * & * \\
%     % & JMLR11(*)\cite{MTS/SB/learnDBN/jmlr/CamposJ11} &  * & * & * & * & * & * & * & * \\
%     & DYNOTEARS (2020)~\cite{MTS/SB/Dynotears_aistats_PamfilSDPGBA20} &  Window & No & Yes & No & * & No & * & * \\
%     & NTS-NOTEARS (2021)~\cite{MTS/SB/NTS_NOTEARS} &  Window & Yes & Yes & No & * & No & * & * \\
%     & IDYNO (2022)~\cite{MTS/SB/Dynotears_interventionalDATA/icml/GaoBNLY22} &  Window & Yes & Yes & No & * & Yes & * & * \\
%     \midrule
%     \multirow{4}{*}{FCM-Based} 
%     &VAR-LiNGAM (2008)~\cite{MTS/FCM/VAR_LINGAM_icml_HyvarinenSH08} &  Window & No & Yes & No & * & * & * & * \\
%     &NCDH (2022)~\cite{MTS/FCM/cikm_WuWWLC22} &  Summary & Yes & No & No & Yes & * & * & * \\
%     &TiMINo (2013)~\cite{MTS/FCM/nips_PetersJS13} &  Summary & Yes & Yes & No & * & * & * & * \\
%     &NBCB (2021)~\cite{MTS/FCM_maybe/pkdd/AssaadDGA21} &  Summary & Yes & Yes & No & * & * & * & * \\
%     \midrule
%     \multirow{11}{*}{Granger Causality} 
%     &HSIC-Lasso-GC (2020)~\cite{MTS/Granger/Kernel_ren2020novel} &  Summary & Yes & No & No & * & * & * & * \\
%     &(R)NN-GC (2015,2018)~\cite{MTS/Granger/NN_GC_MontaltoSFTPM15, MTS/Granger/RNN_GC_WangLQLFWP18} &  Summary & Yes & Yes & No & * & * & * & * \\
%     &MPIR (2019)~\cite{MTS/Granger/MPIR_ICML19TS_workshop} &  Summary & Yes & No & No & * & * & * & * \\
%     &NGC (2022)~\cite{MTS/Granger/pamiNGC22} &  Summary & Yes & No & No & * & * & * & * \\
%     &eSRU\cite{MTS/Granger/iclr20_esru} &  Summary & Yes & No & No & * & * & * & * \\
%     &SCGL (2019)~\cite{MTS/Granger/SCGL_CIKM_XuHY19} &  Summary & Yes & No & No & * & * & * & * \\
%     &GVAR\cite{MTS/Granger/iclr21_GVAR_MarcinkevicsV} &  Summary & Yes & Yes & No & * & * & * & * \\
%     &TCDF\cite{MTS/Attention/TCDF_NautaBS19} &  Window & Yes & Yes & Yes & * & * & * & * \\
%     &CR-VAE (2023)\cite{MTS/Granger/CR_VAE2023} &  Summary & Yes & Yes & No & * & * & * & * \\ 
%     &InGRA\cite{MTS/Attention/icdm_InGRA_ChuWMJZY20} &  Summary & Yes & No & No & * & * & * & * \\
%     &ACD\cite{Discussion/NewForm/ACD_LoweMSW22} &  Summary & Yes & No & Yes & * & * & * & * \\
%     \midrule
%     \multirow{4}{*}{Others} 
%     &DBCL (2010)~\cite{MTS/others/difference_based/uai/VoortmanDD10} &  Summary & Yes & No & Yes & * & * & * & * \\
%     &NGM (2022)~\cite{MTS/Others/NGM_neuralode_iclr_BellotBS22} &  Summary & Yes & Yes & No & * & * & * & * \\
%     &CCM (2012)~\cite{MTS/CCM/work2_main_science_sugihara2012detecting} &  Summary & Yes & No & No & * & * & * & * \\
%     &PCTL(c) (2009,2011)~\cite{MTS/logic/uai/KleinbergM09,MTS/logic/ijcai/Kleinberg11} &  Summary & Yes & No & No & * & * & * & * \\
%     \bottomrule
%     \end{tabular}}
%     \vspace{-4ex}

%     % \begin{tablenotes} %\tnote{}
%     %     \tiny %\footnotesize
%     %     \item[1] The ANLTSM method restricts contemporaneous interactions to be linear, and latent confounders to be linear and contemporaneous.
%     %   \end{tablenotes}

% \end{table}











\subsection{Constraint-Based Approaches}
\label{subsection:CB} %这里是章节的标签，引用时需要
% 需要一句对CB地位的总体描述：


As a family of causal discovery algorithms, constraint-based approaches rely on statistical tests of conditional independence and are easy to understand and widely used.
We first give the main ideas of constraint-based approaches, including general steps and causal assumptions.
The detailed methodologies will be categorized into approaches with and without causal sufficiency assumption, and be introduced respectively.

The general steps are: Firstly, it builds a skeleton between variables based on conditional independence. Secondly, it orients the skeleton according to the orientation criterion in the rules.
The goal is to construct \textit{Completed Partially Directed Acyclic Graphs} (CPDAGs) representing the MEC of the true causal diagram.
Central to these approaches to derive MEC from observations are the causal assumptions.
These methods are usually under the assumptions of causal Markov property and faithfulness, and some also assume causal sufficiency (no unobserved confounders). 
In this section, we first review the main algorithms and their extensions to time-series data assuming causal sufficiency, then introduce the approaches for conditions when the causal sufficiency assumption is not guaranteed.




\footnotetext[3]{A lighter version of the faithfulness assumption, termed adjacency faithfulness, is needed.}
% \footnotetext[2]{Mainly about causalities related to the Hawkes process. }

\renewcommand{\thefootnote}{\arabic{footnote}}




\subsubsection{Methods with causal sufficiency}
\label{subsection:CB_mwcs} %这里是章节的标签，引用时需要

In this part, we review methods with causal sufficiency.
To reveal the principles of these approaches, we first give a short introduction to methods in the non-temporal setting. 
Then several popular constraint-based approaches, which originate from the approaches for non-temporal data, for time series are reviewed on the basis of two types of extensions (transfer entropy and momentary conditional independence tests). 

As for extracting causal relations from non-temporal data, the Sprites-Glymour-Scheines (SGS) algorithm \cite{MTS/CB/SGS_origin} is one of the first constraint-based approaches, being proved to be consistent under independently, identically distributed (i.i.d) observations assuming causal sufficiency. However, it suffers from exhausting the test of independence between all nodes. The very large search problem makes it unsuitable in practice.
The Peter-Clark (PC) algorithm \cite{MTS/CB/PC_origin}, which also assumes causal sufficiency, is introduced to reduce unnecessary conditional independence tests and search procedures. 
Given $d$ non-temporal variables, the detailed procedure of PC algorithm is defined as follows in 3 steps: 
(1) Firstly, the algorithm starts with a completed undirected graph $G$.
(2) Secondly, the algorithm respectively retrieves whether there exist pairs of variables $i$ and $j$ are conditioned on other $n$ variables when $n=0,1,2,...,d-2$. If satisfied, remove undirected edges between $i$ and $j$, and update the conditioned variables to the separation set. It proceeds to the pruned skeleton.  
(3) Finally, it determines the collider (V-structure) to obtain the CPDAG and determines the remaining undirected edges based on other rules.


Although approaches such as SGS and PC are designed in non-temporal settings, constraint-based approaches for time-series data are usually extended from them. 
We will review recent four popular constraint-based methods, which also assume causal sufficiency, for time-series data. Among these methods, two extensions \cite{MTS/CB/oCSE/siamads/0007TB15, MTS/others/information_criterion/uai/AssaadDG22} are based on the causal concept of Transfer Entropy, another two \cite{MTS/CB/PCMCI_runge2019detecting, MTS/CB/insPCMCI_Runge20} of them are extended to time series via momentary conditional independence tests.


\textbf{Extension to time series based on Transfer Entropy.}
Traditional constraint-based approaches can be extended to the scenario of time series based on the concept of Transfer Entropy. 
The Transfer Entropy is a model-free measure of temporal causality, of which the definition and variants will be detailed in subsection \ref{subsection:TE}. 
Here we view the Transfer Entropy measure as an off-the-shelf part and review two representative approaches from the perspective of constraint-based methodology.


The Optimal Causation Entropy (\textbf{oCSE}) Principle  \cite{MTS/CB/oCSE/siamads/0007TB15} is proposed to guide computational and data-efficient causal discovery algorithms from MTS data.
It's based on the theoretical concept of Causation Entropy, a generalization of Transfer Entropy for measuring pairwise relations to network relations of many variables
The oCSE method takes a procedure slightly different from that in PC: instead of limiting as much as possible the size of its conditioning set, it conditions since the start on all potential causes which constitute the past of all available nodes.
The algorithm is summarized in Algorithm~\ref{alg:oCSE}, which consists of aggregative discovery of causal nodes, and progressive removal of non-causal nodes.
In detail, given node $j$, two procedures are conducted jointly to infer its direct causal neighbors: (1) Firstly, it discovers a superset $Pa(x_j)$ of $j$'s direct causal neighbors aggregately based on the maximization of Causation Entropy. (2) Secondly, it prunes away non-direct causal neighbors based on the Causation Entropy criterion, for example, $i$ is removed from $Pa(x_j)$ if $\mathrm{CE}(x_i^t \to x_j^{t+1} | Pa(x_j^t) \backslash \{x_i^t\} ) =0  $. It's a computational and sample-efficient algorithm.
However, it assumes that the hidden dynamics follow a stationary first-order Markov process as the Causation Entropy only models causal relations with time lags equal to one. 
Recently, the \textbf{PCGCE} \cite{MTS/others/information_criterion/uai/AssaadDG22} is proposed to extract extended summary causal graphs for time-series data based on the PC algorithm and the Greedy Causation Entropy, which is a variant of the Causation Entropy. 

    \begin{algorithm}[t]
        \caption{oCSE}
        \label{alg:oCSE}
        \KwIn{Multivariate time series $\mathbf{x}$ with $d$ dimensions, a significant threshold $\alpha$}
	\KwOut{The summary causal graph $G$}
        \BlankLine
        Initialize an empty graph $G$ with $d$ nodes $V$ \\
        \textbf{for} $j \in \{1,...,d\}$ \textbf{do} \\
        \quad \# Aggregative Discovery of Causal Nodes\\
        \quad $z=\infty$ \\
        \quad \textbf{while} $z>0$ and card($Pa(x_j)$)$<d$ \textbf{do} \\
        \quad\quad \textbf{for} $x_i\in V \backslash Pa(x_j)$ \textbf{do} \\
        \quad\quad\quad Compute the p-value ($z_p$) corresponding to the test  $\mathrm{CE}(x_i^t \to x_j^{t+1} | Pa (x_j^t) ) >0  $ \\
        \quad\quad \textbf{if} $z_p > \alpha$ \textbf{then} add edge $x_i \to x_j$ to $G$ \\
        \quad \# Progressive Removal of Non-Causal Nodes\\
        \quad \textbf{for} $x_i \in Pa(x_j)$ \textbf{do} \\
        \quad\quad Compute $z$ corresponding to the test $\mathrm{CE}(x_i^t \to x_j^{t+1} | Pa(x_j^t) \backslash \{x_i^t\} ) =0  $ \\
        \quad\quad \textbf{if} $z>\alpha$ \textbf{then} remove edge $x_i \to x_j$ from $G$ 
    \end{algorithm}





\textbf{Extension to time series via Momentary Conditional Independence Tests.}
The \textbf{PCMCI} algorithm \cite{MTS/CB/PCMCI_runge2019detecting} leverages a variant of the PC algorithm that flexibly combines linear or nonlinear conditional independence tests and extracts causal relations from time-series data. The goal of the algorithm is to discover the window causal graph.
Different from that of PC algorithm, PCMCI starts by constructing a partially connected graph, where all pairs of nodes $(x^{t-k}_i, x^t_j)$ are directed as $x^{t-k}_i \to x^t_j$ if $k>0$. This initialization also caters to temporal priority. The algorithm consists of two stages:
(1) As done in PC, PCMCI removes all unnecessary edges based on conditional independence. It furthermore removes homologous edges based on the assumption of consistency through time.
(2) Momentary Conditional Independence (MCI) is leveraged to deal with autocorrelation, which may lead to spurious correlation.
Here, MCI is a measurement, which conditions on the parents of $x^t_j$ and $x^{t-k}_i$ while testing $X^{t-k}_i \not \! \perp \!\!\! \perp X^t_j | Pa(X^t_j) \textbackslash \{ X^{t-k}_i\},  X^{t-k}_i$. It also provides an interpretable notion of causal strength from $x^{t-k}_i$ to $x^t_j$.
PCMCI has been shown to be consistent and can be flexibly combined with any kind of conditional independence test (linear or nonlinear), such as partial correlation and mutual information. In recent years, there is also a wealth of machine learning approaches on nonparametric tests that address a wide range of independence and dependence types~\cite{MTS/CB/CI_uai_ZhangPJS11, MTS/CB/CI_aistats_Runge18}. 





The \textbf{PCMCI$^+$} algorithm \cite{MTS/CB/insPCMCI_Runge20} extends PCMCI to include the discovery of instantaneous causal relations.
Central to the PCMCI$^+$ algorithm are two basic ideas that deviate from the origin PC algorithm: First,  it conducts the edge removal process separately for lagged and contemporaneous conditioning sets. Second, it leverages MCI to calibrate CI tests under autocorrelation, which is similar to that in PCMCI.
The author in \cite{MTS/CB/insPCMCI_Runge20} also details the curse and blessing of autocorrelation. 





% 还缺一点总写

\subsubsection{Methods without causal sufficiency}
Constraint-based approaches without causal sufficiency will be reviewed in this part.
In the beginning, we give a brief introduction to the Fast Causal Inference (FCI) Algorithm~\cite{MTS/CB/PC_origin} for non-temporal data.
Then, methods for MTS data consist of two categories: (1) Fast causal inference through time-series models, which is extended from FCI. (2) The methodology via momentary conditional independence tests.




The FCI algorithm is a generalization of the PC algorithm, which can be used in the presence of latent confounders and proven to be asymptotically correct. 
It utilizes independence tests on the observed data to extract (partial) information on ancestral relationships between the observed variables, thus the goal of the FCI algorithm is to infer the appropriate PAG. % For the moment, it suffices to state that the FCI ...
The FCI algorithm starts by constructing a complete graph consisting of undirected edges, similar to the PC algorithm.
Then iterative conditional independence tests are conducted for the removal of edges. As a result, the FCI algorithm removes edges that are independent, first when conditioning with Sepsets and the with Possible-Dsep sets. For the remaining undirected edges, ten orientation rules are applied recursively.
The detailed FCI algorithm, including theoretical analysis, demonstrates the algorithm is sound and complete and can be found in \cite{MTS/CB/FCI_completeness_Zhang08}.

% 对ANLTSM的评价：有linear的部分
\textbf{Fast Causal Inference Through Time-series Models.}
A constraint-based method called additive nonlinear time series model (\textbf{ANLTSM}) \cite{MTS/CB/jmlr_ChuG08} is proposed under the assumption that the effects of hidden confounders are linear and contemporaneous.
To escape the curse of dimensionality for nonparametric conditional independence tests, ANLTSM leverages additive regression model, which can be specified as follows :
\begin{equation}
    x_j^t = \sum_{ 1 \leq i \leq d, i \neq j} a_{j,i}x^t_i + \sum_{1 \leq i \leq d, 1 \leq l \leq \tau} f_{j,i,l}(x^{t-l}_i) + \sum_{r=1}^h b_{j,r}u^t_r + e^t. \nonumber
\end{equation}
Here, $a_{j,i}$ and $b_{j,r}$ are constant values, and $f_{j,i,l}(\cdot)$ denotes the smooth univariate function. The unobserved effects in the form of multi-dimensional Gaussian white noise can be categorized into two types: $ e^t $ reflects the latent direct causes of the observed variables, and $(u^t_r)_{1\leq r \leq h}$ denotes latent common causes. And the latent common causes affect the observed variables at the same instant. For $x^t_i$ and $x^t_j$, $u_r^t$ suffices to be stated as a latent common cause if and only if there exists $1 \leq r \leq h$ such that $b_{j,r}b_{i,r} \neq 0$.
Based on the aforementioned additive regression model, the FCI algorithm is leveraged to identify lagged and instantaneous causal relations. For detecting the instantaneous relations, the conditional independence between $x^t_i$ and $x^t_j$ is first tested given the set $S$ by estimating the conditional expectation $\mathbb{E}(x^t_i | x^t_j \cup  S)$, then the significance of prediction relationship between $x^t_i$ and $x^t_j$ is checked using statistical tests such as the F-test or the BIC scores, where the insignificance of the predictor implies the conditional independence between $x^t_i$ and $x^t_j$. The lagged causal relations are identified in a similar way. The remaining edges are oriented based on rules.
This method is shown to be consistent if the data generation caters to the additive nonlinear time series models.
However, the ANLTSM method restricts contemporaneous interactions to be linear, and latent confounders to be linear and contemporaneous.


Another extension of FCI to time-series data is the \textbf{tsFCI} \cite{MTS/CB/FCI_tsFCI_entner2010causal} algorithm, where the FCI algorithm is directly applied via a time window.
In detail, by assuming the observed time-series data comes from a system at equilibrium, the original time-series data is transformed into a set of samples of the random vector, via a sliding window of size $\tau$. 
Then considering every component of the transformed vector as a separate random variable, the original FCI algorithm is directly applicable. As the amount of information derived from standard FCI is quite restricted, the temporal priority and time invariance is further incorporated as background knowledge to make more inferences in the orientation phase.
However, the tsFCI ignores selection variables and contemporaneous causal relations.
Recently, a constrain-based approach named \textbf{SVAR-FCI} \cite{MTS/CB_FCI_SVAR_FCI_MalinskyS18} is proposed that allows for both instantaneous influences and arbitrary latent confounding in the data-generating process. 
Similar to tsFCI, it also uses time invariance to infer additional edge removals.





\textbf{Methodology Via Momentary Conditional Independence Tests.}
It's found that the original FCI algorithm and its temporal variants suffer from low recall in the autocorrelated time-series case due to the low effect size of conditional independence tests \cite{MTS/CB/LPCMCI_GerhardusR20}.
Some researchers aim to extend PCMCI in the presence of unobserved confounding variables to tackle the aforementioned issues. 
In \cite{MTS/CB/LPCMCI_GerhardusR20}, the Latent PCMCI (\textbf{LPCMCI}) algorithm is proposed.
Central to the LPCMCI algorithm are two ideas that: 
First, based on the analysis of the effect size in causal discovery, it uses parents of variables as default conditions and non-ancestors are not tested in the condioning sets, which not only avoids inflated false positives but also reduce the sets to be tested.
Second, it introduces the notions of middle marks and LPCMCI-PAGs as an unambiguous causal interpretation to facilitate the early orientation of edges.
And the LPCMCI algorithm is proven to be order-independent, sound and complete.









\subsection{Score-Based Approaches}
\label{subsection:SB} %这里是章节的标签，引用时需要

% 先需要有一段总写的话：
Another family of causal discovery approaches is based on score function.
The main ideas of score-based approaches will first be introduced, including (dynamic) Bayesian Network, characteristics of score-based approaches compared to their constraint-based counterpart, model scoring, and model search.
Then, we will review combinatorial search approaches and continuous optimization approaches for MTS, respectively.

\subsubsection{Basics of score-based approaches}

The score-based approaches are motivated by the idea that graph structures encoding the wrong (conditional) independence will also result in poor model fit. In the score-based approaches, the causal structure is attached to the concept of \textit{Bayesian Network (BN)} or \textit{Dynamic Bayesian Network (DBN)} \cite{DBN/dean1989model, DBN/murphy2002dynamic} dealing with temporal data. 
In light of this, the score-based methods can generate and probabilistically score multiple models, and then output the most probable one. This contrasts with the constrained-based approaches, which derive and output a single model without quantification regarding how likely it's to be correct.
And the faithfulness assumption is diluted in the scored-based approaches by applying a goodness-of-fit measurement instead of a conditional independence test.
The problem of learning a BN or DBN from observations can be therefore formulated as: given a set of instances, find the network that best matches them, i.e., optimize the objective functions.  % 思考这里的表示可否把model的score function的形式加进来？ 或者name it ?
It consists of two elements: \textit{model scoring} and \textit{model search}. 


\textbf{Model scoring.} Common objective functions fall under two categories: the Bayesian scores which focus on goodness-of-fit and allow the incorporation of prior knowledge, and information-theoretic scores which explicitly consider model complexity, aiming to avoid over-fitting, in addition to the goodness-of-fit \cite{intro/nonts_surveys/BN21}. The family of Bayesian score functions contains Bayesian Dirichlet equivalent (BDe) score \cite{scorefunctions/BDe/ml/HeckermanGC95}, K2 score \cite{scorefunctions/K2_score/corr/abs-1301-0576}, and so on.
The most widely used information-theoretic scores include the Bayesian Information Criterion (BIC) \cite{scorefunctions/BIC/neath2012bayesian} and the Akaike Information Criterion (AIC) \cite{scorefunctions/BIC_AIC/burnham2004multimodel}.

\textbf{Model search / Optimization.} The score-based approaches cast the problem of searching causal structure  $G$ into an optimization program using the aforementioned score functions $S$. The ultimate goal is therefore stated as \cite{peters2017elements}:  
\begin{equation}
    \hat{G} = \mathrm{argmin}_{G\ \mathrm{over} \ \mathbf{x}} S(D,G), \nonumber
\end{equation}
where $D$ represents the empirical data for variables $\mathbf{x}$.
Traditionally, it's a combinatoric graph-search problem, and the solution is generally sub-optimal as finding a globally optimal network is known to be NP-hard \cite{MTS/SB/NP_COMPLETE/aistats/Chickering95}. A line of works, such as Greedy Equivalence Search (GES) \cite{MTS/SB/GES_start_jmlr_Chickering02} involve local heuristics owing to the large search space of graphs. However, they still suffer from the curse of dimensionality and suboptimal problems. 
Recently, an algebraic result characterizing the acyclicity constraint is leveraged in structure learning, which turns the combinatoric problems into a continuously optimizing problem \cite{MTS/SB/linear_NOTEARS/nips/ZhengARX18, MTS/SB/nonlinear_NOTEARS/aistats/ZhengDARX20}, which can be reformulated as: 
    \begin{equation}
    \begin{aligned}
        \mathrm{min}_{ \mathbf{A} \in  \mathbb{R}^{d \times d}          } &S(\mathbf{A})   \nonumber     \\
    	\mathrm{subject\ to\ } G(\mathbf{A} ) &\in \mathrm{DAGs}     \nonumber \\
    \end{aligned}
    \qquad                 % 此处添加空格
    \begin{aligned}
        \mathrm{min}_{ \mathbf{A} \in  \mathbb{R}^{d \times d}          } &S(\mathbf{A})  \nonumber \\
        \mathrm{subject\ to\ } h(&\mathbf{A}) = 0  \nonumber \\
    \end{aligned}
\end{equation}
where $\mathbf{A}$ denotes the adjacency matrix, and $h$ is the function used to enforce acyclicity in the inferred structure. The original acyclicity constraint function is implemented as $h(\mathbf{A}) = \mathrm{tr}(e^{\mathbf{A} \odot  \mathbf{A}   }) - d$ in \cite{MTS/SB/linear_NOTEARS/nips/ZhengARX18}. 
It relies on the augmented Lagrangian method (ALM) \cite{ALM/networks/Yurkiewicz85} to solve the continuous constrained optimization problem.
Various works have further adopted the continuous constrained formulation in neural networks to extract nonlinear causal relations \cite{MTS/SB/nonlinear_NOTEARS/aistats/ZhengDARX20, NOTMTS/GNN/DAG_GNN/icml/YuCGY19, NOTMTS/GNN/DAG_CAN/icassp/GaoSX21}.



In the context of time series, the ultimate goal of score-based approaches is to learn the structure of DBN. A DBN is a probabilistic network where variables are time series, and it can be decomposed into a prior network and a transition network. A prior network provides dependencies between variables in a given time stamp, and a transition network provides dependencies over time. Therefore, a DBN represents contemporaneous and time-delayed effects in the same framework. Based on this extension to time series, we review the score-based methods following a similar paradigm from combinatoric search to continuous constrained optimization.

% \cite{MTS/SB/learnDBN/uai/FriedmanMR98, MTS/SB/CV_DBN_prl/PenaBT05, MTS/SB/learnDBN/jmlr/CamposJ11}
% \cite{MTS/SB/Dynotears_aistats_PamfilSDPGBA20, MTS/SB/NTS_NOTEARS, MTS/Others/SrVARM_www_HsiehSTWH21}.


\subsubsection{Combinatorial search approaches}
% 一段总写的话
To conduct the combinatorial search based on scoring function from MTS data efficiently, researches have developed various approaches including structural expectation-maximization~\cite{MTS/SB/learnDBN/uai/FriedmanMR98}, cross-validation~\cite{MTS/SB/CV_DBN_prl/PenaBT05}, and the decomposition of score functions~\cite{MTS/SB/learnDBN/jmlr/CamposJ11}.

In \cite{MTS/SB/learnDBN/uai/FriedmanMR98}, the author first utilizes Structural Expectation-Maximization (\textbf{Structural EM}) algorithm \cite{SB/structural_EM/icml/Friedman97, SB/structural_EM/uai/Friedman98}, which is originally a standard algorithm for inferring BN, to learn DBN from longitudinal data.
The Structural EM algorithm, combining structural and parametric modification with a single EM process, can be shown to find local optima defined by score functions. 




In \cite{MTS/SB/CV_DBN_prl/PenaBT05}, the $K$-fold \textit{cross-validation (CV)} is leveraged as a computationally feasible scoring criterion for learning DBN. Given the observational data $D$, which is randomly split into $K$ folds $D^1,...,D^K$ of approximately equal size, the CV value of a model $G$ is formulated as $\frac{1}{T} \sum_{k=1}^K \mathrm{log} p(D^k | G, \hat{\theta}^k)$. And a greedy hill-climbing search is used to estimate $E[\mathrm{log}p(D_{T+1} | G, \hat{\theta} )]$.
The procedure starts from the empty graph and updates it gradually by applying the highest scoring single edge additional or removal available. Experiments show that the scoring methods based on cross-validation lead to models generalizing better than those based on BIC of BDe for a wide range of sample sizes. 

Based on the score functions that are decomposable, the paper \cite{MTS/SB/learnDBN/jmlr/CamposJ11} uses structural constraints to cast the problem of structure learning in DBN into a corresponding augmented BN, and presents a branch-and-bound algorithm to guarantee global optimality. The decomposed form of the optimal goal can be formalized as:
\begin{equation}
    (G^{0*}, G'^*) = \mathrm{argmax}_{G^0, G'}  (S_{D_0}(G^0)  + S_{D_{1:T}}(G') ) =( \mathrm{argmax}_{G^0}S_{D_0}(G^0) + \mathrm{argmax}_{G'}  S_{D_{1:T}}(G')    ), \nonumber
\end{equation}
where $G^0$ and $G'$ correspond to the prior network and the transition network respectively.
Structural constraints, as a way to reduce the search space, specify where arcs may or may not be included.
Because of the branch-and-bound properties, the algorithm can be stopped at the best current solution and an upper bound for the global optimum.
The proposed method is shown to be able to handle larger data sets than before, benefiting from the branch-and-bound algorithm and structural constraints.


\subsubsection{Continuous optimization approaches}

% 组合搜索到连续优化的衔接语
Owing to the recent contribution of NOTEARS \cite{MTS/SB/linear_NOTEARS/nips/ZhengARX18}, the score-based learning of DAGs can be reformulated as a continuous constrained optimization problem, which inspires various works \cite{MTS/SB/nonlinear_NOTEARS/aistats/ZhengDARX20, NOTMTS/GNN/DAG_GNN/icml/YuCGY19, NOTMTS/GNN/DAG_CAN/icassp/GaoSX21, NOTEARS/son/sdm/Ng0FLC022, NOTEARS/SON/corr/abs-1911-07420, NOTEARS/SON/iclr/LachapelleBDL20} in structure learning. 
At the heart of this line of the method is an algebraic characterization of acyclicity expressed as a constraint function, which is further leveraged to minimize the least square loss while enforcing acyclicity.
In the context of time series, some works have also adopted this continuous constrained formulation to support structure learning and causal discovery \cite{MTS/SB/Dynotears_aistats_PamfilSDPGBA20, MTS/SB/NTS_NOTEARS, MTS/Others/SrVARM_www_HsiehSTWH21, MTS/SB/Dynotears_interventionalDATA/icml/GaoBNLY22}.





% DYNOTEARS 
\textbf{DYNOTEARS}, introduced in \cite{MTS/SB/Dynotears_aistats_PamfilSDPGBA20}, captures linear relations from time-series data via a continuous optimization approach.
It models the data in the following standard SVAR way:
\begin{equation}
    \mathbf{x}^t = \mathbf{x}^t \mathbf{W} + \mathbf{x}^{t-1} \mathbf{A}^1 + ... + \mathbf{x}^{t-p} \mathbf{A}^p + \mathbf{u}^t , \nonumber
\end{equation}
where $p$ is the order of SVAR model, $\mathbf{u}$ is a vector of centered error variables.
To guarantee the identifiability in SVAR models, the error terms $\mathbf{e}^t$ are assumed either non-Gaussian or standard Gaussian, i.e., $\mathbf{u}^t \sim \mathcal{N}(0, I)$, as the identifiability is proven to hold on the two cases \cite{NOTEARS/optimization/jmlr/HyvarinenZSH10, peters2017elements}.
$\mathbf{W}$ and $\mathbf{A}$ are weighted adjacency matrices, which correspond to intra-slice edges (contemporaneous relationship) and inter-slice edges (time-lagged relationship), respectively. The SEM can further takes the compact form:$\mathbf{X}^t = \mathbf{X}^t \mathbf{W} + \mathbf{X}^{(t-p):(t-1)} \mathbf{A} + \mathbf{U}$.
The procedure of structure learning revolves around minimizing the least-squares loss subject to an acyclicity constraint, which gives the following optimization problem:
\begin{equation}
    \begin{aligned}
        \mathrm{min}_{ \mathbf{W}, \mathbf{A} }\ \ f(\mathbf{W}, \mathbf{A}) \ \ \mathrm{s.t.}& \ \mathbf{W}\ \ \mathrm{is\ \ acyclic},  \nonumber \\
        \mathrm{where}\ \ f(\mathbf{W}, \mathbf{A}) = 
        \frac{1}{2n}|| \mathbf{X}^t - \mathbf{X}^t \mathbf{W} - \mathbf{X}^{(t-p):(t-1)} \mathbf{A}&||_F^2
        + \lambda_{\mathbf W}|| \mathbf{W} ||_1 + \lambda_{\mathbf{A}}||\mathbf{A}||_1.  \nonumber
    \end{aligned}
\end{equation}
To sidestep the key difficulty of solving the optimization problem under the acyclicity constraint, DYNOTEARS follow the work in \cite{MTS/SB/linear_NOTEARS/nips/ZhengARX18}, where the trace exponential function $h(\mathbf{W}) =  \mathrm{tr}(e^{\mathbf{M} \odot  \mathbf{M}   }) - d $ is leveraged as an equivalent formulation of acyclicity. The continuous constrained optimization problem is translated via the augmented Lagrangian method into unconstrained problems of the form:
\begin{equation}
    \mathrm{min}_{\mathbf{W},\mathbf{A}} F(\mathbf{W},\mathbf{A}),\  \mathrm{where} \ 
F(\mathbf{W}, \mathbf{A}) = f(\mathbf{W}, \mathbf{A}) + \frac{\rho}{2} h(\mathbf{W})^2 + \alpha h(\mathbf{W}). \nonumber
\end{equation}
Towards the optimization of the above smooth augmented objective, two solving approaches are presented separately. 
The first approach is to use standard solvers such as L-BFGS-B \cite{NOTEARS/LBFGSB/toms/ZhuBLN97}. An alternative approach is a two-stage procedure similar to those in \cite{NOTEARS/optimization/jmlr/HyvarinenZSH10}, where we can rewrite the equation as $\mathbf{z} = \mathbf{z}\mathbf{W} + \mathbf{U} $ and derive the estimate of $\mathbf{W}$ by using static NOTEARS to the error term $\mathbf{z}$.



% NTS-NOTEARS
\textbf{NTS-NOTEARS} \cite{MTS/SB/NTS_NOTEARS} is a recent advance that adopts the continuous constrained formulation. Compared to DYNOTEARS, which is a linear autoregressive model, NTS-NOTEARS is able to extract both linear and non-linear relations among variables. It achieves this by leveraging 1D convolutional neural networks (CNNs), which exploit a sequential topology in the input data and are thus well-suited neural function approximation models for temporal data. 
$d$ CNNs, each of which the first layer is a 1D convolutional layer with $m$ kernels, are trained jointly where the $j$-th CNN predicts the expectation of targeted variable $x_t^j$ at the specific time $t$ given preceding and contemporaneous input variables.
Each CNN can be viewed as a Markov blanket of the target variable.
The dependence of child variables on their parents in DBN is given as follows:
\begin{equation}
    \mathbb{E}[ x^t_j | Pa(x^t_j) ] = \mathrm{CNN}_j ( \{ \mathbf{x}^{t-k}: 1 \leq k \leq K   \},  \mathbf{x}^{t}_{-j}            )  , \nonumber
\end{equation}
where parents $Pa(x^t_j)$ are derived from the trained CNNs, and $\mathbf{x}^{t}_{-j}$ denotes all variables at time step $t$ except $x_j$. 
In light of NOTEARS-MLP \cite{MTS/SB/nonlinear_NOTEARS/aistats/ZhengDARX20} (a non-linear and NN-based extension of NOTEARS), the dependency strength of an edge in DBN is estimated in the following way:
\begin{equation}
    W^{k}_{ij} = || \phi^k_{i,j} ||_{L}^2\ \mathrm{for} \ k=1,...,K+1 . \nonumber
\end{equation}
In detail, the $ x^{t-k}_i$ belongs to the parent set $Pa(x^t_j)$ on the condition that the estimated dependency strength is larger than threshold weight $ W^{k}_{ij} > W^{k}_{thres}  $.
The optimization procedure follows a similar way as DYNOTEARS. It's also worth noticing that NTS-NOTEARS shows prior knowledge of variable dependencies that can be transformed as additional optimization constraints and incorporated into the L-BFGS-B solver.

To handle both observational and interventional data, an algorithm, called \textbf{IDYNO} \cite{MTS/SB/Dynotears_interventionalDATA/icml/GaoBNLY22}, is proposed recently. 
It first introduces a non-linear objective through neural networks to model complex dynamics, then modifies an objective and general solution approach to handle different distributions on intervention targets.


% NOTEARS的局限性的阐述：boundaries & limitations
We can find that it's a powerful methodology for score-based structure learning to use continuous optimization and avoid the explicit combinatoric traversal of possible causal structures. The past several years have also witnessed numerous applications and extensions of this methodology. However, some boundaries and limitations are further discussed in \cite{Discussion/scale/npl_impairNOTEARS_KaiserS22, Discussion/scale/bewareof_nips_ReisachSW21, MTS/SB/aistats_NgLKL022}, including the influence of data scale and the convergence condition of the augmented Lagrangian method. 
We recommend you take these issues into consideration for further developments and applications of this family of methods.








\subsection{FCM-Based Approaches}  % Algorithm based on Functional causal model
\label{subsection:FCM} %这里是章节的标签，引用时需要

The two families of methods above either face the inseparability of the MEC or the need for large samples to confirm causal faithfulness.   %ATTENTION: can we write:  'confirm causal faithfulness' ?
Causal discovery can also be conducted based on Functional Causal Models (FCM) \cite{General/pearl2000models}, which is also known as SCM in \ref{subsection:key_concepts} and describes a causal system via a set of equations. 
Recent years have witnessed the proliferation of FCM-based approaches for both temporal and non-temporal data.
In this subsection, we first introduce the main ideas of FCM-based approaches, including the functional causal model and the usage of noise in orienting causal relations.
Then two families of FCM-based approaches, \textit{i.e.,} methods using independent component analysis and additive noise model, will be reviewed, respectively.

In FCM, each variable is explained by an equation in terms of its direct causes and some additional noise. For example, the function $x_j = f_j(x_i, u_j) $ explains the causal link $x_i\to x_j$ with some additional noise $u_j$. 
One basic idea of the FCM-based causal discovery approaches is that statistical noise can be a \textit{valuable} source of insight, which caters to recent discoveries \cite{climenhaga2021causal} challenging the orthodoxy that the noise should be treated as a nuisance. To be specific, causal relationships can be identified and estimated with the help of noise.  



\subsubsection{Methods using independent component analysis}
In this part, we first introduce the basic idea of this family of methods by reviewing the original algorithm in non-temporal setting \cite{MTS/FCM/LiNGAM_jmlr_ShimizuHHK06}.
Then, methods for MTS data will be detailed~\cite{MTS/FCM/VAR_LINGAM_icml_HyvarinenSH08, MTS/FCM/VAR_LINGAM_v2_jmlr_HyvarinenZSH10, MTS/FCM/ijcai2013/SchaechtleSB13,MTS/FCM/cikm_WuWWLC22}.

LiNGAM \cite{MTS/FCM/LiNGAM_jmlr_ShimizuHHK06} is a typical FCM-based causal discovery algorithm in non-temporal setting, and has the following assumptions: (1) a linear data generation process, (2) non-Gaussian disturbances, (3) no unobserved confounders. 
In the LiNGAM model, the relations among observations can be formulated as $\mathbf{x} = \mathbf{B}\mathbf{x} + \mathbf{u}$, where $\mathbf{x},\mathbf{B},\mathbf{u}$ respectively denote the vector of variables, the adjacency matrix of the causal graph and the noise vector. The equation can be rewritten as $\mathbf{x}=\mathbf{A}\mathbf{u}$, where $\mathbf{A} = (\mathbf{I}-\mathbf{B})^{-1}$. For the equation, the \textit{independent component analysis (ICA)} method \cite{stone2004independent_ICA} can be used to estimate $\mathbf{A}$, and causal relationships $\mathbf{B}$ can be derived. Along this line, DirectLiNGAM \cite{MTS/FCM/DirectLiNGAM_jmlr_ShimizuISHKWHB11} further leverages the regression model to ensure the original models to converge to the correct solution in a controlled number of steps. Extensions of LiNGAM to time series are as follows.







As a temporal extension of LiNGAM, \textbf{VAR-LiNGAM} \cite{MTS/FCM/VAR_LINGAM_icml_HyvarinenSH08, MTS/FCM/VAR_LINGAM_v2_jmlr_HyvarinenZSH10} estimates the structural autoregressive (SVAR) models by leveraging non-Gaussianity property. SVAR models reflect both instantaneous and time-delayed causal effects and are among the most prevalent tools in empirical economics to analyze dynamic phenomena \cite{MTS/FCM/VAR_LINGAM_econ_explain_moneta2013causal}. In VAR-LiNGAM, a representation of time series is a combination of SVAR and SEM, which is defined as:
\begin{equation}
   \mathbf{x}^t = \sum_{k=0}^\tau \mathbf{B}^k \mathbf{x}^{t-k} + \mathbf{u}^t   \tag{SVAR} 
\end{equation}
where $\mathbf{B}^k$ is the $n\times n$ matrix of the causal effects between the variables $\mathbf{x}$ with time lag $k$. And $\mathbf{u}^t$ are random processes modeling the external influences or `disturbances', which are assumed to be independent, temporally uncorrelated and non-Gaussian.
To estimate the above model, a classic least-squares estimation of the autoregressive (AR) model (time lag $k>0$) is combined, which is formalized as:
\begin{equation}
\mathbf{x}^t = \sum_{k=1}^\tau \mathbf{M}^k \mathbf{x}^{t-k} + \mathbf{n}^t   \tag{VAR}    
\end{equation}
Based on the SVAR and VAR formalization, the basic idea of VAR-LiNGAM is that we can estimate $\mathbf{M}^k$ of VAR model in a classic least-square fashion consistently and efficiently. And we can deduce the estimate of instantaneous causal effect through LiNGAM analysis. As for the time-delayed effect, it can be derived from reparametrization. The ensuing method in detail is defined as follows in four steps:
(1) Firstly, fit the regressions and denote the least-squares estimates of the AR matrices by $\hat{\mathbf{M}}^k$.
(2) Secondly, compute the residuals, i.e., $\hat{\mathbf{n}}^t = \mathbf{x}^t -  \sum_{k=1}^\tau  \hat{\mathbf{M}}^k  \mathbf{x}^{t-k}  $.
(3) Thirdly, perform LiNGAM analysis \cite{MTS/FCM/LiNGAM_jmlr_ShimizuHHK06} based on the equation $\hat{\mathbf{n}}^t = \mathbf{B}^0 \hat{\mathbf{n}}^t + \mathbf{e}^t$ to derive the estimate of instantaneous causal effect $\hat{\mathbf{B}}^0$.
(4) Finally, compute the estimates of the time-delayed causal effect $\hat{\mathbf{B}}^k (k>0)$ as $\hat{\mathbf{B}}^k= (\mathbf{I} - \hat{\mathbf{B}}^0 )\hat{\mathbf{M}}^k$.
The VAR-LiNGAM model degenerates to the LiNGAM model if the order of the autoregressive part is set to zero, i.e., $\tau = 0$. And an intensive application of this approach in empirical economics can be found in \cite{MTS/FCM/VAR_LINGAM_econ_explain_moneta2013causal}.


The VAR-LiNGAM is extended to the identification and estimate of causal models under time-varying situations \cite{MTS/FCM/VAR_LINGAM_extend1_ijcai_HuangZS15}, where Gaussian Process regression is further leveraged to automatically model how the causal model change over time. 
In \cite{MTS/FCM/VAR_LINGAM_extend3_lanne2017identification}, the initial VAR-LiNGAM is generalized to the condition where the inferred graphs can contain cycles. And the proposed model is demonstrated theoretically to be identifiable.
Another algorithm based on LiNGAM, called the Multi-Dimensional Causal Discovery (MCD), is proposed in~\cite{MTS/FCM/ijcai2013/SchaechtleSB13}.
MCD can efficiently discover causal dependencies in multi-dimensional settings, such as time-series data, by integrating data decomposition and projection.



To get rid of constraints of linear \cite{MTS/FCM/VAR_LINGAM_icml_HyvarinenSH08, MTS/FCM/VAR_LINGAM_v2_jmlr_HyvarinenZSH10} or additive assumptions\cite{MTS/FCM/nips_PetersJS13}, an FCM-based algorithm named Nonlinear Causal Discovery via HM-NICA (\textbf{NCDH}) is recently proposed in \cite{MTS/FCM/cikm_WuWWLC22} to extract general nonlinear relations from time series.
At the heart of this algorithm, a nonlinear ICA algorithm is leveraged as a measurement of nonlinear relationships. The observations are assumed to be generated by mutually independent latent components:
\begin{equation}
    \mathbf{x} = \mathbf{f}(\mathbf{S}) \ \mathrm{where} \ \mathbf{f}=(f_1,f_2,...,f_d)^T \ \mathrm{and} \ \mathbf{S}=(S_1,S_2,...,S_d)^T. \nonumber
\end{equation}
Similar to that in linear ICA, $\mathbf{S}$ contains components that are independent of each other, and the goal of nonlinear ICA is to recover $\mathbf{S}$ from $\mathbf{x}$. 
NCDH first leverages the nonlinear ICA combined with HMM \cite{Discussion/Nonstation/maybe/uai/HalvaH20} to separate latent noises. As a remedy for the permutation uncertainty of ICA, a series of independence tests are conducted to determine the corresponding relations between the observed variables and the separated noises. A recursive search algorithm is finally taken to extract the causal relations.


\subsubsection{Methods using additive noise model}

In reality, there're many non-linear causal relationships that violate the assumption of LiNGAM family methods. Despite recent advances (such as NCDH) extracting causal relations in general nonlinear conditions, their usages are restricted. 
Another family of FCM-based approaches is based on the \textit{additive noise model (ANM)} with nonlinear function, which is suitable in more general settings.
In this part, the main ideas of methods using ANM will be given firstly. 
Then we will introduce the detailed methods for MTS data.

It's demonstrated in~\cite{MTS/FCM/ANM/nips/HoyerJMPS08} that the true causal structure can be identified in the ANM with nonlinear functions if the causal minimality condition holds. In ANM, if $x_i\to x_j$, we have $x_j = f(x_i)+u_j$, and the cause $x_i$ and additive noise $u_j$ are independent. 
If the noise $u$ is subject to non-Gaussian distribution and $f(\cdot)$ is a linear function.
In the bivariate case $x_i\to x_j$, we can fit regression models in causal and anti-causal directions, the true orientation can be inferred by testing the independence with residuals. As for the multivariate case, a pairwise strategy can be adopted \cite{DBLP:conf/ANM/P2MSetting_icml_MooijJPS09}. The correctness of this algorithm is discussed in \cite{Peters1314/jmlr/PetersMJS14}.

% To infer the causal direction, we can regress two models, one of $X_i$ on $X_j$ and another of $X_j$ on $X_i$, and test the independence with residuals.

% % , the ANM works similarly to LiNGAM. 



In \cite{MTS/FCM/nips_PetersJS13}, the Time Series Models with Independent Noise (\textbf{TiMINo}) is proposed, which is a causal discovery method for time series based on ANM. 
It inputs time-series data and outputs either a summary time graph or remains undecided, which avoids leading to wrong causal conclusions when the model is mis-specified or the  data is insufficient.
It leverages a similar method as that in non-temporal and multivariate setting \cite{DBLP:conf/ANM/P2MSetting_icml_MooijJPS09}.
In detail, it tries to fit the structural equation models for time series, which can be formulated as follows:
\begin{equation}
    x^t_j = f_j(  Pa( x^{\tau}_j  )^{t - \tau}  , ... , Pa( x^{1}_j  )^{t - 1}, Pa( x^{0}_j  )^{t} ,  u^t_j  ), \nonumber
\end{equation}
where % $PA( x_{j}^t  ) \subseteq  V \textbackslash \{ x_j \} , PA( x_j^k )  \subseteq V $ for all $j \in V$ and $1 \leq k \leq \tau$. And 
error terms $u^t_j$ are jointly independent over variable index $j$ and time index $t$.
There are several options for fitting methods $f$ such as linear models, generalized additive models, and Gaussian process regression models. For inferring causal relations in the additive noise model, independence tests such as cross-correlations and HSIC \cite{DBLP:conf/nips/GrettonFTSSS07} can be leveraged.

There are some drawbacks to those functional causal models, such as VAR-LiNGAM and TiMINo. It's illustrated that those methods are not well scalable across the increase of node numbers \cite{intro/nonts_surveys/glymour2019review}, and those performances are not promising without a large sample size \cite{Discussion/practical_guide/malinsky2018causal}. To overcome those drawbacks, a Noise-Based / Constraint-Based (\textbf{NBCB}) approach is proposed in \cite{MTS/FCM_maybe/pkdd/AssaadDGA21}, where the constraint-based approach is further leveraged based on the original additive noise model for time-series data. %which combines the noise-based model with constraint-based approach for time-series data. 
In detail, the potential causes of each time series are detected by an additive noise model which is similar to that in TiMINo. Unnecessary causal relations are pruned using temporal causal entropy, which is an extension to causation entropy \cite{MTS/CB/oCSE/siamads/0007TB15} measuring the (conditional) dependencies between two-time series.






\subsection{Granger Causality Based Approaches}
\label{subsection:Granger_base} %这里是章节的标签，引用时需要

Granger causality is a popular tool for analyzing time-series data in many real-world applications. 
There exist many causal discovery approaches developed on the basis of Granger causality.
In this subsection, we first introduce definitions of Granger causality.
Before delving into detailed methods, two categories of Granger causality models for MTS (model-free and model-based) will be given and compared. 
Due to the superiority of model-based approaches in more general conditions, the rest of this part will focus on two recent advances in model-based approaches: (1) methods based on kernels (\ref{subsection:kernel_Granger}), and (2) methods based on neural networks (\ref{subsection:NN_Granger}).


% 缺一段介绍结构的总写
% 0. 前面还得再说几句，再进入本subsection的结构
% 1. 基本概念，including (...前面的不知道该怎么说， ...bivariate->multivariate, Model-free, Model-based )
% 2. 具体方法: Methods based on kernel, Methods based on NN



\subsubsection{Basics of Granger causality}

% 这一段都主打形而上
Granger causality analysis, which is first proposed in \cite{granger1969investigating}, is a powerful method that determines cause and effect based on predictability.
A time series $x_i$ Granger-causes $x_j$ if past values of $x_i$ provide unique, statistically significant information about future values of $x_j$.
According to this proposition, $x_i$ is defined to be `causal' for $x_j$ if
\begin{equation}
    \mathrm{var}[x^t_j - \mathcal{P}(x^t_j | \mathcal{H}^{<t})] < \mathrm{var}[x^t_j - \mathcal{P}(x^t_j | \mathcal{H}^{<t} \textbackslash x^{<t}_i)],  \nonumber
\end{equation}
where $\mathcal{P}(x^t_j | \mathcal{H}^{<t})$ denotes the optimal prediction of $x^t_j$ given the history of all relevant information $\mathcal{H}^{<t}$.
Here $\mathcal{H}^{<t} \textbackslash x^{<t}_i$ indicates excluding the information of $x_{<t}^p$ from $\mathcal{H}_{<t}$.
The above definition seems general and does not have specific modeling assumptions, whereas there are also various forms of definition for Granger causality based on different model specifications and statistical tools for better representation power and the convenience of inference, such as autoregression model (in Granger's original paper~\cite{granger1969investigating}) and so on.
And if all relevant variables are observed and no instantaneous connections exist, Granger causal relations are equivalent to causal relations in underlying DAGs \cite{MTS/FCM/nips_PetersJS13, peters2017elements}.


\subsubsection{Early approaches for MTS}
\label{subsection:Granger_early} %这里是章节的标签，引用时需要

% Bivariate到multivariate，几句话过渡
% 两点理由（1.limitation, 2.mts的广泛应用），从双变量引到多变量，并说多变量是目前的研究热点
Earlier methods for identifying Granger causality were limited to bivariate settings. 
Specifically, a well-documented \cite{lutkepohl1982non} issues for Granger causal analysis in bivariate settings is that the causal findings may be misleading without adjusting for all relevant covariates.    % 这句需要一个结果才行
On the one hand, it's necessary to account for more variables to prevent identifying incorrect Granger causal relations~\cite{intro/ts_surveys/AliGranger21}.
On the other hand, MTS widely exist among various fields. 
Inferring Granger causal relations in MTS, which is also termed graphical Granger causality or network Granger causality in some literature, has become a hot research topic.
Various graphical Granger causal analysis models for MTS can be divided into two categories, namely model-free and model-based approaches. 






\textbf{Model-free Methods.} The mainstream of model-free approaches for multivariate Granger causality are based on predictability and need to estimate the conditional probability density functions (CPDFs) \cite{bai2010multivariate}. 
In \cite{diks2016nonlinear}, the estimates of the CPDFs are provided, and the the bivariate Diks-Panchenko nonparametric causality test is extended to the multivariate case.
By introducing conditional variables into the marginal probability density functions, the copula-based Granger causality model \cite{hu2014copula, kim2020copula} can also be extended to multivariate case.
Besides, model-free measures such as transfer entropy and directed information \cite{MTS/Others/information/directedInform/jcns/AmblardM11}, are able to detect nonlinear dependencies. The definitions and some properties of these model-free estimators will be detailed in \ref{subsection:TE}. 
Model-free methods can deal with nonlinear Granger causal relations well.
However, these estimators suffer from high variance and require large amounts of data for reliable estimation, and also suffer from curse of dimensionality when the number of variables grows. 
Thus, in the complex real-word scenarios where is nonlinear and high-dimensional, the utilization of model-free methods to some extend are limited.






\textbf{Model-based Methods.} In contrast to model-free counterparts, model-based methods are computationally efficient and therefore more suitable for inferring Granger causal relations in high-dimensional conditions.   
The model-based inference approach is adopted by the vast majority of Granger causal whereby the measured time series is modeled by a suited parameterized data generative model. And the inferred parameters ultimately reveal the true topology of Granger causality.
Earlier methods along this line are typically using the popular vector autoregressive (VAR) model under the assumption of linear time-series dynamics. For $d$-variate time series $\mathbf{x}$, the VAR model is defined as:
\begin{equation}
    \mathbf{x}^t = \sum_{k=1}^\tau A^k \mathbf{x}^{t-k} +\mathbf{u}^t, \nonumber
\end{equation}
where $A^k$ is a $d \times d$ matrix that specifies how lag $k$ affects the future evolution of the series and $\mathbf{u}^t$ denotes zero mean noises.
In the VAR model, as a straightforward extension form the bivariate case \cite{granger1969investigating}, time series $i$ does not Granger-cause time series $j$ if and only if for all time lag $k$, the component $(j,i)$ of $A^k$ equals zero.
Thus the Granger causal analysis reduces to determine which entries in $A^k$ are zero over all lags.
There are also abundant research works \cite{intro/ts_surveys/maybe_kdd_ArnoldLA07, MTS/Granger/GGM_kdd_LozanoALR09, MTS/Granger/GGM_bioinformatics_shojaie2010discovering, MTS/Granger/LassoGC/jmlr/BasuSM15} reducing the computational complexity via the Lasso penalty and its variants for Granger causal analysis in high-dimensional time series, which are also termed as Lasso Granger causality (\textbf{Lasso-GC}). For these methods, the problem of Granger causal series selection can be  generally formulated as follows based on least square loss:  
\begin{equation}
    \mathrm{min}_{A^1,...,A^{\tau} \in \mathbb{R}^{d \times d}}   \sum_{t=\tau + 1}^T || \mathbf{x}^t - \sum_{k=1}^\tau A^k \mathbf{x}^{t-k} ||_2^2  + \lambda R(\mathbf{A}), \nonumber
\end{equation}
where $R(\cdot)$ is the sparsity-inducing regularizer and has various implementations as shown in table \ref{tab:lasso_sparsity_overview}.
Different penalty terms induce different sparsity patterns in $A^1,...,A^{\tau}$, thus inducing different heuristics and constraints in the Granger causal series selection.
Except for Lasso-GC, another line of works based on VAR models in multivariate setting worth mentioning is the conditional Granger causality index (CGCI) \cite{MTS/Granger/geweke1982measurement}.
For variable $X,Y$ and conditional variables $Z$, by comparing residuals errors of the reduced and full models $\mathrm{CGCI}_{X\to Y|Z} = \mathrm{ln} \frac{ \mathrm{var}(\epsilon_{Y|Z} ) }{ \mathrm{var}  (\epsilon_{Y|XZ}) }$, a distinction between direct and indirect causality in multivariate systems can be made based on CGCI.
Along this line of works, mBTS-CGCI is proposed in \cite{MTS/Granger/mBTS_CGCI/tsp/SiggiridouK16} based on a modified backward-in-time selection (mBTS) to limit the order of VAR models, thus can be better applied to high-dimensional scenarios.


\begin{table}[t]
    \centering
    \caption{Common sparsity-inducing penalty terms, described by \cite{Lasso_penalty_review/nicholson2017varx, MTS/Granger/iclr21_GVAR_MarcinkevicsV}}
    \label{tab:lasso_sparsity_overview}
    \scalebox{0.95}{
    \begin{tabular}{c|c}
    \toprule
    Model Structure & Penalty Function \\ 
    \midrule
    Basic Lasso &  $|| \mathbf{A} ||_1$    \\
    Elastic net &  $\alpha || \mathbf{A} ||_1   + (1 - \alpha) || \mathbf{A} ||_2^2, \alpha \in (0,1)  $  \\
    Lag group Lasso &  $\sum_{k=1}^{\tau} || \mathbf{A}^k  ||_F   $  \\
    Component-wise Lasso & $ \sum_{p=1}^d \sum_{k=1}^{\tau} || {(\mathbf{A}^{k:\tau}     )}_p    ||_2       $ \\
    Element-wise Lasso &  $\sum_{p=1}^d \sum_{q=1}^d \sum_{k=1}^{\tau}  ||  {(\mathbf{A}^{k:\tau}     )}_{p,q}    ||_2    $\\
    Lag-weighted Lasso & $ \sum_{k=1}^{\tau} k^{\alpha} || \mathbf{A}^k ||_1 , \alpha \in (0,1) $ \\
    \bottomrule
    \end{tabular}}
    \vspace{-4ex}
\end{table}



Although the model-based approaches, compared to the model-free counterparts, take advantage of efficiently processing high-dimensional time series,
the fundamental issue of these approaches is the model misspecification. 
Especially, the notion of multivariate Granger causality based on the vanilla VAR model assumes time series follows linear dynamics, whereas many interactions in real-world applications are inherently nonlinear.
Recently, many model-based approaches, which are compatible with nonlinear causal relations, have emerged and can be grouped into two categories: methods based on \textbf{kernel}, and methods based on \textbf{neural networks}.
As the generation on Granger causality, fundamental venation and development orientations have been reviewed above and prospected from classic documents.
In the following part of this subsection, due to their ability to be leveraged in complex real-world scenarios, we will detail the recent advances of model-based methods in nonlinear and high-dimensional settings, especially new perspectives from neural networks.   

\subsubsection{Recent advances based on kernels}
\label{subsection:kernel_Granger}
% 是不是要多加点参考文献，再丰富详细一些
To extract nonlinear causal relations in a model-based approach, establishing a nonlinear parameter model is a common strategy.
A line of works extend Granger causality to kernel methods \cite{MTS/Granger/Kernel_PRE_ancona2004radial,    MTS/Granger/Kernel_PRL_marinazzo2008kernel, MTS/Granger/Kernel_PRE_marinazzo2008kernel, MTS/Granger/Kernel_uai_SindhwaniML13, MTS/Granger/Kernel_ren2020novel}.
In \cite{MTS/Granger/Kernel_PRE_ancona2004radial}, Granger causality is extended to bivariate nonlinear cases by means of radial basis functions.
Furthermore, a Granger causality analysis model is put forward \cite{MTS/Granger/Kernel_PRL_marinazzo2008kernel} based on the theory of \textit{reproducing kernel Hilbert spaces (RKHS)}. The key idea is to embed data into a Hilbert space and search for nonlinear relations in that space. This method is then generalized to the multivariate case in \cite{MTS/Granger/Kernel_PRE_marinazzo2008kernel}.
In \cite{MTS/Granger/Kernel_uai_SindhwaniML13}, a matrix-valued extension of the kernel method is proposed, imposed on a dictionary of vector-valued RKHS. The algorithm is for high-dimensional nonlinear multivariate regression, and can naturally lead to nonlinear generalization of graphical Granger Causality.
Recently, an algorithm based on Hilbert-Schmidt independence criterion Lasso Granger causality (\textbf{HSIC-Lasso-GC}) \cite{MTS/Granger/Kernel_ren2020novel} is proposed.


\subsubsection{Recent advances based on neural networks}
\label{subsection:NN_Granger}

% 写完各部分方法再来写总结归纳
% 先要总写一句其他的话
Neural networks are able to represent nonlinear, complex, and non-additive interactions between variables.
In this part, recent advances of Granger causal methods  based on neural networks will be reviewed, including non-uniform embedding~\cite{MTS/Granger/NN_GC_MontaltoSFTPM15, MTS/Granger/RNN_GC_WangLQLFWP18}, information regularization~\cite{MTS/Granger/MPIR_ICML19TS_workshop}, component-wise neural network modeling~\cite{MTS/Granger/NGC_origin17, MTS/Granger/pamiNGC22, MTS/Granger/iclr20_esru}, low-rank approximation~\cite{MTS/Granger/SCGL_CIKM_XuHY19}, self-explaining networks~\cite{MTS/Granger/iclr21_GVAR_MarcinkevicsV}, attention mechanisms~\cite{MTS/Attention/TCDF_NautaBS19, MTS/Attention/aaai_AME_SchwabMK19}, recurrent variational autoencoders~\cite{MTS/Granger/CR_VAE2023}, and inductive modeling~\cite{MTS/Attention/icdm_InGRA_ChuWMJZY20, Discussion/NewForm/ACD_LoweMSW22}.



\textbf{DL-extensions with Non-uniform Embedding.}
A feature selection procedure termed as a non-uniform embedding (NUE) is proposed in \textbf{NN-GC} \cite{MTS/Granger/NN_GC_MontaltoSFTPM15} to identify the significant Granger causes in the MLP model. 
By greedily adding lagged components of predictor time series as input, an MLP is updated iteratively.
A predictor time series is claimed a significant Granger cause of the target time series if at least one of its lagged components is added when the procedure is terminated.
In \textbf{RNN-GC} \cite{MTS/Granger/RNN_GC_WangLQLFWP18}, the NUE is extended by replacing MLPs with gated RNN models,
However, as this technique requires training and comparing many candidate models, it's costly in high-dimensional settings.

%Name to edit
\textbf{DL-extensions with Information Regularization.} For extracting nonlinear dynamics,
a method with Minimum Predictive Information Regularization (\textbf{MPIR}) \cite{MTS/Granger/MPIR_ICML19TS_workshop} is introduced.
It leverages learnable corruption for predictor variables and minimizes a mutual information-regularized risk, which combines the benefits of the Granger causality paradigm with deep learning models. 
In MPIR, the author states that the naive way to combine neural nets with Granger causality suffers from two major drawbacks: instability and inefficiency.
The solution is to encourage each $\mathbf{x}_i^{t-K:t-1}$ to provide as little information to $x^t_j$ as possible while maintaining good prediction via learned corruption, replacing the naive way which predicts $x^t_j$ with one $\mathbf{x}_i^{t-K:t-1}$ missing at a time. The risk is defined as follows: 
\begin{equation}
   R_{\mathbf{X}, x_j} [f_{\theta}, \mathbf{n}] =  E_{\mathbf{X}^{t-1}, x^t_j, \mathbf{u}}[ ( x^t_j - f_{\theta}(  \Tilde{\mathbf{ X }}^{t-K:t-1}_{ (\mathbf{n}) } )               )^2          ]     + \lambda \cdot    \sum_{p=1}^d I(\Tilde{X}^{t-K:t-1}_{i(n)}; X^{t-K:t-1}_i), \nonumber
\end{equation}
where $\Tilde{\mathbf{ X }}^{t-K:t-1}_{ (\mathbf{n}) } := \mathbf{X}^{t-K:t-1} + \mathbf{n} \odot \mathbf{e}$ (or its element-wise representation,  $\Tilde{X}^{t-K:t-1}_{ i(n) } := {X}^{t-K:t-1}_i + n_i \cdot e_i, i=1,2,...,d$) are the noise-corrupted inputs with learnable noise amplitudes $\mathbf{n}$ and $u_j \sim N(\mathbf{0},\mathbf{I})$.
And $W_{pq} = I(\Tilde{X}^{t-K:t-1}_{i(n^*)}; X^{t-K:t-1}_i)$ is the minimum predictive information at the minimization of $R_{\mathbf{X}, x_j} [f_{\theta}, \mathbf{n}]$, which contains causal information and measures the predictive strength of variable $i$ for predicting variable $j$, conditioned on all the other observed variables. 
To be specific, $W_{ij} = 0$ if $ x_i \perp \!\!\! \perp x_j $.
Besides, as it's inefficient to estimate the mutual information term with a large dimension, an upper bound is derived as an alternative optimization goal. 
Instead of training many candidate models and suffering from instability and inefficiency, this framework only requires training $d$ models separately.

%(, which is similar to the approaches of NGC, TCDF and eSRU).





\textbf{DL-extensions with Component-wise NN Modeling.} Another NN-based approach to measure nonlinear Granger causality is component-wise modeling.
A component-wise framework is proposed in \cite{MTS/Granger/NGC_origin17}, which can be viewed as a generalization of the linear VAR model. In detail, the generation procedure of each variable can be written as follows:
\begin{equation}
    \mathbf{x}^t_j := g_j( \mathbf{x}^{1:(t-1)}_1,... , \mathbf{x}^{1:(t-1)}_i, ..., \mathbf{x}^{1:(t-1)}_d ) + u^t_j,\ \mathrm{for}\ 1\leq j \leq d,  \nonumber
\end{equation}
where $g_j(\cdot)$ is a continuous function, based on regularized neural networks implementation, specifying how the past values of $\mathbf{x}$ determine the future values of variable $\mathbf{x}_j$.
In this context, the time series $\mathbf{x}_i$ is Granger non-causal for time series $\mathbf{x}_j$ ($\mathbf{x}_i  \nrightarrow  \mathbf{x}_j$) if and only if $g_j(\cdot)$ is invariant to $\mathbf{x}^{1:(t-1)}_i$, which can be defined as:
\begin{equation}
   g_j( \mathbf{x}^{1:(t-1)}_1,... , \mathbf{x}^{1:(t-1)}_i, ..., \mathbf{x}^{1:(t-1)}_d ) = g_j( \mathbf{x}^{1:(t-1)}_1,... , \mathbf{x}^{1:(t-1)}_{i'}, ..., \mathbf{x}^{1:(t-1)}_d ), \nonumber
\end{equation}
for all $ ( \mathbf{x}^{1:(t-1)}_1,... ,\mathbf{x}^{1:(t-1)}_d )  $ and all $ \mathbf{x}^{1:(t-1)}_i \neq  \mathbf{x}^{1:(t-1)}_{i'}$.
We will introduce two methods~\cite{MTS/Granger/pamiNGC22, MTS/Granger/iclr20_esru} based on this framework, respectively.

Neural Granger Causality (\textbf{NGC}) is proposed in \cite{MTS/Granger/pamiNGC22} to infer nonlinear Granger causality using structured MLP and LSTM with sparse input layer weights, which are termed as component-wise MLP (\textbf{cMLP}) and component-wise LSTM (\textbf{cLSTM}), respectively.
In the cMLP, each nonlinear output $g_j$ is modeled with a separate MLP as to easily disentangle the effects from inputs to outputs.
The input matrix of the first layer provides information for penalized selection of Granger causality. 
To be specific, in the first layer of $g_j(\cdot)$
\begin{equation}
   h_1^t = \sigma(\sum_{k=1}^{\tau}W^{k}_1 \mathbf{x}^{t-k} + b_1), \nonumber
\end{equation}
if the $i$-th column of weight matrix $W^k_1$ contains zeros for all time lag $k$, then time series $i$ does not Granger-cause series $j$. 
Analogously to the VAR type methods, the Granger causal series are selected by the following encoding selection\cite{MTS/Granger/NGC_origin17} procedure:
\begin{equation}
    \mathrm{min}_{\mathbf{W}} \sum_{t=\tau}^T (x^t_j - g_j(x_{(t-1):(t-\tau)})) + \lambda \sum_{i=1}^{d}  R((W_1)_{:i}), \nonumber
\end{equation}
where sparse inducing penalty $R(\cdot)$ is implemented through group lasso penalty, which extracts causal relations without requiring precise lag specification.
As for the cLSTM, it sidesteps the lag selection problem and the Granger causal information can also be easily interpreted in the vanilla LSTM model.
The input matrix, which is slightly different from that in MLP, is defined as $W^1 =( (W^f)^{\top}, (W^{in})^{\top},(W^o)^{\top},(W^c)^{\top} )^{\top}$, controlling how the past time series affect the forgot gates, input gates, output gates, and cell updates. 
Granger-causal series can be selected based on a group lasso penalty across columns of $W^1$.
In the end, to optimize the non-convex optimization objectives in either cMLP or cLSTM, the proximal gradient descent\cite{parikh2014proximal} is used, which leads to the exact zeros in the input matrix. This property in the optimization procedure meets the requirement for interpreting Granger non-causality in the framework.
To infer the network topology of Granger causality, $d$ models need to be trained with each variable as a response.

Another sample-efficient architecture economy-SRU (\textbf{eSRU}) is proposed in~\cite{MTS/Granger/iclr20_esru}. 
It leverages Statistical Recurrent Units (SRUs) \cite{MTS/Granger/icml17_sru} to model the observed time-series data. 
Here SRUs are a special type of RNNs designed for MTS with time-delayed and nonlinear dependencies and therefore also suited for extracting the network topology of nonlinear Granger causal relations. To be specific, it suffers less from the vanishing and exploding gradient issues owing to an ungated architecture and is able to model both short and long-term temporal dependency among multivariate time series by maintaining multi-time scale summary statistics.
Similar to model-based approaches like cLSTM, the measure of Granger causal relationships can be derived from the input-layer weight parameters of the SRUs.
However, due to the common issue of data scarcity in the causal inference problem, the original framework suffers from overfitting.
Additionally, two modifications are implemented as a remedy for overfitting in eSRU.

\textbf{DL-extensions with Low-rank Approximation.}
The scalable causal graph learning (\textbf{SCGL}) framework is proposed in~\cite{MTS/Granger/SCGL_CIKM_XuHY19}.
The authors first deconstruct data nonlinearity into two types (\ie univariate-level and multivariate-level nonlinearity), which are modeled separately.
The key idea of SCGL is that learning the full size of the adjacency matrix $A \in \mathbb{R}^{d \times d}$ would be unscalable when the size of variables $d$ is quite large.
In practice, the relationship of variables is low-rank in hidden space~\cite{MTS/Granger_confounder/SPLN_automatica_ZorziC17, DBLP:journals/automatica/ChiusoP12}. 
Therefore, it's natural to approximate $A$ via a $k$-rank decomposition, where $k<d$.
The low-rank approximation reduces the noise influence in causal discovery and provides interpretability in downstream time series analysis~\cite{Application/SCGL_use_cikm_HuangXYYWX20}.


% temporal and intervariable nonlinearity.






%Name to edit
\textbf{DL-extensions with Self-explaining Networks.} For better interpretability, the generalized vector autoregression (\textbf{GVAR}) model \cite{MTS/Granger/iclr21_GVAR_MarcinkevicsV} is proposed.
It's based on an extension of self-explaining neural networks \cite{MTS/Granger/nips18_SENN_Alvarez-MelisJ}.
The self-explaining neural networks are inherently interpretable models motivated by restricted properties, and follow the form:
\begin{equation}
    f(\mathbf{x}) = g( \theta (\mathbf{x})_1h(\mathbf{x})_1, ..., \theta (\mathbf{x})_kh(\mathbf{x})_k), \nonumber
\end{equation}
where $g(\cdot)$ and $\mathbf{h}(\mathbf{x})$ denote a link function and the interpretable basis concepts, respectively.
Combined with the vector autoregression model, which is often specified in Granger causal inference, the GVAR model is given by
\begin{equation}
   \mathbf{x}^t = \sum_{l=1}^{\tau} \Psi_{\theta_l}(\mathbf{x}^{t-l})\mathbf{x}^{t-l} + \mathbf{u}^t, \nonumber
\end{equation}
where $\Psi_{\theta_l}: \mathbb{R}^d \to \mathbb{R}^{d\times d}$ is a neural network parameterized by $\theta_l$, of which the output is the matrix corresponding to the strength of influence. In detail, the strength of influence $x^{t-l}_i \to x^t_j$ is measured by the component $(j,i)$ of $\Psi_{\theta_l}(\mathbf{x}^{t-l})$.
The loss function consists of three terms: the MSE loss, a sparsity-inducing regularization (can be chosen from \ref{tab:lasso_sparsity_overview}), and the smooth penalty, which is defined as follows: 
\begin{equation}
   \frac{1}{T-\tau} \sum_{t=\tau+1}^T || \mathbf{x}^t - \widehat{\mathbf{x}}^t||_2^2 +  \frac{\lambda}{T-\tau}    \sum_{t=\tau+1}^T R(\Psi_t) + \frac{\gamma}{T-\tau-1} \sum_{t=\tau+1}^{T-1} || \Psi_{t+1} - \Psi_t  ||_2^2 , \nonumber
\end{equation}
here $\{ \mathbf{x}^t \}_{t=1}^T$ is the observed d-variate time series whereas $\widehat{\mathbf{x}}^t$ is the one-step forecast made by the GVAR model.
Now that the interpreting matrices for each time point $t$ can be derived via $\Psi_{\widehat{\theta}_k}( \mathbf{x}^t)$, the signs of Granger causal effects and their variability in time can also be assessed. 
Furthermore, a procedure of GVAR based on the heuristics of time-reversed Granger causality \cite{MTS/Granger/timereversal_GVAR_WinklerPBMH16}, which expects the relationships to be flipped on time-reversed data, is leveraged to improve the stability of the inferred structures.   
Compared to the aforementioned methods, such as cMLP, cLSTM, eSRU and MPIR, another key difference is that these methods require training $d$ neural networks, whereas GVAR requires training $2\tau$ networks.





\textbf{DL-extensions with Attention Mechanisms.} The temporal causal discovery framework (\textbf{TCDF}) is introduced in \cite{MTS/Attention/TCDF_NautaBS19}, which utilizes attention-based dilated CNN. 
This framework consists of $d$ independent attention-based CNNs with the same architecture but different target variable $X_j$.
For each target variable, a neural network is proposed to derive prediction, attention scores and kernel weights. Intuitively, a high attention score on $X_i$ while forecasting $X_j$ indicates the former contains prediction information towards the latter.
A permutation-based procedure is additionally provided for evaluating variable importance and identifying significant causal links.
TCDF can discover self-causation and time delays between cause and effect. Besides, by assuming that the bidirectional causal relations can not be instantaneous, it can also detect the presence of hidden confounders with equal delays.


Besides, an interpretable multi-variable LSTM with mixture attention is proposed in IMV-LSTM \cite{MTS/Attention/IMV_LSTM19_0002LA19, MTS/Attention/IMV_LSTM18_GuoLL18} to extract variable importance knowledge.
And it's widely used as a baseline for causal discovery in multivariate time series.
However, the topic on attention and its interpretation is to some extent still a controversial and inconclusive topic \cite{DBLP:conf/naacl/JainW19, DBLP:conf/emnlp/WiegreffeP19, DBLP:conf/lrec/GrimsleyMB20}.  
Especially in the context of Granger causal explanation, the naive-trained soft attention mechanisms are noted \cite{DBLP:conf/icml/SundararajanTY17,  MTS/Attention/aaai_AME_SchwabMK19, MTS/Attention/icdm_InGRA_ChuWMJZY20} to provide no incentive to yield accurate attributions.
In \cite{MTS/Attention/aaai_AME_SchwabMK19}, Granger causal attention weights are introduced based on the measures named as the mean Granger-causal error.
The decrease in error when adding $i$ can be computed as:$ \Delta \varepsilon_{X, i} =    \varepsilon_{X \textbackslash \{i\}} - \varepsilon_X$, given the auxiliary prediction error $\varepsilon_X, \varepsilon_{X \textbackslash \{i\}}$  with and without any information from the $i$-th variable. Then the Granger-causal attention factor can be computed as:$ \omega_i(X) = \frac{ \Delta \varepsilon_{X, i} }{  \sum_{j=1}^d  \Delta \varepsilon_{X, j} } $.
The attention factor $\omega_p(X)$ is able to capture Granger causality, which is zero if $p$-th time series is Granger noncausal for the target series. 


\textbf{DL-extensions with Recurrent Variational Autoencoders.}
Recently, the causal recurrent variational autoencoder (\textbf{CR-VAE})~\cite{MTS/Granger/CR_VAE2023} is proposed, where a generative model incorporates Granger causal learning into the data generation process.
By preventing encoding future information before decoding, the encoder of CR-VAE obeys the principle of Granger causality
To be specific, given time lag $\tau$, a CR-VAE model can be written as:
\begin{equation}
    \hat{\mathbf{x}}^{t-\tau : t} = D_{\theta}(\mathbf{x}^{t-\tau:t-1}, E_{\psi}(\mathbf{x}^{t-2\tau-1:t-\tau-1})  ) + \epsilon^t,
   \nonumber
\end{equation}
where $E_{\psi}, D_{\theta}$ represent encoder and decoder.
Another distinct to the classical recurrent VAE is that the CR-VAE leverages a multi-head decoder where the $i$-th head is designed for generating $\mathbf{x}_i$.
Besides, an error-compensation module is leveraged to capture instantaneous effects.
The CR-VAE is not only able to extract causal relations, but also conduct the data-generating process in a transparent manner benefiting from the learned causal matrix.



\textbf{DL-extensions with Inductive Modeling.} The problem of methods with inductive modeling is slightly different from the above methods, where MTS data from massive individuals, which entails different causal mechanisms but shares common structures, is collected.
The goal is to train a model on samples with heterogenous structures to discover Granger causal relations from each individual.
Two approaches with inductive modeling are reviewed here.





An inductive Granger causal modeling (\textbf{InGRA}) is proposed in \cite{MTS/Attention/icdm_InGRA_ChuWMJZY20}, combined with Granger causal attention \cite{MTS/Attention/aaai_AME_SchwabMK19} and prototype learning. % 可以跟在这句后面讲一讲背景
As there often exist real-world scenarios where massive multivariate time series data is collected from heterogeneous individuals sharing commonalities.
Instead of training one or a set of models for each individual, InGRA trains a global model for individuals potentially having different Granger causal structures, devoid of sample inefficiency and over-fitting issues.
Firstly, the Granger causal attention mechanism is leveraged to quantify variable-wise contributions toward prediction. As the Granger causal attention is not robust enough to reconstruct Granger causal topology from limited data of a single individual,  
InGRA secondly leverages prototype learning, of which the key idea is to solve problems for new inputs based on similarity to prototypical cases, to detect common causal structures.
As a result, the Granger causal relations and strengths between the $d-1$ exogenous variables and the target variable are inferred. 



A framework termed amortized causal discovery (\textbf{ACD}) is proposed in \cite{Discussion/NewForm/ACD_LoweMSW22}, which aims to train a single model to infer causal relations across samples with different underlying causal graphs but shared dynamics.
It's an encoder-decoder framework, in which the encoder function is defined to infer Granger causal relations of the input sample whereas the decoder function learns to predict the next time-step given the inferred causal relations. 
In the implementation, a graph neural network is applied to the amortized encoder, and ACD models the functions using variational inference, which is based on the widely used neural relational inference (NRI) model \cite{Discussion/NewForm/ACD_NRI_Kipf18}. Besides, to derive a causal interpretation of the inferred edges, the proof is provided in ACD to relate the zero-edge function to Granger causality.
As a result, the causal relations of previous unseen samples can be inferred without refitting the model.







\subsection{Others}
\label{subsection:MTS_others} %这里是章节的标签，引用时需要


The aforementioned four categories of approaches have been the subjects of many endeavors in causal discovery research.
For the sake of completeness, we present five types of methods that are distinct from the above approaches in this subsection, including causality based on information-theoretic statistics, causal models based on differential equations, nonlinear state-space methods, logic-based methods, and hybrid methods.


\subsubsection{Causality based on information-theoretic statistics}
\label{subsection:TE} %这里是章节的标签，引用时需要


Causal relationships in MTS can be measured based on information-theoretic statistics.
As a model-free measure, it's widely used in constraint-based approaches (\ref{subsection:CB_mwcs}) and Granger causal models (\ref{subsection:Granger_early}).
However, its definitions and characteristics have not been detailed.
In this part, we will first introduce Transfer Entropy~\cite{TE_origin/schreiber2000measuring}, which is the original concept of information-theoretic statistics of causality, and then its variants. 



Transfer Entropy \cite{TE_origin/schreiber2000measuring} is a measure of information flow or effective coupling between two processes, regardless of the actual functional relationship. 
Instead of model-based criterion, which shares the problem that the model might be misspecified, as a model-free measure, it can be combined with a variant of specific structure learning methods. 
In detail, the Transfer Entropy from $i$ to $j$ (with time lag) can be expressed as:
\begin{equation}
  \mathrm{TE}(X^t_i \to X^{t+1}_j)  = h(X^{t+1}_j | X^t_j) - h(X^{t+1}_j | X^t_j, X^t_i),  \nonumber
\end{equation}
where $h(\cdot| \cdot)$ denotes the conditional entropy. Here the term $h(X^{t+1}_j | X^t_j)$ measures the uncertainty of $X^{t+1}_j$ given information about $X^t_j$, and $h(X^{t+1}_j | X^t_j, X^t_i)$ measures the uncertainty of $X^{t+1}_j$ given information about both $X^t_j$ and $X^t_i$. Therefore, we can understand Transfer Entropy $\mathrm{TE}(X^t_i \to X^{t+1}_j)$ in the causal view of reduction of uncertainty about future dynamics of $X_j$ when the current dynamics of $X_i$ is given addition to that of $X_j$.
For Gaussian variables, the equivalence between Transfer Entropy and Granger causality is demonstrated in \cite{Back/Granger_TE_equivalence_barnett2009granger}.
Furthermore, Transfer Entropy is reformulated into a decomposition form and embedded into the framework of graphical models for multivariate in \cite{MTS/others/information/PRE_runge2012escaping}. 
In \cite{MTS/others/information/PRE_runge2012quantifying}, the causal coupling strength for multivariate time series is quantified based on a variant of transfer entropy. 



Although some utilization in multivariate scenarios, Transfer Entropy suffers from the pairwise limitation. And it's reported to fail to distinguish between direct and indirect causality in networks \cite{MTS/CB/oCSE/siamads/0007TB15} .
As a remedy to pairwise limitation, Causation Entropy \cite{MTS/Others/concepts/causationEntropy/sun2014causation}, a model-free information theoretic statistic for inferring causality, is introduced.
In detail, the Causation Entropy from the set of nodes $I$ to the set of nodes $J$ conditioning on the set of nodes $C$ is defined as follows:
\begin{equation}
   \mathrm{CE}(X^t_{\mathbf{I}} \to X^{t+1}_{\mathbf{J}} | X^t_{\mathbf{C}} )  = h(X^{t+1}_{\mathbf{J}} | X^t_{\mathbf{C}}  ) - h(  X^{t+1}_{\mathbf{J}} | X^t_{\mathbf{C}}, X^t_{\mathbf{I}} ), \nonumber
\end{equation}
here $I,J,C$ are all subset of nodes $\{1,2,...,d\}$.
As a type of conditional mutual information, Causation Entropy is a generalization of Transfer Entropy for measuring pairwise relations to network relations of many variables. And similar to the equivalence relations between Transfer entropy and Granger Causality, Causation Entropy also generalizes Granger Causality and Conditional Granger Causality when applied to Gaussian variables.
However, according to its definition, this concept assumes that the hidden dynamics follow a stationary first-order Markov process as the Causation Entropy only models causal relations with time lags equal to one. 
Recently, to measure any lagged or instantaneous relations, an extension of Causation Entropy, named Greedy Causation Entropy, is proposed in \cite{MTS/others/information_criterion/uai/AssaadDG22}.







\subsubsection{Causal models based on differential equations}




Differential equations are a commonly used modeling tool in many fields, and are especially useful if measurements can be done on the relevant time scale.
Compared to the aforementioned causal models, this type of approach is specifically designed to model systems that can be well represented by differential equations~\cite{intro/ts_surveys/peters2022causal}.  % 似乎是一句正确的无用之话 % 2022 DE, 2022 Jonas Peters
In this part, we will first review the relationships between differential equations and causal models, for both discrete and continuous time.
The first difference-based causal discovery framework will be introduced.
Then, we will give the recent advances in this type of method.

There is abundant literature \cite{intro/ts_surveys/peters2022causal, bongers2018causal} \cite{DBLP:journals/corr/abs-1911-10500, DBLP:conf/uai/MooijJS13,DBLP:conf/uai/RubensteinBMS18} discussing the relationship between differential equations and structural causal models.  % 参考文献再斟酌
For discrete time, a difference-based causal discovery framework is first proposed in \cite{MTS/others/difference_based/uai/VoortmanDD10}.
The cross-temporal restriction is satisfied, where all causation across time is due to a derivative $\dot{x}$ causing a change in its integral $x$. 
This characteristic makes the difference-based causal model a restricted form of dynamic SEMs.
And difference-based causality learner (\textbf{DBCL}) is leveraged to extract difference-based causal models from data, which is proven to be able to identify the presence or absence of feedback loops. 
For continuous time, several theoretical endeavors have also been made to derive a causal interpretation of dynamic systems by both ordinary differential equations (ODEs) \cite{DBLP:conf/uai/MooijJS13, DBLP:conf/uai/RubensteinBMS18, UAI19/DBLP:conf/uai/BlomBM19, MTS/others/difference_based/PNAS/pfister2019learning} and stochastic differential equations (SDEs) \cite{hansen2014causal, MTS/others/difference_based/uai18/MogensenMH18}.  % 假设


More recently, under a dynamic causal system where the multivariate time series are irregularly-sampled (in infinitesimal interval of time), an algorithm called neural graphical model (\textbf{NGM}) is proposed in \cite{MTS/Others/NGM_neuralode_iclr_BellotBS22}.
In many applications, the underlying causal system of interest can be represented as a dynamic structural model as follows:
\begin{equation}
   d \mathbf{x}(t) = \mathbf{f}( \mathbf{x}(t) )dt + d\mathbf{w}(t), \ \ \ \mathbf{x}(0)=\mathbf{x}_0, \ \ \ t\in [0,T],  \nonumber
\end{equation}
where $\mathbf{w}(t)$ is a $d$-dimensional standard Brownian motion, $\mathbf{x}_0$ is a Gaussian random variable independent of $\mathbf{w}(t)$, and the function $\mathbf{f}$ describes the causal graph $G$. 
NGM is a learning algorithm based on penalized Neural Ordinary Differential Equations (neural-ODE).
The recovery of causal graph can be cast to penalized optimization problems of the form: 
\begin{equation}
   \mathrm{min}_{\mathbf{f}_\theta}  \frac{1}{n}\sum_{i=1}^n ||  \mathbf{x}(t_i)  - \hat{\mathbf{x}}(t_i)  ||_2^2,\ \ \mathrm{subject}\ \mathrm{to}\ \rho_{n,T}(\mathbf{f}_{\theta}) \ \mathrm{and} \   \hat{\mathbf{x}}(t) = \mathbf{f}_{\theta}(\hat{\mathbf{x}}(t_i))dt,   \nonumber
\end{equation}
where the observation of the systems are at irregular time points $0 \leq t_1 < ... < t_n \leq  T$.

% (similar to that in section \ref{subsection:SB}) 






\subsubsection{Nonlinear state-space methods}



In this part, we will first introduce the basics of nonlinear state-space methods, including the Takens theorem and Convergent Cross Mapping algorithm.
Then variants and recent advances of the original algorithm will be given, to tackle the challenges such as high sensitivity to noise, large sample demands, inconsistent results, and misidentifications.


The state space reconstruction theory proposed by Takens \cite{takens1981detecting} provides a theoretical basis for analyzing the dynamic characteristics of nonlinear systems. % 下文简述并引出CCM
Based on this theory, another approach for determining causality, known as Convergent Cross Mapping (\textbf{CCM}), was first proposed in \cite{MTS/CCM/work2_main_science_sugihara2012detecting}.
Developed for coupled time series, this method leverages Takens' theorem via state space reconstruction. %下文详述
In detail, given two time series $x_1^t$ and $x_2^t$, the attractor manifolds $\mathcal{M}_{x_1}, \mathcal{M}_{x_2}$ are first reconstructed using $x_1^t$ and $x_2^t$, respectively.
Secondly, causality can be detected by measuring the correspondence between $\mathcal{M}_{x_1}$ and $\mathcal{M}_{x_2}$, to be specific, by testing whether every local neighborhood defined on one manifold is preserved in the other.
Figure \ref{fig:CCM_illustration} gives the illustration of CCM. 
This methodology has been successfully applied in many fields \cite{MTS/CCM/brain_ecology_hirata2016detecting, MTS/CCM/ecology_ye2015distinguishing} where nonlinear systems are dynamically coupled.

However, there exist issues for the original CCM method, such as high sensitivity to observation noise, a requirement for a relatively large number of observations, and inconsistent results under different optimal algorithms. % 暂时没给这些issue加参考文献
To overcome these challenges, variants of CCM based on time-lagged analysis \cite{MTS/CCM/ecology_ye2015distinguishing}, deep Gaussian process \cite{MTS/CCM/feng2019detecting} reservoir computing \cite{MTS/CCM/RCC_huang2020detecting} and neural ODE \cite{MTS/CCM/latent_CCM_iclr_BrouwerASM21} were proposed. 
Besides, most CCM-based approaches have been originally developed for bivariate analysis.
Although the same procedures may be used multiple times to ascertain the causal network among multivariate time series, the performance is not guaranteed under high-dimensional conditions \cite{MTS/CCM/huang2020systematic}.
Misidentifying indirect causations as direct ones performs one of the key challenges in multivariate settings. 
Recently, partial cross mapping (PCM), which combines CCM with partial correlation, was proposed \cite{MTS/CCM/leng2020partial} to eliminate indirect causal influences. 


\begin{figure}
    \centering
	\includegraphics[width=1.0\textwidth]{figs/CCM_illustration.pdf}
	% \vspace{-3ex}
	\caption{The illustration of the cross convergent mapping procedure.}
	% \vspace{-5ex}
	\label{fig:CCM_illustration}
\end{figure}




\subsubsection{Logic-based methods}



Another type of methodology, used for causal inference and causal discovery in time-series data, is based on logic formulas.
The original algorithm of this type of approach will first be introduced and combined with its semeiology and definition of potential causality.
Then we will give its variants and recent advances. 

% A type of methodology based on logic formulas is used for causal inference and causal discovery in time-series data.
In logic-based methods, temporal data can be thought of as observations of the sequence of states the system has occupied and is referred to as traces in model checking.
This line of research originates from work in \cite{MTS/logic/uai/KleinbergM09}, where causal relationships are described in terms of temporal logic formulas.
To be specific, it first leverages logic, Probabilistic Computation Tree Logic (\textbf{PCTL}), to define \textit{prima facie (potential) causality} based on temporal priority and the uplift of conditional probability. 
Given the notation in the original work, the prima facie cause is defined if the following conditions all hold: (1) $F_{>0}^{\leq \infty}c$, (2) $c \rightsquigarrow_{\geq p}^{\geq 1, \leq \infty} e$, and (3) $F_{<p}^{\leq \infty}e$, implying that there may exist any number of transitions between $c$ and $e$ and the sum of a set of path probabilities are at least $p$. % 再加含义的表述
To separate the underlying prima facie (potential) causes into genuine and spurious causes, the notion of $\epsilon$-\textit{insignificant cause} is introduced by computing the average difference in probabilities for each prima facie cause of an effect in relation to all other prima facie causes of the effect:
\begin{equation}
  \epsilon_{avg}(c,e) = \frac{ \sum_{x \in X \textbackslash  c}  \epsilon_x(c,e)  }{|X|},   \nonumber
\end{equation}
where $\epsilon_x(c,e) = P(e|c \wedge x) - P(e| \lnot c \wedge x)$. 
A prima facie cause $c$ is an $\epsilon$-\textit{insignificant cause} of $e$ if $\epsilon_{avg}(c,e) < \epsilon$.
The value of $\epsilon$ is chosen based on empirical null hypothesis testing by assuming: (1) data contains two classes, significant and insignificant, (2) the significant class is relatively small to the insignificant class.  
And false discovery rate control is implemented simultaneously. 
This methodology has also applications in fields \cite{MTS/KleinbergS/kleinberg2013causality}. % 金融那篇与2012 book那篇，主要是neural spike train与金融两个领域
% INVESTIGATING CAUSAL RELATIONSHIPS IN STOCK RETURNS WITH TEMPORAL LOGIC BASED METHODS，金融这篇似乎没见刊，不引用了

To expand the methodology to the condition where both discrete and continuous components exist, PCTLc is introduced in \cite{MTS/logic/ijcai/Kleinberg11} to express temporal and probabilistic properties involving discrete and continuous variables, and the significance of relationship in the continuous case is validated via conditional expectation of an effect instead of conditional probability. 
Besides, a variant \cite{MTS/logic/flairs/HuangK15} of this logic-based approach was proposed to improve the accuracy of causal discovery and enable faster computation of causal significance, by showing the computational complexity can be reduced under several conditions.
Following this line of temporal logic form, a recent work \cite{MTS/logic/jair/CostaD21} combines the idea of decision trees and reconsiders the problem of causal discovery to extract temporal causal sequence relationships from real-time time series.

\subsubsection{Hybrid methods: combining score-based and constraint-based approaches}

Hybrid approaches are proposed for the benefit of combining the strengths of both constraint-based (\ref{subsection:CB}) and score-based (\ref{subsection:SB}) approaches.
We cover two parts of hybrid methods, including methods based on max-min hill-climbing heuristics, and methods incorporating the conditional independence tests to improve the local search.

Some researchers develop hybrid approaches based on max-min hill-climbing heuristics \cite{SB/MMH_DB/ml/TsamardinosBA06,MTS/SB/MMHO_DBN/origin/cibcb/LiN13, MTS/SB/MMHO_DBN/tcbb/LiCZN16}. As hybrid local learning methods, Max-Min approaches fuse concepts from both constraint-based techniques to limit the space of potential structures and search-and-score Bayesian methods to search for an optimal structure. They are originally leveraged in the structure learning of BN for static data \cite{SB/MMH_DB/ml/TsamardinosBA06}. The Max-Min hill-climbing Bayesian network (MMHO-DBN), introduced in \cite{MTS/SB/MMHO_DBN/tcbb/LiCZN16}, learns the structure of DBN based on an extension of the max-min hill-climbing heuristic and is leveraged in the modeling of real gene expression time-series data. 

There are also hybrid approaches that combine conditional-independence tests and local search to improve the criterion score \cite{NONMTS/hybrid/pgm/OgarrioSR16,MTS/CB_FCI_SVAR_FCI_MalinskyS18}. Greedy FCI (GFCI) \cite{NONMTS/hybrid/pgm/OgarrioSR16} is a hybrid score that combines features of GES with FCI. SVAR-GFCI \cite{MTS/CB_FCI_SVAR_FCI_MalinskyS18} extends this method to causal structure learning from time series. In \cite{MTS/SB_or_hybrid/FASK_sanchez2019estimating}, both a variant of the PC-stable algorithm referred to as Fast Adjacency Skewness (FASK), and a hybrid two-step algorithm is proposed for extracting causal relations for time-series data.


