
\section{Discussion and New Perspectives}\label{sec:discuss}
% and Future Directions

In this section, we first discuss challenges and practical considerations, including non-stationarity, heterogeneity, unobserved confounders, subsampling, and expert knowledge.
Then, two new perspectives of temporal causal discovery are provided, which in our opinion will be a promising avenue for future research.

\subsection{Challenges and Practical Considerations}



\textbf{Non-stationarity of data:} We are often faced with non-stationarity in practical scenarios, where the probability distributions of temporal variables conditional on their causes or even the causal relations may change across time, especially for temporal data.
In this condition, causal discovery approaches presuming a fixed causal model may give misleading results. 
Whereas, several types of research have shown that non-stationarity contains information for causal discovery \cite{CD_from_change/conf/uai/TianP01, CD_from_change/peters2016causal, Discussion/Nonstation_hetero/ijcai_ZhangHZGS17, Discussion/Nonstation/state_space_icml_Huang0GG19}.
Thus, it's important to properly tackle the non-stationarity in applications.
Non-stationarity may result from the change of underlying systems and can be seen as a soft intervention \cite{soft_interv/korb2004varieties} done by nature. 
Following this idea, a line of work \cite{Discussion/Nonstation_hetero/ijcai_ZhangHZGS17, Discussion/Nonstation_hetero2/jmlr/Huang0ZRSGS20} leverages a surrogate such as time and domain index to account for nonstationarity where the causal relations are changed, and the CD-NOD framework is proposed. 
Instead of leveraging informative non-stationarity to causal structure learning, another set of research focuses on modeling time-varying relationships \cite{Discussion/Nonstation/pr_GaoY22}. 
Besides, the approach for slowly varying non-stationary process, such as evolutionary spectral and locally stationary processes, is proposed in \cite{Discussion/Nonstation/slowly_varying/du2020causal}.






\textbf{Heterogeneity of data:} In causal discovery for practical applications, the heterogeneity of data lies in two levels: (1) The interacting temporal processes are heterogeneous (having different distributions), for instance, causally related meteorological observations from different stations are influenced by several major weather systems separately \cite{Discussion/heterogeneous/pakdd_BehzadiHP19}. (2) The underlying generating process changes across data sets or different domains \cite{intro/nonts_surveys/glymour2019review}, for instance stock prices from different markets \cite{Discussion/Nonstation_hetero2/jmlr/Huang0ZRSGS20} or individual behaviours in different paradigms \cite{MTS/Attention/icdm_InGRA_ChuWMJZY20}.
For the first condition where the heterogeneity exists among temporal variables, the inferred relations of the traditional causal discovery approaches, which have been designed for specific homogeneous data types, may be inaccurate. As a remedy, several variants of Granger causality, based on methods such as generalized linear models and minimum message length, are proposed in \cite{Applications/anomaly/work2_icdm_BehzadiHP17, Discussion/heterogeneous/pakdd_BehzadiHP19, Discussion/heterogeneous/entropy/Hlavackova-Schindler20}.
For the second condition, a line of work \cite{Discussion/Nonstation_hetero/ijcai_ZhangHZGS17,  Discussion/Nonstation_hetero2/jmlr/Huang0ZRSGS20} leverages the distribution shift from heterogeneity as a soft intervention to assist causal structure learning, which is similar to that in non-stationary data.  
Whereas, another line of causal discovery approaches \cite{MTS/Attention/icdm_InGRA_ChuWMJZY20, Discussion/NewForm/ACD_LoweMSW22} in the second condition focuses on inductively modeling typical structure in heterogeneous data within an end-to-end framework. 



\textbf{Unobserved confounders:}
In practice, we are often met with cases where causal sufficiency is violated, \ie, there exist unobserved confounders. 
This challenging setting may lead to incorrect causal relations~\cite{MTS/FCM/VAR_LINGAM_extend2_icml_GeigerZSGJ15}.
As summarized in Table~\ref{tab:ts_category_overview}, most temporal causal discovery approaches cannot handle unobserved confounders in a straightforward way.
Several constraint-based approaches are designed without causal sufficiency and approaches
Besides, unobserved confounders are modeled by applying a structural bias in~\cite{Discussion/NewForm/ACD_LoweMSW22}.
Several recent studies termed as causal representation learning take a new perspective on unobserved confounders.
It will be detailed in subsection (\ref{subsection:causal_rep}).

\textbf{Subsampling:} In real-world applications, temporal data, especially time series, may be sampled at a rate lower than the rate of the underlying causal process due to the difficulties in data collection.
An ordinary causal discovery algorithm for sub-sampled time series may lead to spurious causal relations and missed ones. 
Several remarks and approaches~\cite{Discussion/subsample/work1, Discussion/subsample/work2_icml_GongZSTG15, Discussion/subsample/work3_nips_rateagnostic_PlisDFC15, Discussion/subsample/uai_subsample_aggr_GongZSGT17, Discussion/subsample/work5_pgm_constraintOPT_HyttinenPJED16, Discussion/subsample/biometrika/tank2019identifiability} are proposed for this issue.

\textbf{Expert knowledge: }Expert knowledge can help the causal discovery process in practice.  % 要强调practical issues.
The approaches of fusing expert knowledge can be categorized into three types~\cite{intro/nonts_surveys/BN21}: (1) \textit{Soft constraints}: the learning process can be influenced by the knowledge~\cite{Discussion/knowledge/ausai/ODonnellNHKAH06}. % (\ie, conditions given with a probability $0<p<1$).
(2) \textit{Hard constraints}: the learnt structure must conform to the enforced requirements (\ie, conditions given with a probability $p=0$ or $p=1$). 
In~\cite{Discussion/knowledge/artmed/AsvatourianLML20}, hard constraints are leveraged in structure learning with a time dependant exposure.
Studies in~\cite{MTS/SB/NTS_NOTEARS} add prior knowledge forbidding the existence of intra-slice dependencies, which is helpful to recover edges that are not explicitly encoded by the prior knowledge.
(3) \textit{Interactive learning}: the human input is leveraged in the learning process~\cite{Discussion/knowledge/ecsqaru/MessaoudLA09, Discussion/knowledge/kdd/MelkasSCMNMP21,https://doi.org/10.48550/arxiv.2206.05420, 9222294}.








\subsection{New perspectives}


\subsubsection{Extension in amortized and supervised paradigms}


In the traditional paradigms, causal discovery methods mostly either treat observational data separately or train a distinct model for each individual. 
These methods do not make full use of the common structure across different samples or supervised information from the datasets whose causal structures are clearly explored, thus suffering from several issues such as the small sample challenge and lack the inductive capability.
Recently, causal discovery is conducted in new paradigms to solve this problem. We can roughly categorize them into two groups: methods based on \textbf{amortized modeling} \cite{MTS/Attention/icdm_InGRA_ChuWMJZY20, Discussion/NewForm/ACD_LoweMSW22}, and methods based on \textbf{supervised learning} \cite{benozzo2017supervised, wang2022meta}.
We introduce them in this subsection, which we believe are a promising avenue for future research. 


In amortized modeling, a global causal discovery framework is trained for individuals with different causal structures. 
As for scenarios with temporal data, these approaches have been detailed in \ref{subsection:NN_Granger} as the deep learning extension of Granger causality with inductive modeling.
InGRA \cite{MTS/Attention/icdm_InGRA_ChuWMJZY20} leverages prototype learning to extract common causal structure while ACD \cite{Discussion/NewForm/ACD_LoweMSW22} proposes an encoder-decoder framework to conduct amortized causal discovery. These methods make full use of information from massive samples and are able to infer causal relations for newly arrived individuals, which are useful in real-world applications such as e-commerce, social network, and neuroimages.

Another line of work has predominately focused on treating the inference process as a black box and learning the mapping from sample data to causal graph structures via supervised learning. Here the label information is causal structure and can be easily accessed in synthetic datasets. 
Earlier work \cite{Discussion/NewForm/RCC/jmlr/Lopez-PazMR15, DBLP:conf/aaai/TonSF21} on learning causal relations by supervised learning is restricted to learning pairwise causal direction where the problem is cast into a classification task to distinguish between $X \to Y$ and $Y \to X$ by using observed samples.
It's later extended to discovery graph structure in \cite{Discussion/NewForm/DAG_EQ/corr/abs-2006-04697,petersen2022causal}.
As the labeled information for training is often originated from synthetic data or real-world datasets which have been explored, the requirement of a supervised approach, in which the distributions of training and test data match or highly overlap, is not guaranteed. In \cite{Discussion/NewForm/ML4S/kdd/00040DJWH022, Discussion/NewForm/CSIvA_DeepMind}, methods such as vicinal graph and meta-learning are leveraged in supervised causal discovery to tackle this `domain shift' issue.  
For the temporal setting, a supervised estimation of Granger causality between time series is proposed in \cite{benozzo2017supervised}. As a recent advance, a method for learning causal discovery is proposed in \cite{wang2022meta} where the learned from large datasets with known causal relations outperform the algorithm in the traditional paradigm when testing on temporal datasets such as fMRI. 
% It's also noted in \cite{wang2022meta} that the causal discovery algorithms in traditional paradigm depart from strong human assumptions about causality. In these approaches (such as constraint-based, score-based and Granger causality), human intuition is implemented in different form. 




% \subsubsection{Extension causal discovery towards causal representation learning (to edit)}
\subsubsection{Extension in causal representation learning}
\label{subsection:causal_rep}
% \subsection{Nonlinear ICA, causal representation learning...}

Extracting the causes of particular phenomena whether explicitly or implicitly from a deep learning black box can be beneficial to the downstream tasks.
The aforementioned causal discovery methods focus on inferring relations between observed variables, or start from the premise that the causal variables are given before hand.
Although some approaches learn causal relations under unobserved variables.
There exist real-world observations (e.g., sensor measurements, image pixels in video) which are not well structured to causal variables to begin with. 
As a generalization of causal discovery from observed variables, there has recently been a growing interest in \textbf{causal representation learning} \cite{CausalRepresentation/nontemp/icml/LocatelloPRSBT20, CausalRepresentation/nontemp/towardsCRL/ScholkopfLBKKGB21, CausalRepresentation/nontemp/CausalVAE/YangLCSHW21}, which aims at learning representation of causal factors in an underlying system.
It estimates latent causal variable graphs from observations.




A line of works in causal representation learning identifies independent factors of variations based on disentanglement and Independent Component Analysis (ICA).
At the heart of this methodology is the postulation of mutually independent latent factors.
It's hard to identify true latent variables, especially in general nonlinear cases.
As a remedy, recent approaches \cite{CausalRepresentation/nontemp/icml/LocatelloPRSBT20, CausalRepresentation/iVAE_nontemp/aistats/KhemakhemKMH20, DBLP:conf/aistats/HyvarinenM17, DBLP:conf/nips/HyvarinenM16} leverage additional information in multiple views, auxiliary variables, or temporal structure, combined with deep learning methods like VAEs and contrastive learning.
A connection between ICA and causality has been recently drawn in \cite{CausalRepresentation/IMA/nontemp/nips/GreseleKSSB21, DBLP:conf/uai/Monti0H19}.
In the context of temporal data, the identifiability of causal variables from temporal sequences is discussed in latent temporal causal process estimation (\textbf{LEAP}) \cite{Discussion/latent/iclr_LEAP_YaoSHS022}. It first provides causal identifiability conditions in a nonparametric, nonstationary setting, and a parametric setting. Then it proposes a learning framework to extract latent causal relations, which extends VAE with a learned causal process network by enforcing the assumed conditions.
The non-stationary noise, modeled by flow-based estimators, can be viewed as a soft intervention to aid identification.
In line with LEAP, subsequent works \cite{TDRL_DBLP:journals/corr/abs-2210-13647} extend the identification theory to a more general case.   % Change to NIPS form citation




Another line of work leverage intervention and data augmentation to help to identify latent causal relations. Under data augmentation, it's demonstrated in \cite{CausalRepresentation/line2/nips/KugelgenSGBSBL21} that common contrastive learning methods can block-identify causal variables that remain unchanged. 
For the temporal setting, \textbf{CITRIS} \cite{CausalRepresentation/CITRIS/icml/LippeMLACG22} is proposed. It's a VAE framework learning causal representation where latent causal factors have possibly been interved on.
By using intervention target information for identification, CITRIS is devoid of suffering from functional or distributional form constraints.
Besides, causal factors in CITRIS are considered as either scalars or potentially multidimensional vectors, which is more practical in complex scenarios. Along this line of work, instantaneous causal relations are extracted in iCITRIS \cite{CausalRepresentation/interv/iCITRIS/abs-2206-06169}.





















