% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
% \usepackage{titling}
\usepackage{spconf,amsmath,graphicx}
\usepackage{amssymb}
\usepackage{amsfonts}
% \usepackage{authblk}
% \usepackage{hyperref}
\usepackage{tabularx}
%\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{graphics}
\usepackage{float}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{indentfirst}
\setlength{\parindent}{2em}



% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}





% Title.
% ------
\title{Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning}
%
% Single address.
% ---------------
%\name{Hengxi Zhang, Zhendong Shi, Yuanquan Hu, Ercan Engin Kuruoğlu, Wenbo %Ding^{*}, XiaoPing~Zhang\thanks{*Corresponding Author.}
%}


\name{
    % \texorpdfstring{\hspace*{0pt}}{}%
    % \resizebox{\textwidth}{!}{
    \raggedright 
        {\fontsize{10}{10}\selectfont
        Hengxi Zhang$^{1\dagger}$\thanks{$^*$Corresponding author (Email: ding.wenbo@sz.tsinghua.edu.cn).}, Zhendong Shi$^{1\dagger}$, Yuanquan Hu$^{1}$, Wenbo Ding$^{1*}$\thanks{$^{\dagger}$Contribute equally.}, Ercan E. Kuruoğlu$^{1}$, Xiao-Ping Zhang$^{1,2}$, IEEE Fellow}
    % }
    % \texorpdfstring{\hspace*{-8pt}}{}%
}

\address{$^{1}${\fontsize{11}{12}\selectfont 
Shenzhen International Graduate School, Tsinghua University, Shenzhen, China}\\
$^{2}${\fontsize{11}{12}\selectfont 
Department of Electrical, Computer and Biomedical Engineering, Ryerson University, Toronto, Canada}
}

% \renewcommand{\thefootnote}{\fnsymbol{footnote}} %将脚注符号设置为fnsymbol类型，即特殊符号表示
% \footnotetext[1]{Contribute equally.} 

\begin{document}
\maketitle


%
\begin{abstract}
Due to the rapid dynamics and a mass of uncertainties in the quantitative markets, the issue of how to take appropriate actions to make profits in stock trading remains a challenging one. Reinforcement learning (RL), as a reward-oriented approach for optimal control, has emerged as a promising method to tackle this strategic decision-making problem in such a complex financial scenario. 
In this paper, we integrated two prior financial trading strategies named constant proportion portfolio insurance (CPPI) and time-invariant portfolio protection (TIPP) into multi-agent deep deterministic policy gradient (MADDPG) and proposed two specifically designed multi-agent RL (MARL) methods: CPPI-MADDPG and TIPP-MADDPG for investigating strategic trading in quantitative markets. 
Afterward, we selected 100 different shares in the real financial market to test these specifically proposed approaches.
The experiment results show that CPPI-MADDPG and TIPP-MADDPG approaches generally outperform the conventional ones.

\end{abstract}

\begin{keywords}
Quantitative trading, multi-agent reinforcement learning, constant proportion portfolio insurance, time-invariant portfolio protection
\end{keywords}
\vspace{-.3cm}


\section{Introduction}
\vspace{-.2cm}
% Quant trading 
Compared with traditional trading methods, quantitative trading is widely known for its features of high-frequency, algorithmic, and automated trading, which is difficult to achieve by human beings in such a complex and dynamic stock market~\cite{thakkar2021comprehensive, daniels2003quantitative}.
In the quantitative market, massive noisy signals from stochastic trading behaviors and all kinds of unforeseeable social events make the prediction of the market state grueling~\cite{guo2017quantitative, moskowitz2012time}. 
And human traders can easily be affected by these events as well as their body and psychological conditions, which would make the irrational decisions of trading nearly inevitable~\cite{pham2009continuous, 
fleming2004application}. 
Therefore, different financial individuals and institutes from different research fields have started to explore more effective ways for handling these problems.

% RL
Over the past years, with the development of artificial intelligence techniques, reinforcement learning (RL) has emerged as an efficient method for making decisions in dynamic environments with uncertainties~\cite{an2022deep}. 
The principle behind RL is the Markov decision process (MDP).
Through interacting with the environment, the RL agent, i.e. the decision maker, will iteratively update its strategy according to the rewards, which can be treated as guidance toward the expected target and the goal of the RL agent is hence to maximize the total reward~\cite{sutton2018reinforcement}. 
Following the MDP, researchers from financial fields have tried to build their own specifically designed RL architecture to cope with different financial problems.
% RL works on finance
A deep RL method combined with knowledge distillation was proposed to improve the training reliability in the trading of currency pairs~\cite{tsantekidis2021diversity}.
To investigate the stock portfolio selection problem, a hypergraph-based RL method was designed to learn the policy function of generating appropriate trading actions~\cite{li2022hypergraph}.
Besides, a policy-based RL framework for stock portfolio management was introduced and its performance was also compared with other trading strategies~\cite{zhang2021deep}.

% MARL/MADDPG
Meanwhile, instead of focusing only on a centralized agent that interacts with the environment, people have begun to find that many scenarios, such as multi-robot control and multi-player game, are more like multi-agent system (MAS), where more than one agent is involved~\cite{zhang2021multi}. 
The framework of multi-agent RL (MARL) is hence established to handle those decision-making and optimal control problems with multiple agents inside a common environment~\cite{busoniu2008comprehensive}. 
Similar to the RL, MARL also concentrates on the issues of sequential decision-making, in which each agent needs to take action with its own strategic brain, which can be naturally designed using neural networks~\cite{mnih2015human}. 
However, the situation becomes more complex since the dynamics of each agent will also be treated as the changes in the environment~\cite{vinyals2019grandmaster}. 
To achieve the management of the portfolio under the continuous changes over the market, a MARL-based system was proposed to maximize the return~\cite{lee2020maps}.
Similarly, an MAS stock market simulator was designed to address the issue of assessment over the market activity and reproduce the market metrics~\cite{lussange2021modelling}.

In this work, we integrate an MARL approach named multi-agent deep deterministic policy gradient (MADDPG) into two prior trading strategies, namely constant proportion portfolio insurance (CPPI) and time-invariant portfolio protection (TIPP) respectively for studying how this novel MAS architecture will behave in quantitative markets. 
The rest of this work is organized as follows. Section II introduces the system model, in which we discuss the MAS model and represent how the MADDPG is specifically established with CPPI and TIPP strategies, respectively. Further, the numerical experiment and results are implemented and analyzed in Section III. Finally, the conclusions are drawn in Section IV.

\vspace{-.2cm}

\section{System model}
In this section, we map the problem of strategic trading in quantitative markets into a MARL task. The principle of MADDPG is introduced first, and then we integrate the MADDPG approach into CPPI and TIPP strategies, respectively.

\vspace{-.1cm}

\subsection{Framework of Multi-Agent Reinforcement Learning}

A sequential decision-making problem in the multi-agent scenario can be described as a stochastic game, which can be defined by a set of key elements $\langle N, \mathbb{S},\{\mathbb{A}^{i}\}_{i\in\{1,\cdots,N\}}$, $P,\{R^i\}_{i\in\{1,\cdots,N\}},\gamma \rangle$, where $N$ is the number of agents. At time $t$, under a shared state $S_t\in\mathbb{S}$, each agent takes its action $a^i\in\mathbb{A}^i$ simultaneously. The joint action $\mathbf{a}=\{a^1,\cdots,a^N\}$ leads the environment changes according to the dynamics $P:\mathbb{S}\times\pmb{\mathbb{A}}\rightarrow \Delta(\mathbb{S})$, where $\pmb{\mathbb{A}}:=\mathbb{A}^1\times\cdots\times\mathbb{A}^N$. After that each agent receives its individual reward $r^i$ according to its reward function $R^i:\mathbb{S}\times\pmb{\mathbb{A}}\times\mathbb{S}\rightarrow \mathbb{R}$. $\gamma$ is the discount factor that represents the value of time.

In our setting, we have $N$ agents trading in quantitative markets using different strategies. Their goal is to maximize their individual return while keeping a certain degree of diversity among the portfolios managed by each agent since we want to allocate risks among all agents. The state space shared by our agents is the raw historical closing prices of all stocks. At each step, our agent will output an action of choosing several stocks with the amount of operation (i.e., buy, sell or hold).
\vspace{-.2cm}

\subsection{Multi-Agent Deep Deterministic Policy Gradient}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{fig/quant_MADDPG_frame_1_2_N.pdf}
\caption{A schematic of MADDPG in the quantitative market environment.}
\label{MADDPG_frame}
\vspace{-.5cm}
\end{figure}

We adopt the MADDPG~\cite{lowe2017multi} to train our agents. This approach is specifically established for the implementation in the scenario of quantitative trading, presented as Fig.~\ref{MADDPG_frame}. MADDPG is a multi-agent version of actor-critic method considering a continuous action set with a deterministic policy, where each agent has an actor network $\pi_\theta$ parameterized by $\theta$ and a critic network $Q_\phi$ parameterized by $\phi$. Each agent learns its optimal policy by updating the parameters of its policy networks to directly maximize the objective function, i.e., the cumulative discounted return,
$J(\theta_i)=\mathbb{E}_{s\sim P,\bm{a}\sim\bm{\pi}_\theta}[\sum_{t\geq 0}\gamma^t R_t^i]$ and the direction to take steps by agent $i$ can be presented as the gradient of the cumulative discounted return, shown as,
\vspace{-.5cm}

\begin{equation}\label{actor loss}
%    \!\nabla_{\theta_i}J(\theta_i)\!=\!\mathbb{E}_{s\!\sim\!\mathcal{D}}\![\nabla_{\theta_i}\!\log \pi_{\theta_i}\!(a_i|s)\!\nabla_{a_i}
%    Q_{\phi_i}(s,\!\bm{a})|_{\bm{a}\!=\!\bm{\pi}_{\theta}(s)}\!],
    \begin{split}
    \nabla_{\theta_i}J(\theta_i)=\mathbb{E}_{s\sim\mathcal{D}}\bigg[\nabla_{\theta_i}\log \pi_{\theta_i}(a_i|s)\cdot\\
    \nabla_{a_i}Q_{\phi_i}(s,a_1,\cdots,a_N)|_{a_i=\pi_{\theta_i}(s)}\bigg],
    \end{split}
\end{equation}
where $\mathcal{D}$ is the experience replay buffer containing tuples $(s,s',a_{1},\cdots,a_{N},r_{1},\cdots,r_{N})$ that are stored throughout training. The centralized critic networks are updated by approximating the true action-value function using temporal-difference learning,
\begin{equation}
\begin{split}
    \mathcal{L}(\phi_i)=\mathbb{E}_{s,\bm{a},\bm{r},s'}\bigg[(Q_{\phi_i}(s,a_{1},\cdots,a_{N})-y)^2\bigg],\\
    y=r_i+\gamma Q_{\phi'_{i}}(s',a'_{1},\cdots,a'_{N})|_{a'_{j}=\pi_{\theta'_{i}}(s)},
\end{split}
\end{equation}
where $Q_{\phi'_{i}}$ and $\pi_{\theta'_{i}}$ are target networks with delayed parameters $\phi'_{i}$ and $\theta'_{i}$ like in deep Q-network method~\cite{mnih2015human}. The purpose of introducing target networks is to ease the moving target problem in deep reinforcement learning and stabilize the off-policy learning procedure.

The core idea of MADDPG is to centralize training while execution in a decentralized manner. The centralized critic networks $Q$ utilize other agents' actions, which means that agents can observe each other's policies while training. When execution, only the actor network is used to generate policy. This technique serves as a cure to the non-stationarity problem in MARL.   

\subsection{Strategies of Constant Proportion Portfolio Insurance and Time-Invariant Portfolio Protection Strategy} % 
CPPI is a type of portfolio insurance in which the investor sets a floor based on their asset, then structures asset allocation around the trading decision~\cite{balder2009effectiveness}. 
As shown in Fig.~\ref{CPPI}, the total asset $A$ is separated into two parts, the protection floor $F$ and the cushion $C$, in which the floor $F$ is the minimum guarantee used for protecting the basis of the total asset and the multiple cushions $k*C$ is supposed to be used as the risky asset $E$,
\vspace{-.2cm}
\begin{equation}
    E = k * C = k * (A - F),
\end{equation}
where the risk factor $k$ indicates the measurement of the risk and a higher value denotes a more aggressive trading strategy. As a comparison, TIPP is a variation of CPPI, where the protection floor $F_t$ at time step $t$ is not a fixed value and changes over time according to some percentage of the total asset $A$ and the previous floor $F_{t-1}$~\cite{estep1988tipp},
\vspace{-.2cm}
\begin{equation}
\begin{split}
    F_t = max\{ \phi A_t, F_{t-1}\},\\
    E_t = k (A_t-F_t),
\end{split}
\end{equation}
where $\phi$ represents the floor percentage. As the total asset $A$ increases, the amount of guarantee will accordingly raise. While the guarantee remains unchanged if the portfolio reduces.
\vspace{-.2cm}

\section{Experiments}

In this section, we describe the overall architecture of our specifically proposed methods. 
Our work starts from the classic MADDPG algorithm. Then we adjust the structure of this method to CPPI-MADDPG and TIPP-MADDPG respectively for investigating how they perform with these prior trading strategies.

\subsection{Dataset and Settings}

In our experiment, 100 stocks listed in Shenzhen Stock Exchange through Tushare using Python had been chosen as risk assets, and the stock codes are from 000010.SZ to 300813.SZ. Together with the cash as the risk-free asset, the investment products to be managed may exponentially increase. The data in the training set is from January.1st 2018 to December.31st 2020 while the data in the testing set is from January.1st 2021 to December.31st 2021. In order to better fit the actual situation of the market, we have imposed restrictions on the data such as non-negative remaining balance and transaction cost. We initialize our cash and aim to get the highest profits with the trading strategies mentioned above.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{fig/CPPI_20221027.pdf}
\caption{The principle of CPPI strategy for agent $i$.}
\label{CPPI}
\end{figure}


\begin{algorithm}[htb!]
\caption{MADDPG with quantitative trading strategy}\label{alg:1}
\begin{algorithmic}[1]
\State Initialize $Q_{\phi_i}$, $\pi_{\theta_i}$, $Q_{\phi'_i}$, $\pi_{\theta'_i}$ for all $i\in\{1,\cdots,N\}$.
\While {training not finished}
    \State Initialize initial state $s$ and a random process $\mathcal{N}$ for\newline
    \hspace*{1.25em} action exploration. 
    \For {each episode}
        \State For each agent select action $a_i=\pi_{\theta_i}(s)+\mathcal{N}_t$,\newline
        \hspace*{2.75em} then adjust $a_i$ using CPPI or TIPP.
        \State Execute joint action $\bm{a}$ and observe reward $\bm{r}$ and \newline
        \hspace*{2.75em} next state $s'$.
        \State Store experience $\langle s,\bm{a},\bm{r},s' \rangle$ in replay buffer $\mathcal{D}$.
        \State Sample a minibatch of $K$ experiences from $\mathcal{D}$.
        \For {each agent}
            \State Update the critic $Q_{\phi_i}$ by minimizing Eq.(\ref{critic loss}).
            \State Update the actor $\pi_{\theta_i}$ using Eq.(\ref{actor loss}).
        \EndFor
        \State Update target networks for each agent:
        \begin{equation*}
            \begin{split}
                \phi'_i\leftarrow \tau \phi_i+(1-\tau)\phi'_i,\\
                \theta'_i\leftarrow \tau \theta_i+(1-\tau)\theta'_i.
                \vspace{-.2cm}
            \end{split}
        \end{equation*}
    \EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}


% \subsection{Insurance Strategy using Deep Q-Network}

% To investigate the randomness of the dynamic stock market, we adopt the following tuple $\langle s, a, r \rangle$ to represent the MDP and implement the Deep Q-Network (DQN) method as the comparison:

% \textbf{State} $s=[p, h, b]$: a vector that includes stock price $p \in \mathbb{R}_{+}$, share $h \in \mathbb{Z}_{+} $, and the remaining balance $b \in \mathbb{R}_{+}$.

% \textbf{Action} $a$: The actions for the stock trading consist of selling, buying, and holding, which lead to the decrease, increase, and no change of the remaining balance $h$, respectively.

% \textbf{Reward} $r(s,a,s')$: The reward for taking action $a$ given state $s$ and the transition to the new state $s'$.

% \textbf{Strategy} $\pi(s, a)$: the strategy of an agent. The basic idea of the policy gradient algorithm is to use a parameterized probability distribution $\pi_{\theta}(a|s) = P(a|s;\theta)$ to represent the policy.

% In the implementation of quantitative trading, the actions for stock only have a fixed amount of selling, buying, and holding. Therefore, in each action, we can implement CPPI or TIPP strategy through the remaining balance $h$ in the state. The DQN algorithm focuses on the maximization of expected reward under initial state distribution. By restricting the action through parameters in CPPI and TIPP strategies, we can change the overall strategy according to the risk preference.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{fig/5 result DQN.jpg}
%     \caption{The quantitative trading strategy of classic DDPG model.}
%     \label{result_DQN}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{fig/6 result CPPI-DQN.jpg}
%     \caption{The quantitative trading strategy of CPPI-MADDPG approach.}
%     \label{result_C_MADDPG}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{fig/7 result TIPP-DQN.jpg}
%     \caption{The quantitative trading strategy of TIPP-MADDPG approach.}
%     \label{result_T_MADDPG}
% \end{figure}

% We can observe that with strategy restrictions the transaction frequency has decreased significantly and performs better than the classic MADDPG since the volatility randomness of a single stock is too strong to predict.

\subsection{Multi-Agent Deep Deterministic Policy Gradient with Insurance Strategy}

% In order to expand the application scenario from buying and selling a single stock to portfolio selection of the entire securities market, we need to extend the single-agent DQN to the MADDPG approach.

To investigate the randomness of the dynamic stock market, we adopt the following tuple $\langle s, a, r, s'\rangle$ to represent the MDP:

\textbf{State} $s=[p, h, b]$: a vector that includes $D$ kinds of stock price $p \in \mathbb{R}_{+}^{D}$, share $h \in \mathbb{Z}_{+}^{D} $, and the remaining balance $b \in \mathbb{R}_{+}^{D}$.

\textbf{Action} $a$: An action set for $K$ agents.

\textbf{Reward} $r(s,a,s')$: The reward for taking action $a$ given state $s$ and the transition to the new state $s'$.

\begin{figure*}[ht]
    \centering
    % \figcapskip=-5pt %设置子图与子标题之间的距离
    \includegraphics[width=.78\linewidth]{fig/theo.png}
    \caption{The asset allocations with MADDPG, CPPI-MADDPG, and TIPP-MADDPG strategies .}
    \vspace{-.5cm}
    \label{fig:theo}
\end{figure*}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{fig/quant_icassp.png}\caption{The performances of portfolios with different strategies (The metrics of Total Asset and Time Step: $10^3$ RMB and Day).}
\label{reward}
\vspace{-.4cm}
\end{figure}

\textbf{Strategy} $\pi(s, a)$: the strategy of an agent. The basic idea of the policy gradient algorithm is to use a parameterized probability distribution $\pi_{\theta}(a|s) = P(a|s;\theta)$ to represent the policy.

% Since there will not exist a trading day or a fixed period of time when only buying and selling fixed shares in the real trading market, we need a continuous action space to represent our trading quota.
% In addition, for the necessity of risk avoidance, it is unwise to put all the remaining balances in a single stock (further speaking, a single strategy, a single agent). What we need is the sum of multiple strategies.

Based on the settings above, we specifically designed a novel loss function according to the CPPI and TIPP strategies. On the one hand, our ultimate target is to maximize the benefits brought by the sum of strategies. On the other hand, we need each agent to consider its own specific situation for ensuring that the increase or decrease of the overall benefits will not have much influence on its own decisions. At the same time, in order to avoid all agents moving towards the same strategy, we need to set the correlation between agents as part of the loss function to achieve the purpose of portfolio selection. The loss function for agent $i$ can be expressed as, 
\vspace{-.4cm}

\begin{equation}\label{critic loss}
\begin{split}
     \mathcal{L}
    (\phi_i)= \lambda \mathbb{E}_{s,\bm{a},\bm{r},s'}\bigg[(Q_{\phi_i}(s,a_1,\cdots,a_N)-y)^2\bigg] &\\+ (1-\lambda) \sum_{i=1,i \leq j}^{K}Corr(a_i,a_j)^2
\end{split}
\end{equation}
where $a_i$ is the action vector showing the positional confidence vector of agent $i$ under the restriction of strategy CPPI or TIPP, and $\lambda$ is the hyperparameter that controls the equilibrium.

To study how each agent has made a series of trading decisions over time in the test phase, we visualize the general trading behavior for each agent on 100 shares with MADDPG, CPPI-MADDPG, and TIPP-MADDPG, respectively. 
As shown in Fig.~\ref{fig:theo}, the thermodynamic diagram presents how the agents with different strategies choose to allocate the asset. The agents with MADDPG prefer the relatively uniform allocation while the assets allocated by those with CPPI-MADDPG and TIPP-MADDPG are more sparse. The changes of the total assets with different trading strategies over time are present in Fig.~\ref{reward}.

\begin{table}[t]
    \normalsize
    \caption{Comparison of Different Strategies}
    \label{tab:data}
    \centering
    \resizebox{0.85\linewidth}{!}{
        \centering
        % \resizebox{\linewidth}{!}{}
        \begin{tabular}{c c c c}
          \toprule
          Strategy & AR & SR & MaxD \\
          \midrule 
          UP & $3.36\%$ & $1.03$ & $3.86\%$\\
          MADQN & $6.47\%$ & $1.71$ & $9.43\%$\\
          MADDPG & $8.22\%$ & $1.99$ & $\mathbf{12.26\%}$\\
          CPPI-MADDPG & $7.76\%$ & $\mathbf{2.18}$ & $6.60\%$\\
          TIPP-MADDPG & $\mathbf{9.68\%}$ & $2.09$ & $9.02\%$\\
          \bottomrule
        \end{tabular}
    }
    \vspace{-.5cm}
\end{table}

The performance of our trading strategies are compared with Universal Portfolios (UP), MADQN and MADDPG through Annual Return (AR), Sharpe Ratio (SR), and Maximum Drawdown (MaxD, namely the maximum portfolio value loss from the peak to the bottom). And the performances of AR, SR, and MaxD are shown in Table~\ref{tab:data}.

The UP is a common portfolio method, which makes optimal decisions through the calculation of the correlation of different stock returns. However, it cannot cope with real-time data and performs poorly in the test set. The problem of MADQN is the lack of exploration ability. There are too many uncertain factors in the stock market for the strategy obtained by single agent. CPPI-MADDPG and TIPP-MADDPG can also degenerate into the classic MADDPG strategy under specific parameters, and they can adjust their parameters according to the investors' individual risk preferences. In the case of profitability, the amount of capital to be guaranteed in TIPP strategy will be higher than that in CPPI strategy, so a more aggressive portfolio strategy will be adopted. That is why TIPP strategy has achieved excellent performance in annual return, while CPPI strategy has better performance in SR and MaxD factors.
\vspace{-.2cm}

\section{Conclusion}
In this paper, we proposed two enhanced MADDPG algorithms, namely CPPI-MADDPG and TIPP-MADDPG, to study how both financial trading strategies will benefit MARL for quantitative trading. Both of the methods are tested using the real financial signals in the Shenzhen Stock Exchange. The results of numerical experiments show the feasibility of our methods, which hopefully provide the potential possibility of deployment in the quantitative markets in the future.


% List and number all bibliographical references at the end of the
% paper. The references can be numbered in alphabetic order or in
% order of appearance in the document. When referring to them in
% the text, type the corresponding reference number in square
% brackets as shown at the end of this sentence \cite{C2}. An
% additional final page (the fifth page, in most cases) is
% allowed, but must contain only references to the prior
% literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings}

\end{document}