{
    "arxiv_id": "2303.08777",
    "paper_title": "Distribution-free Deviation Bounds of Learning via Model Selection with Cross-validation Risk Estimation",
    "authors": [
        "Diego Marcondes",
        "Cl√°udia Peixoto"
    ],
    "submission_date": "2023-03-15",
    "revised_dates": [
        "2023-03-16"
    ],
    "latest_version": 1,
    "categories": [
        "stat.ML",
        "cs.LG"
    ],
    "abstract": "Cross-validation techniques for risk estimation and model selection are widely used in statistics and machine learning. However, the understanding of the theoretical properties of learning via model selection with cross-validation risk estimation is quite low in face of its widespread use. In this context, this paper presents learning via model selection with cross-validation risk estimation as a general systematic learning framework within classical statistical learning theory and establishes distribution-free deviation bounds in terms of VC dimension, giving detailed proofs of the results and considering both bounded and unbounded loss functions. We also deduce conditions under which the deviation bounds of learning via model selection are tighter than that of learning via empirical risk minimization in the whole hypotheses space, supporting the better performance of model selection frameworks observed empirically in some instances.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.08777v1"
    ],
    "publication_venue": "arXiv admin note: text overlap with arXiv:2109.03866"
}