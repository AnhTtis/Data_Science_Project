\documentclass[11pt,reqno,oneside]{amsart}
%\pagestyle{plain}
%\usepackage[brazilian]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amscd,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{color}
\usepackage[mathscr]{euscript}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{extsizes}
\usepackage{float}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{nicefrac}
\usetikzlibrary{decorations.pathreplacing}
\definecolor{color1bg}{HTML}{f73d28}%FA8072}
\definecolor{color2bg}{HTML}{FA8072}
\definecolor{bblue}{HTML}{00BFFF}
\definecolor{bblue2}{HTML}{00ffff}


%\numberwithin{equation}{section}
%\usepackage[notcite,notref]{showkeys}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
	\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\usetikzlibrary{arrows,calc}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for different line styles
help lines/.style={dashed, thick},
axis/.style={<->},
important line/.style={thick},
connection/.style={thick, dotted},
}

\usetikzlibrary{shadows}
\usetikzlibrary{backgrounds}
\tikzset{
diagonal fill/.style 2 args={fill=#2, path picture={
		\fill[#1, sharp corners] (path picture bounding box.south west) -|
		(path picture bounding box.north east) -- cycle;}},
reversed diagonal fill/.style 2 args={fill=#2, path picture={
		\fill[#1, sharp corners] (path picture bounding box.north west) |- 
		(path picture bounding box.south east) -- cycle;}}
}
\usetikzlibrary{arrows.meta}

\newcommand\irregularcircle[2]{% radius, irregularity
\pgfextra {\pgfmathsetmacro\len{(#1)+rand*(#2)}}
+(0:\len pt)
\foreach \a in {10,20,...,350}{
	\pgfextra {\pgfmathsetmacro\len{(#1)+rand*(#2)}}
	-- +(\a:\len pt)
} -- cycle
}

\usepackage{rotate}
%\usepackage[noend]{algorithmic}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithms
%\usepackage{setspace}
%\usepackage{extsizes}
%\usepackage{hyperref}

%\usepackage[notcite,notref]{showkeys}

\makeatletter
\@addtoreset{equation}{section}
\makeatother
\newcounter{as}[section]
\renewcommand{\theas}{\thesection.\Alph{as}}
\newcommand{\newas}[1]{\refstepcounter{as}\label{#1}}
\newcommand{\useas}[1]{\ref{#1}}

\renewcommand\theequation{\thesection.\arabic{equation}}
\renewcommand\thefigure{\arabic{figure}}
\renewcommand\thetable{\thesection.\arabic{table}}

\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\DeclareMathOperator*{\argminA}{arg\,min}
\DeclareMathOperator*{\argmaxA}{arg\,max}


\title[Distribution-free Deviation Bounds of Learning]{Distribution-free Deviation Bounds of Learning via Model Selection with Cross-validation Risk Estimation}

%\date{\today}
\author{Diego Marcondes}
\address{Department of Computer Science, Institute of Mathematics and Statistics, Universidade de S\~{a}o
	Paulo, R. do Mat\~{a}o, 1010 - Butant\~{a}, S\~{a}o Paulo - SP,
	05508-090, Brazil. \\
	e-mail: \texttt{dmarcondes@ime.usp.br}}

\author{Cláudia Peixoto}
\address{Department of Applied Mathematics, Institute of Mathematics and Statistics, Universidade de S\~{a}o
	Paulo, R. do Mat\~{a}o, 1010 - Butant\~{a}, S\~{a}o Paulo - SP,
	05508-090, Brazil.}

\allowdisplaybreaks
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}[theorem]{Remark}
%\newenvironment{proof}{\paragraph{Proof:}}{\hfill$\blacksquare$}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}


\newcommand{\mc}[1]{{\mathcal #1}}
\newcommand{\mf}[1]{{\mathfrak #1}}
\newcommand{\mb}[1]{{\mathbf #1}}
\newcommand{\bb}[1]{{\mathbb #1}}
\newcommand{\bs}[1]{{\boldsymbol #1}}
\newcommand{\ms}[1]{{\mathscr #1}}
\newcommand{\bbf}[1]{{\mathbf #1}}
\newcommand{\bl}[1]{{\color{blue} #1}}

\usepackage{lineno}
%\linenumbers





\begin{document}
	\maketitle
	
\begin{abstract}
	Cross-validation techniques for risk estimation and model selection are widely used in statistics and machine learning. However, the understanding of the theoretical properties of learning via model selection with cross-validation risk estimation is quite low in face of its widespread use. In this context, this paper presents learning via model selection with cross-validation risk estimation as a general systematic learning framework within classical statistical learning theory and establishes distribution-free deviation bounds in terms of VC dimension, giving detailed proofs of the results and considering both bounded and unbounded loss functions. We also deduce conditions under which the deviation bounds of learning via model selection are tighter than that of learning via empirical risk minimization in the whole hypotheses space, supporting the better performance of model selection frameworks observed empirically in some instances.
	
	\noindent \textbf{Keywords:} deviation bounds, cross-validation, model selection, statistical learning theory, unbounded loss function
\end{abstract}

\section{Introduction}

The classical framework of Statistical Learning Theory is a triple $(\mathcal{H},\mathbb{A},\mathcal{D}_{N})$, composed by a set $\mathcal{H}$ of hypotheses $h$, called hypotheses space, and a learning algorithm $\mathbb{A}(\mathcal{H},\mathcal{D}_{N})$, which searches $\mathcal{H}$ seeking to minimize a risk measure based on a training sample $\mathcal{D}_{N} = \{Z_{1},\dots,Z_{N}\}$ of a random vector $Z$, with range $\mathcal{Z} \subset \mathbb{R}^{d}, d \geq 1,$ and unknown probability distribution $P$.

Let $\ell: \mathcal{Z} \times \mathcal{H} \mapsto  \mathbb{R}_{+}$ be a loss function. The risk of a hypothesis $h \in \mathcal{H}$ is an expected value of the local loss $\ell(z,h), z \in \mathcal{Z}$. If the expectation is the sample mean of $\ell(z,h)$ under $\mathcal{D}_{N}$, we have the empirical risk $L_{\mathcal{D}_{N}}(h)$, while if the expectation of $\ell(Z,h)$ is under the distribution $P$, we then have the out-of-sample risk $L(h)$. 

In this context, a target hypothesis $h^{\star} \in \mathcal{H}$ is such that its out-of-sample risk is minimum in $\mathcal{H}$, i.e., $L(h^{\star}) \leq L(h), \forall h \in \mathcal{H}$, while an empirical risk minimization (ERM) hypothesis $\hat{h}$ is such that its empirical risk is minimum, i.e., $L_{\mathcal{D}_{N}}(\hat{h}) \leq L_{\mathcal{D}_{N}}(h), \forall h \in \mathcal{H}$.

The algorithm $\mathbb{A}$ returns a $\hat{h}^{\mathbb{A}} \in \mathcal{H}$ seeking to approximate a target hypothesis $h^{\star} \in \mathcal{H}$. The returned hypothesis can be an ERM hypothesis, but this is not necessary. In any case, such learning framework has an important parameter that is problem-specific: the hypotheses space $\mathcal{H}$, which has a strong impact on the generalization quality of the estimated hypothesis $\hat{h}^{\mathbb{A}}$, that is characterized by a small out-of-sample risk.

The fundamental result in Statistical Learning Theory is the Vapnik-Chervonenkis (VC) theory \cite{vapnik1998,vapnik2000,vapnik1974theory,vapnik1971uniform,vapnik1974ordered2,vapnik1974ordered}, which implies that a hypotheses space $\mathcal{H}$ is PAC-learnable \cite{valiant1984} if, and only if, it has finite VC dimension ($d_{VC}(\mathcal{H}) < \infty$) \cite[Theorem~6.7]{shalev2014}. This means that, for any data generating distribution $P$, $L(\hat{h})$ is close to $L(h^{\star})$ with great confidence, if $N$ is sufficiently large. Therefore, it is possible to learn hypotheses with a finite sample, with precision and confidence dependent on the training sample size $N$ and the VC dimension\index{VC dimension} (complexity) of $\mathcal{H}$. 

The VC theory is general, has a structural importance to the field, and is a useful guide for modeling practical problems. From its main results follow a quantity $N(d_{VC}(\mathcal{H}),\epsilon,\delta)$, that is the least $N$, given a hypotheses space $\mathcal{H}$, a margin of error $\epsilon$ and a confidence $\delta$, such that
\begin{equation}
	\label{typeI}
	\mathbb{P}\left(L(\hat{h}) - L(h^{\star}) > \epsilon\right) \leq 1 - \delta.
\end{equation}
The bounds of VC theory are distribution-free, in the sense that inequality \eqref{typeI} holds with $N$ greater than $N(d_{VC}(\mathcal{H}),\epsilon,\delta)$ for all possible data generating distributions $P$. However, since the sample size $N$ depends on data availability, which may be conditioned upon several factors, such as technical difficulties and costs, the size $N(d_{VC}(\mathcal{H}),\epsilon,\delta)$ 
may not be attainable, and hence it may not be feasible to learn on $\mathcal{H}$ with low error and great confidence. 

In this context, the parameters $(N,\epsilon,\delta)$ are usually predetermined, and the only free component to be adjusted on VC theory's bounds is the hypotheses space or, more precisely, its VC dimension. This adjustment is usually performed via a data-driven selection of the hypotheses space, known as model selection (see \cite{ding2018,guyon2010,massart2007,raschka2018} for a review of model selection techniques). From now on, we use the words \textit{model}, \textit{hypotheses space} and \textit{hypotheses subspace} (of a hypotheses space) as synonyms. Once a model is selected, a hypothesis is then learned on it, so a hypothesis is learned via model selection.

Learning via model selection is a two-step framework in which first a model is selected from a collection of candidates, and then a hypothesis is learned on it. Fix a hypotheses space $\mathcal{H}$, a collection of candidate subsets, or models,
\begin{align*}
	\mathbb{C}(\mathcal{H}) = \{\mathcal{M}_{1},\dots,\mathcal{M}_{n}\} & & \text{ such that } & & \mathcal{H} = \bigcup_{i=1}^{n} \mathcal{M}_{i},
\end{align*}
and an empirical risk estimator $\hat{L}: \mathbb{C}(\mathcal{H}) \mapsto \mathbb{R}_{+}$, which for each candidate model attributes a risk estimated from data. The first step of learning via model selection is to select a minimizer of $\hat{L}$ in $\mathbb{C}(\mathcal{H})$:
\begin{equation*}
	\hat{\mathcal{M}} \coloneqq \argminA_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \hat{L}(\mathcal{M}).
\end{equation*}
Once a model is selected, one employs a data-driven algorithm $\mathbb{A}$ to learn a hypothesis $\hat{h}^{\mathbb{A}}_{\hat{\mathcal{M}}}$ in $\hat{\mathcal{M}}$.

A classical instance of model selection is that of variable selection, in which each candidate model contains the hypotheses which depend on a subset of variables \cite{guyon2003,john1994}. A more general approach to model selection is Structural Risk Minimization in which the candidate models form a chain $\mathcal{M}_{1} \subseteq \cdots \subseteq \mathcal{M}_{n}$ of subsets with increasing VC dimension and the risk $\hat{L}$ is a penalization of the resubstitution risk of each model by its complexity (see \cite[Chapter~6]{vapnik1998} for more details, and \cite{anguita2012} for an example).

Actually, there is a whole area of research that focuses on model selection by penalization, in which $\hat{L}$ is a penalized empirical risk. See \cite{massart2007} for an in-depth presentation of model selection by penalization, \cite{arlot2011,bartlett2008,koltchinskii2011,koltchinskii2001} for results in more specific learning frameworks and \cite{barron1999risk} for risk bounds in this instance. Another class of model selection frameworks is formed by those based on cross-validation techniques \cite{arlot2010survey,zhang2015cross,zhang1993model,kohavi1995}. 

An intrinsic characteristic of model selection frameworks is a bias-variance trade off, which is depicted in Figure \ref{fig_error}. On the one hand, when one selects from data a model $\hat{\mathcal{M}}$ among the candidates to learn on, he adds a bias to the learning process if $h^{\star}$ is not in $\hat{\mathcal{M}}$, since the best hypotheses $h^{\star}_{\hat{\mathcal{M}}}$ one can learn on $\hat{\mathcal{M}}$ may have a greater risk than $h^{\star}$, i.e., $L(h^{\star}_{\hat{\mathcal{M}}}) > L(h^{\star})$. Hence, even when the sample size is great, the learned hypothesis may not well generalize relative to $h^{\star}$. We call this bias type III estimation error, as depicted in Figure \ref{fig_error}.

On the other hand, learning within $\hat{\mathcal{M}}$ may have a lesser error than on the whole $\mathcal{H}$, in the sense of $L(\hat{h}^{\mathbb{A}}_{\hat{\mathcal{M}}})$ being close to $L(h^{\star}_{\hat{\mathcal{M}}})$. We call their difference type II estimation error, as depicted in Figure \ref{fig_error}. The actual error of learning in this instance is the difference between $L(\hat{h}^{\mathbb{A}}_{\hat{\mathcal{M}}})$ and $L(h^{\star})$, also depicted in Figure \ref{fig_error} as type IV estimation error. 

A successful framework for model selection should be such that $L(\hat{h}^{\mathbb{A}}_{\hat{\mathcal{M}}}) < L(\hat{h}^{\mathbb{A}})$. In other words, by adding a bias (III), the learning variance (II) within $\hat{\mathcal{M}}$ should be low enough, so the actual error (IV) committed when learning is lesser than the one committed by learning on the whole space $\mathcal{H}$, that is $L(\hat{h}^{\mathbb{A}}) - L(h^{\star})$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\linewidth]{Diagram_GErrors.png}
	\caption{Types II, III, and IV estimation errors when learning on $\hat{\mathcal{M}}$, in which $\hat{h}_{\hat{\mathcal{M}}} \equiv \hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}}$. These errors are formally defined in Section \ref{SecErrors}.}
	\label{fig_error}
\end{figure}

In this context, this paper aims to study the distribution-free asymptotics of learning via model selection, that means bounding the estimation errors in Figure \ref{fig_error}. In special, we seek to obtain bounds that imply sample sizes $N_{MS}(\mathbb{C}(\mathcal{H}),\epsilon,\delta) < N(d_{VC}(\mathcal{H}),\epsilon,\delta)$, depending on the family $\mathbb{C}(\mathcal{H})$ of candidate models, such that 
\begin{equation*}
	\label{typeIMS}
	\mathbb{P}\left(L(\hat{h}^{\mathbb{A}}_{\hat{\mathcal{M}}}) - L(h^{\star}) > \epsilon\right) \leq 1 - \delta
\end{equation*}
whenever the sample size is greater than $N_{MS}(\mathbb{C}(\mathcal{H}),\epsilon,\delta)$, so that generalization may be improved by learning via model selection. Furthermore, we define the target model $\mathcal{M}^{\star}$ among the candidates and establish convergence rates of $\hat{\mathcal{M}}$ to $\mathcal{M}^{\star}$. We focus on frameworks where the risk of each candidate model is estimated via a cross-validation procedure.

We present learning via model selection with cross-validation risk estimation as a general systematic learning framework within classical Statistical Learning Theory, and establish distribution-free deviation bounds in terms of VC dimension, giving detailed proofs of the results and considering both bounded and unbounded loss functions. In order to treat the case of bounded loss functions we extend the classical Statistical Learning Theory \cite{vapnik1998,devroye1996} to learning via model selection, while to treat the case of unbounded loss functions we apply and extend the recent results of \cite{cortes2019}.

Cross-validation techniques for risk estimation and model selection are widely spread in statistics and machine learning, and their asymptotic behavior have been fairly studied. The asymptotic optimality of specific cross-validation methods has been studied in \cite{andrews1991asymptotic,li1987asymptotic,dudoit2005asymptotics} and so-called oracle inequalities have been established for cross-validation methods in \cite{van2006oracle,lecue2012oracle}. However, the understanding of the theoretical properties of cross-validation is quite low in face of its widespread use. This fact has been noted by the recent works \cite{austern2020asymptotics,maillard2021local} which study the asymptotics of cross-validation for specific classes of models. To the best of our knowledge, a study of model selection with cross-validation risk estimation within this distribution-free combinatorial scope has not been done before, and may bring some insights into the benefits of model selection, even if the pessimistic bounds of distribution-free frameworks may not be of practical use.

We expect with the results of this paper to introduce the tools of classical VC theory to the study of model selection with cross-validation risk estimation, which is a simple method that works very well in practice, but lacks a strong theoretical basis in many instances. In \cite[p. 314]{massart2007}, when model selection with cross-validation error estimation is discussed in opposition to model selection with penalization, that is the main object of \cite{massart2007}, it is said that \textit{``It could seem a bit disappointing to discover that a very crude method like	hold-out\footnote{The \textit{hold-out} method that \cite{massart2007} refers to is equivalent to learning via model selection with risk estimated with a validation sample.} is working so well. This is especially true in the classification framework''.} We firmly believe that such a good performance should be explained by some theory that passes through studying it in a distribution-free framework.

In Section \ref{SecPreliminaries}, we present the main concepts of Statistical Learning Theory, and define the framework of learning via model selection with cross-validation risk estimation as a systematic general learning framework. In Sections \ref{boundedL} and \ref{SecUnbounded}, we establish deviation bounds for learning via model selection for bounded and unbounded loss functions, respectively. In Section \ref{examplePartition}, we present an illustrative example where the deviation bounds of learning via model selection are tighter than that of learning directly on $\mathcal{H}$ via empirical risk minimization. We discuss the main results and perspectives of this paper in Section \ref{FinalRemarks}. In Section \ref{SecProof}, we present the proof of the results. In order to increase the self-containment of this paper, enhancing its access to readers not expert in VC theory, we present in Appendix \ref{apVCtheory} an overview of its main results necessary to this paper.

\section{Model selection in Statistical Learning}
\label{SecPreliminaries}

Let $Z$ be a random vector defined on a probability space $(\Omega,\mathcal{S},\mathbb{P})$, with range $\mathcal{Z} \subset \mathbb{R}^{d}, d \geq 1$. Denote $P(z) \coloneqq \mathbb{P}(Z \leq z)$ as the probability distribution of $Z$ at point $z \in \mathcal{Z}$, which we assume unknown, but fixed. Define a sample $\mathcal{D}_{N} = \{Z_{1}, \dots, Z_{N}\}$ as a sequence of independent and identically distributed random vectors, defined on $(\Omega,\mathcal{S},\mathbb{P})$, with distribution $P$.

Let $\mathcal{H}$ be a general set, whose typical element we denote by $h$, which we call hypotheses space. We denote subsets of $\mathcal{H}$ by $\mathcal{M}_{i}$, indexed by the positive integers, i.e., $i \in \mathbb{Z}_{+}$. We may also denote a subset of $\mathcal{H}$ by $\mathcal{M}$ to ease notation. We consider \textit{model} and \textit{subset of} $\mathcal{H}$ as synonyms.

Let $\ell: \mathcal{Z} \times \mathcal{H} \mapsto  \mathbb{R}_{+}$ be a, possibly unbounded, loss function, which represents the loss $\ell(z,h)$ that incurs when one applies a hypothesis $h \in \mathcal{H}$ to \textit{explain} a feature of point $z \in \mathcal{Z}$. Denoting $\ell_{h}(z) \coloneqq \ell(z,h)$ for $z \in \mathcal{Z}$, we assume that, for each $h \in \mathcal{H}$, the composite function $\ell_{h} \circ Z$ is $(\Omega,\mathcal{S})$-measurable.

The risk of a hypothesis $h \in \mathcal{H}$ is defined as
\begin{equation*}
	L(h) \coloneqq \mathbb{E}[\ell_{h}(Z)] =  \int_{\mathcal{Z}} \ell_{h}(z) \ dP(z),
\end{equation*}
in which $\mathbb{E}$ means expectation under $\mathbb{P}$. We define the empirical risk on sample $\mathcal{D}_{N}$ as
\begin{equation*}
	L_{\mathcal{D}_{N}}(h) \coloneqq \frac{1}{N} \sum_{i=1}^{N} \ell_{h}(Z_{i}),
\end{equation*}
that is the empirical mean of $\ell_{h}(Z)$.

We denote the set of target hypotheses of $\mathcal{H}$ as
\begin{equation*}
	h^{\star} \coloneqq \argminA\limits_{h \in \mathcal{H}} L(h),
\end{equation*}
that are the hypotheses that minimize $L$ in $\mathcal{H}$, and the set of the target hypotheses of subsets of $\mathcal{H}$ by
\begin{align*}
	h^{\star}_{i} \coloneqq \argminA\limits_{h \in \mathcal{M}_{i}} L(h) & & h^{\star}_{\mathcal{M}} \coloneqq \argminA\limits_{h \in \mathcal{M}} L(h),
\end{align*}
depending on the subset.

The set of hypotheses which minimize the empirical risk in $\mathcal{H}$ is defined as 
\begin{align}
	\label{ERMH}
	\hat{h}^{\mathcal{D}_{N}} \coloneqq \argminA\limits_{h \in \mathcal{H}} L_{\mathcal{D}_{N}}(h),
\end{align}	
while the ones that minimize it in models are denoted by
\begin{align}
	\label{ERMSub}
	\hat{h}_{i}^{\mathcal{D}_{N}} \coloneqq \argminA\limits_{h \in \mathcal{M}_{i}} L_{\mathcal{D}_{N}}(h) & & \hat{h}_{\mathcal{M}}^{\mathcal{D}_{N}} \coloneqq \argminA\limits_{h \in \mathcal{M}} L_{\mathcal{D}_{N}}(h).
\end{align}
We assume the minimum of $L$ and $L_{\mathcal{D}_{N}}$ is achieved in $\mathcal{H}$, and in all subsets of it that we consider throughout this paper, so the sets above are not empty. To ease notation, we may simply denote $\hat{h}$ as a hypothesis that minimizes the empirical risk in $\mathcal{H}$. In general, we denote hypotheses estimated via an algorithm $\mathbb{A}$ in $\mathcal{H}$ and its subsets by $\hat{h}^{\mathbb{A}}, \hat{h}_{i}^{\mathbb{A}}$ and $\hat{h}_{\mathcal{M}}^{\mathbb{A}}$. In the special case when $\mathbb{A}$ is given by empirical risk minimization in sample $\mathcal{D}_{N}$ we have the hypotheses defined in \eqref{ERMH} and \eqref{ERMSub}

We denote a collection of candidate models by $\mathbb{C}(\mathcal{H}) = \{\mathcal{M}_{i}: i \in \mathcal{J}\}$, for $\mathcal{J} \subset \mathbb{Z}_{+}, \text{\textbar}\mathcal{J}\text{\textbar} < \infty$, and assume that it covers $\mathcal{H}$:
\begin{equation*}
	\mathcal{H} = \bigcup_{i \in \mathcal{J}} \mathcal{M}_{i}.
\end{equation*}
We define the VC dimension under loss function $\ell$ of such a collection as
\begin{equation*}
	\label{VCdimCand}
	d_{VC}(\mathbb{C}(\mathcal{H}),\ell) \coloneqq \max\limits_{i \in \mathcal{J}} d_{VC}(\mathcal{M}_{i},\ell)
\end{equation*}
and assume that $d_{VC}(\mathbb{C}(\mathcal{H}),\ell) < \infty$. Since every model in $\mathbb{C}(\mathcal{H})$ is a subset of $\mathcal{H}$ it follows that $d_{VC}(\mathbb{C}(\mathcal{H}),\ell) \leq d_{VC}(\mathcal{H},\ell)$. In Appendix \ref{apVCtheory} we review the main concepts of VC theory \cite{vapnik1998}. In special, we define the VC dimension of a hypotheses space under loss function $\ell$ (cf. Definition \ref{VCdimension}). When the loss function is clear from the context, or not of relevance to our argument, we denote the VC dimension simply by $d_{VC}(\mathcal{M})$ for $\mathcal{M} \subseteq \mathcal{H}$ and $d_{VC}(\mathbb{C}(\mathcal{H}))$.

\subsection{Model risk estimation}
\label{esti_Lhat}

The risk of a model in $\mathbb{C}(\mathcal{H})$ is defined as
\begin{equation*}
	L(\mathcal{M}) \coloneqq  \min\limits_{h \in \mathcal{M}} L(h) = L(h^{\star}_{\mathcal{M}}),
\end{equation*}
for $\mathcal{M} \in \mathbb{C}(\mathcal{H})$, and we consider estimators $\hat{L}(\mathcal{M})$ for $L(\mathcal{M})$ based on cross-validation. We assume that $\hat{L}$ is of the form
\begin{align}
	\label{form_Lhat}
	\hat{L}(\mathcal{M}) = \frac{1}{m} \ \sum_{j=1}^{m} \ \hat{L}^{(j)}(\hat{h}^{(j)}_{\mathcal{M}}), & & \mathcal{M} \in \mathbb{C}(\mathcal{H}),
\end{align}
in which there are $m$ pairs of independent training and validation samples, $\hat{L}^{(j)}$ is the empirical risk under the $j$-th validation sample, and $\hat{h}^{(j)}_{\mathcal{M}}$ is a hypothesis that minimizes the empirical risk in $\mathcal{M}$ under the $j$-th training sample, denoted by $\mathcal{D}_{N}^{(j)}$. We assume independence between samples within a pair $j$, but there may exist dependence between samples of distinct pairs $j,j^{\prime}$. All training and validation samples are subsets of $\mathcal{D}_{N}$.

In order to exemplify the results obtained in this paper, we will apply them for two estimators of the form \eqref{form_Lhat}, obtained with a validation sample and k-fold cross validation, which we formally define below. When there is no need to specify which estimator of $L(\mathcal{M})$ we are referring, we denote simply $\hat{L}(\mathcal{M})$ to mean an arbitrary estimator with form \eqref{form_Lhat}.

\subsubsection{Validation sample}

Fix a sequence $\{V_{N}: N \geq 1\}$ such that $\lim\limits_{N \rightarrow \infty} V_{N} = \lim\limits_{N \to \infty} N - V_{N} = \infty$, and let 
\begin{align*}
	\mathcal{D}_{N}^{(\text{train})} = \{Z_{l}: 1 \leq l \leq N - V_{N}\} & & \mathcal{D}_{N}^{(\text{val})} = \{Z_{l}: N - V_{N} < l \leq N\}
\end{align*}
be a split of $\mathcal{D}_{N}$ into a training and validation sample. Proceeding in this manner, we have two samples $\mathcal{D}_{N}^{(\text{train})}$ and $\mathcal{D}_{N}^{(\text{val})}$, which are independent. The estimator under the validation sample is given by
\begin{equation*}
	\label{VALLhat}
	\hat{L}_{\text{val}}(\mathcal{M}) \coloneqq L_{\mathcal{D}_{N}^{(\text{val})}}(\hat{h}_{\mathcal{M}}^{(\text{train})}) =  \frac{1}{V_{N}} \sum_{N- V_{N} < l \leq N}  \ell\big(Z_{l},\hat{h}_{\mathcal{M}}^{(\text{train})}\big),
\end{equation*}
in which
\begin{equation*}
	\hat{h}_{\mathcal{M}}^{(\text{train})} = \argminA_{h \in \mathcal{M}} L_{\mathcal{D}_{N}^{(\text{train})}}(h)
\end{equation*}
minimizes the empirical risk in $\mathcal{M}$ under $\mathcal{D}_{N}^{(\text{train})}$.

\subsubsection{K-fold cross-validation}

Fix $k \in \mathbb{Z}_{+}$ and assume $N \coloneqq kn$, for a $n \in \mathbb{Z}_{+}$. Then, let 
\begin{align*}
	\mathcal{D}_{N}^{(j)} \coloneqq \{Z_{l}: (j-1)n < l \leq jn\}, & & j = 1,\dots,k,
\end{align*}
be a partition of $\mathcal{D}_{N}$: 
\begin{align*}
	\mathcal{D}_{N} = \bigcup_{j=1}^{k} \mathcal{D}_{N}^{(j)} & & \text{ and } & & \mathcal{D}_{N}^{(j)} \cap \mathcal{D}_{N}^{(j^{\prime})} = \emptyset \text{ if } j \neq j^{\prime}.
\end{align*}
We define
\begin{equation*}
	\hat{h}^{(j)}_{\mathcal{M}} \coloneqq \argminA_{h \in \mathcal{M}} \ L_{\mathcal{D}_{N}\setminus\mathcal{D}_{N}^{(j)}}(h) = \argminA_{h \in \mathcal{M}} \ \frac{1}{(k-1)n} \sum_{\substack{l \leq (j-1)n \\ \cup \ l > jn}} \ell(Z_{l},h)
\end{equation*}
as the hypothesis which minimizes the empirical risk in $\mathcal{M}$ under the sample $\mathcal{D}_{N}\setminus\mathcal{D}_{N}^{(j)}$, that is the sample composed by all folds, but the $j$-th, and
\begin{equation*}
	\hat{L}_{\text{cv(k)}}^{(j)}(\mathcal{M}) \coloneqq L_{\mathcal{D}_{N}^{(j)}}(\hat{h}^{(j)}_{\mathcal{M}}) =  \frac{1}{n} \sum_{(j-1)n < l \leq jn} \ell(Z_{l},\hat{h}^{(j)}_{\mathcal{M}}),
\end{equation*}
as the validation risk of the $j$-th fold.

The k-fold cross-validation estimator of $L(\mathcal{M})$ is then given by
\begin{equation*}
	\label{CVLhat}
	\hat{L}_{\text{cv(k)}}(\mathcal{M}) \coloneqq \frac{1}{k} \ \sum_{j=1}^{k} \hat{L}_{\text{cv(k)}}^{(j)}(\mathcal{M}),
\end{equation*}
that is the average validation risk over the folds. 

\subsection{Target model and estimation errors}
\label{SecErrors}

The motivation for learning via model selection is presented in Figure \ref{paradigms}, in which the ellipses represent some models in $\mathbb{C}(\mathcal{H})$, and their area is proportional to their complexity, given for example by VC dimension. Assume that $\mathcal{H}$ is all we have to learn on, and we are not willing to consider any hypothesis outside $\mathcal{H}$. Then, if we could choose, we would like to learn on $\mathcal{M}^{\star}$: \textit{the least complex model in $\mathbb{C}(\mathcal{H})$ which contains a target hypothesis $h^{\star}$}. We call $\mathcal{M}^{\star}$ the target model.

In order to formally define the target model, we need to consider equivalence classes of models, as it is not possible to differentiate some models with the concepts of Statistical Learning Theory. Define in $\mathbb{C}(\mathcal{H})$ the equivalence relation given by
\begin{align}
	\label{equiv_class}
	\mathcal{M}_{i} \sim \mathcal{M}_{j} \text{ if, and only if, } d_{VC}(\mathcal{M}_{i}) = d_{VC}(\mathcal{M}_{j}) \text{ and } L(\mathcal{M}_{i}) = L(\mathcal{M}_{j}),
\end{align}
for $\mathcal{M}_{i}, \mathcal{M}_{j} \in \mathbb{C}(\mathcal{H})$: two models in $\mathbb{C}(\mathcal{H})$ are equivalent if they have the same VC dimension and risk. Let 
\begin{equation*}
	\mathcal{L}^{\star} = \argminA\limits_{\mathcal{M} \in \ \nicefrac{\mathbb{C}(\mathcal{H})}{\sim}} L(\mathcal{M})
\end{equation*}
be the equivalence classes which contain a target hypothesis of $\mathcal{H}$, so their risk is minimum. We define the target model $\mathcal{M}^{\star} \in \nicefrac{\mathbb{C}(\mathcal{H})}{\sim}$ as
\begin{equation*}
	\mathcal{M}^{\star} = \argminA\limits_{\mathcal{M} \in \mathcal{L}^{\star}} d_{VC}(\mathcal{M}),
\end{equation*}
which is the class of the smallest models in $\mathbb{C}(\mathcal{H})$, in the VC dimension sense, that are not disjoint with $h^{\star}$. The target model has the lowest complexity among the unbiased models in $\mathbb{C}(\mathcal{H})$.

The target model is dependent on both $\mathbb{C}(\mathcal{H})$ and the data generating distribution, so we cannot establish beforehand, without looking at data, on which model of $\mathbb{C}(\mathcal{H})$ to learn. Hence, in this context, a model selection procedure should, based on data, learn a model $\hat{\mathcal{M}}$ among the candidates $\mathbb{C}(\mathcal{H})$ as an estimator of $\mathcal{M}^{\star}$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\linewidth]{New_Paradigm.png}
	\caption{Decomposition of $\mathcal{H}$ by a $\mathbb{C}(\mathcal{H})$. We omitted some models for a better visualization, since $\mathbb{C}(\mathcal{H})$ should cover $\mathcal{H}$.}
	\label{paradigms}
\end{figure}

However, when learning on a model $\hat{\mathcal{M}} \in \mathbb{C}(\mathcal{H})$ selected based on data, one commits three types of errors:
\begin{align*}
	\label{ee23}
	\textbf{(II)} \ \ L(\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}}) - L(h^{\star}_{\hat{\mathcal{M}}}) & & \textbf{(III)} \ \ L(h^{\star}_{\hat{\mathcal{M}}}) - L(h^{\star}) & & \textbf{(IV)} \ \ L(\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}}) - L(h^{\star}),
\end{align*}
that we call types II, III, and IV estimation errors\footnote{We define type I estimation error in Section \ref{SecLearnOn} (cf. \eqref{typeIe}).}, which are illustrated in Figure \ref{fig_error}. In a broad sense, type III estimation error would represent the bias of learning on $\hat{\mathcal{M}}$, while type II would represent the variance within $\hat{\mathcal{M}}$, and type IV would be the error, with respect to $\mathcal{H}$, committed when learning on $\hat{\mathcal{M}}$ with algorithm $\mathbb{A}$. 

Indeed, type III estimation error compares a target hypothesis $h_{\hat{\mathcal{M}}}^{\star}$ of $\hat{\mathcal{M}}$ with a target hypothesis $h^{\star}$ of $\mathcal{H}$, hence any difference between them would be a systematic bias of learning on $\hat{\mathcal{M}}$ when compared to learning on $\mathcal{H}$. Type II estimation error compares the loss of the estimated hypothesis $\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}}$ of $\hat{\mathcal{M}}$ and the loss of its target, assessing how much the estimated hypothesis varies from a target of $\hat{\mathcal{M}}$, while type IV is the effective error committed, since compares the estimated hypothesis of $\hat{\mathcal{M}}$ with a target of $\mathcal{H}$. 

As is often the case, there will be a bias-variance trade-off that should be minded when learning on $\hat{\mathcal{M}}$, so it is important to guarantee that, when the sample size increases, all the estimation errors tend to zero. Furthermore, it is desired that the learned model $\hat{\mathcal{M}}$ converges to the target model $\mathcal{M}^{\star}$ with probability one, so learning is asymptotically optimal. The proposed learning framework via model selection defined in the next section will take these desires into account.

\subsection{Learning hypotheses via model selection}
\label{LearningFramework}

Learning via model selection is composed of two steps: first learn a model $\hat{\mathcal{M}}$ from $\mathbb{C}(\mathcal{H})$ and then learn a hypothesis on $\hat{\mathcal{M}}$. In this section, we define $\hat{\mathcal{M}}$ and an algorithm $\mathbb{A}$ to learn on it.

\subsubsection{Learning model $\hat{\mathcal{M}}$}

Model selection is performed by applying a $(\Omega,\mathcal{S})$-measurable function $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$, dependent on $\mathbb{C}(\mathcal{H})$, satisfying
\begin{equation}
	\label{diagram}
	\omega \in \Omega \xrightarrow{(\mathcal{D}_{N},\hat{L})} (\mathcal{D}_{N}(\omega),\hat{L}(\omega)) \xrightarrow{\ \ \mathbb{M}_{\mathbb{C}(\mathcal{H})} \ \ } \mathcal{\hat{M}}(\omega) \in \mathbb{C}(\mathcal{H}),
\end{equation}
which is such that, given $\mathcal{D}_{N}$ and an estimator $\hat{L}$ of the risk of each candidate model, learns a $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$. Note from (\ref{diagram}) that $\mathcal{\hat{M}}$ is a $(\Omega,\mathcal{S})$-measurable $\mathbb{C}(\mathcal{H})$-valued function, as it is the composition of measurable functions, i.e., $\mathcal{\hat{M}} \coloneqq \mathcal{\hat{M}}_{\mathcal{D}_{N},\hat{L},\mathbb{C}(\mathcal{H})} = \mathbb{M}_{\mathbb{C}(\mathcal{H})}\big(\mathcal{D}_{N},\hat{L}\big)$. Even though $\mathcal{\hat{M}}$ depends on $\mathcal{D}_{N}, \hat{L}$ and $\mathbb{C}(\mathcal{H})$, we drop the subscripts to ease notation. 

We are interested in a $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$ such that
\begin{equation}
	\label{consistent_LM}
	\mathcal{\hat{M}} = \mathbb{M}_{\mathbb{C}(\mathcal{H})}\big(\mathcal{D}_{N},\hat{L}\big) \xrightarrow{N \rightarrow \infty} \mathcal{M}^{\star} \text{ with probability one}.
\end{equation}
Actually, it is desired the model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$ to be as simple as it can be under the restriction that it converges to the target model. 

A $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$ which satisfies (\ref{consistent_LM}) may be defined by mimicking the definition of $\mathcal{M}^{\star}$, but employing the estimated risk $\hat{L}$ instead of $L$. Define in $\mathbb{C}(\mathcal{H})$ the equivalence relation given by
\begin{align*}
	\mathcal{M}_{i} \hat{\sim} \mathcal{M}_{j} \text{ if, and only if, } d_{VC}(\mathcal{M}_{i}) = d_{VC}(\mathcal{M}_{j}) \text{ and } \hat{L}(\mathcal{M}_{i}) = \hat{L}(\mathcal{M}_{j}),
\end{align*}
for $\mathcal{M}_{i}, \mathcal{M}_{j} \in \mathbb{C}(\mathcal{H})$, which is a random $(\Omega,\mathcal{S})$-measurable equivalence relation. Let
\begin{equation*}
	\hat{\mathcal{L}} = \argminA\limits_{\mathcal{M} \in \ \nicefrac{\mathbb{C}(\mathcal{H})}{\hat{\sim}}} \hat{L}(\mathcal{M})
\end{equation*}
be the classes in $\nicefrac{\mathbb{C}(\mathcal{H})}{\hat{\sim}}$ with the least estimated risk. We call the classes in $\hat{\mathcal{L}}$ the global minimums of $\mathbb{C}(\mathcal{H})$. Then, $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$ selects
\begin{equation}
	\label{Ghat}
	\hat{\mathcal{M}} = \argminA\limits_{\mathcal{M} \in \mathcal{\hat{L}}} d_{VC}(\mathcal{M}),
\end{equation} 
the simplest class among the global minimums.

\subsubsection{Learning hypotheses on $\hat{\mathcal{M}}$}
\label{SecLearnOn}

Once $\hat{\mathcal{M}}$ is selected, we need to learn hypotheses on it and, to this end, we assume there is a sample $\tilde{\mathcal{D}}_{M} = \{\tilde{Z}_{l}: 1 \leq l \leq M\}$ of $M$ independent and identically distributed random vectors with distribution $P$, independent of $\mathcal{D}_{N}$, so we can learn
\begin{align*}
	%\label{learn_inde}
	\hat{h}_{\hat{\mathcal{M}}}^{\tilde{\mathcal{D}}_{M}} \coloneqq \argminA\limits_{h \in \hat{\mathcal{M}}} L_{\tilde{\mathcal{D}}_{M}}(h),
\end{align*}
that are the hypotheses which minimize the empirical risk under $\tilde{\mathcal{D}}_{M}$ on $\hat{\mathcal{M}}$. Figure \ref{learn_hyp} summarizes the systematic framework of learning via model selection.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.5\linewidth]{learn_hyp1}	
	\caption{The systematic framework for learning hypotheses via model selection. A sample of size $N+M$ is split into two, one of size $N$ that is used to estimate $\hat{\mathcal{M}}$ by the minimization of $\hat{L}$ on $\mathbb{C}(\mathcal{H})$, and another of size $M$ is used to learn a hypothesis on $\hat{\mathcal{M}}$ by the minimization of the empirical risk.}
	\label{learn_hyp}
\end{figure*}

We define the type I estimation error as
\begin{align}
	\label{typeIe}
	\textbf{(I)} 
	\sup\limits_{h \in \hat{\mathcal{M}}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h)\text{\textbar},
\end{align}
which represents how well one can estimate the loss uniformly on $\hat{\mathcal{M}}$ by the empirical risk under $\tilde{\mathcal{D}}_{M}$.

\begin{remark}
	We assume that the supremum in \eqref{typeIe} is $(\Omega,\mathcal{S})$-measurable, so it is meaningful to calculate probabilities of events which involve it. We also assume throughout this paper that this supremum, over any fixed $\mathcal{M} \in \mathbb{C}(\mathcal{H}),$ is also $(\Omega,\mathcal{S})$-measurable.
\end{remark}

\subsection{Deviation bounds and main results}

In classical learning theory, or VC theory, there are two kinds of estimation errors, whose tail probabilities are
\begin{align}
	\label{GE1}
	\mathbb{P}&\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L_{\mathcal{D}_{N}}(h) - L(h)\text{\textbar} > \epsilon\right) 
\end{align}
and
\begin{align}
	\label{GE2}
	\mathbb{P}&\left(L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) > \epsilon\right),
\end{align}
for $\epsilon > 0$. In the terminology of this paper, they are called, respectively, type I and II estimation error, when the target hypotheses of $\mathcal{H}$ are estimated by minimizing the empirical risk under sample $\mathcal{D}_{N}$.

When the loss function is bounded, the rate of convergence of \eqref{GE2} to zero is decreasing on the VC dimension of $\mathcal{H}$. This is the main result of VC theory, which may be stated as follows, and is a consequence of Corollaries \ref{cor3TypeI} and \ref{cor1TypeII}. Observe that the bounds do not depend on $P$, and are valid for any distribution $Z$ may have, that is, are distribution-free. A result analogous to Proposition \ref{propVC} will be stated for unbounded loss functions in Section \ref{SecUnbounded}. In what follows, a.s. stands for almost sure convergence or convergence with probability one.

\begin{proposition}
	\label{propVC}
	Assume the loss function is bounded. Fixed a hypotheses space $\mathcal{H}$ with $d_{VC}(\mathcal{H}) < \infty$, there exist sequences $\{B^{I}_{N,\epsilon}: N \geq 1\}$ and $\{B^{II}_{N,\epsilon}: N \geq 1\}$ of positive real-valued increasing functions with domain $\mathbb{Z}_{+}$ satisfying
	\begin{equation*}
		\lim\limits_{N \to \infty} B^{I}_{N,\epsilon}(k) = \lim\limits_{N \to \infty} B^{II}_{N,\epsilon}(k) = 0,
	\end{equation*}
	for all $\epsilon > 0$ and $k \in \mathbb{Z}_{+}$ fixed, such that
	\begin{align*}
		\mathbb{P}&\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L_{\mathcal{D}_{N}}(h) - L(h)\text{\textbar} > \epsilon\right) \leq B^{I}_{N,\epsilon}(d_{VC}(\mathcal{H}))\\
		\mathbb{P}&\left(L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) > \epsilon\right) \leq B^{II}_{N,\epsilon}(d_{VC}(\mathcal{H})).
	\end{align*}	
	Furthermore, the following holds:
	\begin{align*}
		&\sup\limits_{h \in \mathcal{H}} \text{\textbar}L_{\mathcal{D}_{N}}(h) - L(h)\text{\textbar} \xrightarrow[N \to \infty]{a.s.} 0 & & L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) \xrightarrow[N \to \infty]{a.s.} 0.
	\end{align*}
\end{proposition}

The sequences $\{B^{I}_{N,\epsilon}: N \geq 1\}$ and $\{B^{II}_{N,\epsilon}: N \geq 1\}$ are what we call the deviation bounds of learning via empirical risk minimization in $\mathcal{H}$. The main results of this paper are the convergence of $\hat{\mathcal{M}}$ to $\mathcal{M}^{\star}$ with probability one and deviation bounds for types I, II, III, and IV estimation errors when learning via model selection with bounded or unbounded loss function.

In special, it will follow that the established deviation bounds for the type IV estimation error of learning via model selection may be tighter than that for the type II estimation error of learning directly on $\mathcal{H}$ via empirical risk minimization, and hence one may have a lower risk by learning via model selection. This means that by introducing a bias III, which converges to zero, we may decrease the variance II of the learning process, so it is more efficient to learn via model selection. 

In the following sections, we treat the cases of bounded and unbounded loss functions.

\begin{remark}
	The quantity $N(d_{VC}(\mathcal{H}),\epsilon,\delta)$, introduced in \eqref{typeI}, is obtained by solving on $N$ the equation
	\begin{equation*}
		B_{N,\epsilon}^{II}(d_{VC}(\mathcal{H})) = 1 - \delta,
	\end{equation*}
	hence developing deviation bounds is equivalent to obtaining the sample size necessary to solve the learning problem with $\epsilon, \delta$ and $d_{VC}(\mathcal{H})$ fixed.
\end{remark}

\section{Learning via model selection with bounded loss functions}
\label{boundedL}

In Section \ref{SecConvTM}, we show the convergence of $\hat{\mathcal{M}}$ to the target model $\mathcal{M}^{\star}$ with probability one, and in Section \ref{SecConvOnMhat} we establish deviation bounds for the estimation errors of learning via model selection when the loss function is bounded. We assume from now there exists a constant $C > 0$ such that
\begin{align*}
	0 \leq \ell(z,h) \leq C & & \text{ for all } z \in \mathcal{Z}, h \in \mathcal{H}.
\end{align*}

\subsection{Convergence to the target model}
\label{SecConvTM}

We start by studying a result weaker than the convergence of $\hat{\mathcal{M}}$ to $\mathcal{M}^{\star}$, that is, the convergence of $L(\hat{\mathcal{M}})$ to $L(\mathcal{M}^{\star})$.

In order to have $L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})$, one does not need to know exactly $L(\mathcal{M})$ for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$, i.e., one does not need $\hat{L}(\mathcal{M}) = L(\mathcal{M})$, for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$. We argue that it suffices to have $\hat{L}(\mathcal{M})$ close enough to $L(\mathcal{M})$, for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$, so the global minimums of $\mathbb{C}(\mathcal{H})$ will have the same risk as $\mathcal{M}^{\star}$, even if it is not possible to properly estimate their risk. This ``close enough'' depends on $P$, hence is not distribution-free, and is given by the \textit{maximum discrimination error} (MDE) of $\mathbb{C}(\mathcal{H})$ under $P$, which we define as
\begin{equation*}
	\epsilon^{\star} = \epsilon^{\star}(\mathbb{C}(\mathcal{H}),P) \coloneqq \min\limits_{\substack{\mathcal{M} \in \mathbb{C}(\mathcal{H})\\L(\mathcal{M}) > L(\mathcal{M}^{\star}) }} L(\mathcal{M}) - L(\mathcal{M}^{\star}).
\end{equation*}

The MDE is the minimum difference between the out-of-sample risk of a target hypothesis and the best hypothesis in a model which does not contain a target. In other words, it is the difference between the risk of the best model $\mathcal{M}^{\star}$ and the second to best. The meaning of $\epsilon^{\star}$ is depicted in Figure \ref{epsilonstar}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{epsilonstar}
	\caption{The risks of the equivalence classes (cf. \eqref{equiv_class}) of $\mathbb{C}(\mathcal{H})$ in ascending order. The MDE $\epsilon^{\star}$ is the difference between the risk of the target class $\mathcal{M}^{\star}$, and the second to best $\mathcal{M}_{2}$. The colored intervals represent a distance of $\epsilon^{\star}/2$ from the out-of-sample risk of each model, and the colored estimated risks $\hat{L}$ illustrate a case such that the estimated risk is within $\epsilon^{\star}/2$ of the out-of-sample risk for all models. The class $\mathcal{M}_{1}$ has the same risk as $\mathcal{M}^{\star}$, but has a smaller estimated risk, and, by the definition of $\mathcal{M}^{\star}$, greater VC dimension. Note from the representation that, if one can estimate $\hat{L}$ within a margin of error of $\epsilon^{\star}/2$, then $\hat{\mathcal{M}}$ will be a model with the same risk as $\mathcal{M}^{\star}$, in this case $\mathcal{M}_{1}$ (cf. Proposition \ref{proposition_principal}).} \label{epsilonstar}
\end{figure}

The MDE is defined only if there exists at least one $\mathcal{M} \in \mathbb{C}(\mathcal{H})$ such that $h^{\star} \cap \mathcal{M} = \emptyset$, i.e., there is a subset in $\mathbb{C}(\mathcal{H})$ which does not contain a target hypothesis. If  $h^{\star} \cap \mathcal{M} \neq \emptyset$ for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$, then type III estimation error is zero, and type IV reduces to type II. From this point, we assume that $\epsilon^{\star}$ is well defined.

The terminology MDE is used for we can show that a fraction of $\epsilon^{\star}$ is the greatest error one can commit when estimating $L(\mathcal{M})$ by $\hat{L}(\mathcal{M})$, for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$, in order for $L(\mathcal{\hat{M}})$ to be equal to $L(\mathcal{M}^{\star})$. This is the result of the next proposition.

\begin{proposition}
	\label{proposition_principal}
	Assume there exists $\delta > 0$ such that
	\begin{equation}
		\label{cond_prop_principal}
		\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} < \epsilon^{\star}/2\right) \geq 1 - \delta.
	\end{equation}
	Then
	\begin{equation}
		\label{prob_equal}
		\mathbb{P}\left(L(\mathcal{\hat{M}}) = L(\mathcal{M}^{\star})\right) \geq 1-\delta.
	\end{equation}	
\end{proposition}


\begin{remark}
	Since there may exist $\mathcal{M} \in \ \nicefrac{\mathbb{C}(\mathcal{H})}{\sim}$ with $L(\mathcal{M}) = L(\mathcal{\mathcal{M}^{\star}})$ and $d_{VC}(\mathcal{M}) > d_{VC}(\mathcal{M}^{\star})$, condition \eqref{cond_prop_principal} guarantees only that the estimated risk of both $\mathcal{M}$ and $\mathcal{M}^{\star}$ is lesser than the estimated risk of any model with risk greater than theirs, but it may happen that $\hat{L}(\mathcal{M}) < \hat{L}(\mathcal{M}^{\star})$ (see Figure \ref{epsilonstar} for an example). In this instance, we have $\hat{\mathcal{M}} = \mathcal{M}$ and $L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})$.
\end{remark}

Recall we are assuming that $\hat{L}$ is of the form \eqref{form_Lhat}. In this case, we may obtain a bound for \eqref{prob_equal} depending on $\epsilon^{\star}$, on $d_{VC}(\mathbb{C}(\mathcal{H}))$, and on bounds for tail probabilities of type I estimation error under each validation and training sample (cf. Proposition \ref{propVC}). These bounds also depend on the number of maximal models of $\mathbb{C}(\mathcal{H})$, which are models in
\begin{align*}
	\text{Max } \mathbb{C}(\mathcal{H}) = \left\{\mathcal{M} \in \mathbb{C}(\mathcal{H}): \text{ if } \mathcal{M} \subset \mathcal{M}^{\prime} \in \mathbb{C}(\mathcal{H}) \text{ then } \mathcal{M} = \mathcal{M}^{\prime} \right\},
\end{align*}
that are models not contained in any element in $\mathbb{C}(\mathcal{H})$ besides themselves. We denote
\begin{equation*}
	\mathfrak{m}(\mathbb{C}(\mathcal{H})) = \text{\textbar}\text{Max } \mathbb{C}(\mathcal{H})\text{\textbar},
\end{equation*}
the number of maximal models in $\mathbb{C}(\mathcal{H})$. We have the following rate of convergence of $L(\hat{\mathcal{M}})$ to $L(\mathcal{M}^{\star})$, and condition for $\hat{\mathcal{M}}$ to converge to $\mathcal{M}^{\star}$ with probability one.

\begin{theorem}
	\label{theorem_principal_convergence}
	Assume the loss function is bounded. For each $\epsilon > 0$, let $\{B_{N,\epsilon}: N \geq 1\}$ and $\{\hat{B}_{N,\epsilon}: N \geq 1\}$ be sequences of positive real-valued increasing functions with domain $\mathbb{Z}_{+}$ satisfying
	\begin{equation*}
		\lim\limits_{N \to \infty} B_{N,\epsilon}(k) = \lim\limits_{N \to \infty} \hat{B}_{N,\epsilon}(k) = 0,
	\end{equation*}
	for all $\epsilon > 0$ and $k \in \mathbb{Z}_{+}$ fixed, and such that
	\begin{align*}
		\max_{j} \mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon \right) \leq B_{N,\epsilon}(d_{VC}(\mathcal{M})) & \text{ and } &\\ \nonumber 
		\max_{j} \mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon \right) \leq \hat{B}_{N,\epsilon}(d_{VC}(\mathcal{M})), & & 
	\end{align*}
	for all $\mathcal{M} \in \mathbb{C}(\mathcal{M})$, recalling that $L_{\mathcal{D}_{N}^{(j)}}$ and $\hat{L}^{(j)}$ represent the empirical risk under the $j$-th training and validation samples, respectively. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then,
	\begin{align}
		\label{bound_pM}
		\mathbb{P}\left(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star})\right) &\leq m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[B_{N,\epsilon^{\star}/8}(d_{VC}(\mathbb{C}(\mathcal{H}))) + \hat{B}_{N,\epsilon^{\star}/4}(d_{VC}(\mathbb{C}(\mathcal{H})))\right],
	\end{align}
	in which $m$ is the number of pairs considered to calculate \eqref{form_Lhat}. Furthermore, if
	\begin{align}
		\label{as_conv}
		\max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \max_{j} \sup\limits_{h \in \mathcal{M}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} \xrightarrow{\text{a.s.}} 0 & \text{ and } &\\ \nonumber
		\max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \max_{j} \sup\limits_{h \in \mathcal{M}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} \xrightarrow{\text{a.s.}} 0, & & 
	\end{align}
	then
	\begin{equation*}
		\lim_{N \rightarrow \infty} \mathbb{P}\left(\hat{\mathcal{M}} = \mathcal{M}^{\star}\right) = 1.
	\end{equation*}
\end{theorem}

A bound for $\mathbb{P}(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star}))$, and the almost sure convergence of $\hat{\mathcal{M}}$ to $\mathcal{M}^{\star}$ in the case of k-fold cross validation, follow from Proposition \ref{propVC}, recalling that the sample size in each training and validation sample is $(k-1)n$ and $n$, respectively, with $N = kn$. Analogously, we may obtain a bound when an independent validation sample is considered. This result is stated in the next theorem.

\begin{theorem}
	\label{CVModelconvergence}
	Assume the loss function is bounded. If $\hat{L}$ is given by k-fold cross-validation or by an independent validation sample, then $\hat{\mathcal{M}}$ converges with probability one to $\mathcal{M}^{\star}$.
\end{theorem}

From bound \eqref{bound_pM}, follows that we have to better estimate with the training samples, that require a precision of $\epsilon^{\star}/8$ in contrast to a precision $\epsilon^{\star}/4$ with the validation samples. Hence, as is done in k-fold cross validation, it is better to consider a greater sample size for training rather than for validation.

Moreover, from this bound follows that, with a fixed sample size, we can have a tighter bound for $\mathbb{P}(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star}))$ by choosing a family of candidate models with small $d_{VC}(\mathbb{C}(\mathcal{H}))$ and few maximal elements, while attempting to increase $\epsilon^{\star}$. Of course, there is a trade-off between $d_{VC}(\mathbb{C}(\mathcal{H}))$ and the number of maximal elements of $\mathbb{C}(\mathcal{H})$, the only known free quantities in bound $\eqref{bound_pM}$, since the sample size is fixed and $\epsilon^{\star}$ is unknown.

\subsection{Deviation bounds for estimation errors on $\hat{\mathcal{M}}$}
\label{SecConvOnMhat}

Bounds for types I and II estimation errors when learning on a random model with a sample independent of the one employed to compute such random model, may be obtained when there is a bound for them on each $\mathcal{M} \in \mathbb{C}(\mathcal{H})$ under the independent sample. This is the content of Theorem \ref{bound_constant}.

\begin{theorem}
	\label{bound_constant}	
	Fix a bounded loss function. Assume we are learning with an independent sample $\tilde{\mathcal{D}}_{M}$, and that for each $\epsilon > 0$ there exist sequences $\{B^{I}_{M,\epsilon}: M \geq 1\}$ and $\{B^{II}_{M,\epsilon}: M \geq 1\}$ of positive real-valued increasing functions with domain $\mathbb{Z}_{+}$ satisfying
	\begin{equation*}
		\lim\limits_{M \to \infty} B^{I}_{M,\epsilon}(k) = \lim\limits_{M \to \infty} B^{II}_{M,\epsilon}(k) = 0,
	\end{equation*}
	for all $\epsilon > 0$ and $k \in \mathbb{Z}_{+}$ fixed, such that
	\begin{align}
		\label{bound_theoremBC}
		\mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \right) \leq B^{I}_{M,\epsilon}(d_{VC}(\mathcal{M})) & \text{ and } &\\ \nonumber %\\ \nonumber
		\mathbb{P}\left(L(\hat{h}_{\mathcal{M}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{M}}) > \epsilon \right) \leq B^{II}_{M,\epsilon}(d_{VC}(\mathcal{M})), & & 
	\end{align}
	for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then, for any $\epsilon > 0$,
	\begin{align*}
		\textbf{(I)} \ \mathbb{P}&\left(\sup\limits_{h \in \mathcal{\hat{M}}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \right) \leq \mathbb{E}\Big[B^{I}_{M,\epsilon}(d_{VC}(\mathcal{\hat{M}}))\Big] \leq B^{I}_{M,\epsilon}\left(d_{VC}(\mathbb{C}(\mathcal{H}))\right)
	\end{align*}
	and
	\begin{align*}
		\textbf{(II)} \ \mathbb{P}\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{\hat{M}}}) > \epsilon \right) \leq \mathbb{E}\Big[B^{II}_{M,\epsilon}(d_{VC}(\mathcal{\hat{M}}))\Big] \leq B^{II}_{M,\epsilon}\left(d_{VC}(\mathbb{C}(\mathcal{H}))\right),
	\end{align*}
	in which the expectations are over all samples $\mathcal{D}_{N}$, from which $\hat{\mathcal{M}}$ is calculated. Since $d_{VC}(\mathbb{C}(\mathcal{H})) < \infty$, both probabilities above converge to zero when $M \to \infty$.
\end{theorem}

Our definition of $\mathcal{\hat{M}}$ ensures that it is going to have the smallest VC dimension under the constraint that it is a global minimum of $\mathbb{C}(\mathcal{H})$. As the quantities inside the expectations of Theorem \ref{bound_constant} are increasing functions of VC dimension, fixed $\epsilon$ and $M$, we tend to have smaller expectations, thus tighter bounds for types I and II estimation errors. Furthermore, it follows from Theorem \ref{bound_constant} that the sample complexity needed to learn on $\hat{\mathcal{M}}$ is at most that of $d_{VC}(\mathbb{C}(\mathcal{H}))$. This implies that this complexity is at most that of $\mathcal{H}$, but may be much lesser if $d_{VC}(\mathbb{C}(\mathcal{H})) \ll d_{VC}(\mathcal{H})$. We conclude that the bounds for the tail probabilities of types I and II estimation errors on $\hat{\mathcal{M}}$ are tighter than that on $\mathcal{H}$ (cf. Corollaries \ref{cor3TypeI} and \ref{cor1TypeII}), and the sample complexity needed to learn on $\hat{\mathcal{M}}$ is at most that of $\mathbb{C}(\mathcal{H})$, and not of $\mathcal{H}$. 

A bound for type III estimation error may be obtained using methods similar to that we employed to prove Theorem \ref{theorem_principal_convergence}. As in that theorem, the bound for type III estimation error depends on $\epsilon^{\star}$, on bounds for type I estimation error under each training and validation sample, and on $\mathbb{C}(\mathcal{H})$, more specifically, on its VC dimension and number of maximal elements. To ease notation, we denote $\epsilon \vee \epsilon^{\star} \coloneqq \max \{\epsilon,\epsilon^{\star}\}$ for any $\epsilon > 0$.

\begin{theorem}
	\label{theorem_tipeIII}
	Assume the premises of Theorem \ref{theorem_principal_convergence} are in force. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then, for any $\epsilon > 0$,
	\begin{align*}
		\textbf{(III)} &\ \mathbb{P}\left(L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) > \epsilon\right) \\
		&\leq m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[ B_{N,(\epsilon \vee \epsilon^{\star})/8}(d_{VC}(\mathbb{C}(\mathcal{H}))) + \hat{B}_{N,(\epsilon \vee \epsilon^{\star})/4}(d_{VC}(\mathbb{C}(\mathcal{H})))\right].
	\end{align*}
	In particular,
	\begin{align*}
		\lim_{N \rightarrow \infty} \mathbb{P}\left(L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) > \epsilon\right) = 0,
	\end{align*}
	for any $\epsilon > 0$.
\end{theorem}

On the one hand, by definition of $\epsilon^{\star}$, if $\epsilon < \epsilon^{\star}$, then type III estimation error is lesser than $\epsilon$ if, and only if, $L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})$, so this error is actually zero, and the result of Theorem \ref{theorem_principal_convergence} is a bound for type III estimation error in this case. On the other hand, if $\epsilon > \epsilon^{\star}$, one way of having type III estimation error lesser than $\epsilon$ is to have the estimated risk of each $\mathcal{M}$ at a distance at most $\epsilon/2$ from its out-of-sample risk and, as can be inferred from the proof of Theorem \ref{theorem_principal_convergence}, this can be accomplished if one has type I estimation error not greater than a fraction of $\epsilon$ under each training and validation sample considered, so a modification of Theorem \ref{theorem_principal_convergence} applies to this case.

Finally, as the tail probability of type IV estimation error may be bounded by the following inequality, involving the tail probabilities of types II and III estimation errors,
\begin{align}
	\label{triangle} \nonumber
	\textbf{(IV)} \ \mathbb{P}&\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}) > \epsilon\right)\\& \leq \mathbb{P}\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{\hat{M}}}) > \epsilon/2\right) + \mathbb{P}\left(L(h^{\star}_{\mathcal{\hat{M}}}) - L(h^{\star}) > \epsilon/2\right),
\end{align}
a bound for \eqref{triangle} is a direct consequence of Theorems \ref{bound_constant} and \ref{theorem_tipeIII}.

\begin{corollary}
	\label{cor_typeIV}
	Assume the premises of Theorems \ref{theorem_principal_convergence} and \ref{bound_constant} are in force. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then, for any $\epsilon > 0$,
	\begin{align*}
		&\textbf{(IV)} \ \mathbb{P}\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}) > \epsilon\right)\\
		& \leq \mathbb{E}\Big[B^{II}_{M,\epsilon/2}(d_{VC}(\mathcal{\hat{M}}))\Big] \\
		&+ m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[B_{N,(\epsilon/2 \vee \epsilon^{\star})/8}(d_{VC}(\mathbb{C}(\mathcal{H}))) + \hat{B}_{N,(\epsilon/2 \vee \epsilon^{\star})/4}(d_{VC}(\mathbb{C}(\mathcal{H})))\right]\\
		&\leq  B^{II}_{M,\epsilon/2}(d_{VC}(\mathbb{C}(\mathcal{H}))) \\
		&+ m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[B_{N,(\epsilon/2 \vee \epsilon^{\star})/8}(d_{VC}(\mathbb{C}(\mathcal{H}))) + \hat{B}_{N,(\epsilon/2 \vee \epsilon^{\star})/4}(d_{VC}(\mathbb{C}(\mathcal{H})))\right].
	\end{align*}
	In particular,
	\begin{align*}
		\lim_{\substack{N \rightarrow \infty \\ M \rightarrow \infty}} \mathbb{P}\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}) > \epsilon\right) = 0,
	\end{align*}
	for any $\epsilon > 0$.
\end{corollary}

\section{Learning via model selection with unbounded loss functions}
\label{SecUnbounded}

When the loss function is unbounded, we need to consider relative estimation errors and make assumptions about the tail weight of $P$. Heavy tail distributions are classically defined as those with a tail heavier than that of exponential distributions \cite{foss2011}. Nevertheless, in the context of learning, the tail weight of $P$ should take into account the loss function $\ell$. Hence, for $1 < p < \infty$ and a fixed hypotheses space $\mathcal{H}$, we measure the weight of the tails of distribution $P$ by
\begin{equation*}
	\tau_{p} \coloneqq \sup\limits_{h \in \mathcal{H}} \frac{\left(\int_{\mathcal{Z}} \ell^{p}(z,h) \ dP(z)\right)^{\frac{1}{p}}}{\int_{\mathcal{Z}} \ell(z,h) \ dP(z)} = \sup\limits_{h \in \mathcal{H}} \frac{L^{p}(h)}{L(h)},
\end{equation*}
in which $L^{p}(h) \coloneqq \left(\int_{\mathcal{Z}} \ell^{p}(z,h) \ dP(z)\right)^{\frac{1}{p}}$. We omit the dependence of $\tau_{p}$ on $\ell$, $P$ and $\mathcal{H}$ to simplify notation, since they will be clear from context. The weight of the tails of distribution $P$ may be defined based on $\tau_{p}$, as follows. Our presentation is analogous to \cite[Section~5.7]{vapnik1998} and is within the framework of \cite{cortes2019}.

\begin{definition}
	\label{def_tails1}
	We say that distribution $P$ on $\mathcal{H}$ under $\ell$ has:
	\begin{itemize}
		\item Light tails, if there exists a $p > 2$ such that $\tau_{p} < \infty$;
		\item Heavy tails, if there exists a $1 < p \leq 2$ such that $\tau_{p} < \infty$, but $\tau_{p} = \infty$ for all $p > 2$;
		\item Very heavy tails, if $\tau = \infty$ for all $p > 1$.
	\end{itemize}
\end{definition}

We assume that $P$ has at most heavy tails, which means there exists a $p > 1$, that can be lesser than 2, with
\begin{align}
	\label{tauStar}
	\tau_{p} < \tau^{\star} < \infty,
\end{align}
that is, $P$ is in a class of distributions for which bound \eqref{tauStar} holds. From now on, fix a $p > 1$ and a $\tau^{\star}$ such that \eqref{tauStar} holds.

Besides the constraint \eqref{tauStar}, we also assume that the loss function is greater or equal to one: $\ell(z,h) \geq 1$ for all $z \in \mathcal{Z}, h \in \mathcal{H}$. This is done to ease the presentation, and without loss of generality, since it is enough to sum 1 to any unbounded loss function to have this property and, in doing so, not only the minimizers of $L_{\mathcal{D}_{N}}$ and $L$ in each model in $\mathbb{C}(\mathcal{H})$ remain the same, but also $\epsilon^{\star}$ does not change. Hence, by summing one to the loss, the estimated model $\hat{\mathcal{M}}$ and learned hypotheses from it do not change, and the result of the model selection framework is the same. We refer to Remark \ref{remark_geq1} for the technical reason we choose to consider loss functions greater than one.

Finally, we assume that $\ell$ has a finite moment of order $p$, under $P$ and under the empirical measure, for all $h \in \mathcal{H}$. That is, defining\footnote{We elevate \eqref{LNp} to the $1/p$ power to be consistent with the theory presented in Appendix \ref{apVCtheory}.}
\begin{equation}
	\label{LNp}
	L_{\mathcal{D}_{N}}^{p}(h) \coloneqq \left(\frac{1}{N} \sum_{i=1}^{N} \ell^{p}(Z_{i},h)\right)^ {\frac{1}{p}},
\end{equation}
we assume that
\begin{align}
	\label{finite_moments_text}
	\sup\limits_{h \in \mathcal{H}} L_{\mathcal{D}_{N}}^{p}(h)  < \infty & & \text{ and } & & \sup\limits_{h \in \mathcal{H}} L^{p}(h) < \infty,
\end{align}
in which the first inequality should hold with probability one, for all possible samples $\mathcal{D}_{N}$. Since the moments $L^{p}(h)$ are increasing in $p$, \eqref{finite_moments_text} actually implies \eqref{tauStar}, so \eqref{finite_moments_text} is the non-trivial constraint in distribution $P$.

Although this is a deviation from the distribution-free framework, it is a mild constraint in distribution $P$. On the one hand, the condition on $L^{p}$ is usually satisfied for distributions observed in real data (see \cite[Section~5.7]{vapnik1998} for examples with Normal, Uniform, and Laplacian distributions under the quadratic loss function). On the other hand, the condition on $L_{\mathcal{D}_{N}}^{p}$ is more a feature of the loss function, than of the distribution $P$, and can be guaranteed if one excludes from $\mathcal{H}$ some hypotheses with arbitrarily large loss in a way that $h^{\star}$ and $d_{VC}(\mathcal{H})$ remain the same (see Lemma \ref{lemma_norm} and Remark \ref{remark_finite_moments} for more details).

When the loss function is unbounded, besides the constraints in the moments of $\ell$, under $P$ and the empirical measure, we also have to consider variants of the estimation errors. Instead of the estimation errors, we consider the relative estimation errors:
\begin{align*}
	&\textbf{(I)} \sup\limits_{h \in \hat{\mathcal{M}}} \ \frac{\text{\textbar} L(h) - L_{\tilde{\mathcal{D}}_{M}}(h) \text{\textbar}}{L(h)} & & \textbf{(II)} \ \frac{L(\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}}) - L(h^{\star}_{\hat{\mathcal{M}}})}{L(\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}})}\\
	&\textbf{(III)} \ \frac{L(h^{\star}_{\hat{\mathcal{M}}}) - L(h^{\star})}{L(h^{\star}_{\hat{\mathcal{M}}})} & & \textbf{(IV)} \ \frac{L(\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}}) - L(h^{\star})}{L(\hat{h}_{\hat{\mathcal{M}}}^{\mathbb{A}})}
\end{align*}
where algorithm $\mathbb{A}$ is dependent on the estimation technique once $\hat{\mathcal{M}}$ is selected.

In this section, we prove analogues of Theorems \ref{theorem_principal_convergence}, \ref{bound_constant} and \ref{theorem_tipeIII}. Before starting the study of the convergence of $\hat{\mathcal{M}}$ to $\mathcal{M}^{\star}$, we state a result analogous to Proposition \ref{propVC} about deviation bounds of relative type I and II estimation errors on $\mathcal{H}$, which are a consequence of Corollaries \ref{convergence_relativeTI} and \ref{cor2TypeII}. These are novel results of this paper which extend that of \cite{cortes2019}. 

\begin{proposition}
	\label{propVC2}
	Assume the loss function is unbounded and $P$ is such that \eqref{finite_moments_text} hold. Fixed a hypotheses space $\mathcal{H}$ with $d_{VC}(\mathcal{H}) < \infty$, there exist sequences $\{B^{I}_{N,\epsilon}: N \geq 1\}$ and $\{B^{II}_{N,\epsilon}: N \geq 1\}$ of positive real-valued increasing functions with domain $\mathbb{Z}_{+}$ satisfying
	\begin{equation*}
		\lim\limits_{N \to \infty} B^{I}_{N,\epsilon}(k) = \lim\limits_{N \to \infty} B^{II}_{N,\epsilon}(k) = 0,
	\end{equation*}
	for all $\epsilon > 0$ and $k \in \mathbb{Z}_{+}$ fixed, such that
	\begin{align*}
		\mathbb{P}&\left(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L_{\mathcal{D}_{N}}(h) - L(h)\text{\textbar}}{L(h)} > \epsilon\right) \leq B^{I}_{N,\epsilon}(d_{VC}(\mathcal{H})) \text{ and }\\
		\mathbb{P}&\left(\frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} > \epsilon\right) \leq B^{II}_{N,\epsilon}(d_{VC}(\mathcal{H})).
	\end{align*}
	Furthermore, the following holds:
	\begin{align*}
		\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L_{\mathcal{D}_{N}}(h) - L(h)\text{\textbar}}{L(h)} \xrightarrow[N \to \infty]{a.s.} 0 & & \text{ and } & & \frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} \xrightarrow[N \to \infty]{a.s.} 0.
	\end{align*}
\end{proposition}

The results of this section seek to obtain insights about the asymptotic behavior of learning via model selection in the case of unbounded loss functions, rather than obtain the tightest possible bounds. Hence, in some results, the simplicity of the bounds is preferred over its tightness, and tighter bounds may be readily obtained from the proofs.

\subsection{Convergence to the target model}

We start by showing a result similar to Theorem \ref{theorem_principal_convergence}.

\begin{theorem}
	\label{theorem_principal_convergence_unbounded}
	Assume the loss function is unbounded and $P$ is such that \eqref{finite_moments_text} hold. For each $\epsilon > 0$, let $\{B_{N,\epsilon}: N \geq 1\}$ and $\{\hat{B}_{N,\epsilon}: N \geq 1\}$ be sequences of positive real-valued increasing functions with domain $\mathbb{Z}_{+}$ satisfying
	\begin{equation*}
		\lim\limits_{N \to \infty} B_{N,\epsilon}(k) = \lim\limits_{N \to \infty} \hat{B}_{N,\epsilon}(k) = 0,
	\end{equation*}
	for all $\epsilon > 0$ and $k \in \mathbb{Z}_{+}$ fixed, and such that
	\begin{align*}
		\max_{j} \mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}^{(j)}}(h)\text{\textbar}}{L(h)} > \epsilon \right) \leq B_{N,\epsilon}(d_{VC}(\mathcal{M})) & \text{ and } &\\ \nonumber \\ \nonumber
		\max_{j} \mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \frac{\text{\textbar}L(h) - \hat{L}^{(j)}(h)\text{\textbar}}{L(h)} > \epsilon \right) \leq \hat{B}_{N,\epsilon}(d_{VC}(\mathcal{M})), & & 
	\end{align*}
	for all $\mathcal{M} \in \mathbb{C}(\mathcal{M})$, recalling that $L_{\mathcal{D}_{N}^{(j)}}$ and $\hat{L}^{(j)}$ represent the empirical risk under the $j$-th training and validation samples, respectively. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then,
	\begin{align*}
		%\label{bound_pM2}
		&\mathbb{P}\left(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star})\right) \leq 2 m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[\hat{B}_{N,\frac{\delta(1-\delta)}{2}}(d_{VC}(\mathbb{C}(\mathcal{H}))) + B_{N,\frac{\delta(1-\delta)}{4}}(d_{VC}(\mathbb{C}(\mathcal{H})))\right],
	\end{align*}
	in which $m$ is the number of pairs considered to calculate \eqref{form_Lhat} and 
	\begin{equation*}
		\delta \coloneqq \frac{\epsilon^{\star}}{2 \max\limits_{i \in \mathcal{J}} L(\mathcal{M}_{i})}.
	\end{equation*}
	Furthermore, if
	\begin{align}
		\label{as_conv2}
		\max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \max_{j} \sup\limits_{h \in \mathcal{M}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}^{(j)}}(h)\text{\textbar}}{L(h)} \xrightarrow[N \to \infty]{\text{a.s.}} 0 & \text{ and } &\\ \nonumber \\ \nonumber
		\max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \max_{j} \sup\limits_{h \in \mathcal{M}} \frac{\text{\textbar}L(h) - \hat{L}^{(j)}(h)\text{\textbar}}{L(h)} \xrightarrow[N \to \infty]{\text{a.s.}} 0, & & 
	\end{align}
	then
	\begin{equation*}
		\lim_{N \rightarrow \infty} \mathbb{P}\left(\hat{\mathcal{M}} = \mathcal{M}^{\star}\right) = 1.
	\end{equation*}
\end{theorem}

In this instance, a bound for $\mathbb{P}(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star}))$, and the almost sure convergence of $\hat{\mathcal{M}}$ to $\mathcal{M}^{\star}$, in the case of k-fold cross validation and independent validation sample, follow from Proposition \ref{propVC2} in a manner analogous to Theorem \ref{CVModelconvergence}. We state the almost sure convergence in Theorem \ref{CVModelconvergence2}, whose proof is analogous to that of Theorem \ref{CVModelconvergence}, and follows from Corollary \ref{convergence_relativeTI}. 

\begin{theorem}
	\label{CVModelconvergence2}
	Assume the loss function is unbounded and $P$ is such that \eqref{finite_moments_text} hold. If $\hat{L}$ is given by k-fold cross-validation or by an independent validation sample, then $\hat{\mathcal{M}}$ converges with probability one to $\mathcal{M}^{\star}$.
\end{theorem}

\subsection{Convergence of estimation errors on $\hat{\mathcal{M}}$}

The results stated here are rather similar to the case of bounded loss functions, with some minor modifications. Hence, we state the analogous results and present a proof only when it is different from the respective result in Section \ref{SecConvOnMhat}.

Bounds for relative types I and II estimation errors, when learning on a random model with a sample independent of the one employed to compute such random model, may be obtained as in Theorem \ref{bound_constant}. In fact, the proof of the following bounds are the same as in that theorem, with the respective changes from estimation errors to relative estimation errors. Hence, we state the results without a proof.

\begin{theorem}
	\label{bound_constant2}	
	Fix an unbounded loss function and assume $P$ is such that \eqref{finite_moments_text} hold. Assume we are learning with an independent sample $\tilde{\mathcal{D}}_{M}$, and that for each $\epsilon > 0$ there exist sequences $\{B^{I}_{M,\epsilon}: M \geq 1\}$ and $\{B^{II}_{M,\epsilon}: M \geq 1\}$ of positive real-valued increasing functions with domain $\mathbb{Z}_{+}$ satisfying
	\begin{equation*}
		\lim\limits_{M \to \infty} B^{I}_{M,\epsilon}(k) = \lim\limits_{M \to \infty} B^{II}_{M,\epsilon}(k) = 0,
	\end{equation*}
	for all $\epsilon > 0$ and $k \in \mathbb{Z}_{+}$ fixed, such that
	\begin{align*}
		\mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \frac{\text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h)\text{\textbar}}{L(h)}  > \epsilon \right) \leq B^{I}_{M,\epsilon}(d_{VC}(\mathcal{M})) & \text{ and } &\\
		\mathbb{P}\left(\frac{L(\hat{h}_{\mathcal{M}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{M}})}{L(\hat{h}_{\mathcal{M}}^{\tilde{\mathcal{D}}_{M}})} > \epsilon \right) \leq B^{II}_{M,\epsilon}(d_{VC}(\mathcal{M})), & & 
	\end{align*}
	for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then, for any $\epsilon > 0$,
	\begin{align*}
		\textbf{(I)} \ \mathbb{P}&\left(\sup\limits_{h \in \mathcal{\hat{M}}} \frac{\text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h)\text{\textbar}}{L(h)}  > \epsilon \right) \leq \mathbb{E}\Big[B^{I}_{M,\epsilon}(d_{VC}(\mathcal{\hat{M}}))\Big] \leq B^{I}_{M,\epsilon}\left(d_{VC}(\mathbb{C}(\mathcal{H}))\right)
	\end{align*}
	and
	\begin{align*}
		\textbf{(II)} \ \mathbb{P}\left(\frac{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{\hat{M}}})}{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}})} > \epsilon \right) \leq \mathbb{E}\Big[B^{II}_{M,\epsilon}(d_{VC}(\mathcal{\hat{M}}))\Big] \leq B^{II}_{M,\epsilon}\left(d_{VC}(\mathbb{C}(\mathcal{H}))\right),
	\end{align*}
	in which the expectations are over all samples $\mathcal{D}_{N}$, from which $\hat{\mathcal{M}}$ is calculated. Since $d_{VC}(\mathbb{C}(\mathcal{H})) < \infty$, both probabilities above converge to zero when $M \to \infty$.
\end{theorem}

The convergence to zero of relative type III estimation error may be obtained, as in Theorem \ref{theorem_tipeIII}, by the methods used to prove Theorem \ref{theorem_principal_convergence_unbounded}. We state and prove this result, since its proof is slightly different from that of Theorem \ref{theorem_tipeIII}.

\begin{theorem}
	\label{theorem_tipeIII2}
	Assume the premises of Theorem \ref{theorem_principal_convergence_unbounded} are in force. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then, for any $\epsilon > 0$,
	\begin{align*}
		\textbf{(III)} &\ \mathbb{P}\left(\frac{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star})}{L(h_{\hat{\mathcal{M}}}^{\star})} > \frac{\epsilon}{L(\mathcal{M}^{\star})}\right) \\
		& \leq 2m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[\hat{B}_{N,\frac{\delta^\prime(1-\delta^\prime)}{2}}(d_{VC}(\mathbb{C}(\mathcal{H}))) + B_{N,\frac{\delta^\prime(1-\delta^\prime)}{4}}(d_{VC}(\mathbb{C}(\mathcal{H})))\right]
	\end{align*}
	in which
	\begin{equation*}
		\delta^\prime \coloneqq \frac{\epsilon \vee \epsilon^{\star}}{2 \max\limits_{i \in \mathcal{J}} L(\mathcal{M}_{i})}.
	\end{equation*}
	In particular,
	\begin{align*}
		\lim_{N \rightarrow \infty} \mathbb{P}\left(\frac{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star})}{L(h_{\hat{\mathcal{M}}}^{\star})} > \epsilon\right) = 0,
	\end{align*}
	for any $\epsilon > 0$.
\end{theorem}

Finally, a bound on the rate of convergence of type IV estimation error to zero is a direct consequence of Theorems \ref{bound_constant2} and \ref{theorem_tipeIII2}, and the following inequality
\begin{align*}
	&\textbf{(IV)} \ \mathbb{P}\left(\frac{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star})}{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}})} > \frac{\epsilon}{L(\mathcal{M}^{\star})}\right)\\
	&\leq \mathbb{P}\left(\frac{L(\hat{h}_{\mathcal{\hat{\mathcal{M}}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\hat{\mathcal{M}}})}{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}})} > \frac{\epsilon}{2L(\mathcal{M}^{\star})}\right) + \mathbb{P}\left(\frac{L(h^{\star}_{\hat{\mathcal{M}}}) - L(h^{\star})}{L(h^{\star}_{\mathcal{\hat{\mathcal{M}}}})} > \frac{\epsilon}{2L(\mathcal{M}^{\star})}\right),
\end{align*}
which is true since $L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) \geq L(h^{\star}_{\mathcal{\hat{M}}})$.

\begin{corollary}
	\label{cor_typeIV2}
	Assume the premises of Theorems \ref{theorem_principal_convergence_unbounded} and \ref{bound_constant2} are in force. Let $\mathcal{\hat{M}} \in \mathbb{C}(\mathcal{H})$ be a random model learned by $\mathbb{M}_{\mathbb{C}(\mathcal{H})}$. Then, for any $\epsilon > 0$,
	\begin{align*}
		&\mathbb{P}\left(\frac{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star})}{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}})} > \frac{\epsilon}{L(\mathcal{M}^{\star})}\right)\\
		& \leq \mathbb{E}\Big[B^{II}_{M,\frac{\epsilon}{2L(\mathcal{M}^{\star})}}(d_{VC}(\mathcal{\hat{M}}))\Big] \\
		&+ 2m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[\hat{B}_{N,\frac{\delta^\prime(1-\delta^\prime)}{2}}(d_{VC}(\mathbb{C}(\mathcal{H}))) + B_{N,\frac{\delta^\prime(1-\delta^\prime)}{4}}(d_{VC}(\mathbb{C}(\mathcal{H})))\right]\\
		&\leq  B^{II}_{M,\frac{\epsilon}{2L(\mathcal{M}^{\star})}}(d_{VC}(\mathbb{C}(\mathcal{H}))) \\
		&+ 2m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[\hat{B}_{N,\frac{\delta^\prime(1-\delta^\prime)}{2}}(d_{VC}(\mathbb{C}(\mathcal{H}))) + B_{N,\frac{\delta^\prime(1-\delta^\prime)}{4}}(d_{VC}(\mathbb{C}(\mathcal{H})))\right]
	\end{align*}
	with
	\begin{equation*}
		\delta^\prime \coloneqq \frac{\epsilon/2 \vee \epsilon^{\star}}{2 \max\limits_{i \in \mathcal{J}} L(\mathcal{M}_{i})}.
	\end{equation*}
	In particular,
	\begin{align*}
		\lim_{\substack{N \rightarrow \infty \\ M \rightarrow \infty}} \mathbb{P}\left(\frac{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star})}{L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}})}  > \epsilon\right) = 0,
	\end{align*}
	for any $\epsilon > 0$.
\end{corollary}

\section{Enhancing generalization by learning via model selection}
\label{examplePartition}

In this section, we present an example illustrating that the bound for type IV estimation error may be tighter than the classical VC bound for type II estimation error in $\mathcal{H}$.

Let $\mathcal{X}$ be an arbitrary set with $\text{\textbar}\mathcal{X}\text{\textbar} < \infty$ and let $\mathcal{H} = \{h: \mathcal{X} \mapsto \{0,1\}\}$ be the set of all functions from $\mathcal{X}$ to $\{0,1\}$. Assuming $Z = (X,Y)$ and $\mathcal{Z} = \mathcal{X} \times \{0,1\}$, under the simple loss function $\ell((x,y),h) = \mathds{1}\{h(x) \neq y\}$, it follows that $d_{VC}(\mathcal{H}) = \text{\textbar}\mathcal{X}\text{\textbar}$ and $L(h) = \mathbb{P}(h(X) \neq Y)$ is the classification error.

Denote $\Pi = \{\pi: \pi \text{ is a partition of } \mathcal{H}\}$ as the set of all partitions of $\mathcal{X}$. A partition $\pi = \{p_{1},\dots,p_{k}\}$ is a collection of subsets of $\mathcal{X}$ such that
\begin{align*}
	\bigcup_{i=1}^{k} p_{i} = \mathcal{X} & & p_{i} \cap p_{i^\prime} = \emptyset \text{ if } i \neq i^\prime.
\end{align*}
Each partition $\pi \in \Pi$ creates an equivalence class in $\mathcal{X}$ with equivalence between points in a same block of partition $\pi$:
\begin{equation*}
	x \equiv_{\pi} y \iff \exists p \in \pi \text{ such that } \{x,y\} \subset p.
\end{equation*}
For each $\pi \in \Pi$, define the model
\begin{equation*}
	\mathcal{M}_{\pi} \coloneqq \left\{h \in \mathcal{H}: h(x) = h(y) \text{ if } x \equiv_{\pi} y\right\}
\end{equation*}
containing all functions that respect partition $\pi$, that is, classify points in a same block of $\pi$ in a same class. Observe that $d_{VC}(\mathcal{M}_{\pi}) = \text{\textbar}\pi\text{\textbar}$ for all $\pi \in \Pi$. We consider as candidate models
\begin{equation*}
	\mathbb{C}(\mathcal{H}) \coloneqq \left\{\mathcal{M}_{\pi}: \pi \in \Pi\right\},
\end{equation*}
the collection of models that respect each partition of $\mathcal{X}$. Since $\mathcal{H} \in \mathbb{C}(\mathcal{H})$, as $\mathcal{H} = \mathcal{M}_{\mathcal{X}}$ where $\mathcal{X}$ is the partition with singletons as blocks, $\mathbb{C}(\mathcal{H})$ covers $\mathcal{H}$. Furthermore, $d_{VC}(\mathbb{C}(\mathcal{H})) = d_{VC}(\mathcal{H})$. Figure \ref{partitionL} presents an example of $\Pi$ and of models $\mathcal{M}_{\pi}$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{reticulado.pdf}
	\caption{The set $\Pi$ of all partitions of $\mathcal{X} = \{1,2,3,4\}$. The tables present the hypotheses in selected models $\mathcal{M}_{\pi_{1}}, \mathcal{M}_{\pi_{2}}$. The partitions with at most two blocks, that generate $\mathbb{C}_{2}(\mathcal{H})$, are in orange. The edges represent the partial relation $\leq$ between partitions, defined as $\pi_{1} \leq \pi_{2} \iff \forall p \in \pi_{2}, \exists p^{\prime} \in \pi_{1} \text{ such that } p \subset p^{\prime}$. Under this partial relation, $(\Pi,\leq)$ is a complete lattice. Observe that, if $\pi_{1} \leq \pi_{2}$ then $\mathcal{M}_{\pi_{1}} \subset \mathcal{M}_{\pi_{2}}$.}
	\label{partitionL}
\end{figure}

Another possible family of candidate models is formed only by the models which respect a partition with at most two blocks:
\begin{equation*}
	\mathbb{C}_{2}(\mathcal{H}) \coloneqq \{\mathcal{M}_{\pi}: \pi \in \Pi, \text{\textbar}\pi\text{\textbar} \leq 2\}.
\end{equation*} 
These partitions are in orange in the example of Figure \ref{partitionL}. For each $h \in \mathcal{H}$ let 
\begin{equation*}
	\pi_{h} = \left\{\{x \in \mathcal{X}: h(x) = 0\},\{x \in \mathcal{X}: h(x) = 1\}\right\}	
\end{equation*}
be the partition of $\mathcal{X}$ generated by it, which has at most two blocks. Clearly, $h \in \mathcal{M}_{\pi_{h}}$ which is in $\mathbb{C}_{2}(\mathcal{H})$ for all $h \in \mathcal{H}$, what implies that $\mathbb{C}_{2}(\mathcal{H})$ covers $\mathcal{H}$. Furthermore, $d_{VC}(\mathbb{C}_{2}(\mathcal{H})) = 2$ by construction.

Observe that $d_{VC}(\mathcal{M}^{\star}) = 2$ since it is the class of models of form\footnote{Assuming that $h^{\star}$ is not constant. Otherwise, $d_{VC}(\mathcal{M}^{\star}) = 1$.} $\mathcal{M}_{h^{\star}}$. Hence, the target class of both $\mathbb{C}(\mathcal{H})$ and $\mathbb{C}_{2}(\mathcal{H})$ is the same. Actually, the MDE $\epsilon^{\star}$ is also the same in both collections.

We will illustrate an instance where the established bound for type IV estimation error when learning via model selection in $\mathbb{C}_{2}(\mathcal{H})$ is tighter than the respective bound for type II estimation error when learning in the whole $\mathcal{H}$ via empirical risk minimization. We also illustrate that this may still be the case when learning in $\mathbb{C}(\mathcal{H})$ when $\hat{\mathcal{M}} \approx \mathcal{M}^{\star}$.

To this end, we first compare the bounds for type IV estimation error on $\mathbb{C}_{2}(\mathcal{H})$ (cf. Corollary \ref{cor_typeIV}), with samples of size $N$ and $M = N$, when $\hat{L}$ is estimated with a validation sample, with classical VC theory bounds for type II estimation error when learning directly on $\mathcal{H}$ (cf. \eqref{GE2}), by considering the estimator $\hat{h}^{\mathcal{D}_{2N}}$ that is a hypothesis that minimizes the empirical risk in $\mathcal{H}$ of the whole sample of size $2N$. 

On the one hand, by Corollary \ref{cor1TypeII}, we have that
\begin{equation}
	\label{BT2}
	\mathbb{P}\left(L(\hat{h}^{\mathcal{D}_{2N}}) - L(h^{\star}) > \epsilon\right) \leq 8 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - 2N\frac{\epsilon^{2}}{128}\right\}.
\end{equation}
On the other hand, by Corollaries \ref{cor_typeIV}, \ref{cor2TypeI} and \ref{cor1TypeII}, when the independent sample has size $N$, the validation sample has size $cN$ and the training sample has size $(1-c)N$, with $0 < c < 1/2$, it follows that
\begin{align}
	\label{BT4} \nonumber
	\mathbb{P}&\left(L(\hat{h}_{\mathcal{\hat{M}}}(\tilde{\mathcal{D}}_{N})) - L(h^{\star}) > \epsilon\right) \leq 8 \exp\left\{2\left(1 + \ln \frac{N}{2}\right) - N\frac{\epsilon^{2}}{512}\right\} \\ \nonumber
	&+ 8\left(2^{d_{VC}(\mathcal{H}) - 1} - 1\right)\Bigg[\exp\left\{2\left(1 + \ln \frac{cN}{2}\right) - cN\frac{(\epsilon/2 \vee \epsilon^{\star})^{2}}{512}\right\} \\
	&+ \exp\left\{2\left(1 + \ln \frac{(1-c)N}{2}\right) - (1-c)N\frac{(\epsilon/2 \vee \epsilon^{\star})^{2}}{2048}\right\}\Bigg]  .
\end{align}

In Figure \ref{FigureL2}, we present, for selected values of $d_{VC}(\mathcal{H}), \epsilon$ and $\epsilon^{\star}$, the value of $N$ such that the bounds \eqref{BT2} and \eqref{BT4} are equal to $0.05$, considering $c = 0.2$. We first note that, in any case, this value of $N$ is of order at least $10^6$, as is often the case with the pessimistic distribution-free bounds of VC theory. Nevertheless, we see that the bound for type IV estimation error is actually tighter when $2\epsilon \leq \epsilon^{\star}$, meaning that, for $\epsilon$ small enough, one needs fewer samples to properly estimate $h^{\star}$ when learning via model selection on $\mathbb{C}_{2}(\mathcal{H})$, when compared with learning via empirical risk minimization on $\mathcal{H}$ directly.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{Learn_dVCLesser2.pdf}
	\caption{Sample size $N$ needed to have bounds \eqref{BT2} and \eqref{BT4} equal to $0.05$ as a function of $d_{VC}(\mathcal{H})$, for distinct values of $\epsilon^{\star}$ (columns) and $\epsilon$ (lines), and $c = 0.2$. The curves of type II bound \eqref{BT2} are in red, and the ones of type IV bound \eqref{BT4} are in green. When the red curve is below the green one, we have a tighter bound for type II estimation error when learning directly on $\mathcal{H}$ with a sample of size $2N$, while when the green curve is below the red one, we have a tighter bound for type IV estimation error when learning via model selection on $\mathbb{C}_{2}(\mathcal{H})$, with a training sample of size $0.8N$, a validation sample of size $0.2N$, and an independent sample of size $N$. To aid in the visualization, we painted the space between the two curves in green when the bound of type IV estimation error \eqref{BT4} is tighter, and in red when the bound of type II estimation error \eqref{BT2} is tighter.} \label{FigureL2}
\end{figure}

Now, by learning via model selection on $\mathbb{C}(\mathcal{H})$ with samples of size $N$ and $M = N$, when $\hat{L}$ is estimated with a validation sample, and assuming that $d_{VC}(\hat{\mathcal{M}}) \approx d_{VC}(\mathcal{M}^{\star}) = 2$, we have the following bound for type IV estimation error:
\begin{align}
	\label{BT4_2} \nonumber
	\mathbb{P}&\left(L(\hat{h}_{\mathcal{\hat{M}}}(\tilde{\mathcal{D}}_{N})) - L(h^{\star}) > \epsilon\right) \leq 8 \exp\left\{2\left(1 + \ln \frac{N}{2}\right) - N\frac{\epsilon^{2}}{512}\right\} \\ \nonumber
	&+ 8\Bigg[\exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{cN}{d_{VC}(\mathcal{H})}\right) - cN\frac{(\epsilon/2 \vee \epsilon^{\star})^{2}}{512}\right\} \\
	&+ \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{(1-c)N}{d_{VC}(\mathcal{H})}\right) - (1-c)N\frac{(\epsilon/2 \vee \epsilon^{\star})^{2}}{2048}\right\}\Bigg]  .
\end{align}

In Figure \ref{FigureL}, we present, for selected values of $d_{VC}(\mathcal{H}), \epsilon$ and $\epsilon^{\star}$, the value of $N$ such that the bounds \eqref{BT2} and \eqref{BT4_2} are equal to $0.05$, again considering $c = 0.2$. We see in this case that bound \eqref{BT4_2} may also be tighter than \eqref{BT2}, but $\epsilon$ should be much lesser than $\epsilon^{\star}$, for instance,  $5\epsilon < \epsilon^{\star}$. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{Learn_dVCLesser2.pdf}
	\caption{Sample size $N$ needed to have bounds \eqref{BT2} and \eqref{BT4_2} equal to $0.05$ as a function of $d_{VC}(\mathcal{H})$, for distinct values of $\epsilon^{\star}$ (columns) and $\epsilon$ (lines), and $c = 0.2$. The curves of type II bound \eqref{BT2} are in red, and the ones of type IV bound \eqref{BT4_2} are in green. When the red curve is below the green one, we have a tighter bound for type II estimation error when learning directly on $\mathcal{H}$ with a sample of size $2N$, while when the green curve is below the red one, we have a tighter bound for type IV estimation error when learning via model selection on $\mathbb{C}(\mathcal{H})$ with a training sample of size $0.8N$, a validation sample of size $0.2N$ and an independent sample of size $N$. To aid in the visualization, we painted the space between the two curves in green when the bound of type IV estimation error \eqref{BT4_2} is tighter, and in red when the bound of type II estimation error \eqref{BT2} is tighter.} \label{FigureL}
\end{figure}

\section{Discussion}
\label{FinalRemarks}

We presented learning via model selection with cross-validation risk estimation as a systematic data-driven framework consisting of selecting the simplest global minimum of a family of candidate models, and then learning a hypothesis on it with an independent sample, seeking to approximate a target hypothesis of $\mathcal{H}$. We studied the distribution-free asymptotics of such a framework by showing the convergence of the estimated model to the target one, and of the estimation errors to zero, for both bounded and unbounded loss functions. The case of bounded loss functions was treated with the usual tools of VC theory, while the case of unbounded loss functions required some new technical results, which are an extension of those in \cite{cortes2019} and were established in Appendix \ref{apVCtheory}.

We introduced the maximum discrimination error $\epsilon^{\star}$ and evidenced the possibility of better learning with a fixed sample size by properly modeling the family of candidate models, seeking to (a) have $\mathcal{M}^{\star}$ with small VC dimension and (b) have a great MDE $\epsilon^{\star}$. More interesting than the consistency, which is expected from a probabilistic perspective, is that the rate of the convergences, specially of type IV estimation error, evidence scenarios in which it may be better to learn via model selection than directly on $\mathcal{H}$ via ERM. 

From the results established, follow that the family of candidate models plays an important role on the rate of convergence of $\mathbb{P}(L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star}))$ to one, and of the estimation errors to zero, through $\mathfrak{m}(\mathbb{C}(\mathcal{H})), \epsilon^{\star}$ and $d_{VC}(\mathcal{M}^{\star})$. Moreover, these results also shed light on manners of improving the quality of the learning, i.e., decreasing the estimation errors, specially type IV, when the sample size is fixed. We now discuss some implications of the results of this paper.

First, since $\hat{\mathcal{M}}$ converges to $\mathcal{M}^{\star}$ with probability one, 
\begin{equation*}
	\mathbb{E}(G(\mathcal{\hat{M}})) \xrightarrow{N \rightarrow \infty} G(\mathcal{M}^{\star}),
\end{equation*}
by the Dominated Convergence Theorem, in which $G: \mathbb{C}(\mathcal{H}) \mapsto  \mathbb{R}$ is any real-valued function, since the domain of $G$ is finite. The convergence of $\mathbb{E}(G(\mathcal{\hat{M}}))$ ensures that the expectations of functions of $\hat{\mathcal{M}}$ on the right-hand side of inequalities in Theorems \ref{bound_constant} and \ref{bound_constant2} and Corollaries \ref{cor_typeIV} and \ref{cor_typeIV2} tend to the same functions evaluated at $\mathcal{M}^{\star}$, when $N$ tends to infinity. Hence, if one was able to isolate $h^{\star}$ within a model $\mathcal{M}^{\star}$ with small VC dimension, the bounds for types I, II and IV estimation errors will tend to be tighter for a sample of a given size $N + M$.

Second, if the MDE of $\mathbb{C}(\mathcal{H})$ under $P$ is great, then we need less precision when estimating $L(\mathcal{M})$ for $L(\hat{\mathcal{M}})$ to be equal to $L(\mathcal{M}^{\star})$, and for types III and IV estimation errors to be lesser than a $\epsilon \ll \epsilon^{\star}$ with high probability, so fewer samples are needed to learn a model as good as $\mathcal{M}^{\star}$ and to have lesser types III and IV estimation errors. Moreover, the sample complexity to learn this model is that of the most complex model in $\mathbb{C}(\mathcal{H})$, hence is at most the complexity of a model with VC dimension $d_{VC}(\mathbb{C}(\mathcal{H}))$, which may be lesser than that of $\mathcal{H}$. 

Since the bounds were established for a general hypotheses space with finite VC dimension in a distribution-free framework, they are not the tightest possible in specific cases, hence an interesting topic for future research would be to apply the methods used here to obtain tighter bounds for restricted classes of hypotheses spaces, candidate models and/or data generating distributions. The results of this paper may be extended when distribution-dependent bounds for types I and II estimation errors are available in the framework of Propositions \ref{propVC} and \ref{propVC2}. Observe that our results assume there exist bounds $B_{N,\epsilon}^{I}(\mathcal{M})$ and $B_{N,\epsilon}^{II}(\mathcal{M})$ for these estimation errors in each $\mathcal{M} \in \mathbb{C}(\mathcal{H})$. We assumed these bounds depended on $\mathcal{M}$ by means only of $d_{VC}(\mathcal{M})$, but the results may be extended if they depend on $\mathcal{M}$ through other complexity measure or if they are distribution-dependent. We leave the study of such extensions for future research. Moreover, when deducing the bounds, we have not made use of the dependence between the samples in different folds and just applied a union bound to consider each pair of samples separately. The bounds could be improved if this dependence was considered, but it would be necessary to restrict the study to a specific class of models, since such a result should not be possible in a general framework.

Furthermore, although outside the scope of this paper, the computational cost of computing $\hat{\mathcal{M}}$ by solving optimization problem \eqref{Ghat} should also be taken into consideration when choosing family $\mathbb{C}(\mathcal{H})$. This family of candidate models should have some structure that allows an efficient computation of \eqref{Ghat}, or the computation of a suboptimal solution with satisfactory practical results. In the case of variable selection, this could be performed via U-curve algorithms  \cite{u-curve1,u-curve2,u-curve3,ucurveParallel,reis2018}, that perform efficient searches on Boolean lattices, and an extension of such algorithms could aid the computation of \eqref{Ghat} when $\mathbb{C}(\mathcal{H})$ is a non-Boolean lattice. We leave this topic for future researches. 

Finally, the choice of learning with an independent sample on $\hat{\mathcal{M}}$ simplifies the bounds, but may not be practical in some instances since requires a sample \textit{great enough} to be divided into two. A interesting topic for future researches would be to consider other forms of learning on $\hat{\mathcal{M}}$ as, for example, by reusing the whole sample $\mathcal{D}_{N}$ to learn on $\hat{\mathcal{M}}$ by empirical risk minimization, what could work better with shortage of samples, but would be liable to biases.

\section{Proof of results}
\label{SecProof}

\subsection{Results of Section \ref{boundedL}}

\begin{proof}[\textbf{Proof of Proposition \ref{proposition_principal}}]
	If
	\begin{equation*}
		\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} < \epsilon^{\star}/2
	\end{equation*}
	then, for any $i \in \mathcal{J}$ such that $L(\mathcal{M}_{i}) > L(\mathcal{M}^{\star})$, we have
	\begin{align}
		\label{ineq11}
		\hat{L}(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}^{\star}) > L(\mathcal{M}_{i}) - L(\mathcal{M}^{\star}) - \epsilon^{\star} \geq 0,
	\end{align}
	in which the last inequality follows from the definition of $\epsilon^{\star}$. From \eqref{ineq11} follows that the global minimum of $\nicefrac{\mathbb{C}(\mathcal{H})}{\hat{\sim}}$ with the least VC dimension, that is $\hat{\mathcal{M}}$, is such that $L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})$. Indeed, from \eqref{ineq11} follows that $\hat{L}(\mathcal{M}) > \hat{L}(\mathcal{M}^{\star})$ for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$ such that $L(\mathcal{M}) > L(\mathcal{M}^{\star})$.
	
	Hence, since $\hat{L}(\hat{\mathcal{M}}) \leq \hat{L}(\mathcal{M}^{\star})$, we must have $L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})$. Therefore, we have the inclusion of events
	\begin{equation}
		\label{inclusion_star}
		\left\{\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} < \epsilon^{\star}/2\right\} \subset \left\{L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})\right\},
	\end{equation}
	which proves the result.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{theorem_principal_convergence}}]
	We will apply Proposition \ref{proposition_principal}. Denoting $\hat{h}_{i}^{(j)} \coloneqq \hat{h}_{\mathcal{M}_{i}}^{(j)}$,
	\begin{align}
		\label{dp1} \nonumber
		\mathbb{P}&\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} \geq \epsilon^{\star}/2\right) \leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sum_{j=1}^{m} \frac{1}{m} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/2\right)\\ \nonumber
		&\leq \mathbb{P}\left(\max_{j} \max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/2\right)\\ \nonumber
		&\leq \mathbb{P}\left(\bigcup_{j=1}^{m} \left\{\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/2\right\}\right)\\ \nonumber
		&\leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/2\right)\\ \nonumber
		&= \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - L(\hat{h}^{(j)}_{i}) + L(\hat{h}^{(j)}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/2\right)\\ \nonumber
		&\leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} L(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i}) + \max\limits_{i \in \mathcal{J}} \text{\textbar}L(\hat{h}^{(j)}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/2\right)\\ \nonumber
		&\leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} L(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i}) > \epsilon^{\star}/4\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\hat{h}^{(j)}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/4\right)\\
		&\leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} L(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i}) > \epsilon^{\star}/4\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup_{h \in \mathcal{M}_{i}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon^{\star}/4\right)
	\end{align}
	in which in the first inequality we applied the definition of $\hat{L}(\mathcal{M})$. For each $j$, the first probability in \eqref{dp1} is equal to
	\begin{align*}
		\mathbb{P}&\left(\max\limits_{i \in \mathcal{J}} L(\hat{h}^{(j)}_{i}) - L_{\mathcal{D}_{N}^{(j)}}(\hat{h}^{(j)}_{i}) + L_{\mathcal{D}_{N}^{(j)}}(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i}) > \epsilon^{\star}/4\right)\\
		&\leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} L(\hat{h}^{(j)}_{i}) - L_{\mathcal{D}_{N}^{(j)}}(\hat{h}^{(j)}_{i}) + L_{\mathcal{D}_{N}^{(j)}}(h^{\star}_{i}) - L(\mathcal{M}_{i}) > \epsilon^{\star}/4\right)\\
		&\leq \mathbb{P}\left(\left\{\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\hat{h}^{(j)}_{i}) - L_{\mathcal{D}_{N}^{(j)}}(\hat{h}^{(j)}_{i})\text{\textbar} > \epsilon^{\star}/8\right\} \bigcup \left\{\max\limits_{i \in \mathcal{J}} \text{\textbar} L_{\mathcal{D}_{N}^{(j)}}(h^{\star}_{i}) - L(\mathcal{M}_{i}) \text{\textbar} > \epsilon^{\star}/8\right\}\right)\\
		&\leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup_{h \in \mathcal{M}_{i}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon^{\star}/8\right),
	\end{align*}
	in which the first inequality follows from the fact that $L_{\mathcal{D}_{N}^{(j)}}(\hat{h}^{(j)}_{i}) \leq L_{\mathcal{D}_{N}^{(j)}}(h^{\star}_{i})$, and the last follows since $L(\mathcal{M}_{i}) = L(h^{\star}_{i})$. We conclude that
	\begin{align*}
		&\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} \geq \epsilon^{\star}/2\right) \\
		&\leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup_{h \in \mathcal{M}_{i}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon^{\star}/8\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup_{h \in \mathcal{M}_{i}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon^{\star}/4\right).
	\end{align*}	
	If $\mathcal{M}_{1} \subset \mathcal{M}_{2}$ then, for any $\epsilon > 0$ and $j = 1, \dots, m$, we have the following inclusion of events
	\begin{align*}
		&\left\{\sup_{h \in \mathcal{M}_{1}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon\right\} \subset \left\{\sup_{h \in \mathcal{M}_{2}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon\right\}\\
		&\left\{\sup_{h \in \mathcal{M}_{1}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon\right\} \subset \left\{\sup_{h \in \mathcal{M}_{2}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon\right\},
	\end{align*}
	hence it is true that
	\begin{align*}
		&\left\{\max\limits_{i \in \mathcal{J}} \sup_{h \in \mathcal{M}_{i}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon^{\star}/4\right\} \subset \left\{\max_{\mathcal{M} \in \text{ Max } \mathbb{C}(\mathcal{H})} \sup_{h \in \mathcal{M}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon^{\star}/4\right\}\\
		&\left\{\max\limits_{i \in \mathcal{J}} \sup_{h \in \mathcal{M}_{i}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon^{\star}/8\right\} \subset \left\{\max_{\mathcal{M} \in \text{ Max } \mathbb{C}(\mathcal{H})} \sup_{h \in \mathcal{M}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon^{\star}/8\right\},
	\end{align*}
	which yields
	\begin{align}
		\label{conlusion_cond} \nonumber
		\mathbb{P}&\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} \geq \epsilon^{\star}/2\right) \\  \nonumber
		&\leq \sum_{j=1}^{m} \sum_{\mathcal{M} \in \text{ Max } \mathbb{C}(\mathcal{H})} \mathbb{P}\left(\sup_{h \in \mathcal{M}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon^{\star}/8\right) + \mathbb{P}\left(\sup_{h \in \mathcal{M}} \text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar} > \epsilon^{\star}/4\right)\\ \nonumber
		&\leq m \sum_{\mathcal{M} \in \text{ Max } \mathbb{C}(\mathcal{H})} \left[B_{N,\epsilon^{\star}/8}(d_{VC}(\mathcal{M})) + \hat{B}_{N,\epsilon^{\star}/4}(d_{VC}(\mathcal{M}))\right]\\
		&\leq m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[B_{N,\epsilon^{\star}/8}(d_{VC}(\mathbb{C}(\mathcal{H}))) + \hat{B}_{N,\epsilon^{\star}/4}(d_{VC}(\mathbb{C}(\mathcal{H})))\right],
	\end{align}
	in which the last inequality follows from the fact that both $\hat{B}_{N,\epsilon^{\star}/4}$ and $B_{N,\epsilon^{\star}/8}$ are increasing functions, and $d_{VC}(\mathbb{C}(\mathcal{H})) = \max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} d_{VC}(\mathcal{M})$. The result follows from Proposition \ref{proposition_principal} since
	\begin{equation*}
		\{L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star})\} \subset \left\{\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} \geq \epsilon^{\star}/2\right\}.
	\end{equation*}	
	
	If the almost sure convergences \eqref{as_conv} hold, then
	\begin{equation}
		\label{as_proof}
		\hat{L}(\mathcal{M}) \xrightarrow[N \to \infty]{\text{a.s.}} L(\mathcal{M})
	\end{equation}
	for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$, since, if $L(h) = \hat{L}^{(j)}(h) = L_{\mathcal{D}_{N}}^{(j)}(h)$ for all $j = 1,\dots,m$ and $h \in \mathcal{H}$, then $\hat{L}(\mathcal{M}) = L(\mathcal{M})$ for all $\mathcal{M} \in \mathbb{C}(\mathcal{H})$. Observe that
	\begin{align}
		\label{incl}
		\left\{\max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \text{\textbar}L(\mathcal{M}) - \hat{L}(\mathcal{M})\text{\textbar} = 0\right\} \subset \left\{\hat{\mathcal{M}} = \mathcal{M}^{\star}\right\},
	\end{align}
	since, if the estimated risk $\hat{L}$ is equal to the out-of-sample risk $L$, then the definitions of $\hat{\mathcal{M}}$ and $\mathcal{M}^{\star}$ coincide. As the probability of the event on the left hand-side of \eqref{incl} converges to one if \eqref{as_proof} is true, we conclude that, if \eqref{as_conv} hold, then $\hat{\mathcal{M}}$ converges to $\mathcal{M}^{\star}$ with probability one.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{CVModelconvergence}}]
	We need to show that \eqref{as_conv} holds in these instances. For any $\epsilon > 0$, by Corollary \ref{cor3TypeI},	
	\begin{align*}
		&\mathbb{P}\left(\max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} \max_{j} \sup\limits_{h \in \mathcal{M}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon\right) \leq \sum_{j=1}^{m} \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L_{\mathcal{D}_{N}^{(j)}}(h) - L(h)\text{\textbar} > \epsilon\right)\\
		&\leq m \ 8 \exp\left\{d_{VC}(\mathcal{H}) \left(1 + \ln \frac{N_{j}}{d_{VC}(\mathcal{H})} - N_{j}\frac{\epsilon^{2}}{32C^{2}}\right)\right\}
	\end{align*}
	in which $N_{j}$ is the size of the $j$-th training sample. By the inequality above, and Borel-Cantelli Lemma \cite[Theorem~4.3]{billingsley2008}, the first convergence in \eqref{as_conv} holds. The second convergence holds since the inequality above is also true, but with $L_{\mathcal{D}_{N}^{(j)}}$ and $N_{j}$ interchanged by $\hat{L}^{(j)}$ and $\hat{N}_{j}$, the empirical risk and size of the $j$-th validation sample.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{bound_constant}}]
	We first note that
	\begin{align}
		\label{Sum1} \nonumber
		\mathbb{P}&\left(\sup\limits_{h \in \mathcal{\hat{M}}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \right)  = \mathbb{E} \Bigg(\mathbb{P}\left(\sup\limits_{h \in \mathcal{\hat{M}}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \text{\textbar}\mathcal{\hat{M}}\right)\Bigg)\\ \nonumber
		& = \sum_{i \in \mathcal{J}} \mathbb{P}\left(\sup\limits_{h \in \mathcal{\hat{M}}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \text{\textbar}\mathcal{\hat{M}} = \mathcal{M}_{i}\right) \mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}_{i})\\
		& = \sum_{i \in \mathcal{J}} \mathbb{P}\left(\sup\limits_{h \in \mathcal{M}_{i}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \text{\textbar}\mathcal{\hat{M}} = \mathcal{M}_{i}\right) \mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}_{i}).
	\end{align}
	Fix $\mathcal{M} \in \mathbb{C}(\mathcal{H})$ with $\mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}) > 0$. We claim that
	\begin{align*}
		%\label{cond_independence}
		\mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \text{\textbar}\mathcal{\hat{M}} = \mathcal{M}\right) = \mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon\right).
	\end{align*}
	Indeed, since $\tilde{\mathcal{D}}_{M}$ is independent of $\mathcal{D}_{N}$, the event 
	\begin{equation*}
		\left\{\sup_{h \in \mathcal{M}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon\right\}
	\end{equation*}
	is independent of $\{\mathcal{\hat{M}} = \mathcal{M}\}$, as the former depends solely on $\tilde{\mathcal{D}}_{M}$, and the latter solely on $\mathcal{D}_{N}$. Hence, by applying bound $\eqref{bound_theoremBC}$ to each positive probability in the sum \eqref{Sum1}, we obtain that
	\begin{align*}
		\mathbb{P}\left(\sup\limits_{h \in \mathcal{\hat{M}}} \text{\textbar}L_{\tilde{\mathcal{D}}_{M}}(h) - L(h) \text{\textbar} > \epsilon \right) & \leq \sum_{i \in \mathcal{J}} B_{M,\epsilon}^{I}(d_{VC}(\mathcal{M}_{i}))  \mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}_{i})\\
		& = \mathbb{E} \left(B_{M,\epsilon}^{I}(d_{VC}(\mathcal{\hat{M}}))\right) \leq B_{N,\epsilon}^{I}(d_{VC}(\mathbb{C}(\mathcal{H}))),
	\end{align*}
	as desired, in which the last inequality follows from the fact that $B_{M,\epsilon}^{I}$ is an increasing function and $d_{VC}(\mathbb{C}(\mathcal{H})) = \max_{\mathcal{M} \in \mathbb{C}(\mathcal{H})} d_{VC}(\mathcal{M})$.
	
	The bound for type II estimation error may be obtained similarly, since
	\begin{align*}
		\mathbb{P}&\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{\hat{M}}}) > \epsilon \right) = \mathbb{E} \Bigg(\mathbb{P}\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{\hat{M}}}) > \epsilon \text{\textbar} \mathcal{\hat{M}} \right)\Bigg)\\
		& = \sum_{i \in \mathcal{J}} \mathbb{P}\left(L(\hat{h}_{\mathcal{\hat{M}}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{\hat{M}}}) > \epsilon \text{\textbar} \mathcal{\hat{M}} = \mathcal{M}_{i} \right) \mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}_{i})\\
		& = \sum_{i \in \mathcal{J}} \mathbb{P}\left(L(\hat{h}_{\mathcal{M}_{i}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{M}_{i}}) > \epsilon \text{\textbar} \mathcal{\hat{M}} = \mathcal{M}_{i} \right) \mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}_{i})\\
		& = \sum_{i \in \mathcal{J}} \mathbb{P}\left(L(\hat{h}_{\mathcal{M}_{i}}^{\tilde{\mathcal{D}}_{M}}) - L(h^{\star}_{\mathcal{M}_{i}}) > \epsilon \right) \mathbb{P}(\mathcal{\hat{M}} = \mathcal{M}_{i}),
	\end{align*}
	and $B^{II}_{M,\epsilon}(d_{VC}(\mathcal{M}_{i}))$ is a bound for the probabilities inside the sum by \eqref{bound_theoremBC}. The assertion that types I and II estimation errors are asymptotically zero when $d_{VC}(\mathbb{C}(\mathcal{H})) < \infty$ is immediate from the established bounds.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{theorem_tipeIII}}]
	We first show that
	\begin{align}
		\label{lemma_inside}
		\mathbb{P}\left(L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) > \epsilon\right) \leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})\text{\textbar} > (\epsilon \vee \epsilon^{\star})/2 \right).
	\end{align}
	If $\epsilon \leq \epsilon^{\star}$ then, by the inclusion of events \eqref{inclusion_star} in the proof of Proposition \ref{proposition_principal}, we have that
	\begin{align}
		\label{incl1}
		\left\{\max\limits_{i \in \mathcal{J}} \text{\textbar}\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})\text{\textbar} < (\epsilon \vee \epsilon^{\star})/2\right\} \subset \left\{L(\hat{\mathcal{M}}) = L(\mathcal{M}^{\star})\right\} \subset \left\{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) < \epsilon\right\},
	\end{align}
	since $L(h^{\star}_{\hat{\mathcal{M}}}) = L(\hat{\mathcal{M}})$ and $L(h^{\star}_{\mathcal{M}^{\star}}) = L(\mathcal{M}^{\star})$, so \eqref{lemma_inside} follows in this case.
	
	Now, if $\epsilon > \epsilon^{\star}$ and $\max\limits_{i \in \mathcal{J}} \text{\textbar}\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})\text{\textbar} < \epsilon/2$, then
	\begin{align*}
		L(\hat{\mathcal{M}}) - L(\mathcal{M}^{\star}) &= [L(\hat{\mathcal{M}}) - \hat{L}(\mathcal{M}^{\star})] - [L(\mathcal{M}^{\star}) - \hat{L}(\mathcal{M}^{\star})]\\
		&\leq [L(\hat{\mathcal{M}}) - \hat{L}(\hat{\mathcal{M}})] - [L(\mathcal{M}^{\star}) - \hat{L}(\mathcal{M}^{\star})]\\
		&\leq \epsilon/2 + \epsilon/2 = \epsilon,
	\end{align*}
	in which the first inequality follows from the fact that the minimum of $\hat{L}$ is attained at $\hat{\mathcal{M}}$, and the last inequality follows from $\max\limits_{i \in \mathcal{J}} \text{\textbar}\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})\text{\textbar} < \epsilon/2$. Since $L(\hat{\mathcal{M}}) - L(\mathcal{M}^{\star}) = L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star})$, we also have the inclusion of events
	\begin{align}
		\label{incl2}
		\left\{\max\limits_{i \in \mathcal{J}} \text{\textbar}\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})\text{\textbar} < (\epsilon \vee \epsilon^{\star})/2\right\} \subset \left\{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) < \epsilon\right\},
	\end{align}
	when $\epsilon > \epsilon^{\star}$. From \eqref{incl1} and \eqref{incl2} follows \eqref{lemma_inside}, as desired.
	
	Substituting $\epsilon^{\star}$ by $\epsilon \vee \epsilon^{\star}$ in \eqref{conlusion_cond} we obtain
	\begin{align}
		\label{conclusion2} \nonumber
		\mathbb{P}&\left(\max\limits_{i \in \mathcal{J}} \text{\textbar}L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})\text{\textbar} \geq (\epsilon \vee \epsilon^{\star})/2\right) \leq\\
		& m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[B_{N,(\epsilon \vee \epsilon^{\star})/8}(d_{VC}(\mathbb{C}(\mathcal{H}))) + \hat{B}_{N,(\epsilon \vee \epsilon^{\star})/4}(d_{VC}(\mathbb{C}(\mathcal{H})))\right].
	\end{align}
	The result follows combining \eqref{lemma_inside} and \eqref{conclusion2}.	
\end{proof}

\subsection{Results of Section \ref{SecUnbounded}}

\begin{proof}[\textbf{Proof of Theorem \ref{theorem_principal_convergence_unbounded}}]
	We claim that
	\begin{equation}
		\label{implication1}
		1 - \delta < \frac{\hat{L}(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} < 1 + \delta, \ \forall i \in \mathcal{J} \implies \max\limits_{i \in \mathcal{J}} \ \text{\textbar}\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})\text{\textbar} < \frac{\epsilon^{\star}}{2}.
	\end{equation}
	Indeed, the left-hand side of \eqref{implication1} implies
	\begin{equation*}
		\begin{cases}
			L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i}) < \frac{\epsilon^{\star} L(\mathcal{M}_{i})}{2 \max\limits_{i \in \mathcal{J}} L(\mathcal{M}_{i})} < \frac{\epsilon^{\star}}{2}\\
			\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i}) < \frac{\epsilon^{\star} L(\mathcal{M}_{i})}{2 \max\limits_{i \in \mathcal{J}} L(\mathcal{M}_{i})} < \frac{\epsilon^{\star}}{2}\\
		\end{cases} \ \forall i \in \mathcal{J},
	\end{equation*}
	as desired. In particular, it follows from inclusion \eqref{inclusion_star} in the proof of Proposition \ref{proposition_principal} that
	\begin{equation}
		\label{writeP}
		\mathbb{P}\left(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star})\right) \leq \mathbb{P}\left(\min\limits_{i \in \mathcal{J}} \frac{\hat{L}(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \leq 1 - \delta\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \frac{\hat{L}(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \geq 1 + \delta\right)
	\end{equation}
	hence it is enough to bound both probabilities on the right-hand side of the expression above.
	
	The first probability in \eqref{writeP} may be written as
	\begin{align}	
		\label{writeP2}
		\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{L(\mathcal{M}_{i}) - \hat{L}(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \geq \delta\right) \leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{L(\mathcal{M}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})}{L(\mathcal{M}_{i})} \geq \delta\right),
	\end{align}
	in which the inequality follows from a union bound. Since $x \mapsto  \frac{x - \alpha}{x}$ is increasing, and $L(\mathcal{M}_{i}) \leq L(\hat{h}^{(j)}_{i})$ for every $j = 1,\dots,m$, each probability in \eqref{writeP2} is bounded by
	\begin{equation}
		\label{res1}
		\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{L(\hat{h}^{(j)}_{i}) - \hat{L}^{(j)}(\hat{h}^{(j)}_{i})}{L(\hat{h}^{(j)}_{i})} \geq \delta\right) \leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup\limits_{h \in \mathcal{M}_{i}} \ \frac{\text{\textbar}L(h) - \hat{L}^{(j)}(h)\text{\textbar}}{L(h)} \geq \delta\right).
	\end{equation}
	
	We turn to the second probability in \eqref{writeP} which can be written as
	\begin{equation}
		\label{writeP3}
		\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}(\mathcal{M}_{i}) - L(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \geq \delta\right) \leq \sum_{j=1}^{m} \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \geq \delta\right),
	\end{equation}
	in which again the inequality follows from a union bound. In order to bound each probability in \eqref{writeP3} we intersect its event with
	\begin{equation*}
		\max\limits_{i \in \mathcal{J}} \frac{L(\hat{h}_{i}^{(j)})}{L(\mathcal{M}_{i})} \leq \frac{1}{1 - \delta} \iff \max\limits_{i \in \mathcal{J}} \frac{L(\hat{h}_{i}^{(j)}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \leq \delta,
	\end{equation*}
	and its complement, to obtain
	\begin{align}
		\label{res2} \nonumber
		&\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \geq \delta\right) \leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \frac{L(\hat{h}_{i}^{(j)}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \delta\right)\\ \nonumber
		&+ \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \left(\frac{L(\hat{h}_{i}^{(j)})}{L(\mathcal{M}_{i})}\right) \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \delta,\max\limits_{i \in \mathcal{J}} \frac{L(\hat{h}_{i}^{(j)})}{L(\mathcal{M}_{i})} \leq \frac{1}{1 - \delta}\right)\\ \nonumber
		&\leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \frac{L(\hat{h}_{i}^{(j)}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \delta\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \delta(1-\delta)\right)\\
		&\leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup\limits_{h \in \mathcal{M}_{i}} \frac{\text{\textbar}L_{\mathcal{D}_{N}}^{(j)}(h) - L(h)\text{\textbar}}{L(h)} \geq \frac{\delta}{2}\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \delta(1-\delta)\right)
	\end{align}
	in which the last inequality follows from Lemma \ref{lemmaTypeItoII}.
	
	It remains to bound the second probability in \eqref{res2}. We have that it is equal to
	\begin{align}
		\label{res3} \nonumber
		&\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\hat{h}^{(j)}_{i}) + L(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \delta(1-\delta)\right)\\ \nonumber
		&\leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{\hat{L}^{(j)}(\hat{h}^{(j)}_{i}) - L(\hat{h}^{(j)}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \frac{\delta(1-\delta)}{2}\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \ \frac{L(\hat{h}^{(j)}_{i}) - L(\mathcal{M}_{i})}{L(\hat{h}_{i}^{(j)})} \geq \frac{\delta(1-\delta)}{2}\right)\\
		&\leq \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup\limits_{h \in \mathcal{M}_{i}} \ \frac{\text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar}}{L(h)} \geq \frac{\delta(1-\delta)}{2}\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup\limits_{h \in \mathcal{M}_{i}} \ \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} \geq \frac{\delta(1-\delta)}{4}\right),
	\end{align}
	in which the last inequality follows again from Lemma \ref{lemmaTypeItoII}. From (\ref{writeP}-\ref{res3}), follows that
	\begin{align*}
		\mathbb{P}\left(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star})\right)\leq &2 \sum_{j=1}^{m} \Bigg[\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup\limits_{h \in \mathcal{M}_{i}} \ \frac{\text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar}}{L(h)} \geq \frac{\delta(1-\delta)}{2}\right) +\\
		&\mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \sup\limits_{h \in \mathcal{M}_{i}} \ \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} \geq \frac{\delta(1-\delta)}{4}\right)\Bigg]\\
		&\leq 2 \sum_{j=1}^{m} \sum_{\mathcal{M} \in \text{ Max } \mathbb{C}(\mathcal{H})} \Bigg[\mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \ \frac{\text{\textbar}\hat{L}^{(j)}(h) - L(h)\text{\textbar}}{L(h)} \geq \frac{\delta(1-\delta)}{2}\right) +\\
		&\mathbb{P}\left(\sup\limits_{h \in \mathcal{M}} \ \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} \geq \frac{\delta(1-\delta)}{4}\right)\Bigg],
	\end{align*}
	in which the inequality holds by the same arguments as in \eqref{conlusion_cond}, hence
	\begin{equation*}
		\mathbb{P}\left(L(\hat{\mathcal{M}}) \neq L(\mathcal{M}^{\star})\right) \leq 2m \ \mathfrak{m}(\mathbb{C}(\mathcal{H})) \left[\hat{B}_{N,\frac{\delta(1-\delta)}{2}}(d_{VC}(\mathbb{C}(\mathcal{H}))) + B_{N,\frac{\delta(1-\delta)}{4}}(d_{VC}(\mathbb{C}(\mathcal{H})))\right].
	\end{equation*}
	
	If the almost sure convergences \eqref{as_conv2} hold, then $L(h) = L_{\mathcal{D}_{N}}^{(j)}(h) = \hat{L}^{(j)}(h)$ for all $j$ and $h \in \mathcal{H}$, and the definitions of $\mathcal{M}^{\star}$ and $\hat{\mathcal{M}}$ coincide.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{theorem_tipeIII2}}]
	We show that
	\begin{align}
		\label{implication2}
		\mathbb{P}\left(L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) > \epsilon\right) \geq \mathbb{P}\left(\frac{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star})}{L(h_{\hat{\mathcal{M}}}^{\star})} > \frac{\epsilon}{L(\mathcal{M}^{\star})}\right),
	\end{align}
	so from \eqref{lemma_inside} and \eqref{implication1} will follow that
	\begin{equation*}
		\mathbb{P}\left(\frac{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star})}{L(h_{\hat{\mathcal{M}}}^{\star})} > \frac{\epsilon}{L(\mathcal{M}^{\star})}\right) \leq \mathbb{P}\left(\min\limits_{i \in \mathcal{J}} \frac{\hat{L}(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \leq 1 - \delta^\prime\right) + \mathbb{P}\left(\max\limits_{i \in \mathcal{J}} \frac{\hat{L}(\mathcal{M}_{i})}{L(\mathcal{M}_{i})} \geq 1 + \delta^\prime\right),
	\end{equation*}
	and the result is then direct from the proof of Theorem \ref{theorem_principal_convergence_unbounded}. But \eqref{implication2} is clearly true since
	\begin{align*}
		\frac{L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star})}{L(h_{\hat{\mathcal{M}}}^{\star})} > \frac{\epsilon}{L(\mathcal{M}^{\star})} \implies L(h_{\hat{\mathcal{M}}}^{\star}) - L(h^{\star}) > \epsilon \frac{L(h_{\hat{\mathcal{M}}}^{\star})}{L(\mathcal{M}^{\star})} \geq \epsilon.
	\end{align*}		
\end{proof}

\FloatBarrier

\section*{Acknowledgments}

D. Marcondes was funded by grant \#2022/06211-2, São Paulo Research Foundation (FAPESP), during the writing of this paper.

\appendix
	\section{Vapnik-Chervonenkis theory}
	\label{apVCtheory}
	
	In this appendix, we present the main ideas and results of classical Vapnik-Chervonenkis (VC) theory, the stone upon which the results in this paper are built. The presentation of the theory is a simplified merge of \cite{vapnik1998}, \cite{vapnik2000}, \cite{devroye1996} and \cite{cortes2019}, where the simplicity of the arguments is preferred over the refinement of the bounds. Hence, we present results which support those in this paper and outline the main ideas of VC theory, even though are not the tightest available bounds. We omit the proofs, and note that refined versions of the results presented here may be found at one or more of the references.
	
	This appendix is a review of VC theory, except for novel results presented in Section \ref{ApUnbounded} for the case of unbounded loss functions, where we obtain new bounds for relative type I estimation error by extending the results in \cite{cortes2019}. We start defining the shatter coefficient and VC dimension of a hypotheses space under loss function $\ell$.
	
	\begin{definition}[Shatter coefficient]
		\label{shatter} 
		Let $\mathcal{G} = \{I: \mathcal{Z} \mapsto  \{0,1\}\}$ be a set of binary functions with domain $\mathcal{Z}$. The $N$-shatter coefficient of $\mathcal{G}$ is defined as
		\begin{equation*}
			S(\mathcal{G},N) = \max\limits_{(z_{1},\dots,z_{N}) \in \mathcal{Z}^{N}} \text{\textbar}\big\{\big(I(z_{1}),\dots,I(z_{N})\big): I \in \mathcal{G}\big\}\text{\textbar},
		\end{equation*}
		for $N \in \mathbb{Z}_{+}$, in which $\text{\textbar}\cdot\text{\textbar}$ is the cardinality of a set.
	\end{definition}
	
	\begin{definition}[Vapnik-Chervonenkis dimension]
		\label{VCdimension} 
		Fixed a hypotheses space $\mathcal{H}$ and a loss function $\ell$, set
		\begin{align*}
			C = \sup\limits_{\substack{z \in \mathcal{Z} \\ h \in \mathcal{H}}} \ell(z,h),
		\end{align*}
		in which $C$ can be infinity. Consider, for each $h \in \mathcal{H}$ and $\beta \in (0,C)$, the binary function $I(z;h,\beta) = \mathds{1}\{\ell(z,h) \geq \beta\}$, for $z \in \mathcal{Z}$, and denote
		\begin{align*}
			\mathcal{G}_{\mathcal{H},\ell} = \Big\{I(\cdot;h,\beta): h \in \mathcal{H}, \beta \in (0,C)\Big\}.
		\end{align*}
		We define the shatter coefficient of $\mathcal{H}$ under loss function $\ell$ as
		\begin{equation*}
			S(\mathcal{H},\ell,N) \coloneqq S(\mathcal{G}_{\mathcal{H},\ell},N).
		\end{equation*}
		The Vapnik-Chervonenkis (VC) dimension of $\mathcal{H}$ under loss function $\ell$ is the greatest integer $k \geq 1$ such that $S(\mathcal{H},\ell,k) = 2^{k}$, and is denoted by $d_{VC}(\mathcal{H},\ell)$. If $S(\mathcal{H},\ell,k) = 2^{k}$, for all integer $k \geq 1$, we denote $d_{VC}(\mathcal{H},\ell) = \infty$.
	\end{definition} 
	
	\begin{remark}
		If there is no confusion about which loss function we are referring, or when it is not of importance to our argument, we omit $\ell$ and denote the shatter coefficient and VC dimension simply by $S(\mathcal{H},N)$ and $d_{VC}(\mathcal{H})$. We note that if the hypotheses in $\mathcal{H}$ are binary valued functions and $\ell$ is the simple loss function $\ell((x,y),h) = \mathds{1}\{h(x) \neq y\}$, then $\mathcal{H} = \mathcal{G}_{\mathcal{H},\ell}$, and its $N$-th shatter coefficient is actually the maximum number of dichotomies that can be generated by the functions in $\mathcal{H}$ with $N$ points.
	\end{remark}
	
	\subsection{Generalized Glivenko-Cantelli Problems}
	
	The main results of VC theory are based on a generalization of the Glivenko-Cantelli Theorem, which can be stated as follows. Recall that $\mathcal{D}_{N} = \{Z_{1},\dots,Z_{N}\}$ is a sequence of independent random vectors with a same distribution $P(z) \coloneqq \mathbb{P}(Z \leq z)$, for $z \in \mathcal{Z} \subset \mathbb{R}^{d}$, defined in a probability space $(\Omega,\mathcal{S},\mathbb{P})$. 
	
	In order to easy notation, we assume, without loss of generality, that $\Omega = \mathbb{R}^{d}$, $\mathcal{S}$ is the Borel $\sigma$-algebra of $\mathbb{R}^{d}$, the random vector $Z$ is the identity $Z(\omega) = \omega$, for $\omega \in \Omega$, and $\mathbb{P}$ is the unique probability measure such that $\mathbb{P}(\{\omega:\omega \leq z\}) = P(z)$, for all $z \in \mathbb{R}^{d}$. Define
	\begin{align*}
		P_{\mathcal{D}_{N}}(z) \coloneqq \frac{1}{N} \sum_{i=1}^{N} \mathds{1}\{Z_{i} \leq z\}, & & z \in \mathcal{Z}
	\end{align*}
	as the empirical distribution of $Z$ under sample $\mathcal{D}_{N}$.
	
	The assertion of the theorem below is that of \cite[Theorem~12.4]{devroye1996}. Its bottom line is that the empirical distribution of random variables converges uniformly to $P$ with probability one.
	
	\begin{theorem}[Glivenko-Cantelli Theorem]
		\label{glivenko_cantelli}
		Assume $d = 1$ and $\mathcal{Z} = \mathbb{R}$. Then, for a fixed $\epsilon > 0$ and $N$ great enough,
		\begin{equation}
			\label{GCbound}
			\mathbb{P}\left(\sup\limits_{z \in \mathbb{R}} \text{\textbar}P(z) - P_{\mathcal{D}_{N}}(z)\text{\textbar} > \epsilon\right) \leq 8(N+1) \exp\left\{-N \frac{\epsilon^2}{32}\right\}.
		\end{equation}
		Applying Borel-Cantelli Lemma \cite[Theorem~4.3]{billingsley2008} to \eqref{GCbound} yields
		\begin{equation*}
			\lim\limits_{N \to \infty} \sup\limits_{z \in \mathbb{R}} \text{\textbar}P(z) - P_{\mathcal{D}_{N}}(z)\text{\textbar} = 0 \text{ with probability one.}
		\end{equation*}
		In other words, $P_{\mathcal{D}_{N}}$ converges uniformly almost surely to $P$.
	\end{theorem}
	
	Theorem \ref{glivenko_cantelli} has the flavor of VC theory results: a rate of uniform convergence of the empirical probability of a class of events to their real probability, which implies the almost sure convergence. Indeed, letting $\mathcal{S}^{\star} \subset \mathcal{S}$ be a class of events, that is not necessarily a $\sigma$-algebra, and denoting
	\begin{equation*}
		\mathbb{P}_{\mathcal{D}_{N}}(A) = \frac{1}{N} \sum_{i=1}^{N} \mathds{1}\{Z_{i} \in A\},
	\end{equation*}
	as the empirical probability of event $A \in \mathcal{S}$ under sample $\mathcal{D}_{N}$, the probability in \eqref{GCbound} can be rewritten as
	\begin{equation}
		\label{partial_convergence}
		\mathbb{P}\left(\sup\limits_{A \in \mathcal{S}^{\star}} \text{\textbar}\mathbb{P}(A) - \mathbb{P}_{\mathcal{D}_{N}}(A)\text{\textbar} > \epsilon\right),
	\end{equation}
	in which $\mathcal{S}^{\star} = \{A_{z}:z \in \mathbb{R}\}$ with $A_{z} = \{\omega \in \Omega: \omega \leq z\}$. If probability \eqref{partial_convergence} converges to zero when $N$ tends to infinity for a class $\mathcal{S}^{\star} \subsetneq \mathcal{S}$, we say there exists a \textit{partial uniform convergence} of the empirical measure to $\mathbb{P}$.
	
	Observe that in \eqref{partial_convergence} not only the class $\mathcal{S}^{\star}$ is fixed, but also the probability measure $\mathbb{P}$, hence partial uniform convergence is dependent on the class and the probability. Nevertheless, in a distribution-free framework, such as that of learning (cf. Section \ref{SecPreliminaries}), the convergences of interest should hold for any data generating distribution, which is the case, for example, of Glivenko-Cantelli Theorem, that presents a rate of convergence \eqref{GCbound} which does not depend on $P$, holding for any probability measure and random variable $Z$. Therefore, once a class $\mathcal{S}^{\star}$ of interest is fixed, partial uniform convergence should hold for any data generating distribution, a problem which can be stated as follows.
	
	Let $\mathcal{P}$ be the class of all possible probability distributions of a random variable with support in $\mathcal{Z}$, and let $\mathcal{S}^{\star}$ be a class of events. The \textit{generalized Glivenko-Cantelli problem} (GGCP) is to find a positive constant $a$ and a function $b: \mathbb{Z}_{+} \mapsto \mathbb{R}_{+}$, such that $\lim\limits_{N \to \infty} b(N)/\exp cN = 0, \forall c > 0$, satisfying, for $N$ great enough,\footnote{In the presentation of \cite[Chapter~2]{vapnik1998} it is assumed that $b$ is a positive constant, not depending on sample size $N$. Nevertheless, having $b$ as a function of $N$ of an order lesser than exponential does not change the qualitative behavior of this convergence, that is, also guarantees the almost sure converge due to Borel-Cantelli Lemma.}
	\begin{equation}
		\label{Gen_GCbound}
		\sup\limits_{P \in \mathcal{P}} \mathbb{P}\left(\sup\limits_{A \in \mathcal{S}^{\star}} \text{\textbar}\mathbb{P}(A) - \mathbb{P}_{\mathcal{D}_{N}}(A)\text{\textbar} > \epsilon\right) \leq b(N) \exp\{-a\epsilon^{2}N\},
	\end{equation}
	in which $\mathbb{P}$ is to be understood as dependent on $P$, since it is the unique probability measure on the Borel $\sigma$-algebra that equals $P$ on the events $\{\omega \in \Omega:\omega \leq z\}, z \in \mathbb{R}^{d}$. If the events are of the form $A = \{w \in \Omega: Z(w) \leq z\}, z \in \mathbb{R}$, then \eqref{Gen_GCbound} is equivalent to \eqref{GCbound}, although in the latter it is implicit that it holds for any distribution $P$.
	
	The investigation of GGCP revolves around deducing necessary and sufficient conditions on the class $\mathcal{S}^{\star}$ for \eqref{Gen_GCbound} to hold. We will study these conditions in order to establish the almost sure convergence to zero of type I estimation error (cf. \eqref{GE1}) when the loss function is binary, what may be stated as a GGCP.
	
	\subsection{Convergence to zero of type I estimation error}
	\label{ApTypeI}
	
	\subsubsection{Binary loss functions}
	
	Fix a hypotheses space $\mathcal{H}$, a binary loss function $\ell$, and consider the class $\mathcal{S}^{\star} = \{A_{h}: h \in \mathcal{H}\}$, such that $\mathds{1}\{z \in A_{h}\} = \ell(z,h) \in \{0,1\}, z \in \mathcal{Z}, h \in \mathcal{H}$, that is, if $z \in A_{h}$ the loss is one, and otherwise it is zero. For example, if $Z = (X,Y)$, the hypotheses in $\mathcal{H}$ are functions from the range of $X$ to that of $Y$, and $\ell$ is the simple loss function, then $A_{h}$ may be explicitly written as
	\begin{equation*}
		A_{h} = \{\omega: h(X(\omega)) \neq Y(\omega)\}.
	\end{equation*}
	In this instance, the probability in the left-hand side of \eqref{Gen_GCbound} may be written as
	\begin{equation}
		\label{typeIexp}
		\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}\mathbb{E}(\ell(Z,h)) - \mathbb{E}_{\mathcal{D}_{N}}(\ell(Z,h))\text{\textbar} > \epsilon\right),
	\end{equation}
	in which $\mathbb{E}$ is expectation with respect to $\mathbb{P}$ and $\mathbb{E}_{\mathcal{D}_{N}}$ is the empirical mean under $\mathcal{D}_{N}$. With the notation of Section \ref{SecPreliminaries}, this last probability equals
	\begin{equation*}
		\label{typeIappendix}
		\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} > \epsilon\right),
	\end{equation*}
	the tail probability of type I estimation error in $\mathcal{H}$.
	
	For each fixed $h \in \mathcal{H}$, we are comparing in \eqref{typeIexp} the mean of a binary function with its empirical mean, so we may apply Hoeffding's inequality \cite{hoeffding1963} to obtain 
	\begin{equation*}
		\mathbb{P}\left(\text{\textbar}\mathbb{E}(\ell(Z,h)) - \mathbb{E}_{\mathcal{D}_{N}}(\ell(Z,N))\text{\textbar} > \epsilon\right) \leq 2 \exp\{-2\epsilon^{2}N\},
	\end{equation*}
	from which follows a solution of type I estimation error GGCP when the cardinality of $\mathcal{H}$ is finite, by applying an elementary union bound:
	\begin{align*}
		\mathbb{P}\Bigg(\sup\limits_{h \in \mathcal{H}} \text{\textbar}\mathbb{E}(\ell(Z,h)) -\mathbb{E}_{\mathcal{D}_{N}}(\ell(Z,N))\text{\textbar} > \epsilon\Bigg) &\leq \sum_{h \in \mathcal{H}} \mathbb{P}\left(\text{\textbar}\mathbb{E}(\ell(Z,h)) - \mathbb{E}_{\mathcal{D}_{N}}(\ell(Z,N))\text{\textbar} > \epsilon\right) \\
		&\leq 2 \text{\textbar}\mathcal{H}\text{\textbar} \exp\{-2\epsilon^{2}N\},
	\end{align*}
	what establishes the almost sure convergence to zero of type I estimation error when $\mathcal{H}$ is finite and $\ell$ is binary.
	
	In order to treat the case when $\mathcal{H}$ has infinitely many hypotheses, we rely on a modification of Glivenko-Cantelli Theorem, which depends on the shatter coefficient of a class $\mathcal{S}^{\star} \subset \mathcal{S}$ of events in the Borel $\sigma$-algebra of $\mathbb{R}^{d}$, defined below.
	
	\begin{definition}
		\label{shatter_borel}
		Fix $\mathcal{S}^{\star} \subset \mathcal{S}$ and let
		\begin{equation*}
			\mathcal{G}_{\mathcal{S}^\star} = \{h_{A}(z) = \mathds{1}\{z \in A\}: A \in \mathcal{S}^\star\}
		\end{equation*}
		be the characteristic functions of the sets in $\mathcal{S}^{\star}$. We define the shatter coefficient of $\mathcal{S}^{\star}$ as
		\begin{equation*}
			S(\mathcal{S}^{\star},N) \coloneqq S(\mathcal{G}_{\mathcal{S}^{\star}},N),
		\end{equation*}
		in which $S(\mathcal{G}_{\mathcal{S}^{\star}},N)$ is the shatter coefficient of $\mathcal{G}_{\mathcal{S}^{\star}}$ (cf. Definition \ref{shatter}). From this definition follows that
		\begin{equation*}
			d_{VC}(\mathcal{S}^{\star}) = d_{VC}(\mathcal{G}_{\mathcal{S}^{\star}}).
		\end{equation*}
	\end{definition}
	
	The shatter coefficient and VC dimension of a class $\mathcal{S}^{\star}$ are related to the dichotomies this class can build with $N$ points by considering whether a point is in each set or not. From a simple modification of the proof of Theorem \ref{glivenko_cantelli} presented in \cite[Theorem~12.4]{devroye1996} follows a result due to \cite{vapnik1971uniform}.
	
	\begin{theorem}
		\label{theorem_GGCP}
		For any probability measure $\mathbb{P}$ and class of sets $\mathcal{S}^{\star} \subset \mathcal{S}$, for fixed $N \in \mathbb{Z}$ and $\epsilon > 0$, it is true that
		\begin{equation*}
			\mathbb{P}\left(\sup\limits_{A \in \mathcal{S}^{\star}} \text{\textbar}\mathbb{P}(A) - \mathbb{P}_{\mathcal{D}_{N}}(A)\text{\textbar} > \epsilon\right)  \leq 8 S(\mathcal{S}^{\star},N) \exp\left\{-N \frac{\epsilon^2}{32}\right\}.
		\end{equation*}
	\end{theorem}
	
	From this theorem follows a bound for tail probabilities of type I estimation error when $\ell$ is binary.
	
	\begin{corollary}
		\label{cor1TypeI}
		Fix a hypotheses space $\mathcal{H}$ and a loss function $\ell: \mathcal{Z} \times \mathcal{H} \mapsto \{0,1\}$. Let $\mathcal{S}^{\star} = \{A_{h}: h \in \mathcal{H}\}$, with
		\begin{equation*}
			\mathds{1}\{z \in A_{h}\} = \ell(z,h), z \in \mathcal{Z}, h \in \mathcal{H}.
		\end{equation*}
		Then,
		\begin{equation}
			\label{Ap1Eq4}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} > \epsilon\right) \leq 8 \ S(\mathcal{H},N) \exp\left\{-N \frac{\epsilon^2}{32}\right\},
		\end{equation}
		with
		\begin{equation*}
			S(\mathcal{H},N) \coloneqq S(\mathcal{S}^{\star},N).
		\end{equation*}
	\end{corollary}
	
	\begin{remark}
		We remark that $S(\mathcal{S}^{\star},N) = S(\mathcal{G}_{\mathcal{H},\ell},N)$, as defined in Definition \ref{VCdimension}, when the loss $\ell$ is binary. Observe that $\mathcal{S}^{\star}$ depends on $\ell$, although we omit the dependence to easy notation.
	\end{remark}
	
	The calculation of the quantities on the right-hand side of \eqref{Ap1Eq4} is not straightforward, since the shatter coefficient is not easily determined for arbitrary $N$. Nevertheless, the shatter coefficient may be bounded by a quantity depending on the VC dimension of $\mathcal{H}$. This is the content of \cite[Theorem~4.3]{vapnik1998}.
	
	\begin{theorem}
		\label{theorem_shaterDVC}
		If $d_{VC}(\mathcal{H}) < \infty$, then
		\begin{equation*}
			\ln S(\mathcal{H},N) \begin{cases}
				= N \ln 2, & \text{ if } N \leq d_{VC}(\mathcal{H})\\
				\leq d_{VC}(\mathcal{H}) \left(1 + \ln \frac{N}{d_{VC}(\mathcal{H})}\right), & \text{ if } N > d_{VC}(\mathcal{H}) 
			\end{cases}.
		\end{equation*}
	\end{theorem}
	
	\begin{remark}
		Theorem \ref{theorem_shaterDVC} is true for any loss function $\ell$, not only binary.
	\end{remark}
	
	Combining this theorem with Corollary \ref{cor1TypeI}, we obtain the following result.
	
	\begin{corollary}
		\label{cor2TypeI}
		Under the hypotheses of Corollary \ref{cor1TypeI} it holds
		\begin{equation}
			\label{Ap1Eq5}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} > \epsilon\right) \leq 8 \ \exp\left\{d_{VC}(\mathcal{H}) \left(1 + \ln \frac{N}{d_{VC}(\mathcal{H})}\right) - N \frac{\epsilon^2}{32}\right\}.
		\end{equation}
		In particular, if $d_{VC}(\mathcal{H}) < \infty$, not only \eqref{Ap1Eq5} converges to zero, but also
		\begin{equation*}
			\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} \xrightarrow[N \to \infty]{} 0,
		\end{equation*}
		with probability one by Borel-Cantelli Lemma.
	\end{corollary}
	
	From Corollary \ref{cor2TypeI} follows the convergence to zero of type I estimation error when the loss function is binary and $d_{VC}(\mathcal{H})$ is finite. We now extend this result to real-valued bounded loss functions.
	
	\subsubsection{Bounded loss functions}
	
	Assume the loss function is bounded, that is, for all $z \in \mathcal{Z}$ and $h \in \mathcal{H}$,
	\begin{equation}	
		\label{bounded}
		0 \leq \ell(z,h) \leq C < \infty,
	\end{equation}
	for a positive constant $C \in \mathbb{R}_{+}$. Throughout this section, a constant $C$ satisfying \eqref{bounded} is fixed.
	
	For any $h \in \mathcal{H}$, by definition of Lebesgue-Stieltjes integral, we have that
	\begin{equation*}
		L(h) = \int_{\mathcal{Z}} \ell(z,h) \ dP(z) = \lim\limits_{n \to \infty} \sum_{k=1}^{n-1} \frac{C}{n} \mathbb{P}\left(\ell(Z,h) > \frac{kC}{n}\right),
	\end{equation*}
	recalling that $Z$ is a random variable with distribution $P$. In a same manner, we may also write the empirical risk under $\mathcal{D}_{N}$ as
	\begin{equation*}
		L_{\mathcal{D}_{N}}(h) = \frac{1}{N} \sum_{i=1}^{N} \ell(Z_{i},h) = \lim\limits_{n \to \infty} \sum_{k=1}^{n-1} \frac{C}{n} \mathbb{P}_{\mathcal{D}_{N}}\left(\ell(Z,h) > \frac{kC}{n}\right),
	\end{equation*}
	recalling that $\mathbb{P}_{\mathcal{D}_{N}}$ is the empirical measure according to $\mathcal{D}_{N}$.
	
	From the representation of $L$ and $L_{\mathcal{D}_{N}}$ described above, we have that, for each $h \in \mathcal{H}$ fixed,
	\begin{align*}
		\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} &= \text{\textbar}\lim\limits_{n \to \infty} \sum_{k=1}^{n-1} \frac{C}{n} \left(\mathbb{P}\left(\ell(Z,h) > \frac{kC}{n}\right) - \mathbb{P}_{\mathcal{D}_{N}}\left(\ell(Z,h) > \frac{kC}{n}\right)\right) \text{\textbar}\\
		&\leq \text{\textbar}\lim\limits_{n \to \infty} \sum_{k=1}^{n-1} \frac{C}{n} \sup\limits_{0 \leq \beta \leq C} \left(\mathbb{P}\left(\ell(Z,h) > \beta\right) - \mathbb{P}_{\mathcal{D}_{N}}\left(\ell(Z,h) > \beta\right)\right) \text{\textbar}\\
		&\leq \lim\limits_{n \to \infty} \sum_{k=1}^{n-1} \frac{C}{n} \sup\limits_{0 \leq \beta \leq C} \text{\textbar}\mathbb{P}\left(\ell(Z,h) > \beta\right) - \mathbb{P}_{\mathcal{D}_{N}}\left(\ell(Z,h) > \beta\right)\text{\textbar}\\
		&= C \sup\limits_{0 \leq \beta \leq C} \text{\textbar}\int_{\mathcal{Z}} \mathds{1}\{\ell(z,h) > \beta\} \ dP(z) - \frac{1}{N} \sum_{i=1}^{N} \ \mathds{1}\{\ell(Z_{i},h) > \beta\}\text{\textbar}.
	\end{align*}
	We conclude that
	\begin{align*}
		&\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}(h)}\text{\textbar} > \epsilon\right) \\
		&\leq \mathbb{P}\left(\sup\limits_{\substack{h \in \mathcal{H}\\ 0 \leq \beta \leq C}} \text{\textbar}\int_{\mathcal{Z}} \mathds{1}\{\ell(z,h) > \beta\} \ dP(z) - \frac{1}{N} \sum_{i=1}^{N} \ \mathds{1}\{\ell(Z_{i},h)\}\text{\textbar} > \frac{\epsilon}{C}\right).
	\end{align*}
	
	Since the right-hand side of the expression above is a GGCP with
	\begin{equation*}
		\mathcal{S}^{\star} = \{\{z \in \mathcal{Z}:\ell(z,h) > \beta\}: h \in \mathcal{H},0 \leq \beta \leq C\}
	\end{equation*}
	and, recalling the definition of shatter coefficient of $\mathcal{H}$ under a real-valued loss function $\ell$ (cf. Definition \ref{shatter}), we note that
	\begin{align*}
		S(\mathcal{G}_{\mathcal{H},\ell},N)  = S(\mathcal{S}^{\star},N) & & \text{ hence } & & d_{VC}(\mathcal{H}) = d_{VC}(\mathcal{S}^{\star}) 
	\end{align*}
	so a bound for the tail probabilities of type I estimation error when the loss function is bounded follows immediately from Theorems \ref{theorem_GGCP} and \ref{theorem_shaterDVC}.
	
	\begin{corollary}
		\label{cor3TypeI}
		Fix a hypotheses space $\mathcal{H}$ and a loss function $\ell: \mathcal{Z} \times \mathcal{H} \mapsto \mathbb{R}_{+}$, with $0 \leq \ell(z,h) \leq C$ for all $z \in \mathcal{Z}, h \in \mathcal{H}$. Then,
		\begin{equation}
			\label{Ap1Eq6}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} > \epsilon\right) \leq 8 \ \exp\left\{d_{VC}(\mathcal{H}) \left(1 + \ln \frac{N}{d_{VC}(\mathcal{H})}\right) - N \frac{\epsilon^2}{32C^{2}}\right\}.
		\end{equation}
		In particular, if $d_{VC}(\mathcal{H}) < \infty$, not only \eqref{Ap1Eq6} converges to zero, but also
		\begin{equation*}
			\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} \xrightarrow[N \to \infty]{} 0,
		\end{equation*}
		with probability one by Borel-Cantelli Lemma.
	\end{corollary}
	
	It remains to treat the case of unbounded loss functions, which requires a different approach.
	
	\subsubsection{Unbounded loss functions}
	\label{ApUnbounded}
	
	In this section, we establish conditions on $P$ and $\mathcal{H}$ for the convergence in probability to zero of type I relative estimation error when $\ell$ is unbounded. The framework treated here is that described at the beginning of Section \ref{SecUnbounded}.
	
	In order to easy notation, we denote
	\begin{equation*}
		\ell(\mathcal{D}_{N},h) \coloneqq \left(\ell(Z_{1},h),\dots,\ell(Z_{N},h)\right) \in \mathbb{R}^{N}\setminus\{0\},
	\end{equation*}
	the vector sample point loss, so it follows from \eqref{LNp} that, for $1 \leq q \leq p$,
	\begin{equation}
		\label{def_sample_pmoment}
		L_{\mathcal{D}_{N}}^{q}(h) \coloneqq \frac{\lVert \ell(\mathcal{D}_{N},h) \rVert_{q}}{N^{\frac{1}{q}}},
	\end{equation}
	in which $\lVert \cdot \rVert_{q}$ is the $q$-norm in $\mathbb{R}^{N}$. Recall that we assume that $P$ has at most heavy tails, which means there exists a $p > 1$, that can be lesser than 2, with
	\begin{align}
		\label{tauStarAp}
		\tau_{p} < \tau^{\star} < \infty,
	\end{align}
	and that
	\begin{align}
		\label{finite_moments}
		\sup\limits_{h \in \mathcal{H}} L_{\mathcal{D}_{N}}^{p}(h)  < \infty & & \text{ and } & & \sup\limits_{h \in \mathcal{H}} L^{p}(h) < \infty,
	\end{align}
	in which the first inequality should hold with probability one. 
	
	The first condition in \eqref{finite_moments} is more a feature of the loss function, than of distribution $P$. Actually, one can bound $L_{\mathcal{D}_{N}}^{p}(h)$ by a quantity depending on $N$ and $L_{\mathcal{D}_{N}}^{q}(h)$ with $1 \leq q < p$, for any sample $\mathcal{D}_{N}$ of any distribution $P$. This is the content of the next lemma, which will be useful later on, and that implies the following: if $\sup_{h \in \mathcal{H}} L_{\mathcal{D}_{N}}^{1}(h) = \sup_{h \in \mathcal{H}} L_{\mathcal{D}_{N}}(h) < \infty$, then $\sup_{h \in \mathcal{H}} L_{\mathcal{D}_{N}}^{p}(h) < \infty$ for any $1 < p < \infty$, for $N$ and $\mathcal{H}$ fixed. 
	
	\begin{lemma}
		\label{lemma_norm}
		For fixed $\mathcal{H}$, $N \geq 1$ and $1 \leq q < p$, it follows that
		\begin{equation*}
			1 \leq \frac{L_{\mathcal{D}_{N}}^{p}(h)}{L_{\mathcal{D}_{N}}^{q}(h)} \leq N^{\frac{1}{q} - \frac{1}{p}}
		\end{equation*}
		for all $h \in \mathcal{H}$.
	\end{lemma}
	\begin{proof}
		Recalling definition \eqref{def_sample_pmoment}, we have that
		\begin{equation*}
			\frac{L_{\mathcal{D}_{N}}^{p}(h)}{L_{\mathcal{D}_{N}}^{q}(h)} = N^{\frac{1}{q} - \frac{1}{p}} \frac{\lVert \ell(\mathcal{D}_{N},h) \rVert_{p}}{\lVert \ell(\mathcal{D}_{N},h) \rVert_{q}},
		\end{equation*}
		so it is enough to show that
		\begin{equation*}
			N^{\frac{1}{p} - \frac{1}{q}} \leq \frac{\lVert \ell(\mathcal{D}_{N},h) \rVert_{p}}{\lVert \ell(\mathcal{D}_{N},h) \rVert_{q}} \leq 1.
		\end{equation*}
		
		Now, the right inequality above is clear, since if $w \in \mathbb{R}^{N}$ is such that $\lVert w \rVert_{q} = 1$, then
		\begin{align*}
			\lVert w \rVert_{p}^{p} = \sum_{i=1}^{N} \text{\textbar}w_{i}\text{\textbar}^{p} \leq \sum_{i=1}^{N} \text{\textbar}w_{i}\text{\textbar}^{q} = 1,
		\end{align*}
		so the result follows when $\lVert w \rVert_{q} = 1$ by elevating both sided to the $1/p$ power. To conclude the proof it is enough to see that, for any $w \in \mathbb{R}^{N}\setminus\{0\}$,
		\begin{equation*}
			\lVert w \rVert_{p} = \lVert w \rVert_{q} \lVert \frac{w}{\lVert w \rVert_{q}} \rVert_{p} \leq \lVert w \rVert_{q} \lVert \frac{w}{\lVert w \rVert_{q}} \rVert_{q} = \lVert w \rVert_{q}.
		\end{equation*}
		
		The left inequality is a consequence of H\"older's inequality, since, for $w \in \mathbb{R}^{N}$,
		\begin{align*}
			\sum_{i=1}^{N} \text{\textbar}w_{i}\text{\textbar}^{q} \cdot 1 \leq \left(\sum_{i=1}^{N} \text{\textbar}w_{i}\text{\textbar}^{p}\right)^{\frac{q}{p}} N^{1 - \frac{q}{p}},
		\end{align*}
		and the result follows by taking the $1/q$ power on both sides.
	\end{proof}
	
	For unbounded losses, rather than considering the convergence of type I estimation error to zero, we will consider the convergence of the relative type I estimation error, defined as
	\begin{equation}	
		\label{Ap1Eq9}
		\textbf{(I)} \ \ \sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} .
	\end{equation}
	In order to establish bounds for the tail probabilities of \eqref{Ap1Eq9} when \eqref{tauStarAp} and \eqref{finite_moments_text} hold, we rely on the following novel technical theorem.
	
	\begin{theorem}
		\label{change_denominator}
		Let $q = \sqrt{p}$. For any hypotheses space $\mathcal{H}$, loss function $\ell$ satisfying $\ell(h,z) \geq 1$, and $0 < \epsilon < 1$, it holds
		\begin{align*}
			&\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)}  > \tau^{\star} \epsilon\right) \leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L(h) - L_{\mathcal{D}_{N}}(h)}{L^{p}(h)} > \epsilon\right)\\
			&+ \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{\prime}(h) - L^{\prime}(h)}{L_{\mathcal{D}_{N}}^{\prime q}(h)} > \frac{\epsilon}{N^{\frac{1}{q} - \frac{1}{p}}}\right) + \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L_{\mathcal{D}_{N}}^{p}(h)} > \frac{\epsilon(1-\epsilon)}{N^{\frac{1}{q} - \frac{1}{p}}}\right)
		\end{align*}
		in which $L^{\prime},L_{\mathcal{D}_{N}}^{\prime}$ and $L_{\mathcal{D}_{N}}^{\prime k}$ are the respective risks and $k$ moments of loss function $\ell^{\prime}(z,h) \coloneqq (\ell(z,h))^{q}$.
	\end{theorem}
	\begin{proof}
		We first note that
		\begin{align*}
			&\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L(h)} > \tau^{\star}\epsilon \implies \sup\limits_{h \in \mathcal{H}} \left(\frac{L^{q}(h)}{L(h)} \frac{1}{\tau^{\star}}\right) \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L^{q}(h)} > \epsilon\\
			&\implies \sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L^{q}(h)} > \epsilon
		\end{align*}
		since
		\begin{align*}
			\sup\limits_{h \in \mathcal{H}} \left(\frac{L^{q}(h)}{L(h)} \frac{1}{\tau^{\star}}\right) \leq \sup\limits_{h \in \mathcal{H}} \left(\frac{L^{p}(h)}{L(h)} \frac{1}{\tau^{\star}}\right) \leq 1
		\end{align*}
		by \eqref{tauStarAp}. With an analogous deduction, it follows that
		\begin{equation*}
			\sup\limits_{h \in \mathcal{H}} \frac{L(h) - L_{\mathcal{D}_{N}}(h)}{L(h)} > \tau^{\star}\epsilon \implies \sup\limits_{h \in \mathcal{H}} \frac{L(h) - L_{\mathcal{D}_{N}}(h)}{L^{p}(h)} > \epsilon.
		\end{equation*}
		Hence, the probability on the left hand-side of the statement is lesser or equal to
		\begin{align}
			\label{Ap1Eq10}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L(h) - L_{\mathcal{D}_{N}}(h)}{L^{p}(h)} > \epsilon\right) + \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L^{q}(h)} > \epsilon\right),
		\end{align}
		so it is enough to properly bound the second probability in \eqref{Ap1Eq10}. 	
		
		In order to do so, we will intersect the event inside the probability with the following event, and its complement:
		\begin{align*}
			\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)}{L^{q}(h)} \leq 1 \iff \sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{q}(h) - L^{q}(h)}{L_{\mathcal{D}_{N}}^{p}(h)} \leq \delta,
		\end{align*}
		in which
		\begin{equation*}
			\delta \coloneqq \frac{\epsilon}{N^{\frac{1}{q} - \frac{1}{p}}}.
		\end{equation*}
		Proceeding in this way, we conclude that
		\begin{align}
			\label{Ap1Eq12} \nonumber
			&\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L^{q}(h)} > \epsilon\right) \leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{q}(h) - L^{q}(h)}{L_{\mathcal{D}_{N}}^{p}(h)} > \delta\right)\\ \nonumber
			&+ \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \left(\frac{L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)}{L^{q}(h)}\right) \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)} > \epsilon,\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)}{L^{q}(h)} \leq 1\right)\\
			&\leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{q}(h) - L^{q}(h)}{L_{\mathcal{D}_{N}}^{p}(h)} > \delta\right) + \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)} > \epsilon\right).
		\end{align}
		To bound the first probability above, we recall the definition of $L_{\mathcal{D}_{N}}^{p}(h)$ and note that $a^{\frac{1}{q}} - b^{\frac{1}{q}} \leq a - b$ if $q > 1$ and $1 \leq b \leq a$, so that
		\begin{equation}
			\label{Ap1Eq11}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{q}(h) - L^{q}(h)}{L_{\mathcal{D}_{N}}^{p}(h)} > \delta\right) \leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{\left(L_{\mathcal{D}_{N}}^{q}(h)\right)^{q} - \left(L^{q}(h)\right)^{q}}{N^{-\frac{1}{p}} \lVert \ell(\mathcal{D}_{N},h)\rVert_{p}} > \delta\right).
		\end{equation}
		Define the loss function $\ell^{\prime}(z,h) \coloneqq (\ell(z,h))^{q}$, and let $L^{\prime},L^{\prime k},L_{\mathcal{D}_{N}}^{\prime}$ and $L_{\mathcal{D}_{N}}^{\prime k}$ be the risks and $k$ moments according to this new loss function. Then, the probability in \eqref{Ap1Eq11} can be written as
		\begin{equation}
			\label{Ap1Eq13}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{\prime}(h) - L^{\prime}(h)}{\left(N^{-1}\lVert \ell^{\prime}(\mathcal{D}_{N},h) \rVert_{\frac{p}{q}}^{\frac{p}{q}}\right)^{\frac{1}{p}}} > \delta\right) \leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}^{\prime}(h) - L^{\prime}(h)}{L_{\mathcal{D}_{N}}^{\prime q}(h)} > \delta\right)
		\end{equation}
		since $\frac{p}{q} = q$ and $N^{\frac{1}{p}} \leq N^{\frac{1}{q}}$.
		
		We turn to the second probability in \eqref{Ap1Eq12}. By applying Lemma \ref{lemma_norm}, and recalling the definition of $\delta$, we have that $L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)$ is equal to
		\begin{align*}
			L_{\mathcal{D}_{N}}^{p}(h) \left(\frac{L_{\mathcal{D}_{N}}^{q}(h)}{L_{\mathcal{D}_{N}}^{p}(h)} - \delta\right) \geq L_{\mathcal{D}_{N}}^{p}(h) \left(\frac{1}{N^{\frac{1}{q} - \frac{1}{p}}} - \frac{\epsilon}{N^{\frac{1}{q} - \frac{1}{p}}}\right) = \frac{L_{\mathcal{D}_{N}}^{p}(h)}{N^{\frac{1}{q} - \frac{1}{p}}} \left(1 - \epsilon\right),
		\end{align*}
		from which follows
		\begin{equation}
			\label{Ap1Eq14}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L_{\mathcal{D}_{N}}^{q}(h) - \delta L_{\mathcal{D}_{N}}^{p}(h)} > \epsilon\right) \leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{L_{\mathcal{D}_{N}}^{p}(h)} > \frac{\epsilon(1-\epsilon)}{N^{\frac{1}{q} - \frac{1}{p}}}\right).
		\end{equation}
		The result follows by combining \eqref{Ap1Eq10}, \eqref{Ap1Eq12}, \eqref{Ap1Eq13} and \eqref{Ap1Eq14}.
	\end{proof}
	
	An exponential bound for relative type I estimation error \eqref{Ap1Eq9} depending on $p$, $\tau^{\star}$ and $d_{VC}(\mathcal{H})$ is a consequence of Theorems \ref{theorem_shaterDVC} and \ref{change_denominator}, and results in \cite{cortes2019}, which we now state. Define, for a $0 < \varsigma < 1$ fixed,
	\begin{align*}
		\Gamma(p,\epsilon) &= \frac{p-1}{p}(1+\varsigma)^{\frac{1}{p}} + \frac{1}{p} \left(\frac{p}{p-1}\right)^{p-1} \left(1 + \left(\frac{p-1}{p}\right)^{p} \varsigma^{\frac{1}{p}}\right)^{\frac{1}{p}} \left[1 + \frac{\ln(1/\epsilon)}{\left(\frac{p}{p-1}\right)^{p-1}}\right]^{\frac{p-1}{p}},		
	\end{align*}
	for $0 < \epsilon < 1, 1 < p \leq 2$, and
	\begin{align*}
		\Lambda(p) = \left(\frac{1}{2}\right)^{\frac{2}{p}} \left(\frac{p}{p-2}\right)^{\frac{p-1}{p}} + \frac{p}{p-1} \varsigma^{\frac{p-2}{2p}}
	\end{align*}
	for $p > 2$.
	
	\begin{theorem}
		\label{theorem_unbounded}
		Fix a hypotheses space $\mathcal{H}$ and an unbounded loss function $\ell$, and assume that \eqref{finite_moments} is in force. Then, the following holds:
		\begin{itemize}
			\item If $P$ has light tails, so that \eqref{tauStarAp} holds for a $p > 2$ fixed, then		
			\begin{align*}
				\mathbb{P}\Bigg(\sup\limits_{h \in \mathcal{H}} & \frac{L(h) - L_{\mathcal{D}_{N}}(h)}{\sqrt[p]{\left(L^{p}(h)\right)^{p} + \varsigma}} > \Lambda(p) \epsilon\Bigg)\\& < 4 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}N}{4}\right\}
			\end{align*}
			and
			\begin{align*}
				\mathbb{P}\Bigg(\sup\limits_{h \in \mathcal{H}} & \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{\sqrt[p]{\left(L_{\mathcal{D}_{N}}^{p}(h)\right)^{p} + \varsigma}} > \Lambda(p) \epsilon\Bigg) \\&< 4 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}N}{4}\right\},
			\end{align*}
			for $0 < \epsilon < 1$ and $0 < \varsigma < \epsilon^{2}$.
			
			\item If $P$ has heavy tails, so that \eqref{tauStarAp} holds only for a $1 < p \leq 2$ fixed, then
			\begin{align*}
				\mathbb{P}\Bigg(\sup\limits_{h \in \mathcal{H}} & \frac{L(h) - L_{\mathcal{D}_{N}}(h)}{\sqrt[p]{\left(L^{p}(h)\right)^{p} + \varsigma}} > \Gamma(p,\epsilon) \epsilon\Bigg) \\&< 4 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}N^{\frac{2(p-1)}{p}}}{2^{\frac{p+2}{2}}}\right\}
			\end{align*}
			and
			\begin{align*}
				\mathbb{P}\Bigg(\sup\limits_{h \in \mathcal{H}} & \frac{L_{\mathcal{D}_{N}}(h) - L(h)}{\sqrt[p]{\left(L_{\mathcal{D}_{N}}^{p}(h)\right)^{p} + \varsigma}} > \Gamma(p,\epsilon) \epsilon\Bigg) \\&< 4 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}N^{\frac{2(p-1)}{p}}}{2^{\frac{p+2}{2}}}\right\},
			\end{align*}
			for $0 < \epsilon < 1$ and $0 < \varsigma^{\frac{p-1}{p}} < \epsilon^{\frac{p}{p-1}}$.
		\end{itemize}
	\end{theorem}
	
	Theorem \ref{theorem_unbounded}, together with Theorem \ref{change_denominator}, imply the following corollary, which is an exponential bound for relative type I estimation error when $P$ has heavy or light tails. The value of $\varsigma$ in the definitions of $\Lambda(p)$ and $\Gamma(p,\epsilon)$ below can be arbitrarily small.
	
	\begin{corollary}
		\label{convergence_relativeTI}
		Fix a hypotheses space $\mathcal{H}$, an unbounded loss function $\ell$ and $\epsilon > 0$. The following holds:
		\begin{itemize}
			\item If \eqref{tauStarAp} holds for a $p \geq 4$ fixed, then
			\begin{align*}
				\mathbb{P}&\Bigg(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} > \tau^{\star} \Lambda(\sqrt{p}) \epsilon\Bigg) \\
				&< 12 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}(1-\epsilon)^{2}N^{1 - \frac{2}{\sqrt{p}} + \frac{2}{p}}}{4}\right\}
			\end{align*}
			
			\item If \eqref{tauStarAp} holds for a $1 < p < 4$ fixed, then
			\begin{align*}
				\mathbb{P}&\Bigg(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} > \tau^{\star} \Gamma\left(\sqrt{p},\frac{\epsilon}{N^{\frac{1}{\sqrt{p}} - \frac{1}{p}}}\right)\epsilon\Bigg) \\
				&< 12 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}N^{\frac{2(\sqrt{p} - 1)}{\sqrt{p}} - \frac{2}{\sqrt{p}} + \frac{2}{p}}}{2^{\frac{\sqrt{p} + 2}{2}}} \right\}.
			\end{align*}
		\end{itemize}
		In both cases, if $d_{VC}(\mathcal{H}) < \infty$, then, by Borel-Cantelli Lemma,
		\begin{equation*}
			\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} \xrightarrow[N \to \infty]{} 0,
		\end{equation*}
		with probability one.
	\end{corollary}
	
	Corollary \ref{convergence_relativeTI} establishes the convergence to zero of relative type I estimation error, and concludes our study of type I estimation error convergence in classical VC theory.
	
	\begin{remark}
		We simplified the bounds in Corollary \ref{convergence_relativeTI} since, by combining Theorems \ref{change_denominator} with \ref{theorem_unbounded}, we obtain a bound with three terms of different orders in $N$, where the exponential in each of them is multiplied by four. The term of the greatest order is that we show in Corollary \ref{convergence_relativeTI}, with the exponential multiplied by twelve, since we can bound the two terms of lesser order by the one of the greatest order. This worsens the bound for fixed $N$, but ease notation and has the same qualitative effect of presenting an exponential bound for relative type I estimation error, which implies its almost sure convergence to zero.
	\end{remark}
	
	\begin{remark}
		Observe that $d_{VC}(\mathcal{M},\ell) = d_{VC}(\mathcal{M},\ell^{\prime})$, in which $\ell^{\prime}(z,h) = \left(\ell(z,h)\right)^{q}, q = \sqrt{p},$ as defined in Theorem \ref{change_denominator}, hence we can bound all three terms of the inequality in this theorem by $d_{VC}(\mathcal{M},\ell)$, as is done in Corollary \ref{convergence_relativeTI}. The VC dimension equality is true since $\mathcal{G}_{\mathcal{M},\ell} = \mathcal{G}_{\mathcal{M},\ell^{\prime}}$ (cf. Definition \ref{VCdimension}), as each function $g_{\beta,h}(z) = \mathds{1}\{\ell(z,h) \geq \beta\}$ in $\mathcal{G}_{\mathcal{M},\ell}$ has a correspondent $g^{\prime}_{\beta^{q},h}(z) = \mathds{1}\{\ell^{\prime}(z,h) \geq \beta^{q}\}$ in $\mathcal{G}_{\mathcal{M},\ell^{\prime}}$ that is such that $g \equiv g^{\prime}$. 
	\end{remark}
	
	\begin{remark}
		\label{remark_finite_moments}
		Condition \eqref{finite_moments} is not actually satisfied by many $\mathcal{H}$, for instance it does not hold for linear regression under the quadratic loss function. However, one can actually consider a $\mathcal{M} \subset \mathcal{H}$ such that \eqref{finite_moments} is true, with $d_{VC}(\mathcal{M}) = d_{VC}(\mathcal{H})$ and $L(h^{\star}) = L(h^{\star}_{\mathcal{M}})$, without loss of generality. For example, in linear regression one could consider only hypotheses with parameters bounded by a \textit{very large} constant $\gamma$, excluding hypotheses that are unlikely to be the target one. Observe that, in this example, it is better to consider the bounds for relative type I estimation error of unbounded loss functions, rather than consider that the loss function is bounded by a very large constant $C = \mathcal{O}(\gamma^2)$, which would generate bad bounds when applying Corollary \ref{cor3TypeI}. The results for unbounded loss functions holds for bounded ones, with $p$ arbitrarily large.
	\end{remark}
	
	\begin{remark}
		\label{remark_geq1}
		The main reason we assume that $\ell(z,h) \geq 1$, for all $z \in \mathcal{Z}$ and $h \in \mathcal{H}$, is to simplify the argument before \eqref{Ap1Eq11}, which could fail if the losses were lesser than one. We believe this assumption could be dropped at the cost of more technical results. Nevertheless, the results in Corollary \ref{convergence_relativeTI} present an exponential bound for
		\begin{equation*}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)}{L(h) + 1}\text{\textbar} > \epsilon\right)
		\end{equation*}
		for any unbounded loss function $\ell$. We note that, if we had not imposed this constraint in the loss function, we would have to deal with the denominators in the estimation errors, which could then be zero. This could have been easily accomplished by summing a constant to the denominators and then making it go to zero after the bounds are established, that is, find bounds for
		\begin{equation*}
			\mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)}{L(h) + \varsigma}\text{\textbar} > \epsilon\right),
		\end{equation*}
		and then make $\varsigma \to 0$. This is done in \cite{cortes2019}. By considering loss functions greater or equal to one, we have avoided the need to have heavier notations and more technical details when establishing the convergence of relative estimation errors. 
	\end{remark}
	
	\subsection{Convergence to zero of type II estimation error}
	
	A bound for type II estimation error \eqref{GE2} and relative type II estimation error, defined as
	\begin{equation*}
		\textbf{(II)} \ \ \frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})}
	\end{equation*}
	follow immediately from a bound obtained for the respective type I error. This is a consequence of the following elementary inequality, which can be found in part in \cite[Lemma~8.2]{devroye1996}.
	
	\begin{lemma}
		\label{lemmaTypeItoII}
		For any hypotheses space $\mathcal{H}$ and possible sample $\mathcal{D}_{N}$,
		\begin{align}
			\label{first_inequality}
			L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) \leq 2 \ \sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar},
		\end{align}
		and, if $\ell(z,h) \geq 1$, for all $z \in \mathcal{Z}$ and $h \in \mathcal{H}$, then
		\begin{equation}
			\label{second_inequality}
			\frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} \leq 2 \ \sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)}.
		\end{equation}
		These inequalities yield
		\begin{align}	
			\label{firstPinequality}
			\mathbb{P}\left(L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) > \epsilon\right) &\leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar} > \epsilon/2\right)
		\end{align}
		and
		\begin{equation}	
			\label{secondPinequality}
			\mathbb{P}\left(\frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} > \epsilon\right) \leq \mathbb{P}\left(\sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)} > \epsilon/2\right).
		\end{equation}
	\end{lemma}
	\begin{proof}
		The first inequality follows from
		\begin{align*}
			L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) &= L(\hat{h}^{\mathcal{D}_{N}}) - L_{\mathcal{D}_{N}}(\hat{h}^{\mathcal{D}_{N}}) + L_{\mathcal{D}_{N}}(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})\\
			&\leq  L(\hat{h}^{\mathcal{D}_{N}}) - L_{\mathcal{D}_{N}}(\hat{h}^{\mathcal{D}_{N}}) + L_{\mathcal{D}_{N}}(h^{\star}) - L(h^{\star})\\
			&\leq 2 \ \sup\limits_{h \in \mathcal{H}} \text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}.
		\end{align*}
		For the second one, analogous to the deduction above, we have that
		\begin{align*}
			\frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} &\leq  \frac{L(\hat{h}^{\mathcal{D}_{N}}) - L_{\mathcal{D}_{N}}(\hat{h}^{\mathcal{D}_{N}})}{L(\hat{h}^{\mathcal{D}_{N}})} + \frac{L_{\mathcal{D}_{N}}(h^{\star}) - L(h^{\star})}{L(h^{\star})}\\
			&\leq 2 \ \sup\limits_{h \in \mathcal{H}} \frac{\text{\textbar}L(h) - L_{\mathcal{D}_{N}}(h)\text{\textbar}}{L(h)},
		\end{align*}
		since $L(\hat{h}^{\mathcal{D}_{N}}) \geq L(h^{\star})$. The inequalities \eqref{firstPinequality} and \eqref{secondPinequality} are direct from \eqref{first_inequality} and \eqref{second_inequality}.
	\end{proof}
	
	Combining Lemma \ref{lemmaTypeItoII} with Corollaries \ref{cor2TypeI} and \ref{cor3TypeI} we obtain the consistency of type II estimation error, when $d_{VC}(\mathcal{H}) < \infty$ and the loss function is bounded, what also concerns binary loss functions.
	
	\begin{corollary}
		\label{cor1TypeII}
		Fix a hypotheses space $\mathcal{H}$ and a loss function $\ell: \mathcal{Z} \times \mathcal{H} \mapsto \mathbb{R}_{+}$, with $0 \leq \ell(z,h) \leq C$ for all $z \in \mathcal{Z}, h \in \mathcal{H}$. Then,
		\begin{equation}
			\label{Ap1Eq7}
			\mathbb{P}\left(L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) > \epsilon\right) \leq 8 \ \exp\left\{d_{VC}(\mathcal{H}) \left(1 + \ln \frac{N}{d_{VC}(\mathcal{H})}\right) - N \frac{\epsilon^2}{128C^{2}}\right\}.
		\end{equation}
		In particular, if $d_{VC}(\mathcal{H}) < \infty$, not only \eqref{Ap1Eq7} converges to zero, but also
		\begin{equation*}
			L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star}) \xrightarrow[N \to \infty]{} 0,
		\end{equation*}
		with probability one by Borel-Cantelli Lemma.
	\end{corollary}
	
	Finally, combining Lemma \ref{lemmaTypeItoII} with Corollary \ref{convergence_relativeTI}, we obtain the consistency of relative type II estimation error when $d_{VC}(\mathcal{H}) < \infty$, the loss function is unbounded, and $P$ satisfies \eqref{tauStarAp}.
	
	\begin{corollary}
		\label{cor2TypeII}
		Fix a hypotheses space $\mathcal{H}$, an unbounded loss function $\ell$ and $\epsilon > 0$. The following holds:
		\begin{itemize}
			\item If \eqref{tauStarAp} holds for a $p \geq 4$ fixed, then
			\begin{align*}
				\mathbb{P}\Bigg(&\frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} > \tau^{\star} \Lambda(\sqrt{p}) \epsilon\Bigg) \\
				&< 12 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}(1-\epsilon/2)^{2}N^{1 - \frac{2}{\sqrt{p}} + \frac{2}{p}}}{16}\right\}
			\end{align*}
			
			\item If \eqref{tauStarAp} holds for a $1 < p < 4$ fixed, then
			\begin{align*}
				\mathbb{P}\Bigg(&\frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} > \tau^{\star} \Gamma\left(\sqrt{p},\frac{\epsilon}{N^{\frac{1}{\sqrt{p}} - \frac{1}{p}}}\right)\epsilon\Bigg) \\
				&< 12 \exp\left\{d_{VC}(\mathcal{H})\left(1 + \ln \frac{2N}{d_{VC}(\mathcal{H})}\right) - \frac{\epsilon^{2}N^{\frac{2(\sqrt{p} - 1)}{\sqrt{p}} - \frac{2}{\sqrt{p}} + \frac{2}{p}}}{2^{\frac{\sqrt{p} + 6}{2}}} \right\}.
			\end{align*}
		\end{itemize}
		In any case, if $d_{VC}(\mathcal{H}) < \infty$, then, by Borel-Cantelli Lemma,
		\begin{equation*}
			\sup\limits_{h \in \mathcal{H}} \frac{L(\hat{h}^{\mathcal{D}_{N}}) - L(h^{\star})}{L(\hat{h}^{\mathcal{D}_{N}})} \xrightarrow[N \to \infty]{} 0,
		\end{equation*}
		with probability one.
	\end{corollary}
	
	This ends the study of type II estimation error convergence.

\bibliographystyle{plain}
\bibliography{ref}


\end{document}

