\section{Experiments \& Results}\label{sec5}

In this section, we perform quantitative and qualitative evaluations on the different generative content within SmartBook, such as the strategic questions (\textit{section headings}) and the corresponding summaries (\textit{section content}). Specifically, we evaluate both how informative the content is as well as how much it can benefit analysts. For strategic questions, the evaluation is based on how useful these might be to analysts (Section \ref{sec:strategic_questions}). The summaries for these questions, which corresponds to the section content, are evaluated based on how much relevant information is contained within them (Section \ref{sec:section_content}).

For our experiments, the news articles that are used to identify the major events are collected daily from the CNN website\footnote{Here is a URL corresponding to news for March 1: \href{https://www.cnn.com/europe/live-news/russia-ukraine-war-news-03-1-23/index.html}{link}} related to the Ukraine-Russia Crisis.  Using this data source, we organized SmartBook into chapters according to major events within a biweekly time span. The overall time period considered ranges from Sept 1, 2022, to Jan 31, 2023. 


%\heng{show reports before and after human editing}

%\heng{For chapter construction/question generation, we should also compare automatic question generation vs. human generation, show a table, to demonstrate that human generated questions are often too vague or too complex for machine to understand, and they are not up to date.}

%\vicki{subsections need to be organized because we should add performance analysis for each component (event naming, question generation, summarization)}

%\subsection{Implementation Details}


%We collected news articles related to the Ukraine Crises daily from various media sources, including CNN, VOA, etc.  We chose three automatically generated biweekly chapter summaries for human expert analysis to asses the report quality. The selected biweekly time spans include Sept 15-30, Oct 1-15, and Oct 15-30 \Yi{TODO: check details}. 

\subsection{Strategic Importance of Questions}
\label{sec:strategic_questions}

%\vicki{If we cannot compare with expert analysts:  we can (1) assume experts will get 100\%(perfect, upper bound) results and automatic results have achieved xx\% of human performance; (2) put this subsection after other analysis because the conclusion will be weaker}
We first evaluate the quality of the strategic questions, which are used as section headings in our SmartBook. The evaluation is done by drawing parallels to the construction of a situation report, which involves the participation of both senior and junior analysts.  A senior analyst's role is to come up with strategic questions which are then passed onto a junior analyst. The junior analyst now gathers tactical information that can help answer or provide more background to the strategic questions.

Specifically, our human evaluation of the questions within SmartBook measures the following aspects:
\begin{itemize}
    \item \textbf{Strategic Importance}: This evaluation is from the perspective of a senior analyst. Evaluating the strategic importance of a question requires introspection on whether the question provided within SmartBook would have been asked as a strategic question by a senior analyst. The scoring rubric involves marking the question as one of three categories, namely, \textit{Not Strategic}, \textit{Some Strategic Value}, and \textit{Definitely Strategic}.\\
    
    \item \textbf{Relevant Tactical Information}: Another dimension of evaluating the quality of a question is by measuring how much relevant tactical information useful to a junior analyst can be gathered using that question. In this case, the question is evaluated based on its corresponding SmartBook summary, in terms of how much of the question-relevant tactical information in the summary is helpful for an intelligence analyst to gain deeper insights into the situation. The scoring rubric is based on the summary, categorized into \textit{No information is tactical}, \textit{Some information is tactical}, and \textit{Most information is tactical}. Table \ref{tab:tactical_info_examples} shows some examples with tactical information relevant to the question highlighted in blue.
\end{itemize}

 \begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.5}
    \footnotesize
    \begin{tabular}{p{18.75em}|p{20.5em}}
      \textbf{Question:}  What were the motives behind the attack on Russian missile in Dnipro?
  &  \textbf{Question:} What is the capability of Ukraine's air defense systems in repelling the missile attacks from Russian missiles\\
  \hline
    \textbf{Summary:} The Russian missile attack on the city of Dnipro on Saturday was \textcolor{blue}{part of the Kremlin's efforts to limit Ukraine's ability to heat and power itself in the middle of winter}. The attack was reportedly \textcolor{red}{carried out with a Kh-22 missile}, an older and less accurate weapon than most modern missiles. It is believed that the attack was \textcolor{blue}{part of the Kremlin's attempts to retake a town, Soledar, and the nearby city of Bakhmut, as a way to grind down the best Ukrainian forces and prevent them from launching counterattacks} elsewhere. Nine people were killed, including a 15-year old girl, Kyrylo Tymoshenko, and \textcolor{red}{Russia denied responsibility for the attack}, claiming it was caused by Ukrainian air defences.
     & 
     \textbf{Summary:} Ukraine's air defense systems have demonstrated their capability in repelling the missile attacks from Russian forces in Kyiv. [1] reported that \textcolor{blue}{Ukrainian air defense forces shot down 37 of "about 40" rockets that entered the city's airspace}, with one person injured. [2] revealed that a commander, who asked to be identified only by the call sign "Hera" for security reasons, stated that \textcolor{blue}{a cruise missile was shot down by a machine gun} from the city's territorial defense mobile group. [3] mentioned that U.S. President Joe Biden had \textcolor{blue}{pledged to deliver one Patriot surface-to-air missile battery system} to Ukraine, which is one of the most advanced U.S. air defense systems and is \textcolor{blue}{capable of intercepting threats such as aircraft and ballistic missiles.}\\
     \hline
    \end{tabular}
    \caption{Table showing summaries for two strategic questions corresponding to a SmartBook chapter on Russian missile attacks. The tactically useful and relevant information has been highlighted in \textcolor{blue}{blue}. Tactically useful but irrelevant information has been highlighted in \textcolor{red}{red}.}
    \label{tab:tactical_info_examples}
\end{table}

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/question_eval.jpg}
    \includegraphics[width=1.0\linewidth]{figures/SmartBook_question_student_eval.png}
    % \caption{Figure showing results from human evaluation of the questions (section headings) in SmartBook by both NLP students and expert intelligence analysts. Judgements are based on strategic importance of the question and tactical information that can be gathered using the question.}
    \caption{Results from the human evaluation of the questions (section headings) in SmartBook. Judgments are based on the strategic importance of the question and tactical information that can be gathered using the question.}
    \label{fig:question_eval}
\end{figure}

For the evaluation, we randomly selected 25 chapters from SmartBook, which amounted to 125 strategic questions in total. Each example was annotated by 3 human evaluators. Figure \ref{fig:question_eval} shows the results. We observe that most questions are strategic, with at least 82\% of the questions having some strategic value. Further, we can see that these questions can help gather relevant tactical information in roughly 93\% of the cases, as judged by the annotators that most summaries to these questions have information that is tactically relevant.
%To also measure for differences in perspectives, the evaluation was done independently by expert intelligence analysts as well as NLP students (3 annotators were used in each case). 
 %\textbf{TODO: Revanth} Add insights after expert analysis is finished.





\subsection{Quality of Generated Section Content}
\label{sec:section_content}


We also evaluate the quality of summaries generated for the strategic questions. Summaries within SmartBook are generated by using question-driven claim extraction and validation (section \ref{subsubsec:claim_extraction}) before generating the final summary. For comparison, we introduce a \textit{query-focused summarization} baseline that directly uses the news articles as input and uses a large language model (LLM) to generate a query-focused summary\footnote{We use the same Instruct-GPT davinci-003 model as in Section \ref{subsubsec:summary_generation}. The prompt used is the same as in the summary generation example in Figure \ref{fig:overall_workflow}, except the entire news article texts are passed as context.}. Further, SmartBook uses news articles (relevant to the event) as the background corpus. We compare this against using the web as the background corpora, by obtaining relevant web pages from internet search and summarizing them using a large language model. This baseline simulates a competitive strategy of directly obtaining information from an LLM-enabled web search engine\footnote{Specifically, we leverage \href{https://www.perplexity.ai}{perplexity.ai}, which combines web-search with LLM summarization. The query used is the strategic question, along with a phrase ``\textit{with respect to the Ukraine-Russia crisis between $<$timeline$>$}'' to ground the query.} (the \textit{web search + LLM} baseline is similar to Bing + ChatGPT).

We randomly selected 50 strategic question sections to perform the human evaluation. 
Each data sample consists of a question and the generated summaries from three approaches: Smartbook, Web Search + LLM, and Query-focused summarization. Three annotators are asked to access the anonymous summaries in terms of coherence, relevance, and strategicness with the scores ranging from 1 (worst) to 5 (best). For coherence and relevance, we follow the guidelines as in \cite{fabbri2021summeval}. Simply, \textit{coherence} measures the quality of all sentences collectively regardless of the query and \textit{relevance} quantifies whether the summary contains key points that answer the given question. We define \textit{strategicness} as an indication of whether the summary provides non-trivial and insightful information for analysts, and suggests the breadth and depth of the provided key points. 
%Finally, the averaged scores are normalized to a 0-100 scale.


\input{tables/sum_human.tex}


Table~\ref{tab:sum_human} shows the results from the human evaluation of the summaries. We observe that SmartBook outperforms alternative competitive strategies along with coherence, relevance, and strategicness. The benefit of our question-driven claim extraction step (3 vs 1) can be seen from the considerably more relevant summaries within SmartBook, compared to the direct query-focused summarization of the news articles without an explicit extraction step. Further, we see that directly obtaining information from the internet (2 vs 3) can give strategically less important content compared to the focused news-driven summarization within SmartBook.  A qualitative example comparing the three approaches is shown in Table \ref{tab:summaries_comparison}.

\input{tables/summaries_comparison.tex}

% annotation guideline: https://docs.google.com/document/d/1zkHCrtidrqrT06NZEH6liLZrn9vwHuHYs0PUPZzq8Dw/edit
% batch 1: https://docs.google.com/spreadsheets/d/11VPV3uvwBNo9p-N5n7mhOtPVqwcP9fokFZ0pXHmGuME/edit#gid=535389842
% batch 2: https://docs.google.com/spreadsheets/d/1b2ggHXats8EMmIRM3GyyNfUSK7020wEYbgeQjD2t2pY/edit#gid=0

\subsection{Editing by Expert Analyst}
\label{subsec:human_expert_editing}
SmartBook aims to assist analysts in situation report generation, by serving as an initial draft that contains potentially strategic questions and summaries with tactical content. To quantitatively assess how beneficial the automatically generated chapter summaries from our proposed SmartBook framework can be, we asked an expert intelligence analyst to edit the summaries until they deemed the summary acceptable to them.  

We aim to measure the extent to which the expert analyst makes changes to the summaries within SmartBook. Specifically, such edits can be quantified in the form of (a) the percentage of tokens inserted and deleted, and (b) Levenshtein edit distance \cite{navarro2001guided}, which measures character level changes required to transform one string (old summary) to the other (new summary). Further, we also evaluate the commonly used token-overlap-based metrics such as BLEU \cite{papineni2002bleu} and ROUGE \cite{lin2004rouge} scores, by considering the edited summary as the reference. The expert editing was performed over 94 randomly sampled summaries.

Empirical results indicate that the token overlap between the generated and post-edited summaries is high, with a BLEU score of 59.0\% and a Rouge-L score of 74.1\%. This suggests that the generated summaries are sufficiently good-quality such that not much human expert revision is necessary. We acknowledge that a gap still exists between the automatically generated summaries and human expert summary curation, as the Levenshtein edit distance computed at the character level is 34.4\%. Interestingly, 15\% of the generated summaries are determined as perfect, with no edits made by the expert analyst. The general observation is that more content tends to be inserted rather than deleted by the expert analyst. The percentage of tokens inserted is 49.6\%, whereas the percentage of tokens deleted is only 2.3\%, pointing to the importance of summary generation to be more detailed. 

%\subsection{Our Language Modeling Framework demonstrates capability in Generating Quality Analyst Reports}
%To quantitatively assess the automatically generated chapter summaries from our proposed SmartBook framework, we measure its edit distance after human expert revision. Specifically, we utilize the following text comparison metrics
%: %token-based and semantic-based 

% \ken{I think factual consistency and coherence are two important dimensions that need to be evaluated. You can use factuality metrics (e.g., FactCC) to evaluate the factual consistency between each generated summary and the relevant contexts. You can also use DiscoScore to evaluate the coherence of each generated section.}
% \vicki{I will take care of summarization part}

%\begin{itemize}
%    \item \textbf{\% tokens inserted, \% tokens deleted} 
%    \item \textbf{Levenshtein Edit distance} - this is defined as the minimum number of changes required to convert string \textit{a} into string \textit{b}, done by inserting, deleting or replacing a character in string \textit{a} \cite{navarro2001guided}.
%    \item \textbf{BLEU score} - this measures how much the tokens in the generated summaries appeared in the human reference summaries \cite{post-2018-call}.
%    \item \textbf{ROUGE score} - this measures how much the tokens in the human reference summaries appeared in the machine generated summaries \cite{lin2004rouge}. \ken{not exactly correct, I think you use ROUGE F1 scores in your experiments and ROUGE F1 sores also consider the precision of n-grams in the machine-generated summaries. }
%\end{itemize}


\input{tables/table2}
Table \ref{tab:human_edits} shows an example of edits (shown in color) made by an intelligence analyst for a machine-generated summary in SmartBook. We can see that human analyst mainly tends to add additional tactical information (in blue) in order to elaborate on certain aspects (e.g. what is special about the ``kamikaze'' type of drone). Further, the analyst can also draw some interesting insights (in red) based on the information in the summary. Overall, this shows that SmartBook can act as a good starting point for analysts to expand upon for the generation of situation reports.

%Table \ref{tab:human_edits} visualizes several examples of generated summaries and their expert revisions. Overall, the generated summaries are all grammatically and linguistically fluent. Human analyst editors may tend to elaborate on certain information. \Yi{For example, in an automatic clustering and generation about the role that kamikaze drones play in Ukraine, an expert analyst may add what is special about the "kamikaze" type of drone (e.g., these drones attack once and don’t come back; they are produced from Iran; etc.)}.
%\ken{I think you can consider adding an unsupervised multi-document summarization method as a baseline (e.g., manling's paper or OTExtSum) }

%\clearpage

\subsection{Error Categorization for Summaries}
\label{subsec:error_categories}
In Section \ref{subsec:human_expert_editing}, we saw that 15\% of the summaries had no errors and were approved by an expert analyst as needing no edits. To further understand the different types of errors in the remaining summaries, we asked the analyst to categorize the errors within them. The analyst was also shown the strategic question and the corresponding extracted contexts that were used to generate the summary, with the summary error categories being as follows:

\begin{itemize}
    \item \textit{No relevant contexts:} None of the extracted contexts are relevant to the question (and thereby the summary is expected to be fully irrelevant too).
    \item \textit{Inaccurate information in summary:} Summary has incorrect information, that is not reflective of the underlying input contexts.
    \item \textit{Incoherent summary:} Summary is incomprehensible and unclear.
    \item \textit{Incomplete summary:} Important information in the input contexts is missing in the summary.
    \item \textit{Irrelevant information in summary:} Summary has material that is not relevant to the question, despite some extracted contexts being relevant.
\end{itemize}

\begin{wrapfigure}{r}{0.4\linewidth}
\vspace{-2em} 
        \centering
        \includegraphics[width=0.85\linewidth]{figures/error_categories.png}
        \caption{Distribution of different error categories for the summaries within SmartBook.}
        \label{fig:error_cat}
        \vspace{-2em}  
\end{wrapfigure}

Figure \ref{fig:error_cat} shows the distribution of error categories for the summaries. It can be seen that incompleteness of summaries is a predominant error, with more than 50\% of the summaries missing important information or not being sufficiently complete. Also, very few summaries are incoherent, which is expected, since large language models such as GPT-3 have been shown \cite{goyal2022news, zhang2023benchmarking} to generate fluent and easy to read output. However, the hallucination problem \cite{ji2022survey, tam2022evaluating} of such LLMs is evident from the considerable number of summaries with inaccurate information. 


%In Figure \ref{fig:error_cat}, we summarize the common causes of text edits by the expert analysts in our generated summaries. Out of the edits that occurred, the most common reason (71.3\%) is \textit{\textbf{coverage/completeness}}, in which the summary had material \textit{\textbf{not relevant}} to the analytic question. The second most common reason (56.4\%) is because the summary did not sufficiently address the analytic question.

%Coherence – the summary was difficult to understand
%Accuracy – inaccurate information presented in the summary

%\begin{figure}[htbp]
 %   \centering
 %   \includegraphics[height=6cm]{figures/error_categorization.pdf}
 %   \caption{Coverage/Completeness and Relevance were the most commonly identified error categories during expert analyst revision.}
 %   \label{fig:error_cat}
%\end{figure}

%\subsection{Remaining Challenges}


%\subsection{SmartBook helps Expert Analyst overcome Time-Sensitivity, in Intelligence Report generation}
%\Yi{TODO (Revanth?): Add experimental results on time comparison; human writing intelligence report with the help of SmartBook should likely be faster}




