\section{Formulation of Situation Report Generation}\label{sec2}

% \vicki{Should this chapter be formulated as Task Definition? We can also clarify input/output here}

A situation report is a document that an intelligence analyst produces as part of their work in collecting, analyzing, and disseminating critical information. The report is typically focused on a specific issue or topic and provides a comprehensive overview of the information available on that topic. The purpose of a situation report is to provide an accurate and objective assessment of a situation or issue, based on the analysis of available data and information. The ultimate goal of such reports is to help policy-makers, military officials, and other experts keep track of developments and provide them with the necessary information to make informed decisions and take appropriate actions. 

With this in mind, we aim to generate a situation report, given a collection of documents from a variety of sources, that embody the below characteristics: 
\begin{itemize}
    \item \textbf{Information salience}: The report should be able to automatically identify and extract the most relevant and crucial information across multiple documents. Such information may include key events, trends, statistics, or developments related to the subject of interest. \\%Utilizing natural language processing algorithms and techniques, such as topic modeling and named entity recognition, can aid in the identification of salient information.
    
    \item \textbf{Having a logical structure}: A situation report needs to have a clear logical structure to help the reader understand, follow, and easily access information. Such logical structure can be imposed in the form of chapters, with individual section headings within each chapter being descriptive enough to give a high-level idea of the main points of the chapter.\\
    
    \item \textbf{Organized as timelines}: Situation reports cover event progressions over considerably long periods of time. Hence, it is beneficial to organize such reports in the form of timelines, which enables seamless report updates with new events. Further, timelines present events in an easy-to-follow chronological order, as well as identify potential trends or patterns that may emerge over time.\\
    
    \item \textbf{Grounded factual content}: To be reliable, a situation report must be grounded in verifiable sources. Grounded factual content helps to build credibility and trust in the situation report as grounding provides the ability to cross-check the presented information before the report can be used to inform policy and strategic planning.
\end{itemize}

%\vicki{question: what is `it`?}
For our formulation of the automatic generation of situation reports, we narrow the scope with regard to the following aspects:

\begin{itemize}
    \item \textbf{Restriction to news}: While situation reports can be created from a variety of data sources such as news articles, open-source materials, and classified information, we limit the knowledge sources to be primarily news articles. This restriction is motivated by the widespread availability of news articles and the verifiability of news publishers.\\
    \item \textbf{Withholding conclusions and recommendations}: A typical situation report is expected to contain the analyst's conclusions and suggestions, in addition to providing insights and implications of various events. Instead, our focus for automatic report generation is on providing strategic insightful information, and grounding it to reputable sources, while leaving the subjective part of drawing conclusions and making recommendations to the human experts.
\end{itemize}


\section{Method}\label{sec3}

% \vicki{TODO: for each subsection emphasize motivations and skip some details, make less like demo}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/SmartBook_arch2.png}
    \caption{\small Overall workflow for constructing \textit{SmartBook}. Given the articles corresponding to a specific timeline, the figure shows the process for obtaining the chapters, their section headings, and the corresponding section content.}
    \label{fig:overall_workflow}
\end{figure}

%\begin{wrapfigure}{r}{0.61\linewidth}
%\vspace{-2em} 
%        \centering
%        \includegraphics[width=1.0\linewidth]{figures/SmartBook.png}
%        \caption{\small Overall workflow for constructing \textit{SmartBook}. Given the articles corresponding to a specific timeline, the figure shows the process for obtaining the chapters, their section headings and the corresponding section content.}
%        \label{fig:overall_workflow}
%        \vspace{-1.5em}
%    \end{

%In this section, we outline our framework for the automatic generation of intelligence analysis reports based on a corpus of news articles, organized in the form of timelines to ensure a coherent, chronological representation of event developments. Each timeline spans a duration of two weeks, offering a manageable and focused analysis while effectively capturing significant news events. Major events identified within the two-week timeline duration serve as the foundation for corresponding chapters in the intelligence report. To further enhance clarity and coherence, we implement a logical structure for each chapter by automatically generating section headings in the form of strategic questions related to the major event. These individual sections comprise the core of our intelligence report's content and contain grounded, query-focused summaries that directly address the strategic questions, providing readers with a comprehensive understanding of the event's context and implications in a single, cohesive paragraph.
In this section, we describe our framework for the automatic creation of situation reports given a corpus of news articles. Our automatic situation report is organized in the form of timelines to provide a coherent, chronological representation of event developments, facilitating users tracking and understanding of situation context. Each timeline spans a duration of 2 weeks, allowing for manageable capturing and focused analysis of significant newsworthy occurrences.
Building upon this, major events identified within the 2-week timeline duration serve as the foundation for corresponding chapters (section \ref{subsec:major_event}). To further guide detailed chapter analysis, we incorporate a logical structure through automatically generating section headings in the form of strategic questions relating to each major event (section \ref{subsec:section_headings}). 
These individual sections comprise the core content of our situation report and contain grounded, query-focused summaries which address strategic questions (section \ref{subsec:content_gen}), providing readers with a comprehensive understanding of event context and implications.
Figure \ref{fig:overall_workflow} gives an overall workflow diagram for our approach to constructing \textit{SmartBook}, with each of the steps detailed below.

\subsection{Major Events in Timelines as Chapters}\label{subsec:major_event}

For each timeline, we first identify major events which are in the form of news clusters within the timeline. %(section \ref{subsubsec:identify_events})
To facilitate information navigation, we then introduce how to derive a high-level chapter name for each  event cluster, and use it as keyphrase to retrieve additional news articles for enriched situation report generation.% (section \ref{subsubsec:name_chapters}).
%\st{further query for relevant news corpus expansion} } %The natural question that then follows is how to
%\st{Given a cluster of news articles that correspond to a major event, we automatically  obtain a name for the event (section} \ref{subsubsec:name_chapters}\st{), which aids our situation report generation as follows: a) The event name is used as the name for the chapter within the timeline, b) The event name is queried against a larger news corpus to obtain an expanded set of event-specific news articles.}
%and obtain corresponding event-specific news corpora. 


%\subsubsection{Identifying Clusters for Major Events}\label{subsubsec:identify_events}

Our approach to identifying major events within a timeline begins with collecting news articles\footnote{Specifically, we use a running list of daily news snippets provided by CNN. For example \url{https://edition.cnn.com/europe/live-news/russia-ukraine-war-news-12-21-22/}} for a 2-week duration corresponding to the timeline. 
%\st{\footnote{For example \url{https://edition.cnn.com/europe/live-news/russia-ukraine-war-news-12-21-22/}} and aggregate them for the 2-week duration. The motivation behind using these daily snippets is two-way: a) these snippets are in the form of concise informative summaries, and b) the snippets are released at frequent durations, meaning they can be clustered to identify the major event they talk about.}
As the number of major events is unknown apriori, we cluster the daily news summaries into major event groups utilizing the agglomerative hierarchical clustering algorithm \cite{jain1988algorithms}, based on their term frequency-inverse document frequency (TF-IDF) scores, which assign higher weights to rare or document-specific words \cite{sparck1972statistical}.
%\st{We leverage TF-IDF} \cite{sparck1972statistical} \st{for clustering the daily news snippets to group those about the same major event. Simply put, the TF-IDF score for a word in the document combines its relevance via term frequency (TF) with the inverse document frequency (IDF) which assigns higher weights for rare or document-specific words.
%We use agglomerative hierarchical clustering} \cite{jain1988algorithms} \st{to group the documents based on their TF-IDF scores. This clustering method is chosen because the number of major events or clusters is unknown apriori (unlike in K-means).} 
Finally, we are left with clusters of news snippets, each providing a focused view to a major event. But because news summary snippets are condensed in detail, we want to enrich the comprehensiveness of major event cluster with chapter name and news corpus expansion, as detailed next.


%Now, we describe how we leverage TF-IDF for clustering the daily news snippets to group those about the same major event. We start by preprocessing each news snippet to remove stop words, punctuation, and other unnecessary information, and then stem and lemmatize the words to reduce the data dimensionality. For each preprocessed document, we create a matrix of word frequencies, corresponding to the term frequency (TF) part of the TF-IDF formula, using scikit-learn's \textit{TfidfVectorizer}. We also calculate the inverse document frequency (IDF) for each word by dividing the total number of documents by the number of documents that contain the word, to assign higher weights to rare or document-specific words. \vicki{too low-level detail?}

%The text of each snippet is preprocessed by removing stop words, punctuation, and any other unnecessary information. We also stemmed and lemmatized the words to reduce the dimensionality of the data.

%Next, we created a matrix of word frequencies for each document using the term frequency (TF) part of the TF-IDF formula. We used scikit-learn's \textit{TfidfVectorizer} to accomplish this. We then calculated the inverse document frequency (IDF) for each word in the matrix by dividing the total number of documents by the number of documents that contain the word. This gave higher weights to words that are more rare or specific to a particular document.
%We combine TF and IDF, through their dot product, to get the TF-IDF score for each word, and then use agglomerative hierarchical clustering \cite{jain1988algorithms} to group the documents based on their TF-IDF scores. 


%\subsubsection{Naming Chapters from Event Clusters}\label{subsubsec:name_chapters}
A key to further enriching event cluster representation is to derive a corresponding short chapter name, to facilitate information readability and retrieval, for the timeline within the situation report.
%\st{We name each major event as a chapter for the timeline within the situation report. Here, we describe our approach for generating a short event name given the event cluster.} 
To achieve this, we utilize a sequence-to-sequence transformer-based \cite{vaswani2017attention} language model, BART \cite{lewis2020bart}, that has been trained on the NewsHead 
 dataset \cite{headline2020} for multi-document news headline generation. Each cluster within NewsHead contains up to five news articles and a crowd-sourced headline of up to 35 characters which describes the major information covered by the story in the cluster. Specifically, we take the concatenated title and text from all the news snippets within the event cluster as input sequences into the BART language model to generate a short event heading.
%\st{use a sequence-to-sequence transformer-based} \cite{vaswani2017attention} \st{pretrained model, specifically BART} \cite{lewis2020bart}\st{, that takes the concatenated news snippet title and text as input (all the news snippets within the event cluster are concatenated into an input sequence) to generate a short event heading as the output. This event heading is used as the chapter name within our situation report. Our BART model is trained using NewsHead} \cite{headline2020}\st{, a multi-document news headline generation dataset.}
With the generated chapter name, we query google news to obtain an expanded set of news articles relevant to the event. In particular, we get news articles corresponding to the given timeline by formatting the query as follows, \textcolor{gray}{query = \textit{$<$Chapter name$>$ after:$<$Timeline start date$>$ before:$<$Timeline end date$>$}}, with start and end dates in the \textcolor{gray}{\textit{yyyy:mm:dd}} format.




\subsection{Strategic Questions as Section Headings}\label{subsec:section_headings}
Beyond simply describing event details in each major event chapter, SmartBook aims to provide information from a strategic perspective that can help aid decision-making and policy planning.
%\st{For each chapter (which corresponds to a major event), SmartBook aims to provide information that can help aid decision-making and policy planning. In this regard, the content within each chapter analyzes the event from a strategic perspective, by extending beyond simply describing the event details to looking with an investigative lens, such as identifying possible motivations of the actors in the event and future implications of the event.}
Thus, in this work, we generate chapter content with the use of questions that cover various strategic aspects of the event, which include  the possible motivations of the actors in an event and the future implications of the event. These strategic questions are organized in the form of section headings to incorporate structure into the chapter. As we detail next, the event-related questions are generated by prompting GPT-3 \cite{brown2020language} %(section \ref{subsubsec:question_gen}) 
with a grounded context in the form of news articles from the event cluster. The generated questions then undergo a post-processing step of de-duplication 
%(section \ref{subsubsec:postprocess_question}) 
to ensure clear and unique section headings. 
%\Yi{Overall comment: I don't think we need two subsections underneath here because each is a paragraph and so we might not want to chop it up too much to break cohesive flow and make paper read like incremental hacks (?)}

%\clearpage

%\subsubsection{Generating Strategic Question Sets using GPT-3}\label{subsubsec:question_gen}
%\begin{wrapfigure}{r}{0.61\linewidth}
%\vspace{-2em} 
%        \centering
%        \includegraphics[width=0.96\linewidth]{figures/gpt3_prompt.png}
%        \caption{\small An example input prompt to GPT-3 while generating strategic questions for a chapter.}
%        \label{fig:gpt3_prompt}
%        \vspace{-2em}  
%    \end{wrapfigure}
Recent work \cite{sharma2021generative, wang2022towards} has shown that GPT-3 is capable of generating natural questions that require long-form and informative answers, in comparison to existing approaches \cite{murakhovska-etal-2022-mixqg, du-etal-2017-learning} that mainly generate questions designed for short and specific answers. In this work, we prompt GPT-3 to explicitly generate strategic questions about the event. Further, to mitigate hallucinations \cite{ji2022survey, maynez2020faithfulness} which are common in large language models, we ground the input context to GPT-3 with news articles from the event cluster (from section \ref{subsec:major_event}) to ensure the generated questions are relevant to the event. Figure \ref{fig:overall_workflow} contains an example with the prompt used to generate strategic questions with GPT-3 for a chapter named \textit{Retreat of Russian Troops from Lyman}. News articles within the context are shown, with the prompt \hl{highlighted} in bold at the end. 
To ensure diversity in the generated questions, we sample multiple question sets from GPT-3 using nucleus sampling \cite{holtzman2019curious}. 

%\subsubsection{Merging Question Sets by De-duplication}\label{subsubsec:postprocess_question}
%\noindent \textbf{Question De-Duplication}\quad 
Within the generated question sets, we observe that questions may sometimes be repeated across different sets. Figure  \ref{fig:overall_workflow} contains an example, with the duplicate questions within the generated question sets marked in \textcolor{blue}{blue}. The problem is similar to that of detecting duplicate question pairs given a large collection of questions. For this, we leverage a publicly available\footnote{\url{https://huggingface.co/cross-encoder/quora-roberta-large}} RoBERTa-large \cite{liu2019roberta} model trained on the Quora Duplicate Question Pairs\footnote{\url{https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs}} dataset. Given a pair of questions, the model predicts a score between 0 and 1 on how likely the two given questions are duplicate. We follow this approach for filtering out duplicate questions in order to merge the multiple question sets generated by GPT-3 into a single set of diverse and unique strategic questions about the event. %in order to

%\begin{figure}[!htb]
%\vspace{-0.75em}
%    \centering
%    \includegraphics[width=1.0\linewidth]{figures/deduplication.png}
%    \caption{\small Example showing the final question set after de-duplication of the question sets generated from the prompt in figure \ref{fig:gpt3_prompt}. Duplicate items within the question sets are shown with the same colour.} 
%    \label{fig:deduplication}
%    \vspace{-1em}
%\end{figure}

\subsection{Grounded Summaries as Section Content}\label{subsec:content_gen}
With the strategic questions obtained for each chapter as section headings, we can now incorporate query-focused summarization to generate section content, by treating the strategic question as query and the event-specific news articles as background corpus.
%\st{We approach section content generation using a query-focused summarization methodology, with the strategic question (section heading) as the query and the event-specific news articles as the background corpus.} 
Our approach employs an extract-then-summarize framework, where we first identify relevant context for the strategic question corresponding to the section. % and then combined them into a summary. 
Specifically, we leverage a question-driven claim extraction approach (section \ref{subsubsec:claim_extraction}) to identify relevant claims within news articles. %\st{that are related to the strategic questions}. \st{Then, the extracted claims are validated (section} \ref{subsubsec:content_validation}) \st{to filter for the most relevant ones.} 
%\st{Finally, the relevant claims (and corresponding contexts) are summarized (section 4.3.3) to obtain the section content.}
Finally, to obtain the section content, we summarize the relevant claims and their corresponding contexts using a novel prompting mechanism, which also generates citations linking each summary fragment to its source (section \ref{subsubsec:summary_generation}). 
This citation linking \textit{not only} helps ground the summary fragment generation to appropriate factual input context \textit{but also} ensures that experts can cross-check and verify information as needed, enhancing the reliability of SmartBook situation report for decision-making assistance.

%\st{Given the application of situation reports for decision-making, it is crucial to link the generated summary fragments to their appropriate sources, so that experts can cross-check and verify if needed. Hence, the section content has citations automatically added, in order to ground fragments in the generated summary to the appropriate factual input context.}

\subsubsection{Question-Driven Claim Extraction with Validation}
\label{subsubsec:claim_extraction}
%In order to gather information for an intelligence analysis report, it is necessary to identify and evaluate various hypotheses that can help to explain the situation at hand. This typically involves extracting relevant claims from source documents, such as news articles, which can be a time-consuming and costly process when done manually. However, recent research has shown that directed topic-specific queries, such as strategic questions, can be used to identify claims within news articles that are relevant to a particular event. To accomplish this, we employ a Question Answering (QA) approach using a transformer-based model, which takes snippets of the news corpus (each consisting of 300-350 words) as input along with the strategic question and outputs short answers to these questions.
Traditionally, a situation report requires foraging for different claims and hypotheses from the source documents (i.e., news articles) that help explain a situation \cite{toniolo2023human}, which is expensive to obtain through manual crowd-sourcing.
%In order to gather information for an intelligence analysis report, one must identify hypotheses that could potentially explain the situation. Traditionally, this involves manually sifting through source documents, such as news articles, which can be a time-consuming and expensive process.
However, recent work \cite{reddy2022newsclaims, reddy2022zero} has shown that directed queries, such as strategic questions in our case, can be used to automatically extract claims from news articles relevant to a particular topic. Following \cite{reddy2022zero}, we adopt a Question Answering (QA) formulation to identify claims relevant to a given strategic question. Specifically, we design a QA pipeline, utilizing a transformer-based RoBERTa-large encoder model \cite{liu2019roberta} variant\footnote{We use the \href{https://huggingface.co/tasks/question-answering\#inference}{question-answering} pipeline provided by huggingface.} that has been trained on SQuAD 2.0 \cite{rajpurkar2018know} and Natural Questions \cite{kwiatkowski2019natural}. The pipeline takes as input the news corpus split into snippets %containing 300-350 words each
along with the strategic question, and outputs short answer extractions to these questions. 
%in snippets of the news corpus (split into 300-350 words each), and the strategic question, as input. %The model is an extractive question answering system that utilizes RoBERTa-large \cite{liu2019roberta} as the underlying encoder, and is trained on SQuAD 2.0 \cite{rajpurkar2018know} and Natural Questions \cite{kwiatkowski2019natural}.
%The news corpus is split into snippets (300-350 words) which are passed as input, along with the strategic question, to a transformer-based QA model\footnote{We use the \href{https://huggingface.co/tasks/question-answering\#inference}{question-answering} pipeline provided by huggingface.} to identify short answers to these questions. 
The identified short answers are then expanded, by including the 3-sentence window around it to provide additional context. 

However, there is still a risk of false positives \cite{tan2018know, chakravarti2021towards} being identified as candidate answers with high confidence, thus necessitating the validation \cite{reddy2020answer, zhang2021joint} of extracted answers. To this end, we employ an answer sentence selection model \cite{Garg_2020} that validates each of the extracted contexts (from Section \ref{subsubsec:claim_extraction}) separately against the strategic question. We concatenate the question and extracted context as input to a binary classification model\footnote{Model is available at \href{https://github.com/alexa/wqa_tanda\#tanda-models-transferred-on-asnq-then-fine-tuned-with-wiki-qa}{TANDA github}.}, with an underlying RoBERTa-large backbone, that is trained on Natural Questions \cite{kwiatkowski2019natural} and WikiQA \cite{yang-etal-2015-wikiqa}. The output of the model is a validation score, between 0 (incorrect answer selection) and 1 (correct answer selection), used to select the top-5 relevant contexts for summarization.

Situation reports often rely on a variety of sources, including text, images, videos, and audio recordings, to provide a holistic view of events. Different modalities contain complementary information, which can help gain a more comprehensive understanding or reveal novel insights. Figure \ref{fig:multimodal_example} shows an example of how images provide additional evidence to the claims in SmartBook. Hence, for each relevant claim, we also link the associated multimedia knowledge elements by running our publicly available multimedia knowledge  
extraction system GAIA~\cite{li2020gaia}.
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/multimodal_example.png}
    \caption{Figure showing an example for how multimodal information (in the form of images) supports and provides additional context to the claims presented in SmartBook. In this example, the presence of anti-aircraft weapons (as seen in the image) in Ukraine provides background for the discussion in NATO on whether to impose a no-fly zone.}
    \label{fig:multimodal_example}
\end{figure}


%\Yi{Combined three paragraphs into one here for flow, to promote method connection; if don't approve, then I suggest let's split into two paragraph as opposed to three} 

%Foraging for information to incorporate into an intelligence analysis report involves identifying different hypotheses which can help explain the situation. In practice \cite{toniolo2023human}, this involves extraction of relevant claims that have been presented in the source documents (which are news articles in this case) and is typically done via expensive manual crowd-sourcing. Recent work \cite{reddy2022newsclaims, reddy2022zero} has shown that claims within news articles relevant to a topic (in this case, the major event) can be identified using directed topic-specific queries (in this case, the strategic questions). Following \cite{reddy2022zero}, we approach the extraction of claims relevant to a topic as a Question Answering (QA) task, with the news articles corresponding to the event as the background corpus from which these contexts need to be identified. 

%The model is an extractive question answering system \cite{alberti2019bert}, which uses Roberta-large \cite{liu2019roberta} as the underlying encoder and is trained on SQuAD 2.0 \cite{rajpurkar2018know} and Natural Questions \cite{kwiatkowski2019natural}. The extracted short answers are then expanded by considering a 3-sentence window around the answer as the answer context for the given question.
%\subsubsection{Claim Validation} \label{subsubsec:content_validation}

%Extractive question answering systems can identify false positives \cite{tan2018know, chakravarti2021towards} as candidate answers with high confidence, thereby requiring validation \cite{reddy2020answer, zhang2021joint} of the extracted answers. For answer validation, we leverage an answer sentence selection model \cite{Garg_2020} that validates each of the extracted contexts (from section \ref{subsubsec:claim_extraction}) separately against the given strategic question. The question and extracted context are concatenated and passed as input to a Roberta-large \cite{liu2019roberta} based binary classification model\footnote{Model is available at \href{https://github.com/alexa/wqa_tanda\#tanda-models-transferred-on-asnq-then-fine-tuned-with-wiki-qa}{TANDA github}.} that is trained on Natural Questions \cite{kwiatkowski2019natural} and WikiQA \cite{yang-etal-2015-wikiqa}. The model outputs a score in the range of 0 to 1, which is used as the validation score. The top-5 contexts with the highest validation scores are then picked as the relevant contexts to use for summarization. 
%\Yi{Note: Took out the subsection header here for claim validation to improve methodology flow and not make everything too choppy/hacky}

\subsubsection{Grounded Summary Generation with LLMs}
\label{subsubsec:summary_generation}

%\begin{wrapfigure}{r}{0.61\linewidth}
%\vspace{-2em} 
%        \centering
%        \includegraphics[width=1.0\linewidth]{figures/gpt3_summary_prompt.png}
%        \caption{\small An example input prompt to GPT-3 for combining the relevant claim contexts into a grounded summary to be used as section content.}
%        \label{fig:gpt3_summary_prompt}
%        \vspace{-2em}  
%    \end{wrapfigure}

Given the set of relevant claim contexts for a strategic question, we aim to generate a concise summary as \textbf{section content} within SmartBook chapters. Observing that humans overwhelmingly prefer summaries from prompt-based large language models (LLMs) \cite{brown2020language, chowdhery2022palm} over models fine-tuned on article-summary pairs \cite{lewis2020bart, zhang2020pegasus, liu2022brio} due to better controllability and easier extension to novel scenarios \cite{goyal2022news, bhaskar2022zero, reddy2022sumren}, %. Such zero-shot prompt-based approaches for summarization have been shown \cite{goyal2022news} to have better controllability and easy extension to novel scenarios. 
%Specifically, 
we leverage the Instruct-tuned 175B GPT-3 model (text-davinci-003) \cite{brown2020language, ouyang2022training} for zero-shot summarization. %via prompting. 
Specifically, we concatenate the top-5 most relevant contexts (from Section \ref{subsubsec:claim_extraction}) as input, along with an instruction along the lines of \textit{``summarize the above, regarding $<$strategic question X$>$, with citations''}, to form the input prompt for the summary generation. It is noteworthy that the use of citations from the input text context in the language model prompting is novel and impactful, as it ensures the generated summary is accurate and trustworthy. Moreover, this approach also enables users to easily verify the summary information against sources used in the input. Ultimately, the intuitive and structured incorporation of timeline chunking, major event chapter clustering, and query-focused section summaries within chapters enables our proposed SmartBook formulation to generate insightful reports for time-sensitive, emerging situations. 

%SmartBook presents a structured and intuitive formulation for generating insightful situation reports through incorporating timeline chunking of news, major event chapters, and query-focused summaries as section content within chapters.} 




