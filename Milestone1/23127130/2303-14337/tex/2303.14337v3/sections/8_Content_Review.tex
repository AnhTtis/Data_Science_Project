

\section{Content Review: Evaluating the Generated Content}
\label{sec:content_review_study}

%\heng{It will be more exciting to add the examples comparing with gpt+bing, including pages 9-14, 81-82 and in this set of slides: https://blender.cs.illinois.edu/ALTA\_NLP\_SmartBook\_HengJi.pdf}


In this section, we seek to evaluate the quality of the text summaries generated by \textsc{SmartBook}. Our approach is complementary to the usability study in \S{\ref{sec:usability_study}}, which assessed whether \textsc{SmartBook} is useful to intelligence analysts and decision-makers. We first evaluate the system's summaries for readability, based on coherence, relevance, and tactical usefulness of the information (in \S{\ref{sec:readability_study}}). Next, we evaluate whether the automatically generated situation reports can serve as a useful preliminary draft with an editing study to ascertain how extensively an expert analyst edits the summaries to deem them acceptable (in \S{\ref{sec:editing_study}}).

%To do so, an evaluation schema was developed by the expert intelligence analyst, based on coherence, relevance and tactical usefulness of the information. 
\subsection{Participant Recruitment}
%\revanth{Should we be calling the students as text evaluators or text annotators or text analysts?}
The studies involved two participant groups: one senior expert analyst (SA1) and six text evaluators (TE1-TE10). 
The expert, affiliated with a US national defense and security firm, possessed a decade of experience in intelligence analysis, training, and AI-based tool assessment. This included researching intelligence tutoring systems and testing AI-based search tools for analysts. The text evaluators, all fluent in English and US-based, held Computer Science degrees ranging from Bachelor's to PhDs, and had prior text annotation experience. There were three natural language processing graduate students and three undergraduates in computer science. The participants reflected the composition of a training unit, with the senior expert providing supervision and training in system evaluation. Compensation varied between groups, with the expert working under a research contract and evaluators receiving \$20 per hour.
%Two types of participants were recruited: (1) an expert analyst and (2) 6 text evaluators. The expert analyst, sourced through a partnership with an independent US-based national defense and security firm, had 10 years of experience collaborating with intelligence analysts, particularly in the realms of training and tool evaluation. This included researching intelligence tutoring systems and testing AI-based search tools with for analysts. The six text evaluators had advanced degrees in Computer Science (ranging from Bachelors to PhDs) and prior experience in text annotation. They included three natural language processing graduate students and three computer science undergraduates, all fluent in English and based in the US. The recruitment of a single expert analyst and six text evaluators was curated to reflect the dynamics of a training unit, where the text evaluators underwent supervision and training for system evaluation by the expert analyst trainer. Compensation was structured differently for each group: the expert analyst operated under an independent research contract, while each text evaluator received 20 USD per hour. 

%The intelligence analyst trainer had 10 years of experience in intelligence analyst training and intelligence tool evaluation, and was sourced through a partnership with an independent US-based national defense and security firm. 
%Next, six text analysts with advanced degrees in Computer Science (ranging from Bachelors to PhDs) and at least a year of text annotation experience were recruited. These included three natural language processing graduate students and three computer science undergraduate students, all fluent in English and based in the US. The recruitment of the single intelligence analyst trainer and six text analysts was curated to reflect a trained unit dynamic, where the text analysts underwent supervision and training by the intelligence analyst trainer for the system evaluation. Compensation was structured differently for each group: the intelligence analyst operated under an independent research contract, while each text analyst received 20 USD / hour. 

\subsection{Readability Study}
\label{sec:readability_study}

\subsubsection{Baselines}

\textsc{SmartBook} leverages question-driven claim extraction (as described in \S{\ref{sec:smartbook}}) to pinpoint relevant information, from which it then generates the final summary. For baseline comparisons, we consider two alternatives. One is a \textit{query-focused summarization} baseline that takes the news articles as input directly (no extraction) and uses a large language model (LLM) to generate a query-focused summary\footnote{We use the same model to generate SmartBook and query-focused summaries. The prompt used is the same as in the summary generation example in Figure \ref{fig:overall_workflow}, except the entire news article texts are passed as context.}. This baseline's query here corresponds to the \textsc{SmartBook}'s strategic question. 
Further, since \textsc{SmartBook} uses news articles that are relevant to the event as the background corpus, we also compare it against a second web search + LLM baseline, using the web as the background corpora, first obtaining relevant web pages from internet search and then summarizing them using a large language model. This second baseline simulates a competitive strategy of directly obtaining information from an LLM-enabled web search engine\footnote{Specifically, we leverage \href{https://www.perplexity.ai}{perplexity.ai}, which combines web-search with LLM summarization. The query used is the strategic question, along with a phrase ``\textit{with respect to the Ukraine-Russia crisis between $<$timeline$>$}'' to ground the query.} (the \textit{web search + LLM} baseline is similar to a Bing search + a ChatGPT query for summary). We randomly selected summaries for 50 of \textsc{SmartBook}'s strategic questions to perform the readability evaluation.

\subsubsection{Study Setup}

For each generated summary, three text evaluators are asked to assess in terms of coherence, relevance, and usefulness with the scores ranging from 1 (worst) to 5 (best). For coherence and relevance, we follow the guidelines as in \cite{fabbri2021summeval}. Simply, \textit{coherence} measures the quality of all summary sentences collectively regardless of the question. \textit{Relevance} quantifies whether the summary contains key points that answer the given question. We define \textit{usefulness} as an indication of whether the summary provides non-trivial and insightful information for analysts, and suggests the breadth and depth of the provided key points. 

\input{tables/sum_human}
\input{tables/summaries_comparison}

\subsubsection{Results}
%\input{tables/sum_human}

Table~\ref{tab:sum_human} shows the results from the evaluation study of the summaries. We observe that SmartBook outperforms alternative competitive strategies on coherence, relevance, and usefulness. The benefit of our question-driven claim extraction step can be seen from the considerably more relevant summaries within SmartBook, compared to the direct query-focused summarization of the news articles without an explicit extraction step (row (3) vs (1)). Further, we see that directly obtaining information from the internet can give less useful content compared to the focused news-driven summarization within \textsc{SmartBook} (row (2) vs (3)). A set of summary examples is shown in Table \ref{tab:summaries_comparison}.
This evaluation demonstrates across the metrics that \textsc{SmartBook} provides valuable content in its summaries and outperforms the two baselines lacking an extraction step. 

\subsection{Editing Study}
\label{sec:editing_study}

\subsubsection{Study Setup}
% Revise to make it read like more a basic evaluation setup.

 The study was conducted with the expert analyst. As an initial exercise, the expert analyst was provided with 5 strategic questions from \textsc{SmartBook} and wrote summaries without the assistance of AI-assisted tools in the report generation. Following this, the analyst was encouraged to actively explore the features of \textsc{SmartBook}. The analyst was provided with 94 randomly sampled summaries from SmartBook-generated situation reports, and was asked to edit them till their content was acceptable as a professional report from an intelligence analyst. The \textsc{SmartBook} interface to the summaries were provided, and the analyst completed edits in a text editing software of their choice.

\subsubsection{Results}
\label{sec:first_draft}
%\input{tables/summaries_comparison}

 Given the revised summaries from the expert analyst, we aimed to measure the extent to which they made changes to the original summary content. Specifically, these edits were quantified in the form of (a) commonly used token-overlap-based metrics such as BLEU [51] and ROUGE [38] scores, and (b) Levenshtein edit distance [47], which measures the character-level changes required to transform the original summary to the revised summary. %These analyses were completed across all 94 randomly sampled summaries from SmartBook. 
Empirical results indicate that token overlap between generated and post-edited texts were high, with BLEU and ROUGE-L scores respectively at 59.0\% and 74.1\%. These metrics suggest that the generated reports from SmartBook are of sufficiently good-quality, such that extensive human expert revision may not be necessary. However, we acknowledge that a gap still exists between the automatically generated and human expert summary curation, as the Levenshtein edit distance computed at the character level is 34.4\%. Interestingly, 15\% of generated summaries were determined to be perfect, with no edits made by the expert analyst. 

Next, we examined how the edits were made. The general observation was that more content was inserted than was deleted by the expert analyst. The percentage of tokens inserted was 49.6\%, whereas the percentage of tokens deleted was only 2.3\%, suggesting that automated summary generation may generally need to be more detailed. 
While investigating the efficacy of SmartBook as an AI-assisted tool, the expert analyst reported: 

\begin{quote}
"Pointing out to individual claims, while possible, can be hard sometimes since you will be abstracting information across multiple claims." and that "SmartBook saves time." (SA1)
\end{quote}


%By quantitatively and qualitatively measuring the expert analyst's interaction with SmartBook, we understand that it acts a collaborative partner for situation report generation.
\input{tables/table2}
Table \ref{tab:human_edits} shows an example of edits (in color) made by an expert analyst for a machine-generated summary in SmartBook. We can see here that the human analyst added additional tactical information (in blue) to elaborate on certain aspects (e.g. what is special about the ``kamikaze'' type of drone). Further, the analyst can also added some interesting insights (in green) based on the information in the summary. Overall, this shows that \textsc{SmartBook} provides a good starting point for analysts to expand upon for the generation of situation reports. 

%{\textcolor{red}{\textbf{I am NOT seeing any green in Table 5. Was this lost somehow in creating it? if it cannot be found, then the caption should be adjusted and one sentence in the last paragraph, starting with "Further,..." should be dropped  -- Clare}}}

\subsubsection{Error Analysis}
\label{sec:error_analysis}
As noted in \S{\ref{sec:first_draft}}, it is noteworthy that 15\% of summaries produced by \textsc{SmartBook} needed no modifications, highlighting its proficiency in creating acceptable reports in some scenarios without human intervention. This suggests that as technology advances and iterative refinements are applied, this rate will likely improve, reducing the workload for analysts in the future. 
To gain a better understanding of the different types of errors in the remaining summaries, we asked the expert analyst to categorize the errors within them. The analyst was also shown the strategic question and the corresponding extracted contexts that were used to automatically generate the summary. The summary errors were categorized as follows:

\begin{itemize}
    \item \textit{No relevant contexts:} None of the extracted contexts are relevant to the question (and thereby the summary is expected to be fully irrelevant too).
    \item \textit{Inaccurate information in summary:} Summary has incorrect information, that is not reflective of the underlying input contexts.
    \item \textit{Incoherent summary:} Summary is incomprehensible and unclear.
    \item \textit{Incomplete summary:} Important information in the input contexts is missing in the summary.
    \item \textit{Irrelevant information in summary:} Summary has material that is not relevant to the question, despite some extracted contexts being relevant.
\end{itemize}

\begin{wrapfigure}{r}{0.4\linewidth}
\vspace{-2em} 
        \centering
        \includegraphics[width=0.85\linewidth]{figures/error_categories.png}
        \caption{Distribution of different error categories for the summaries within SmartBook.}
        \label{fig:error_cat} 
        \vspace{-1em} 
\end{wrapfigure}

Figure \ref{fig:error_cat} shows the distribution of error categories for the summaries. It can be seen that incompleteness of summaries is a predominant error, with more than 50\% of the summaries missing important information or not being sufficiently complete.  While the predominance of incomplete summaries could be a concern, this can also be framed positively: it represents a conservative approach ensuring that \textsc{SmartBook} does not over-extend based on limited data, and instead offers a foundational understanding.  Other errors corresponded to summaries with inaccurate (18.6\%) or irrelevant (14.6\%) information. Very few summaries were judged incoherent, as expected given that large language models such as GPT-3 have been shown \cite{goyal2022news, zhang2023benchmarking} to generate fluent and easy-to-read output. 


