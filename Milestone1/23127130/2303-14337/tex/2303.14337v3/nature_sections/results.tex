\section{Results}

% First, we present results from a collaborative study (in \S{\ref{sec:analyst_study}}) wherein we understand intelligence analysts' expectations from an AI-assisted situation report generation, and collaboratively source design requirements for the same. To gain an operational understanding of the range of processes that take place during intelligence analysis and generating situation reports, we gather information on: (1) regular practices in authoring situation reports and (2) general needs and expectations from intelligence analysts for AI-driven systems. To gain a better understanding of the composition process of situation reports, we expanded the design opportunities identified with subsequent collaborative design (CD) sessions with intelligence analysts. The goal was to capture tangible design strategies and recommendations from users about how they, as intelligence analysts, navigate, research, and author their situation reports. Using the four design strategies, we developed \textit{SmartBook}, an AI-assisted system for situation report generation that provides analysts with a first-draft report to work from as they respond to time-critical information requirements on emerging events.

% The paper evaluates the SmartBook system using the Ukraine-Russia crisis, chosen for its complexity in political, military, and social aspects, which tests the system's capability in a multifaceted real-world scenario. The crisis's politicized nature and the varying narratives from different media require the system to be accurate and unbiased. 
% To comprehensively evaluate SmartBook and its efficacy in aiding intelligence analysis, our approach encompassed two distinct but interrelated studies: a utility study and a content review. The utility study (in (\S{\ref{sec:utility_study}}) aimed to answer four research questions concerning SmartBook's usability, focusing on its effectiveness for intelligence analysts and decision-makers in generating and interacting with situation reports. Complementing this, the content review (in \S{\ref{sec:content_review}})  assessed the quality of SmartBook's output, focusing on usefulness, readability, and relevance, alongside an editing study where an expert analyst edited the generated reports to understand their utility as a preliminary draft. 

% To demonstrate the capabilities and design philosophy behind SmartBook, our AI-driven system is architected around key features tailored for enhancing the intelligence analysis process: automatic event timeline construction, strategic question formulation, claim extraction, and grounded summary generation. These components are strategically developed to address both the analytical needs of intelligence analysts and the informational demands of decision-makers, providing a comprehensive tool for the generation of nuanced situation reports.

%Our initial exploration involves a qualitative exploration to ascertain SmartBook's efficacy in streamlining report generation, enhancing data accuracy, and improving the user experience for analysts. Beyond mere functional validation, we delve into the practical impact of SmartBook through a series of case studies conducted in collaboration with intelligence agencies. These studies aim to gauge the system's utility in real-world scenarios, focusing on its ability to present complex information in an accessible manner, facilitate rapid understanding of evolving situations, and support strategic decision-making processes, which are used to construct the SmartBook end-user experience.

%In the subsequent sections, we detail the results of our quantitative and qualitative evaluations, offering insights into how SmartBook's features contribute to its overall effectiveness. While a comprehensive technical breakdown of SmartBook’s underlying mechanisms is reserved for the “Methods” section, it is crucial to note that our findings underscore the significance of integrating AI with a user-centric design to empower intelligence analysts and decision-makers alike. 

To comprehensively evaluate \name{} and its efficacy in aiding intelligence analysis, we performed three distinct but interrelated studies: a utility evaluation, a content review and an editing study. The utility evaluation~(in (\S{\ref{sec:utility_eval}}) aimed to answer three research questions concerning \name{}'s usability, focusing on its effectiveness for intelligence analysts and decision-makers in generating and interacting with situation reports. Complementing this, the content review~(in \S{\ref{sec:content_review_study}}) assessed the quality of \name{}'s automatically generated questions and summaries, focusing on readability, coherence, and relevance. Finally, the editing study~(in \S{\ref{sec:editing_study}}) involved an expert analyst editing the generated reports to assess their value as a preliminary first draft.


\subsection{Utility Study: Evaluating the System}
\label{sec:utility_eval}

During this user study, we conducted an interview with semi-structured questions and a post-study questionnaire on the usability of the \name{}. Each user study session was structured in four segments: (i)~an introductory overview, (ii)~a free-form investigation, (iii)~a guided exploration, and (iv)~a concluding reflective discussion. The participants were ten intelligence analysts (from \S{\ref{sec:formative_study}}) and two decision-makers from Canadian government boards. These decision-makers engaged in the initial qualitative study but did not answer subsequent post-study questionnaires, due to time constraints. The study investigated the following research questions:
\begin{itemize}
    %\item \textbf{RQ1:} Can intelligence analysts successfully use \name{} to generate situation reports?
    \item \textbf{RQ1:} How do intelligence analysts interact and leverage the features within \name{}?
    \item \textbf{RQ2:} Do intelligence analysts find \name{} intuitive, usable, trustable and useful?
    \item \textbf{RQ3:} How do decision-makers interact, perceive and use \name{}?
\end{itemize}

 Upon commencement of the study,~(i) participants received a concise introduction to \name{}, outlining its core premise. This orientation was designed to acquaint them with the system without biasing their exploration. Subsequently,~(ii) participants were invited to freely investigate \name{}, with the specific task of exploring a minimum of three questions across five chapters of their choosing. This approach afforded substantial freedom, enabling interactions with the user interface that reflected their natural inclinations and interests. Following the free-form investigation, participants were systematically introduced~(iii) to the \name{} framework. This process involved tracing the journey from a chosen question related to a specific event within a designated timespan, to the corresponding claims, contexts, and sources, resulting in a summarized answer. Subsequently, a semi-structured interview~(iv) was conducted to gather reflective feedback on the participants' experience with \name{}, focusing on its efficacy and potential areas of enhancement. Finally, to conclude the session, participants were asked to complete a post-study questionnaire. This questionnaire was focused on assessing the usability of \name{}, gathering quantitative data to complement the qualitative insights gained from the semi-structured interviews. Upon collecting, discussing and iterating on the data, behaviors and insights were merged into the following themes:

 \begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/new_old_graph.jpg}
    \vspace{-3em}
    \caption{Quantitative results from the post-study questionnaire in the utility evaluation.}
    \label{fig:userstudy}
\end{figure}



\subsubsection{UI Understandability and Interaction}
The structured layout of the user interface (UI) was notably effective, as seen in the free-form investigation~(ii) with a 100\% feature discovery rate (10 out of 10 participants). All participants
successfully engaged with key features, underscoring the UI's design intuitiveness. This high feature discovery rate indicates that the UI aligns with users' cognitive patterns, facilitating
intuitive interaction without extensive training, providing us with a positive response to \textbf{RQ1}. This sentiment was further emphasized in the post-study questionnaire. As seen in the results in Figure~\ref{fig:userstudy}, 70\% of participants strongly agreed that most intelligence analysts would learn to use the system very quickly, while 80\% strongly disagreed that the system was difficult to navigate and use. 
    However, we observed mixed views on UI flexibility, specifically in personalization and customization. While most found the tool easily integrateable into their workflow, a minority (30\%) remained neutral. When further prompted, two participants described the UI as a ``one size fits all'' solution, lacking personalization.

\subsubsection{Building Trust on \name{}} 
In the study, trust in the \name{} system developed progressively, resembling the formation of trust in a human analyst. Initially, users exhibited skepticism, thoroughly scrutinizing the system's sources and assertions. This included validating the authenticity of sources, the accuracy and pertinence of ratings, and the correct representation of context. As users became more acquainted with \name{} and evaluated its reliability, their dependence on extensive source verification lessened. As can be seen in Figure \ref{fig:userstudy}, three out of ten participants did not feel the need to conduct additional research beyond the presented information to trust the tool, while another three felt additional research was necessary. This trust was found to be context-dependent, with two participants noting that their trust varied based on the ``impact severity'' or the potential negative consequences of disseminating incorrect information. Nonetheless, a majority (eight out of ten) concurred that the information provided was accurate and reliable, a positive answer to \textbf{RQ2}. 

\subsubsection{Perceived Benefits for Intelligence Analysts}
The primary benefit identified was the substantial reduction in time and effort required for compiling and analyzing complex data. \name{}'s automation of initial report generation processes, such as data collection and summarization, was highly valued by analysts as it allowed more focus on thorough analysis and strategic planning. As seen in Figure \ref{fig:userstudy}, all participants agreed on the tool's utility in assisting intelligence analysts in creating situation reports, expressing satisfaction with the system-generated reports, thereby providing a positive response to \textbf{RQ2}. Regarding the necessity of significant edits to the system-generated reports, half participants suggested that this need was not inherently due to report deficiencies but varied based on the report's intended purpose or audience. We later conducted a study (described in \S{\ref{sec:editing_study}}) to understand the extent of edits needed.

\subsubsection{\name{} as a Learning Tool for Decision-Makers}
Decision-makers highly valued \name{}'s ability to rapidly deliver accurate and easily digestible information. Their primary engagement with the interface centered around utilizing the summaries, to learn about different topics in various degrees. The system's effectiveness in simplifying complex data into structured, clear formats was particularly appreciated, to help aid in swift understanding and decision-making processes. Although data lineage and source transparency were recognized as important, these were considered secondary to the primary need for timely and format-specific information delivery. While \name{}'s target users are intelligence analysts, the decision-makers' view highlights \name{}'s dual functionality as both an analytical tool and a decision-support system, providing a key capability for the high-paced and information-intensive needs of decision-makers, addressing \textbf{RQ3}.

\subsection{Content Review: Evaluating the Generated Content}
\label{sec:content_review_study}

Our content review study primarily involves evaluation of the quality of automatically generated strategic questions and summaries within \name{}. The studies involved two participant groups: one senior expert intelligence analyst and six text evaluators. The expert analyst, affiliated with a US national defense and security firm, possessed a decade of experience in intelligence analysis, training, and AI-based tool assessment. The text evaluators, all fluent in English and US-based, involved undergraduate and graduate students with experience in Natural Language Processing.

%\subsection{Designing SmartBook}
%\label{sec:analyst_study}
%The development of SmartBook, an AI-driven system for generating intelligence situation reports, consisted of a two-phase human-centered design approach, encompassing both formative and collaborative design studies. Initially, a formative design phase was instituted to gather foundational insights from users, involving detailed analysis of current practices and challenges faced by intelligence analysts. This phase engaged experienced intelligence analysts through semi-structured interviews and surveys, with the objective of pinpointing precise requirements for AI assistance in report generation. The collected data informed the design of SmartBook, ensuring that the tool's features were directly aligned with the needs of its users. [Insert Reference for Impact of Formative Study]

%Subsequently, the collaborative design phase brought users and developers together to refine and validate the initial design concepts. Through interactive sessions, participants provided real-time feedback on prototype functionalities, emphasizing usability, efficiency, and the accuracy of generated reports. This participatory approach allowed for the iterative improvement of SmartBook, ensuring that the final product not only met but exceeded user expectations. [Input Reference for Impact of Collaborative Design]

%The culmination of these efforts is SmartBook, a tool that integrates AI capabilities with a user-centric design. Key features of SmartBook include the organization of information into chronological timelines, facilitating easy navigation and comprehension of complex events over time. Strategic questions are generated as section headings, guiding the analysis towards crucial insights and fostering a deeper understanding of the implications of reported events. Furthermore, SmartBook employs XXX algorithms to extract relevant claims and synthesize grounded summaries, ensuring that reports are both informative and verifiable. Through this rigorous design process, SmartBook leverages formative and collaborative design methodologies to create a tool that not only serves the practical needs of intelligence analysts but also enhances the decision-making capabilities of its end users.

% Reference each feature through the image.

%\subsubsection{Defining report quality with psychometric evaluations}
%\label{sec:pschometric}


%\subsubsection{Input SubHeading Title Here}
%\label{sec:readability}

\subsubsection{Evaluation of Strategic Questions}

Evaluation of the quality of strategic questions, which are used as section headings in \name{}, is done by drawing parallels to the traditional construction process of a situation report, which involves the participation of
both senior and junior analysts. A senior analyst’s role is usually to come up with strategic questions, which are then passed onto a junior analyst. The junior analyst then gathers tactical information that can help answer or provide more background to the strategic questions. Our human evaluation of the questions within \name{} measures the following aspects, based on guidelines defined by the expert intelligence analyst:
\begin{itemize}
    \item \textbf{Strategic Importance}: Evaluating the strategic importance of a question requires introspection on whether the
question provided within \name{} probes for an insightful aspect of the event. The scoring rubric involves marking the question as one of three categories, namely, `Not Strategic', `Some Strategic Value', and `Definitely Strategic'. %\heng{give examples of strategic and not strategic questions} 
This metric is based on the level of speculative nature, such as questioning the background/motives/reasons behind actions taken by actors (governments/military, etc) involved in the event.
    \item \textbf{Tactical Information}: Another dimension of evaluating the quality of a question is how much relevant tactical information can be gathered using that question. In this case, the question is evaluated based on its corresponding \name{} summary, in terms of whether the question-relevant tactical information in the summary is helpful for an analyst to gain deeper insights into the situation.  Tactical information is defined as content that is neither obvious nor trivially obtained (e.g. cannot be obtained from a news header).  The scoring rubric is categorized as `No information is tactical', `Some information is tactical', and `Most information is tactical'. Table \ref{tab:tactical_info_examples} in supplementary shows examples for summaries with corresponding tactical information.
\end{itemize}

We randomly selected 25 chapters from \name{}, comprising 125 strategic questions, with each question evaluated by three annotators. Results show that most questions are strategic (detailed split in Figure \ref{fig:question_eval} in supplementary), with at least 82\% having some
strategic value. Further, we see that these questions can help gather relevant tactical information in roughly 93\% of the cases, as judged by the annotators that most summaries to these questions have tactically relevant information. When assessing the diversity of strategic questions within each chapter, evaluators noted that 64\% of chapters contained questions addressing different aspects, 28\% had up to two questions on similar aspects, and 8\% included more than two questions on similar aspects.
%Moreover, when evaluated for the diversity of the strategic questions within each chapter, 64\% of the chapters were noted to have questions probing different aspects, 28\% had up to two questions about similar aspects, with the remaining 8\% having more than two questions about similar aspects.

\subsubsection{Evaluation of \name{} Summaries}

Next, we assess the quality of text summaries generated by \name{}, which uses question-driven claim extraction to identify and summarize relevant information. For baseline comparisons, we utilize two methods: firstly, a \textit{query-focused summarization} baseline that processes entire news articles directly with an LLM to generate summaries for the strategic question as the query. This approach uses the same LLM and prompt as in \name{}, except the entire news article texts are passed as input context without an explicit claim extraction step. Secondly, we compare against a web search + LLM baseline, where relevant web pages sourced from the internet serve as the input context, which are then summarized by an LLM. This method simulates using an LLM-enabled web search engine (like \href{https://www.perplexity.ai}{perplexity.ai}), incorporating the strategic question as the query along with a phrase ``\textit{concerning the Ukraine-Russia crisis between $<$timeline$>$}''. Since the LLM-enabled web search was using GPT-3.5~\cite{openai2021gpt35} at the time of experiments, we used GPT-3.5 as the LLM in SmartBook and other baselines for a fair comparison.
We randomly select summaries corresponding to 50 strategic questions from \name{} to perform the evaluation.
%Next, we evaluate the quality of the automatically summarized text. \textsc{SmartBook} leverages question-driven claim extraction to pinpoint relevant information, from which it then generates the final summary. For baseline comparisons, we consider two alternatives. One is a \textit{query-focused summarization} baseline that takes the news articles as input directly (no extraction) and uses an LLM (same as used in SmartBook) to generate a query-focused summary with the strategic question as the query. We use the same model to generate SmartBook and query-focused summaries. The prompt used is the same as in the summary generation example in Figure \ref{fig:overall_workflow}, except the entire news article texts are passed as context. This baseline's query here corresponds to the \textsc{SmartBook}'s strategic question. Further, since \textsc{SmartBook} uses news articles that are relevant to the event as the background corpus, we also compare it against a second web search + LLM baseline, using the web as the background corpus, first obtaining relevant web pages from internet search and then summarizing them using a large language model. This second baseline simulates a competitive strategy of directly obtaining information from an LLM-enabled web search engine (\href{https://www.perplexity.ai}{perplexity.ai}). The query used is the strategic question, along with a phrase ``\textit{with respect to the Ukraine-Russia crisis between $<$timeline$>$}'' to ground the query. We randomly selected summaries for 50 of \textsc{SmartBook}'s strategic questions to perform the readability evaluation.
\input{tables/sum_human}
\input{tables/summaries_comparison}
For each generated summary, three evaluators are asked to assess the coherence, relevance, and usefulness with scores ranging from 1 (worst) to 5 (best). Simply, \textit{coherence} measures the quality of all summary sentences collectively regardless of the question. \textit{Relevance} quantifies whether the summary contains key points that answer the given question. We define \textit{usefulness} as an indication of whether the summary provides non-trivial and insightful information for analysts, and suggests the breadth and depth of the provided key points. 
Table~\ref{tab:sum_human} shows results from the evaluation study. We observe that \name{} outperforms alternative competitive strategies in coherence, relevance, and usefulness. The advantage of the question-driven claim extraction in \name{} is evident, yielding significantly more relevant summaries compared to the direct query-focused summarization without such a step (row (3) vs (1)). Additionally, summaries generated using information sourced from the Web are less useful than those generated through \name{}'s news-driven approach (row (2) vs (3)). Table \ref{tab:summaries_comparison} displays an example showing the difference in outputs. Overall, the evaluation demonstrates that across the metrics, \name{} excels over the baselines in providing high-quality summaries.


%Table~\ref{tab:sum_human} shows the results from the evaluation study. We observe that SmartBook outperforms alternative competitive strategies on coherence, relevance, and usefulness. The benefit of our question-driven claim extraction step can be seen from the considerably more relevant summaries within SmartBook, compared to the direct query-focused summarization of the news articles without an explicit extraction step (row (3) vs (1)). Further, we see that directly obtaining information from the Internet can give less useful content compared to the focused news-driven summarization within \textsc{SmartBook} (row (2) vs (3)). A set of summary examples is shown in Table \ref{tab:summaries_comparison}. This evaluation demonstrates across the metrics that \textsc{SmartBook} provides valuable content in its summaries and outperforms the two baselines lacking an extraction step.

We also evaluated the citation quality in \name{}'s summaries using metrics for citation precision and recall as defined by \citet{gao2023enabling}. Citation recall assesses whether the output sentence is fully supported by the cited context, whereas citation precision identifies any irrelevant citations. These metrics were calculated using the 11B-parameter TRUE model \cite{honovich-etal-2022-true-evaluating}, which is trained on a collection of natural language inference datasets, and is frequently employed to evaluate attribution accuracy~\cite{bohnet2022attributed, gao2023rarr}.  Overall, 97\% of the sentences in \name{}'s summaries included citations, while 29.5\% had multiple citations. We observed a citation precision and recall of 64.7\% and 69.2\%, respectively.
%Finally, we also evaluate quality of citations in SmartBook's summaries. Attribution quality is measured based on the citation precision and recall metrics introduced in~\citet{gao2023enabling}. Citation recall determines if the output sentence is entirely supported by cited context and citation precision identifies any irrelevant citations. The metrics are computed using TRUE~\cite{honovich-etal-2022-true-evaluating}, a 11B-parameter model trained on a collection of natural language inference datasets, commonly used~\cite{bohnet2022attributed, gao2023rarr} to evaluate attribution by checking whether the cited context entails the claims in the sentence. Overall, 97\% of the sentences in SmartBook's summaries have citations, while 29.5\% have multiple citations. We observe a citation precision of 64.7\% and citation recall of 69.2\%.

%\revanth{It's already added in Section 3.1.1} \heng{I remember you have some quantitative evaluation on generated questions, maybe it's good to add that back}

\subsection{Editing Study: Evaluation as a Preliminary Draft}
\label{sec:editing_study}
\name{} serves as an initial draft for intelligence analysts to refine or adapt to their specific needs.  We evaluate \name{}'s effectiveness in producing situation reports through an editing study with an expert analyst, by measuring how much of the content the analyst directly accepts or further edits. The analyst actively explored \textsc{SmartBook}'s features and subsequently edited 94 randomly selected summaries from \name{}-generated situation reports, until they met professional intelligence reporting standards. %The \textsc{SmartBook} interface to the summaries was provided, and the analyst completed edits in a text editing software of their choice.

  Based on the revised summaries by the expert analyst, we assessed the alterations made to the original content. We quantified these changes using token-overlap metrics like BLEU~\cite{papineni2002bleu} and ROUGE~\cite{lin2004rouge} scores, and Levenshtein edit distance, which calculates character-level modifications needed to transform the original into the revised summary.
  %Specifically, these edits were quantified in the form of (a) commonly used token-overlap-based metrics such as BLEU [51] and ROUGE [38] scores, and (b) Levenshtein edit distance [47], which measures the character-level changes required to transform the original summary to the revised summary. %These analyses were completed across all 94 randomly sampled summaries from SmartBook. 
Empirical results show high token overlap between the generated and post-edited texts, with BLEU and ROUGE-L scores respectively at 59.0\% and 74.1\%, indicating that the \name{}-generated reports are of sufficiently good quality and extensive human expert revision may not be necessary. However, we acknowledge that a gap still exists between the automatically generated summaries and human expert curation, as the Levenshtein edit distance computed at the character level is 34.4\%. Notably, 15\% of the generated summaries had no edits made by the expert analyst.

Further analysis revealed that the analyst predominantly added rather than removed content, with insertions at 49.6\% and deletions at 2.3\%. This suggests that automated summary generation may generally need to be more detailed.  
%Next, we examined how the edits were made. The general observation was that more content was inserted than was deleted by the expert analyst. The percentage of tokens inserted was 49.6\%, whereas the percentage of tokens deleted was only 2.3\%, suggesting that automated summary generation may generally need to be more detailed. 
\begin{comment}
While investigating the efficacy of SmartBook as an AI-assisted tool, the expert analyst reported: 

\begin{quote}
"Pointing out to individual claims, while possible, can be hard sometimes since you will be abstracting information across multiple claims." and that "SmartBook saves time." (SA1)
\end{quote}
\end{comment}
\input{tables/table2}
Table \ref{tab:human_edits} shows an example of edits (in color) made by an expert analyst for a machine-generated summary in \name{}. We can see here that the human analyst added additional tactical information (in blue) to elaborate on certain aspects (e.g. what is special about the ``kamikaze'' type of drone). Further, the analystalso added some interesting insights (in green) based on the information in the summary. Overall, this shows that \textsc{SmartBook} provides a good starting point for analysts to expand upon for the generation of situation reports. 

It is noteworthy that 15\% of summaries produced by \name{} needed no modifications, highlighting its proficiency in creating acceptable reports in some scenarios without human intervention. This suggests that as technology advances and iterative refinements are applied, this rate will likely improve, reducing the workload for analysts in the future. 
To gain a better understanding of the different types of errors in the remaining summaries, we asked the expert analyst to categorize the errors within them. The analyst was also shown the strategic question and the corresponding extracted contexts that were used to automatically generate the summary. The summary errors were categorized as follows:

\begin{itemize}
    \item \textit{No relevant contexts:} None of the extracted contexts are relevant to the question (and thereby the summary is expected to be irrelevant too).
    \item \textit{Inaccurate information in summary:} Summary has incorrect information, that is not reflective of the underlying input contexts.
    \item \textit{Incoherent summary:} Summary is incomprehensible and unclear.
    \item \textit{Incomplete summary:} Important information in the input contexts is missing in the summary.
    \item \textit{Irrelevant information in summary:} Summary has material that is not relevant to the question, despite some extracted contexts being relevant.
\end{itemize}

\begin{wrapfigure}{r}{0.3\linewidth}
\vspace{-1.65em} 
        \centering
        \includegraphics[width=0.9\linewidth]{figures/error_categories.png}
        \vspace{-0.5em} 
        \caption{Distribution of different error types for summaries within \name{}.}
        \label{fig:error_cat} 
        \vspace{-1em} 
\end{wrapfigure}

Figure \ref{fig:error_cat} shows the distribution of error categories for the summaries. It can be seen that incompleteness of summaries is a predominant error, with more than 50\% of the summaries missing important information or not being sufficiently complete.  While the predominance of incomplete summaries could be a concern, this can also be framed positively: it represents a conservative approach ensuring that \textsc{SmartBook} does not over-extend based on limited data, and instead offers a foundational understanding.  Other errors corresponded to summaries with inaccurate (18.6\%) or irrelevant (14.6\%) information. Directing the LLMs to reference input documents while generating summaries does improve factuality, aligning with recent findings~\cite{gao-etal-2023-enabling}. Nevertheless, LLMs' tendency to hallucinate~\cite{ji2022survey, tam2022evaluating}, calls for validation and cross-referencing techniques (discussed in \S{\ref{sec:future_extensions}}) to address these issues. Finally, we see that very few summaries were judged incoherent, as expected given that large language models have been shown \cite{goyal2022news, zhang2023benchmarking} to generate fluent and easy-to-read output. 

