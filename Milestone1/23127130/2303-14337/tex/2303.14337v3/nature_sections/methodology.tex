\section{\name{} Framework}


\subsection{System Design}

The development of \name{}, an AI-driven system for generating situation reports, followed an iterative human-centered design approach. The initial phase focused on designing and developing the backend workflow, along with a preliminary frontend interface, and included multiple evaluation stages. Throughout this process, numerous presentations were made to stakeholders in both government and the private sector, which provided valuable feedback. The second phase involved engaging intelligence analysts and decision-makers through formative and collaborative design studies to better understand their needs and expectations.
%The development of SmartBook, an AI-driven system for generating situation reports, consisted of a two-phase human-centered design approach, encompassing both formative and collaborative design studies. 
The formative study phase (\S{\ref{sec:formative_study}}) involves semi-structured interviews to gather insights from users, involving detailed analysis of expectations from AI systems. Subsequently, the collaborative design phase (\S{\ref{sec:collaborative_design}}) brought users and developers together to refine and validate the initial design concepts. Through interactive sessions, participants provided real-time feedback on prototype functionalities to obtain precise requirements for AI assistance in report generation. The culmination of these efforts is a set of design strategies that ensure \name{} incorporates a user-centric design to serve the practical needs of intelligence analysts.

\subsubsection{Formative Study}
\label{sec:formative_study}

The formative study aims to gather information on the general needs and expectations from intelligence analysts for AI-driven systems. The study was conducted on ten intelligence analysts with experience in government and military roles (details on recruitment in Supplementary \S{\ref{sec:analyst_recruitment}}). Over a two-week period, semi-structured interviews were conducted with these analysts to examine their understanding, perspectives, and recommendations regarding AI use in professional settings.
%The authoring experience section focused on participants’ personal experiences in creating intelligence reports, including their methodologies, challenges, and key aspects of their authoring process. %The AI-assisted tools section explored their understanding, attitudes, and recommendations regarding AI use in professional settings.
The study highlighted emerging themes related to analysts’ perceptions and expectations
of AI-assisted authoring tools, which we briefly describe below:
\begin{itemize}
    \item \textit{Viewing technology as a means to enhance human capability}: An overwhelming majority (9 out of 10) emphasized the crucial role of AI in enhancing their capabilities, with these tools regarded not as mere process accelerators, but as essential elements that enrich their work by improving research efficiency, idea generation, and clarity of information. This perspective contrasts with the simplistic media depiction of these technologies as mere replacements for human effort.  
    
    \item \textit{Trusting and relying on machines, as with humans}: The majority of participants (8 out of 10) exhibited a tendency to attribute human-like qualities of trust and reliability to AI systems. The criteria for trusting AI closely resembled those for human interactions: the ability to provide dependable information, transparency in reasoning, and a foundation in verifiable facts. Interestingly, analysts did not set higher standards for AI than for human colleague.  This parity in trust and reliability criteria suggests that participants viewed AI as an equal collaborative partner, assessing its competence and trustworthiness on the same grounds as a human team member.
    
    \item \textit{Training and guiding AI}: Our study identified a split in intelligence analysts’ perspectives on their role in training and guiding AI systems. Four out of ten participants advocated for substantial control over AI, emphasizing the need for an interactive system that allows them to influence everything from information source selection to narrative shaping in reports. In contrast, the majority (six out of ten) favored a more hands-off approach, highlighting that situation report creation follows well-established, standardized procedures suitable for AI implementation. They perceived AI involvement as an extension of routine oversight, akin to reviewing a junior colleague's work.
\end{itemize}


\subsubsection{Collaborative Design}
\label{sec:collaborative_design}

To gain an operational understanding of intelligence analysis process and generating situation reports, we expanded the design opportunities identified in the formative study (\S\ref{sec:formative_study}) with subsequent collaborative design sessions with the ten analysts. The goal was to capture tangible design strategies and recommendations from users about how they, as intelligence analysts, navigate, research, and author their situation reports. 

We conducted study sessions with the analysts, with each session consisting a workflow review using storyboards and hands-on training with a simulation report exercise. In our study, participants engaged with a low-fidelity storyboard (shown in
Figure \ref{fig:storyboard}), where each panel depicted a distinct phase in situation report creation. Participants were tasked with providing detailed descriptions of each storyboard panel to ensure comprehension of the depicted scenario and workflow. Participants simulated each storyboard step using sample situations to gain practical workflow experience, and were advised to utilize diverse resources, including web search engines like Google and Bing, and Large Language Models (LLMs) such as ChatGPT~\cite{achiam2023gpt}, for task completion.

Data analysis from the collaborative design sessions showed three main themes: enhancing analytical efficiency, transparency in AI systems, and customization flexibility. Participants highlighted the need for interfaces that reflect their mental models, reducing cognitive load and allowing them to focus on strategic aspects. There was a significant emphasis on understanding AI systems' underlying logic for trust, with a preference for transparent methods and traceable data sources to verify the credibility of automated outputs. Additionally, users expressed a desire for tools that support varying analytical styles and complexities, and that can integrate information from diverse sources to provide a comprehensive analysis.
\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{tables/storyboard.jpg}
    %\vspace{-1em}
    \caption{Storyboard used in the collaborative design sessions with intelligence analysts.}
    \label{fig:storyboard}
\end{figure}
From the findings of the formative study and the collaborative design above, we identified the following design strategies:

\begin{itemize}
    \item \textbf{DS1:} Given the emphasis on reducing cognitive load and enhancing analytical efficiency, the system will be designed with an interface, that mirrors intelligence analysts' natural processes of data analysis and report generation.
    \item \textbf{DS2:} To increase efficiency, the system will integrate features to automate time-intensive tasks such as question curation and preliminary research, thereby reducing analysts' manual workload and enabling greater focus on strategic analysis and decision-making.
    \item \textbf{DS3:} The design, addressing the need for trust and reliability, will convey clear explanations of the system’s data processing algorithms and criteria. This includes transparent data sourcing, providing references within reports, and tools for users to easily understand and verify the system's conclusions. The design will also facilitate incremental trust-building through consistent and validated performance over time.
    \item \textbf{DS4:} Addressing the themes of customization and flexibility, the system will offer a high degree of adaptability to accommodate various analytical styles and levels of detail in reporting. It will include features for adjusting the depth of analysis, focusing on specific data sets, and seamlessly integrating various data sources.
    %\item \textbf{DS5:} \textcolor{blue}{The system will be equipped with transparent mechanisms that elucidate the processes of data handling and summary creation (\S{\ref{sec:4_2_1}}). By making these processes clear and understandable, the design aims to deepen user comprehension of the automated functions, thus enhancing confidence and trust in the system's outputs.}
\end{itemize}

\subsection{System Architecture}
\label{sec:smartbook_system}

\begin{figure}[t]
% \def\w{0.55\linewidth}
\centering
% \begin{tabular}{*3c}
% \includegraphics[height=\w, trim=35 30 35 10]{Web.pdf}
\includegraphics[width=1\linewidth]{tables/smartbook.jpg}
% \end{tabular}
\caption{A screenshot of \name{}'s front-end interface. Within the given situation, the user can navigate timelines (F1), explore strategic questions related to an event (F2), read the overarching summary on a given strategic question (F3), control the depth and length of information (F4), investigate all the claims in the summary (F5), trace each claim to corresponding summary fragment (F6), investigate the source metadata (F7) and read the context from which the claims were extracted (F8).}
\label{fig:system1}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/SmartBook_arch2.png}
    \vspace{-1em}
    \caption{\small Backend workflow for constructing \textit{\name{}}. Given the articles corresponding to a specific timeline, the figure shows the process for obtaining the chapters, their section headings, and the corresponding section content.}
    %\vspace{-1em}
    \label{fig:overall_workflow}
\end{figure}
The above four design strategies helped shape \name{}, an AI-assisted system for situation report generation that provides analysts with a first-draft report to work from as they respond to time-critical information requirements on emerging events.
\name{} consists of: 1) An intuitive user interface (shown in Figure \ref{fig:system1}) with design strategies from \S{\ref{sec:collaborative_design}}, and 2) a back-end framework (shown in Figure \ref{fig:overall_workflow}) that, when given a collection of documents from a variety of news sources, automatically generates a situation report.

Our automatic situation report is organized into coherent, chronological timelines spanning two weeks each, enhancing the tracking and comprehension of events' developments. Within these timelines, major events are identified by clustering news articles, forming the basis for subsequent chapters (see \S{\ref{subsec:major_event}}). To guide detailed chapter analysis, we incorporate a logical structure with automatically generated section headings in the form of strategic questions covering various aspects of each major event (details in \S{\ref{subsec:section_headings}}). \name{} generates content addressing these questions from a strategic perspective, by pinpointing relevant claims in news articles (see \S{\ref{subsec:claim_extraction}}). Each section contains query-focused summaries that answer the strategic questions, providing a comprehensive view of the event's contexts and implications (see \S{\ref{subsec:content_gen}}). These summaries include citation links that allow for factual verification and cross-checking by experts.

%Our automatic situation report c in the form of timelines to provide a coherent, chronological representation of event developments, facilitating users' tracking and understanding of the overall situation context. Each timeline spans a duration of 2 weeks, allowing for a manageable cycle of capturing and focused analysis of significant newsworthy occurrences. Building upon this, major events are identified via clustering news articles within the 2-week timeline duration and serve as the foundation for corresponding chapters (details in section \ref{subsec:major_event}). To further guide detailed chapter analysis, we incorporate a logical structure through automatically generating section headings in the form of strategic questions relating to each major event (details in section \ref{subsec:section_headings}). \name{} generates chapter content with strategic perspective by using such strategic questions covering various aspects of the event. The individual sections comprise the core content of our situation report and contain grounded, query-focused summaries which address the strategic questions (details in section \ref{subsec:content_gen}), providing readers with a comprehensive understanding of event context and implications. The generated summaries feature citation linking, which grounds each summary fragment in its factual input context and allows experts to cross-check and verify information as needed. 

\name{} is designed for efficiency, utilizing smaller models~\cite{liu2019roberta, lewis2020bart} for tasks that have training data available, such as event headline generation, duplicate question detection, and claim extraction. Conversely, for more complex tasks—such as generating long-form summaries with citations or identifying strategic questions, we employ large language models~\cite{openai2021gpt35,achiam2023gpt}. 
%Ultimately, the intuitive and structured incorporation of timeline chunking, major event chapter clustering, and query-focused section summaries within chapters enables our proposed SmartBook formulation to generate reliable, insightful reports for time-sensitive, emerging situations. 
In this section, we describe the various components within \name{}, along with emphasizing the advantages of each aspect of \name{}'s design for users (i.e., intelligence analysts) and for recipients of the final \name{} report (i.e., decision-makers), who both initiate information requirements and are downstream readers.

\subsubsection{Major Events within Timespans as Chapters}\label{subsec:major_event}

Situation reports cover event progressions over considerably long periods. Hence, it is beneficial to organize such reports in the form of timelines (F1 in Fig. \ref{fig:system1}), which enables seamless report updates~\cite{ma2023structured} with new events and helps facilitate~\cite{singh2016expedition} users tracking and understanding of situation context (informed by DS1). Timelines aid intelligence analysts in understanding event progression and predicting future trends by organizing events chronologically and highlighting cause-and-effect relationships. For readers, especially those less familiar with the subject, timelines provide a visual guide to easily grasp the sequence and significance of events in a scenario. Our automatic situation report has timelines to provide a coherent, chronological representation of event developments (DS1, DS2).



%For each timespan with the timeline, we first identify major events within news article clusters during that period. As the number of major events is unknown a priori, we cluster the daily news summaries from articles in the timespan into major event groups. We employ the agglomerative hierarchical clustering algorithm \cite{jain1988algorithms}, based on their term frequency-inverse document frequency (TF-IDF) scores, which assigns higher weights to rare or document-specific words \cite{sparck1972statistical}. 
In each timespan, we first identify major events by clustering daily news summaries from the period into major event groups using an agglomerative hierarchical clustering algorithm~\cite{jain1988algorithms} based on term frequency-inverse document frequency (TF-IDF) scores~\cite{sparck1972statistical}. 
Finally, we are left with clusters of news snippets, each providing a focused view of a major event. However, since news summary snippets are condensed in detail, we improve the comprehensiveness of each event cluster by expanding the news corpus, as described next. First, to create a chapter name for \name{} and also use it for additional news article retrieval, we generate a concise headline for each event cluster. To achieve this, we utilize a sequence-to-sequence transformer-based \cite{vaswani2017attention} language model, BART \cite{lewis2020bart}, %\revanth{Added a sentence in overview saying smaller models are used for constrained tasks for efficency}\heng{why choose smaller model like BART? improve efficiency? perhaps in the future we want to integrate the system into smartphone?} 
that takes the concatenated title and text from all the news snippets within the event cluster as input and generates a short event heading. We made use of the model trained on the NewsHead dataset \cite{headline2020} for generating multi-document headlines. In that dataset, each cluster contained up to five news articles, and a crowd-sourced headline of up to 35 characters described the major information in that cluster. We then use these chapter names as the query to retrieve additional relevant news articles via Google News.
%Finally, we query google news with the generated chapter name to obtain an expanded set of news articles relevant to the event. 

\subsubsection{Strategic Questions as Section Headings}\label{subsec:section_headings}
%\revanth{We linked to the screengrab in fig 3 to show examples}\heng{give more examples on what kind of chapter and section titles are generated}

A situation report should have a logical structure and descriptive section titles (F2 in Fig. \ref{fig:system1}) for clarity and easy access to information for intelligence analysts (DS2). \name{} not only describes event details in each chapter but also aims to present strategic insights that support decision-making and policy planning. To guide such detailed analysis, we incorporate a logical structure by automatically generating section headings in the form of strategic questions relating to each major event. These questions cover insightful details such as the motivations behind actions in an event and its potential future consequences.

%A situation report should be logically structured with descriptive section titles for ease of understanding and information access (F2 in Fig. \ref{fig:system1}). Structured reports benefit intelligence analysts by simplifying complex situation analysis, offering easy navigation, clear information hierarchy, and improved context understanding. For less experienced readers or those seeking specific information, the logical structure enhances information access, and comprehension, while providing an intuitive mental map, making reports more reader-friendly (DS2).  Beyond simply describing event details in each major event chapter, \name{} aims to provide information from a strategic perspective that can help aid decision-making and policy planning. To guide such detailed chapter analysis, we incorporate a logical structure by automatically generating section headings in the form of strategic questions relating to each major event. These questions cover details such as the possible motivations of the actors in an event and the future implications of the event. As we detail next, the event-related questions are generated by prompting the LLM with a grounded context in the form of news articles from the event cluster. The generated questions then undergo a post-processing step of de-duplication to ensure clear and unique section headings. 

Recent work \cite{sharma2021generative, wang2022towards} has shown that LLMs are capable of generating comprehensive, natural questions that require long-form and informative answers, in comparison to existing approaches \cite{murakhovska-etal-2022-mixqg, du-etal-2017-learning} that mainly generate questions designed for short and specific answers. In this work, we direct the LLM (GPT-4) to generate strategic questions about specific events, using news articles from the event cluster to anchor the context and reduce model-generated hallucinations~\cite{ji2022survey, maynez2020faithfulness}. %In this work, we prompt the LLM to generate strategic questions about the event. Further, to mitigate hallucinations \cite{ji2022survey, maynez2020faithfulness} which are common in large language models, we ground the input context to the LLM with news articles from the event cluster (from section \ref{subsec:major_event}) to ensure the generated questions are relevant to the event. 
To ensure diversity in the generated questions, we sample multiple question sets using nucleus sampling~\cite{holtzman2019curious}. 
%Within the generated question sets, we observe that questions may occasionally be repeated across different sets. Figure \ref{fig:overall_workflow} contains an example, with the duplicate questions within the generated question sets marked in \textcolor{blue}{blue}. 
Our analysis reveals that questions may occasionally be repeated across different sets, as depicted in Figure \ref{fig:overall_workflow} where duplicates are highlighted in \textcolor{blue}{blue}. To address this, we perform question de-duplication using a RoBERTa-large \cite{liu2019roberta} model trained on the Quora Duplicate Question Pairs dataset, thereby consolidating the sets into a singular, diverse collection of strategic questions relevant to the event. %Given a pair of questions, the model predicts a score between 0 and 1 on how likely the two given questions are duplicates. 
%We follow this approach for filtering out duplicate questions to merge the multiple question sets generated by the LLM into a single set of diverse and unique strategic questions about the event. 

\subsubsection{Extraction of Claims and Hypotheses}\label{subsec:claim_extraction}
 Intelligence analysts, given the high stakes nature of their work but limited time, need systems that quickly identify key information in documents (DS2). This enables them to focus on urgent matters without sorting through irrelevant data. Hence, automated situation report generation should be able to identify and extract the most scenario-relevant and crucial information across multiple documents (F5 in Fig. \ref{fig:system1}). Readers of the situation report benefit from information salience because they are presented with a concise, relevant overview of a situation. Essential points need to be highlighted, to enhance readability and clarity. Moreover, we also present the bias of each news source (taken from \href{https://www.allsides.com}{AllSides}) to help analysts consider information presented from different perspectives.

Providing readers with a comprehensive understanding of event context requires foraging for different claims and hypotheses from the source documents (i.e., news articles) that help explain a situation \cite{toniolo2023human}. We adopt a Question Answering (QA) formulation to identify claims relevant to a given strategic question, driven by the ability of directed queries to systematically extract relevant claims from news articles, as demonstrated in recent research~\cite{reddy2022newsclaims, reddy2022zero}. %\heng{why formulate this as a QA problem?} 
%motivated by recent studies \cite{reddy2022newsclaims, reddy2022zero} which have shown that directed queries can automatically extract claims from news articles relevant to a specific topic. 
Our QA pipeline utilizes a transformer-based RoBERTa-large encoder model \cite{liu2019roberta} that has been trained on SQuAD 2.0 ~\cite{rajpurkar2018know} and Natural Questions ~\cite{kwiatkowski2019natural}. The model takes as input the news corpus split into snippets along with the strategic question, and outputs answer extractions to these questions. The corresponding sentences that contain these answers are considered as the extracted claims. However, the risk of high-confidence false positives~\cite{chakravarti2021towards} necessitates validation~\cite{reddy2020answer, zhang2021joint} of these answers.
%The identified short answers are expanded using the 3-sentence window around it to provide additional context. 
To address this, we utilize an answer sentence selection model~\cite{Garg_2020} that verifies each context against the strategic question. The model is a binary classifier with a RoBERTa-large backbone trained on datasets such as Natural Questions~\cite{kwiatkowski2019natural} and WikiQA~\cite{yang-etal-2015-wikiqa}, and outputs a validation score ranging from 0 (incorrect) to 1 (correct), which is used to select the top-5 relevant contexts for summarization.
%However, there is still a risk of false positives \cite{tan2018know, chakravarti2021towards} being identified as candidate answers with high confidence, thus necessitating the validation \cite{reddy2020answer, zhang2021joint} of extracted answers. To this end, we employ an answer sentence selection model \cite{Garg_2020} that validates each of the extracted contexts separately against the strategic question. We concatenate the question and extracted context as input to a binary classification model, with an underlying RoBERTa-large backbone, that is trained on Natural Questions \cite{kwiatkowski2019natural} and WikiQA \cite{yang-etal-2015-wikiqa}. The output of the model is a validation score, between 0 (incorrect answer selection) and 1 (correct answer selection), used to select the top-5 relevant contexts for summarization.

\subsubsection{Grounded Summaries as Section Content}\label{subsec:content_gen}
Considering the issue of hallucination in LLM-based summarization~\cite{ji2023survey, li2023halueval, bang2023multitask}, factuality is far more important than creativity for situation report generation. A reliable situation report must be anchored in verifiable sources to ensure credibility (DS3). This supports analysts in drawing robust, evidence-based conclusions, and the embedded links act as a springboard to more extensive research for readers wanting to dive deeper (F6, F7, F8 in Fig. \ref{fig:system1}). Additionally, we offer summaries with varied detail levels—brief (2-3 sentences), standard (4-6 sentences), and extended (2 paragraphs)—to cater to different reader preferences (F4 in Fig. \ref{fig:system1}) (DS4).


%Given that hallucination is a major issue for LLM-based summarization~\cite{ji2023survey, li2023halueval, bang2023multitask}, we note that for the task of situation report generation, factuality is far more important than creativeness.
%Hence, to be reliable, a situation report must be grounded in verifiable sources, as grounded factual content helps to build credibility (DS3). Reliable information is crucial for analysts, enabling them to verify facts from sources and draw solid, evidence-based conclusions. This reduces the need for additional fact-checking, as evidence is directly linked. Moreover, for those readers wanting to dive deeper or explore related topics, the embedded links act as a springboard to more extensive research (F6, F7, F8 in Fig. \ref{fig:system1}). Further, we present summaries with varying levels of detail (less detailed, normal and more detailed corresponding to 2-3 sentences, 4-6 sentences and 2 paragraphs respectively) (F4 in Fig. \ref{fig:system1}) to allow customization of the level of detail at which readers prefer to consume content (DS4).

Using strategic questions obtained for each chapter as section {\textit{headings}}, we incorporate query-focused summarization to generate each section's {\textit{content}}. A concise summary is generated for each section in \name{} based on the relevant claim contexts (F3 in Fig \ref{fig:system1}).
%Given the set of relevant claim contexts for a strategic question, we aim to generate a concise summary as section content (F3 in Fig \ref{fig:system1}) within \name{} chapters. 
Recent work~\cite{goyal2022news, bhaskar2022zero, reddy2022sumren} has shown that humans overwhelmingly prefer summaries from prompt-based LLMs \cite{brown2020language, chowdhery2022palm} over models fine-tuned on article-summary pairs \cite{lewis2020bart, zhang2020pegasus, liu2022brio}, due to better controllability and easier extension to novel scenarios. For summary generation, we feed the LLM (GPT-4) with the top-5 most relevant contexts (from \S{\ref{subsec:claim_extraction}}) and instructions to summarize with respect to the given strategic question and include citations. This approach not only aids in maintaining accuracy by grounding on claim contexts but also enhances the trustworthiness of the summaries by allowing verification against the cited sources.

%We concatenate the top-5 most relevant contexts (from Section \ref{subsec:claim_extraction}) as input, along with an instruction along the lines of \textit{``summarize the above, regarding $<$strategic question X$>$, with citations''}, to form the input prompt for the LLM for summary generation. It is noteworthy that the addition of citations enables users to verify the generated summary against sources used in the input to determine whether it is accurate and trustworthy. %\revanth{Added citation quality evaluation into 3.2}\heng{did we evaluate how reliable the citation linking is?}

\begin{comment}
\subsection{Comparison with Prior Work}

\subsubsection{AI for Intelligence Analysis}

AI has ushered in a new era for intelligence analysis, as traditional methods can be often tedious and time-consuming. AI, however, can manage vast data sets and swiftly analyze information, ensuring accurate and timely insights. The intelligence analysis begins with data collection and ingestion, where automated tools play a pivotal role in efficient categorization and storage as highlighted by studies~\cite{stottlemyre2015humint, billman2006collaborative,pohl2012automatic}. These tools gather data from diverse sources, simplifying access for further analysis.
In the domain of data visualization and examination~\cite{prunckun2010handbook}, AI significantly enhances the ability to discern complex data patterns, aiding analysts in drawing meaningful inferences. Furthermore, the assessment of evidence reliability is crucial. AI-driven tools can systematically evaluate information credibility, ensuring the use of verified data~\cite{barnes2016making, toniolo2014making}. Further, they can examine the depth and accuracy of insights, verifying the validity of intelligence reports~\cite{brody2011automatic}.

AI innovations, such as those exemplified by HITIQA~\cite{small-etal-2004-hitiqa-scenario}, have facilitated scenario-based question answering, essential for dissecting specific situations and deriving precise insights. This approach is crucial in intelligence analysis, where generating potential hypotheses is a key aspect~\cite{heuer1999psychology, cerutti2018cispaces}. While these developments represent significant progress, challenges persist. Studies~\cite{van2020improving,xia2019trace,pirolli2005sensemaking,pioch2006polestar,toniolo2023human} point out that current tools often require extensive crowdsourcing or necessitate human-machine collaboration for report drafting, highlighting the need for further improvement in this area.

SmartBook brings in a significant advancement in automated situation report generation. SmartBook aims to produce initial report drafts, allowing analysts to channel their energies on critical thinking and comprehensive assessments. Additionally, the system's integration of a large language model aids in topic discovery, navigating the longstanding issue of analysts being unaware of unknown factors. By generating strategic questions for varying events, it guides analysts towards previously unseen patterns, prompting the exploration of new hypotheses, even when confronted with contradictory information. In an age where data inundation is a constant challenge for intelligence analysts, tools like SmartBook offer not just relief, but a pathway to enhanced efficiency and deeper insights. This research underscores the promise of SmartBook, positioning it as a significant milestone in AI-assisted intelligence analysis. 

\subsubsection{Multi-Document Summarization}
Multi-document summarization aims to extract critical information across multiple documents into a few-sentence summary~\citep{fabbri-etal-2019-multi, abdi2017query, li2014improving, xiong2018multi, abdi2018qmos, abdi2018machine, deyoung2021ms2}.
Large Language Models (LLMs) have also shown unprecedented ability to generate summaries~\citep{zhang2023extractive,zhang2023benchmarking, yang2023exploring, zhang2023enhancing, wang2023zero, zeng2023meta}.
SmartBook differs from these summarization tools with its structural narratives, its focus on situational intelligence, and its automatic event-related question discovery.
First, the current summarization methods lack the organizational structure needed for intelligence situation reports, which require synthesizing diverse data points into a coherent narrative. 
SmartBook, by contrast, generates inherently structured reports that are broken down into time spans, major events, strategic questions, and their grounded summaries.
This layered structure, reminiscent of a book, provides a comprehensive view and intuitive navigation through the complex scenario being analyzed.
Second, while news summarization aims to provide a condensed version of events, SmartBook not only states what happened but also explores multiple aspects including implications, potential trajectories, and strategic importance of these events.
Third, previous query-based methods rely on human effort to identify important events as guidance or prompts, while SmartBook introduces an innovative mechanism to discover pertinent event-related questions automatically, allowing the system to probe deeper into events to offer users richer insights. 
\end{comment}