\section{Introduction}\label{sec1}

%\heng{We should also talk about criteria for a good situation report - use the bullets in page 6 of these slides: https://blender.cs.illinois.edu/ALTA\_NLP\_SmartBook\_HengJi.pdf}
In today's rapidly changing world, intelligence analysts face the constant challenge of staying informed amidst an overwhelming influx of news, rumors, and evolving narratives. To understand unfolding events, it is essential to extract common truths from heterogeneous data sources. Currently, intelligence analysts prepare \textit{situation reports} that provide an overview of the state of affairs, potential risks or threats, and perspectives, along with recommended actions to guide action planning and strategic development~\cite{world2020coronavirus,world2022emergency}. Situation reports are expected to present salient information about key events and trends with a clear logical structure to facilitate understanding, tracking, and easy access to information.
%\heng{need to add: what is situation report? give data friendly space as an example. why human generated report has limitations? Here are some points from my slides: The key to situation understanding for unfolding events is to extract the common truths across heterogeneous data for creating situation reports for guiding action planning and strategic development. Manual construction of situation reports by expert analysts time-consuming, resource-intensive and unscalable often limited to a few topics, scenarios or regions can be biased, incomplete, difficult to keep up-to-date incapable of consolidating information across foreign languages and multiple data modalities Situation reports are expected to have following characteristics: Salient information about key events, trends, statistics relevant to subject of interest. Clear logical structure to help understand, follow, and easily access information Organized as Timelines to seamlessly update for new events and cover event progressions can be covered over longer time periods. Grounded Factual Content to build credibility and trust by allowing for cross-checking of information presented} \heng{expand these citations by adding papers after 2022}
 The downstream consumers of these reports (general public, decision-makers, and stakeholders) are not just looking for raw information; they are seeking clarity, context, and actionable insights. However, these reports, being manually crafted, come with limitations: they are time-consuming to produce~\cite{national2019decadal}, might exhibit biases~\cite{biases1, belton2020cognitive}, factual errors~\cite{trustllm2024}, and may be restricted due to the large volume of information they must process~\cite{doggette2020information, overload1}.
Intelligence analysts spend a lot of time sifting through vast and complex data sources~\cite{golson2018resist, national2011intelligence} ({\em e.g.,} social media rumor propagation, news outbreak, background statistics, domain expert opinions, etc.), and readers often grapple with information that's either too dense or not sufficiently comprehensive. Given the sheer volume of information, it is challenging for analysts to do deep analysis and critical thinking, to be able to formulate strategic questions and generate insights.
We hypothesize that automatic situation report generation can bridge the gap between reading and writing intelligence information. 
However, existing summarization approaches~\cite{fabbri-etal-2019-multi, abdi2017query, li2014improving} generate plain-form text and do not yield the details, structures, and high-level strategic information to support decision-making for the downstream consumer. In particular, current large language model-based approaches such as GPT-4/ChatGPT~\cite{openai2021gpt35} may return no answers or incorrect answers to user questions as they do not update information in real-time and lack fact-checking mechanisms ({e.g.,} Table~\ref{tab:chatgpt}). %\heng{maybe also mention the reason from LLM hallucination}

To establish a basis for the automated generation of situation reports, we carry out a formative study (see \S{\ref{sec:formative_study}}) to grasp intelligence analysts' expectations from AI-driven systems. Our findings reveal that analysts are open to AI assistance, seek clarity in AI operations, and vary in their desire to control the AI system. %\revanth{I asked Brad about this, but we don't have any timing stats recorded unfortunately} \heng{I wonder whether it's possible to report the time we saved for human intel analysts at generating reports, compared to the overall time they need to spend to generate a report from scratch.} 
To comprehend the composition process of situation reports, we augmented the initial study's findings with collaborative design sessions (see \S{\ref{sec:collaborative_design}}), aiming to define practical design strategies and recommendations, particularly on how human intelligence analysts navigate, research, and write their reports. On a high level, we infer that the system interface must visually align with analysts' sequential thought processes, provide transparency of information flow to build trust, and enable analysts to have some level of control over the AI tool's processes. These conclusions consequently guided the design strategies for automating the generation of situation reports.

\input{tables/chatgpt}

Building on the foundation set by the formative study and collaborative design, we present \textbf{\name{}}, a framework designed to assist human analysts who author situation reports. SmartBook ingests data from multiple sources to generate a comprehensive report with information updated regularly. Human analysts typically source information by asking questions based on their own understanding of the situation. However, these questions can be static or too vague, and become outdated as the situation evolves rapidly. In contrast, \name{} automatically discovers questions %\heng{before this talk about the problems of human generated questions: Questions from human analyst are static and become outdated as the conflict has been evolving too rapidly e.g., What are the reports on Naval activity in the Black Sea? The above question doesnâ€™t have much relevant content from the past 3 weeks. Some human-generated questions are too vague or complex to parse e.g., Is Russian positioning forces inside of Russian for possible follow-on actions into Ukraine after initial invasion? Human-generated questions often require multi-hop reasoning and aggregation e.g., What is the reporting size of Russian and Ukrainian forces currently involved in Ukraine? Can we automatically generate questions from news articles? What was the reason for the withdrawal of Russian troops from Lyman? What are the possible motives for sabotage of the Nord Stream gas pipelines? } 
important for situation analysis and gathers salient information for generating the report. For all questions about a major event, the report contains summaries with tactical information coming from relevant claims, presented with local context and links to source news articles. \name{} structures data in a manner that mirrors current intelligence analyst workflows--breaking down events into timelines, chapters, and question-based sections. Each section contains a grounded query-focused summary with its relevant claims. This intuitive structure facilitates easier assimilation of information for both reading and writing. Designed with a human-centered approach, our goal is to augment the capabilities of human analysts rather than replace them. Figure \ref{fig:1} shows an example from \name{} for the Ukraine-Russia crisis, with the structured hierarchy of timespans, chapters, and corresponding sections.

\begin{figure}[t]
\includegraphics[width=\textwidth]{figures/SmartBook_example.png}
\caption{Figure showing an example from SmartBoook for the Ukraine-Russia Crisis. \name{} is organized on a timeline, with 2-week time spans containing chapters and corresponding sections. The section headings are strategic questions and each section content consists of a grounded summary with links to relevant claims, each highlighted for factual evidence and knowledge elements. The italicized entities and events, which comprise the associated knowledge elements, are highlighted in red and blue respectively.} 
\label{fig:1}
\end{figure}

In our comprehensive evaluation of \name{}, we conducted two complementary studies: a utility study (in \S{\ref{sec:utility_eval}}) for assessing usability and interaction, and a content review (in \S{\ref{sec:content_review_study}}) for examining the quality of text summaries. The user study, involving intelligence analysts and decision-makers, focused on key research questions to explore \name{}'s usability, intuitiveness, and effectiveness in situation report generation. Participants engaged in semi-structured interviews and post-study questionnaires, reflecting on their experience with the system. The content review complemented this by assessing the readability, coherence, and relevance of \name{}-generated reports, including an editing study where an expert analyst revised the summaries to meet their standards of acceptability.  The primary goal of this process was to evaluate the viability of using \name{} as a tool for creating preliminary drafts of situation reports. The findings indicate that the content generated by \name{} was mostly accurate, requiring minimal edits to correct a few factual errors. However, most of the effort in the editing process was focused on adding evidence to support the summaries. The results of the editing study suggest that while \name{} provides a solid foundation, it significantly benefits from human refinement. %Smartbook generated content was noted to be  mostly accurate, with minimal edits needed to correct a few factual errors, while most of the effort was on adding evidence for the summaries. The editing study suggests that while SmartBook lays a solid foundation, it benefits from human refinement.
%The system's generated reports were noted for their accuracy and relevance, enhancing efficiency in report generation. Furthermore, the editing study with an expert analyst suggested that while SmartBook lays a solid foundation, it benefits from human refinement. \heng{elaborate: Smartbook generated content is mostly accurate, human analysts need minimal efforts to correct a few factual errors; they spent most of their efforts on adding evidence for the summaries} Overall, SmartBook emerged as a robust tool for intelligence analysis, balancing automated efficiency with the need for human expertise.
The contributions of this work are as follows:
\begin{itemize}
    \item A comprehensive formative study and collaborative design process for identifying the design strategies to guide the automated generation of situation reports.
    \item \textbf{\name{}}, an automated framework that generates comprehensive, up-to-date situation reports from various sources and presents them in an intuitive and user-friendly manner. It identifies critical strategic questions, ensuring that downstream readers receive targeted, relevant, and evidence-grounded information to aid their decision-making processes.
    \item A thorough utility evaluation involving intelligence analysts and  decision-makers investigating the usability of the system.
    \item A content review to grade the quality of the information generated, along with an editing study to understand how viable \name{}  is for producing preliminary first drafts of situation reports.    
    %\item A publicly accessible interface for SmartBook and underlying code to foster collaboration, feedback, and further innovation in the intelligence analysis domain.
\end{itemize}
    
%\wordcount