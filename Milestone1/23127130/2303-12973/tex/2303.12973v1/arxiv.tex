\documentclass{article}


\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{booktabs}
%\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amstext}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{subcaption}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\title{Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation}


\author{
 Wenbo Hu$^*$ \\
  %School of Coumputing and Information\\
  Hefei University of Technology\\
  %Pittsburgh, PA 15213 \\
  \texttt{wenbohu@hfut.edu.cn} \\
  %% examples of more authors
   \And
 Xin Sun$^*$ \\
  %School of Coumputing and Information\\
  University of Science and Technology of China \\
  %Pittsburgh, PA 15213 \\
  \texttt{sunxingreater@gmail.com} \\
  \And
 Qiang Liu, Shu Wu \\
  %School of Coumputing and Information\\
  Institute of Automation, Chinese Academy of Sciences\\
  %Pittsburgh, PA 15213 \\
  \texttt{qiang.liu@nlpr.ia.ac.cn,shu.wu@nlpr.ia.ac.cn} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
\def\thefootnote{*}\footnotetext{Equal contribution}\def\thefootnote{\arabic{footnote}}
\maketitle
\begin{abstract}
In recommendation systems, a large portion of the ratings are missing due to the selection biases, which is known as Missing Not At Random. The counterfactual inverse propensity scoring (IPS) was used to weight the imputation error of every observed rating. Although effective in multiple scenarios, we argue that the performance of IPS estimation is limited due to the uncertainty miscalibration of propensity estimation. In this paper, we propose the uncertainty calibration for the propensity estimation in recommendation systems with multiple representative uncertainty calibration techniques.
Theoretical analysis on the bias and generalization bound shows the superiority of the calibrated IPS estimator over the uncalibrated one. Experimental results on the coat and yahoo datasets shows that the uncertainty calibration is improved and hence brings the better recommendation results.

\end{abstract}





\section{Introduction}
The Missing Not At Random (MNAR) is an critical issue in modern recommendation systems, which is derived from the missing ratings of the users~\cite{pradel2012ranking}.
There are two kind of selections bias that leads to MNAR~\cite{marlin2007collaborative}: (a) user selection bias: a user only rate the items that he likes and (b) item selection bias: items' exposure are selectively provided by recommendation systems.
To solve this problem, the inverse propensity scoring (IPS) approach is used to deal with the selection bias from the perspective of solving MNAR using counterfactual estimation~\cite{seaman2013review,little2019statistical}. It inversely scores the prediction error of every rating using the propensity of that rating~\cite{swaminathan2015self,schnabel2016recommendations}. The robustness and accuracy of the inverse probability estimation is the key to the counterfactual learning for the recommendation systems. The imputation errors and propensities are simultaneously considered in a doubly robust way for recommendation on MNAR~\cite{wang2019doubly} and reinforcement learning~\cite{jiang2016doubly}.
\begin{figure}[htbp]
\centering
\includegraphics[width=.7\columnwidth]{pics/coat_intro.pdf}
\caption{For recommendation with MNAR on the Coat shopping dataset, we use the raw inverse propensity estimator with and without the platt scaling calibration and give the scatter plot of the expected propensity vs the fraction of observed ratings. The diagonal line is the perfect uncertainty calibration result. As can be seen, the raw IPS estimator is miscalibrated.}
\label{fig:intro-miscalibration}
\end{figure}

For machine learning methods which are widely used in recommending systems, the uncertainty quantification is often not well characterized and tends to produce over-confident predictions, not only for the deep learning models~\cite{guo2017calibration} but also for the shallow models, such as the logistic regression~\cite{vaicenavicius2019evaluating}.
The uncertainty quantification can be characterized by using Bayesian methods, ensemble methods or calibration with binning and scaling~\cite{abdar2021review}.
The uncertainty of personalized ranking probabilities is learned by uncertainty calibration methods~\cite{menon2012predicting,kweon2022obtaining} and later applied in the online advertising systems~\cite{wei2022posterior,xu2022ukd}.

The propensity scoring probabilities are, in fact, miscalibrated which further limits the effectiveness of IPS, although validated in recommendation system and other applications.
As shown in Fig.~\ref{fig:intro-miscalibration}, the expected propensity-scores are not calibrated with the fraction of the observed samples, compared with the perfectly calibration (the diagonal line).
In the sense of uncertainty calibration, the expected or predicted propensity scoring, for example $95\%$, should have the same level of the fraction of the observed sample ($95\%$).
The uncertainty originates in the inaccurate prediction of the inverse propensity scoring part and the following recommendation on MNAR with miscalibrated inverse propensities would be inaccurate.

In this paper, we propose the uncertainty calibration method for the propensity estimation in recommendation, which is the first work on the uncertainty quantification and calibration for the recommendation systems with selection biases.


The contribution of this paper are as follows:
\begin{itemize}
    \item We analyze the inverse propensity-scores approach and reveal the uncertainty miscalibration which limits the debiasing performance of the inverse propensity scoring estimator in the recommendation systems.
    \item We propose to use calibration error to estimate the quality of propensity-scores and propose the thoughtful uncertainty calibration methodology for the propensity-scores.
    \item We give the theoretical analysis and experimental results to show the show the superiority of uncertainty calibration for the unbiased performance estimation in recommendation systems.
\end{itemize}



\section{Preliminaries}
In this section, we give the preliminaries of the counterfactual propensity estimation and the uncertainty quantification.
\subsection{Counterfactual Propensity Estimation for Recommendation with MNAR}
The recommendation model aims to predict the ratings of users to items and recommend the high rating items to the corresponding users~\cite{ricci2011introduction}.
Define ${R}=\{r_{u,i}\}\in\mathcal{R}^{m\times n}$ be the true rating matrix where every element $r_{u,i}$ represents the rating of user $u$ towards item $j$. The recommendation algorithm learns the rating prediction matrix $\hat{R}$ by minimizing the difference between the prediction matrix $\hat{R}$ and the true rating matrix ${R}$:
\begin{equation}
    E = \frac{1}{|\mathcal{D}|}\sum_{u,i\in\mathcal{D}}e_{u,i},
\end{equation}
where $\mathcal{D}$ is the set of all user-item pairs and $e$ can be MAE or MSE between a true rating and a prediction rating. 
In practice, only part of the ratings are available and the goal is to do matrix completion for the rating matrix.
Define $O=\{o_{u,i}\}$ as the indicator matrix where every indicator $o_{u,i}$ represents that the rating $r_{u,i}$ is available when $o_{u,i}=1$ and unavailable when  $o_{u,i}=0$. 
In real settings that most of the ratings are unavailable, a naive estimator averages the prediction errors of the available items:
\begin{equation}
	\label{eqn:naive}
    E_{\textrm{naive}}=\frac{1}{|\mathcal{D}|}\sum_{o_{u,i}=1,u,i\in\mathcal{D}}e_{u,i}.
\end{equation}
The naive prediction is biased when the ratings are Missing Not At Random which is resulted from the selection biases of the real recommendation system~\cite{marlin2007collaborative}.

To reduce the selection bias of the naive estimator, the inverse propensity scoring considers the counterfactual evaluation and reweights the error of the observed ratings of the inverse display probability~\cite{swaminathan2015self,sato2020unbiased}.
Specifically, the propensity $p_{u,i}$, namely the probability that the rating $r_{u,i}$ is available, is estimated using a machine learning classifier $f$, such as naive Bayes.
With the inverse display probability, the prediction error of IPS is obtained via:
\begin{equation}
	\label{eqn:IPS}
    E_{\textrm{IPS}}=\frac{1}{|\mathcal{D}|}\sum_{u,i\in\mathcal{D}}\frac{e_{u,i}}{p_{u,i}}.
\end{equation}

Another line of research aim to use an imputation model to predict the more accurate rating predictions~\cite{steck2010training}. The imputation error is denoted as $\hat{e}_{u,i}$ and this the  error-imputation-based (EIB) estimator has the following prediction error: 
\begin{equation}
    E_{\text{EIB}}=\frac{1}{|\mathcal{D}|}\sum_{u,i\in\mathcal{D}}\left(o_{u,i}e_{u,i}+(1-o_{u,i}\hat{e}_{u,i})\right).
\end{equation}
A more recent progress, the doubly robust estimator, is to combine the IPS and the error-imputation-based (EIB) estimators via joint learning to have the best of the both worlds~\cite{wang2019doubly}:
\begin{equation}
    E_{\text{DR}}=\frac{1}{|\mathcal{D}|}\sum_{u,i\in\mathcal{D}}\left(\hat{e}_{u,i}+\frac{o_{u,i}(e_{u,i}-\hat{e}_{u,i})}{\hat{p}_{u,i}}\right).
\end{equation}

The inverse propensity scoring is applied to the recommendation, post-click conversion rate and click-through rate with MNAR~\cite{yuan2019improving,guo2021enhanced}.





\subsection{Uncertainty Quantification for Deep Learning}
Although deep learning has shown dominate prediction performance in various domains~\cite{lecun2015deep}, such as computer vision, natural language processing, recommendation, etc, the predictions of deep learning models are revealed to be overconfident and miscalibrated~\cite{guo2017calibration}.
Overconfident predictions may do harm to the accuracy, robustness and reliability of the deep learning model.
Therefore, it is imperative to characterize the uncertainty of the deep learning models~\cite{kendall2017uncertainties,abdar2021review}. 


To formalize, the propensity probability is well-calibrated if the propensity probability equals the correctness ratio of the available ratings~\cite{kull2017beta}.
For example, if the model predicts the propensity be $0.95$, the ground-truth is expected to have $95\%$ of the ratings been available.
Then, the miscalibration can be measured by the expected calibration error (ECE), the expectation of the coverage probability difference of the prediction intervals. In practice, we partition the range of the cumulative probability into $n$ bins with the same space and sum up the value of every bin via:
\begin{equation}
	\label{eqn:ece}
    \mathrm{ECE}(f)=\frac{1}{n}\sum_{j=1}^{n}|\xi_{j}-\hat{\xi}_{j}|,
\end{equation}
where $\xi$ and $\hat{\xi}$ are the expected confidence and the probability that the intervals covers the true data samples.

As for the methodology, one effective way is Bayesian generative modeling and the representative models includes Bayesian neural networks, deep Gaussian processes, etc~\cite{wilson2020bayesian,wang2020survey}.
The Bayesian neural networks are generally computational expensive for training and some approximate methods are developed, such as MC-Dropout~\cite{kendall2017uncertainties} and deep ensembles~\cite{lakshminarayanan2017simple}.
Alternatively, the uncertainties can also be obtained from the calibration of the inaccurate uncertainties.
Some methods uses the scaling and binning methods for calibration of both classification and regression methods~\cite{platt1999probabilistic,niculescu2005predicting,guo2017calibration,kuleshov2018accurate}.




\section{Uncertainty Calibration for Propensity Estimation}
In this section, we present our counterfactual propensity estimation with the uncertainty calibration. We also give theoretical guarantee of our uncertainty calibration model for recommendation with MNAR.
\subsection{Uncertainty Calibration for Propensity Estimation}
The propensity probability $p$ is the key of the inverse propensity scoring and the IPS estimator is unbiased if the inverse propensity scoring is assumed to be accurate~\cite{vermeulen2015bias}. 
The propensities are learned via a machine learning model $f: x  \rightarrow p, p\in [0, 1]$ and the model can be naive Bayes, logistic regression or deep neural networks, which are generally miscalibrated with overconfident predictions~\cite{wang2019doubly}. 
This miscalibration is demonstrated in Fig.~\ref{fig:intro-miscalibration} and in most of the cases, the probability predictions are over-confident.
To reduce biases and get calibrated propensity probability, an uncertainty calibration $q$ is considered together with the propensity learning model $f$.




Two methodologies are considered here to model the propensity probability calibration: 1) uncertainty probability quantification and 2) post-processing uncertainty calibration.

\subsubsection{Uncertainty Probability Quantification for Propensities}
The uncertainty probability quantification  considers a generative probability quantification model $q(P|\Theta)$ and $\Theta$ is the model parameters.

Since performing a fully Bayesian model is challenging and costly, we propose to use two approximated uncertainty quantification methods for the propensity estimation: 1) Monte Carlo Dropout~\cite{gal2016dropout} and 2) deep ensembles~\cite{lakshminarayanan2017simple}. 

Monte Carlo (MC) Dropout randomly turns off the neurons when performing testing on the original trained deep neural network and samples from it for multiple times ($T$) produces the approximate posterior distribution by model avaraging:
\begin{equation}\label{eqn:model_average}
	q(p|x,\Theta)\sim\frac{1}{T}\sum_{t}^{T} q_{t}(p|x, f(x), \Theta_{t}).
\end{equation}

Deep ensembles learns multiple model replicas with different random initialization and they have no interactions when training. We get an approximate propensity probability distribution by combining and averaging the replicas via Eqn.~\ref{eqn:model_average}.
Compared with MC-Dropout, the deep ensembles performs somewhat better because the model ensembles learns very different model distributions while MC-dropout only differs in the testing stage. Moreover, the deep ensembles are generally more computational expensive since the model ensembles are trained in multiple times.

\subsubsection{Post-processing Calibration for Propensities}
Apart from quantifying the uncertainty directly, we can also perform a post-processing calibration and learn accurate predictive uncertainties from the inaccurate softmax probabilities (or other model output probabilities)~\cite{platt1999probabilistic,guo2017calibration}.
Platt scaling learns the accurate inverse propensities via:
\begin{equation}
    \label{eqn:platt}
    q(f(x))=\sigma(b\cdot f(x)+c),
\end{equation}
where $f(x)$ is the original propensity outputs, $\sigma$ is the sigmoid function and $b,c$ are learnable parameters for the sigmoid function~\cite{platt1999probabilistic}.
It can be shown that the platt scaling is equivalent to the class condition Gaussian likelihoods with the same variance.
For multi-class classification, the platt scaling can equipped with a temperature for softmax soften, which is called temperature scaling~\cite{guo2017calibration}.
In this paper, we use the platt scaling for this NMAR binary setting.


\begin{algorithm}
\caption{Algorithm Sketch of uncertainty calibration for inverse propensity scoring in recommendation}\label{alg:cap}
\begin{algorithmic}
\Require  The item dataset that is missing not at random $\mathcal{D}$ and their ratings ${R}$

\State Train the propensity model (binary classifier)  on the observation dataset.
\If{Uncertainty Quantification is used}
    \State Obtain multiple model ensemble/testing model using Eqn.~\ref{eqn:model_average}
\ElsIf{Post-processing Calibration is used}
    \State Calibrating the overconfident predicts to calibrated ones using Eqn.~\ref{eqn:platt}
\EndIf
\State Output propensity-scores $\hat{p}$ for observed samples 
\State Train a recommendation model $\phi$ using $\hat{p}$ and ${R}$
\end{algorithmic}
\end{algorithm}

With the calibrated propensity probabilities, we can train the recommendation on data missing not at random using a two step process: first train the IPS model, get the inverse calibrated propensities and then train the recommendation model using these inverse calibrated propensities. We present this process in Algorithm~\ref{alg:cap}.

\subsection{Theoretical Analysis of Uncertainty Calibration using Expected Calibration Errors}
By calibrating the propensity uncertainty, the expected calibration error is reduced and we now give theoretical analysis of the proposed method.

We first derive the bias of the IPS estimator in Eqn.~\ref{eqn:IPS}~\cite{wang2019doubly}:
\begin{lemma}
    \label{lemma:bias}
    Given inverse propensities of all user-item pairs $\hat{p}_{u,i}$, the bias of the IPS estimator in Eqn.~\ref{eqn:IPS} and the propensity bias are:
	\begin{eqnarray}
		\mathcal{E}_{\text{IPS}}=\left|\sum_{u,i\in\mathcal{D}}\frac{\nabla_{u,i}e_{u,i}}{|\mathcal{D}|}\right|, \\
        \nabla=\frac{\hat{p}_{u,i}-p_{u,i}}{\hat{p}_{u,i}}.
	\end{eqnarray}
\end{lemma}
Lemma \ref{lemma:bias} is proved and quoted from ~\cite{wang2019doubly} and shows that the bias of the IPS estimator is proportional to the propensity biases.
It can directly derived that if the IPS estimator is well calibrated, then the bias term in Lemma~\ref{lemma:bias} will be zero which means that the well calibrated IPS estimator produces a unbiased estimator.

\begin{theorem}
    \label{theorem:upper_bound}
	For a calibrated IPS estimator, the bias is smaller than the uncalibrated IPS estimator:
    \begin{equation}
        \mathcal{E}_{\text{CIPS}}=\left|\sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}e_{u,i}}{|\mathcal{D}|}\right|  
		\leq \mathcal{E}_{\text{IPS}},
    \end{equation}
if the propensity is calibrated:
\begin{equation}
	\tilde{\nabla}_{u,i}=\frac{\tilde{p}_{u,i}-p_{u,i}}{\tilde{p}_{u,i}}, \tilde{p}=q(f(x)).
\end{equation}
\end{theorem}
\noindent$q$ is a specific uncertainty calibration method, such as MC-Dropout, deep ensembles and the platt scaling.

Theorem~\ref{theorem:upper_bound} gives the insights that the uncertainty and ECE is the key of IPS and as can be seen in the experiments, the ECE of IPS is greatly reduced, which induces better counterfactual recommendation results with MNAR.

It was also rigorously analyzed in the literature that not only the uncalibrated deep learning model and the shallow models, such as the logistic regression, is inherently over-confident.

The ECE of a well-specific logistic regression is positive and cannot be eliminated. We refer the details to ~\cite{bai2021don,vaicenavicius2019evaluating}.
Therefore, in theory, the original IPS estimator will also suffer from the miscalibrated uncertainty and the large bias.

\begin{corollary}
	The unbiased and better calibration arguments in Theorem~\ref{theorem:upper_bound} also holds for the doubly robust estimator in~\cite{wang2019doubly}, which consists of the IPS estimator and the error-imputation-based estimator.
\end{corollary}

The prediction inaccuracy of a prediction model is supposed to be reduced by the uncertainty calibration for the IPS estimator.
Given the observation of the rating matrix $R$, the optimal rating prediction $\hat{R}^{*}$ is learned by the calibrated IPS estimator over the hypothesis space $\mathcal{H}$. We then present the generalization bound and the bias-variance decomposition of the calibrated IPS estimator using the expected calibration errors~\cite{schnabel2016recommendations,wang2019doubly}. 
\begin{theorem}\label{theorem:generalization-bound}
	For any finite hypothesis space $\mathcal{H}$ of the recommendation prediction estimations, the prediction error of the optimal prediction matrix $\hat{R}^{*}$ using the calibrated inverse propensity scoring estimator has the following generalization bound:
    \begin{equation}\label{eqn:generalization-bound}
       \mathcal{E}(\hat{R}^{*},R^{o})+
       \sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}}{|\mathcal{D}|}+
       \sqrt{\frac{\log \frac{2|\mathcal{H}|}{\eta}}{2|\mathcal{D}|^2}\sum_{u,i\in\mathcal{D}}\frac{1}{\hat{p}_{ui}^2}},
    \end{equation}
    where the star superscript means the optimal prediction and the tilde means the calibrated IPS estimator.
    $R^{o}$ is the observed rating matrix $R^{o}=\{r_{ui}, o_{ui}=1\}$.
    The second term and third corresponds to the bias term and variance term respectively.
\end{theorem}

We merely assume that the calibrated IPS has a lower estimation error and the calibration loss. Moreover, the IPS predictions are generally overestimated. With this two assumptions in mind, we have the following corollary.

\begin{corollary}\label{corollary:bias-variance}
Compared with the inverse propensity scoring estimator, the prediction error bound of the calibrated doubly robust estimator has a smaller bias and has a upper bound that is propotional to $\text{ECE}$:
    \begin{equation}
        \sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}}{|\mathcal{D}|} \leq \sum_{u,i\in\mathcal{D}}\frac{n\cdot\text{ECE}}{|\mathcal{D}|},
    \end{equation}
    where $n$ is the number of the bins for ECE.
\end{corollary}

This corollary reveals the bias-variance tradeoff of the real application performance of the calibrated inverse propensity scoring estimator.
The smaller bias is due to the reduced propensity bias.
The smaller propensity estimation error and the ECE induce better recommendation results.


We defer the proofs to Appendix~\ref{appendix:bias-ips} and \ref{appendix:generalization-bound} due to the lack of space.



\section{Experiments}
In this section, we first introduce the experimental setting, including the dataset, metrics and baselines, and then we present the experimental results of calibration and recommendation on two representative datasets.

\begin{figure*}[htbp!]
    \centering
    \subcaptionbox{Calibration Curve}{\includegraphics[width=0.4\textwidth]{pics/coat_curve.pdf}}
    \subcaptionbox{propensity-scores Histgram}{\includegraphics[width=0.4\textwidth]{pics/coat_hist.pdf}}
    \caption{Calibration Curve and Propensity Histograms of Calibrated IPS on the Coat Shopping Dataset}
    \label{fig:coat}
\end{figure*}



\subsection{Experimental Setting}
\textbf{Datasets} \\
To evaluate debiasing recommendation methods, we use two representative datasets, Coat Shopping, and Yahoo! R3, which includes MAR (Missing At Random) test sets~\cite{schnabel2016recommendations,wang2019doubly}:
\begin{itemize}
    \item Coat Shopping: it contains ratings from 290 users of 300 items. There are 6960 MNAR ratings  and 4640 MAR ratings;
    \item Yahoo! R3: it includes approximately 300K ratings among which 54000 are MAR ratings.
\end{itemize}
For both datasets, we randomly split the MNAR ratings into training set and validation set. The training set accounts for 90\% of MNAR ratings and the validation set accounts for 10\%. The MAR ratings are used as test set. 

We focus on the ranking metrics in this paper and the Coat shopping and Yahoo! R3 datasets only have the rating information on a five point scale. For all the user, item pairs with a rating larger or equal to 4, we treat them as positive samples. For those ratings less than 4, we regard them as negative samples. 

\textbf{Observation Dataset Construction} \\
To represent the observing information, We construct the observation datasets using the original two dataset and we use them to train the propensity score estimation model.If a user-item pair has ratings, it is observed and We regard it as positive sample. We randomly sampled the same number of negative samples from all the unobserved user-item pairs. In the following experiments, we denote these two observation datasets as o-coat and o-yahoo. %using more scientific symbols. e.g. O_u,i

We train a binary-classification model on o-coat and o-yahoo respectively. The model input is a (user, item) pair and outputs the confidence that the (user, item) pair is
observed.  We call the binary-classification model as propensity model. The output of positive samples is saved as raw propensity-scores to train the recommendation model.


\textbf{Methods Settings} \\
The we use aforementioned calibration methods for the uncalibrated inverse propensity scoring and select the neural collaborative filtering (\textit{NeuMF}) as the recommendation base method~\cite{he2017neural}.
We also denote the representative IPS models with and without calibration methods:
\begin{itemize}
    \item For \textit{raw} method~\cite{swaminathan2015self}, we train the uncalibrated IPS method for recommendation on data missing not at random.
    \item  For \textit{MC dropout}~\cite{gal2016dropout}, we keep the dropout on during the inference stage. For a testing user-item pair, we put it in the propensity model and inference ten times and average all the results to get a calibrated propensity score.  

    \item For \textit{deep ensembles}~\cite{lakshminarayanan2017simple}, we initialize ten models with different random initializations. Besides, we randomly shuffle the training dataset for each model. To get calibrated results, a tested by these 10 models, and results are averaged. 

    \item For \textit{platt scaling}~\cite{platt1999probabilistic}, we use the LBFGS to optimize the cross entropy loss and learn the parameters $b$ and $c$ in Eqn.~\ref{eqn:platt}. 
\end{itemize}
\begin{table}[htbp]
\centering
\begin{tabular}{c|cc}
\toprule
\diagbox{Methods}{Datasets}    & Coat shopping & Yahoo! R3 \\ \midrule
raw         & 0.1458        & 0.1131    \\
MC Dropout     & 0.1369        & 0.1064    \\
Deep Ensemble    & 0.1408        & 0.1039    \\
Platt Scaling &\textbf{0.0433} & \textbf{0.0301}\\ 

\bottomrule
\end{tabular}
\caption{Expectation Calibration Errors of Calibrated IPS}
\label{tb:ece}
\end{table}

\textbf{Evaluation Metric}\\
For the uncertainty calibration, the expected calibration error (ECE) in Eqn.~\ref{eqn:ece} is used evaluate the calibration results.

For the recommendation results, we use the discount cumulative gain (DCG) and the recall to evaluate all the methods. The calculation formulas of DCG and recall are detailed in Appendix~\ref{appendix:evaluation-metrics}.
\subsection{Calibration Results of Inverse Propensity Scoring}
We run the three calibration methods for the inverse propensity scoring and draw the calibration curves and estimated ECE for the propensity model. For ECE, we set the number of bins as $100$. 


As can be seen in Table~\ref{tb:ece}, the calibration methods, especially the platt scaling, greatly reduce the expected calibration error.
We also give the calibration curves and the propensity histograms of the calibrated IPS in Figure~\ref{fig:coat}, in which Raw means uncalibrated propensity scores.
Figure~\ref{fig:coat}(a) shows that calibration helps narrow the gap between raw propensity model and perfect propensity model (the diagonal line). Because propensity-based debiasing methods only use the propensity-scores of positive samples(observed samples), the right side of calibration curve further validates the calibration for the inverse propensities. 




\begin{table*}[!htbp]
\centering
\begin{tabular}{llccccccc}
\toprule
\multirow{2}{*}{Datasets}      & \multirow{2}{*}{Methods} & \multicolumn{3}{c}{DCG@K}                     & \multicolumn{3}{c}{Recall@K}          & \multirow{2}{*}{Average}        \\ \cmidrule(r){3-5} \cmidrule(r){6-8} 
                               &                          & K=2           & K=4           & K=6           & K=2           & K=4           & K=6           \\ \midrule
\multirow{5}{*}{Coat Shopping} & ${\rm Neumf}_{base}$          & 0.7478 & 1.0152 & 1.1989 & 0.8705 & 1.4435 & 1.9371 & 1.2021 \\
                               & Raw                      & 0.7472 & 1.0204 & 1.2010 & 0.8738 & 1.4591 & 1.9451 & 1.2077\\
                               & MC Dropout               & \textbf{0.7651} & \underline{1.0322} & \underline{1.2101} & \textbf{0.8962} & \underline{1.4675} & 1.9443 & \underline{1.2192}\\
                               & Deep Ensembles           & 0.7584 & 1.0256 & 1.2065 & 0.8848 & 1.4574 & \underline{1.9454} & 1.2127\\
                               & Platt Scaling            & \underline{0.7627} & \textbf{1.0405} & \textbf{1.2259} & \underline{0.8890} & \textbf{1.4823} & \textbf{1.9806} & \textbf{1.2301}\\
                               \midrule \midrule
\multirow{5}{*}{Yahoo! R3}     & ${\rm Neumf}_{base}$          & 0.5277 & 0.7352 & 0.8630 & 0.6333 & 1.0769 & 1.4202 & 0.8760\\
                               & Raw                      & \underline{0.5433} & 0.7395 & 0.8669 & \underline{0.6468} & 1.0661 & 1.4086 & 0.8785\\
                               & MC Dropout               & 0.5410 & 0.7406 & 0.8663 & 0.6452 & 1.0720 & 1.4094 & 0.8790\\
                               & Deep Ensembles           & 0.5342 & \underline{0.7412} & \underline{0.8692} & 0.6404 & \underline{1.0831} & \underline{1.4270} & \underline{0.8825}\\
                               & Platt Scaling            & \textbf{0.5470} & \textbf{0.7535} & \textbf{0.8778} & \textbf{0.6528} & \textbf{1.0941} & \textbf{1.4275} & \textbf{0.8921}\\
                               \bottomrule
\end{tabular}
\caption{Overall IPS-based recommendation performance on Coat Shopping and Yahoo! R3. The best results are shown in boldface and the second best results are marked using underline.}
\label{tb:rec_results}
\end{table*}





\begin{table*}[!htbp]
\centering
\begin{tabular}{llccccccc}
\toprule
                                &                           & \multicolumn{3}{c}{DCG@K}                                            & \multicolumn{3}{c}{Recall@K}                \\ \cmidrule(r){3-5} \cmidrule(r){6-8} 
\multirow{-2}{*}{Dataset}       & \multirow{-2}{*}{Methods} & K=2                                  & K=4           & K=6           & K=2           & K=4           & K=6            & \multirow{-2}{*}{Average} \\ \midrule
& DRJL\_Raw    & 0.7454 & 1.0185 & 1.2014 & 0.8717 & 1.4570 & 1.9489 & 1.2071\\
& DRJL\_Dropout  & 0.7496 & \underline{1.0271} & 1.2073 & \underline{0.8835} & \underline{1.4764} & 1.9603 & 	1.2173\\
& DRJL\_Ensembles & \textbf{0.7546} & 1.0254 & \underline{1.2132} & \underline{0.8835} & 1.4637 & \underline{1.9684} & \underline{1.2181}\\
\multirow{-4}{*}{Coat Shopping} & DRJL\_Platt                    & \underline{0.7539}                        & \textbf{1.0389} & \textbf{1.2241} & \textbf{0.8852} & \textbf{1.4940} & \textbf{1.9928} & \textbf{1.2315}\\  \midrule \midrule
& DRJL\_Raw & 0.5450 & 0.7405 & \underline{0.8779} & 0.6402 & 1.0587 & 1.4112 & 0.8789\\
& DRJL\_Dropout & \underline{0.5501} & \underline{0.7597} & 0.8774 & \underline{0.6539} & 1.0822 & 1.4183 & \underline{0.8902}\\
& DRJL\_Ensembles & 0.5431 & 0.7491 & 0.8747 & 0.6510 & \underline{1.0923} & \underline{1.4293} & 0.8899\\
\multirow{-4}{*}{Yahoo! R3} & DRJL\_Platt                    & \textbf{0.5532}                        & \textbf{0.7555} & \textbf{0.8816} & \textbf{0.6602} & \textbf{1.0926} & \textbf{1.4314} & \textbf{0.8957}\\
\bottomrule
\end{tabular}
\caption{Overall DRJL recommendation performance on Coat Shopping and Yahoo! R3. The best results are shown in boldface and the second best results are marked using underline.}

\label{tb:dr}
\end{table*}


\begin{figure*}[htbp!]
    \centering
    \subcaptionbox{Calibration Curve}{\includegraphics[width=0.4\textwidth]{pics/yahoo_curve.pdf}}
    \subcaptionbox{propensity-scores Histogram}{\includegraphics[width=0.4\textwidth]{pics/yahoo_hist.pdf}}
    \caption{Calibration Curve and Propensity Histogram of Calibrated IPS on the Yahoo! R3 Shopping Dataset}
    \label{fig:yahoo-calibration}
\end{figure*}

Figure~\ref{fig:coat}(b) gives the propensity histograms of the calibrated IPS methods trained on Coat Shopping dataset. As can be seen, the calibrated propensity-scores not only have lower ECE but also are less polarized. 
The ECE and polarization are two different significant aspects for the inverse propensity scoring. Figure~\ref{fig:yahoo-calibration}  presents the calibration curve and propensity histgogram results on the Yahoo! R3 Dataset which has the same conclusion with Figure~\ref{fig:coat}.


\subsection{Recommendation Results}



We use the uncalibrated and calibrated propensity-scores to train the debiasing recommendation models respectively. For baselines, we train the recommendation model without propensity-scores which means that all samples have the same weight for loss.  Table~\ref{tb:rec_results} shows the overall IPS recommendation performance in in terms of DCG@K and Recall@K ($K=2,4,6$)  on two real world datasets. We repeat the experiments ten times and report the mean results to relieve the randomness. From the table, we can see that IPS method with uncalibrated propensity-scores achieves a little improvement on recommendation compared with the baseline methods. The recall metric of the Yahoo! R3 dataset even dropped somewhat, which shows that low-grade propensity-scores in terms of calibration will not help the IPS-based training process.
With calibrated propensity-scores, the IPS-based debiasing method is significantly improved. 

From the results of Table~\ref{tb:ece} and Table~\ref{tb:rec_results}, it can be observed that the propensity-scores with a lower calibration error will achieve a higher recommendation results. The propensity-scores calibrated by Platt Scaling has the lowest calibration error and it outperform other calibration techniques for most recommendation evaluation metrics. 

As our method improve the quality of the estimation of propensity-scores, it can easily extend to other propensity-scores-based debiasing methods. We also run experiments on DRJL\cite{wang2019doubly} (Doubly Roubust Joint Learning), a debiasing method using both impute errors and propensity-scores. Table \ref{tb:dr} shows the results. Calibrated propensity-scores outperform the raw propensity-scores on all the evaluation indicators, which represents the effectiveness of the uncertainty calibration on other propensity-scores-based methods. 




\section{Related Works}
\subsection{Recommendation with Selection Bias}

Missing-not-at-random selection bias frequently occurs in recommender systems.
In real applications, we can only observe feedback of displayed user-item pairs \cite{marlin2009collaborative,sato2020unbiased}.
For debiasing recommender systems, the inverse propensity score (IPS) approach \cite{schnabel2016recommendations,swaminathan2015self} proposes to re-weight observed samples with the inverse of displayed probabilities.
The IPS estimator often faces a high variance \cite{gilotte2018offline} which can be reduced by a self-normalized inverse propensity scoring (SNIPS) estimator\cite{schnabel2016recommendations}.
Furthermore, Doubly Robust (DR) estimator \cite{jiang2016doubly,wang2019doubly} is proposed to simultaneously consider imputation errors and propensities in a doubly robust, for reducing the high variance in IPS.


Recently, some improvements have been further presented from different perspectives, such as asymmetrically tri-training \cite{saito2020asymmetric}, considering information theory \cite{wang2020information}, utilizing adversarial training \cite{xu2020adversarial}, proposing better doubly robust estimators \cite{guo2021enhanced}, incorporating in knowledge distillation \cite{xu2022ukd}, considering bias-variance trade-off~\cite{dai2022generalized} and combining with multi-task learning \cite{wang2022escm}.
Besides, some works on debiasing recommendation rely on a small amount of random unbiased data \cite{bonner2018causal,yuan2019improving,chen2021autodebias}.
However, random data requires high costs, especially in real applications.



\subsection{Uncertainty Calibration and Quantification}
Beyond the platt scaling, the temperature scaling is proposed for uncertainty calibration for the multi-class classification~\cite{guo2017calibration}.
The platt scaling and the temperature scaling is actually adopting a Gaussian assumption. For a richer and high-skewed distribution, the Beta calibration can be used~\cite{kull2017beta}.
Besides the above parametric calibration methods that assume a distribution assumption, the non-parametric methods can also be considered, such as the histogram binning~\cite{zadrozny2001obtaining} and the isotonic regression~\cite{zadrozny2002transforming}.

From the Bayesian Generative model perspective, only two approximate methods, MC Dropout and Deep Ensembles are used to quantify the uncertainty of IPS in this paper. In principle, the Gaussian process, Bayesian neural works and other probabilistic graphical models also work for the uncertainty quantification of IPS~\cite{zhu2017big} but may faces the challenge of the practical approximate inference on the large scale datasets~\cite{liao2020uncertainty,mccandless2009bayesian}.



\section{Conclusions}
In this paper, we firstly reveal the uncertainty miscalibration for the inverse propensity scoring for the recommendation on data missing not at random.
We propose the uncertainty calibration for the inverse propensity scoring and compare three uncertainty calibration methods for this problem.
The calibrated IPS is proved to have a smaller bias according to our theoretical analysis.
Experimental results on two representative datasets, Coat Shopping and Yahoo! R3, shows that 
with the calibrated inverse propensities, the recommendation accuracy is notably improved. 

\bibliographystyle{plain}
\bibliography{refs}

\clearpage
\appendix
{\Large\textbf{\quad Appendix}}

\setcounter{table}{0}
\setcounter{figure}{0}
\setcounter{theorem}{0}
\setcounter{corollary}{0}
\section{Bias of Inverse Propensity Estimator}
\label{appendix:bias-ips}
\begin{theorem}
    \label{theorem:upper_bound-appendix}
	For a calibrated IPS estimator, the bias is smaller than the uncalibrated IPS estimator:
    \begin{equation}
        \mathcal{E}_{\text{CIPS}}=\left|\sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}e_{u,i}}{|\mathcal{D}|}\right|  
		\leq \mathcal{E}_{\text{IPS}},
    \end{equation}
if the propensity is calibrated:
\begin{equation}
	\tilde{\nabla}_{u,i}=\frac{\tilde{p}_{u,i}-p_{u,i}}{\tilde{p}_{u,i}}, \tilde{p}=q(f(x)),
\end{equation}
\end{theorem}
$q$ is a specific uncertainty calibration method, such as MC-Dropout, deep ensembles and the platt scaling.
\begin{proof}
	For a calibrated propensity, the propensity bias has a smaller bias and then the estimator bias smaller according to Lemma~\ref{lemma:bias}.
\end{proof}
\begin{corollary}
	The unbiased and better calibration arguments in Theorem~\ref{theorem:upper_bound} also holds for the doubly robust estimator in~\cite{wang2019doubly}, which consists of the IPS estimator and the error-imputation-based estimator.
\end{corollary}

\begin{proof}
	It was shown in~\cite{wang2019doubly} that the bias term of the doubly estimator is also proportional to the IPS bias:
	\begin{equation}
		\mathcal{E}_{\text{IPS}}=\left|\sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}\delta_{u,i}}{|\mathcal{D}|}\right|,
	\end{equation}
where $\delta_{u,i}$ is the error derivation for missing ratings.
This completes the proof.
\end{proof}

\section{Generalization Bound of Calibrated Inverse propensity Scoring}
\label{appendix:generalization-bound}
Given the observation of the rating matrix $R$, the optimal rating prediction $\hat{R}^{*}$ is learned by the calibrated IPS estimator over the hypothesis space $\mathcal{H}$. We then present the generalization bound and the bias-variance decomposition of the calibrated IPS estimator using the expected calibration errors~\cite{schnabel2016recommendations}. 
\begin{theorem}\label{theorem:generalization-bound-appendix}
	For any finite hypothesis space $\mathcal{H}$ of the recommendation prediction estimations, the prediction error of the optimal prediction matrix $\hat{R}^{*}$ using the calibrated doubly robust estimator has the following generalization bound:
    \begin{equation}\label{eqn:generalization-bound-appendix}
       \mathcal{E}(\hat{R}^{*},R^{o})+
       \sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}}{|\mathcal{D}|}+
       \sqrt{\frac{\log \frac{2|\mathcal{H}|}{\eta}}{2|\mathcal{D}|^2}\sum_{u,i\in\mathcal{D}}\frac{1}{\hat{p}_{ui}^2}},
    \end{equation}
    where the star superscript means the optimal prediction and the tilde means the calibrated IPS estimator.
    $R^{o}$ is the observed rating matrix $R^{o}=\{r_{ui}, o_{ui}=1\}$
    The second term and third corresponds to the bias term and variance term respectively.
\end{theorem}

\begin{proof}
Following the generalization bounds of the IPS and DR scoring models in \cite{schnabel2016recommendations,wang2019doubly}, we replace the propensity error with the calibrated one $\tilde{\nabla}$ and get the generalization bound of the calibrated IPS model.
\end{proof}

We merely assume that the calibrated IPS has a lower estimation error and the calibration loss. Moreover, the IPS predictions are generally overestimated. With this two assumptions in mind, we have the following corollary.

\begin{corollary}\label{corollary:bias-variance-appendix}
Compared with the inverse propensity scoring estimator, the prediction error bound of the calibrated doubly robust estimator has a smaller bias and has a upper bound that is proportional to $\text{ECE}$:
    \begin{equation}\label{eqn:ece-bound-appendix}
        \sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}}{|\mathcal{D}|} \leq \sum_{u,i\in\mathcal{D}}\frac{n\cdot \text{ECE}}{|\mathcal{D}|},
    \end{equation}
    where $n$ is the number of the bins for ECE.
\end{corollary}
\begin{proof}
    The calibrated propensity has a lower bias so the bias term of the calibrated IPS is reduced:
    \begin{equation}
        \sum_{u,i\in\mathcal{D}}\frac{\tilde{\nabla}_{u,i}}{|\mathcal{D}|} < \sum_{u,i\in\mathcal{D}}\frac{{\nabla}_{u,i}}{|\mathcal{D}|}.
    \end{equation}
    For the upper bound that consists of ECE, we first rewrite ECE as:
    \begin{equation}
        \text{ECE}=\sum_{j=1}^{n}|\xi_{j}-\hat{\xi}_{j}|=\sum_{i=1}^{n}\left|\sum_{i=1}^{B_{j}}p_{ji}-\sum_{i=1}^{B_{j}}\tilde{p}_{ji}\right|,
    \end{equation}
    where $B_{ji}$ is the number of samples in the $j$-th bin and $p_{ji}$ is the propensity of the $i$-th sample in the $j$-th bin.
    By taking the absolute value for every bin , we can get the result of Eqn.~
    \ref{eqn:ece-bound-appendix}.
\end{proof}

\section{Evaluation Metrics}
\label{appendix:evaluation-metrics}
DCG and Recall are calculated as follows:
\begin{eqnarray}
    {\rm DCG}(K) = \sum_{k=1}^K \frac{Rel_{k}}{\log_2 (k+1)}, \\
    {\rm Recall}(K) = \sum_{k=1}^K Rel_{k},
\end{eqnarray}
where $k$ is ranking order, $K$ is a hyperparameter of the DCG metric and $Rel_k$ is a binary indicator which represents whether $k$-th sample is a positive sample.


\end{document}
