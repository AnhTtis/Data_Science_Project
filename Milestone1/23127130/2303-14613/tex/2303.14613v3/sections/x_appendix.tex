\clearpage

\appendix

\section{Details of User Study}
\label{sec:details_of_user_study}
A comparison pair contains two $10$-second videos that are played in order from left to right. We generate each pair using the same speech and character model. The user study questionnaires are created using the Human Behavior Online (HBO) tool offered by the Credamo platform \cite{credamo}. This tool is designed to conduct psychological experiment sample collection without complex programming. All tests and questionnaires are composed of $24$ video pairs. An experiment takes an average of $10$ minutes to complete. We recruit participants from the US and China through Credamo. Participants who take tests with sound are required to speak English fluently. Following \cite{alexanderson2022diffusiongesture}, an attention check is randomly introduced in the experiment to screen valid samples. Specifically, a text message: \emph{attention: please select the rightmost option} appears both at the bottom of the video pair for the whole duration of the question and in the video during the transition gap between the two clips. Samples that fail the attention check are not used for final results.

\subsection{Motion-Quality Study}

\subsubsection{BEAT Dataset}
We select 24 speech segments from the BEAT dataset's test set to create gestures, generating 24 video clips for each method. We compare four approaches: GT, Ours, Ours (w/o transcript), and CaMN, leading to 12 potential pairwise combinations for side-by-side demonstrations. This results in a total of 288 video pairs (24 speech samples $\times$ 12 combinations). Each participant is asked to assess 24 video pairs, encompassing all 24 speech samples. Each of the 12 possible comparison combinations appears twice. The speech samples and pairing of comparisons are randomized for each participant.

\subsubsection{ZeroEGGS Dataset}
\label{subsubsec:motion-quality_study_ZeroEGGS}
We select $6$ audio clips from the ZeroEGGS test recordings in neutral style (\emph{003\_Neutral\_2} and \emph{004\_Neutral\_3}) to synthesize gestures based on $4$ different styles (\emph{happy}, \emph{sad}, \emph{angry}, and \emph{old}), yielding $24$ video clips for each system. We compare two systems, ZE and Ours, in this study. During the assessment, each participant encounters all 24 video clips ($6$ audio clips $\times$ $4$ styles) once in a randomized order, with the outcomes of ZE and Ours evenly distributed in the front position of each video pair.

\subsection{Style-Control Study on the ZeroEGGS Dataset}
The configurations for the \emph{style correctness (w/ dataset label)} and \emph{style correctness (w/ random prompt)} tests are identical, except for the input text prompt. 
In each study, we use the same $6$ audio clips from experiment \ref{subsubsec:motion-quality_study_ZeroEGGS} to generate motions conditioned on $4$ text prompts. This leads to 24 video clips for each system, MD-ZE and Ours, in each test. 
For the \emph{style correctness (w/ dataset label)}, the text prompts are: $\{$\emph{the person is happy}, \emph{the person is sad}, \emph{the person is angry}, \emph{an old person is gesticulating}$\}$. For the \emph{style correctness (w/ random prompt)} test, the text prompts are: $\{$\emph{Hip-hop rapper}, \emph{holding a cup with the right hand}, \emph{looking around}, \emph{a person just lost job}$\}$.
Again, each participant evaluates these $24\times{}2$ video clips in pairs in a randomized sequentially, where the resulting motion of MD-ZE and Ours evenly distributed in the front position of each video pair.


\section{Implementation Details of Baselines}
\label{sec:implementation_details_of_baselines}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/chatGPT_prompts.pdf}
    \caption{Prompt inputs to ChatGPT \cite{openai2022chatgpt}.}
    \Description{}
    \label{fig:chatGPT_prompts}
\end{figure}
%
At the time of writing this work, the authors of CaMN \cite{liu2021beatdataset} have not provided the pre-trained generation model. Instead, they offered training codes for a toy dataset and a pre-trained motion auto-encoder for the calculation of FGD. We run the provided training codes on a larger dataset used in the original paper, discarding the unreleased emotion label from the conditions of the model. The FGD value of the reproduced model is $122.5$, which is close to the value reported in the original paper ($123.7$). The visual quality of gestures synthesized by the reproduced model is similar to that shown in the video demo of CaMN. We then follow the configuration above and train a new CaMN model on a part of the BEAT dataset used in our work (Section \ref{subsec:system_setup}). This model achieves better performance on FGD ($110.23$) and is utilized as the baseline in this paper.

For the baseline MD-ZE on the ZeroEggs dataset, the two components of this baseline, i.e., MotionDiffuse (MD) \cite{zhang2022motiondiffuse} and ZeroEGGS (ZE) \cite{ghorbani2022zeroeggs}, are constructed using the official pre-trained models. Note that the skeletons of the two models are different. We thus retarget the motion prompt generated by MD to fit the interface of ZE. Specifically, we first convert the generated motion prompt, represented as SMPL \cite{loper2015smpl} joint positions, into joint rotations and save them as a BVH file. Then, we retarget the prompt in SMPL skeleton to the ZeroEGGS skeleton using a Blender add-on, \emph{BVH Retargeter} \cite{padovani2020bvhretargeter}.

\section{Prompts for ChatGPT}
\label{sec:prompts_for_chatgpt}
\fig\ref{fig:chatGPT_prompts} demonstrates the prompt inputs for ChatGPT \cite{openai2022chatgpt} in Section \ref{subsec:application}.