\section{Evaluation}
\label{sec:results}
In this section, we first describe the setup of our system then evaluate our results, compare them with other systems, introduce several potential applications of our system, and validate various design choices of our framework through ablation study.

\subsection{System Setup}
\label{subsec:system_setup}

\subsubsection{Data}
We train and test our system on two high-quality speech-gesture datasets: \emph{ZeroEGGS} \cite{ghorbani2022zeroeggs} and \emph{BEAT} \cite{liu2021beatdataset}. The ZeroEGGS dataset contains two hours of full-body motion capture and audio from monologues performed by an English-speaking female actor in $19$ different styles. We acquire the synchronized transcripts using an automatic speech recognition (ASR) tool \cite{alibaba2009asr}. The BEAT dataset contains $76$ hours of multimodal speech data, including audio, transcripts, and full-body motion captured from $30$ speakers performing in eight emotional styles and four different languages. We only use the speech data of English speakers, which amounts to about $35$ hours in total.

\subsubsection{Settings}
Our system generates motions at $60$ frames per second. We train the motion VQ-VAE (Section \ref{subsec:motion_representation}) with a downsampling rate $d = 8$, a batch size of $32$, and $C = 512$. To learn the gesture-transcript embedding space (Section \ref{sec:gesture-transcript_embedding_learning}), the values of $C_s$, $\tau$, and $B$ are set to $768$, $0.07$, and $32$, respectively. As for the diffusion module (Section \ref{sec:stylized_co-speech_gesture_diffusion_model}), the denoising network is based on a $12$-layer, $768$-feature wide, encoder-only transformer with $12$ attention heads. The number of diffusion steps is $N=1000$, the training batch size is $128$ per GPU, and the parameters $\delta^a$, $C_{\eqword{CLIP}}$, $C_{\eqword{ada}}$, $w_{\eqword{noise}}$, $w_{\eqword{semantic}}$, $w_{\eqword{style}}$ are set to $8$, $768$, $768$, $1.0$, $0.1$, and $0.07$, respectively. We train the motion VQ-VAE using regular speech clips of $4$ seconds in length and other models with sentence-level clips ranging from $1$~second to $15$~seconds in length. We train all these models using two NVIDIA Tesla V100 GPUs for about five days. During inference, it takes our system about 1.5 seconds to generate an 1-second (60 frames) gesture clip on a single Tesla V100 GPU.

\subsection{Results}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/results_ours.pdf}
    \caption{Gestures synthesized by our system conditioned on three different types of style prompts (text, video, and motion) for the same speech. The character performs \emph{angry} gestures when given the text prompt \emph{the person is angry}. The \emph{Hip-hop} style gestures are imitated from a Hip-hop music video \cite{khalifa2011hippopvideo}. Semantic information of a non-human video such as \emph{trees sway with the wind} \cite{rijavec2018windtreevideo} can also be perceived by our system and guides the character to sway from side to side. As for the motion prompt, our system successfully generates \emph{arm-up} gestures similar to the motion example.}
    \Description{}
    \label{fig:results_ours}
\end{figure*}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/sub-body_style_control.pdf}
    \caption{Results of body part-level style control. A set of style prompts are applied to different body parts to achieve fine-grained style control.}
    \Description{}
    \label{fig:sub-body_style_control}
\end{figure*}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/effect_of_style_prompt.pdf}
    \caption{Effect of the input style prompt controlled by adjusting the scale $s$ of the classifier-free guidance. A larger $s$ results in a more pronounced style effect.}
    \Description{}
    \label{fig:effect_of_style_prompt}
\end{figure}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/time-varied_style_control.pdf}
    \caption{Results of the time-varied style control by changing style prompt at each sentence.}
    \Description{}
    \label{fig:time-varied_style_control}
\end{figure}
%
\fig\ref{fig:results_ours} shows the visualization results of our system generating gestures conditioned on three different types of style prompts (text, video, and motion), with the test speech taken from the ZeroEGGS dataset. Our system successfully generates realistic gestures with reasonable styles, as required by the corresponding prompts. The character performs \emph{angry} gestures when given the text prompt \emph{the person is angry}. The \emph{Hip-hop} style gestures are imitated from a Hip-hop music video \cite{khalifa2011hippopvideo}. Semantic information from a non-human video, such as \emph{trees sway with the wind} \cite{rijavec2018windtreevideo}, can also be perceived by our system, guiding the character to sway from side to side. As for the motion prompt, our system successfully generates \emph{arm-up} gestures similar to the motion example.

\fig\ref{fig:sub-body_style_control} demonstrates the results of body part-aware style control using our system. We employ different prompts to control the styles of various body parts. The resulting motions produce these styles while maintaining a natural coordination among the body parts.

As discussed in Section \ref{subsubsec:training_of_denosing_network}, the scale factor $s$ of the classifier-free guidance scheme controls the effect of the input style prompt. Increasing $s$ will enhance or even exaggerate the given style, which can be seen in \fig\ref{fig:effect_of_style_prompt}, where the hands of the character rise higher when increasing~$s$ with the text prompt \emph{raise both hands}.

Our system allows the style of gestures to change with every sentence. As shown in \fig\ref{fig:time-varied_style_control}, we change the style prompts from \emph{raise both hands} to \emph{out of breath}, and then to \emph{Usain Bolt}. The generated gestures accurately match the styles and maintain smooth transitions between different styles.

Moreover, \fig\ref{fig:application_style_prompt_generation} shows the visualization results of comparing style-conditional synthesis (the first row) with style-unconditional synthesis (the second row). Specifying style prompts for each sentence can effectively guide the character's performance, making the co-speech gestures more vivid.
%
\begin{table*}[t]
    \centering
    \caption{Average scores of user study with $95\%$ confidence intervals. \emph{Our system without transcript input (w/o transcript)} excludes the $\mathcal{L}_{\eqword{semantic}}$ while the semantic-aware attention layer is replaced by the causal self-attention layer. Then the generator is retrained to synthesize gestures based solely on the audio. 
    For \emph{style correctness (w/ dataset label)}, the text prompt contains the style label of the dataset and only describes the motion style that appears in the dataset. Note that we do not use any style label for training. Meanwhile, the text prompt utilized in the \emph{style correctness (w/ random prompt)} test does not contain any style label of the dataset, and the motion style that the text prompt describes may not exist in the dataset.}
    \label{tab:user_study}

    \newcolumntype{Y}{>{\raggedleft\arraybackslash}X}
    \newcolumntype{Z}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\linewidth}{llYYYY}
        \toprule
        Dataset & System & Human Likeness $\uparrow$ & Appropriateness $\uparrow$ & \makecell{Style Correctness $\uparrow$ \\ (w/ dataset label)} & \makecell{Style Correctness $\uparrow$ \\ (w/ random prompt)} \\ 
        \toprule
        \multirow{4}*{BEAT} & GT & $0.47 \pm 0.08$ & $0.73 \pm 0.08$ & - & - \\
        \cline{2-6}
        & CaMN & $-0.99 \pm 0.12$ & $-1.06 \pm 0.10$ & - & - \\
        & Ours (w/o transcript) & $0.23 \pm 0.10$ & $-0.33 \pm 0.07$ & - & - \\
        & Ours & $\bm{0.31 \pm 0.07}$ & $\bm{0.51 \pm 0.07}$ & - & - \\
        
        \midrule
        \multirow{3}*{ZeroEGGS} & ZE & $-0.25 \pm 0.10$ & $-1.33 \pm 0.12$ & - & - \\
        & MD-ZE & - & - & $-1.65 \pm 0.15$ & $-1.62 \pm 0.17$ \\
        & Ours & $\bm{0.25 \pm 0.10}$ & $\bm{1.33 \pm 0.12}$ & $\bm{1.65 \pm 0.15}$ & $\bm{1.62 \pm 0.17}$ \\
        \bottomrule
    \end{tabularx}

\end{table*}

\subsection{Comparison}
Following the convention of recent gesture generation research \cite{alexanderson2020stylegesture,ghorbani2022zeroeggs,ao2022rhythmicgesticulator}, we evaluate the generated motions through a series of user studies. Quantitative evaluations are also conducted, with results provided in later sections as a reference.

\subsubsection{Baselines}
The BEAT dataset is released with a baseline approach, Cascaded Motion Network (CaMN), which takes the audio, transcript, emotion labels, speaker ID, and facial blendshape weights as inputs to generate gestures using a cascaded architecture. Here, we ignore the fingers and facial weights of the generated motion. Similarly, the ZeroEGGS dataset also provides a baseline, the ZeroEGGS algorithm (ZE), which encodes speech audio and a motion exemplar to synthesize target stylized gestures. ZE achieves the best performance among all deep-based models in the $2022$ GENEA Challenge \cite{yoon2022genea}. Because no model in previous studies supports text prompt-conditioned synthesis, we construct a baseline, MD-ZE, by combining a powerful text prompt-to-motion model, MotionDiffuse (MD) \cite{zhang2022motiondiffuse}, and ZeroEGGS. Specifically, MD first transfers the given text prompt into a motion exemplar, then ZE generates target stylized gestures based on the motion exemplar and input speech. For more implementation details of these baselines, please refer to Appendix \ref{sec:implementation_details_of_baselines}.
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/comparison_results.pdf}
    \caption{Qualitative comparison between our system and MD-ZE \cite{zhang2022motiondiffuse, ghorbani2022zeroeggs} using speech excerpts from the user study. The left text prompt displays the style label (angry) from the dataset, while the right prompt describes a motion style absent in the dataset.}
    \Description{}
    \label{fig:comparison_results}
\end{figure*}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/application_style_prompt_generation.pdf}
    \caption{Qualitative comparison between style-conditional synthesis (first row) and style-unconditional synthesis (second row). Co-speech gestures are improved by providing a style prompt for each sentence within the speech transcript.
    }
    \Description{}
    \label{fig:application_style_prompt_generation}
\end{figure*}

\subsubsection{User Study}
\label{subsubsec:user_study}
We conduct user studies using pairwise comparisons, similar to the method described in \cite{alexanderson2022diffusiongesture}. In each test, participants are presented with two 10-second video clips synthesized by different models (including the ground truth) for the same speech, played sequentially. Participants are required to select their preferred clip based on the instruction displayed below the videos and rate their preference on a scale of $0$ to $2$, with $0$ indicating no preference. The other clip in the video pair automatically receives the opposite score (e.g., if the participant rates the preferred video $1$, the unselected video gets a score of $-1$). We recruit participants through the Credamo platform \cite{credamo}.

We conduct four types of preference tests: \emph{human likeness}, \emph{appropriateness}, \emph{style correctness (with dataset label)}, and \emph{style correctness (with random prompt)}. All tests include attention checks. For the \emph{human likeness} test, participants are asked whether the generated motion resembles the motion of a real human. The video clips are muted to prevent any influence from the speech. In the \emph{appropriateness} test, participants rate whether the generated motion matches the rhythm and semantics of the speech. For the \emph{style correctness (w/ dataset label)} test, participants are required to assess how well the generated motion represents the given text style prompt. Note that the text prompt contains a style label provided by the dataset in this test, and the videos are muted. Lastly, a similar test, \emph{style correctness (with random prompt)}, is conducted to evaluate the system's generalizability. The text prompt used in this test does not contain any style label from the dataset, and the motion style that the text prompt describes may not exist in the dataset. We provide more details about the user study in Appendix \ref{sec:details_of_user_study}.

On the BEAT dataset, we only compare our system with CaMN using \emph{human likeness} and \emph{appropriateness} tests, as CaMN does not support text prompt-conditioned style control in the original paper. In practice, we compare four methods: the ground-truth gestures (GT), our system (Ours), our system without transcript input (w/o transcript) for ablation, and CaMN. All the systems take only speech as input, with our system generating motion in the style-unconditional mode (w/o style prompt), and the extra speaker ID input of CaMN defaulting to the ground truth.

In this user study, $100$ and $98$ subjects pass the attention checks for the human likeness and appropriateness tests, respectively. Table \ref{tab:user_study} shows the average scores obtained in these tests. We conducted a one-way ANOVA and a post-hoc Tukey multiple comparison test for each user study. For the \emph{human likeness} test, GT, Ours, and Ours (w/o transcript) are statistically tied and outperform CaMN ($p < 0.001$). As for the \emph{appropriateness} test, Ours receives a higher score compared to other methods ($p < 0.001$), but the score of Ours (w/o transcript) drops significantly due to the lack of semantic information. Based on these two motion quality-related studies, we conclude that our diffusion-based model outperforms CaMN, and the semantic modules (including the semantics-aware attention layer and $\mathcal{L}_{\eqword{semantic}}$) are crucial to ensuring semantic consistency between speech and generated gestures.

On the ZeroEGGS dataset, we evaluate our system using all four preference tests. For the \emph{human likeness} and \emph{appropriateness} tests, we compare our system with ZE. For the \emph{style correctness} tests, we compare our system with MD-ZE. We let our system generate gestures based on motion prompts for a fair comparison, where the motion prompt is randomly sampled from the training set containing four types of style (happy, sad, angry, and old). Two speech recordings in \emph{neutral} style are selected as the test set to prevent the potential style information embedded in the speech from affecting the generated styles. Four text prompts are prepared for the \emph{style correctness (w/ dataset label)} test: $\{$\emph{the person is happy}, \emph{the person is sad}, \emph{the person is angry}, \emph{an old person is gesticulating}$\}$. As for the \emph{style correctness (w/ random prompt)} test, the test style prompts either specify a speaker style (\emph{Hip-hop rapper}), define a pose (\emph{holding a cup with the right hand} and \emph{looking around}), or describe an abstract condition (\emph{a person just lost job}). Note that there is no ground truth for these test style prompts in the ZeroEGGS dataset.

After the attention checks, we recorded the answers of $99$ participants for the \emph{human likeness} and \emph{appropriateness} tests. As shown in Table~\ref{tab:user_study}, the difference between human-likeness scores of Ours and ZE is not significant ($p < 0.05$), while Ours outperforms ZE in the speech-matching metric (appropriateness) by a clear margin ($p < 0.001$). This is because ZE only takes speech audio as input and may lack sufficient semantic information. 
In the \emph{style correctness} tests, $100$ valid subjects score Ours higher compared to MD-ZE for text prompts containing style labels of the dataset ($p < 0.001$). The left part of \fig\ref{fig:comparison_results} offers a visualization demo, where the result of MD-ZE reflects some sense of sadness, but the motion of Ours is more vivid. These results confirm the efficiency of our system in style control. Our method remains robust even when given random prompts, ahead of MD-ZE ($p < 0.001$), and generates accurate stylized gestures. The visualization results are shown in the right part of Figure~\ref{fig:comparison_results}.
%
\begin{table*}[t]
    \centering
    \caption{Quantitative evaluation on the BEAT and ZeroEGGS datasets. Motion quality-related metrics (FGD, SRGR, and SC) are only calculated on the big BEAT dataset to guarantee the accuracy of approximation of the motion distribution (FGD and SRGR) and the generalizability of the learned semantic space (SC). Style-related metric (SRA) is measured on the ZeroEGGS dataset because it has a variety of styles and the comparison system MD-ZE is trained on it. This table reports the mean ($\pm$ standard deviation) values for each metric by synthesizing on the test data $10$ times.}
    \label{tab:quantitative_evaluation}

    \newcolumntype{Y}{>{\raggedleft\arraybackslash}X}
    \newcolumntype{Z}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\linewidth}{llYYYY}
        \toprule
        Dataset & System & FGD $\downarrow$ & SRGR $\uparrow$ & SC $\uparrow$ & SRA ($\%$) $\uparrow$ \\ 
        \toprule
        \multirow{4}*{BEAT} & GT & - & $1.00$ & $0.80$ & - \\
        \cline{2-6}
        & CaMN & $110.23 \pm 0.00$ & $0.25 \pm 0.00$ & $0.33 \pm 0.00$ & - \\
        & Ours (w/o transcript) & $97.82 \pm 2.56$ & $0.09 \pm 0.02$ & $0.11 \pm 0.03$ & - \\
        & Ours & $\bm{85.17 \pm 3.35}$ & $\bm{0.51 \pm 0.08}$ & $\bm{0.58 \pm 0.15}$ & - \\
        
        \midrule
        \multirow{4}*{ZeroEGGS} & MD-ZE & - & - & - & $47.50 \pm 0.97$ \\
        & Ours (w/o $\mathcal{L}_{\eqword{style}}$) & - & - & - & $64.28 \pm 2.17$ \\
        & Ours (concatenation fusion) & - & - & - & $68.15 \pm 2.02$ \\
        & Ours & - & - & - & $\bm{71.53 \pm 1.01}$ \\
        \bottomrule
    \end{tabularx}
\end{table*}

\subsection{Quantitative Evaluation}
\label{subsec:quantitative_evaluation}
We quantitatively measure the motion quality, speech-gesture content matching, and style correctness using three metrics : Fr{\'e}chet Gesture Distance (FGD) \cite{yoon2020trimodalgesture}, Semantics-Relevant Gesture Recall (SRGR) \cite{liu2021beatdataset}, Semantic Score (SC), and Style Recoginition Accuracy (SRA) \cite{jang2022motionpuzzle}. 

The FGD measures the distance between the distributions of latent features calculated from generated and real gestures, respectively. This metric is typically used to assess the perceptual quality of gestures. A lower FGD usually suggests higher motion quality. 

It has been shown that vanilla L1 distance and Probability of Correct Keypoint (PCK) are not suitable for assessing gesture performance due to the inherent many-to-many correspondence between speech and gestures \cite{yoon2020trimodalgesture,liu2021beatdataset}. To address this issue, we adopt the SRGR metric proposed by \citet{liu2021beatdataset}, which uses manually-labeled semantic scores as the weights for PCK. This metric ensures that semantically relevant gestures align closely with the ground truth, while allowing for variation in other gestures, such as beat gestures. Besides, SRGR partially captures the diversity of generated gestures according to \cite{liu2021beatdataset}. A higher SRGR indicates better performance.

To evaluate the semantic coherence between speech and generated gestures, we propose a new metric, namely the semantic score (SC), to calculate the semantic similarity between generated motion and the ground-truth transcripts in the gesture-transcript embedding space (Section \ref{sec:gesture-transcript_embedding_learning}).  SC can be computed as
\begin{align}
    \eqword{SC} = \cos(\vect{z}^{t}, \vect{z}^{g*}_0),
\end{align}
where $\vect{z}^{t}$ and $\vect{z}^{g*}$ are the ground-truth transcript encoding and the gesture encoding of the generated motion, respectively. SC $\in [-1, 1]$ and a higher SC suggests better speech-gesture content matching.

Following \cite{jang2022motionpuzzle}, we pre-train a classifier on the ZeroEGGS dataset to predict motion style labels. The dataset contains 19 distinct styles, but we exclude some ambiguous ones, such as \emph{agreement}, \emph{disagreement}, \emph{oration}, \emph{pensive}, \emph{sarcastic}, and \emph{threatening}. For testing, we use all speech recordings in the \emph{neutral} style as input speech. Text prompts are generated using a prompt template (\emph{the person is [style label]}), where the style label (e.g., happy and sad) is from the dataset.  We then use the neutral speech and synthetic text prompts as inputs to generate stylized gestures. Lastly, we employ the style classifier to measure the style recognition accuracy (SRA) on the test set. A higher SRA indicates better performance in terms of style control. It is important to note that SRA is limited to covering styles that appear in the dataset.

We conduct motion quality-related evaluations, specifically FGD, SRGR, and SC, on the BEAT dataset only, as a large dataset is necessary to ensure accurate approximation of the motion distribution (FGD and SRGR) and generalizability of the learned semantic space (SC). For style-related evaluations (SRA), we measure performance on the ZeroEGGS dataset only, as it has a variety of styles and the baseline system MD-ZE is trained on it. The FGD, SRGR and SC are calculated using sentence-level motion segments, while 10-second segments are used to measure the SRA. We compute the mean ($\pm$ standard deviation) values for each metric by synthesizing on the test data 10 times. The test set of the BEAT dataset is composed of the first file of both \emph{conversation} and \emph{self-talk} sessions for each speaker who appears in the training set.

As shown in Table \ref{tab:quantitative_evaluation}, our system outperforms the baseline CaMN in motion quality-related metrics (FGD, SRGR and SC). The SC value of our system significantly decreases when the transcript input is discarded, highlighting the importance of the gesture-transcript embedding module. In terms of style control, our system outperforms the baseline MD-ZE in the SRA metric by a clear margin, which is consistent with the results of the user study (see Section \ref{subsubsec:user_study}). The SRA results of different ablation settings, i.e., Ours (w/o $\mathcal{L}_{\eqword{style}}$) and Ours (concatenation fusion), demonstrate the necessity of these components in our system.

\subsection{Application}
\label{subsec:application}
Our system enables several interesting applications. One such application involves enhancing co-speech gestures by specifying the style of each sentence in the speech and using style prompts to guide the performance of the character. This process can be automated by a large language model, such as ChatGPT \cite{openai2022chatgpt}. Specifically, we can instruct ChatGPT to generate a style prompt for each sentence in the speech and then generate stylized co-speech gestures accordingly. \fig\ref{fig:application_style_prompt_generation} demonstrates the editing results (the first row) obtained by using generated text prompts, which are more vivid than the style-unconditional results (the second row).

Another application is to let ChatGPT write a story and creating the corresponding style prompts. Then, we can translate the generated transcript into audio using a Text-To-Speech tool \cite{murfai2022tts} and use the audio as the speech input. This can result in a skillful storyteller. As shown in \fig\ref{fig:application_text_generation}, we let ChatGPT write a short joke about \emph{travel} with suitable text prompts and synthesize gestures using them. The generated gestures successfully portray the joke with diverse body styles.

The full prompt inputs to ChatGPT of these application examples are detailed in Appendix \ref{sec:prompts_for_chatgpt}. Please refer to the supplementary video for animation results.
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/application_text_generation.pdf}
    \caption{Qualitative result of gesture editing (Section \ref{subsec:application}). Both the speech transcript and text prompts are generated by ChatGPT \cite{openai2022chatgpt}. Additionally, we translate the generated transcript into audio using a Text-To-Speech tool \cite{murfai2022tts} and use the resulting audio as the speech input. Note that such synthesized voices are not seen during training.}
    \label{fig:application_text_generation}
    \Description{}
\end{figure*}

\subsection{Ablation Study}
We analyze the impact of the gesture-transcript embedding, the style loss, and the style fusion mechanism on our performance. The results are reflected in Table \ref{tab:user_study}, Table \ref{tab:quantitative_evaluation}, and the supplementary video.

\subsubsection{Gesture-Transcript Embedding}
In this experiment, we omitted the transcript embedding input of the denoising network, replaced the semantics-aware attention layer with the causal self-attention layer, and retrained the generator without the semantic loss $\mathcal{L}_{\eqword{semantic}}$. The supplementary video demonstrates that the generated motion maintains rhythmic harmony but lacks reasonable semantics. Moreover, semantics-aware metrics, such as appropriateness (Table \ref{tab:user_study}) and SC (Table \ref{tab:quantitative_evaluation}), also show a significant drop for our model (w/o transcript). These results confirm that the gesture-transcript embedding module effectively enhances the semantic consistency between speech and gestures.

\subsubsection{Style Loss}
In this experiment, we retrain the denoising network without the style loss $\mathcal{L}_{\eqword{style}}$. The supplementary video demonstrates that the recognizability of the generated style is reduced. The style recognition accuracy (SRA) in Table \ref{tab:quantitative_evaluation} also decreases. An intuitive explanation is that the style-relevant \emph{knowledge} embedded in CLIP serves as a guide for the stylization of the generated gestures through the style loss. This guidance approximates style-aware supervision, even in the absence of explicit labels.

\subsubsection{Style Fusion Mechanism}
In this experiment, we replace the AdaIN-based style embedding fusion scheme with direct concatenation, where the style embedding is broadcasted and concatenated to the intermediate deep features in the generator for style modification. Although the SRA value of Ours (concatenation fusion) drops only  slightly compared to AdaIN, the supplementary video shows that the motion generated by Ours (concatenation) exhibits jittering and unnatural movements. 