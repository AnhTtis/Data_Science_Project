\section{Stylized Co-Speech Gesture Diffusion Model}
\label{sec:stylized_co-speech_gesture_diffusion_model}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/denoising_network.pdf}
    \caption{Architecture of the denoising network. The model is a multi-layer transformer with a causal attention structure. It takes the audio and transcript of a speech and a style prompt as input and estimates the diffusion noise. Three CLIP-based encoders are learned to support different types of style prompts. The multimodal features are incorporated into the network at different stages via semantics-aware layers and AdaIN layers respectively. \emph{Norm} means the layer normalization and \emph{FFN} is the feed-forward network.}
    \Description{}
    \label{fig:denoising_network}
\end{figure*}

The core of our system is a conditional latent generative model $\mathcal{G}$ which synthesizes a sequence of latent gesture codes $\vect{Z}=[\vect{z}_l]_{l=1}^{L}$ conditioned on a speech and a style prompt. The latent sequence $\vect{Z}$ is then decoded into gestures using the VQ-VAE decoder $\mathcal{D}_{\eqword{VQ}}$ learned in Section~\ref{subsec:motion_representation}. Formally, the generator $\mathcal{G}$ computes
\begin{align} 
    \vect{Z} = \mathcal{G}(\vect{A}, \vect{T}, \vect{P}),
\end{align}
where $\vect{A}$ and $\vect{T}$ denote the audio and transcript of the speech respectively and $\vect{P}$ is the style prompt. 
The speech audio $\vect{A}=[\vect{a}_i]_{i=1}^{L}$ is parameterized as a sequence of acoustic features resampled into the same length of the gesture representation. Each $\vect{a}_i$ encodes the onsets and amplitude envelopes that reflect the beat and volume of speech respectively. The speech transcript $\vect{T}$ is preprocessed as described in Section~\ref{sec:gesture-transcript_embedding_learning}. The generator $\mathcal{G}$ utilizes $\vect{A}$ to infer low-level gesture styles such as rhythm and stress, $\vect{T}$ for the semantics-level features, and $\vect{P}$ to determine the overall style of the gestures.

At inference time, the generator $\mathcal{G}$ is reformulated as an autoregressive model, where a gesture is determined by not only the speech context and style prompt but also the previous motion. Formally, the latent sequence $\vect{Z}=[\vect{z}_l]_{l=1}^{L}$ is generated as
\begin{align}
    \label{eqn:autoregressive_generative_model}
    {\vect{z}_l^*} = \mathcal{G}([\vect{z}_i^*]_{i=1}^{l-1}, [\vect{a}_i]_{i=1}^{l+\delta^a}, \vect{T}, \vect{P}),
\end{align}
where we use the asterisk ($*$) to indicate quantities already generated by $\mathcal{G}$. Note that the generator leverages $\delta^a$ frames of future audio features to determine the current gestures.

\subsection{Latent Diffusion Models}
The generator $\mathcal{G}$ is based on the latent diffusion model \cite{rombach2022latentdiffusion}, which is a variant of diffusion models that applies the forward and reverse diffusion processes in a pretrained latent feature space. The \emph{diffusion process} is modeled as a Markov noising process. Starting from a latent gesture sequence $\vect{Z}_0$ drawn from the gesture dataset, the diffusion process progressively adds Gaussian noise to the real data until its distribution approximates $\mathcal{N}(\vect{0}, \vect{I})$. The distribution of the latent sequences thus evolves as
\begin{align}
    q(\vect{Z}_n | \vect{Z}_{n-1}) = \mathcal{N}(\sqrt{\alpha_n}\vect{Z}_{n-1}, (1-\alpha_n)\vect{I}),
\end{align}
where $\vect{Z}_n$ is the latent sequence sampled at diffusion step $n$, $n\in\{1, \dots, N\}$, and $\alpha_n$ is determined by the variance schedules. In contrast, the \emph{reverse diffusion process}, or the \emph{denoising process}, estimates the added noise in a noisy latent sequence. Starting from a sequence of random latent codes $\vect{Z}_N \sim \mathcal{N}(\vect{0}, \vect{I})$, the denoising process progressively removes the noise and recovers the original motion $\vect{Z}_0$.

To achieve conditional gesture generation, we train a network $\vect{E}_{\theta}$, the so-called \emph{denoising network}, to predict the noise conditioned on the noisy motion codes, the diffusion step, the speech context, and the style prompt. This network can be formulated as
\begin{align}
    \label{eqn:denoising_network}
    \vect{E}^{*}_n = \vect{E}_{\theta}(\vect{Z}_n, n, \vect{A}, \vect{T}, \vect{P}).
\end{align}
At inference time, the generator $\mathcal{G}$ leverages the sampling algorithm of DDPM~\cite{ho2020ddpm} to synthesize gestures. It first draws a sequence of random latent codes $\vect{Z}_N^*\sim{}\mathcal{N}(\vect{0}, \vect{I})$ then computes a series of denoised sequences $\{\vect{Z}_n^*\},{n=N-1,\dots,0}$ by iteratively removing the estimated noise $\vect{E}_n^*$ from $\vect{Z}_n^*$. Lastly, the final latent codes $\vect{Z}_0^*$ will be decoded into the gesture motion.

As illustrated in \fig\ref{fig:denoising_network}, our denoising network has a transformer architecture \cite{vaswani2017transformer}. We employ the causal attention layer proposed by \citet{vaswani2017transformer} that only allows the intercommunication of the current and preceding data for the causality. This architecture can be easily transformed into the autoregressive model in \eqn\eqref{eqn:autoregressive_generative_model}. Note that we extend the definition of \emph{current data} when dealing with audio features by including $\delta^a$ future frames.

The denoising network fuses the multimodal conditions $(\vect{A}, \vect{T}, \vect{P})$ in a hierarchical manner: the low-level audio features that relate to the speech rhythm and stress are first included by concatenating $\vect{A}$ with the noisy latent sequence $\vect{Z}_n$; then, the high-level transcript features $\vect{T}$ that correspond to the speech semantics are incorporated via a \emph{semantics-aware attention layer}; and lastly, the style prompt $\vect{P}$ is involved through a \emph{CLIP-guided AdaIn layer} to control the overall style of the generated gestures. 

\subsubsection{Semantics-Aware Attention Layer}
Inspired by recent successful attention-based multimodal systems \cite{jaegle2021perceiver,rombach2022latentdiffusion}, we develop a semantics-aware attention layer based on the cross-attention mechanism \cite{vaswani2017transformer} to incorporate the input transcript $\vect{T}$. Specifically, we first extract the transcript features $\vect{Z}^t$ from $\vect{T}$ using the pretrained text encoder $\mathcal{E}_T$ as described in Section~\ref{sec:gesture-transcript_embedding_learning} and compute the semantic saliency $\vect{s}^t$ using \eqn\eqref{eqn:semantic-saliency}. Then, we project $\vect{Z}^t$ to the \emph{key} $\vect{K} \in \mathbb{R}^{L_t \times C_t}$ and \emph{value} $\vect{V} \in \mathbb{R}^{L_t \times C_t}$ of the attention mechanism using learnable projection matrices and calculate the \emph{query} $\vect{Q} \in \mathbb{R}^{L \times C_t}$ using the intermediate features of the denoising network. Lastly, the semantics-aware attention layer is implemented as
\begin{align}
    \eqword{Attention}(\vect{Q}, \vect{K}, \vect{V}) = \eqword{softmax}(\frac{\vect{Q}\vect{K}^T}{\sqrt{C_t}} \cdot \vect{S}^t) \cdot \vect{V},
\end{align}
where $\vect{S}^t \in \mathbb{R}^{L \times L_t}$ is the temporally-broadcasted semantic saliency matrix of $\vect{s}^t$ that guides the network to pay additional attention to the semantically important words.

\subsubsection{CLIP-Guided AdaIN Layer}
\label{subsec:CLIP-guided-adain}
We employ an adaptive instance normalization (AdaIN) layer \cite{huang2017adain} to inject the information of the style prompts into the denoising network. Specifically, we leverage a pretrained CLIP encoder $\mathcal{E}_{\eqword{CLIP}}$ to convert the input style prompt into a style embedding $\vect{z}^s \in \mathbb{R}^{C_{\eqword{CLIP}}}$. Then, we learn a MLP network to map the style embedding $\vect{z}^s$ to $2 C_{\eqword{ada}}$ parameters that modify the per-channel mean and variance of the AdaIn layer with $C_{\eqword{ada}}$ channels.

We employ the text encoder $\mathcal{E}_{\eqword{CLIP-T}}$ of the CLIP model \cite{radford2021clip} for the text prompts and the motion encoder $\mathcal{E}_{\eqword{CLIP-M}}$ of the MotionCLIP model \cite{tevet2022motionclip} for the motion prompts. We further develop a CLIP-based video encoder $\mathcal{E}_{\eqword{CLIP-V}}$ for the video prompts, which consists of a pretrained CLIP image encoder \cite{radford2021clip} followed by a $6$-layer transformer that temporally aggregates the image features of the video into an embedding $\vect{z}^s$ in the CLIP space. Note that all these CLIP encoders are pretrained in a separate stage and their weights are frozen when training the denoising network.

\subsection{Training}
\label{subsubsec:training_of_denosing_network}
Following the standard training process of denoising diffusion models \cite{ho2020ddpm,rombach2022latentdiffusion}, we train the denoising network $\vect{E}_{\theta}$ by drawing random tuples $(\vect{Z}_0,n,\vect{A},\vect{T},\vect{P})$ from the training dataset, then corrupting $\vect{Z}_0$ into $\vect{Z}_n$ by adding random Gaussian noises $\vect{E}$, applying denoising steps to $\vect{Z}_n$, and optimizing the loss 
\begin{align}
    \mathcal{L}_{\eqword{net}} = 
    w_{\eqword{noise}}\mathcal{L}_{\eqword{noise}} +
    w_{\eqword{semantic}}\mathcal{L}_{\eqword{semantic}} +
    w_{\eqword{style}}\mathcal{L}_{\eqword{style}}.
\end{align}
Specifically, the ground-truth gesture motion $\vect{M}_0$ and its latent representation $\vect{Z}_0$, the audio $\vect{A}$, and the transcript $\vect{T}$ are extracted from the same speech sentence. $n$ is drawn from the uniform distribution $\mathcal{U}\{1,N\}$. We do not assume that the gesture dataset contains detailed style labels. Instead, we consider the motion clips $\vect{M}_P$ of random lengths but encompassing $\vect{M}_0$ as the style prompts $\vect{P}$. As suggested by \citet{kim2022CVPRDiffusionClip}, we iteratively apply denoising steps to each training tuple until obtaining $\vect{Z}_0^*$.

We first calculate the standard noise estimation loss of the diffusion models \cite{ho2020ddpm} defined as
\begin{align}
    \mathcal{L}_{\eqword{noise}} = \norm{\vect{E} - \vect{E}_{\theta}(\vect{Z}_n, n, \vect{A}, \vect{T}, \vect{P})}_{2}^{2},
\end{align}
In addition, we include a semantic loss to ensure the semantic correctness of the generated gestures. This loss is defined in the gesture-transcript joint embedding space learned in Section \ref{sec:gesture-transcript_embedding_learning}. Specifically,
\begin{align}
    \mathcal{L}_{\eqword{semantic}} = 1 - \cos(\vect{z}^g_0, \vect{z}^{g*}_0),
\end{align}
where $\cos(\cdot, \cdot)$ is the cosine distance, $\vect{z}^g_0$ and $\vect{z}^{g*}_0$ are the gesture encodings of the ground-truth and generated motions, respectively, computed using the gesture encoder $\mathcal{E}_G$ pretrained in Section \ref{sec:gesture-transcript_embedding_learning}. At last, we employ a perceptual loss to encourage the generator to follow the style prompts. This style loss is defined as
\begin{align}
    \mathcal{L}_{\eqword{style}} = 1 - \cos(\mathcal{E}_{\eqword{CLIP-M}}(\vect{M}_0), \mathcal{E}_{\eqword{CLIP-M}}(\vect{M}^{*}_0)),
\end{align}
where $\mathcal{E}_{\eqword{CLIP-M}}$ is the pretrained motion encoder, and $\vect{M}^{*}_0=\mathcal{D}_{\eqword{VQ}}(\vect{Z}_0^*)$ is the generated gestures.

We utilize the classifier-free guidance \cite{ho2022classifierfree} to train our model. Specifically, we let $\vect{E}_{\theta}$ learn both the style-conditional and unconditional distributions by randomly setting $\vect{P} = \varnothing$ and thus disabling the AdaIN layer by $10\%$ chance during training. At inference time, the predicted noise is computed using 
\begin{align}
    \vect{E}^*_n&= s \vect{E}_{\theta}(\vect{Z}_n, n, \vect{A}, \vect{T}, \vect{P}) + (1-s)  \vect{E}_{\theta}(\vect{Z}_n, n, \vect{A}, \vect{T}, \varnothing)
\end{align}
instead of \eqn\eqref{eqn:denoising_network}. This scheme further allows us to control the effect of the style prompt $\vect{P}$ by adjusting the scale factor $s$.

\subsubsection{CLIP Video Encoder}
We develop a CLIP-based video encoder $\mathcal{E}_{\eqword{CLIP-V}}$ in Section~\ref{subsec:CLIP-guided-adain} to enable video clips as the style prompts. $\mathcal{E}_{\eqword{CLIP-V}}$ encapsulates a pretrained CLIP image encoder \cite{radford2021clip} whose weights are frozen and a learnable transformer network. To learn $\mathcal{E}_{\eqword{CLIP-V}}$, we render a random motion sequence  $\vect{M}$ into a video and optimize the loss function
\begin{align}
    \label{eqn:clip-video-encoder-loss}
    \mathcal{L}_{\eqword{video}} = 1 - \cos(\eqword{sg}(\mathcal{E}_{\eqword{CLIP-M}}(\vect{M})), \mathcal{E}_{\eqword{CLIP-V}}(\eqword{R}(\vect{M}; \vect{r}))) ,
\end{align}
where $\eqword{sg}$ represents the \emph{stop gradient} operator that prevents the gradient from backpropagating through it, $\eqword{R}$ denotes the rendering operator that renders $\vect{M}$ into a video with camera parameters $\vect{r}$ configured similarly to \cite{aberman2020adain}, and $\mathcal{E}_{\eqword{CLIP-M}}$ is the pretrained motion encoder. This loss function ensures $\mathcal{E}_{\eqword{CLIP-V}}$ to map video clips into the same shared CLIP embedding space.

\subsection{Style Control of Body Parts}
Inspired by \cite{zhang2022motiondiffuse}, we extend our system to allow fine-grained styles control on individual body parts using \emph{noise combination}. Considering a partition $\mathcal{O}$ of the character's body, where each body part $o\in\mathcal{O}$ consists of several joints, we learn $O=|\mathcal{O}|$ individual motion VQ-VAEs to represent the motions of each body part as latent codes $\vect{Z}^{o} = \mathcal{E}_{\eqword{VQ}}^o(\vect{M}^o)$. The full-body motion codes $\vect{Z}^{\mathcal{O}} \in \mathbb{R}^{O \times (L \times C)}$ is then computed by stacking the motion codes of each body part. We then train a new latent diffusion model $\vect{E}_{\theta}$ based on $\vect{Z}^{\mathcal{O}}$ in the same way as introduced in the previous sections. At inference time, we predict full-body noises $\{\vect{E}_{n,o}^*\}_{o\in\mathcal{O}}$ conditioned on a set of style prompts $\{\vect{P}_o\}_{o\in\mathcal{O}}$ for every body part, where each $\vect{E}_{n,o}^*= \vect{E}_{\theta}(\vect{Z}_n^{\mathcal{O}}, n, \vect{A}, \vect{T}, \vect{P}_o)$. These noises can be simply fused as $\vect{E}_n^* = \sum_{o\in\mathcal{O}}\vect{E}_{n,o}^* \cdot M_o$, where $\{M_o\}$ are binary arrays indicating the partition of bodies in $\mathcal{O}$. To achieve better motion quality, we add a smoothness item to the denoising direction as suggested by~\citet{zhang2022motiondiffuse}, which is
\begin{align}
    \vect{E}_n^* = \sum_{o\in\mathcal{O}}\Bigl(\vect{E}_{n,o}^* \cdot M_o\Bigr) + 
    w_{\eqword{body}} \cdot \nabla_{\vect{Z}_n^{\mathcal{O}}}\Bigl(\sum_{i,j \in\mathcal{O}, i\neq{}j }\norm{\vect{E}_{n,i}^* - \vect{E}_{n,j}^*}_2\Bigr),
\end{align}
where $\nabla$ denotes the gradient calculation, $w_{\eqword{body}}$ is set to 0.01.