\section{Introduction}
\label{sec:introduction}
Gestures are the spontaneous and stylized movements of hands and arms that occur while people talk. They energize the speech and reveal the idiosyncratic imagery of thoughts \cite{mcneill1992hand}. Recently, deep neural networks have been successfully applied to synthesize natural-looking gestures based on speech input, which facilitates the creation of human-like 3D avatars. However, the deep learning-based system often suffers from a lack of controllability, making synthesizing arbitrary stylized gestures under user control remain a challenging task. 

Previous neural network systems that achieve style control in gesture creation can be grouped into two categories: label-based and example-based systems. The label-based systems are typically trained on motion data with paired style labels. They allow editing of predefined styles, such as speaker identities \cite{ahuja2020stylegesture,yoon2020trimodalgesture}, emotions \cite{liu2021beatdataset}, and fine-grained styles like specific hand positions \cite{alexanderson2020stylegesture}.  However, the capacity of such systems is limited by the number and granularity of the style labels, while obtaining such style labels is costly. The example-based systems, in contrast, generate gestures of an arbitrary style by imitating an example given as a motion clip \cite{ghorbani2022zeroeggs} or a video \cite{liu2022hierarchicalgesture}. The styles characterized by these examples are often vague and can hardly convey user intent accurately. A user may need to try several times with different example data to get the  desired result.

Recently, the Contrastive-Language-Image-Pretraining (CLIP) model \cite{radford2021clip} successfully learns the connection between natural language and images. It enables several image generation systems \cite{patashnik2021styleclip,gal2022stylegannada,ramesh2022dalle2,rombach2022latentdiffusion} and motion generation systems \cite{tevet2022motionclip,zhang2022motiondiffuse,tevet2022humanmotiondiffusion} to allow users to specify desired content and style in natural language, namely, using \emph{text prompt}s. The core of the CLIP model is a large-scale shared latent space for visual and textual modalities. It learns such a space using contrastive learning, which technique can be adapted to include other modalities, such as human motions \cite{tevet2022motionclip}, into the same space. From another perspective, the CLIP model offers a flexible interface that allows users to describe their requirements accurately using multiple forms of input, such as a \emph{text prompt}, a \emph{motion prompt}, or even a \emph{video prompt}. The CLIP model can extract semantically consistent latent representation of these prompts, which can be used by an powerful generative model, such as diffusion models \cite{ho2020ddpm,song2020improvedscore}, to fulfill the user needs.

In this work, we present GestureDiffuCLIP, a co-speech gesture synthesis system that takes advantage of the CLIP latents to enable automatic creation of stylized gestures based on various style prompts. Our system learns a latent diffusion model \cite{rombach2022latentdiffusion} as the gesture generator and incorporates the CLIP representation of style into it via an adaptive instance normalization (AdaIN) \cite{huang2017adain} mechanism. The system accepts text, motion, and video prompts as style descriptors and create high-quality, realistic, semantically correct co-speech gestures. It can be further extended to allow fine-grained style control of individual body parts. 

Predicting gestures from utterances is inherently an many-to-many mapping problem. A variety of gestures can correspond to the same speech while semantic gestures and their corresponding speech are usually not perfectly aligned temporally. Such ambiguities can cause an end-to-end deep learning system to learn a \emph{mean} gesture motion and lose the semantic correspondence \cite{ao2022rhythmicgesticulator,abzaliev2022semanticclip,kucherenko2021speech2properties2gestures}. To alleviate this problem, we learn a gesture-transcript joint embedding space using contrastive learning combined with a temporal aggregation mechanism. This joint embedding space provides semantic cues for the generator and a semantic loss that effectively guides the system to learn the semantic correspondence between gestures and speech.

It is difficult to collect a large-scale gesture dataset covering a diverse range of styles and annotating rich and fine-grained labels. To sidestep the need for such data, we developed a self-supervised learning scheme to distill knowledge from the pretrained CLIP models. Specifically, we treat each gesture motion as its own style prompt and let the system to reconstruct the motion based on the CLIP latents extracted by the pretrained motion encoder. Even though the system never see other forms of style prompts during training, it still creates satisfactory styles corresponding to arbitrary text or video prompts in a zero-shot manner.

In summary, the principal contributions of this work include:
\begin{itemize}
    \item We present a novel CLIP-guided prompt-conditioned co-speech gesture synthesis system that generates realistic, stylized gestures. To the best of our knowledge, it is the first system that supports using multimodal prompts to control the style in cross-modality motion synthesis. 
    
    \item We demonstrate a successful adaptation of latent diffusion models to allow high-quality motion synthesis and propose an efficient network architecture based on transformer and AdaIN layers to incorporates style guidance into the diffusion model.
    
    \item We propose a contrastive learning strategy to learn the semantic correspondence between gestures and transcripts. The learned joint embeddings enable synthesizing gestures with convincing semantics.
    
    \item We develop a self-supervised training mechanism that effectively distills knowledge from a large-scale multi-modality pretrained model and releases the need for training data with detailed labels.
\end{itemize}