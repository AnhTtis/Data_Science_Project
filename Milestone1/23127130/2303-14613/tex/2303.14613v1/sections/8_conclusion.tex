\section{Conclusion}
\label{sec:conclusion}
In this paper, we have presented GestureDiffuCLIP, a CLIP-guided co-speech gesture synthesis system that generates stylized gestures based on arbitrary style prompts while ensuring semantic and rhythmic harmony with speech. We leverage powerful CLIP-based encoders to extract style embeddings from style prompts and incorporate them into a diffusion model-based generator through an AdaIN layer. This architecture effectively guides the style of the generated gestures. The CLIP latents make our system highly flexible, supporting short texts, motion sequences, and video clips as style prompts. We also develop a semantics-aware mechanism to ensure semantic consistency between speech and generated gestures, where a joint embedding space is learned between gestures and speech transcripts using contrastive learning. Our system can be extended to achieve style control of individual body parts through noise combination. We conduct an extensive set of experiments to evaluate our framework. Our system outperforms all baselines both qualitatively and quantitatively, as evidenced by FGD, SC, and SRA metrics, and user study results. Regarding application, we demonstrate that our system can effectively enhance co-speech gestures by specifying style prompts for each speech sentence and using these prompts to guide the character's performance. We can further automate this process by employing a large language model like ChatGPT. Finally, we carry out related ablation studies to analyze the design of our system.

There are several potential directions for future exploration:
(a) Our system is based on a latent diffusion model, which effectively ensures motion quality but requires a large number of diffusion steps during inference, making it challenging to synthesize motions in real-time. Acceleration techniques, such as PNDM \cite{liu2022pseudo}, could be considered in the future to optimize the sampling efficiency. (b) Our system achieves zero-shot style control using unseen style prompts and generalizes to some unseen voices, such as the one synthesized by the Text-to-Speech tool used in \fig\ref{fig:application_text_generation}. However, as a common limitation of deep learning-based approaches, the capacity and robustness of our system are limited by the training data. Audio input with acoustic features significantly different from the training dataset can lead to unsatisfactory results, where the generated motion may fail to follow the rhythm of the speech or match the speech content. Improving the system's generalizability to arbitrary audio is a practical problem worth further exploration. (c) The generated motion of our system contains slight foot sliding, which is a common problem in kinematically-based methods \cite{holden2017pfnn,tevet2022humanmotiondiffusion}. This can be alleviated via IK-based post-processing, but unnatural transitions of contact status may occur after the post-process \cite{li2022ganimator}. Building a physically-based co-speech gesture generation system could essentially solve the above problem. (d) We learn the gesture-transcript joint embedding space using a CLIP-style contrastive learning framework, which has been shown to be effective in extracting semantic information from both modalities and enables applications such as motion-based text retrieval and word saliency identification. CLIP-style models are usually scalable and potentially more powerful when trained on larger datasets \cite{radford2021clip}. Exploring such potential would be interesting future research.