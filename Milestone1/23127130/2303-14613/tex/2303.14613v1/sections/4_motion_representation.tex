\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/gesture-transcript_embedding_learning.pdf}
    \caption{We learn an gesture-transcript joint embedding space using contrastive learning. A transcript encoder is trained to convert a transcript sentence $\vect{T}$ into a sequence of feature codes $\vect{Z}^t$, which are then aggregated into a transcript embedding vector $\vect{z}^t$ via max pooling. Similarly, the corresponding gesture sequence $\hatvect{Z}$ is processed by a gesture encoder, resulting in a feature sequence $\vect{Z}^g$ and the corresponding embedding $\vect{z}^g$. The encoders are trained using a contrastive loss that maximizes the similarity between the embeddings $\vect{z}^t$ and $\vect{z}^g$ of paired transcripts and gestures.}
    \Description{}
    \label{fig:gesture-transcript_embedding_learning}
\end{figure*}

\section{Motion Representation}
\label{subsec:motion_representation}
A gesture motion $\vect{M}=[\vect{m}_k]_{k=1}^{K}$ is a sequence of poses, where $K$ denotes the length of the motion. Each pose $\vect{m}_k\in\mathbb{R}^{3+6J}$ consists of the displacement of the character and the rotations of its $J$ joints. We parameterize the rotations as 6D vectors \cite{zhou20196dvector}, though other rotation representations can possibly be used instead. This raw motion representation, however, often contains redundant information. Following recent successful systems \cite{ao2022rhythmicgesticulator,rombach2022latentdiffusion,dhariwal2020jukebox}, we learn a compact motion representation using VQ-VAE \cite{van2017vqvae} to ensure motion quality and diversity.

Specifically, we train VQ-VAE as an encoder-decoder pair
\begin{align}
    \vect{Z} = \mathcal{E}_{\eqword{VQ}}(\vect{M})  \quad \Leftrightarrow \quad \vect{M} = \mathcal{D}_{\eqword{VQ}}(\vect{Z}) .
\end{align}
The encoder $\mathcal{E}_{\eqword{VQ}}$ converts $\vect{M}$ into a downsampled sequence of latent codes $\vect{Z}=[\vect{z}_l]_{l=1}^{L}$, where $\vect{z}_l\in\mathbb{R}^{C}$ and $C$ is the dimension of the latent space. We refer to the ratio $d=K/L$ as the encoder's downsampling rate, which is determined by the network structure.  The decoder $\mathcal{D}_{\eqword{VQ}}$ operates on a quantized version of the latent space. It maintains a \emph{codebook} consisting of $N_{\eqword{VQ}}$ latent vectors. When reconstructing the original motion $\vect{M}$ from $\vect{Z}$, the decoder maps each $\vect{z}_k$ to its nearest codebook vector $\hatvect{z}_l$ and decodes the quantized latent sequence $\hatvect{Z}=[\hatvect{z}_l]_{l=1}^{L}$ into $\vect{M}$.

Our VQ-VAE model has a network structure similar to that of Jukebox \cite{dhariwal2020jukebox}, which consists of a cascade of 1D convolutional networks. The encoder $\mathcal{E}_{\eqword{VQ}}$ and decoder $\mathcal{D}_{\eqword{VQ}}$ are learned following the standard VQ-VAE training process \cite{van2017vqvae,dhariwal2020jukebox}. They are then frozen in the rest of training. 
Both the latent sequence $\vect{Z}$ and its quantized version $\hatvect{Z}$ are used as the motion representation by the other components of the system. Specifically, we learn the gesture-transcript joint embeddings on the quantized latent sequence $\hatvect{Z}$ in Section~\ref{sec:gesture-transcript_embedding_learning}, while the latent diffusion model synthesizes gesture motions as $\vect{Z}$ in Section~\ref{sec:stylized_co-speech_gesture_diffusion_model}.