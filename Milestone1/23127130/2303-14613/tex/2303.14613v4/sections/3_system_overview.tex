\section{System Overview}
\label{sec:system_overview}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/system_overview.pdf}
    \caption{Our system consists of two core components: (a) a latent diffusion model that takes speech audio and transcript as input and generate co-speech gestures, and (b) a CLIP-based encoder that extracts style embeddings from an arbitrary style prompt and incorporates them into the diffusion model via an adaptive instance normalization (AdaIN) layer. The system allows using short texts, video clips, and motion sequences to define gesture styles by encoding them into the same CLIP embedding space using corresponding pretrained encoders.
    }
    \Description{}
    \label{fig:system_overview}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/gesture-transcript_embedding_learning.pdf}
    \caption{
        We learn an gesture-transcript joint embedding space using contrastive learning. A transcript encoder is trained to convert a transcript sentence $\vect{T}$ into a sequence of feature codes $\vect{Z}^t$, which are then aggregated into a transcript embedding vector $\vect{z}^t$ via max pooling. Similarly, the corresponding gesture sequence $\hatvect{Z}$ is processed by a gesture encoder, resulting in a feature sequence $\vect{Z}^g$ and the corresponding embedding $\vect{z}^g$. The encoders are trained using a contrastive loss that maximizes the similarity between the embeddings $\vect{z}^t$ and $\vect{z}^g$ of paired transcripts and gestures.
        }
    \Description{}
    \label{fig:gesture-transcript_embedding_learning}
\end{figure*}

Our system takes the audio and transcript of a speech as input, synthesizing realistic, stylized full-body gestures that align with the speech content rhythmically and semantically. It allows using a short piece of text, namely a \emph{text prompt}, a video clip, namely a \emph{video prompt}, or a motion sequence, namely a \emph{motion prompt}, to describe a desired style. The gestures are then generated to embody the style as much as possible.

We build the system based on latent diffusion models \cite{rombach2022latentdiffusion}, which apply diffusion and denoising steps in a pretrained latent space. We learn this latent motion space using VQ-VAE \cite{van2017vqvae}, providing compact motion embeddings that ensure motion quality and diversity. As illustrated in \fig\ref{fig:system_overview}, our system is composed of two major components: (a) an end-to-end neural generator that accepts speech audio and text transcript as input and generates speech-matched gesture sequences using latent diffusion models; and (b) a CLIP-based encoder that extracts style embeddings from style prompts and integrates them into the diffusion models via an adaptive instance normalization (AdaIN) layer \cite{huang2017adain} to guide the style of the generated gestures. Furthermore, we learn a joint embedding space between corresponding gestures and transcripts using contrastive learning, which provides useful semantic cues for the generator and a semantic loss that effectively directs the generator to learn semantically meaningful gestures during training.

The system employs classifier-free diffusion guidance \cite{ho2022classifierfree} alongside with a self-supervised learning scheme,  enabling training on motion data without style labels. In the following sections, we will elaborate on the components and their training process within our system.