\section{Conclusion}
\label{sec:conclusion}
In this paper, we have presented GestureDiffuCLIP, a CLIP-guided co-speech gesture synthesis system that generates stylized gestures based on arbitrary style prompts while ensuring semantic and rhythmic harmony with speech. We leverage powerful CLIP-based encoders to extract style embeddings from style prompts and incorporate them into a diffusion model-based generator through an AdaIN layer. This architecture effectively guides the style of the generated gestures. The CLIP latents make our system highly flexible, supporting short texts, motion sequences, and video clips as style prompts. We also develop a semantics-aware mechanism to ensure semantic consistency between speech and generated gestures, where a joint embedding space is learned between gestures and speech transcripts using contrastive learning. Our system can be extended to achieve style control of individual body parts through noise combination. We conduct an extensive set of experiments to evaluate our framework. Our system outperforms all baselines both qualitatively and quantitatively, as evidenced by FGD, SRGR, SC, and SRA metrics, and user study results. Regarding application, we demonstrate that our system can effectively enhance co-speech gestures by specifying style prompts for each speech sentence and using these prompts to guide the character's performance. We can further automate this process by employing a large language model like ChatGPT, enabling a
skillful storyteller.
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/result_video_prompt.pdf}
    \caption{Gestures synthesized by our system conditioned on video prompts: Yoga poses \cite{adriene2021yoga1video,adriene2021yoga2video}, flying bird \cite{wildlife2019birdvideo}, dinosaur \cite{dinosaur2018dinosaurvideo}, fire \cite{fire2016firevideo}, and lightning \cite{manabouttown12021lightning}.}
    \Description{}
    \label{fig:video_prompt_result}
\end{figure*}

In our system, styles refer to the overall appearance of motion or, more precisely, the aspects of gestures that are independent of input speech content. This encompasses both stylistic features and motion constraints. Our system processes these two types of style within a unified framework, potentially enabling users to define them using a combined prompt, such as \emph{the person is happy and holds a cup of coffee in their right hand} (stylistic feature + constraints), when a powerful language model is available. Another interesting extension to our current framework could be to allow users to combine different forms of prompts for a more accurate description of a style, such as specifying a stylistic feature with a text prompt and defining motion details using a video or motion prompt. This presents a compelling direction for future work.

Our system achieves zero-shot style control using unseen style prompts and generalizes to some unseen voices, such as the one synthesized by the Text-to-Speech tool used in \fig\ref{fig:application_text_generation}. However, as is common with deep learning-based approaches, the capacity and robustness of our system are constrained by the training data and the network architecture. For instance, due to the limitations of the vanilla CLIP text encoders, our system cannot accept excessively long text prompts, and users may find that some descriptions are not interpreted accurately. When employing motion clips or human videos as style prompts, our system consistently endeavors to generate gestures with similar poses to those displayed in the prompt. Nevertheless, as demonstrated in Figure \ref{fig:video_prompt_result}, poses that diverge significantly from the dataset, such as many yoga poses, may not be accurately reproduced. Furthermore, since non-human videos typically lack a lucid motion semantic interpretation, the semantic connection between the video content and the generated motion style can be ambiguous. As illustrated in Figure \ref{fig:video_prompt_result}, our video encoder extracts arm poses from the bird video and the dinosaur video but interprets the lightning video as the \emph{raise both hands} prompt, possibly due to the shape of the lightning and the clouds. Lastly, audio inputs with acoustic features that differ substantially from the training dataset can result in unsatisfactory outcomes, where the generated motion may fail to adhere to the speech rhythm or correspond with the speech content. Enhancing the system's generalizability and interpretability remains a practical challenge for further exploration.

We learn the gesture-transcript joint embedding space using a CLIP-style contrastive learning framework, which has been shown to be effective in extracting semantic information from both modalities and enabling applications such as motion-based text retrieval and word saliency identification. CLIP-style models typically scale well and have the potential for increased power when trained on larger datasets \cite{radford2021clip}. Investigating this potential presents an intriguing avenue for future research.

The generated motion of our system exhibits slight foot sliding, which is a common problem in kinematically-based methods \cite{holden2017pfnn,tevet2022humanmotiondiffusion}. This can be alleviated via IK-based post-processing, but unnatural transitions in contact status may arise after post-processing \cite{li2022ganimator}. Developing a physically-based co-speech gesture generation system could fundamentally address this problem.

Finally, our system is based on a latent diffusion model, which effectively ensures motion quality but requires a large number of diffusion steps during inference, making it challenging to synthesize motions in real-time. Acceleration techniques, such as PNDM \cite{liu2022pseudo}, could be considered in the future to optimize the sampling efficiency.