\section{Motion Representation}
\label{subsec:motion_representation}
A gesture motion $\vect{M}=[\vect{m}_k]_{k=1}^{K}$ is a sequence of poses, where $K$ denotes the motion length. Each pose $\vect{m}_k\in\mathbb{R}^{3+6J}$ consists of the displacement of the character and the rotations of its $J$ joints. We parameterize the rotations as 6D vectors \cite{zhou20196dvector}, though alternative rotation representations can potentially be used instead. The raw motion representation, however, often contains redundant information. To ensure motion quality and diversity while enabling fast inference, we follow recent successful systems \cite{ao2022rhythmicgesticulator,rombach2022latentdiffusion,dhariwal2020jukebox} and learn a compact motion representation using VQ-VAE \cite{van2017vqvae}.

Specifically, we train VQ-VAE as an encoder-decoder pair
\begin{align}
    \vect{Z} = \mathcal{E}_{\eqword{VQ}}(\vect{M})  \quad \Leftrightarrow \quad \vect{M} = \mathcal{D}_{\eqword{VQ}}(\vect{Z}) .
\end{align}
The encoder $\mathcal{E}_{\eqword{VQ}}$ converts $\vect{M}$ into a downsampled sequence of latent codes $\vect{Z}=[\vect{z}_l]_{l=1}^{L}$, where $\vect{z}_l\in\mathbb{R}^{C}$ and $C$ is the dimension of the latent space. We refer to the ratio $d=K/L$ as the encoder's downsampling rate, which is determined by the network structure. The decoder $\mathcal{D}_{\eqword{VQ}}$ operates on a quantized version of the latent space. It maintains a \emph{codebook} consisting of $N_{\eqword{VQ}}$ latent vectors. When reconstructing the original motion $\vect{M}$ from $\vect{Z}$, the decoder maps each $\vect{z}_k$ to its nearest codebook vector $\hatvect{z}_l$ and decodes the quantized latent sequence $\hatvect{Z}=[\hatvect{z}_l]_{l=1}^{L}$ into $\vect{M}$.

Our VQ-VAE model has a network structure similar to that of Jukebox \cite{dhariwal2020jukebox}, which consists of a cascade of 1D convolutional networks. The encoder $\mathcal{E}_{\eqword{VQ}}$ and decoder $\mathcal{D}_{\eqword{VQ}}$ are learned following the standard VQ-VAE training process \cite{van2017vqvae,dhariwal2020jukebox}. They are then frozen in the rest of training. Both the latent sequence $\vect{Z}$ and its quantized version $\hatvect{Z}$ are used as the motion representation by the other components of the system. Specifically, we learn the gesture-transcript joint embeddings on the quantized latent sequence $\hatvect{Z}$ in Section~\ref{sec:gesture-transcript_embedding_learning}, while the latent diffusion model synthesizes gesture motions as $\vect{Z}$ in Section~\ref{sec:stylized_co-speech_gesture_diffusion_model}.