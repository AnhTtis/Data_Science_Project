\section{Gesture-Transcript Joint Embeddings}
\label{sec:gesture-transcript_embedding_learning}

The many-to-many mapping between speech content and gestures poses challenges in generating semantically correct motions. To alleviate this problem, we learn a joint embedding space for gestures and speech transcripts, enabling the discovery of semantic connections between the two modalities.

\subsection{Architecture}
As shown in \fig\ref{fig:gesture-transcript_embedding_learning}, we train two encoders, a gesture encoder $\mathcal{E}_G$ and a transcript encoder $\mathcal{E}_T$, to map the gesture motion and speech transcripts into the shared embedding space respectively. Both the encoders process the input speech in sentences. The speech transcripts are tokenized using the {T5 tokenizer \cite{xue2021mt5}} and temporally associated with the audio using the {Montreal Forced Aligner (MFA) \cite{mcauliffe2017mfa}}. This procedure also aligns the transcripts with the gestures. The speech data is subsequently segmented into sentences based on the transcripts. Following this, we compute:
\begin{align}
    \vect{Z}^t=\mathcal{E}_T(\vect{T}) , \quad  \quad \vect{Z}^g=\mathcal{E}_G(\hatvect{Z}) ,
\end{align}
where $\vect{T}\in \mathcal{W}^{L_t}$ denotes a tokenized transcript sentence parameterized as a sequence of word embeddings $w\in\mathcal{W}$, $\hatvect{Z}\in \mathbb{R}^{L_g \times C}$ is the quantized latent representation of the corresponding gesture sequence. The output of the encoders, $\vect{Z}^t\in\mathbb{R}^{L_t \times C_{{s}}}$ and $\vect{Z}^g\in\mathbb{R}^{L_g \times C_{{s}}}$, are sequences of feature vectors of the same dimension $C_{{s}}$. Note that the lengths of these sequences, $L_t$ and $L_g$, can be different.

In a speech, a semantic gesture and the utterance of its corresponding word or phrase often lack perfect alignment \cite{liang2022seeg}. This misalignment can confuse the encoder if the temporal correspondence between the two modalities is rigidly enforced during training. To alleviate this issue, we aggregate semantics-relevant information in each feature sequence via max pooling
\begin{align}
    \vect{z}^t = \eqword{max\_pooling}(\vect{Z}^t)  , \quad 
    \vect{z}^g = \eqword{max\_pooling}(\vect{Z}^g) .
\end{align}
Then $\vect{z}^t,\vect{z}^g \in \mathbb{R}^{C_{{s}}}$ are considered the embeddings of the transcripts and gestures, respectively. 

We employ a powerful pretrained language model, T5-base \cite{xue2021mt5}, as the text encoder $\mathcal{E}_T$. The motion encoder $\mathcal{E}_G$ is a 12-layer, 768-feature wide, encoder-only transformer with 12 attention heads, pretrained on the gesture dataset by predicting masked motions in a way similar to BERT \cite{devlin2019bert}. Both encoders are subsequently fine-tuned using contrastive learning, as detailed below.

\subsection{Contrastive Learning}
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/contrastive_loss.pdf}
    \caption{An illustration of the CLIP-style contrastive loss used to train the gesture and transcript encoders.}
    \Description{}
    \label{fig:contrastive_loss}
\end{figure}
%
We apply CLIP-style contrastive learning \cite{radford2021clip} to fine-tune the encoders. Given a batch of pairs of gesture and transcript embeddings $\mathcal{B}=\{(\vect{z}^t_i,\vect{z}^g_i)\}_{i=1}^{B}$, where $B$ is the batch size, the goal of the training is to maximize the similarity of the embeddings $(\vect{z}^t_i,\vect{z}^g_i)$ of the real pairs in the batch while minimizing the similarity of the incorrect pairs $(\vect{z}^t_i, \vect{z}^g_j)_{i\neq{}j}$. As illustrated in \fig\ref{fig:contrastive_loss}, this learning objective can be expressed as the sum of the gesture-to-text ($\text{g2t}$) cross entropy and the text-to-gesture ($\text{t2g}$) cross entropy computed across the batch. Formally, the loss function is
\begin{align}
    \mathcal{L}_{\eqword{contrast}} = \mathbb{E}_{\mathcal{B}\sim\mathcal{D}}&\Bigl[
        \mathcal{H}_{\mathcal{B}}\left(\vect{y}^{\eqword{g2t}}(\vect{z}^g_i), \vect{p}^{\eqword{g2t}}(\vect{z}^g_i)\right) 
        \nonumber \\ 
    &+ \mathcal{H}_{\mathcal{B}}\left(\vect{y}^{\eqword{t2g}}(\vect{z}^t_j), \vect{p}^{\eqword{t2g}}(\vect{z}^t_j)\right)\Bigr].
\end{align}
Each cross entropy $\mathcal{H}$ is computed between a one-hot encoding $\vect{y}$ and a softmax-normalized distribution $\vect{p}$. $\vect{y}$ specifies the true correspondence between the gestures and transcripts in the training batch $\mathcal{B}$. $\vect{p}$ computes the similarity between an embedding of one modality and those of the other modality. Specifically,
\begin{align}
    \label{eqn:multimodal_similarity}
    \vect{p}^{\eqword{g2t}}(\vect{z}^g_i) = \frac{\exp(\vect{z}^g_i \cdot \vect{z}^t_i / \tau)}{\sum^B_{j=1}\exp(\vect{z}^g_i \cdot \vect{z}^t_j / \tau)} 
    ,\quad
    \vect{p}^{\eqword{t2g}}(\vect{z}^t_j) = \frac{\exp(\vect{z}^t_j \cdot \vect{z}^g_j / \tau)}{\sum^B_{i=1}\exp(\vect{z}^t_j \cdot \vect{z}^g_i / \tau)},
\end{align}
where $\tau$ is the temperature of softmax.

In real data, there is often no corresponding gesture for an utterance, and a semantic feature may correspond to several different gestures. Such noisy correspondence could cause instability in contrastive learning. We employ the \emph{momentum distillation} (MoD) \cite{li2021albef} technique to alleviate this problem. The key idea of MoD is to learn from the pseudo-targets generated by a momentum model. During training, we maintain a momentum version of the encoders by updating their network parameters in an exponential-moving-average (EMA) manner. Then, we use the momentum models to calculate multimodal features $\tildevect{z}^t$ and $\tildevect{z}^g$ for the training gesture-transcript pairs and compute the pseudo-targets $\tildevect{p}^{\eqword{g2t}}$ and $\tildevect{p}^{\eqword{t2g}}$ by substituting these features into \eqn\eqref{eqn:multimodal_similarity}. The contrastive loss is then modified as
\begin{align}
    \mathcal{L}_{\eqword{contrast}}^{\eqword{MoD}} &= (1 - w_{\eqword{contrast}})\mathcal{L}_{\eqword{contrast}} \nonumber \\
    &+ w_{\eqword{contrast}}\mathbb{E}_{\mathcal{B} \sim \mathcal{D}}\Bigl[D_{KL}\left(\tildevect{p}^{\eqword{g2t}}(\tildevect{z}^g_i) || \vect{p}^{\eqword{g2t}}(\vect{z}^g_i)\right) \nonumber \\
    &+ D_{KL}\left(\tildevect{p}^{\eqword{t2g}}(\tildevect{z}^t_j) || \vect{p}^{\eqword{t2g}}(\vect{z}^t_j)\right)\Bigr],
\end{align}
where $D_{KL}(\cdot || \cdot)$ is the KL divergence and $w_{\eqword{contrast}}$ is set to $0.4$.

\subsection{Applications of the Joint Embeddings}
%
\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/motion-based_transcripts_retrieval.pdf}
        \caption{Transcripts retrieved based on example gestures. Note that a gesture can natually accompany several semantics.}
        \label{fig:motion-based_transcripts_retrieval}
    \end{subfigure} \\
    \vspace{5pt}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/semantic_saliency.pdf}
        \caption{Semantic saliency curves of two sentences. The peaks of the curves indicate the words with high semantic importance which are likely to be accompanied by semantic gestures.}
        \label{fig:semantic_saliency}
    \end{subfigure}
    \caption{Applications of the gesture-transcript joint embeddings. (a) Motion-based transcripts retrieval . (b) Semantic saliency identification.}
    \label{fig:applications_of_gesture-transcript_embedding}
    \Description{}
\end{figure}
%
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/denoising_network.pdf}
    \caption{Architecture of the denoising network. The model is a multi-layer transformer with a causal attention structure. It takes the audio and transcript of a speech, along with a style prompt, as input and estimates the diffusion noise. Three CLIP-based encoders are learned to support different types of style prompts. The multimodal features are integrated  into the network at various stages through semantics-aware layers and AdaIN layers, respectively.
    \emph{Norm} refers to the layer normalization and \emph{FFN} is the feed-forward network.}
    \Description{}
    \label{fig:denoising_network}
\end{figure*}
%
The joint embedding space, along with the encoders, provides an efficient method for measuring semantic similarity between gestures and transcripts. To demonstrate its effectiveness, we map a gesture motion into this space and retrieve the closest sentences from the transcript dataset based on the cosine distance of the embeddings. \fig\ref{fig:motion-based_transcripts_retrieval} shows some results. It can be observed that the retrieved sentences may have various meanings, but can all be naturally paired with the query gestures.

Besides, the computation of the embeddings involves the max pooling operator, which aggregates the most semantics-relevant information. Consequently, we can estimate the saliency of each pose or word in a gesture sequence or a transcript sentence, respectively, using the embeddings. Specifically, given a sentence $\vect{T}$ along with its encoded feature sequence $\vect{Z}^t$ and embedding vector $\vect{z}^t$, we compute the semantic saliency of each word as
\begin{align}
    \label{eqn:semantic-saliency}
    \vect{s}^t = \eqword{softmax}\left(\vect{Z}^t \cdot \vect{z}^t\right).
\end{align}
As illustrated in \fig\ref{fig:semantic_saliency}, the words with high semantic importance that are likely to be accompanied by semantic gestures will exhibit high saliency scores. This information can be considered as an important semantic cue, which will be used in our system to guide the gesture generator  in creating semantically correct gestures.