\section{Related Work}
\label{sec:related_work}
\subsection{Co-Speech Gesture Synthesis}
The early approaches for generating co-speech gestures often involve creating linguistic rules to translate speech input into a sequence of pre-collected gesture segments, which are typically referred to as rule-based methods \cite{cassell1994rulefullbody,cassell2001beat,kipp2004gesture,kopp2006bml}. \citet{wagner2014rulereview} provide a comprehensive review of these methods. Rule-based methods produce interpretable and controllable results, but creating gesture datasets and rules requires significant effort. To alleviate the manual effort of designing rules in rule-based methods, data-driven approaches have gradually become predominant in this field. \citet{nyatsanga2023data_driven_gesture_survey} offer a thorough survey of these methods. Early data-driven approaches aim to directly learn mapping rules from data through statistical models \cite{neff2008videogesture,levine2009prosodygesture,levine2010gesturecontroller} and combine them with predefined gesture units for gesture generation. Later, the powerful modeling capability of deep neural networks makes it possible to train complex end-to-end models using raw speech-gesture data directly. One option is deterministic models, such as MLP \cite{kucherenko2020gesticulator}, CNN \cite{habibie2021videogesture}, RNN \cite{yoon2019robot,yoon2020trimodalgesture,bhattacharya2021affectivegesture,liu2022hierarchicalgesture}, and Transformer \cite{bhattacharya2021text2gestures}. Another choice is generative models, including flow-based models \cite{alexanderson2020stylegesture,ye2022styleflowgesture}, VAEs \cite{li2021audio2gesture,ghorbani2022zeroeggs}, and VQ-VAE \cite{yi2022talkshow,yazdian2022gesture2vec,liu2022vqgesturevideo}. Due to the inherent many-to-many relationship between speech and gesture, end-to-end models can generate natural-looking gestures but face challenges in ensuring content matching between speech and generated gestures \cite{yoon2022genea}. To address this issue, some neural systems aim to explicitly model both rhythm and semantics from the perspective of model structure \cite{kucherenko2021speech2properties2gestures,ao2022rhythmicgesticulator,liu2022disco} or training supervision strategy \cite{liang2022seeg}. Furthermore, hybrid systems, such as the combination of deep features and motion graphs \cite{zhou2022gesturemaster}, have been proposed to harness the advantages of different approaches. Recently, diffusion models \cite{sohldickstein2015diffusion,song2020improvedscore,ho2020ddpm} have demonstrated impressive results in image synthesis \cite{ramesh2022dalle2} and human motion generation \cite{tevet2022humanmotiondiffusion, zhang2022motiondiffuse}. Inspired by these works, our system adapts the latent diffusion model \cite{rombach2022latentdiffusion} for the co-speech gesture generation task and achieves appealing results.

\subsection{Style Control for Human Motion}
A typical approach to style control for human motion involves specifying a motion clip as a reference and transferring the reference clip's style to the source motion. This task is also known as \emph{style transfer}. Early works in motion style transfer integrate traditional machine learning techniques with manually defined features to infer motion styles \cite{hsu2005motion_style_translation,ma2010motion_style_transfer,xia2015realtime_motion_style_transfer,yumer2016spectral_motion_style_transfer}. Recently, deep learning-based methods have significantly enhanced motion quality. \citet{holden2016deepmotion} first propose a learning framework enabling motion style control through optimization in the motion manifold space. \citet{du2019stylemotioncvae} improve transfer efficiency by training a conditional VAE. \citet{mason2018few-shot_motion_style_transfer} use few-shot learning to generate stylized locomotion. \citet{aberman2020adain} employ a temporally invariant adaptive instance normalization (AdaIN) layer for target style injection, eliminating the need for paired data during training. \citet{wen2021stylemotionflow} achieve unsupervised style transfer using a flow model. \citet{jang2022motionpuzzle} introduce a method capable of controlling styles for individual body parts.

Previous co-speech gesture synthesis systems with style control can be categorized based on whether or not they require style labels. For methods needing labeled data, early works can only learn an individual style for one generator \cite{levine2010gesturecontroller,neff2008videogesture,ginosar2019stylegesture}. \citet{ahuja2022lowresource} propose a strategy that efficiently adapts the source generator to another speaker style using low-resource data. Some works learn a speaker style embedding space with labeled speaker-motion data, enabling gesture style control by sampling from this space \cite{ahuja2020stylegesture,yoon2020trimodalgesture,bhattacharya2021affectivegesture}. \citet{alexanderson2020stylegesture} aimat controlling fine-grained styles, such as gesturing speed and spatial scope, using preprocessed control signal-motion data. Their later work \cite{alexanderson2022diffusiongesture} utilizes a diffusion model for audio-driven motion synthesis, achieving label-based style control by training the model on labeled data. For methods not requiring style labels, \citet{habibie2022motionmatching} propose a motion matching framework to achieve flexible style control. Other studies achieve arbitrary style control by imitating an example given as a video \cite{liu2022hierarchicalgesture} or a motion clip \cite{ghorbani2022zeroeggs,ye2022styleflowgesture,kuriyama2022tokenizedgestures}.  In this work, we utilize a CLIP-based encoder to extract a style embedding from an arbitrary text prompt and incorporate it into the generator via an AdaIN layer, guiding the synthesis of stylized gestures. Our system supports fine-grained multimodal style prompts as opposed to label-based style control. It employs a self-supervised learning scheme and eliminates the need for labeled data. Additionally, we use an autoregressive model rather than a parallel model, making it potentially suitable for real-time applications.