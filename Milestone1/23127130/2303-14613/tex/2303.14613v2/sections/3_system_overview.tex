\section{System Overview}
\label{sec:system_overview}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/system_overview.pdf}
    \caption{Our system consists of two core components:
    (a) a latent diffusion model that takes speech audio and transcript as input and generate co-speech gestures, and (b) a CLIP-based encoder that extracts style embeddings from an arbitrary style prompt and incorporates them into the diffusion model via an adaptive instance normalization (AdaIN) layer. The system allows using short texts, video clips, and motion sequences to define gesture styles by encoding these style prompts to the same CLIP embedding space using corresponding pretrained encoders.}
    \Description{}
    \label{fig:system_overview}
\end{figure}

Our system takes the audio and transcript of a speech as input and synthesizes realistic stylized full-body gestures that match the speech content both rhythmically and semantically. It allows using a short piece of text, namely a \emph{text prompt}, a video clip, namely a \emph{video prompt}, or a motion sequence, namely a \emph{motion prompt}, to describe a desired style. The gestures are then generated to produce the style as much as possible.

We build the system based on the latent diffusion models \cite{rombach2022latentdiffusion}, which apply diffusion and denoising steps in a pretrained latent space. We learn this latent motion space using VQ-VAE \cite{van2017vqvae}, which offers compact motion embeddings ensuring quality and diversity of motion. As illustrated in \fig\ref{fig:system_overview}, our system is composed of two major components: (a) an end-to-end neural generator that takes speech audio and text transcript as input and generates speech-matched gesture sequences using latent diffusion models; and (b) a CLIP-based encoder that extracts style embeddings from the style prompts and incorporates them into the diffusion models via an adaptive instance normalization (AdaIN) layer \cite{huang2017adain} to guide the style of the generated gestures. We further learn a joint embedding space between corresponding gestures and transcripts using contrastive learning, which provides useful semantic cues for the generator and a semantic loss that effectively guides the generator to learn semantically meaningful gestures during training.

The system is trained with the classifier-free diffusion guidance \cite{ho2022classifierfree}, combined with a self-supervised learning scheme to enable training on motion data without style labels. In the following sections, we will provide details about the components and how they are trained in our system.