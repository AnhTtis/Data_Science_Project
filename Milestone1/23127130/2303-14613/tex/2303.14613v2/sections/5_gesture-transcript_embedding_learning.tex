\section{Gesture-Transcript Joint Embeddings}
\label{sec:gesture-transcript_embedding_learning}
The many-to-many mapping between speech content and gestures brings challenges to generating semantically correct motions. To alleviate this problem, we learn a joint embedding space of gestures and speech transcripts to mine for the semantic connections between the two modalities.

\subsection{Architecture}
As shown in \fig\ref{fig:gesture-transcript_embedding_learning}, we train two encoders, a gesture encoder $\mathcal{E}_G$ and a transcript encoder $\mathcal{E}_T$, to map the gesture motion and speech transcripts into the shared embedding space respectively. Both the encoders process the input speech in sentences. The speech transcripts are tokenized using the {T5 tokenizer \cite{xue2021mt5}} and temporally associated with the audio using the {Montreal Forced Aligner (MFA) \cite{mcauliffe2017mfa}}. This procedure also aligns the transcripts with the gestures. The speech data is then segmented into sentences based on the transcripts. Then we compute
\begin{align}
    \vect{Z}^t=\mathcal{E}_T(\vect{T}) , \quad  \quad \vect{Z}^g=\mathcal{E}_G(\hatvect{Z}) ,
\end{align}
where $\vect{T}\in \mathcal{W}^{L_t}$ denotes a tokenized transcript sentence, parameterized as a sequence of word embeddings $w\in\mathcal{W}$, and $\hatvect{Z}\in \mathbb{R}^{L_g \times C}$ is the quantized latent representation of the corresponding gesture sequence. The output of the encoders, $\vect{Z}^t\in\mathbb{R}^{L_t \times C_{{s}}}$ and $\vect{Z}^g\in\mathbb{R}^{L_g \times C_{{s}}}$, are sequences of feature vectors of the same dimension $C_{{s}}$. Note that the lengths of these sequences, $L_t$ and $L_g$, can be different.

A semantic gesture and the utterance of its corresponding word or phrase are often not perfectly aligned in a speech \cite{liang2022seeg}. Such misalignment can confuse the encoder if the temporal correspondence between the two modalities is rigidly enforced during training. To alleviate this problem, we aggregate the semantics-relevant information in each feature sequence via max pooling
\begin{align}
    \vect{z}^t = \eqword{max\_pooling}(\vect{Z}^t)  , \quad 
    \vect{z}^g = \eqword{max\_pooling}(\vect{Z}^g) .
\end{align}
Then $\vect{z}^t,\vect{z}^g \in \mathbb{R}^{C_{{s}}}$ are considered the embeddings of the transcripts and gestures, respectively. 

We employ a powerful pretrained language model, T5-base \cite{xue2021mt5}, as the text encoder $\mathcal{E}_T$. The motion encoder $\mathcal{E}_G$ is a 12-layer, 768-feature wide, encoder-only transformer with 12 attention heads, which is pretrained on the gesture dataset by predicting masked motions in a way similar to BERT \cite{devlin2019bert}. Both the encoders are then fine-tuned using contrastive learning.

\subsection{Contrastive Learning}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/contrastive_loss.pdf}
    \caption{An illustration of the CLIP-style contrastive loss used to train the gesture and transcript encoders.}
    \Description{}
    \label{fig:contrastive_loss}
\end{figure}

We apply CLIP-style contrastive learning \cite{radford2021clip} to fine-tune the encoders. Given a batch of pairs of gesture and transcript embeddings $\mathcal{B}=\{(\vect{z}^t_i,\vect{z}^g_i)\}_{i=1}^{B}$, where $B$ is the batch size, the goal of the training is to maximize the similarity of the embeddings $(\vect{z}^t_i,\vect{z}^g_i)$ of the real pairs in the batch while minimizing the similarity of the incorrect pairs $(\vect{z}^t_i, \vect{z}^g_j)_{i\neq{}j}$. As illustrated in \fig\ref{fig:contrastive_loss}, this learning objective can be defined as the summation of the gesture-to-text ($\text{g2t}$) cross entropy and the text-to-gesture ($\text{t2g}$) cross entropy computed across the batch. Formally, the loss function is
\begin{align}
    \mathcal{L}_{\eqword{contrast}} = \mathbb{E}_{\mathcal{B}\sim\mathcal{D}}&\Bigl[
        \mathcal{H}_{\mathcal{B}}\left(\vect{y}^{\eqword{g2t}}(\vect{z}^g_i), \vect{p}^{\eqword{g2t}}(\vect{z}^g_i)\right) 
        \nonumber \\ 
    &+ \mathcal{H}_{\mathcal{B}}\left(\vect{y}^{\eqword{t2g}}(\vect{z}^t_j), \vect{p}^{\eqword{t2g}}(\vect{z}^t_j)\right)\Bigr].
\end{align}
Each cross entropy $\mathcal{H}$ is computed between a one-hot encoding $\vect{y}$ and a softmax-normalized distribution $\vect{p}$. $\vect{y}$ specifies the true correspondence between the gestures and transcripts in the training batch $\mathcal{B}$. $\vect{p}$ computes the similarity between an embedding of one modality and those of the other modality. Specifically,
\begin{align}
    \label{eqn:multimodal_similarity}
    %\vect{y}^{\eqword{g2t}}(\vect{z}^g_i) &= ,
    \vect{p}^{\eqword{g2t}}(\vect{z}^g_i) = \frac{\exp(\vect{z}^g_i \cdot \vect{z}^t_i / \tau)}{\sum^B_{j=1}\exp(\vect{z}^g_i \cdot \vect{z}^t_j / \tau)} 
    %\nonumber\\    
    %\vect{y}^{\eqword{t2g}}(\vect{z}^g_i) &= ,
    ,\quad
    \vect{p}^{\eqword{t2g}}(\vect{z}^t_j) = \frac{\exp(\vect{z}^t_j \cdot \vect{z}^g_j / \tau)}{\sum^B_{i=1}\exp(\vect{z}^t_j \cdot \vect{z}^g_i / \tau)},
\end{align}
where $\tau$ is the temperature of softmax. We further employ the \emph{momentum distillation} (MoD) \cite{li2021albef} technique to improve learning performance under noisy supervision. The key idea of MoD is to learn from the pseudo-targets generated by a momentum model. During training, we maintain a momentum version of the encoders by updating their network parameters in an exponential-moving-average (EMA) manner. Then, we use the momentum models to calculate multimodal features $\tildevect{z}^t$ and $\tildevect{z}^g$ for the training gesture-transcript pairs and compute the pseudo-targets $\tildevect{p}^{\eqword{g2t}}$ and $\tildevect{p}^{\eqword{t2g}}$ by substituting these features into \eqn\eqref{eqn:multimodal_similarity}. The contrastive loss is then modified as
\begin{align}
    \mathcal{L}_{\eqword{contrast}}^{\eqword{MoD}} &= (1 - w_{\eqword{contrast}})\mathcal{L}_{\eqword{contrast}} \nonumber \\
    &+ w_{\eqword{contrast}}\mathbb{E}_{\mathcal{B} \sim \mathcal{D}}\Bigl[D_{KL}\left(\tildevect{p}^{\eqword{g2t}}(\tildevect{z}^g_i) || \vect{p}^{\eqword{g2t}}(\vect{z}^g_i)\right) \nonumber \\
    &+ D_{KL}\left(\tildevect{p}^{\eqword{t2g}}(\tildevect{z}^t_j) || \vect{p}^{\eqword{t2g}}(\vect{z}^t_j)\right)\Bigr],
\end{align}
where $D_{KL}(\cdot || \cdot)$ is the KL divergence and $w_{\eqword{contrast}}$ is set to $0.4$.

\subsection{Applications of the Joint Embeddings}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/motion-based_transcripts_retrieval.pdf}
        \caption{Transcripts retrieved based on example gestures. Note that a gesture can natually accompany several semantics.}
        \label{fig:motion-based_transcripts_retrieval}
    \end{subfigure} \\
    \vspace{5pt}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/semantic_saliency.pdf}
        \caption{Semantic saliency curves of two sentences. The peaks of the curves indicate the words with high semantic importance which are likely to be accompanied by semantic gestures.}
        \label{fig:semantic_saliency}
    \end{subfigure}
    \caption{Applications of the gesture-transcript joint embeddings. (a) Motion-based transcripts retrieval . (b) Semantic saliency identification.}
    \label{fig:applications_of_gesture-transcript_embedding}
    \Description{}
\end{figure}

The learned joint embedding space combined with the encoders offers an effective way of measuring the semantic similarity between gestures and transcripts. To demonstrate this, we map a gesture motion into this space and retrieve the closest sentences from the transcript dataset based on the cosine distance of the embeddings. \fig\ref{fig:motion-based_transcripts_retrieval} shows some results. It can be observed that the retrieved sentences may have various meanings but can all be naturally accompanied by the query gestures.

Besides, the computation of the embeddings involves the max pooling operator that aggregates the most semantics-relevant information. We can thus estimate the saliency of each pose or word in a gesture sequence or a transcript sentence respectively using the embeddings. Specifically, given a sentence $\vect{T}$ and its encoded feature sequence $\vect{Z}^t$ and embedding vector $\vect{z}^t$, we compute the semantic saliency of each word as
\begin{align}
    \label{eqn:semantic-saliency}
    \vect{s}^t = \eqword{softmax}\left(\vect{Z}^t \cdot \vect{z}^t\right).
\end{align}
As illustrated in \fig\ref{fig:semantic_saliency}, the words with high semantic importance and that are likely to be accompanied by semantic gestures will have high saliency scores. This information can be considered as an important semantic cue, which will be used in our system to guide the gesture generator to create semantically correct gestures.