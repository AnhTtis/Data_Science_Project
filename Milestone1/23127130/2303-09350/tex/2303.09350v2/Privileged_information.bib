
@article{ben2006analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}
@inproceedings{chen2017,
     author    = {Yunpeng Chen and Xiaojie Jin and Jiashi Feng and Shuicheng Yan},
     title     = {Training Group Orthogonal Neural Networks with Privileged Information},
     booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
                  Artificial Intelligence, {IJCAI-17}},
     pages     = {1532--1538},
     year      = {2017},

   }
  @article{Li2022,
   title = {Domain adaptive twin support vector machine learning using privileged information},
   journal = {Neurocomputing},
   volume = {469},
   pages = {13-27},
   year = {2022},
   author = {Yanmeng Li and Huaijiang Sun and Wenzhu Yan},
  keywords = {Domain adaptive learning, Privileged information, Twin support vector machine, Classification}
 }
            
@inproceedings{ben2012hardness,
  title={On the hardness of domain adaptation and the utility of unlabeled target samples},
  author={Ben-David, Shai and Urner, Ruth},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={139--153},
  year={2012},
  organization={Springer}
}

@inproceedings{zhao2019learning,
  title={On learning invariant representations for domain adaptation},
  author={Zhao, Han and Des Combes, Remi Tachet and Zhang, Kun and Gordon, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={7523--7532},
  year={2019},
  organization={PMLR}
}

@article{
breitholtz2022practicality,
title={Practicality of generalization guarantees for unsupervised domain adaptation with neural networks},
author={Adam Breitholtz and Fredrik Daniel Johansson},
journal={Transactions on Machine Learning Research},
year={2022},
}
@book{foml_mohri,
title={Foundations of Machine Learning},
author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
publisher={MIT Press},
year={2018},
edition={Second}
}
@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778}
  }
@article{shimodaira2000improving,
  title={Improving predictive inference under covariate shift by weighting the log-likelihood function},
  author={Shimodaira, Hidetoshi},
  journal={Journal of statistical planning and inference},
  volume={90},
  number={2},
  pages={227--244},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{johansson2019support,
  title={Support and invertibility in domain-invariant representations},
  author={Johansson, Fredrik D and Sontag, David and Ranganath, Rajesh},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={527--536},
  year={2019},
  organization={PMLR}
}
@article{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}
@inproceedings{cortes2010,
 author = {Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Bounds for Importance Weighting},
 volume = {23},
 year = {2010}
}

@article{jonschkowski_patterns_2016,
	title = {Patterns for {Learning} with {Side} {Information}},
	abstract = {Supervised, semi-supervised, and unsupervised learning estimate a function given input/output samples. Generalization of the learned function to unseen data can be improved by incorporating side information into learning. Side information are data that are neither from the input space nor from the output space of the function, but include useful information for learning it. In this paper we show that learning with side information subsumes a variety of related approaches, e.g. multi-task learning, multi-view learning and learning using privileged information. Our main contributions are (i) a new perspective that connects these previously isolated approaches, (ii) insights about how these methods incorporate different types of prior knowledge, and hence implement different patterns, (iii) facilitating the application of these methods in novel tasks, as well as (iv) a systematic experimental evaluation of these patterns in two supervised learning tasks.},
	urldate = {2022-05-09},
	journal = {arXiv:1511.06429 [cs, stat]},
	author = {Jonschkowski, Rico and Höfer, Sebastian and Brock, Oliver},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.06429},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The first two authors contributed equally to this work},
	file = {arXiv Fulltext PDF:/home/adam/Zotero/storage/6C7VSBMF/Jonschkowski et al. - 2016 - Patterns for Learning with Side Information.pdf:application/pdf;arXiv.org Snapshot:/home/adam/Zotero/storage/P96WUXUY/1511.html:text/html},
}
	@article{lecun_gradient-based_1998,
           title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
           volume = {86},
           language = {en},
           number = {11},
           journal = {proceedings of the IEEE},
           author = {Lecun, Yann},
           year = {1998},
           pages = {47},
} 

@inproceedings{wu2019domain,
  title={Domain adaptation with asymmetrically-relaxed distribution alignment},
  author={Wu, Yifan and Winston, Ezra and Kaushik, Divyansh and Lipton, Zachary},
  booktitle={International conference on machine learning},
  pages={6872--6881},
  year={2019},
  organization={PMLR}
}

@inproceedings{beery2018recognition,
  title={Recognition in terra incognita},
  author={Beery, Sara and Van Horn, Grant and Perona, Pietro},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={456--473},
  year={2018}
}

@inproceedings{pearl2011transportability,
  title={Transportability of causal and statistical relations: A formal approach},
  author={Pearl, Judea and Bareinboim, Elias},
  booktitle={Twenty-fifth AAAI conference on artificial intelligence},
  year={2011}
}

@techreport{cifar10,
  author      = "Alex Krizhevsky",
  title       = "Learning Multiple Layers of Features from Tiny Images",
  institution= "",
  year        = "2009"
}
@inproceedings{long2013,
author = {Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S.},
title = {Transfer Feature Learning with Joint Distribution Adaptation},
year = {2013},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Computer Vision},
pages = {2200–2207},
numpages = {8},
keywords = {Transfer learning, feature learning, joint distribution adaptation},
series = {ICCV '13}
}


@inproceedings{orihashi_unsupervised_2020,
           title = {Unsupervised {Domain} {Adaptation} for {Dialogue} {Sequence} {Labeling} {Based} on {Hierarchical} {Adversarial} {Training}},
           abstract = {This paper presents a novel unsupervised domain adaptation method for dialogue sequence labeling. Dialogue sequence labeling is a supervised learning task that estimates labels for each     utterance in the given dialogue document, and is useful for many applications such as topic segmentation and dialogue act estimation. Accurate labeling often requires a large amount of labeled training data    , but it is difﬁcult to collect such data every time we need to support a new domain, such as contact centers in a new business ﬁeld. In order to solve this difﬁculty, we propose an unsupervised domain adap    tation method for dialogue sequence labeling. Our key idea is to construct dialogue sequence labeling using labeled source domain data and unlabeled target domain data so as to remove domain dependencies at     utterance-level and dialogue-level contexts. The proposed method adopts hierarchical adversarial training; two domain adversarial networks, an utterance-level context independent network and a dialogue-lev    el context dependent network, are introduced for improving domain invariance in the dialogue sequence labeling. Experiments on Japanese simulated contact center dialogue datasets demonstrate the effectivene    ss of the proposed method.},
           language = {en},
           urldate = {2022-09-14},
           booktitle = {Interspeech 2020},
           publisher = {ISCA},
          author = {Orihashi, Shota and Ihori, Mana and Tanaka, Tomohiro and Masumura, Ryo},
          month = oct,
          year = {2020},
          pages = {1575--1579},
          file = {Orihashi et al. - 2020 - Unsupervised Domain Adaptation for Dialogue Sequen.pdf:/home/adam/Zotero/storage/6EBIQUYU/Orihashi et al. - 2020 - Unsupervised Domain Adaptation for Dialogue Sequen    .pdf:application/pdf},
}        
@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	urldate = {2021-01-06},
	journal = {arXiv:1505.07818 [cs, stat]},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
	month = may,
	year = {2016},
	note = {arXiv: 1505.07818},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published in JMLR: http://jmlr.org/papers/v17/15-239.html},
	}
@InProceedings{shen2022,
  title = 	 {Connect, Not Collapse: Explaining Contrastive Learning for Unsupervised Domain Adaptation},
  author =       {Shen, Kendrick and Jones, Robbie M and Kumar, Ananya and Xie, Sang Michael and Haochen, Jeff Z. and Ma, Tengyu and Liang, Percy},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {19847--19878},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  abstract = 	 {We consider unsupervised domain adaptation (UDA), where labeled data from a source domain (e.g., photos) and unlabeled data from a target domain (e.g., sketches) are used to learn a classifier for the target domain. Conventional UDA methods (e.g., domain adversarial training) learn domain-invariant features to generalize from the source domain to the target domain. In this paper, we show that contrastive pre-training, which learns features on unlabeled source and target data and then fine-tunes on labeled source data, is competitive with strong UDA methods. However, we find that contrastive pre-training does not learn domain-invariant features, diverging from conventional UDA intuitions. We show theoretically that contrastive pre-training can learn features that vary subtantially across domains but still generalize to the target domain, by disentangling domain and class information. We empirically validate our theory on benchmark vision datasets.}
}

@article{li_discriminative_2018,
	title = {Discriminative multi-view {Privileged} {Information} learning for image re-ranking},
	abstract = {Conventional multi-view re-ranking methods usually perform asymmetrical matching between the region of interest (ROI) in the query image and the whole target image for similarity computation. Due to the inconsistency in the visual appearance, this practice tends to degrade the retrieval accuracy particularly when the image ROI, which is usually interpreted as the image objectness, accounts for a smaller region in the image. Since Privileged Information (PI), which can be viewed as the image prior, enables well characterizing the image objectness, we are aiming at leveraging PI for further improving the performance of the multi-view re-ranking accuracy in this paper. Towards this end, we propose a discriminative multi-view re-ranking approach in which both the original global image visual contents and the local auxiliary PI features are simultaneously integrated into a unified training framework for generating the latent subspaces with sufficient discriminating power. For the on-the-fly re-ranking, since the multi-view PI features are unavailable, we only project the original multi-view image representations onto the latent subspace, and thus the re-ranking can be achieved by computing and sorting the distances from the multi-view embeddings to the separating hyperplane. Extensive experimental evaluations on the two public benchmarks Oxford5k and Paris6k reveal our approach provides further performance boost for accurate image re-ranking, whilst the comparative study demonstrates the advantage of our method against other multi-view re-ranking methods.},
	urldate = {2022-05-09},
	journal = {arXiv:1808.04437 [cs]},
	author = {Li, Jun and Xu, Chang and Yang, Wankou and Sun, Changyin and Tao, Dacheng and Zhang, Hong},
	month = jul,
	year = {2018},
	note = {arXiv: 1808.04437},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:/home/adam/Zotero/storage/SLZ7FSBC/Li et al. - 2018 - Discriminative multi-view Privileged Information l.pdf:application/pdf;arXiv.org Snapshot:/home/adam/Zotero/storage/BQAQYWTD/1808.html:text/html},
}

@article{kallus_role_2020,
	title = {On the role of surrogates in the efficient estimation of treatment effects with limited outcome data},
	abstract = {We study the problem of estimating treatment effects when the outcome of primary interest (e.g., long-term health status) is only seldom observed but abundant surrogate observations (e.g., short-term health outcomes) are available. To investigate the role of surrogates in this setting, we derive the semiparametric efficiency lower bounds of average treatment effect (ATE) both with and without presence of surrogates, as well as several intermediary settings. These bounds characterize the best-possible precision of ATE estimation in each case, and their difference quantifies the efficiency gains from optimally leveraging the surrogates in terms of key problem characteristics when only limited outcome data are available. We show these results apply in two important regimes: when the number of surrogate observations is comparable to primary-outcome observations and when the former dominates the latter. Importantly, we take a missing-data approach that circumvents strong surrogate conditions which are commonly assumed in previous literature but almost always fail in practice. To show how to leverage the efficiency gains of surrogate observations, we propose ATE estimators and inferential methods based on flexible machine learning methods to estimate nuisance parameters that appear in the influence functions. We show our estimators enjoy efficiency and robustness guarantees under weak conditions.},
	urldate = {2022-05-09},
	journal = {arXiv:2003.12408 [cs, stat]},
	author = {Kallus, Nathan and Mao, Xiaojie},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.12408},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/adam/Zotero/storage/2XU53845/Kallus and Mao - 2020 - On the role of surrogates in the efficient estimat.pdf:application/pdf;arXiv.org Snapshot:/home/adam/Zotero/storage/Q47UJFGM/2003.html:text/html},
}

@inproceedings{ziheng_wang_classifier_2015,
	address = {Boston, MA, USA},
	title = {Classifier learning with hidden information},
	abstract = {Traditional data-driven classiﬁer learning approaches become limited when the training data is inadequate either in quantity or quality. To address this issue, in this paper we propose to combine hidden information and data to enhance classiﬁer learning. Hidden information represents information that is only available during training but not available during testing. It often exists in many applications yet has not been thoroughly exploited, and existing methods to utilize hidden information are still limited. To this end, we propose two general approaches to exploit different types of hidden information to improve different classiﬁers. We also extend the proposed methods to deal with incomplete hidden information. Experimental results on different applications demonstrate the effectiveness of the proposed methods for exploiting hidden information and their superior performance to existing methods.},
	language = {en},
	urldate = {2022-05-09},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Ziheng Wang} and {Qiang Ji}},
	month = jun,
	year = {2015},
	pages = {4969--4977},
	file = {Ziheng Wang and Qiang Ji - 2015 - Classifier learning with hidden information.pdf:/home/adam/Zotero/storage/VQUUJ38S/Ziheng Wang and Qiang Ji - 2015 - Classifier learning with hidden information.pdf:application/pdf},
}

@inproceedings{chen_boosting_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Boosting with {Side} {Information}},
	abstract = {In many problems of machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that aims to incorporate side information for improved learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information, which we term as AdaBoost+. In particular, AdaBoost+ employs a novel classification label imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from the features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance.},
	language = {en},
	booktitle = {Computer {Vision} – {ACCV} 2012},
	publisher = {Springer},
	author = {Chen, Jixu and Liu, Xiaoming and Lyu, Siwei},
	editor = {Lee, Kyoung Mu and Matsushita, Yasuyuki and Rehg, James M. and Hu, Zhanyi},
	year = {2013},
	keywords = {Expression Recognition, Face Image, Local Binary Pattern, Side Information, Training Data},
	pages = {563--577},
	file = {Full Text PDF:/home/adam/Zotero/storage/A5I6W5YW/Chen et al. - 2013 - Boosting with Side Information.pdf:application/pdf},
}

@article{vapnik_learning_2015,
	title = {Learning {Using} {Privileged} {Information}: {Similarity} {Control} and {Knowledge} {Transfer}},
	volume = {16},
	abstract = {This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classiﬁcation of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for signiﬁcantly accelerating the speed of Student’s learning using privileged information: (1) correction of Student’s concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Vapnik, Vladimir and Izmailov, Rauf},
	year = {2015},
	pages = {2023--2049},
	file = {Vapnik and Izmailov - Learning Using Privileged Information Similarity .pdf:/home/adam/Zotero/storage/BNWK9GAS/Vapnik and Izmailov - Learning Using Privileged Information Similarity .pdf:application/pdf},
}

@article{shaikh_transfer_2020,
	title = {Transfer learning privileged information fuels {CAD} diagnosis of breast cancer},
	volume = {31},
	abstract = {The efficiency in breast cancer from imaging-based computer-aided diagnosis (CAD) has been revealed in recent years. As a fact, the methods grounded on a single modality constantly lack behind multimodal CAD imaging. However, owing to the restrictions of imaging devices, expressly in rural hospitals, single-modal imaging becomes a favorite in clinical practice for diagnosis. A fresh learning model trending nowadays known as learning using privileged information (LUPI) adopts additional privileged information (PI) modality to help during the training stage, but PI does not contribute in the testing stage. Meanwhile, the link exists between PI and training samples; the same is then reassigned to the learned model. We propose a LUPI-based CAD framework for breast cancer using privileged information in this work. The work offers both a classifier- or feature-level LUPI, in which the information is shifted from the additional PI modality to the diagnosis modality. A thorough comparison has been made among six classifier-level algorithms and six feature-level LUPI algorithms. The experimental results on both the acquired primary datasets show that all classifier-level and deep learning-based feature-level LUPI algorithms can enhance the performance of a single-modal imaging-based CAD for breast cancer by relocating PI.},
	language = {en},
	number = {1},
	urldate = {2022-05-09},
	journal = {Machine Vision and Applications},
	author = {Shaikh, Tawseef Ayoub and Ali, Rashid and Beg, M. M. Sufyan},
	month = feb,
	year = {2020},
	keywords = {Breast cancer, Breast mammography, Deep learning (DL), Feature representation, Learning using privileged information (LUPI), Ultrasound (US)},
	pages = {9},
	file = {Full Text PDF:/home/adam/Zotero/storage/KKBCJ6SH/Shaikh et al. - 2020 - Transfer learning privileged information fuels CAD.pdf:application/pdf},
}

@inproceedings{robinson_strength_2020,
	title = {Strength from {Weakness}: {Fast} {Learning} {Using} {Weak} {Supervision}},
	shorttitle = {Strength from {Weakness}},
	abstract = {We study generalization properties of weakly supervised learning, that is, learning where only a few "strong" labels (the actual target for prediction) are present but many more "weak" labels are available. In particular, we show that pretraining using weak labels and finetuning using strong can accelerate the learning rate for the strong task to the fast rate of O(1/n), where n is the number of strongly labeled data points. This acceleration can happen even if, by itself, the strongly labeled data admits only the slower O(1/{\textbackslash}sqrt\{n\}) rate. The acceleration depends continuously on the number of weak labels available, and on the relation between the two tasks. Our theoretical results are reflected empirically across a range of tasks and illustrate how weak labels speed up learning on the strong task.},
	language = {en},
	urldate = {2022-05-09},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Robinson, Joshua and Jegelka, Stefanie and Sra, Suvrit},
	month = nov,
	year = {2020},
	pages = {8127--8136},
	file = {Full Text PDF:/home/adam/Zotero/storage/EQQ7KUT5/Robinson et al. - 2020 - Strength from Weakness Fast Learning Using Weak S.pdf:application/pdf;Supplementary PDF:/home/adam/Zotero/storage/4E3CPTFZ/Robinson et al. - 2020 - Strength from Weakness Fast Learning Using Weak S.pdf:application/pdf},
}

@article{vapnik_new_2009,
	series = {Advances in {Neural} {Networks} {Research}: {IJCNN2009}},
	title = {A new learning paradigm: {Learning} using privileged information},
	volume = {22},
	shorttitle = {A new learning paradigm},
	abstract = {In the Afterword to the second edition of the book “Estimation of Dependences Based on Empirical Data” by V. Vapnik, an advanced learning paradigm called Learning Using Hidden Information (LUHI) was introduced. This Afterword also suggested an extension of the SVM method (the so called SVMγ+ method) to implement algorithms which address the LUHI paradigm (Vapnik, 1982–2006, Sections 2.4.2 and 2.5.3 of the Afterword). See also (Vapnik et al., 2008, Vapnik et al., 2009) for further development of the algorithms. In contrast to the existing machine learning paradigm where a teacher does not play an important role, the advanced learning paradigm considers some elements of human teaching. In the new paradigm along with examples, a teacher can provide students with hidden information that exists in explanations, comments, comparisons, and so on. This paper discusses details of the new paradigm11In this article we changed the terminology. We will call this paradigm Learning Using Privileged Information (LUPI) (instead of LUHI) since the word privilege better reflects the core idea of the new paradigm. and corresponding algorithms, introduces some new algorithms, considers several specific forms of privileged information, demonstrates superiority of the new learning paradigm over the classical learning paradigm when solving practical problems, and discusses general questions related to the new ideas.},
	language = {en},
	number = {5},
	urldate = {2022-05-09},
	journal = {Neural Networks},
	author = {Vapnik, Vladimir and Vashist, Akshay},
	month = jul,
	year = {2009},
	keywords = {Hidden information, Learning with teacher, Machine learning, Oracle SVM, Privileged information, SVM, SVM+},
	pages = {544--557},
	file = {ScienceDirect Snapshot:/home/adam/Zotero/storage/B3V52XM6/S0893608009001130.html:text/html},
}

@inproceedings{jung2022efficient,
  title={Efficient learning of nonlinear prediction models with time-series privileged information},
  author={Jung, Bastian and Johansson, Fredrik Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{pechyony_pi,
author={Pechyony, Dmitry and Vapnik, Vladimir },
title={On the Theory of Learnining with Privileged Information},
year={2010},
publisher={Curran Associates, Inc.},
volume={23},
booktitle={Advances in Neural Information Processing Systems}
}
@article{he_multi-view_2019,
	title = {Multi-view transfer learning with privileged learning framework},
	volume = {335},
	abstract = {In this paper, we present a multi-view transfer learning model named Multi-view Transfer Discriminative Model (MTDM) for both image and text classification tasks. Transfer learning, which aims to learn a robust classifier for the target domain using data from a different distribution, has been proved to be effective in many real-world applications. However, most of the existing transfer learning methods map across domain data into a high-dimension space which the distance between domains is closed. This strategy always fails in the multi-view scenario. On the contrary, the multi-view learning methods are also difficult to extend in the transfer learning settings. One of our goals in this paper is to develop a model which can perform better in both multi-view and transfer learning settings. On the one hand, the problem of multi-view is implemented by the paradigm of learning using privileged information (LUPI), which could guarantee the principle of complementary and consensus. On the other hand, the model adequately utilizes the source domain data to build a robust classifier for the target domain. We evaluate our model on both image and text classification tasks and show the effectiveness compared with other baseline approaches.},
	language = {en},
	urldate = {2022-05-09},
	journal = {Neurocomputing},
	author = {He, Yiwei and Tian, Yingjie and Liu, Dalian},
	month = mar,
	year = {2019},
	keywords = {Learning using privileged information, Multi-view learning, Support vector machine, Transfer learning},
	pages = {131--142},
	file = {ScienceDirect Snapshot:/home/adam/Zotero/storage/ZJMD5FF7/S0925231219300311.html:text/html},
}

@article{tang_multiview_2018,
	title = {Multiview {Privileged} {Support} {Vector} {Machines}},
	volume = {29},
	abstract = {Multiview learning (MVL), by exploiting the complementary information among multiple feature sets, can improve the performance of many existing learning tasks. Support vector machine (SVM)-based models have been frequently used for MVL. A typical SVM-based MVL model is SVM-2K, which extends SVM for MVL by using the distance minimization version of kernel canonical correlation analysis. However, SVM-2K cannot fully unleash the power of the complementary information among different feature views. Recently, a framework of learning using privileged information (LUPI) has been proposed to model data with complementary information. Motivated by LUPI, we propose a new multiview privileged SVM model, multi-view privileged SVM model (PSVM-2V), for MVL. This brings a new perspective that extends LUPI to MVL. The optimization of PSVM-2V can be solved by the classical quadratic programming solver. We theoretically analyze the performance of PSVM-2V from the viewpoints of the consensus principle, the generalization error bound, and the SVM-2K learning model. Experimental results on 95 binary data sets demonstrate the effectiveness of the proposed method.},
	language = {eng},
	number = {8},
	journal = {IEEE transactions on neural networks and learning systems},
	author = {Tang, Jingjing and Tian, Yingjie and Zhang, Peng and Liu, Xiaohui},
	month = aug,
	year = {2018},
	pmid = {28809717},
	pages = {3463--3477},
}

@inproceedings{lee_learning_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning with {Privileged} {Information} for {Efficient} {Image} {Super}-{Resolution}},
	abstract = {Convolutional neural networks (CNNs) have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Most SR methods based on CNNs have focused on achieving performance gains in terms of quality metrics, such as PSNR and SSIM, over classical approaches. They typically require a large amount of memory and computational units. FSRCNN, consisting of few numbers of convolutional layers, has shown promising results, while using an extremely small number of network parameters. We introduce in this paper a novel distillation framework, consisting of teacher and student networks, that allows to boost the performance of FSRCNN drastically. To this end, we propose to use ground-truth high-resolution (HR) images as privileged information. The encoder in the teacher learns the degradation process, subsampling of HR images, using an imitation loss. The student and the decoder in the teacher, having the same network architecture as FSRCNN, try to reconstruct HR images. Intermediate features in the decoder, affordable for the student to learn, are transferred to the student through feature distillation. Experimental results on standard benchmarks demonstrate the effectiveness and the generalization ability of our framework, which significantly boosts the performance of FSRCNN as well as other SR methods. Our code and model are available online: https://cvlab.yonsei.ac.kr/projects/PISR.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Lee, Wonkyung and Lee, Junghyup and Kim, Dohyung and Ham, Bumsub},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {Distillation, Privileged information, Super-resolution},
	pages = {465--482},
	file = {Full Text PDF:/home/adam/Zotero/storage/C4BVY7RE/Lee et al. - 2020 - Learning with Privileged Information for Efficient.pdf:application/pdf},
}
 @article{xu_generative-discriminative_2020,
           title = {Generative-{Discriminative} {Complementary} {Learning}},
           volume = {34},
           number = {04},
           journal = {Proceedings of the AAAI Conference on Artificial intelligence},
           author = {Xu, Yanwu and Gong, Mingming and Chen, Junxiang and Liu, Tongliang and Zhang, Kun and Batmanghelich, Kayhan},
           month = apr,
          year = {2020},
          pages = {6526--6533},
 }

 @inproceedings{hoffman_learning_2016,
           address = {Las Vegas, NV, USA},
           title = {Learning with {Side} {Information} through {Modality} {Hallucination}},
           language = {en},
           urldate = {2022-05-16},
           booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
          publisher = {IEEE},
          author = {Hoffman, Judy and Gupta, Saurabh and Darrell, Trevor},
          month = jun,
          year = {2016},
          pages = {826--834},
          file = {Hoffman et al. - 2016 - Learning with Side Information through Modality Ha.pdf:/home/adam/Zotero/storage/NNYF99BT/Hoffman et al. - 2016 - Learning with Side Information through Modality Ha.p    df:application/pdf},
 }

@article{wang_learning_2018,
	title = {Learning with privileged information for multi-{Label} classification},
	volume = {81},
	abstract = {In this paper, we propose a novel approach for learning multi-label classifiers with the help of privileged information. Specifically, we use similarity constraints to capture the relationship between available information and privileged information, and use ranking constraints to capture the dependencies among multiple labels. By integrating similarity constraints and ranking constraints into the learning process of classifiers, the privileged information and the dependencies among multiple labels are exploited to construct better classifiers during training. A maximum margin classifier is adopted, and an efficient learning algorithm of the proposed method is also developed. We evaluate the proposed method on two applications: multiple object recognition from images with the help of implicit information about object importance conveyed by the list of manually annotated image tags; and multiple facial action unit detection from low-resolution images augmented by high-resolution images. Experimental results demonstrate that the proposed method can effectively take full advantage of privileged information and dependencies among multiple labels for better object recognition and better facial action unit detection.},
	language = {en},
	urldate = {2022-05-09},
	journal = {Pattern Recognition},
	author = {Wang, Shangfei and Chen, Shiyu and Chen, Tanfang and Shi, Xiaoxiao},
	month = sep,
	year = {2018},
	keywords = {Multi-label classification, Privileged information, Similarity constraints},
	pages = {60--70},
	file = {Submitted Version:/home/adam/Zotero/storage/VY5NFWML/Wang et al. - 2018 - Learning with privileged information for multi-Lab.pdf:application/pdf;ScienceDirect Snapshot:/home/adam/Zotero/storage/L4CG9W4N/S0031320318301225.html:text/html},
}

@article{you_privileged_2017,
	title = {Privileged {Multi}-label {Learning}},
	abstract = {Electronic proceedings of IJCAI 2017},
	urldate = {2022-05-09},
	author = {You, Shan and Xu, Chang and Wang, Yunhe and Xu, Chao and Tao, Dacheng},
	year = {2017},
	pages = {3336--3342},
	file = {Snapshot:/home/adam/Zotero/storage/JGAWSQU7/466.html:text/html},
}

@inproceedings{yang_miml-fcn_2017,
	address = {Honolulu, HI},
	title = {{MIML}-{FCN}+: {Multi}-{Instance} {Multi}-{Label} {Learning} via {Fully} {Convolutional} {Networks} with {Privileged} {Information}},
	shorttitle = {{MIML}-{FCN}+},
	abstract = {Multi-instance multi-label (MIML) learning has many interesting applications in computer visions, including multi-object recognition and automatic image tagging. In these applications, additional information such as bounding-boxes, image captions and descriptions is often available during training phrase, which is referred as privileged information (PI). However, as existing works on learning using PI only consider instance-level PI (privileged instances), they fail to make use of bag-level PI (privileged bags) available in MIML learning. Therefore, in this paper, we propose a two-stream fully convolutional network, named MIML-FCN+, uniﬁed by a novel PI loss to solve the problem of MIML learning with privileged bags. Compared to the previous works on PI, the proposed MIML-FCN+ utilizes the readily available privileged bags, instead of hard-to-obtain privileged instances, making the system more general and practical in real world applications. As the proposed PI loss is convex and SGDcompatible and the framework itself is a fully convolutional network, MIML-FCN+ can be easily integrated with stateof-the-art deep learning networks. Moreover, the ﬂexibility of convolutional layers allows us to exploit structured correlations among instances to facilitate more effective training and testing. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed MIML-FCN+, outperforming state-of-the-art methods in the application of multi-object recognition.},
	language = {en},
	urldate = {2022-05-09},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Hao and Zhou, Joey Tianyi and Cai, Jianfei and Ong, Yew Soon},
	month = jul,
	year = {2017},
	pages = {5996--6004},
	file = {Yang et al. - 2017 - MIML-FCN+ Multi-Instance Multi-Label Learning via.pdf:/home/adam/Zotero/storage/QJI568UA/Yang et al. - 2017 - MIML-FCN+ Multi-Instance Multi-Label Learning via.pdf:application/pdf},
}

@article{xu_distance_2015,
	title = {Distance {Metric} {Learning} {Using} {Privileged} {Information} for {Face} {Verification} and {Person} {Re}-{Identification}},
	volume = {26},
	abstract = {In this paper, we propose a new approach to improve face verification and person re-identification in the RGB images by leveraging a set of RGB-D data, in which we have additional depth images in the training data captured using depth cameras such as Kinect. In particular, we extract visual features and depth features from the RGB images and depth images, respectively. As the depth features are available only in the training data, we treat the depth features as privileged information, and we formulate this task as a distance metric learning with privileged information problem. Unlike the traditional face verification and person re-identification tasks that only use visual features, we further employ the extra depth features in the training data to improve the learning of distance metric in the training process. Based on the information-theoretic metric learning (ITML) method, we propose a new formulation called ITML with privileged information (ITML+) for this task. We also present an efficient algorithm based on the cyclic projection method for solving the proposed ITML+ formulation. Extensive experiments on the challenging faces data sets EUROCOM and CurtinFaces for face verification as well as the BIWI RGBD-ID data set for person re-identification demonstrate the effectiveness of our proposed approach.},
	language = {eng},
	number = {12},
	journal = {IEEE transactions on neural networks and learning systems},
	author = {Xu, Xinxing and Li, Wen and Xu, Dong},
	month = dec,
	year = {2015},
	pmid = {25781961},
	keywords = {Algorithms, Artificial Intelligence, Biometric Identification, Face, Humans, Image Processing, Computer-Assisted, Learning, Pattern Recognition, Automated},
	pages = {3150--3162},
}
@book{wainwright_2019,
place={Cambridge}, 
series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint}, 
publisher={Cambridge University Press},
author={Wainwright, Martin J.}, year={2019},
collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@article{tang_coupling_2021,
	title = {Coupling loss and self-used privileged information guided multi-view transfer learning},
	volume = {551},
	abstract = {Transfer learning builds models for the target domain by leveraging the information from another related source domain, in which the distributions of two domains are usually quite distinct. Real-world data are often characterized by multiple representations known as multi-view features. In the multi-view transfer learning field, existing methods aim to address the following two issues. Firstly, due to the distributional difference between the two domains, the classifier trained on the source domain may underperform on the target domain. Moreover, the lack of data from the target domain generally occurs in the training phase. Secondly, how to fully exploit the relations among multiple features is challenging when such multi-view representations emerge in the source and target domains. In this paper, we propose a new coupling loss and self-used privileged information guided multi-view transfer learning method (MVTL-CP). The first issue is addressed by utilizing the weighted labeled data from the source domain to learn a precise classifier for the target domain. Following the consensus and complementarity principles, we tackle the second issue by making the best use of multiple views. Furthermore, we analyze the consistency between views and the generalization capability of MVTL-CP. Comprehensive experiments confirm the effectiveness of our proposed model.},
	language = {en},
	urldate = {2022-05-09},
	journal = {Information Sciences},
	author = {Tang, Jingjing and He, Yiwei and Tian, Yingjie and Liu, Dalian and Kou, Gang and Alsaadi, Fawaz E.},
	month = apr,
	year = {2021},
	keywords = {Coupling loss, Multi-view learning, Privileged information, Support vector machine, Transfer learning},
	pages = {245--269},
	file = {ScienceDirect Snapshot:/home/adam/Zotero/storage/JUZPYNC8/S0020025520310665.html:text/html},
}

@inproceedings{sharmanska_learning_2013,
	title = {Learning to {Rank} {Using} {Privileged} {Information}},
	abstract = {Many computer vision problems have an asymmetric distribution of information between training and test time. In this work, we study the case where we are given additional information about the training data, which however will not be available at test time. This situation is called learning using privileged information (LUPI). We introduce two maximum-margin techniques that are able to make use of this additional source of information, and we show that the framework is applicable to several scenarios that have been studied in computer vision before. Experiments with attributes, bounding boxes, image tags and rationales as additional information in object classification show promising results.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Sharmanska, Viktoriia and Quadrianto, Novi and Lampert, Christoph H.},
	month = dec,
	year = {2013},
	note = {ISSN: 2380-7504},
	keywords = {Computer vision, Learning to rank, object classification, Optimization, privileged information during training, Seals, Support vector machines, Training, Vectors, Whales},
	pages = {825--832},
	file = {IEEE Xplore Full Text PDF:/home/adam/Zotero/storage/TQYQGHII/Sharmanska et al. - 2013 - Learning to Rank Using Privileged Information.pdf:application/pdf},
}

@article{yan_image_2016,
	title = {Image {Classification} by {Cross}-{Media} {Active} {Learning} {With} {Privileged} {Information}},
	volume = {18},
	abstract = {In this paper, we propose a novel cross-media active learning algorithm to reduce the effort on labeling images for training. The Internet images are often associated with rich textual descriptions. Even though such textual information is not available in test images, it is still useful for learning robust classifiers. In light of this, we apply the recently proposed supervised learning paradigm, learning using privileged information, to the active learning task. Specifically, we train classifiers on both visual features and privileged information, and measure the uncertainty of unlabeled data by exploiting the learned classifiers and slacking function. Then, we propose to select unlabeled samples by jointly measuring the cross-media uncertainty and the visual diversity. Our method automatically learns the optimal tradeoff parameter between the two measurements, which in turn makes our algorithms particularly suitable for real-world applications. Extensive experiments demonstrate the effectiveness of our approach.},
	number = {12},
	journal = {IEEE Transactions on Multimedia},
	author = {Yan, Yan and Nie, Feiping and Li, Wen and Gao, Chenqiang and Yang, Yi and Xu, Dong},
	month = dec,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Multimedia},
	keywords = {Active learning, cross-media analysis, Data models, Electronic mail, image classification, image-text joint modeling, Internet, Measurement uncertainty, Training, Uncertainty, Visualization},
	pages = {2494--2502},
	file = {IEEE Xplore Full Text PDF:/home/adam/Zotero/storage/KCNKJT5S/Yan et al. - 2016 - Image Classification by Cross-Media Active Learnin.pdf:application/pdf;IEEE Xplore Abstract Record:/home/adam/Zotero/storage/Y5CWHI9Q/7552533.html:text/html},
}

@article{niu_exploiting_2016,
	title = {Exploiting {Privileged} {Information} from {Web} {Data} for {Action} and {Event} {Recognition}},
	volume = {118},
	abstract = {In the conventional approaches for action and event recognition, sufficient labelled training videos are generally required to learn robust classifiers with good generalization capability on new testing videos. However, collecting labelled training videos is often time consuming and expensive. In this work, we propose new learning frameworks to train robust classifiers for action and event recognition by using freely available web videos as training data. We aim to address three challenging issues: (1) the training web videos are generally associated with rich textual descriptions, which are not available in test videos; (2) the labels of training web videos are noisy and may be inaccurate; (3) the data distributions between training and test videos are often considerably different. To address the first two issues, we propose a new framework called multi-instance learning with privileged information (MIL-PI) together with three new MIL methods, in which we not only take advantage of the additional textual descriptions of training web videos as privileged information, but also explicitly cope with noise in the loose labels of training web videos. When the training and test videos come from different data distributions, we further extend our MIL-PI as a new framework called domain adaptive MIL-PI. We also propose another three new domain adaptation methods, which can additionally reduce the data distribution mismatch between training and test videos. Comprehensive experiments for action and event recognition demonstrate the effectiveness of our proposed approaches.},
	language = {en},
	number = {2},
	urldate = {2022-05-09},
	journal = {International Journal of Computer Vision},
	author = {Niu, Li and Li, Wen and Xu, Dong},
	month = jun,
	year = {2016},
	keywords = {Action recognition, Domain adaptation, Event recognition, Learning using privileged information, Multi-instance learning},
	pages = {130--150},
	file = {Full Text PDF:/home/adam/Zotero/storage/8GYHGB2W/Niu et al. - 2016 - Exploiting Privileged Information from Web Data fo.pdf:application/pdf},
}

@article{li_learning_2019,
	title = {Learning using privileged information improves neuroimaging-based {CAD} of {Alzheimer}'s disease: a comparative study},
	volume = {57},
	shorttitle = {Learning using privileged information improves neuroimaging-based {CAD} of {Alzheimer}'s disease},
	abstract = {The neuroimaging-based computer-aided diagnosis (CAD) for Alzheimer's disease (AD) has shown its effectiveness in recent years. In general, the multimodal neuroimaging-based CAD always outperforms the approaches based on a single modality. However, single-modal neuroimaging is more favored in clinical practice for diagnosis due to the limitations of imaging devices, especially in rural hospitals. Learning using privileged information (LUPI) is a new learning paradigm that adopts additional privileged information (PI) modality to help to train a more effective learning model during the training stage, but PI itself is not available in the testing stage. Since PI is generally related to the training samples, it is then transferred to the learned model. In this work, a LUPI-based CAD framework for AD is proposed. It can flexibly perform a classifier- or feature-level LUPI, in which the information is transferred from the additional PI modality to the diagnosis modality. A thorough comparison has been made among three classifier-level algorithms and five feature-level LUPI algorithms. The experimental results on the ADNI dataset show that all classifier-level and deep learning based feature-level LUPI algorithms can improve the performance of a single-modal neuroimaging-based CAD for AD by transferring PI. Graphical abstract Graphical abstract for the framework of the LUPI-based CAD for AD.},
	language = {eng},
	number = {7},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Li, Yan and Meng, Fanqing and Shi, Jun and {Alzheimer’s Disease Neuroimaging Initiative}},
	month = jul,
	year = {2019},
	pmid = {31028606},
	keywords = {Aged, Aged, 80 and over, Algorithms, Alzheimer Disease, Alzheimer’s disease, Databases, Factual, Deep learning, Deep Learning, Feature representation, Humans, Image Processing, Computer-Assisted, Learning using privileged information, Magnetic resonance imaging, Magnetic Resonance Imaging, Middle Aged, Neuroimaging, Positron emission tomography, Positron-Emission Tomography, Support Vector Machine},
	pages = {1605--1616},
}
@InProceedings{Sarafianos_2017_ICCV,
author = {Sarafianos, Nikolaos and Vrigkas, Michalis and Kakadiaris, Ioannis A.},
title = {Adaptive SVM+: Learning With Privileged Information for Domain Adaptation},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops},
month = {Oct},
year = {2017}
}
@article{lapin_learning_2014,
	title = {Learning using privileged information: {SVM}+ and weighted {SVM}},
	volume = {53},
	shorttitle = {Learning using privileged information},
	abstract = {Prior knowledge can be used to improve predictive performance of learning algorithms or reduce the amount of data required for training. The same goal is pursued within the learning using privileged information paradigm which was recently introduced by Vapnik et al. and is aimed at utilizing additional information available only at training time – a framework implemented by SVM+. We relate the privileged information to importance weighting and show that the prior knowledge expressible with privileged features can also be encoded by weights associated with every training example. We show that a weighted SVM can always replicate an SVM+ solution, while the converse is not true and we construct a counterexample highlighting the limitations of SVM+. Finally, we touch on the problem of choosing weights for weighted SVMs when privileged features are not available.},
	language = {en},
	urldate = {2022-05-09},
	journal = {Neural Networks},
	author = {Lapin, Maksim and Hein, Matthias and Schiele, Bernt},
	month = may,
	year = {2014},
	pages = {95--108},
	file = {Lapin et al. - 2014 - Learning using privileged information SVM+ and we.pdf:/home/adam/Zotero/storage/N8YJVA8X/Lapin et al. - 2014 - Learning using privileged information SVM+ and we.pdf:application/pdf},
}
@book{sugiyama_density_2012,
	series = {Cambridge books online},
	title = {Density {Ratio} {Estimation} in {Machine} {Learning}},
	publisher = {Cambridge University Press},
	author = {Sugiyama, M. and Suzuki, T. and Kanamori, T.},
	year = {2012},
	lccn = {2011051726},
}
@phdthesis{mootianthesis,
    author={Saeid Motiian},
    title = {Domain Adaptation and Privileged Information for
Visual Recognition},
    year={2019},
    type={phdthesis},
    address={https://researchrepository.wvu.edu/etd/6271},
    note={Graduate Theses, Dissertations, and Problem Reports. 6271.}
}
  @inproceedings{lin2014microsoft,
   author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, Larry},
   title = {Microsoft COCO: Common Objects in Context},
   booktitle = {ECCV},
   year = {2014},
   month = {September},
   publisher = {European Conference on Computer Vision},
   edition = {ECCV},
 }

@inproceedings{shi_learning_2017,
	address = {Honolulu, HI},
	title = {Learning and {Refining} of {Privileged} {Information}-{Based} {RNNs} for {Action} {Recognition} from {Depth} {Sequences}},
	abstract = {Existing RNN-based approaches for action recognition from depth sequences require either skeleton joints or handcrafted depth features as inputs. An end-to-end manner, mapping from raw depth maps to action classes, is nontrivial to design due to the fact that: 1) single channel map lacks texture thus weakens the discriminative power; 2) relatively small set of depth training data. To address these challenges, we propose to learn an RNN driven by privileged information (PI) in three-steps: An encoder is pretrained to learn a joint embedding of depth appearance and PI (i.e. skeleton joints). The learned embedding layers are then tuned in the learning step, aiming to optimize the network by exploiting PI in a form of multi-task loss. However, exploiting PI as a secondary task provides little help to improve the performance of a primary task (i.e. classiﬁcation) due to the gap between them. Finally, a bridging matrix is deﬁned to connect two tasks by discovering latent PI in the reﬁning step. Our PI-based classiﬁcation loss maintains a consistency between latent PI and predicted distribution. The latent PI and network are iteratively estimated and updated in an expectation-maximization procedure. The proposed learning process provides greater discriminative power to model subtle depth difference, while helping avoid overﬁtting the scarcer training data. Our experiments show signiﬁcant performance gains over stateof-the-art methods on three public benchmark datasets and our newly collected Blanket dataset.},
	language = {en},
	urldate = {2022-05-09},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shi, Zhiyuan and Kim, Tae-Kyun},
	month = jul,
	year = {2017},
	pages = {4684--4693},
	file = {Shi and Kim - 2017 - Learning and Refining of Privileged Information-Ba.pdf:/home/adam/Zotero/storage/DHJGPLRU/Shi and Kim - 2017 - Learning and Refining of Privileged Information-Ba.pdf:application/pdf},
}

@article{guo_learning_2018,
	title = {Learning using privileged information for {HRRP}-based radar target recognition},
	volume = {12},
	abstract = {A novel machine learning method named extended support vector data description with negative examples (ESVDD-neg) is developed to classify the fast Fourier transform-magnitude feature of complex high-resolution range profile (HRRP), motivated by the problem of radar automatic target recognition. The proposed method not only inherits the close non-linear boundary advantage of support vector data description with negative examples model but also incorporates a new learning paradigm named learning using privileged information into the model. It leads to the appealing application with no assumptions regarding the distribution of data and needs less training samples and prior information. Besides, the second order central moment is selected as privileged information for better recognition performance, weakening the effect of translation sensitivity, and the normalisation contributes to eliminating the amplitude sensitivity. Hence, there will be a remarkable improvement of recognition accuracy not only with small training dataset but also under the condition of low signal-to-noise ratio. Numerical experiments based on two publicly UCI datasets and HRRPs of four aircrafts demonstrate the feasibility and superiority of the proposed method. The noise robust ESVDD-neg is ideal for HRRP-based radar target recognition.},
	language = {en},
	number = {2},
	urldate = {2022-05-09},
	journal = {IET Signal Processing},
	author = {Guo, Yu and Xiao, Huaitie and Kan, Yingzhi and Fu, Qiang},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-spr.2016.0625},
	keywords = {close nonlinear boundary advantage, complex high-resolution range profile, ESVDD-neg, extended support vector data description-with-negative examples, fast Fourier transform-magnitude feature classification, fast Fourier transforms, HRRP-based radar target recognition, learning (artificial intelligence), learning paradigm, low signal-to-noise ratio, machine learning method, radar automatic target recognition problem, radar computing, radar signal processing, radar target recognition, signal classification, translation sensitivity, UCI datasets},
	pages = {188--197},
	file = {Snapshot:/home/adam/Zotero/storage/PRP6PRYU/iet-spr.2016.html:text/html;Full Text PDF:/home/adam/Zotero/storage/CQCG6KYN/Guo et al. - 2018 - Learning using privileged information for HRRP-bas.pdf:application/pdf},
}

@inproceedings{ishida_learning_2017,
	title = {Learning from {Complementary} {Labels}},
	volume = {30},
	abstract = {Collecting labeled data is costly and thus a critical bottleneck in real-world classification tasks. To mitigate this problem, we propose a novel setting, namely learning from complementary labels for multi-class classification. A complementary label specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than collecting ordinary labels, since users do not have to carefully choose the correct class from a long list of candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from them. In this paper, we show that an unbiased estimator to the classification risk can be obtained only from complementarily labeled data, if a loss function satisfies a particular symmetric condition. We derive estimation error bounds for the proposed method and prove that the optimal parametric convergence rate is achieved. We further show that learning from complementary labels can be easily combined with learning from ordinary labels (i.e., ordinary supervised learning), providing a highly practical implementation of the proposed method. Finally, we experimentally demonstrate the usefulness of the proposed methods.},
	urldate = {2022-05-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ishida, Takashi and Niu, Gang and Hu, Weihua and Sugiyama, Masashi},
	year = {2017},
	file = {Full Text PDF:/home/adam/Zotero/storage/5S27SXJ2/Ishida et al. - 2017 - Learning from Complementary Labels.pdf:application/pdf},
}

@inproceedings{taherkhani_weakly_2019,
	address = {Seoul, Korea (South)},
	title = {A {Weakly} {Supervised} {Fine} {Label} {Classifier} {Enhanced} by {Coarse} {Supervision}},
	abstract = {Objects are usually organized in a hierarchical structure in which each coarse category (e.g., big cat) corresponds to a superclass of several ﬁne categories (e.g., cheetah, leopard). The objects grouped within the same coarse category, but in different ﬁne categories, usually share a set of global features; however, these objects have distinctive local properties that characterize them at a ﬁne level. This paper addresses the challenge of ﬁne image classiﬁcation in a weakly supervised fashion, whereby a subset of images is tagged by ﬁne labels (i.e., ﬁne images), while the remaining are tagged by coarse labels (i.e., coarse images). We propose a new deep model that leverages coarse images to improve the classiﬁcation performance of ﬁne images within the coarse category. Our model is an end-to-end framework consisting of a Convolutional Neural Network (CNN) which uses ﬁne and coarse images to tune its parameters. The CNN outputs are then fanned out into two separate branches such that the ﬁrst branch uses a supervised low rank self-expressive layer to project the CNN outputs to the low rank subspaces to capture the global structures for the coarse classiﬁcation, while the other branch uses a supervised sparse selfexpressive layer to project them to the sparse subspaces to capture the local structures for the ﬁne classiﬁcation. Our deep model uses coarse images in conjunction with ﬁne images to jointly explore the low rank and sparse subspaces by sharing the network parameters during the training which causes the data obtained by the CNN to be well-projected to both sparse and low rank subspaces for classiﬁcation.},
	language = {en},
	urldate = {2022-05-09},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Taherkhani, Fariborz and Kazemi, Hadi and Dabouei, Ali and Dawson, Jeremy and Nasrabadi, Nasser},
	month = oct,
	year = {2019},
	pages = {6458--6467},
	file = {Taherkhani et al. - 2019 - A Weakly Supervised Fine Label Classifier Enhanced.pdf:/home/adam/Zotero/storage/9W3VSTU5/Taherkhani et al. - 2019 - A Weakly Supervised Fine Label Classifier Enhanced.pdf:application/pdf},
}

@inproceedings{lopez-paz_unifying_2016,
	title = {Unifying distillation and privileged information},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR} 2016)},
	author = {Lopez-Paz, David and Bottou, Leon and Schölkopf, Bernhard and Vapnik, Vladimir},
	year = {2016},
}

@article{hayashi_long-term_2019,
	title = {Long-{Term} {Prediction} of {Small} {Time}-{Series} {Data} {Using} {Generalized} {Distillation}},
	abstract = {A novel method for long-term prediction of small time-series data designed in the framework of generalized distillation to utilize the middle-time data between the input and output times as "privileged information," which is available only in the training phase and not in the test phase is proposed. The recent increase of "big data" in our society has led to major impacts of machine learning and data mining technologies in various fields ranging from marketing to science. On the other hand, there still exist areas where only small-sized data are available for various reasons, for example, high data acquisition costs or the rarity of targets events. Machine learning tasks using such small data are usually difficult because of the lack of information available for training accurate prediction models. In particular, for long-term time-series prediction, the data size tends to be small because of the unavailability of the data between input and output times in training. Such limitations on the size of time-series data further make long-term prediction tasks quite difficult; in addition, the difficulty that the far future is more uncertain than the near future.In this paper, we propose a novel method for long-term prediction of small time-series data designed in the framework of generalized distillation. The key idea of the proposed method is to utilize the middle-time data between the input and output times as "privileged information," which is available only in the training phase and not in the test phase. We demonstrate the effectiveness of the proposed method on both synthetic data and real-world data. The experimental results show the proposed method performs well, particularly when the task is difficult and has high input dimensions.},
	journal = {2019 International Joint Conference on Neural Networks (IJCNN)},
	author = {Hayashi, S. and Tanimoto, Akira and Kashima, H.},
	year = {2019},
}

@article{serra-toro_exploring_2014,
	title = {Exploring some practical issues of {SVM}+: {Is} really privileged information that helps?},
	volume = {42},
	journal = {Pattern Recognition Letters},
	author = {Serra-Toro, Carlos and Traver, V. Javier and Pla, Filiberto},
	month = jun,
	year = {2014},
	pages = {40--46},
}

@inproceedings{tang_retaining_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Retaining {Privileged} {Information} for {Multi}-{Task} {Learning}},
	abstract = {Knowledge transfer has been of great interest in current machine learning research, as many have speculated its importance in modeling the human ability to rapidly generalize learned models to new scenarios. Particularly in cases where training samples are limited, knowledge transfer shows improvement on both the learning speed and generalization performance of related tasks. Recently, Learning Using Privileged Information (LUPI) has presented a new direction in knowledge transfer by modeling the transfer of prior knowledge as a Teacher-Student interaction process. Under LUPI, a Teacher model uses Privileged Information (PI) that is only available at training time to improve the sample complexity required to train a Student learner for a given task. In this work, we present a LUPI formulation that allows privileged information to be retained in a multi-task learning setting. We propose a novel feature matching algorithm that projects samples from the original feature space and the privilege information space into a joint latent space in a way that informs similarity between training samples. Our experiments show that useful knowledge from PI is maintained in the latent space and greatly improves the sample efficiency of other related learning tasks. We also provide an analysis of sample complexity of the proposed LUPI method, which under some favorable assumptions can achieve a greater sample efficiency than brute force methods.},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Fengyi and Xiao, Cao and Wang, Fei and Zhou, Jiayu and Lehman, Li-wei H.},
	year = {2019},
	note = {event-place: Anchorage, AK, USA},
	keywords = {electronic health records, multi-task learning, privileged information},
	pages = {1369--1377},
}

@article{vovk_game_1998,
	title = {A game of prediction with expert advice},
	abstract = {We consider the following problem. At each point of discrete time the learner must make a prediction; he is given the predictions made by a pool of experts. Each prediction and the outcome, which is disclosed after the learner has made his prediction, determine the incurred loss. It is known that, under weak regularity, the learner can ensure that his cumulative loss never exceeds cL+a ln n, where c and a are some constants, n is the size of the pool, and L is the cumulative loss incurred by the best expert in the pool. We find the set of those pairs (c, a) for which this is true.]1998 Academic Press 1. MAIN RESULT Our learning protocol is as follows. We consider a learner who acts in the following environment. There are a pool of n experts and the nature, which interact with the learner in the following way. At each trial t, t=1, 2,...: 1. Each expert i, i=1,..., n, makes a prediction \#t(i)  \# 1,},
	journal = {J. Comput. and System Sci},
	author = {Vovk, V.},
	year = {1998},
	pages = {153--173},
	file = {Citeseer - Full Text PDF:/home/adam/Zotero/storage/KFE695MG/Vovk - 1998 - A game of prediction with expert advice.pdf:application/pdf;Citeseer - Snapshot:/home/adam/Zotero/storage/EQ7ZGKJM/download.html:text/html},
}

@article{liu_adaboost-based_2022,
	title = {{AdaBoost}-based transfer learning with privileged information},
	volume = {593},
	abstract = {Transfer learning aims to improve the learning of the target domain with the help of knowledge from the source domain. Recently, learning using privileged information (LUPI) has been proposed to learn an accurate classifier with privileged information which is only obtained in the training stage. In this paper, we propose a new AdaBoost-based Transfer Learning with Privileged Information (AdaTLPI) method to solve the transfer learning problem with privileged information, in which AdaBoost is taken into account to combine the weak classifiers into a strong classifier. In the model, we utilize shared parameter to transfer knowledge from the source domain to the target domain. We then incorporate privileged information about the source and target domains into a unified model and AdaBoost is used to learn a strong classifier by combining the obtained weak classifiers. Finally, we present an effective optimization algorithm to solve the proposed objective model and present the boundary of training error of the proposed method. The experimental results manifest that the proposed method can outperform the previous methods.},
	journal = {Information Sciences},
	author = {Liu, Bo and Liu, Laiwang and Xiao, Yanshan and Liu, Changdong and Chen, Xiaodong and Li, Weibin},
	year = {2022},
	keywords = {Learning using privileged information (LUPI), Transfer learning (TL)},
	pages = {216--232},
}

@article{zech2018variable,
  title={Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study},
  author={Zech, John R and Badgeley, Marcus A and Liu, Manway and Costa, Anthony B and Titano, Joseph J and Oermann, Eric Karl},
  journal={PLoS medicine},
  volume={15},
  number={11},
  pages={e1002683},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}

@inproceedings{lopez-paz_randomized_2014,
	address = {Bejing, China},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Randomized {Nonlinear} {Component} {Analysis}},
	volume = {32},
	abstract = {Classical methods such as Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) are ubiquitous in statistics. However, these techniques are only able to reveal linear relationships in data. Although nonlinear variants of PCA and CCA have been proposed, these are computationally prohibitive in the large scale. In a separate strand of recent research, randomized methods have been proposed to construct features that help reveal nonlinear patterns in data. For basic tasks such as regression or classification, random features exhibit little or no loss in performance, while achieving drastic savings in computational requirements. In this paper we leverage randomness to design scalable new variants of nonlinear PCA and CCA; our ideas extend to key multivariate analysis tools such as spectral clustering or LDA. We demonstrate our algorithms through experiments on real-world data, on which we compare against the state-of-the-art. A simple R implementation of the presented algorithms is provided.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lopez-Paz, David and Sra, Suvrit and Smola, Alex and Ghahramani, Zoubin and Schoelkopf, Bernhard},
	editor = {Xing, Eric P. and Jebara, Tony},
	month = jun,
	year = {2014},
	note = {Issue: 2},
	pages = {1359--1367},
}
@INPROCEEDINGS{DADA2019,
  author={Vu, Tuan-Hung and Jain, Himalaya and Bucher, Maxime and Cord, Matthieu and Pérez, Patrick Pérez},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={DADA: Depth-Aware Domain Adaptation in Semantic Segmentation}, 
  year={2019},
  volume={},
  number={},
  pages={7363-7372}
}
@inproceedings{silva_financial_2010,
	title = {Financial distress model prediction using {SVM}+},
	booktitle = {Proceedings of the {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Silva, Catarina and Vieira, Armando and Gaspar-Cunha, Antonio and Carvalho das Neves, Joao},
	month = jul,
	year = {2010},
	pages = {1--7},
}

@article{feyereisl_privileged_2012,
	title = {Privileged information for data clustering},
	volume = {194},
	abstract = {Many machine learning algorithms assume that all input samples are independently and identically distributed from some common distribution on either the input space X, in the case of unsupervised learning, or the input and output space X×Y in the case of supervised and semi-supervised learning. In the last number of years the relaxation of this assumption has been explored and the importance of incorporation of additional information within machine learning algorithms became more apparent. Traditionally such fusion of information was the domain of semi-supervised learning. More recently the inclusion of knowledge from separate hypothetical spaces has been proposed by Vapnik as part of the supervised setting. In this work we are interested in exploring Vapnik’s idea of ‘master-class’ learning and the associated learning using ‘privileged’ information, however within the unsupervised setting. Adoption of the advanced supervised learning paradigm for the unsupervised setting instigates investigation into the difference between privileged and technical data. By means of our proposed aRi-MAX method stability of the K-Means algorithm is improved and identification of the best clustering solution is achieved on an artificial dataset. Subsequently an information theoretic dot product based algorithm called P-Dot is proposed. This method has the ability to utilize a wide variety of clustering techniques, individually or in combination, while fusing privileged and technical data for improved clustering. Application of the P-Dot method to the task of digit recognition confirms our findings in a real-world scenario.},
	journal = {Information Sciences},
	author = {Feyereisl, Jan and Aickelin, Uwe},
	year = {2012},
	keywords = {Clustering, Hidden information, Machine learning, Master-class learning, Privileged information},
	pages = {4--23},
	annote = {Intelligent Knowledge-Based Models and Methodologies for Complex Information Systems},
}

@article{sharmanska_learning_2014,
	title = {Learning to {Transfer} {Privileged} {Information}},
	abstract = {We introduce a learning framework called learning using privileged information (LUPI) to the computer vision field. We focus on the prototypical computer vision problem of teaching computers to recognize objects in images. We want the computers to be able to learn faster at the expense of providing extra information during training time. As additional information about the image data, we look at several scenarios that have been studied in computer vision before: attributes, bounding boxes and image tags. The information is privileged as it is available at training time but not at test time. We explore two maximum-margin techniques that are able to make use of this additional source of information, for binary and multiclass object classification. We interpret these methods as learning easiness and hardness of the objects in the privileged space and then transferring this knowledge to train a better classifier in the original space. We provide a thorough analysis and comparison of information transfer from privileged to the original data spaces for both LUPI methods. Our experiments show that incorporating privileged information can improve the classification accuracy. Finally, we conduct user studies to understand which samples are easy and which are hard for human learning, and explore how this information is related to easy and hard samples when learning a classifier.},
	urldate = {2022-05-09},
	journal = {arXiv:1410.0389 [cs, stat]},
	author = {Sharmanska, Viktoriia and Quadrianto, Novi and Lampert, Christoph H.},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.0389},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {Comment: 20 pages with figures},
	file = {arXiv Fulltext PDF:/home/adam/Zotero/storage/CSUP8LH3/Sharmanska et al. - 2014 - Learning to Transfer Privileged Information.pdf:application/pdf;arXiv.org Snapshot:/home/adam/Zotero/storage/WPW5XTX5/1410.html:text/html},
}
@inproceedings{karlsson_lupts,
  author    = {Rickard Karlsson and
               Martin Willbo and
               Zeshan Hussain and
               Rahul G. Krishnan and
               David A. Sontag and
               Fredrik D. Johansson},
  title     = {Using Time-Series Privileged Information for Provably Efficient Learning
               of Prediction Models},
booktitle={Proceedings of The 25th International Conference on Artificial Intelligence and Statistics 2022},
  year      = {2021},
}
@article{fouad_incorporating_2013,
	title = {Incorporating privileged information through metric learning},
	volume = {24},
	abstract = {In some pattern analysis problems, there exists expert knowledge, in addition to the original data involved in the classification process. The vast majority of existing approaches simply ignore such auxiliary (privileged) knowledge. Recently a new paradigm-learning using privileged information-was introduced in the framework of SVM+. This approach is formulated for binary classification and, as typical for many kernel-based methods, can scale unfavorably with the number of training examples. While speeding up training methods and extensions of SVM+ to multiclass problems are possible, in this paper we present a more direct novel methodology for incorporating valuable privileged knowledge in the model construction phase, primarily formulated in the framework of generalized matrix learning vector quantization. This is done by changing the global metric in the input space, based on distance relations revealed by the privileged information. Hence, unlike in SVM+, any convenient classifier can be used after such metric modification, bringing more flexibility to the problem of incorporating privileged information during the training. Experiments demonstrate that the manipulation of an input space metric based on privileged data improves classification accuracy. Moreover, our methods can achieve competitive performance against the SVM+ formulations.},
	language = {eng},
	number = {7},
	journal = {IEEE transactions on neural networks and learning systems},
	author = {Fouad, Shereen and Tino, Peter and Raychaudhury, Somak and Schneider, Petra},
	month = jul,
	year = {2013},
	pmid = {24808523},
	keywords = {Algorithms, Artificial Intelligence, Biometry, Databases, Factual, Humans, Information Theory, Learning},
	pages = {1086--1098},
}

@inproceedings{hernandez-lobato_mind_2014,
	title = {Mind the {Nuisance}: {Gaussian} {Process} {Classification} using {Privileged} {Noise}},
	volume = {27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hernández-lobato, Daniel and Sharmanska, Viktoriia and Kersting, Kristian and Lampert, Christoph H and Quadrianto, Novi},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
	year = {2014},
}

@misc{mseg,
  author = {Lambert, John and Liu, Zhuang and Sener, Ozan and Hays, James and Koltun, Vladlen},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MSeg: A Composite Dataset for Multi-domain Semantic Segmentation},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@inproceedings{irvin2019chexpert,
  title={Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
  author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  pages={590--597},
  year={2019}
}

@inproceedings{nih,
  title={ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases},
  author={Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2097--2106},
  year={2017}
}

@article{fasterrcnn,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{lin2017fpn,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2117--2125},
  year={2017}
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.},
  author={Ren, S and He, K and Girshick, R and Sun, J},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016}
}

@manual{skorch,
  author       = {Marian Tietz and Thomas J. Fan and Daniel Nouri and Benjamin Bossan and {skorch Developers}},
  title        = {skorch: A scikit-learn compatible neural network library that wraps PyTorch},
  month        = jul,
  year         = 2017
}