\documentclass[nohyperref,twocolumn]{article}
\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{siunitx}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath,amsfonts,mathtools,amssymb,amsthm,dsfont}
\usepackage[round]{natbib}
\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
\usepackage{xcolor}
\usepackage{color,colortbl}
\usepackage{newfloat}
\usepackage{listings}
\usepackage{amsfonts,dsfont}
\usepackage{soul}
\usepackage{tikz}
\usetikzlibrary{arrows, decorations.markings}
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
% \floatstyle{ruled}
% \newfloat{listing}{tb}{lst}{}
% \floatname{listing}{Listing}

% Header
\input{header}
\definecolor{Gray}{gray}{0.9}
\title{Unsupervised domain adaptation \\ by learning using privileged information}

\usepackage{authblk}

\renewcommand*{\Affilfont}{\normalsize}
\setlength{\affilsep}{2em}


\author[1]{Adam Breitholtz}
\author[1]{Anton Matsson}
\author[1]{Fredrik D. Johansson}
\affil[1]{Department of Computer Science \& Engineering, Chalmers University of Technology}

\date{}

\usepackage[margin=0.9in]{geometry}
\setlength{\columnsep}{.25in}


\begin{document}
\maketitle
\begin{abstract}
Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors in domain transfer compared to classical learning.
\end{abstract}
%
% INTRODUCTION
%
\section{Introduction}
\label{sec:introduction}
%
Deployment of machine learning (ML) systems relies on generalization from training samples to new instances in a target domain. When these instances differ in distribution from the source of training data, performance tends to degrade and guarantees are often weak. For example, a supervised ML model trained to identify medical conditions in X-ray images from one hospital may work poorly in another if the two hospitals have different equipment or examination protocols~\citep{zech2018variable}. If no labeled examples are available from the target domain, the so-called \emph{unsupervised domain adaptation} (UDA) problem~\citep{ben2006analysis}, strong assumptions are needed for success.

A common assumption in UDA is that the object of the learning task is identical in source and target domains but that input distributions differ~\citep{shimodaira2000improving}. This ``covariate shift'' assumption is plausible in our X-ray example above; doctors are likely to give the same diagnosis based on X-rays of the same patient from similar but different equipment. %%less complex sentence
Additionally, however, guarantees for consistent domain transfer require either distributional overlap between inputs from source and target domains or known parametric forms of the labeling function~\citep{ben2012hardness,wu2019domain,johansson2019support}. Without these, adaptation cannot be verified by statistical means.

Input domain overlap is implausible for our running example and for the high-dimensional tasks that have become standard benchmarks in the community, including image classification~\citep{long2013,ganin_domain-adversarial_2016} and sentence labeling~\citep{orihashi_unsupervised_2020}. If hospitals have different X-ray equipment, the probability of observing identical images from source and target domains is zero~\citep{zech2018variable}. Even when covariate shift and overlap are satisfied, large domain differences can have a dramatic effect on sample complexity~\citep{breitholtz2022practicality}. Despite promising developments~\citep{shen2022}, realistic guarantees for practical domain transfer remain elusive.

Incorporating side information in training has been proposed as a means to improve generalization without domain shift. Through learning using privileged information (LUPI)~\citep{vapnik_new_2009,lopez-paz_unifying_2016}, algorithms that are given access to auxiliary variables during training, but not in deployment, have been proven to learn from fewer labeled examples compared to algorithms learning without these variables~\citep{karlsson_lupts}. In X-ray classification, privileged information (PI) can come from graphical annotations or clinical notes made by radiologists, which are unavailable when the system is used (see Figure~\ref{fig:illustration_x-ray}). While PI has been used also in domain adaptation, see e.g., \citep{Sarafianos_2017_ICCV,DADA2019}, the literature has yet to characterize its benefits in generalizing across domains.
%
\paragraph{Contributions.} In this work, we define \emph{unsupervised domain adaptation by learning using privileged information} (DALUPI), where unlabeled examples with privileged information, are available from the target domain  (Section~\ref{sec:pi_setup}). For this problem, we give conditions under which it is possible to identify a model which predicts optimally in the target domain, without assuming statistical overlap between source and target input domains (Section~\ref{sec:analysis}).  Next, we instantiate this problem setting in multi-class and multi-label image classification and propose practical algorithms for these tasks (Section~\ref{sec:method}). On image classification tasks with boxes indicating regions of interest as PI, including multi-label diagnosis from chest X-rays, we compare our algorithms to baselines for supervised learning and unsupervised domain adaptation. We find that they perform favorably to the alternatives (Section~\ref{sec:experiments}), particularly when input overlap is violated and when training set sizes are small. 
% 
% PROBLEM SETUP
%
\section{Unsupervised domain adaptation with and without privileged information}
\label{sec:setup}
\label{sec:uda_setup}
In unsupervised domain adaptation (UDA), our goal is to learn a hypothesis $h$ to predict outcomes (or labels) $Y \in \cY$ for problem instances represented by input covariates $X \in \cX$, drawn from a target domain with density $\cT(X,Y)$. During training, we have access to labeled samples $(x, y)$ only from a source domain $\cS(X,Y)$ and unlabeled samples $\tilde{x}$ from $\cT(X)$. As running example, we think of $\cS$ and $\cT$ as radiology departments at two different hospitals, of $X$ as the X-ray image of a patient, and of $Y$ as their diagnoses.
%
We aim to minimize prediction errors in terms of the \emph{target risk} $R_\cT$ of a hypothesis $h \in \cH$ from a hypothesis set $\cH$, with respect to a loss function $L : \cY \times \cY \rightarrow \bbR_+$, solving
\begin{equation}\label{eq:risk}
\min_{h \in \cH} R_\cT(h) \mbox{, }\;\; R_\cT(h) \coloneqq \E_{X,Y\sim \cT}[L(h(X), Y)]~.
\end{equation}
%
A solution to the UDA problem returns a minimizer of \eqref{eq:risk} without ever observing labeled samples from $\cT$. However, if $\cS$ and $\cT$ are allowed to differ arbitrarily, finding such a solution cannot be guaranteed~\citep{ben2012hardness}. Under the assumption of \emph{covariate shift}~\citep{shimodaira2000improving}, the labeling function and learning task is the same on both domains, but the covariate distribution differs.
\begin{thmasmp}[Covariate shift] \label{asmp:covshift}%
For domains $\cS, \cT$ on $\cX \times \cY$, we say that \emph{covariate shift} holds with respect to $X$ if
$$
\exists x : \cT(x) \neq \cS(x) \;\;\mbox{and}\;\; \forall x : \cT(Y \mid x) = \cS(Y \mid x)~.
$$%
\end{thmasmp}
%
In our example, covariate shift means that radiologists at either hospital would diagnose two patients with the same X-ray in the same way, but that they may encounter different distributions of patients and images. 
Learning a good target model from labeled source-domain examples appears feasible under Assumption~\ref{asmp:covshift}. However, to guarantee consistent learning in general, $\cS(x)$ must also sufficiently \emph{overlap} the target input domain $\cT(x)$.
%
\begin{thmasmp}[Domain overlap]\label{asmp:overlap} A domain $\cS$ overlaps another domain $\cT$ with respect to a variable $Z$ on $\cZ$ if
$$
\forall z \in \cZ : \cT(Z=z) > 0 \implies \cS(Z=z) > 0~.
$$%
\end{thmasmp}
%
Covariate shift and domain overlap w.r.t. inputs $X$ are standard assumptions in UDA, used by most informative guarantees~\citep{zhao2019learning}. However, overlap is often violated in practice. This is true in widely used benchmark tasks such as MNIST vs MNIST-M~\citep{ganin_domain-adversarial_2016}, where one image domain is in black-and-white and the other in full color, and in the real-world example  given by \citet{zech2018variable}, who found that it was possible to detect which site an X-ray image came from due to differences in protocols. 
Minimization of generalization bounds based on metrics such as the discrepancy distance~\citep{ben2006analysis} or integral probability metrics~\citep{long2013} does not rely on overlap, but is not guaranteed to return a target-optimal model either~\citep{johansson2019support}. Next, we study how PI can be used to identify such a model.
%
% DALUPI
%
\subsection{UDA with privileged information}\label{sec:pi_setup}
\def\cm{\checkmark}
\def\no{}
\begin{table}[t]
\caption{A summary of the different settings we consider in this work, what data is assumed to be available during training and if guarantees for identification are known for the setting under the assumptions of Proposition~\ref{prop:identification}. The parentheses around source samples for DALUPI indicate that we need not necessarily observe these for the setting. Note that at test time only $x$ from $\cT$ is observed. $^*$Under the more generous assumption of overlapping support in the input space $\cX$, guarantees exist for all these settings.}
\label{tab:settings}
\centering
\begin{tabular}{lccccccc}
\toprule
Setting &\multicolumn{3}{c}{Observed $\cS$} & \multicolumn{3}{c}{Observed $\cT$} & Guarantee\\
& $x$ & $w$ & $y$ & $\tilde{x}$ & $\tilde{w}$ & $\tilde{y}$ & for $R_\cT$\\  
\midrule
SL-T & \no & \no & \no & \cm & \no & \cm & \cm \\%
\midrule
SL-S & \cm & \no & \cm & \no & \no & \no & $^*$ \\%
UDA        & \cm & \no & \cm & \cm & \no & \no & $^*$ \\%
LUPI       & \cm & \cm & \cm & \no & \no & \no & $^*$ \\%
%DALuPI     & \no & \cm & \cm & \cm & \cm & \no & \cm \\%
DALUPI    & (\cm) & \cm & \cm & \cm & \cm & \no & \cm \\%
\bottomrule
\end{tabular}
\end{table}
In high-dimensional UDA problems, overlap in input covariates $X$ may be violated due to information that is irrelevant to the learning task. A famous example is background pixels in image object detection~\citep{beery2018recognition}, which may vary across domains and have spurious correlation with the label $Y$, confusing the learner. Nevertheless, the image may \emph{contain} information which is both sufficient to make good predictions and supported in both domains. For X-rays, this could be a subset of pixels indicating a medical condition, ignoring parts that merely indicate differences in protocol or equipment~\citep{zech2018variable}. When a trained system is used, such information is not identified---the whole X-ray is given as input---but can be supplied during training as added supervision in a variable $W$; for example, a region of interest indicated by a bounding box, akin to the related examples of~\citep{sharmanska_learning_2014}. As such, using $W$ during training is an example of \emph{learning using privileged information} (LUPI)~\citep{vapnik_new_2009}. 

We define \emph{domain adaptation by learning using privileged information} (DALUPI) as follows. During training, learners observe samples of covariates $X$, labels $Y$ and privileged information $W\in\mathcal{W}$ from $\cS$ in a dataset $D_\cS = \{(x_i, w_i, y_i)\}_{i=1}^m$, as well as samples of covariates and privileged information from $\cT$, $D_\cT = \{(\tilde{x}_i, \tilde{w}_i)\}_{i=1}^n$. At test time, the trained models only observe covariates $X$ from $\cT$. Our goal remains to minimize the target risk~\eqref{eq:risk}. Figure~\ref{fig:illustration_x-ray} illustrates several applications with this structure.

In Table~\ref{tab:settings}, we list common learning paradigms for domain adaptation. Supervised learning, SL-S, refers to learning from labeled samples from $\cS$ without privileged information. SL-T refers to supervised learning with (infeasible) oracle access to labeled samples from $\cT$. UDA refers to the setting described in Section~\ref{sec:uda_setup} and LUPI to traditional learning using privileged information~\citep{vapnik_new_2009}. 
% 
% ANALYSIS
%
\section{Provable domain adaptation with sufficient privileged information}
\begin{figure}[t]
\centering
\includegraphics[width=1\columnwidth]{fig/example_2.png}
\caption{Examples of domain adaptation by learning using privileged information (DALUPI). During training, input samples $X$ and privileged information $W$ are available from both source and target domains. Labels $Y$ are only available for inputs from the source domain. At test time, a target sample $X$ is observed. Top: Chest X-ray images $X$ together with pathology masks $W$ and diagnosis label $Y$. Middle: Tokenizations $W$ accompany recorded speech $X$ during learning of a speech recognition system. Bottom: Clinical notes $W$ add information to a system learning to predict disease progression from patient features $X$. \label{fig:illustration_x-ray}}
\end{figure}
\label{sec:analysis}
It is well-known that privileged information can be used to improve sample efficiency (reduce variance) in learning~\citep{vapnik_learning_2015,pechyony_pi,jung2022efficient}. Next, we will show that privileged information can provide \emph{identifiability} of the target risk---that it may be computed from the observational distribution---even when overlap is not satisfied in the input variable $X$.\footnote{Identifiability of the target risk is related to transportability of causal and statistical parameters~\citep{pearl2011transportability}.}

To identify $R_\cT$ \eqref{eq:risk} without overlap in $X$, we require the additional assumption that $W$ is sufficient for $Y$ given $X$.
\begin{thmasmp}[Sufficiency of privileged information]\label{asmp:sufficiency}
Privileged information $W$ is sufficient for the outcome $Y$ given covariates $X$ if $Y \perp X \mid W$ in both $\cS$ and $\cT$.
\end{thmasmp}
Assumption~\ref{asmp:sufficiency} is satisfied when $X$ provides no more information about $Y$ in the presence of $W$. In our running example, if $W$ is the subset of X-ray pixels corresponding to an area indicating a medical problem, the other pixels in $X$ may be unnecessary to make a good prediction of $Y$. Moreover, if covariate shift and overlap are satisfied w.r.t. $W$, i.e., there is some probability of observing the restricted pixel subset in both domains, we have the following proposition.
\begin{thmprop}\label{prop:identification}
Let Assumptions~\ref{asmp:covshift} and \ref{asmp:overlap} be satisfied w.r.t. $W$ (not necessarily w.r.t. $X$) and let Assumption~\ref{asmp:sufficiency} hold as stated. Then, the target risk $R_\cT$ is identified and
\begin{align*}
R_\cT(h) & = \sum_{x} \cT(x) \sum_w \cT(w\mid x) \sum_{y} \cS(y\mid w) L(h(x), y)~.
\end{align*}%
\end{thmprop}%
\begin{proof}%
By definition, $R_\cT(h) = \sum_{x,y}\cT(x, y) L(h(x), y)$. We then marginalize over privileged information $W$, 
\begin{align*}
\cT(y\mid x) %& = \E_{\cT(W\mid x)}[\cT(Y\mid W, x) \mid x] \\ 
& = \E_{\cT(W\mid x)}[\cT(y\mid W) \mid x] \\
& = \sum_{w : \cS(w)>0} \cT(w \mid x) \cS(y\mid w)~, 
%& = \sum_{w : \cT(w)>0} \cT(w \mid x) \cS(y\mid w) \\
%& = \sum_{w : \cS(w)>0} \cT(w \mid x) \cS(y\mid w)~.
\end{align*}
where the first equality follows by sufficiency and the second by covariate shift and overlap in $W$. $\cT(x), \cT(w \mid x)$ and $\cS(y\mid w)$ are observable through training samples.
\end{proof}
Proposition~\ref{prop:identification} allows us to relax the standard assumption of overlap in $X$ at the cost of collecting additional variables. When $W$ is a subset of the information in $X$, as in our running example,  overlap in $X$ implies overlap in $W$, but not vice versa. Moreover, under Assumption~\ref{asmp:sufficiency}, if covariate shift holds for $X$, it holds also for $W$.

Proposition~\ref{prop:identification} shows that there are conditions where privileged information allows for identification of a target-optimal hypothesis where identification is not possible without it, e.g., when overlap is violated in $X$~\citep{johansson2019support}. Intuitively, $W$ guides the learner toward the information in $X$ which is relevant for the label. The conditions on $W$ may be compared to the properties of the representation mapping in Construction 4.1 of~\citep{wu2019domain}. Finally, we note that Proposition~\ref{prop:identification} does not require that $X$ is ever observed in the source domain $\cS$. 
\subsection{A simple two-stage algorithm and its risk}
%
Under Assumption~\ref{asmp:sufficiency}, a natural strategy for predicting the outcome $Y$ is to first infer the privileged information $W$ from $X$ using a function $\hf$, and then predict $Y$ using a function $\hg$ applied to the inferred value, $\hh = \hg(\hf(X))$. We may find such functions $\hf$, $\hg$ by solving two separate empirical risk minimization (ERM) problems, as follows. 
%
\begin{equation}\label{eq:twostage_erm}
\begin{aligned}
\hat{f} & \coloneqq \argmin_{f \in \cF} \hat{R}^{W}_{\cT}(f), \hat{R}^{W}_{\cT}(f) = \frac{1}{n}\sum_{i=1}^{n}L(f(\tilde{x}_i), \tilde{w}_i) \\
\hat{g} & \coloneqq \argmin_{g \in \cG} \hat{R}^{Y}_{\cS}(g), \;\hat{R}^{Y}_{\cS}(g) = \frac{1}{m} \sum_{i=1}^{m}L(g(w_i), y_i)
\end{aligned}
\end{equation}
Hypothesis classes $\cF, \cG$ may be chosen so that the class $\cH = \{h = g \circ f ; (f,g) \in \cF \times \cG\}$ has a desired form. 
%
We can bound the generalization error of an estimator $\hat{h} = \hat{g} \circ \hat{f}$ of the form above, when $W \in \mathbb{R}^{d_\cW}$ and $L$ is the squared loss, by placing a Lipschitz assumption on the space of prediction functions $\cG$, i.e., $\forall g\in \cG, w, w' \in \mathcal{W},~  \|g(w)-g(w')\|_2 \leq M \|w-w'\|_2$. For this, we first define the $\rho$-weighted prediction risk in the source domain, 
$$
R^{Y,\rho}_\cS(g) = \E_{\cS(W,Y)}[\rho(W)L(g(W), Y)]
$$
where $\rho$ is the density ratio of $\cT$ and $\cS$, $\rho(w)=\frac{\cT(w)}{\cS(w)}$. We define the empirical weighted risk  $\hat{R}^{Y,\rho}_S(g)$ analogously to the above. When the density ratio $\rho$ is not known one might use density estimation~\citep{sugiyama_density_2012} or probabilistic classifiers to estimate it. We now have the following result which we prove for univariate $Y$; the result can be generalized to multiple classes with standard methods. 
\begin{thmprop}\label{prop:bound}
Assume that $\cG$ comprises M-Lipschitz mappings from the privileged information space $\mathcal{W} \subseteq \bbR^{d_W}$ to $\cY$. Further, assume that both the ground truth privileged information $W$ and label $Y$ are deterministic in $X$ and $W$, respectively. 
Let $\rho$ be the domain density ratio of $W$ and let Assumptions~\ref{asmp:covshift}--\ref{asmp:sufficiency} (Covariate shift, Overlap and Sufficiency) hold with respect to $W$. Further, let $L$ be the squared Euclidean distance and assume that $L(\hf(w),Y)$ is uniformly bounded over $\cW$ by a constant $B$ and let $d$ and $d'$ be the pseudo-dimensions of $\mathcal{G}$ and $\mathcal{F}$, respectively. Assume that there are $n$ observations each from the source (labeled) and target (unlabeled) domains. Then, for any $h = f \circ g \in \cF \times \cG$, with probability at least $1-\delta$,
\begin{align*}
\frac{R_\cT(h)}{2} & \leq \hat{R}^{Y,\rho}_\cS(g) +M^2 \hat{R}^W_\cT(f)  \\ 
& + \mathcal{O}\left( \sqrt[3/8]{\frac{d\log\frac{n}{d}}{n}}
 + \sqrt{\frac{d'\log\frac{n}{d'}}{n}} \right)~.
\end{align*}
\end{thmprop}
\begin{proofsketch}
Decomposing the risk of $h \circ \phi$ , we get 
\begin{align*}
 & R_\cT(h)  = \E_\cT[(g(f(X)) - Y)^2] \\
%& \leq 2\E_\cT[(g(W) - Y)^2 + (g(f(X)) - g(W))^2] \\
%& \leq 2\E_\cT[(g(W) - Y)^2 + M^2\|f(X)) - g(W)\|^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2] +  2M^2\E_\cT[\|(f(X) - W)\|^2] \\
& = 2R^Y_\cT(g) +  2M^2R^W_\cT(f) =2R^{Y,\rho}_S(g) +  2M^2R^W_\cT(f)~.
\end{align*}
The first inequality follows the relaxed triangle inequality and the Lipschitz property, and the third equality from Overlap and Covariate shift. Treating each component of $\hat{w}$ as independent, using standard PAC learning results, and application of Theorem 3 from \citet{cortes2010} along with a union bound argument, we get the stated result. See Appendix \ref{app:proof} for a more detailed proof.
\end{proofsketch}
As a result of Proposition~\ref{prop:bound}, when $\cF$ and $\cG$ contain the ground-truth mappings between $X$ and $W$ and between $W$ and $Y$, in the limit of infinite samples, minimizers of \eqref{eq:twostage_erm} minimize $R_\cT$ as well. 
% 
% METHOD
%
\section{Image classification under domain shift with privileged information}
\label{sec:method}
Image classification remains one of the primary benchmark tasks for unsupervised domain adaptation. In our experiments, we consider multi-class and multi-label image classification tasks where privileged information $W$ highlights regions of interest in the images, related to the labels $Y$. Specifically, $W$ is the pixels contained within one or more bounding boxes, represented by pixel coordinates $T\in\bbR^{4}$.

In the multi-class case, each image has a single bounding box whose corresponding pixels determine a single label $Y$ of the image. In the multi-label case, an observation $(x_i, w_i, y_i)$ may contain multiple bounding boxes $j\in\{1,\ldots, J\}$, with coordinates $t_{ij}$, each of which is assigned a multi-class label $u_{ij}$. The image label $y_i$ constitutes the set of observed bounding box labels in the image. Next, we propose algorithms that leverage privileged information for improved domain transfer in these two tasks.
%
\subsection{DALUPI-TS: A two-stage algorithm for multi-class image classification}
\label{sec:twostage}
Our first algorithm, which we call DALUPI-TS, conforms to our theoretical analysis and instantiates \eqref{eq:twostage_erm} for the multi-class image classification problem by separately fitting functions $\hat{f} \in \cF$ and $\hat{g} \in \cG$ to infer privileged information $W$ and predict $Y$. 
$\hat{f}$ takes an input image $X$ and (i) estimates the location of a single bounding box, (ii) cuts out the corresponding part of the image $X$, and (iii) resizes it to a set dimension. Steps (ii)--(iii) are hard coded (not learned). The pixels $W$ defining a region of interest are fed to $\hat{g}$ which outputs a prediction of the image label $Y$. 

In experiments, we use convolutional neural networks for both $\hf$ and $\hg$.
The two estimators are trained separately on $(\tilde{x}, \tilde{w})\sim\cT$ and $(w,y)\sim\cS$, respectively, as in \eqref{eq:twostage_erm}, and then evaluated on target domain images where the output of $\hf$ is used for prediction with $\hg$. We use the mean squared error loss for $\hf$ and categorical cross-entropy loss for $\hg$. 
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{fig/fasterrcnn.png}
\caption{Faster R-CNN~\cite{ren2016faster}. The RoI pooling layer and the classification and regression layers are part of the Fast R-CNN detection network \cite{girshick2015fast}.}
\label{fig:fasterrcnn}
\end{figure}
\subsection{DALUPI-E2E: An end-to-end solution for multi-label image  classification}\label{sec:endtoend}
For multi-label classification, we adapt Faster R-CNN \citep{ren2016faster} outlined in Figure \ref{fig:fasterrcnn} and described below. Faster R-CNN uses a region proposal network (RPN) to generate region proposals which are fed to a detection network for classification and bounding box regression. This way of solving the task in subsequent steps has similarities with our two-stage algorithm although Faster R-CNN can be trained end-to-end. We make small modifications to the training procedure of the original model in the end of this section.

The RPN generates region proposals relative to a fixed number of reference boxes---anchors---centered at the locations of a sliding window moving over convolutional feature maps. Each anchor is assigned a binary label $p\in\{0, 1\}$ based on its overlap with ground-truth bounding boxes; positive anchors are also associated with a ground-truth box with location $t$. The RPN loss for a single anchor is
\begin{equation}
    L^{\text{RPN}}(\hat{p}, p, \hat{t}, t) := 
    L_{\text{cls}}^{\text{RPN}}(\hat{p}, p) +
    pL_{\text{reg}}^{\text{RPN}}(\hat{t}, t),
    \label{eq:rpn}
\end{equation}
where $\hat{t}$ represents the refined location of the anchor and $\hat{p}$ is the estimated probability that the anchor contains an object. The binary cross-entropy loss and a smooth $L_{1}$ loss are used for the classification loss $L^{\text{RPN}}_{\text{cls}}$ and the regression loss $L^{\text{RPN}}_{\text{reg}}$, respectively. For a mini-batch of images, the total RPN loss is computed based on a subset of all anchors, sampled to have a ratio of up to 1:1 between positive and negative ditto.

A filtered set of region proposals are projected onto the convolutional feature maps. For each proposal, the detection network---Fast R-CNN \cite{girshick2015fast}---outputs a probability $\hat{p}(k)$ and a predicted bounding box location $\hat{t}(k)$ for each class $k$. Let $\hat{p}=(\hat{p}(0), \ldots, \hat{p}(K))$, where $\sum_{k}\hat{p}(k) =1$, $K$ is the number of classes and $0$ represents a catch-all background class. For a single proposal with ground-truth coordinates $t$ and multi-class label $u\in\{0, \ldots, K\}$, the detection loss is
\begin{equation}
    L^{\text{det}}(\hat{p}, u, \hat{t}_{u}, t) = 
    L_{\text{cls}}^{\text{det}}(\hat{p}, u) +
    \mathbf{I}_{u \geq 1}L_{\text{reg}}^{\text{det}}(\hat{t}_u, t),
    \label{eq:roi}
\end{equation}
where $L_{\text{cls}}^{\text{det}}(\hat{p}, u)=-\log{\hat{p}(u)}$ and $L_{\text{reg}}^{\text{det}}$ is a smooth $L_{1}$ loss. To obtain a probability vector for the entire image, we maximize, for each class $k$, over the probabilities of all proposals.
% Add figure here to force it to appear on a single page
\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{fig/mnist_source.pdf}
    \caption{Source test accuracy.}
    \label{fig:source}
\end{subfigure}%
\begin{subfigure}[b]{0.33\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{fig/mnist_target.pdf}
    \caption{Target test accuracy.}
    \label{fig:target}
\end{subfigure}%
\begin{subfigure}[b]{0.16\textwidth}
\centering
\lineskip=0pt
\includegraphics[width=0.74\linewidth]{fig/saliency_0.2_img.pdf}\\
\includegraphics[width=0.74\linewidth]{fig/saliency_0.2_map.pdf}%
\caption{SL-S, $\epsilon = 0.2$.}\label{fig:saliencymniste02}
\end{subfigure}%
\begin{subfigure}[b]{0.16\textwidth}
\centering
\lineskip=0pt
\includegraphics[width=0.74\linewidth]{fig/saliency_1_img.pdf}\\
\includegraphics[width=0.74\linewidth]{fig/saliency_1_map.pdf}
\caption{SL-S, $\epsilon = 1.0$.}\label{fig:saliencymniste10}
\end{subfigure}%
\caption{Digit classificastion task. Model performance on source (a) and target (b) domains as a function of association $\epsilon$ between background and label in $\cS$. As the skew increases, the performance of the source models on the target data deteriorates. The rightmost panels show two example images (top) from the dataset as well as saliency maps (bottom) from SL-S when trained with source domain skew $\epsilon=0.2$ (c) and $\epsilon=1$ (d). 
\label{fig:mnistexp}}
\end{figure*}%
During training, Faster R-CNN requires that all input images $x$ come with at least one ground-truth annotation (bounding box) $w$ and its corresponding label $u$. To increase sample-efficiency, we enable training the model using non-annotated but labeled samples $(x, y)$ from the source domain and annotated but unlabeled samples $(\tilde{x}, \tilde{w})$ from the target domain. In the RPN, no labels are needed, and we simply ignore anchors from non-annotated images when sampling anchors for the loss computation. For the computation of \eqref{eq:roi}, we handle the two cases separately. We assign the label $u=-1$ to all ground-truth annotations from the target domain and multiply $L_{\text{cls}}^{\text{det}}$ by the indicator $\mathbf{I}_{u \geq 0}$. For non-annotated samples $(x, y)$ from the source domain, there are no box-specific coordinates $t$ or labels $u$ but only the labels $y$ for the entire image. In this case, \eqref{eq:roi} is undefined and we instead compute the binary cross-entropy loss between binarized labels and the probability vector for the entire image.

We train the RPN and the detection network jointly as described in \cite{ren2016faster}. To extract feature maps, we use a Feature Pyramid Network \cite{lin2017fpn} on top of a ResNet-50 architecture \cite{resnet}. We use the modfied model---DALUPI-E2E---in the experiments in Section \ref{sec:coco} and \ref{sec:chest}. Note that we may also train the model in a LUPI setting, where no information from the target domain is used, in which case we call it LUPI-E2E.
%
% EXPERIMENTS
%
\section{Experiments} \label{sec:experiments}
We compare our models, DALUPI-TS and DALUPI-E2E, to baseline algorithms in three image classification tasks where privileged information is available as one or more bounding boxes highlighting regions of interest related to the image label. We use DALUPI-TS in a digit classification task in Section \ref{sec:exp_digit}. DALUPI-E2E is then used in the experiments in Section \ref{sec:coco} and \ref{sec:chest}, where we consider classification of entities from the MS-COCO dataset and pathologies in chest X-ray images. LUPI-E2E is included in the entity classification task.

As baselines, we use classifiers trained on labeled examples from the source and target domain, respectively. The latter serves as an oracle comparison since labels are unavailable from the target in our setting. Both models are ResNet-based convolutional neural networks adapted for the different experiments and we refer to them as SL-S and SL-T, respectively. We also compare our models to a domain-adversarial neural network (DANN) \cite{ganin_domain-adversarial_2016} trained on labeled data from the source domain and unlabeled data from the target domain.

For each experiment setting (task, skew, amount of privileged information, etc.), we train 10 models from each relevant model class using randomly selected hyperparameters. All models are evaluated on a hold-out validation set from the source domain and the best performing model in each class is then evaluated on hold-out test sets from both domains. For SL-T, we use a hold-out validation set from the target domain. We repeat this procedure over 5--10 seeds controlling the data splits and the random number generation in PyTorch, which is used in our implementation. We report \SI{95}{\percent} confidence intervals computed by bootstrapping over the seeds. We refer to Appendix \ref{app:expdetails} for further details on experiments. 
%
\subsection{Digit classification}
\label{sec:exp_digit}
%
We construct a synthetic image dataset, based on the assumptions of Proposition~\ref{prop:identification}, to verify that there are problems where domain adaptation using privileged information is guaranteed successful transfer but standard UDA is not.\\ 
We start with images from CIFAR-10~\citep{cifar10} which have been upscaled to $128\times128$. Then we insert a random  $28 \times 28$ digit image, with a label in the range 0--4, from the MNIST dataset~\citep{lecun_gradient-based_1998} into a random location of the CIFAR-10 image, forming the input image $X$ (see Figures~\ref{fig:saliencymniste02}--\ref{fig:saliencymniste10} for examples). The label $Y \in \{0,\dots,4\}$ is determined by the MNIST digit. We store the bounding box around the inserted digit image and use the pixels contained within it as privileged information $W$. This is done until no more digit images remain. The domains are constructed from using CIFAR-10's first five and last five classes as source and target  backgrounds, respectively. Both source and target datasets contain \SI{15298} images each. 

To increase the difficulty of the task, we make the digit be the mean colour of the dataset and make the digit background transparent so that the border of the image is less distinct. This may slightly violate Assumption~\ref{asmp:overlap} w.r.t. $W$ since the backgrounds differ between domains. To understand how successful transfer depends on domain overlap and access to sufficient privileged information, we include a \emph{skew parameter} $\epsilon\in [\frac{1}{c},1]$, where $c=5$ is the number of digit classes, which determines the correlation between digits and backgrounds. For a source image $i$ with digit label $Y_i \in \{0,...,4\}$, we select a random CIFAR-10 image with class $B_i \in\{0,\dots,4\}$ with probability
$$
P(B_i=b \mid Y_i = y) = \left\{
\begin{array}{ll}
\epsilon, & \mbox{if}\; b = y \\
(1-\epsilon)/(c-1), & \mbox{otherwise}
\end{array}
\right.
.
$$
For target images, digits and backgrounds are matched uniformly at random. Note that the choice of $\epsilon=\frac{1}{c}$ yields a uniform distribution and $\epsilon=1$ being equivalent to the background carrying as much signal as the privileged information. We hypothesise that the latter part would be the worst possible case where confusion of the model is likely, which would lead to poor generalization performance. 

In Figure \ref{fig:target}, we can indeed observe the conjectured behavior where, as the skew $\epsilon$ increases, the performance of SL-S decreases substantially on the target domain. Thus an association with the background might harm generalization performance which has been observed in previous work where background artifacts in X-ray images confused the resulting classifier. 

We can also observe that DANN does not seem to be robust to the increase in correlation between the label and the background. In contrast, DALUPI-TS is unaffected by the increased skew as the subset of pixels only carries some of the background information with it, while containing sufficient information to make good predictions. Interestingly, DALUPI-TS also seems to be as good or slightly better than the oracle SL-T in this setting. This may be due to improved sample efficiency from using PI.
\subsection{Entity classification} \label{sec:coco}
We evaluate DALUPI-E2E and LUPI-E2E on an entity classification task based on images from the MS-COCO dataset~\citep{lin2014microsoft}. We define the source and target domains as indoor and outdoor images, respectively, and consider four entity classes for the label $Y$: person, cat, dog, and bird. We extract indoor images by filtering out images from the super categories ``indoor'' and ``appliance'' that also contain at least one of the entity classes. Outdoor images are extracted in the same way using the super categories ``vehicle'' and ``outdoor''. Images that for some reason occur in both domains (for example an indoor image with a toy car) are removed along with any gray-scale images. We also include 1000 background examples, i.e., images with none of the entities present, in both domains. 

In total, there are \SI{5231} images in the source domain and \SI{5719} images in the target domain; the distribution of labels are provided in Appendix \ref{app:expdetails}.
From these pools we randomly sample \SI{3000}, \SI{1000}, and \SI{1000} images for training, validation, and testing, respectively. As privileged information $W$, we use bounding boxes localizing the different entities. We assume that the assumptions of Proposition~\ref{prop:identification} hold. In particular, it is reasonable to assume that the pixels contained in a box are sufficient for classifying the object.

The purpose of this experiment is to investigate the effect of additional privileged information (PI). We give LUPI-E2E access to all $(x, y)$ samples from the source domain and increase the fraction of inputs for which PI is available, $n_{\text{PI}}(\mathcal{S})$, from 0 to 1. For DALUPI-E2E, we increase the fraction of $(\tilde{x}, \tilde{w})$ samples from the target domain, $n_{\text{PI}}(\mathcal{T})$, from 0 to 1, while keeping $n_{\text{PI}}(\mathcal{S})=1$. We train SL-S and SL-T using all available data and increase the fraction of unlabeled target samples DANN sees from 0.2 to 1.

The models' test AUC, averaged over the four entity classes, in the target domain is shown in Figure \ref{fig:cocoexp}. The performance of LUPI-E2E increases as $n_{\text{PI}}(\mathcal{S})$ increases, but even with access to all PI in the source domain, LUPI-E2E barely beats SL-S. However, when additional $(\tilde{x}, \tilde{w})$ samples from the target domain are added, DALUPI-E2E quickly outperforms SL-S and eventually reaches the performance of SL-T. We note that DANN does not benefit as much from the added unlabeled target samples as DALUPI-E2E does. The large gap between LUPI-E2E and SL-S for $n_{\text{PI}}(\mathcal{S})=0$ is not too surprising; we do not expect an object detector to work well without bounding box supervision.
\begin{figure}[t]
\centering
\includegraphics[width=.97\columnwidth]{fig/coco_target.pdf}%
\caption{Model performance in the target domain for the entity classification task. The point performance of SL-S and SL-T is extended across the x-axes for visual purposes. DANN is trained with an increasing fraction of target samples $\tilde{x}$ but uses no PI. \label{fig:cocoexp}}%
\end{figure}
\subsection{X-ray classification} \label{sec:chest}
As a real-world experiment, we study detection of pathologies in chest X-ray images. We use the ChestX-ray8 dataset \cite{nih} as source domain and the CheXpert dataset \cite{irvin2019chexpert} as target domain. As privileged information, we use bounding boxes localizing each pathology. For the CheXpert dataset, only pixel-level segmentations of pathologies are available, and we create bounding boxes that tightly enclose the segmentations. It is not obvious that the pixels within such a bounding box are sufficient for classifying the pathology, and we suspect that we may violate the assumptions of Proposition~\ref{prop:identification} in this experiment. However, as we find below, DALUPI-E2E improves empirical performance compared to baselines for small training sets, thereby demonstrating increased sample efficiency.

We consider the three pathologies that exist in both datasets and for which there are annotated findings: atelectasis (ATL; collapsed lung), cardiomegaly (CM; enlarged heart), and pleural effusion (PE; water around the lung). 
There are 457 and 118 annotated images in the source and target domain, respectively. We train DALUPI-E2E and DANN using all these images. SL-S is trained with the 457 source images and SL-T with the 118 target images as well as 339 labeled but non-annotated target images. Neither SL-S, SL-T nor DANN use any privileged information. In the annotated images, there are 180/146/153 and 75/66/64 examples of ATL/CM/PE in each domain respectively. Validation and test sets are sampled from the non-annotated images.

In Table \ref{tab:chestxray0} we present the per-class AUCs in the target domain. DALUPI-E2E outperforms all baseline models, including the target oracle, in detecting CM. For ATL and PE, it performs similarly to the other feasible models. That SL-T is better at predicting PE is not surprising given that this pathology is most prevalent in the target domain. In Figure \ref{fig:saliencychest1}, we show a single-finding image from the target test set with ground-truth label CM. The predicted bounding box of DALUPI-E2E with the highest probability is added to the image. DALUPI-E2E identifies the region of interest and makes a correct classification. The bottom panel shows the saliency map for the ground truth class for SL-S. We see that the gradients are mostly constant, indicating that the model is uncertain. In Figure \ref{fig:saliencychest2}, we show AUC for CM for DALUPI-E2E, SL-S, and SL-T trained with additional examples \emph{without} bounding box annotations. We see that SL-S catches up to the performance of DALUPI-E2E when a large amount of labeled examples are provided. These results indicate that identifiability is not the primary obstacle for adaptation, and that PI improves sample efficiency.
\begin{table}[t]
\caption{X-ray task. Test AUC for the three pathologies in the target domain for all considered models. Boldface indicates the best-performing feasible model; SL-T uses target labels.}
\label{tab:chestxray0}
\centering
\resizebox{0.88\textwidth}{!}{\begin{minipage}{\textwidth}
\begin{tabular}{lccc}
\hline
             & ATL                             & CM                              & PE                              \\ \hline
SL-T         & \multicolumn{1}{l}{57 (56, 58)} & \multicolumn{1}{l}{59 (55, 63)} & \multicolumn{1}{l}{79 (78, 80)} \\ \hline
SL-S         & 55 (55, 56)                     & 61 (58, 64)                     & 73 (70, 75)                     \\
DANN         & \textbf{56 (54, 57)}            & 61 (56, 66)                     & 73 (71, 75)                     \\
DALUPI-E2E & 55 (55, 56)                     & \textbf{72 (71, 73)}            & \textbf{74 (72, 76)}            \\ \hline
\end{tabular}
\end{minipage}}
\end{table}
%
\begin{figure}[t!]
\centering
\begin{subfigure}[b]{0.22\textwidth}
\centering
\includegraphics[width=0.75\linewidth, height=12em]{fig/lupi_vs_source.pdf}
\caption{DALUPI-E2E vs SL-S.}\label{fig:saliencychest1}
\end{subfigure}%
\begin{subfigure}[b]{0.27\textwidth}
\centering
\lineskip=0pt
\includegraphics[width=\linewidth]{fig/chestxray_CM_target.pdf}
\caption{Test AUC for CM in $\mathcal{T}$.}\label{fig:saliencychest2}
\end{subfigure}%
\caption{X-ray task. (a): An example image from the target test set with label CM. The red rectangle indicates the bounding box with the highest probability as predicted by DALUPI-E2E with no extra data. The saliency map for CM for the corresponding SL-S is shown below. (b): AUC for CM in the target test set for SL-S, SL-T, and DALUPI-E2E trained with additional $(x, y)$ samples. \label{fig:chestexp}}
\end{figure}
%
% RELATED WORK
%
\section{Related work}
\label{sec:related}
%
\input{literature}
%
% DISCUSSION
%
\section{Discussion}
\label{sec:discussion}
%
We have presented our framework DALUPI which leverages privileged information for unsupervised domain adaptation. The framework builds on an alternative set of assumptions which can more realistically be satisfied in high-dimensional problems than classical assumptions of covariate shift and domain overlap, at the cost of collecting a larger variable set for training. Our analysis of this setting inspired practical algorithms for multi-class and multi-label image classification.

Our experiments show tasks where domain adaptation using privileged information is successful while regular adaptation methods fail, in particular when the assumptions of our analysis are satisfied. We observe empirically also that methods using privileged information are more sample-efficient than comparable non-privileged learners, in line with the literature. In fact, DALUPI models occasionally even outperform oracle models trained using target labels due to their sample efficiency. Thus, we recommend considering these methods in small-sample settings.

To avoid assuming that domain overlap is satisfied with respect to input covariates, we require that privileged information is sufficient to determine the label. This can be limiting in problems where sufficiency is difficult to verify. However, in our motivating example of image classification, a domain expert could deliberately choose PI so that sufficiency is reasonably satisfied.  In future work, the framework might be applied to a more diverse set of tasks, with different modalities to investigate if the findings here can be replicated. Using PI may be viewed as ``building in'' domain-knowledge in the structure of the adaptation problem and we see this as promising approach for further UDA research.
%
\bibliographystyle{icml2022}
\bibliography{Privileged_information}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Experimental details} \label{app:expdetails}
%% 
In this section give further details of the experiments. All code is written in Python and we use PyTorch in combination with skorch \cite{skorch} for our implementations of the networks. For Faster R-CNN, we adapt the implementation provided by torchvision through the function \verb|fasterrcnn_resnet50_fpn|. DANN is implemented with different optimizers for the discriminator and the generator. Here, the generator is the main network consisting of a ResNet-based encoder and a classifier network. We set the parameter $\lambda$, which controls the amount of domain adaption regularization, to a fixed value 0.1 in all experiments. We add a gradient penalty to the discriminator loss which we control with the parameter ``gradient penalty'' specified in each experiment. We also include a parameter controlling the number of discriminator steps per generator steps (default 1). The source and target baselines are based on the ResNet-50 architecture if nothing else is stated. All models are trained using NVIDIA Tesla A40 GPUs. In experiment, we use the Adam optimizer. The code will be made available.

\subsection{Digit classification}
We use \SI{20}{\percent} of the available source and target data in the test set. We likewise use \SI{20}{\percent} of the training data for validation purposes.
For the baselines SL-S and SL-T we use a ResNet-18 network without pretrained weights. We change the final fully connected layer from stock to the following sequence:
\begin{itemize}
    \item fully connected layer with 256 neurons
    \item batch normalization layer
    \item dropout layer with $p=0.2$
    \item fully connected layer with 128 neurons
    \item fully connected layer with 5 neurons.
\end{itemize}
For DALUPI-TS we use a non-pretrained ResNet-18 for the $\hat{f}$ function where we replace the default fully connected layer with a fully connected layer with 4 neurons to predict the coordinates of the bounding box. The predicted bounding box is resized to a $28\times28$ square no matter the initial size. We use a simpler convolutional neural network for the $\hat{g}$ function with the structure as follows:
\begin{itemize}
    \item convolutional layer with 16 out channels, kernel size of 5, stride of 1, and padding of 2
    \item max pooling layer with kernel size 2, followed by a ReLU activation
    \item convolutional layer with 32 out channels, kernel size of 5, stride of 1, and padding of 2
    \item max pooling layer with kernel size 2, followed by a ReLU activation
    \item dropout layer with  $p=0.4$
    \item fully connected layer with 50 neurons
    \item dropout layer with  $p=0.2$
    \item fully connected layer with 5 neurons.
\end{itemize}
The training is stopped when the best validation error does not improve over 5 epochs or when 100 epochs have been trained, whichever occurs first. The training of the DANN model is stopped when the validation loss has not improved over 10 epochs. DANN is pretrained on ImageNet.
\subsubsection{Hyperparameters}
We randomly choose hyperparameters from the following predefined sets of values:
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04}, \SI{5.0e-04}, \SI{1.0e-03})
        \item weight decay: (\SI{1.0e-06}, \SI{1.0e-05},  \SI{1.0e-04}, \SI{1.0e-03}).
    \end{itemize}
    \item DALUPI-TS
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04}, \SI{5.0e-04}, \SI{1.0e-03}).
    \end{itemize}
    \item DANN
    \begin{itemize}
     \item batch size: $(64)$
        \item learning rate: (\SI{1.0e-03})
        \item gradient penalty: $(0, 0.01, 0.1)$
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item number of trainable layers (encoder): $(1, 2, 3)$
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item width of discriminator network: $(64, 128, 256)$
        \item depth of discriminator network: $(2, 3)$.
    \end{itemize}
\end{itemize}


\subsection{Entity classification}

In the entity classification experiment, we train all models for at most 50 epochs. If the validation AUC does not improve for 10 subsequent epochs, we stop the training earlier. No pretrained weights are used in this experiment since we find that the task is too easy to solve with pretrained weights. For DALUPI-E2E and LUPI-E2E we use the default anchor sizes for each of the feature maps (32, 64, 128, 256, 512), and for each anchor size we use the default aspect ratios (0.5, 1.0, 2.0). We use the binary cross entropy loss for SL-S, SL-T, and DANN.

We use the 2017 version of the MS-COCO dataset \cite{lin2014microsoft}. In Table \ref{tab:cocolabels} we describe the label distribution in the defined source and target domains, respectively. How the domains are defined are described in Section \ref{sec:coco}.
\begin{table}[h]
\centering
\caption{Marginal label distribution in source and target domains for the entity classification task based on the MS-COCO dataset. The background class contains images where none of the four entities are present.}
\label{tab:cocolabels}
\begin{tabular}{lllllll}
\toprule
Domain & Person & Dog  & Cat  & Bird & Background \\ \midrule
Source & 2963   & 569  & 1008 & 213  & 1000       \\
Target & 3631   & 1121 & 423  & 712  & 1000       \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Hyperparameters}
We randomly choose hyperparameters from the following predefined sets of values. For information about the specific parameters in LUPI-E2E and DALUPI-E2E, we refer to the paper by \citet{ren2016faster}. RoI and NMS refer to region of interest and non-maximum suppression, respectively.
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04}, \SI{5.0e-04}, \SI{1.0e-03})
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item dropout (encoder): (0, 0.1, 0.2, 0.3).
    \end{itemize}

    \item DANN:
    \begin{itemize}
       \item batch size: $(16, 32, 64)$
       \item learning rate: (\SI{1.0e-04}, \SI{5.0e-04}, \SI{1.0e-03})
       \item discriminator updates per generator updates: (1, 2)
       \item gradient penalty: (0, 0.01, 0.1)
       \item weight decay (generator): (\SI{1.0e-4}, \SI{1.0e-3})
       \item weight decay (discriminator): (\SI{1.0e-4}, \SI{1.0e-3})
       \item number of trainable layers (encoder): (1, 2, 3, 4, 5)
       \item dropout (encoder): (0, 0.1, 0.2, 0.3)
       \item width of discriminator network: (64, 128, 256)
       \item depth of discriminator network: (2, 3)
       \item nonlinear classifier: (\verb|True|, \verb|False|).
    \end{itemize}
    \item LUPI-E2E and DALUPI-E2E:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item IoU foreground threshold (RPN): (0.6, 0.7, 0.8, 0.9)
        \item IoU background threshold (RPN): (0.2, 0.3, 0.4)
        \item batchsize per image (RPN): (32, 64, 128, 256)
        \item fraction of positive samples (RPN): (0.4, 0.5, 0.6, 0.7)
        \item NMS threshold (RPN): (0.6, 0.7, 0.8)
        \item RoI pooling output size (Fast R-CNN): (5, 7, 9)
        \item IoU foreground threshold (Fast R-CNN): (0.5, 0.6)
        \item IoU background threshold (Fast R-CNN): (0.4, 0.5)
        \item batchsize per image (Fast R-CNN): (16, 32, 64, 128)
        \item fraction of positive samples (Fast R-CNN: (0.2, 0.25, 0.3)
        \item NMS threshold (Fast R-CNN): (0.4, 0.5, 0.6)
        \item detections per image (Fast R-CNN): (25, 50, 75, 100).
    \end{itemize}

\end{itemize}

\subsection{X-ray classification}

In the X-ray classification experiment, we train all models for at most 50 epochs. If the validation AUC does not improve for 10 subsequent epochs, we stop the training earlier. We then fine-tune all models, except DANN, for up to 20 additional epochs. The number of encoder layers that are fine-tuned is a hyperparameter; we consider different values here. We start the training with weights pretrained on ImageNet. For DALUPI-E2E and LUPI-E22 we use the default anchor sizes for each of the feature maps (32, 64, 128, 256, 512), and for each anchor size we use the default aspect ratios (0.5, 1.0, 2.0). We use the binary cross entropy loss for SL-S, SL-T, and DANN.

\begin{table}[h]
\caption{Marginal distribution of labels of images and bounding boxes in the source and target domain, respectively, for the chest X-ray classification experiment. ATL=Atelectasis; CM=Cardiomegaly; PE=Effusion; NF=No Finding. \label{tab:chestxraylabels}}
\centering
\begin{tabular}{lllll}
\toprule
Data                             & ATL   & CM    & PE    & NF \\ \midrule
$x\sim\mathcal{S}$                  & 11559 & 2776  & 13317 & 60361 \\
$w\sim\mathcal{S}$ & 180   & 146   & 153   & -     \\ \midrule
$\tilde{x}\sim\mathcal{T}$                  & 14278 & 20466 & 74195 & 16996 \\
$\tilde{w}\sim\mathcal{T}$ & 75    & 66    & 64    & -     \\ \bottomrule
\end{tabular}
\end{table}

In total, there are \SI{83 519} (457) and \SI{120435} (118) images (annotated images) in the source and target domain, respectively. The marginal label distributions are shown in Table \ref{tab:chestxraylabels}. Here, ``NF'' refers to images with no confirmed findings. In both domains, we randomly sample \SI{10000} samples for training and validation and leave the rest of the data in a training pool. All annotated images are reserved for training. We merge the default training and validation datasets before splitting the data. For the source dataset (ChestX-ray8), the bounding boxes can be found together with the dataset. The target segmentations can be found here: \url{https://stanfordaimi.azurewebsites.net/datasets/23c56a0d-15de-405b-87c8-99c30138950c}. 

\subsubsection{Hyperparameters}
We choose hyperparameters randomly from the following predefined sets of values. For information about the specific parameters in DALUPI-E2E, we refer to the paper by \citet{ren2016faster}. RoI and NMS refer to region of interest and non-maximum suppression, respectively. We do not perform any fine-tuning of DANN but we decay the learning rate with a factor 10 every $n$th epoch, where the step size $n$ is a hyperparameter included below. Note that $n=100$ means no learning rate decay.
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: (16, 32, 64)
        \item learning rate: (\SI{1.0e-4}, \SI{0.5e-3}, \SI{1.0e-3})
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item dropout: (0, 0.1, 0.2, 0.3)
        \item learning rate (fine-tuning): (\SI{1.0e-5}, \SI{1.0e-4})
        \item number of layers to fine-tune: (3, 4, 5).
    \end{itemize}

    \item DANN:
    \begin{itemize}
       \item batch size: $(16, 32, 64)$
       \item learning rate: (\SI{1.0e-04}, \SI{5.0e-04}, \SI{1.0e-03})
       \item discriminator updates per generator updates: (1, 2)
       \item gradient penalty: (0, 0.01, 0.1)
       \item weight decay (generator): (\SI{1.0e-4}, \SI{1.0e-3})
       \item weight decay (discriminator): (\SI{1.0e-4}, \SI{1.0e-3})
       \item number of trainable layers (encoder): (1, 2, 3, 4, 5)
       \item dropout (encoder): (0, 0.1, 0.2, 0.3)
       \item width of discriminator network: (64, 128, 256)
       \item depth of discriminator network: (2, 3)
       \item nonlinear classifier: (\verb|True|, \verb|False|)
       \item step size $n$ for learning rate decay: (15, 30, 100).
    \end{itemize}
    \item LUPI-E2E and DALUPI-E2E:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item IoU foreground threshold (RPN): (0.6, 0.7, 0.8, 0.9)
        \item IoU background threshold (RPN): (0.2, 0.3, 0.4)
        \item batchsize per image (RPN): (32, 64, 128, 256)
        \item fraction of positive samples (RPN): (0.4, 0.5, 0.6, 0.7)
        \item NMS threshold (RPN): (0.6, 0.7, 0.8)
        \item RoI pooling output size (Fast R-CNN): (5, 7, 9)
        \item IoU foreground threshold (Fast R-CNN): (0.5, 0.6)
        \item IoU background threshold (Fast R-CNN): (0.4, 0.5)
        \item batchsize per image (Fast R-CNN): (16, 32, 64, 128)
        \item fraction of positive samples (Fast R-CNN: (0.2, 0.25, 0.3)
        \item NMS threshold (Fast R-CNN): (0.4, 0.5, 0.6)
        \item detections per image (Fast R-CNN): (25, 50, 75, 100)
        \item learning rate (fine-tuning): (\SI{1.0e-5}, \SI{1.0e-4})
        \item number of layers to fine-tune: (3, 4, 5).
    \end{itemize}
\end{itemize}
%
\section{Additional results for chest X-ray experiment}
\label{app:additional_x-ray}

\begin{figure*}[t!]
\centering
\begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{fig/chestxray_source.pdf}
    \caption{Source test AUC.}
    \label{fig:chestsource_app}
\end{subfigure}%
\begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{fig/chestxray_target.pdf}
    \caption{Target test AUC.}
    \label{fig:chesttarget_app}
\end{subfigure}%
\caption{Classification of chest X-ray images. Model performance on source (a) and target (b) domains. The AUC is averaged over the three pathologies: ATL, CM and PE.
The \SI{95}{\percent} confidence intervals are computed using bootstrapping over 5 seeds. \label{fig:chestexp_app}}
\end{figure*}

In Figure \ref{fig:saliencychest2} in Section \ref{sec:chest}, we present AUC for the pathology CM when additional training data are added. In Figure \ref{fig:chestexp_app}, we show the \emph{average} AUC when additional training data of up to \SI{30000} samples are added. We see that, once given access to a much larger amount of labeled samples, SL-S and DALUPI-E2E perform comparably in the target domain.

\section{Proof of Proposition 2}
\label{app:proof}

\paragraph{Proposition 2.}
\emph{
Assume that $\cG$ comprises M-Lipschitz mappings from the privileged information space $\mathcal{W} \subseteq \bbR^{d_W}$ to $\cY$. Further, assume that both the ground truth privileged information $W$ and label $Y$ are deterministic in $X$ and $W$ respectively. 
Let $\rho$ be the domain density ratio of $W$ and let Assumptions~\ref{asmp:covshift}--\ref{asmp:sufficiency} (Covariate shift, Overlap and Sufficiency) hold w.r.t. $W$. Further, let the loss $L$ be uniformly bounded by some constant $B$ and let $d$ and $d'$ be the pseudo-dimensions of $\mathcal{G}$ and $\mathcal{F}$ respectively. Assume that there are $n$ observations each from the source (labeled) and target (unlabeled) domains. Then, with $L$ the squared Euclidean distance, for any $h = f \circ g \in \cF \times \cG$, w.p. at least $1-\delta$,
}
\begin{align*}
\frac{R_\cT(h)}{2} & \leq \hat{R}^{Y,\rho}_\cS(g) +M^2 \hat{R}^W_\cT(f)  \\ 
& + 2^{5/4}\sqrt{d_2(\cT\|\cS)}\sqrt[\frac{3}{8}]{\frac{d\log \frac{2ne}{d}+\log \frac{4}{\delta}}{n}} \\
& + d_\cW BM^2 \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{d_{\cW}}{\delta}}{2n}} \right).
\end{align*}

\begin{proof}Decomposing the risk of $h \circ \phi$ , we get 
\begin{align*}
& R_\cT(h) = \E_\cT[(g(f(X)) - Y)^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2 + (g(f(X)) - g(W))^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2 + M^2\|f(X)) - g(W)\|^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2] +  2M^2\E_\cT[\|(f(X) - W)\|^2] \\
& = 2R^Y_\cT(g) +  2M^2R^W_\cT(f) =2\underbrace{R^{Y,\rho}_S(g)}_{(I)} +  2M^2\underbrace{R^W_\cT(f)}_{(II)}.
\end{align*}
The first inequality follows from the relaxed triangle inequality, the second inequality from the Lipschitz property and the third equality from Overlap and Covariate shift. We will bound these quantities separately starting with $(I)$. 

 We assume that the pseudo-dimension of $\cG$, $d$ is bounded. Further, we assume that the second moment of the density ratios, equal to the R\'enyi divergence $d_2(\cT \| \cS)=\Sigma_{w\in cG} \cT(w) \frac{\cT(w)}{\cS(w)}$ are bounded and that the density ratios are non-zero for all $w\in \cG$. Let $D_1=\{w_i,y_i\}_{i=0}^{n}$ be a dataset drawn i.i.d from the source domain. Then by application of Theorem 3 from \citet{cortes2010} we obtain with probability $1-\delta$ over the choice of $D_1$,
$$
(I)=R^{Y,\rho}_S(g)\leq   \hat{R}^{Y,\rho}_S(g) + 2^{5/4}\sqrt{d_2(\cT\|\cS)}\sqrt[3/8]{\frac{d\log \frac{2ne}{d}+\log \frac{4}{\delta}}{n}}
$$
Now for $(II)$ we treat each component of $w\in \cW$ as a regression problem independent from all the others. So we can therefore write the risk as the sum of the individual component risks
$$
R^W_\cT(f)=\Sigma_{i=1}^{d_\cW} R^W_{\cT,i}(f)
$$
Let the pseudo-dimension of $\cF$ be denoted $d$, $D_2=\{x_i,w_i\}_{i=0}^{n}$ be a dataset drawn i.i.d from the target domain. Then, using theorem 11.8 from \citet{foml_mohri} we have that for any
$\delta>0$, with probability at least $1-\delta$ over the choice of $D_2$, the following inequality holds for all hypotheses $f\in \cF$ for each component risk
\begin{align*}
R^W_{\cT,i}(f)& \leq \hat{R}^W_{\cT,i}(f) + B \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{1}{\delta}}{2n}}\right) \\
\end{align*}
We then simply make all the bounds hold simultaneously by applying the union bound and having it so that each bound must hold with probability $1-\frac{\delta}{d_\cW}$ which results in 
\begin{align*}
R^W_\cT(f)&=\Sigma_{i=1}^{d_\cW} R^W_{\cT,i}(f) \leq \Sigma_{i=1}^{d_\cW} \hat{R}^W_{\cT,i}(f) + \Sigma_{i=1}^{d_\cW} B \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{d_\cW}{\delta}}{2n}}  \right ) \\ &=\hat{R}^W_{\cT}(f) + d_\cW B \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{d_\cW}{\delta}}{2n}} \right)
\end{align*}
Combination of these two results then yield the proposition statement.

Consistency follows as $Y$ is a deterministic function of $W$ and $W$ is a deterministic fundtion of $X$ and both $\cH$ and $\cF$ are well-specified. Thus both empirical risks and sample complexity terms will converge to 0 in the limit of infinite samples. 
\end{proof}
\section{t-SNE plot for digit classification experiment}
We present some t-SNE plots for the SL-S model for the digit classification experiment in Figure \ref{fig:tsnemnist} and \ref{fig:tsnemnist2}. We clearly see that when the skew is low the target is well separated. However, with full correlation between the label and the background we see that the separation is lost. The perplexity is set to 100 when producing these plots.

\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.48\textwidth}
    \centering
\includegraphics[width=.92\textwidth]{fig/tsne_source_0.2.png}
        \caption{t-SNE plot of the source test set, $\epsilon=0.2$.}
        \label{fig:source0.2tsne}
    \end{subfigure}%
        \begin{subfigure}{0.48\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{fig/tsne_target_0.2.png}
        \caption{t-SNE plot of the target test set, $\epsilon=0.2$.}
        \label{fig:target0.2tsne}
    \end{subfigure}%
    \caption{t-SNE when the skew is $\epsilon=0.2$.}
    \label{fig:tsnemnist}
\end{figure*}
\begin{figure*}[t!]
    \centering
      \begin{subfigure}{0.48\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{fig/tsne_source_1.png}
        \caption{t-SNE plot of the source test set, $\epsilon=1$.}
        \label{fig:source1tsne}
    \end{subfigure}%
        \begin{subfigure}{0.48\textwidth}
    \centering
        \includegraphics[width=.92\textwidth]{fig/tsne_target_1.png}
        \caption{t-SNE plot of the target test set, $\epsilon=1$.}
        \label{fig:target1tsne}
    \end{subfigure}%
    \caption{t-SNE when the skew is $\epsilon=1$.}
    \label{fig:tsnemnist2}
\end{figure*}
\end{document}
