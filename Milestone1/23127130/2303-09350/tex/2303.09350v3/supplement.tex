% \documentclass[twoside]{article}

% \usepackage{aistats2024}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%
%\usepackage[accepted]{aistats2024}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

% \begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.

%\onecolumn


\section{Experimental Details} \label{app:expdetails}
%% 
In this section, we give further details of the experiments. All code is written in Python and we mainly use PyTorch in combination with skorch \cite{skorch} for our implementations of the networks. For Faster R-CNN, we adapt the implementation provided by torchvision through the function \verb|fasterrcnn_resnet50_fpn|. For DANN and MDD, we use the ADAPT TensorFlow implementation \citep{de2021adapt} with a ResNet-50-based encoder.
%except in the multi-class experiment where we use our own implementation of DANN in PyTorch.
We initially set the trade-off parameter $\lambda$, which controls the amount of domain adaption regularization, to 0 and then increase it to 0.1 in \SI{10000} gradient steps according to the formula $\lambda = \beta(2/(1 + e^{-p}) - 1) / C$, where $p$ increases linearly from 0 to 1, $\beta$ is a parameter specified for each experiment, and $C = 2/(1 + e^{-1}) - 1$. For MDD, we fix the margin parameter $\gamma$ to 3. The source and target baselines are based on the ResNet-50 architecture if nothing else is stated.

We use the Adam optimizer in all experiments. Learning rate decay is treated as a hyperparameter. For ADAPT models (DANN and MDD), the learning rate is either constant or decayed according to $\mu_0 / (1 + \alpha p)^{3/4}$, where $\mu_0$ is the initial learning rate, $p$ increases linearly from 0 to 1, and $\alpha$ is a parameter specified in each experiment (see below). For non-ADAPT models, the learning rate is either constant or decayed by a factor $0.1$ every $n$th epoch, where $n$ is another hyperparameter.

All models were trained using NVIDIA Tesla A40 GPUs and the  development and evaluation of this study required approximately \SI{30000} hours of GPU training. The code will be made available.

For all models except LUPI and DALUPI, the classifier network following the encoder is a simple MLP with two settings: either it is a single linear layer from inputs to outputs or a three-layer network with ReLU activations between the layers. This choice is treated as a hyperparameter in our experiments. The nonlinear case has the following structure where $n$ is the number of input features:
\begin{itemize}
    \item fully connected layer with $n$ neurons
    \item ReLU activation layer
    \item fully connected layer with $n//2$ neurons
     \item ReLU activation layer
    \item fully connected layer with $n//4$ neurons.
\end{itemize}

\subsection{DALUPI With Two-stage Classifier}
\label{app:twostage}
Here, we describe in more detail how we construct our two-stage classifier for the digit classification task. 
%The privileged information $W$ highlights a region of interest in the image $X$, related to the label $Y$, in the form of pixels contained by bounding boxes with coordinates $T$. 
Each image $x_i$ has a single label $y_i \in \{0, \ldots, 4\}$ determined by the MNIST digit. Privileged information is given by a single bounding box with coordinates $t_i\in\bbR^{4}$ enclosing a subset of pixels $w_i$ corresponding to the digit.

We first learn $\hat{d}$ which is a function that takes target image data, $\tilde{x}_i$, and bounding box coordinates, $t_i$, and learns to output bounding box coordinates, $\hat{t}_i$, which should contain the privileged information $w_i$. Note that we do not exactly follow the setup in \eqref{eq:twostage_erm} since we do not need to actually predict the pixel values within the bounding box. If we find a good enough estimator of $t_i$ we should minimize the loss of $f$ in \eqref{eq:twostage_erm}. To obtain the privileged information we apply a deterministic function $\phi$ which crops and scales an image using the associated bounding box, $t_i$. We can now write the composition of these two functions as $\hf(x_i)=\phi(x_i,\hat{d}(x_i))$ which outputs the privileged information. The function $\phi$ is hard-coded and therefore not learned.

In the second step, we train $\hg$ by learning to predict the label from the privileged information $w_i$, which is a cropped version of $x_i$ where the cropping is defined by the bounding box $t_i$ around the digit. This cropping and resizing is performed by $\phi$. When we evaluate the performance of this classifier we combine the two models into one, $\hh(x) = \hg(\phi(x,\hat{d}(x)))$. We use the mean squared error loss for learning $\hat{d}$ and categorical cross-entropy (CCE) loss for $\hg$.

The entire training procedure is described in Algorithm \ref{alg:two-stage}. Finally, $\hh$ is evaluated on target domain images where the output of $\phi(x,\hf(x))$ is used for prediction with $\hg$.
%%% here we have an algorithm block describing the 
\begin{algorithm}
\caption{Training of the two-stage model.}
\label{alg:two-stage}
\begin{algorithmic}[1]

\Procedure{Two\_stage }{$\tilde{x}_i,w_i,t_i,y_i$}       %\Comment{This is a test}
        \State Empirically minimize $\frac{1}{m}\sum_{i=1}^{m}\|d(\tilde{x}_i)-t_i\|^2$  and obtain $\hat{d}$.
    
        \State Empirically minimize $\frac{1}{n}\sum_{i=1}^{n} CCE(g(w_i),y_i)$ and obtain $\hat{g}$.

        \State Compose $\hat{d}$, $\hat{g}$ and $\phi$ into $\hh(x)=\hg(\phi(x,\hat{d}(x)))$.
\EndProcedure

\end{algorithmic}
\end{algorithm}
%\hltodo{Add algorithm description with empirical risks and relation to (2). Make the relation between $t$ and $w$ as clear as possible.}
%

\subsection{DALUPI With Faster R-CNN}
\label{app:faster-rcnn}
For multi-label classification, we adapt Faster R-CNN \citep{ren2016faster} outlined in Figure \ref{fig:fasterrcnn} and described below. Faster R-CNN uses a region proposal network (RPN) to generate region proposals which are fed to a detection network for classification and bounding box regression. This way of solving the task in subsequent steps has similarities with our two-stage algorithm although Faster R-CNN can be trained end-to-end. We make small modifications to the training procedure of the original model in the end of this section.
%
\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{fig/fasterrcnn.pdf}
\caption{Faster R-CNN~\citep{ren2016faster} architecture. The RoI pooling layer and the classification and regression layers are part of the Fast R-CNN detection network \citep{girshick2015fast}.}
\label{fig:fasterrcnn}
\end{figure}

The RPN generates region proposals relative to a fixed number of reference boxes---anchors---centered at the locations of a sliding window moving over convolutional feature maps. Each anchor is assigned a binary label $p\in\{0, 1\}$ based on its overlap with ground-truth bounding boxes; positive anchors are also associated with a ground-truth box with location $t$. The RPN loss for a single anchor is
\begin{equation}
    L^{\text{RPN}}(\hat{p}, p, \hat{t}, t) := 
    L_{\text{cls}}^{\text{RPN}}(\hat{p}, p) +
    pL_{\text{reg}}^{\text{RPN}}(\hat{t}, t),
    \label{eq:rpn}
\end{equation}
where $\hat{t}$ represents the refined location of the anchor and $\hat{p}$ is the estimated probability that the anchor contains an object. The binary cross-entropy loss and a smooth $L_{1}$ loss are used for the classification loss $L^{\text{RPN}}_{\text{cls}}$ and the regression loss $L^{\text{RPN}}_{\text{reg}}$, respectively. For a mini-batch of images, the total RPN loss is computed based on a subset of all anchors, sampled to have a ratio of up to 1:1 between positive and negative ditto.

A filtered set of region proposals are projected onto the convolutional feature maps. For each proposal, the detection network---Fast R-CNN \cite{girshick2015fast}---outputs a probability $\hat{p}(k)$ and a predicted bounding box location $\hat{t}(k)$ for each class $k$. Let $\hat{p}=(\hat{p}(0), \ldots, \hat{p}(K))$, where $\sum_{k}\hat{p}(k) =1$, $K$ is the number of classes and $0$ represents a catch-all background class. For a single proposal with ground-truth coordinates $t$ and multi-class label $u\in\{0, \ldots, K\}$, the detection loss is
\begin{equation}
    L^{\text{det}}(\hat{p}, u, \hat{t}_{u}, t) = 
    L_{\text{cls}}^{\text{det}}(\hat{p}, u) +
    \mathbf{I}_{u \geq 1}L_{\text{reg}}^{\text{det}}(\hat{t}_u, t),
    \label{eq:roi}
\end{equation}
where $L_{\text{cls}}^{\text{det}}(\hat{p}, u)=-\log{\hat{p}(u)}$ and $L_{\text{reg}}^{\text{det}}$ is a smooth $L_{1}$ loss. To obtain a probability vector for the entire image, we maximize, for each class $k$, over the probabilities of all proposals.

During training, Faster R-CNN requires that all input images $x$ come with at least one ground-truth annotation (bounding box) $w$ and its corresponding label $u$. To increase sample-efficiency, we enable training the model using non-annotated but labeled samples $(x, y)$ from the source domain and annotated but unlabeled samples $(\tilde{x}, \tilde{w})$ from the target domain. In the RPN, no labels are needed, and we simply ignore anchors from non-annotated images when sampling anchors for the loss computation. For the computation of \eqref{eq:roi}, we handle the two cases separately. We assign the label $u=-1$ to all ground-truth annotations from the target domain and multiply $L_{\text{cls}}^{\text{det}}$ by the indicator $\mathbf{I}_{u \geq 0}$. For non-annotated samples $(x, y)$ from the source domain, there are no box-specific coordinates $t$ or labels $u$ but only the labels $y$ for the entire image. In this case, \eqref{eq:roi} is undefined and we instead compute the binary cross-entropy loss between the per-image label and the probability vector for the entire image.

We train the RPN and the detection network jointly as described in \cite{ren2016faster}. To extract feature maps, we use a Feature Pyramid Network \cite{lin2017fpn} on top of a ResNet-50 architecture \cite{resnet}. We use the modfied model---DALUPI---in the experiments in Section \ref{sec:coco} and \ref{sec:chest}. In Section \ref{sec:coco}, we also train this model in a LUPI setting, where no information from the target domain is used.

\subsection{Digit Classification With Single Bounding Box as PI} \label{app:digit}
In this experiment, we separate \SI{20}{\percent} of the available source and target data into a test set. We likewise use \SI{20}{\percent} of the training data for validation purposes.
For the baselines SL-S and SL-T we use a ResNet-18 network without pretrained weights.
For DALUPI we use a non-pretrained ResNet-18 for the function $\hat{f}$ where we replace the default fully connected layer with a fully connected layer with 4 neurons to predict the coordinates of the bounding box. The predicted bounding box is resized to a $28\times28$ square no matter the initial size. We use a simpler convolutional neural network for the function $\hat{g}$ with the following structure:
\begin{itemize}
    \item convolutional layer with 16 output channels, kernel size of 5, stride of 1, and padding of 2
    \item max pooling layer with kernel size 2, followed by a ReLU activation
    \item convolutional layer with 32 output channels, kernel size of 5, stride of 1, and padding of 2
    \item max pooling layer with kernel size 2, followed by a ReLU activation
    \item dropout layer with  $p=0.4$
    \item fully connected layer with 50 out features, followed by ReLU activation
    \item dropout layer with  $p=0.2$
    \item fully connected layer with 5 out features.
\end{itemize}

The model training is stopped when the best validation accuracy (or validation loss for $\hat{f}$) does not improve over 10 epochs or when the model has been trained for 100 epochs, whichever occurs first.

\subsubsection{Hyperparameters}
We randomly choose hyperparameters from the following predefined sets of values:
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item step size $n$ for learning rate decay: (15, 30, 100)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): (0, 0.1, 0.2, 0.5)
        \item nonlinear classifier: (\verb|True|, \verb|False|).
    \end{itemize}
    \item DALUPI:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item step size $n$ for learning rate decay: (15, 30, 100)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03}).
    \end{itemize}
    \item DANN:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item width of discriminator network: $(64, 128, 256)$
        \item depth of discriminator network: $(2, 3)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
    \item MDD:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item maximum norm value for classifier weights: (0.5, 1.0, 2.0)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
\end{itemize}


\subsection{Binary Classification With Binary Attribute Vector}
In our experiment based on CelebA~\citep{Liu2015}, the input $x$ is an RGB image which has been resized to 64Ã—64 pixels, the target $y$ is a binary label for gender of the subject of the image, and the privileged information $w$ are 7 binary-valued attributes. The attributes used in this experiment are: \verb|Bald|, \verb|Bangs|, \verb|Mustache|, \verb|Smiling|, \verb|5_o_Clock_Shadow|,
\verb|Oval_Face| and \verb|Heavy_Makeup|. We use a
subset of the CelebA dataset with 2000 labeled source examples and 3000 unlabeled target examples. We use 1000 samples each for the validation set, source test set and target test set respectively. When we use privileged information from the source domain, in addition to the target, we use an additional 30000 $(x,w)$ samples with PI. To conform with the experiment done in \citet{xie2020n} we only train for 25 epochs. The early stopping patience for the DALUPI method is 15 for each network.  
\label{app:celeb}


\subsubsection{Hyperparameters}
We randomly choose hyperparameters from the following predefined sets of values:
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item step size $n$ for learning rate decay: (15, 30, 100)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): (0, 0.1, 0.2, 0.5)
        \item nonlinear classifier: (\verb|True|, \verb|False|).
    \end{itemize}
    \item DALUPI:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-05}, \SI{1.0e-04}, \SI{1.0e-03})
        \item step size $n$ for learning rate decay: (15, 30, 100)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03}).
    \end{itemize}
    \item DANN:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item width of discriminator network: $(64, 128, 256)$
        \item depth of discriminator network: $(2, 3)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
    \item MDD:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item maximum norm value for classifier weights: (0.5, 1.0, 2.0)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
\end{itemize}
\subsection{Entity Classification} \label{app:coco}

In the entity classification experiment, we train all models for at most 50 epochs. If the validation AUC does not improve for 10 subsequent epochs, we stop the training earlier. No pretrained weights are used in this experiment since we find that the task is too easy to solve with pretrained weights. For DALUPI and LUPI we use the default anchor sizes for each of the feature maps (32, 64, 128, 256, 512), and for each anchor size we use the default aspect ratios (0.5, 1.0, 2.0). We use the binary cross entropy loss for SL-S, SL-T, DANN, and MDD.
%For these models, the classifier network may be either linear or nonlinear (see the description of DANN in Appendix \ref{app:digit}); this choice is treated as a hyperparameter.

We use the 2017 version of the MS-COCO dataset \cite{lin2014microsoft}. As decribed in Section \ref{sec:coco}, we extract indoor images by sorting out images from the super categories ``indoor'' and ``appliance'' that also contain at least one of the entity classes. Outdoor images are extracted in the same way using the super categories ``vehicle'' and ``outdoor''. Images that match both domains (for example an indoor image with a toy car) are removed, as are any gray-scale images. We also include 1000 negative examples, i.e., images with none of the entities present, in both domains. In total, there are \SI{5231} images in the source domain and \SI{5719} images in the target domain. From these pools, we randomly sample \SI{3000}, \SI{1000}, and \SI{1000} images for training, validation, and testing, respectively. In Table \ref{tab:cocolabels} we describe the label distribution in both domains. All images are resized to $320\times 320$.
\begin{table}[t]
\centering
\caption{Marginal label distribution in source and target domains for the entity classification task based on the MS-COCO dataset. The background class contains images where none of the four entities are present.}
\label{tab:cocolabels}
\begin{tabular}{lllllll}
\toprule
Domain & Person & Dog  & Cat  & Bird & Background \\ \midrule
Source & 2963   & 569  & 1008 & 213  & 1000       \\
Target & 3631   & 1121 & 423  & 712  & 1000       \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Hyperparameters}
We randomly choose hyperparameters from the following predefined sets of values. For information about the specific parameters in LUPI and DALUPI, we refer to the paper by \citet{ren2016faster}. Here, RoI and NMS refer to region of interest and non-maximum suppression, respectively.
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item step size $n$ for learning rate decay: (15, 30, 100)
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item dropout (encoder): (0, 0.1, 0.2, 0.5)
        \item nonlinear classifier: (\verb|True|, \verb|False|).
    \end{itemize}

    \item DANN:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item width of discriminator network: $(64, 128, 256)$
        \item depth of discriminator network: $(2, 3)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
    \item MDD:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item maximum norm value for classifier weights: (0.5, 1.0, 2.0)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
    
    \item LUPI and DALUPI:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item step size $n$ for learning rate decay: (15, 30, 100)
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item IoU foreground threshold (RPN): (0.6, 0.7, 0.8, 0.9)
        \item IoU background threshold (RPN): (0.2, 0.3, 0.4)
        \item batchsize per image (RPN): (32, 64, 128, 256)
        \item fraction of positive samples (RPN): (0.4, 0.5, 0.6, 0.7)
        \item NMS threshold (RPN): (0.6, 0.7, 0.8)
        \item RoI pooling output size (Fast R-CNN): (5, 7, 9)
        \item IoU foreground threshold (Fast R-CNN): (0.5, 0.6)
        \item IoU background threshold (Fast R-CNN): (0.4, 0.5)
        \item batchsize per image (Fast R-CNN): (16, 32, 64, 128)
        \item fraction of positive samples (Fast R-CNN): (0.2, 0.25, 0.3)
        \item NMS threshold (Fast R-CNN): (0.4, 0.5, 0.6)
        \item detections per image (Fast R-CNN): (25, 50, 75, 100).
    \end{itemize}

\end{itemize}

\subsection{X-ray Classification} \label{app:chestxray}

In the X-ray classification experiment, we train all models for at most 50 epochs. If the validation AUC does not improve for 10 subsequent epochs, we stop the training earlier. We then fine-tune all models, except DANN and MDD, for up to 20 additional epochs. The number of encoder layers that are fine-tuned is a hyperparameter for which we consider different values. We start the training with weights pretrained on ImageNet. For DALUPI and LUPI we use the default anchor sizes for each of the feature maps (32, 64, 128, 256, 512), and for each anchor size we use the default aspect ratios (0.5, 1.0, 2.0). We use the binary cross entropy loss for SL-S, SL-T, DANN, and MDD.

\begin{table}[t]
\caption{Marginal distribution of labels of images and bounding boxes in the source and target domain, respectively, for the chest X-ray classification experiment. ATL=Atelectasis; CM=Cardiomegaly; PE=Effusion; NF=No Finding. \label{tab:chestxraylabels}}
\centering
\begin{tabular}{lllll}
\toprule
Data                             & ATL   & CM    & PE    & NF \\ \midrule
$x\sim\mathcal{S}$                  & 11559 & 2776  & 13317 & 60361 \\
$w\sim\mathcal{S}$ & 180   & 146   & 153   & -     \\ \midrule
$\tilde{x}\sim\mathcal{T}$                  & 14278 & 20466 & 74195 & 16996 \\
$\tilde{w}\sim\mathcal{T}$ & 75    & 66    & 64    & -     \\ \bottomrule
\end{tabular}
\end{table}

In total, there are \SI{83 519} (457) and \SI{120435} (118) images (annotated images) in the source and target domain, respectively. The distributions of labels and bounding box annotations are provided in Table \ref{tab:chestxraylabels}. Here, ``NF'' refers to images with no confirmed findings. In the annotated images, there are 180/146/153 and 75/66/64 examples of ATL/CM/PE in each domain respectively. Validation and test sets are sampled from non-annotated images and contain \SI{10000} samples each.  All annotated images are reserved for training. We merge the default training and validation datasets before splitting the data and resize all images to $320\times 320$. For the source dataset (ChestX-ray8), the bounding boxes can be found together with the dataset. The target segmentations can be found here: \url{https://stanfordaimi.azurewebsites.net/datasets/23c56a0d-15de-405b-87c8-99c30138950c}. 

\subsubsection{Hyperparameters}
We choose hyperparameters randomly from the following predefined sets of values. For information about the specific parameters in DALUPI, we refer to the paper by \citet{ren2016faster}. RoI and NMS refer to region of interest and non-maximum suppression, respectively. 
%We do not perform any fine-tuning of DANN but we decay the learning rate with a factor 10 every $n$th epoch, where the step size $n$ is a hyperparameter included below. Note that $n=100$ means no learning rate decay.
\begin{itemize}
    \item SL-S and SL-T:
    \begin{itemize}
        \item batch size: (16, 32, 64)
        \item learning rate: (\SI{1.0e-4}, \SI{1.0e-3})
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item dropout (encoder): (0, 0.1, 0.2, 0.5)
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item number of layers to fine-tune: (3, 4, 5)
        \item learning rate (fine-tuning): (\SI{1.0e-5}, \SI{1.0e-4}).
    \end{itemize}

        \item DANN:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item number of trainable layers (encoder): (1, 2, 3, 4, 5)
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item width of discriminator network: $(64, 128, 256)$
        \item depth of discriminator network: $(2, 3)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
    \item MDD:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item initial learning rate: (\SI{1.0e-04}, \SI{1.0e-03})
        \item parameter $\alpha$ for learning rate decay: (0, 1.0)
        \item weight decay: (\SI{1.0e-04}, \SI{1.0e-03})
        \item number of trainable layers (encoder): (1, 2, 3, 4, 5)
        \item dropout (encoder): $(0, 0.1, 0.2, 0.5)$
        \item nonlinear classifier: (\verb|True|, \verb|False|)
        \item maximum norm value for classifier weights: (0.5, 1.0, 2.0)
        \item parameter $\beta$ for adaption regularization decay: (0.1, 1.0, 10.0).
    \end{itemize}
    
    \item DALUPI:
    \begin{itemize}
        \item batch size: $(16, 32, 64)$
        \item learning rate: (\SI{1.0e-04})
        \item weight decay: (\SI{1.0e-4}, \SI{1.0e-3})
        \item IoU foreground threshold (RPN): (0.6, 0.7, 0.8, 0.9)
        \item IoU background threshold (RPN): (0.2, 0.3, 0.4)
        \item batchsize per image (RPN): (32, 64, 128, 256)
        \item fraction of positive samples (RPN): (0.4, 0.5, 0.6, 0.7)
        \item NMS threshold (RPN): (0.6, 0.7, 0.8)
        \item RoI pooling output size (Fast R-CNN): (5, 7, 9)
        \item IoU foreground threshold (Fast R-CNN): (0.5, 0.6)
        \item IoU background threshold (Fast R-CNN): (0.4, 0.5)
        \item batchsize per image (Fast R-CNN): (16, 32, 64, 128)
        \item fraction of positive samples (Fast R-CNN): (0.2, 0.25, 0.3)
        \item NMS threshold (Fast R-CNN): (0.4, 0.5, 0.6)
        \item detections per image (Fast R-CNN): (25, 50, 75, 100)
        \item learning rate (fine-tuning): (\SI{1.0e-5}, \SI{1.0e-4})
        \item number of layers to fine-tune: (3, 4, 5).
    \end{itemize}
\end{itemize}
%

\section{Additional Results for MNIST Experiment}
In Figure \ref{fig:saliencymniste02} and \ref{fig:saliencymniste10}, we show some example images from the MNIST task with associated saliency maps from the source-only model for different values of the skew parameter $\epsilon$. We can see that for a lower value of epsilon the SL-S model activations seem concentrated on the area with the digit, while when the correlation with the background is large the model activations are more spread out.
\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.4\columnwidth}
\centering
\lineskip=0pt
\includegraphics[width=0.5\linewidth]{fig/saliency_0.2_img.pdf}%
\includegraphics[width=0.5\linewidth]{fig/saliency_0.2_map.pdf}%
\caption{SL-S, $\epsilon = 0.2$}\label{fig:saliencymniste02}
\end{subfigure}%
\hspace{3cm}
\begin{subfigure}[b]{0.4\columnwidth}
\centering
\lineskip=0pt
\includegraphics[width=0.5\linewidth]{fig/saliency_1_img.pdf}%
\includegraphics[width=0.5\linewidth]{fig/saliency_1_map.pdf}
\caption{SL-S, $\epsilon = 1.0$}\label{fig:saliencymniste10}
\end{subfigure}%
\caption{Example images (top) and saliency maps (bottom) from SL-S when trained with source skew $\epsilon=0.2$ (a) and $\epsilon=1$ (b).}%\hltodo{Fix this..}
\end{figure}%


\section{Additional Results for Chest X-ray Experiment}
\label{app:additional_x-ray}

%\hltodo{Explain figure}

\begin{figure*}[t!]
\centering
\begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{fig/chestxray_source.pdf}
    \caption{Source test AUC.}
    \label{fig:chestsource_app}
\end{subfigure}%
\begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=.95\textwidth]{fig/chestxray_target.pdf}
    \caption{Target test AUC.}
    \label{fig:chesttarget_app}
\end{subfigure}%
\caption{Classification of chest X-ray images. Model performance on source (a) and target (b) domains. The AUC is averaged over the three pathologies: ATL, CM and PE.
The \SI{95}{\percent} confidence intervals are computed using bootstrapping the results over five seeds. \label{fig:chestexp_app}}
\end{figure*}
\begin{figure}
   \centering
         \includegraphics[width=0.45\textwidth]{fig/chestxray_CM_target.pdf}
         \caption{Test AUC for CM in $\mathcal{T}$. DALUPI outperforms the other models when no extra $(x, y)$ samples are provided. Adding examples without bounding box annotations improves the performance of SL-S and SL-T, eventually causing the latter to surpass DALUPI. \label{fig:saliencychest2}}
\end{figure}
In Figure \ref{fig:chestexp_app}, we show the \emph{average} AUC when additional training data of up to \SI{30000} samples are added in the chest X-ray experiment. We see that, once given access to a much larger amount of labeled samples, SL-S and DALUPI perform comparably in the target domain.

In Figure \ref{fig:saliencychest2}, we show AUC for the pathology CM when additional training data \emph{without} bounding box annotations are added. We see that SL-S catches up to the performance of DALUPI when a large amount of labeled examples are provided. These results indicate that identifiability is not the primary obstacle for adaptation, and that PI improves sample efficiency.

% \begin{figure}
% \centering
% \lineskip=0pt
% \includegraphics[width=0.45\linewidth]{fig/chestxray_CM_target.pdf}
% \caption{Test AUC for CM in $\mathcal{T}$. \label{fig:saliencychest2}}
% \end{figure}


\section{Proof of Proposition~\ref{prop:identification}}
\label{app:identifiability}
\begin{thmprop*}
Let Assumptions~\ref{asmp:covshift} and \ref{asmp:overlap} be satisfied w.r.t. $W$ (not necessarily w.r.t. $X$) and let Assumption~\ref{asmp:sufficiency} hold as stated. Then, the target risk $R_\cT$ is identified for hypotheses $h : \cX \rightarrow \cY$, 
\begin{align*}
R_\cT(h) & = \sum_{x} \cT(x) \sum_w \cT(w\mid x) \sum_{y} \cS(y\mid w) L(h(x), y)~.
\end{align*}%
and, for $L$ the squared loss, a minimizer of $R_\cT$ is 
$
h_\cT^*(x)  = \sum_{w}\cT(w \mid x) \E_\cS[Y\mid w]~.
$
\end{thmprop*}%
\begin{proof}%
By definition, $R_\cT(h) = \sum_{x,y}\cT(x, y) L(h(x), y)$. We marginalize over $W$ to get
\begin{align*}
\cT(x, y) & =  \cT(x)\E_{\cT(W\mid x)}\left[\cT(Y\mid W, x) \mid x] \right]  \\ 
& = \cT(x) \E_{\cT(W\mid x)}[\cT(y\mid W) \mid x] \\
& = \cT(x)\sum_{w : \cT(w)>0} \cT(w \mid x) \cS(y\mid w) \\
& = \cT(x)\sum_{w : \cS(w)>0} \cT(w \mid x) \cS(y\mid w)~.
\end{align*}
where the second equality follows by sufficiency and the third by covariate shift and overlap in $W$. $\cT(x), \cT(w \mid x)$ and $\cS(y\mid w)$ are observable through training samples. That $h^*_\cT$ is a minimizer follows from the first-order condition of setting the derivative of the risk with respect to $h$ to 0. This strategy yields the well-known result that
$$
h^*_\cT = \argmin_{h} \E_\cT[(h(X)-Y^2)] = \E_\cT[Y\mid X]~.
$$
By definition and the previous result, we have that
\begin{align*}
\E_\cT[Y\mid X=x] & = \sum_{y} y \frac{\cT(x, y)}{\cT(x)} \\
& = \sum_{y} \sum_{w : \cS(w)>0} \cT(w \mid x) \cS(y\mid w) y \\
& =\sum_{w} \cT(w \mid x) \E_\cS[Y \mid x]
\end{align*}
and we have the result.
 %Note that identification is also possible to show when we have covariate shift w.r.t. $X$ instead, but it requires overlap. Using similar arguments we obtain $\cT(x, y) =  \cT(x) \sum_{w : \cS(w)>0} \cS(w \mid x) \cS(y\mid w).$
\end{proof} 


\section{Proof of Proposition 2}
\label{app:proof}

\paragraph{Proposition 2.}
\emph{
Assume that $\cG$ comprises M-Lipschitz mappings from the privileged information space $\mathcal{W} \subseteq \bbR^{d_W}$ to $\cY$. Further, assume that both the ground truth privileged information $W$ and label $Y$ are deterministic in $X$ and $W$ respectively. 
Let $\rho$ be the domain density ratio of $W$ and let Assumptions~\ref{asmp:covshift}--\ref{asmp:sufficiency} (Covariate shift, Overlap and Sufficiency) hold w.r.t. $W$. Further, let the loss $L$ be uniformly bounded by some constant $B$ and let $d$ and $d'$ be the pseudo-dimensions of $\mathcal{G}$ and $\mathcal{F}$ respectively. Assume that there are $n$ observations from the source (labeled) domain and $m$ from the target (unlabeled) domain. Then, with $L$ the squared Euclidean distance, for any $h = h \circ f \in \cG \times \cF$, w.p. at least $1-\delta$,
}
\begin{align*}
\frac{R_\cT(h)}{2} & \leq \hat{R}^{Y,\rho}_\cS(g) +M^2 \hat{R}^W_\cT(f)  \\ 
& + 2^{5/4}\sqrt{d_2(\cT\|\cS)}\sqrt[\frac{3}{8}]{\frac{d\log \frac{2me}{d}+\log \frac{4}{\delta}}{m}} \\
& + d_\cW BM^2 \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{d_{\cW}}{\delta}}{2n}} \right).
\end{align*}

\begin{proof}Decomposing the risk of $h \circ \phi$ , we get 
\begin{align*}
& R_\cT(h) = \E_\cT[(g(f(X)) - Y)^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2 + (g(f(X)) - g(W))^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2 + M^2\|f(X)) - g(W)\|^2] \\
& \leq 2\E_\cT[(g(W) - Y)^2] +  2M^2\E_\cT[\|(f(X) - W)\|^2] \\
& = 2R^Y_\cT(g) +  2M^2R^W_\cT(f) =2\underbrace{R^{Y,\rho}_S(g)}_{(I)} +  2M^2\underbrace{R^W_\cT(f)}_{(II)}.
\end{align*}
The first inequality follows from the relaxed triangle inequality, the second inequality from the Lipschitz property and the third equality from Overlap and Covariate shift. We will bound these quantities separately starting with $(I)$. 

 We assume that the pseudo-dimension of $\cG$, $d$ is bounded. Further, we assume that the second moment of the density ratios, equal to the R\'enyi divergence $d_2(\cT \| \cS)=\Sigma_{w\in cG} \cT(w) \frac{\cT(w)}{\cS(w)}$ are bounded and that the density ratios are non-zero for all $w\in \cG$. Let $D_1=\{w_i,y_i\}_{i=0}^{m}$ be a dataset drawn i.i.d from the source domain. Then by application of Theorem 3 from \citet{cortes2010} we obtain with probability $1-\delta$ over the choice of $D_1$,
$$
(I)=R^{Y,\rho}_S(g)\leq   \hat{R}^{Y,\rho}_S(g) + 2^{5/4}\sqrt{d_2(\cT\|\cS)}\sqrt[3/8]{\frac{d\log \frac{2me}{d}+\log \frac{4}{\delta}}{m}}
$$
Now for $(II)$ we treat each component of $w\in \cW$ as a regression problem independent from all the others. So we can therefore write the risk as the sum of the individual component risks
$$
R^W_\cT(f)=\Sigma_{i=1}^{d_\cW} R^W_{\cT,i}(f)
$$
Let the pseudo-dimension of $\cF$ be denoted $d$, $D_2=\{x_i,w_i\}_{i=0}^{n}$ be a dataset drawn i.i.d from the target domain. Then, using theorem 11.8 from \citet{foml_mohri} we have that for any
$\delta>0$, with probability at least $1-\delta$ over the choice of $D_2$, the following inequality holds for all hypotheses $f\in \cF$ for each component risk
\begin{align*}
R^W_{\cT,i}(f)& \leq \hat{R}^W_{\cT,i}(f) + B \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{1}{\delta}}{2n}}\right) \\
\end{align*}
We then simply make all the bounds hold simultaneously by applying the union bound and having it so that each bound must hold with probability $1-\frac{\delta}{d_\cW}$ which results in 
\begin{align*}
R^W_\cT(f)&=\Sigma_{i=1}^{d_\cW} R^W_{\cT,i}(f) \leq \Sigma_{i=1}^{d_\cW} \hat{R}^W_{\cT,i}(f) + \Sigma_{i=1}^{d_\cW} B \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{d_\cW}{\delta}}{2n}}  \right ) \\ &=\hat{R}^W_{\cT}(f) + d_\cW B \left(\sqrt{\frac{2d'\log \frac{en}{d'}}{n}}+\sqrt{\frac{\log\frac{d_\cW}{\delta}}{2n}} \right)
\end{align*}
Combination of these two results then yield the proposition statement.

Consistency follows as $Y$ is a deterministic function of $W$ and $W$ is a deterministic fundtion of $X$ and both $\cH$ and $\cF$ are well-specified. Thus both empirical risks and sample complexity terms will converge to 0 in the limit of infinite samples. 
\end{proof}

\section{Proof Sketch for PAC-Bayes Bound}

\label{app:pacbayesproof}
We will here detail a proof sketch for a PAC-Bayes version of the bound we propose in the main text. For the purposes of this bound we will consider the quantity $\E_{h\sim \psi}R_\cT(h)$, where $\psi$ is a posterior distribution over classifiers $h\sim \psi$. As we are basing the bound on the two-step methodology where we train two different classifiers on separate datasets we assume that we can obtain the posteriors over the component functions separately and independently i.e. $h=f\circ g\sim \psi=\psi_f\times \psi_g$, where $f\sim \psi_f$ and $g\sim \psi_g$.
Let the assumptions from proposition 2 hold here.
Similar to the previous section we decompose the risk into two parts
\begin{align*}
& \E_{h\sim \psi}R_\cT(h) = \E_{h\sim \psi}\E_\cT[(g(f(X)) - Y)^2] \\
& = \E_{h\sim \psi}[2R^Y_\cT(g) +  2M^2R^W_\cT(f) ]=2\E_{h\sim \psi}\underbrace{R^Y_\cT(g)}_{(I)} +  2M^2\E_{h\sim \psi}\underbrace{R^W_\cT(f)}_{(II)}.
\end{align*}
We note that since we now have expectations over the composite function $h$ on expressions which depend on only one of the components we can, for example, write the following:
 $$
 \E_{h\sim \psi}R^Y_\cT(g) = \E_{g\sim \psi_g}R^Y_\cT(g)
 $$ %% why does this hold?
  This holds as we assume that f and g are not dependent on each other. Therefore, we can just marginalize out the part which is not in use. From this point we can use some of the available bounds from the literature to estimate the resulting part e.g. Corollary 1 from \citet{breitholtz2022practicality}. 
 Application of this result yields the following bound on the first term
 $$
 \underset{g \sim \psi_g}{\mathbb{E}} R^Y_\mathcal{T}(g) \leq  \frac{1}{\gamma} \underset{g \sim \psi_g}{\mathbb{E}} \hat{R^{Y,\rho}_\mathcal{S}}(g) + \beta_\infty\frac{\text{KL}(\psi_g \Vert \pi_g)+ \ln(\frac{1}{\delta})}{2\gamma(1-\gamma)m}~.
 $$
 Thereafter we can use another bound from the literature to estimate the second term, e.g. Theorem 6 from \citet{germain_pac-bayes_2020}. Using this we obtain the following:
 $$
  \underset{f \sim \psi_f}{\mathbb{E}} R^W_\cT(f) \leq \frac{\alpha}{1-e^{-\alpha}}\left (\underset{f \sim \psi_f}{\mathbb{E}} \hat{R}^W_\cT(f) +  \frac{\text{KL}(\psi_f \Vert \pi_f)+\ln(\frac{1}{\delta})}{n\alpha} \right )~.
$$
Then a bound can be constructed by combining these two results using a union bound argument.
\begin{align*}
\E_{h\sim \psi}R_\cT(h) &\leq \frac{2}{\gamma} \underset{g \sim \psi_g}{\mathbb{E}} \hat{R^{Y,\rho}_\mathcal{S}}(g) 
+ \beta_\infty\frac{\text{KL}(\psi_g \Vert \pi_g)+ \ln(\frac{2}{\delta})}{2\gamma(1-\gamma)m} \\
&+ \frac{2M^2\alpha}{1-e^{-\alpha}}\left (\underset{f \sim \psi_f}{\mathbb{E}} \hat{R}^W_\cT(f) +  \frac{\text{KL}(\psi_f \Vert \pi_f)+\ln(\frac{2}{\delta})}{n\alpha} \right )
\end{align*}% \hltodo{Check this argument}
%% be more clear about posteriors
%Then we combine these two bounds using a union bound argument.

\section{A Bound on the Target Risk Without Suffiency}
\label{app:no_sufficiency}
The sufficiency assumption is used to replace $\cT(y \mid x)$ with $\cT(y \mid w)$ in the proof of Proposition~\ref{prop:identification}. If sufficiency is violated but it is plausible that the degree of insufficiency is comparable across domains, we can still obtain a bound on the target risk which may be estimated from observed quantities. One way to formalize such an assumption is that there is some $\gamma \geq 1$, for which  
\begin{equation}\label{eq:weak_sufficiency}
\sup_{x\in \cT(x \mid w)} \cT(y\mid w, x)/\cT(y \mid w) \leq \gamma \sup_{x\in \cS(x \mid w)} \cS(y\mid w, x)/\cS(y \mid w) 
\end{equation}
This may be viewed as a relaxation of suffiency. If Assumption~\ref{asmp:sufficiency} holds, both left-hand and right-hand sides of the inequality are 1. Under \eqref{eq:weak_sufficiency}, with $\Delta_\gamma(w,y)$ equal to the right-hand side the inequality, 
$$
R_\cT(h) \leq \sum_{x} \cT(x) \sum_w \cT(w\mid x) \sum_{y} \Delta_\gamma(w,y) \cS(y\mid w) L(h(x), y)~.
$$
However, the added assumption is not verifiable statistically.



% \section{t-SNE plot for digit classification experiment}
% We present some t-SNE plots for the SL-S model for the digit classification experiment in Figure \ref{fig:tsnemnist} and \ref{fig:tsnemnist2}. We clearly see that when the skew is low the target is well separated. However, with full correlation between the label and the background we see that the separation is lost. The perplexity is set to 100 when producing these plots.

% \begin{figure*}[t!]
%     \centering
%       \begin{subfigure}{0.48\textwidth}
%     \centering
% \includegraphics[width=.92\textwidth]{fig/tsne_source_0.2.png}
%         \caption{t-SNE plot of the source test set, $\epsilon=0.2$.}
%         \label{fig:source0.2tsne}
%     \end{subfigure}%
%         \begin{subfigure}{0.48\textwidth}
%     \centering
%         \includegraphics[width=.92\textwidth]{fig/tsne_target_0.2.png}
%         \caption{t-SNE plot of the target test set, $\epsilon=0.2$.}
%         \label{fig:target0.2tsne}
%     \end{subfigure}%
%     \caption{t-SNE when the skew is $\epsilon=0.2$.}
%     \label{fig:tsnemnist}
% \end{figure*}
% \begin{figure*}[t!]
%     \centering
%       \begin{subfigure}{0.48\textwidth}
%     \centering
%         \includegraphics[width=.92\textwidth]{fig/tsne_source_1.png}
%         \caption{t-SNE plot of the source test set, $\epsilon=1$.}
%         \label{fig:source1tsne}
%     \end{subfigure}%
%         \begin{subfigure}{0.48\textwidth}
%     \centering
%         \includegraphics[width=.92\textwidth]{fig/tsne_target_1.png}
%         \caption{t-SNE plot of the target test set, $\epsilon=1$.}
%         \label{fig:target1tsne}
%     \end{subfigure}%
%     \caption{t-SNE when the skew is $\epsilon=1$.}
%     \label{fig:tsnemnist2}
% \end{figure*}



%\end{document}

