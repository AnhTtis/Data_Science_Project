
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/
%-- coding: UTF-8 --


\documentclass[10pt,journal,compsoc]{IEEEtran}
% \usepackage[UTF8]{ctex}
\usepackage[figuresright]{rotating}

\usepackage[utf8]{inputenc}
\usepackage[table,xcdraw]{xcolor}
\usepackage{framed}

\usepackage{multicol}
\usepackage{tabularx}

\usepackage{tabularray}
% \usepackage{longtable}

\usepackage{multirow}
\usepackage[T1]{fontenc} %added by linxin
\usepackage{longtable}

\usepackage{graphicx}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
\usepackage{threeparttable}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
% \ifCLASSINFOpdf
%    \usepackage[pdftex]{graphicx}
%   % declare the path(s) where your graphic files are
%    \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
% \else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
% \fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
% \usepackage{multirow}
\usepackage{array}
\usepackage[backref]{hyperref}

% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% \usepackage{pdfpages}
% \usepackage{subfigure}
% \usepackage{tabularx}

% *** SUBFIGURE PACKAGES ***
% \ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
 % \usepackage{array}
\usepackage{booktabs} %调整表格线与上下内容的间隔
% \usepackage{multirow}
% \else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Review on Machine Theory of Mind}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.
% \author{Yuanyuan~Mao,
%         Shuang~Liu,~\IEEEmembership{Fellow,~OSA,}
%         Pengshuai~Zhao,~\IEEEmembership{Life~Fellow,~IEEE},
%         Qin Ni,
%         Xin Lin,
%         Liang He}% <-this % stops a space
% \IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332.\protect\\
% % note need leading \protect in front of \\ to get a newline within \thanks as
% % \\ is fragile and will error, could use \hfil\break instead.
% E-mail: xlin@cs.ecnu.edu.cn;

\author{Yuanyuan~Mao,
        Shuang~Liu,
        Pengshuai~Zhao,
        Qin Ni,
        Xin Lin,
        Liang He% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Yuanyuan Mao, Xin Lin and Liang He are with East China Normal University, Department of Computer Science and Technology Shanghai, ShangHai, CN.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
% E-mail: see http://www.michaelshell.org/contact.html
\IEEEcompsocthanksitem  Shuang~Liu, Pengshuai~Zhao and Qin Ni are with Shanghai Normal University, Shanghai Engineering Research Center of Intelligent Education and Bigdata
Shanghai, Shanghai, CN.\protect\\
% \IEEEcompsocthanksitem  are with Shanghai Normal University, College of Information, Mechanical and Electrical Engineering
% Shanghai, Shanghai, CN.
}% <-this % stops an unwanted space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.\\(Corresponding author: Xin Lin (Email: xlin@cs.ecnu.edu.cn) and Qin Ni (Email: niqin@shnu.edu.cn))}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{
% Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015
}%
{
Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals
}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
 Theory of Mind (ToM) is the ability to attribute mental states to others, the basis of human cognition. 
% Artificial intelligence (AI) with ToM can predict human behavior more accurately and make AI more like human, more warm, and more reliable. 
At present, there has been growing interest in the AI with cognitive abilities, for example in healthcare and the motoring industry. 
Beliefs, desires, and intentions are the early abilities of infants and the foundation of human cognitive ability, as well as for machine with ToM. In this paper, we review recent progress in machine ToM on beliefs, desires, and intentions.
And we shall introduce the experiments, datasets and methods of machine ToM on these three aspects, summarize the development of different tasks and datasets in recent years, and compare well-behaved models in aspects of advantages, limitations and applicable conditions, hoping that this study can guide researchers to quickly keep up with latest trend in this field. Unlike other domains with a specific task and resolution framework, machine ToM lacks a unified instruction and a series of standard evaluation tasks, which make it difficult to formally compare the proposed models. We argue that, one method to address this difficulty is now to present a standard assessment criteria and dataset, better a large-scale dataset covered multiple aspects of ToM.


 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
social agents/robotics, cognitive models, affective computing, artificial intelligence
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{
\section{Introduction}\label{sec:introduction}}
\IEEEPARstart{T}{heory}
 of Mind (ToM), defined as the ability to attribute mental states to others, is the basis of human cognition. For instance, if someone walk towards the fridge, open the fridge, look inside and close it, we can infer that he is hungry and is looking for something to eat. Although there is slightly difference between each definition of ToM in \cite{premack1978does}, \cite{wimmer1983beliefs}, \cite{leslie1987pretense}, \cite{tager2000componential}, it is generally accepted that ToM represents a set of cognitive skills attribute mental states (beliefs, intentions, knowledge, perspectives, etc.) to others and recognize that these mental states may differ from one’s own. 
Langley \cite{langley2022theory} suggested that AI is excepted to tackle "hot" cognition such as how a person is thinking influenced by their emotional state in addition to "cold" cognition. "Hot" cognition refers to cognition related to social cognition, including ToM. It contrasts with "cold" cognition, in which the processing of information is independent of emotional involvement. So far, there are ground-breaking achievements on "cold" cognitive tasks, such as object detection and image classification. 
AI with the ability to process "hot" cognition can understand human behavioral motive and predict actions more accurately, and further make AI more like human with warmth and reliability to provide emotional support especially for the elderly and patient with autism.
At present, there are less researches on "hot" cognition. However, it is shown that the requirement of social-moral skills in robot is more than that of objective reasoning\cite{2017What}.

ToM includes many aspects such as beliefs, deisres, knowledge and so on. The current concern in the computer field is emotion analysis, human-robot interaction, etc. However, the abilities of beliefs, desires and intentions (BDI) are acquired in the early stage of infants and they are the basis of human cognitive ability. Due to the difficulty in BDI's representation, compared with other cognitive abilities, such as a smiling face to show a feeling of joy, the tasks of machine ToM have not  been formally defined and there is not yet a systemetic solution in spite of some existing research.

In this paper, we introduce the work of machine ToM on BDI, and introduce them in terms of experiments, datasets, and methods. We hope that this study can guide researchers to quickly understand  trend in the field.

% In order to understand other people’ s mental states with others within social and occupational environments, this form of social cognition is essential . The same capability of inferring human mental states is a prerequisite for artificial intelligence (AI) to be integrated into society, for example in healthcare and the motoring industry.
\subsection{Theory of Mind in Children}
Until the late 1990s, it was shown that the majority of children at 3-5 have certain ability to identify someone's mental states, as they pass the test designed to assess the ability of ToM \cite{10.2307/1130707}, \cite{wellman2001meta}. By 3 or 4 months of age, infants have understand basic intuitive physics. They are aware of the facts that two objects will not occupy the same space and that objects will exist and move continuously in time and space \cite{baillargeon1987young}, \cite{spelke1990principles}, \cite{kim1992infants}. At later ages, infants' understanding about physical conceptions is enriched \cite{oakes1990infant}, \cite{kim1992infants}, \cite{needham1993intuitions}, \cite{xu1996infants} which help them to reason about human behavior further. At 12 months of age joint attention is developed. When someone is pointing at an object, infants would look the object being pointed at, instead of his finger. By 14-18 months, through gaze direction, children begin to understand the mental states of desires, intentions and the causal relation between emotions and goals \cite{saxe2004understanding}. Liszkowski et al. \cite{liszkowski200612} showed that children as young as 12-18 months were able to infer an adult’s behavior and aid them. Toddlers between 18 and 24 months begin to distinguish between real and pretend events and often start to engage in pretend play around this age. Around the age of 3-4 children begin to understand the differences between their own and others’ beliefs and knowledge, thereby beginning to comprehend false beliefs, but this ability does not become fully stable until age 5-6. 
\subsection{Assessing Theory of Mind}
The majority of ToM research in children focuses on the comprehension of false belief. The false belief paradigm was initially proposed by Wimmer and Perner \cite{wimmer1983beliefs} and has since been adapted and applied to a range of contexts by Wellman et al. \cite{wellman2001meta}. The classic Sally-Anne test proposed by Wimmer and Perner \cite{wimmer1983beliefs}, a test of first-order false belief is a well-known paradigm. In this task Anne transferred the toy in the basket to the box when Sally is absent. The mastering of false belief is considered to provide stringent evidence of a mature ToM \cite{hala1997all}. In addition, various ToM early capacities are also involved such as understanding intentional actions, engaging in pretend play, joint attention and imitation \cite{callaghan2005synchrony}, \cite{colonnesi2005emergence}.
Lately, the focus of research has moved from specific false belief understanding to a more developmental view \cite{wellman2000developing}, \cite{steele2003brief} aiming at a wide range of ToM components that children develop between their second and sixth year \cite{wellman2000developing}.
In this period, ToM evolves from a simple desire theory to a complete belief-desire theory, from true beliefs to false beliefs, and from the understanding of first-order beliefs to second-order beliefs.
Various kinds of implicit non-verbal and simplified measures have been used with infants, including looking time used with infants as an indicator of violations of expectation \cite{onishi200515}, \cite{surian2007attribution}, \cite{trauble2010early}, anticipatory looking 
\cite{clements1994implicit}, \cite{surian2007attribution}, \cite{surian2012will} and interactive measures such as spontaneous helping \cite{buttelmann2009eighteen}, \cite{knudsen201218}, \cite{southgate2010seventeen}. 
With the implicit methods, it is showed that some ToM abilities may already be present in
infancy, a conclusion that could not be reached using standard measures because of the extraneous factors inherent to the tests \cite{Slaughter2015Theory}. Some claim that implicit tasks are valid methods to measure ToM \cite{Carruthers2013Mindreading}, \cite{Kulke2018Is}, others suggest that the robustness, reliability and replicability of implicit ToM are unknown yet \cite{2018How}.
\subsection{Machine Theory of Mind}
 Some work evaluated machine ToM with the children's ToM approaches. For example, the Sally-Anne test is formalised into natural language and images story to evaluate the model's understanding of false belief \cite{grant2017can}, \cite{nematzadeh2018evaluating}, \cite{eysenbach2016mistaken}. Some studies \cite{baker2017rational}, \cite{2018Machine}, \cite{gandhi2021baby} created 2D grid environment to show the trajectory of an agent with ToM, inspired by classic tasks in infancy's ToM test \cite{woodward1998infants} so as to test the machine's understanding of beliefs, desires, intentions and other mental states. AI needs to cooperate with others (human-like agent or human) to complete a task, and infers  mental states of others and predicts their actions. Thus, some experiments are designed as human-robot interaction experiments \cite{hadfield2016cooperative}, \cite{puig2020watch_WAH}, \cite{overcook}.
 Research in cognitive science suggests that we can view others as utility maximizers who make a decision constantly to maximize the rewards they obtain while minimizing the costs. Based on this assumption, we can model human behavior and infer mental states. BDI models \cite{georgeff1998belief} have been proposed to emulate the functioning of the human mind in a simplistic way. Bayesian networks can easily represent the knowledge and learning process of infants by using a causal map: an abstract, coherent, learned representation of the causal relations among events.
The principles of reinforcement learning (RL) resemble the assumptions that we make about others' behavior. 
Taking advantage of this similarity, we can formalize our model of people’s minds as being roughly equivalent to a RL model. Under this approach, mental-state inference from observable behavior is equivalent to inverse reinforcement learning (IRL, Figure \ref{fig:IRL}). But this idea has its problems that rationality of individuals is limited by their information state, the cognitive limitations of their minds, and time constraints which are not properly accounted for in the current frameworks. Therefore, under this assumption, human mental states cannot be completely restored. In CIRL \cite{hadfield2016cooperative} where a robot and a human cooperate on a task, they share a reward function, and the human make decisions according to the reward function and the robot's task is to learn the reward function during this task. In CIRL setting, it can be guaranteed that human take actions rationally, but there still remains the issue of the non-rational factors in human behavior. Thanks to the development of machine learning, there are many works with deep-learning-based approaches to solve machine ToM task\cite{2018Machine}, \cite{nguyen2022learning}, \cite{shu2021agent}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{figures/IRL.png}
\caption{Core (a) ToM components and (b) "model-based" reinforcement learning components\cite{jara2019theory}.}
\label{fig:IRL}
\end{figure}
Some researchers believe that deep-learning-based approaches which direct map of behaviors to mental states have been able to implicitly grasp the irrational factors of human mental states (e.g. end-to-end neural network models that input human observations and represent human mental states with a hidden vector). Deep-learning-based approaches have produced good results, but they require a lot of training data. 
Rabinowitz et al. \cite{2018Machine} proposed a computational model, called ToMnet as a meta-learning problem and the results were impressive. However, Jara-Ettinger \cite{jara2019theory} points out that ToMnet required 32 million samples to learn to perform goal inference at a level similar to that of a 6-month-old infant. If infants learned ToM this way, 175000 labelled demonstrations would be required every day during those 6 months. 
 % But the computational model based on ToM, modeling causality of human mental states can provide an inference process  and be credible. 

Previous research has claimed epistemic logic (EL) to be a suitable formalism for representing essential aspects of ToM in particular to reason about the first- and higher-order beliefs. The EL system builds a symbolic model and keeps track of beliefs of all observed agents, and how these beliefs change when events occur in the environment. One potential problem is that the EL system supports only symbolic input or is embedded in the AI system that relies on the AI's perceptual system to handle sensory data. Since this approach is not universal, it is not covered in detail in this paper.
In \cite{2004Reasoning}, \cite{2020Implementing}, \cite{2018The}  can be found how EL models are build to solve the false belief task.


In summary, machine ToM has many challenges. First, tasks in ToM include many aspects supports of sub-tasks, from classic psychological experiments to human-robot interaction experiments. Secondly, one task can be formalized into different formats, such as logic forms, natural language and images. The diversity of these experiments make it difficult to devise a common algorithm to conduct them. Besides, it is expected that the computational model can accurately infer human mental states, but also we are interested in the process of modeling human mental states and explanation of the computational model. 
In this paper, we introduce the experiments, datasets and methods of machine ToM on BDI. We summarize the development of different tasks and datasets in recent years, and compare well-behaved models in aspects of advantages, limitations and applicable conditions, hoping that this study can guide researchers to quickly understand the trends in the field.

% \subsubsection{ }

% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.



% \IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
% for IEEE Computer Society journal papers produced under \LaTeX\ using
% IEEEtran.cls version 1.8b and later.
% % You must have at least 2 lines in the paragraph with the drop letter
% % (should never be an issue)
% I wish you the best of success.

% \hfill mds
 
% \hfill August 26, 2015

% \subsection{Subsection Heading Here}
% Subsection text here.

% % needed in second column of first page if using \IEEEpubid
% %\IEEEpubidadjcol

% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.





% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.


\section{Experiments}
\label{sec:Experiments}
\subsection{Grid-world Experiment}
Gridworld is a rectangular grid world environment including a finite Markov decision process. The cells of the gridworld correspond to the states of the environment with four actions: up (north), down (south), left (west), right (east). The actions cause the agent to move one cell in the respective direction on the grid.

\subsubsection{Sally-Anny Experiments}
Sally-Anny experiment\cite{wimmer1983beliefs} is a classical false belief task, which examines the ability to reason about another person's belief or first belief. Many false belief experiments are designed to imitate the Sally-Anny experiment. Baron-cohen et al.\cite{1985Does} examined children's ability to reason about other people's false beliefs. In this task, Sally put her toy in the basket first and then went out. While Sally was outside, Anne took the toy out of Sally's basket and put them in Anne's box. Then, Sally returns to the room. Participants were asked the following questions: " where does Sally find her toy? " (belief questions); " where are the toy? " (reality questions); " where were the toy in the first place? " (memory questions); The first question tested participants' ability to reason about Sally's belief in the location of toy\cite{2018Evaluating}. Reality and memory questions are used to confirm that children's correct answers to questions of belief are not due to chance, but rather to their correct understanding of the state of the world and the beliefs of others.

Taking Sally-Anny experiment as a model, some simulated false belief experiments appear, the essence of which is position exchange. For example, Rabinowitz et al.~\cite{2018Machine} used the false belief experiment in grid world design, in which the designers varied the distance between the agent and the target object. Nguyen et al.\cite{nguyen2020cognitive} also mapped the Sally-Anny experiment to the grid experiment, each step of the false belief experiment is mapped. Dung Nguyen et al.\cite{nguyen2022learning} implemented false belief using the key-door experiment, which used a method of exchanging the initial location of a target. The corresponding experimental designs of these three kinds of computer simulation experiments and Sally-Anny experiment are shown in Table~\ref{tab:sally-anny}.

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{table}[h]
\renewcommand\arraystretch{1.5}
\caption{Sally-Anny and Computer simulation experiment}
    \label{tab:sally-anny}
    \scalebox{0.8}{
    \begin{tabular}{|l|l|l|l|}
        
        
        \hline
        \multicolumn{2}{p{5em}|}{Sally-Anny.\cite{1985Does}}&\multicolumn{2}{p{30em}}{{\tabincell{l}{
          a) Sally places a marble in a basket.\\
          b) Sally moves away.\\ 
          c) Anne puts the marble to a box.\\
          d) Where will Sally look for her marble when returning (the basket \\or the box)?}}}\\
        \hline
        \multicolumn{2}{p{5em}|}{ToMnet.\cite{2018Machine}}&\multicolumn{2}{p{30em}}{{\tabincell{l}{a) An agent is trained to be a blue-object-prefereing agent.\\b) Agent is forced to reach a subgoal.\\c) The location of the preferred object is swapped.\\d) At the subgoal, where will agent go to find the preferred\\ blue object (its original or new location)?}}}\\
        \hline
        \multicolumn{2}{p{5em}|}{CogToM.\cite{nguyen2020cognitive}}&\multicolumn{2}{p{30em}}{{\tabincell{l}{a) The agent keeps the initial state of theenvironment.\\b) Find target under the action strategy.\\c) The target position is changed.\\d) Will the agent observes the error information and change the\\ action track ?}}}\\
        \hline
        \multicolumn{2}{p{5em}|}{Key-Door.\cite{nguyen2022learning}}&\multicolumn{2}{p{30em}}{{\tabincell{l}{a) The agent looks for the red door.\\b) The agent looks for the red key that opens the red door.\\c) The positions of the different colored doors are swapped. \\d) Will the agent continue to look for the red door?}}}\\
        \hline
        
        
    \end{tabular}
    }

\end{table}
In a word, false belief experiments can be realized by changing the original distance, changing the original position of the target or exchanging the original position of the object. 
\subsubsection{Food-Trucks}
\label{sec:food_truck}
Physical intelligence and social intelligence are two of the most important components of human beings' impressive cognitive gifts. Human physical intelligence uses the intuitive theory of the world's physical laws to maintain an accurate description of the state of the environment. Human social intelligence uses psychological theories to infer the mental states of other subjects. With the development of computer vision and AI programming, machines have more body intelligence similar to humans, the ability to reason about the Belief-Desire-Intention of other subjects is still lacking. Figure~\ref{fig:Food_truck} shows an example of Food-Trucks.

The Food-Trucks experiment described an experimental paradigm designed to reflect the use of theory of mind in the context of the natural environment, to achieve quantitative changes in belief, desire and intention~\cite{baker2017rational_BTOM}. The Food-Trucks experiment described a 'Foraging' scenario in which observers (human subjects and computational models) could see agents walking in a campus environment from a bird's-eye view, and have lunch at one of several Food-Trucks. Agent may want to eat in a particular truck, but not sure where the truck is, and needs to think carefully about where to go based on his own desire and belief. In this experimental scenario, the elves represent the agent's location, with the black trace of the arrow overlay recording the agent's movement history. On the diagonal corner of the environment are two yellow shaded units, representing where the truck can be parked, each containing a different truck or no truck. The shaded gray area of each frame represents the area outside the agent's current view.

\begin{figure}[ht]

\centering
\includegraphics[width=0.50\textwidth]{figures/Food_truck.png}
\caption{A Food-Trucks example.The white area is the vision of the agent. After a series of trajectory, the agent moves towards the favorite truck for dining.\cite{baker2017rational_BTOM}}
\label{fig:Food_truck}
\end{figure}

In this experiment, the task was to infer the agent's desire for each truck and its belief in its location, based on a single observed trajectory. This experiment was adapted to a Bayesian-based model framework that approximates a planning mechanism which formalizes ToM into a theory-based Bayesian cognitive science, formalizing the behavior of others as arising from near-rational beliefs and plans that depend on desire, it is realized by maximizing expected gain and minimizing expected loss in partially observable Markov decision process (POMDP), which will be illustrated in Section~\ref{sec:btom}.
\subsubsection{2D Grid-world}
2D grid-world design is a continuous decision-making problem, which is often used in various mind of theory experiments. It is usually designed to set goals and obstacles in a grid world with the size of N $\times$ N. The goal of an agent is to avoid obstacles and reach the target object. The predictive ability of models is tested by showing the trajectories of agents. The grid size is usually set to N = 10, this set can place 0-50 or more obstacles and generally 0-4 targets. Obstacles are usually set in size of 1 $\times$ 1, ranging from 0 to 50 in number. Target settings are generally 0-4, which can use different shapes or different colors to represent different goals, the agent is set to eventually achieve one of the goals. The actions of the agent include: up, down, left, right, stop, turn, according to the setting of the scene. %, there can be search, take and other actions. 
The serial movement of the agent from the initial position to the ending position forms a trajectory, which can be represented by an arrow.

In 2018, Rabinowitz et al.\cite{2018Machine} proposed a neural network-based framework for the theory of mind, enabling the ability to make richer predictions about agent characteristics and mental states using a small number of behavioral observations. Two experiments, random agents experiment and inferential goal-directed behavior experiment, are designed by using a 2D grid-world. In the random agents experiment, the agent is fully observable to the grid world. The size of the grid world is 11 $\times$ 11, and there are randomly walls and four randomly placed different objects, which are the targets of the agent. Every time an agent consumes a target, it generates a certain reward, along with a negative reward for each step taken and for hitting a wall. The inference goal-directed behavior experiment demonstrates how the model learns to derive the goals of reward-seeking agents. The setup is the same as the random agent experiment, which consists of four different objects placed in random positions. Consuming an object gives the agent a reward and causes the event to terminate. Moving and hitting the wall causes the agent to consume the reward. Unlike the previous experiment, in this experiment the agent plans its behavior through the optimal strategy and the value iteration, to get the maximum reward value in the end. Those experiments have shown how the model learns a general model for agents in the training, and how to construct a specific model for agents while observing a new behaviour.

Inspired by Rabinowitz et al.~\cite{nguyen2020cognitive}, in 2020, Nguyen et al. proposed a cognitive model, CogToM\cite{nguyen2020cognitive}, that relies on Instance-based learning empirical decision theory to demonstrate ToM's development by observing the behavior of other subjects. In this paper, we use an 11 $\times$ 11 size grid world that contains randomly located obstacles (black bars), ranging from 0 to 6 in number and 1 $\times$ 1 in size. Within each grid, there are four targets with different values, represented as objects of four colors (blue, green, orange, purple), which are placed in random positions that do not overlap with the obstacle. Starting from a random position, the agent takes certain actions (up, down, left, right) to reach one of the four objects. A sequence of actions from the initial position to the end position forms a trajectory (red dotted line) that is generated by the strategy (decision sequence) taken by the agent. The researchers also designed two sets of experiments. One is arbitrary goal with random agents (AGRA), the other is goal-directed task with reinforcement learning agents. The setup of these two experiments is much the same as the above one. Figure~\ref{fig:CogToM} shows an example of the goal-directed task. In the AGRA experiment, the goal of the random agent is to obtain one of four color objects within the 31-step limit. According to different action strategies, they create different types of stochastic agents, and finally generate the trajectories of these agents in the randomly generated grid world. Slightly differently, in the experiment, the random agent had no reward function when consuming any of the four items stopped the event. In the Goal-Directed Task experiment, the task was set to reach a specific object with the highest return in 31 steps. Consuming anything else will end the show. The difference is that the observation of all or part of the agent's trajectory in the grid world increases the difficulty of model prediction to a certain extent.These two experiments prove that the CogToM model can predict the target and behavior trajectory of the agent with high accuracy after less observation of the agent's behavior.
\begin{figure}[ht]

\centering
\includegraphics[width=0.20\textwidth]{figures/CogToM.png}
\caption{An example of the Goal-Directed Task\cite{nguyen2020cognitive}.}
\label{fig:CogToM}
\end{figure}

In the same year, Yun Shiuan Chuang et al.\cite{chuang2020using} proposed an artificial neural network (ToMnet+) based on theory of mind to learn and predict social preferences based on implicit information of agent and social goal behavior interaction. The researchers simulated human social networks through grid experiments. They simulated 30 virtual agent social networks in total. Each of them has four social goals with different social rewards. Each agent was set up 10000 two-dimensional 12*12 grid worlds. In each grid world, 1-4 targets and 0-50 barriers are placed at random locations. The agent can only move vertically or horizontally, and each target has a social reward value, which is obtained through the social support questionnaire based on the 10 point Likert scale. The ultimate goal of the agent is to move the least steps to obtain the maximum reward value. Figure \ref{fig:gridworld} shows an example grid world of the social game for humans.This experiment, to some extent, truly reflects the participants' social support networks, which are hidden from the model, thus verifying that ToMNet+ can infer simple social networks.
\begin{figure}[ht]

\centering
\includegraphics[width=0.40\textwidth]{figures/gridworld.png}
\caption{An example grid world of the social game for humans\cite{chuang2020using}. }
\label{fig:gridworld}
\end{figure}
Tan Zhi-xuan et al.\cite{2020Online} put develop a Sequential Inverse Plan Search algorithm in order to make the agent realize to infer the target of others by observing their both optimal and non-optimal sequences of actions. The algorithm makes use of the on-line planning assumption and restricts the computation by gradually extending the inferential plan when new actions are observed. The algorithm is tested by the game of searching for gems, and the next target of the agent is inferred according to its current behavior trajectory, so as to deduce the agent's later behavior trajectory. The scavenger hunt designs in 2D grid world that includes three colored gems(the agent’s target), three doors, and three keys. The agent needs to get the key to open the door through certain behavior track to obtain the corresponding color gem. The design of the experiment can be well applied to goal prediction, or path planning models.

In 2021, DJ Strouse et al. have studied how to train human-friendly agents without using artificial data, and proposed a two-stage reinforcement learning-based method for agent training which called virtual collaborative games (FCP)~\cite{overcook}. This method was evaluated by an \emph{overcooked} game, which was identified as a coordination challenge for AI. The overcooked game is a paid game for two players. Two players are rewarded by coordinating cooking and delivering soup. In the cooking environment, the player is assigned to serve as a chef in the kitchen of a grid world, with the task of player is to serve as much tomato soup as possible in an episode. This involves a series of high-level sequential actions: collecting tomatoes, placing them in a cooking pot, making them into soup, collecting a dish, obtaining the soup, and delivering it. For successful delivery, both parties will receive equal rewards. To accomplish this task effectively, players must learn to navigate in the kitchen and interact with objects in the right order, while maintaining awareness of their partner's behavior in order to coordinate with them. Each player will observe his or her own grid world and can perform one of six actions at each step: rest, move \{up, down, left, right\}, interact. There are five main types of units in the environment: the Tomato Station, the dish station, the counter, the cooking pot, and the delivery location. Players learn how to navigate the map, interact with the items, and place the items in the right place. Finally, they send the finished dishes to the delivery point. There are five environmental layouts in this cooking environment: cramped room, asymmetric advantages, coordination ring, counter circuit and forced coordination. These five environmental layouts present different coordination difficulties from low to high. In order to carry out the contrast experiment, the researcher proposed four methods to train the agent (one of the players): self-play (SP), population-play (PP), behavioral cloning play (BCP), fictitious co-Play (FCP). The method is evaluated by the degree of liking of real people to the agents obtained by different training methods.The results of the experiments show that FCP agents scored significantly higher than all baselines and humans reported a strong subjective preference to partnering with FCP agents.

\subsubsection{3D Grid-world}
Compared with the 2D world, the actions and routes in the 3D world are more flexible and diverse.

Kanishk Gandhi et al in 2021. proposed the Baby Intuitions Benchmark (BIB)~\cite{gandhi2021baby}, which uses an anticipatory paradigm based on developmental cognitive science to propose reasoning tasks. The tasks enable machines to predict the rationality of an agent's behavior for a given video sequence, used to test a machine's ability to reason about the potential intentions of other agents by looking only at their behavior. Based on the infant psychology experiment, this paper designed a computer simulation experiment to evaluate the intelligence agents' ability to recognize the goals, preferences and actions of others. Among the psychological theories to be emulated are as follows. 1) Babies' goals are object-based rather than location-based. 2)Babies can attribute certain preferences to certain people. 3)Babies understand that solid objects can not be traversed. 4)Babies can attribute certain preferences to certain people. 5) Babies can understand that solid objects can not be traversed Infants can adjust the sequence of actions to achieve higher goals. Aiming at these five psychological theories, the experimental design is shown as table~\ref{tab:BIB}.

\begin{table*}[h]

    \caption{Experiment Design of BIB.}
    
    \Large
    \label{tab:BIB}
    \resizebox{\linewidth}{!}{
    \scriptsize
    \renewcommand\arraystretch{1.2}
    \begin{tblr}{
        colspec={Q[c,m,0.2\textwidth] Q[c,m,0.3\textwidth] Q[c,m,0.2\textwidth] Q[c,m,0.3\textwidth]},
        hspan=minimal,}
    \hline[1pt]
    \SetRow{gray8}    Psychological Theory  &  Psychological Experiment&BIB &Experiment Design\\
    \hline 
    \SetCell[r=1]{l,m}{Infants attribute object-based—as opposed to location-based—goals to agents.\cite{woodward1998infants,1999Infants}}  &  \SetCell[r=1]{l,m}{When 5-and 9-month-old infants saw a hand repeatedly reaching to a ball on the left over a bear on the right, they then looked longer when the hand reached to the left for the bear, even though the direction of the reach was more similar in that event to the events in the previous trials.These results suggest that the infants expected that the hand would reach consistently to a particular goal object as opposed to a particular goal location.\cite{woodward1998infants,1999Infants}} &  \SetCell[r=1]{l,m}{Can an AI system represent an agent as having a particular object-based goal?\cite{gandhi2021baby}}  & \SetCell{l,m}{It was achieved in the grid world by exchanging the location of two targets in a familiar environment;} \\
    \hline
    \SetCell[r=1]{l,m}{Infants are capable of attributing specific preferences to specific agents.\cite{BURESH2007287}}  &  \SetCell[r=1]{l,m}{While 9-and 13-month-old infants looked longer at test when an actor reached for a toy that they did not prefer during habituation, infants showed no expectations when the habituation and test trials featured different actors.\cite{BURESH2007287}} &  \SetCell[r=1]{l,m}{Can an AI system bind specific preferences for goal objects to specific agents?\cite{gandhi2021baby}}  & \SetCell{l,m}{It was realized by setting different positions in the agent and two targets;} \\
    \hline
    \SetCell[r=1]{l,m}{Infants understand the principle of solidity.\cite{Ren1992The}}  &  \SetCell[r=1]{l,m}{16-month-old infants expected an agent, facing two identical objects, to reach for the one in the container without a lid versus the one in the container with a lid.\cite{doi:10.1177/0956797612457395}} &  \SetCell[r=1]{l,m}{Can an AI system understand that there maybe obstacles that restrict an agent’s actions and that an agent will move to a previously nonpreferred object when their preferred object becomes inaccessible?\cite{gandhi2021baby}}  & \SetCell{l,m}{It was realized by blocking the four obstacles on the preferred object of the agent;} \\
    \hline
    \SetCell[r=1]{l,m}{Infants represent an agent’s sequence of actions as instrumental to achieving a higher-order goal.\cite{2005Pulling}}  &  \SetCell[r=1]{l,m}{12-month-old infants understand an actor’s pulling a cloth as a means to getting the otherwise out-of-reach object placed on it.\cite{2005Pulling}} &  \SetCell[r=1]{l,m}{Can an AI system represent an agent’s sequence of actions as instrumental, directed towards a higher-order goal object?\cite{gandhi2021baby}}  & \SetCell{l,m}{The three experimental settings were no barriers, inconsequential barriers(which did not affect access to the preferred subject), and blocking barriers(which did not allow direct access to the preferred subject and required a key to remove the barrier before continuing access);} \\
    \hline
    \SetCell[r=1]{l,m}{Infants expect agents to move efficiently towards their goals.\cite{Gy1995Taking}}  &  \SetCell[r=1]{l,m}{12-month-old infants repeatedly saw a small circle jumping over an obstacle to get to a big circle. At test, the obstacle was removed, and the small circle either performed the same, now inefficient, action to get to the big circle or performed the straight, now efficient action. Infants were surprised when the agent performed the familiar but inefficient action.\cite{Gy1995Taking}} &  \SetCell[r=1]{l,m}{Can an AI system understand that agents act efficiently towards a goal object?\cite{gandhi2021baby}}  & \SetCell{l,m}{It was realized by removing obstacles, whether the agent acts effectively to achieve the goal;} \\
    \hline[1pt]
    \end{tblr}
    }
    
\end{table*}
In the same year, the AGENT dataset proposed by Tianmin Shu et al.~\cite{shu2021agent} which simulates more physical environments than BIB experiments. Both provide supplementary tools for the core psychological reasoning of machine agents.This dataset is a benchmark of large 3D animation dataset generated by programs, inspired by cognitive development studies in intuitive psychology, to assess the ability of a benchmarking machine to model the mental states of other agents, which are at the heart of human intuitive psychology. The dataset was constructed around four scenarios: goal preference, action efficiency, unobserved constraints, and cost-reward tradeoffs, exploring key concepts in core intuitive psychology. The experimental design of each scenario consists of two stages, the familiarization stage and the test stage, which are used to train and test the modeling ability of the agent respectively. The experimental design is shown in Table~\ref{tab:agent}.


\begin{table*}[h]
\renewcommand\arraystretch{1.2}
\large
    \caption{Experiment Design of AGENT}
    \label{tab:agent}
    \resizebox{\linewidth}{!}{
    \scriptsize
    \begin{tblr}{
        colspec={Q[c,m,0.2\textwidth] Q[c,m,0.3\textwidth] Q[c,m,0.5\textwidth]},
        hspan=minimal,}
    \hline[1pt]
    \SetRow{gray8}    Experiment  &  Psychological Theory & Experiment Design \\
    \hline 
    \SetCell[r=4]{c,m}{Goal Preferences}   &  \SetCell[r=4]{l,m}{Young infants distinguish in their reasoning about human action and object motion.\cite{woodward1998infants}}  & \SetCell{l,m}{1-In familiar and test experiments, the cost of reaching the preferred and secondary targets is the same;} \\
                                &                                   &    \SetCell{l,m}{2-In familiar experiments, the cost of reaching the preferred and the secondary target is the same; in test experiments, the cost of reaching the secondary target is greater;} \\
                                &                                   &    \SetCell{l,m}{3-In familiar experiments, the cost of reaching the preferred target is greater; In test experiments, the cost of reaching both is the same;} \\
                                &                                   &    \SetCell{l,m}{4-In familiar experiments, the cost of reaching the preferred target is greater; In test experiments, the cost of reaching the secondary target is greater;}\\
    \hline
    \SetCell[r=5]{c,m}{Action Efficiency}  &  \SetCell[r=5]{l,m}{*}  & \SetCell{l,m}{1-In familiar experiments, there is gap constraint between the agent and a single target; In test experiments, the constraint is deleted, the expected path is more efficient path, and the surprised path is original path;} \\
                                &                                            &    \SetCell{l,m}{2-In familiar experiments, the agent and the target are placed on both sides of the obstacle, but on the same side in test experiments;} \\
                                &                                            &    \SetCell{l,m}{3-In familiar experiments, the agent and the target are placed on both sides of the higher obstacle, but on both sides of the lower obstacle in the test experiment; } \\
                                &                                            &    \SetCell{l,m}{4-In familiar and test experiments with the same size of the obstacle; In test experiment, the door into the obstacle;}\\
                                &                                           &    \SetCell{l,m}{5-In familiar experiments, the width of obstacles is narrower but is wider in test experiments which ensuring that the model does not simply ignore constraints and predict the path closest to a straight line;}\\
    \hline
    \SetCell[r=2]{c,m}{Unobser-ved constraints}  &  \SetCell[r=2]{l,m}{After seeing an agent that performs a costly action, infants can infer that there must be an unobserved physical constraint that explains this action.\cite{2003One}}  & \SetCell{l,m}{1-Familiar experiments showed that the agent reached the target through a curved path behind the occluder, and the test experiments showed that there was or not was obstacle behind the occluder;} \\
                                &                                           &    \SetCell{l,m}{2-Familiar experiments showed that the agent reached the target through a curved path behind the occluder; Test experiments showed that there were obstacles behind the occluder, but whether or not there were gates on the obstacles.} \\
    \hline
    \SetCell[r=2]{c,m}{Cost-Reward Trade-offs}  &  \SetCell[r=2]{l,m}{Infants can infer what goal objects agents prefer from observing the level of cost they willingly expend for their goals.\cite{2017Ten}}  & \SetCell{l,m}{1-Familiar experiments showed whether the agent was willing to reach the goal through the same obstacle and whether the same goal sets different difficulty obstacles were willing to reach the goal again. No obstacles were placed in test experiments and the distance between the two targets was the same;} \\
                                &                                            &    \SetCell{l,m}{2-Familiar experiments with the experiment above, and test experiments found out which one took a lower price on the preferred target;} \\
    \hline[1pt]
    \end{tblr}
    }
    
\end{table*}


\subsection{Human-robot Interaction}
In the field of human-robot interaction (HRI), most robots communicate information by producing multi-modal behaviors. For example, they use different color lights or hand and head movements to express emotions, or through language to express cognition, etc.
\subsubsection{Nonverbal Communication}
A large part of our social communication is through our non-verbal behaviors such as facial expressions, body language, etc. For example, we nod our heads in affirmation, shake our heads in disapproval or reluctance, and stare at each other intently to show that we are listening to each other, etc. Our ability to express and recognize other's social emotional states with non-verbal communication is the core of social intelligence, and it is an important task to promote human-robot interaction to realize this skill.
A well-known nonverbal communication is the unpredictability of the call response between the speaker's cue and the listener's response in face-to-face communication~\cite{2019A}. The speaker gets feedback from the audience through subtle non-verbal cues such as intonation, gaze direction, pause, etc. The listener then responds to these nonverbal cues, either verbally or nonverbally. 
In 2019, Jin Joo Lee defined a dual computing framework for human-robot interactions~\cite{2019A}. The researchers used Bayesian Theory of Mind to model binary story-telling interactions. The proposed model enables agents to have better attention recognition and the ability to communicate attention effectively. The DBN with a myopic policy is used to model the interaction. The interaction consists of two different roles: the storyteller that uses speech cues to influence and deduce the attention state of the listener and the listener that influences perception through the response and conveying attention. The storyteller gets feedback from the listener by changing non-verbal cues, such as rhythm or gaze direction. Listener responds verbally ("I see") , verbally ("Uh-huh") , and nonverbally (nodding) to the storyteller's suggestion that they want feedback.
The experimental method is a human experiment in which children tell stories to robots. The researchers recruited 14 child-parent pairs to experiment in which children tell stories to robots and the average age of them was 5.63 years old. They were arranged to study before the experiment which is divided into four stages: the first is the teaching stage, in which children are told that they will tell a story to two robots of different colors (blue and red) consecutively. The robots only use sounds such as "beep" or "poof" or "hum". Later, the experimental assistant helps children generate a story about the storybook they choose. Then children tell their stories to the two robots respectively. They were asked to make sure that the two robots noticed their stories. Parents were asked to watch live videos of children interacting with robots and then they filled out a questionnaire about the robot abilities. Parents gave a likert five point to measure their perception of robot active listening skills.
\subsubsection{Object Name Learning}
In 2020, Massimiliano Patacchiola et al. proposed a hybrid cognitive architecture called \emph{Thrive}, aiming to enable robots to provide the ability to distinguish different information providers and learn from reliable sources~\cite{2020A}. The framework is based on the actor critic framework combined with Bayesian network. The researchers embedded the model into the iCub humanoid robot and reproduced two psychological experiments. The research results are consistent with the real data, which reveals the trust based learning mechanism of children and robots.
One of them reproduces the famous tag search task in psychology and is named Object Name Learning. The researchers in this article used the iCub humanoid robot embedded in the model to imitate the internal experimental logic of the tag search task, and realized the experiment that the robot identifies reliable informants and learns object names. The experiment is divided into four different stages. In the process of object learning, the experimenter showed the robot six known objects (balls, cups, books, shoes, dogs and chairs), so that the robot had a basic understanding of objects. In the psychological experiment, the experimenter presents a group of familiar objects to the child. Two different informants have given these things different names. One informant always gives the right name (blue clothes), while another informant always gives the wrong name (red clothes). In this experiment, two informants show the robot a known object and name the object. After getting familiar with it, the psychological experimenter asked the children, "Which of these people is not good at answering questions?". In this experiment, the experimenter asked the same question. The answer is coded as correct only if the robot correctly points out the name of the unreliable informant. In the recognition experiment, two information providers gave different labels to a strange new object, and the children had to choose a name for the new object. In this experiment, two information providers showed a new object to the robot and said, "this is a [object name]". Then the researcher showed the same object to the robot and asked "What is this?" If the name provided by the robot comes from an unreliable source or a different name, it is considered to be the wrong answer. Figure~\ref{fig:Learn_name} shows this process.
\begin{figure}[ht]

\centering
\includegraphics[width=0.50\textwidth]{figures/Learn_name.png}
\caption{Learn object name after the psychological experiment\cite{2020A}.}
\label{fig:Learn_name}
\end{figure}

\section{Datasets} 
With the deepening of research on machine ToM, datasets based on ToM have been proposed for different tasks with various forms. 
Figure \ref{fig:dataset_category} shows the categories of tasks in datasets of ToM. The datasets can be divided into belief reasoning, desire reasoning, intention reasoning and joint reasoning datasets according their tasks. At present, there are many belief reasoning datasets and intention reasoning datasets, but few is dedicated to desire reasoning. Among them, the main task of belief reasoning datasets is aim to evaluate the ability of false belief detection. Intention reasoning datasets are accompanied by AI interaction because reasoning human's intentions is the first stage during human-robot interation. The datasets of joint reasoning are mainly carried out in 2D grid and recently some of them are conducted in 3D animation.

The ideal source of datasets would be generated from human being, but the data collection process from human is expensive. Besides, human mental states are complex and it is possible to introduce noise when collecting real data about human. In addition, our aim is to evaluate the machine ToM, and we except that the model focuses on reasoning about human mental states rather than processing data (such as the recognition of objects in images). Therefore, most datasets are synthetic data presented in 2D or 3D animation formats.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{figures/data_static.pdf}
\caption{Datasets of ToM and categories. Datesets (white and grey circles) gravitate around the ToM category (colored circles) to which they pertain. The darker the color of the circle, the newer the dataset is. The joint datasets are located orbital overlap of each related concept.}
\label{fig:dataset_category}
\end{figure}
% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\subsection{Datasets for Belief Reasoning Tasks}
Most of the datasets are devoted to instantiate Sally-Anne test to evaluate the ability of belief reasoning. Grant et al.~\cite{grant2017can} proposed a question-answer dataset formalized Sally-Anne test in natural language stories by asking a series of questions about belief and created the first benchmarks about false-belief reasoning. Although some research have improved this dataset to reduce its predictable regularities, they still use templates to generate data, making less diversity between samples. Table~\ref{tab:tom-bAbi} shows an example of this QA tasks.
In addition, there are some image and video datasets, which are more challenging than text datasets because images implicitly describe the human perception of the entire world.


The QA dataset of Grant et al.~\cite{grant2017can} for belief reasoning is in the form of natural language stories that are accompanied by questions about the state of world described in the story. To generate stories and corresponding questions, they emulate the Facebook bAbi dataset~\cite{weston2015towards_facebook_bAbi}) generation procedure.
They defined three story templates to generate data-observable beliefs, unobservable actions, observable actions. In addition to setting the template, each story is also configured with two versions of true belief and false belief. In true belief stories, the agent has beliefs consistent with the real world while; in false belief, due to his partial observation, the agent has a false belief. Thus, there six different conditions by varying their template type as well as true/false beliefs.


\begin{table}[htbp]
 \caption{Examples for BAbi-style QA Task From ToM-bAbi\cite{2018Evaluating}}
    \label{tab:tom-bAbi}
    \renewcommand{\arraystretch}{1.2}
    \normalsize
    \scalebox{0.78}{
    \begin{tabular}{|l|l|l|l|l|l}
        \hline
        \multicolumn{6}{c}{{\makebox[0.6\textwidth][c]{\textbf{Story}}}}\\
        \hline
        \multicolumn{3}{c}{\makebox[0.1\textwidth][c]{True Belief}}&\multicolumn{3}{c}{False Belief}\\
        \hline
        \multicolumn{3}{c}{Anne entered the kitchen.}&\multicolumn{3}{c}{Anne entered the kitchen.}\\
        \multicolumn{3}{c}{Sally entered the kitchen.}&\multicolumn{3}{c}{Sally entered the kitchen.}\\
        \multicolumn{3}{c}{The milk is in the fridge.}&\multicolumn{3}{c}{The milk is in the fridge.}\\
        \multicolumn{3}{c}{Anne moved the milk to the pantry.}&\multicolumn{3}{c}{ Sally exited the kitchen.}\\
        \multicolumn{3}{c}{}&\multicolumn{3}{c}{Anne moved the milk to the pantry. }\\
        \hline
        \multicolumn{6}{c}{\textbf{Question}}\\
        \hline
        \multicolumn{2}{c}{Memory}&\multicolumn{4}{c}{Where was the milk at the beginning?}\\
        \multicolumn{2}{c}{Reality}&\multicolumn{4}{c}{Where is the milk really?}\\
        \multicolumn{2}{c}{First-order}&\multicolumn{4}{c}{Where will Sally look for the milk?}\\
        \multicolumn{2}{c}{Second-order}&\multicolumn{4}{c}{ Where does Anne think that Sally searches for the milk?}\\
        \hline
    \end{tabular}
    }
\end{table}

The task of the dataset (Grant et al.\cite{grant2017can}) can not guarantee that the model understands the state of the world, because the model only needs to correctly answer one questions of one task. Nematzade et al.\cite{nematzadeh2018evaluating} proposed tom-bAbi dataset, which the model needs to answer the initial location of the object and the correct current location, so that we can distinguish whether the model answered the question correctly by chance or has an understanding of the state of the world. Besides, this datasets include second-order belief question such as "where does Anne think that Sally xxx ?" ToM-bAbi is generated in a similar way to Facebook bAbi dataset and it added noise with unrelated sentences to the story as noise, such as "the telephone rings"  . 


The two datasets mentioned above are generated using a template approach, and different generators are used for different tasks, so that the model can easily distinguish the type of task and find the correct answer, even though tom-bAbi dataset added noise in story. Le et al. \cite{le2019revisiting} proposed tomi dataset to reduce regularity in tom-bAbi dataset. Firsty, they use the same randomized generation method for all stories to avoid biases and create a balanced dataset over all types of stories. Second, adding random distractors, such as unrelated action, distractors statements about locations and objects leads to stories a lot less predictable. Third, they increase the difficulty of the dataset evaluation, only if the model can correctly answer all question types, can the model be considered to correctly understand and infer the state of the world and the mental state of the subject.


In addition to the QA dataset, there are also several datasets of belief reasoning with images. Eysenbach \cite{eysenbach2016mistaken} provides a dataset of false belief detection generated in everyday life. A sample of the dataset consists of eight frames in which characters have false beliefs about their environment. Characters in the video don't know they have the false belief, but a third-party agent can clearly tell who has the false belief. They asked crowdsourced worker to generate and annotate video scenes. For each person in each frame, workers had to answer whether the person had a false belief. Ultimately, 1213 scenarios were collected and passed quality control. This dataset can perform three tasks 1) who is mistaken 2) when are they mistaken to perform 3) joint reasoning on the problem of who and when.


BOSS\cite{duan2022boss} dataset is a benchmark for human belief prediction in object-context scenarios that provides 900 videos of social interactions, in addition to providing information on gaze, pose and contextual information in the environment. The two participants were required to accomplish a collaborative task by inferring and interpreting each other’s beliefs through non-verbal communication.


\subsection{Datasets for Intention Reasoning Tasks}
There exist more rich datasets in intention reasoning tasks, in forms of 2D grid, 3D animation and videos from human. 
In 2019, Carroll et al.\cite{strouse2021collaborating} published a dataset to evaluate whether AI can infer human intentions and cooperate with the human to complete a specific task in 2D grid, a popular game overcooked, in which players control chefs in a kitchen to cook and serve dishes. Carroll et al.\cite{strouse2021collaborating} created a web interface for the game to collect trajectories of humans. For each layout it gathered ~16 human-human trajectories (for a total of 18k environment time steps). It partitions the joint trajectories into two subsets, and split each trajectory into two single-agent trajectories. In recent years, many simulation platforms have been proposed, which can be used for human-robot interaction environment, some of which support 3D realistic scenes, with objects interacting and human-like agent. In Duan et al.\cite{duan2022surveyembodies}, the existing embodied AI platforms in recent years are compared and summarized. Compared with other platforms, there are fewer objects and it is a grid environment in overcooked, so the scene is relatively single. Only eight different scenarios are included in this dataset.


Puig et al.\cite{puig2020watch_WAH} presented a challenge called Watch And Help (WAH) for social perception and human-robot collaboration. Similar to overcooked tasks, AI needs to infer the intention of a human to accomplish the cooperation task. The task is a two-step process - watch stage and help stage. The AI first watches a video of Alice (a human like agent) successfully performing a household activity in a 3D scene, and then in a new scene, AI needs to help Alice to achieve the same goal with a minimum number of steps.
WAH designed a multi-agent simulation platform and created a benchmark containing 1011 tasks and 2 test sets. The dataset provided scene graph and behavior trajectories of each task, and we could use the emulator to generate videos of each task.


TIA \cite{wei2018and} is also a household dataset based on joint reasoning of tasks, intentions and attention. It provides 809 videos where a character performs a task and ask three questions: 1) where the human is looking - attention prediction; 2) why the human is looking there - intention prediction; and 3) what task the human is performing - task recognition. Each frame includes four types of data: the RGB image at resolution of 1920 $\times$ 1080, the depth image, the 3D human skeleton, and the egocentric RGB image at resolution of 1280 $\times$ 960 used for annotating the ground-truth attention points.


In contrast to overcooked, WAH and TIA datasets both provide large scale 3D data and involve more diverse scenarios in kitchen, living room and bath room. TIA dataset needs to answer the question of what is the intention of human. Overcooked pays attention to the interaction between AI and human, and it needs to infer human intention and cooperate with human, as a two-stage task.

\subsection{Datasets for Desire Reasoning Tasks}
At present, there are few datasets based on desire reasoning, and only one dataset focused on desire reasoning called MSED\cite{jia2022beyond_desire} proposed in 2022.


MSED is the first multi-modal dataset annotated with three sentiment classes, six emotion classes and six desire classes. The raw data was collected from online photo-sharing resources (i.e., Getty Image, Flickr and Twitter) by retrieving keywords with based on 16 basic desires theory, e.g., curiosity, romance, family, vengeance etc.
They crawled retrieved text-image posts on the first ten pages. After data filter, MSED dataset contains 9,190 text-image pairs, with 4683 samples for desire infer.
\subsection{Datasets for Joint Reasoning}
Baker et al.\cite{baker2017rational_BTOM} proposed an available dataset in 2D grid environment used for belief-desire joint reasoning. 73 scenarios are manually designed for the training of Bayesian network. Because this dataset is a small-scale designed to evaluate the reasoning ability of a Bayesian-based model and can not be used in deep-learing-based model.


BIB\cite{gandhi2021baby}and AGENT (Shu et al., 2021)\cite{shu2021agent} are large-scale and synthetic dataset of 3D video which are suitable for machine learning.
The scenes of BIB are conducted in 2D grid environment, and the dataset provides 2000 videos with both 2D and 3D versions of the stimuli rendered and scene configuration files describing the objects and agents present in the scene.
There are 8400 videos of 3D animations in AGENT of an agent moving under various physical constraints and interacting with various objects. There exists conceptual overlap with BIB while with distinct concepts such as unobserved constraints and cost-reward trade-off. In addition to that, AGNET is in 3D continuous space, so more diverse objects such as ramps, platforms, doors, and bridges, can be added to the scene while BIB contains scenes in maze-like environments that require more limited knowledge of physical constraints: mazes with walls. On the other hand, AGENT involves training on many different leave-out splits, where most splits have relatively minor differences between training and testing. AGENT and BIB provide complementary tools for benchmark machine agents’ core psychology reasoning, and relevant models could make use of both.

\section{Models}
The existing models on machine ToM are mainly divided into two directions: "model-based" and "cue-based". The "model-based" approaches build a causal relationship of  human mental states and then infer mental states from observations. The "cue-based" approaches assume that mentalizing is based on a direct mapping from behaviors to mental states. "Model-based" approaches are represented by Bayesian-based approaches and inverse reinforcement learning (IRL), while "cue-based" approaches are mainly represented by deep-learning-based approaches. In this article, we will introduce Bayesian-based approaches and deep-learning-based approaches in detail. Bayesian-based approaches have good explanatory because they model human mental states, but they can not fully reflect human mental states because they use a simplified ToM model. The deep-learning-based approaches are often applicable to the end-to-end training method. They directly map the action and mental states, and can get implicit mental states. In this section, we focus on two mainstream approaches and 
 the existing models are compared in Table \ref{model_compare} for the four approaches mentioned in this paper.
\begin{center}
\begin{table*}[h]
% \large
\normalsize

\renewcommand\arraystretch{1.3}
\caption{Models in theory of mind}
\centering
 % \resizebox{\textwidth}{
\resizebox{2\columnwidth}{!}{
%引入tabular环境
\begin{tabular}{m{1cm}<{\raggedright}|m{2cm}<{\raggedright}|m{3cm}<{\raggedright}m{4cm}<{\raggedright}m{7cm}<{\raggedright}m{7cm}<{\raggedright}}
% \begin{tabular}{p{0.5cm}|p{2cm}|p{4cm}p{6cm}p{9cm}p{6cm}}
%添加顶部横线 
\hline
%输入标题
% \specialrule{0.00em}{3pt}{3pt}
\textbf{Year} & \textbf{Model} & \textbf{Scene} & \textbf{Purpose} & \textbf{Advantage} & Limitation\\
% \specialrule{0.00em}{3pt}{3pt}

%添加标题和内容之间的横线
\hline
% \specialrule{0.00em}{2pt}{2pt}
\multicolumn{6}{c}{\textit{Deep-Learning-Based Approaches}}\\
% \specialrule{0.00em}{2pt}{2pt}
 % & & & &Deep-Learning-Based Approaches& & \\
\hline
2018 & ToMnet\cite{2018Machine}	& 2D grid& By learning from the past trajectory of an agent to infer mental states and predict the next actions in a new scene& Learning the mental state of agents with different behavior
	& A large amount of data is required for training\\
\hline	
2021	&ToMnet-G\cite{shu2021agent}	&3D scene&To predict the trajectory of an agent in a 3D scene &Extending ToMnet with GNN and pushes predictive scenarios from 2D to 3D
	&Lack of strong generalization ability within and across scenarios (once trained, only predicted under the same layout of the same scenario)	\\
\hline	
2020  & ToMnet+\cite{chuang2020using}	& 2D grid	& Inferring Human Social Interaction Preference by Using Computer Model to Observe the Behavior Data of the Third Person & Added preference inference based on ToMnet to infer human implicit preferences
&Predictions relying on learning from previous trajectory features and cannot be migrated \\
\hline
2022	&Trait-ToM\cite{nguyen2022learning}	&2D grid& Inferring other actors' mental states and goals by observing their past and current behavior.&	Proposing stable personality traits hold key prior information that affects mental states, achieved through 'quick weights', with significantly better performance than ToMnet	&Predictions rely on learning from previous trajectory features and cannot be migrated \\ 
\hline
% \specialrule{0.00em}{2pt}{2pt}
\multicolumn{6}{c}{\textit{Bayesian-Based Approaches}}\\
% \specialrule{0.00em}{2pt}{2pt}
 % & & & && & \\
\hline
2017&BTOM\cite{baker2017rational}	&Food-trucks 2D grid	&Joint reasoning of beliefs and desires  &	Extending the classical expectation-utility surrogate model to sequential actions in complex, partially observable domains	& Uniform distribution for belief hypothesis and unapplicable for general reasoning\\
\hline
2019&ATOM\cite{narang2019inferring}	& 3D multi-agent AR environment  & Inferring shared intention in a virtual environment& Explaining the underlying irrationality of human behavior and the dynamic nature of individual intentions & Inference ability and gaze response were limited to a single user, and there was no dependency between annotation and motion cues \\
\hline
2020&	MMDP\cite{2020Too}	& 2D grid & Inferring user intention and taking action to complete a cooperative task & With agents sharing tasks and conflict avoidance in multi-agent cooperation considered & Assumeing that humans are fully rational and does not take into account the false beliefs during cooperation\\
\hline

2019 & Speaker and listener\cite{2019A}	&Human-robot interaction experiment& Improved attention recognition method&Incorporating the intentional posture of its interacting partner into the intention infer&	Less performance gains and limited accuracy that can be extracted by modeling these advanced behaviors result in more narrow improvements\\
\hline
2020	&Thrive\cite{2020A}	&Human-robot interaction trust experiment&An integration of recent findings on potential mechanisms of trust in a computational model&	A demonstration of that ToM is important for building empathy trust, and uncovered trust-based learning mechanisms in children and robots.&Fewer influencing factors involved in trust-based learning\\
\hline
2022	&XAI\cite{yuan2022situ}	&Human-robot interaction game scene	&Proving the interpretability of mental theory&	Proof of the interpretability of mental theory by human-robot interaction	&Machines' overly rich interpretation of the decision-making process will be unacceptable to humans\\	
\hline
% \specialrule{0.00em}{2pt}{2pt}
\multicolumn{6}{c}{\textit{Epistemic-Logic-Based Approach}}\\
% \specialrule{0.00em}{2pt}{2pt}
 % & & & && & \\
\hline
2020	&CogTom\cite{grassiotto2021cogtom}&
Simple Text&  
First  order false belief detection
&False order beliefs can be detected.& Only symbolic language as input and first order belief detection\\
\hline
2020	&Lasse\cite{2020Implementing}&
images&  
First- and higher-order beliefs inference
&First- and second- order beliefs can be inferred dynamically.& Not incorporating other aspects of ToM such as intentions and desires.\\
\hline
% \specialrule{0.00em}{2pt}{2pt}
\multicolumn{6}{c}{\textit{Inverse-Reinforcement-Learning-Based Approach}}\\
% \specialrule{0.00em}{2pt}{2pt}
\hline
2018	&Dhruv\cite{2018An}&
CIRL game&  
Value alignment
&Reducing the complexity of IRL model and relax CIRL’s assumption of human rational-
ity.& A simple game setting with the low dimensional and discrete actions.\\
\hline

2021 &Tian\cite{Tian2021Learning}&
Pac-man game in 2D grid&  
Value alignment in multi-agent settings
&Considering intelligence levels when reasoning about ToM.& A discrete states and actions, and the high-cost computation complexity.\\
\hline

%添加底部横线
\end{tabular}
}
\label{model_compare}
\end{table*}

\end{center}
\subsection{Deep-Learning-Based Approaches}
%add method state
\subsubsection{ToMnet}
DeepMind researchers have proposed a new neural network, ToMnet\cite{2018Machine} (ToMnet architecture
shown in Figure \ref{fig_tom}.), with the ability to understand its own mental state as well as that of the others. Agents with different behavioral characters collect colored squares of their targets and generate trajectories. ToMnet can observe the 
 history trajectories that represent ToM of an agent and predict the next action for the agent in a new scene. ToMnet is composed of three modules: \textit{a character net}, \textit{a mental state net}, and \textit{a prediction net}, each containing a number of small computational units and connections learned from experience, similar to the human brain. \textit{Character net} extracts the characters of agents based on their past actions represented by $e_{char}$ . \textit{Mental state net} understands the current mental state of agents $e_{mental}$; \textit{Prediction net} receives $e_{char}$ and $e_{mental}$, and speculates on the subsequent actions depending on the new situation and their characters. One of the limitations is that this model requires a large amount of data for training. Notwithstanding its limitations, it is shown that ToMnet can flexibly infer ToM over a range of different species of agents and has good generalization when mastering character over a new specie of agents that ToMnet has never seen.

 Chuang et al. proposed the ToMnet+ \cite{chuang2020using} adapted by ToMnet to infer the social network by observing the interactions between agents and targets. ToMnet+ tries to infer the current agent's social network from the past trajectories, and uses the inferred social network to help predict the best target for the agent in obtaining social support. Experiments have shown that using the ToMnet+ model to predict where an agent wants to go is over 80\% accurate.

\begin{figure}[htbp]
    \centering
    % includegraphics[width=0.5\textwidth,trim=50 50 50 50,clip]{figures/logo.pdf}

    \subfloat[ToMnet architecture]{\includegraphics[width=0.60\textwidth,trim=0 0 350 220,clip]{figures/tom.pdf}\label{fig: sub_figure1}%
    \label{fig_tom}}
    \hfil
    \subfloat[Trait-ToM architecture]{\includegraphics[width=0.49\textwidth,trim=40 30 50 40,clip]{figures/trait_tom.pdf}\label{fig: sub_figure3}
    \label{fig_trait}}
    \hfil
    \subfloat[ToMnet-G architecture]{\includegraphics[width=0.47\textwidth,trim=33 30 63 40,clip]{figures/tomnet_G.pdf}\label{fig: sub_figure4}
    \label{fig_tomnet_G}}
    \caption{The architecture of ToMnet \cite{2018Machine}, Trait-ToM\cite{nguyen2022learning} and ToMnet-G\cite{shu2021agent}. The three model are both consist of three modules: $character$ $net$ (the purple block), $mental$ $state$ $net$ (the orange block), and $prediction$ $net$ (the blue block). The $character$ $net$ parses an
    agent’s past trajectories. The $mental$ $state$ $net$ parses the agent’s trajectory on the current episode. These features from  $character$ $net$ and $mental$ $state$ $net$ are fed into the prediction net. The $prediction$ $net$ predictions about future behaviour in current state. ToMnet uses vector to represent agent's behaviour character and mental state while Trait-Tom uses fast weights.}
    \label{fig_model}
\end{figure}

\subsubsection{Trait-ToM}
Dung Nguyen et al. proposed the architecture of Trait-ToM \cite{nguyen2022learning} model. There are three sub-models (shown in Figure \ref{fig_trait}) where the character net is called \textit{trait model} and others are like ToMnet. Trait-ToM outputs intention as one of the label, while ToMnet does not. And unlike ToMnet which uses a vector to represent the character, Trait-ToM uses fast weights computed by \textit{trait model}. The fast-weight traits can directly modulate the function of the prediction net through multiplicative mechanisms and are higher in accuracy than ToMnet. In \textit{trait model}, a long-term and short-term memory network is used to maintain a dynamic state vector at each time step $t$ for every trajectory from state-action pair embedding. Finally, after averaging all past trajectories embedding, the hypernetwork is used to generate weights and biases, and feeds them to the prediction network as a parameter of this network with $e_{char}$. 
It is shown in Table \ref{tab:tom_and_trait_tom_result} that the Trait-ToM  outperforms the ToMnet in predicting the intention, especially in more realistic agent (S1 and S2).

\subsubsection{ToMnet-G}
Previous studies of deep-learning-based approaches of machine ToM  have not dealt with 3D scene. Tianmin Shu et al. extended ToMnet to predict the trajectory in a 3D scene, called ToMnet-G \cite{shu2021agent} (shown in Figture \ref{fig_tomnet_G}). ToMnet-G uses GNN to encode the states in a scene, using nodes to represent all objects in the scene and edges to represent the interaction between objects. Node information include objects types, bounding boxes, and colors. In order to better apply ToMnet-G to partially observable scene, it reconstructs the initial transitions of the past video and the test video. By sampling the trajectory, the network is trained using the mean square error loss of trajectory prediction. This model is suitable for 3D scene and predicts a continuous trajectory. The result shows that BIPaCk (a Bayesian-based model) outperforms ToMnet-G in almost all conditions, but BIPaCk requires an accurate reconstruction of the 3D state and a built-in model of the physical dynamics, which will not necessarily be available in real world scenes.

 \begin{table}[h]
 \caption{ToMnet and Trait-ToM Performance.} 
\label{tab:tom_and_trait_tom_result}
\tabcolsep=0.08cm
\renewcommand\arraystretch{1.2}

\scalebox{0.82}{
\begin{tabular}{llllllll}
\hline
\multirow{3}{*}{Stream} & \multirow{3}{*}{Model}     & \multicolumn{2}{l}{Random-Actor}       & \multicolumn{2}{l}{Hypo-Actor}         & \multicolumn{2}{l}{Hypo-Actor}            \\
                        &                            & \multicolumn{2}{l}{(Full Observation)} & \multicolumn{2}{l}{(Full Observation)} & \multicolumn{2}{l}{(Partial Observation)} \\ \cline{3-8} 
                        &                            & Action             & Intention         & Action             & Intention         & Action              & Intention           \\ \hline
\multirow{4}{*}{S1}     & \multirow{2}{*}{ToMnet}    & 37.8               & 53.21             & 54.26              & 53.43             & 49.65               & 51.94               \\
                        &                            & (0.64)             & (2.83)            & (0.94)             & (3.18)            & (0.96)              & (2.71)              \\ \cline{2-8} 
                        & \multirow{2}{*}{Trait-ToM} & 39.46              & 55.64             & 54.37              & 56.45             & 50.93               & 55.21               \\
                        &                            & (0.47)             & (3.02)            & (1.09)             & (2.48)            & (1.10)              & (2.43)              \\ \hline
\multirow{4}{*}{S2}     & \multirow{2}{*}{ToMnet}    & 32.48              & 36.6              & 50.29              & 46.59             & 44.3                & 43.96               \\
                        &                            & (1.71)             & (3.84)            & (2.32)             & (2.67)            & (2.45)              & (3.09)              \\ \cline{2-8} 
                        & \multirow{2}{*}{Trait-ToM} & 38.22              & 47.26             & 52.85              & 54.6              & 48.8                & 53.32               \\
                        &                            & (1.25)             & (2.77)            & (0.81)             & (2.35)            & (1.20)              & (2.30)              \\ \hline
\multirow{4}{*}{M}      & \multirow{2}{*}{ToMnet}    & 39.3               & 88.14             & 59.55              & 75.92             & 54.13               & 74.71               \\
                        &                            & (0.34)             & (1.05)            & (1.46)             & (2.35)            & (1.23)              & (1.83)              \\ \cline{2-8} 
                        & \multirow{2}{*}{Trait-ToM} & \textbf{40.84}     & \textbf{90.95}    & \textbf{60.53}     & \textbf{82.1}     & \textbf{55.52}      & \textbf{79.85}      \\
                        &                            & \textbf{(0.42)}    & \textbf{(0.57)}   & \textbf{(2.15)}    & \textbf{(1.52)}   & \textbf{(1.71)}     & \textbf{(1.11)}     \\ \hline
\multicolumn{8}{p{37em}}{\small $\bullet$ The two models are trained with various training setting (M, S1 and S1, for more details, see \cite{nguyen2022learning}, Table 1. S1 and S2 are more realistic scenarios, e.g. the observer can only see one type of agent at a time. In the mixed setting (M), agents in one batch are i.i.d sampled from 32 types of agent.}

\end{tabular}
}

\end{table}

\subsection{Bayesian-Based Approaches}
\subsubsection{BToM}
\label{sec:btom}
Baker et al.(2017) proposed the Bayesian Theory of Mind (BToM) \cite{baker2017rational_BTOM} model to explain human's beliefs and desires (More details about the experiment  can be found in Section \ref{sec:food_truck}).
BToM formalizes mentalization as Bayesian inference for a generative model of a rational agent. BToM uses POMDP to represent the agent’s planning and inference about the world. Figure \ref{fig:Folk-psychological schema for theory of mind} shows a simplified ToM framework of an agent-based for rational planning and state estimation, inspired by the classical theory of expected utility maximization decision making, but extended to the scene where agents plan uncertain actions in space and time due to incomplete information. 
POMDP captures three central causal principles of the core emphasized by the figure as follows. (I) A rational actor forms perceptions that are a rational function of the state of the world, their own state, and the nature of their perceptual apparatus - for a visual-guided actor, anything in their sight should be registered in their world model (Perception) . (II) The formation of beliefs, which are rational inferences based on the combination of their perceptions and their prior knowledge (Inference). (III) Plans for a reasonable sequence of actions - actions  based on their beliefs that can be expected to effectively and reliably achieve their aspirations (Planning).

BToM combines the POMDP generation model with the hypothesis space of candidate mental states and the prior space of these hypotheses. In the case of a given agent's behavior in a situational environment, Bayesian inference is performed on beliefs, desires, and perceptions.

\begin{figure}[htbp]
\centering
 \includegraphics[width=0.35\textwidth,trim=550 160 0 0,clip]{figures/BTOM.pdf}
\caption{A simplified ToM framework in BTOM \cite{baker2017rational}. Agent in BTOM, the observer views human as rational planner. Agent is observable to world state, human state and actions (outside the blue box), but unobservable to the mental states and process of inference ( inside the blue box).}
\label{fig:Folk-psychological schema for theory of mind}
\end{figure}
\subsubsection{MMDP}
Sarah et al. \cite{2020Too} develop hierarchical planning architecture, a learning mechanism that enables agents to rapidly infer the intentions of other agents in a multi-agents cooperative task. Similar to BTOM, it views the agent decision-making process as a Markov decision process (MDP) and the model is a multi-agent MDP (MMDP). The architecture builds on two steps: high-level planning to infer others intention and decide its own intention, low-level planning to select action. In high-level planning, each agent views others as MDP and infers their intention by inverse planning. Low-level planning takes the sub-tasks selected by high-level planning and the next best action while modeling the movements of other agents. In this step, agents need to address two types of low-level coordination problems: (1) avoiding collisions while working on distinct sub-tasks, and (2) cooperating as necessary to solve a shared sub-task. When working on distinct sub-tasks, the agent treats others as static except for the agent who has collisions sub-task, so to simplify the model and task the best responds. When working a shared sub-task, it computes joint policies to view its and the other's decisions as a whole. While Bayesian delegation solves some aspects of commonsense coordination, one challenge is that when
agents share sub-task, they have no way of knowing when they have completed their individual "part" of the joint effort.

\subsubsection{AToM}
Sahil Narang et al. proposed a real-time algorithm called Agents with Theory of Mind (AToM) \cite{narang2019inferring} using Bayesian theory of mind, which can infer the hidden intentions of user avatars in a virtual environment based on observed proxemics and gaze-based cues. This method provides users with the first view in the multi-agent virtual environment. 
% The potential intentions of users mainly include two kinds, letting agents interact face-to-face in the static state, or avoiding agents. 
Each virtual agent independently perceives the user's social cues, using information such as theory of mind, gaze, actions, and expressions to infer the user's hidden intentions using Bayesian Theory of Mind based on the Markov assumption. Then the agent uses a velocity-space reasoning algorithm to compute velocity with respect
to visible entities in its neighborhood, including other agents and the
user and generates gaze-based response. 
% Sahil Narang et al. set the following metrics to assess the inference capabilities of AToM: 1) error detection rate, i.e., the proportion of agents making inference errors about user intentions; 2) error inference interval, the maximum continuous time interval for each agent to mis infer the interaction intention; 3) target inference detection time, the target agent correctly infers the earliest interaction intention. The target inference determines the time, and the target agent in the simulation consistently infers the latest time of the interaction intention.
% The ATOM model is evaluated on the following four benchmarks, Standing Agents, Crossing Flow, Anti-podal Circle Crossing, and Shibuya Crossing.
The evaluation results show that the performance is significantly better than the previous methods. 
The previous methods are largely limited to the 2D scene with discrete actions. In contrast, this work focuses on role and multi-agent interactions in more complex environments with continuous action spaces. However, there are still some shortcomings in this model. For example, it defines intention simply based on observable features: the gaze and movement, which makes it easier to infer intentions.
\subsubsection{XAI}
Yuan, Luyao, et al. propose an explainable artificial intelligence (XAI) \cite{yuan2022situ} 
system. In the existing XAI system, human users’ active interactions or inputs to the system only influence explanations of robots’ decisions but rarely influence the model’s decision-making process.
In this study, the author designed a clever \textit{human-machine collaborative exploration} game to explore the process of value alignment between robots and humans (Figure \ref{fig:xai} shows the algorithmic flow of the computational model).
% Without knowing the value function, the robot scouts (as a team) need quickly infer the user’s value and make three movement proposals in each step, one for each scout and can either accept or reject a proposal.
It is not difficult for the robots to simplify this game to POMDP setting and propose the optimal plan, if the robot scouts already know about the user’s value function. 
Thus, the difficulty is that robots need to align with the human values with sparsely supervised interactions.
Given that the user tends to provide helpful pedagogical feedback to facilitate alignment, the robots adopt a human-centric amelioration of iterative teacher-aware learning (ITAL)\cite{yuan2021iterative} to learn the value function.
It integrates two levels of ToM. Here, it is referred to the ability to infer humans' value from their actions as level-1 ToM and the comprehension of explicit information in users' feedback as level-2 ToM.
In each step of interaction, the robots present proposals and customized explanations, 
that reveal robots’ current estimation of human values and justify the proposed plan.
 After the users seed feedback for the proposal and explanations, the system formulates the overall generation as a hidden Markov model-based sequential generation process capable of adopting the temporal dynamics of the human user’s explanation utility.
\begin{figure}[htbp]
    \centering
    % \includegraphics[]
    \includegraphics[width=0.54\textwidth,trim=48 120 0 0,clip]{figures/XAI.pdf}
    \caption{Algorithmic flow of the computational model. Given game observations and human feedback to previous proposals, the robots update their mental state and make new task plans. On the basis of the plans and current beliefs, new proposals and explanations are generated and sent to the human for feedback in the next round.}
    \label{fig:xai}
\end{figure}

% \subsection{Others}
% \subsubsection{CogToM}
% Fabio Grassiotto et al. proposed a cognitive architecture for the theory of mind (CogToM) \cite{grassiotto2021cogtom}, which realized the conversion from intention to belief by manually establishing a one-to-one mapping between them, and completed the Sally-Anne test and a number of the Facebook bAbI dataset tasks \cite{weston2015towards_facebook_bAbi}.
% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.40\textwidth]{figures/CogTom_structure.png}
% \caption{CogToM Processing Blocks}
% \label{fig:CogToM Processing Blocks}
% \end{figure}
% The architecture of CogTom (shown in Figure \ref{fig:CogToM Processing Blocks}) simulates the minddreaming model  (Baron Cohen, 1997\cite{baron1997mindblindness}). The Intentionality Detector (ID) extracts agents from entities identified by external systems and establishes a list of agents. The Eye Direction Detector (EDD) converts the objects seen by the eyes of each agent obtained by the external system into a more easily handled representation. The Shared Attention Mechanism (SAM) extracts the objects that are noticed by multiple agents according to the EDD results. The Theory of Mind Mechanism (ToMM) refers to the results of EDD to generate the belief of each agent on each visible object. In addition, there are Intention Handler (IH) and Affordance Handler (AH) modifying the set of beliefs in the ToMM module. IH establishes the mapping relationship between intention and belief to modify belief according to intention, and AH establishes the mapping relationship between affordances (McClelland, 2017, the behavior provided by things may let people know what they can be used for) and belief to modify belief according to affordances. Finally, a Memory System stores the information generated by each module.

% The authors decomposed each task into several scenes, and simulated the output of the external system under each scene as the input of CogTom using five tables (entities, positioning, eye directions, affordances, intentions). CogTom traverses every scene. If the agent can see the object in a certain scene, it will regard the object's affordance as the initial state of the agent's belief in the object (that is, the agent knows what the object can be used to do). If there is an intention related to the object at this time, it will update the belief through IH according to the logic established manually in advance. CogTom provides a console for outputting information. Users can query agent's beliefs, object related beliefs, and the location of each object in each scene.

% CogTom proposed a good architecture, simulated the mindreading model and introduced the concept of affordance in psychology. By judging whether the agent can see the object, it decides whether to change the agent's belief in the object according to the intention, and then completes the false-belief task. Although it can complete the Sally-Anne task and several the Facebook bAbI tasks, it is achieved by manually establishing the connection between intention and belief. CogTom is not an expected universal model that can enable the machine to learn how to convert intentions into beliefs by itself. In addition, CogTom requires a lot of input and relies heavily on the processing of external systems.
\section{Summary}
This paper gives a comprehensive review on machine ToM in terms of beliefs, desires and intentions, covering the experiments, datasets and models in deep-learning-based approaches and Bayesian-based approaches. In this paper, the psychological experiments commonly used in recent years in machine ToM are described, and the variation of psychological experiments is summarized and compared. It is simple and easy to use grid experiment to reproduce psychological experiment. Thus, most of these experiments are based on grid experiments and psychological experiments as theoretical basis for model evaluation. Although many experiments are supported by psychological experiments, there is a lack of machine experimental paradigm for ToM. In addition, we find that the format and scale of datasets are constantly developing in recent years. The datasets extend from simple 2D gird to animation, and real 3D scenes and there are several large-scale datasets applicable for deep-learning-based approaches. There have not yet been a universal dataset that can be used for the evaluation of the three concepts. Finally, we compare the existing models. Both methods have their own advantages and limitations. At present, deep-learning-based approaches represented by the ToMnet framework are constantly proposed to solve different problems, but researchers still have to find a way to address the problems of the interpretability and the need for a large amount of training data for deep-learning-based model. Bayesian-based approaches also has some serious limitations. Once the networks are build, Bayesian-based approaches are difficult to extend and not as flexible as deep-learning-based approaches. Besides, there remains the issue of reflecting human's ToM in real life with a complex decision-making process instead of a simplified ToM.
% In addition, Bayesian-based approaches depend on the accuracy of the previous data processing,thus they are not suitable for real world.
Despite quite a few existing research, there is little work to compare the performance of the two approaches.
% As a cognitive ability, ToM can handle many tasks in our daily life without a spefic dataform, such natural language, eye gestures, movements, etc. Therefore, the evaluation of ToM can be adapted from psychological experiments, or through the way of human-robot interaction. Either way, we need to be able to take a current task and generate a unified dataset, preferably on a large scale, in order to better examine the deep learning model, and we want this data set to be able to measure tom's various capabilities.

Generally, as a cognitive ability, ToM comprises many tasks in our daily life and can be formalized in different data formats, such as natural language, images, logical forms, etc. As a result, there are currently no clearly defined tasks and benchmark evaluation, leading to the difficulty that the proposed models can not be uniformly compared.
% The evaluation of ToM can be adapted from psychological experiments, or through the way of human-robot interaction. 

Solutions to this problem are now widely discussed. We argue that, on remedy is now to propose a standard task and then to build a large-scale dataset suitable for deep-learning-based approaches and covering ToM's multiple abilities assessment so to evaluate the models of different approaches in all aspects. 


\clearpage
%\begin{longtable}
% \appendices
% \section{Table \ref{model_compare} Models in theory of mind}
% Appendix one text goes here.

 
% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \label{appendices}

%\end{longtable}
% \appendices

% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


This work is supported by National Key Research and Development Program of China (2021ZD0111000/2021ZD0111004), the Science and Technology Commission of Shanghai Municipality Grant (No. 21511100101, 22511105901). Xin Lin and Qin Ni are the corresponding authors.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
% \bibliography{./BIB/data_set.bib,./BIB/intruction.bib,./BIB/Model.bib,./BIB/Experiment.bib}
% \bibliography{./intruction.bib}
\bibliography{./main.bbl}

%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
% \begin{thebibliography}{10}


% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Biography/myy.png}}]{Mao Yuanyuan}
% or if you just want to reserve a space for a photo:
 is currently pursuing for a master’s degree in computer science and technology with the College of  , East China Normal University. Her research interests include machine learning and machine cognitive assessment.Contact her at 51215901051@stu.ecnu.edu.cn.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Biography/liushuang.png}}]{Shuang Liu}
% or if you just want to reserve a space for a photo:
 is currently pursuing for a master’s degree in computer technology with the College of Information, Mechanical and Electrical Engineering, Shanghai Normal University. Her research interests include machine learning and machine cognitive assessment.Contact her at 1000513395@smail.shnu.edu.cn.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Biography/zhaops.png}}]{Pengshuai Zhao}
% or if you just want to reserve a space for a photo:
 is currently pursuing for a master’s degree in computer technology with the College of Information, Mechanical and Electrical Engineering, Shanghai Normal University. His research interests include machine learning and machine cognitive assessment.Contact he at spz45683968@gmail.com

\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Biography/niqin.png}}]{Qin Ni}
% or if you just want to reserve a space for a photo:
is an associate professor in college of information, mechanical and electrical engineering of Shanghai Normal University, PR China. She received the PhD degree in system and information service from the Technical University of Madrid, Spain in 2016 with the scholarship from China Scholarship Council. She took the post of visiting scholar at the University of Ulster, U.K. in 2015. Her research interests include educational data mining, adaptive learning and smart environment. Contact her at niqin@shnu.edu.cn.

\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Biography/linxin.png}}]{Xin Lin}
% or if you just want to reserve a space for a photo:
received the BEng and PhD degrees both in computer science and engineering from Zhejiang University, China. He is currently an professor in the Department of Computer Science, East China Normal University. His research interests include knowledge base, machine TOM and graph computing.
\end{IEEEbiography}


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{figures/Biography/heliang.png}}]{Liang He}
% or if you just want to reserve a space for a photo:
received his bachelor’s degree and PhD degree from the Department of Computer Science and Technology, East China Normal University, Shanghai, China. He is now a professor and the associate dean of the School of Computer Science and Technology, East China Normal University. His current research interest includes knowledge processing, user behavior analysis, and context-aware computing. He has been awarded the Star of the Talent in Shanghai. He is also a council member of the Shanghai Computer Society,
a member of the Academic Committee, the director of the technical committee of Shanghai Engineering Research Center of Intelligent Service Robot, and a technology foresight expert of the Shanghai Science and Technology in focus areas.
\end{IEEEbiography}

% that's all folks
\end{document}


