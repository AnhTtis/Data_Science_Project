% We introduced two diffusion model-based methods to generate 3D meshes, which are \DDM and \LDM. Both \LDM and \DDM use specially designed encoders and decoders to transform the mesh data, which can not be trained, to regular data formulation as latent representation. \LDM achieves state-of-the-art shape generation performance and some applications (to be finished). In the future, using key points as latent representation could have more applications in controllable 3D shapes generation and local editing. 
In this work, we propose to use point clouds as an intermediate representation of meshes.
We train generative models on the point clouds sampled from the surface of the meshes, then we use SAP to reconstruct meshes from the generated point clouds.
Meshes generated in this way demonstrate diverse topology.
We propose to further encode dense point clouds to features at a sparse set of latent points, and train two DDPM in this latent space to learn the distribution of the positions and features of the latent points, respectively.
Our sparse latent point diffusion model outperforms DDPMs directly trained on point clouds in terms of both sample quality and generation speed.
In addition, this sparse latent point representation allows us to explicitly control the shape of generated shapes, perform both global and local interpolations, and shape combination.