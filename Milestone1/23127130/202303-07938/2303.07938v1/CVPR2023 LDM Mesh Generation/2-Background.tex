\begin{figure*}[t]
\vspace{-2em}
    \hspace{3em}
    \begin{subfigure}{0.9\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{Figures/method_figures/Figs-Pipeline.png}
    \caption{The autoencoder encodes a mesh to features at the sparse latent points and decodes it back to a mesh.}
    \label{fig:autoencoder_overview}
    \end{subfigure}
    
    \begin{subfigure}{0.56\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/method_figures/point_diffusion_model.png}
    \caption{The DDPM learns the distribution of the sparse latent points.}
    \label{fig:latent_point_diffusion}
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}{0.42\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/method_figures/feature_diffusion_model.png}
    \caption{The DDPM learns the distribution of features at latent points.}
    \label{fig:latent_feature_diffusion}
    \end{subfigure}
    % \vspace{-1.5em}
    \caption{Overview of our sparse latent point diffusion model and the two latent diffusion models.}
    \vspace{-1.5em}
    \label{fig:latent_ddpm}
\end{figure*}

\subsection{Denoising Diffusion Probabilistic Model}
\label{sec:ddpm}
Denoising Diffusion Probabilistic Models (DDPMs) are a kind of generative models that learn the distribution of samples in a given dataset.
A DDPM consists of two processes: A diffusion process and a reverse process. 
The diffusion process is defined as 
\begin{align}
\label{eqn:diffusion_process}
    q(\vx^1,\cdots,\vx^T|\vx^0) = \prod_{t=1}^T q(\vx^t|\vx^{t-1}), \\
    \text{ where }
    q(\vx^t|\vx^{t-1})=\gN(\vx^t;\sqrt{1-\beta_t}\vx^{t-1},\beta_t \mI),
\end{align}
$\vx^0$ is a clean sample from the dataset, $\vx^1,\cdots,\vx^T$ are latent variables, $T$ is the number of diffusion steps,
$\gN$ denotes the Gaussian distribution and $\beta_t$'s are predefined small positive constants.
See Appendix A.4 for details of these hyper-parameters.
The diffusion process gradually adds noise to the clean sample $\vx^0$ and eventually turns it into a Gaussian noise $\vx^T$ given that $T$ is large enough.
The reverse process is defined as 
\begin{align}
\label{eqn:reverse_process}
\begin{split}
    p_{\bm{\theta}}(\vx^0,\cdots,\vx^{T-1}|\vx^T)=\prod_{t=1}^T p_{\bm{\theta}}(\vx^{t-1}|\vx^t), \\
    \text{ where }
    p_{\bm{\theta}}(\vx^{t-1}|\vx^t) = \gN(\vx^{t-1};\bm{\mu}_{\bm{\theta}}(\vx^t, t), \sigma_t^2\mI),
\end{split}
\end{align}
and the mean $\bm{\mu}_{\bm{\theta}}(\vx^t, t)$ is parameterized by a neural network.
We use the proposed method in~\cite{ho2020denoising} to reparameterize $\bm{\mu}_{\bm{\theta}}(\vx^t, t)$ as 
\begin{align}
\bm{\mu}_{\bm{\theta}}(\vx^t, t) = \frac{1}{\sqrt{\alpha_t}}\left(\vx^t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\bm{\epsilon}_{\bm{\theta}}(\vx^t, t)\right),
\end{align}
where $\alpha_t = 1 - \beta_t$, $\bar{\alpha}_t = \prod_{i=1}^t\alpha_i$.
The reverse process simulates the reverse process of the diffusion process:
It iteratively uses the network $\bm{\epsilon}_{\bm{\theta}}(\vx^t, t)$ to denoise a Gaussian noise and turn it into a clean sample.
To generate a sample from the DDPM, we first sample $\vx^T$ from the Gaussian distribution, then use Equation~\ref{eqn:reverse_process} to iteratively sample $\vx^{T-1}, \vx^{T-2}, \dots, \vx^0$ and finally obtain the sample $\vx^0$.

We use the simplified loss proposed in~\cite{ho2020denoising} to train the DDPM:
\begin{align}
\label{eqn:training objective}
    L(\bm{\theta}) = 
    \mathbb{E}_{\vx^0 \sim p_{\text{data}}}\ 
    \|\bm{\epsilon} - \bm{\epsilon}_{\bm{\theta}}(\sqrt{\bar{\alpha}_t}\vx^0 + \sqrt{1-\bar{\alpha}_t}\bm{\epsilon}, t)\|^2,
\end{align}
where $p_{\text{data}}$ is the distribution of the dataset, $t$ is sampled uniformly from $1,2,\dots,T$, and $\bm{\epsilon}$ is sampled from a Gaussian noise.
We can see that the network $\bm{\epsilon}_{\bm{\theta}}$ actually learns to predict the noise $\bm{\epsilon}$ added to the clean sample $\vx^0$.
In other words, the network $\bm{\epsilon}_{\bm{\theta}}$ learns to denoise noisy samples.
The architecture of the denoising network $\bm{\epsilon}_{\bm{\theta}}$ depends on the data format of $\vx^0$.  
If $\vx^0$ are images, a common choice of $\bm{\epsilon}_{\bm{\theta}}$ is an Unet that predicts a per-pixel adjustment of the input noisy image $\vx^t$.
If $\vx^0$ are point clouds, we can choose Point-Voxel CNN~\cite{zhou20213d} or PointNet++~\cite{lyu2021conditional} for $\bm{\epsilon}_{\bm{\theta}}$ that can predict a per-point displacement of the noisy input point cloud $\vx^t$.

% We suppose that p latent = N(03N; I3N3N) is the latent distribution and that p data is the distribution of the entire point cloud xi in the dataset, where N is the Gaussian distribution. The diffusion and reverse processes are Markov chains that make up the conditional DDPM. The length of both procedures is equal to T, which is equal to 1000 in most situations. 

\subsection{Latent Diffusion Model}
Latent diffusion models~\cite{vahdat2021score,rombach2022high} are proposed for high-resolution image synthesis.
Directly training DDPMs on high-resolution images and sampling from them are quite time-consuming.
Latent diffusion models circumvent this problem by first encoding a high-resolution image to a low-dimensional latent space, and then training DDPMs in this latent space.
Samples generated in this latent space are then decoded back to images.
We follow the same procedures as~\cite{rombach2022high} to train latent diffusion models.
First, train an autoencoder in the data space.
Then, train a DDPM using encoded samples from the dataset, namely, we can regard the variable $\vx^0$ in Section~\ref{sec:ddpm} as the encoded variable of the original data sample by the pre-trained autoencoder.

% LDM divide training into two separate stages: First, LDM train an autoencoder that generates a representational space that is perceptually comparable to the data space and is lower dimensional (and thus efficient). Importantly, and in contrast to earlier work, LDM train DMs in the acquired latent space, which exhibits better scaling qualities with regard to the spatial dimensionality, so LDM do not need to rely on excessive spatial compression. With only one network pass, the reduced complexity also enables effective image production from the latent space.

