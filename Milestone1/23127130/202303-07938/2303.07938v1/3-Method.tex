\newcommand{\PositionNet}{\text{PositionNet }}
\newcommand{\conditioner}{\text{condition }}
\newcommand{\FeatureTransfer}{\text{Feature Transfer }}
\newcommand{\FT}{\text{FT }}
\newcommand{\FeatureExtractionNet}{Condition Feature Extraction subnet}
\newcommand{\DenoiseNet}{Denoise subnet}


It is difficult to directly train a generative model on meshes, because meshes have irregular data structures. In general, a mesh is composed of vertices and faces. Vertices are points in the 3D space, while faces characterize the connections among vertices.
It is easy for a generative model to model the positions of vertices, but it is difficult to model the connections among vertices.
To tackle this problem, 
we propose to use point clouds with normals as an intermediate representation of meshes for their simple structure and efficient representation.
Point clouds with normals can be sampled from the surface of meshes ($2048$ points in our experiments).
Then we can use existing generative models~\cite{luo2021diffusion, zhou20213d, DBLP:conf/eccv/CaiYAHBSH20} to model the distribution of point clouds. 
Finally, we use SAP~\cite{peng2021shape} to reconstruct meshes from the generated point clouds.
SAP is composed of an upsampling network and a Differentiable Poisson Surface Reconstruction (DPSR) algorithm.
We refer readers to the original work or Appendix A.1 for details of SAP.
% we propose to borrow ideas from latent diffusion model. 
% We can first encode a mesh to a latent space with a data structure that is convenient to model. 
% Then we train a generative model in this latent space defined by the encoder. 
% To generate a mesh, we can use a decoder to decode samples generated by the generative model to meshes.

% In the original latent diffusion models that model the distribution of images~\cite{vahdat2021score,rombach2022high}, 
% both the encoder and decoder are neural networks trained in the autoencoder framework.
% We argue that the encoder and decoder do not have to be neural networks. 
% They could also be any transformations or algorithms, as long as the reconstruction error is small.
% For example, to tackle the data format of meshes, we can choose the encoder to be a rasterization algorithm that transforms a mesh to a 3D grid of signed distance values from the mesh surface. And the decoder is a marching cube algorithm that reconstructs the mesh from the 3D grid of signed distance values. 
% The distribution of 3D grids of signed distance values can be easily modeled by a DDPM using a 3D-CNN-style Unet. 
% However, to reduce the reconstruction error, we need to use a 3D grid of high resolution, which greatly increases the computational cost to model the distributions of the 3D grid.
% Therefore, we need to find a latent representation of meshes that are both convenient and efficient to model by a generative model.
% Particularly, we choose point clouds as the latent representation of meshes in this work for their simple structure and efficient representation compared with 3D grids.

% \subsection{\DDM}
% \label{sec:pointcloud_representation}
% As mentioned in the last section, we choose point clouds as the latent representation of meshes. Therefore, the encoder can be seen as a sampling algorithm that uniformly samples points from the surface of a mesh. We choose to sample $2048$ points in all of our experiments.
% The decoder is a surface reconstruction algorithm that reconstructs the mesh from the point cloud.
% There are many surface reconstruction algorithms in the literature. In this work, we choose the learning-based method proposed in~\cite{peng2021shape}, we will refer it as Shape as Points (SAP) in rest part of the paper.
% SAP is composed of an upsampling network and a Differentiable Poisson Surface Reconstruction (DPSR) algorithm.
% The upsampling network upsamples the point cloud and estimates normals of each point. 
% The DPSR algorithm solves a Poisson equation defined by the dense point cloud with normals using the spectral method, and obtains an indicator field discretized on a 3D grid. 
% Then the mesh is recovered by extracting the 0-isosurface of the 3D indicator grid.
% We refer readers to the original work~\cite{peng2021shape} for details of the DPSR algorithm.

% For the upsampling network, we use the improved PointNet++ proposed in~\cite{lyu2021conditional}.
% The improved PointNet++ is able to extract features for every point in a point cloud. The features contain both global information of the overall 3D shape and local details.
% To upsample the input point cloud by a factor of $\gamma$,
% we can interpret the first part of the extracted feature for each point as $\gamma$ spacial displacements. 
% We can add the $\gamma$ displacements to the original position of the input point to obtain $\gamma$ new points.
% We interpret the second part of the extracted feature as $\gamma$ normals and attach them to the $\gamma$ new points.
% In this way, we can upsample the input point cloud by a factor of $\gamma$ and predict normals for every point. We choose $\gamma=10$ in all of our experiments.
% The detailed architecture of the upsampling network is in the \textcolor{red}{appendix}.
% We adopt the same training procedures as~\cite{peng2021shape} to train the upsampling network.
% We refer readers to~\cite{peng2021shape} or the \textcolor{red}{appendix} for details of the training algorithm. 


% After choosing the encoder and decoder, we can use a DDPM to model the distribution of the latent point clouds.
% The modeling method is the same as DPM~\cite{luo2021diffusion}, PVD~\cite{zhou20213d}, and is described in Section~\ref{sec:ddpm}.
% We can also use other generative models~\cite{DBLP:conf/iccv/TreeGan, cai2020learning, li2021sp} to learn the distribution of the latent point clouds. 




% \section{Sparse Latent Point Representation}
As mentioned above, we can use point clouds with normals as the intermediate representation of meshes.
However, we think that point clouds are still a redundant representation of 3D shapes. 
% Previous works have shown that it is still quite time-consuming for a DDPM (typically $1000$ steps) to generate a point cloud (typically $2048$ points). 
In addition, point clouds are difficult to manipulate and control. 
% It is difficult to explicitly control the generation result of a point cloud DDPM.
To this end, we propose to further encode point clouds with normals to some sparse latent points with features as shown in Figure~\ref{fig:autoencoder_overview}.
The intuition behind this representation is that a 3D shape can be decomposed to its skeleton that encodes the overall structure of the shape, and features located on the skeleton that encodes the local geometric details of the shape.
To make the sparse latent points stretch over a given point cloud, we use farthest point sampling (FPS) to sample a given number ($16$ in our experiments) of points as the sparse latent points.
We use two strategies to choose the first point in FPS: The first is choosing the centroid (mean coordinates of all the points) as the first point in FPS. The second is randomly choosing a point as the first point.
We then design a point cloud encoder to encode a point cloud with normals to features attached to the sampled sparse latent points. 
We also design a point cloud decoder to decode the sparse latent points with features back to the input point cloud with normals.
The details of the point cloud encoder and decoder are explained in the next section.

% The encoder of this method not only includes the method of sampling from mesh but also encodes the 2048 points into sparse points with features. It also encodes the 2048 points into sparse points with features. The decoder in \LDM consists of three parts, decode sparse points to 2048 points, upsampling 2048 points to 20480 points, and solving the mesh structure using Poisson's equation. 

% The denoising network of \LDM still uses the improved point++ network. \LDM uses two diffusion models to generate the points cloud and features. The network that generates the points cloud generates the positions of the points in the point cloud. Then, based on the point locations, we use the second diffusion model to generate the information on features.

\begin{figure*}[t]
    \vspace{-2em}
    \begin{subfigure}{0.53\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/method_figures/Figs-PC_Encoder_details.png}
    \caption{The point cloud encoder.}
    \label{fig:pcd_encoder}
    \end{subfigure}
    % \hspace{1em}
    \begin{subfigure}{0.46\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/method_figures/Figs-PC_Decoder_details.png}
    \caption{The point cloud decoder.}
    \label{fig:pcd_decoder}
    \end{subfigure}
    \vspace{-0.7em}
    \caption{Architecture of the point cloud autoencoder.}
    \vspace{-1.5em}
    \label{fig:pcd_autoencoder}
\end{figure*}
\subsection{Architecture of the autoencoder}
As mentioned above, we need a point cloud encoder to encode a point cloud to a sparse set of points with features, and a decoder to decode the sparse latent points back to the input point cloud. 
In this section, we explain the detailed architectures of the point cloud encoder and decoder.
\vspace{-1.5em}
\paragraph{Point cloud encoder.}
The encoder needs to encode a point cloud to features at the FPS sampled sparse set of points.
The overview of the encoder is shown in Figure~\ref{fig:pcd_encoder}.
It mainly consists of the improved Set Abstraction (SA) modules with attention mechanism proposed in PDR~\cite{lyu2021conditional}.
We briefly repeat the design of the SA module.
The input of the SA module is a set of points with a feature attached to each point. The SA module subsamples the input points by farthest point sampling (FPS) and then propagates features to the subsampled points. 
Specifically, for every point in the subsampled points, it finds $K$ nearest neighbors in the input points. 
Then it transforms the features of the neighbors by a shared Multi-layer perceptron (MLP) and aggregates the transformed features to this point through the attention mechanism. 
We refer readers to the original work~\cite{lyu2021conditional} for details of the SA module.
In our encoder, there are $4$ cascaded SA modules. They iteratively downsample the input point cloud ($2048$ points) to $1024, 256, 64, 32$ points and propagate features to the downsampled points. The features of the input point cloud are simply the 3D coordinates and normals of every point.

Recall that the encoder needs to encode the input point cloud to features at the sampled sparse latent points ($16$ points). 
We achieve this by mapping features at the output of the last level SA module (consisting of $32$ points) to the sparse latent points.
We use the feature transfer (FT) module proposed in PDR~\cite{lyu2021conditional} to map features from the last level SA module to the sparse latent points.
The FT module can map features from one set of points to the second set of points, and the mapping is parameterized by a neural network. 
It is worth noting that the FT module requires points in the second set to have features, and these features are used as queries to aggregate features from the first set of points to the second set.
We refer readers to Appendix A.2 or the original work~\cite{lyu2021conditional} for details of the FT module.
To utilize the FT module, we first use a lightweight PointNet++ to extract features for the sparse latent points themselves.
Then we use it to map features from the last level SA module to the sparse latent points. 
The mapped features are concatenated to the original features at the sparse latent points to form the final features that represent the input point cloud.

Overall, as shown in Figure~\ref{fig:pcd_encoder}, our point cloud encoder contains $4$ SA modules to hierarchically extract features from the input point cloud, a light-weight PointNet++ to extract features for the sparse latent points, and an FT module to map features from the last level SA module to the sparse latent points. 
% See detailed hyper-parameters of these modules in Appendix A.3.


% The task of the encoder is to transfer the mesh to the key points process. 
% Briefly, it is to do a downsampling process using the SA module, which will be explained later. The last layer is the FT module. 

% In contrast, the difference between LION and us is that LION's downsampling is not controlled, and it can downsample up to 32 points, but we use a controlled way of downsampling, and we use the FT module for the last layer.
\vspace{-1.5em}
\paragraph{Point cloud decoder.}
As mentioned above, the point cloud encoder can encode an input point cloud to features at the sampled sparse latent points.
Next, we explain the point cloud decoder that can decode the sparse latent points with features back to the input point cloud with normals.
Its overall structure is shown in Figure~\ref{fig:pcd_decoder}.
It contains $3$ point upsampling (PU) modules that gradually upsample the sparse latent points ($16$ points) to $2048$ points.

The input to the $l$-th PU module is a set of points $\mX^l = \{x_{j}^l \in \mathbb{R}^{3} | 1 \leq j \leq N^{l} \}$, with features attached to each point $\mF^{l} = \{f_{j}^{l} \in \mathbb{R}^{d^{l}} | 1 \leq j \leq N^{l} \}$, where $N^{l}$ is the number of input points to the $l$-th PU module, $f_j^{l}$ is the feature at point $x_j^{l}$, and $d^{l}$ is the dimension of the feature.
The input to the first PU module is the sparse latent points $\mX^1$ and their features $\mF^1$.
The PU module first uses a shared Multi-layer Percpetron (MLP) to transform the feature $f_j^{l}$ at every point $x_j^{l}$ to $\gamma$ displacements. 
Then the displacements are added to the original point  $x_j^{l}$ to obtain $\gamma$ new points. 
In this way, the input points are upsampled by a factor of $\gamma$.
To enforce the uniformness of the upsampled points, after upsampling, we use FPS to downsample the upsampled points by half.
Overall, the input points are upsampled by a factor of $\gamma/2$ and we obtain the upsampled points $\mX^{l+1}=\{x_j^{l+1} \in \mathbb{R}^{3} | 1 \leq j \leq N_{l+1} \}$, where $N_{l+1} = \gamma N_{l}/2$.
Note that the upsampled points $\mX^{l+1}$ are controlled by the features $\mF^{l}$ at the input points $\mX^{l}$ and the learned MLP.

Next, we use the $(l+1)$-th PU module to further upsample the output of the $l$-th PU module, $\mX^{l+1}$, but first we need to compute features for points in $\mX^{l+1}$, because the PU module needs features at the input points to perform upsampling.
We think that the features should consist of two parts. The first part is from the upsampled points $\mX^{l+1}$ themselves. 
This part of the feature characterizes the shape of the current point cloud $\mX^{l+1}$, and instructs how we can further upsample and refine it to make it more plausible.
We use a improved PointNet++~\cite{lyu2021conditional} to extract this part of the feature from the set $\mX^{l+1}$, and denote it as $\mF_1^{l+1} = \{f_{1,j}^{l+1} \in \mathbb{R}^{d_1^{l+1}} | 1 \leq j \leq N_{l+1} \}$, where $f_{1,j}^{l+1}$ is the feature at point $x_j^{l+1}$, and $d_1^{l+1}$ is the dimension of the feature.

The second part of the feature should come from the previous level PU module, namely, $\mX^{l}$.
This is because we want the information in the features at the sparse latent points can propagate along the PU modules layer by layer to control the shape of the final decoded point cloud.
Afterall, all information of the input point cloud is encoded to the features at the sparse latent points, and we want the decoded shape to be consistent with the information stored in the features at the sparse latent points.
Therefore, to obtain the second part of the feature for every point in $\mX^{l+1}$, we use the FT module mentioned in the point cloud encoder to map features from $\mX^{l}$ to $\mX^{l+1}$, and the first part feature $\mF_1^{l+1}$ at $\mX^{l+1}$ are used as queries.
After obtaining the second part of the features, $\mF_2^{l+1} = \{f_{2,j}^{l+1} \in \mathbb{R}^{d_2^{l+1}} | 1 \leq j \leq N_{l+1} \}$, it is concatenated with the first part feature $\mF_1^{l+1}$ to obtain the final feature for $\mX^{l+1}$: $\mF^{l+1} = \{(f_{1,j}^{l+1}, f_{2,j}^{l+1}) \in \mathbb{R}^{d^{l+1}} | 1 \leq j \leq N_{l+1} \}$, where $d^{l+1} = d_1^{l+1} + d_2^{l+1}$. 

After upsampling the point cloud $\mX^{l}$ to $\mX^{l+1}$, and obtaining the features $\mF^{l+1}$ at $\mX^{l+1}$, we can use the PU module to further upsample $\mX^{l+1}$.
By applying the PU module and FT module iteratively, we can gradually upsample the sparse latent points to a point cloud of $2048$ points.
For the last PU module, we let it predict both $\gamma$ displacements and $\gamma$ normals, so that the final output point cloud has $2048$ points with normals.
Overall, the input to the point cloud decoder is the sparse latent points $\mX^{1}$ ($16$ points) and their features $\mF^{1}$.
The decoder outputs the intermediate results $\mX^{2}$ ($256$ points), $\mX^{3}$ ($1024$ points), the final reconstructed point cloud $\mX^{4}$ ($2048$ points) and normals $\mF^{4}$.
% See Appendix A.3 for more details of the decoder.
\vspace{-1.5em}
\paragraph{Training of the autoencoder.}
The point cloud autoencoder is trained to encode the input point cloud and then reconstruct the point cloud.
The input to the autoencoder is point cloud $\mX_{\text{in}}$ ($2048$ points) with normals $\mF_{\text{in}}$ sampled from the meshes in the dataset.
The supervision is added on all the intermediate upsampling results in the point cloud decoder: $\mX^{2}, \mX^{3}$, $\mX^{4}$.
The loss is the sum of the Chamfer distance (CD) between $\mX_{\text{in}}$ and $\mX^{2}, \mX^{3}$, $\mX^{4}$, respectively.
Note that when computing the CD loss between $\mX_{\text{in}}$ and $\mX^{2}, \mX^{3}$, we first downsample $\mX_{\text{in}}$ using farthest point sampling to the same number of points as $\mX^{2}$ and $\mX^{3}$, respectively.
We also add a normal consistency loss between the ground-truth normals $\mF_{\text{in}}$ and the predicted normals $\mF^4$ with a weight of $0.1$. See Appendix B.3 for details of this loss.
We further add a slight Kullbackâ€“Leibler divergence loss (weight $10^{-5}$) between the encoded features $\mF^{1}$ and a standard normal distribution.
This regularization term is to encourage the latent feature space to be simple and smooth, so that we can perform manipulation and interpolation in this space.
Before the encoder encodes the input point cloud $\mX_{\text{in}}$ to the sampled sparse latent points $\mX^{1}$, we add a Gaussian noise with a standard deviation of $0.04$ to the point positions in $\mX^{1}$.
This is to make the autoencoder more robust to the positions of the sparse latent points, so that even if the positions of the sparse latent points are not perfect (\eg, human-edited sparse latent points), the autoencoder can still well reconstruct the input point cloud. 


\subsection{Train DDPMs in the Sparse Latent Point Space}
\label{sec:train_latent_ddpm}
After training the autoencoder, we can train latent DDPMs in the latent space of the autoencoder, while freezing the parameters of the autoencoder.
Specifically, for each point cloud, we can encode it to features $\mF^{1}$ at sparse latent points $\mX^{1}$.
We train two DDPMs in this latent space. The first one learns the distribution of the sparse latent points $\mX^{1}$ and is illustrated in Figure~\ref{fig:latent_point_diffusion}.
The sparse latent points can be seen as a point cloud with very few points. Therefore, its distribution can be effectively learned by a DDPM designed for point clouds, except that we can use a light-weight PointNet++ as the denoising network $\bm{\epsilon}_{\bm{\theta}}(\vx^t, t)$ in Equation~\ref{eqn:training objective}.

The second DDPM learns the distribution of the feature $\mF^{1}$ given the sparse latent points $\mX^{1}$, which is illustrated in Figure~\ref{fig:latent_feature_diffusion}.
For algebraic simplicity, we use $\vf$ to denote the vector form of $\mF^{1}$, namely, concatenating all feature vectors in $\mF^{1}$ to a single  vector $\vf$. Similarly, let $\vx$ be the vector form of $\mX^{1}$.
The second DDPM can be seen as a conditional DDPM that generates features $\vf$ conditioned on the positions of the sparse latent points $\vx$.
To achieve this, we can simply replace the diffusion variable $\vx$ in Equation~\ref{eqn:diffusion_process} with $\vf$, and adapt the reverse process in Equation~\ref{eqn:reverse_process} to
\vspace{-1em}
\begin{align}
\begin{split}
    p_{\bm{\phi}}(\vf^{0},\cdots,\vf^{T-1}|\vf^{T},\vx)&=\prod_{t=1}^T p_{\bm{\phi}}(\vf^{t-1}|\vf^{t}, \vx), \\
    \text{ where }
    p_{\bm{\phi}}(\vf^{t-1}|\vf^{t}, \vx) &= \gN(\vf^{t-1};\bm{\mu}_{\bm{\phi}}(\vf^{t}, \vx, t), \sigma_t^2\mI),
\end{split}
\end{align}
and $\bm{\mu}_{\bm{\phi}}(\vf^{t}, \vx, t)$ is parameterized as 
\begin{align}
\bm{\mu}_{\bm{\phi}}(\vf^{t}, \vx, t) = \frac{1}{\sqrt{\alpha_t}}\left(\vf^{t}-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\bm{\epsilon}_{\bm{\phi}}(\vf^{t}, \vx, t)\right).
\end{align}
To feed the input $\vf^{t}$ and $\vx$ to the denoising network $\bm{\epsilon}_{\bm{\phi}}$, we concatenate each feature in $\vf^{t}$ to the corresponding point in $\vx$.
In other words, the input to $\bm{\epsilon}_{\bm{\phi}}$ can be seen as a sparse point cloud with noisy features attached to each point, and the output of the network $\bm{\epsilon}_{\bm{\phi}}$ is to predict the noise added to each feature in $\vf$.
Therefore, we can use the improved PointNet++ in PDR~\cite{lyu2021conditional} as the denoising network $\bm{\epsilon}_{\bm{\phi}}$.
Correspondingly, the training loss of the network $\bm{\epsilon}_{\bm{\phi}}$ is 
\begin{align}
    L(\bm{\phi}) = 
    \mathbb{E}_{(\mX_{\text{in}}, \mF_{\text{in}}) \sim p_{\text{data}}}\ 
    \|\bm{\epsilon} - \bm{\epsilon}_{\bm{\phi}}(\sqrt{\bar{\alpha}_t}\vf + \sqrt{1-\bar{\alpha}_t}\bm{\epsilon}, \vx, t)\|^2, \nonumber
\end{align}
where $\mX_{\text{in}}$ is the point cloud ($2048$ points) sampled from a mesh in the dataset, $\mF_{\text{in}}$ are the corresponding normals, $\vx$ are sampled sparse latent points from $\mX_{\text{in}}$, $\vf$ is the encoded feature at $\vx$ obtained by the trained authoencoder, $t$ is sampled uniformly from $1,2,\dots,T$, and $\bm{\epsilon}$ is sampled from a Gaussian noise.



The detailed architecture of the two DDPMs is provided in Appendix A.3. 
After training the two DDPMs, we can use them to perform both unconditional 3D shape generation or controllable generation conditioned on the positions of the sparse latent points. 
To perform unconditional 3D point cloud generation, we can simply cascade the two DDPMs together: The first DDPM generates a set of sparse latent points, and the second DDPM generates features at the sparse latent points. Finally, the point cloud decoder decodes the sparse latent points with features to a point cloud.
To achieve controllable generation, we can manipulate the positions of the sparse latent points, then feed the human-adjusted sparse latent points to the second DDPM to generate plausible features on them, and finally decode them to a point cloud.

% The task of the decoder is to reconstruct the mesh from key points. The method we use is upsampling the structure to get the points cloud, then upsampling the points to construct the mesh, and lastly, constructing the mesh with a Poisson solver. 
% xt
% \paragraph{Points Upsampling Module} 
% This module takes key points as input and meshes data as output. This module takes key points upsampling several times. Gradually we will get the total points cloud. 

% \begin{enumerate}
%     \item We use key points to predict some displacements and get more point position data from these displacements. 
%     \item Use the FPS downsampling method to get a uniform point cloud distribution. Specifically by discarding part of the uneven distribution of point clouds to get a uniform distribution.
%     \item After getting the point cloud, we have to extract the corresponding features, use pointnet++ to extract our features, use our features to query the nearest point's features of the previous layer to get a new feature and then transfer it to this layer. Moreover, concatenate it with its features.
% \end{enumerate}

