{
    "arxiv_id": "2303.10372",
    "paper_title": "Just Noticeable Visual Redundancy Forecasting: A Deep Multimodal-driven Approach",
    "authors": [
        "Wuyuan Xie",
        "Shukang Wang",
        "Sukun Tian",
        "Lirong Huang",
        "Ye Liu",
        "Miaohui Wang"
    ],
    "submission_date": "2023-03-18",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
    ],
    "abstract": "Just noticeable difference (JND) refers to the maximum visual change that human eyes cannot perceive, and it has a wide range of applications in multimedia systems. However, most existing JND approaches only focus on a single modality, and rarely consider the complementary effects of multimodal information. In this article, we investigate the JND modeling from an end-to-end homologous multimodal perspective, namely hmJND-Net. Specifically, we explore three important visually sensitive modalities, including saliency, depth, and segmentation. To better utilize homologous multimodal information, we establish an effective fusion method via summation enhancement and subtractive offset, and align homologous multimodal features based on a self-attention driven encoder-decoder paradigm. Extensive experimental results on eight different benchmark datasets validate the superiority of our hmJND-Net over eight representative methods.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10372v1"
    ],
    "publication_venue": null
}