% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[final]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bbding}

\usepackage{algorithm} 
\usepackage{algorithmic}  
% \usepackage[ruled,linesnumbered]{algorithm2e} 
\usepackage[algo2e,ruled,linesnumbered]{algorithm2e}
\SetKwInOut{Parameter}{parameter}

\SetCommentSty{mycommfont}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{black}{#1}}
\usepackage{xcolor}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX 
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{3666} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\usepackage[colorlinks,linkcolor=red]{hyperref}
\usepackage[capitalize]{cleveref}


\begin{document}

% Support for easy cross-referencing
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}





%%%%%%%%% TITLE - PLEASE UPDATE
\title{OTAvatar$\colon$One-shot Talking Face Avatar with Controllable Tri-plane Rendering}

\author{{Zhiyuan Ma$^{1,2}$\thanks{Equal contribution.} \qquad Xiangyu Zhu$^{3}$\samethanks \qquad Guojun Qi$^4$ \qquad Zhen Lei$^{1,2,3}$\thanks{Corresponding author.} \qquad Lei Zhang$^{1}$ } \\
$^1$The Hong Kong Polytechnic University \\ 
$^2$Center for Artificial Intelligence and Robotics, HKISI CAS \\
$^3$State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA \\
$^4$OPPO Research \\
 \tt\small \ zm2354.ma@connect.polyu.hk, xiangyu.zhu@nlpr.ia.ac.cn \\ \tt\small guojunq@gmail.com, zlei@nlpr.ia.ac.cn, cslzhang@comp.polyu.edu.hk
}
\maketitle

\begin{abstract}
    % We propose to construct face avatars by a generalized controllable tri-plane rendering solution, and each personalized avatar can be constructed given one portrait as reference. Current implicit methods are notorious for the huge computational cost, while the low-latency is the major requirement for implementation. Besides, for face avatar animation, the generalizability and controllablity are considered as the major objectives while most existing methods have not managed to accommodate both sides. Our proposed OTAvatar achieves and balances the three dimensions of fast-inference, generalizability and controllablity by imposing motion-control on EG3D which is an efficient Volume rendering generative model. Our solution can be concluded as disentangling identity and motion-related latent code during optimization-based inversion. Specifically, we propose a refinement module to prompt motion-related latent code so that only identity-related fraction is required via inversion optimization. Since the refinement module can be jointly trained along with inversion optimization, our proposed method can significantly improve training efficiency. By introducing less than 3\% extra parameters, we achieve controllable rendering of generalized face avatar at 35 FPS on A100, given only one portrait as the reference. Experiment shows promising performance of same-identity / cross-identity reenactment on subjects out of the training set, and the 3D consistency is maintained. 
    
    
Controllability, generalizability and efficiency are the major objectives of constructing face avatars represented by neural implicit field. However, existing methods have not managed to accommodate the three requirements simultaneously. They either focus on static portraits, restricting the representation ability to a specific subject, or suffer from substantial computational cost, limiting their flexibility. In this paper, we propose One-shot Talking face Avatar (OTAvatar), which constructs face avatars by a generalized controllable tri-plane rendering solution so that each personalized avatar can be constructed from only one portrait as the reference. Specifically, OTAvatar first inverts a portrait image to a motion-free identity code. Second, the identity code and a motion code are utilized to modulate an efficient CNN to generate a tri-plane formulated volume, which encodes the subject in the desired motion. Finally, volume rendering is employed to generate an image in any view. The core of our solution is a novel decoupling-by-inverting strategy that disentangles identity and motion in the latent code via optimization-based inversion. Benefiting from the efficient tri-plane representation, we achieve controllable rendering of generalized face avatar at $35$ FPS on A100. Experiments show promising performance of cross-identity reenactment on subjects out of the training set and better 3D consistency. \textcolor{black}{The code is available at} \href{https://github.com/theEricMa/OTAvatar}{https://github.com/theEricMa/OTAvatar}.
    
    % Specifically, we propose a refinement module to prompt motion-related latent code so that only identity-related part is required via inversion optimization. Since the refinement module can be jointly trained along with inversion optimization, our proposed method can significantly improve training efficiency. By introducing less than 3\% extra parameters, we achieve controllable rendering of generalized face avatar at 35 FPS on A100, given only one portrait as the reference. Experiment shows promising performance of same-identity / cross-identity reenactment on subjects out of the training set, and the 3D consistency is maintained. 

    % We propose OTAvatar, a unified solution for the fast, generalized and controllable face avatar with Volume rendering. 



% 

% In Face Avatar Synthesizing, Volume rendering is popularized for its strong 3D consistency compared to 2D methods. However, current research either overfitted to a single identity for motion control or generalized to multiple identities with no motion control. In this work, we step towards the generalized controllable avatar animation with Volume rendering. Confronted with the inaccessibility of large-scale multi-view talking face dataset, we refer to the pre-trained Volume rendering generative model and train the motion control in off-the-shelf monocular talking videos. Our solution can be concluded as disentangling identity and motion-related latent code during optimization-based inversion. Specifically, we propose a refinement module to prompt motion-related latent code so that only identity-related fraction is required via inversion optimization. Since the refinement module can be jointly trained along with inversion optimization, our proposed method can significantly improve training efficiency.  Our method demonstrates that by introducing less than 3\% extra parameters, a pre-trained Volume rendering generative model can be capable of controlling the avatar on not only poses but also expressions. The experiment shows the promising performance of same-identity / cross-identity reenactment on subjects out of the training set, and the 3D consistency is maintained. 

    % Volume Rendering is popularized for its strong 3D consistency, and its application in talking face avatars has demonstrated extreme pose tolerance. Current research either over-fitted to single identity for motion control or generalized to multiple identity with no motion control, the possibility of generalized controllable avatar animation with Volume rendering has not yet been explored. We analyze the problem lies in 1) the computational cost on training a generalized controllable model from scratch and 2) the unavailability of large-scale multi-view dataset. One solution is 1) implementing pre-trained generative Volume rendering model and 2) training controllability on off-the-shelf monocular videos. Even though 2) has been proved feasible in 2D GANs, pre-trained inversion encoder was implemented so as to avoid computational waste on optimizing latent code. However, due to the non-existence of such inversion encoder for 3D GANs, the computationally-costly optimization-based optimization becomes the only choice, in which case the time for controllability training can be occupied to be 1\% or less. To tackle with the notorious cost, we propose a novel training scheme to release more computation cost for learning the controllability along with optimizing the inversion, the whole training time can be handled in normal range. Our method shows that by introducing less than 3\% extra parameters, pre-trained Volume rendering generative model can be capable of controlling the avatar on not only poses but also expressions. Experiments shows promising performance of same-identity / cross-identity reenactment on subjects out of the training set, and the 3D consistency is maintained. 

\end{abstract}



%%%%%%%%% BODY TEXT
\begin{figure}[ht]
  \centering
   \includegraphics[width=1\linewidth]{Demo_v2.drawio.pdf}

   \caption{\textbf{OTAvatar animation results}. The source subjects in HDTF\cite{zhang2021flow} dataset are animated by OTAvatar using a single portrait as the reference. We use the pose and expression coefficients of 3DMM to represent motion and drive the avatar. Note that these subjects are \textbf{not included} in the training data of OTAvatar.}
   \label{fig:animation result}
\end{figure}

\section{Introduction}
Neural rendering has achieved remarkable progress and promising results in 3D reconstruction. Thanks to the differentiability in neuron computation, the neural rendering methods bypass the expensive cost of high-fidelity digital 3D modeling, thus attracting the attention of many researchers from academia and industry. In this work, we aim to generate a talking face avatar via neural rendering techniques, which is controllable by driving signals like video and audio segments. Such animation is expected to inherit the identity from reference portraits, while the expression and pose can keep in sync with the driving signals. 



The early works about talking face focus on expression animation consistent with driving signals in constrained frontal face images~\cite{chung2016lip, prajwal2020lip, chen2019hierarchical}. It is then extended to in-the-wild scenes with pose variations
~\cite{ji2021audio,wang2018vid2vid, zhou2021pose, zhou2018talking}. 
Many previous works are 2D methods, where the image warping adopted to stimulate the motion in 3D space is learned in 2D space~\cite{ren2021pirenderer, siarohin2019first, siarohin2021motion, wang2022latent, yin2022styleheat}. These methods tend to collapse under large pose variation. In contrast, there are 3D methods that can address the pose problem, since the head pose can be treated as a novel view. Both explicit \cite{fernando2004gpu, levoy1988display} and implicit 3D representations\cite{mildenhall2021nerf} are introduced to face rendering\cite{Lombardi:2019, raj2021pva, Mihajlovic:KeypointNeRF:ECCV2022, park2021nerfies, guo2021ad, zheng2022imavatar, hong2022headnerf}. However, these methods either overfit to a single identity or fail to produce high-quality animation for different identities. 

In this work, we propose a one-shot talking face avatar (OTAvatar), which can generate mimic expressions with good 3D consistency and be generalized to different identities with only one portrait reference. Some avatar animation examples are shown in Fig.~\ref{fig:animation result}. Given a single reference image, OTAvatar can drive the subject with motion signals to generate corresponding face images. We realize it under the framework of volume rendering. Current methods usually render static subjects~\cite{Mihajlovic:KeypointNeRF:ECCV2022, raj2021pva}. Although there are works~\cite{guo2021ad,zheng2022imavatar,park2021nerfies, park2021hypernerf,Gafni_2021_CVPR} proposed to implement dynamic rendering for face avatars, they need one model per subject. Therefore, the generalization is poor. HeadNeRF\cite{hong2022headnerf} is a similar work to us. However, its reconstruction results are unnatural in the talking face case. 

Our method is built on a 3D generative model pre-trained on a large-scale face database to guarantee identity generalization ability. Besides, we employ a motion controller to decouple the motion and identity in latent space when performing the optimization-based GAN inversion, to make the motion controllable and transferable to different identities. The network architecture is compact to ensure inference efficiency. In the utilization of our OTAvatar, given a single reference image, we fix the motion code predicted by the controller and only optimize the identity code so that a new avatar of the reference image can be constructed. Such a disentanglement enables the rendering of any desired motion by simply alternating the motion-related latent code to be that of the specific motion representation. 


%There have been a good many works for talking face. The earliest works generate expression on constrained frontal faces\cite{chen2019hierarchical} and in-the-wild faces\cite{prajwal2020lip, thies2016face2face, wang2018vid2vid, wang2019fewshotvid2vid}. Then the research focuses on making the head pose also controllable\cite{zhou2021pose}, while a huge percentage of efforts opt to represent head pose using image warping\cite{ren2021pirenderer, wang2022latent, siarohin2019first, yin2022styleheat}. However, since image warping is merely trained to stimulate 3D motion in the 2D space, these methods collapse when generating large head poses. On the other side, 3D methods with either explicit \cite{fernando2004gpu, levoy1988display} or implicit 3D representation\cite{mildenhall2021nerf} are introduced to face rendering\cite{Lombardi:2019, raj2021pva, Mihajlovic:KeypointNeRF:ECCV2022, park2021nerfies, guo2021ad, zheng2022imavatar, hong2022headnerf}. However, to animate face avatar with controllable motion i.e. expression, current 3D methods either overfits the network to single identity\cite{guo2021ad, zheng2022imavatar} or cannot promise quality for identity-generalized animation\cite{hong2022headnerf}. 



% 2D based methods \cite{ren2021pirenderer, yin2022styleheat, doukas2021headgan} implement 3DMM coefficients to warp portraits in terms of pose and expression. But the generation collapses as the pose variation goes beyond certain threshold\cite{drobyshev2022megaportraits}, since the warping field itself is merely trained to stimulate 3D motion in the 2D space. 3D rendering methods can explicitly guarantee view-consistency. The classical volume ray marching \cite{fernando2004gpu, levoy1988display} was introduced to face avatar animation\cite{Lombardi:2019, }, but the size of volume constraints either the rendering quality or the inference speed. Mildenhall et.al \cite{mildenhall2021nerf} proposed NeRF which is the implicit scene representation, and its applications in face avatar were proposed\cite{guo2021ad, zheng2022avatar,hong2022headnerf }.

%Talking face generation via neural rendering aims at rendering high fidelity human faces that is controllable by driving signals like video and audio segments through neural networks.  Such generation is expected to inherit the identity detail from reference portraits, while the expression and pose keep in sync with the driving signals. Such neuron rendering methods bypass the notorious cost on modelling high-fidelity digital avatar, thanks to the differentiability in neuron computation. Therefore corresponding researches start to prevail for growing applications on industries including meta universe and virtual reality.








% In the 2D area, the task of talking face generation has been investigated for long, with investigations in inference latency \cite{Thies_2016_CVPR}, in-the-wild \cite{chung2016lip} or cross-domain\cite{drobyshev2022megaportraits, hong2022depth} performance , as well as the extra conditions like emotion\cite{wang2020mead, ji2022eamm, ji2021audio} and text\cite{song2022talking}. One of the most challenging aspect lies in three-phase equilibrium i.e. the consistency of identity detail, the controllability of expression attribute, the robustness against the pose variation. Most existing methods refer to warping field to control the head pose then refine the detail \cite{wang2022latent, ren2021pirenderer, yin2022styleheat, wang2021one, wang2021safa, zeng2022fnevr}. But the majority of them requires a driving frame to infer the motion via warping, which is unfeasible for 3d talking head, 

%2D based methods \cite{ren2021pirenderer, yin2022styleheat, doukas2021headgan} implement 3DMM coefficients to warp portraits in terms of pose and expression. But the generation collapses as the pose variation goes beyond certain threshold\cite{drobyshev2022megaportraits}, since the warping field itself is merely trained to stimulate 3D motion in the 2D space. 3D rendering methods can explicitly guarantee view-consistency. The classical volume ray marching \cite{fernando2004gpu, levoy1988display} was introduced to face avatar animation\cite{Lombardi:2019, }, but the size of volume constraints either the rendering quality or the inference speed. Mildenhall et.al \cite{mildenhall2021nerf} proposed NeRF which is the implicit scene representation, and its applications in face avatar were proposed\cite{guo2021ad, zheng2022avatar,hong2022headnerf }.

% The static avatar has achieved photo-realistic quality\cite{chan2021pi, niemeyer2021giraffe, schwarz2020graf, Chan2022, raj2021pva, Mihajlovic:KeypointNeRF:ECCV2022, Schwarz2020NEURIPS}, but the dynamic NeRF for controllable face avatar still remains a challenging problem. Guo et.al\cite{guo2021ad} proposed the decomposition with the expression in 3DMM coefficients as the extra input to NeRF, while pose being the explicit camera pose. Zheng et.al\cite{zheng2022avatar} opt to extend fine-grained control mechanisms afforded by conventional 3DMMs to NeRF. But these methods are subject-specific, which means they cannot be applied to arbitrary persons. The most related method to us is Hong et.al\cite{hong2022headnerf}. It supports multiple identity rendering and 3DMM control, but it cannot synthesize natural talking and the reconstruction of target identity is unsatisfying.  



% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% % \usepackage{graphicx}
% \begin{table}[]
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}llllll@{}}
% \toprule
%            & Consistency & Generalizable               & Control      & Resolution & Speed(/FPS) \\ \midrule
% PIRenderer\cite{ren2021pirenderer} & 2D     & \CheckmarkBold             & \CheckmarkBold  &           &        \\
% StyleHEAT \cite{yin2022styleheat}      & 2D     & \CheckmarkBold              & \CheckmarkBold  &          &        \\
% KeypointNeRF \cite{Mihajlovic:KeypointNeRF:ECCV2022}     & 3D     & \CheckmarkBold               & \XSolidBrush  &            &        \\
% PVA  \cite{raj2021pva}  & 3D     & \CheckmarkBold              & \XSolidBrush  &             &        \\
% ADNeRF  \cite{hong2022headnerf}   & 3D     & \XSolidBrush                & \CheckmarkBold  &            &        \\
% IMAvatar \cite{zheng2022imavatar}  & 3D     & \XSolidBrush                & \CheckmarkBold  &          &        \\
% HeadNeRF \cite{hong2022headnerf}  & 3D     & \CheckmarkBold              & \CheckmarkBold  &512       &25  \\
%  \textbf{Ours}       & 3D     & \CheckmarkBold               & \CheckmarkBold     &512  &\textbf{35}  \\ \bottomrule
% \end{tabular}%
% }
% \caption{\textbf{The functionality of current methods.} We categorize
%  current talking face generation methods into 2D or 3D-based methods, and identify whether the model can be generalized to animate multiple identities, as well as if the expression and pose can be controlled. We also measure the inference speed by frames per second (FPS). }
% \label{tab:my-table}
% \end{table}


%We analyze the major challenge for the 3D-consistent face avatar animation lies in three aspect: controllability, support control on motion; generalizability, reconstruct novel identity given a few references; efficiency, inference in real-time. But current methods mostly cannot satisfy all these requirements. \cite{Mihajlovic:KeypointNeRF:ECCV2022, raj2021pva} renders only static subjects. \cite{guo2021ad, zheng2022imavatar} is controllable but renders only one objects methods per model. And all of these methods suffer from high inference latency. Even though HeadNeRF\cite{hong2022headnerf} proposed balanced solution, the reconstitution and motion are both unnatural since the it is trained on multi-view dataset which contains limited identities and talking corpus. To tackle with this, our solution is to refer to pre-trained Volume rendering generative model for generalizability, implement compact architecture for efficiency, and realize the motion control in latent code manipulation. We incorporate optimization-based GAN inversion in our training scheme, and using a refinement model to decouple the motion and identity-fraction of latent code by refining the motion-related latent fraction on top of motion-free identity latent fraction during the inversion optimizing. In case of the computational cost afforded for the training such disentanglement, we propose a simple training scheme that can effectively reduce the training cost to normal range, which is designed to enable the training of refinement module during the inversion of each data mini-batch while promising the proposed disentanglement. In the test time, the prediction of the refinement module is set fixed to be treated as a prompt so that only the identity-fraction of latent code will be optimized. Such disentanglement enables the rendering of any desired motion by simply alternating the motion-related latent code to be that of the specific motion representation, in our case is the 3DMM coefficient. 

 \vspace{+3mm}
The major contributions of this work can be summarized as follows.
\begin{itemize}
    \item We make the first attempt for one-shot 3D face reconstruction and motion-controllable rendering by taming a pre-trained 3D generative model for motion control. 

    \item We propose to decouple motion-related and motion-free latent code in inversion optimization by prompting the motion fraction of latent code ahead of the optimization using a decoupling-by-inverting strategy. 
    %We also design a training scheme to curtail training costs to the normal range.
    
 
    \item Our method can photo-realistically render any identity with the desired expression and pose at 35FPS. The experiment shows promising results of natural motion and 3D consistency on both 2D and 3D datasets. 
    
\end{itemize}

% previous 2D based methods has shown feasible in the fashion of learning-based portrait inversion, but these work requires first training a latent encoders like [PSP, ...]. 

% On the other size, optimization-based portrait inversion has prevailed, with investigations in broadening the ability of manipulating attribute e.x. smiling, mustache. Among them, [StyleRig] has made attempt to expression and pose control, but has not achieved reenacting talking video on given portrait. 

% We analyze that the major challenge that hinders these generative Volume rendering models from stepping towards controllable animation is the burdensome computational cost on identity reconstruction via GAN Inversion, which should have been solved by well-trained style encoders. Corresponding researches of encoder design for editable Generative Adversarial Networks has achieves great success in 2D generation by extracting editing representation from image within single forward process, and its efficiency has motivated research effort which introduced pre-trained 2D generative model of face reenactment. Similar techniques can be seamlessly applied to Volume rendering generative model by designing a novel encoder, as done by [] for mask-driven generation. However, the none-existence of such encoder remains an unsettled issue for most GANs and a general solution is still in urgent need. Introducing optimization-based inversion to generative Volume rendering model has another technical convenience that the explicit configuration of camera pose makes the portrait inversion free of face alignment, which mimics the conventional analysis by synthesis for id reconstruction.


\begin{figure*}[ht]
  \centering
   \includegraphics[width=1\linewidth]{Latent_Manipulation_v5-cropped.pdf}

   \caption{\textbf{Overview of OTAvatar.} OTAvatar contains a 3D face animator network and a latent code decoupling strategy, namely decoupling-by-inverting. The 3D face animator is composed of a pre-trained 3D face generator\cite{Chan2022} and a motion controller module. The decoupling-by-inverting algorithm is an optimization-based image inversion that can decouple the latent code $\mathbf{w}$ into identity code $\mathbf{w}_{id}$ and motion code $\mathbf{w}_x$. When the model is well-trained,  the motion-free identity code can be inferred from a single reference image, and an avatar of an unseen subject can be constructed. The identity code can be integrated with any other motion code predicted by the controller to animate the identity with desired motion.}
   \label{fig:overview of OTAvatar}
\end{figure*}





\section{Related Works}

% \subsection{Motion Representation}
%  For face animation, the mainstream motion representation can be categorized as keypoints driven\cite{ wang2018video, zakharov2019few, wang2019fewshotvid2vid, zakharov2020fast} (via facial landmarks), expression/pose driven\cite{ren2021pirenderer, yin2022styleheat} (via 3DMM coefficients) and raw image driven\cite{siarohin2019first}, set aside audio-driven representation which cannot model head movement. While raw image driven methods extract motion representation using an encoder trained from scratch and support end-to-end motion animation, it lacks interoperability for manipulation as required by down-stream application like virtual avatar. On the other side, using off-the-shelf processing methods, one can extract face landmarks or 3DMM coefficients at ease. While facial landmark serves as a good 2D-space representation, 3DMM is able to decouple any 3D face into subject-related factors e.x. shape, texture and motion-related factors which are expression and pose, such explicit interoperability enables 3DMM being the solutions of most existing commercial projects, and the pose/expression driven methods proliferate in current researches\cite{ren2021pirenderer, yin2022styleheat, guo2021ad, hong2022headnerf, zheng2022imavatar}.

\subsection{Talking Face}

    % Talking face generation seeks to generate faces with controllable motion while maintaining the identity attribute. Current methods either learn to map the driving image to implicit motion representation\cite{bansal2018recycle, siarohin2019first, wang2022latent} or exact interpretable coefficients, i.e., facial landmarks and 3DMM coefficients using off-the-shelf landmark detectors\cite{bulat2017far} and face reconstruction  methods\cite{3ddfa_cleardusk, guo2020towards, zhu2017face, DECA:Siggraph2021, deng2019accurate}. Facial landmarks provide edge information and are wildly incorporated with image-translation methods\cite{bansal2018recycle, wang2018video, wu2018reenactgan, wang2019fewshotvid2vid, }, however the edge map is determined by not only pose and expression but also identity-specific face shape, making the cross-identity reenactment rather tricky. In contrast, 3DMM coefficients can decouple identity-agnostic expression and pose information and have been wildly implemented to represent motion\cite{chan2021pi, yin2022styleheat, hong2022headnerf, guo2021ad, doukas2021headgan}.

    Regarding the driving signal, the talking face methods can be roughly divided into three categories: audio-driven, image-driven and coefficients-driven. Our approach is most related to coefficients-driven methods, which use either facial landmarks or 3DMM coefficients to represent motion. Facial landmarks involve identity and expression information and is challenging to transfer motion across different subjects. 3DMM coefficients disentangle the identity, expression and pose, and thus they are good driven signals to drive different faces. 
    
    Thies et al.~\cite{thies2016face2face} used 3D rendering to maintain the shape and illumination attributes when transferring expression and refining mouth details with a mouth retrieval algorithm. Geng et al.~\cite{geng20193d} used 3DMM to render a given subject with different expressions, followed by a neural network to refine the detail and harmonize with the background. These methods cannot change the face pose. Ren et al.~\cite{ren2021pirenderer} mapped 3DMM expression and pose to a high-dimensional motion representation, then predicted the dense image flow to reenact faces via image warping. Doukas et al.~\cite{doukas2021headgan} used additional 3DMM mesh fitting results to assist the dense image flow prediction. Yin et al.~\cite{yin2022styleheat} integrated pre-trained StyleGAN\cite{chong2021stylegan} to generate high-resolution talking face prediction by warping low-resolution feature maps. These warping-based methods follow the two-stage workflow by first warping images and then refining facial detail, demonstrating superior performance for same identity reenactment. But in cross-identity reenactment, especially when there is a large pose variation against the source portrait, the animation tends to collapse, since image warping is merely trained to stimulate 3D motion in the 2D space. Our work animates face avatars using 3D rendering and explicitly ensures 3D consistency to handle large pose variations.
    
% Talking face seeks to synthesize faces with controllable motion. The motion signal can be audio-driven\cite{wiles2018x2face, guo2021ad, yin2022styleheat, chen2019hierarchical, zhou2021pose, zhou2018talking, chung2016lip} to control the corresponding expressions or video-driven\cite{wang2018vid2vid, wang2019fewshotvid2vid, siarohin2019first, drobyshev2022megaportraits, Reenactgan} for both expression and pose control. In the video-driven setting, a couple of driving and source images are provided, and the generation is expected to inherit the identity information from the source image while mimicking the motion in the driving image. Some methods extract the expression and pose representations using the face edge map drawn from facial landmarks\cite{wang2018vid2vid, zakharov2019few, zakharov2019few}. However, the edge map is determined by not only pose and expression but also identity-specific face shape, making the cross-identity reenactment rather tricky. On the other side, some works refer to represent the motion using image warping\cite{ren2021pirenderer, yin2022styleheat, wang2022latent, siarohin2019first}. Since image warping is merely trained to stimulate 3D motion in the 2D space, the animation often collapses under large pose variations. Our work animates face avatars using the 3D rendering method and explicitly promises 3D consistency, therefore would handle extremely large pose variations.

     %Talking face generation seeks to generate faces with controllable motion while maintaining the identity attribute. Such motion can be audio-driven\cite{wiles2018x2face, guo2021ad, yin2022styleheat, chen2019hierarchical, zhou2021pose, zhou2018talking, chung2016lip} for expression or video-driven\cite{wang2018vid2vid, wang2019fewshotvid2vid, siarohin2019first, drobyshev2022megaportraits, Reenactgan, } for both expression and pose control. Within the video-driven setting, a couple of driving and source images are provided, and the generation is expected to inherit identity information from the source image while mimicking the motion in the driving image. Previously, some methods represented the expression and pose using the face edge map drawn from facial landmarks\cite{wang2018vid2vid, zakharov2019few, zakharov2019few}. However, the edge map is determined by not only pose and expression but also identity-specific face shape, making the cross-identity reenactment rather tricky. On the other side, a good many works refer to represent the motion using image warping\cite{ren2021pirenderer, yin2022styleheat, wang2022latent, siarohin2019first}, but since image warping is merely trained to stimulate 3D motion in the 2D space, the animation collapse under large pose variation. 

\subsection{Volume Rendering}

Volume rendering is a 3D rendering strategy and has demonstrated its success in novel view synthesis. Existing methods either use volume~\cite{fernando2004gpu, Lombardi:2019} to explicitly represent 3D space or implicitly store the 3D scene in the MLP network~\cite{mildenhall2021nerf}. These methods are introduced to animate 3D-consistent portraits. Yu et al.~\cite{yu2021pixelnerf} and Raj et al.~\cite{raj2021pva} retrieved the spatial information from sparse support views to render portraits. On the other side, The 3D face generative model can reconstruct a given identity using GAN Inversion. These models vary from pure MLP architecture~\cite{chan2021pi}, to low-resolution neural rendering followed by an up-sampling strategy~\cite{schwarz2020graf, niemeyer2021giraffe}, and to using more efficient 3D presentation~\cite{Chan2022}. All these methods only support rendering static portraits. To animate the avatar with controllable motion, AD-NeRF\cite{guo2021ad} and NeRFace\cite{gafni2021dynamic} introduce either the audio feature or 3DMM expression coefficients to the NeRF model to render dynamic heads. Nerfies\cite{park2021nerfies} employs a deformation field conditioned on spatial points to represent the face motion of different frames. HyperNeRF\cite{park2021hypernerf} adds an ambient slicing network to tackle the topological drawback of Nerfies. Though these methods have achieved motion control and view consistency for face avatars, they all overfit each NeRF model to one subject, thus lack generalizability.  HeadNeRF~\cite{hong2022headnerf} is trained on both multi-view and frontal face datasets and can be generalized to multiple identities and support motion control using 3DMM coefficients, but its rendering quality is not satisfactory compared with the given portrait, and the motion control is unnatural with random jitters. In contrast, our method can reconstruct photo-realistic face avatars in one shot and animate natural talking motion. 

\subsection{Generative Prior}
\textcolor{black}{
Many methods rely on pre-trained generative models. Employing a generative model as the prior has the following advantages. First, it provides rich and diverse facial priors, such as texture, color, shape and pose, which can help to restore realistic and faithful facial details. For instance, Yang et al.\cite{Yang_2021_CVPR} leveraged a pre-trained GAN model as a prior decoder for blind face restoration. Second, it also enables one-shot image manipulation both inside and outside of the domain. Well-designed schemes for disentanglement are necessary to provide acceptable performance on fine-grained out-of-domain image manipulation. Zhang et al.\cite{zhang2022generalized} suggested separating the attributes into global style—like texture and color—and structure—like shape and pose, and performing domain-adaptive generation by transferring the decoupled style attributes. The image manipulation also includes synthesizing human talking videos, Ivan et al. \cite{skorokhodov2022stylegan} modeled the temporal dynamics of video frames through the latent code disentanglement. Unlike other talking head generation methods\cite{yin2022styleheat,siarohin2019first}, it only synthesizes center-aligned faces. The recently presented 3D GANs have been popularized for their ability to generate 3D-consistent photo-realistic human faces with explicit pose control. Katja et.al \cite{schwarz2020graf} introduced 3D scene volume rendering without resorting to computationally demanding voxel-based representation, and the model is trained from unposed 2D photos in an adversarial manner. Chan et al.\cite{Chan2022} proposed to generate high-quality geometry and multi-view consistent images from 2D photos through a hybrid explicit-implicit network architecture based on StyleGAN2\cite{karras2020analyzing}. Despite the fact that 3D GANs' explicit pose control and 3D-consistent generation are appealing for talking face synthesis, no previous work has studied their application to talking face avatars. 
}

    
    % implicit\cite{mildenhall2021nerf} 3D representations and has been applied to render 3D-consistent faces\cite{guo2021ad, raj2021pva, park2021nerfies, park2021hypernerf,}. Current rendering methods either can be generalized to render multiple static subjects\cite{raj2021pva, chen2021mvsnerf, Mihajlovic:KeypointNeRF:ECCV2022}, or render motion-controllable avatar from a specific subject\cite{guo2021ad, zheng2022imavatar, park2021nerfies, park2021hypernerf}.  On the other side, a bunch of 3D GANs\cite{schwarz2020graf, niemeyer2021giraffe, chan2021pi, Chan2022} are proposed and can achieve one-shot static reconstruction from a specific view using GAN Inversion\cite{zhu2020indomain}. Nevertheless, most existing methods do not support both identity-generalized and motion-controllable rendering. Even though  the generation quality is not promising on the given portrait, and the motion control is neither smooth nor accurate. In contrast, our method can reconstruct photo-realistic face avatars in one shot and animate natural talking motion. 

% \subsection{Image Editing}

% GAN Inversion~\cite{zhu2016generative} is a popular method in face editing. It projects image into the latent space of the generative model. The related methods can be roughly divided into three categories, i.e., optimization-based, learning-based and hybrid ones~\cite{xia2022gan}. Optimization-based methods~\cite{abdal2019image2stylegan, abdal2020image2stylegan++, } iteratively optimizes the latent code for each image; learning-based methods learn an image encoder to map image to its latent code in one propagation
% ~\cite{richardson2021encoding, tov2021designing}; hybrid methods~\cite{wang2022high} use the encoder prediction as a better initialization followed by optimization process. Equipped with these methods, StyleGAN~\cite{chong2021stylegan} is usually applied in different tasks~\cite{zhang2022generalized, xu2022high, abdal2021styleflow, patashnik2021styleclip, tewari2020stylerig}, including face animation~\cite{yin2022styleheat, tian2021good, fox2021stylevideogan}. 
    %Inspired by the disentangled feature space and generation quality of generation networks like StyleGAN\cite{chong2021stylegan}, face editing by GAN Inversion\cite{zhu2016generative} is popularized. GAN Inversion projects image into the latent space of the generative model. It can generally be into optimization-based, learning-based, or hybrid approaches \cite{xia2022gan}. Optimization-based inversion\cite{abdal2019image2stylegan, abdal2020image2stylegan++, } iteratively optimize the latent code for each image; learning-based methods learn an image encoder to map image to its latent code in one propagation\cite{richardson2021encoding, tov2021designing}; hybrid methods\cite{wang2022high, } use the encoder prediction as a better initialization followed by optimization. Both learning-based and hybrid methods require training an encoder for each different generative network. Equipped with these methods, StyleGAN is applied in different tasks\cite{zhang2022generalized, xu2022high, abdal2021styleflow, patashnik2021styleclip, tewari2020stylerig}. StyleGAN has also been applied to face animation\cite{yin2022styleheat, tian2021good, fox2021stylevideogan}. Set aside \cite{tian2021good} which does not expect identity consistency, to faithfully invert given portrait to latent code and animate, both \cite{yin2022styleheat, fox2021stylevideogan} requires a well-trained style encoder ahead of the task.
    
% In order to manipulate the attributes of images generated by pre-trained generative networks like \cite{chong2021stylegan}. Either supervised \cite{shen2020interpreting, jahanian2019steerability, goetschalckx2019ganalyze,zhang2022generalized} or unsupervised \cite{voynov2020unsupervised, peebles2020hessian, shen2021closed} approaches is proposed to edit facial attributes like color, geometric transformation. Due to implicit control of geometric transformation in latent space, implementing pre-trained 2D GANs for controllable face avatar is challenging. But 3D GANs can be a better solution, since the explicit rigid transformation of head has been represented by the camera pose, therefore only the non-rigid transformation of face avatar requires latent manipulation. Due to the non-existence of pre-trained editing encoder \cite{richardson2021encoding, tov2021designing} for 3D GANs, we refer to optimization-based image inversion before learning the motion control. Therefore how to balance the computation cost between optimizing image inversion and learning motion control is the major problem that our work needs to solve. Out of the same reason, among current 3D GANs using Volume rendering\cite{Chan2022, chan2021pi, niemeyer2021giraffe, schwarz2020graf}, we choose EG3D\cite{Chan2022} for its fast inference speed, since the inference latency is of great importance for constraining the training cost. It can also promise the latency in the test time. 
    
  
  

    

    
    
 % Talking face generated faces are expected to maintain the identity attributes of the source image and mimic the motion of the driving image. Early works use image-to-image translation to reenact the faces. 




% \subsection{2D/3D Face Animation}
% For 2D animation, facial landmark can serve as great edge-ware motion representation for GANs to fill in textures within these edges, but such representation suffers catastrophic animation defect when landmarks are self-occluded caused by, for example, large poses. Such drawback still exists in warping based model\cite{ren2021pirenderer,yin2022styleheat}, since 2D image warping cannot be guaranteed to perfectly simulate the actual motion happened in 3D space. Recently, the NeRF(Neuron Radiance Field\cite{mildenhall2021nerf}) has brings Volume rendering to the public. Its strict 3D formulation enables strong multi-view consistency and has been introduced to face rendering\cite{hong2022headnerf,guo2021ad, zheng2022imavatar,gao2021dynamic,park2021nerfies,park2021hypernerf,Chan2022,chan2021pi, Mihajlovic:KeypointNeRF:ECCV2022, }. However, current methods either over-fit one NeRF for each single identity\cite{guo2021ad, zheng2022imavatar, gao2021dynamic, park2021nerfies, park2021hypernerf}, or is able to generalize to render multiple identities but the motion is uncontrollable\cite{chen2021mvsnerf,Mihajlovic:KeypointNeRF:ECCV2022,chan2021pi,yu2021pixelnerf}. The most corresponding method\cite{hong2022headnerf} can reconstruct desired identity given single reference image and support controllable animation, but since the model is trained in the dataset recorded in the multi-view stereo with limited identities and talking corpus, the reconstruction is unsatisfying and the talking animation demonstrates unnatural jitters. To this end, our solution is to train the talking control on large amount of monocular talking videos, and use pre-trained generative model to guarantee the identity consistency.

% \subsection{Latent Manipulation}


% \subsection{Neuron Radiance Field}

% The NeRF firstly proposed to render static scenes. To facility dynamic animation to NeRF, \cite{du2021neural, li2021neural, gao2021dynamic} models scene deformation using 3D flows, while \cite{xian2021space} learns the spatial-temporal irradiance fields, these methods focus on the time-varying scenes but cannot be generalized to render beyond the time range. \cite{wang2021learning} introduces 3D-aware primitive to Volume rendering of expression-controllable faces. However, it requires multi-view talking corpus as input, which is expensive to construct. \cite{hong2022headnerf, Gao2022nerfblendshape, guo2021ad, zheng2022imavatar, gafni2021dynamic, park2021nerfies, park2021hypernerf} use audio or expression conditions as the input to drive the implicit field deformation on monocular videos, but they still need hundreds or thousands of frames to train a single NeRF model. 





% \subsection{Generalizable NeRF}
% Another drawback of the vanilla NeRF is the inability of rendering multiple scenes with one model. Therefore, another streamline of work focus on generalizing NeRF to reconstruct plural static scenes. Specifically for face avatar, \cite{zhuang2022mofanerf, hong2022headnerf} use 3DMM parameter to model the facial attribute and motion of face avatar, they cannot achieve the photo-metric reconstruction on given portrait image, since the multi-view face dataset is of small scale. Another percentage of methods condition the NeRF on features extracted from a few frames of different views\cite{yu2021pixelnerf, raj2021pva, chen2021mvsnerf}. However, these methods are unable to render dynamic scenes. 


\section{Method}
Talking face avatar aims to synthesize face images with controllable expression and pose. In this paper, we investigate the possibility of endowing volume rendering with the ability to 1) build a faithful identity representation in one shot, 2) enable natural motion control, and 3) achieve real-time inference speed.
%by an efficient architecture
 Our framework takes an identity code and a motion signal as input and generates a tri-plane-based~\cite{Chan2022} volume through a CNN architecture. 
 %where the identity code is extracted from only one portrait image. 
 By performing volume rendering on the tri-plane representation, where each point is projected on the three feature planes and the sampled features are fused to occupancy and color, a face image at a given camera view can be generated as:

% \begin{equation}
% \label{equ-briefview}
% \mathbf{I}(\mathbf{p}) = \mathcal{R}(\emph{G}(\mathbf{w}_{id},\mathbf{w}_{mo};\theta), \mathbf{p}),
% \end{equation}

\begin{equation}
\label{equ-briefview}
\mathbf{I}(\mathbf{p}) = \mathcal{R}(\emph{G}(\mathbf{w}_{id},\mathbf{x};\Theta), \mathbf{p}),
\end{equation}
where $\mathbf{w}_{id}$ is the identity code, $\mathbf{x}$ is the motion signal, $\emph{G}(\cdot,\cdot;\Theta)$ is a face volume generation network with $\Theta$ as its network weights to reconstruct an encoded portrait in three orthogonal feature planes, $\mathcal{R}(\mathbf{V}, \mathbf{p})$ is the volume rendering with $\mathbf{V}$ as the volume and $\mathbf{p}$ as the camera view, and $\mathbf{I}(\mathbf{p}) \in \mathbb{R}^{3 \times H \times W}$ is the rendered result. In our implementation, the identity code $\mathbf{w}_{id}$ can be extracted by optimizing on a single reference image using our proposed decoupling-by-inverting strategy (Sec.~\ref{chaper:Latent code generation}), achieving one-shot avatar reconstruction. The motion signal $\mathbf{x}$ is 3DMM coefficients for flexible control. We call $\emph{G}(\cdot,\cdot;\Theta)$ as \textbf{3D Animator} because of its ability to animate an avatar with desired motion. Compared with 2D methods, we construct a neural 3D space to promise multi-view consistency. Compared with NeRF, we build a 3D space in the propagation of a CNN, and directly sample features on the tri-plane representation, saving the MLP computation on each sampled point. 


% As for the controllability, we propose Decoupling by Inverting, which disentangles motion-related latent fraction and motion-free identity representation in the latent space of the generator. As shown in Fig.\ref{fig:overview of OTAvatar}, given reference or source image $Y_s$ as input, we first estimate its motion-free identity representation $\mathbf{w}_r$ in the latent space, conditioned on its motion $x_s$ and head view $p_s$ using (more detail in Sec.\ref{chaper:Latent code generation})
% \begin{equation}
%   \mathbf{w}^*_r=\underset{\mathbf{w}_r}{\arg \min }  \mathcal{L}(Y_s, G(Ca(\mathbf{w}_r,x_s), p_s)).
%   \label{eq: gan inversion with refinement}
% \end{equation}
% Here, following \cite{yin2022styleheat, ren2021pirenderer}, a concatenation of pose and expression coefficients of 3DMM extracted from source image $Y_s$ are implemented as the motion descriptor $x_s$, while the 3DMM pose is also implemented as the camera view $p_s$ as did in \cite{guo2021ad, hong2022headnerf}; $\mathcal{L}$ measures the distance between the generation $G(\mathbf{w}_s, p)$ and its ground truth $Y_s \sim \mathcal{Y} \in \mathbb{R}^{3 \times H \times W}$ (will be discussed in Sec.\ref{Chapter:Loss Terms}); $Ca$ is our proposed motion calibration model and $\mathbf{w}_s = Ca(\mathbf{w}_r, x_s)$ (will be discussed in Sec.\ref{chaper:Latent code generation}) . Then takes as input any desired motion $x_d$ and camera view $p_d$, we incorporate $x_d$ into $\mathbf{w}_r$ to obtain its motion-related latent code $\mathbf{w}_d = Ca(\mathbf{w}_r, x_d)$, which can be decoded into its tri-plane 3D spatial representation $T(\mathbf{w}_d)$. Using Volume rendering we can animate the given identity of specific motion conditioned on camera view $p_d$, using the forward pass $Y_d = R(T(\mathbf{w}_d), p_d) = G(\mathbf{w_d}, p_d)

% Talking face avatar aims at synthesizing 3D-consistent talking face with the expression and pose being controllable. In this paper, we investigate the possibility of Volume rendering methods on 1) reconstructing faithful 3D identity representation in one shot and 2) realizing natural motion control with Volume rendering, and 3) promoting the inference speed using efficient architecture. For efficiency, we refer to the tri-plane\cite{Chan2022}, which is a compact representation for 3D space, and write the 3D rendering on given tri-plane $T$ as $R(T,p)$, where $R$ refers to Volume rendering and $p$ is the camera view (will be discussed in Sec.\ref{chapter:Tri-plane 3D Representation and Rendering}). Such tri-plane can be generated in StyleGAN-like modulation fashion, where only a high-dimensional latent code is required as input, we write the tri-plane generated using latent code $\mathbf{w}$ as $T(\mathbf{w})$. EG3D\cite{Chan2022} is a 3D generative model constructed using the Tri-plane, the rendering of EG3D can be formulated via $Y = R(T(\mathbf{w}), p) = G(\mathbf{w}, p) $, where $Y \sim \mathcal{Y} \in \mathbb{R}^{3 \times H \times W}$ is the rendering result and $G$ represents EG3D network . Similar to other generative model (both 2D and 3D), EG3D is able to faithfully reconstruct given subject with single portrait reference\cite{abdal2019image2stylegan}. To this end, by implementing pre-trained EG3D as our generator, the efficiency and generalizability of our three goals are satisfied.

% As for the controllability, we propose Decoupling by Inverting, which disentangles motion-related latent fraction and motion-free identity representation in the latent space of the generator. As shown in Fig.\ref{fig:overview of OTAvatar}, given reference or source image $Y_s$ as input, we first estimate its motion-free identity representation $\mathbf{w}_r$ in the latent space, conditioned on its motion $x_s$ and head view $p_s$ using (more detail in Sec.\ref{chaper:Latent code generation})
% \begin{equation}
%   \mathbf{w}^*_r=\underset{\mathbf{w}_r}{\arg \min }  \mathcal{L}(Y_s, G(Ca(\mathbf{w}_r,x_s), p_s)).
%   \label{eq:gan inversion with refinement}
% \end{equation}
% Here, following \cite{yin2022styleheat, ren2021pirenderer}, a concatenation of pose and expression coefficients of 3DMM extracted from source image $Y_s$ are implemented as the motion descriptor $x_s$, while the 3DMM pose is also implemented as the camera view $p_s$ as did in \cite{guo2021ad, hong2022headnerf}; $\mathcal{L}$ measures the distance between the generation $G(\mathbf{w}_s, p)$ and its ground truth $Y_s \sim \mathcal{Y} \in \mathbb{R}^{3 \times H \times W}$ (will be discussed in Sec.\ref{Chapter:Loss Terms}); $Ca$ is our proposed motion calibration model and $\mathbf{w}_s = Ca(\mathbf{w}_r, x_s)$ (will be discussed in Sec.\ref{chaper:Latent code generation}) . Then takes as input any desired motion $x_d$ and camera view $p_d$, we incorporate $x_d$ into $\mathbf{w}_r$ to obtain its motion-related latent code $\mathbf{w}_d = Ca(\mathbf{w}_r, x_d)$, which can be decoded into its tri-plane 3D spatial representation $T(\mathbf{w}_d)$. Using Volume rendering we can animate the given identity of specific motion conditioned on camera view $p_d$, using the forward pass $Y_d = R(T(\mathbf{w}_d), p_d) = G(\mathbf{w_d}, p_d)$. 



% \begin{figure*}[ht]
%   \centering
%    \includegraphics[width=1\linewidth]{CVPR_2023_Towards Generalized Talking Face Avatar via Volume Rendering/Speed-up Inversion while Decoupling_v3.pdf}
%    \caption{ \textbf{Training and inference schemes of decoupling-by-inverting}. During training, the controller module predicts the motion codes to make up $\mathbf{w}_s = \mathbf{w}_{id} + \emph{C}(\mathbf{x}_s ; \Theta_{c})$ and $\mathbf{w}_d = \mathbf{w}_{id} + \emph{C}(\mathbf{x}_d ; \Theta_{c})$.  We first optimize $\mathbf{w}_{id}$ then update $\Theta_{c}$ at different steps, using the dual-objectives of $\mathcal{L}_s + \mathcal{L}_d$ in Eqn.~\ref{equ-controller-training}. For detail, please see Algo.~\ref{alg:Inverting while Decoupling Speed-Up Algorithm}. During inference, there is only one source image  provided. With the well-trained controller, we set $\mathbf{w}_s = \mathbf{w}_{id} + \emph{C}(\mathbf{x}_s ; \Theta_{c})$ and use the objective of $\mathcal{L}_s$ in Eqn.~\ref{equ-gan inversion with control test} ~to optimize $\mathbf{w}_{id}$. }
%    \label{fig:pipeline}
% \end{figure*}


\subsection{3D Animator Network Structure}
\label{chapter:Tri-plane 3D Representation and Rendering}

\textbf{Tri-plane volume representation.} The output of the 3D animator network $\emph{G}(\cdot,\cdot;\Theta)$ in Eqn.~\ref{equ-briefview} is a tri-plane volume representation, which is composed of three feature planes:
\begin{equation}
\label{equ-triplane}
\mathbf{V}_{tri} = (\mathbf{F}_{xy}, \mathbf{F}_{xz}, \mathbf{F}_{yz}) =  \emph{G}(\mathbf{w}_{id},\mathbf{x};\Theta),
\end{equation}
where $\mathbf{F}_{xy}$, $\mathbf{F}_{xz}$, and $\mathbf{F}_{yz}$ are three axis-aligned orthogonal feature maps in the 3D space, implicitly forming the volume $\mathbf{V}_{tri}$. When performing volume rendering, for each queried point $(x,y,z)$, we project it onto each of three feature maps and retrieve the corresponding features $(\mathbf{F}_{xy}(x,y), \mathbf{F}_{xz}(x,z), \mathbf{F}_{yz}(y,z))$. The summation of features is sent to a lightweight MLP to decode color and opacity. Compared to fully implicit MLP architectures\cite{guo2021ad, zheng2022imavatar, chan2021pi} or volume representation\cite{Lombardi:2019}, we can efficiently regress the three 2D feature maps through a CNN architecture. In our implementation, 
$\emph{G}$ is a deconvolutional network~\cite{chong2021stylegan} that outputs three $256 \times 256 \times 32$ feature maps. 
%{\color{black}(what about the upsampling on low-resolution Volume rendering?)}

\textbf{Animator structure.} Towards animating the face avatar of a given portrait, an intuitive solution is representing $\mathbf{w}_{id}$ with a pre-defined feature and training $\emph{G}(\cdot)$ from scratch. However, we find that the model does not work well due to the sub-optimal $\mathbf{w}_{id}$. For example, when employing 3DMM shape coefficients~\cite{deng2019accurate} and face recognition features\cite{deng2019arcface}, the identity information is not preserved in the rendering results because $\mathbf{w}_{id}$ does not encode face appearances well. These models are trained on cropped facial images which contain no torso and hairstyle. Therefore, $\mathbf{w}_{id}$ losses such information that is necessary when animating portraits. The network even collapses if we make $\mathbf{w}_{id}$ learnable. 

In this work, we employ a two-phase strategy to achieve one-shot avatar reconstruction: 1) building a 3D face generator, and 2) making the generator controllable. 
% Among the current 3D face generation methods~\cite{niemeyer2021giraffe, schwarz2020graf, chan2021pi, Chan2022} which generates a face image from a latent code, 
% Considering the great presentation power of 3D face generation model~\cite{niemeyer2021giraffe, schwarz2020graf, chan2021pi, Chan2022}, 
We build our 3D face generator on a pre-trained EG3D\cite{Chan2022} network, which incorporates tri-plane representation for efficient 3D face generation:
\begin{equation}
\label{equ-eg3d}
\mathbf{V}_{tri} = \emph{G}_{eg}(\mathbf{w};\Theta_{eg}),
\end{equation}
%{\color{black}($\Theta_{eg}$ or $\Theta_{3d}$ is better)}
where $\mathbf{w} $ is an uninterpretable latent code \textcolor{black}{drawn from the latent space of the generator $\emph{G}_{eg}$. The latent space is determined by the generator weight $\Theta_{eg}$}.
%and part of which is drawn from a standard Gaussian distribution, and $\Theta_{eg}$ are the pre-trained parameters.
%which is kept frozen during training. 
To control the latent code $\mathbf{w}$ by the given identity represented by $\mathbf{w}_{id}$ and motion signal $\mathbf{x}$, we propose a motion controller module $\emph{C}$ parameterized by $\Theta_{c}$, which maps motion signal $\mathbf{x}$ to the motion code $\mathbf{w}_x$, such that:
\begin{equation}
\label{equ-controller}
\mathbf{w} = \mathbf{w}_{id} + \mathbf{w}_{x} = \mathbf{w}_{id} + \emph{C}(\mathbf{x}; \Theta_{c}).
\end{equation}
%\textcolor{black}{and the addition of $\mathbf{w}_{id}$ and  a latent code $\mathbf{w}_x$ in the} . 
Injecting Eqn.~\ref{equ-controller} to Eqn.~\ref{equ-eg3d}  , we have the structure of the 3D animator:
\begin{equation}
\begin{aligned}
\label{equ-3d-animator-net}
\emph{G}(\mathbf{w}_{id},\mathbf{x};\Theta) &= \emph{G}_{eg}(\mathbf{w}_{id} + \emph{C}(\mathbf{x}; \Theta_{c}); \Theta_{eg}), \\
\Theta &= \Theta_{eg} \cup  \Theta_{c}.
\end{aligned}
\end{equation}
% {\color{black}($\mathbf{w}_{id} \in \mathcal{W}$  or $\in \mathcal{W}^*$)}
In our implementation, the motion signal $\mathbf{x}$ is constructed by 3DMM pose and expression coefficients~\cite{deng2019accurate}, which are more flexible compared with image-based driving signals~\cite{siarohin2019first, siarohin2021motion, wang2022latent}, especially in cross-identity driving. 
%$\mathbf{w}_{id}$ is a subject-appearance descriptive feature, depending on a given reference image and the interpreter, $\mathbf{w}_{x}$ the predicted motion code conditioned on motion $\mathbf{x}$ .

\textbf{Controller structure.} We use the concatenation of 3DMM pose and expression coefficients to represent motion $\mathbf{x}$. Practically, we use the adjacent coefficients of pose and expression within a particular radius to represent the motion signal of the current frame, firstly the weighted summation of the coefficients across the temporal dimension is conducted to avoid noises, which is achieved by three 1D convolution layers\cite{ren2021pirenderer, yin2022styleheat}; secondly, a five-layer MLP is employed to transform the weighted summation into a motion feature; finally, a codebook with learnable orthogonal bases~\cite{wang2022latent} is built, on which each motion feature is projected to get the final motion code $\mathbf{w}_x$. The aforementioned operation is summarized as follows:
%Afterward, the motion code and the identity code are summed to serve as the latent code of the 3D face generator:
% \begin{equation}
% \begin{aligned}
%     \mathbf{w} = &~\mathbf{w}_{id} + \mathbf{x}\\
%     \mathbf{x} = x(\mathbf{c}) = \mathbf{D} * ~& \mathrm{MLP}(\sum_{t=-T}^{T}w_{t}\mathbf{c}_{t}; \Theta_{mlp})  \\
%     \label{equ-controller}
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
    % \mathbf{w} = &~\mathbf{w}_{id} + \mathbf{w}_{x} = \\\
    %\mathbf{w}_x 
    \mathbf{w}_x = \emph{C}(\mathbf{x}; \Theta_{c}) = & ~\mathbf{D} * ~\emph{F}_{M}(\emph{F}_{T}(\mathbf{x}; \Theta_{T}); \Theta_{M}) , 
    \label{equ-controller-2}
\end{aligned}
\end{equation}
where $\emph{F}_{T}$ parameterized by $\Theta_{T}$ is the temporal smoothing network of 1D convolution layers, $\emph{F}_{M}$ parameterized by $\Theta_{M}$ is the five-layer MLP which projects motion to the magnitudes of bases contained in the codebook $\mathbf{D}$. It can be seen that the magnitude projection parameters, temporal smoothing weights, and the codebook are the parameters of the controller $\emph{C}$ to be trained, that is:
\begin{equation}
\Theta_{c}=\mathbf{D}  \cup  \Theta_{T} \cup \Theta_{M}.
\end{equation}
%Please refer to the supplemental materials for more details.


% In 2D methods, the motion of images is formulated with warping, which cannot promise multi-view consistency. While in previous 3D methods, 3D spaces are formulated with volumes, or tensors with the size of $N^3 \times C$ where $N$ is the spatial resolution and $C$ is the channel dimension. Given the camera view $p$ which includes the camera's intrinsic and extrinsic, the image of the desired view can be rendered. This volume explicitly caches the space and improves the test-time inference speed, but only for static scenes. For deformable scenes, one has to recreate the volume for each kind of deformation, which is computation-intensive. On the other hand, the implicit neuron representation\cite{mildenhall2021nerf} queries MLP to get the color and density for each sample point in Volume rendering, and can render deformable scenes in multiple ways\cite{guo2021ad, hong2022headnerf}. Despite its drastic scene compression, the latency can be extended to tens of seconds per frame. The tri-plane architecture\cite{Chan2022} compresses the volume to three axis-aligned orthogonal feature planes of size $N^2 \times C$, the computational and memory cost of the space representation can be reduced to around $1/N$. Chan et.al\cite{Chan2022} proposed EG3D which generates tri-plane given latent code, then for Volume rendering, any 3D points $x \in \mathbb{R}^3$ in the tri-planes space can be queried by projecting it onto each of the three feature planes, retrieving the corresponding feature vector $F_{x y}, F_{x z}, F_{y z}$ via bi-linear interpolation, and aggregating the three feature vectors via summation. An additional lightweight decoder network, implemented as a shallow MLP, interprets the aggregated 3D features $F$ as color and density. These quantities are rendered into RGB images using volume rendering. This network achieves high-quality rendering and efficiency in both memory cost and inference speed. 


% In 2D methods, the motion of the image is formulated with warping, which cannot promise multi-view consistency. While in previous 3D methods, 3D spaces are formulated with volumes, or tensors with the size of $N^3 \times C$ where $N$ is the spatial resolution and $C$ is the channel dimension. Given the camera view $p$ which includes camera intrinsic and extrinsic, the image of desired view can be rendered. This volume explicitly caches the space and improve the test-time inference speed, but only for static scenes. For deformable scenes, one has to recreate the volume for each kind of deformation, which is computation-intensive. On the other hand, the implicit neuron representation\cite{mildenhall2021nerf} queries MLP to get the color and density for each sample point in Volume rendering, and can render deformable scenes in multiple ways\cite{guo2021ad, hong2022headnerf}. Despite its drastic scene compression, the latency can be extended to tens of seconds per frame. The tri-plane architecture\cite{Chan2022} compresses the volume to three axis-aligned orthogonal feature planes of size $N^2 \times C$, the computational and memory cost of the space representation can be reduced to around $1/N$. Chan et.al\cite{Chan2022} proposed EG3D which generates tri-plane given latent code, then for Volume rendering, any 3D points $x \in \mathbb{R}^3$ in the tri-planes space can be queried by projecting it onto each of the three feature planes, retrieving the corresponding feature vector $F_{x y}, F_{x z}, F_{y z}$ via bi-linear interpolation, and aggregating the three feature vectors via summation. An additional lightweight decoder network, implemented as a shallow MLP, interprets the aggregated 3D features $F$ as color and density. These quantities are rendered into RGB images using volume rendering. This network achieves high-quality rendering and efficiency in both memory cost and inference speed. 

% \textbf{Pre-trained 3D Generator}. To tackle with the shortage of multi-view talking face dataset for training generalized talking face avatars with Volume rendering, our solution is to refer to pre-trained Volume rendering generator, since it can reconstruct 3D representation given single reference portrait using GAN Inversion, which is exactly what 3D models trained on multi-view dataset managed to achieve\cite{raj2021pva, Mihajlovic:KeypointNeRF:ECCV2022, hong2022headnerf, gafni2021dynamic}. Then the challenge comes to how to enable controllability on the pre-trained generative model. In 2D area, similar methods have been investigated\cite{yin2022styleheat, tian2021good, fox2021stylevideogan}, by enabling natural motion to pre-trained StyleGAN\cite{chong2021stylegan}. Set aside \cite{tian2021good} which does not expect identity consistency, to faithfully invert image to latent code, both \cite{yin2022styleheat, fox2021stylevideogan} refers learning-base inversion methods\cite{richardson2021encoding, tov2021designing, wang2022high}. These inversion encoder enables extracting latent code for each portrait in single forward pass, while no current works are done for designing such encoder for 3D GANs. Therefore we are supposed to turn to optimization-based inversion\cite{abdal2019image2stylegan}, which iteratively optimize the latent code until obtaining a good reconstruction of the reference image. To this end, the challenge is transformed to, how to enable motion control in optimization-based inversion for 3D GANs, which we will discuss in the next section.







% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}




% \textbf{Motion Prompting module}. Following\cite{ren2021pirenderer, hong2022headnerf}, the pose and expression of 3DMM coefficients are implemented for representing motion, and the coefficients of a window with continuous frames are used to represent the motion of the center frame. This sliding window can avoid the 3DMM errors and noises by extracting relations between adjacent frames, we follow \cite{ren2021pirenderer} to obtain motion presentation from the sequential coefficients. As shown in Fig.[Refinement Module], these coefficients are transformed to coefficients of a code-book, which contains a set of learn-able orthogonal motion direction\cite{wang2022latent}. The MLP module will output the magnitude or contribution of each direction to the motion refinement. The motion refinement on identity latent code is realized by

   
   % We propose decoupling-by-inverting to decouple the identity and motion information from latent codes of the pre-trained generator for both training and inference, using optimization-based inversion. Specifically, we propose a controller module to predict motion code $\mathbf{w}_{x} = \emph{C}(\mathbf{x}; \Theta_{c})$ given 3DMM coefficients $\mathbf{x}$ as motion representation.
   
   % 1) In the training stage, we calibrate source and target motion $\mathbf{x}_s, \mathbf{x}_d$ onto identity code $\mathbf{w}_{id}$, by predicting motion code $\mathbf{w}_{\mathbf{x}_s}$ and $\mathbf{w}_{\mathbf{x}_d}$ and add them to  $\mathbf{w}_{id}$. 
   
   
   % in order to optimize the identity code $\mathbf{w}_{id}$ and disentangle it from the motion code, we calibrate the motion code $\mathbf{w}_{\mathbf{x}_s}$ and $\mathbf{w}_{\mathbf{x}_d}$ predicted by the controller $\emph{C}$  onto $\mathbf{w}_{id}$

   
   % 1) In the training stage, in order to optimize the identity code $\mathbf{w}_{id}$ and disentangle it from the motion code, we train a control module $\emph{C}$ to refine $\mathbf{w}_r$ in terms of the motion fraction by $\mathbf{w}_s = R(\mathbf{w}_r, x_s; \theta)$ and $\mathbf{w}_t = R(\mathbf{w}_r, x_t; \theta)$, the outputs $\mathbf{w}_s, \mathbf{w}_t$ are expected to be rendered to $Y_s$ and $Y_t$. For the tuple of $Y_s$ and $Y_t$, the optimization of $w_r$ and training of $R$ parameterized by $\theta$ are configured to be separate along multiple forward passes. 2) In the inference stage, only a single image $Y_s$ is provided for animation. We first prompt the motion-related latent fraction by providing source motion $x_s$ to the well-trained $R$. Therefore only the identity-related $\mathbf{w}_r$ in $R(\mathbf{w}_r, x_s;\theta)$ requires optimization. Such decoupled inversion optimization enables the rendering of any desired motion $x_t$ simply via alternating the refinement to be $R(\mathbf{w}_r, x_t; \theta)$.
   



\subsection{Controller Training}
\label{sec:alternative manner}

%The purpose of the controller training is approaching the latent code $\mathbf{w}$ of the 3D face generator $\mathcal{G}$ from the decoupled identity code $\mathbf{w}_{id}$ and the motion code $\mathbf{x}$, with $\mathbf{w}_{id}$ being identity descriptive and clean from irrelevent information. 
Given a couple of source frame $\mathbf{I}_s$ and driving frame $\mathbf{I}_d$, along with their motion signals $\mathbf{x}_s, \mathbf{x}_d$ and camera views $\mathbf{p}_s, \mathbf{p}_d$, we perform the dual-objective optimization:
\begin{equation}
\label{equ-controller-training}
\begin{aligned}
  \mathbf{w}^*_{id} &= \underset{\mathbf{w}_{id}}{\arg \min } (  \mathcal{L}_s + \mathcal{L}_d ),\\
  \mathcal{L}_s = \mathcal{L}(I_s, \hat{I}_s) &= \mathcal{L}(I_s, \mathcal{R}(\emph{G}(\mathbf{w}_{id}, \mathbf{x}_s; \Theta), \mathbf{p}_s), \\
  \mathcal{L}_d = \mathcal{L}(I_d, \hat{I}_d) &= \mathcal{L}(I_d, \mathcal{R}(\emph{G}(\mathbf{w}_{id}, \mathbf{x}_d; \Theta), \mathbf{p}_d).
\end{aligned}    
\end{equation}
It is implemented to estimate the identity code $\mathbf{w}_{id}$ shared between $\mathbf{I}_s$ and $\mathbf{I}_d$.
%when the controller parameter $\Theta_{c}$ in $\Theta = \Theta_{eg} \cup \Theta_{c}$ is being updated.
$\emph{G}$, $\mathcal{R}$, and $\mathcal{L}$ indicate 3D face generator, volume rendering, and loss function, respectively. The 3D face animator $G$ is parameterized by $\Theta = \Theta_{eg} \cup \Theta_{c}$. $\Theta_{eg}$ includes the parameters of the pre-trained 3D face generator, and $\Theta_{c}$ includes the parameters of the controller which is the focus of our training scheme. 

% We are supposed to point out that the above method requires iterative calculation on each couple of $Y_s$ and $Y_d$, since estimating a good inversion can not be guaranteed in one shot. If we train our model with same data throughput as previous methods, the estimated training cost would be dozens of times longer. Our solution is utilizing existed mini-batch more thoroughly. 

\textbf{Training scheme}. The purpose of controller training is to decouple the identity code and motion code from the latent code of the generator, making identity and motion replaceable in our generalized avatars. To this end, we propose the \textit{decoupling-by-inverting} training strategy, which is generally an alternation of identity optimization and controller training at different steps. Specifically, we first freeze $\Theta_{c}$ and perform $N_{id}$ steps of back-propagation on $\mathbf{w}_{id}$. 
%When a stable $\mathbf{w}_{id}$ is achieved, 
Then we freeze $\mathbf{w}_{id}$ and perform $N_{mo}$ steps of back-propagation on $\Theta_{c}$ so that the controller learns to add the motion code on top of the identity code.
%When $N_{id} \gg N_{mo}$, we find the most of identity information is encoded in $\mathbf{w}_{id}$ and not leaked to $\Theta_{c}$, making $\mathbf{w}_{id}$ replaceable in avatar construction. 
In our implementation, $N_{id}=90$ and $N_{mo}=10$. 
%Figure~\ref{fig:pipeline} illustrates the training strategy. 
After $N = N_{id} + N_{mo}$ steps, we slightly finetune $\Theta_{eg}$ with a learning rate of $10^{-4}$ to make the rendering fit the driving signal better. A brief pseudo-code of the whole scheme is provided in Alg.~\ref{alg:Inverting while Decoupling Speed-Up Algorithm}.

\subsection{One-shot Avatar Construction}
\label{chaper:Latent code generation}
In the inference stage, given a single portrait image $\mathbf{I}_{r}$, its 3DMM coefficients $\mathbf{x}_{r}$, and camera view $\mathbf{p}_{r}$ as the reference of avatar construction, we can estimate the identity code $\mathbf{w}_{r}$ of the reference portrait via: 
\begin{equation}
  \mathbf{w}_{r}=\underset{\mathbf{w}_{id}}{\arg \min }  \mathcal{L}(\mathbf{I}_r, 
  \mathcal{R}(\emph{G}(\mathbf{w}_{id}, \mathbf{x}_r), \mathbf{p}_r).
  \label{equ-gan inversion with control test}
\end{equation}
Afterwards, we can animate the identity with given driving motion $\mathbf{x}_d$ and camera view $\mathbf{p}_d$, either from the same or different subject, through the following operation: 
\begin{equation}
    \label{equ-same/cross reenactment}
    \mathbf{I}(\mathbf{w}_{r}, \mathbf{x}_d, \mathbf{p}_d) = \mathcal{R}(\emph{G}(\mathbf{w}_{r}, \mathbf{x}_d), \mathbf{p}_d),
\end{equation}
where $\mathbf{I}(\mathbf{w}_{r}, \mathbf{c}_d, \mathbf{p}_d)$ has the identity estimated as $\mathbf{I}_r$, and is animated with the motion $\mathbf{c}_d$ and camera view $\mathbf{p}_d$. We name Eqn.~\ref{equ-gan inversion with control test} as the \textit{decoupling-by-inverting} inference strategy since it can estimate decoupled identity code from latent code in the iterative optimization-based GAN Inversion, and can be used to render different motions. 

% we calculate the $w_r$ using
% \begin{equation}
%   \mathbf{w_s}^*=\underset{\mathbf{w_s}}{\arg \min }  \mathcal{L}(Y_s, G(\mathbf{w_s}, p_s)).
%   \label{eq:gan inversion}
% \end{equation}

% Where $p$ is the head pose of the source image $p_s$. Nevertheless, such optimization is unable to decouple the identity and motion component. Such disentanglement is of great help for controllable rendering, since by alternating the motion fraction of latent code, we can animation the identity with different motions. Our solution is to predict the motion fraction ahead of Eq.\ref{eq:gan inversion}, using our proposed $R(\mathbf{w}_r,x) = w$. The refinement module can complement motion-related latent fraction on the motion-free identity code $w_r$. Combining $R$ with Eq.\ref{eq:gan inversion}, for source or reference image $Y_s$, 


\subsection{Loss Terms}
\label{Chapter:Loss Terms}

The loss $\mathcal{L}(\mathbf{I}, \hat{\mathbf{I}})$ in Eqn.~\ref{equ-controller-training} and Eqn.~\ref{equ-gan inversion with control test} measures the error between the animated result $\hat{\mathbf{I}}$ and the ground truth $\mathbf{I}$. First, a pre-trained VGG-19 network is implemented to calculate the distance of multi-scale activation maps $\phi_i(\textbf{I})$ via:
\begin{equation}
\mathcal{L}_c=\sum_i\left\|\phi_i(\mathbf{I})-\phi_i(\hat{\mathbf{I}})\right\|_1,
\end{equation}
where $\phi_i$ is the $i$-th layer of VGG. Second, the distance between the Gram matrices $G_j^\phi$ constructed from the $j$-th activation maps $\phi_j$\cite{ren2021pirenderer} of $\mathbf{I}$ and $\hat{\mathbf{I}}$ is measured:
\begin{equation}
\mathcal{L}_s=\sum_j\left\|G_j^\phi(\mathbf{I})-G_j^\phi(\hat{\mathbf{I}})\right\|_1.
\end{equation}
Third, we utilize the L1 distance performed on the eyes and mouth regions to supervise the expression detail:
\begin{equation}
\mathcal{L}_m=\sum_n\left\|R_n(\mathbf{I})-R_n(\hat{\mathbf{I}})\right\|_1,
\end{equation}
where $R_n$ is either the eyes or mouth region extracted using RoI-align on the bounding boxes calculated using landmarks. Finally, the ID Loss is performed to preserve the identity consistency:
\begin{equation}
\mathcal{L}_{i d}=1-\cos (E(\mathbf{I}), E(\hat{\mathbf{I}})),
\end{equation}
where $E$ is the Arcface face recognition model \cite{deng2019arcface}. 
%We also normalize $\mathbf{w}_{id}$ in the $\mathcal{P}$-norm space\cite{zhu2020domain} as applied in \cite{kang2021gan}, hoping the optimized identity code locate in an editable position of the latent space.

The animation loss $\mathcal{L}(\mathbf{I}, \hat{\mathbf{I}})$ is a weighted summation of the above loss terms and is implemented in both training and test-time one-shot avatar construction. Additionally, in the training, we maintain the stability of tri-plane representation with monotonic and TV loss\cite{Chan2022}, and $\mathcal{L}_1$ loss on $\Theta_{c}$ to avoid the latent code being out of the latent distribution of $\emph{G}_{eg}$. 
%{\color{black}(revise the captions of Fig,2,3 and Alg.1)}

% The intuition of the above training scheme can be more clarified when compared to its baseline, in which case we spend $N$ forward passes for reconstructing $w_s, w_d$ for source and driving image separately, then only one forward pass is for training the refinement module $R$. It is clear that such forward pass requires $N$ times larger cost if we assume the $R$ requires fixed amount of training times, however by continuously correcting the refinement of $R$ on current input, the percentile of image inversion can be diluted. 
% We also performs experiments on testifying whether repetitively improve the prediction on small mini-batch of data can impede the $R$ from finding the global best solution. 


% $\mathbf{w}_{r \rightarrow s}, \mathbf{w}_{r  \rightarrow t}$ and the coupling with $\mathbf{w}_r$.


% since for two images sampled from same video, it is intuitive to conclude that the shared component $\mathbf{w}_r$ between $\mathbf{w}_s$ and $\mathbf{w}_d$ accounts for the identity information of the subject in the video, while the private fragments $\mathbf{w}_{r \rightarrow s}$ and $\mathbf{w}_{r \rightarrow d}$ account for the variations of motion i.e. expression and pose. We formulate the interaction between $\mathbf{w}_r$ and $\mathbf{w}_{r \rightarrow s}, \mathbf{w}_{r \rightarrow d}$ as 

% Our solution is implementing pre-trained Volume rendering generative model for its generalized identity reconstruction. The challenge then lies in how to control the animation result in terms of pose and expression. On the one hand, the camera extrinsic as required by Volume rendering inherently makes pose variation feasible, but it only models the rigid transformation of head motion. On the other hand, the flexibility on the latent space of generative models brings with a good replenishment, that it can realize non-rigid transformation on both expression and pose by latent manipulation, as demonstrate in 2D GANs\cite{chong2021stylegan}. Specifically, for training, as is shown in Fig.\ref{fig:pipeline}, our model takes in a pair of source and driving images and their affiliated components (i.e. camera poses and motion representations) that are randomly sampled from one video sequence. Our training objective is to reconstruct the driving image given source image and driving motion. For testing, frames of the driving video are sequentially animated with the corresponding motion representation and the same identity attribute.

% Our training objective is to reconstruct the target image by combining the signal-free identity latent code from source image with learned latent identity-free refinement from target image. For testing, frames of the driving video are sequentially processed with the same source image to manipulate the source identity. 

% We provide an illustration of our method in Fig.\ref{fig:overview of OTAvatar}. Our method includes a motion-driven latent refinement model $R$, that can be integrated with pre-trained Volume rendering generative model $G$ also known as latent decoder, by adding motion-related high-dimensional latent refinement to the motion-free identity latent representation that can render the same identity with desired motion via $G$. In general, our method split the animating of talking face avatars to two steps. In the first step, $G$ inverts source image $Y_{s} \sim \mathcal{Y} \in \mathbb{R}^{3 \times H \times W}$ into its latent codes $w_{s}$ in the latent space, given the camera parameters $p_{s} \sim \mathcal{P} \in \mathbb{R}^{25}$\cite{Chan2022} which include extrinsic in $\mathbb{R}^{16}$ and intrinsic in $\mathbb{R}^9$. While iteratively optimizing $w_{s}$, our method decouples $w_{s}$ into 1) the reference components $\mathbf{w}_{r}$ that shared for all generated frames, which is the identity representation, as well as 2) the frame-specific component $\mathbf{w}_{r \rightarrow s}$ which is the representation of motion-related latent fraction. In the second step, $\mathbf{w}_{r \rightarrow s}$ is replaced with the driving alternative $\mathbf{w}_{r \rightarrow d}$ which is predicted by our refinement model $R$ given driving motion $x_d$, then $G$ takes the mixed latent code $\hat{\mathbf{w}}_{d} =  \mathbf{w}_{r} \circ \mathbf{w}_{r \rightarrow d} $ where the coupling of two latent fragment is noted as $\circ$, the output $\hat{Y}_d \sim \mathcal{Y} \in \mathbb \in \mathbb{R}^{3 \times H \times W}$ will be rendered conditioned on $p_d \sim \mathcal{P} \in \mathbb{R}^{25}$. We opt to represent the  driving motion $x_d \sim \mathcal{X} $ using the expression and pose coefficients of 3DMM following\cite{ren2021pirenderer, yin2022styleheat}. 


% \subsection{Inverting while Decoupling}
% \label{sec:inverting while decoupling}

% For any Volume rendering generative model, the rendering forward pass can be formulated as $Y = G(\mathbf{w}, p)$, unlike traditional GANs which take only latent code $w$ as input to synthesize image $Y$, Volume rendering generative networks\cite{chan2021pi, Chan2022, niemeyer2021giraffe, schwarz2020graf} also require $p$ which includes camera intrinsic and extrinsic. Optimization-based image inversion with Volume rendering generative model typically reconstruct a target image $Y$ given its camera $p$ by optimizing the latent code via

% \begin{equation}
% \begin{aligned}
%   \mathbf{w}_s &= \mathbf{w}_{r} \circ \mathbf{w}_{r \rightarrow s}  \\
%   \mathbf{w}_t &= \mathbf{w}_{r}  \circ \mathbf{w}_{r \rightarrow d} ,
%   \label{eq:latent code coupling}
% \end{aligned}
% \end{equation}
% To decouple these two latent fractions, our solution is to learn the backward decoupling via forward entangling. First, we expect the $\mathbf{w}_{r \rightarrow s}, \mathbf{w}_{r \rightarrow d}$ to be predictable by motion representation $x_s$, $x_d$. Denoting the refinement module as $R(\mathbf{w}, x)$, we expect the above latent coupling to be achieved by the following refinement

% \begin{equation}
% \begin{aligned}
%     \mathbf{w}_s &= R(\mathbf{w}_r, x_s; \theta) \\
%     \mathbf{w}_d &= R(\mathbf{w}_r, x_d; \theta),
% \end{aligned}
% \end{equation}
% where the refinement module $R$ parameterized by $\theta$ predicts motion-related latent fraction and operate the coupling with motion-free $\mathbf{w}_r$. 

% Assume we have a well-trained $R$, in the test time, similar to Eq.\ref{eq:gan inversion} we perform 


% to optimize $\mathbf{w}_r$ which can then be incorporated with any 3DMM motion representation to render the subject with desired expression and pose. During the training, we expect the identity representation can be optimized for each mini-batch of data, however the $R$ should also be trained. Therefore, we use to the following dual-objective 
% \begin{equation}
% \begin{aligned}
%   \mathbf{w}^*_r = \underset{\mathbf{w}_r}{\arg \min } (  \mathcal{L}_s + \mathcal{L}_t ),
%   \label{eq:gan inversion with refinement in training}
% \end{aligned}
% \end{equation}




% we split the overall $N$ forward passes into three stages: 1) motion-free latent fraction $w_r$ optimization, optimizing the shared $w_r$  for $N_{w}$ iterations; 2) refinement module $R$ train for $N_{R}$ iterations; 3) latent coder $G$ fine-tuning for $N_{G}$ iterations. $N = N_w + N_R + N_G$. We also give a brief pseudo code in Algo.\ref{alg:Inverting while Decoupling Speed-Up Algorithm}. 



% Despite of the notorious computational cost on optimization-based image inversion, we can train our latent refinement network $R$ in the same-time, therefore the overall training time can be free of purely reconstruction and the convergence speed can be guaranteed.


% thus the talking face animation can then be simplified into learning a refinement module to predict $\mathbf{w}_{r \rightarrow s}$ and $\mathbf{w}_{r \rightarrow t}$. 




% In this way, source image $Y_s$ and target image $Y_t$ can be inverted to $\mathbf{w}_s$ and $\mathbf{w}_t$ given their separated camera condition $p_s, p_t$.  $\mathbf{w}_s$ and $\mathbf{w}_t$ contain both motion-free $\mathbf{w}_r$ and motion-related attributes e.x. pose and expression $\mathbf{w}_{r \rightarrow t}, \mathbf{w}_{r \rightarrow s}


% Instruct by previous controllable portrait image generation method and Volume rendering based models, our model can animate 3D face avatar $\hat{I}$ by transmitting the expression and pose of a given image $I_{s}$, while other attributes like illumination and identity in 3D-consistent manner. In the following sections, we first introduce our tiny latent disentanglement block and its architecture in section 3.1; then we elaborate on our delicate training scheme and its advantage in section 3.2; next we conclude our training loss and regularization term in section 3.3.


% \subsection{Refinement Module}

% The linear combinations of orthogonal base has been proved success in not only data-based 3DMM models and non-linear learning-based models [MeshTalk, LIA]. We expect the similar idea can be applied to the latent code space, and decompose the latent code refinement  to the weight sum of several base directions. Take the refinement from identity code to the motion of source frame $\mathbf{w}_{r \rightarrow s}$ as an example, we model such refinement with
% \begin{equation}
% \mathbf{w}_{r \rightarrow d}=\sum_{i=1}^M \alpha_i(x_s) \mathbf{d}_{\mathbf{i}}.
% \label{eq: linear motion decomposition}
% \end{equation}

% The trainable vector set $\{d_1, \dots, d_M \in \mathbb{R}^N } $ represents the orthogonal basic directions which will form up the basic component for navigating the latent refinement and speed up the convergence. The scalar function set $\{ \alpha_1(x), \dots, \alpha_M(x) \in \mathbb{R}^N }$ presented the contribution of each $d_i$ to the refinement, and take the motion representation $x$ as input.




% \section{Reconstruction v.s. Manipulation}

% First manipulate then reconstruct, the manipulation speeds up reconstruction to achieve such reconstruction within 100 iterations.

% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.



\section{Experiments}

\subsection{Experiment Setting}


In this section, we first describe the implementation detail of our proposed method, the dataset used, and the baselines of our work. Then we compare our proposed method with previous 2D and 3D methods on motion controllability and multi-view consistency. 


\textbf{Implementation details}. 
We use the off-the-shelf 3D face reconstruction model to extract the expression and pose coefficients of 3DMM\cite{deng2019accurate}. The motion of any timestamp is represented by the window of adjacent 27 frames of expression and pose coefficients. In practice, we find that the extracted head poses contain too much noises to serve as the camera pose. Therefore, we  extract the camera pose using the method in~\cite{guo2021ad}. Our model is implemented in Pytorch using four A100 GPUs. The total batch size is 24, with six images per GPU. We use Adam optimizer with a 0.0001 learning rate to train the motion controller $\emph{C}$ and finetune the generator $\emph{G}$. Exponential Moving Average (EMA) is employed to update $\Theta_{c}$ in Alg.~\ref{alg:Inverting while Decoupling Speed-Up Algorithm} since it can stabilize the training. The model is trained for 2000 iterations, and each iteration contains $N = N_{id} + N_{mo} = 100$ steps to optimize the identity codes and train the motion controller. For each mini-batch of data, a new batch of latent codes are initialized and optimized using the Adam optimizer with a 0.01 learning rate. Both the 64$\times$64 resolution volume rendering results and 512$\times$512 super-resolution results are implemented to calculate the loss.  %{\color{black}introduce how to get c, p by 3DMM.}
\textcolor{black}{During training, the identity code $\mathbf{w}_{id}$ is optimized in $\mathcal{W}$ space. After being extended to the $\mathcal{W}^+$ space by channel-wisely repeated 14 times, it is combined with the motion code $\mathbf{w}_x$ and then fed into the face generator $\emph{G}$.}
It takes less than two days to train the network entirely.
\textcolor{black}{During inference, we first optimize the identity code in $\mathcal{W}$ space, then use another $N=100$ steps to finetune it in $\mathcal{W}^+$ space~\cite{abdal2019image2stylegan, abdal2020image2stylegan++}. }


\textbf{Dataset}. Our model is trained on the HDTF\cite{zhang2021flow} dataset, which contains frontal talking faces from 362 videos and over 300 subjects. Following \cite{yin2022styleheat}, we use the pre-processing step in \cite{siarohin2019first} and resize images to 512$\times$512 resolution. The test dataset contains 20 videos of subjects out of the training set. To evaluate the novel subject generalizability and controllability 
%in the multiple-view fashion
in the 3D-consistent fashion, we sample subjects from the Multiface\cite{wuu2022multiface} dataset, which is particularly proposed for 3D-consistent face avatars. 

\textbf{Baseline methods}. We mainly compare our method with previous works that incorporate 3DMM coefficients to reenact face avatars in either 2D/3D fashion and are generalizable to unseen identities. For 2D methods, PIRenderer~\cite{ren2021pirenderer} and StyleHEAT~\cite{yin2022styleheat} achieve SOTA performance on face reenactment.
%that use the pose and expression coefficients of 3DMM to drive the face avatar. 
For 3D methods, HeadNeRF~\cite{hong2022headnerf} is the most advanced volume rendering method that incorporates controllability and generalizability. \textcolor{black}{Beyond coefficients-based methods, the SOTA image-driven method FOMM~\cite{siarohin2019first} is also taken into comparison}.

% \textbf{3DMM coefficients}. We use pre-extracted 3DMM pose, and expression coefficients to represent the pose and expression variation in our animation. In a 3DMM, the 3D shape $S$ of a face can be parameterized by 

% $$\mathbf{S}=\overline{\mathbf{S}}+\boldsymbol{\alpha} \mathbf{B}_{i d}+\boldsymbol{\beta} \mathbf{B}_{e x p},$$

% where $\overline{\mathbf{S}}$ is the average face shape, $\mathbf{B}_{i d}$ and $\mathbf{B}_{e x p}$ are the bases of identity and expression computed via Principal Component Analysis (PCA) based on 200 scans of human faces [BFM].  Coefficients $\boldsymbol{\alpha} \in \mathbb{R}^{80}$ and $\boldsymbol{\beta} \in \mathbb{R}^{64}$ describe the facial shape and expression respectively. The head rotation
% and translation are expressed as $\mathbf{R} \in S O(3)$ and $\mathbf{t} \in \mathbb{R}^3$. With parameter set $\mathbf{x}_i \equiv\left\{\boldsymbol{\beta}_i, \mathbf{R}_i, \mathbf{t}_i\right}$,  the desired motions of face $i$ can be represented, with the 

% Following \cite{ren2021pirenderer, yin2022styleheat}, the motion descriptor is defined as $\mathbf{x} \equiv \mathbf{x}_{i-k: i+k} \equiv\left\{\boldsymbol{\beta}_i, \mathbf{R}_i, \mathbf{t}_i, \ldots, \boldsymbol{\beta}_{i \pm k}, \mathbf{R}_{i \pm k}, \mathbf{t}_{i \pm k}}\right.$ where $k$ is the radius of the window. This sliding window will help the model learn a low-pass representation that will erase the random jitters or noises on the coefficients. However, the Volume rendering requires more accurate and stable camera poses, therefore, we opt to replace the camera pose pre-process in \cite{Chan2022, deng2019accurate} with the alternative in \cite{athar2022rignerf} to get intrinsic and extrinsic.


\begin{figure}[ht]
  \centering
   \includegraphics[width=1.\linewidth]{HDTF.drawio_arxiv.pdf}

   \caption{\textbf{Qualitative result for cross-identity reenactment}. Examples are sampled from the HDTF dataset~\cite{zhang2021flow}. Both source and driving subjects are not included in the training set. }
    \label{fig:hdtf comparison}
\end{figure}

\begin{figure*}[ht]
  \centering
   \includegraphics[width=1\linewidth]{Multiface_v4.drawio.pdf}
   \caption{\textbf{Qualitative result for multi-view reenactment}. Examples are sampled from the multi-view dataset~\cite{wuu2022multiface}. All methods use the first frame of the frontal-view portrait to extract the identity feature, and take the expressions of sequential frames and poses of different camera views to generate the talking face. Note that this subject is not included in the training set of any methods. }
    \label{fig:multiface comparison}
    \vspace{+2mm}
\end{figure*}




\textbf{Evaluation metrics}. 
We adopt peak-signal-to-noise ratio (PSNR), structural similarity index measure (SSIM)~\cite{wang2004image}, and learned perceptual image patch similarity (LPIPS)~\cite{zhang2018unreasonable} to evaluate the visual quality. To measure the realism of the synthesized results and the identity preservation, we use frechet inception distance (FID)~\cite{heusel2017gans}  and cosine similarity of identity embedding (CSIM)~\cite{deng2019arcface} based on ArcFace model, respectively. Besides, the average expression distance (AED), average pose distance (APD), and average keypoint distance (AKD) are employed to evaluate the facial expression and pose.

% We adopt
% peak-signal-to-noise ratio (PSNR), structural similarity index measure (SSIM)~\cite{wang2004image}, learned perceptual image patch similarity (LPIPS)~\cite{zhang2018unreasonable} to measure the visual quality, use frechet inception distance (FID)~\cite{heusel2017gans} to measure the realism of the synthesized results, use the cosine similarity of identity embedding (CSIM) using ArcFace\cite{deng2019arcface} model to measure the identity preservation, use the average expression distance (AED), average pose distance (APD) and average keypoint distance (AKD) to compute the differences between generated images and target images in terms of 3DMM expression and pose. 

 %For the 1) multi-view reenactment case, we use the subjects sampled from Multiface dataset to evaluate the multi-view consistency, note that none of the data in this dataset has been incorporated in the training of both our methods and baselines. We use the first frame of the frontal camera view as the source portraits, then implement the expression coefficients of other frames and the pose of other camera views to drive the face avatar.  The Multiface dataset contains multi-view ground truth, therefore all the above criteria can be implemented. 
 %For the 2) cross-identity reenactment, we choose one challenging video in the HDTF test set with large pose variation to drive the other subjects. As we do not have the ground truth, only the CSIM between the source and animation results, FID, AED, APD, and AKD between the driving and animation results are implemented.



% \begin{table*}[]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}l|llllllll|lllll@{}}
% \toprule
%            & \multicolumn{8}{c|}{Multi-view Reenactment} & \multicolumn{5}{c}{Cross-Identity Reenactment} \\ 
%            \midrule 
%            & FID $\downarrow$  & LIPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & CSIM $\uparrow$ & AED $\downarrow$ & APD $\downarrow$ & AKD $\downarrow$ & FID $\downarrow$ & CSIM  $\uparrow$    & AED $\downarrow$     & APD   $\downarrow$ & AKD $\downarrow$  \\ 
%            \midrule
% PIRenderer &   \textbf{100.6}   &  0.299      &  20.04  &   0.586   &   0.493   &  2.203   &   0.680  & 6.566 & 103.7     &    0.632        & 3.018          &   0.501  &  4.977     \\ 
% \cmidrule(r){1-1}
% StyleHEAT   &  123.8    &  \textbf {0.284}    &   20.03   &   0.632   &  0.387    &  2.179   &  0.472  & 5.522          &  239.1      &  0.614  &  2.860          &     0.471  & 3.592   \\ 
% \cmidrule(r){1-1}
% HeadNeRF   &  212.3    &  0.367      &   17.60   &   0.546   &  0.239    &  2.086   &   0.776  & 4.166          & 233.0      &  0.282  &  2.873          &     0.567  & \textbf{3.465}   \\ 
% \cmidrule(r){1-1}
% Ours       &   137.3   &   0.288    &   \textbf{21.19}      &  \textbf{0.657} &  \textbf{0.574}    &  \textbf{1.874}   &  \textbf{0.428}   & \textbf{3.731}   &\textbf{101.8}   & \textbf{0.694} &  \textbf{2.850}   &   \textbf{0.405}  & 4.307   \\ 
% \bottomrule
% \end{tabular}%
% }
% \caption{\textbf{Quantitative comparison in multi-view and cross-identity reenactment}.}
% \label{tab:quantative comparison}
% \end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|cccccccc|ccccc@{}}
\toprule
 &
  \multicolumn{8}{c|}{Multi-View Reenactment} &
  \multicolumn{5}{c}{Cross-Identity Reenactment} \\ \cmidrule(l){2-14} 
 &
  PSNR $\uparrow$ &
  SSIM $\uparrow$ &
  CSIM $\uparrow$ &
  AED $\downarrow$ &
  APD $\downarrow$ &
  AKD $\downarrow$ &
  LIPIPS $\downarrow$ &
  FID $\downarrow$ &
  CSIM  $\uparrow$ &
  AED $\downarrow$ &
  APD   $\downarrow$ &
  AKD $\downarrow$ &
  FID $\downarrow$ \\ \midrule
FOMM &
  20.75 &
  0.639 &
  0.505 &
  2.004 &
  0.545 &
  5.052 &
  0.308 &
  101.6 &
  0.672 &
  3.196 &
  0.500 &
  4.198 &
  113.2 \\
PIRenderer &
  20.04 &
  0.586 &
  0.493 &
  2.203 &
  0.680 &
  6.566 &
  0.299 &
  \textbf{100.6} &
  0.632 &
  3.018 &
  0.498 &
  4.977 &
  103.7 \\
StyleHEAT &
  20.03 &
  0.632 &
  0.387 &
  2.179 &
  0.472 &
  5.522 &
  \textbf{0.284} &
  123.8 &
  0.614 &
  2.860 &
  0.471 &
  3.592 &
  239.1 \\
HeadNeRF &
  17.60 &
  0.546 &
  0.239 &
  2.086 &
  0.776 &
  4.166 &
  0.367 &
  212.3 &
  0.282 &
  2.873 &
  0.567 &
  \textbf{3.465} &
  233.0 \\
Ours &
  \textbf{21.19} &
  \textbf{0.657} &
  \textbf{0.574} &
  \textbf{1.874} &
  \textbf{0.428} &
  \textbf{3.731} &
  0.288 &
  137.3 &
  \textbf{0.694} &
  \textbf{2.850} &
  \textbf{0.405} &
  4.307 &
  \textbf{101.8} \\ \bottomrule
\end{tabular}%
}
\caption{\textbf{Quantitative comparisons on multi-View reenactment and cross-identity reenactment.} }
\label{tab:quantative comparison}
\end{table*}


\subsection{Evaluation Result}
We evaluate OTAvatar on animating photo-realistic talking head videos and compare them with state-of-the-art models that support identity-generalized animation methods. In the HDTF~\cite{zhang2021flow} dataset, we examine the identity-motion disentanglement by transferring the motion of one subject to drive the other subjects, namely cross-identity reenactment. The employed motion is extracted from a video with large motion variation to evaluate the result in extreme conditions.
In the Multiface~\cite{wuu2022multiface} dataset, we evaluate the consistency of the animation in different views, namely multi-view reenactment. Note that none of the data in this dataset has been used in training our method and baselines. For each talking corpus, we choose the first frame of the frontal-view camera recording as the reference and enforce the network to animate the following frames in frontal view and other views. 
%We alternate the input of 3DMM pose coefficients to the desired one and measure the distance between the generated result and the ground truth.

%compare the generation with the ground truth. 

% \begin{table*}[]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{@{}l|llllllll|lllll@{}}
% \toprule
%            & \multicolumn{8}{c|}{Multi-view Reenactment} & \multicolumn{5}{c}{Cross-Identity Reenactment} \\ 
%            \midrule 
%            & FID $\downarrow$  & LIPIPS $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & CSIM $\uparrow$ & AED $\downarrow$ & APD $\downarrow$ & AKD $\downarrow$ & FID $\downarrow$ & CSIM  $\uparrow$    & AED $\downarrow$     & APD   $\downarrow$ & AKD $\downarrow$  \\ 
%            \midrule
% PIRenderer &   \textbf{100.6}   &  0.299      &  20.04  &   0.586   &   0.493   &  2.203   &   0.680  & 6.566 & 103.7     &    0.632        & 3.018          &   0.501  &  4.977     \\ 
% \cmidrule(r){1-1}
% StyleHEAT   &  123.8    &  \textbf {0.284}    &   20.03   &   0.632   &  0.387    &  2.179   &  0.472  & 5.522          &  239.1      &  0.614  &  2.860          &     0.471  & 3.592   \\ 
% \cmidrule(r){1-1}
% HeadNeRF   &  212.3    &  0.367      &   17.60   &   0.546   &  0.239    &  2.086   &   0.776  & 4.166          & 233.0      &  0.282  &  2.873          &     0.567  & \textbf{3.465}   \\ 
% \cmidrule(r){1-1}
% Ours       &   137.3   &   0.288    &   \textbf{21.19}      &  \textbf{0.657} &  \textbf{0.574}    &  \textbf{1.874}   &  \textbf{0.428}   & \textbf{3.731}   &\textbf{101.8}   & \textbf{0.694} &  \textbf{2.850}   &   \textbf{0.405}  & 4.307   \\ 
% \bottomrule
% \end{tabular}%
% }
% \caption{The results for different methods in multi-view and cross-identity reenactment.}
% \label{tab:quantative comparison}
% \end{table*}

\textbf{Qualitative comparison}. Fig.~\ref{fig:hdtf comparison} shows the results of different methods on cross-identity generation. Compared to FOMM~\cite{siarohin2019first}, PIRenderer~\cite{ren2021pirenderer} and StyleHEAT~\cite{yin2022styleheat}, which use warping fields for reenacting faces, our method can handle extreme pose variation and maintain identity consistency. Compared to the 3D method of HeadNeRF, our model fully reconstructs the identity-specific detail and synthesizes more natural expression. The multi-view consistency results are shown in Fig.~\ref{fig:multiface comparison}. First, the 2D warping method suffers facial malformation, which becomes more serious in larger poses. Second, though HeadNeRF renders accurate head poses, the generated results have noticeable deterioration compared to the ground truth. Finally, the face avatar constructed by our method preserves identity details and multi-view consistency. 
%Please see the demo video in the supplementary materials for more details. 

\textbf{Quantitative comparison}. The quantitative comparison among competing methods is shown in Table~\ref{tab:quantative comparison}. We can see that the proposed method achieves the best performance in terms of most of the criteria. 
%Besides, the better AKD of HeadNeRF than other methods validates the better view consistency of 3D methods.

\textbf{Inference speed}. Table~\ref{tab:inference speed} lists the inference speed comparison among state-of-the-art 3D face avatar methods. By utilizing the pre-trained 3D face generator and the compact motion controller (0.8M parameters), our method achieves the highest inference speed, demonstrating its effectiveness.

\begin{table}[ht]
% \begin{center}
\centering
\setlength{\tabcolsep}{0.7cm}
% \begin{tabular}{@{}cc@{}}
\begin{tabular}{cc}
\toprule
Methods  & Frames Per Second $\uparrow$ \\ \midrule
IMAvatar\cite{zheng2022imavatar} & 0.03                   \\
ADNeRF\cite{guo2021ad} & 0.13                     \\
HeadNeRF\cite{hong2022headnerf} & 25                     \\
Ours     & \textbf{35}                     \\ \bottomrule
\end{tabular}%
\caption{\textbf{Quantitative comparison on the inference speed}. The comparison is conducted on an A100 GPU. }
\label{tab:inference speed}
% \end{center}
\end{table}

\begin{figure}[ht]
  \centering
   \includegraphics[width=.7\linewidth, ]{Joint_Training_v5.pdf}
    \caption{\textbf{Qualitative comparison of decoupling-by-inverting training and joint training}. Joint training cannot preserve identity information in one-shot avatar construction.}
   %\caption{\textbf{Qualitative comparison on the necessity of joint training}. We compare the model trained using our proposed separate updating strategy to the one that implements the joint training strategy. If we jointly train controller $\emph{C}$ and optimize identity code $\mathbf{w}_{id}$, the animation result tends to be inconsistent.}
    \label{fig:joint training comparison}
\end{figure}

\subsection{Ablation Study}

\textbf{Decoupling-by-inverting}. The controller is trained by the decoupling-by-inverting strategy described in Sec.~\ref{sec:alternative manner}, where the training alternates between $N_{id}$ steps of identity code optimization and $N_{mo}$ steps of controller parameters training. To validate the effectiveness of the alternative manner, we discard it and jointly train the controller parameters and the identity code. The qualitative comparison is in Fig.~\ref{fig:joint training comparison}. We see that the jointly trained model cannot preserve identity during the animation because the controller manages to overfit the identity information in the training set, and the identity information is not fully encoded in the identity code. The performance degradation caused by joint training is also shown in Table ~\ref{tab:loss ablation}. We also analyze the setting of identity optimization step $N_{id}$ and controller training step  $N_{mo}$, and list the results in Table~\ref{tab:updating strategy}. We observe that $N_{id} = 90, N_{mo} = 10$ achieves a balance of expression reconstruction (AED) and identity preserving (CSIM).


% As shown in  Fig.\ref{fig:joint training comparison}, if the controller $\mathcal{C}$ is jointly trained while optimizing $\mathbf{w}_{id}$, then the reconstruction on the given identity tends to be inconsistent. We argue that is because the $\mathcal{C}$ managed overfit the identity information in the training set. In contrast, the proposed separate updating strategy can render consistent avatars. As for how the percentile of training controller $\mathcal{C}$ or optimizing identity code $\mathbf{w}_{id}$ would affect the performance, we list the comparison in Tab.~\ref{tab:updating strategy}. We observe that $N_{id} = 90, N_{mo} = 10$ achieves the best result in terms of expression, which is evaluated by AED. In terms of identity reconstruction consistency. It means during the $N = 100$ forward passes conducted on each data mini-batch, the first $90$ should spend on optimizing the $\mathbf{w}_{id}$ while the rest $10$ forward passes are used for training the controller module $\mathcal{C}$.




% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% % \usepackage{graphicx}
% \begin{table}[]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{cccc}
% \hline
%                &                & \multirow{2}{*}{AED} & \multirow{2}{*}{CSIM} \\ \cline{1-2}
% N\_\{mo}      & N\_\{id}      &                      &                       \\ \hline
% 1              & 99             &                      &                       \\
% 5              & 95             &                      &                       \\
% 10             & 90             & 2.850                & 0.694                 \\
% 20             & 80             & 3.231                & 0.592                 \\
% 50             & 50             & 3.004                & 0.703                 \\
% \multicolumn{2}{c}{Joint}       & 2.905                & 0.390                 \\ \hline
% \multicolumn{2}{c}{w/o lmk}     & 3.101                & 0.687                 \\
% \multicolumn{2}{c}{w/o id}      & 3.164                & 0.592                 \\
% \multicolumn{2}{c}{w/o finetue} & 3.145                & 0.662                 \\ \hline
% \end{tabular}%
% }
% \caption{The ablation study on \textbackslash{}mathbf}
% \label{tab:ablation-on-nw}
% \end{table}

\textbf{Losses}. We also perform an ablation study on the loss functions, including the landmark region losses $\mathcal{L}_{m}$ and ID losses $\mathcal{L}_{id}$, as shown in Table~\ref{tab:loss ablation}. We can see that the absence of landmark region losses $\mathcal{L}_m$  causes a deterioration of expression reconstruction, measured by AED, while the absence of ID loss causes a performance drop in both expression and identity consistency.


%We conduct experiments on the necessity of the introduced landmark region losses $\mathcal{L}_m$ and ID losses $\mathcal{L}_{id}$. As shown in Tab.~\ref{tab:loss ablation}, compared to the experiment $N_{id} = 90, N_{mo} = 10$ , the absence of landmark region losses $\mathcal{L}_m$  would cause the average expression distance to decrease, showing its importance for expression control. Besides, the absence of ID loss would cause a performance decrease in both expression and identity consistency. 


\begin{table}[ht]
\centering
    \setlength{\tabcolsep}{0.5cm}
    % \begin{center}        
        \begin{tabular}{cccc}
            \toprule
            % \hline
            $\text{N}_\text{mo}$ & $\text{N}_\text{id}$ & AED $\downarrow$   & CSIM $\uparrow$  \\ 
            \midrule
            % \hline
            1   & 99 & 3.285 & \textbf{0.7200} \\
            5   & 95  & 3.178 & 0.716 \\
            10  & 90  & \textbf{2.850} & 0.694 \\
            20  & 80  & 3.231 & 0.592 \\
            \bottomrule
            % \hline
        \end{tabular}         
        % \end{center}
        \caption{\textbf{Ablation study on the decoupling-by-inverting hyperparameters.} Experiments are conducted under the different settings of $N_{mo}$ and $N_{id}$ with $N = N_{mo} + N_{id} = 100 $. }
        \label{tab:updating strategy}
\end{table}

\begin{table}[ht]
\centering
    \setlength{\tabcolsep}{0.5cm}
    % \begin{center}        
        \begin{tabular}{ccc}
        \toprule
        % \hline
              &        AED $\downarrow$   & CSIM $\uparrow$  \\ 
        % \hline
        \midrule
        ours   &     \textbf{2.850} & \textbf{0.694} \\
        joint &     3.009 & 0.689 \\
        w/o $\mathcal{L}_\text{m}$      & 3.101 & 0.687 \\
        w/o $\mathcal{L}_\text{id}$      & 3.164 & 0.592 \\
        w/o finetune & 3.145 & 0.687 \\ 
        \bottomrule
        % \hline
        \end{tabular}        
        % \end{center}
        \caption{\textbf{Ablation study on the \textcolor{black}{joint training}, loss terms, and finetuning}. 
        %\textcolor{black}{Loss terms include eyes and mouth region loss $\mathcal{L}_\text{m}$, and the ID Loss  $\mathcal{L}_\text{id}$
        % \textcolor{black}{Experiments are conducted when the identity code optimization and motion controller training are jointly conducted  i.e. ''joint'', remove the eyes and mouth region consistency i.e. ''w/o $\mathcal{L}_\text{m}$'', remove the identity consistency i.e. ''w/o $\mathcal{L}_\text{id}$'', and do not finetune 3D generator i.e. ''w/o finetune''. } 
        }
        \label{tab:loss ablation}
\end{table}


\textbf{3D generator finetuning}. When the controller training and identity code optimization are finished, we further slightly finetune the pre-trained EG3D~\cite{Chan2022} face generator with a learning rate of 0.0001. In Table~\ref{tab:loss ablation}, we show the results without finetuning. One can see that slightly finetuning the face generator in the final stage improves identity and expression reconstruction performance.

%We conduct this experiment to examine the necessity of finetuning pre-trained 3D GAN via the Line.~\ref{alg:fine-tune G} in Alg.~\ref{alg:Inverting while Decoupling Speed-Up Algorithm}. Via slight finetuning with a learning rate of 0.0001 for 2000 iterations, the pre-trained GAN performed better in acting different motions and constructed identity attributes.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
% \begin{table}[]
% \centering
% \resizebox{.5\columnwidth}{!}{
%     \begin{tabular}{@{}llll@{}}
%         \toprule
%         $\text{N}_\text{mo}$ & $\text{N}_\text{id}$ & AED   & CSIM  \\ 
%         \midrule
%         1   & 99  & 3.178 & \textbf{0.716} \\
%         10  & 90  & \textbf{2.850} & 0.694 \\
%         20  & 80  & 3.231 & 0.592 \\
%         50  & 50  & 3.004 & 0.703 \\ \bottomrule
% \label{tab:updating strategy}
% \end{tabular}%
% }
% \end{table}



% \begin{table}[t]
% \setlength{\tabcolsep}{0.5cm}
% \centering
%         \begin{tabular}{cccc}
%             % \toprule
%             \hline
%             $\text{N}_\text{mo}$ & $\text{N}_\text{id}$ & AED   & CSIM  \\ 
%             % \midrule
%             \hline
%             1   & 99  & 3.178 & \textbf{0.716} \\
%             10  & 90  & \textbf{2.850} & 0.694 \\
%             20  & 80  & 3.231 & 0.592 \\
%             50  & 50  & 3.004 & 0.703 \\ 
%             % \bottomrule
%             \hline
%         \end{tabular}
%         \label{tab:updating strategy}
%         \caption{}
% \end{table}

% \begin{ta}
% % \begin{minipage}[l]{.05\linewidth}
% %     \centering
% %     \quad
% % \end{minipage}%
% \begin{minipage}[c]{.48\linewidth}
%     \flushright %\centering
%     \resizebox{0.8\columnwidth}{!}{
%         \begin{tabular}{@{}lll@{}}
%         \toprule
%               &        AED   & CSIM  \\ 
%         \midrule
%         Joint training   &     2.905 & 0.390 \\
%         w/o $\mathcal{L}_\text{m}$      & 3.101 & 0.687 \\
%         w/o $\mathcal{L}_\text{id}$      & 3.164 & 0.592 \\
%         w/o finetune & 3.145 & 0.882 \\ \bottomrule
%         \label{tab:loss ablation}
%         \end{tabular}%

%     }
% \end{minipage}%

% \begin{minipage}[r]{.44\linewidth}
%     \centering 
%     (a)
% \end{minipage}
% \begin{minipage}[l]{.05\linewidth}
%     \centering
%     \quad
% \end{minipage}%
% \begin{minipage}[r]{.48\linewidth}
%     \centering 
%     (b)
% \end{minipage}

% \caption{\textbf{Ablation study on separate updating strategy and training losses}}
% \end{table}



% \begin{minipage}{\textwidth}
% \begin{table}[H]
% \begin{minipage}{0.48\linewidth}
% \centering
% \begin{tabular}{@{}llll@{}}
% \toprule
% Nmo & Nid & AED   & CSIM  \\ \midrule
% 1   & 99  & 3.178 & \textbf{0.716} \\
% 10  & 90  & \textbf{2.850} & 0.694 \\
% 20  & 80  & 3.231 & 0.592 \\
% 50  & 50  & 3.004 & 0.703 \\ \bottomrule
% \end{tabular}%
% \caption{ Hyper parameter}
% \end{minipage}
% \begin{minipage}{0.48\linewidth}  
% \centering
% \begin{tabular}{@{}lll@{}}
%              & AED   & CSIM  \\
% Joint        & 2.905 & 0.390 \\
% w/o lmk      & 3.101 & 0.687 \\
% w/o id       & 3.164 & 0.592 \\
% w/o finetune & 3.145 & 0.882
% \caption{ Joint training and losses }
% \end{tabular}%
% \end{minipage}
% \end{table}
% \end{minipage}


\section{Conclusion}
We proposed a novel framework of one-shot 3D-consistent talking face avatar, namely OTAvatar, using volume rendering. It jointly addressed the three challenges of face avatars, i.e., generalizability, controllability, and efficiency. For identity-generalized rendering, we implemented a pre-trained 3D face generator to reconstruct faces faithfully, given a single portrait reference. For motion control, we proposed a motion controller module, which predicts the motion code conditioned on 3DMM coefficients. For efficiency, benefiting from the compact architecture of both the 3D generator and controller, our model can animate avatars at a high speed. Besides, we proposed a decoupling-by-inverting approach, which is both a training scheme and a test-time disentangle strategy that decouples the latent code into identity and motion codes via GAN inversion, so that one can animate the avatar with different motions using the motion codes predicted by the controller. 
%We conduct comprehensive experiments to evaluate our framework against current methods.
% We conduct comprehensive experiments to evaluate the capability of our proposed framework and its advantage against current methods.
Comprehensive experiments were conducted to prove the advantage of our proposed framework.

% including the comparison with current methods and ablation studies. 

\section*{Acknowledgement}
This work is supported in part by the Chinese National Natural Science Foundation Projects $\#62176256$, $\#62276254$, and the InnoHK program.

%We refer to the pre-trained efficient Volume rendering network to promise efficiency and generalizability and propose a compact controller to control the face avatar's motion in the generator's latent space. We also design an efficient training strategy that incorporates optimization-based inversion. Experiments demonstrate that our method can animate natural head motion while maintaining 3D consistency and is generalizable to animate novel identities in different views. 


% Our method incorporates reconstructing identities via iterative optimization in the training scheme, therefore, the inference speed of the pre-trained Volume rendering model is of great importance. We choose the efficient EG3D as the pre-training network. It proposed the triple-plane architecture that represents a scene more efficiently and explicitly. Note that the proposed training scheme does not require any network-specific loss term, and is supposed to be extended to any other Volume rendering generative models if necessary.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

% ############################## supplemetory material #######################
\clearpage

\section{Supplementary Material}

    % The following materials are provided in this section:
    % \begin{itemize}
    %     \item The pseudo-code of Inverting-by-Decoupling training scheme.
    %     \item Identity code interpolation.
    %     % \vspace{-0.2cm}
    %     \item More qualitative comparison of multi-view consistency (\cf Section 4.2 in the main paper).
    %     % \vspace{-0.2cm}
    %     \item The network architecture of the controller (\cf Section 2.1 in the main paper).
    %     % \vspace{-0.2cm}
    %     \item Analysis of the controller (\cf Section 4.2 in the main paper).
    %     % \vspace{-0.2cm}
    %     \item Additional implementation detail (\cf Section 4.1 in the main paper).
    %     % \vspace{-0.2cm}
    
    % \end{itemize}


  
\subsection{Pesudo Code}

The pseudo-code for our suggested inverting-by-decoupling training scheme is appended in this section.

\begin{algorithm}[ht]
    % \DontPrintSemicolon
    % \SetAlgoLined
    % \SetNoFillComment

    \caption{Training Scheme of Decoupling-by-Inverting}
    \label{alg:Inverting while Decoupling Speed-Up Algorithm}

    % \SetKwInOut{KwIn}{Input}
    % \SetKwInOut{KwOut}{Output}

    \KwIn{Talking Face Dataset $\mathcal{D}$, \\
    3D face animator $\emph{G}(\cdot, \cdot ; \Theta) = \emph{G}_{eg}(\cdot + \emph{C}(\cdot; \Theta_{c});\Theta_{eg})$ \\
    
    where $\Theta = \Theta_{eg} \cup \Theta_{c}$ \\ 

    % \KwOut{\Theta_{eg}, \Theta_{ca}}

    }
    % \KwOut{ 3D face Generator $\emph{G}_{eg}(\cdot)$ with parameter $\theta_{eg}$, \\
    % \quad \quad \quad calibration module $\emph{Ca}(\cdot, \cdot)$ with parameter $\theta_{}$
    % }
        
    $\Theta_{c} \gets$ random initialization \\  
    \For{$i \gets 1$ \KwTo $T$}{        
        \tcc{collect source and target data point} 
        $\mathcal{V} \gets$ random video clip sampled from $\mathcal{D}$ \\
        $\mathbf{I}_s , \mathbf{x}_s, \mathbf{p}_s \gets$ data of random frame sampled from $\mathcal{V}$ \\
        $\mathbf{I}_d , \mathbf{x}_d, \mathbf{p}_d \gets$ data of random frame sampled from $\mathcal{V}$

        $\mathbf{w}_{id} \gets \mathbf{w}_{avg}$ \tcp*[f]{initialize $\mathbf{w}_{id}$ using average} \;
        
        $\theta_{c} \gets \Theta_{c}$  \tcp*[f]{initialize $\theta_{c}$ using EMA weights} \; 
        
        $\theta \gets \Theta_{eg} \cup \theta_{c}$ \tcp*[f]{assemble \emph{G} with trainable $\theta_{c}$} \;
        
        \For{$n \gets 1$ \KwTo $N_{id} + N_{mo}$}{
            
            % \tcp*[l]{initialized using average}\;
            \tcc{calculate optimization objectives}
            % $\mathbf{w}_s \gets \emph{F}_{ca}(\mathbf{w}_{id}, \mathbf{x}_s; \theta) $ and \\
            % $\mathbf{w}_d \gets \emph{F}_{ca}(\mathbf{w}_{id}, \mathbf{x}_d; \theta) $ \; \label{alg:loss_propogation_1}
            
            % $\mathcal{L}_s \gets \mathcal{L}(\mathbf{I}_s, \mathcal{R}(\emph{G}_{eg} (\mathbf{w}_s; \Theta_{eg}), p_s))$  and \\
            % $\mathcal{L}_d \gets \mathcal{L}(\mathbf{I}_d, \mathcal{R}(\emph{G}_{eg} (\mathbf{w}_d; \Theta_{eg}), p_d))$ \; \label{alg:loss_propogation_2}

            
            $\mathcal{L}_s \gets \mathcal{L}(\mathbf{I}_s, \mathcal{R}(\emph{G}(\mathbf{w}_{id}, \mathbf{x}_s ; \theta), \mathbf{p}_s))$ \label{alg:loss_propogation_s}

            $\mathcal{L}_d \gets \mathcal{L}(\mathbf{I}_d, \mathcal{R}(\emph{G}(\mathbf{w}_{id}, \mathbf{x}_d ; \theta), \mathbf{p}_d))$ \label{alg:loss_propogation_d}
            
            \eIf{$n < N_{id}$}{
                \tcc{optimize identity code}
                Update $\mathbf{w}_{id}$ using $\nabla_{\mathbf{w}_{id}} (\mathcal{L}_s + \mathcal{L}_d)$
            }{
                \tcc{train motion calibration}
                Update $\theta_{c}$ using $\nabla_{\theta_{c}} (\mathcal{L}_s + \mathcal{L}_d)$
            }
        }
        Calculate $\mathcal{L}_s$, $\mathcal{L}_t$ using line.\ref{alg:loss_propogation_s}, line.\ref{alg:loss_propogation_d}
        
        Finetune $\Theta_{eg}$ on $\mathcal{L}_s + \mathcal{L}_t$ \label{alg:fine-tune G}
        
        $\Theta_{ca}  \gets \beta \Theta_{c} + (1 - \beta) \theta_{c}  $  \tcp*[f]{Update EMA weights} \; 
    }
 
\end{algorithm}


    
\subsection{Identity code interpolation}



In this paper, we use a motion controller $\emph{C}$ and decoupling-by-inverting strategy to disentangle the latent code of face generator to identity code $\mathbf{w}_{id}$ and motion code $\mathbf{w}_{x}$. In this section, we examine the disentanglement by performing the task of identity interpolation. The interpolated identity is generated by:

 \begin{figure}[t]
  \centering
   \includegraphics[width=1.1\linewidth, ]{Controller.pdf}
    \caption{\textbf{Controller architecture}. We show the details of the architecture of \emph{C} in (a) and the sub-module $\emph{F}_T$ in (b). The controller takes as input 3DMM coefficients and output motion code in $\mathcal{W}^+$ space\cite{xia2022gan}.}
   %\caption{\textbf{Qualitative comparison on the necessity of joint training}. We compare the model trained using our proposed separate updating strategy to the one that implements the joint training strategy. If we jointly train controller $\emph{C}$ and optimize identity code $\mathbf{w}_{id}$, the animation result tends to be inconsistent.}
    \label{fig:controller}
\end{figure}

\begin{equation}
\label{equ-identity interpolation}
\mathbf{w}^{'}_{id} = \alpha \mathbf{w}_{id\_a} + (1 - \alpha) \mathbf{w}_{id\_b}
\end{equation}
where $\mathbf{w}_{id\_a}$ and $\mathbf{w}_{id\_b}$ are the identity codes estimated from two source reference images using decoupling-by-inverting strategy. The interpolated latent code $\mathbf{w}^{'}_{id}$ is then combined with any motion and generates animations. The animation result in shown in Fig.~\ref{fig:identity interpolation}. It shows that our model can animate consistent motion with smooth-varying identity attributes. And it validates that our method achieves the disentanglement of motion and identity in the latent space of the pre-trained face generator. The animation result is also appended in the supplementary video.

\begin{table}[ht]
	% \setlength{\tabcolsep}{0.1cm}
	\centering
	\begin{tabular}{cccccc}
	\hline
& CSIM  & AED   & APD   & AKD   & FID   \\
% \midrule
\hline
$\mathbf{w}_x \in \mathcal{W}$ & \textbf{0.719} & 3.352 & 0.453 & 4.783 & 104.6 \\
w/o code book                                   & 0.662 & 3.342 & 0.457 & 4.974 & 103.2 \\
Ours                                            & 0.694 & \textbf{2.850} & \textbf{0.405} & \textbf{4.307} & \textbf{101.8} \\ 
    \hline
    \end{tabular}
\caption{\textbf{Ablation study on the controller architecture}. Experiments are conducted on the cross-identity reenactment.}
\label{tab:ablation_on_controller}
\end{table}

 \begin{figure*}[t]
  \centering
   \includegraphics[width=0.8\textwidth, ]{Latent_Interpolation.pdf}
    \caption{\textbf{The interpolation results of identity code}. Our model can generate smooth-varying identity attribute beyond motion control.}
   %\caption{\textbf{Qualitative comparison on the necessity of joint training}. We compare the model trained using our proposed separate updating strategy to the one that implements the joint training strategy. If we jointly train controller $\emph{C}$ and optimize identity code $\mathbf{w}_{id}$, the animation result tends to be inconsistent.}
    \label{fig:identity interpolation}
\end{figure*}

\subsection{More Qualitative Comparison}
    
\textbf{Multi-view dataset}. We show more qualitative comparison result on the multi-view stereo dataset Multiface~\cite{wuu2022multiface} in Fig.~\ref{fig:multiface_woman}. To further compare the robustness against the pose variation, we choose an overhead view as the single reference. 

\textbf{Monocular dataset}. To evaluate the 3D consistency in the monocular talking dataset, we change the pose coefficients to be rotating while keeping the expression coefficients in sync with the driving frames, as shown in Fig.~\ref{fig:hdtf_multiview}. 

 From both comparisons, we observe PIRenderer~\cite{ren2021pirenderer} and StyleHEAT~\cite{yin2022styleheat} suffer from drastic unnatural image distortion, while our methods can maintain the multi-view consistency and depict natural motion on the expression; compared to the 3D method of HeadNeRF~\cite{hong2022headnerf}, we achieve a more faithful reconstruction of the subject on the skin color and torso. For more detail, please refer to the supplementary video.

%We conduct all experiments with the PyTorch.
\subsection{The Network Architecture of the Controller}

We proceed to describe the controller architecture in this section. Fig.~\ref{fig:controller} shows the details of the motion controller $\emph{C}$. We input the window of adjacent 27 frames of expression and pose coefficients, they form up the motion signal of size 73$\times$27. We use three 1D convolution layers, noted as $\emph{F}_T$ to compress the noises in the sequential 3DMM coefficients. After that, a five-layer MLP is implemented to transform the feature into the magnitudes of 20 orthogonal bases of size 512 in the code book. We calculate 20$\times$14 of such magnitudes, by first outputting 20$\times$15 scalars and then adding 20 of them to the others. Then the 20$\times$14 scalars are multiplied with the orthogonal bases in the code book $\emph{D}$ and transformed to the motion code of size 14$\times$512 . Here 14 is the maximum number of latent codes $\mathbf{w} \in \mathcal{W}$ as input to the face generator $G$~\cite{Chan2022}. The 14$\times$512 latent features form up the $\mathcal{W}^+$ space~\cite{xia2022gan}. In our implementation, it is formulated as the summation of the identity code $\mathbf{w}_{id}$ and the motion code $\mathbf{w}_x$.  


        
 \begin{figure*}[t]
  \centering
   \includegraphics[width=0.9\linewidth, ]{Multiface_woman.drawio_v2.pdf}
    \caption{\textbf{3D Consistency on multi-view dataset}. We demonstrate additional visualization comparison on the Multiface dataset~\cite{wuu2022multiface}, with more drastic camera view variation. All methods use the source frame to extract the identity feature, then extract 3DMM coefficients of pose and expression from the driving frame to generate the talking face. This subject is not included in the training set of any methods.}
   %\caption{\textbf{Qualitative comparison on the necessity of joint training}. We compare the model trained using our proposed separate updating strategy to the one that implements the joint training strategy. If we jointly train controller $\emph{C}$ and optimize identity code $\mathbf{w}_{id}$, the animation result tends to be inconsistent.}
    \label{fig:multiface_woman}
\end{figure*}

 \begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth, ]{HDTF_multiview.drawio.pdf}
    \caption{\textbf{3D Consistency on monocular dataset}. We demonstrate additional visualization comparison on the HDTF dataset~\cite{zhang2021flow}, with more drastic camera view variation. All methods use the source frame to extract the identity feature, then use the coefficients of pose and expression as visualized by the face meshes to generate talking faces. This subject is not included in the training set of any methods.}
   %\caption{\textbf{Qualitative comparison on the necessity of joint training}. We compare the model trained using our proposed separate updating strategy to the one that implements the joint training strategy. If we jointly train controller $\emph{C}$ and optimize identity code $\mathbf{w}_{id}$, the animation result tends to be inconsistent.}
    \label{fig:hdtf_multiview}
\end{figure*}

    we conduct experiments to evaluate the effectiveness of the proposed controller architecture. One ablation model is constructed by reducing the number of output scalars to 1$\times$20. It is multiplied with the code book $\emph{D}$ to make up the single motion code of size 1$\times$512, it is further repeated 14 times to fit the required shape of 14$\times$512. This model is written as $\mathbf{w}_x \in \mathcal{W}$ in Table.~\ref{tab:ablation_on_controller}. In another ablation model, we replace the code book with vanilla 20$\times$512 linear weights which can also transform every 20 scalars to 512-dimensional latent code. From Table.~\ref{tab:ablation_on_controller}, we observe the performance deterioration on both ablation models on the motion controllability, which is evaluated by AED, APD, and AKD, and also on the image quality as quantified by FID. Even though representing motion code in $\mathcal{W}$ facilitates higher identity consistency as measured by CSIM, it neglects the fact that each of the 14 latent codes contributes differently to the motion deformation, as is testified in 2D GANs~\cite{chong2021stylegan}, therefore the motion controllability is reduced.

% \subsection{Additional Implementation Detail}
%     During training, the identity code $\mathbf{w}_{id}$ is optimized in $\mathcal{W}$ space, it is repeated 14 times to add with the motion code $\mathbf{w}_x$ then input to the face generator $\emph{G}$. During inference, we first optimize the identity code in $\mathcal{W}$ space, then use another $N=100$ steps to further optimize it in $\mathcal{W}^+$ space~\cite{abdal2019image2stylegan, abdal2020image2stylegan++}.  


\subsection{User Study}

\textcolor{black}{To assess the quality of the animation, we conduct a user study. We sent the results animated by our method and baselines to 20 people (the students in the university). Users are asked to evaluate the animation based on 1) motion that is in sync with driving videos and, and 2) identity similarity compared with the source portrait. Their ratings are averaged, scaled to a maximum of 10, and shown in Table. \ref{tab:user study}. Our method is the most desired in motion controllability and identity consistency.}

\begin{table}
\centering
    \setlength{\tabcolsep}{0.5cm}
    % \begin{center}        
        \begin{tabular}{ccc}
        \toprule
        % \hline
              &        Motion $\uparrow$   & Identity $\uparrow$  \\ 
        % \hline
        \midrule
        FOMM\cite{siarohin2019first}   &     4.1 & 5.3 \\
        PIRender\cite{ren2021pirenderer} &     3.5 & 4.3 \\
        StyleHEAT\cite{yin2022styleheat}      & 7.4 & 5.7 \\
        HeadNeRF\cite{hong2022headnerf}      & 4.6 & 3.8 \\
        Ours & \textbf{9.0} & \textbf{8.5} \\ 
        \bottomrule
        % \hline
        \end{tabular}        
        % \end{center}
        \caption{\textbf{User study on the animation quaility}. 
        %\textcolor{black}{Loss terms include eyes and mouth region loss $\mathcal{L}_\text{m}$, and the ID Loss  $\mathcal{L}_\text{id}$
        % \textcolor{black}{Experiments are conducted when the identity code optimization and motion controller training are jointly conducted  i.e. ''joint'', remove the eyes and mouth region consistency i.e. ''w/o $\mathcal{L}_\text{m}$'', remove the identity consistency i.e. ''w/o $\mathcal{L}_\text{id}$'', and do not finetune 3D generator i.e. ''w/o finetune''. } 
        }
        \label{tab:user study}
\end{table}




\end{document}








% Their evaluations are averaged, scaled from 1 to 10, and represented in the figure. As shown in the figure \ref{fig:user study}. }

%  \begin{figure}[t]
%   \centering
%    \includegraphics[width=1\linewidth, ]{CVPR_2023_Towards Generalized Talking Face Avatar via Volume Rendering/User study on the animation quaility - Made with DesignCap.jpg}
%     \caption{\textbf{User study on the animation quality}.}
%    %\caption{\textbf{Qualitative comparison on the necessity of joint training}. We compare the model trained using our proposed separate updating strategy to the one that implements the joint training strategy. If we jointly train controller $\emph{C}$ and optimize identity code $\mathbf{w}_{id}$, the animation result tends to be inconsistent.}
%     \label{fig:user study}
% \end{figure}

% We take a fast user study in the rebuttal period. We sent the results to 20 persons (the students in the university) for their most preferred animation results among ours and baselines, and 16 persons considered the results of the proposed method to be the best. This user study will be extended and added in the revised version.


