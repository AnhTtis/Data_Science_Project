\documentclass[a4paper,11pt, reqno]{amsart}
\usepackage{amsmath}

%\usepackage[small]{titlesec}
%\titleformat*{\section}{\bfseries}
%\titleformat{\subsection}[runin]{\bfseries\itshape}{\thesubsection.}{3pt}{\space}[.]
%\titleformat{\subsubsection}[runin]{\itshape}{\thesubsection.}{3pt}{\space}[.]

%\renewcommand{\figurename}{\texsc{Fig.}}
\usepackage[labelfont=sc]{caption}
\usepackage{cancel}
\usepackage{ stmaryrd }

%\usepackage{libertine}
\usepackage[long, nolistings, short]{optional} %
%\usepackage[short]{optional} %
%\usepackage{comment}
%\excludecomment{snippet}

\usepackage{ifthen}
\usepackage{subcaption} % provides subfigure

\usepackage[english]{babel}
\usepackage{color}
\usepackage{amsmath, latexsym, amsthm, amsfonts,bm,amssymb,mathscinet} 
\usepackage{natbib}
\setlength{\bibsep}{0pt plus 0.3ex}
\usepackage[a4paper, text={5.8in,8.5in},centering]{geometry}

%\usepackage{epsfig}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{wrapfig}

\usepackage[utf8]{inputenc}
\usepackage[dvipsnames]{xcolor}
%\usepackage[colorlinks,linkcolor=Maroon, citecolor=Maroon,urlcolor=blue]{hyperref}
\usepackage[colorlinks,linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}


\usepackage[normalem]{ulem}
\usepackage{enumitem}
\usepackage{bbm}

\usepackage{booktabs} \newcommand{\tablestretch}[1]{\renewcommand{\arraystretch}{#1}}
%\usepackage{media9}
%\usepackage[buttonsize=0.7em]{animate}
%\usepackage{cancel}

\newcommand{\qtext}[1]{\quad\text{#1}\quad}

%\newcommand{\forw}[1]{\langle #1 \vert}
%\newcommand{\forwcirc}[1]{\langle \circ #1 \vert}
\newcommand{\forw}[1]{\cF_{ #1}}
\newcommand{\forwcirc}[1]{\cF^\circ_{ #1} }
\newcommand{\forwdiamond}[1]{\cF^\diamond_{ #1} }

\newcommand{\backw}[1]{\cB_{#1}}
%\newcommand{\backw}{\cB}

\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}

%% add notes via
\newcommand{\note}[1]{{\color{Red} #1}}
\newcommand{\noteout}[1]{{\color{magenta} #1}}


\newcommand{\veryshortarrow}[1][3pt]{\mathrel{%
   \hbox{\rule[\dimexpr\fontdimen22\textfont2-.2pt\relax]{#1}{.4pt}}%
   \mkern-4mu\hbox{\usefont{U}{lasy}{m}{n}\symbol{41}}}}


\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother
\newcommand{\pf}{\veryshortarrow}



\usepackage{tikz-cd}
\usetikzlibrary{circuits.logic.US}
\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\usetikzlibrary{cd}
\usetikzlibrary{arrows}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{shapes.geometric}
\tikzset{ed/.style={auto,inner sep=2pt,font=\scriptsize}} %
\tikzset{>=stealth}

\tikzset{vert/.style={draw,circle, minimum size=6mm, inner sep=0pt, fill=white}}
\tikzset{vertblank/.style={ minimum size=6mm, inner sep=0pt, fill=white}}

\tikzset{vertbig/.style={draw,circle, minimum size=8mm, inner sep=0pt, fill=white}}
\tikzset{->-/.style={decoration={
      markings,
      mark=at position #1 with {\arrow{>}}},postaction={decorate}}}


\tikzset{edge/.style={line width=0.5pt, decoration={markings,mark=at position 1 with %
    {\arrow[scale=1.5,>=stealth]{>}}},postaction={decorate}}}

\tikzset{dotted/.style={black!30, line width=0.5pt}}



\pgfdeclarelayer{edgelayer}
\pgfdeclarelayer{nodelayer}
\pgfsetlayers{edgelayer,nodelayer,main}
\input{comonoids.tikzstyles}


\tikzset{none/.style={%
     append after command={%
       \pgfextra{\node [right] at (\tikzlastnode.mid east) {{\tiny\tikzlastnode}};}
     }}}
\tikzstyle{none}=[]

\newcounter{snippet}
\setcounter{snippet}{1}

\opt{nolistings}{


%\newenvironment{snippet}{\VerbatimEnvironment\begin{Verbatim}}{\end{Verbatim}\addtocounter{snippet}{1}}
\newenvironment{snippet}{\opt{long}{}\VerbatimEnvironment\begin{Verbatim}}{\end{Verbatim}\addtocounter{snippet}{1}\color{black}}
\newcommand{\minline}[1]{{\Verb!#1!}}
}
    
\opt{listings}{
\usepackage{minted}

\newenvironment{snippet}{
    \VerbatimEnvironment\begin{minted}[frame=lines,framesep=2mm,fontsize=\footnotesize, xleftmargin=0.5em, mathescape, linenos, label=Snippet \arabic{snippet}, labelposition=bottomline, escapeinside=||, mathescape=true]{julia}}{\end{minted}\addtocounter{snippet}{1}}


\newcommand{\minline}[1]{\mintinline{julia}{#1}}
}

\newcommand{\hto}{\ensuremath{\,\mathaccent\shortmid\rightarrowtriangle\,}}

\pagestyle{plain}
\DeclareSymbolFont{bbsymbol}{U}{bbold}{m}{n}
\DeclareMathSymbol{\bbsemi}{\mathbin}{bbsymbol}{"3B}
\DeclareMathSymbol{\bbcomma}{\mathbin}{bbsymbol}{"2C}
\newcommand{\comp}{}
%\DeclareUnicodeCharacter{20F0}{\textsuperscript{\textasteriskcentered}}
%\newunicodechar{ âƒ°}{\ensuremath{^*}}

%\newcommand{\Dup}{\bcopier} 
\newcommand{\Dup}{{\large\lhd}}%triangleleft}
\newcommand{\Dedup}{\bcopierb}

\newcommand{\dd}{{\,\mathrm d}}
\newcommand{\si}{\sigma}
\renewcommand{\a}{\alpha}
\newcommand{\e}{\mathrm{e}}
\renewcommand{\b}{\beta}
\renewcommand{\th}{\theta}
\newcommand{\la}{\lambda}
\newcommand{\ga}{\gamma}
\newcommand{\ka}{\kappa}
\newcommand{\legeregel}{\par\vspace{1ex}\noindent}
\newcommand{\isd}{\stackrel{d}{=}}
\newcommand\Sgn{\mathop{\mathrm{Sgn}}\nolimits}
\newcommand{\eps}{\varepsilon}
\newcommand{\nphi}{\phi}
\renewcommand{\phi}{\varphi}
\newcommand{\asnaar}{\stackrel{\mathrm{a.s.}}{\longrightarrow}}
\newcommand{\pto}{\stackrel{\mathrm{p}}{\longrightarrow}}
\newcommand{\wto}{\rightsquigarrow}
\newcommand{\asto}{\stackrel{\mathrm{a.s.}}{\longrightarrow}}
\newcommand{\dto}{\stackrel{\mathrm{\scr{D}}}{\longrightarrow}}
\renewcommand{\scr}[1]{{\mathcal #1}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\GG}{\mathbb{G}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\ind}{\mathbf{1}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\CC}{\text{I\!\!\!C}}  
\newcommand{\NN}{\text{I\!N}}
\renewcommand{\vec}[1]{\underline{ #1}}
\newcommand{\ra}{\rightarrow}
\newcommand{\ua}{\uparrow}
\newcommand{\da}{\downarrow}
\renewcommand{\Pr}{\mathrm{Pr}}
\newcommand{\M}{\scr{M}}
\newcommand{\D}{\scr{D}}
\newcommand{\mb}[1]{\mathbf{ #1}}
\newcommand{\ft}[1]{\frametitle{#1}}
\newcommand{\bem}{\begin{bmatrix}}
\newcommand{\enm}{\end{bmatrix}}
\newcommand{\bs}[1]{{\boldsymbol #1}}
\newcommand{\T}{{\prime}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\DD}{\nabla}
\newcommand{\id}{\operatorname{id}}
\providecommand{\trace}{{\operatorname{tr}}}
\renewcommand{\P}{\mathbb{P}} %
\newcommand{\Q}{\mathbb{Q}} %

\newcommand{\GP}{\ensuremath{\mathcal{G}\mathcal{P}}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}%[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{rem}[thm]{Remark}
\newtheorem{ex}[thm]{Example}
\newtheorem{defn}{Definition}
\newtheorem{ass}[thm]{Assumption}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem{exer}{Exercise}
\newcommand{\pr}{\ensuremath{{\rm P}}}
\newcommand{\prob}[1]{\ensuremath{{\rm P}\!\left( #1 \right)}}
\newcommand{\prc}[2]{\ensuremath{{\rm P}( #1 \, |\, #2)}}
\newcommand{\var}[1]{\ensuremath{{\rm Var}\!\left( #1 \right)}}
\newcommand{\cov}[2]{\ensuremath{{\rm Cov}\!\left( #1 , #2 \right)}}
\newcommand{\corr}[2]{\ensuremath{{\rho}\mspace{-1mu}\left( #1 , #2 \right)}}
\newcommand{\mean}[1]{\ensuremath{\bar{#1}}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\forward}[1]{{#1}_*}
\newcommand{\backward}[1]{{#1}^\dagger}


\newcommand{\on}[1]{\operatorname{#1}}
\newcommand{\simind}{\stackrel{\rm ind}{\sim}}
\newcommand{\simiid}{\stackrel{\rm iid}{\sim}}


\newcommand{\Bm}{\begin{bmatrix}}
\newcommand{\Em}{\end{bmatrix}}
\newcommand{\Hd}{\tilde{P}}
\newcommand{\Md}{\tilde{M}^\dagger}

\DeclareRobustCommand{\revpf}{\text{\reflectbox{$\pf$}}}

\newcommand{\hedge}[3]{h_{#1\pf#2}(#3)}
\newcommand{\hedgeto}[2]{h_{\pf#1}(#2)}
\newcommand{\tildehedge}[3]{\tilde{h}_{#1\pf#2}(#3)}
\newcommand{\tildehedgeto}[2]{\tilde{h}_{\pf#1}(#2)}

\newcommand{\hedges}[2]{h_{\pf#1}(#2)}
\newcommand{\hvertex}[2]{h_{#1}(#2)}
\newcommand{\tildehvertex}[2]{\tilde{h}_{#1}(#2)}

\newcommand{\hedgec}[4]{h_{#1\pf #2}(#3; #4)}
%\newcommand{\pedge}[4]{\kappa_{#1\pf#2}(#3; #4)}
%\newcommand{\pdaggeredge}[4]{\kappa^\dagger_{#2\revpf#1}(#3; #4)}
%\newcommand{\pedgetheta}[5]{\kappa^{#5}_{#1\pf#2}(#3; #4)}
%\newcommand{\pstaredge}[4]{\kappa^\star_{#1\pf#2}(#3; #4)}
%\newcommand{\pcircedge}[4]{\kappa^\circ_{#1\pf#2}(#3; #4)}
%\newcommand{\tildepedge}[4]{\tilde{\kappa}_{#1\pf#2}(#3; #4)}
\newcommand{\pedge}[4]{\kappa_{#1 \pf#2}(#3; #4)}
\newcommand{\pdaggeredge}[4]{\kappa^\dagger_{#2\revpf#1}(#3; #4)}
\newcommand{\pedgetheta}[5]{\kappa^{#5}_{#1\pf#2}(#3; #4)}
\newcommand{\pstaredge}[4]{\kappa^\star_{#1\pf#2}(#3; #4)}
\newcommand{\pcircedge}[4]{\kappa^\circ_{#1\pf#2}(#3; #4)}
\newcommand{\tildepedge}[4]{\tilde{\kappa}_{#1\pf#2}(#3; #4)}

%\newcommand{\optic}[2]{\langle {#1} \mid {#2}\rangle}
%\newcommand{\optico}[2]{\langle \circ {#1} \mid {#2}\rangle}
%\newcommand{\opticd}[2]{\langle \diamond {#1} \mid {#2}\rangle}
\newcommand{\optic}[2]{\langle \cF_{#1} \mid \cB_{#2}\rangle}
\newcommand{\optico}[2]{\langle \cF^\circ{#1} \mid \cB_{#2}\rangle}
\newcommand{\opticd}[2]{\langle \cF^\diamond_{#1} \mid \cB_{#2}\rangle}


\newcommand{\ev}{\operatorname{ev}}

\newcommand{\weight}[3]{w_{#1\pf#2}(#3)}

\newcommand{\logdet}{\operatorname{log\,det}}
\newcommand{\N}{\mathrm{N}}
\parindent0pt
\parskip1ex

 \newcommand{\independent}{\mathop{{\rotatebox[origin=c]{90}{$\models$}}}}
\newcommand{\pder}[2][]{\frac{\partial #1}{\partial #2}}
\makeatletter
%\def\fcmp{\mathbin{\raise 0.6ex\hbox{\oalign{\hfil$\scriptscriptstyle      \mathrm{o}$\hfil\cr\hfil$\scriptscriptstyle\mathrm{9}$\hfil}}}}


\def\lstAZ{A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z}
\def\lstaz{a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z}

\def\lstAZBB{B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, T, U, V, W, X, Y, Z}

\newcommand{\MkScr}[1]{\expandafter\def\csname s#1\endcsname{\mathscr{#1}}}
\newcommand{\MkUp}[1]{\expandafter\def\csname u#1\endcsname{\mathrm{#1}}}
\newcommand{\MkBold}[1]{\expandafter\def\csname b#1\endcsname{\mathbf{#1}}}
\newcommand{\MkFrak}[1]{\expandafter\def\csname f#1\endcsname{\mathfrak{#1}}}
\newcommand{\MkCal}[1]{\expandafter\def\csname c#1\endcsname{\mathcal{#1}}}
\newcommand{\MkBB}[1]{\expandafter\def\csname #1#1\endcsname{\mathbb{#1}}}

\@for\i:=\lstAZ\do{%
	\expandafter\MkScr \i  %
	\expandafter\MkFrak \i  %
	\expandafter\MkUp \i %
	\expandafter\MkBold \i %	
	\expandafter\MkCal \i  %
}    
\@for\i:=\lstaz\do{%
	\expandafter\MkUp \i   }    

\@for\i:=\lstAZBB\do{%
	\expandafter\MkBB \i     }

\makeatother


%\newcommand{\DeclareMyOperator}[1]{%
%	\expandafter\DeclareMathOperator\csname #1\endcsname{#1}
%}
%\newcommand{\DeclareMathOperators}{\forcsvlist{\DeclareMyOperator}}


\DeclareFontFamily{U}{matha}{\hyphenchar\font45}
\DeclareFontShape{U}{matha}{m}{n}{
      <5> <6> <7> <8> <9> <10> gen * matha
      <10.95> matha10 <12> <14.4> <17.28> <20.74> <24.88> matha12
      }{}
\DeclareSymbolFont{matha}{U}{matha}{m}{n}
\DeclareMathSymbol{\varrightharpoonup}{3}{matha}{"E1}
\DeclareMathSymbol{\varleftharpoonup}{3}{matha}{"E0}

\newcommand{\before}{\leadsto}
\renewcommand{\tilde}{\widetilde}

\DeclareMathOperator{\pa}{pa}
\DeclareMathOperator{\anc}{anc}
\DeclareMathOperator{\ch}{ch}
\DeclareMathOperator{\sample}{sample}


\newsavebox\bsbcopier
\savebox\bsbcopier{%
  \begin{tikzpicture}[baseline=0pt,line width=0.5pt]
    \node[bn,scale=0.7] (a) at (0, 2.8pt) {};
    \draw (a) -- +(-180:.21);
    \draw (a) -- +(-45:.21);
    \draw (a) -- +(45:.21);
  \end{tikzpicture}} 
\newsavebox\bsbcopierb
\savebox\bsbcopierb{%
  \begin{tikzpicture}[baseline=0pt,line width=0.5pt]
    \node[bn,scale=0.7] (a) at (0, 2.8pt) {};
    \draw (a) -- +(0:.21);
    \draw (a) -- +(-225:.21);
    \draw (a) -- +(-135:.21);
  \end{tikzpicture}}

\newsavebox\bsinver
\savebox\bsinver{%
  \begin{tikzpicture}[baseline=0pt,line width=0.5pt]
    \node[wn,scale=0.7] (a) at (0, 2.8pt) {};
    \draw (a) -- +(0:.21);
    \draw (a) -- +(-180:.21);
  \end{tikzpicture}}

\newcommand{\bcopier}{\mathord{\usebox\bsbcopier}}
\newcommand{\bcopierb}{\mathord{\usebox\bsbcopierb}}
\newcommand{\inver}{\mathord{\usebox\bsinver}}


\newcommand{\blue}{\color{blue}}

\newcommand{\bw}[0]{\mathrm{bw}}
\newcommand{\fw}[0]{\mathrm{fw}}

\usepackage{mathtools}

\DeclarePairedDelimiterXPP\expec[1]{\E}[]{}{
\newcommand\given{\nonscript\:\delimsize\vert\nonscript\:}
#1}
\def\Expec{\expec*}


\makeatletter
\renewcommand*{\@fnsymbol}[1]{\ensuremath{\ifcase#1\or *\or 
    \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
    \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother
\definecolor{darkgreen}{rgb}{0.1, 0.3, 0.23}

\newcommand{\extended}[1]{\opt{long}{\color{Maroon} #1\color{black}}}
\newcommand{\abridged}[1]{\opt{short}{\color{darkgreen}#1\color{black}}}

\date{}

\title{Compositionality in algorithms for smoothing}


\date{\today}

\author{Moritz Schauer}
\address{ Chalmers University of Technology and University of Gothenburg}
\email{smoritz@chalmers.se}
 \urladdr{www.math.chalmers.se/~smoritz/}

\author{Frank van der Meulen}
\address{Department of Mathematics \\ Vrije Universiteit Amsterdam }
\email{f.h.van.der.meulen@vu.nl}
 \urladdr{https://fmeulen.github.io/}

\keywords{backward information filter, Bayesian hidden Markov Model, bidrectional data flow, category theory, compositionality, forward-backward algorithm,  optics}
\subjclass[2020]{Primary: 62M05, 18M35 ; Secondary: 18M05}

%62M05 (1973-now) Markov processes: estimation; hidden Markov models 
%18M35 (2020-now) Categories of networks and processes, compositionality 
%18M05 (2020-now) Monoidal categories, symmetric monoidal categories [See also 19D23] 

\begin{document}
\maketitle

%\tableofcontents





\numberwithin{equation}{section}
\sloppy 

\begin{abstract}
Backward Filtering Forward Guiding (BFFG) is a bidirectional algorithm proposed in \cite{mider2021continuous} and studied more in depth in a general setting in \cite{van2020automatic}. In category theory, optics have been proposed for modelling systems with bidirectional data flow. We connect BFFG with optics and prove that different ways of composing the building blocks of  BFFG correspond to equivalent optics. 
\end{abstract}


\section{Introduction}

The past decade has witnessed an increased application of category theory to machine learning (\cite{shiebler2021category}, \cite{fong2019backprop}). A key underlying idea is to focus on compositionality and abstract away details.  State space models, also known as hidden Markov models, consist of a latent Markov process that  is only partially observed, possibly subject to noise. A visualisation as a directed graph is given in Figure \ref{fig:ssm}. Here, $\bullet$ and $\circ$ correspond to latent and observed vertices respectively. 
\begin{figure}
%	\begin{center}
\begin{tikzpicture}[style={scale=0.52}]
	\tikzstyle{empty}=[fill=white, draw=black, shape=circle,inner sep=1pt, line width=0.7pt]
	\tikzstyle{solid}=[fill=black, draw=black, shape=circle,inner sep=1pt,line width=0.7pt]
	\begin{pgfonlayer}{nodelayer}
		\node [style=empty,label=below:{$r$},] (00-) at (-8, 0) {};
		\node [style=solid,label=below:{$t_0$},] (00) at (-6, 0) {};
		\node [style=empty,label={$v_0$},] (0obs) at (-6, 1.5) {};

		\node [style=solid,label=below:{$t_1$},] (0) at (-4, 0) {};
		\node [style=empty,label={$v_1$},] (1obs) at (-4, 1.5) {};

		\node [style=solid,label=below:{$t_2$}] (1) at (-2, 0) {};
		\node [style=empty,label={$v_2$},] (2obs) at (-2, 1.5) {};

		\node [] (2) at (-0, 0) {};
%		\node [style=empty,label={$y_3$},] (3obs) at (-0, 1.5) {};

		\node [style=none] (end) at (1.0, 0) {.};

		\node [style=solid,label=below:{$t_{n-1}$}] (nn) at (2, 0) {};
		\node [style=empty,label={$v_{n-1}$},] (nnobs) at (2, 1.5) {};

		\node [style=solid,label=below:{$t_n$}] (n) at (4, 0) {};
		\node [style=empty,label={$v_{n}$},] (nobs) at (4, 1.5) {};

		\node [style=none] (end) at (1.0, 0) {};

	\end{pgfonlayer}
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge,color=red] (00-) to (00);
		\draw [style=edge] (00) to (0);
		\draw [style=edge] (0) to (1);
		\draw [style=edge] (1) to (2);
		\draw [style=edge] (nn) to (n);

		\draw [style=edge,color=blue] (00) to (0obs);
		\draw [style=edge,color=blue] (0) to (1obs);
		\draw [style=edge,color=blue] (1) to (2obs);
%		\draw [style=edge] (2) to (3obs);
		\draw [style=edge,color=blue] (nn) to (nnobs);
		\draw [style=edge,color=blue] (n) to (nobs);

		\draw [style=dashed box] (2) to (nn);
	\end{pgfonlayer}
\end{tikzpicture}
%\end{center}
\caption{Visualisation of a state-space-model. The process starts from the known root vertex $r$ and evolves over times $t_0, t_1,\ldots, t_n$. At each time $t_i$, a partial observation at vertex $v_i$ is assumed. \label{fig:ssm}}
\end{figure}
Let $X_s$ denote the value at vertex $s$. The values at the leaf vertices are generated as follows
\begin{align*}
	X_{t_0} \mid X_{r} &\sim \kappa_{r,t_0}(X_r,\cdot) \\  X_{t_i} \mid X_{t_{i-1}} & \sim \kappa_{t_{i-1}, t_i}(X_{t_{i-1}},\cdot), \qquad 1\le i \le n  \\  X_{v_i} \mid X_{t_i} & \sim \kappa_{v_i, t_i}(X_{t_i},\cdot).
\end{align*}
Here, each of the $\kappa$ are Markov kernels (Cf.\ Section \ref{sec:mk}) that represent the conditional probability distributions by which the process evolves on the directed graph. 
State-space models have widespread use in many fields including engineering, economics and biology. 
%Filtering and smoothing algorithms aim to infer the latent process from observations. 
\begin{figure}
\begin{tikzpicture}

\tikzstyle{empty}=[fill=white, draw=black, shape=circle,inner sep=1pt, line width=0.7pt]
\tikzstyle{solid}=[fill=black, draw=black, shape=circle,inner sep=1pt,line width=0.7pt]

\begin{pgfonlayer}{nodelayer}
		\node [style=empty,label={$r$},] (r) at (-8.5, 2) {};
		\node [style=solid,label={$t_1$},] (t1) at (-6.5, 2) {};
		\node [style=solid,label={$t_2$},] (t2) at (-4.5, 2) {};
		\node [style=solid,label={$t_4$},] (t4) at (-3, 1.5) {};
		\node [style=solid,label={$t_3$},] (t3) at (-3, 2.5) {};
		\node [style=empty,label={{\blue $v_1$}},] (v1) at (-1, 2.5) {};
		\node [style=empty,label={{\blue $v_2$}},] (v2) at (-1, 1.5) {};


\end{pgfonlayer}
\begin{pgfonlayer}{edgelayer}
		\draw [style=edge, color = red ] (r) to (t1);
		\draw [style=edge] (t1) to (t2);
		\draw [style=edge] (t2) to (t3);	
		\draw [style=edge] (t2) to (t4);
		\draw [style=edge, color=blue] (t3) to (v1);
		\draw [style=edge, color=blue] (t4) to (v2);
\end{pgfonlayer}

\end{tikzpicture}
\caption{Example of a stochastic process on a directed tree. The root-node is denoted by $r$ which, without loss of generality, is assumed to be known. We assume at each vertex sits a random quantity, and only at the leaf vertices $v_1$ and $v_2$ a realisation of that random quantity is observed.\label{fig:directed_tree}}
%\end{center}
\end{figure}

In this paper we consider the more general setup where  $X$ is a stochastic process on a directed tree. The process is assumed to  be observed only at its leaf vertices. A simple example  is given in Figure \ref{fig:directed_tree}.
%\begin{center}
The smoothing problem  consists of inferring the distribution of the process at all interior (non-observed) vertices, conditional on the observations at the leaves. For the example in Figure \ref{fig:directed_tree} this amounts to $\{X_{t_i},\, 1\le i \le 4\}$, if $X_s$ denotes the value of $X$ at the vertex labeled $s$. \cite{cappe2005springer} present multiple approaches towards this problem for the specific setting of state-space models: {\it (i)} normalised forward-backward recursion 
{\it (ii)} forward decomposition  {\it (iii)} backward decomposition. The computations involved can only be done in closed-form in very specific settings, such as when the latent state only takes values in a finite set. The celebrated Kalman filter-smoother for linear Gaussian systems is another well-known example (\cite{Bishop07}).  \cite{van2020automatic} introduced the {\it Backward Filtering Forward Guiding (BFFG)} algorithm, which uses the forward decomposition with  simplified dynamics in the backward filtering pass for $X$.   BFFG provides a general framework for dealing with the smoothing problem encompassing the Kalman en finite-state space settings as special cases.  

The algorithm is characterised by a {\it bidirectional data flow}: there is both a forward and backward pass. Within category theory, lenses and optics have been proposed as building blocks for bidirectionality. It is the aim of this paper to provide a categorical perspective of the BFFG-algorithm using optics.
 In particular, in theorems \ref{thm:seq} and \ref{thm:par} we provide compositionality results for Kalman smoothing. More precisely, we show that the optic obtained from parallel/sequential composing Markov kernels is equivalent to the corresponding composition of optics of the separate Markov kernels. Compositionality of the well-known Forward Filtering Backward Sampling (FFBS) algorithm (\cite{fruhwirth1994data}, \cite{carter1994gibbs}) is a direct  consequence of our results. 
 %A similar result is established for a product of Markov kernels. 
 
From a practical point of view, we believe that a description of an algorithm in categorical language enhances its understanding and greatly simplifies its implementation. For BFFG this means that certain forward and backward maps need to be implemented, their pairing to form an optic, and finally composition rules for optics. 

\subsection{Outline}
We start with a recap of a few notions from category theory in Section \ref{sec:cat}. Specific attention is given to Markov kernels and optics. In Section \ref{sec:bffg} we summarise the BFFG-algorithm from \cite{van2020automatic}.  Sections \ref{sec:seq} and \ref{sec:par} show equivalence of different ways of  sequential- and parallel composition of the algorithm respectively. Theorems \ref{thm:seq} and \ref{thm:par} are the main results. In Section \ref{sec:sampling} we discuss sampling from the smoothing distribution. We conclude with an example in Section \ref{sec:example}.



%To give a gist of the results we show, suppose a graph with two nodes, say $s$ and $t$, where there is arrow pointing from $s$ to $t$. The distribution of  $x_t$ (the value at vertex $t$) conditional on $x_s$ is governed by the Markov kernel $\kappa$. 





%
% The conditioned process possesses the same dependency structure as the unconditioned process. In case of a state-space, this is proved rigorously in Chapter .. in \cite{cappe2005springer}. In case the process takes only values in a finite set, the map from conditional to unconditional process is easily described. At any interior node $s$, let $\scr{V}_s$ denote the set of leaves descending from $s$.  Define $h_s(x)=p(x_{\scr{V}_s} \mid x)$ denote the probability of observing $x_{\scr{V}_s}$ as the leaves, conditional on the process being in state $x$ at vertex $s$. Then, the conditioned process has 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Definitions from category theory}\label{sec:cat}
%As \cite[chapter 1.1]{spivak2013category} writes ``Category theory ... was invented for powerful communication of ideas between different fields and subfields within mathematics. 

We start with the definition of a category, taken from \cite{pierce1991basic} (Definition 1.1.1).
\begin{defn}
	A {\bf category} $C$ comprises
	\begin{enumerate}
		\item a collection of objects;
		\item a collections of arrows (often called morphisms);
		\item operations assigning to each arrow $f$ an object $\mathrm{dom} f$, its domain, and an object $\mathrm{cod} f$, its codomain (we write $f\colon A\to B$ to show that $\mathrm{dom} f=A$ and $\mathrm{cod} f=B$);
		\item a composition operator assigning to each pair of arrows $f$ and $g$ with $\mathrm{cod} f= \mathrm{dom} g$, a composite aarrow $g\circ f\colon \mathrm{dom} f \to \mathrm{cod} g$ satisfying the  the following associative law: for any $f\colon A\to B$, $g\colon B\to C$ and $h\colon C\to D$  \[ h\circ (g \circ f) = (h\circ g) \circ f\: ;\]
\item for each object $A$, an identity arrow $\id_A\colon A\to A$ satisfying the identity law
\[ \text{for any  } f \colon A\to B :\quad \id_B\circ f=f \quad \text{and} \quad f \circ \id_A = f. \]
	\end{enumerate}
\end{defn}


%\begin{defn}
%Let $C$ and $D$ be categories. A {\bf functor} $F\colon C\to D$ is a map taking each $C$-object $A$ to a a $D$-object $F(A)$ and each $C$-arrow $f\colon A\to B$ to a $D$-arrow $F(f) \colon f(A) \to f(B)$, such that for all $C$-objects $A$ and composable $C$-arrows $f$ and $g$
%\begin{enumerate}
%	\item $F(\id_A)= \id_{F(A)}$;
%	\item $F(g\circ f) = F(g) \circ F(f)$.
%\end{enumerate}
%\end{defn}
%\note{so far this is nowhere used}

\subsection{Markov kernels}\label{sec:mk}
The following recap on Markov kernels is taken from  from Section 2.1 in \cite{van2020automatic}
Let $S=(E,\fB)$ and $S'=(E',\fB')$ be 
Borel measurable spaces. A Markov kernel between $S$ and $S'$ is denoted by $\kappa\colon S \rightarrowtriangle S'$  (note the special type of arrow), where $S$ is the ``source''  and $S'$ the ``target''. That is, $\kappa\colon E \times \fB' \to [0,1]$, where {\it (i)} for fixed $B\in \fB'$ the map $x\mapsto \kappa(x, B)$ is $\fB$-measurable and {\it (ii)} for fixed $x\in E$, the map $B\mapsto \kappa(x, B)$ is a probability measure on $S'$. 
On a measurable space $S$, denote the sets of bounded measures and bounded measurable functions (equipped with the supremum norm)  by $\bM(S)$ and $\mathbf{B}(S)$ respectively. 
The kernel $\kappa$ induces a {\it pushforward} of the measure $\mu$ on $S$ to $S'$   via 
\begin{equation}\label{eq:pushforward}
\mu \kappa(\cdot)  = \int_{E} \kappa(x, \cdot) \mu(\!\dd x), \qquad \mu \in \bM(S).
\end{equation}
The linear continuous operator $\kappa\colon \mathbf B(S') \rightarrow \mathbf B(S)$ is defined by 
\begin{equation}\label{eq:pullback}
\kappa h(\cdot) = \int_{E} h(y) \kappa(\cdot, \dd y),\qquad  h \in \mathbf{B}(S').
\end{equation} 
We will refer to this operation as the {\it pullback} of $h$ under the kernel $\kappa$. 
Markov kernels $\kappa_1\colon S_0 \to S_1 $ and $\kappa_2\colon S_1 \to S_2$  
can be composed  by the Chapman-Kolmogorov equations, here written as juxtaposition
\begin{equation}\label{eq:chapman}
(\kappa_1 \comp \kappa_2)(x_0, \cdot) = \int_{E_1}  \kappa_2(x_1, \cdot) \kappa_1(x_0, \dd x_1),\qquad x_0 \in E_0.
\end{equation}
The unit $\id$ for this composition is the  identity function considered as a Markov kernel, $\id(x, \dd y) = \delta_{x}(\!\dd y)$.
The product kernel $\kappa \otimes \kappa'$ on $S \otimes S'$
 is defined on the cylinder sets by \begin{equation}\label{eq:markovpar}
(\kappa  \otimes \kappa') ((x, x'), B \times B') = \kappa (x, B) \kappa' ( x', B'), \end{equation}  	(where $x\in E$, $x' \in E'$, $B \in \fB$, $B' \in \fB'$) 
and then extended to a kernel on the product measure space.

%\subsection{BorelStoch}
%A recap of Markov kernels () is given in the Appendix. 

The category \textbf{BorelStoch} has as objects measurable spaces $S, S'$ and arrows given by Markov kernels $\kappa\colon S \rightarrowtriangle S'$. If $\kappa_{S,T} \colon S \rightarrowtriangle T$ and $\kappa_{T,U} \colon T \rightarrowtriangle T$ then a composite arrow $\kappa_{S,U}$ can be defined via the Chapman-Kolmogorov equation \eqref{eq:chapman}, which we write  in diagrammatic order as $\kappa_{S,U}= \kappa_{S,T} \kappa_{T,U}$. The identity morphism is $\id$. 

\subsection{Computational graph on a directed tree}\label{sec:compgraph}
Using Markov kernels, we can define a ``computational graph'' of a stochastic process on a directed tree. We explain this for the  tree depicted in Figure \ref{fig:directed_tree}. Though this visualisation is popular in the literature on Bayesian networks -- highlighting vertices at which random quantities ``sit''-- from a categorical point of view it is more natural to convert this figure into the string diagram given in Figure \ref{fig:string_directed_tree}. Here, the duplication kernel $\Dup$ is defined by
\begin{equation}\label{eq:dup}
\Dup(x, \dd y) = \delta_{x}(y_1) \delta_x(y_2), \qquad y = (y_1, y_2). 	
\end{equation}
This encodes making a copy of $x$ when the process branches into conditionally independent subbranches. 
The ``computational graph'' of the dynamics along the tree is given by 
\[  \kappa_{r,t_1} \kappa_{t_1,t_2}\Dup\, (\kappa_{t_2,t_3} \otimes \kappa_{t_3,t_4}) (\kappa_{t_3,v_1} \otimes \kappa_{t_4,v_2})  \]
We aim to transform this graph into a procedure that produces a weighted sample $(\omega, X^\circ_{t_1},  X^\circ_{t_2}, X^\circ_{t_3}, X^\circ_{t_4})$ from the smoothing distribution such that
\[ \EE \left[ f(X_{t_1},  X_{t_2}, X_{t_3}, X_{t_4}) \mid X_{v_1}=x_1, X_{v_2}=x_2\right]= \EE \left[ \omega f(X^\circ_{t_1},  X^\circ_{t_2}, X^\circ_{t_3}, X^\circ_{t_4})\right] \]
for test-functions $f$. 

\begin{figure}
\begin{tikzpicture}

\tikzstyle{empty}=[fill=white, draw=black, shape=circle,inner sep=1pt, line width=0.7pt]
\tikzstyle{solid}=[fill=black, draw=black, shape=circle,inner sep=1pt,line width=0.7pt]

\begin{pgfonlayer}{nodelayer}
		\node [style=empty,] (r) at (-10.5, 2) {};
		\node [style=solid,] (t1) at (-8.5, 2) {};
		\node [style=solid,] (t2) at (-4.5, 2) {};
		\node [style=solid,] (t4) at (-2, 1) {};
		\node [style=solid,] (t3) at (-2, 3) {};
		\node [style=empty,] (v1) at (0, 3) {};
		\node [style=empty,] (v2) at (0, 1) {};
		\node [style=morphism, draw=black] (k6) at (-9.5, 2) {$\kappa_{r,t_1}$};
		\node [style=morphism, draw=black] (k6) at (-7.5, 2) {$\kappa_{t_1,t_2}$};
		\node [style=morphism, draw=black] (k6) at (-5.75, 2) {$\Dup$};

		\node [style=morphism, draw=black] (k6) at (-3.25, 2.5) {$\kappa_{t_2,t_3}$};
		\node [style=morphism, draw=black] (k6) at (-1, 3) {$\kappa_{t_3,v_1}$};
		\node [style=morphism, draw=black] (k6) at (-3.25, 1.5) {$\kappa_{t_2,t_4}$};
		\node [style=morphism, draw=black] (k6) at (-1, 1) {$\kappa_{t_4,v_2}$};


\end{pgfonlayer}
\begin{pgfonlayer}{edgelayer}
		\draw [style=none, color = red ] (r) to (t1);
		\draw [style=none] (t1) to (t2);
		\draw [style=none] (t2) to (t3);	
		\draw [style=none] (t2) to (t4);
		\draw [style=none, color=blue] (t3) to (v1);
		\draw [style=none, color=blue] (t4) to (v2);
	
%			\draw [style=none, draw=black] (k6) to (k6.center);

\end{pgfonlayer}

\end{tikzpicture}
\caption{String diagram corresponding to Figure \ref{fig:directed_tree}. $\Dup$ is is the duplication kernel defined in \eqref{eq:dup}. \label{fig:string_directed_tree}}
%\end{center}
\end{figure}



%
%The {\it pushforward} of a measure $\mu$ by the Markov kernel $\kappa$ is defined by 
%\begin{equation}\label{eq:pushforward} (\mu \kappa)(A) = \int \kappa(x,A) \mu(\dd x). \end{equation}
%Furthermore, the {\it pullback} of a measurable function $h$ by $\kappa$ is defined by 
%\begin{equation}\label{eq:pullback}
%(\kappa h)(x) = \int \kappa(x,\dd y) h(y). 	
%\end{equation}


\subsection{Optics}\label{sec:optics}
Optics were introduced in \cite{Riley2018} for modelling systems with bidirectional data flow. Here, we present the formulation as given in  \cite{gavranovic2022space}.
\begin{defn}
An {\bf optic}  from $(A, A')$ to $(B, B')$ is an equivalence class  of triplets $(M, \fw, \bw)$ with 
\begin{itemize}
	\item $M\colon \scr{C}$ type of the internal state;
	\item $\fw \colon A \to M \otimes B$ the forward map;
	\item $\bw \colon M \otimes B' \to A'$ the backward map.
\end{itemize}
In the language of category theory, objects are tuples $(A,A')$ and $(B, B')$ and morphisms equivalence classes of triplets like $(M,\fw, \bw)$. 
Two optics $(M, f, f')$ and $(N,g,g')$ are equivalent, $(M, f, f') \sim (N,g,g')$ if there exists either $r\colon N\to M$ or $r\colon M \to N$ such that 
$f; (r\otimes B)=g$ and  $(r\otimes B'); g' = f'$.
We write $\mathrm{optic}(\scr{C})\begin{bmatrix}
	A & B \\ A' & B' \end{bmatrix}$. 
\end{defn}
The map $r$ is called the residual morphism. The requirement on $r$ is that the following  diagrams commute:
\begin{center}	
\begin{tikzcd}
A \arrow[rd, "g"] \arrow[r, "f"] & M\otimes B \arrow[d, "r\otimes B"] \\
							  & N\otimes B
\end{tikzcd}	\qquad \qquad 
\begin{tikzcd}
M\otimes B' \arrow[r, "f'"] \arrow[d, "r\otimes B'", swap] & A'  \\
							   N\otimes B' \arrow[ru, "g'", swap]
\end{tikzcd}	


\end{center}	
On optic from  $(A,A')$ to $(B, B')$ can be be visualised easily:
%\begin{figure}
\begin{equation}\label{eq:optic}
	\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap) at (0, 0) {$\fw$};
		\node [style=none] (in_bw) at (-1, 0) {$A$};	
		\node [style=none] (out_bw) at (1, 0) {$B$};	
		
		\node [style=morphism] (fwmap) at (0, -1) {$\bw$};
		\node [style=none] (in_fw) at (-1, -1) {$A'$};	
		\node [style=none] (out_fw) at (1, -1) {$B'$};	

		\node [style=none] (message) at (0.25, -0.5) {$M$};	
	\end{pgfonlayer}	
	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw) to (bwmap);
		\draw [style=edge] (bwmap) to (out_bw);		
		
		\draw [style=edge] (fwmap) to (in_fw);
		\draw [style=edge] (out_fw) to (fwmap);		

		\draw [style=edge] (bwmap) to (fwmap) ;		
	\end{pgfonlayer}

\end{tikzpicture}
\end{equation}
Here  $m$ represents the internal state. 
%\caption{Visualisation of optic , where $m$ represents the internal state.\label{fig:optic} }
%\end{figure}


%The following figure clarifies the structure of an optic \begin{equation}\label{eq:optic}
%	\includegraphics[scale=0.4]{optic.png}
%\end{equation}
\begin{defn}[Composition of optics]
Optics can be composed (Definition 3 in \cite{gavranovic2022space}): consider two optics 
\[ \begin{bmatrix}
	A \\ A'
\end{bmatrix} \xrightarrow{(M_1, \fw_1, \bw_1)}\begin{bmatrix}
	B \\ B'
\end{bmatrix} \quad \text{and} \quad  \begin{bmatrix}
	B \\ B'
\end{bmatrix} \xrightarrow{(M_2, \fw_2, \bw_2)}\begin{bmatrix}
	C \\ C'
\end{bmatrix}, \]
then the composite $(M, \fw, \bw)$ is defined by
\begin{align*}
	M & := M_1 \otimes M_2 \\
	\fw & := A \xrightarrow{\fw_1} M_1 \otimes B \xrightarrow{M_1 \otimes\fw_2} M_1 \otimes (M_2 \otimes C) \\
	\bw & := M_1\otimes (M_2 \otimes C')  \xrightarrow{M_1 \otimes \bw_2} M_1 \otimes B' \xrightarrow{\bw_w} A'
\end{align*}
\end{defn}
A visualisation clarifies the composition rule. 
%This composition can be visualised  by the diagram
%\begin{equation}\label{composition_optics}
%	\includegraphics[scale=0.4]{optics_composed.png}
%\end{equation}
%\begin{figure}
%\begin{equation}\label{eq:optics_composed}
\begin{center}
\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap2) at (0, 0) {$\fw_1$};
		\node [style=none] (in_bw2) at (-1, 0) {$A$};	
		\node [style=none] (out_bw2) at (1, 0) {$B$};	
		
		\node [style=morphism] (fwmap2) at (0, -1) {$\bw_1$};
		\node [style=none] (in_fw2) at (-1, -1) {$A'$};	
		\node [style=none] (out_fw2) at (1, -1) {$B'$};	

		\node [style=none] (message2) at (0.25, -0.5) {$M_1$};	
	\end{pgfonlayer}	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw2) to (bwmap2);
		\draw [style=edge] (bwmap2) to (out_bw2);		
		
		\draw [style=edge] (fwmap2) to (in_fw2);
		\draw [style=edge] (out_fw2) to (fwmap2);		

		\draw [style=edge] (bwmap2) to (fwmap2) ;		
	\end{pgfonlayer}

	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap) at (3, 0) {$\fw_2$};
		\node [style=none] (in_bw) at (2, 0) {$B$};	
		\node [style=none] (out_bw) at (4, 0) {$C$};	
		
		\node [style=morphism] (fwmap) at (3, -1) {$\bw_2$};
		\node [style=none] (in_fw) at (2, -1) {$B'$};	
		\node [style=none] (out_fw) at (4, -1) {$C'$};	

		\node [style=none] (message) at (3.25, -0.5) {$M_2$};	
	\end{pgfonlayer}	
	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw) to (bwmap);
		\draw [style=edge] (bwmap) to (out_bw);		
		
		\draw [style=edge] (fwmap) to (in_fw);
		\draw [style=edge] (out_fw) to (fwmap);		

		\draw [style=edge] (bwmap) to (fwmap) ;		
	\end{pgfonlayer}

\node[draw,inner sep=2mm, label=below:,fit=(bwmap) (fwmap) (message) (bwmap2),color=blue] {};
\end{tikzpicture}
\end{center}	
%\end{equation}


	



\begin{defn}[Tensor product of optics]
Consider two optics 
\[ \begin{bmatrix}
	A_1 \\ A_1'
\end{bmatrix} \xrightarrow{(M_1, \fw_1, \bw_1)}\begin{bmatrix}
	B_1 \\ B_1'
\end{bmatrix} \quad \text{and} \quad  \begin{bmatrix}
	A_2 \\ A_2'
\end{bmatrix} \xrightarrow{(M_2, \fw_2, \bw_2)}\begin{bmatrix}
	B_2 \\ B_2'
\end{bmatrix}, \]
then the tensor product $(M, \fw, \bw)$
\[ \Bm A_1 \otimes A_2 \\ A_1'\otimes A_2'\Em \xrightarrow{(M, \fw, \bw)} \Bm B_1 \otimes B_2 \\ B_1' \otimes B_2'\Em \]
 is defined by
\begin{align*}
	M & := M_1 \otimes M_2 \\
	\fw & := (A_1,A_2) \xrightarrow{\fw_1\otimes \fw_2} (M_1 \otimes B_1, M_2 \otimes B_2) \\
	\bw & := (M_1\otimes B_1)\otimes (M_2,\otimes B_2)   \xrightarrow{(M_1 \otimes \bw_1) \otimes (M_2 \otimes \bw_2)} B_1' \otimes B_2'
\end{align*}
\end{defn}
This  can be visualised by putting the boxes in the diagram on top rather than next to one another.
%%%%%%

\cite{dependent_optics} defines  {\it dependent optics}. In this case the composition rules are just as for ``ordinary'' optics, but the object $(A_1, A_2)$ need not have $A_1$ and $A_2$ be of the same type. Rather, both can depend on a third object. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Backward Filtering Forward Guiding}\label{sec:bffg}
 In this section we recap the BFFG-algorithm introduced in \cite{mider2021continuous} and  generalised in \cite{van2020automatic}. We recap section 3 from the latter paper and refer to \cite{bffg_introduction} for a  non-technical introduction to it.

For exposition, assume a stochastic process on a line-graph, with vertices denotes by $\{0,1,\ldots, n+1\}$. The root-vertex $0$ is assumed to be known. The transition from vertex $i-1$ to $i$ is governed by the Markov kernel $\kappa_i$. Then, if at node $0$ the process equals $x_0$, the distribution at vertex 1 is $\delta_{x_0} \kappa_1$, the pushforward of $\delta_{x_0}$ (Cf.\ Equation \eqref{eq:pushforward}). Pushing forward iteratively, we see that 
the random quantity at vertex $i$, denoted by $X_i$, has  distribution
\begin{equation}\label{eq:linegraph} \delta_{x_0} \kappa_1 \kappa_2 \cdots \kappa_i. \end{equation}
Brackets are unnecessary by associativity of Markov composition. 
Suppose we observe $x_{n+1}$ at vertex $n+1$. See Figure \ref{fig:linegraph}, where we have added the Markov kernels along the branches. 

Assume that  $\kappa_{n+1}(x,\dd y) = p_{n+1}(x,y) \nu(\dd y)$ for a dominating measure $\nu$. 
By repeatedly applying  the pullback operation \eqref{eq:pullback}, we  obtain the \emph{backward filter} 
\begin{equation}\label{eq: hn2}
\begin{split}
h_n(x) &= p_{n+1}(x, x_{n+1})\\
h_{i-1} &= \kappa_i \kappa_{i+1}\cdots \kappa_{n} h_{n}.
\end{split}
\end{equation}
Note that for each $0\le i \le n$, $h_i(x)$ gives the density of the observed $x_{n+1}$, conditional on $X_i=x$. 

The  {\it $h$-transform} maps   $(\kappa_i, h_i)$ to $\kappa_i^\star$, where for  $(\kappa_i, h_i)$, $i   \in \{1, \dots, n\},$
\begin{equation}\label{eq:kappa_to_kappastar}
\frac{\kappa_i^\star(x, \dd y)}{\kappa_i(x, \dd y)}  = \frac{h_i(y) }{(\kappa_i h_i)(x)} =: m(x,y).
\end{equation}
Viewed as such, the transform from $\kappa_i$ to $\kappa^\star_i$ depends on  $m$, which we interpret as a {\it message} obtained from backward filtering, to be used in the pushforward of a measure.


If $\mu_i$ is the distribution of $X_i$, conditional on $X_{n+1}=x_{n+1}$, then $\mu_1=\delta_{x_0} \kappa^\star_1$, $\mu_2=\mu_1 \kappa^\star_2 $, $\mu_3 = \mu_2\kappa^\star_3$...
Hence, we see that there is a bidirectional dataflow
\begin{itemize}
	\item $\{h_i\}$ are computed backwards;
	\item the measure $\mu_{i-1}$ is pushed forwards using $\kappa^\star_i$ which uses the message $m$.
\end{itemize}
The messages can be computed in the backward pass to be afterwards used in the forward pass. This motivates the definitions in the upcoming section. 
%(not for $\kappa^\star_{n+1}$, which is just a Dirac measure in the observation). 



\begin{figure}
%	\begin{center}
\begin{tikzpicture}[style={scale=0.52}]

\tikzstyle{empty}=[fill=white, draw=black, shape=circle,inner sep=1pt, line width=0.7pt]
\tikzstyle{solid}=[fill=black, draw=black, shape=circle,inner sep=1pt,line width=0.7pt]

\begin{pgfonlayer}{nodelayer}
		\node [style=empty,label={$0$},] (n0) at (0, 0) {};
		\node [style=solid,label={$1$},] (n1) at (4, 0) {};
		\node [style=solid,label={$2$},] (n2) at (8, 0) {};
		\node [style=solid,] (intermediate) at (11, 0) {};
		\node [style=solid,label={$n$},] (n) at (14, 0) {};
		\node [style=empty,label={$n+1$},] (n+1) at (18.5, 0) {};
		\node [style=morphism] (kap1) at (2, 0){$\kappa_{1}$};
		\node [style=morphism] (kap2) at (6, 0){$\kappa_{2}$};
		\node [style=morphism] (kap2) at (16, 0){$\kappa_{n+1}$};
\end{pgfonlayer}

\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (n0) to (n1);
		\draw [style=edge] (n1) to (n2);
		\draw [style=edge] (n2) to (intermediate);
		\draw [style=edge] (n) to (n+1);
		\draw [style=dashed box] (intermediate) to (n);
\end{pgfonlayer}



\end{tikzpicture}
%\end{center}
\caption{Stochastic process on a line graph with one observation corresponding to the composition of Markov kernels specified in Equation \eqref{eq:linegraph}. \label{fig:linegraph}}
\end{figure}


\subsection{Forward- and backward maps}
In the upcoming definitions we assume kernels $\kappa\colon S \rightarrowtriangle S'$ and $\tilde\kappa\colon S \rightarrowtriangle S'$, where 
$S = (E, \fB)$ and $S' = (E', \fB')$ are measure spaces. 
%Let $\bB(S)$ denote the set of bounded measurable functions on $S$. Let $\bM(S)$ denote the set of bounded measures on $S$. 


\begin{defn}\label{def:backwardmap} For a Markov kernel $\kappa\colon S \rightarrowtriangle S'$ and function $h \in \bB(S')$ define the {\it backward map} $\backw\kappa \colon \bB(S') \to \bB(S \times S')\times \bB(S)$ by 
\begin{equation}\label{eq:backw}
\backw{\kappa}(h) = \left(m, \kappa h\right),\quad \text{where} \quad  m(x,y) = \frac{h(y) }{(\kappa h)(x)}.
\end{equation}
\end{defn}
This map returns both the  pullback $\kappa g$  and an appropriate {\it message} $m$ for the map $\forw\kappa$ specified in the following definition.
\begin{defn}\label{def:forwardmap}
For a Markov kernel $\kappa\colon S \rightarrowtriangle S'$, message $m\in \bB(S \times S')$ (as defined in \eqref{eq:backw}) and measure $\mu \in \bM(S)$ define the {\it forward map} $\forw\kappa: \bB(S \times S')\times \bM(S)\to \bM(S')$ by 
\begin{equation}\label{eq:forw}
 \forw{\kappa}(m, \mu)  = \nu, \quad \text{with} \quad \nu(\!\dd y) = \int m(x,y) \mu(\!\dd x) \kappa(x, \dd y). 
 \end{equation}
\end{defn}
The pairing of the backward- and forward maps can be conveniently summarised by the following diagram, which is just like \eqref{eq:optic}
\begin{center}
\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap) at (0, 0) {$\cB_{\kappa}$};
		\node [style=none] (in_bw) at (-1, 0) {$\bB_1$};	
		\node [style=none] (out_bw) at (1, 0) {$\bB_0$};	
		
		\node [style=morphism] (fwmap) at (0, -1) {$\cF_{\kappa}$};
		\node [style=none] (in_fw) at (-1, -1) {$\bM_1$};	
		\node [style=none] (out_fw) at (1, -1) {$\bM_0$};	

		\node [style=none] (message) at (0.25, -0.5) {$m$};	
	\end{pgfonlayer}	
	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw) to (bwmap);
		\draw [style=edge] (bwmap) to (out_bw);		
		
		\draw [style=edge] (fwmap) to (in_fw);
		\draw [style=edge] (out_fw) to (fwmap);		

		\draw [style=edge] (bwmap) to (fwmap) ;		
	\end{pgfonlayer}

\end{tikzpicture}
\end{center}
If $\mu$ is a probability measure, then $\cF_{\kappa}(m,\mu)$ is a  probability measure as well. 

In case the pullback is intractable or computationally demanding, the kernels $\kappa$ in the backward map  be replaced by simpler kernels $\tilde\kappa$.  If  $\tilde\kappa_1\colon S_0 \to S_1$, this can be visualised as follows:
\begin{center}
\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap) at (0, 0) {$\cB_{\tilde\kappa}$};
		\node [style=none] (in_bw) at (-1, 0) {$\bB_1$};	
		\node [style=none] (out_bw) at (1, 0) {$\bB_0$};	
		
		\node [style=morphism] (fwmap) at (0, -1) {$\cF_{\kappa}$};
		\node [style=none] (in_fw) at (-1, -1) {$\bM_1$};	
		\node [style=none] (out_fw) at (1, -1) {$\bM_0$};	

		\node [style=none] (message) at (0.25, -0.5) {$m$};	
	\end{pgfonlayer}	
	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw) to (bwmap);
		\draw [style=edge] (bwmap) to (out_bw);		
		
		\draw [style=edge] (fwmap) to (in_fw);
		\draw [style=edge] (out_fw) to (fwmap);		

		\draw [style=edge] (bwmap) to (fwmap) ;		
	\end{pgfonlayer}

\end{tikzpicture}
\end{center}
In this case, if $\mu$ is a probability measure, the output of the forward map will typically {\it not} be a probability measure, but rather a finite measure. 

In case the backward map is used with an approximate kernel $\tilde\kappa$, rather than $\kappa$, we write $\cB_{\tilde\kappa}(g)=(m, \tilde\kappa g)$, rather than with $h$. Hence, one should think of $g$ as an approximate substitute for $h$. Then, we can reinterpret the forward map as producing a {\it weighted} probability measure: 
if $\varpi\ge 0$ and $\mu$ is a probability measure then
\[
 \forw{\kappa}(m, \varpi \cdot \mu)(\!\dd y) = (\varpi w_\kappa(m, \mu)) \cdot \nu(\!\dd y) \]
where the {\it weight} $w_\kappa(m, \mu)$ and probability measure $\nu$ are defined by 
\begin{equation}\label{eq:wnu}
\begin{split}
 w_\kappa(m,\mu) &= \iint  m(x, y) \kappa(x,\dd y) \mu(\dd x) =  \int \frac{(\kappa g)(x)}{(\tilde \kappa g)(x)} \mu(\dd x)\\
\nu(\!\dd y) &=  w^{-1}_\kappa(m, \mu) \int  \frac{g(y) }{(\tilde \kappa g)(x)  }  \mu(\!\dd x) \kappa(x, \dd y). 
 \end{split}
 \end{equation}
Hence, by using an approximate $\tilde\kappa$ in the backward map, we ``pick up'' a weight $w_\kappa(m,\mu)$.

%Visualisation of optic from  $(\bB_1, \bM_1)$ to $(\bB_0, \bM_0)$, induced by $\tilde\kappa$ in backward filtering and $\kappa$ in forward guiding. 
The backward map $\cB$, forward map $\cF$ and message $m$ are exactly what constitutes a dependent optic. The objects are pairs $(\bB(S), \bM(S))$, which both depend on the measurable space $S$.  The only difference with the preceding is that $\fw$ in Section \ref{sec:optics}, the forward map, corresponds to the backward map $\cB$ here. That is,  we first go backward in time and subsequently forward in time.  From a categorical perspective, this distinction is irrelevant.

{\bf Notation: }
The (dependent) optic induced by $\tilde\kappa$ (used in the backward map) and $\kappa$ (used in the forward map) will be denoted by $\scr{O}(\kappa, \tilde\kappa)$.  

The Backward Filtering Forward Guiding algorithm consists of composing optics  and  is executed by providing $g_{n+1}$ as input to $\cB_{\tilde\kappa_n}$ and $\delta_{x_0}$ as input to $\cF_{\kappa_1}$. 
%The composite of these $n$ optics corresponds visually to one block on the top, and one on the bottom, just as in the composition of optics. %\eqref{eq:optics_composed}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Backward Filtering Forward Guiding in terms of optics}\label{sec:connecting}





\section{Equivalence of sequential composition}\label{sec:seq}
We consider the composition of two Markov kernels, say $\kappa_{0,1}$ and $\kappa_{1,2}$, and corresponding kernels $\tilde\kappa_{0,1}$ and $\tilde\kappa_{1,2}$ (used in the backward map). We introduce the short-hand notation $\cB_{0,1}=\cB_{\tilde\kappa_{0,1}}$ and  $\cF_{0,1}=\cF_{\kappa_{1,2}}$. 


The optics induced by $(\tilde\kappa_{0,1}, \kappa_{0,1})$ and $(\tilde\kappa_{1,2}, \kappa_{1,2})$ can be composed as follows:
\begin{center}

\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap2) at (0, 0) {$\cB_{1,2}$};
		\node [style=none] (in_bw2) at (-1, 0) {$\bB_2$};	
		\node [style=none] (out_bw2) at (1, 0) {$\bB_1$};	
		
		\node [style=morphism] (fwmap2) at (0, -1) {$\cF_{1,2}$};
		\node [style=none] (in_fw2) at (-1, -1) {$\bM_2$};	
		\node [style=none] (out_fw2) at (1, -1) {$\bM_1$};	

		\node [style=none] (message2) at (0.25, -0.5) {$m_{1,2}$};	
	\end{pgfonlayer}	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw2) to (bwmap2);
		\draw [style=edge] (bwmap2) to (out_bw2);		
		
		\draw [style=edge] (fwmap2) to (in_fw2);
		\draw [style=edge] (out_fw2) to (fwmap2);		

		\draw [style=edge] (bwmap2) to (fwmap2) ;		
	\end{pgfonlayer}
	
%
%\end{tikzpicture}
%\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap) at (3, 0) {$\cB_{0,1}$};
		\node [style=none] (in_bw) at (2, 0) {$\bB_1$};	
		\node [style=none] (out_bw) at (4, 0) {$\bB_0$};	
		
		\node [style=morphism] (fwmap) at (3, -1) {$\cF_{0,1}$};
		\node [style=none] (in_fw) at (2, -1) {$\bM_1$};	
		\node [style=none] (out_fw) at (4, -1) {$\bM_0$};	

		\node [style=none] (message) at (3.25, -0.5) {$m_{0,1}$};	
	\end{pgfonlayer}	
	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw) to (bwmap);
		\draw [style=edge] (bwmap) to (out_bw);		
		
		\draw [style=edge] (fwmap) to (in_fw);
		\draw [style=edge] (out_fw) to (fwmap);		

		\draw [style=edge] (bwmap) to (fwmap) ;		
	\end{pgfonlayer}

%\node[draw,inner sep=2mm,label=below:composition of optics
%,fit=(bwmap) (fwmap) (message) (bwmap2)] {};

\node[draw,inner sep=2mm, label=below:,fit=(bwmap) (fwmap) (message) (bwmap2),color=blue] {};


\end{tikzpicture}
\end{center}




However, since Markov kernels can be composed -- using the Chapman-Kolmogorov equation \eqref{eq:chapman} -- there is another way to compose:
\begin{center}
\begin{tikzpicture}[style={scale=1.5}]
	\begin{pgfonlayer}{nodelayer}
		\node [style=morphism] (bwmap) at (0, 0) {$\cB_{0,2}$};
		\node [style=none] (in_bw) at (-1, 0) {$\bB_2$};	
		\node [style=none] (out_bw) at (1, 0) {$\bB_0$};	
		
		\node [style=morphism] (fwmap) at (0, -1) {$\cF_{0,2}$};
		\node [style=none] (in_fw) at (-1, -1) {$\bM_2$};	
		\node [style=none] (out_fw) at (1, -1) {$\bM_0$};	

		\node [style=none] (message) at (0.25, -0.5) {$m_{0,2}$};	
	\end{pgfonlayer}	
	
	\begin{pgfonlayer}{edgelayer}
		\draw [style=edge] (in_bw) to (bwmap);
		\draw [style=edge] (bwmap) to (out_bw);		
		
		\draw [style=edge] (fwmap) to (in_fw);
		\draw [style=edge] (out_fw) to (fwmap);		

		\draw [style=edge] (bwmap) to (fwmap) ;		
	\end{pgfonlayer}

\end{tikzpicture}
\end{center}
where $\kappa_{0,2}=\kappa_{0,1}\kappa_{1,2}$.
%\begin{center}
%	\includegraphics[scale=0.6]{optic_bffg_composed.png}
%\end{center}

Hence, there are two constructions to compose 
\begin{enumerate}
\item {\bf construction 1:} compose Markov kernels to get $\kappa_{0,2}=\kappa_{0,1}\kappa_{1,2}$ (similarly $\tilde\kappa_{0,2}=\tilde\kappa_{0,1} \tilde\kappa_{1,2}$), then form the optic;
	\item {\bf construction 2:} compose optics $(m_{0,1}, \cB_{0,1}, \cF_{0,1})$ and $(m_{1,2}, \cB_{1,2}, \cF_{1,2})$.
\end{enumerate}
%This type of question is strikingly similar to the one posed in Section 3.3 in \cite{gavranovic2022space}. 
\begin{thm}\label{thm:seq}
	Both optics are in the same equivalence class. That is
	\[ \scr{O}(\kappa_{0,2}, \tilde\kappa_{0,2}) \sim \scr{O}(\kappa_{0,1}, \tilde\kappa_{0,1}) \scr{O}(\kappa_{1,2}, \tilde\kappa_{1,2}), \]
the right-hand-side denoting composition of optics (in diagrammatic order). 	
\end{thm}
\begin{proof}
For the first construction, the message sent is
\[ m_{0,2}(x,y) = \frac{g(y)}{(\kappa_{0,2} g)(x)}\]
On the other hand, it we compose optics, there are two messages
\begin{itemize}
	\item the message $m_{1,2}(x,y)=g(y)/(\tilde\kappa_{1,2}g)(x)$ sent by $\cB_{1,2}$;
 	\item the message \[m_{0,1}(x,y)=\frac{(\tilde{\kappa}_{1,2}g)(y)}{(\tilde{\kappa}_{0,1} \tilde{\kappa}_{1,2} g)(x)}=\frac{(\tilde{\kappa}_{1,2}g)(y)}{(\tilde{\kappa}_{0,2} g)(x)}\] sent by $\cB_{0,1}$;
\end{itemize}
Note that for any $z$
\begin{equation}\label{eq:messages_cancel} m_{0,1}(x,z)m_{1,2}(z,y) = 	\frac{\cancel{(\tilde{\kappa}_{1,2}g)(z)}}{(\tilde{\kappa}_{0,2} g)(x)} \frac{g(y)}{\cancel{(\tilde\kappa_{1,2} g)(z)}} = m_{0,2}(x,y). \end{equation}
Hence, there is the residual map $r$ that takes $(m_{0,1},m_{1,2})$ as input to produce $m_{0,2}$ with
\[ \left[r(m_{0,1},m_{1,2})\right](x,y)= m_{0,1}(x,z) m_{1,2}(z,y) \]
where $z$ can be chosen arbitrarily. 

From construction 1 the forward map outputs the measure $\nu_{2}=\cF_{0,2}(m_{0,2}, \mu_0)$ given by 
\[ \nu_{2}(\dd y) = \int m_{0,2}(x,y) \mu_0(\dd x) \kappa_{0,2}(x,\dd y). \]
In the second construction, $\cF_{0,1}(m_{0,1}, \mu_0)$ outputs the measure $\nu_1$ with 
\[ \nu_1(\dd y) = \int m_{0,1}(x,y) \mu_0(\dd x) \kappa_{0,1}(x,\dd y) \]
which is subsequently passed through to output  $\cF_{1,2}(m_{1,2}, \nu_1)$ given by
\[ \nu(\dd y) = \int m_{1,2}(x,y)  \nu_1(\dd x) \kappa_{1,2}(x,\dd y). \]
We have 
\begin{align*}
	\nu(\dd y) & = \int m_{1,2}(x,y) \int m_{0,1}(z,x) \mu_0(\dd z) \kappa_{0,1}(z,\dd x) \kappa_{1,2}(x,\dd y) \\ & = \int m_{0,2}(z,y) \mu_0(\dd z) \int  \kappa_{0,1}(z,\dd x) \kappa_{1,2}(x,\dd y) \\ & = \int m_{0,2}(z,y) \mu_0(\dd z) \kappa_{0,2}(z, \dd y) = \nu_{2}(\dd y).
\end{align*}
Here, the second equality follows from \eqref{eq:messages_cancel} and the third from the Chapman-Kolmogorov equations (Eq.\ \eqref{eq:chapman}).


\end{proof}

\section{Equivalence of parallel composition}\label{sec:par}



In the following let   $i\in \{1,2\}$.
Define a tensor-product on  $\bB(S_1\times S_2)$ by 
\begin{equation}\label{eq:tensor_boundedfunctions}
	 (g_1\odot g_2)(x_1, x_2)=g_1(x_1) g_2(x_2), \qquad g_i \in \bB(S_i) .
\end{equation}
Define a tensor-product on  $\bM(S_1\times S_2)$ by 
\[ (\mu_1 \otimes \mu_2)(B_1 \times  B_2) = \mu_1(B_1)  \mu_2(B_2),\qquad \mu_i \in \bM(S_i). \]




Recall \eqref{eq:markovpar} defining a tensor product of Markov kernels. 
There are two constructions to apply parallel two Markov kernels $\kappa_1$ and $\kappa_2$
\begin{enumerate}
	\item {\bf construction 1:} take the product of Markov kernels to get $\kappa_{1\otimes 2}=\kappa_{1} \otimes \kappa_{2}$ (similarly for $\tilde\kappa_{1\otimes 2}=\tilde\kappa_{1} \otimes \tilde\kappa_{2}$), then form the optic;
	\item {\bf construction 2:} take tensor product of optics $(m_{1}, \cB_{1} \cF_{1})$ and $(m_{2}, \cB_{2}, \cF_{2})$.
\end{enumerate}


%\item {\bf construction 1:} compose Markov kernels to get $\kappa_{0,2}=\kappa_{0,1}\kappa_{1,2}$ (similarly for $\tilde\kappa$'s), then form the optic;
%	\item {\bf construction 2:} compose optics $(M_i, \cB_{\tilde\kappa_i}, \cF_{\kappa_i})$.


\begin{thm}\label{thm:par}
Both optics are in the same equivalence class. 	That is
	\[ \scr{O}(\kappa_{1\otimes2}, \tilde\kappa_{1\otimes 2}) \sim \scr{O}(\kappa_{1}, \tilde\kappa_{1}) \otimes \scr{O}(\kappa_{2}, \tilde\kappa_{2}), \]
the right-hand-side denoting tensor product of optics. 	
\end{thm}
\begin{proof}
  For the first construction, the pullback of $g_1\odot g_2$ by $\tilde\kappa_1 \otimes \tilde\kappa_2$ is given by 
\[  (\tilde\kappa_1\otimes \tilde\kappa_2)(g_1\odot g_2) = (\tilde\kappa_1 g_1) \odot (\tilde\kappa_2 g_2). \]
Therefore, the message produced by $\cB_{1\otimes 2}$ is given by 
\[	m_{1\otimes 2}(x,y) = \frac{(g_1\odot g_2)(y)}{(\tilde\kappa_1 \otimes \tilde\kappa_2)(g_1\odot g_2)(x)}= \prod_{i=1}^2 \frac{g_i(y_i)}{(\tilde\kappa_i g_i)(x_i)}, \] where  $x=(x_1,x_2)$ and $y=(y_1,y_2)$.

In the second construction, we obtain pullbacks $\tilde\kappa_i g_i$ and messages
\[ m_i(x_i, y_i) = \frac{g_i(y_i)}{(\tilde\kappa_i g_i)(x_i)}. \]
This implies that  there exists a residual map $r$ that takes $(m_1, m_2)$ to produce $m_{1\otimes 2}$ with
\[ \left[r(m_1,m_2)\right](x,y)= m_1(x_1,y_1) m_2(x_2,y_2). \]
Then $\nu_{1\otimes 2}= \cF_{\kappa_1\otimes \kappa_2}(m_{1\otimes 2},\mu_1\otimes \mu_2)$ is given by 
\begin{align*} \nu_{1\otimes 2}(\dd y)  &= \int m_{1\otimes 2}(x,y) (\kappa_1 \otimes \kappa_2)(x,\dd y) (\mu_1\otimes \mu_2)(\dd x) \\ & = \prod_{i=1}^2 m_i(x_i,y_i) \kappa_i(x_i, \dd y_i) \mu_i(\dd x_i) = \forw{\kappa_1}(m_1,\mu_1) \otimes  \forw{\kappa_2}(m_2,\mu_2).
\end{align*}	
Hence, the outputs of the forward maps in both constructions agree. 
\end{proof}
Note that this result is only valid if we push forward a product measure $\mu_1\otimes \mu_2$. For direct masses, this poses no restriction: $\delta_{(x_1,x_2)}(\dd y_1 \dd y_2)= \delta_{x_1}(\dd y_1) \delta_{x_2}(\dd y_2)$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Sampling}\label{sec:sampling}


Theorem \ref{thm:par} only applies to pushing forward product measures in $\forw\kappa$. 
Rather than pushing forward a measure using the map $\forw{}$, one can also push forward a weighted sample, which does not suffer from such restrictions. Moreover, in Bayesian computation this is the more interesting setting.
If the forward map is applied to the weighted measure $\omega\cdot \delta_x$ then
\[ \cF_\kappa(m,  \omega\cdot \delta_x) = \omega  m(x,y) \kappa(x,\dd y) = \omega \frac{g(y)}{(\tilde{\kappa} g)(x)} \kappa(x,\dd y) = \omega' \kappa^g(x,\dd y). \]
where 
\[ \omega' = \omega w_\kappa(m, \delta_x) \quad \text{and} \quad \kappa^m(x,\dd y)=
m(x,y) \kappa(x,\dd y)=\frac{g(y)\kappa(x,\dd y)}{\int g(y)\kappa(x,\dd y)}. \]
 To define sampling  within the language of category we need the following two results.
\begin{lem}[\cite{kallenberg2002foundations}, Lemma 4.21]\label{lem:splituniforms}
If $Z \sim U(0,1)$, then there exist measurable functions $r_1, r_2,\ldots $ such that $Z_i = r_i(Z)$, $i\in \NN$ are independent with the $U(0,1)$ distribution. 
\end{lem}

\begin{lem}[\cite{kallenberg2002foundations}, Lemma 4.22]\label{lemma: randomisation}
For a Markov kernel $\kappa$ between a measure space $(E, \fB)$ and a Borel space $(E', \fB')$, there is a measurable function $f_\kappa\colon E\times [0,1] \to E'$  and a random variable $z \sim U[0,1]$ such that the random variable $f_\kappa(x, z)$ has law $\kappa(x, \cdot)$ for each $x \in E$. 
%The random variable $z$ is  called an \emph{innovation variable}.
%Further, if $\kappa'$ is a kernel with source   $(E', \fB')$ into a third Borel space, there exist independent random variables  $Z,\, Z' \sim U[0,1]$ such that  $f_{\kappa'}(f_\kappa(x, Z), Z') \sim (\kappa\kappa')(x, \cdot)$.
\end{lem}
Let $\xi=(\omega, x, z)$, where $z \sim U(0,1)$. By Lemma \ref{lem:splituniforms}, there exist measurable functions $r_1$ and $r_2$ such that $r_1(z)$ and $r_2(z)$ are independent $U(0,1)$-distributed. By Lemma \ref{lemma: randomisation}
the random variable $f_{\kappa^m}(x,r_1(z))$ has distribution $\kappa^m(x,\cdot)$.  
Hence, $\xi$ can be pushed forward to $\xi'=(\omega', x', z')$ upon setting
\[ \omega' = \omega w_\kappa(m, \delta_x), \qquad x'=f_{\kappa^m}(x,r_1(z)), \qquad z' = r_2(z). \]
This construction is fully compositional. We use the shorthand notation  $\xi' = \cF^\diamond_\kappa  (m, \xi)$ which combines a weighted sample and random number with the message $m$ to produce a new weighted sample and random number. We call $\cF^\diamond$ the {\it forward sampling map}.  
Hence, rather than pushing forward a measure we can push forward the triplet $\xi$ using $\cF^\diamond$. 

{\bf Notation:} The (dependent) optic induced by $\tilde\kappa$ (used in the backward map) and $\kappa$ (used in the forward {\it sampling} map) will be denoted by $\scr{O}^\diamond(\kappa, \tilde\kappa)$.  


\medskip

 Suppose, as in Section \ref{sec:seq}, we first apply $\kappa_{0,1}$, followed by $\kappa_{1,2}$. 
Then composing the forward sampling maps gives
\begin{equation}
	\label{eq:compose_forwardsampling}
	 \cF^\diamond_{\kappa_{1,2}}\left(m_{1,2}, \cF^\diamond_{\kappa_{0,1}}(m_{0,1}, \xi)\right). 
\end{equation}
Alternatively, we can choose to first compose the Markov kernels to $\kappa_{0,2}=\kappa_{0,1}\kappa_{1,2}$ and then apply the forward sampling map. This gives
$\cF^\diamond_{\kappa_{0,2}}(m_{0,2},\xi)$. An argument using Equation \eqref{eq:messages_cancel} reveals that the $\omega'$ obtained in both constructions is the same. It is trivial to see that $z'$ in both constructions is $U(0,1)$-distributed. 
According to \eqref{eq:compose_forwardsampling}, $x'$ is sampled in two steps
\begin{enumerate}
	\item sample $\tilde{x}$ from $\kappa_{0,1}^{m_{0,1}}(x,\cdot)$;
	\item sample $x'$ from $\kappa_{1,2}^{m_{1,2}}(\tilde{x}, \cdot)$. 
\end{enumerate}
That is, $x'$ is sampled from the measure $\nu$ with
\begin{align*}
	 \nu(\dd y)&= \int_{\tilde{x}} \kappa_{0,1}^{m_{0,1}}(x,\dd \tilde{x}) \kappa_{1,2}^{m_{1,2}}(\tilde{x}, \dd y) \\ &= \int_{\tilde{x}} m_{0,1}(x,\tilde{x}) \kappa_{0,1}(x,\dd \tilde{x}) m_{1,2}(\tilde{x},y) \kappa_{1,2}(\tilde{x}, \dd y) \\ &= m_{0,2}(x,\dd y) \int_{\tilde{x}} \kappa_{0,1}(x,\dd \tilde{x})\kappa_{1,2}(\tilde{x}, \dd y) \\ & = m_{0,2}(x,y) \kappa_{0,2}(x, \dd y)= \kappa_{0,2}^{m_{0,2}}(x, \dd y).
\end{align*}
This is exactly the measure $x'$ would be sampled from, if we had chosen to use the forward map $\cF^\diamond_{\kappa_{0,2}}(m_{0,2},\xi)$. Therefore, we conclude that composing forward sampling maps or composing Markov kernels (followed by the forward sampling map), produce samples $x'$ that have the same distribution. 


\medskip

If the computational graph contains the duplication kernel $\Dup$, then the same kernel will be used in the backward map. Then it is trivial to see that $w_\Dup(m,\delta_x)=1$ and that $x'=(x,x)$. However, we need to ensure that $z'$ contains {\it two} independent $U(0,1)$-distributed samples. Hence, if $\kappa=\tilde\kappa=\Dup$, then $\cF^\diamond_\Dup(m, \xi)$ is defined to be
\[ \omega'=\omega,\qquad x' = (x,x),\qquad z' = (r_1(z), r_2(z)). \]


\begin{comment}
	

\section{Remainders}

The maps $f_\kappa$ are the morphisms of the monoidal category \textbf{Sample}. 
%
%
%Consider $\kappa \mapsto f_\kappa$ from definition~\ref{lemma: randomisation}.
%Such randomisations  (or  randomness pushback in the terminology of \cite{Fritz2020})   form a  monoidal category \textbf{Sample}. 
Denote  $\Lambda = ([0,1], \fB([0,1]))$ and assume it to be equipped with Lebesgue measure $\lambda_{\on{Leb}}$. 
Our construction relies on the following lemma.



% Our construction relies on the existence of reproduction functions  $r, r'\colon \Lambda \to \Lambda$ such that $r$ and $r'$  are independent uniformly distributed  random variables on $\Lambda$. 
%\begin{lem}
%Denote  $\Lambda = ([0,1], \fB([0,1]))$ and assume it to be equipped with Lebesgue measure $\lambda_{\on{Leb}}$.   Define the reproduction functions  $r \colon  \to \Lambda$ and  $r' \colon  \to \Lambda$ by
%\begin{equation}\label{eq:r}
%r(z) = \sum_{j = 1}^\infty 2^{-2j+1} z_{2j-1} \qquad r'(z)= \sum_{j = 1}^\infty 2^{-2j} z_{2j}
%\end{equation}
%where $z \in [0,1]$ has binary expansion $z = \sum_{j=1}^n 2^{-j} z_i $,\footnote{Opting for a non-terminating representation instead of a terminating one when both are available.}
%Then $r(z)$ and $r'(z)$  are independent uniformly distributed  random variables on $\Lambda$.
%\end{lem}

\begin{defn}
\begin{enumerate}
\item Let \textbf{Sample} be the category with objects Borel measure spaces $S = (E, \fB)$, $S' = (E', \fB')$... 
\item For $f,\, f' \colon S \otimes \Lambda \to S'$ define the equivalence relation $f \sim f'$ if 
\begin{equation}\label{equivalence}
 \int_A f(x,y)   \lambda_{\on{Leb}}(\dd y) = \int_A f'(x, y)   \lambda_{\on{Leb}}(\dd y) \qquad \text{for all  $x \in E$ and $A \in \fB'$} 
\end{equation}
Morphisms $f \colon S \rightarrowtriangle S'$ are equivalence classes of measurable functions $f\colon  S \otimes  \Lambda \to  S'$
 under the equivalence relation $\sim$. 
 \item 
 A representative of the identity $\cI_{S,S} $ is $f(x, z) = x$,   $x \in E$. Composition $f'\circ f$ of $f \colon S \rightarrowtriangle S'$ and $f' \colon S' \rightarrowtriangle S''$ is defined for representatives of $f, f'$ by
\[
 (f' \circ f)(x, z) = f'(f(x, r(z)), r'(z)).
\]
 
\item 
The product $\otimes$ is defined on objects by 
 $S \otimes S'$ and a representative of $f \otimes f' \colon S\otimes S' \rightarrowtriangle T\otimes T'$ is given by
\[
(f \otimes f')((x,x'), z) = (f(x, r(z)), f'(x', r'(z)) ).
 \]
 \end{enumerate}
\end{defn}




Suppose $\kappa \colon S \rightarrowtriangle S'$ and $\tilde\kappa \colon S \rightarrowtriangle S'$. % and  $\tilde\kappa \colon S \rightarrowtriangle S'$. 
In forward sampling, a weighted sample $(\varpi, x)$ (with $\varpi$ denoting the weight) is pushed forward to $(\varpi', x')$, where $ \varpi' =  \varpi w_\kappa(m, \delta_x))$ ($m$ is the message sent by $\backw{\tilde\kappa}$) and $x'$ is sampled from $\nu(\dd y)= w_{\kappa}^{-1}(m,\delta_x)\int m(x,y) \kappa(x, \dd y)$. 
By Lemma \ref{lemma: randomisation}, there exists a  map $f_\kappa^m \colon [0,1] \to S'$ such that $x' = f_\kappa^m(z)$. Therefore, there exists a mapping that sends the triple $(\varpi, x, z)$  to $(\varpi', x')$. To obtain compositionality in forward sampling this map is not sufficiently rich: it should also return a new innovation variable $z'$. 

\begin{defn}
Let $m$ be the message sent by $\backw{\tilde\kappa}$, $x$ be a sample with weight $\varpi$ and $z$ be an innovation variable. The {\it forward sampling map} is defined  by
\[
\begin{split}
& \cF^\diamond_\kappa  (m, \varpi, x, z) = (\varpi', x', z') \\
& \varpi' =  \varpi w_\kappa(m, \delta_x) \qquad x'=  f^m_{\kappa} (x, r(z)) \qquad z'= r'(z).
\end{split}
\]
The {\it sampling optic} is defined by  $F^\diamond(\kappa, \tilde\kappa) = \opticd{\kappa}{\tilde\kappa}$. 

\note{Need to add equivalence relation, saying when
\[ F^\diamond(\kappa_1, \tilde\kappa_1) \simeq  F^\diamond(\kappa_2, \tilde\kappa_2). \] }
\end{defn}

\begin{defn}
The composition of two  sampling optics is defined by 
\[
\opticd{\kappa_1}{\tilde\kappa_1}\opticd{\kappa_2}{\tilde\kappa_2}(h,\varpi, x, z) = (\tilde\kappa_1 \tilde\kappa_2 h, \forwdiamond{\kappa_2}(m_2, \forwdiamond{\kappa_1}(m_1,\varpi, x, z)))
\]
where $(m_2, h') = \backw{\tilde\kappa_2}(h)$ and $(m_1, h'')=\backw{\tilde\kappa_1}(h')$.
\end{defn}

\begin{lem}
We have \note{to be shown}
\[	 \opticd{\kappa_1}{\tilde\kappa_1}\opticd{\kappa_2}{\tilde\kappa_2}\simeq \opticd{\kappa_{12}}{\tilde\kappa_{12}}. \]
\end{lem}
\begin{proof}
By definition $\opticd{\kappa_{12}}{\tilde\kappa_{12}}(h,\varpi, x, z)= (\tilde\kappa_{12} h, \forwdiamond{\kappa_{12}}(m_{12},\varpi, x, z))$ where $(m_{12},h)=\backw{\tilde\kappa_{12}}(h)$.  

Verifying that the pullback composes follows directly from the Chapman-Kolmogorov equations. 
If $(\varpi', x', z') = \forwdiamond{\kappa_1}(m_1,\varpi, x, z)$
and $(\varpi'', x'', z'') = \forwdiamond{\kappa_2}(m_2, \forwdiamond{\kappa_1}(m_1,\varpi, x, z))$
then 
\begin{align*} \varpi'' &= \varpi w_{\kappa_1}(m_1,\delta_x)w_{\kappa_2}(m_2,\delta_{x'})
\\ x'' & = f^{m_2}_{\kappa_2}(x', r(r'(z)) ).  \\ z'' &= r'(r'(z)), \end{align*}
where $x'=f_{\kappa_1}^{m_1}(x,r(z))$. 
We need to show that this is equivalent to 
\begin{align*}
	\bar\varpi &=\varpi w_{\kappa_{12}}(m_{12}, \delta_x) \\
	\bar{x} &= f_{\kappa_{12}}^{m_{12}}(x, r(z))\\
	\bar{z} & = r'(z)
\end{align*}
\note{This is not trivial.} For the weights:
\begin{align*} w_{\kappa_{12}}(m_{12}, \delta_x) & = \iint  m_{12}(z,y) \delta_x(\dd z) \kappa_{12}(x,\dd y) 	\\ & = \int m_{12}(x,y) \kappa_{12}(x,\dd y)=
\int m_1(x,z) m_2(z,y) \int \kappa_1(x,\dd z) \kappa_2(z,\dd y) \\ & = \int m_1(x,z) \kappa_1(x,\dd z) w_{\kappa_2}(m_2, \delta_z) 
\end{align*}
where the third equality follows from \eqref{eq:comp m} and the Chapman-Kolmogorov equation. 

Under the equivalence relation on \textbf{Sample} we have $x'' \sim \tilde x$ and $z'' \tilde z$.
\end{proof}


\begin{cor}
The mapping
$F \colon \mathbf{BorelStoch} \to {\on{\bf Optic}}_{\on{\bf Set}}$ defined by $F(\kappa, \tilde\kappa) = \langle \forwcirc\kappa \mid \backw{\tilde\kappa}\rangle$ is a functor.
\end{cor}


\begin{defn}\label{def:parallel optics sampling}
Assume Markov kernels
$\kappa \colon S_i \rightarrowtriangle T_i$ and $\tilde\kappa \colon S_i \rightarrowtriangle T_i$. Let  $h_i \in \mathbf{B}(T_i)$.  Suppose $(\varpi_i, x_i, z_i)\in [0,\infty) \times E_i \times [0,1]$. The {\it parallel product of the optics} $\opticd{\kappa_1}{\tilde\kappa_1}$ with $\opticd{\kappa_2}{\tilde\kappa_2}$ is defined by
\begin{multline}\label{eq:parallel sampling} \left(\opticd{\kappa_1}{\tilde\kappa_1} \otimes \opticd{\kappa_2}{\tilde\kappa_2}\right)(h_1\odot h_2, (\varpi_1, x_1, z_1)\times (\varpi_2, x_2, z_2)) \\ = \left(h_1'\odot h_2', \forwdiamond{\kappa_1}(m_1,\varpi_1, x_1, z_1) \otimes  \forwdiamond{\kappa_2}(m_2,\varpi_2, x_2, z_2)\right), \end{multline}
where $(m_i, h_i')= \backw{\kappa_i}(h_i)$ ($i=1,2$). 
\end{defn}


\begin{thm}
\begin{equation}\label{eq:opticsparallel sampling} \opticd{\kappa_1}{\tilde\kappa_1} \otimes \opticd{\kappa_2}{\tilde\kappa_2} \simeq \opticd{\kappa_1\otimes \kappa_2}{\tilde\kappa_1 \otimes \tilde\kappa_2}. \end{equation} \note{right now, this is all that is in the proof}
$F^\circ(\kappa, \tilde\kappa) = \langle  \forwcirc{\kappa}\mid \backw{\tilde\kappa}\rangle$ defined for
parallel morphisms $\kappa,  \tilde\kappa\colon S \rightarrowtriangle T$ is a strong monoidal (bi-)functor on the category of parallel \textbf{BorelStoch} morphisms into $\on{\textbf{Optic}}_{(\on{\textbf{Set}}, \times, I)}$.
\end{thm}

\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{An example}\label{sec:example}

In Section \ref{sec:compgraph} we presented the computational graph corresponding to Figure \ref{fig:directed_tree}. 
For the backward information filter, we augment this with
\[  \tilde\kappa_{r,t_1} \tilde\kappa_{t_1,t_2}\Dup\, (\tilde\kappa_{t_2,t_3} \otimes \tilde\kappa_{t_3,t_4}) (\tilde\kappa_{t_3,v_1} \otimes \tilde\kappa_{t_4,v_2})  \]
for chosen kernels $\tilde\kappa$. 


Recall the definition of the tensor product $\odot$ in Equation \eqref{eq:tensor_boundedfunctions}.  
The BFFG algorithm is initialised from $g_1 \odot g_2$, where $g_i(x)=\bar{g}_i(x,v_i)$ with
\[ \kappa_{t_3, v_1}(x,\dd y) = \bar{g}_1(x,y)\nu(\dd y) \quad \text{and} \quad \kappa_{t_4, v_2}(x,\dd y) = \bar{g}_2(x,y)\nu(\dd y),  \]
where $\nu$ is a dominating measure. 
Then, BFFG consists of computing
\[  \cO^\diamond(\kappa_{r,t_1} \kappa_{t_1,t_2}\Dup\, (\kappa_{t_2,t_3} \otimes \kappa_{t_3,t_4}),  \tilde\kappa_{r,t_1} \tilde\kappa_{t_1,t_2}\Dup\, (\tilde\kappa_{t_2,t_3} \otimes \kappa_{t_3,t_4})) \]
which, by Theorem \ref{thm:seq} and Theorem \ref{thm:par}, is equivalent to
 \[  \cO^\diamond(\kappa_{r,t_1},\tilde\kappa_{r,t_1}) \cO^\diamond(\kappa_{t_1,t_2}, \tilde\kappa_{t_1,t_2}) \cO^\diamond(\Dup,\Dup) \left(\cO^\diamond(\kappa_{t_2,t_3}, \tilde\kappa_{t_2,t_3}) \otimes \cO^\diamond(\kappa_{t_3,t_4}, \tilde\kappa_{t_3,t_4})\right).
\]
%Then, the backward information filter iteratively propagates the $g$-functions from right to left according to $ \tilde\kappa_{r,t_1} \tilde\kappa_{t_1,t_2}\Dup\, (\tilde\kappa_{t_2,t_3} \otimes \kappa_{t_3,t_4})(g_1\odot g_2)$. 




\bigskip

{\bf Acknowledgment:} We thank Bruno Gavranovi\'c (University of Strathclyde) for helpful discussions. 












%
%\bigskip
%
%
%\begin{figure}
%  \begin{subfigure}[b]{.5\textwidth}
%\begin{tikzpicture}[style={scale=0.6}]
%\tikzstyle{empty}=[fill=white, draw=black, shape=circle,inner sep=1pt, line width=0.7pt]
%\tikzstyle{solid}=[fill=black, draw=black, shape=circle,inner sep=1pt,line width=0.7pt]
%
%\begin{pgfonlayer}{nodelayer}
%		\node [style=solid,label={{\blue $r$}}] (00) at (-7.8, 0) {};
%		\node [style=solid,label={{\blue $0$}}] (0) at (-6, 0) {};
%		\node [style=solid,label={{\blue $5$}},] (5) at (-4, -0.5) {};
%		\node [style=solid,label={{\blue $1$}},] (1) at (-4.5, 2) {};
%		\node [style=empty,label={{\blue $2$}},] (2) at (-3, 1) {};
%		\node [style=solid,label={{\blue $3$}},] (3) at (-3, 3) {};
%		\node [style=solid,label={{\blue  $6$}}] (6) at (-2, -0.5) {};
%		\node [style=empty,label={{\blue $7$}}] (7) at (-0, -0.5) {};
%		\node [style=empty,label={{\blue $4$}},] (4) at (-1, 3) {};
%\end{pgfonlayer}
%
%\begin{pgfonlayer}{edgelayer}
%		\draw   (00) to (0);
%		\draw [style=edge] (0) to (5);
%		\draw [style=edge] (0) to (1);
%		\draw [style=edge] (1) to (2);
%		\draw [style=edge] (1) to (3);
%		
%	
%		
%		\draw [style=edge] (3) to (4);
%		\draw [style=edge] (5) to (6);
%		\draw [style=edge] (6) to (7);
%\end{pgfonlayer}
%\end{tikzpicture}
%\end{subfigure}\hfill
%  \begin{subfigure}[b]{.5\textwidth}
%
%
%\begin{tikzpicture}[style={scale=0.5}]
%	\begin{pgfonlayer}{nodelayer}
%		\node [style=morphism, draw=black] (k1) at (6, -1) {$\kappa_{0,1}$};
%		\node [style=morphism, draw=black] (Delta1) at (6, 1) {{\blue $\Delta$}};
%		\node [style=morphism, draw=black] (k5) at (9, -1) {$\kappa_{0,5}$};
%		\node [style=morphism, draw=black] (k0) at (7, -5) { $\kappa_{r,0}$};
%		\node [style=morphism, draw=black] (Delta0) at (7, -3) {{\blue $\Delta$}};
%		\node [style=none] (bottom) at (7, -7) {};
%		\node [style=morphism, draw=black] (k3) at (5, 2.5) {$\kappa_{1,3}$};
%%		\node [style=morphism] (id) at (7, 1.5) {$id$};
%		\node [style=none] (id) at (7, 2.5) {};
%		\node [style=none] (idright) at (9, 2.5) {};
%		\node [style=morphism, draw=black] (k6) at (9, 1) {$\kappa_{5,6}$};
%		
%		\node [style=morphism, draw=black] (k4) at (5, 4.5) {$\kappa_{3,4}$};
%		\node [style=morphism, draw=black] (k2) at (7, 4.5) {$\kappa_{1,2}$};
%		\node [style=morphism, draw=black] (k7) at (9, 4.5) {$\kappa_{6,7}$};
%	\end{pgfonlayer}
%	\begin{pgfonlayer}{edgelayer}
%		\draw [style=none, draw=black] (bottom) to (k0.center);
%		\draw [style=none, bend right=45, draw=black] (Delta0) to (k5.center);
%	\draw [style=none, bend left=45, draw=black] (Delta0) to (k1.center);
%		\draw [style=none, draw=black] (k0.center) to (Delta0.center);
%	
%	\draw [style=none, draw=black] (k5.center) to (k6.center);
%	\draw [style=none, draw=black] (k1.center) to (Delta1.center);
%		\draw [style=none, draw=black] (k6.center) to (idright.center);
%	\draw [style=none, bend right=25, draw=black] (Delta1) to (id.center);
%
%	\draw [style=none, bend left=45, draw=black] (Delta1) to (k3.center);
%	\draw [style=none, draw=black] (k3.center) to (k4.center);
%	\draw [style=none, draw=black] (id.center) to (k2.center);
%	\draw [style=none, draw=black] (idright.center) to (k7.center);
%
%	\end{pgfonlayer}
%\end{tikzpicture}
%\end{subfigure}
%
%\caption{adfadfasdfa my caption}
%\end{figure}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Some open questions}
%Why assume messages to be bounded? It seems restrictive and unnecessary. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{literature.bib}
\bibliographystyle{plainnat}

%\appendix



\end{document}



%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optics}\label{app: optics}



Essentially an optic is a diagrammatic scheme as shown below
\begin{center}
\begin{tikzpicture}
    \node[vert] (l) at (0, 0) {$\forw{}$};
    \node[vert] (r) at (4, 0) {$\backw{}$};

    \node (S) [left of=l] {$B$};
    \node (A) [below right = 0.7 and 1 of l] {$A$};
    \node (S') [right of=r] {$B'$};
    \node (A') [below left = 0.7 and 1 of r] {$A'$};

    \draw[<-] (S) -- (l);
    \draw[<-] (l) to[out=south east,in=west] (A);

    \draw[->] (S') -- (r);
    \draw[->] (r) to[out=south west,in=east] (A');

    \draw[<-] (l) to[out=north east, in=west] ++(1,1)
     to ++(2,0)
     to[out=east, in=north west] (r)
    ;

    \node[draw,dashed,fit=(A) (A'), inner xsep = 8pt] (box) {};
    \draw[dashed] (box.90) -- +(0,2.25);
\end{tikzpicture}
\end{center}
The circled vertices in the diagram correspond to two maps \[\backw{} \colon B' \to M \times A' \quad \text{and} \quad \forw{} \colon M\times A \to B,\]  representing a backward pass followed by a forward pass. 
They are written symbolically as pair $\optic{}{}$,\footnote{Our optics are right-to-left. In the literature, the same optic would be written $\optic{}{}$ and drawn from left to the right.} where 
 $\backw{}$ 
takes as input a value $b' \in B'$  and produces as output a message $m \in M$ and a value $a' \in A'$. 
The message can be consumed by $\forw{}$ to produce an output $b \in B$ from an input $a \in A$. Here the forward pass depends on the message of the backward pass.


 \cite{Riley2018} defines the category {\bf Optic} with
 \begin{enumerate}
 	\item objects given by pairs $(A, A')$, $(B, B')$;
 	\item arrows $f \colon (A,A') \to (B, B')$ given by  $f=\optic{}{}$.
 \end{enumerate}
% , this category has  of objects in a symmetric monoidal category $(\cD, \otimes, \cI)$
%and morphisms represented by pairs $\optic{}{}$ of morphisms in $\cD$.\footnote{Where $\backw{}\colon B' \rightarrowtriangle  M'\otimes A' $, $\forw{}\colon M'\otimes A \rightarrowtriangle B$ for $M$.} $M$ is interpreted as message (domain). Optics are equivalent modulo changes in representation of the same message. For a morphism $g\colon M \rightarrowtriangle M$ both
%$\langle(g \otimes \id)\forw{} \mid  \backw{}\rangle$ and $\langle \forw{}, \backw{} (g \otimes \id)\rangle$
%are representatives of the same optic and a transformation $g$ of a message can be equivalently applied before or after sending the message ($(g \otimes \id)$ acts on the message alone). 
Suppose $f \colon (A,A') \to (B, B')$ and $g \colon (B,B') \to (C, C')$. Write $f=\optic{f}{f}$ and $g=\optic{g}{g}$.  Then  define composition of optics by 
\[ h:=	f g = g \circ f \colon  (A,A') \to (C,C') \] by 
\[ \backw{h}(c') = (m_1 \otimes m_2, a'),  \]
where $(m_2, b') = \backw{g}(c')$ and $(m_1, a') = \backw{f}(b')$, and \[
\forw{h}(m_1 \otimes m_2, a) := \forw{g}(m_2, \forw{f}(m_1, a)).\]
This 
is best understood in a diagrammatic sense, as shown in the following diagram.
Here, the two optics are composed inserting $\optic{f}{f}$ into the dashed box of  $\optic{g}{g}$,
and connecting the morphism to form new $\forw{h}$ and $\backw{h}$ (dashed circles):

\begin{equation}
    \begin{tikzpicture}[baseline=(R), every node/.style={scale=0.8}]
        \begin{scope}[on grid]
        \node[vert] (l') at (0, 0) {$\forw{g}$};
        \node[vert, below right = 0.7 and 1 of l'] (l) {$\forw{f}$};
        \node[vert] (r') at (5, 0) {$\backw{g}$};
        \node[vert, below left = 0.7 and 1 of r'] (r) {$\backw{f}$};
        \node (A) [below right = 0.7 and 1 of l] {$A$};
        \node (A') [below left = 0.7 and 1 of r] {$A'$};
        \node (R) [left of=l'] {$C$};
        \node (R') [right of=r'] {$C'$};
        \draw[<-] (R) -- (l');
        \draw[->] (R') -- (r');
        \draw[<-] (l') to[out=north east, in=west] ++(1,1)
         to ++(3,0)
         to[out=east, in=north west] (r')
        ;
        \draw[<-] (l) to[out=north east, in=west] ++(1,1)
         to ++(1,0)
         to[out=east, in=north west] (r)
        ;
        \draw[<-] (l') to[out=south east,in=west] (l);
        \draw[<-] (r) to[out=east, in=south west] (r');
        \draw[<-] (l) to[out=south east,in=west] (A);
        \draw[->] (r) to[out=south west,in=east] (A');
        \node[draw,dashed,fit=(l) (r), inner xsep = 16pt, inner ysep = 30pt] (box2) {};
        \node[draw,dashed,fit=(A) (A'), inner xsep = 8pt] (box) {};
        \end{scope}
        \end{tikzpicture}
\quad
    \begin{tikzpicture}[baseline=(R), every node/.style={scale=0.8}]
        \begin{scope}[on grid]
        \node[vert] (l_1) at (0, 0) {};
        \node[vert, below right = 0.7 and 1 of l'] (l_2) {};
        \node[vert] (r_1) at (5, 0) {};
        \node[vert, below left = 0.7 and 1 of r'] (r_2) {};
        \node (C) [below right = 0.7 and 1 of l] {$A$};
        \node (C') [below left = 0.7 and 1 of r] {$A'$};
        \node (R) [left of=l'] {$C$};
        \node (R') [right of=r'] {$C'$};
        \draw[<-] (R) -- (l');
        \draw[->] (R') -- (r');
        \draw[<-] (l') to[out=north east, in=west] ++(1,1)
         to ++(3,0)
         to[out=east, in=north west] (r')
        ;
        \draw[<-] (l) to[out=north east, in=west] ++(1,1)
         to ++(1,0)
         to[out=east, in=north west] (r)
        ;
        \draw[<-] (l') to[out=south east,in=west] (l);
        \draw[<-] (r) to[out=east, in=south west] (r');
        \draw[<-] (l) to[out=south east,in=west] (A);
        \draw[->] (r) to[out=south west,in=east] (A');
        \node[circle,draw,dashed,fit=(l_1) (l_2), inner sep = 3pt, label=110:$\forw{h}$] (l_12) {};
        \node[circle,draw,dashed,fit=(r_1) (r_2), inner sep = 3pt, label=80:$\backw{h}$] (r_12) {};
        \node[draw,dashed,fit=(C) (C'), inner xsep = 8pt] (box) {};
        \end{scope}
        \end{tikzpicture}
\end{equation}



%Sequential (left-to-right) composition of optics, $\optic{1}{1}$ and $\optic{2}{2}$
%%$ \optic{1}{1} \optic{2}{2} = \optic{1,2}{1,2}$
%into a new optic $\optic{1,2}{1,2}$ defined by the morphisms $\forw{1,2}$ and $\backw{1,2}$
%The composition on the left-hand-side is -- as the right hand side illustrates -- another optic
%${\langle \forw{1,2} \mid \backw{1,2} \rangle}$.
%Algebraically, the new morphisms, written as functions, are given by 




















\newpage

These morphisms are denoted as $\optic{}{} \colon (A, A') \hto (B, B')$, with the arrow accented to distinguish from morphisms in the category $\cD$. We write
\begin{equation}\label{eq:modulo}
\langle(g \otimes \id_{A'})\forw{} \mid  \backw{}\rangle = \langle \forw{} \mid \backw{} (g \otimes \id_{B'})\rangle
\end{equation}

For two morphisms $f\colon A \rightarrowtriangle B$, $b\colon B' \rightarrowtriangle A'$, define \note{F: why do we switch notation to $f$, $b$?}
\begin{equation}\label{eq:iota}\iota(f,b) = \langle f \circ \lambda , \lambda^{-1}  \circ b\rangle \end{equation}  as
the optic which applies $b$ and $f$ and sends an empty/void message (here $\lambda$ is the unitor $\lambda\colon \cI \times A \rightarrowtriangle A$, in case of functions as morphisms in {\bf Set}, $\lambda(m, a) = a, m\in I$, $m \in A$). 



Secondly, optics can be applied in parallel. With the product $\otimes$ the category becomes symmetric monoidal, where $(B, B') \otimes (T, T') = (B \otimes T, B' \otimes T')$, the unit object is $(\cI, \cI)$
and $\optic{1}{1} \otimes \optic{2}{2}$ is defined by the diagram
\[
\begin{tikzpicture}[every node/.style={scale=0.8}]
\begin{scope}[on grid]

\node[vert] (l) at (0, 0) {$\forw{1}$};
\node[vert] (r) at (6, 0) {$\backw{1}$};

\node (S) [left of=l] {$B$};
\node (A) [below right = 2 and 2 of l] {$A$};
\node (S') [right of=r] {$B'$};
\node (A') [below left = 2 and 2 of r] {$A'$};

\draw[<-] (S) -- (l);
\draw[<-] (l) to[out=south east,in=west] (A);

\draw[->] (S') -- (r);
\draw[->] (r) to[out=south west,in=east] (A');

\draw[<-] (l) to[out=north east, in=west] ++(1,0.75)
 to ++(4,0)
 to[out=east, in=north west] (r)
;

\node[vert] (l') at (0, -1.5) {$\forw{2}$};
\node[vert] (r') at (6, -1.5) {$\backw{2}$};

\node (T) [left of=l'] {$T$};
\node (B) [below right = 1 and 2 of l'] {$S$};
\node (T') [right of=r'] {$T'$};
\node (B') [below left = 1 and 2 of r'] {$S'$};

\draw[<-] (T) -- (l');
\draw[<-] (l') to[out=south east,in=west] (B);

\draw[->] (T') -- (r');
\draw[->] (r') to[out=south west,in=east] (B');

\draw[<-] (l') 
 to[out=north east, in=west] ++(2,1.5)
 to ++(2,0)
 to[out=east, in=north west] (r')
;

\node[draw,dashed,fit=(A) (A') (B) (B'), inner xsep = 12pt] (box) {};

\end{scope}
\end{tikzpicture}
\end{figure}

Algebraically, the new morphisms, written as functions, are given by 
\begin{equation}
\begin{aligned}
\label{eq: optic product}
\backw{1\times 2}(b' \otimes t') &:= (m_1 \otimes m_2, a'\otimes s') \text { where }  (m_1, a') = \backw1(b'), (m_2, s') = \backw2(t'), \\
\forw{1\times 2}(m_1 \otimes m_2, a\otimes s) &:= \forw2(m_2, a) \otimes \forw1(m_1, s).
\end{aligned}
\end{equation}





\section{On approach inspired by Fong-Spivak-Tuyeras -- backprop as a functor}


Fix $(\kappa, \tilde\kappa)$. Define
\[ A_i = (\bB(S_i), \cM(S_i)) , i \in \NN. \]
{\bf Option 1.} \qquad 
A $\cF\cB$-algorithm $A_1 \to A_2$ is a tuple $(\backw{\tilde\kappa}, \forw{\kappa}, M)$ where $M$ is a set
and $\backw{\tilde\kappa}$ and $\forw{\kappa}$ are operators 
\begin{align*}
 \backw{\tilde\kappa} &\colon  \bB(S_1) \to M \times \bB(S_2) \\	
 \forw{\kappa} & \colon M \times \cM(S_2) \to \cM(S_1) .
\end{align*}
This is alike definition II.1. 
We then define an equivalence relation on $\cF\cB$-algorithms.
Let $(\backw{\tilde\kappa}, \forw{\kappa}, M)$ and  $(\backw{\tilde\kappa}^\T, \forw{\kappa}^\T, M^\T)$ be $\cF\cB$-algorithms of the type $A_1\to A_2$. We consider them to be equivalent if there is a bijection $f\colon M \to M^\T$ such that the following hold for each $(h_1, \mu_1) \in A_1$, $(h_2, \mu_2) \in A_2$:
\begin{align*} &  \pi_2\left(\backw{\tilde\kappa}^\T(h_1)\right) = \pi_2\left(\backw{\tilde\kappa}(h_1)\right) \\
&	\forw{\kappa}^\T(f(m), \mu_2) = \forw{\kappa}(m,\mu_2), 
\end{align*}	
where $\pi_2$ is the projection on the second component. 
%where 	$\backw{\tilde\kappa}(h)=(m, \tilde\kappa h)$ and 
%$\backw{\tilde\kappa}^\T(h) = (f^{-1}(m), \tilde\kappa h)$. 












\section{Introduction}\label{sec:category}

Introducing a categorical perspective is motivated by the admission that we have already used the concepts introduced shortly in all but name in the previous section   and by the following testament: ``We should approach the problem of statistical modelling and
computation in a modular, composable, functional way, guided by underpinning principles from category theory.'' \cite{talk_Wilkinson2017}.

To formalise this setup, we use the optic, a construction from category theory. Optics, originally introduced as Getter/Setters and then generalised, have recently been discovered  as devise to formalise the task of automatic (reverse mode) differentiation of computer programs (communicated by Keno Fischer, see also   \cite{Fischer2020}).

A key element in our string diagrams are Markov kernels. 

\section{Optics and compositional structure of BFFG}\label{sec:opticsBFFG1}

The pair $\optic{}{}$ (with  the backward and forward maps defined in  Definition~\ref{def:backwardmap} and Definition~\ref{def:forwardmap}  respectively)   has a categorical interpretation as morphism in a corresponding category: the category of optics. 
The defining characteristic of a category is its composition rule, in this case specifying how to obtain $\optic{\kappa_{12}}{\tilde\kappa_{12}}$ as the composition of $\optic{\kappa_1}{\tilde\kappa_1}$ and $\optic{\kappa_2}{\tilde\kappa_2}$.

To explain this, consider the kernels $\kappa\colon S \rightarrowtriangle S'$ and $\tilde\kappa\colon S \rightarrowtriangle S'$. The building block of the presented message passing diagrams is given in \eqref{optic}, with $\forw{}=\forw{\kappa}$ and $\backw{}=\backw{\tilde\kappa}$.

A right-to-left {\it optic} $p\colon (\bB(S'), \cM(S')) \rightarrowtriangle (\bB(S), \cM(S))$ is an element of the set of pairs $\optic{}{}$, where $\backw \colon \bB(S') \to M \otimes \bB(S)$ and $\forw\colon M\otimes \cM(S) \to \cM(S')$. Hence, in the diagram, an element $(h', \mu')$ in the domain is mapped to the element $(h,\mu)$ in the codomain.\note{F: I don't fully get the role of domain/codomain for optics.} The space of messages $M$ is a subspace of $\cB(S\times S')$.
We compiled  a short account on the category of optics in Appendix \ref{app: optics}. A key reference is \citep{Riley2018}. 


We may consider the maps $\forw\kappa$ and $\backw{\tilde\kappa}$ as morphisms in the category $\cD \subset (\on{\bf Set}, \times, I).$
Then the pairs ${\langle \forw\kappa \mid \backw{\tilde\kappa} \rangle}$ are morphisms in a corresponding category of right-to-left optics ${\on{\bf Optic}}_{\cD}$ derived from $\cD$.


	




 For BFFG, 
 $\backw{}$ and $\forw{}$ are specified in equations \eqref{eq:backw} and \eqref{eq:forw} respectively. 
We may consider the maps $\forw{}$ and $\backw{}$ as morphisms in the category $\cD \subset \on{\bf Set}.$
Then the pair $\optic{}{}$ is a morphism in a corresponding category of right-to-left optics ${\on{\bf Optic}}_{\cD}$ derived from $\cD$.
In the diagram, the circled vertices in the diagram correspond to two morphisms, and the sources and targets of the morphisms (objects of the category) are annotated in the diagram. 
%(If the morphisms are functions and the category is ${\on{\bf Set}}$, then the sources and targets are domains and codomains.)




 Finally, we define the category \textsf{ForwardBackward} as the category with objects sets $A_i$ and morphisms equivalence classes of $\cF\cB$-algorithms. This is alike Proposition II.4. 

{\color{gray} \small 
{\bf Option 2.} \qquad Define the category \textsf{ForwardBackward} as the category with objects sets $A_i$ and morphisms $A_1 \rightarrowtriangle A_2$ defined as  maps 
\[ f_{\kappa, \tilde\kappa} \colon : \bB(S_1) \times \cM(S_2) \to \bB(S_2) \times \cM(S_1) \]
with 
\[ f_{\kappa, \tilde\kappa}(h_1, \mu_2) = (\tilde\kappa h_1, \mu_1) \qquad \mu_1(\dd y) = \int \frac{h_1(y)}{(\kappa h_1)(x)} \mu_2(\dd x) \kappa(x,\dd y). \]
}

\medskip

Next, define composition, identities and monoidal product, written in terms of representatives, respecting the equivalence relation on $\cF\cB$:

 {\it  Composition.} 
	For $i\in \{1,2\}$ assume Markov kernels
$\kappa_i  \colon S_{i-1}\rightarrowtriangle S_i$ and $\tilde\kappa_i  \colon S_{i-1}\rightarrowtriangle S_i$  

Suppose we have a pair of $\cF\cB$-algorithms
\[ A_0 \stackrel{(\backw{\tilde\kappa_1}, \forw{\kappa_1}, M_1)}{\rightarrowtriangle} A_1 \stackrel{(\backw{\tilde\kappa_2}, \forw{\kappa_2}, M_2)}{\rightarrowtriangle}  A_2.  \]
The composite $\cF\cB$-algorithm $A_0 \rightarrowtriangle A_2$ is defined to be $(\backw{}, \forw{}, M)$ with
\begin{align*}
	\backw{}(h) &= \left(\backw{\tilde\kappa_2} \circ \pi_2 \circ \backw{\tilde\kappa_1}\right)(h) \\
	m & = (m_1, m_2) = \left( (\pi_1 \circ \backw{\tilde\kappa_1})(h), (\pi_1 \circ \backw{})(h) \right) \\
	\forw{}(m,\mu) & = \forw{\kappa_2}\left(m_2,\forw{\kappa_1}(m_1, \mu) \right) \\
	M &= M_1 \times M_2 
\end{align*}
\note{For functioriality we then need to show that $(\backw{}, \forw{}, M)$ is equivalent to $(\backw{\tilde\kappa_{12}}, \forw{\kappa_{12}}, M_1 \times M_2)$. }
	
 {\it Identities.} For each object $A$ we have the identity map 
 \[ (\backw{\id}, \forw{\id}, \RR^0) \colon A \rightarrowtriangle A, \]
with $\id$-the identity in the category of Markov kernels. 

 {\it Monoidal product. } The monoidal product of objects $A$ and $B$ is simply their cartesian product $A \times B$ as sets.  The monoidal product of morphisms $(\backw{1}, \forw{1}, M_1) \colon A_1 \rightarrowtriangle B_1$ and  $(\backw{2}, \forw{2}, M_2) \colon A_2 \rightarrowtriangle B_2$ is defined to be $(\backw{1} \times \backw{2}, \forw{1} \times \forw{2}, M_1 \times M_2)$. 


\begin{prop}[Analogue of Proposition II.4]
There exists a symmetric monoidal category  \textsf{ForwardBackward}  whose objecgts are sets and whose morphisms are equivalence classes of $\cF\cB$-algorithms. 
\end{prop}
\begin{proof}
\emph{Checking associativity}: 	suppose
\[ A_0 \stackrel{(\backw{1}, \forw{1}, M_1)}{\rightarrowtriangle} A_1 \stackrel{(\backw{2}, \forw{2}, M_2)}{\rightarrowtriangle}  A_2 \stackrel{(\backw{3}, \forw{3}, M_3)}{\rightarrowtriangle} A_3.  \]
Denote $g_i = (\backw{i}, \forw{\kappa_i}, M_i)$, we need to show that \[(g_1 \circ g_2) \circ g_3 = g_1 \circ (g_2 \circ g_3).\]
Denote the LHS by $(\backw{\ell}, \forw{\ell}, M_\ell)$ and the RHS by $(\backw{r}, \forw{r}, M_r)$. We have
\begin{align*}
	\backw{\ell}(h) = \left( \backw{3} \circ \pi_2 \circ (\backw{2}\circ \pi_2 \circ \backw{1})\right)(h) \\
			 \left( (\backw{3} \circ \pi_2 \circ \backw{2})\circ \pi_2 \circ \backw{1}\right)(h)  = \backw{r}(h).
\end{align*}
We have $M_\ell \cong M_r$ since 
\[ M_\ell = (M_1 \times M_2) \times M_3 \qquad M_r = M_1 \times (M_2 \times M_3). \]
 
\end{proof}


%%%%%%%%%%%%%%%%%
We now consider ${\langle \forw \mid \backw \rangle}\colon (S, \bB(S)) \hto (T, \bB(T))$ as morphism in ${\on{\bf Optic}_{(\on{\bf Set}, \times, I)}}$. The space of messages $M$ is a subspace of $\cB(S\times T)$.


By \eqref{eq:modulo} we need to find $g$, with
$g\times \id$ invertible on the range of $\backw{\tilde\kappa_1,\tilde\kappa_2}$, such that
\begin{equation}
\begin{split}
 \forw{\kappa_{12}}(m', \mu) &=\forw{\kappa_1,\kappa_2}(g(m'), \mu), \quad m'(z,y) = \frac{ h_2(y)}{\kappa_{12} h_2(z)} \\
\backw{\tilde\kappa_{12}}(h_{2}) & = (g^{-1}(m_1 , m_2), h_0) \text{ where } ((m_1, m_2), h_0) = \backw{\tilde\kappa_1, \tilde\kappa_2}(h_2).
\end{split}
\end{equation}
Define $g(m') =  \Phi^{-1}\circ m_g$ where $\Phi(m_1, m_2)((z, x), (x', y)) =  m_{2}(x',y) m_{1}(z,x)$.

 and $m_g((z, x), (x', y)) =  m'(z, y)\cdot \frac{\tilde\kappa_2 h_2(x')}{\tilde \kappa_2 h_2(x)} $.
For the second relation \note{which is?}, by the first equation of display \eqref{eq:composed},  $\backw{\tilde\kappa_{12}}(h_{2}) = (m', h_0)$. Now
\[
m_g((z, x), (x', y))= \frac{h_2(y)}{(\tilde\kappa_2 h_2)(x)}   \dfrac{(\tilde\kappa_2 h_2)(x')}{(\tilde\kappa_1\tilde\kappa_2 h_2)(z)}  =
m_{2}(x',y) m_{1}(z,x) = \Phi(m_1, m_2)((z, x), (x', y)). \]
Thus $(g(m'), h_0) = ((m_1, m_2), h_0)$ equals $\backw{\tilde\kappa_1, \tilde\kappa_2}(h_2)$ and the relation follows.

To verify the first relation \note{which is?}, define $\nu_1 =\forw{\kappa_1}(m_1, \mu)$ and $\nu_2 	=\forw{\kappa_2}(m_2, \nu_1) $ so 
\[
\nu_2=\forw{\kappa_1, \kappa_2}((m_1, m_2), \mu).
\]
By applying the definition of the forward map in \eqref{eq:forw} twice, we get
\[ \nu_2(\!\dd y) = \int \int m_{2}(x,y) m_{1}(z,x) \mu(\!\dd x) \kappa_1(z, \dd x) \kappa_2(x, \dd y)   \]
and substituting $h_1 = \tilde\kappa_2 h_2$ gives cancellation of terms in the numerator of the second term and denominator of the first term. Hence
\begin{align*} \nu_2(\!\dd y) &= \int \int \frac{h_2(y)}{(\tilde\kappa_1 \tilde\kappa_2 h_2)(z) }\kappa_1(z, \dd x) \kappa_2(z, \dd y) \mu(\!\dd z) \\ & = \int  \frac{h_2(y)}{(\tilde\kappa_{12} h_2)(z) } \kappa_{12}(z,\dd y) \mu(\!\dd z) \\ & = \int m' (z,y) \kappa_{12}(z,\dd y) \mu(\!\dd z). \end{align*}
 This is exactly as in the second equation of display \eqref{eq:composed}.




%
%Kernels can be composed sequentially and in parallel, and those compositions make up the structure of a probabilistic program.
%In this section we show that the optics can be composed likewise, and that composition of optics (in sequentially and in parallel) preserves the structure of the program.  %Not a directed tree, this requires sampling the forward pass. 
%

% For the $h$-transform, 
% $\backw{}$ and $\forw{}$ are specified in equations \eqref{eq:backw} and \eqref{eq:forw} respectively. 
%We may consider the maps $\forw{}$ and $\backw{}$ as morphisms in the category $\cD \subset \on{\bf Set}.$


\subsection{Optics and compositional monoidal structure of BFFG}\label{subsec:opticsBFFG2}

The  symmetric monoidal structure on {\bf BorelStoch}  is given by product measure spaces at the level of objects, and the 
product Markov kernel $\kappa \otimes \kappa'$ at the level of morphisms.


{\bf BorelStoch} is a symmetric monoidal category extended by a comultiplication, that is identically duplicating something,  $\Dup \colon S \rightarrowtriangle S \otimes S$ 
  see \cite{Fritz2020} and \cite{jacobs2019structured}.\footnote{and a counit  $\on{del} \colon S \rightarrowtriangle \cI$, for each  object $(E, \fB)$, that is forgetting something. } $\cI$ is the monoidal unit object. \note{F: is this $\id \otimes \id$? It's an object} See also \cite{Panangaden1999}.


\noteout{What is missing is obtaining the optic of a product kernel as product of the optics of its parts. The caveat is that in the backward pass we require backward marginalisation. In the forward pass, we can apply sampling.
The difficulty is how to replace the optic of a product kernel with its product of optics within sequential composition. 
We treat this topic in appendix \ref{app: product}.


The product structure for Markov kernels extends to their application as pullbacks. Recall the product $h \odot h'$ defined in \eqref{eq:product_h}. 
 Define the corresponding pointwise product for messages by
\[
(m{\,\tilde \odot\,} m')((x,x'),(y,y')) = m'(x,y) \cdot m(x',y') .
\]
We occasionally use $\otimes$ as postfix operator (as it acts on products of measures) and $\odot$ as prefix operator (as it acts on functions.)
The following lemma shows product rules for the forward and backward map obtained from a product of kernels. 
}

%\begin{lem}\label{lemma: sun}
%Let $h \in \mathbf{B}(T)$ $h' \in \mathbf{B}(T')$ and assume Markov kernels
%$\kappa \colon S \rightarrowtriangle T$, $\kappa  \colon S' \rightarrowtriangle T'$. Suppose 
%$\mu \in \cM(S)$, $\mu' \in \cM(S')$, 
%$m \in \bB(S\otimes T)$ and $m' \in \bB(S'\otimes T')$.
%Then
%\begin{align*}
%(\kappa \otimes \kappa')(h \odot h')   &= (\kappa h) \odot (\kappa' h')\\
%\backw{\kappa \otimes \kappa'}( h \odot h') &= \Phi(\backw{\kappa }( h ), \backw{\kappa'}( h')) \\ 
%\forw{\kappa \otimes \kappa'}( m {\,\tilde \odot\,}  m', \mu\otimes \mu') & = \forw{\kappa}( m , \mu) \otimes \forw{\kappa'}( m',   \mu')
%\end{align*}
%where  $\Phi((m, h) ,  (m', h')) = (m {\,\tilde \odot\,}  m', h \odot h')$. 
%\end{lem}
%\begin{proof}
%The first statement follows from Fubini's theorem. If \[m(x,y) = \dfrac{h(y) }{(\kappa h)(x)} \quad \text{and}\quad  m'(x,y) = \dfrac{h'(y) }{(\kappa' h')(x)},\] then 
%\[ \backw{\kappa \otimes \kappa'}( h \odot h') = (m {\,\tilde \odot\,}  m', (\kappa \otimes \kappa')(h \odot h')) = (m {\,\tilde \odot\,}  m', (\kappa h) \odot (\kappa' h')). \]
%Further, if $\bar\nu = \forw{\kappa \otimes \kappa'}( m{\,\tilde \odot\,} m', \mu\otimes \mu')$, then
%\begin{align*} \bar\nu(\!\dd (y,y')) &= \int m(x,y)m(x',y') (\mu \otimes \mu')(\!\dd (x,x')) (\kappa \otimes \kappa')((x,x'), \dd (y, y')) \\ &= \nu(\!\dd y) \nu'(\!\dd y') = (\nu \otimes \nu')(\!\dd (y ,  y')),\end{align*}
%where $\nu = \forw\kappa(m, \mu)$ and $\nu'=\forw{\kappa'}(m', \mu')$. 
%\end{proof}

\noteout{
We need to show \url{https://en.wikipedia.org/wiki/Monoidal_functor}
So we need to consider for example
\[ \left(\langle \forw{\kappa_1} \mid \backw{\tilde\kappa_1}\rangle \otimes  \langle \forw{\kappa_2} \mid \backw{\tilde\kappa_2}\rangle \right) \otimes \langle \forw{\kappa_3} \mid \backw{\tilde\kappa_3}\rangle  \]
and
\[	\langle \forw{\kappa_1\otimes \kappa_2} \mid \backw{\tilde\kappa_1\otimes \tilde\kappa_2}\rangle \otimes \langle \forw{\kappa_3} \mid \backw{\tilde\kappa_3}\rangle \]
and
\[	\langle \forw{(\kappa_1\otimes \kappa_2) \otimes \kappa_3} \mid \backw{(\tilde\kappa_1\otimes \tilde\kappa_2) \otimes \tilde\kappa_3}\rangle  \]
}



\subsection{Optics and compositional monoidal structure of BFFG under sampling}

The monoidal structure in Definition \ref{def:parallel optics} only applies to pushing forward product measures in $\forw\kappa$. 
Rather than pushing forward a measure using the map $\forw{}$, one can also push forward a weighted sample, which does not suffer from such restrictions. Moreover, in Bayesian computation this is the more interesting setting. To define sampling  within the language of category we need the following result.
\begin{lem}[\cite{kallenberg2002foundations}, p.~56]\label{lemma: randomisation}
For a Markov kernel $\kappa$ between a measure space $(E, \fB)$ and a Borel space $(E', \fB')$, there is a measurable function $f_\kappa\colon E\times [0,1] \to E'$  and a random variable $Z \sim U[0,1]$ such that the random variable $f_\kappa(x, Z)$ has law $\kappa(x, \cdot)$ for each $x \in E$. The random variable $Z$ is  called an \emph{innovation variable}.

Further, if $\kappa'$ is a kernel with source   $(E', \fB')$ into a third Borel space, there exist independent random variables  $Z,\, Z' \sim U[0,1]$ such that  $f_{\kappa'}(f_\kappa(x, Z), Z') \sim (\kappa\kappa')(x, \cdot)$. \note{Rather than $f$, we could write $\sigma$ to highlight "sampling"}
\end{lem}
The maps $f_\kappa$ are the morphisms of the monoidal category \textbf{Sample}. Exact definitions on objects, morphisms, identity, rule of composition and tensor product are summarised in Section  \ref{app: action}. 

Suppose $\kappa \colon S \rightarrowtriangle S'$ and $\tilde\kappa \colon S \rightarrowtriangle S'$. % and  $\tilde\kappa \colon S \rightarrowtriangle S'$. 
In forward sampling, a weighted sample $(\varpi, x)$ (with $\varpi$ denoting the weight) is pushed forward to $(\varpi', x')$, where $ \varpi' =  \varpi w_\kappa(m, \delta_x))$ ($m$ is the message sent by $\backw{\tilde\kappa}$) and $x'$ is sampled from $\nu(\dd y)= w_{\kappa}^{-1}(m,\delta_x)\int m(x,y) \kappa(x, \dd y)$. 
By Lemma \ref{lemma: randomisation}, there exists a  map $f_\kappa^m \colon [0,1] \to S'$ such that $x' = f_\kappa^m(z)$. Therefore, there exists a mapping that sends the triple $(\varpi, x, z)$  to $(\varpi', x')$. To obtain compositionality in forward sampling this map is not sufficiently rich: it should also return a new innovation variable $z'$. For this, we can use the following result which proof  follows along the lines of  Lemma 3.21 in \cite{kallenberg2002foundations}. 
\begin{lem}
Denote  $\Lambda = ([0,1], \fB([0,1]))$ and assume it to be equipped with Lebesgue measure $\lambda_{\on{Leb}}$.   Define the reproduction functions  $r \colon  \to \Lambda$ and  $r' \colon  \to \Lambda$ by
\begin{equation}\label{eq:r}
r(z) = \sum_{j = 1}^\infty 2^{-2j+1} z_{2j-1} \qquad r'(z)= \sum_{j = 1}^\infty 2^{-2j} z_{2j}
\end{equation}
where $z \in [0,1]$ has binary expansion $z = \sum_{j=1}^n 2^{-j} z_i $,\footnote{Opting for a non-terminating representation instead of a terminating one when both are available.}
Then $r(z)$ and $r'(z)$  are independent uniformly distributed  random variables on $\Lambda$.
\end{lem}


\begin{defn}
Let $m$ be the message sent by $\backw{\tilde\kappa}$, $x$ be a sample with weight $\varpi$ and $z$ be an innovation variable. The {\it forward sampling map} is defined  by
\[
\begin{split}
& \cF^\diamond_\kappa  (m, \varpi, x, z) = (\varpi', x', z') \\
& \varpi' =  \varpi w_\kappa(m, \delta_x) \qquad x'=  f^m_{\kappa} (x, r(z)) \qquad z'= r'(z).
\end{split}
\]
The {\it sampling optic} is defined by  $F^\diamond(\kappa, \tilde\kappa) = \opticd{\kappa}{\tilde\kappa}$. 

\note{Need to add equivalence relation, saying when
\[ F^\diamond(\kappa_1, \tilde\kappa_1) \simeq  F^\diamond(\kappa_2, \tilde\kappa_2). \] }
\end{defn}
Note that $F^\diamond(\kappa, \tilde\kappa)$ acts upon $(h, \varpi, x, z)$. 


The following definition is alike definition \ref{def:composition optics}. 
\begin{defn}
The composition of two  sampling optics is defined by 
\[
\opticd{\kappa_1}{\tilde\kappa_1}\opticd{\kappa_2}{\tilde\kappa_2}(h,\varpi, x, z) = (\tilde\kappa_1 \tilde\kappa_2 h, \forwdiamond{\kappa_2}(m_2, \forwdiamond{\kappa_1}(m_1,\varpi, x, z)))
\]
where $(m_2, h') = \backw{\tilde\kappa_2}(h)$ and $(m_1, h'')=\backw{\tilde\kappa_1}(h')$.
\end{defn}

\begin{lem}
We have \note{to be shown}
\[	 \opticd{\kappa_1}{\tilde\kappa_1}\opticd{\kappa_2}{\tilde\kappa_2}\simeq \opticd{\kappa_{12}}{\tilde\kappa_{12}}. \]
\end{lem}
\begin{proof}
By definition $\opticd{\kappa_{12}}{\tilde\kappa_{12}}(h,\varpi, x, z)= (\tilde\kappa_{12} h, \forwdiamond{\kappa_{12}}(m_{12},\varpi, x, z))$ where $(m_{12},h)=\backw{\tilde\kappa_{12}}(h)$.  

Verifying that the pullback composes follows directly from the Chapman-Kolmogorov equations. 
If $(\varpi', x', z') = \forwdiamond{\kappa_1}(m_1,\varpi, x, z)$
and $(\varpi'', x'', z'') = \forwdiamond{\kappa_2}(m_2, \forwdiamond{\kappa_1}(m_1,\varpi, x, z))$
then 
\begin{align*} \varpi'' &= \varpi w_{\kappa_1}(m_1,\delta_x)w_{\kappa_2}(m_2,\delta_{x'})
\\ x'' & = f^{m_2}_{\kappa_2}(x', r(r'(z)) ).  \\ z'' &= r'(r'(z)), \end{align*}
where $x'=f_{\kappa_1}^{m_1}(x,r(z))$. 
We need to show that this is equivalent to 
\begin{align*}
	\bar\varpi &=\varpi w_{\kappa_{12}}(m_{12}, \delta_x) \\
	\bar{x} &= f_{\kappa_{12}}^{m_{12}}(x, r(z))\\
	\bar{z} & = r'(z)
\end{align*}
\note{This is not trivial.} For the weights:
\begin{align*} w_{\kappa_{12}}(m_{12}, \delta_x) & = \iint  m_{12}(z,y) \delta_x(\dd z) \kappa_{12}(x,\dd y) 	\\ & = \int m_{12}(x,y) \kappa_{12}(x,\dd y)=
\int m_1(x,z) m_2(z,y) \int \kappa_1(x,\dd z) \kappa_2(z,\dd y) \\ & = \int m_1(x,z) \kappa_1(x,\dd z) w_{\kappa_2}(m_2, \delta_z) 
\end{align*}
where the third equality follows from \eqref{eq:comp m} and the Chapman-Kolmogorov equation. 

Under the equivalence relation on \textbf{Sample} we have $x'' \sim \tilde x$ and $z'' \tilde z$.
\end{proof}


\begin{cor}
The mapping
$F \colon \mathbf{BorelStoch} \to {\on{\bf Optic}}_{\on{\bf Set}}$ defined by $F(\kappa, \tilde\kappa) = \langle \forwcirc\kappa \mid \backw{\tilde\kappa}\rangle$ is a functor.
\end{cor}


\begin{defn}\label{def:parallel optics sampling}
Assume Markov kernels
$\kappa \colon S_i \rightarrowtriangle T_i$ and $\tilde\kappa \colon S_i \rightarrowtriangle T_i$. Let  $h_i \in \mathbf{B}(T_i)$.  Suppose $(\varpi_i, x_i, z_i)\in [0,\infty) \times E_i \times [0,1]$. The {\it parallel product of the optics} $\opticd{\kappa_1}{\tilde\kappa_1}$ with $\opticd{\kappa_2}{\tilde\kappa_2}$ is defined by
\begin{multline}\label{eq:parallel sampling} \left(\opticd{\kappa_1}{\tilde\kappa_1} \otimes \opticd{\kappa_2}{\tilde\kappa_2}\right)(h_1\odot h_2, (\varpi_1, x_1, z_1)\times (\varpi_2, x_2, z_2)) \\ = \left(h_1'\odot h_2', \forwdiamond{\kappa_1}(m_1,\varpi_1, x_1, z_1) \otimes  \forwdiamond{\kappa_2}(m_2,\varpi_2, x_2, z_2)\right), \end{multline}
where $(m_i, h_i')= \backw{\kappa_i}(h_i)$ ($i=1,2$). 
\end{defn}


\begin{thm}
\begin{equation}\label{eq:opticsparallel sampling} \opticd{\kappa_1}{\tilde\kappa_1} \otimes \opticd{\kappa_2}{\tilde\kappa_2} \simeq \opticd{\kappa_1\otimes \kappa_2}{\tilde\kappa_1 \otimes \tilde\kappa_2}. \end{equation} \note{right now, this is all that is in the proof}
$F^\circ(\kappa, \tilde\kappa) = \langle  \forwcirc{\kappa}\mid \backw{\tilde\kappa}\rangle$ defined for
parallel morphisms $\kappa,  \tilde\kappa\colon S \rightarrowtriangle T$ is a strong monoidal (bi-)functor on the category of parallel \textbf{BorelStoch} morphisms into $\on{\textbf{Optic}}_{(\on{\textbf{Set}}, \times, I)}$.
\end{thm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following string diagram illustrates the message flow in the particular example of the `probabilistic program' $(\kappa_{1a} \otimes \kappa_{1b}) \kappa_2$:  

\begin{equation}
%\begin{center}
\begin{tikzpicture}[scale=0.85, every node/.style={scale=0.85}]
	\begin{pgfonlayer}{nodelayer}
		\node  (0) at (-1.5, 2.5) {};
		\node [style=vert] (1) at (-2.5, 4) {$\forwcirc{\kappa_{\scriptstyle 1b}}$};
		\node [style=vert] (1b) at (-3.5, 4) {$\forwcirc{\kappa_{\scriptstyle 1a}}$};
		\node (2) at (-4.5, 5.5) {};
		\node [style=vert] (5) at (-5.5, 7) {$\forwcirc{\kappa_{\scriptstyle 2}}$};
		\node [style=vertblank] (6) at (1.5, 2.5) {$\odot$};
		\node [style=vert] (7) at (2.5, 4) {$\backw{\kappa_{\scriptstyle 1a}}$};
		\node [style=vert] (7b) at (3.5, 4) {$\backw{\kappa_{\scriptstyle 1b}}$};
		\node [style=vertblank] (8) at (4.5, 5.5) {$\;\odot^{-1}$};
		\node [style=vert] (10) at (5.5, 7) {$\backw{\tilde\kappa_{\scriptstyle 2}}$};
		\node (11) at (-0.5, 1.5) {};
		\node  (12) at (0.5, 1.5) {};
 		\node [style=vertblank] (in) at (6.5, 8.5) {$h_{2}$};
		\node [style=vertblank] (out) at (-6.5, 8.5) {$(\varpi_2, X^\circ_2)$};
	\end{pgfonlayer}
	\begin{pgfonlayer}{edgelayer}
		\draw[double,->]  [in=30, out=-150] (10) to  node[midway,right] {$h_{1}$}  (8);
		\draw[->]  [in=30, out=-180] (8) to node[midway,left] {$^{\textstyle h_{1a}}$}  (7);
		\draw[->]  [in=30, out=-135] (8) to node[midway,right] {$h_{1b} $}  (7b);
		\draw[->]  [in=30, out=-150] (7) to node[midway,left] {$h_{0a}$} (6);
		\draw[->]  [in=0, out=-150] (7b) to node[midway,right] {$h_{0b}$} (6);
		\draw[double,->]  [in=45, out=-150] (6) to node[midway,right] {$h_{0}$} (12.center);
		\draw[double]  [in=-30, out=150] (11) to node[midway,left] {$(\varpi_0, X^\circ_0)$}   (0.center);
		\draw[->]  [in=-45, out=150] (0.center) to node[midway,right] {$(\varpi_0^{\frac12}, X^\circ_{0b})$}  (1);
		\draw[->]  [in=-45, out=180] (0.center) to node[midway,left] {$(\varpi_0^{\frac12}, X^\circ_{0a})$}  (1b);
		\draw  [in=-45, out=135] (1) to  node[midway,right] {$ $} (2.center);
		\draw  [in=-45, out=150] (1b) to  node[midway,left] {$ $} (2.center);
		\draw[double,->]  [in=-30, out=150] (2.center) to  node[near start,left] {$(\varpi_{1a}\varpi_{1b}, (X^\circ_{1a},X^\circ_{1b}))$} (5)  ;
		\draw[->]  [in=-30, out=150] (5) to  (out);
		\draw[->]  [in=30, out=-150] (in) to  (10);
%		 \draw[->]   (20) to  (5);
		
		\draw[->]   (7b)   to[out=120,in=0] ++(-1,+1)  -- ($(1) + (1,+1)$) to[out=180,in=45]  (1)  node[midway,above] {$m_{1b}$};
		\draw[->]   (7)   to[out=150,in=0] ++(-1.5,+1.5)  -- ($(1) + (1,+1.5)$) to[out=180,in=30]  (1b)  node[midway,above] {$m_{1a}$};
		\draw[->]   (10)   to[out=120,in=0] ++(-1,+1) -- ($(5) + (1,+1)$) to[out=180,in=45] (5)   node[midway,above] {$m_{2}$};
	\end{pgfonlayer}

\end{tikzpicture}
%\end{center}
\end{equation} 
 

 


%%%%%%%%%%%%%%%%
\section{Actions of Markov kernels}\label{app: actions}

 A Markov kernel in \textbf{BorelStoch} can act as pushforward on a measure  and as pullback on a function (note that these are not pushforward and pullback as defined in category theory).
 
  These actions constitute two functors $G$ (pushforward of measures) and $H$ (pullback of bounded measurable functions) from \textbf{BorelStoch} into the category  $(\on{\bf Set}, \times, I)$ with monoidal unit $I$ \note{Leave out here? (we denote functions $f\colon A \to B$ seen as morphisms in \textbf{Set} also by $f\colon A \rightarrowtriangle B$.) }
 
 
More precisely, if $S$ and $T$ are Borel spaces, then   $G$ is the functor from \textbf{BorelStoch} to  \textbf{Set} with 
\begin{align*}
  \text{objects:} &\quad  \text{$\cM(S)$, the set of bounded measures on  $S$} \\
  \text{morphisms:} &\quad  \text{functions $G(\kappa)\colon \cM(S)  \rightarrowtriangle \cM(T)$ with $[G(\kappa)](\mu) = \mu\kappa$ (equation \eqref{eq: pushforward})}	
\end{align*}
and $H$ is the functor $H$ from \textbf{BorelStoch} to  \textbf{Set} with 
\begin{align*}
  \text{objects:} &\quad  \text{$\bB(S)$, the set of bounded measurable functions on  $S$} \\
  \text{morphisms:} &\quad  \text{functions $H(\kappa)\colon \bB(S)  \rightarrowtriangle \bB(T)$ with $[H(\kappa)](h)= \kappa h$ %(equation \eqref{eq: pullback})}	
(pullback.)}	
\end{align*}


Before  we denoted $G(\kappa)$ and $H(\kappa)$ by $\kappa$ as well.  Note that $G(\on{del})$, the counit as Markov kernel acting on a measure, preserves one piece of information: the weight, and algorithmically, in sums over the weights these weights may need to be included.


From  Lemma \ref{lemma: sun} we obtain the following proposition (communicated by Evan Patterson).
\begin{prop}\label{prop: lax}
\begin{enumerate}[label=(\roman*)]
\item
Product measures can be pushed forward in parallel by parallel kernels:
Write $S = (E, \fB)$, $S' = (E', \fB')$, $T = (F, \fC)$, $T' = (F', \fC')$ etc.
The functor $G$ equipped with the natural transformation $\otimes_{S,S'} \colon G(S) \times G(S')  \rightarrowtriangle G(S \otimes S')$ which takes pairs of measures to their product 
 is a lax monoidal functor from $(\on{\bf BorelStoch}, \otimes, \cI)$ to  $(\on{\bf Set}, \times, I)$.

\item
Product form functions $h$ can be pulled back in parallel by parallel kernels: 
The functor $H$ equipped with the natural transformation $\odot_{S,S'} \colon H(S) \times H(S')  \rightarrowtriangle H(S\otimes S')$ which takes pairs of functions to their pointwise product 
 is a lax monoidal functor from $(\on{\bf BorelStoch}^{\on{op}}, \otimes, \cI)$ to  $(\on{\bf Set}, \times, I)$.
 \end{enumerate}
\end{prop}
\begin{proof}See appendix \ref{sec:proofs}.\end{proof}



Though one cannot push forward a measure which is not of product form in parallel, one can after marginalisation (with Proposition \ref{prop: marginalisation}), as illustrated by the following diagram: 
\begin{center}
\begin{tikzcd}[ampersand replacement=\&]
G(S\otimes S')  \arrow[r, "M_{S,S'}"] \arrow[d, "G(\kappa_1\otimes \kappa_2) "']
  \& M(G(S\otimes S')) \arrow[d, " G(\kappa_1\otimes \kappa_2) "] \arrow{r}{\otimes^{-1}_{S,S'}}[swap]{\simeq} \& G(S) \times G(S')  \arrow[d, "G(\kappa_1)\times G(\kappa_2) "] \\
G(T\otimes T')  \arrow[r, "M_{T,T'}"] \& M(G(T\otimes T'))  \arrow{r}{\otimes^{-1}_{T,T'}}[swap]{\simeq}  \& G(T) \times G(T') 
\end{tikzcd}
\end{center}

Likewise, one  cannot pull backward a function $h$ which is not of product form in parallel, but one can after backward marginalisation (under the assumptions of Proposition \ref{prop: backmarginalisation}), as shown by the following diagram: 
\begin{center}
\begin{tikzcd}
H(T\otimes T')  \arrow[r, "M^*_{T,T'}"] \arrow[d, "H(\kappa_1\otimes \kappa_2)"']
  & M^*(H(T\otimes T')) \arrow[d, "H(\kappa_1\otimes\kappa_2)"] \arrow{r}{\odot^{-1}_{T,T'}}[swap]{\simeq}& H(T) \times H(T') \arrow[d, "H(\kappa_1)\times H(\kappa_2)\hspace{0.5cm} "] \\
H(S\otimes S')  \arrow[r, "M^*_{S,S'}"] & M^*(H(S\otimes S')) \arrow{r}{\odot^{-1}_{S,S'}}[swap]{\simeq}   & H(S) \times H(T') 
\end{tikzcd}
\end{center}


\section{Actions and sampling}\label{app: action}
 A Markov kernel in \textbf{BorelStoch} can act as pushforward on a measure  and as pullback on a function (note that these are not pushforward and pullback as defined in category theory).
 
  These actions constitute two functors $G$ (pushforward of measures) and $H$ (pullback of bounded measurable functions) from \textbf{BorelStoch} into the category  $(\on{\bf Set}, \times, I)$ with monoidal unit $I$.

Product form functions $h$ can be pulled back in parallel by parallel kernels: 
The functor $H$ equipped with the natural transformation $\odot_{S,S'} \colon H(S) \times H(S')  \rightarrowtriangle H(S\otimes S')$ which takes pairs of functions to their pointwise product  is a lax monoidal functor from $(\on{\bf BorelStoch}^{\on{op}}, \otimes, \cI)$ to  $(\on{\bf Set}, \times, I)$. \emph{Lax} reflects that $\otimes$ lacks invertibility on $S\otimes S'$ and  $\odot$ lacks invertibility on $H(S)\odot H(S')$.   See appendix \ref{app: actions}.

Though one cannot pull backward a function $h$ which is not of product form in parallel, one can after backward marginalisation (under the assumptions of Proposition \ref{prop: backmarginalisation}).


%The limitation in pushing forward product measures does not apply to sampling: one can sample forward in parallel from a sample of a joint distribution. \note{I think this line will only raise questions, better leave it out}


Recall the definition~of $\kappa \mapsto f_\kappa$ from definition~\ref{lemma: randomisation}.
Such randomisations  (or  randomness pushback in the terminology of \cite{Fritz2020})   form a  monoidal category \textbf{Sample}. Denote  $\Lambda = ([0,1], \fB([0,1]))$ and assume it to be equipped with Lebesgue measure $\lambda_{\on{Leb}}$.  Our construction of a randomization relies on the existence of reproduction functions  $r, r'\colon \Lambda \to \Lambda$ such that $r$ and $r'$  are independent uniformly distributed  random variables on $\Lambda$. If $z \in [0,1]$ has binary expansion $z = \sum_{j=1}^n 2^{-j} z_i $,\footnote{Opting for a non-terminating representation instead of a terminating one when both are available.}
\begin{equation}\label{eq:r and rprime}
r(z) =\sum_{j = 1}^\infty 2^{-2j+1} z_{2j-1},  \quad r'(z) =  \sum_{j = 1}^\infty 2^{-2j} z_{2j}.
\end{equation}
The proof follows along the lines of  Lemma 3.21 in \cite{kallenberg2002foundations}.

\begin{defn}
\begin{enumerate}
\item Let \textbf{Sample} be the category with objects Borel measure spaces $S = (E, \fB)$, $S' = (E', \fB')$... 
\item For $f,\, f' \colon S \otimes \Lambda \to S'$ define the equivalence relation 
\begin{equation}\label{equivalence}
f \sim f' \quad \Leftrightarrow \quad \int_A f(x,y)   \lambda_{\on{Leb}}(\dd y) = \int_A f'(x, y)   \lambda_{\on{Leb}}(\dd y) \qquad \text{for $x \in E$ and $A \in \fB'$} 
\end{equation}
Morphisms $f \colon S \rightarrowtriangle S'$ are equivalence classes of measurable functions $f\colon  S \otimes  \Lambda \to  S'$
 under the equivalence relation $\sim$. 
 \item 
 A representative of the identity $\cI_{S,S} $ is $f(x, z) = x$,   $x \in E$. Composition $f'\circ f$ of $f \colon S \rightarrowtriangle S'$ and $f' \colon S' \rightarrowtriangle S''$ is defined for representatives of $f, f'$ by
\[
 (f' \circ f)(x, z) = f'(f(x, r(z)), r'(z)).
\]
 
\item 
The product $\otimes$ is defined on objects by 
 $S \otimes S'$ and a representative of $f \otimes f' \colon S\otimes S' \rightarrowtriangle T\otimes T'$ is given by
\[
(f \otimes f')((x,x'), z) = (f(x, r(z)), f'(x', r'(z)) ).
 \]
 \end{enumerate}
\end{defn}

 
\begin{prop}
The functor $J\colon \kappa \mapsto f_\kappa $  is a \emph{strong} monoidal functor.
\end{prop}
\begin{proof}
 For $\kappa\colon S \rightarrowtriangle T$,  $\kappa'\colon S' \rightarrowtriangle T'$ 
the following diagram commutes, the natural transformation is  the identity isomorphism.
\begin{center}
\begin{tikzcd}
\arrow[d, "J(\kappa)\otimes J(\kappa')"']
J(S) \otimes J(S') \arrow[r,  "\simeq"']  & J(S  \otimes S')  \arrow[d, "J(\kappa \otimes \kappa')"] \\
J(T) \otimes J(T') \arrow[r,  "\simeq"'] 
& J(T) \otimes J(T')
\end{tikzcd}
\end{center}
Here $\simeq$ denotes isomorphisms.
The remaining conditions for strong monoidal functors (cf.~the entry \emph{monoidal functor} in \cite{nlab:homepage}) are easily checked.
\end{proof}


This means: as long as we are sampling, if $\kappa = \kappa_a \otimes \kappa_b$ we may freely switch between sampling $f_\kappa((x_a, x_b), Z)$ and creating a pair from $f_{\kappa_a}(x_a, Z_a)$ and $f_{\kappa_b}(x_b, Z_b)$ sampled in parallel with independent uniform innovations $Z_a = r(Z), Z_b = r'(Z)$ up to equivalence with respect to \eqref{equivalence}.











