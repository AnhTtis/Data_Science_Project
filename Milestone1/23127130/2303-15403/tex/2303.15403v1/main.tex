\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


%my package
% \usepackage{kotex}
\usepackage{xcolor}
\usepackage{floatpag}
\usepackage{enumitem}
\usepackage{cuted}
\usepackage{multirow}
\usepackage{caption}
\usepackage{lipsum}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\usepackage{tocloft}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\input{macros}
% \usepackage{color}
% \definecolor{mycolor}{rgb}{0,0,0} % Define black color

% \setcounter{minitocdepth}{1}
% \setcounter{\secttocdepth}{1}
\setcounter{parttocdepth}{1}
% \mtcsettitle{parttoc}{}
% \mtcsetfeature{parttoc}{pagenumbers}{off}
% \mtcsetfeature{parttoc}{nobreak}
% \renewcommand{\ptcfont}{}{\color{black}}
% \renewcommand{\ptcSfont}{\color{mycolor}\normalfont}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{5774\vspace{-3em}} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newtheorem{assumption}{Assumption}
% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}
\renewcommand{\cftsecpagefont}{\color{black}\normalfont}
\doparttoc
\faketableofcontents
%%%%%%%%% TITLE
\title{Training-free Style Transfer Emerges from h-space in Diffusion models\vspace{-0.5em}}

\author{Jaeseok Jeong$^*$\\
 \\
% Institution1 address\\
% {\tt\small jete\_jeong@yonsei.ac.kr}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Mingi Kwon$^*$\\
Yonsei University\\
% First line of institution2 address\\
% {\tt\small kwonmingi@yonsei.ac.kr}
\and
Youngjung Uh\\
 \\
% First line of institution3 address\\
% {\tt\small yj.uh@yonsei.ac.kr}
}


\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}
% \def\thefootnote{*}\footnotetext{Equal contribution}
 
\begin{strip}
\centering
  \includegraphics[width=0.9\textwidth]{./figures/teaser.pdf}
      \vspace{-0.3em}
    \captionof{figure}{
    \ours{} allows (a) style mixing by content injection within the trained domain, (b) local style mixing by injecting masked content features, and (c) harmonization-like style transfer with out-of-domain style references. All results are produced by frozen pretrained diffusion models.
    Furthermore, flexibility of \ours{} enables content injection into any style.}
    \vspace{-0.5em}
    \label{fig:teaser}
    
\end{strip}


% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}\vspace{-0.8em}
Diffusion models (DMs) synthesize high-quality images in various domains.
However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. Recently, StyleCLIP-like editing of DMs is found in the bottleneck of the U-Net, named \thspace{}.
In this paper, we discover that DMs inherently have disentangled representations for content and style of the resulting images: \thspace{} contains the content and the skip connections convey the style. Furthermore, we introduce a principled way to inject content of one image to another considering progressive nature of the generative process. 
Briefly, given the original generative process, 1) the feature of the source content should be gradually blended, 2) the blended feature should be normalized to preserve the distribution, 3) the change of skip connections due to content injection should be calibrated.
% \js{\warigari{} prevents the skip connection from being affected by the injection.}
% manipulation of $\vx_t$ with \warigari{} enables enhancing style during the injection. 
Then, the resulting image has the source content with the style of the original image just like image-to-image translation. Interestingly, injecting contents to styles of unseen domains produces harmonization-like style transfer.
To the best of our knowledge, our method introduces the first training-free feed-forward style transfer only with an unconditional pretrained frozen generative network. The code is available at this \textcolor{black}{\href{https://curryjung.github.io/DiffStyle/}{\color{red} our project page}}.


\end{abstract}

%%%%%%%%% BODY TEXT
%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
% 원래흐름: DM 잘함 -> real과 비슷한거 만들기 -> real을 의도에맞게 바꾸기 -> asyrp
% 새 흐름: DM이 random generation 잘함 -> process를 control하고싶음 -> inversion 잘해서 edit 가능 -> asyrp
Diffusion models (DMs) have gained recognition in various domains due to their remarkable performance in random generation~\cite{ho2020denoising,song2020denoising}. Naturally, researchers and practitioners seek ways to control the generative process. In this sense, text-to-image DMs provide a way to reflect a given text for generating diverse images using classifier-free guidance~\cite{nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high,balaji2022ediffi,gafni2022make}. In the same context, image guidance synthesizes random images that resemble the reference images that are given for the guidance~\cite{choi2021ilvr,avrahami2022blended,lugmayr2022repaint,meng2021sdedit,chung2022improving}. Although these approaches provide some control, the generative process still has remaining randomness in unspecified aspects of the scene.

On the other hand, deterministic DMs, such as ODE samplers, have been used to edit real images while preserving most of the original image~\cite{song2020denoising,song2020score,jolicoeur2021gotta,liu2022pseudo,lu2022dpm}. DiffusionCLIP~\cite{kim2021diffusionclip} and Imagic~\cite{kawar2022imagic} first embed an input image into noise and finetune DMs for editing. Still, their finetuned DMs are limited to a single attribute change or a single image, respectively. Critically, they do not provide insight into the intermediate features of DMs.

Recently, Asyrp~\cite{kwon2022diffusion} discovered a hidden latent space of pretrained DMs located at the bottleneck of the U-Net, named \thspace{}. Shifting the latent feature maps along a certain direction enables semantic attribute changes, such as adding a smile. When combined with deterministic inversion, it allows real image manipulation using a pretrained frozen DM. However, its application is limited to changing certain attributes, and it does not provide as explicit operations as in generative adversarial networks, such as replacing feature maps.

In this paper, we discover that the bottleneck feature of the U-Net contains the content, and the skip connections convey the style. We design a new generative process for \textit{content injection} given the content and style images. In \fref{fig:teaser}, we briefly showcase the application of content injection. 
Using in-domain style images produces style-mixing-like results, while using external images from the style transfer literature produces a harmonization-like effect.


Our method, named DiffStyle, blends the intermediate features of the content image in \thspace{} into the generative process from the inverted style image, i.e., $\vx_T$.
% \js{Our method, named DiffStyle, blends intermediate features from the inversion process of the content image into the \thspace{} of the sampling process starting from the inversion $\vx_T$ of the style images.}
% Our method, named DiffStyle, starts from intermediate features from the original inversion process of content and style images, namely the bottleneck and the skips.
% Then, DiffStyle produces the resulting images via the new generative process. 
It progressively combines the features with proper normalization that keeps the correlation between the skip connections and the combination along the injection. \fref{fig:overview} shows overview of our method. Additionally, we calibrate the generative process to better reflect the style elements while preserving the content.

DiffStyle enables content injection using pretrained unconditional diffusion models. Considering the bottleneck has spatial dimensions, applying DiffStyle with target region masks leads to local style mixing. To the best of our knowledge, our method is the first to tackle these applications without additional training or extra networks. It provides convenience for users to experiment with existing pretrained DMs. In the experiments, we analyze the effect of individual components and demonstrate diverse usecases. Although there is no comparable method with perfect fit, we compare DiffStyle against closely related methods, including DiffuseIT \cite{kwon2022diffuseIT}.



\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/sampling_graphical_v3.pdf}
    \vspace{-1.5em}
    \caption{\textbf{Overview of \ours{}.} During the content injection, the bottle neck feature map is recursively injected during the sampling process started from the inverted $\vx_T$ of style images. The target content is reflected into the result image while preserving the original style elements.}
    \vspace{-0.5em}
    \label{fig:overview}
\end{figure}

\section{Preliminary and related work}

\subsection{Denoising Diffusion Implicit Model (DDIM)}
\label{ddim}
Diffusion models learn the distribution of data by estimating denoising score matching with $\epsilont$. In the denoising diffusion probabilistic model (DDPM) \cite{ho2020denoising}, the forward process is defined as a Markov process that diffuses the data through parameterized Gaussian transitions.
DDIM \cite{song2020denoising}  redefines DDPM as $q_{\sigma}(\vx_{t-1}|\vx_{t}, \vx_{0})=\mathcal{N}(\sqrt{\alpha_{t-1}} \vx_{0}+\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}} \cdot \frac{\vx_{t}-\sqrt{\alpha_{t}} \vx_{0}}{\sqrt{1-\alpha_{t}}}, \sigma_{t}^{2} \boldsymbol{I})$, where $\{\beta_{t}\}^{T}_{t=1}$ is the variance schedule and $\alpha_{t} = \prod^{t}_{s=1} (1-\beta_s)$. 
Accordingly, the reverse process becomes: 
\begin{equation}
\begin{aligned}
\label{ddim_reverse}
    \vx_{t-1}= & \sqrt{\alpha_{t-1}} \underbrace{\left(\frac{\vx_{t}-\sqrt{1-\alpha_{t}} \epsilont\left(\vx_{t}\right)}{\sqrt{\alpha_{t}}}\right)}_{\text {"predicted } \vx_{0} \text { " }} \\
    & +\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}} \cdot \epsilont\left(\vx_{t}\right)}_{\text {"direction pointing to } \vx_{t} \text { " }}+\underbrace{\sigma_{t} \vz_{t}}_{\text {random noise }},
\end{aligned}
\end{equation}
where $\sigma_t = \eta \sqrt{\left(1-\alpha_{t-1}\right) /\left(1-\alpha_{t}\right)} \sqrt{1-\alpha_{t} / \alpha_{t-1}}$. When $\eta=0$, the process becomes deterministic.

\subsection{Asymmetric reverse process (Asyrp)}
Asyrp \cite{kwon2022diffusion} introduces the asymmetric reverse process for using \thspace{} as a semantic latent space. \thspace{} is the bottleneck of U-Net, which is distinguished from the latent variable $\vx_t$. For real image editing, they invert $\vx_0\sim p_{real}(\vx)$ into $\vx_{T}$ through the DDIM forward process, and generate $\tvx_0$ using the new $\tildeh_t$ in the modified DDIM reverse process. They use an abbreviated version of \eref{ddim_reverse}. We follow the notation of Asyrp throughout this paper:
\begin{equation}
\label{sampling_eq}
    \vx_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\vx_t)) + \Dt(\epsilont(\vx_t)) + \sigma_{t} \vz_{t},
\end{equation}
where $\Pt(\epsilont(\vx_t))$ denotes the predicted $\vx_0$ and $\Dt(\epsilont(\vx_t))$ denotes the direction pointing to $\vx_t$. We abbreviate $\Pt(\epsilont(\vx_t))$ as $\Pt$ and $\Dt(\epsilont(\vx_t))$ as $\Dt$ when the context clearly specifies the arguments. Following Asyrp, we omit $\sigma_{t} \vz_{t}$ when $\eta = 0$. 
Then, Asyrp becomes:
\begin{equation}
\label{bm-sampling}
    \tvx_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tvx_t|\tildeh_t)) + \Dt(\epsilont(\tvx_t|\vh_t)) + \sigma_{t} \vz_{t},
\end{equation}
where $\tilde{\vx}_T=\vx_T$ and then $\epsilont(\tvx_t|\tildeh_t)$ replaces the original U-Net feature maps $\vh_t$ with $\tildeh_t$.
They show that the modification of \thspace{} in both $\Pt$ and $\Dt$ brings a negligible change in the results. Therefore, the key idea of Asyrp is to modify only \thspace{} of $\Pt$ while preserving $\Dt$. 

Quality boosting, introduced by Asyrp, is a stochastic noise injection when the image is almost determined. It enhances fine details and reduces the noise of images while preserving the identity of the image. The whole process of Asyrp is as follows.
\begin{equation}
\begin{aligned}
&\tilde{\vx}_{t-1} = \\
    &\begin{cases}
    &\sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\tildeh_t)) +\Dt \quad \text { if } T \ge t \ge \tedit \\
    &\sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\vh_t)) +\Dt \quad \text { if } \tedit > t  \ge \tboost \\
    &\sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\vh_t)) +\Dt + \sigma_{t}^{2}\vz  \ \text { if } \tboost > t
    \end{cases}
\end{aligned}
\end{equation}

which consists of editing, denoising, and quality boosting intervals where the hyperparameter $\tedit$ determines the editing interval and $\tboost$ determines the quality boosting interval. Following Asyrp, we apply quality boosting to all figures except for ablation studies.
\subsection{Literature}

% \usepackage{natbib}
% \bibliographystyle{plainnat}
After DDPMs \cite{ho2020denoising} provide a universal approach for DMs, Song et al. \cite{song2020score} unified DMs with score-based models in SDEs. Subsequent works have focused on improving generative performance of DMs ~\cite{nichol2021improved,karras2022elucidating,choi2022perception,song2020denoising,watson2022learning}. Meanwhile, ADM \cite{dhariwal2021diffusion} introduced gradient-guidance to control generative process~\cite{sehwag2022generating,avrahami2022blended,liu2021more,nichol2021glide}, but their method does not guarantee detailed manipulation. Other works attempt to manipulate the resulting images by replacing latent variables in DMs and generating random images with the color or strokes of the desired images \cite{choi2021ilvr,meng2021sdedit} but they fall short of content injection or style transfer. Recently, Asyrp \cite{kwon2022diffusion} introduced the use of the bottleneck of U-Net as a semantic latent space (\thspace{}) through the asymmetric reverse process. However, their focus was only on semantic editing, e.g., making a person smile.

Research on image editing through the manipulation of semantic latent space has been done in other generative models such as GANs.~\cite{goodfellow2020generative,ling2021editgan,harkonen2020ganspace,chefer2021image,shen2020interfacegan,yuksel2021latentclr,patashnik2021styleclip,gal2021stylegan,dai2019style} 
There have been a number of works that renovate GANs focusing on styles.~\cite{huang2017arbitrary,gatys2016image,dumoulin2016learned,chen2017stylebank} And it is known that a semantic latent space is available for style transfers.~\cite{huang2017arbitrary,yanai2017conditional,an2020real,huang2018multimodal,johnson2016perceptual,lin2021drafting,kim2021exploiting,karras2020analyzing,choi2018stargan,kim2021exploiting,chong2022jojogan} While most of them require fine-tuning, \ours{} does not.

\section{Method}
\label{sec:method}
In this section, we explore the interesting properties of \thspace{} with Asyrp~\cite{kwon2022diffusion} and design a method for content injection. We start from simply replacing $\vh_t$ of one sample with that of another sample and observe its drawbacks in \sref{sec:replace}. Then we introduce an important requirement for mixing two $\vh_t$'s in \sref{sec:slerp}. Furthermore, we propose \warigari{} to retain the crucial style elements in \sref{sec:style_calibration}.

\subsection{Role of \textit{\textbf{h-space}}}
\label{sec:replace}
\thspace{}, the deepest bottleneck of the U-Net in the diffusion models (DMs), contains the semantics of the resulting images to some extent. In other words, a change in \thspace{} with Asyrp~\cite{kwon2022diffusion} leads to editing the resulting image. Formally, setting $\tildeh_t = \vh_t + \deltah_t$ for $t\in[T,\tedit]$ modifies the semantics, where $\deltah_t$ is the direction of desired attribute. 
The reverse process becomes $\tilde{\vx}_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\tildeh_t)) +\Dt(\epsilont(\tvx_t|\vh_t))$, where $\tildeh_t = \vh_t + \deltah_t^{\text{attr}}$.

We start with a question: does $\vh$ solely specify the semantics of the resulting image as in the latent codes in GANs? I.e., would replacing $\vh$ totally change the output?

To answer the question, we invert two images $\Ione{}$ and $\Itwo{}$ to noises $\xTone{}$ and $\xTtwo{}$ via forward process, respectively.
Then we replace $\{\vht\}$\footnote{Note that the reverse process is recursive. The reason we denote $\{\vht\}$ instead of $\vht^{(1)}$ is that it differs from $\vht^{(1)}$ after the first replacement.} from $\xTone{}$ with $\{\vhttwo{}\}$ from $\xTtwo{}$ during the reconstruction (i.e., reverse process).
Formally, $\tilde{\vx}_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\vhttwo{})) +\Dt(\epsilont(\tvx_t|\vh_t))$, $\tilde{\vx}_{T} = \xTone{}$.


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/intuition_v2.pdf}
    % \vspace{-0.5em}
    % \caption{}
    \caption{\textbf{Preliminary experiment.} Na\"ive replacement of $\vh$ somehow combines the content and the style. However, it severely degrades image quality.}  % (i.e., style transfer) but also leads to degradation of image quality. }
    \vspace{-0.5em}
    \label{fig:replace}
\end{figure}


Interestingly, the resulting images with the replacement contain the people in $\Itwo{}$ with some style elements of $\Ione{}$ such as color distributions and backgrounds as shown in \fref{fig:replace}.
This phenomenon suggests that the main content is specified by $\vh$ and the other aspects come from the other components, e.g., features in the skip connections. Henceforth, we name $\vhttwo{}$ as $\vhtcon{}$.

However, the replacement causes severe distortion in the images. We raise another question: how do we prevent the distortion? Note that Asyrp slightly adjusts $\vht$ with a small change $\deltah_t$. On the other hand, replacing $\vht$ as $\vhtcon{}$ completely removes $\vht{}$. Assuming that the maintenance of $\vht{}$ might be the key factor, we try an alternative in-between: adding $\vhtcon{}$ to $\vht{}$. We observe far less distortion in \fref{fig:ablation} (a).

With these preliminary experiments, we hypothesize that the replacement and the addition drive the disruption of the inherent correlations in the feature map.
The subsequent sections provide grounding analyses and methods to address the problem.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/h1_skip_correlation_illustration_v2.pdf}
    \caption{\textbf{Correlation between $\vht$ and skip connection.} $\vht$ is highly correlated with the matching skip connection. (a) illustrates examples of matching and non-matching skip connections. (b) shows correlation between each $\tildeh_t$ and skip connection. \textbf{r} is Pearson correlation coefficient and p-values of \textbf{r} are less than 1e-15. Non-matching skip connections seriously distort the correlation.}
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{fig:ht_norm}
\end{figure}

% \subsection{Gradually replacing the feature maps}
\subsection{Preserving statistics with Slerp}
\label{sec:slerp}


In DMs, \thspace{} is concatenated with skip connections and fed into the next layer. However, the previous work does not take into account the relationship between them.
We observe an interesting relationship between $\vh_t$ and its matching skip connections $\vg_t$ within a generative process and introduce requirements for replacing $\vh_t$. 
We compute two versions of the correlation between the norms, $|\vh_t|$ and $|\vg_t|$:
\begin{equation}
\label{eq:homo}
    r_\text{homo}=\frac{\sum_{i}\left(|\vh^{(i)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}} 
\end{equation}
\begin{equation}
\label{eq:hetero}
r_\text{hetero}=\frac{\sum_{j\neq i}\left(|\vh^{(j)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}}
\end{equation}
% $$r_\text{homo}=\frac{\sum_{i}\left(|\vh^{(i)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}},$$ \\
% $$r_\text{hetero}=\frac{\sum_{j\neq i}\left(|\vh^{(j)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}},$$
where $n$ is the number of samples and $s_*$ denotes standard deviation of $*$. We omit $t$ for brevity.

\fref{fig:ht_norm} (b) shows that $r_\text{homo}$, the correlation between $\vh_t$ and its matching skip connections, is roughly larger than 0.3 and is strongly positive when the timestep is close to $T$. On the other hand, $r_\text{hetero}$, the correlations between $\vh_t$ and the skip connections in different samples, lie around zero. Furthermore, the alternative in-between process brings $r_\text{alt}$ closer to $r_\text{homo}$ by setting $\tilde{\vh}=\vh^{(i)}+\vh^{(j)}$, achieving less distortion.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/table0_qualitative_horizontal_v3.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Improvement in quality with Slerp.} (a) shows the result of $\vht+\vhtcon$. It has some artifacts. (b) shows the result of Slerp with $\gamma=0.5$ brings better quality. Techniques described later are not applied here for fair comparison.}  %We do not use other techniques such as quality boosting for comparison.}
    % (c) shows the result of $\Dt$ scaling with $\lambda_t=0.999$ which has the best quality.}
    % \caption{Overview of Asyrp++}
    \vspace{-1.0em}
    \label{fig:ablation}
\end{figure}


Hence, we hypothesize that the correlation between $|\vh|$ and $|\vg|$ should remain consistent after the modification to preserve the quality of the generated images. 
To fulfill the hypothesis and achieve the closest difference between $r_{alt}$ and $r_{homo}$, we introduce normalized spherical interpolation (Slerp) between $\vht$ and $\vhtcon{}$ : 
\begin{equation}
\label{eq:just_norm}
 \tildeh_t = f(\vht, \vhtcon{}, \gamma) = \text{Slerp}(\vht, \frac{\vhtcon{}}{\left\|\vhtcon{}\right\|}\cdot \left\| \vht \right\|, \gamma),
\end{equation}
where $\gamma\in[0, 1]$ is a coefficient of $\vhtcon{}$. We note that Slerp requires the inputs to have the same norm. Normalizing $\vhtcon{}$ to match the norm of $\vht$ ensures a consistent correlation between $|\text{Slerp}(\cdot)|$ and $|\vg_t^{(1)}|$ to be the same with the correlation between $|\vht|$ and $|\vg_t^{(1)}|$.
Replacing $\vht$ with $\tilde{\vht}$ using Slerp exhibits fewer artifacts and better content preservation, as shown in \fref{fig:ablation} (b). Besides the improvement, we can control how much content will be injected by adjusting the $\vht\text{-to-}\vhtcon{}$ ratio through parameter $\gamma_t$ of Slerp.
 We provide an approximation of the total amount of injected content in \sref{supp:cumulative}.


\subsection{Style calibration}
\label{sec:style_calibration}
So far, we have revealed that mixing features in \thspace{} injects the content.
% Our research has revealed that when we modify only the \thspace{}, we can inject the content elements of the image. 
Although modifying the \thspace{} using Slerp mostly preserves the style of the original image, any changes made to the \thspace{} inevitably affects the skip connections of the next denoising step $t-1$, leading to partially losing style component. Hence, we propose \textit{style calibration} that compensates the change in the skip connections by adjusting $\tilde\vx_t$.
% To preserve style elements, we focus on that the style elements are carried by the skip connections from $\vx_T$, which need to be preserved despite changes in the \thspace{}. In this context, we propose \textit{style calibration} which helps maintain the $\vx_t$ to better reflect style elements while remaining content.

The goal of style calibration is to produce $\tilde\vx'_{t-1}$ which better preserves the original style with the injected result. We model the implicit change from $\tilde\vx_t$ to $\tilde\vx'_t$ that brings similar change by the injection and introduce a hyperparameter $\omega$ that controls the strength of the change. To this end, we define a slack variable $\mathbf{v}=\vx_t+\mathop{d\mathbf{v}}$ and find $\mathop{d\mathbf{v}}$ such that $\Pt(\epsilont(\mathbf{v}))\approx \Pt(\epsilont(\vx_t|\tildeh_t))$. With the DDIM equation,
% from $\mathbf{v}$,
% to $\tilde\vx_{t-1}$, 
\vspace{-0.5em}
\begin{equation}
\label{eq:Pt}
\sqrt{\alpha_t}\Pt = \vx_t - \sqrt{1-\alpha_t} \epsilont(\vx_t),
% \sqrt{\alpha_t}\tilde\Pt' = \tilde\vx'_t - \sqrt{1-\alpha_t} \epsilont(\tilde\vx'_t).
\vspace{-0.5em}
\end{equation}
we define infinitesimal as 
\vspace{-0.5em}
\begin{equation}
\label{eq:infi}
\sqrt{\alpha_t}\mathop{d\Pt} = \mathop{d\vx_t} - \sqrt{1-\alpha_t}J(\epsilont)\mathop{d\vx_t}.
% \sqrt{\alpha_t}\mathop{d\tilde\Pt'} = \mathop{d\tilde\vx'_t} - \sqrt{1-\alpha_t}J(\epsilont)\mathop{d\tilde\vx'_t}
% \mathop{d\epsilont(\tilde\vx'_t)}.
\vspace{-0.5em}
\end{equation}
% by introducing a slack infinitesimal $\mathop{d\mathbf{v}}$.
Further letting $\mathop{d\vx_t}=\omega\mathop{d\mathbf{v}}$ and
% Further letting $\mathop{d\mathbf{v}}=\omega\mathop{d\tilde\vx'_t}$ and
$J(\epsilont)\mathop{d\mathbf{v}}=\mathop{d\epsilont}$ induces  %(\tilde\vx'_t)}$ induces
% $J(\epsilont)\mathop{d\tilde\vx'_t}=\mathop{d\epsilont(\tilde\vx'_t)}$ induces
% By introducing a slack infinitesimal $\mathop{d\mathbf{v}}=\omega\mathop{d\tilde\vx'_t}$ and letting $J(\epsilont)\mathop{d\mathbf{v}}=\mathop{d\epsilont(\tilde\vx'_t)}$, we induce:
\begin{equation}
\label{eq:dx}
\mathop{d\vx_t} = \sqrt{\alpha_t}\mathop{d\Pt} + \omega\sqrt{1-\alpha_t}\mathop{d\epsilont}.  % (\tilde\vx'_t)}.
\end{equation}

\if 0
    For the style calibration, we need to handle the $\vx_t$ which contains style elements. Let $\vx'_t$ as the corresponding latent variable of the reverse process which may share predicted $\vx_0$ with $\Pt(\epsilont(\tilde{\vx}_t|\tildeh_t))$ i.e., content elements. We approximate $\vx'_t=\tilde{\vx}_t+\mathop{d\vx_t}$ which can lead similar results compared to that of Slerp. Note that we use the original DDIM reverse process with $\vx'_t$. \fref{fig:style_calibration} illustrates it. 
    For the approximation, we start from: 
    \begin{assumption}
     $\exists$ $\mathop{dv}$ such that $\epsilont(\tilde{\vx}_t|\tildeh_t) \approx \epsilont(\tilde{\vx}_t+\mathop{dv})$.
    \end{assumption}
    
    Let the $\Pt'$ be $\Pt(\epsilont(\vx'_t)) \approx \Pt(\epsilont(\tilde{\vx}_t|\tildeh_t))=\Pt(\epsilont(\tilde{\vx}_t))+\mathop{d\Pt}$. Following DDIM, $\sqrt{\alpha_t}\Pt = \mathbf{x}_t - \sqrt{1-\alpha_t} \epsilont(\mathbf{x}_t)$. From the DDIM equation, we can define infinitesimal as
    $\sqrt{\alpha_t}\mathop{d\Pt} = \mathop{d\vx_t} - \sqrt{1-\alpha_t} J(\epsilont) \mathop{d\vx_t}$.
    % From the equation derived in DDIM: $\sqrt{\alpha_t}\Pt = \mathbf{x}_t - \sqrt{1-\alpha_t} \epsilont(\mathbf{x}_t)$, we can define infinitesimal as
    % $\sqrt{\alpha_t}\mathop{d\Pt} = \mathop{d\vx_t} - \sqrt{1-\alpha_t} J(\epsilont) \mathop{d\vx_t}$.
    With $J(\epsilont)\mathop{dv}=\mathop{d\epsilont}$ and approximation of $\omega\mathop{dv}=\mathop{d\vx_t}$, we can rewrite it as:
    \begin{equation}
    \label{eq:dx}
    \mathop{d\vx_t} = \sqrt{\alpha_t}\mathop{d\Pt} + \omega\sqrt{1-\alpha_t}\mathop{d\epsilont}.
    \end{equation}
\fi
Then, we define $\tvx'_t = \tvx_t+d\vx_t$ and obtain $\tilde\vx'_{t-1}$ by a denoising step.

In addition, $\Pt(\epsilont(\tvx'_t))$ in \eref{eq:dx} has larger standard deviation than $\Pt(\epsilont(\tvx_t))$. We regularize it to have the same standard deviation of $\Pt(\epsilont(\tvx_t))$ by 
% Since $\Pt$ is from the edited $\epsilont$, regularization with $\tvx'_t$ should be performed for original DDIM reverse process in \eref{eq:dx}.
% For the regularization, we keep the mean of $\Pt$ and normalize the pixel-to-pixel standard deviation of $\Pt$ to be the same as $\Pt(\epsilont(\tilde{\vx}_t))$; 
\begin{equation}
\mathop{d\Pt}= \frac{\Pt' - \bar{\Pt'}}{|\Pt'|}|\Pt|+\bar{\Pt'}-~\Pt(\epsilont(\tilde{\vx}_t)),
% \mathop{d\Pt}= \text{normalized }\Pt(\epsilont(\tvx'_t))-~\Pt(\epsilont(\tilde{\vx}_t)).
\end{equation}
where $\Pt'=\Pt(\epsilont(\tvx'_t))$.
% Then we can approximate $\vx'_t$ with an appropriate value of~$\omega$. 
Then we control $\vx'_t$ with an $\omega$. 

% To do so, we regularize $\Pt'$ by keeping the mean of $\Pt'$ and normalizing the pixel-to-pixel standard deviation of $\Pt'$ to be the same as $\Pt$; $\mathop{d\Pt}=$ normalized~$\Pt'-\Pt(\epsilont(\tilde{\vx}_t))$. Then we can approximate $\vx'_t$ using an appropriate value of $\omega$. 

When we further expand \eref{eq:dx},
\begin{equation}
\label{eq:dx2}
\mathop{d\vx_t} \approx (\omega-1)\sqrt{1-\alpha_t}(\epsilont(\tilde{\vx}_t|\tildeh_t)-\epsilont(\tilde{\vx}_t)).
\end{equation}
Interestingly, setting $\omega=1$ reduces $\mathop{d\vx_t}$ to $\mathbf{0}$, i.e., injection does not occur. And setting $\omega \approx 0$\footnote{$\omega$ can not be 0 because of its definition.} drives $\tvx'_{t-1}$ close to $\tvx_{t-1}$, i.e., style calibration does not occur.
% Interestingly, when we set $\omega=1$, $\mathop{d\vx_t}$ becomes 0, leading the result to resemble the style image. And when we set $\omega \approx 0$\footnote{$\omega$ can not be 0 because of its definition.}, the original DDIM reverse process with $\vx'_t$ becomes closer to the result of Slerp.
Intuitively, by \eref{eq:dx2}, $\vx'_t$ may share the predicted $\vx_0$ with $\Pt(\epsilont(\tilde{\vx}_t|\tildeh_t))$ and contains style elements. In other words, we maintain the style elements by adding $\mathop{d\vx_t}$ directly in \textit{x-space} while the content injection is conducted in \thspace{}. 
% Note that we use $\vx'_t$ for the DDIM reverse process without perturbation, which does not harm the correlation between $\vht$ and skip connections.

Style calibration consists of four steps. First, we inject the contents as $\tilde{\vx}_t \to \tilde{\vx}_{t-1}$. Second, we regularize $\Pt$ to preserve the original signal distribution after injection. Third, we solve the DDIM equation $\tvx'_t=\tilde{\vx}_t+\mathop{d\vx_t}$ by using \eref{eq:dx}. Finally, we step through a reverse process $\tvx'_t \to \tvx'_{t-1}$.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/style_calibration.pdf}
    \caption{\textbf{Style calibration.} The result of DDIM reverse process with given approximated $\vx'_t$ may be similar to the result of a corresponding injected result $\tvx_{t-1}$. As $\omega$ gets close to 1, more style elements are added through $\mathop{d\vx_t}$. Note that the effect of style calibration is different from modifying $\gamma$ because it remains predicted $\vx_0$ by solving the DDIM equation.}
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{fig:style_calibration}
\end{figure}


\subsection{Content injection and style transfer}
We observe that \thspace{} contains content and skip connection from $\vx_T$ specifies style elements. We utilize this phenomenon for content injection and style transfer. Note that it is possible to obtain inverted $\vx_T$ from any arbitrary real image. Therefore, even if we use out-of-domain images such as artistic images, \ours{} successfully transfers the style of the images. Furthermore, spatial mixing of \thspace{} enables local style mixing by injecting content into the corresponding target area. For the local style mixing, each $\vht$ is masked before Slerp and the mixed $\vht$ is inserted into the original feature map. We provide Algorithm \ref{algo:content_injection} for them and an illustration of spatial $h_t$ mixing in \fref{fig:spatial_slerp}. Note that we omit style calibration and quality boosting \cite{kwon2022diffusion} in the algorithm for simplicity.

\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

 \begin{algorithm}[h!]
    \caption{DiffStyle}
    \label{algo:content_injection}
    \DontPrintSemicolon
    \SetAlgoNoLine
    \SetAlgoVlined
    % \SetKwProg{Re}{Require}{:}{}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwFunction{MultiTransfer}{MultiTransfer}
    \KwIn{$\vx_T$ (inverted latent variable from style image $\Istyle{}$),$\{\vhtcon{}\}_{t=t_{edit}}^{T} $(obtained from content image $\Icon{}$), $\epsilon_{\theta}$ (pretrained model), $m$ (feature map mask), $f$ (Slerp)
    } 
    \KwOut{$\tilde{\vx}_0$ (transferred image)}
    \BlankLine
    $\tvx_t \xleftarrow{} \vx_T$
    \For{$t=T,...,1$}{ 
    \If{$t\ge t_{edit}$}{
        Extract feature map $\vht$ from $\epsilon_{\theta}(\tvx{}_t)$;
        $\tildeh_t \xleftarrow{} f((m \otimes \vht), (m \otimes \vhtcon), \gamma)$ \par
        \qquad \qquad \qquad \qquad \qquad 
        $\oplus (1-m) \otimes \vht$\
        $\tilde{\epsilon} \xleftarrow{} \epsilon_{\theta}(\tvx_t | \tildeh_t)$, 
        $\epsilon \xleftarrow{} \epsilon_{\theta}(\tvx_t)$
    }
    \Else{
        $\tilde{\epsilon} = \epsilon \xleftarrow{} \epsilon_{\theta}(\tvx_t)$, 
    }
    $\tvx_{t-1}\xleftarrow{}\sqrt{\alpha_{t-1}} (\frac{\tvx{}_{t}-\sqrt{1-\alpha_{t}}\tilde{\epsilon}}{\sqrt{\alpha_{t}}}) 
    +\sqrt{1-\alpha_{t-1}}\epsilon$
    }
\end{algorithm}



\section{Experiments}
In this section, we present analyses on \ours{} and showcase our applications.
\paragraph{Setting} We use the official pretrained checkpoints of DDPM++ \cite{song2020score,meng2021sdedit} for CelebA-HQ~\cite{karras2018progressive} and LSUN-church/-bedroom~\cite{yu2015lsun}, iDDPM~\cite{nichol2021improved} for AFHQv2-Dog~\cite{choi2020stargan}, and ADM with P2-weighting~\cite{dhariwal2021diffusion,choi2022perception} for \textsc{MetFaces}~\cite{karras2020training}. The images have a resolution of $256\times256$ pixels. We freeze the model weights.
We use $\tedit$=400, $\omega$=0.3, $\gamma$=0.3, and $\tboost$=200 to produce high-quality images. For more implementation details, please refer to \aref{supp:implement}.

\paragraph{Metrics} GRAM loss (Style loss) \cite{gatys2016image} indicates the style difference between the style image and the result image. ID computes the cosine similarity between face identity~\cite{deng2019arcface} of the content image and the result image to measure content consistency. Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} provides the overall image quality. To compute FID, we compare generated 5K images using 50 steps of the reverse process and 25k images from a training subset of CelebA-HQ. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/slerp_ratio_id_style.pdf}
    \vspace{-1.0em}
    \caption{\textbf{Choice of $\gamma$.} (b) shows that $\gamma$ should be less than $0.6$ since the ID change via content injection converges at the point. If $\gamma > 0.6$, we only lose style faithfulness and suffer image degradation without any advantage.}
    % \caption{Overview of Asyrp++}
    \vspace{-1.0em}
    \label{fig:gamma_id_style}
\end{figure}



\subsection{Analyses}
\label{sec:analyses}
In this section, we provide a guideline for choosing the content injection ratio considering both quality and content consistency. We also show the versatility of style calibration and propose the best interval for editing. Furthermore, we provide quantitative results which support assumptions suggested in \sref{sec:method}: \thspace{} has content elements.

\paragraph{Content injection ratio $\mathcal{\gamma}$}
\label{sec:gamma}
We suggest that the original $\vht$ should be partially kept in \sref{sec:replace}. \fref{fig:gamma_id_style} supports that the content injection ratio $\gamma$ should be less than 0.6 for image quality (FID) and style faithfulness, and $\gamma>0.6$ does not increase ID similarity.
We provide more observations on $\gamma$ in \aref{supp:gamma}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{./figures/warigari_qual_v3.pdf}
    % \vspace{-10pt}
    % \vspace{-2.0em}
    \caption{\textbf{Effectiveness of \Warigari{}.} \Warigari{} retains style elements while preserving content elements. We do not use other techniques such as quality boosting for comparison.}
    % \caption{Overview of Asyrp++}
    \vspace{-1em} %공간부족해지면 넣기.
    \label{fig:warigari_qual}
\end{figure}


\paragraph{The effect of \Warigari{}}
\fref{fig:warigari_qual} shows
% As shown in \fref{fig:warigari_qual}, we found 
that \warigari{} leads to a better reflection of the style elements such as makeups and hair color. 
Note that, depending on the style calibration strength $\omega$, there is a trade-off relationship between Gram loss and ID similarity as well as FID. We report them at various $\omega{}$ in \fref{fig:warigari_quan}.
We discover that increasing $\omega{}$ allows more effective style transfer. More details can be found in \aref{supp:omega}.

\begin{table}[!t]
\centering
\begin{tabular}{l|ccc}
\multicolumn{1}{l|}{} & FID $\downarrow$  & ID $\uparrow$    & Gram loss $\downarrow$ \\ \hline
Reconstruction        & 38.71 &        &            \\ \hline
$\vht{} + \vhtcon{}$                & 49.94 & 0.3581 & 0.0415     \\
Lerp                  & 36.89 & 0.4040 & 0.0318     \\
Slerp                 & \textbf{32.09} & \textbf{0.4390} & \textbf{0.0310}             
\end{tabular}
\vspace{-0.5em}
\caption{\textbf{Performance of various configurations} Slerp improves FID, ID similarity between target content images and synthesized images over other methods. }
\vspace{-1.0em}
\label{tab:ablation}
\end{table}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/test_mixing.pdf}
    % \vspace{-0.5em}
    \captionof{figure}{Qualitative results of content injection in AFHQ-Dog, CelebA-HQ LSUN-Church / Bedroom, and \metfaces{}.}
    % \vspace{2.0em}
    \label{fig:mixing}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/test_celeba.pdf}
    \captionof{figure}{Qualitative results of on CelebA-HQ. We provide the results of local editing (left) and style transfer with out-of-domain images (right).}
    \label{fig:celeba}
    \vspace{-1em}
\end{figure*}

\paragraph{Quantitative comparison} \tref{tab:ablation} shows the quantitative result of each configuration investigated in \sref{sec:method}. Reconstruction reports FID of the official checkpoint of DDPM++ \cite{meng2021sdedit} through its forward and reverse process without any modification on \thspace{}. We observe that $\vht+\vhtcon$ harms FID with severe distortion.
Slerp outperforms $\vht+\vhtcon$ in all aspects. 
We note that our FID score is lower than that of the reconstruction images, indicating that the quality of the generated images is similar to that of inversion images. Reconstructed images are known to have a higher FID than randomly generated images in DMs with a low number of score function evaluation (NFE). \cite{asperti2022image,zhu2023boundary} We suppose that the FID of our method is lower than reconstruction images because the manipulation in \thspace{} makes the generation process closer to random generation. 
% \aref{supp:qualitative} provides more qualitative results. 

\tref{tab:ablation} further shows the superiority of Slerp over linear interpolation (Lerp). It implies that the normalization for preserving the correlation between $\vht$ and skips $\vg_t$ is important.
% \tref{tab:ablation} also provides a comparison between Slerp and linear interpolation (Lerp) to further support our analysis. 
% Slerp outperforms Lerp, indicating that the normalization of Slerp is an essential component for preserving the correlation between $\vht$ and skips $\vg_t$.
% It means that Slerp achieves better image quality than Lerp due to our adjustments in the relations of $\vh_t$ and the skip connection to correct the variation of $\vx_0$. 
% It implies that modifications to the relationship between $\vh_t$ and the skip connection 
% Lerp produces more severe distortions on images, especially when the difference between the norms of $\vht$ and $\vhtcon$ is larger. \todo{$\leftarrow$grounding exp?}
Furthermore, \fref{afig:compare_lerp_slerp} shows that Slerp resolves the remaining artifacts that reside in the resulting images by Lerp. %, which supports that the consistency of the correlation is important in DMs. 
Comparison between Slerp and Lerp will be further discussed in \sref{supp:lerp}.


\paragraph{Editing interval $[T, \tedit]$}
We observe that there is a trade-off between ID similarity and Gram loss when using a suboptimal $\tedit$ and specific value of $\tedit$ leads to better FID, as shown in \fref{fig:t_end}. 
We choose $\tedit=400$ for its balance among the three factors.
This choice also aligns with that of Asyrp \cite{kwon2022diffusion} for editing toward unseen domains, which requires a large change, such as injecting content.
Notably, we find that $\tedit=400$ is also suitable for achieving style transfer with artistic images.


\subsection{Applications}
\paragraph{Content injection}
\fref{fig:mixing} shows that \ours{} works on various architectures and datasets. In \fref{fig:celeba} (a), the masked content injection successfully brings the style to the desired area. See \aref{supple:moreresults} for more results.

\paragraph{Style transfer}
In addition, we can use arbitrary style images, even if they are out-of-domain. \fref{fig:teaser} (c) and \fref{fig:celeba} (b) show results with artistic images as style. In style transfer, we do not use quality boosting \cite{kwon2022diffusion} since they aim to improve the quality and realism of $\vx_0$ which may not be desirable when transferring the style of an out-of-domain image onto the target image. We provide more results in \aref{supple:moreresults}.



\subsection{Comparison with existing methods}
 We first note that there is no competitor with perfect compatibility: frozen pretrained diffusion models, no extra guidance from pretrained models, and versatility (content injection, masked injection, and style transfer). Still, we compare our content injection with DiffuseIT~\cite{kwon2022diffuseIT} which uses pretrained DINO ViT \cite{caron2021emerging} for guidance.
\fref{fig:diffuseIT_compare} shows that DiffuseIT struggles when there is a large gap between the content and style regarding color distributions. More qualitative comparisons with existing methods~\cite{park2020swapping,kim2021exploiting,deng2021stytr,wu2022ccpl} and user study are deferred to \aref{supp:qualitative}.



\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/why_tend400_v3.pdf}
    % \vspace{-1.6em}
    \caption{\textbf{Choice of $\tedit{}.$} We observe that $\tedit=400$ shows the best quality.}
    % \vspace{-1em}
    \label{fig:t_end}
\end{figure}       


\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/diffuseIT_compare_v2.pdf}
    % \vspace{-1.0em}
    \caption{\textbf{Comparison with DiffuseIT} \ours{} is effective even in situations where there is a large discrepancy between the color distributions of the style and content.} 
    % \vspace{-0.5em}
    \label{fig:diffuseIT_compare}
\end{figure}  

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/stable_diffusion.pdf}
    % \vspace{-1.6em}
    \caption{\textbf{\ours{} on Stable diffusion} Although we observe similar phenomenons, the content elements of latent-level DMs is different from pixel-level DMs; More semantic elements is injected to the style image.}
    % \vspace{-1em}
    \label{fig:stablediffusion}
\end{figure}    

\section{Conclusion and discussion}
\label{sec:conclusion}
In this paper, we propose a training-free content injection and style transfer using pretrained DMs. The components in our method are designed to preserve the statistical properties of the original reverse process so that the resulting images are free from artifacts even when the style image is out-of-domain. We hope that our method and its analyses help the research community to harness the nice properties of DMs for various image synthesis tasks.

Although DiffStyle achieves high quality content injection, the small resolution of the \thspace{} hinders fine control of the injecting region. We provide content injection with various masks in \fref{fig:limitation_mask}.

While adopting out-of-domain images as style leads to harmonization-like style transfer, adopting content-less out-of-domain images as content leads to meaningless results. We provide them in \fref{fig:limitation_ood}. We suggest that $\vht$ is not the universal representation for arbitrary content.

In addition, we provide pilot results of \ours{} on Stable diffusion in \fref{fig:stablediffusion}. It works somewhat similarly but the phenomenon is not as clear as in non-latent diffusion models. The bottleneck of Stable diffusion appears more semantically rich, possibly due to its diffusion in VAE's latent space.
 Unveiling the mechanisms in latent diffusion models remains our future work. Please refer to \aref{supple:stable} for the details.

Lastly, we briefly discuss the effect of the scheduling strategy of the injecting ratio $\gamma$ in \aref{appendix:gamma_scheduling}. Further investigation would be an interesting research direction.

\section*{Acknowledgement}
The authors thank Yong-Hyun Park for constructive discussion.
%-------------------------------------------------------------------------

\clearpage{}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage{}

\def\supp{1}
\input{supp.tex}


\end{document}