% \ifx \supp \undefined
%     \documentclass[10pt,twocolumn,letterpaper]{article}
%     \usepackage[review]{cvpr}
    
%     % Include other packages here, before hyperref.
%     \usepackage{xcolor}
%     \usepackage{graphicx}
%     \usepackage{amsmath}
%     \usepackage{amssymb}
%     \usepackage{floatpag}
%     \usepackage{enumitem}
%     \usepackage{mathtools}
%     \usepackage{multirow}

%     \usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}
    
%     % Support for easy cross-referencing
%     \usepackage[capitalize]{cleveref}
%     \crefname{section}{Sec.}{Secs.}
%     \Crefname{section}{Section}{Sections}
%     \Crefname{table}{Table}{Tables}
%     \crefname{table}{Tab.}{Tabs.}
    
    
%     %%%%%%%%% PAPER ID  - PLEASE UPDATE
%     \def\cvprPaperID{2833} % *** Enter the CVPR Paper ID here
%     \def\confName{CVPR}
%     \def\confYear{2023}
    
%     \input{cvpr2023-author_kit-v1_1-1/latex/macros}

%     \begin{document}
% \fi




% \title{Supplementary Materials for \\  }

\maketitle
\thispagestyle{empty}


\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\setcounter{figure}{0}
\setcounter{table}{0}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\centering  
\Large
\textbf{Training-free Content Injection using h-space in Diffusion models} \\
\vspace{0.5em}Supplementary Material \\
\vspace{1.0em}
}]
% \appendix %
% \mtcsettitle{minitoc}{}
\appendix
\addcontentsline{toc}{section}{Supple}
\renewcommand{\contentsname}{}
\renewcommand{\partname}{} % remove "Part"
\renewcommand{\thepart}{} % remove p
% \renewcommand{\ptcSfont}{\small\bf}
% \addtocontents{parttoc}{\protect\setlength{\parskip}{-3.0pt}\protect\setlength{\parindent}{-3.0pt}} % Remove spacing before parttoc
\part{} %
% \parttoc %

% We provide the following supplementary materials:
% \begin{enumerate}[label=\Alph*, nosep]
%     \item Implementation details
%     \item Varying the strength of content injection
%     \item Effect of style calibration
%     \item More results and comparison
%     \item More analyses of Slerp
%     \item Discussion details
%     \item $\gamma{}$ scheduling
%     \item More related work
%     \item Stable diffusion experiment details
    
    % \item Derivation of the approximation and choice of $\lambda_t$.
    % \item Analyses on the sampling calibration
        % \begin{enumerate}
        %     \item Choice of the interval for \dtscaling{}
        %     \item Excessive $\lambda_t$
        % \end{enumerate}
        % \begin{enumerate}
        %     \item More qualitative results 
        %     \item Comparison with SOTA methods
        % \end{enumerate}
% \end{enumerate}
 \vspace{-2.0em}

 \begin{algorithm}[h!]
    \caption{\ours{}}
    \label{algo:full}
    \DontPrintSemicolon
    \SetAlgoNoLine
    \SetAlgoVlined
    % \SetKwProg{Re}{Require}{:}{}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwFunction{MultiTransfer}{MultiTransfer}
    \KwIn{$\vx_T$ (inverted latent variable from original image $\Istyle{}$),$\{\vhtcon{}\}_{t=t_{edit}}^{T} $(obtained from content image $\Icon{}$), $\epsilon_{\theta}$ (pretrained model), $m$ (feature map mask), $f$ (Slerp), $\omega{}$ (calibration parameter)
    } 
    \KwOut{$\tilde{\vx}_0$ (transferred image)}
    \BlankLine
    $\tvx_t \xleftarrow{} \vx_T$
    \For{$t=T,...,1$}{ 
    \If{$t\ge t_{edit}$}{
    \tcp{step1: Content injection}
        Extract feature map $\vht$ from $\epsilon_{\theta}(\tvx{}_t)$;
        $\tildeh_t \xleftarrow{} f((m \otimes \vht), (m \otimes \vhtcon), \gamma), \omega$ \par
        \qquad \qquad \qquad \qquad \qquad 
        $\oplus (1-m) \otimes \vht$\\
        \tcp{step2: Latent calibration}
        $\tilde{\epsilon} \xleftarrow{} \epsilon_{\theta}(\tvx_t | \tildeh_t)$, 
        $\epsilon \xleftarrow{} \epsilon_{\theta}(\tvx_t)$\\
        $\mu_{\Pt(\tilde{\epsilon})}, \sigma_{\Pt(\tilde{\epsilon})} \xleftarrow{} \Pt(\tilde{\epsilon})$\\
        $\mu_{\Pt({\epsilon})}, \sigma_{\Pt({\epsilon})} \xleftarrow{} \Pt({\epsilon})$\\
        $\Pt' = \mu_{\Pt(\tilde{\epsilon})} + (\Pt(\tilde{\epsilon})-\mu_{\Pt(\tilde{\epsilon})})*\sigma_{\Pt({\epsilon})}$\\
        $d\Pt = \Pt' - \Pt({\epsilon})$\\
        $d\epsilon = \tilde{\epsilon} - \epsilon$\\
        $d\vx = \sqrt{\alpha_t}*d\Pt + \omega*\sqrt{(1-\alpha_t)}*d\epsilon$\\
        $\tilde{\vx_t}'=\tilde{\vx_t}+d\vx$\\
        $\tilde{\epsilon} = \epsilon \xleftarrow{} \epsilon_{\theta}(\tilde{\vx_t}')$
        
    }
    \Else{
        $\tilde{\epsilon} = \epsilon \xleftarrow{} \epsilon_{\theta}(\tvx_t)$, 
    }
    $\tvx_{t-1}\xleftarrow{}\sqrt{\alpha_{t-1}} (\frac{\tvx{}_{t}-\sqrt{1-\alpha_{t}}\tilde{\epsilon}}{\sqrt{\alpha_{t}}}) 
    +\sqrt{1-\alpha_{t-1}}\epsilon$
    }
\end{algorithm}

\section{Implementation details}
\label{supp:implement}
To perform the reverse process for figures, we use 1000 steps, while for tables and plots, we use 50 steps.
% We use 50 inference steps for efficiency, not 1000, in all figures, tables, and plots.
% We apply quality boosting~\cite{kwon2022diffusion} except style transfer with artistic references for all figures in the paper and use 1000 timesteps for inference. 
During inference, we injecte $\vht{}$ sparsely only at the timesteps where the content injection applied within the 50 inference steps. For the remaining timesteps, we use the original DDIM sampling. This approach enables us to achieve the same amount of content injection across different inference steps.

For local mixing, we spatially apply Slerp on $\vht$, which has a dimension of $8 \times 8 \times 256$, as demonstrated in \fref{fig:spatial_slerp}. In face swapping, we use a portion of $\vht{}$ that corresponds to the face area for Slerp. In \sref{sec:method}, we use the editing interval [$T$=1000, $t_{edit}$=400], and do not use quality boosting to eliminate stochasticity for comparison purposes, i.e., $\tboost=0$. 
% The reasoning behind choosing $t_{edit}$ will be discussed further in \sref{sec:analyses}.

% As we use 1000 steps for inference, we sparsely inject $\vht{}$ only at the timesteps where the content injection is applied in 50 inference steps and replace the rest of timesteps with the original DDIM sampling. As a result, we could get the same amount of content injection in different inference steps.
% % There are two options to acquire the same amount of content injection in different inference steps. 1) If we want to use the same $\gamma{}$ evaluated in 50 timesteps for 1000 inference steps, we could sparsely inject $\vht{}$ only at the time steps where the content injection is applied in 50 inference steps and replace the rest of timesteps with original DDIM sampling. 
% % % The same $\gamma{}$  with the same injection time steps brings a similar amount of changes in content. 
% % 2) If we do not want to skip some timesteps for content injection, properly scaled $\gamma{}$ could be a solution. In \cite{kwon2022diffusion}, if the sum of $\delta_{h}$ is preserved, the same amount of change can be expected in different inference steps. Similarly, if we choose an adequate small $\gamma{}$ value compared to the one evaluated in 50 steps, we can perform the same amount of content injection in 1000 inference steps. Nonetheless, we do not use the second method since we can hardly expect the total amount of content injection. Content injection is conducted recursively at every injection steps which makes the change of content seriously sensitive to $\gamma{}$. More injection steps increase the sensitivity and accurate calibration of $\gamma{}$ is required.
% For the local content injection, we apply Slerp spatially on $\vht{}$ as shown in \fref{fig:spatial_slerp}. Note that $\vht{}$ has a dimension of $8 \times 8 \times 256$. For face swapping, a part of $\vht{}$ corresponding to a face area is used for Slerp.
% In \sref{sec:method}, we opt to use the editing interval [$T$=1000, $t_{edit}$=400], and not to use quality boosting to eliminate stochasticity for comparison, i.e., $\tboost=0$. The choice of $t_{edit}$ will be discussed further in \sref{sec:analyses}.
%First, we make a feature mask which has same dimension with $\vht{}$ and fill a part of the mask where we want to inject contents with 1 and the rest with 0.  

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{./figures/supple_mask_illustration.pdf}
%     \vspace{-0.5em}
%     \caption{\textbf{Illustration of local content injection.} A mask $m$ determines the area of Slerp. }
%     % \caption{Overview of Asyrp++}
%     \label{afig:mask_illustration}
% \end{figure}


\section{Varying the strength of content injection}
% with varying $\gamma$}
% controls strength of content injection }
\label{supp:gamma}

\fref{afig:gamma} illustrates the results of content injection with different values of Slerp ratio $\gamma{}$. As observed in \fref{fig:gamma_id_style}b, there is a positive correlation between $\gamma{}$ and the amount of content change. However, increasing $\gamma{}>0.6$ barely leads to any content change but degrades the quality of images with distortions and artifacts.
As the recursive injection of content by $\gamma{}$ exponentially decreases the original $\vht{}$ component along the reverse process, according to \eref{eq:cumulative}, we expect linear change of content in the image by linearly controlling $\alpha$ that specifies $\gamma=\alpha^{1/T}$. 
%Note that we do not consider the influence of the networks for the approximation here.
% we observe positive correlation between $\gamma$ and amount of the content change. However, increasing  barely leads to content change, but only degrades the quality of images with distortion and artifacts.
% From \fref{fig:gamma_id_style}-(b), we suppose that the higher $\gamma$ brings more content injection up to $\gamma=0.6$.
% In other words, $\gamma$ can determine how much content will be injected. After $\gamma=0.6$, ID similarity between a content image and the resulting image decreases, which means that content injection is saturated. 
% \fref{afig:gamma} shows the results of content injection with respect to different values of $\gamma{}$. We observe that if we use $\gamma{}$ larger than $0.6$, increasing $\gamma{}$ barely changes content representations, but only degrades the quality of images with distortion and artifacts. 
% \fref{afig:gamma_horn} shows that the trend of the variance of $\vx_t$ changes with respect to $\gamma$ during sampling process. Larger $\gamma$ pushes the variance further from the reconstruction leading to inferior image quality. \js{We observe that sampling away from the distribution of DDIM reconstruction causes artifacts in resulting images.}
% \fref{afig:gamma_horn} explains the quality problem caused by excessive values of $\gamma{}$. The inconsistency of the distributions from the original distribution reflects $\gamma{}$. The larger $\gamma{}$ we use, the larger inconsistency we get. It is one of the main causes of quality degradation.
% As recursively injecting content by $\gamma$ exponentially decreases the $\vht$ component along the reverse process according to \eref{eq:cumulative}, we may expect linear visual change of content by linearly controlling $\alpha$ that specifies $\gamma=\alpha^{1/T}$. Note that we does not take the influence of the networks into account for the approximation. \todo{}


% Nevertheless, we should compensate for the gap due to the disturbance through the networks. Hence, \eref{eq:cumulative} is only an approximation.




\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/supple_mask_illustration.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Illustration of local mixing} Mask $m$ determines the area of feature map. Slerp of masked $\vht$ enables content injection into designated space.}
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{fig:spatial_slerp}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{./figures/supple_slerp_ratio_extend.pdf}
    \captionof{figure}{$\gamma$ controls how much content will be injected. We do not use other techniques such as quality boosting for comparison.}
    \label{afig:gamma}
\end{figure*}





% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{./figures/slerp_ratio_horn.pdf}
%     \vspace{-0.5em}
%     \caption{\textbf{Effect of increasing $\gamma$}. We report the standard deviation of $\vx_t$ in different $\gamma$. }
%     % \caption{Overview of Asyrp++}
%     \vspace{-0.5em}
%     \label{afig:gamma_horn}
% \end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/style_calibration_qual.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Effect of increasing $\omega{}$}. Increasing $\omega{}$ reflects style elements stronger and $\omega{}=0$ shows the result without latent calibration.}
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{afig:omega_qual}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/warigari_quan.pdf}
    % \vspace{-1.6em}
    \caption{\textbf{Quantitative results of latent calibration with varying $\omega{}$.}
    \Warigari{} ensures that the resulting image remains close to the original image, minimizing content injection loss and preserving image quality.
    }
    \vspace{-1em}
    \label{fig:warigari_quan}
\end{figure}  

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/PnP-MasaCtrl.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Utilizing latent calibration to other methods.}. Increasing $\omega{}$ reflects injected results stronger when using other methods. For Stable Diffusion, we only use $\omega{}>0.6$. }
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{afig:stylecalibrationothers}
\end{figure}

\section{Effect of latent calibration}
\label{supp:omega}

In this section, we present an analysis of the parameter $\omega{}$ which specifies the strength of the original element. \fref{afig:omega_qual} displays the resulting images with sweeping $\omega{}$. As $\omega{}$ increases, the style elements become more prominent. 
% When $\omega{}=0$, the result is without latent calibration.
We note that latent calibration with $\omega{}=0$ is not rigorously defined and we report the results without latent calibration when $\omega=0$. In \fref{fig:warigari_quan}, we observe a trade-off between Gram loss and ID similarity, as well as FID, depending on the value of $\omega{}$. However, despite this trade-off, increasing $\omega{}$ results in more effective conservation of the original image.

Because \warigari{} also can control the strength of feature-injected results, we can utilize \warigari{} for other feature-injecting methods, e.g., Plug-and-Play \cite{tumanyan2023plug} and MasaCtrl \cite{cao2023masactrl}. \fref{afig:stylecalibrationothers} shows that increasing $\omega$ increases the strength of editing.
% the result of style calibration with Plug-and-Play and MasaCtrl that edit images by injecting features. 
\if 0
We observe if we use Stable Diffusion, normalization of $\Pt$ is not necessary because Stable Diffusion is diffused on the latent space of VAE, which is different from pixel-level image space.
\fi 
% In this section, we provide an analysis of the parameter $\omega$. \fref{afig:omega_qual} shows resulting images with varying values of $\omega$.
% As $\omega{}$ increases, the style elements become more prominent. When $\omega{}=0$, the result is without style calibration. Note that style calibration with $\omega{}=0$ is not strictly defined.
% % Increasing $\omega{}$ led to increasing style elements and $\omega{}=0$ shows the result without style calibration. Note that strictly, style calibration with $\omega{}=0$ is not defined.

% \fref{fig:warigari_quan} shows there is a trade-off relationship between Gram loss and ID similarity as well as FID depending on the value of $\omega$. 
% However, increasing $\omega{}$ resulted in more effective style transfer, despite the trade-off between Gram loss and ID similarity as well as FID.



\section{More results and comparison}


\subsection{More qualitative results}
\label{supple:moreresults}

We provide more qualitative results of CelebA-HQ, AFHQ, \metfaces{}, LSUN-church, and LSUN-bedroom in \fref{fig:supple_celeba}-\ref{fig:supple_style_transfer_more}  (located at the end for compact arrangement).
We also provide a result of ImageNet in \fref{fig:supple_imagenet_skip_slerp}a.
\begin{table}[b]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|c}
                        % \hline
                        & Method               & Preference (\%) \\
                        \hline
\multirow{2}{*}{Content injection}       & Swapping Autoencoder~\cite{park2020swapping} & 40.11      \\
                        & Ours                 & \textbf{59.89}      \\
                        \hline
\multirow{2}{*}{Local content injection} & StyleMapGAN~\cite{kim2021exploiting}          & 33.56      \\
                        & Ours                 & \textbf{66.44}      \\
                        \hline
\multirow{3}{*}{Artistic style transfer} & StyTr$^2$~\cite{deng2021stytr}            & 20.89      \\
                        & CCPL~\cite{wu2022ccpl}                 & 21.44      \\
                        & Ours                 & \textbf{57.67}     \\
                        % \hline
\end{tabular}
}
\caption{User study with 90 participants.}
\label{atab:comparison}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/lerp_slerp_compare_qual.pdf}
    % \vspace{-1.5em}
    \captionof{figure}{\textbf{Comparison between Slerp and Lerp.} Slerp reduces artifacts and distortions in Lerp. Note that We do not use other techniques such as quality boosting to evaluate the effect of Slerp only.}
    % \vspace{2.0em}
    \label{afig:compare_lerp_slerp}
\end{figure*}

\subsection{Comparison with the other methods.}
\label{supp:qualitative}

\tref{atab:comparison} presents the results of a user study conducted with 90 participants to compare our method with existing methods. The participants were asked a question: ``Which image is more natural while faithfully reflecting the original image and the content image?". We randomly selected ten images for content injections and thirty images for style transfer without any curation. The example images are shown in \fref{fig:supple_swapping_autoencoder}-\ref{fig:supple_styletransfer_comparison} (located at the end for clear spacing). Even though \ours{} works on pretrained diffusion models without further training for the task, our method outperforms the others. We selects the recent methods from the respective tasks for comparison. 

Although content injection does not define domains of images, it resembles image-to-image translation in that both of their results preserve content of input images while adding different elements.
% Image-to-Image translation (I2I translation) is not our task since we do not specify explicit domains but focus on directly injecting content from one image to another image. Nevertheless, the resulting images of in-domain \ours{} look similar to the results of the other I2I translation works. 
Therefore, we show the differences between \ours{} and those works in \fref{fig:supple_mixing_compare}. The resulting image of \ours{} well reflects overall color distribution, color-related attributes (e.g. makeup), and non-facial elements (e.g. long hair, bang hair, decorations on a head) of the original images. Ours also reflect facial expression, jawline, and overall pose of the content image. On the other hand, the other works do not accurately reflect color-related attributes from the original images and also ignore fine-grained detail or spatial structure of the original image. They focus on preserving the structure of the content image.
 % We randomly chose ten images for content injections, and thirty images for style transfer without curation. The example images are shown in \fref{fig:supple_swapping_autoencoder}-\ref{fig:supple_styletransfer_comparison} (located at the end for clear spacing). Our method outperforms the others even though \ours{} works on pretrained diffusion models and does not involve any further training for the task. We chose the recent methods from the respective tasks for comparison. 

\subsection{Comparison with DiffuseIT}
% \fref{fig:rebuttal} (a) shows the superiority of our method in reflecting the style and preserving the content compared to DiffuseIT. \cite{kwon2022diffuseIT} Furthermore, DiffuseIT requires extra supervision using DINO while ours simply modify the intermediate variables. 
We provide more qualitative comparison with DiffuseIT \cite{kwon2022diffuseIT} which uses DINO ViT \cite{caron2021emerging}.
% in \fref{fig:supple_diffuseIT}. 
As shown in \fref{fig:supple_diffuseIT}, \ours{} shows comparable results without extra supervision. \ours{} is highly proficient at accurately and authentically reflecting the color of the original image while avoiding artificial contrast, especially when there is a significant difference in color between the content and the original image (e.g., black and white). In contrast, DiffuseIT may not be able to fully capture the color of the original image in these scenarios. This discrepancy is due to the starting point of the reverse process. DiffuseIT utilizes the inverted $\xT$ of the content image to sample and manipulate noise to match the target original image. The large gap in color distribution between the content and original images makes it challenging for DiffuseIT to overcome this difference entirely. Conversely, \ours{} initially samples from the inverted $\xT$ of the original image, making it easier to maintain the color of the original image. The original image is preserved through the skip connection.
 
 % and remains effective in producing desirable results even when faced with substantial variations in color distribution between the style and content. 
% We report the result of the user study with 90 participants for the quantitative result (\tref{atab:comparison}). Although \ours{} does not train for specific tasks, \ours{} achieves higher scores than other methods. For content injection, we used FFHQ \cite{karras2019style} for a fair comparison with Swapping AutoEncoder. We carefully made masks for StyleMapGAN for a fair comparison. We provide qualitative results in \fref{fig:supple_swapping_autoencoder}-\ref{fig:supple_styletransfer_comparison} (located at the end for clear spacing). We randomly select ten images for content injections, respectively, and thirty images for style transfer. We use a simple question: "Which image looks more natural and well reflects style and content?"
% "Images generated with style and content references. Which image is better?".

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/large_gap_norms.pdf}
    \vspace{-0.5em}
    \caption{We choose $\vht{}$ from the top 20 and bottom 20 samples in their norms among 500 samples. 
    % To validate Slerp, we divide samples into two groups.
    Each line represents a trajectory of $\lVert h \rVert_{2}$ during the reconstruction of a sample.
}
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{afig:large_gap_norms}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/norm_visualize.pdf}
    \vspace{-0.5em}
    \caption{Visual comparison of Slerp and Lerp. The larger difference in norms of $\vht$ and $\vhtcon$ leads to a larger gap between the results. Lerp followed by normalization is closer to Slerp than Lerp.
    % The more difference in norm there is, the more difference between Slerp and Lerp becomes. We observe that using Lerp followed by normalization has a similar result to Normalized Slerp.
}
    % \caption{Overview of Asyrp++}
    \vspace{-0.5em}
    \label{afig:norm_visualize}
\end{figure}


\section{More analyses of Slerp}
\subsection{Comparison with Lerp}
\label{supp:lerp}

The intuition behind using Slerp is that we should preserve the correlation between $\vht$ and its matching skip connection (\sref{sec:slerp}). Here, we explore an alternative: Lerp.
When $\vht$ and $\vhtcon$ have different norms, using Lerp results in more artifacts in the final image as shown in \fref{afig:compare_lerp_slerp}. This difference in norms of $\vht$ is reported in \fref{afig:large_gap_norms}.
\fref{afig:norm_visualize} illustrates the difference between Slerp, Lerp, and Lerp followed by normalization. Lerp may change the norm of $\textbf{f}(\vht, \vhtcon, \gamma)$ when the norm of $\vht$ and $\vhtcon$ are different, leading to a decrease in image quality. However, Lerp followed by normalization produces results similar to Slerp. Still, we choose Slerp because it is easier to implement and less prone to errors.


% According to the analysis on the relationship between $\vht{}$ and its skip connection, we opt to use normalized spherical interpolation of two $\vht{}$'s for content injection. As mentioned above, the norm of $\vht{}$ should be maintained along the asymmetric reverse process. Content injection via Slerp allows the norm to move along the trajectory supposed to follow. If the difference in the norms of two objectives $\vht{}$'s is not huge, normalized Slerp works as similar as Lerp. However, if we choose $\vht{}$'s that have huge difference norms, Slerp shows more robustness than Lerp. To validate Slerp, we divide samples into two groups, one with large $\vht{}$ norms and the other with small $\vht{}$ norms (\fref{afig:large_gap_norms}). After that, we inject the contents of one group into another with Lerp and Slerp. The qualitative comparison of Lerp and Slerp is in \fref{afig:compare_lerp_slerp}. As shown in the figure, we could find more artifacts in the results of Lerp compared to Slerp.



\subsection{Cumulative content injection}
\label{supp:cumulative}
In addition to improving the quality of images, our approach allows us to control the amount of content injection by adjusting the $\vht\text{-to-}\vhtcon$ ratio through Slerp parameter $\gamma_t$. A small $\gamma_t$ results in a smaller amount of content injection. As mentioned in \sref{sec:replace}, preserving the $\vht$ component improves quality. However, there is a trade-off between the content injection rate and quality, and therefore, the value of $\vht$ needs to be constrained. Further experiments to determine the proper range of $\gamma{}$ are discussed in \sref{sec:gamma}.
% Small $\gamma{}$ means a small portion of content injection. \js{From \sref{sec:replace}, we suggest that preserving $\vht$ improves quality. Since there is trade-off relation ship between content injection rate and quality, the value of $\gamma{}$ should be constrained. \sref{sec:gamma} provides further experiment to find proper range of $\gamma$} 
 
 Note that the effects of Slerp are cumulative along the reverse process as the content injection at $t$ affects the following reverse process in $[t-1,\tedit]$.
% As content injection is defined for $t\in[T,\tedit]$, $\vhtcon$ is repeatedly injected during this interval. 
We provide an approximation of the total amount of injected content as follows. Assuming that the angle between $\vht$ and $\vhtcon$ is close to 0 and the results of content injection at $t$ are directly passed to the next \thspace{} at $t-1$ without any loss, then
% Assume the optimal case that the angle between $\vht$ and $\vhtcon$ is close to 0 and the results of content injection at $t$ are directly passed to the next \thspace{} at $t-1$ without any loss, then
$$\tildeh_{t} = (1-\gamma) \vht + \gamma \vhtcon \approx{} f(\vht, \vhtcon, \gamma)$$
and $$ \vh_{t-1} \approx{} \tildeh_t.$$
Along the reverse process, $\tildeh_t$ is recursively fed into the next stage.
After $n$ content injections, we get 

\begin{equation}
\label{eq:cumulative}
\tildeh_{t-n} \approx{} (1-\gamma)^n \vh_t + \gamma \sum_{i=1}^{n} (1-\gamma)^{i-1} h_{t-i}^{content}.
\end{equation}
As $0 \leq \gamma \leq 1$, the proportion of $\vht$ decreases exponentially and the proportion of $\vhtcon$ accumulates during the content injection stage. It indicates that a large proportion of content is injected compared to $\gamma{}$ of Slerp. For further details regarding the ablation study on $\gamma{}$, please refer to \sref{sec:gamma}.
%the proportion of injected content is larger than $\gamma_t$ of Slerp.  Please refer to \sref{sec:gamma} for the ablation study on $\gamma{}$. 



% Following \eref{eq:cumulative}, the part of $\vhtone$ decreases in proportion to the exponential of $(1-\gamma)$. And the rest of the part is filled with the information of $\vhttwo$. If \eref{eq:cumulative} is correct, we could expect linear content manipulation by exponentially adjusting $\gamma$. 
% However, it only holds in the optimal case where the results of content injection are delivered to the next timestep without any loss. In the real world, content injection is decoded and encoded by DMs to get to the next timestep. Therefore, we could only approximate $\gamma$ by \eref{eq:cumulative} to measure how much content will be injected.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{./figures/limitation_ood.pdf}
    % \vspace{-1.6em}
    \caption{\textbf{Content image from unseen domain} Other than original images, $\vhtcon{}$ obtained from unseen domain results in poor images.}
    % \vspace{-1.5em}
    \label{fig:limitation_ood}
\end{figure}


% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.9\linewidth]{./figures/limitation_mask3.pdf}
%     % \vspace{-1.6em}
%     \caption{\textbf{Local style mixing with various feature map mask sizes.} Adjusting the size and position of feature map mask enables to handle the area of content injection, facilitating control of local style mixing.}
%     % \vspace{-1.5em}
%     \label{fig:limitation_mask}
% \end{figure}



\section{Discussion details}
\label{appendix:limitations}

As mentioned in \sref{sec:conclusion}, \fref{fig:limitation_ood} shows that using out-of-domain images as content leads to completely distorted results. It implies that $\vht$ cannot be considered a universal representation for all types of content. 
% \fref{fig:limitation_ood} shows the constraint.

\fref{fig:limitation_mask} shows the local mixing with various feature map mask sizes. Using the feature map mask, we can designate the specific area where the content injection is applied. 
Unfortunately, the \thspace{} has small spatial dimensions, limiting the resolution of the mask for local mixing.
% Nonetheless, the \thspace{} has a small dimension, which restricts local style mixing to the resolution of the \thspace{} mask.
% Unfortunately, the \thspace{} has a small dimension and local editing is limited to the resolution of the \thspace{} mask.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{./figures/gradually_schedule.pdf}
    % \vspace{-10pt}
    \vspace{-1.0em}
    \caption{\textbf{Various interpolation ratio schedule.} $\gamma{}$ is content injection rate.}
    % \caption{Overview of Asyrp++}
    % \vspace{-1em} %공간부족해지면 style calibration기.
    \label{fig:rebuttal}
\end{figure}

\section{$\gamma{}$ scheduling}
\label{appendix:gamma_scheduling}

\fref{fig:rebuttal} provides the results from alternative schedules. 
Gradually decreasing the injection along the generative process enhances realism, however, it may not accurately represent the content.
Conversely, gradually increasing the injection better preserves the content but results in more artifacts. We keep the total amount of injection fixed in this experiment.


% \section{Other diffusion-based concurrent work}
% \fref{fig:rebuttal} (a) shows the superiority of our method in reflecting the style and preserving the content compared to DiffuseIT. \cite{kwon2022diffuseIT} Furthermore, DiffuseIT requires extra supervision using DINO while ours simply modify the intermediate variables. 




\section{More related work}
After \cite{ho2020denoising,song2020score} proposed a universal approach for Diffuson models (DMs), subsequent works have focused on controlling the generative process of DMs ~\cite{zhang2023adding,parmar2023zero,li2023gligen,couairon2022diffedit,gal2022image,yang2022paint,kumari2023multi,xie2022smartbrush,choi2021ilvr,meng2021sdedit,avrahami2022blended,mokady2022null,kim2021diffusionclip,wallace2022edict}. Especially, \cite{park2023unsupervised,kwon2022diffusion,zhu2023boundary,tumanyan2023plug,baranchuk2021label} have uncovered the role of intermediate feature maps of diffusion models and utilized it for image editing, segmentation, and translation. However, we are the first to analyze the role of the latent variables $\vx_t$ in DMs and apply it to content injection.

The research on controlling the generative process has been done in other generative models such as GANs~\cite{goodfellow2020generative}. \cite{gatys2015neural,isola2017image} introduce style transfer and image-to-image translation with GANs and there have been a number of works that focused on the style of images ~\cite{hoffman2018cycada,choi2020stargan,choi2018stargan,yoo2019photorealistic,baek2021rethinking,park2019semantic,wang2018high}. After StyleGAN~\cite{karras2019style,karras2018progressive,karras2020analyzing}, more diverse methodologies have been proposed~\cite{kim2021exploiting,karras2020analyzing,choi2018stargan,kim2021exploiting,chong2022jojogan}. However, most of them require training.



% Research on image editing through the manipulation of semantic latent space has been done in other generative models such as GANs.~\cite{goodfellow2020generative,ling2021editgan,harkonen2020ganspace,chefer2021image,shen2020interfacegan,yuksel2021latentclr,patashnik2021styleclip,gal2021stylegan,dai2019style} 
% There have been a number of works that renovate GANs focusing on styles.~\cite{huang2017arbitrary,gatys2016image,dumoulin2016learned,chen2017stylebank} And it is known that a semantic latent space is available for style transfers.~\cite{huang2017arbitrary,yanai2017conditional,an2020real,huang2018multimodal,johnson2016perceptual,lin2021drafting,kim2021exploiting,karras2020analyzing,choi2018stargan,kim2021exploiting,chong2022jojogan} However, most of them require training. Note that \ours{} does not.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/more_comparisons_iccv_rebuttal.pdf}
    % \vspace{-10pt}
    \vspace{-1.0em}
    \caption{\textbf{More comparisons} \ours{} shows different mixing strategy compared to the other methods.}
    % \caption{Overview of Asyrp++}
    % \vspace{-1em} %공간부족해지면 style calibration기.
    \label{fig:supple_mixing_compare}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{./figures/imagenet_results_skip_slerp.pdf}
    % \vspace{-10pt}
    \vspace{-1.0em}
    \caption{(a) \ours{} works on ImageNet. (b) Skip connection injection does not provide meaningful results. }
    % \caption{Overview of Asyrp++}
    % \vspace{-1em} %공간부족해지면 style calibration기.
    \label{fig:supple_imagenet_skip_slerp}
\end{figure}



\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/content_injection_on_other_layer.pdf}
    \captionof{figure}{
    The importance of h-space. When we inject features into additional layers, the results are disrupted. It supports h-space has semantic information and is the reason why we inject features into only h-space.
    }
    \label{fig:content_injection_on_other_layer}
    \vspace{-1em}
\end{figure*}




\section{Stable diffusion experiment details}
\label{supple:stable}

We provide more details of experiments with Stable diffusion. In \fref{fig:stablediffusion}, we use conditional random sampling with Stable diffusion v2.  In order to apply \ours{} on Stable diffusion, there are 3 options with conditional guidance. 1) content injection only with unconditional output, 2) content injection only with conditional output, 3) content injection with both conditional/unconditional outputs. 
We find that using only the unconditional output for content injection resulted in poor outcomes, while the other two options produced similar results. Thus, we use only the conditional output for content injection in \fref{fig:stablediffusion}.

Moving on to the implementation details for Stable diffusion, we set the scale to 9.0, use 50 steps for DDIM sampling, and employ the following prompts: for an original image, ``a highly detailed epic cinematic concept art CG render digital painting artwork: dieselpunk steaming robot" and for a content image: ``digital painting artwork: a cube-shaped robot with big wheels", for an original image: ``8k, wallpaper car" and for a content image: ``concept, 8k, wallpaper sports car, ferrari bg", for an original image: ``a realistic photo of a woman." and 
for a content image, ``a realistic photo of a muscle man.", original image: ``A digital illustration of a small town, 4k, detailed, animation, fantasy" and for an original image: ``A digital illustration of a dense forest, trending in artstation, 4k, fantasy."

\section{Definition of content}
\label{supple:moredetail}
% \todo{More details of content definition userstudy}
% \input{iccv2023AuthorKit/dt_scaling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We provide more details of content definition used in \sref{sec:analyses}. 
We classify each of the attributes to determine whether they are from the content image or the original image by CLIP score (CS);
\begin{equation}
\label{eq1}
\text{CLIPScore}(x,a) = 100 * \text{sim}(\mathbf{E}_{\mathbf{I}}(x),\mathbf{E}_{\mathbf{T}}(a)),
\end{equation}
where $x$ is a single image, $a$ is a given text of attribute, $\text{sim}(*,*)$ is cosine similarity, and $\mathbf{E}_{\mathbf{I}}$ and $\mathbf{E}_{\mathbf{T}}$ are CLIP image encoder and text encoder respectively.

First, we calculate the CS between the desired texts and images, original image $x_o$, content image $x_c$, and result image $x_r$. Then, if the $|\text{CS}(x_o,a)-\text{CS}(x_r,a)| > |\text{CS}(x_c,a)-\text{CS}(x_r,a)|$ then we regard the attribute is from the content image and vice versa.

In order to ignore the case that $x_o$ and $x_c$ have similar attributes, the classified result was ignored when the difference between the two values was very small. Formally, if $\| |\text{CS}(x_o,a)-\text{CS}(x_r,a)| - |\text{CS}(x_c,a)-\text{CS}(x_r,a)| \| < \lambda_{th}$, we pass that sample for that attribute. We use 5k images and set $\lambda_{th}=0.2$.

The result shows that content includes glasses, square jaw, young, bald, big nose, and facial expressions and the remaining elements include hairstyle, hair color, bang hair, accessories, beard, and makeup.

For the user study, we show the resulting image and ask people to choose the content or original image for each attribute.
We use randomly chosen 100 images and aggregate the responses from 50 participants.




% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{./figures/warigari_qual_v2.pdf}
%     % \vspace{-10pt}
%     \vspace{-2.0em}
%     \caption{\textbf{Qualitative results of \dtscaling{}}}
%     % \caption{Overview of Asyrp++}
%     % \vspace{-1em} %공간부족해지면 넣기.
%     \label{fig:warigari_qual}
% \end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/swapping_autoencoder_comparison.pdf}
    \captionof{figure}{\textbf{Qualitative comparison of content injection on FFHQ.} \ours{} is shown to be effective in reflecting content elements while preserving the overall color distribution of the original image. }
    \label{fig:supple_swapping_autoencoder}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/stylemapgan_comparison.pdf}
    \captionof{figure}{\textbf{Qualitative comparison of local mixing on CelebA-HQ.} Despite providing StyleMapGan with detailed segmentation guidance, there are noticeable artifacts in the resulting images, especially at the border lines of the mask. Furthermore, due to the differences in pose between the content and the original images, StyleMapGan struggles to seamlessly integrate the two images, resulting in less-than-optimal outcomes.}
    \label{fig:supple_stylemapgan}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/styletransfer_comparison.pdf}
    \captionof{figure}{\textbf{Qualitative comparison between \ours and style transfer methods with artistic references on CelebA-HQ.} \ours{} allows using images from unseen domains as the original images, enabling the target content can be reflected on the artistic references. \ours{} produces a harmonization-like effect without severe content distortion. Some high-level semantic color patterns of the original images are better reflected by \ours{}  than the others.}
    \label{fig:supple_styletransfer_comparison}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{./figures/supp_diffuseIT_v3.pdf}
    \captionof{figure}{\textbf{More qualitative comparison with DiffuseIT.} \ours{} excels in fully and naturally reflecting the original color without creating artificial contrast, particularly when there is a significant gap between the content color and the style color (e.g., black and white). In contrast, DiffuseIT may not fully capture the original color in such cases.}
    \label{fig:supple_diffuseIT}
    \vspace{-1em}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_celeba.pdf}
    \captionof{figure}{Qualitative results of content injection on CelebA-HQ.}
    \label{fig:supple_celeba}
    \vspace{-1em}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_celeba_mask.pdf}
    \captionof{figure}{Qualitative results of local editing on CelebA-HQ.}
    \label{fig:supple_celeba_mask}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_afhq.pdf}
    \captionof{figure}{Qualitative results of content injection on AFHQ. }
    \label{fig:supple_afhq}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_metface.pdf}
    \captionof{figure}{Qualitative results of content injection on \metfaces{}.}
    \label{fig:supple_metface}
    \vspace{-1em}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_church.pdf}
    \captionof{figure}{Qualitative results of content injection on LSUN-church.}
    \label{fig:supple_church}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_bedroom.pdf}
    \captionof{figure}{Qualitative results of content injection on LSUN-bedroom.}
    \label{fig:supple_bedroom}
    \vspace{-1em}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{./figures/supple_style_transfer_more.pdf}
    \captionof{figure}{Qualitative results of content injection into artistic references with CelebA-HQ .}
    \label{fig:supple_style_transfer_more}
    \vspace{-1em}
\end{figure*}


\clearpage

% \ifx \supp \undefined
%     {\small
%     \bibliographystyle{ieee_fullname}
%     \bibliography{egbib}
%     }
% \end{document}