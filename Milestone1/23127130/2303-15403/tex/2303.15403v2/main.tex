% WACV 2024 Paper Template
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
\usepackage{wacv}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[accsupp]{axessibility}
% \usepackage{natbib}

%our library
\usepackage{cuted}
\input{macros}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{1288} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2024}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Training-free Content Injection using h-space in Diffusion Models}
% \title{Training-free Style Transfer Emerges from h-space in Diffusion models}

\author{Jaeseok Jeong$^*$\\
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Mingi Kwon$^*$\\
% Department of Artificial Intelligence\\
Yonsei University\\
Seoul, Republic of Korea\\
{\tt\small \{jete\_jeong,kwonmingi,yj.uh\}@yonsei.ac.kr}
\and
Youngjung Uh$^\dag$\\
\vspace{-2.0em}
}

\maketitle
\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}
\def\thefootnote{$\dag$}\footnotetext{corresponding author}
% \begin{strip}
% \centering
%   \includegraphics[width=0.9\textwidth]{./figures/teaser.pdf}
%       \vspace{-0.3em}
%     \captionof{figure}{
%     \ours{} allows (a) style mixing by content injection within the trained domain, (b) local style mixing by injecting masked content features, and (c) harmonization-like style transfer with out-of-domain style references. All results are produced by frozen pretrained diffusion models.
%     Furthermore, flexibility of \ours{} enables content injection into any style.}
%     \vspace{-0.5em}
%     \label{fig:teaser}
    
% \end{strip}

%%%%%%%%% ABSTRACT
\begin{abstract}
Diffusion models (DMs) synthesize high-quality images in various domains.
However, controlling their generative process is still hazy because the intermediate variables in the process are not rigorously studied. 
Recently, the bottleneck feature of the U-Net, namely \thspace{}, is found to convey the semantics of the resulting image. It enables StyleCLIP-like latent editing within DMs.
% Recently, StyleCLIP-like editing of DMs is found in the bottleneck of the U-Net, named \thspace{}.
In this paper, we explore further usage of \thspace{} beyond attribute editing, and introduce a method to inject the content of one image into another image by combining their features in the generative processes. Briefly, given the original generative process of the other image, 1) we gradually blend the bottleneck feature of the content with proper normalization, and 2) we calibrate the skip connections to match the injected content.
Unlike custom-diffusion approaches, our method does not require time-consuming optimization or fine-tuning. Instead, our method manipulates intermediate features within a feed-forward generative process. Furthermore, our method does not require supervision from external networks. \href{https://curryjung.github.io/InjectFusion/}{Project Page}
% In this paper, we discover that DMs inherently have disentangled representations for content and style of the resulting images: \thspace{} contains the content and the skip connections convey the style. 
% Upon this, we propose \todo{method name} to inject content of one image to another. \todo{method name} considers progressive nature of the generative process to provide artifact-free results. 
% Furthermore, we introduce a principled way to inject content of one image to another, considering progressive nature of the generative process. Briefly, given the original generative process, 1) the feature of the source content should be gradually blended, 2) the blended feature should be normalized to preserve the distribution, 3) the change of skip connections due to content injection should be calibrated.
% \js{\warigari{} prevents the skip connection from being affected by the injection.}
% manipulation of $\vx_t$ with \warigari{} enables enhancing style during the injection. 
% Then, the resulting image has the content with the \todo{style} of the original image. Interestingly, injecting contents to styles of unseen domains produces harmonization-like effect.
% To the best of our knowledge, our method introduces the first training-free feed-forward \todo{style transfer} without external networks. Code will be made publicly available.
% the first training-free feed-forward style transfer only with an unconditional pretrained frozen generative network. 
%The code is available at this \textcolor{black}{\href{https://curryjung.github.io/InjectFusion/}{\color{red} our project page}}
\end{abstract}


% \footnote{equal contribution, corresponding author}

%%%%%%%%% BODY TEXT
\vspace{-1.5em}
\section{Introduction}
\label{sec:intro}
% 원래흐름: DM 잘함 -> real과 비슷한거 만들기 -> real을 의도에맞게 바꾸기 -> asyrp
% 새 흐름: DM이 random generation 잘함 -> process를 control하고싶음 -> inversion 잘해서 edit 가style calibration -> asyrp
Diffusion models (DMs) have gained recognition in various domains due to their remarkable performance in random generation~\cite{ho2020denoising,song2020denoising}. Naturally, researchers and practitioners seek ways to control the generative process. In this sense, text-to-image DMs provide a way to reflect a given text for generating diverse images using classifier-free guidance~\cite{nichol2021glide,ramesh2022hierarchical,saharia2022photorealistic,rombach2022high,balaji2022ediffi,gafni2022make}. In the same context, image guidance synthesizes random images that resemble the reference images that are given for the guidance~\cite{choi2021ilvr,avrahami2022blended,lugmayr2022repaint,meng2021sdedit,chung2022improving}. 
On the other hand, deterministic DMs, such as ODE samplers, have been used to edit real images while preserving most of the original image~\cite{song2020denoising,song2020score,jolicoeur2021gotta,liu2022pseudo,lu2022dpm}. DiffusionCLIP~\cite{kim2021diffusionclip} and Imagic~\cite{kawar2022imagic} first embed an input image into noise and finetune DMs for editing. 
While these approaches provide some control for DMs,  the intermediate variables in the process are not rigorously studied, as opposed to the latent space of generative adversarial networks (GANs).
Critically, previous studies do not provide insight into the intermediate features of DMs.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{./figures/simplified_teasor_v2.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Overview of \ours{}.} 
    During the content injection, the bottleneck feature map is recursively injected during the sampling process started from the inverted $\vx_T$ of images. The target content is reflected in the result images while preserving the original images.}
    \vspace{-1.5em}
    \label{fig:overview}
\end{figure}


Recently, Asyrp~\cite{kwon2022diffusion} discovered a hidden latent space of pretrained DMs located at the bottleneck of the U-Net, named \thspace{}. Shifting the latent feature maps along a certain direction enables semantic attribute changes, such as adding a smile. When combined with deterministic inversion, it allows real image manipulation using a pretrained frozen DM. However, its application is limited to changing certain attributes, and it does not provide as explicit operations as in GANs, such as replacing feature maps.

In this paper, we explore further usage of \thspace{} beyond attribute editing and introduce a method that injects the content of one image into another image.
% by combining their features in the generative processes. 
\fref{fig:overview} overviews our new generative process for content injection. It starts by inverting two images into noises. Instead of running generative processes from them individually, we set one generative process as an original and inject the bottleneck features of the other generative process. As the bottleneck features convey the semantics of the resulting image, it is equivalent to injecting the content. The injection happens recursively along the timesteps.
% In this paper, we discover that the bottleneck feature of the U-Net contains content-like semantic information, and the skip connections convey style-like color-dependent information, especially on pixel-level DMs. Based on this observation, we design a new generative process for \textit{content injection} given the content and style images. In \fref{fig:overview}, we briefly showcase the concept application of content injection; the \thspace{} of the content image is injected into the sampling process of a given arbitrary style image.
% Using in-domain style images produces style-mixing-like results, while using external images from the style transfer literature produces a harmonization-like effect.

However, unlike GAN, DMs are usually designed with U-Net which has skip connections. If one directly changes the bottleneck only, it distorts the relation between the skip connection and the bottleneck. Our method, named InjectFusion, treats this problem with two methods. 
1) \ours{} blends the content bottleneck to the original bottleneck gradually along the generative process. The blended feature is properly normalized to keep the correlation with the skip connections.
% 1) InjectFusion blends the intermediate features of the content image in \thspace{} into the generative process from the inverted style image, i.e., $\vx_T$. It progressively combines the features with proper normalization that keeps the correlation between the skip connections and the combination along the injection. 
2) \ours{} calibrates the latent $\vx_t$ directly to preserve the correlation between \thspace{} and skip connections.
% 2) We introduce style calibration which makes InjectFusion completely free from mixing feature maps. Although blending feature maps alleviates the mismatch between skip-connection and feature maps, the ideal way might be finding appropriate input $x_t$ which results are similar to results from mixing feature maps. 
This calibration is not only able to be used for InjectFusion but also for any other feature manipulation methods.

% \fref{fig:overview} shows overview of our method. Additionally, we calibrate the generative process to better reflect the style elements while preserving the content.
InjectFusion enables content injection using pretrained unconditional diffusion models without any training. To the best of our knowledge, our method is the first to tackle these applications without additional training or extra networks. It provides convenience for users to experiment with existing pretrained DMs. In the experiments, we analyze the effect of individual components and demonstrate diverse use cases. Although there is no comparable method with a perfect fit, we compare InjectFusion against closely related methods, including DiffuseIT \cite{kwon2022diffuseIT}.


\section{Background}
In this section, we review various approaches for controlling the results of DMs and cover preliminaries.
\subsection{Diffusion models and controllability}

% \usepackage{natbib}
% \bibliographystyle{plainnat}
After DDPMs \cite{ho2020denoising} provide a universal approach for DMs, Song et al. \cite{song2020score} unify DMs with score-based models in SDEs. Subsequent works have focused on improving generative performance of DMs ~\cite{nichol2021improved,karras2022elucidating,choi2022perception,song2020denoising,watson2022learning}. Other works attempt to manipulate the resulting images by replacing latent variables in DMs and generating random images with the color or strokes of the desired images \cite{choi2021ilvr,meng2021sdedit} but they fall short of content injection.

Recently, some works have proposed to control DMs by manipulating latent features in DMs. Asyrp \cite{kwon2022diffusion} considers the bottleneck of U-Net as a semantic latent space (\thspace{}) through the asymmetric reverse process. However, it focuses only on semantic editing, e.g., making a person smile. Plug-and-Play \cite{tumanyan2023plug} injects an intermediate feature in DMs to provide structural guidance. However, it does not consider the correlation between the skip connection and the feature. Similarly, injecting self-attention features enables semantic image editing by retaining structure or objects/characters \cite{tumanyan2023plug, cao2023masactrl}. However, they should rely on text prompts to determine the destinations, which is often vague and insufficient in describing abstract and fine-grained visual concepts.
% Furthermore, optimizing the input of the value in text conditioning enhances the preservation of an accurate attention map in editing for selected regions while avoiding undesirable changes in non-selected regions \cite{li2023stylediffusion}.



ADM \cite{dhariwal2021diffusion} introduces gradient-guidance to control generative process~\cite{sehwag2022generating,avrahami2022blended,liu2021more,nichol2021glide}, but it does not allow detailed manipulation. The guidance controls the reverse process of DMs and can be extended to image-guided image translation without extra training but it depends on the external model (e.g. DINO ViT \cite{caron2021emerging}) and struggles to overcome a huge disparity in color distribution. \cite{kwon2022diffuseIT} 
% Research on image editing through the manipulation of semantic latent space has been done in other generative models such as GANs.~\cite{goodfellow2020generative,ling2021editgan,harkonen2020ganspace,chefer2021image,shen2020interfacegan,yuksel2021latentclr,patashnik2021styleclip,gal2021stylegan,dai2019style} There have been a number of works that renovate GANs focusing on styles.~\cite{huang2017arbitrary,gatys2016image,dumoulin2016learned,chen2017stylebank} And it is known that a semantic latent space is available for style transfers.~\cite{huang2017arbitrary,yanai2017conditional,an2020real,huang2018multimodal,johnson2016perceptual,lin2021drafting,kim2021exploiting,karras2020analyzing,choi2018stargan,kim2021exploiting,chong2022jojogan} While most of them require fine-tuning, \ours{} does not.

\subsection{Injecting contents from exemplar images}

For given exemplar images with an object, Dreambooth variants~\cite{ruiz2022dreambooth, kumari2023multi} fine-tune pretrained DMs to generate different images containing the object. Instead of fine-tuning the whole model, LoRA variants \cite{lora_repo, zhang2023adding, li2023gligen} introduce auxiliary networks or fine-tune a tiny subset of the model.
As opposed to modifying models, textual inversion variants~\cite{gal2022image, han2023highly} embed visual concepts into text embeddings for the same task.
% In recent years, some works adopt an approach where visual concepts are optimized into text embedding, or models, resulting in the generation of new images that contain the given concepts. \cite{gal2022image,kumari2023multi,ruiz2022dreambooth,ruiz2022dreambooth } 
% However, specific visual elements are lost along the inversion process. These methods require optimization using multiple visual examples and lack the ability to specify the structure of the generated images. Similarly, instruction embedding of \cite{brooks2023instructpix2pix} can be optimized  \cite{nguyen2023visual}, and one can train an explicit prompt encoder which takes visual features from pretrained visual encoder \cite{sun2023imagebrush} but they require paired visual example images and additional training.
However, these methods require extra training or optimization steps to reflect the exemplars. On the other hand, our method does not require training or optimization but works on frozen pretrained models.
In addition, while these methods rely on the form of text to reflect the exemplars, our method directly works on the intermediate features in the model.

ControlNet variants \cite{zhang2023adding, mou2023t2i,li2023gligen} can inject structural contents as a condition in the form of an edge map, segmentation mask, pose, and depth map. However, the control is limited to structure and shape. Our method preserves most of the content in the exemplar.
% , and also depend on text prompts through the generative process.

% However, learned concepts by optimizing the text embeddings or modifying the models should be injected into the generative process conditioned with text prompts, and require time-consuming optimization with multiple visual exemplars.
Some works utilize the inversion capability of DMs \cite{hertz2022prompt, brooks2023instructpix2pix, mokady2023null, tumanyan2023plug, cao2023masactrl}, which enables injecting contents during the reconstruction process. However, most of them rely on language to insert the contents.


% \citep{tumanyan2023plug} allows maintaining structure using self-attention injection and
%  \cite{zhang2023adding} achieves detailed guidance of shape with visual examples (e.g. edge map) with additional training. However, they should rely on text prompts to determine style.


% injecting objects specified by text into an image
% on the other hand, we inject objects specified by an image
% After large-scale text2image diffusion models \cite{rombach2022high,saharia2022photorealistic,balaji2022ediffi, ramesh2022hierarchical}, text-driven image editing allows inserting desired objects from text into real images via inversion process \cite{hertz2022prompt, brooks2023instructpix2pix, mokady2023null, tumanyan2023plug, cao2023masactrl} using prompts. 

\subsection{Style transfer}

Recently, neural style transfer \cite{gatys2015neural} has evolved with the advancement of DMs and neural network architecture \cite{dosovitskiy2020image}. Some style transfer methods leverage a style encoder \cite{ruta2023aladin} to enable pretrained DMs to be conditioned on the visual embedding from style reference images \cite{tarres2023parasol,ruta2023diff}. StyleDrop \cite{sohn2023styledrop} achieves outstanding performance in extracting style features from visual examples but how to control content and shape has not been provided. Since it is vision transformer \cite{dosovitskiy2020image}, universal spatial control approach of DMs \cite{zhang2023adding} cannot be adapted 

Exploiting external segmentation mask models and explicit appearance encoder enables decomposing the structure and appearance in \cite{goel2023pair} for style transfer, but it requires training DMs and the encoder from scratch.

% \ours{} is not style transfer.

\subsection{Denoising Diffusion Implicit Model (DDIM)}
\label{ddim}
Diffusion models learn the distribution of data by estimating denoising score matching with $\epsilont$. In the denoising diffusion probabilistic model (DDPM) \cite{ho2020denoising}, the forward process is defined as a Markov process that diffuses the data through parameterized Gaussian transitions.
DDIM \cite{song2020denoising}  redefines DDPM as $q_{\sigma}(\vx_{t-1}|\vx_{t}, \vx_{0})=\mathcal{N}(\sqrt{\alpha_{t-1}} \vx_{0}+\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}} \cdot \frac{\vx_{t}-\sqrt{\alpha_{t}} \vx_{0}}{\sqrt{1-\alpha_{t}}}, \sigma_{t}^{2} \boldsymbol{I})$, where $\{\beta_{t}\}^{T}_{t=1}$ is the variance schedule and $\alpha_{t} = \prod^{t}_{s=1} (1-\beta_s)$. 
Accordingly, the reverse process becomes: 
\begin{equation}
\begin{aligned}
\label{ddim_reverse}
    \vx_{t-1}= & \sqrt{\alpha_{t-1}} \underbrace{\left(\frac{\vx_{t}-\sqrt{1-\alpha_{t}} \epsilont\left(\vx_{t}\right)}{\sqrt{\alpha_{t}}}\right)}_{\text {"predicted } \vx_{0} \text { " }} \\
    & +\underbrace{\sqrt{1-\alpha_{t-1}-\sigma_{t}^{2}} \cdot \epsilont\left(\vx_{t}\right)}_{\text {"direction pointing to } \vx_{t} \text { " }}+\underbrace{\sigma_{t} \vz_{t}}_{\text {random noise }},
\end{aligned}
\end{equation}
where $\sigma_t = \eta \sqrt{\left(1-\alpha_{t-1}\right) /\left(1-\alpha_{t}\right)} \sqrt{1-\alpha_{t} / \alpha_{t-1}}$. When $\eta=0$, the process becomes deterministic.

\subsection{Asymmetric reverse process (Asyrp)}
Asyrp \cite{kwon2022diffusion} introduces the asymmetric reverse process for using \thspace{} as a semantic latent space. \thspace{} is the bottleneck of U-Net, which is distinguished from the latent variable $\vx_t$. For real image editing, they invert $\vx_0\sim p_{real}(\vx)$ into $\vx_{T}$ through the DDIM forward process, and generate $\tvx_0$ using the new $\tildeh_t$ in the modified DDIM reverse process. They use an abbreviated version of \eref{ddim_reverse}. We follow the notation of Asyrp throughout this paper:
\begin{equation}
\label{sampling_eq}
    \vx_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\vx_t)) + \Dt(\epsilont(\vx_t)) + \sigma_{t} \vz_{t},
\end{equation}
where $\Pt(\epsilont(\vx_t))$ denotes the predicted $\vx_0$ and $\Dt(\epsilont(\vx_t))$ denotes the direction pointing to $\vx_t$. We abbreviate $\Pt(\epsilont(\vx_t))$ as $\Pt$ and $\Dt(\epsilont(\vx_t))$ as $\Dt$ when the context clearly specifies the arguments. Following Asyrp, we omit $\sigma_{t} \vz_{t}$ when $\eta = 0$. 
Then, Asyrp becomes:
\begin{equation}
\label{bm-sampling}
    \tvx_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tvx_t|\tildeh_t)) + \Dt(\epsilont(\tvx_t|\vh_t)) + \sigma_{t} \vz_{t},
\end{equation}
where $\tilde{\vx}_T=\tvx_T$ and then $\epsilont(\tvx_t|\tildeh_t)$ replaces the original U-Net feature maps $\vh_t$ with $\tildeh_t$.
They show that the modification of \thspace{} in both $\Pt$ and $\Dt$ brings a negligible change in the results. Therefore, the key idea of Asyrp is to modify only \thspace{} of $\Pt$ while preserving $\Dt$. 

Quality boosting, introduced by Asyrp, is a stochastic noise injection when the image is almost determined. It enhances fine details and reduces the noise of images while preserving the identity of the image. The whole process of Asyrp is as follows.
\begin{equation}
\begin{aligned}
&\tilde{\vx}_{t-1} = \\
    &\begin{cases}
    &\sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\tildeh_t)) +\Dt \quad \text { if } T \ge t \ge \tedit \\
    &\sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\vh_t)) +\Dt \quad \text { if } \tedit > t  \ge \tboost \\
    &\sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\vh_t)) +\Dt + \sigma_{t}^{2}\vz  \ \text { if } \tboost > t
    \end{cases}
\end{aligned}
\end{equation}

which consists of editing, denoising, and quality boosting intervals where the hyperparameter $\tedit$ determines the editing interval and $\tboost$ determines the quality boosting interval. Following Asyrp, we apply quality boosting to all figures except for ablation studies.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{./figures/replace_add_slerp_compare_new.pdf}
    \vspace{-0.5em}
    \caption{\textbf{Illustration of content injection methods}. (a) and (b) provide content injection but suffer quality degradation. Compared to them, (c) allows successful content injection by preserving statistics in DMs and gradually increasing the ratio of the target content.}
    \vspace{-0.5em}
    \label{fig:slerp_compare}
\end{figure}

\section{Method}
\label{sec:method}
In this section, we explore the interesting properties of \thspace{} with Asyrp~\cite{kwon2022diffusion} and design a method for content injection. We start by simply replacing $\vh_t$ of one sample with that of another sample and observe its drawbacks in \sref{sec:replace}. Then we introduce an important requirement for mixing two $\vh_t$'s in \sref{sec:slerp}. Furthermore, we propose \warigari{} to retain the crucial elements in \sref{sec:style_calibration}.

\subsection{Role of \textit{\textbf{h-space}}}
\label{sec:replace}
\thspace{}, the deepest bottleneck of the U-Net in the diffusion models (DMs), contains the semantics of the resulting images to some extent. In other words, a change in \thspace{} with Asyrp~\cite{kwon2022diffusion} leads to editing the resulting image. Formally, setting $\tildeh_t = \vh_t + \deltah_t$ for $t\in[T,\tedit]$ modifies the semantics, where $\deltah_t$ is the direction of desired attribute. 
The reverse process becomes $\tilde{\vx}_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\tildeh_t)) +\Dt(\epsilont(\tvx_t|\vh_t))$, where $\tildeh_t = \vh_t + \deltah_t^{\text{attr}}$.

We start with a question: Does $\vh$ solely specify the semantics of the resulting image as in the latent codes in GANs? I.e., would replacing $\vh$ totally change the output?

To answer the question, we invert two images $\Ione{}$ and $\Itwo{}$ to noises $\xTone{}$ and $\xTtwo{}$ via forward process, respectively.
Then we replace $\{\vht\}$\footnote{Note that the reverse process is recursive. The reason we denote $\{\vht\}$ instead of $\vht^{(1)}$ is that it differs from $\vht^{(1)}$ after the first replacement.} from $\xTone{}$ with $\{\vhttwo{}\}$ from $\xTtwo{}$ during the reconstruction (i.e., reverse process).
Formally, $\tilde{\vx}_{t-1} = \sqrt{\alpha_{t-1}}\ \Pt(\epsilont(\tilde{\vx}_t|\vhttwo{})) +\Dt(\epsilont(\tvx_t|\vh_t))$, $\tilde{\vx}_{T} = \xTone{}$; which is illustrated in \fref{fig:slerp_compare}a.

\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/intuition_v2.pdf}
    \vspace{-1.5em}
    % \caption{}
    \caption{\textbf{Preliminary experiment.} Na\"ive replacement of $\vh$ somehow combines the content and the original image. However, it severely degrades image quality.}  % (i.e., style transfer) but also leads to degradation of image quality. }
    \vspace{-0.5em}
    \label{fig:replace}
\end{figure}


Interestingly, the resulting images with the replacement contain the people in $\Itwo{}$ with some elements of $\Ione{}$ such as color distributions and backgrounds as shown in \fref{fig:replace}.
This phenomenon suggests that the main content is specified by $\vh$ and the other aspects come from the other components, e.g., features in the skip connections. Henceforth, we name $\vhttwo{}$ as $\vhtcon{}$.

However, the replacement causes severe distortion in the images. We raise another question: how do we prevent the distortion? Note that Asyrp slightly adjusts $\vht$ with a small change $\deltah_t$. On the other hand, replacing $\vht$ as $\vhtcon{}$ completely removes $\vht{}$. Assuming that the maintenance of $\vht{}$ might be the key factor, we try an alternative in-between: adding $\vhtcon{}$ to $\vht{}$; which is illustrated in \fref{fig:slerp_compare}b. We observe far less distortion in \fref{fig:ablation}a.

With these preliminary experiments, we hypothesize that the replacement and the addition drive the disruption of the inherent correlations in the feature map.
The subsequent sections provide grounding analyses and methods to address the problem.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{./figures/table0_qualitative_horizontal_v3.pdf}
    \vspace{-1.5em}
    \caption{\textbf{Improvement in quality with Slerp.} (a) shows the result of $\vht+\vhtcon$. It has some artifacts. (b) shows the result of Slerp with $\gamma=0.5$ brings better quality. Techniques described later are not applied here for fair comparison.}  %We do not use other techniques such as quality boosting for comparison.}
    % (c) shows the result of $\Dt$ scaling with $\lambda_t=0.999$ which has the best quality.}
    % \caption{Overview of Asyrp++}
    \vspace{-1.0em}
    \label{fig:ablation}
\end{figure}

% \subsection{Gradually replacing the feature maps}
\subsection{Preserving statistics with Slerp}
\label{sec:slerp}


In DMs, \thspace{} is concatenated with skip connections and fed into the next layer. However, Asyrp~\cite{kwon2022diffusion} does not take into account the relationship between them.
We observe an interesting relationship between $\vh_t$ and its matching skip connections $\vg_t$ (illustrated in \fref{fig:ht_norm}a) within a generative process and introduce requirements for replacing $\vh_t$. 
We compute two versions of the correlation between the norms, $|\vh_t|$ and $|\vg_t|$:
\begin{equation}
\label{eq:homo}
    r_\text{homo}=\frac{\sum_{i}\left(|\vh^{(i)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}} 
\end{equation}
\begin{equation}
\label{eq:hetero}
r_\text{hetero}=\frac{\sum_{j\neq i}\left(|\vh^{(j)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}}
\end{equation}
% $$r_\text{homo}=\frac{\sum_{i}\left(|\vh^{(i)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}},$$ \\
% $$r_\text{hetero}=\frac{\sum_{j\neq i}\left(|\vh^{(j)}|-\bar{|\vh|}\right)\left(|\vg^{(i)}|-\bar{|\vg|}\right)}{(n-1) s_{|\vh|} s_{|\vg|}},$$
where $n$ is the number of samples and $s_*$ denotes standard deviation of $*$. We omit $t$ for brevity.

\fref{fig:ht_norm}b shows that $r_\text{homo}$, the correlation between $\vh_t$ and its matching skip connections, is roughly larger than 0.3 and is strongly positive when the timestep is close to $T$. On the other hand, $r_\text{hetero}$, the correlations between $\vh_t$ and the skip connections in different samples, lie around zero.
We try an alternative $\tilde{\vh}=\vh^{(i)}+\vh^{(j)}$ and find its correlation is closer to $r_\text{homo}$ than $r_\text{hetero}$ and it produces less distortion.
% Furthermore, the alternative in-between process brings $r_\text{alt}$ closer to $r_\text{homo}$ by setting $\tilde{\vh}=\vh^{(i)}+\vh^{(j)}$, achieving less distortion.
% Furthermore, the alternative in-between process brings $r_\text{alt}$ closer to $r_\text{homo}$ by setting $\tilde{\vh}=\vh^{(i)}+\vh^{(j)}$, achieving less distortion.


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/h1_skip_correlation_illustration_v2.pdf}
    \vspace{-1.5em}


    \caption{\textbf{Correlation between $\vht$ and skip connection.} $\vht$ is highly correlated with the matching skip connection. (a) illustrates examples of matching and non-matching skip connections. (b) shows correlation between each $\tildeh_t$ and skip connection. \textbf{r} is Pearson correlation coefficient and p-values of \textbf{r} are less than 1e-15. Non-matching skip connections seriously distort the correlation.}
    % \caption{Overview of Asyrp++}
    \vspace{-1.5em}
    \label{fig:ht_norm}
\end{figure}


Hence, we hypothesize that the correlation between $|\vh|$ and $|\vg|$ should remain consistent after the modification to preserve the quality of the generated images. 
To ensure the correlation of $\tilde{\vh}_t$ equals to $r_\text{homo}$,
% To fulfill the hypothesis and achieve the closest difference between $r_{alt}$ and $r_{homo}$,
we introduce normalized spherical interpolation (Slerp) between $\vht$ and $\vhtcon{}$ : 
\begin{equation}
\label{eq:just_norm}
 \tildeh_t = f(\vht, \vhtcon{}, \gamma) = \text{Slerp}(\vht, \frac{\vhtcon{}}{\left\|\vhtcon{}\right\|}\cdot \left\| \vht \right\|, \gamma),
\end{equation}
where $\gamma\in[0, 1]$ is a coefficient of $\vhtcon{}$. (See \fref{fig:slerp_compare}c.) We note that Slerp requires the inputs to have the same norm. Normalizing $\vhtcon{}$ to match the norm of $\vht$ ensures a consistent correlation between $|\text{Slerp}(\cdot)|$ and $|\vg_t^{(1)}|$ to be the same with the correlation between $|\vht|$ and $|\vg_t^{(1)}|$.
Replacing $\vht$ with $\tilde{\vh}_t$ using Slerp exhibits fewer artifacts and better content preservation, as shown in \fref{fig:ablation}b. Besides the improvement, we can control how much content will be injected by adjusting the $\vht\text{-to-}\vhtcon{}$ ratio through parameter $\gamma_t$ of Slerp.
 We provide an approximation of the total amount of injected content in \sref{supp:cumulative}.


\subsection{Latent calibration}
\label{sec:style_calibration}
So far, we have revealed that mixing features in \thspace{} injects the content.
% Our research has revealed that when we modify only the \thspace{}, we can inject the content elements of the image. 
Although Slerp preserves the correlation between \thspace{} and skip connection, altering only $\vh_t$ with fixed skip connection may arrive at $\tvx_{t-1}$ that could not be reached from $\tvx_t$.
Hence, we propose \textit{\warigari{}} that achieves the similar change due to $\tilde{\vh}_t$ by modifying $\tvx_t$.
% Although modifying the \thspace{} using Slerp does not affect the skip connection at the same timestep, any changes made to the \thspace{} inevitably affect the skip connections of the next denoising step $t-1$, leading to artifacts or partially losing the distribution of the original image. Hence, we propose \textit{\warigari{}} that compensates for the change in the skip connections by adjusting $\tilde\vx_t$.
% With \textit{\warigari{}}, although we do not change any feature maps inside U-Net, we can blend content and style images.
% To preserve style elements, we focus on that the style elements are carried by the skip connections from $\vx_T$, which need to be preserved despite changes in the \thspace{}. In this context, we propose \textit{style calibration} which helps maintain the $\vx_t$ to better reflect style elements while remaining content.

Specifically, after we compute $\tvx_{t-1}$, we define a slack variable $\mathbf{v}=\tvx_t+\mathop{d\mathbf{v}}$ and find $\mathop{d\mathbf{v}}$ such that $\Pt(\epsilont(\mathbf{v}))\approx \Pt(\epsilont(\tvx_t|\tildeh_t))$. It ensures $\tvx'_0$ predicted from $\mathbf{v}$ is as similar as possible to $\tvx_0$ predicted from injecting $\tilde{\vh}_t$ to $\tvx_t$.
% The goal of \warigari{} is to produce $\tilde\vx'_{t-1}$ which better preserves the original image on the injected content. Intuitively, after we get content-injected $\tvx_{t-1}$, we obtain $\tilde\vx'_{t}$ by \warigari{} which results semantically the same as $\tvx_{t-1}$ but better quality due to not using feature injection. 
We model the implicit change from $\tilde\vx_t$ to $\tilde\vx'_t$ that brings similar change by the injection and introduce a hyperparameter $\omega$ that controls the strength of the change. To this end, we define a slack variable $\mathbf{v}=\tvx_t+\mathop{d\mathbf{v}}$ and find $\mathop{d\mathbf{v}}$ such that $\Pt(\epsilont(\mathbf{v}))\approx \Pt(\epsilont(\tvx_t|\tildeh_t))$. With the DDIM equation,
% from $\mathbf{v}$,
% to $\tilde\vx_{t-1}$, 
\vspace{-0.5em}
\begin{equation}
\label{eq:Pt}
\sqrt{\alpha_t}\Pt = \tvx_t - \sqrt{1-\alpha_t} \epsilont(\tvx_t),
% \sqrt{\alpha_t}\tilde\Pt' = \tilde\vx'_t - \sqrt{1-\alpha_t} \epsilont(\tilde\vx'_t).
\vspace{-0.5em}
\end{equation}
we define infinitesimal as 
\vspace{-0.5em}
\begin{equation}
\label{eq:infi}
\sqrt{\alpha_t}\mathop{d\Pt} = \mathop{d\tvx_t} - \sqrt{1-\alpha_t}J(\epsilont)\mathop{d\tvx_t}.
% \sqrt{\alpha_t}\mathop{d\tilde\Pt'} = \mathop{d\tilde\vx'_t} - \sqrt{1-\alpha_t}J(\epsilont)\mathop{d\tilde\vx'_t}
% \mathop{d\epsilont(\tilde\vx'_t)}.
\vspace{-0.5em}
\end{equation}
% by introducing a slack infinitesimal $\mathop{d\mathbf{v}}$.
Further letting $\mathop{d\tvx_t}=\omega\mathop{d\mathbf{v}}$ and
% Further letting $\mathop{d\mathbf{v}}=\omega\mathop{d\tilde\vx'_t}$ and
$J(\epsilont)\mathop{d\mathbf{v}}=\mathop{d\epsilont}$ induces  %(\tilde\vx'_t)}$ induces
% $J(\epsilont)\mathop{d\tilde\vx'_t}=\mathop{d\epsilont(\tilde\vx'_t)}$ induces
% By introducing a slack infinitesimal $\mathop{d\mathbf{v}}=\omega\mathop{d\tilde\vx'_t}$ and letting $J(\epsilont)\mathop{d\mathbf{v}}=\mathop{d\epsilont(\tilde\vx'_t)}$, we induce:
\begin{equation}
\label{eq:dx}
\mathop{d\tvx_t} = \sqrt{\alpha_t}\mathop{d\Pt} + \omega\sqrt{1-\alpha_t}\mathop{d\epsilont}.  % (\tilde\vx'_t)}.
\end{equation}

\if 0
    For the style calibration, we need to handle the $\vx_t$ which contains style elements. Let $\vx'_t$ as the corresponding latent variable of the reverse process which may share predicted $\vx_0$ with $\Pt(\epsilont(\tilde{\vx}_t|\tildeh_t))$ i.e., content elements. We approximate $\vx'_t=\tilde{\vx}_t+\mathop{d\vx_t}$ which can lead to similar results compared to that of Slerp. Note that we use the original DDIM reverse process with $\vx'_t$. \fref{fig:style_calibration} illustrates it. 
    For the approximation, we start from: 
    \begin{assumption}
     $\exists$ $\mathop{dv}$ such that $\epsilont(\tilde{\vx}_t|\tildeh_t) \approx \epsilont(\tilde{\vx}_t+\mathop{dv})$.
    \end{assumption}
    
    Let the $\Pt'$ be $\Pt(\epsilont(\vx'_t)) \approx \Pt(\epsilont(\tilde{\vx}_t|\tildeh_t))=\Pt(\epsilont(\tilde{\vx}_t))+\mathop{d\Pt}$. Following DDIM, $\sqrt{\alpha_t}\Pt = \mathbf{x}_t - \sqrt{1-\alpha_t} \epsilont(\mathbf{x}_t)$. From the DDIM equation, we can define infinitesimal as
    $\sqrt{\alpha_t}\mathop{d\Pt} = \mathop{d\vx_t} - \sqrt{1-\alpha_t} J(\epsilont) \mathop{d\vx_t}$.
    % From the equation derived in DDIM: $\sqrt{\alpha_t}\Pt = \mathbf{x}_t - \sqrt{1-\alpha_t} \epsilont(\mathbf{x}_t)$, we can define infinitesimal as
    % $\sqrt{\alpha_t}\mathop{d\Pt} = \mathop{d\vx_t} - \sqrt{1-\alpha_t} J(\epsilont) \mathop{d\vx_t}$.
    With $J(\epsilont)\mathop{dv}=\mathop{d\epsilont}$ and approximation of $\omega\mathop{dv}=\mathop{d\vx_t}$, we can rewrite it as:
    \begin{equation}
    \label{eq:dx}
    \mathop{d\vx_t} = \sqrt{\alpha_t}\mathop{d\Pt} + \omega\sqrt{1-\alpha_t}\mathop{d\epsilont}.
    \end{equation}
\fi
Then, we define $\tvx'_t = \tvx_t+d\tvx_t$ and obtain $\tilde\vx'_{t-1}$ by a typical denoising step.

In addition, $\Pt(\epsilont(\tvx'_t))$ in \eref{eq:dx} has larger standard deviation than $\Pt(\epsilont(\tvx_t))$. We regularize it to have the same standard deviation of $\Pt(\epsilont(\tvx_t))$ by 
% Since $\Pt$ is from the edited $\epsilont$, regularization with $\tvx'_t$ should be performed for original DDIM reverse process in \eref{eq:dx}.
% For the regularization, we keep the mean of $\Pt$ and normalize the pixel-to-pixel standard deviation of $\Pt$ to be the same as $\Pt(\epsilont(\tilde{\vx}_t))$; 
\begin{equation}
\mathop{d\Pt}= \frac{\Pt' - \bar{\Pt'}}{|\Pt'|}|\Pt|+\bar{\Pt'}-~\Pt(\epsilont(\tilde{\vx}_t)),
% \mathop{d\Pt}= \text{normalized }\Pt(\epsilont(\tvx'_t))-~\Pt(\epsilont(\tilde{\vx}_t)).
\end{equation}
where $\Pt'=\Pt(\epsilont(\tvx'_t))$.
% Then we can approximate $\tvx'_t$ with an appropriate value of~$\omega$. 
Then we control $\vx'_t$ with an $\omega$. 

% To do so, we regularize $\Pt'$ by keeping the mean of $\Pt'$ and normalizing the pixel-to-pixel standard deviation of $\Pt'$ to be the same as $\Pt$; $\mathop{d\Pt}=$ normalized~$\Pt'-\Pt(\epsilont(\tilde{\vx}_t))$. Then we can approximate $\vx'_t$ using an appropriate value of $\omega$. 

When we further expand \eref{eq:dx} by the definition of $\Pt$,
\begin{equation}
\label{eq:dx2}
\mathop{d\tvx_t} \approx (\omega-1)\sqrt{1-\alpha_t}(\epsilont(\tilde{\vx}_t|\tildeh_t)-\epsilont(\tilde{\vx}_t)).
\end{equation}
Interestingly, setting $\omega=1$ reduces $\mathop{d\tvx_t}$ to $\mathbf{0}$, i.e., injection does not occur. And setting $\omega \approx 0$\footnote{$\omega$ can not be 0 because of its definition.} drives $\tvx'_{t-1}$ close to $\tvx_{t-1}$, i.e., \warigari{} does not occur.
% Interestingly, when we set $\omega=1$, $\mathop{d\vx_t}$ becomes 0, leading the result to resemble the style image. And when we set $\omega \approx 0$\footnote{$\omega$ can not be 0 because of its definition.}, the original DDIM reverse process with $\vx'_t$ becomes closer to the result of Slerp.
Intuitively, by \eref{eq:dx2}, $\tvx'_t$ may share the predicted $\tvx_0$ with $\Pt(\epsilont(\tilde{\vx}_t|\tildeh_t))$ and contains original elements. In other words, we maintain the original elements by adding $\mathop{d\tvx_t}$ directly in \textit{x-space} while the content injection is conducted in \thspace{}. 
% Note that we use $\vx'_t$ for the DDIM reverse process without perturbation, which does not harm the correlation between $\vht$ and skip connections.

\Warigari{} consists of four steps. First, we inject the contents as $\tilde{\vx}_t \to \tilde{\vx}_{t-1}$ with Slerp. Second, we regularize $\Pt$ to preserve the original signal distribution after injection. Third, we solve the DDIM equation $\tvx'_t=\tvx_t+\mathop{d\tvx_t}$ by using \eref{eq:dx}. Finally, we step through a reverse process $\tvx'_t \to \tvx'_{t-1}$. In summary, we obtain target $\tilde{\vx}_{t-1}$ by Slerp and generate $\tvx'_{t-1}$ without feature injection with calculated the corresponding $\tvx'_t$. Please refer to Algorithm \ref{algo:full} for details.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{./figures/style_calibration.pdf}
    \caption{\textbf{\Warigari{}.} The result of DDIM reverse process with given approximated $\tvx'_t$ can be similar to the result of a corresponding injected result $\tvx_{t-1}$. As $\omega$ gets close to 1, more original elements are added through $\mathop{d\vx_t}$. Note that the effect of \warigari{} is different from modifying $\gamma$ because it remains predicted $\tvx_0$ by solving the DDIM equation.}
    % \caption{Overview of Asyrp++}
    \vspace{-1.5em}
    \label{fig:style_calibration}
\end{figure}


\subsection{Full generative process}
We observe that \thspace{} contains content and skip connection from $\vx_T$ conveys the original elements. We utilize this phenomenon for in-domain samples and out-of-domain artistic samples. Note that it is possible to obtain inverted $\vx_T$ from any arbitrary real image. Therefore, even if we use out-of-domain images such as artistic images, \ours{} successfully retain the original elements in the images. Furthermore, local mixing of \thspace{} enables injecting content into the corresponding target area as shown in \fref{fig:limitation_mask}.

 \begin{algorithm}[b!]
    \caption{InjectFusion}
    \label{algo:content_injection}
    \DontPrintSemicolon
    \SetAlgoNoLine
    \SetAlgoVlined
    % \SetKwProg{Re}{Require}{:}{}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwFunction{MultiTransfer}{MultiTransfer}
    \KwIn{$\vx_T$ (inverted latent variable from from image $\Istyle{}$),$\{\vhtcon{}\}_{t=t_{edit}}^{T} $(obtained from content image $\Icon{}$), $\epsilon_{\theta}$ (pretrained model), $m$ (feature map mask), $f$ (Slerp)
    } 
    \KwOut{$\tilde{\vx}_0$ (transferred image)}
    \BlankLine
    $\tvx_t \xleftarrow{} \vx_T$
    \For{$t=T,...,1$}{ 
    \If{$t\ge t_{edit}$}{
        Extract feature map $\vht$ from $\epsilon_{\theta}(\tvx{}_t)$;
        $\tildeh_t \xleftarrow{} f((m \otimes \vht), (m \otimes \vhtcon), \gamma)$ \par
        \qquad \qquad \qquad \qquad \qquad 
        $\oplus (1-m) \otimes \vht$\
        $\tilde{\epsilon} \xleftarrow{} \epsilon_{\theta}(\tvx_t | \tildeh_t)$, 
        $\epsilon \xleftarrow{} \epsilon_{\theta}(\tvx_t)$ \\
        Adapt Latent calibration (Algorithm \ref{algo:full})
        % \textcolor{red}{latent calibration (Algorithm 2) TODO}
    }
    \Else{
        $\tilde{\epsilon} = \epsilon \xleftarrow{} \epsilon_{\theta}(\tvx_t)$, 
    }
    $\tvx_{t-1}\xleftarrow{}\sqrt{\alpha_{t-1}} (\frac{\tvx{}_{t}-\sqrt{1-\alpha_{t}}\tilde{\epsilon}}{\sqrt{\alpha_{t}}}) 
    +\sqrt{1-\alpha_{t-1}}\epsilon$
    }
\end{algorithm}

For the local mixing, each $\vht$ is masked before Slerp and the mixed $\vht$ is inserted into the original feature map. We provide Algorithm \ref{algo:content_injection} for them and an illustration of spatial $\vh_t$ mixing in \fref{fig:spatial_slerp}. Note that we omit \warigari{} in the algorithm for simplicity. The full algorithm is provided in Appendix Algorithm~\ref{algo:full}.

\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}





\section{Experiments}
In this section, we present analyses on \ours{} and showcase our applications.
\paragraph{Setting} We use the official pretrained checkpoints of DDPM++ \cite{song2020score,meng2021sdedit} for CelebA-HQ~\cite{karras2018progressive} and LSUN-church/-bedroom~\cite{yu2015lsun}, iDDPM~\cite{nichol2021improved} for AFHQv2-Dog~\cite{choi2020stargan}, and ADM with P2-weighting~\cite{dhariwal2021diffusion,choi2022perception} for \textsc{MetFaces}~\cite{karras2020training} and ImageNet~\cite{imagenet}. The images have a resolution of $256\times256$ pixels. We freeze the model weights.
We use $\tedit$=400, $\omega$=0.3, $\gamma$=0.6, and $\tboost$=200 to produce high-quality images. For more implementation details, please refer to \aref{supp:implement}.

\paragraph{Metrics} GRAM loss (style loss) \cite{gatys2016image} indicates the style difference between the original image and the resulting image. ID computes the cosine similarity between face identity~\cite{deng2019arcface} of the content image and the resulting image to measure content consistency. Fr\'echet Inception Distance (FID)~\cite{heusel2017gans} provides the overall image quality. To compute FID, we compare generated 5K images from fixed 5K original-content image pairs using 50 steps of the reverse process and 25k images from the training set of CelebA-HQ without the overlap of the pairs.
%\todo{CLIP score}


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/slerp_ratio_id_style.pdf}
    \vspace{-2.0em}
    \caption{\textbf{Choice of $\gamma$.} (b) shows that $\gamma$ should be less than $0.6$ since the ID change via content injection converges at the point. If $\gamma > 0.6$, the resulting image only departs from the original image and suffers quality degradation without any advantage.}
    % \caption{Overview of Asyrp++}
    \vspace{-1.0em}
    \label{fig:gamma_id_style}
\end{figure}



\subsection{Analyses}
\label{sec:analyses}
In this section, we define what elements come from the original and the content image. We provide a guideline for choosing the content injection ratio $\gamma$ considering both quality and content consistency. We also show the versatility of latent calibration and propose the best interval for editing. Furthermore, we provide quantitative results that support assumptions suggested in \sref{sec:method}: \thspace{} has content elements.


\begin{table}[t]
\footnotesize
\centering
\resizebox{\columnwidth}{!}{

\begin{tabular}{l|cccccccc}
      [\%] & \multicolumn{1}{l}{Nose} & \multicolumn{1}{l}{Eyes} & \multicolumn{1}{l}{Jaw line} & \multicolumn{1}{l}{Expression} & \multicolumn{1}{l}{Hair color} & \multicolumn{1}{l}{Glasses} & \multicolumn{1}{l}{Skin color} & \multicolumn{1}{l}{Make up} \\ \hline
Original   & 28.06                     & 43.57                    & 24.67                         & 36.73                                  & \textbf{95.74}                 & 5.63                        & \textbf{94.15}                 & \textbf{90.60}              \\
Content & \textbf{71.94}           & \textbf{56.43}           & \textbf{75.33}               & \textbf{63.27}                        & 4.26                           & \textbf{94.37}               & 5.85                           & 9.30                       
\end{tabular}%
}
\resizebox{\columnwidth}{!}{
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|cccclcc}
%      & \multicolumn{5}{c}{Style mixing}                                                                                                                             & \multicolumn{2}{c}{Local style mixing}                     \\
% (b)        & \multicolumn{1}{l}{Ours} & \multicolumn{1}{l}{DiffuseIT} & \multicolumn{1}{l}{StarGan v2} & \multicolumn{1}{l}{JoJoGAN} & \multicolumn{1}{l|}{SAE}           & \multicolumn{1}{l}{Ours} & \multicolumn{1}{l}{StyleMapGAN} \\ \hline
% Original   & \textbf{3.53}            & 2.61                          & 2.94                           & 3.29                        & \multicolumn{1}{l|}{2.81}          & \textbf{4.08}            & 3.00                            \\
% Content & 3.19                     & 3.45                          & 3.18                           & 3.01                        & \multicolumn{1}{l|}{\textbf{4.17}} & \textbf{3.58}            & 3.5                             \\
% Natural & 3.25                     & 2.67                          & 3.00                           & 2.28                        & \multicolumn{1}{l|}{\textbf{3.43}} & \textbf{3.91}            & 2.66                            \\ \hline
% Average & 3.25                     & 2.91                          & 3.04                           & 2.86                        & \multicolumn{1}{c|}{\textbf{3.47}} & \textbf{3.86}            & 3.05                           
% \end{tabular}%
}
\caption{\textbf{User study to define content} We conduct the user study with 50 participants. Users choose where the attributes of the resulting images come from.}
\vspace{-1em}
\label{tab:clip}
\end{table}

\paragraph{Definition of content}
We measured the CLIP score on CelebA attributes to reveal what information comes from the content and original images. We classify the attribute of the mixed image as closer to the original or content image with the CLIP score. 
% We further conduct a user study asking which elements are coming from which input in \tref{tab:clip}.
In short, content includes \textit{glasses, square jaw, young, bald, big nose, and facial expressions} and the remaining elements include \textit{hairstyle, hair color, bang hair, accessories, beard, and makeup}. Please see the details in \aref{supple:moredetail}.
Furthermore, we conduct a user study in \tref{tab:clip} to support the result of the CLIP score. It aligns with the results using CLIP score for classifying.

We define the retained elements of the original image as the color-dependent attributes and the content as the semantics and shape.
% It aligns with the results using CLIP. 
\fref{fig:supple_church} and \fref{fig:supple_bedroom} show that DMs trained on the scenes with complex layouts have different notions of content and retained elements: rough shapes of churches are considered as content and room layouts including the location of beds are considered as contents.

\paragraph{Content injection ratio $\mathcal{\gamma}$}
\label{sec:gamma}
We suggest that the original $\vht$ should be partially kept in \sref{sec:replace}. \fref{fig:gamma_id_style} supports that the content injection ratio $\gamma$ should be less than 0.6 for image quality (FID) and preservation of the original image, and $\gamma>0.6$ does not increase ID similarity.
We provide more observations on $\gamma$ in \aref{supp:gamma}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{./figures/warigari_qual_v3.pdf}
    % \vspace{-10pt}
    \vspace{-0.5em}
    \caption{\textbf{Effectiveness of \Warigari{}.} \Warigari{} recovers elements of original images while preserving content elements. We do not use other techniques such as quality boosting for comparison.}
    % \caption{Overview of Asyrp++}
    \vspace{-1em} %공간부족해지면 style calibration기.
    \label{fig:warigari_qual}
\end{figure}

\paragraph{The effect of \warigari{}}
\fref{fig:warigari_qual} shows
% As shown in \fref{fig:warigari_qual}, we found 
that \warigari{} leads to a better reflection of the original elements such as makeup and hair color. 
Note that, depending on the latent calibration strength $\omega$, there is a trade-off relationship between Gram loss and ID similarity as well as FID. We report them at various $\omega{}$ in \fref{fig:warigari_quan}.
We discover that increasing $\omega{}$ favors preserving the original images. More details including the efficiency of adapting latent calibration to other methods, Plug-and-Play\cite{tumanyan2023plug} and MasaCtrl\cite{cao2023masactrl}, can be found in \aref{supp:omega}.




\begin{table}[!t]
\centering
\begin{tabular}{l|ccc}
\multicolumn{1}{l|}{} & FID $\downarrow$  & ID $\uparrow$    & Gram loss $\downarrow$ \\ \hline
%Reconstruction        & 38.71 &        &            \\ \hline
$\vht{} + \vhtcon{}$                & 49.94 & 0.3581 & 0.0415     \\
Lerp                  & 36.89 & 0.4040 & 0.0318     \\
Slerp                 & \textbf{32.09} & \textbf{0.4390} & \textbf{0.0310}             
\end{tabular}
% \vspace{-0.5em}
\caption{\textbf{Performance of various configurations} Slerp improves FID, ID similarity between target content images and synthesized images over other methods. }
\vspace{-0.5em}
\label{tab:ablation}
\end{table}

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/why_tend400_v3.pdf}
    \vspace{-1.5em}
    \caption{\textbf{Choice of $\tedit$} We observe that $\tedit=400$ shows the best quality.} 
    \vspace{-0.5em}
    \label{fig:t_end}
\end{figure}  

\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\linewidth]{./figures/diffuseIT_compare_v2.pdf}
    \vspace{-1.5em}
    \caption{\textbf{Comparison with DiffuseIT} \ours{} is effective even in situations where there is a large discrepancy between the color distributions of the original image and the content image.} 
    \vspace{-1.5em}
    \label{fig:diffuseIT_compare}
\end{figure}  

% \begin{strip}


% \end{strip}


% \begin{strip}
% \centering
%   \includegraphics[width=0.9\textwidth]{./figures/teaser.pdf}
%       \vspace{-0.3em}
%     \captionof{figure}{
%     \ours{} allows (a) style mixing by content injection within the trained domain, (b) local style mixing by injecting masked content features, and (c) harmonization-like style transfer with out-of-domain style references. All results are produced by frozen pretrained diffusion models.
%     Furthermore, flexibility of \ours{} enables content injection into any style.}
%     \vspace{-0.5em}
%     \label{fig:teaser}
    
% \end{strip}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{./figures/test_celeba.pdf}
%     \captionof{figure}{Qualitative results of on CelebA-HQ. We provide the results of local editing (left) and style transfer with out-of-domain images (right).}
%     \label{fig:celeba}
%     \vspace{-1em}
% \end{figure*}

\paragraph{Quantitative comparison} \tref{tab:ablation} shows the quantitative result of each configuration investigated in \sref{sec:method}. Reconstruction reports FID of the official checkpoint of DDPM++ \cite{meng2021sdedit} through its forward and reverse process without any modification on \thspace{}. We observe that $\vht+\vhtcon$ harms FID with severe distortion.
Slerp outperforms $\vht+\vhtcon$ in all aspects. 
\if 0
We note that our FID score is lower than that of the reconstruction images, indicating that the quality of the generated images is similar to that of inversion images. Reconstructed images are known to have a higher FID than randomly generated images in DMs with a low number of score function evaluations (NFE). \cite{asperti2022image,zhu2023boundary} We suppose that the FID of our method is lower than reconstruction images because the manipulation in \thspace{} makes the generation process closer to random generation. 
\fi
% \aref{supp:qualitative} provides more qualitative results. 

\tref{tab:ablation} further shows the superiority of Slerp over linear interpolation (Lerp). It implies that the normalization for preserving the correlation between $\vht$ and skips $\vg_t$ is important.
% \tref{tab:ablation} also provides a comparison between Slerp and linear interpolation (Lerp) to further support our analysis. 
% Slerp outperforms Lerp, indicating that the normalization of Slerp is an essential component for preserving the correlation between $\vht$ and skips $\vg_t$.
% It means that Slerp achieves better image quality than Lerp due to our adjustments in the relations of $\vh_t$ and the skip connection to correct the variation of $\vx_0$. 
% It implies that modifications to the relationship between $\vh_t$ and the skip connection 
% Lerp produces more severe distortions on images, especially when the difference between the norms of $\vht$ and $\vhtcon$ is larger. \todo{$\leftarrow$grounding exp?}
Furthermore, \fref{afig:compare_lerp_slerp} shows that Slerp resolves the remaining artifacts that reside in the resulting images by Lerp. %, which supports that the consistency of the correlation is important in DMs. 
Comparison between Slerp and Lerp will be further discussed in \sref{supp:lerp}.


\paragraph{Editing interval $[T, \tedit]$}
We observe that there is a trade-off between ID similarity and Gram loss when using a suboptimal $\tedit$ and specific value of $\tedit$ leads to better FID, as shown in \fref{fig:t_end}. 
We choose $\tedit=400$ for its balance among the three factors.
This choice also aligns with that of Asyrp \cite{kwon2022diffusion} for editing toward unseen domains, which requires a large change, such as injecting content.
Notably, we find that $\tedit=400$ is also suitable for achieving content injection into artistic images.

\paragraph{Choice of the content injection layer}
% \paragraph{Content injection on the other intermediate features}
Except for \thspace{}, the other intermediate layers in the U-Net can be candidate feature spaces for content injection. However, \fref{fig:content_injection_on_other_layer}a shows that content injection works well only on \thspace{}, while it produces artifacts and loses injected content on the other feature spaces. Injecting skip connection while content injection does not alleviate the problems as shown in \fref{fig:content_injection_on_other_layer}b.

% \paragraph{Skip connection injection}


\subsection{Qualitative results}
\paragraph{In-domain original images}
\fref{fig:mixing}a,b shows \ours{} on AFHQv2-Dog~\cite{choi2020stargan} \textsc{MetFaces}~\cite{karras2020training}.
%AFHQv2-Dog~\cite{choi2020stargan} \textsc{MetFaces}~\cite{karras2020training}. 
See \aref{supple:moreresults} for more results on various architectures and datasets.

\paragraph{Artistic original images}
In addition, we can use arbitrary original images, even if they are out-of-domain. \fref{fig:mixing}c shows results with artistic images as style. For the artistic references, we do not use quality boosting \cite{kwon2022diffusion} since they aim to improve the quality and realism of $\vx_0$ which may not be desirable when transferring the elements of an out-of-domain image onto the target image. We provide more results in \aref{supple:moreresults}.



\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]
    {./figures/main_yesi.pdf}
    \vspace{-0.5em}
    \captionof{figure}{
    \textbf{Qualitative results of \ours{}.} 
        (a), (b) \ours{} allows image mixing by content injection within the trained domain, and (c) out-of-domain artistic references to be original images. All results are produced by frozen pretrained DMs.
 }
    \vspace{-1.0em}
    \label{fig:mixing}
\end{figure*}

\begin{figure}[!t]
    \centering
    % \vspace{-0.5em}
    \includegraphics[width=\linewidth]{./figures/limitation_mask4.pdf}
    \vspace{-1.0em}
    
    \caption{\textbf{Local style mixing with various feature map mask sizes.} Adjusting the size and position of the feature map mask enables to handle the area of content injection, facilitating control of local style mixing.}
    \vspace{-0.5em}
    \label{fig:limitation_mask}
\end{figure}

\subsection{Comparison with existing methods}
 We first note that there is no competitor with perfect compatibility: frozen pretrained diffusion models, and no extra guidance from external off-the-shelf models.
 %, and versatility (content injection, masked injection, and style transfer). 
 Still, we compare our content injection with DiffuseIT~\cite{kwon2022diffuseIT} which guides pretrained DMs using DINO ViT \cite{caron2021emerging}.
\fref{fig:diffuseIT_compare} shows that DiffuseIT struggles when there is a large gap between the content image and the original image regarding color distributions. More qualitative comparisons with existing methods~\cite{park2020swapping,kim2021exploiting,deng2021stytr,wu2022ccpl, choi2020stargan, chong2022jojogan} and user study are deferred to \aref{supp:qualitative}.







\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{./figures/stable_diffusion_v2.pdf}
    \vspace{-1.0em}
    % \vspace{-0.5em}
    
    \caption{\textbf{\ours{} on Stable diffusion} Although we observe similar phenomenons, the content elements of latent-level DMs is different from pixel-level DMs; More semantic elements is injected to the original image.}
    \vspace{-1.5em}
    \label{fig:stablediffusion}
\end{figure}    




\section{Conclusion and discussion}
\label{sec:conclusion}
In this paper, we have proposed a training-free content injection using pretrained DMs. The components in our method are designed to preserve the statistical properties of the original reverse process so that the resulting images are free from artifacts even when the original images are out-of-domain. We hope that our method and its analyses help the research community to harness the nice properties of DMs for various image synthesis tasks.

Although InjectFusion achieves high-quality content injection, the small resolution of the \thspace{} hinders fine control of the injecting region. We provide content injection with various masks in \fref{fig:limitation_mask}.

While out-of-domain images can be used as the original image (i.e., style), injecting content-less out-of-domain images leads to meaningless results. We provide them in \fref{fig:limitation_ood}. We suggest that $\vht$ is not the universal representation for arbitrary content.

In addition, we provide pilot results of \ours{} on Stable diffusion in \fref{fig:stablediffusion}. It works somewhat similarly but the phenomenon is not as clear as in non-latent diffusion models. The bottleneck of Stable diffusion appears to be more semantically rich, possibly due to its diffusion in VAE's latent space.
 Unveiling the mechanisms in latent diffusion models remains our future work. Please refer to \aref{supple:stable} for the details.

Lastly, we briefly discuss the effect of the scheduling strategy of the injecting ratio $\gamma$ in \aref{appendix:gamma_scheduling}. Further investigation would be an interesting research direction.

\subsubsection*{Acknowledgments}
This work was supported by the National Research Foundation of Korea (NRF) funded by the Korean government (MSIT) (RS-2023-00223062).
%-------------------------------------------------------------------------



%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}



\clearpage{}

\def\supp{1}
\input{supp}

\end{document}
