\section{Introduction}

Recently, generative models have made remarkable progress across various domains. 
Diffusion models \citep{thermodynamics, ddpm}, in particular, have shown state-of-the-art generation performance across multiple tasks, including text-to-image (TTI) generation \citep{rombach2022high, saharia2022photorealistic, ramesh2022hierarchical}. 
In addition to generating images from text prompts, several works have used a pretrained TTI model for extended applications, such as personalized text-to-image \citep{ruiz2022dreambooth, gal2022image, kumari2022multi} or text-guided image editing \citep{hertz2022prompt, mokady2022null, kawar2022imagic}.



\begin{figure}[t]
\begin{center}
\makebox[0.12\textwidth]{\colorbox{pink}{\textbf{Training video}} A man is doing a pushup.}\\
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_1.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_2.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_3.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_4.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_5.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_6.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_7.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/training/frame_8.pdf}

\makebox[0.12\textwidth]{A \textcolor{blue}{\textbf{Iron Man}} is doing a pushup.}\\
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_1.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_2.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_3.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_4.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_5.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_6.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_7.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/hulk/frame_8.pdf}

\makebox[0.12\textwidth]{A \textcolor{blue}{\textbf{digital illustration}} that a man is doing a pushup.}\\
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_1.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_2.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_3.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_4.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_5.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_6.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_7.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/illu/frame_8.pdf}

\makebox[0.12\textwidth]{A \textcolor{blue}{\textbf{gorilla}} is doing a pushup, \textcolor{blue}{\textbf{cartoon style}}.}\\
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_1.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_2.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_3.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_4.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_5.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_6.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_7.pdf}
\includegraphics[width=0.11\textwidth]{figures/teaser/gorilla_cartoon/frame_8.pdf}
\caption{Edit-A-Video performs text-guided video editing from a single $<$text, video$>$ pair and a text-to-image model.}
\label{fig:teaser}
\vspace{-5mm}
\end{center}
\end{figure}

Inspired by the success of the diffusion-based TTI models, various works have extended their output modality to videos, including text-to-video (TTV) generation and text-guided video editing.
Among them, text-guided video editing models perform editing by leveraging the prior knowledge of the TTV model \citep{molad2023dreamix, esser2023structure} or the inflated TTI model that captures temporal information \citep{wu2022tune}. 
Several works \citep{wu2022tune, ceylan2023pix2video} generate the whole video while editing, resulting in the editing of unwanted areas. 
Other methods \citep{liu2023videop2p, qi2023fatezero} have proposed using image editing techniques to specify areas to be edited within each frame. 
However, these approaches have limitations in that they do not consider temporal information when calculating the editing regions for each frame.

In this paper, we propose Edit-A-Video, a framework designed for achieving temporal consistency in text-guided video editing.
Edit-A-Video is a two-stage framework (see Fig. \ref{fig1} for illustration). 
In the first stage, a pretrained 2D TTI model is inflated to a 3D TTV model with attention modules for spatio-temporal modeling, which is finetuned using a single source video.
In the second stage, the source video is edited to match the target text description by inverting the source video to the Gaussian noise and injecting attention maps from the source to the target along the denoising process.

As previously mentioned, one of the main challenges in diffusion-based video editing is the accurate handling of the target object and the background. 
We observe that the edited video suffers from a \textit{background inconsistency problem}, where the edited video contains abrupt temporal changes in the background, which significantly degrades the quality. 
To tackle this issue, we propose a novel blending method tailored to the task, called \textit{temporal-consistent blending (TC Blending)}. TC Blending extends a spatial local blending technique originally proposed for a still image \citep{hertz2022prompt} and automatically generates a sharp, spatio-temporally consistent blending mask that closely approximates the region to be edited in the source video.

Together with the proposed TC Blending method, Edit-A-Video can effectively generate a realistic video that matches the target text description and captures the dynamic actions of the source video ensuring a smooth transition, while also maintaining the spatio-temporal consistency of the background.
We carry out extensive experiments on one-shot editing over various videos and prompts, and demonstrate that Edit-A-Video is the most favorable method compared to baselines through subjective human preference study along with qualitative analysis.
We further conduct an in-depth analysis by comparing to baselines based on automatic evaluation of numerical metrics, which shows the improvement of consistency and text alignment.
We also demonstrate the effectiveness of the proposed TC Blending in terms of consistency and overall editing quality through ablation studies.

In summary, our key contributions are as follows:
\begin{itemize}
    \item This study presents Edit-A-Video, a one-shot video editing framework that effectively combines a pretrained text-to-image model and editing techniques suitable for video editing.
    \item Along with the extension of image editing techniques to video, we propose a novel blending method called temporal-consistent blending (TC Blending), alleviating the background inconsistency problem. 
    \item We analyze the effect of injecting each attention map for spatio-temporal consistency in our framework.
\end{itemize}
