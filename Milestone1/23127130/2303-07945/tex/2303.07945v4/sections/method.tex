\section{Method}
\begin{figure}[t]
\begin{center}
    \includegraphics[width=\linewidth]{figures/arch.pdf}
\caption{\textbf{Overall Editing Procedure of Edit-A-Video} In stage 1, Edit-A-Video inflates the 2D UNet into 3D model by appending the temporal module and evolving the 2D conv into 3D conv and self-attention into sparse spatio-temporal attention. Then, the source video is inverted to a specific Gaussian noise by DDIM inversion, and null-text embedding is optimized so that the source video can be reconstructed along with the null-text guidance in stage 2(a). Finally, in stage 2(b), the edited video is generated from the inverted noise and target text through attention map injection, TC Blending, and optimized null-text embedding.} 
    \label{fig1}
    \vskip -0.1in
    \vspace{-5mm}
\end{center}
\end{figure}
In this section, we introduce Edit-A-Video, a framework designed to consistently edit a given video into a desired object or style using a diffusion-based TTI model.
In Sec. \ref{framework}, we formulate our 2-stage editing procedure, extending TTI model to learn the temporal relationship and editing the contents of the source video according to the target prompt \\textcolor{red}{via} attention map injection.
To achieve more consistent editing frame by frame, we propose a temporal-consistent blending (TC Blending) method in Sec. \ref{sc}. 
Finally, we discuss the role and effect of three types of attention modules in our methodology in Sec. \ref{hparams}.

\subsection{Framework} \label{framework}

As shown in Fig. \ref{fig1}, Edit-A-Video follows a two-step process to edit the given video corresponding to a target prompt. 
In the first stage, we inflate the TTI model to TTV model using the method of Tune-A-Video \citep{wu2022tune}.
Unlike the TTI model, which has two types of attention (self-attention in images and cross-attention between text and image), our inflated TTV model has three types of attention: cross-attention, temporal attention, and sparse spatio-temporal attention. 
We only train these attention modules in the inflated 3D TTV model using a single video, ensuring frame-by-frame consistency during video editing.

In the second stage, we extract the inversion trajectory $\{z_{t}\}_{t=1}^{T}$ starting from the source video to the Gaussian noise ${z_{T}}$ and train the null-text embeddings so that the generation trajectory from ${z_{T}}$ is still close to the inversion trajectory under the classifier-free guidance.
Starting from the latent variable ${z_{T}}$ of the source video, we edit the video by injecting three types of attention maps of the source video into the generation process of edited video, extending the editing methods in the image domain \citep{hertz2022prompt, mokady2022null}.
Since the inflated model has newly added attentions previously absent in the TTI model, we analyze the role and effect of each attention module and describe in Sec. \ref{hparams}.

However, when we edit the video as described above, we observe that unwanted region is edited inconsistently along the frames, which results in undesirable and abrupt artifacts. 
We call this issue the \textit{background inconsistency problem}.
To mitigate this issue, we analyze the cause and propose a \textit{temporal-consistent blending}, which we call \textit{TC Blending} for short, in the following section.


\begin{figure*}[t]
\begin{center}
\makebox[0.12\textwidth]{A man is on the surfing $\rightarrow$ A \textcolor{blue}{\textbf{wooden man sculpture}} is surfing}\\

\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_1.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_2.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_3.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_4.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_5.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_6.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_7.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_vis/step_8.pdf}

\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_1.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_2.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_3.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_4.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_5.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_6.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_7.pdf}
\includegraphics[width=0.11\textwidth]{figures/ablation/surfing_result/step_8.pdf}

\caption{\textbf{Background Inconsistency Problem} The spatial local blending mask originally proposed in \cite{hertz2022prompt} cannot consider the temporal consistency, resulting in temporally variant background after editing, which can be identified in the waves close to the wooden sculpture.}
\label{fig:bg}
\end{center}
\vspace{-1.3em}
\end{figure*}


\subsection{Temporal-Consistent Blending} \label{sc}
When Edit-A-Video edits an object, only the specific region that correlates to the target object should be modified, while the rest of the video should be preserved.
In previous work of image editing, PTP \citep{hertz2022prompt} proposed the local blending method, which approximates a mask of the object region from the cross-attention map and performs editing only in the masked region.
However, since the cross-attention of the inflated 3D model is computed frame-by-frame, the local blending mask from the cross-attention map considers only the spatial dimension and lacks modeling of temporal dependency.
This indicates that the smoothness of blending masks across frames cannot be ensured, resulting in the potential occurrence of color-variant artifacts when a specific region is included in the blending mask of one frame but not in another.
This background inconsistency problem is demonstrated clearly in Fig. \ref{fig:bg}, where the background region close to the target object of editing is severely distorted, and the frames are highly inconsistent across the temporal axis.

To address this problem, we propose a novel blending method called temporal-consistent blending (TC Blending), which acquires a spatio-temporally consistent blending mask.
For efficient temporal modeling, we use the sparse ST attention map in our 3D inflated model as a proxy for the interaction between the current frame mask and the first and previous frame masks.
The sparse ST attention map is calculated by taking the current frame feature $z_{f}$ as query, the first frame feature $z_{1}$ and the previous frame feature $z_{f-1}$ as key:
\begin{equation}
    \mathrm{ST}\mbox{-}\mathrm{Attn}_{f} = \mathrm{Softmax}(QK^T/\sqrt{d}), 
    Q = W^{Q}z_{f}, K = W^{K}[z_{1}; z_{f-1}],
\end{equation}
where $f$ is the index of current frame, [;] denotes the concatenation, and $d$ is the feature dimension of the projection layer.  

Intuitively, the sparse ST attention map of the current frame with the first frame ensures that the target object is maintained in every mask, while that with the previous frame enforces a smooth transition in the blending mask sequence.
With TC Blending, it is possible to achieve a sharp blending mask that accurately detects the target object in the frame and ensures a smooth transition in the blending masks, reducing background inconsistencies.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/scattn.pdf}
    \caption{\textbf{TC Blending Mask Computation} The first and previous frames interact with the corresponding sparse spatio-temporal attention map and yield the new temporal-consistent blending mask.}
    \label{fig2}
    \vskip -0.1in
\end{figure} 
Following \cite{hertz2022prompt}, we first acquire the cross-attention maps $m \in {\rm I\!R^{\mathit{F} \times \mathit{H} \times \mathit{W}}}$ according to the original word and new word as initial blending maps, where the $F,H,W$ are the temporal and spatial dimension of the feature.
Then, for each attention map $m$, we normalize it frame-wise to balance the scale between each frame and flattening:
\begin{equation}
    \tilde{m} = \mathrm{Flatten}(m/ \sum_{H,W} (m)), \text{where } \tilde{m} \in {\rm I\!R^{\mathit{F}x\mathit{HW}}}.
\end{equation}

Afterward, we encourage the interaction between the frame-wise maps by computing the weighted average of the first and previous map $[\tilde{m}_{1}; \tilde{m}_{f-1}] \in {\rm I\!R^{(\mathit{HW}\times 2)}}$ with sparse ST attention map $\mathrm{ST}\mbox{-}\mathrm{Attn}_{f} \in {\rm I\!R^{(\mathit{HW}\times2)\times(\mathit{HW})}}$ of the current frame, which is shown in Fig.~\ref{fig2}.
Then, we binarize it by thresholding:
\begin{equation}
    \alpha_{f} = B(\hat{m}_{f}, \tau), \hat{m}_{f} = [\tilde{m}_{1}; \tilde{m}_{f-1}] \times \mathrm{ST}\mbox{-}\mathrm{Attn}_{f},
\end{equation}
where $B$ is the binarizing function with threshold $\tau$.

Similar to PTP, we utilize the union of binary blending masks for the original word and new word, denoted as $\alpha$, as the final blending mask. This allows us to edit both the areas of the original object and the target object.
We use this binary blending mask $\alpha$ to perform local editing, where we preserve the background outside the mask and only generate the contents inside the mask:
\begin{equation}
    \hat{z}_{t} = \bar{z}_{t} \odot (1 - \alpha) + z_{t}^{*} \odot \alpha,
\end{equation}
where $\bar{z}_{t}$ is the reconstruction of the source video, $z_{t}^{*}$ is the edited video, and $\odot$ is element-wise multiplication.

\subsection{Hyperparameters for Editing}  \label{hparams}
The core idea of Prompt-to-Prompt (PTP) to preserve the spatial layout of input is by injecting 2D cross-attention maps and self-attention maps from pretrained TTI models.
The duration of the injection process, a portion of timestep over the entire sampling step until which the attention map is injected, is a key factor in controlling the reflection ratio of the target prompt. 
Edit-A-Video, on the other hand, uses sparse spatio-temporal attention and temporal attention instead of self-attention.
As a result, the effect of the duration of attention injection may differ from that of PTP.
In this section, we describe the effect of injection for each type of attention, and the corresponding samples are visualized in the Supplementary Materials.


\textbf{Cross-Attention}
The cross-attention layer performs an attention operation between text tokens and frames, taking into account the spatial layout of each text token in the frames. There is a trade-off depending on the duration of the injection phase. If injection occurs only at the beginning of generation, the generated frames are strongly conditioned on the target text prompt, but hard to maintain the spatial layout. 
On the other hand, if injection occurs throughout the entire generation process, 
the spatial layout of the source video is well preserved, yet it does not include the concepts from the text prompt. 
Empirically, we found that the duration of $0.2$ is sufficient to retain the spatial layout of the source video, while the generated video represents the semantics of the target text.

\textbf{Temporal Attention}
The temporal attention (T-Attn) layer is an additional attention module computed along the time axis of a video to model the temporal relationship between frames, yet it does not model spatial dependency. 
We found that T-Attn maps are distributed uniformly and that the duration of injection does not have a significant impact. We set a duration value of 0.8 for temporal attention. However, varying this value does not have a notable impact on the editing quality, and the corresponding qualitative results are in Supplementary Materials.

\textbf{Sparse Spatio-Temporal Attention}
The sparse spatio-temporal attention (ST-Attn) layer is an attention method designed for video, where the attention matrix of the current frame is calculated only on the first and previous frames.
ST-Attn replaces the self-attention layer from the pretrained TTI models, where the model only requires the dependencies between pixels in a single image. 
In addition to temporal attention, ST-Attn improves temporal consistency by attending to other frames while maintaining efficiency by only visiting two frames. 
We observed that insufficient duration of ST-Attn results in the inability to adequately represent the dynamic action of the source video. On the contrary, an excessively long duration tends to capture not only the actions of the source video but also the objects within it, which hinders the editing process toward the target object. 
Therefore, to maintain the dynamic of the source video and facilitate editing toward the target object, we set the duration of sparse spatio-temporal attention to 0.5.