\section{Background}
\subsection{Denoising Diffusion Probabilistic Models (DDPM)}
Denoising Diffusion Probabilistic Models (DDPM) \citep{thermodynamics, ddpm} is a type of probabilistic generative model that consists of a forward process that gradually transforms data into $z\sim N(0,I)$ and a reverse process, the opposite trajectory. Following notation in \cite{ddpm}, the forward process of diffusion is defined as follows:
\begin{equation}
  \begin{aligned}
    \label{forward diffusion}
    q(x_t|x_{t-1})&:=N(x_t;\sqrt{1-\beta_t}x_{t-1}, \beta_tI),\\
    x_t&=\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_t,
  \end{aligned}
\end{equation}
where $z_t\sim N(0,I)$, $\beta_t$ is a user-defined noise schedule, and $\bar{\alpha}_t:=\prod_{i=1}^{t}(1-\beta_i)$. Using Eq. \ref{forward diffusion}, we can obtain the posterior $q(x_{t-1}|x_t,x_0)=N(x_{t-1};\frac{1}{\sqrt{\bar{\alpha}_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t),\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_{t}}\beta_t)$.

For sampling data from noise, we define a reverse process that gradually transforms noise into data for sampling. Since $q(x_{t-1}|x_t)$ is intractable, \cite{ddpm} approximate this term to the transition network $p_\theta(x_{t-1}|x_t)$, which is defined in a Gaussian form as below.
\begin{equation}
    \begin{aligned}
    \label{reverse diffusion}
    p_\theta(x_{t-1}|x_t)&:=N(x_{t-1};\mu_\theta(x_t,t), \Sigma_\theta(x_t,t)), \\   
    \mu_\theta(x_t,t)&=\frac{1}{\sqrt{\bar{\alpha}_t}}(x_t-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t,t)),
    \end{aligned}
\end{equation}
where $\Sigma_\theta(x_t,t)$ can be a fixed value \citep{ddpm} or trainable parameters \citep{pmlr-v139-nichol21a}, and $\epsilon_\theta(x_t,t)$ can be trained by minimizing $\mathop{\mathbb{E}}_{t,x_0,\epsilon}[||\epsilon-\epsilon_\theta(x_t,t)||^2]$ \citep{ddpm}. On the contrary, DDIM \citep{ddim} proposes to set $\Sigma_\theta(x_t,t)$ to 0 for a subset of the reverse diffusion process, which enables a deterministic and fast sampling. In this paper, we follow the formulation of DDIM \citep{ddim}.

\subsection{Tune-A-Video}
Compared to text-to-image generation, text-to-video generation~\citep{singer2022make,ho2022imagen,esser2023structure} is a data-hungry task since the training of TTV model requires a large-scale $<$text, video$>$ paired dataset. 
Recently, Tune-A-Video \citep{wu2022tune} successfully trained a TTV model with a single video leveraging prior knowledge of a pretrained TTI model, while maintaining consistency between frames. To generate a temporally-coherent video, Tune-A-Video inflated 3x3 2D convolution layers from TTI models to 1x3x3 3D convolution layers and appended additional temporal attention (T-Attn) modules.
To enhance the temporal consistency more, Tune-A-Video replaces spatial self-attention with a novel attention layer named as sparse spatio-temporal attention (ST-Attn).
Instead of full frame attention which computes the attention for every other frame, ST-Attn computes the attention of current frames only on the first and the previous frame.
It reduces the computational cost of full frame attention which is quadratic with respect to the number of frames to linear cost, while maintaining the temporal consistency in video generation.
By tuning three types of attention layers (T-Attn, ST-Attn, and cross-attention between text and video) from the inflated model, Tune-A-Video is capable of generating temporally-coherent videos.

Despite its ability to synthesize video with temporal consistency, Tune-A-Video fails to maintain the contents that should remain unedited since it generates entire video frames using a modified text prompt.
For example, the background regions of the video generated by Tune-A-Video are modified and are not preserved with respect to the source video.
In contrast, Edit-A-Video edits a video with given text prompts while maintaining the attributes unrelated to the modified text based on the novel blending method.

\subsection{Real Image Editing with Null-Text Inversion} \label{null-text}

Text-guided image editing is a task to edit the content of a given image only specified by the text prompts.
While TTI models can produce high-quality images from the given text prompts, the model tends to generate completely different images as the source text is modified to serve as the editing target.
\cite{hertz2022prompt} observe that the cross-attention maps between the image feature and the source text reflect the spatial layout of the source image. 
Based on the remarkable property of cross-attention maps, \cite{hertz2022prompt} propose a novel framework, Prompt-to-Prompt (PTP), for image editing which injects the attention maps of the source image into the generation process conditioned on the target text.
PTP further enhances the degree of preservation of the source image by approximating the desired editing target region from the cross-attention maps and blending the target region of the edited image with the source image.

Although PTP is a strong framework for synthetic image editing, a latent vector corresponding to the real image is required for real image editing. 
DDIM inversion is one of the methods to invert the real image to a latent vector.
However, DDIM trajectory is severely distorted when a classifier-free guidance~\citep{ho2021classifierfree} is applied through the reverse process, which results in the poor reconstruction quality of the real image. 
As an effective approach for the limitation, Null-Text Inversion (NTI) \citep{mokady2022null} optimizes the null-text embedding which is used in classifier-free guidance so that the reverse process trajectory with classifier-free guidance does not deviate from the DDIM inversion trajectory.
NTI results in the near-perfect reconstruction of the image even under classifier-free guidance. 
Together with the PTP framework, NTI shows that text-guided image editing can also be applied to real image.

Despite the success of text-guided image editing, image editing methods cannot be directly applied to video editing.
Compared to image editing, maintaining temporal consistency is crucial when it comes to video editing.
We hence append sparse spatio-temporal attention (ST-Attn) and temporal attention (T-Attn) on top of NTI and PTP framework and propose a novel blending method called temporal-consistent blending (TC Blending) for successful video editing.

\subsection{Text-Guided Video Editing}
Recently, text-based image creations expand their domain from the image to the video. Text-to-Video models synthesize high-quality videos corresponding to a given text.
Likewise, following the text-guided image editing methods, various approaches for text-guided video editing have also been proposed. 
Text-guided video editing modifies the given source video to reflect the target text while preserving some content from the source video.
Earlier works~\citep{esser2023structure, molad2023dreamix} edit the video by exploiting the TTV model trained on a large-scale $<$text, video$>$ paired dataset, which is computationally challenging to most practitioners.
Other approaches~\citep{bar2022text2live, Lee_2023_CVPR} utilize the neural layered atlas (NLA) of the source video for editing. 
While they enable video editing by applying image editing methods to the NLA of the video, acquiring NLA is time-consuming and lacks efficiency.

In contrast to prior approaches, there are several works that leverage the TTI model to perform text-guided video editing similar to ours. Some previous works~\citep{wu2022tune, ceylan2023pix2video, vid2vid-zero} generate the entire video frames including the area not related to the target object during editing, which makes it difficult to maintain the content of the source video. Similar to Edit-A-Video, some concurrent works~\citep{qi2023fatezero, liu2023videop2p} perform editing through attention map control. 
These methods improve editing performance by blending the source video and target region specified by the target text using a mask extracted from the cross-attention map. 
However, the cross-attention map is calculated between text and each frame and does not consider temporal information between frames.
Thus, the frame-wise blending mask extracted from cross-attention map can be temporally inconsistent, resulting in undesirable artifacts including abrupt changes in the background.
To mitigate the aforementioned issue, Edit-A-Video proposes TC blending, a blending method that is capable of extracting sharp and frame-consistent masks by considering temporal information from sparse spatio-temporal attention.