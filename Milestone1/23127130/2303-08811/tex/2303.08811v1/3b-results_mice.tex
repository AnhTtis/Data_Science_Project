\subsection{Experiments on Mouse Triplet Dataset}

\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-6em}
\includegraphics[width=0.5\textwidth]{figures/mabe_sketch.pdf}
\caption{\footnotesize{\em Multi-Agent Behavior (MABe) - Mouse Triplets Challenge}. (A) Keypoint tracking approaches are used to extract keypoints from many positions on the mouse body in a video.
(B) Methods are evaluated across 13 different tasks. 
\vspace{-0.8em}}  \label{fig:mousedata}
\end{wrapfigure}

\subsubsection{Experimental Setup and Tasks}

\paragraph{Dataset description.}
The mouse triplet dataset \cite{sun2022mabe22} is part of the Multi-Agent Behavior Challenge (MABe 2022), hosted at CVPR 2022. This large-scale dataset was introduced to address the lack of standardized benchmarks for representation learning of animal behavior. It consists of a set of trajectories from three mice interacting in an open-field arena. A total of 5336 one-minute clips, recorded from a top-view camera, were curated and processed to track twelve anatomically defined keypoints on each mouse, as shown in Figure~\ref{fig:mousedata}.  

As part of this benchmark, a set of 13 common behavior analysis tasks were identified, and are used to evaluate the performance of representation learning methods. 
Over the course of these sequences, the mice might exhibit individual and social behaviors. Some might unfold at the frame level, like chasing or being chased, others at the sequence level, like light cycles affecting the behavior of the mice or mouse strains that inherently differentiate behavior.

\input{tables/mabe_benchmark}

\vspace{-2mm}
\paragraph{Training protocol.}
We process the trajectory data to extract 36 features characterizing each mouse individually, including head orientation, body velocity and joint angles. We construct the short-term TCN and long-term TCN encoders to have representation of size 32 each, and receptive fields approximated to be 60 and 1200 frames respectively. We compute the histogram of actions over 1 second, i.e. $L=30$ and use $K=32$ bins. More details can be found in Appendix~B.

We model the dynamics of each mouse independently, which means that at time t, and for each frame $t$, we produce embeddings $\z_{t,1}$, $\z_{t,2}$ and $\z_{t,3}$, for each mouse respectively. Since our work is about modeling temporal behavior dynamics, we do not focus on modeling animal interactions, and opt to use a simple auxiliary loss to predict the distances between the trio at time $t$. Our input features do not include any information about the global position of the mice in the arena, so the model can only rely on the inherent behavior and movement of each individual mouse to draw conclusions about their level of closeness. We build a network $h$ that takes in the embeddings of two mice $i$ and $j$ and predicts the  distance $d_{i,j}$ between them. 
\begin{equation}
    \mathcal{L}_{\mathrm{aux}} = \|h(\z_{t,i}, \z_{t,j}) - \mathrm{d}_{i,j}\|_2^2
\end{equation}

The model is trained for 500 epochs using the Adam optimizer with a learning rate of $10^{-3}$.

\vspace{-2mm}
\paragraph{Evaluation protocol.}
To evaluate the representational quality of our model, we compute representations $\z_{t,1}$, $\z_{t,2}$ and $\z_{t,3}$ for each mouse respectively, which we then aggregate into a single mouse triplet embedding using two different pooling strategies. First, we apply average pooling to get $\x_{t, \mathrm{avg}}$. Second, we apply max pooling and min pooling, then compute the difference to get $\x_{t, \mathrm{minmax}}$. Both aggregated embeddings are concatenated into a 128-dim embedding for each frame in the sequence.
Evaluation of each one of the 13 tasks, is performed by training a linear layer on top of the frozen representations, producing a final F1 score or a mean squared error depending on the task. 

\vspace{-2mm}
\subsubsection{Results}
We compare our model against PCA, trajectory VAE (TVAE) \cite{co2018self}, TS2Vec \cite{yue2022ts2vec}, 
and the top performing models in the MABe2022 challenge \cite{sun2022mabe22} which are adapted respectively from Perceiver \cite{jaegle2021perceiver}, GPT \cite{brown2020language}, PointNet \cite{qi2017pointnet} and BERT \cite{devlin2018bert}. All methods are trained using some form of reconstruction-based objective, with both T-Perceiver and T-BERT using masked modeling. Both T-BERT and T-PointNet also supplement their training with a contrastive learning objective using positives from the same sequence.
It is also important to note that the training set labels from two tasks (Lights and Chase) are made publicly available, and are used as additional supervision in the T-Perceiver and T-BERT models. We do not use any supervision for BAMS.

Our model achieves a new state-of-the-art result on the MABe Multi-Agent Behavior 2022 -  Mouse Triplets Challenge, as can be seen in Table~\ref{tab:mouseacc}. We rank first overall based upon our rank on all tasks, outperform all other approaches on all the global sequence-level tasks, and rank 1st on 4 out of 9 of the frame-level subtasks, while remaining competitive on the rest.  
We point out that the other top performing model on the frame-level tasks explicitly models the interaction between mice by introducing hand-crafted pairwise features. This is outside the scope of this work, as we do not focus on social interactions beyond predicting the distance between mice.

We observe that our proposed method results in significant gaps on sequence-level tasks. In particular, we observe a marked improvement on the prediction of strain over the second-place model at an accuracy of 78.63\%, with BAMS yielding a 10\% improvement in accuracy at 88.23\%. These big improvements suggest that we might have identified and addressed a critical problem in behavior modeling. In the next section, we conduct a series of ablations to further dissect our model's performance.

Multi-timescale embedding separation also enables us to probe our model for timescale-specific features. In Table~\ref{tab:mouseacc}, we report the decoding performance with short-term embeddings and long-term embeddings respectively. We find that sequence-level behavioral factors are better revealed in the long-term space, while the frame-level factors are more distinct in the short-term space. That being said, decoding performance is still best when using both timescales. 

BAMS is pre-trained with all available trajectory data. We also test BAMS in the inductive setting, where we only pre-train using the training split (only 1800 out of 5336) of the dataset. Results are reported in Appendix~\ref{app:inductive} We find that the performance drop is modest, and that BAMS trained in this setting still beats all other methods.

\subsubsection{Ablations}

\begin{wraptable}{r}{0.45\textwidth}
\vspace{-1.8em}
\centering
\caption{\footnotesize{{\em  Ablations on the Mouse Triplet Dataset.} We report the average sequence-level MSE and F1-score, and the average frame-level F1-score.
}}
\vspace{0.5em}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{cccccc}
\hline
\rule{0pt}{4ex}   
\rot[65]{\small Hist. of actions} & \rot[65]{\small Bootstrapping} & \rot[65]{\makecell{\small Multi-timescale}} & Seq MSE & Seq F1 & Frame F1 \\ 
\hline
\rule{0pt}{2.4ex}
\checkmark & \checkmark & \checkmark & \textbf{0.090415}  & \textbf{80.12}  &  \textbf{19.85}\\ 
 & \checkmark & \checkmark & 0.093100 & 68.30 & 19.46 \\
\checkmark &  & \checkmark & 0.090717 & 78.11 & 19.45  \\
\checkmark & \checkmark & & 0.092483 & 73.37 & 19.52  \\
\hline
\end{tabular}}
\label{tab:ablations}
\end{wraptable}

To understand the role of the different proposed components in enabling us to achieve state-of-the-art performance, we conduct a series of ablations that we report in Table~\ref{tab:ablations}. %A full task-by-task breakdown of the results can be found in Appendix~C.
First, we compare the performance of BAMS when trained with the traditional reconstruction-based objective (multi-step sequential prediction). BAMS without the \hist~prediction objective, performs comparably with many of the top-entries, though performance is ultimately improved when using this novel learning objective. We note that we had to reduce the number of prediction frames to 10, as the training fails if we go beyond. With our \hist~objective, we can use 30 frames, which strongly suggests that this loss can be stably applied for longer-range predictions compared to traditional losses. 

Next, we analyze the role of the multi-timescale bootstrapping in improving the quality of the representation. When removing the bootstrapping objective, we find a 2\% drop in the sequence-level averaged F1-score, as well as modest drops in frame-level performance. This suggests that this learning objective may help to resolve global and intermediate-scale features. We also ablate the multi-timescale component, i.e. we perform bootstrapping but in the same space, by using a single TCN that spans the receptive field of the two used originally. This results in sub-optimal performance, which emphasizes the idea that having different objectives in different spaces is critical to prevent interference and provide ideal performance.
