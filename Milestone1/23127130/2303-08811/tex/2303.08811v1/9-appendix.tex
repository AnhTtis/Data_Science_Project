\newpage
\onecolumn
\appendix

\section{Experimental details: Simulated Quadrupeds}

\subsection{Data generation}

\paragraph{Simulation details.} We record a total of 5182 trajectories. 2756 were generated for robots of type ANYmal B and 2426 sequences were generated for robots of type ANYmal C. These are quadruped robots, which means that they have four legs. Each leg has 3 degrees of freedoms - hip, shank and thigh. The position and velocities of these degrees of freedom for all 4 legs were recorded. This results in 24 features  for each robot.
Robots are generated while traversing an procedurally generated environment with different terrain types and traversal difficulty, as show in Figure~\ref{fig:robsim}. We only keep trajectories that correspond to a successful traversal.  

\begin{figure*}[h!]
\centering
   \includegraphics[width=0.95\textwidth]{figures/robots_sim.png}
   \caption{\footnotesize{\em Procedurally generated environments}. (A) Screenshots from the simulator showing a robot walking down some stairs, and a view of the terrain lanscape. (B) Visualization of the different terrain sections, characterized by a terrain type and different levels of difficulty. Terrain are made more difficult to traverse by either making them more rough or have steeper slopes.}  \label{fig:robsim}
   \vspace{-3mm}
\end{figure*}


\vspace{-2mm}
\paragraph{Tasks.}
To evaluate the representation quality of our model, we use multi-task probes that correspond to different long-term and short-term behavioral factors.
\begin{itemize}
\setlength\itemsep{0.2em}
    \item Robot type: the robot can either be of type "ANYmal B" or "ANYmal C". These robots have the same degrees of freedom and tracked joints but differ by their morphology. This is a sequence-level task.
    \item Linear velocity: the command of the robot is a constant velocity vector. The amplitude of the velocity dictates how fast the robot is commanded to traverse the environment. A higher velocity would translate into more clumpsy and more risk-taking behavior. This is a sequence-level task.
    \item Terrain type: the environment is generated with multiple segments of five terrain types that are categorized as: flat surfaces, pits, hills, ascending and descending stairs. This is a frame-level task.
    \item Terrain slope: the slope of the surface the robot is walking on. This is a frame-level task.
    \item Terrain difficulty: the different terrain segments have different difficulty levels based on terrain roughness or steepness of the surface. This is a frame-level task.
\end{itemize}

\paragraph{Why this dataset.}
Simulation-based data collection enables access to information that is generally inaccessible or hard to acquire in a real-world setting. Unlike noisy measurements coming from the camera-based feature extractor in the case of the mouse dataset, physics engines do not suffer from the problem of noise. Instead, they provide accurate ground-truth information about the creature and the world state free of charge. Access to such information is at times critical for scrutinizing the capabilities of the learning algorithms.



\subsection{Visualizing differences between short-term and long-term embeddings}

In Figure~\ref{fig:robosmooth}, we visualize how the short-term and long-term embeddings evolve over time, for a single sample sequence. We note a clear difference in the smoothness in the two timescales. In the short-term embeddings, we note a clear block structure corresponding to different blocks of behavior that span a few seconds, while in the long-term embeddings the representation is more stable over time. This suggests that, as expected, the bootstrapping objectives are forming representations with different levels of granularity. 

\begin{figure*}[h!]
\centering
   \includegraphics[width=0.95\textwidth]{figures/smooth_robo.png}
   \caption{\footnotesize{\em Visualization of the short-term and long-term embeddings.} We visualize for a single sequence how the short-term and long-term embeddings evolve over time.}  \label{fig:robosmooth}
   \vspace{-3mm}
\end{figure*}


\section{Experimental details: Mouse Triplet}

\subsection{Feature extraction}
Each mouse in the arena is tracked using 12 anatomically defined keypoints. We process these keypoints to extract 36 different features characterizing each mouse individually, similar to \cite{segalin2021mouse}. We separate the keypoints into two different areas, the head and the body, for each we extract different measures of displacement, that we express in the frame of the mouse, i.e. these features are invariant to the pose of the mouse relative to the arena. These features include:
\begin{itemize}
    \setlength\itemsep{0.1em}
    \item Head linear velocity vector that we express using polar coordinates. 
    \item Head angular velocity denoting the change in the heading direction in the arena.
    \item Body linear velocity vector that we express using polar coordinates.
    \item Body angular velocity denoting the change in the direction of the body with respect to the arena.
    \item Angular and linear velocities of the fore paws and the hind paws.
    \item Spine length change, depicting the expansion and contraction of the mouse's body. 
    \item Angles formed by the tail with respect to the body.
\end{itemize}

We normalize all features before training. We also use cosine and sinus of the angles instead of the angles. During training, we did not use any form of augmentation. 

\vspace{-2mm}
\paragraph{Noise in the data.}
Because of errors in pose estimation and tracking, there are sometimes errors in the tracking data, notably some identity swap issues \cite{sun2022mabe22}. To address this, we simply zero out all of the corresponding features and flag the frame as invalid. A binary feature is also add to the input features indicating whether or not the frame is valid. When predicting future actions, we only compute the error over windows in which at least 80\% of the frames are valid.


\subsection{Difference between Histogram of Actions and previous objectives}

Our novel objective consists in predicting the future histogram of actions instead of predicting the future sequence of actions. In Figure~\ref{fig:hoa_example}, we show what the target is for a sample from the MABe Mouse Triplet dataset. Note that the time dimension is collapsed, blurring the exact unrolling of the future events, but preserving the set of values that these actions will sweep. Note that the loss (EMD) is applied for each action feature.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/bams_vs_previous_work.pdf}
    \caption{Prediction target for a sample of the MABe Mouse Triplet dataset.}
    \label{fig:hoa_example}
\end{figure}

\subsection{Training details}

\paragraph{Architecture.}
We use two TCNs \cite{bai2018tcn}. Each TCN is built using multiple residual blocks, each residual block is composed of two convolutional layers, and use PReLU activation, dropout and weight normalization. All convolutions are dilated with a rate $r$, that increases after each residual block. The formula is $r^i$ where $i$ is the index of the residual block.
The first TCN is the short-term encoder, which uses 4 blocks with output sizes $[64, 64, 32, 32]$ and a dilation rate $r=2$. The second TCN is the long-term encoder, which uses 5 blocks with output size $[64, 64, 64, 32, 32]$ and a dilation rate $r=4$. The output of both encoders are concatenated to form a $64$d embedding. The predictor is a multi-layer perceptron (MLP) that has 4 hidden layers. 

\vspace{-2mm}
\paragraph{Training.}
We train the model for 500 epochs using the Adam optimizer with a learning rate of $10^{-3}$ and weight decay $4 \cdot 10^{-5}$, we decrease the learning rate to $10^{-4}$ after 100 epochs. We use a batch size of 96, and compute the future histogram of action prediction error for each timestep $t$ starting at 5 seconds after the start of each sequence, in order to allow the model to aggregate enough context. We set the learning rate of the predictors used for bootstrapping to be $10$ times higher than the learning rate used for the rest of the weights.

\vspace{-2mm}
\paragraph{Evaluation.}
During the development of the model, we test our model on the public test splits, and only look at the performance on the private set after finishing any hyperparameter tuning. We repeat the training and evaluation 5 times and report the average performance.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/bams_linear_evaluation.pdf}
    \caption{Linear evaluation protocol. The model is frozen, and for each task, a single linear layer is trained to predict the corresponding labels.}
    \label{fig:lineareval}
\end{figure}


\subsection{BAMS in the inductive setting} \label{app:inductive}
The mouse triplet dataset (5336 sequences) has three different sets, a training set (1800 sequences), a private test set and a public test set. During training of the representation learning model, we can either pre-train on all of the available data (transductive setting) or on the training set only (inductive setting). During linear evaluation, the different linear layers are trained using labels from the training set and the performance is reported on the public test set (during the challenge) and then on the private test set (to rank models). 

We train BAMS in the inductive setting and report the performance in Table~\ref{tab:mouseaccsupp}. We find that even when BAMS is trained with approximately one third of the data, the drop in performance is modest. More importantly, BAMS preserves its ranking compared to other methods, and still is the state-of-the-art.

\input{tables/mabe_tables_supp.tex}

\subsection{Notes on TS2Vec experiments}

TS2Vec \cite{yue2022ts2vec} employs two types of contrastive losses to learn representations. The first of these losses is an instance contrastive loss which contrasts a sequence with all other sequences in a batch which are treated as negative examples, while two subsequences extracted from the same sequence  are treated as positive examples. The second loss is a temporal contrastive loss which acts along a single time series.  Temporal representations of nearby time points are taken as positive examples,  while the rest of the time points within the same sequence are taken as negative examples. 
The results for the three versions of TS2vec, namely TS2Vec-I, which uses only instance contrastive loss, TS2Vec-T, which uses only temporal contrastive loss,and TS2Vec IT, which uses both instance and temporal contrastive losses, are listed in table \ref{tab:TS2vecmouseacc}.
Our TS2Vec experiments on the mouse dataset showed that using temporal contrastive loss resulted in worse performance across all tasks as compared to only using instance contrastive loss. For this reason, we only report results for TS2vec that only employs instance contrastive loss.

We note that for both TS2Vec and TS2Vec-IT, we ran into out-of-memory errors when creating instance-level or global contrast. Contrastive learning methods usually incur high computational costs, we find that our method, which doesn't rely on negative examples, can scale better when working with longer sequences and larger datasets. 

\begin{table*}[h!]
\centering
\caption{{\em TS2Vec Linear readouts of mouse behavior.}}
\vspace{0.1in}
\resizebox{0.99\textwidth}{!}{
\vspace{-2mm}
\begin{tabular}{l|cccc|ccccccccc}
\hline
 & \multicolumn{4}{c|}{\textit{Sequence-level subtasks}} & \multicolumn{9}{c}{\textit{Frame-level subtasks}}\\
%Model & F1-score & MSE & T1\tnote{*} & T2\tnote{*} & T3 & T13 & T4 & T5 & T6  & T7  & T8 & T9  & T10  & T11 & T12 \\   
\textit{Model} & Day ($\downarrow$) & Time ($\downarrow$) & Strain & Lights & Approach & Chase & Close  & Contact  & Huddle & O/E & O/G  & O/O & Watching \\   
\hline
TS2Vec-I & 0.09380 & 0.09422 & 57.12 & 65.60 & 1.29  & 0.66 &  59.53 & 46.13 & 24.74 & 0.35 &  1.09 &  0.74 & 12.37\\
TS2Vec-T & 0.09882 & 1.0252 & 45.82 & 46.69 & 0.72 & 0.14 & 45.19 &  34.93 & 9.38 & 0.186 & 0.38 & 0.38 & 05.31\\
TS2Vec-IT & 0.09846 & 1.01646 & 46.67 & 44.28 & 0.67 & 0.13 & 44.56 &  33.87 & 9.79 & 0.178 & 0.42 & 0.42 & 04.58\\
 \hline
\end{tabular}
}
\label{tab:TS2vecmouseacc}
\end{table*}

