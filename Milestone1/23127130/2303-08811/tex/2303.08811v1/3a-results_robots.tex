\section{Experiments}

\subsection{Simulated Quadrupeds Experiment}

\subsubsection{A synthetic dataset of simulated legged robots}
To test our model's ability to separate behavioral factors that vary in complexity and contain distinct multi-timescale dynamics, we introduce a new dataset generated from a heterogeneous population of quadrupeds traversing different terrains. 
Simulation enables access to information that is generally inaccessible or hard to acquire in a real-world setting.  It provides accurate ground-truth information about the agent and the world state, free of charge. We believe the presented dataset can be helpful to the field in evaluating multi-task behavior representation methods.

\begin{figure*}[b!]
\centering
   \includegraphics[width=\textwidth]{figures/bams_robots_latent_space.pdf}
   \caption{ %\footnotesize
   {\em Quadrupeds walking on procedurally generated map.}
   (A) Illustration showing the robots walking on the procedurally generated map, with segments of different terrain types.  ANYmal B and ANYmal C have different morphology. We plot of subset of the tracked data as the robots traverse different terrains, and note a shift in kinematics for each terrain as well as a difference between robots.  
   (B) Visualizing BAMS's learned robot embeddings. We perform a PCA projection of the long-term embeddings learned across sequences. We identify the main factors of variance to be the robot type and the velocity of the robot respectively.
   \label{fig:robots}}
   \vspace{-3mm}
\end{figure*}

\vspace{-2mm}
\paragraph{Agents.}
We use advanced robotic systems \cite{isaacgym} that imitates 4-legged creatures capable of various locomotion skills. These robots are trained to walk on challenging terrains using reinforcement learning \cite{rudin2022learning}. 
We use two robots that differ by their morphology, ANYmal B and ANYmal C. To create heterogeneity in the population, we randomize the body mass of the robot as well as the target traversal velocity. We track a set of 24 proprioceptive features including linear and angular velocities of the robots' joints.

\vspace{-2mm}
\paragraph{Procedurally generated environments.}
Using NVIDIAâ€™s Isaac Gym \cite{isaacgym} simulation environment, we procedurally generate maps composed of multiple segments of different terrains types (Figure~\ref{fig:robots}). We consider five different terrains including flat surfaces, pits, hills, ascending and descending stairs. We also vary the roughness and slope of the terrain, effectively controlling the difficulty of terrain traversal.

\vspace{-2mm}
\paragraph{Experimental setup.}
We collect 5182 trajectories of robots walking through terrains. We record for 3 minutes at a frequency of 50Hz. For evaluation, we split the dataset into train and test sets (20/80 split) and use multi-task probes that correspond to different long-term and short-term behavioral factors. More details can be found in Appendix A.

\begin{table*}[t!]
\centering
\caption{{\em  Linear readouts of robot behavior.}  For each task, we report the linear decoding performance on sequence-level and frame-level tasks. Tasks marked with * are classification tasks for which the F1-score is reported, for the remaining tasks, we report the mean-squared error.}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|ccc}
\hline
\rule{0pt}{2.2ex} & \multicolumn{2}{c|}{\textit{Sequence-level Tasks}} & \multicolumn{3}{c}{\textit{Frame-level Tasks}} \\
\textit{Model} & Robot Type* ($\uparrow$) & Linear velocity ($\downarrow$)  & Terrain type* ($\uparrow$)  & Terrain slope ($\downarrow$)  & Terrain difficulty ($\downarrow$)  \\ \hline
PCA & 99.72  & 0.069 & 08.83 & 0.037 & 0.790 \\
TCN & 99.93 & 0.102 & 33.03 & 0.037 & 0.080 \\
BAMS & 99.96 & 0.038 & \bf{39.89} & \bf{0.033} & \bf{0.078} \\ 
\rowcolor{Gray}
\rotatebox[origin=c]{180}{$\Lsh$} short-term & {\bf 100.0}  & 0.094 & 34.86 & 0.036 & 0.079 \\
\rowcolor{Gray}
\rotatebox[origin=c]{180}{$\Lsh$} long-term  & 99.88 & \bf{0.020} & 32.39 & 0.036 & \bf{0.078} \\ 
\hline
\end{tabular}}
\label{tab:robots}
\end{table*}


\subsubsection{Results}
Results in Table~\ref{tab:robots} suggest that our model performs well on these diverse prediction tasks. 
A major advantage of our method is the separation of the short-term and long-term  dynamics, which enables us to more clearly identify the multi-scale factors. 
While some of the tasks are represented best in the mixed model, we find that the linear velocity is more decodable in the long-term embedding, while terrain type is more decodable in the short-term embedding.
We further analyze the formed representations by visualizing the embeddings in the different spaces. In Figure~\ref{fig:robots}-B, we visualize the long-term embedding space and find that our model is able to capture the main factors of variance in the dataset, corresponding to the robot type and the velocity at which the robots are moving. This suggest that in the absence of labels, the learned embedding can provide valuable insights into how the recorded population is distributed without the need for annotations. In Appendix A, we visualize the extracted embeddings of a single sequence over time. We find that the long-term embedding is more stable and smooth, while the short term embeddings reveal different blocks of behavior that change more frequently.
