
\section{Related Work}

\subsection{Animal behavior analysis}
\paragraph{Common components}

Recently, there has been a democratization of automated methods for pose estimation and animal tracking that has made it possible to conduct large scale behavioral studies in many scientific domains.  These tools abstract behavior trajectories from video recordings, and facilitate the modeling of behavioral dynamics. 
Most pipelines for analyzing animal behavior consist of three key steps \cite{luxem2022video}: 
(1) pose estimation \cite{sun2021self,wu2020deep, forys2020real, pereira2019fast,deeplabcut}, (2) spatial-temporal feature extraction \cite{luxem2020identifying, CHEN2021332}, and (3) quantification and phenotyping of behavior \cite{Nair2022.04.19.488776, MARSHALL2022102522, wiltschko2020revealing}. % [Add more references to related work.] 
In our work, we consider the analysis of behavior after pose estimation or keypoint extraction is performed. However, one could imagine using our multi-timescale bootstrapping approach for representation learning in video analysis, where self-supervised losses have been proposed recently for keypoint discovery \cite{sun2021self, wan2019self, jakab2020self}.

\vspace{-3mm} 
\paragraph{Disentanglement of animal behavior in videos.}~In recent work \cite{shi2021learning, li2018disentangled, villegas2017decomposing}, two distinct disentangled behaviorial embeddings are learned from video, separating non-behavioral features (context, recording condition, etc.) from the dynamic behavioral factors (pose). This has even been applied to situations with multiple individuals, performing disentanglement on each individual \cite{hsieh2018learning}.
This is performed by training two encoders, typically variational autoencoders (VAEs) \cite{kingma2019introduction}, on either multi-view dynamic information or a single image. These encoders create explicitly separated embeddings of behavioral and context features.
In comparison to this work, rather than seperating behavior from context, our model considers the explicit separation of behavioral embeddings across multiple timescales, and considers the construction of a global embedding that is consistent over long timescales.

\vspace{-3mm} 
\paragraph{Modeling social behavior.}~ For social and multi-animal datasets, there are a number of other challenges that arise. Simba \cite{nilsson2020simple} and MARS  \cite{segalin2021mouse} have similar overall workflows for detecting keypoints and pose of many animals and classifying social behaviors. More recently, a semi-supervised approach TREBA has been introduced \cite{sun2021task} for building behavior embeddings using task programming. TREBA is built on top of the trajectory VAE \cite{co2018self}, a variational generative model for learning representations of physical trajectories in space. In our work, we do not consider a reconstruction objective but a future prediction objective, in addition to bootstrapping the behavior representations at different timescales.

\subsection{Representation learning for sequential data}
The self-supervised learning (SSL) framework has gained a lot of popularity recently due to its impressive performance in many domains \cite{devlin2018bert,chen2020simple,chung2018speech2vec}. Many SSL methods are built based on the concept of \textit{instance-specific} alignment loss: Different views of each datapoint are created based on pre-selected augmentations, and the views that are produced from the same datapoint are treated as positive examples; while the views that are produced from different datapoints are treated as negative examples. 
While contrastive methods like SimCLR \cite{chen2020simple} utilize both positive examples and negative examples to guide the learning, BYOL \cite{grill2020bootstrap} proposes a framework in which augmentations of a sample are brought closer together in the representation space through a predictive regression loss. Recent work \cite{guo2020bootstrap,brave,niizumi2021byol,myow} applies BYOL to learn representations of sequential data. In such cases, neighboring samples in time are considered to be positive examples of each other, assuming temporal smoothness of the semantics underlying the sequence. The model is trained such that neighboring samples in time are mapped close to each other in the representation space. However, these methods use a single scale to define similarities unlike our method. 

Recently, self-supervised methods such as TS2vec \cite{yue2022ts2vec} learn self-supervised representations for sequential data by generating positive sub-sequence views through more complex temporal augmentations that can be integrated at instance or local level. Positive sub-sequences are temporally contrasted with other representations within the same sequence as well as contrasted with representations of other available sequences.
The sequential representations used in contrastive loss are repeatedly max-pooled to allow  multi-scale representations learning. 
However, unlike our method, the  same space of representations are used for learning these multi-scale representations. We learn a separate space of representations for different time scales so that representations at different time scales are not entangled.

The idea of using multi-scale feature extractors can be found in representation learning. In \cite{brave}, a video representation learning framework, two different encoders process a narrow view and a broad view respectively. The narrow view corresponds to a video clip of a few seconds, while the broad view spans a larger timescale. The objective, however, is different to ours, as the narrow and broad representations are brought closer to each other, in the goal of encoding their mutual information. This strategy is also used in graph representation learning where a local-neighborhood of node is compared to its global neighborhood \cite{hassani2020contrastive, hu2020graph}. Our method differs in that we bootstrap the embeddings at different timescales separately, this is important to maintain the fine granularity specific to each timescale, thus revealing richer information about behavioral dynamics.

\subsection{Behavioral stylometry}
Our approach inherently has ties to the field of stylometry and its application to behavior. Commonly, stylometry has been used for attribution studies, such as identifying authorship of a text \cite{tweedie1996neural} or the composer of a piece of music \cite{brinkman2016musical}, as well as tasks such as classifying the age of an author \cite{goswami2009stylometric}. In more recent efforts, stylometry has been used to de-anonymize programmers from their code \cite{caliskan2015anonymizing}, discriminate human-generated from machine-generated text \cite{bakhtin2019real}, and extract identity from hand-written text \cite{hafemann2017learning}.

In its applications to behavioral data analysis, stylometry has been applied for biometric identification of students during online exams \cite{monaco2013behavioral}, as well as transformer-based stylometric approaches have been applied to detect an individuals decision making strategy while playing chess \cite{mcilroy2021detecting}. An underlying assumption of behavioral stylometry is that behavioral situations are often far to complex for humans to perform a truly unbiased and accurate analysis, but modern computational methods can provide more objective and clearer analysis, which can be extremely informative, especially for tasks as complex as behavior.
