\section{Introduction}

\begin{figure*}[t!]
\centering
   \includegraphics[width=\textwidth]{figures/bams_overview_v2.pdf}
   \caption{\footnotesize{\em Overview of our approach}. Bootstrap Across Multiple Scales (BAMS) uses two temporal convolutional networks with two latent spaces, each with their own receptive field sizes. The model is trained on a novel learning objective that consists in predicting future action distributions instead of future action sequences. In this figure, we use sample data from the MABe dataset, only a subset of the channels are shown.
   \label{fig:overview}}
   \vspace{-3mm}
\end{figure*}

Large-scale video datasets that capture animal behavior are now becoming critical components of many analyses in neuroscience, cognitive science, and in the study of social behavior and decision making \cite{marks2022deep, segalin2021mouse, sturman2020deep, bates2022deep}.  In these settings, different individuals or animals (like mice, worms, or flies) interact with their environment and/or other individuals while their pose (or meaningful keypoints in the video) is tracked. Studying the dynamics of behavior data, especially in
complex and naturalistic behavior \cite{long2020automatic, kennedy2022and}, as well as over multi-animal interactions \cite{kabra2013jaaba, chen2020alphatracker, fujii2021learning} and social behaviors \cite{segalin2021mouse, anderson2014toward}, provides rich information about movement and decision making that can be used to build insights into the link between the brain and behavior.

In order to learn latent factors of behavioral patterns without using annotations, a promising solution is to build models of  behavior in a unsupervised manner \cite{jia2022selfee, yang2022simper}. Unsupervised models are of particular interest in this domain as it becomes hard to identify  complex behaviors which can be composed of many ``syllables'' of movement \cite{wiltschko2015mapping}, and are thus hard and tedious to annotate. Recent work in this direction build such representations using generative modeling and reconstruction-based objectives, typically by performing open loop \cite{task_programming,co2018self, CHEN2021332} or closed loop \cite{co2018self} prediction of observations or actions multiple timesteps into the future.

However, when using a reconstruction or prediction objective to analyze behavior, future actions become hard to predict and models can be myopic, focusing only on fine-scale patterns in the data \cite{wiltschko2015mapping}. To circumvent this overly local learning of dynamics, there have been a number of efforts to build models of long-term behavioral style \cite{mcilroy2021detecting,bromley1993signature} and use more instance-level learning methods to extract a single representation for an entire sequence. However, these models then lose their ability to give time-varying representations that capture the dynamic nature of different behaviors. It is still an outstanding challenge to build representations that can capture both short-term behavioral dynamics along with longer-term trends and global structure.

In this work, we develop a new self-supervised approach for learning from behavior. Our method consists of two core innovations:
(i) A novel action-prediction approach that aims to predict the {\em distribution of actions} over future timesteps, without modeling exactly when each action is taken, and (ii) A novel multi-scale architecture that builds {\em separate latent spaces to accommodate short- and long-term dynamics}. We combine both of these innovations and show that our approach can capture both long-term and short-term attributes of behavior and work flexibly to solve a variety of different downstream tasks.

To test our approach, we utilize behavioral datasets that contain multiple tasks that vary in complexity and contain distinct multi-timescale dynamics. 
We first generate a synthetic dataset of the multi-limb kinematics and behavior from quadruped robots. Using NVIDIA's Isaac Gym \cite{makoviychuk2021isaac}, we vary the robot's morphological and dynamical properties, as well as aspects of the environment such as terrain type and difficulty. Using this realistic robot behavior data, we  demonstrate that our method can effectively build dynamical models of behavior that accurately elicit both the robot and environment properties,  without any explicit training signal encouraging the learning of this information. 

Having established that our approach can successfully predict the complex and physically realistic behavior of an artificial creature, we apply it to a multi-agent mouse behavior benchmark \cite{sun2022mabe22} and challenge with multiple tasks that vary in their frame-level (local) vs. sequence-level (global) labels and properties. On this benchmark, we rank first overall on the leaderboard \footnote{\href{https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-mouse-triplets/leaderboards?post_challenge=true}{https://www.aicrowd.com/challenges/multi-agent-behavior-challenge-2022/problems/mabe-2022-mouse-triplets/leaderboards?post\_challenge=true}} (averaged across 13 tasks), first on all of the 4 global tasks, and top-3 on all the 9 frame-level local subtasks. In one of the global tasks (decoding the strain of the mouse), we achieve impressive performance over the other methods, with a 10\% gap over the next best performing method. Our results demonstrate that our approach can provide representations that can be used to decode meaningful information from behavior that spans many timescales (longer approach interactions, grooming etc) as well as global attributes like the time of day or the strain of the mouse. 

The contributions of this work include:
\begin{itemize}[leftmargin=2em]
    \item A self-supervised framework that learns representations in two separate long-term and short-term embedding spaces in order to preserve the granularity in the short-term space. Bootstrapping is performed within each timescale, using a latent predictive loss across positive views only, and hence can process much longer sequences than contrastive methods that require negative views.
        \vspace{-1mm}
    \item A novel prediction task for behavioral analysis and cloning, that we call \hist~(histogram of actions), which aims to predict the future distribution of keypoints instead of the precise ordering of future states. We use an efficient implementation of the 1D 2-Wasserstein divergence as a measure of distributional fit between the true movement distribution and the predicted movements. 
    \vspace{-1.5mm}
    \item New state-of-the-art performance. When applied  to the Multi-agent  (MABe)  Benchmark Mouse Triplet Challenge, our model achieves top scores on all of the sequence-level (global) tasks and competitive performance on all frame-level (local) tasks with top scores on 4 out of 9 subtasks without additional supervision. Overall, our method is first in aggregate performance across the leaderboard.
    \vspace{-1.5mm}
    \item A procedurally generated dataset of walking quadruped robots with perfect ground truth annotations and multiple tasks with both local and global structure. We believe that this can provide a robust benchmark for future work in this direction and will release the dataset and tasks to the community.
\end{itemize}
