%% LyX 2.3.6.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{float}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{fact}[thm]{\protect\factname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}
\ifx\proof\undefined
\newenvironment{proof}[1][\protect\proofname]{\par
	\normalfont\topsep6\p@\@plus6\p@\relax
	\trivlist
	\itemindent\parindent
	\item[\hskip\labelsep\scshape #1]\ignorespaces
}{%
	\endtrivlist\@endpefalse
}
\providecommand{\proofname}{Proof}
\fi
\theoremstyle{plain}
\newtheorem{cor}[thm]{\protect\corollaryname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[paperwidth=8.5in, paperheight=11in, margin=1in]{geometry}

\author{ 
Zijian Liu\thanks{Stern School of Business, New York University, \texttt{zl3067@stern.nyu.edu}.}  
 \and 
Zhengyuan Zhou\thanks{Stern School of Business, New York University, \texttt{zzhou@stern.nyu.edu}.}}

\date{}

\makeatother

\providecommand{\corollaryname}{Corollary}
\providecommand{\factname}{Fact}
\providecommand{\lemmaname}{Lemma}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\global\long\def\E{\mathbb{\mathbb{E}}}%
\global\long\def\F{\mathcal{F}}%
\global\long\def\R{\mathbb{R}}%
\global\long\def\N{\mathbb{\mathbb{N}}}%
\global\long\def\bzero{\mathbb{\mathbf{\mathbf{0}}}}%
\global\long\def\na{\nabla}%
\global\long\def\indi{\mathds{1}}%
\global\long\def\dom{\mathcal{X}}%
\global\long\def\pa{\partial}%
\global\long\def\hp{\widehat{\partial}}%
\global\long\def\argmin{\mathrm{argmin}}%

\title{Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises:
High-Probability Bound, In-Expectation Rate and Initial Distance Adaptation}
\maketitle
\begin{abstract}
Recently, several studies consider the stochastic optimization problem
but in a heavy-tailed noise regime, i.e., the difference between the
stochastic gradient and the true gradient is assumed to have a finite
$p$-th moment (say being upper bounded by $\sigma^{p}$ for some
$\sigma\geq0$) where $p\in(1,2]$, which not only generalizes the
traditional finite variance assumption ($p=2$) but also has been
observed in practice for several different tasks. Under this challenging
assumption, lots of new progress has been made for either convex or
nonconvex problems, however, most of which only consider smooth objectives.
In contrast, people have not fully explored and well understood this
problem when functions are nonsmooth. This paper aims to fill this
crucial gap by providing a comprehensive analysis of stochastic nonsmooth
convex optimization with heavy-tailed noises. We revisit a simple
clipping-based algorithm, whereas, which is only proved to converge
in expectation but under the additional strong convexity assumption.
Under appropriate choices of parameters, for both convex and strongly
convex functions, we not only establish the first high-probability
rates but also give refined in-expectation bounds compared with existing
works. Remarkably, all of our results are optimal (or nearly optimal
up to logarithmic factors) with respect to the time horizon $T$ even
when $T$ is unknown in advance. Additionally, we show how to make
the algorithm parameter-free with respect to $\sigma$, in other words,
the algorithm can still guarantee convergence without any prior knowledge
of $\sigma$. Furthermore, an initial distance adaptive convergence
rate is provided if $\sigma$ is assumed to be known.
\end{abstract}
%
\input{introduction.tex}

\input{preliminaries.tex}

\input{algo.tex}

\input{conclusion.tex}

\clearpage

\bibliographystyle{plain}
\bibliography{ref}

\newpage


\appendix
\onecolumn

\input{appendix.tex}
\end{document}
