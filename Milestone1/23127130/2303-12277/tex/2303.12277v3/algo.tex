
\section{Algorithm and its Convergence Guarantee\label{sec: algo}}

\begin{algorithm}[h]
\caption{\label{alg:algo}Projected SGD with Clipping}

\textbf{Input}: $x_{1}\in\dom$, $M_{t}>0$, $\eta_{t}>0$.

\textbf{for} $t=1$ \textbf{to} $T$ \textbf{do}

$\quad$$g_{t}=\left(1\land\frac{M_{t}}{\left\Vert \hp F(x_{t})\right\Vert }\right)\hp F(x_{t})$

$\quad$$x_{t+1}=\Pi_{\dom}(x_{t}-\eta_{t}g_{t}).$

\textbf{end for}
\end{algorithm}

The projected clipped SGD algorithm is shown in Algorithm \ref{alg:algo}.
The algorithm itself is simple to understand. Compared with SGD, the
only difference is to clip the stochastic gradient $\hp F(x_{t})$
with a threshold $M_{t}$. In the next two sections, we will show
that properly picked $M_{t}$ and $\eta_{t}$ guarantee both high-probability
and in-expectation convergence for Algorithm \ref{alg:algo}. Again,
we remark that our results in Sections \ref{subsec:convex} and \ref{subsec:str-convex}
can be extened to any norm $\|\cdot\|$ on $\R^{d}$. Theorem \ref{thm:lip-dog-prob}
in \ref{subsec:dog} still holds when changing the $\ell_{2}$ norm
to the Mahalanobis norm, i.e., $\|x\|=\sqrt{x^{\top}Ax}$ for $A\succ0$.

\subsection{General Convergence Theorems When $\mu=0$\label{subsec:convex}}

In this section, we present the convergence theorems of Algorithm
\ref{alg:algo} for convex functions, i.e., $\mu=0$.

First, when $T$ is not assumed to be known, Theorem \ref{thm:lip-prob}
gives any-time high-probability convergence bounds for two cases,
i.e., whether the noise level $\sigma$ is known or not. As far as
we know, Theorem \ref{thm:lip-prob} is the first to describe an any-time
high-probability convergence rate for nonsmooth convex optimization
problems when the noise is assumed to be heavy-tailed.
\begin{thm}
\label{thm:lip-prob}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Under the choices
of $M_{t}=2G\lor Mt^{\frac{1}{p}}$ where $M\geq0$ can be any real
number and $\eta_{t}=\frac{\alpha}{G\sqrt{t}}\land\frac{\alpha}{M_{t}}$
where $\alpha=\frac{\beta}{\log(4/\delta)}$ and $\beta>0$ can be
any real number, for any $T\geq1$ and $\delta\in(0,1)$, the following
bound holds with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\left(\beta\left(1+(\sigma/M)^{2p}\right)\log^{2}T+\log(1/\delta)\left(\beta+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\beta}\right)\right)\left(\frac{G}{\sqrt{T}}\lor\frac{M}{T^{\frac{p-1}{p}}}\right)\right).
\]
Especially, by setting $M=\sigma$ when $\sigma$ is known, we have
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\left(\beta\log^{2}T+\log(1/\delta)\left(\beta+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\beta}\right)\right)\left(\frac{G}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\end{thm}
%
\begin{rem}
The choice of $\alpha=\frac{\beta}{\log(4/\delta)}$ is only for optimizing
the dependence on $\log(1/\delta)$. Our theoretical analysis works
for any $\alpha>0.$ Additionally, it is possible to choose $\eta_{t}=\frac{\alpha_{1}}{G\sqrt{t}}\land\frac{\alpha_{2}}{M_{t}}$
for different $\alpha_{1},\alpha_{2}>0$. However, we keep the same
$\alpha$ for simplicity in Theorem \ref{thm:lip-prob} and the following
Theorems \ref{thm:lip-prob-fix}, \ref{thm:lip-exp} and \ref{thm:lip-exp-fix}.
\end{rem}
%
We note that whenever $\sigma$ is known or not, our choice always
leads to the (nearly) optimal rate $\widetilde{O}(T^{\frac{1-p}{p}})$
in $T$. Moreover, if we assume $\sigma$ is known and consider the
choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ when $\sigma=0$,
in other words, the deterministic case, the clipping magnitude $M_{t}$
will be $M_{t}=2G$ and the step size $\eta_{t}$ is $\eta_{t}=\frac{\alpha}{G\sqrt{t}}\land\frac{\alpha}{2G}=O(\frac{\alpha}{G\sqrt{t}})$.
Recall that the norm of any subgradient is bounded by $G$, which
implies $M_{t}=2G$ won't have any effect now. Hence, the algorithm
will be the totally same as the traditional Projected SGD. The corresponding
bound will also be the (nearly) optimal rate $\widetilde{O}(G/\sqrt{T})$.
We would like to emphasize that the appearance of the term $\log^{2}T$
is due to the time-varying step size rather than the analysis technique
for the high-probability bound. Notably, the dependence on $\delta$
is only $\log(1/\delta)$ rather than the sub-optimal $\log(T/\delta)$
in previous works.

Next, in Theorem \ref{thm:lip-prob-fix}, we state the fixed time
bound, i.e., the case of known $T$. As mentioned above, the extra
term $\log^{2}T$ will be removed.
\begin{thm}
\label{thm:lip-prob-fix}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Additionally,
assume $T$ is known. Under the choices of $M_{t}=2G\lor MT^{\frac{1}{p}}$
where $M\geq0$ can be any real number and $\eta_{t}=\frac{\alpha}{G\sqrt{T}}\land\frac{\alpha}{M_{t}}$
where $\alpha=\frac{\beta}{\log(4/\delta)}$ and $\beta>0$ can be
any real number, for any $T\geq1$ and $\delta\in(0,1)$, the following
bound holds with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\left(\beta(\sigma/M)^{2p}+\log(1/\delta)\left(\beta+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\beta}\right)\right)\left(\frac{G}{\sqrt{T}}\lor\frac{M}{T^{\frac{p-1}{p}}}\right)\right).
\]
Especially, by setting $M=\sigma$ when $\sigma$ is known, we have
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\log(1/\delta)\left(\beta+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\beta}\right)\left(\frac{G}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\end{thm}
%
To finish the high-probability bounds, we would like to make a comprehensive
comparison with \cite{zhang2022parameter}, which is the only existing
work showing a high-probability bound of $\widetilde{O}(\epsilon\log(1/\delta)T^{-1}+(\sigma+G)\log(T/\delta)\log(\|x_{1}-x_{*}\|T/\epsilon)\|x_{1}-x_{*}\|T^{\frac{1-p}{p}})$
(where $\epsilon>0$ is any user-specified parameter and $\|\cdot\|$
is the Mahalanobis norm\footnote{The bound in \cite{zhang2022parameter} is proved in the Hilbert space.
Hence, $\|\cdot\|$ will be the Mahalanobis norm when specialized
to $\R^{d}$.}) in the related literature. We need to emphasize our work is different
in several aspects.
\begin{enumerate}
\item The algorithm in \cite{zhang2022parameter} is much more complicated
than ours. To be more precise, their main algorithm needs to call
several outer algorithms. The outer algorithms themselves are even
very involved. This difference is because we only focus on convex
optimization, in contrast, their algorithm is designed for online
convex optimization, which is known to be more general. Hence, when
only considering solving the heavy-tailed nonsmooth convex optimization,
we believe our algorithm is much easier to be implemented.
\item When choosing $M_{t}$ and $\eta_{t}$, \cite{zhang2022parameter}
requires not only the time horizon $T$ but also the noise $\sigma$,
which means their result is neither an any-time bound nor parameter-free
with respect to $\sigma$. In comparison, our Theorem \ref{thm:lip-prob}
doesn't require $T$. We also show how to set $M_{t}$ and $\eta_{t}$
when $\sigma$ is unknown. Besides, the dependence on $\delta$ in
both Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix} is always
$O(\log(1/\delta))$, which is significantly better than $O(\log(T/\delta))$
in \cite{zhang2022parameter}.
\item However, our result is not as good as \cite{zhang2022parameter} for
the dependence on the initial distance $\|x_{1}-x_{*}\|$. As one
can see, our obtained bound is always in the form of $O(\beta+\|x_{1}-x_{*}\|^{2}/\beta)$,
which is worse than $\|x_{1}-x_{*}\|\log(\|x_{1}-x_{*}\|)$ in \cite{zhang2022parameter}.
To deal with this issue, a high-probability bound, $\widetilde{O}(\log(\|x_{1}-x_{*}\|/r)(r+\|x_{1}-x_{*})(G\log(1/\delta)T^{-\frac{1}{2}}+\sigma\log(1/\delta)^{1-\frac{1}{p}}T^{\frac{1-p}{p}}))$
where $r>0$ can be any number, is provided in Theorem \ref{thm:lip-dog-prob}
Section \ref{subsec:dog}. As a tradeoff, compared with Theorems \ref{thm:lip-prob}
and \ref{thm:lip-prob-fix}, Theorem \ref{thm:lip-dog-prob} needs
to assume a known $\sigma$ and can at most be applied to the same
Mahalanobis norm used in \cite{zhang2022parameter} 
\item Finally, it is worth pointing out that our proof techniques are very
different from \cite{zhang2022parameter}. Our analysis is done in
a direct way compared to the reduction-based manner in \cite{zhang2022parameter}.
\end{enumerate}
Now, we turn to provide the first (nearly) optimal in-expectation
convergence rate of clipping algorithms in Theorems \ref{thm:lip-exp}
(any-time bound) and \ref{thm:lip-exp-fix} (fixed time bound), which
correspond to the cases of unknown $T$ and known $T$ respectively.
\begin{thm}
\label{thm:lip-exp}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Under the choices
of $M_{t}=2G\lor Mt^{\frac{1}{p}}$ where $M\geq0$ can be any real
number and $\eta_{t}=\frac{\alpha}{G\sqrt{t}}\land\frac{\alpha}{M_{t}}$
where $\alpha>0$ can be any real number, for any $T\geq1$, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha\left(\log T+(\sigma/M)^{2p}\log^{2}T\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G}{\sqrt{T}}\lor\frac{M}{T^{\frac{p-1}{p}}}\right)\right).
\]
Especially, by setting $M=\sigma$ when $\sigma$ is known, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha\log^{2}T+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\end{thm}
%
\begin{thm}
\label{thm:lip-exp-fix}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Additionally,
assume $T$ is known. Under the choices of $M_{t}=2G\lor MT^{\frac{1}{p}}$
where $M\geq0$ can be any real number and $\eta_{t}=\frac{\alpha}{G\sqrt{T}}\land\frac{\alpha}{M_{t}}$
where $\alpha>0$ can be any real number, for any $T\geq1$, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha\left(1+(\sigma/M)^{2p}\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G}{\sqrt{T}}\lor\frac{M}{T^{\frac{p-1}{p}}}\right)\right).
\]
Especially, by setting $M=\sigma$ when $\sigma$ is known, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\end{thm}
%
We first remark that the choices of $M_{t}$ and $\eta_{t}$ in Theorems
\ref{thm:lip-exp} and \ref{thm:lip-exp-fix} are the same as them
in Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix}. Hence,
our $M_{t}$ and $\eta_{t}$ guarantee both high-probability and in-expectation
convergence. Next, compared with the any-time bounds in Theorem \ref{thm:lip-exp},
the extra logarithmic factors are removed in the fixed time bounds
in Theorem \ref{thm:lip-exp-fix}. Additionally, the rates for the
case of known $\sigma$ are always adaptive to the noise. In particular,
when $T$ is known and $p=2$, our result matches the traditional
bound of SGD perfectly.

Lastly, let us talk about the differences with the prior work \cite{vural2022mirror}
providing the only in-expectation bound but for a different algorithm.
\begin{enumerate}
\item The algorithm in \cite{vural2022mirror} is based on MD, but more
importantly, requires the property of uniform convexity (see Definition
1 in \cite{vural2022mirror}) for the mirror map. However, our in-expectation
bounds are for the algorithm employing the clipping method, which
is widely used to deal with heavy-tailed problems in several areas
but lacks theoretical justifications in nonsmooth convex optimization.
\item \cite{vural2022mirror} only assumes $\E[\|\widehat{\pa}F(x)\|^{p}\vert x]\leq\sigma^{p}$
for some $p\in(1,2]$ and $\sigma>0$ (strictly speaking, the norm
in \cite{vural2022mirror} is $\ell_{q}$ norm for some $q\in[1,\infty]$,
however, our method can be extended to an arbitrary norm including
$\ell_{q}$ norm as a subcase). This assumption is equivalent to
$\E[\|\widehat{\pa}F(x)\|^{p}\vert x]\leq O((\sigma+G)^{p})$ under
our assumptions, which means \cite{vural2022mirror} needs both $G$
and $\sigma$ as input but their final rate doesn't adapt to $\sigma$.
In contrast, we not only give a rate being adaptive to the noise when
$\sigma$ is known but also show how to run our algorithm without
any prior knowledge of $\sigma$.
\item Besides, our parameter settings not only guarantee in-expectation
convergence but also admit provable high-probability bounds as shown
in Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix}. But \cite{vural2022mirror}
only provides the in-expectation result for their algorithm.
\item Finally, our proof strategy is completely different from \cite{vural2022mirror}
as there is no clipping step in which. We believe that our techniques
in the proof will lead to a better understanding of the clipping method.
\end{enumerate}
%

\subsection{Initial Distance Adaptive Convergence Rate When $\mu=0$\label{subsec:dog}}

As mentioned above, in this section, we show that Algorithm \ref{alg:algo}
can achieve an initial distance adaptive convergence under sophisticated
parameters. In the traditional bound for SGD, a quadratic dependence
on the initial distance $O(\alpha+\|x_{1}-x_{*}\|^{2}/\alpha)$ always
shows up where $\alpha>0$ is the learning rate. Such a term also
appears in our above results, e.g., Theorem \ref{thm:lip-prob}. Hence,
theoretically speaking, the optimal learning rate $\alpha^{*}=\Theta(\|x_{1}-x_{*}\|)$.
If the domain $\dom$ is bounded with diameter $D$, one can set $\alpha=\Theta(D)$
as a proxy of $\alpha^{*}$. However, in the general unbounded case,
e.g., $\dom=\R^{d}$, the strategy of $\alpha=\Theta(D)$ is no longer
useful as $D=\infty$ now.

One may think it is impossible to achieve a better dependence on $\|x_{1}-x_{*}\|$
for SGD based algorithm if no prior information on $x_{*}$ is known,
whereas \cite{mcmahan2012no} is the first to improve it to the order
of $O(\|x_{1}-x_{*}\|\log(\|x_{1}-x_{*}\|))$ on $\R$. More surprisingly,
the problem considered in \cite{mcmahan2012no} is online learning,
which can cover the optimization problem considered in this paper.
Later on, different algorithms (see, e.g., \cite{mcmahan2014unconstrained,orabona2016coin,zhang2022pde})
are proposed to achieve such an initial distance adaptive bound on
$\R^{d}$. \cite{zhang2022parameter} is the first to extend such
kind of algorithms to deal with online learning problems with heavy-tailed
noises. However, all of these methods are designed for online learning
problems originally causing the algorithms to be complicated when
using them to deal with convex optimization problems. 

Recently, three different works \cite{carmon2022making,defazio2023learning,ivgi2023dog}
come up with different methods to achieve the initial distance adaptive
bound for convex optimization. In our paper, we borrow the key idea
provided in \cite{ivgi2023dog}, i.e., using the term $r_{t}=(\max_{s\in\left[t\right]}\|x_{1}-x_{s}\|)\lor r$
in the step size where $r>0$ can be any real number to approximate
the optimal choice $\Theta(\|x_{1}-x_{*}||)$, to achieve the better
dependence on $\|x_{1}-x_{*}\|$ as shown in the following theorem.
\begin{thm}
\label{thm:lip-dog-prob}Suppose Assumptions (1)-(5) hold with $\mu=0$.
Given $\delta\in(0,1)$, under the choices of 
\begin{itemize}
\item $w_{t\geq1}>0$ is a non-decreasing sequence satisfying $\sum_{t=1}^{T}\frac{1}{tw_{t}}\leq W<\infty$
for any $T\geq1$ and some $W\in\R$;
\item $M_{t}=2G\lor\sigma(tw_{t}/\log(4/\delta)){}^{\frac{1}{p}}$;
\item $\eta_{t}=r_{t}\gamma_{t}$ and $\gamma_{t}=\frac{\alpha_{1}}{G\sqrt{tw_{t}}}\land\frac{\alpha_{2}}{M_{t}}$
where $r_{t}=(\max_{s\in\left[t\right]}\|x_{1}-x_{s}\|)\lor r$ and
$r>0$ can be set arbitrarily;
\item $\alpha_{1}=\frac{1}{\sqrt{32W}},\alpha_{2}=\frac{1}{8\left(\frac{16}{3}+8\sqrt{5W}+4W\right)\log\frac{4}{\delta}}\land\frac{1}{\sqrt{16\left(\frac{32}{3}+8\sqrt{5W}+80W\right)\log\frac{4}{\delta}}}$;
\end{itemize}
then with probability at least $1-\delta$, for any sufficiently large
$T\geq\Omega(\log\frac{r+\left\Vert x_{1}-x_{*}\right\Vert }{r})$,
there is
\begin{align}
F(\bar{x}_{I(T)})-F(x_{*})\leq & O\left(\left(1+\log\frac{r+\left\Vert x_{1}-x_{*}\right\Vert }{r}\right)\left(r+\left\Vert x_{1}-x_{*}\right\Vert \right)\right.\nonumber \\
 & \quad\left.\times\left(\frac{G\sqrt{Ww_{T}}}{\sqrt{T}}\lor\frac{G\left(1+W\right)\log\frac{1}{\delta}}{T}\lor\frac{\sigma\left(1+W\right)\left(w_{T}\right)^{\frac{1}{p}}\left(\log\frac{1}{\delta}\right)^{1-\frac{1}{p}}}{T^{\frac{p-1}{p}}}\right)\right).\label{eq:dog-bound}
\end{align}
where
\[
\bar{x}_{T}=\frac{\sum_{t=1}^{T}r_{t}x_{x}}{\sum_{t=1}^{T}r_{t}},I(T)\in\mathrm{argmax}_{t\in\left[T\right]}\sum_{s=1}^{t}\frac{r_{s}}{r_{t+1}}.
\]

Under the first example, $w_{t}=1+\log^{2}(t)$ and $W=1+\frac{\pi}{2}$,
given in Fact \ref{fact:dog-order}, there is
\[
F(\bar{x}_{I(T)})-F(x_{*})\leq O\left(\left(1+\log\frac{r+\left\Vert x_{1}-x_{*}\right\Vert }{r}\right)\left(r+\left\Vert x_{1}-x_{*}\right\Vert \right)\left(\frac{G\log T}{\sqrt{T}}\lor\frac{G\log\frac{1}{\delta}}{T}\lor\frac{\sigma\left(\log T\right)^{\frac{2}{p}}\left(\log\frac{1}{\delta}\right)^{1-\frac{1}{p}}}{T^{\frac{p-1}{p}}}\right)\right).
\]
\end{thm}
%
We provide two examples of $w_{t}$ before explaining more about the
theorem.
\begin{fact}
\label{fact:dog-order}The following two choices satisfy the requirements
on $w_{t}$ in Theorem \ref{thm:lip-dog-prob}:
\begin{itemize}
\item $w_{t}=1+\log^{2}(t)$ and $W=1+\frac{\pi}{2}$
\item $w_{t}=\left[y^{(n+1)}(t)\right]^{1+\varepsilon}\prod_{i=1}^{n}y^{(i)}(t)$
and $W=1+\frac{1}{\varepsilon}$ where $y(t)=1+\log(t)$, $y^{(n)}(t)=y(y^{(n-1)}(t))$
is the $n$-times composition with itself for any non-negative integer
$n$, and $\varepsilon>0$ can be chosen arbitrarily.
\end{itemize}
\end{fact}
%
There are several points we would like to discuss here. First, the
requirement of $T\geq\Omega(\log\frac{r+\left\Vert x_{1}-x_{*}\right\Vert }{r})$
is not necessary, we indeed prove that $O((\frac{r_{1}+\|x_{1}-x_{*}\|}{r})^{\frac{1}{T}})\times$R.H.S.
of (\ref{eq:dog-bound}) holds for any $T\geq1$. For simplicity,
$T$ is assumed to be large enough to make $O((\frac{r_{1}+\|x_{1}-x_{*}\|}{r})^{\frac{1}{T}})=O(1)$.
Next, we would like to emphasize that Theorem \ref{thm:lip-dog-prob}
still holds under the Mahalanobis norm (the same as \cite{zhang2022parameter}).
However, for the general norm combined with the framework of MD, how
to achieve this initial distance adaptive extension still remains
unclear to us. Besides, our bound is an any time bound (without knowing
$T$) and achieves $O(\log(1/\delta))$ (rather than $O(\log(T/\delta))$)
dependence simultaneously, which are both better than \cite{zhang2022parameter}.
However, compared with Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix},
Theorem \ref{thm:lip-dog-prob} requires knowing the $p$-th moment
$\sigma$ and the failure probability $\delta$ in advance as a tradeoff.
Additionally, compared with \cite{ivgi2023dog}, our proof is very
different since we consider the heavy-tailed noises.

\subsection{General Convergence Theorems When $\mu>0$\label{subsec:str-convex}}

In this section, we focus on establishing the convergence rate of
Algorithm \ref{alg:algo} for strongly convex objectives, i.e., $\mu>0$.
In this case, even when $T$ is assumed to be known, we no longer
consider using $T$ to set $M_{t}$ and $\eta_{t}$ since a step size
$\eta_{t}$ depending on $T$ is rarely used under the strong convexity
assumption.

The first result, Theorem \ref{thm:str-prob}, describes the high-probability
behavior of Algorithm \ref{alg:algo}. To our best knowledge, this
is the first high-probability bound for nonsmooth strongly convex
optimization with heavy-tailed noises matching the in-expectation
lower bound of $\Omega(T^{\frac{2(1-p)}{p}})$.
\begin{thm}
\label{thm:str-prob}Suppose Assumptions (1)-(5) hold with $\mu>0$
let $\bar{x}_{T}=\frac{2}{T(T+1)}\sum_{t=1}^{T}tx_{t}$. Under the
choices of $M_{t}=2G\lor Mt^{\frac{1}{p}}$ and $\eta_{t}=\frac{4}{\mu(t+1)}$
where $M\geq0$ can be any real number, for any $T\geq1$ and $\delta\in(0,1)$,
the following two bounds hold simultaneously with probability at least
$1-\delta$,
\begin{align*}
F(\bar{x}_{T})-F(x_{*}) & \leq O\left(\log^{2}(1/\delta)\left(\frac{G^{2}+\sigma^{2}}{\mu T}+\frac{M^{2}+\sigma^{2p}M^{2-2p}+\sigma^{p}G^{2-p}}{\mu T^{\frac{2(p-1)}{p}}}\right)\right);\\
\left\Vert x_{T+1}-x_{*}\right\Vert ^{2} & \leq O\left(\log^{2}(1/\delta)\left(\frac{G^{2}+\sigma^{2}}{\mu^{2}T}+\frac{M^{2}+\sigma^{2p}M^{2-2p}+\sigma^{p}G^{2-p}}{\mu^{2}T^{\frac{2(p-1)}{p}}}\right)\right).
\end{align*}
Especially, by setting $M=\sigma$ when $\sigma$ is known, we have
\begin{align*}
F(\bar{x}_{T})-F(x_{*}) & \leq O\left(\log^{2}(1/\delta)\left(\frac{G^{2}}{\mu T}+\frac{\sigma^{2}+\sigma^{p}G^{2-p}}{\mu T^{\frac{2(p-1)}{p}}}\right)\right)\\
\left\Vert x_{T+1}-x_{*}\right\Vert ^{2} & \leq O\left(\log^{2}(1/\delta)\left(\frac{G^{2}}{\mu T}+\frac{\sigma^{2}+\sigma^{p}G^{2-p}}{\mu T^{\frac{2(p-1)}{p}}}\right)\right).
\end{align*}
\end{thm}
%
Our choices of $M_{t}$ and $\eta_{t}$ are inspired by \cite{zhang2020adaptive}
but $M_{t}$ is very different at the same time. We need to emphasize
that the parameter $G$ in $M_{t}=Gt^{\frac{1}{p}}$ used in \cite{zhang2020adaptive}
is not the same as our definition of $G$ since \cite{zhang2020adaptive}
only assumes $\E[\|\hp F(x)\|^{p}\vert x]\leq G^{p}$, $\forall x\in\dom$
rather than separates the assumption on noises independently. Under
our assumptions, there is only $\E[\|\hp F(x)\|^{p}\vert x]\leq O((\sigma+G)^{p})$,
which implies $M_{t}=Gt^{\frac{1}{p}}$ in \cite{zhang2020adaptive}
is equivalent to $M_{t}=\Theta((\sigma+G)t^{\frac{1}{p}})$. But this
diverges from our choice of $M_{t}=Mt^{\frac{1}{p}}$. As one can
see, $M_{t}$ in our settings doesn't rely on $\sigma$, this property
makes our choices more practical.

Finally, though an in-expectation bound of clipping algorithms for
the strongly convex case has been established in \cite{zhang2020adaptive},
we provide a refined rate in Theorem \ref{thm:str-exp}.
\begin{thm}
\label{thm:str-exp}Suppose Assumptions (1)-(5) hold with $\mu>0$
let $\bar{x}_{T}=\frac{2}{T(T+1)}\sum_{t=1}^{T}tx_{t}$. Under the
choices of $M_{t}=2G\lor Mt^{\frac{1}{p}}$ and $\eta_{t}=\frac{4}{\mu(t+1)}$
where $M\geq0$ can be any real number, for any $T\geq1$, we have
\begin{align*}
\E\left[F(\bar{x}_{T})-F(x_{*})\right] & \leq O\left(\frac{G^{2}+\sigma^{2}}{\mu T}+\frac{\sigma^{p}M^{2-p}+\sigma^{2p}M^{2-2p}}{\mu T^{\frac{2(p-1)}{p}}}\right);\\
\E\left[\left\Vert x_{T+1}-x_{*}\right\Vert ^{2}\right] & \leq O\left(\frac{G^{2}+\sigma^{2}}{\mu^{2}T}+\frac{\sigma^{p}M^{2-p}+\sigma^{2p}M^{2-2p}}{\mu^{2}T^{\frac{2(p-1)}{p}}}\right).
\end{align*}
Especially, by setting $M=\sigma$ when $\sigma$ is known, we have
\begin{align*}
\E\left[F(\bar{x}_{T})-F(x_{*})\right] & \leq O\left(\frac{G^{2}}{\mu T}+\frac{\sigma^{2}}{\mu T^{\frac{2(p-1)}{p}}}\right)\\
\E\left[\left\Vert x_{T+1}-x_{*}\right\Vert ^{2}\right] & \leq O\left(\frac{G^{2}}{\mu^{2}T}+\frac{\sigma^{2}}{\mu^{2}T^{\frac{2(p-1)}{p}}}\right).
\end{align*}
\end{thm}
%
We briefly discuss Theorem \ref{thm:str-exp} here before finishing
this section. First, we remark that the clipping magnitude $M_{t}$
and step size $\eta_{t}$ are the same as Theorem \ref{thm:str-prob}
without any extra modifications. Additionally, these two in-expectation
bounds are both optimal as they attain the best possible in-expectation
rate $\Omega(T^{\frac{2(1-p)}{p}})$. It is also worth pointing out
that our results have a more explicit dependence on the noise level
$\sigma$ and the Lipschitz constant $G$ compared with \cite{zhang2020adaptive}
as in which there is no explicit assumption on noises as mentioned
before. Notably, the two rates are both adaptive to the noise $\sigma$
while not requiring any prior knowledge on $\sigma$ to set $M_{t}$
and $\eta_{t}$. In other words, when $\sigma=0$, we obtain the optimal
rate $O(T^{-1})$ automatically even not knowing $\sigma$.

\section{Theoretical Analysis\label{sec: analysis}}

We show the ideas for proving our theorems in this section. In Section
\ref{subsec:Two-lemmas}, two of the most fundamental lemmas used
in the proof are presented. In Sections \ref{subsec:lip-prob} and
\ref{subsec:lip-exp}, we focus on the high-probability rate and in-expectation
bound respectively for the case $\mu=0$. Several lemmas used to prove
these two results will be given, the omitted proofs of which are delivered
in Section \ref{sec:app-missing-proofs}. To the end, the proofs of
Theorems \ref{thm:lip-prob}, \ref{thm:lip-prob-fix} and \ref{thm:lip-exp},
\ref{thm:lip-exp-fix} are provided. The proof of the initial distance
adaptive bound is provided in Section \ref{sec:app-dog}. The analysis
for the strongly convex case (i.e., $\mu>0$) is deferred into Section
\ref{sec:app-str} in the appendix. 

Before going through the proof, we introduce some notations used in
the analysis. Let $\F_{t}=\sigma(\hp F(x_{1}),\cdots,\hp F(x_{t}))$
be the natural filtration. Under this definition, $x_{t}$ is $\F_{t-1}$
measurable. $\E_{t}[\cdot]$ denotes $\E[\cdot\mid\F_{t-1}]$ for
brevity. We also employ the following definitions:
\begin{align*}
\Delta_{t}\coloneqq & F(x_{t})-F(x_{*});\quad\pa_{t}\coloneqq\E_{t}\left[\hp F(x_{t})\right]\in\pa F(x_{t});\\
\xi_{t}\coloneqq & g_{t}-\pa_{t};\quad\xi_{t}^{u}\coloneqq g_{t}-\E_{t}\left[g_{t}\right];\quad\xi_{t}^{b}\coloneqq\E_{t}\left[g_{t}\right]-\pa_{t};\\
d_{t}\coloneqq & \left\Vert x_{t}-x_{*}\right\Vert ;\quad D_{t}\coloneqq\max_{s\in\left[t\right]}d_{s};\quad\mathfrak{D}_{t}\coloneqq D_{t}\lor\alpha;
\end{align*}
where $\alpha>0$ is the parameter used to set the step size $\eta_{t}$.
We remark that $\xi_{t},\xi_{t}^{u}\in\F_{t}$ and $\pa_{t},\xi_{t}^{b},d_{t},D_{t},\mathfrak{D}_{t}\in\F_{t-1}$.

\subsection{Fundamental Lemmas\label{subsec:Two-lemmas}}

To start with the analysis, we introduce Lemmas \ref{lem:err-bound}
and \ref{lem:basic}, which serve as foundations in our proof. 
\begin{lem}
\label{lem:err-bound}For any $t\in\left[T\right]$, if $M_{t}\geq2G$,
we have
\begin{align*}
\|\xi_{t}^{u}\| & \le2M_{t};\quad\E_{t}[\|\xi_{t}^{u}\|^{2}]\le10\sigma^{p}M_{t}^{2-p};\\
\|\xi_{t}^{b}\| & \le2\sigma^{p}M_{t}^{1-p};\quad\|\xi_{t}^{b}\|^{2}\leq10\sigma^{p}M_{t}^{2-p}.
\end{align*}
\end{lem}
%
Several similar results (except the bound on $\|\xi_{t}^{b}\|^{2}$)
to Lemma \ref{lem:err-bound} appear in \cite{zhang2020adaptive,gorbunov2020stochastic,gorbunov2021near,zhang2022parameter,sadiev2023high,nguyen2023high,liu2023breaking}
before. However, every existing analysis only considers $\ell_{2}$
norm. When a general norm is used, we provide an extended version
of Lemma \ref{lem:err-bound}, Lemma \ref{lem:err-bound-general}
in Section \ref{sec:general-norm} in the appendix along with the
proof. We refer the interested reader to Section \ref{sec:general-norm}
for details.

From a high-level overview, Lemma \ref{lem:err-bound} tells us how
small the errors $\xi_{t}^{u}$ and $\xi_{t}^{b}$ can be when $M_{t}\geq2G$.
Note that the part of $\xi_{t}^{u}$ will get larger as $M_{t}$ becomes
bigger, in contrast, the error $\xi_{t}^{b}$ (consider the bound
of $\|\xi_{t}^{b}\|\leq2\sigma^{p}M_{t}^{1-p}$) will decrease since
$1-p<0$ given $p\in(1,2]$. So $M_{t}$ should be chosen appropriately
to balance the order between $\|\xi_{t}^{u}\|$ and $\|\xi_{t}^{b}\|$.
Besides, note that our choice of $M_{t}$ always satisfies the condition
$M_{t}\geq2G$. Hereinafter, we will apply Lemma \ref{lem:err-bound}
directly in the analysis.
\begin{lem}
\label{lem:basic}For any $t\in\left[T\right]$, we have
\[
\Delta_{t}+\frac{\eta_{t}^{-1}}{2}d_{t+1}^{2}-\frac{\eta_{t}^{-1}-\mu}{2}d_{t}^{2}\leq\langle\xi_{t},x_{*}-x_{t}\rangle+\eta_{t}\left(2\left\Vert \xi_{t}^{u}\right\Vert ^{2}+2\left\Vert \xi_{t}^{b}\right\Vert ^{2}+G^{2}\right).
\]
\end{lem}
%
Lemma \ref{lem:basic} is the basic inequality used to bound both
the function value gap $\Delta_{t}$ and the distance term $d_{t}^{2}$.
The same as Lemma \ref{lem:err-bound}, we will present a generalized
version (Lemma \ref{lem:basic-general} in Section \ref{sec:general-norm})
for the general norm with its proof in the appendix.

Now let us explain Lemma \ref{lem:basic} a bit more here. If $\xi_{t}=\bzero$,
then one can view Lemma \ref{lem:basic} as a one-step descent lemma.
However, even after taking the expectation on both sides, the term
$\E[\langle\xi_{t},x_{*}-x_{t}\rangle]$ won't vanish since $\xi_{t}=g_{t}-\pa_{t}$
but $g_{t}$ is not an unbiased gradient estimator of $\pa_{t}$ due
to the clipping. So one of the hard parts of the analysis is how to
deal with the term $\langle\xi_{t},x_{*}-x_{t}\rangle$ both in expectation
and in a high-probability way. 

Another challenge is to deal with $\|\xi_{t}^{u}\|^{2}$ and $\|\xi_{t}^{b}\|^{2}$.
For $\|\xi_{t}^{b}\|^{2}$, Lemma \ref{lem:err-bound} already tells
us how to bound it. As for $\|\xi_{t}^{u}\|^{2}$, though it can be
bounded by $4M_{t}^{2}$ (by Lemma \ref{lem:err-bound} again). However,
this simple bound is not enough to obtain the correct order. For the
in-expectation analysis, after taking expectations on both sides of
Lemma \ref{lem:basic}, we can instead use the bound $\E_{t}[\|\xi_{t}^{u}\|^{2}]\leq10\sigma^{p}M_{t}^{2-p}$
in Lemma \ref{lem:err-bound}, which turns to be in a strictly smaller
order compared with $4M_{t}^{2}$ under our choice of $M_{t}$. Hence,
for the more complicated high-probability analysis, a hint of using
the bound on $\E_{t}[\|\xi_{t}^{u}\|^{2}]$ arises from analyzing
the in-expectation bound. As such, a natrual decomposition, $\|\xi_{t}^{u}\|^{2}=\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}]+\E_{t}[\|\xi_{t}^{u}\|^{2}]$,
shows up. We make this thought formally and show a high-probability
bound of $\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}]$ in Lemma
\ref{lem:xi-u-lip} in next section.

\subsection{High-Probability Analysis when $\mu=0$\label{subsec:lip-prob}}

In this section, our ultimate goal is to prove the high-probability
convergence rate, i.e., Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix}.
To save space, only the lemmas used for the case of unknown $T$ will
be stated formally. We will describe how the lemmas will change for
known $T$ accordingly in the remarks.

First, we present Lemma \ref{lem:normalize}, which is a powerful
tool when specialized to the case $\mu=0$. Lemma \ref{lem:normalize}
is immediately obtained from the definition of $\eta_{t}$, hence,
the proof of which is omitted.
\begin{lem}
\label{lem:normalize}When $\mu=0$, under our choices of $M_{t}$
and $\eta_{t}$ whenever $T$ is known or not, for any $t\in\left[T\right]$,
we have
\[
\eta_{t}M_{t}\leq\alpha.
\]
\end{lem}
%
Next, we introduce Lemma \ref{lem:basic-lip-prob}, which can be viewed
as a finer result of Lemma \ref{lem:basic} for $\mu=0$. 
\begin{lem}
\label{lem:basic-lip-prob}When $\mu=0$, under the choices of $M_{t}=2G\lor Mt^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{G\sqrt{t}}\land\frac{\alpha}{M_{t}}$,
for any $\tau\in\left[T\right]$, we have
\[
d_{\tau+1}^{2}+\sum_{t=1}^{\tau}2\eta_{t}\Delta_{t}\leq\mathfrak{D}_{\tau}\left(d_{1}+h(\tau)+\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle +\frac{4\eta_{t}^{2}}{\alpha}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\right)
\]
where
\[
h(\tau)=2(1+40(\sigma/M)^{p})\alpha\log(e\tau).
\]
\end{lem}
%
\begin{rem}
\label{rem:basic-lip-prob}For the case of known $T$, under the choices
of $M_{t}=2G\lor MT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{G\sqrt{T}}\land\frac{\alpha}{M_{t}}$,
$h(\tau)$ in Lemma \ref{lem:basic-lip-prob} will be $2(1+40(\sigma/M)^{p})\alpha$.
\end{rem}
%
Lemma \ref{lem:basic-lip-prob} is interesting in several ways. As
mentioned above, to use the conditional expectation bound on $\|\xi_{t}^{u}\|^{2}$,
the term $\eta_{t}^{2}(\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}])$
appears, which can be bounded by Freedman's inequality (Lemma \ref{lem:freedman}).
Next, the inner product $\eta_{t}\langle\xi_{t},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle$
seems very strange at first glance. However, this term can help us
to obtain an $O(\log(1/\delta))$ dependence on $\delta$ finally
instead of the sub-optimal $\log(T/\delta)$ shown in previous works.
We briefly explain why this is the right term here. Due to $\xi_{t}$
in the inner product, to use the bound on $\|\xi_{t}^{u}\|$ and $\|\xi_{t}^{b}\|$,
it is natural to consider $\eta_{t}\langle\xi_{t},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle=\eta_{t}\langle\xi_{t}^{b}+\xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle$.
For the term $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle$,
it will be bounded by $O(\log T)$ in the proof of Theorem \ref{thm:lip-prob}
directly. The other term, $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle$,
is more interesting. A key observation is that $\eta_{t}\langle\xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle$
is a martingale difference sequence, which may allow us to use the
concentration inequality to bound the summation again. Because of
the divisor $\mathfrak{D}_{t}$, $\eta_{t}\langle\xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\rangle$
admit an almost surely time uniform bound which implies we can apply
Freedman's inequality directly. As a result, we are able to obtain
a time-uniform high-probability bound at once.

In contrast, lots of existing works are to bound $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$,
however, $\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$ doesn't
have an almost surely bound necessarily. For such a reason, prior
works need to first bound the distance $d_{t}=\|x_{t}-x_{*}\|$ with
a high probability by induction then go back to bound $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$
for every $\tau$ by employing the high-probability bound on $d_{t}$.
This kind of roundabout argument leads to the sub-optimal dependence
of $\log(T/\delta)$.

Now we provide the desired bound described above in Lemma \ref{lem:lip-prob-concen},
As one can see, our bound holds for any $\tau\in[T]$ uniformly, hence,
which lifts the extra $\log T$ term. We refer the reader to Section
\ref{sec:app-missing-proofs} for more details of our proof of Lemma
\ref{lem:lip-prob-concen}.
\begin{lem}
\label{lem:lip-prob-concen}When $\mu=0$, under the choices of $M_{t}=2G\lor Mt^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{G\sqrt{t}}\land\frac{\alpha}{M_{t}}$,
we have with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$,
\[
\sum_{t=1}^{\tau}\eta_{t}\left\langle \xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle \leq5\left(\log\frac{4}{\delta}+\sqrt{(\sigma/M)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha.
\]
\end{lem}
%
\begin{rem}
\label{rem:lib-prob-loglog}For the case of known $T$, under the
choices of $M_{t}=2G\lor MT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{G\sqrt{T}}\land\frac{\alpha}{M_{t}}$,
the term $\log(eT)$ in Lemma \ref{lem:lip-prob-concen} will be removed
(or replaced by $1$ equivalently).
\end{rem}
%
Next, in Lemma \ref{lem:xi-u-lip}, we provide an any time high-probability
bound of the term $\sum_{t=1}^{\tau}\frac{\eta_{t}^{2}}{\alpha}(\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}])$
by using Freedman's inequality.
\begin{lem}
\label{lem:xi-u-lip}When $\mu=0$, under the choices of $M_{t}=2G\lor Mt^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{G\sqrt{t}}\land\frac{\alpha}{M_{t}}$,
we have with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$,
\[
\sum_{t=1}^{\tau}\frac{\eta_{t}^{2}}{\alpha}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq9\left(\log\frac{4}{\delta}+\sqrt{(\sigma/M)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha.
\]
\end{lem}
%
\begin{rem}
\label{rem:xi-u-lip}For the case of known $T$, under the choices
of $M_{t}=2G\lor MT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{G\sqrt{T}}\land\frac{\alpha}{M_{t}}$,
the term $\log(eT)$ in both bounds in Lemma \ref{lem:xi-u-lip} will
be removed (or replaced by $1$ equivalently).
\end{rem}
%
Equipped with the above lemmas, we are finally able to prove Theorems
\ref{thm:lip-prob} and \ref{thm:lip-prob-fix}.

\subsubsection{Proof of Theorem \ref{thm:lip-prob} }

\begin{proof}[Proof of Theorem \ref{thm:lip-prob}]
We first define a constant $K$ as follows
\begin{align}
K\coloneqq & \alpha^{2}+\left(d_{1}+2\alpha\log(eT)+107(\sigma/M)^{p}\alpha\log(eT)+69\alpha\log\frac{4}{\delta}\right)^{2}\label{eq:lip-prob-def-k}\\
= & O\left(\alpha^{2}\left((1+(\sigma/M)^{2p})\log^{2}T+\log^{2}(1/\delta)\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).\nonumber 
\end{align}

We sart with Lemma \ref{lem:basic-lip-prob} to get for any $\tau\in\left[T\right]$,
there is
\begin{align}
d_{\tau+1}^{2}+\sum_{t=1}^{\tau}2\eta_{t}\Delta_{t}\leq & \mathfrak{D}_{\tau}\left(d_{1}+h(\tau)+\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle +\frac{4\eta_{t}^{2}}{\alpha}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\right)\nonumber \\
= & \mathfrak{D}_{\tau}\left(d_{1}+h(\tau)+\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t}^{b},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle +2\eta_{t}\left\langle \xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle +\frac{4\eta_{t}^{2}}{\alpha}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\right)\label{eq:lip-1}
\end{align}
\begin{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t}^{b},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle $:
We know
\begin{align}
\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t}^{b},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle  & \leq\sum_{t=1}^{\tau}2\eta_{t}\left\Vert \xi_{t}^{b}\right\Vert \frac{d_{t}}{\mathfrak{D}_{t}}\overset{(a)}{\leq}\sum_{t=1}^{\tau}2\cdot\eta_{t}\cdot2\sigma^{p}M_{t}^{1-p}\nonumber \\
 & \overset{(b)}{\leq}\sum_{t=1}^{\tau}\frac{4(\sigma/M)^{p}\alpha}{t}\leq4(\sigma/M)^{p}\alpha\log(e\tau)\label{eq:lip-xi-b}
\end{align}
where $(a)$ is due to $\left\Vert \xi_{t}^{b}\right\Vert \leq2\sigma^{p}M_{t}^{1-p}$
by Lemma \ref{lem:err-bound} and $d_{t}\leq\mathfrak{D}_{t}$. $(b)$
is by $\eta_{t}M_{t}\leq\alpha$ from Lemma \ref{lem:normalize} and
$M_{t}\geq Mt^{\frac{1}{p}}$ from our choice.
\item Bounding the term $\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle $:
By Lemma \ref{lem:lip-prob-concen}, we have with probability at least
$1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}2\eta_{t}\left\langle \xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{t}}\right\rangle \leq10\left(\log\frac{4}{\delta}+\sqrt{(\sigma/M)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha.\label{eq:lip-xi-u}
\end{equation}
\item Bounding the term $\sum_{t=1}^{\tau}\frac{4\eta_{t}^{2}}{\alpha}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)$:
By Lemma \ref{lem:xi-u-lip}, we have with probability at least $1-\frac{\delta}{2}$,
for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}\frac{4\eta_{t}^{2}}{\alpha}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq36\left(\log\frac{4}{\delta}+\sqrt{(\sigma/M)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha.\label{eq:lip-xi-u-2}
\end{equation}
\end{itemize}
Combining (\ref{eq:lip-1}), (\ref{eq:lip-xi-b}), (\ref{eq:lip-xi-u})
and (\ref{eq:lip-xi-u-2}), we have with probability at least $1-\delta$,
for any $\tau\in\left[T\right]$:
\begin{align*}
d_{\tau+1}^{2}+\sum_{t=1}^{\tau}2\eta_{t}\Delta_{t}\leq & \mathfrak{D}_{\tau}\left(d_{1}+h(\tau)+4(\sigma/M)^{p}\alpha\log(e\tau)+46\left(\log\frac{4}{\delta}+\sqrt{(\sigma/M)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha\right)\\
\overset{(c)}{\leq} & \mathfrak{D}_{\tau}\left(d_{1}+2\alpha\log(eT)+84(\sigma/M)^{p}\alpha\log(eT)+46\left(\log\frac{4}{\delta}+\sqrt{(\sigma/M)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha\right)\\
\leq & \mathfrak{D}_{\tau}\left(d_{1}+2\alpha\log(eT)+107(\sigma/M)^{p}\alpha\log(eT)+69\alpha\log\frac{4}{\delta}\right)\\
\leq & \frac{\mathfrak{D_{\tau}^{2}}+\left(d_{1}+2\alpha\log(eT)+107(\sigma/M)^{p}\alpha\log(eT)+69\alpha\log\frac{4}{\delta}\right)^{2}}{2}\\
\overset{(d)}{\leq} & \frac{D_{\tau}^{2}+\alpha^{2}+\left(d_{1}+2\alpha\log(eT)+107(\sigma/M)^{p}\alpha\log(eT)+69\alpha\log\frac{4}{\delta}\right)^{2}}{2}\\
\overset{(e)}{=} & \frac{D_{\tau}^{2}+K}{2}
\end{align*}
where $(c)$ is by plugging in $h(\tau)=2(1+40(\sigma/M)^{p})\alpha\log(e\tau)\leq2(1+40(\sigma/M)^{p})\alpha\log(eT)$;
$(d)$ is by $\mathfrak{D}_{\tau}^{2}=(D_{\tau}\lor\alpha)^{2}\leq D_{\tau}^{2}+\alpha^{2}$;
$(e)$ is due to the definition of $K$ (see (\ref{eq:lip-prob-def-k})).
Hence, by using $\Delta_{t}\geq0$, we have for any $\tau\in\left[T\right]$,
\[
d_{\tau+1}^{2}\leq\frac{D_{\tau}^{2}+K}{2},
\]
which implies $d_{t}^{2}\leq D_{t}^{2}\leq K$ for any $t\in\left[T+1\right]$
by simple induction.

Finally, we consider time $T$ to get with probability at least $1-\delta$
\[
\sum_{t=1}^{T}2\eta_{t}\Delta_{t}\leq d_{T+1}^{2}+\sum_{t=1}^{T}2\eta_{t}\Delta_{t}\leq\frac{D_{T}^{2}+K}{2}\leq K.
\]
Note that $\eta_{t}$ is non-increasing and $F(\bar{x}_{T})-F(x_{*})\leq\frac{\sum_{t=1}^{T}\Delta_{t}}{T}$
by the convexity of $F$ where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$,
we conclude that
\[
F(\bar{x}_{T})-F(x_{*})\leq\frac{K}{2\eta_{T}T}.
\]
Plugging $K$ and $\eta_{t}$, we get the desired result.
\end{proof}


\subsubsection{Proof of Theorem \ref{thm:lip-prob-fix}}

\begin{proof}[Proof of Theorem \ref{thm:lip-prob-fix}]
Following a similar proof of Theorem \ref{thm:lip-prob}, we will
obtain that with probability at least $1-\delta$, for any $\tau\in\left[T\right]$,
there is
\[
d_{\tau+1}^{2}+\sum_{t=1}^{\tau}2\eta_{t}\Delta_{t}\leq K
\]
where $K$ is currently defined as
\begin{align}
K\coloneqq & \alpha^{2}+\left(d_{1}+2\alpha+107(\sigma/M)^{p}\alpha+69\alpha\log\frac{4}{\delta}\right)^{2}\label{eq:lip-prob-fix-def-k}\\
= & O\left(\alpha^{2}\left((\sigma/M)^{2p}+\log^{2}(1/\delta)\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).\nonumber 
\end{align}

Hence, we have with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq\frac{1}{T}\sum_{t=1}^{T}\Delta_{t}\leq\frac{K}{2\eta_{T}T}
\]
where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Finally, plugging
in $K$ and $\eta_{T}$, the proof is finished.
\end{proof}


\subsection{In-Expectation Analysis when $\mu=0$\label{subsec:lip-exp}}

Now we turn to the in-expectation bound of Algorithm \ref{alg:algo}
for the general convex case. We introduce Lemma \ref{lem:basic-lip-exp}.
which is enough to let us prove Theorems \ref{thm:lip-exp} and \ref{thm:lip-exp-fix}.
\begin{lem}
\label{lem:basic-lip-exp}When $\mu=0$, for any $\tau\in\left[T\right]$,
we have
\[
\frac{\E\left[d_{\tau+1}^{2}\right]-\E\left[d_{1}^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\alpha h(\tau)}{2}+2(\sigma/M)^{p}\alpha\sum_{t=1}^{\tau}\frac{\sqrt{\E\left[d_{t}^{2}\right]}}{t}
\]
where $h(\tau)$ is defined in Lemma \ref{lem:basic-lip-prob}.
\end{lem}
%
\begin{rem}
For the case of known $T$, under the choices of $M_{t}=2G\lor MT^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{G\sqrt{T}}\land\frac{\alpha}{M_{t}}$,
in Lemma \ref{lem:basic-lip-exp}, $\sqrt{\E[d_{t}^{2}]}/t$ will
be replaced by $\sqrt{\E[d_{t}^{2}]}/T$ and $h(\tau)$ will be changed
according to Remark \ref{rem:basic-lip-prob}.
\end{rem}
%
Note that Lemma \ref{lem:basic-lip-prob} is a very interesting result.
On the L.H.S., there is $\E[d_{\tau+1}^{2}]$, however, the R.H.S.
only has $\sqrt{\E[d_{t}^{2}]}$ for any $t\le\tau$. If we assume
$\E[d_{t}^{2}]$ can be uniformly bounded by some $K$ for any $t\leq\tau$,
we know $\sum_{t=1}^{\tau}\sqrt{\E[d_{t}^{2}]}/t=O(\sqrt{K}\log T)$,
which implies $\E[d_{\tau+1}^{2}]=O(\alpha h(\tau)+(\sigma/M)^{p}\alpha\sqrt{K}\log T+d_{1}^{2})$.
Such a result tells us $\E[d_{\tau+1}^{2}]$ can still be bounded
by $K$ once $K$ is picked properly. So we can expect a uniform bound
on $\max_{s\in\left[t\right]}\E[d_{s}^{2}]$ for any $t\leq T$. We
will show how to demonstrate this idea formally in the proof of Theorems
\ref{thm:lip-exp} and \ref{thm:lip-exp-fix}.

\subsubsection{Proof of Theorem \ref{thm:lip-exp}}

\begin{proof}[Proof of Theorem \ref{thm:lip-exp}]
We first define the notation $\mathcal{D}_{t}=\max_{s\in\left[t\right]}\sqrt{\E\left[d_{s}^{2}\right]}$.
Next, consider the following constant
\begin{align}
K\coloneqq & 16(\sigma/M)^{2p}\alpha^{2}\log^{2}(eT)+4(1+40(\sigma/M)^{p})\alpha^{2}\log(eT)+2d_{1}^{2}\label{eq:lip-exp-def-K}\\
= & O\left(\alpha^{2}\left(\log T+(\sigma/M)^{2p}\log^{2}T\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).\nonumber 
\end{align}

We invoke Lemma \ref{lem:basic-lip-exp} to get for any time $\tau\in\left[T\right]$,
\begin{align*}
\frac{\E\left[d_{\tau+1}^{2}\right]-\E\left[d_{1}^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq & \frac{\alpha h(\tau)}{2}+2(\sigma/M)^{p}\alpha\sum_{t=1}^{\tau}\frac{\sqrt{\E\left[d_{t}^{2}\right]}}{t}\\
\overset{(a)}{\leq} & \frac{\alpha h(\tau)}{2}+2\alpha\sum_{t=1}^{\tau}\frac{\mathcal{D}_{\tau}}{t}\leq\frac{\alpha h(\tau)}{2}+2(\sigma/M)^{p}\alpha\log(eT)\mathcal{D}_{\tau}\\
\Rightarrow\frac{\E\left[d_{\tau+1}^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\overset{(b)}{\leq} & \frac{\mathcal{D}_{\tau}^{2}}{4}+4(\sigma/M)^{2p}\alpha^{2}\log^{2}(eT)+(1+40(\sigma/M)^{p})\alpha^{2}\log(eT)+\frac{d_{1}^{2}}{2}\\
\overset{(c)}{=} & \frac{\mathcal{D}_{\tau}^{2}}{4}+\frac{K}{4}
\end{align*}
where $(a)$ is by the definition of $\mathcal{D}_{\tau}$; $(b)$
is by using due to AM-GM inequality for the term $2(\sigma/M)^{p}\alpha\log(eT)\mathcal{D}_{\tau}$
and plugging in $\alpha h(\tau)/2=(1+40(\sigma/M)^{p})\alpha^{2}\log(e\tau)\leq(1+40(\sigma/M)^{p})\alpha^{2}\log(eT)$;
$(c)$ is from the definition of $K$ (see (\ref{eq:lip-exp-def-K})).
Thus, for any $\tau\in\left[T\right]$, there is
\[
\frac{\E\left[d_{\tau+1}^{2}\right]}{2}\leq\frac{\E\left[d_{\tau+1}^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\mathcal{D}_{\tau}^{2}}{4}+\frac{K}{4}\Rightarrow d_{\tau+1}^{2}\leq\frac{\mathcal{D}_{\tau}^{2}}{2}+\frac{K}{2},
\]
which implies $\E\left[d_{t}^{2}\right]\leq\mathcal{D}_{t}^{2}\leq K$
for any $t\in\left[T+1\right]$ by simple induction.

Finally, for time $T$, we know
\[
\sum_{t=1}^{T}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\E\left[d_{T+1}^{2}\right]}{2}+\sum_{t=1}^{T}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\mathcal{D}_{T}^{2}}{4}+\frac{K}{4}\leq\frac{K}{2}.
\]
Note that is $\eta_{t}$ non-increasing and $\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq\frac{\sum_{t=1}^{T}\E\left[\Delta_{t}\right]}{T}$
by the convexity of $F$ where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$,
we conclude that
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq\frac{K}{2\eta_{T}T}.
\]
Plugging $K$ and $\eta_{t}$, we get the desired result.
\end{proof}


\subsubsection{Proof of Theorem \ref{thm:lip-exp-fix}.}

\begin{proof}[Proof of Theorem \ref{thm:lip-exp-fix}]
Following the same line in the proof of Theorem \ref{thm:lip-exp},
we only need to notice that the constant $K$ now is defined as
\begin{align}
K\coloneqq & 16(\sigma/M)^{2p}\alpha^{2}+4(1+40(\sigma/M)^{p})\alpha^{2}+2d_{1}^{2}\label{eq:lip-exp-known-def-K}\\
= & O\left(\alpha^{2}\left(1+(\sigma/M)^{2p}\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).\nonumber 
\end{align}
By similar steps, we will reach $\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq\frac{K}{2\eta_{T}T}$
again where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Plugging
$K$ and $\eta_{t}$ for two cases respectively, we get the desired
result.
\end{proof}

