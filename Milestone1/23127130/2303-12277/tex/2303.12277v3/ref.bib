@inproceedings{McMahanS10,
  author    = {H. Brendan McMahan and
               Matthew J. Streeter},
  title     = {Adaptive Bound Optimization for Online Convex Optimization},
  booktitle = {Conference on Learning Theory {(COLT)}},
  pages     = {244--256},
  publisher = {Omnipress},
  year      = {2010}
}

@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@inproceedings{antonakopoulos2021adaptive,
title={Adaptive Extra-Gradient Methods for Min-Max Optimization and Games},
author={Kimon Antonakopoulos and Veronica Belmega and Panayotis Mertikopoulos},
booktitle={International Conference on Learning Representations {(ICLR)}},
year={2021},
}

@article{ene2021variational,
  title={Adaptive and Universal Algorithms for Variational Inequalities with Optimal Convergence
s},
  author={Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2010.07799},
  year={2021}
}

@inproceedings{MohriYang16,
  title={Accelerating online convex optimization via adaptive prediction},
  author={Mohri, Mehryar and Yang, Scott},
  booktitle={Artificial Intelligence and Statistics {(AISTATS)}},
  pages={848--856},
  year={2016}
}

@inproceedings{KavisLBC19,
  author    = {Ali Kavis and
               Kfir Y. Levy and
               Francis Bach and
               Volkan Cevher},
  title     = {UniXGrad: {A} Universal, Adaptive Algorithm with Optimal Guarantees
               for Constrained Optimization},
  booktitle = {Advances in Neural Information Processing Systems {(NeurIPS)}},
  pages     = {6257--6266},
  year      = {2019}
}

@inproceedings{levy2018online,
  title={Online adaptive methods, universality and acceleration},
  author={Levy, Kfir Y and Yurtsever, Alp and Cevher, Volkan},
  booktitle={Advances in Neural Information Processing Systems {(NeurIPS)}},
  pages={6500--6509},
  year={2018}
}

@inproceedings{Cutkosky19,
  author    = {Ashok Cutkosky},
  title     = {Anytime Online-to-Batch, Optimism and Acceleration},
  booktitle = {International Conference of Machine Learning {(ICML)}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {1446--1454},
  publisher = {{PMLR}},
  year      = {2019}
}

@inproceedings{Levy17,
  author    = {Kfir Y. Levy},
  title     = {Online to Offline Conversions, Universality and Adaptive Minibatch
               Sizes},
  booktitle = {Advances in Neural Information Processing Systems {(NeurIPS)}},
  pages     = {1613--1622},
  year      = {2017}
}

@article{lan2019unified,
  title={A unified variance-reduced accelerated gradient method for convex optimization},
  author={Lan, Guanghui and Li, Zhize and Zhou, Yi},
  journal={arXiv preprint arXiv:1905.12412},
  year={2019}
}
@article{song2020variance,
  title={Variance Reduction via Accelerated Dual Averaging for Finite-Sum Optimization},
  author={Song, Chaobing and Jiang, Yong and Ma, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={315--323},
  year={2013}
}
@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}
@article{dubois2021svrg,
  title={SVRG Meets AdaGrad: Painless Variance Reduction},
  author={Dubois-Taine, Benjamin and Vaswani, Sharan and Babanezhad, Reza and Schmidt, Mark and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2102.09645},
  year={2021}
}
@inproceedings{allen2016improved,
  title={Improved SVRG for non-strongly-convex or sum-of-non-convex objectives},
  author={Allen-Zhu, Zeyuan and Yuan, Yang},
  booktitle={International conference on machine learning},
  pages={1080--1089},
  year={2016},
  organization={PMLR}
}
@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@inproceedings{bach2019universal,
  title={A universal algorithm for variational inequalities adaptive to smoothness and noise},
  author={Bach, Francis and Levy, Kfir Y},
  booktitle={Conference on Learning Theory},
  pages={164--194},
  year={2019},
  organization={PMLR}
}
@inproceedings{ene2021adaptive,
  title={Adaptive Gradient Methods for Constrained Convex Optimization and Variational Inequalities},
  author={Ene, Alina and Nguyen, Huy L and Vladu, Adrian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={7314--7321},
  year={2021}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}
@article{lin2015universal,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  journal={arXiv preprint arXiv:1506.02186},
  year={2015}
}
@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k\^{} 2)},
  author={Nesterov, Yurii},
  booktitle={Doklady an ussr},
  volume={269},
  pages={543--547},
  year={1983}
}
@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}
@article{woodworth2016tight,
  title={Tight complexity bounds for optimizing composite objectives},
  author={Woodworth, Blake E and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3639--3647},
  year={2016}
}
@article{li2021anita,
  title={ANITA: An Optimal Loopless Accelerated Variance-Reduced Gradient Method},
  author={Li, Zhize},
  journal={arXiv preprint arXiv:2103.11333},
  year={2021}
}
@inproceedings{reddi2018convergence,
  title={On the Convergence of Adam and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  pages={6677--6686},
  year={2019},
  organization={PMLR}
}
@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={983--992},
  year={2019},
  organization={PMLR}
}
@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey and others},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}
@inproceedings{chen2018convergence,
  title={On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{zou2018weighted,
  title={Weighted adagrad with unified momentum},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Sun, Ju and Liu, Wei},
  journal={arXiv preprint arXiv:1808.03408},
  year={2018}
}
@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}
@article{roux2012stochastic,
  title={A stochastic gradient method with an exponential convergence rate for finite training sets},
  author={Roux, Nicolas Le and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1202.6258},
  year={2012}
}
@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={2},
  year={2013}
}
@inproceedings{mairal2013optimization,
  title={Optimization with first-order surrogate functions},
  author={Mairal, Julien},
  booktitle={International Conference on Machine Learning},
  pages={783--791},
  year={2013},
  organization={PMLR}
}
@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014}
}
@article{su2014differential,
  title={A differential equation for modeling Nesterovâ€™s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={2510--2518},
  year={2014}
}
@article{lan2018random,
  title={Random gradient extrapolation for distributed and stochastic optimization},
  author={Lan, Guanghui and Zhou, Yi},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={4},
  pages={2753--2782},
  year={2018},
  publisher={SIAM}
}
@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}
@article{cutkosky2019momentum,
  title={Momentum-Based Variance Reduction in Non-Convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{levy2021storm+,
  title={STORM+: Fully Adaptive SGD with Recursive Momentum for Nonconvex Optimization},
  author={Levy, Kfir and Kavis, Ali and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{huang2021super,
  title={SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients},
  author={Huang, Feihu and Li, Junyi and Huang, Heng},
  journal={arXiv preprint arXiv:2106.08208},
  year={2021}
}
@article{lan2018optimal,
  title={An optimal randomized incremental gradient method},
  author={Lan, Guanghui and Zhou, Yi},
  journal={Mathematical programming},
  volume={171},
  number={1},
  pages={167--215},
  year={2018},
  publisher={Springer}
}
@article{allen2018katyusha,
  title={Katyusha x: Practical momentum method for stochastic sum-of-nonconvex optimization},
  author={Allen-Zhu, Zeyuan},
  journal={arXiv preprint arXiv:1802.03866},
  year={2018}
}
@article{allen2018natasha,
  title={Natasha 2: Faster Non-Convex Optimization Than SGD},
  author={Allen-Zhu, Zeyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={2675--2686},
  year={2018}
}
@inproceedings{allen2016variance,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={International conference on machine learning},
  pages={699--707},
  year={2016},
  organization={PMLR}
}
@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={arXiv preprint arXiv:1807.01695},
  year={2018}
}
@article{zhou2018stochastic,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{tan2016barzilai,
  title={Barzilai-Borwein Step Size for Stochastic Gradient Descent},
  author={Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  pages={685--693},
  year={2016}
}
@inproceedings{li2020almost,
  title={Almost tune-free variance reduction},
  author={Li, Bingcong and Wang, Lingda and Giannakis, Georgios B},
  booktitle={International Conference on Machine Learning},
  pages={5969--5978},
  year={2020},
  organization={PMLR}
}
@inproceedings{joulani2020simpler,
  title={A simpler approach to accelerated optimization: iterative averaging meets optimism},
  author={Joulani, Pooria and Raj, Anant and Gyorgy, Andras and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Machine Learning},
  pages={4984--4993},
  year={2020},
  organization={PMLR}
}
@inproceedings{kavis2021high,
  title={High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize},
  author={Kavis, Ali and Levy, Kfir Yehuda and Cevher, Volkan},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@inproceedings{reddi2016stochastic,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={International conference on machine learning},
  pages={314--323},
  year={2016},
  organization={PMLR}
}

@article{lei2017non,
  title={Non-convex finite-sum optimization via scsg methods},
  author={Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{arjevani2019lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={arXiv preprint arXiv:1912.02365},
  year={2019}
}

@article{nguyen2017stochastic,
  title={Stochastic recursive gradient algorithm for nonconvex optimization},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:1705.07261},
  year={2017}
}

@article{nguyen2021inexact,
  title={Inexact SARAH algorithm for stochastic optimization},
  author={Nguyen, Lam M and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Methods and Software},
  volume={36},
  number={1},
  pages={237--258},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{tran2019hybrid,
  title={Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization},
  author={Tran-Dinh, Quoc and Pham, Nhan H and Phan, Dzung T and Nguyen, Lam M},
  journal={arXiv preprint arXiv:1905.05920},
  year={2019}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15042--15053},
  year={2020}
}

@article{ene2022high,
  title={High Probability Convergence for Accelerated Stochastic Mirror Descent},
  author={Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2210.00679},
  year={2022}
}


@article{cutkosky2021high,
  title={High-probability bounds for non-convex stochastic optimization with heavy tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4883--4895},
  year={2021}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  author={Lan, Guanghui},
  year={2020},
  publisher={Springer}
}
@inproceedings{harvey2019tight,
  title={Tight analyses for non-smooth stochastic gradient descent},
  author={Harvey, Nicholas JA and Liaw, Christopher and Plan, Yaniv and Randhawa, Sikander},
  booktitle={Conference on Learning Theory},
  pages={1579--1613},
  year={2019},
  organization={PMLR}
}
@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  publisher={JMLR. org}
}
@article{kakade2008generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  year={2008}
}
@article{rakhlin2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1109.5647},
  year={2011}
}
@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}
@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1},
  pages={365--397},
  year={2012},
  publisher={Springer}
}
@techreport{devolder2011stochastic,
  title={Stochastic first order methods in smooth convex optimization},
  author={Devolder, Olivier and others},
  year={2011},
  institution={CORE}
}
@article{dvurechensky2016stochastic,
  title={Stochastic intermediate gradient method for convex problems with stochastic inexact oracle},
  author={Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={Journal of Optimization Theory and Applications},
  volume={171},
  number={1},
  pages={121--145},
  year={2016},
  publisher={Springer}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{nazin2019algorithms,
  title={Algorithms of robust stochastic optimization based on mirror descent method},
  author={Nazin, Alexander V and Nemirovsky, Arkadi S and Tsybakov, Alexandre B and Juditsky, Anatoli B},
  journal={Automation and Remote Control},
  volume={80},
  number={9},
  pages={1607--1627},
  year={2019},
  publisher={Springer}
}

@article{li2020high,
  title={A high probability analysis of adaptive SGD with momentum},
  author={Li, Xiaoyu and Orabona, Francesco},
  journal={arXiv preprint arXiv:2007.14294},
  year={2020}
}
@article{liu2022convergence,
  title={On the Convergence of AdaGrad on {$\mathbb{R}^{d} $}: Beyond Convexity, Non-Asymptotic Rate and Acceleration},
  author={Liu, Zijian and Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2209.14827},
  year={2022}
}


@article{freedman1975tail,
  title={On tail probabilities for martingales},
  author={Freedman, David A},
  journal={the Annals of Probability},
  pages={100--118},
  year={1975},
  publisher={JSTOR}
}

@article{crawshaw2022robustness,
  title={Robustness to Unbounded Smoothness of Generalized SignSGD},
  author={Crawshaw, Michael and Liu, Mingrui and Orabona, Francesco and Zhang, Wei and Zhuang, Zhenxun},
  journal={arXiv preprint arXiv:2208.11195},
  year={2022}
}

@article{liu2022meta,
  title={META-STORM: Generalized Fully-Adaptive Variance Reduced SGD for Unbounded Functions},
  author={Liu, Zijian and Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2209.14853},
  year={2022}
}

@article{liu2020optimal,
  title={An optimal hybrid variance-reduced algorithm for stochastic composite nonconvex optimization},
  author={Liu, Deyi and Nguyen, Lam M and Tran-Dinh, Quoc},
  journal={arXiv preprint arXiv:2008.09055},
  year={2020}
}

@inproceedings{li2021page,
  title={PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={6286--6295},
  year={2021},
  organization={PMLR}
}

@article{cutkosky2022lecture,
  title={Lecture Notes for EC525: Optimization for Machine Learning},
  author={Cutkosky, Ashok},
  journal={EC525: Optimization for Machine Learning},
  year={2022}
}

@inproceedings{davis2020high,
  title={High probability guarantees for stochastic convex optimization},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  booktitle={Conference on Learning Theory},
  pages={1411--1427},
  year={2020},
  organization={PMLR}
}

@inproceedings{liu2022adaptive,
  title={Adaptive accelerated (extra-) gradient methods with variance reduction},
  author={Liu, Zijian and Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy},
  booktitle={International Conference on Machine Learning},
  pages={13947--13994},
  year={2022},
  organization={PMLR}
}

@inproceedings{carmon2022recapp,
  title={Recapp: Crafting a more efficient catalyst for convex optimization},
  author={Carmon, Yair and Jambulapati, Arun and Jin, Yujia and Sidford, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={2658--2685},
  year={2022},
  organization={PMLR}
}


@inproceedings{zhou2019direct,
  title={Direct acceleration of SAGA using sampled negative momentum},
  author={Zhou, Kaiwen and Ding, Qinghua and Shang, Fanhua and Cheng, James and Li, Danli and Luo, Zhi-Quan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1602--1610},
  year={2019},
  organization={PMLR}
}

@article{krizhevsky2009cifar10,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex and Hinton, Geoffrey and others},
	year={2009},
	publisher={Citeseer}
}

@inproceedings{ren2016resnet,
	title={Deep residual learning for image recognition},
	author={Ren, Shaoqing and Sun, Jian and He, K and Zhang, X},
	booktitle={CVPR},
	volume={2},
	pages={4},
	year={2016}
}

@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

@article{csimcsekli2019heavy,
  title={On the heavy-tailed theory of stochastic gradient descent for deep neural networks},
  author={{\c{S}}im{\c{s}}ekli, Umut and G{\"u}rb{\"u}zbalaban, Mert and Nguyen, Thanh Huy and Richard, Ga{\"e}l and Sagun, Levent},
  journal={arXiv preprint arXiv:1912.00018},
  year={2019}
}

@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={5827--5837},
  year={2019},
  organization={PMLR}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{dzhaparidze2001bernstein,
  title={On Bernstein-type inequalities for martingales},
  author={Dzhaparidze, Kacha and Van Zanten, JH},
  journal={Stochastic processes and their applications},
  volume={93},
  number={1},
  pages={109--117},
  year={2001},
  publisher={Elsevier}
}

@article{bennett1962probability,
  title={Probability inequalities for the sum of independent random variables},
  author={Bennett, George},
  journal={Journal of the American Statistical Association},
  volume={57},
  number={297},
  pages={33--45},
  year={1962},
  publisher={Taylor \& Francis}
}

@article{liu2023breaking,
  title={Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex Stochastic Optimization with Heavy-Tailed Noise},
  author={Liu, Zijian and Zhang, Jiawei and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2302.06763},
  year={2023}
}

@article{zhang2022parameter,
  title={Parameter-free Regret in High Probability with Heavy Tails},
  author={Zhang, Jiujia and Cutkosky, Ashok},
  journal={arXiv preprint arXiv:2210.14355},
  year={2022}
}

@inproceedings{vural2022mirror,
  title={Mirror descent strikes again: Optimal stochastic convex optimization under infinite noise variance},
  author={Vural, Nuri Mert and Yu, Lu and Balasubramanian, Krishna and Volgushev, Stanislav and Erdogdu, Murat A},
  booktitle={Conference on Learning Theory},
  pages={65--102},
  year={2022},
  organization={PMLR}
}

@article{sadiev2023high,
  title={High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  author={Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2302.00999},
  year={2023}
}

@article{nguyen2023high,
  title={High Probability Convergence of Clipped-SGD Under Heavy-tailed Noise},
  author={Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy Le},
  journal={arXiv preprint arXiv:2302.05437},
  year={2023}
}

@article{gorbunov2021near,
  title={Near-optimal high probability complexity bounds for non-smooth stochastic optimization with heavy-tailed noise},
  author={Gorbunov, Eduard and Danilova, Marina and Shibaev, Innokentiy and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2106.05958},
  year={2021}
}

@inproceedings{carmon2022making,
  title={Making sgd parameter-free},
  author={Carmon, Yair and Hinder, Oliver},
  booktitle={Conference on Learning Theory},
  pages={2360--2389},
  year={2022},
  organization={PMLR}
}

@article{lu2018relatively,
  title={Relatively smooth convex optimization by first-order methods, and applications},
  author={Lu, Haihao and Freund, Robert M and Nesterov, Yurii},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={1},
  pages={333--354},
  year={2018},
  publisher={SIAM}
}

@inproceedings{raginsky2009information,
  title={Information complexity of black-box convex optimization: A new look via feedback information theory},
  author={Raginsky, Maxim and Rakhlin, Alexander},
  booktitle={2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={803--510},
  year={2009},
  organization={IEEE}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovski, Arkadi and Yudin, David},
  year={1983},
  journal={Wiley-Interscience},
  publisher={Wiley-Interscience}
}

@inproceedings{faw2022power,
  title={The power of adaptivity in sgd: Self-tuning step sizes with unbounded gradients and affine variance},
  author={Faw, Matthew and Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan and Shakkottai, Sanjay and Ward, Rachel},
  booktitle={Conference on Learning Theory},
  pages={313--355},
  year={2022},
  organization={PMLR}
}

@article{jakovetic2022nonlinear,
  title={Nonlinear gradient mappings and stochastic optimization: A general framework with applications to heavy-tail noise},
  author={Jakovetic, Dusan and Bajovic, Dragana and Sahu, Anit Kumar and Kar, Soummya and Milosevic, Nemanja and Stamenkovic, Dusan},
  journal={arXiv preprint arXiv:2204.02593},
  year={2022}
}

@article{lou2022beyond,
  title={Beyond sub-gaussian noises: Sharp concentration analysis for stochastic gradient descent},
  author={Lou, Zhipeng and Zhu, Wanrong and Wu, Wei Biao},
  journal={Journal of Machine Learning Research},
  volume={23},
  pages={1--22},
  year={2022}
}


@article{wang2021convergence,
  title={Convergence rates of stochastic gradient descent under infinite noise variance},
  author={Wang, Hongjian and Gurbuzbalaban, Mert and Zhu, Lingjiong and Simsekli, Umut and Erdogdu, Murat A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18866--18877},
  year={2021}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven Chu Hong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21285--21296},
  year={2020}
}

@inproceedings{gurbuzbalaban2021fractional,
  title={Fractional moment-preserving initialization schemes for training deep neural networks},
  author={Gurbuzbalaban, Mert and Hu, Yuanhan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2233--2241},
  year={2021},
  organization={PMLR}
}

@inproceedings{hodgkinson2021multiplicative,
  title={Multiplicative noise and heavy tails in stochastic optimization},
  author={Hodgkinson, Liam and Mahoney, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4262--4274},
  year={2021},
  organization={PMLR}
}

@inproceedings{camuto2021asymmetric,
  title={Asymmetric heavy tails and implicit bias in gaussian noise injections},
  author={Camuto, Alexander and Wang, Xiaoyu and Zhu, Lingjiong and Holmes, Chris and Gurbuzbalaban, Mert and Simsekli, Umut},
  booktitle={International Conference on Machine Learning},
  pages={1249--1260},
  year={2021},
  organization={PMLR}
}

@article{mirek2011heavy,
  title={Heavy tail phenomenon and convergence to stable laws for iterated Lipschitz maps},
  author={Mirek, Mariusz},
  journal={Probability Theory and Related Fields},
  volume={151},
  number={3-4},
  pages={705--734},
  year={2011},
  publisher={Springer}
}

@article{parletta2022high,
  title={High Probability Bounds for Stochastic Subgradient Schemes with Heavy Tailed Noise},
  author={Parletta, Daniela A and Paudice, Andrea and Pontil, Massimiliano and Salzo, Saverio},
  journal={arXiv preprint arXiv:2208.08567},
  year={2022}
}

@article{ivgi2023dog,
  title={DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule},
  author={Ivgi, Maor and Hinder, Oliver and Carmon, Yair},
  journal={arXiv preprint arXiv:2302.12022},
  year={2023}
}

@article{mcmahan2012no,
  title={No-regret algorithms for unconstrained online convex optimization},
  author={Mcmahan, Brendan and Streeter, Matthew},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@article{orabona2016coin,
  title={Coin betting and parameter-free online learning},
  author={Orabona, Francesco and P{\'a}l, D{\'a}vid},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{mcmahan2014unconstrained,
  title={Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations},
  author={McMahan, H Brendan and Orabona, Francesco},
  booktitle={Conference on Learning Theory},
  pages={1020--1039},
  year={2014},
  organization={PMLR}
}

@inproceedings{zhang2022pde,
  title={PDE-based optimal strategy for unconstrained online learning},
  author={Zhang, Zhiyu and Cutkosky, Ashok and Paschalidis, Ioannis},
  booktitle={International Conference on Machine Learning},
  pages={26085--26115},
  year={2022},
  organization={PMLR}
}

@article{defazio2023learning,
  title={Learning-Rate-Free Learning by D-Adaptation},
  author={Defazio, Aaron and Mishchenko, Konstantin},
  journal={arXiv preprint arXiv:2301.07733},
  year={2023}
}