
\section{Preliminaries\label{sec:Preliminaries}}

\textbf{Notations}: Let $\left[d\right]$ denote the set $\left\{ 1,2,\cdots,d\right\} $
for any integer $d\geq1$. $\langle\cdot,\cdot\rangle$ is the standard
Euclidean inner product on $\R^{d}$ and $\|\cdot\|$ represents the
$\ell_{2}$ norm. $\mathrm{int}(A)$ stands for the interior points
of any set $A\subseteq\R^{d}$. Given a closed and convex set $C$,
$\Pi_{C}(\cdot)$ is the projection operator onto $C$, i.e., $\Pi_{C}(x)=\argmin_{z\in C}\|z-x\|$.
$a\lor b$ and $a\land b$ are defined as $\max\left\{ a,b\right\} $
and $\min\left\{ a,b\right\} $ respectively. Given a function $f$,
$\partial f(x)$ denotes the set of subgradients at $x$.

We focus on the following optimization problem in this work:

\textbf{
\[
\min_{x\in\dom}F(x)
\]
}where $F$ is convex and $\dom\subseteq\mathrm{int}(\mathrm{dom}(F))\subseteq\R^{d}$
is a closed convex set. The requirement of $\dom\subseteq\mathrm{int}(\mathrm{dom}(F))$
is only to guarantee the existence of subgradients for every point
in $\dom$ with no other special reason. We remark that there is no
compactness assumption on $\dom$. Additionally, our analysis relies
on the following assumptions

\textbf{1. Existence of a local minimizer}: $\exists x_{*}\in\arg\min_{x\in\dom}F(x)$
satisfying $F(x_{*})>-\infty$.

\textbf{2. }$\mu$\textbf{-strongly convex}: $\exists\mu\geq0$ such
that $F(x)\geq F(y)+\langle g,x-y\rangle+\frac{\mu}{2}\|x-y\|^{2},\forall x,y\in\dom,g\in\pa F(y)$.

\textbf{3. }$G$\textbf{-Lipschitz}: $\exists G>0$ such that $\|g\|\leq G,\forall x\in\dom,g\in\pa F(x)$.

\textbf{4. Unbiased gradient estimator}: We can access a history-independent,
unbiased gradient estimator $\hp F(x)$ for any $x\in\dom$, i.e.,
$\E[\widehat{\pa}F(x)\vert x]\in\pa F(x),\forall x\in\dom$.

\textbf{5. Bounded $p$-th moment noise}: There exist $p\in(1,2]$
and $\sigma\geq0$ denoting the noise level such that $\E[\|\hp F(x)-\E[\widehat{\pa}F(x)\vert x]\|^{p}\vert x]\leq\sigma^{p}$.

We briefly discuss the assumptions here. Assumptions 1-3 are standard
in the nonsmooth convex optimization literature. For Assumption 2,
the objective will degenerate to the convex function when $\mu=0$.
Assumption 4 is commonly used in stochastic optimization. Assumption
5 is the definition of heavy-tailed noise. Lastly, we would like to
mention that the reason for using the $\ell_{2}$ norm is only for
convenience. When considering a general norm, similar results to our
theorems (except Theorem \ref{thm:lip-dog-prob}) still hold after
changing the algorithmic framework into MD. A more detailed discussion
will be given in Section \ref{sec:general-norm} in the appendix.
