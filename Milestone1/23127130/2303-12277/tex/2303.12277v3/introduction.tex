
\section{Introduction}

In this paper, we consider the constrained optimization problem
$\min_{\dom}F(x)$ where $F(x)$ is convex and Lipschitz and $\dom\subseteq\R^{d}$
is a closed convex set and possible to be $\R^{d}$. With a stochastic
oracle $\hp F(x)$ satisfying $\E[\hp F(x)\vert x]\in\pa F(x)$ where
$\pa F(x)$ denotes the set of subgradients at $x$, the classic algorithm,
stochastic gradient descent (SGD) \cite{robbins1951stochastic}, guarantees
a convergence rate of $O(T^{-\frac{1}{2}})$ in expectation after
$T$ iterations running under the finite variance condition for the
noise, i.e., $\E[\|\hp F(x)-\E[\hp F(x)\vert x]\|^{2}\vert x]\leq\sigma^{2}$
for some $\sigma\geq0$ representing the noise level. Due to the easy
implementation and empirical success of SGD, it has become one of
the most standard and popular algorithms for optimization problems
nowadays.

However, a huge part of studies, e.g., \cite{mirek2011heavy,simsekli2019tail,zhang2020adaptive,zhou2020towards,gurbuzbalaban2021fractional,camuto2021asymmetric,hodgkinson2021multiplicative},
points out that the finite variance assumption may be too optimistic
and is indeed violated in different machine learning tasks from empirical
observations. Instead, it is more proper to assume the bounded $p$-th
moment noise. Specifically, the noise is considered to satisfy $\E[\|\hp F(x)-\E[\hp F(x)\vert x]\|^{p}\vert x]\leq\sigma^{p}$
for some $p\in(1,2]$ and $\sigma\geq0$, which is known as heavy-tailed.
In particular, if $p=2$, this is exactly the finite variance assumption.
In contrast, the case of $p\in(1,2)$ is much more complicated as
the existing theory for vanilla SGD becomes invalid and the SGD algorithm
itself may fail to converge.

Following this new challenging and more realistic assumption, several
works propose different algorithms to overcome this problem. When
specialized to our case, i.e., a convex and Lipschitz objective, \cite{vural2022mirror}
is the first and the only one to provide an algorithm based on mirror
descent (MD) \cite{nemirovskij1983problem} achieving the in-expectation
rate of $O(T^{\frac{1-p}{p}})$, which matches the lower bound $\Omega(T^{\frac{1-p}{p}})$
\cite{nemirovskij1983problem,raginsky2009information,vural2022mirror}.
In addition, if strong convexity is assumed, \cite{zhang2020adaptive}
gives the first clipping-based algorithm to achieve the rate of $O(T^{\frac{2(1-p)}{p}})$
in expectation, in which a biased estimator $g=\min\left\{ 1,M/\|\hp F(x)\|_{2}\right\} \hp F(x)$
is used to deal with the heavy-tailed issue where $M>0$ denotes the
clipping magnitude. Besides, in the same work, they also establish
the first lower bound of $\Omega(T^{\frac{2(1-p)}{p}})$ to show their
algorithm is optimal. However, even though the two bounds in \cite{zhang2020adaptive,vural2022mirror}
are both optimal with respect to $T$, both of them are not adaptive
to $\sigma$. To be more precise, when $\sigma=0$, they can not recover
the optimal deterministic rates of $O(T^{-\frac{1}{2}})$ and $O(T^{-1})$
for convex and strongly convex cases respectively.

Despite two time-optimal in-expectation bounds have been proved for
our problem, another crucial part, the high-probability convergence
guarantee, still lacks, which turns out to be more helpful in describing
the convergence behavior for an individual running. Notably, if we
instead consider a smooth optimization problem (i.e., $F(x)$ is differentiable
and the gradient of $F(x)$ is Lipschitz), \cite{cutkosky2021high,sadiev2023high,nguyen2023high,liu2023breaking}
make different progress in both convex and nonconvex optimization.
Naturally, one may wonder whether the high-probability bounds can
also be proved in the Lipschitz case. Motivated by this important
gap, we give an affirmative answer to this question in this work by
considering the same clipping algorithm in \cite{zhang2020adaptive}
and show (nearly) optimal high-probability convergence rates for both
convex and strongly convex objectives. Remarkably, under the same
settings of parameters used for the high-probability bounds, we also
prove refined in-expectation rates that are adaptive to $\sigma$.
Hence, we give an exhaustive analysis for stochastic nonsmooth convex
optimization with heavy-tailed noises.

\subsection{Our Contributions}

We use a simple clipping algorithm to handle noises with only bounded
$p$-th moment for $p\in(1,2]$ and establish several new results
for different cases.
\begin{itemize}
\item When the function is assumed to be $G$-Lipschitz and convex
\begin{itemize}
\item We provide the first high-probability bound for the stochastic nonsmooth
convex optimization with heavy-tailed noises. Notably, our choices
of the clipping magnitude $M_{t}$ and step size $\eta_{t}$ are very
flexible. The corresponding rate $\widetilde{O}(T^{\frac{1-p}{p}})$
always matches the lower bound $\Omega(T^{\frac{1-p}{p}})$ only up
to logarithmic factors whenever the time horizon $T$ and the noise
level $\sigma$ is known or not. In other words, we give an any-time
bound that is parameter-free with respect to $\sigma$ at the same
time. Moreover, the bound will be adaptive to $\sigma$ when $\sigma$
is assumed to be known.\\
Besides, the dependence on the failure probability $\delta$ is $O(\log(1/\delta))$
in all the above cases. Our high-probability analysis is done in a
direct style in contrast to the induction-based proof in prior works,
which always leads to a sub-optimal dependence $O(\log(T/\delta))$.
It is also worth emphasizing that our proof neither makes any compact
assumption on the constrained set $\dom$ nor requires the knowledge
of the distance between $x_{1}$ and $x_{*}$ where $x_{1}$ and $x_{*}$
are the initial point and a local minimizer in $\dom$ respectively.
\item Under the same settings of $M_{t}$ and $\eta_{t}$ used in the proof
of high-probability bounds, we also show a nearly optimal in-expectation
convergence rate $\widetilde{O}(T^{\frac{1-p}{p}})$. To our best
knowledge, this is the first in-expectation bound for a clipping algorithm
under this problem. Especially, when $T$ is assumed to be known,
the extra logarithmic factors in $\widetilde{O}(T^{\frac{1-p}{p}})$
can be removed, which leads to the best possible rate of $O(T^{\frac{1-p}{p}})$.
Moreover, if both $\sigma$ and $T$ can be used to set $M_{t}$ and
$\eta_{t}$, the rate at this time will be $O((G+\sigma)T^{-\frac{1}{2}}+\sigma T^{\frac{1-p}{p}})$,
which is adaptive to $\sigma.$
\item When $\sigma$ and $\delta$ are assumed to be known in advance, we
also prove an initial distance adaptive bound under more careful choices
of parameters. More precisely, the dependence on the initial distance
is only in the order of $O(\log\frac{r+\|x_{1}-x_{*}\|}{r}(r+\|x_{1}-x_{*}\|))$
where $r>0$ can be any number rather than the traditional quadratic
bound $O(\alpha+\|x_{1}-x_{*}\|^{2}/\alpha)$ where $\alpha>0$ can
be viewed as the learning rate needed to be tuned. Moreover, the convergence
rate is still optimal in $T$ up to logarithmic factors, adaptive
to $\sigma$ and only has $O(\log(1/\delta))$ dependence on $\delta$.
\end{itemize}
\item When the objective is assumed to be $G$-Lipschitz and strongly convex
\begin{itemize}
\item We give the first high-probability convergence analysis and show an
optimal convergence rate $O(\log^{2}(1/\delta)((G^{2}+\sigma^{2})T^{-1}+(M^{2}+\sigma^{2p}M^{2-2p}+\sigma^{p}G^{2-p})T^{\frac{2(1-p)}{p}}))$
with probability at least $1-\delta$ where $M>0$ can be any real
number used to decide the clipping magnitude. The same as the convex
case, the choices of $M_{t}$ and $\eta_{t}$ don't require any prior
knowledge of the time horizon $T$ and the noise level $\sigma$,
either. Especially, when $\sigma$ is known, setting $M=\sigma$ leads
to the noise adaptive rate $O(\log^{2}(1/\delta)(G^{2}T^{-1}+(\sigma^{2}+\sigma^{p}G^{2-p})T^{\frac{2(1-p)}{p}}))$.
\item With the same $M_{t}$ and $\eta_{t}$ used for high-probability bounds,
our algorithm also guarantees a rate of $O((G^{2}+\sigma^{2})T^{-1}+(\sigma^{p}M^{2-p}+\sigma^{2p}M^{2-2p})T^{\frac{2(1-p)}{p}})$
in expectation where $M$ can be any positive real number. It is worth
mentioning that, unlike the convex case, such an $\sigma$-adaptive
rate doesn't need $\sigma$ to set $M_{t}$ and $\eta_{t}$.
\end{itemize}
\end{itemize}

\subsection{Related Work}

We review the literature related to nonsmooth convex optimization
with heavy-tailed noises. As for the heavy-tailed smooth problems
(either convex or non-convex), the reader can refer to \cite{csimcsekli2019heavy,cutkosky2021high,wang2021convergence,jakovetic2022nonlinear,sadiev2023high,nguyen2023high,liu2023breaking}
for recent progress.

\textbf{High-probability convergence with heavy-tailed noises:} As
far as we know, there doesn't exist any prior work establishing the
high-probability convergence rate when considering nonsmooth convex
(or strongly convex) optimization with heavy-tailed noises. However,
a previous paper \cite{zhang2022parameter} is very close to our problem,
in which the authors focus on online nonsmooth convex optimization
and present an algorithm with a provable high-probability convergence
bound to address heavy-tailed noises. One can employ their algorithm
to solve our problem as online convex optimization is more general.
But there are still lots of differences between our work and \cite{zhang2022parameter}.
A comprehensive comparison can be found in Section \ref{sec: algo}.

If only considering the finite variance case (i.e., $p=2$) with convex
objectives, \cite{parletta2022high} provides a high-probability bound
$O(\log(1/\delta)T^{-\frac{1}{2}})$ where $\delta$ is the failure
probability. But they require a bounded domain $\dom$ in the proof,
which is a restrictive assumption and significantly simplifies the
analysis. \cite{gorbunov2021near} is the first to show a high-probability
rate when the domain of the problem is $\R^{d}$. Whereas to set up
the parameters in their algorithm, the initial distance $\|x_{1}-x_{*}\|_{2}$
(or any upper bound on it) needs to be known, which is also hard to
estimate when $\dom$ is unbounded. Besides, the dependence on $\delta$
in \cite{gorbunov2021near} is sub-optimal $\log(T/\delta)$.

\textbf{In-expectation convergence with heavy-tailed noises: }For
the Lipschitz convex problem, \cite{vural2022mirror} is the first
and the only work to show an $O(T^{\frac{1-p}{p}})$ rate under the
framework of MD. However, unlike the popular clipping method, their
algorithm is based on the uniform convexity property (see Definition
1 in \cite{vural2022mirror}) of the mirror map. Therefore, an in-expectation
bound for the clipping-based algorithm still lacks in this case. When
the functions are additionally considered to be strongly convex, \cite{zhang2020adaptive}
is the first and the only work to prove an $O(T^{\frac{2(1-p)}{p}})$
convergence rate in expectation by combining SGD and clipped gradients.

However, we would like to mention that both \cite{vural2022mirror}
and \cite{zhang2020adaptive} only assume that $\E[\|\widehat{\pa}F(x)\|^{p}\vert x]$
is uniformly bounded by a constant $C^{p}$ for some $C>0$ unlike
our assumption of $\E[\|\hp F(x)-\E[\widehat{\pa}F(x)\vert x]\|^{p}\vert x]\leq\sigma^{p}$.
This difference can let us obtain a refined in-expectation bound,
which is able to be adaptive to $\sigma$. In other words, when $\sigma=0$,
our in-expectation bounds will automatically recover the well-known
optimal rates of $O(T^{-\frac{1}{2}})$ and $O(T^{-1})$ for convex
and strongly convex cases respectively. We refer the reader to Section
\ref{sec: algo} for more detailed comparisons with these two previous
works.

\textbf{Lower bound with heavy-tailed noises: }When the noises only
have $p$-th finite moment for some $p\in(1,2]$, for the convex functions,
\cite{nemirovskij1983problem,raginsky2009information,vural2022mirror}
show that the convergence rate of any first-order algorithm cannot
be faster than $\Omega(T^{\frac{1-p}{p}})$. If the strong convexity
is additionally assumed, \cite{zhang2020adaptive} is the first to
establish a lower bound of $\Omega(T^{\frac{2(1-p)}{p}})$.
