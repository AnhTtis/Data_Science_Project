
\section{Algorithms and Results\label{sec: algo}}

\begin{algorithm}[h]
\caption{\label{alg:algo}Projected SGD with Clipping}

\textbf{Input}: $x_{1}\in\dom$, $M_{t}>0$, $\eta_{t}>0$.

\textbf{for} $t=1$ \textbf{to} $T$ \textbf{do}

$\quad$$g_{t}=\left(1\land\frac{M_{t}}{\left\Vert \hp F(x_{t})\right\Vert }\right)\hp F(x_{t})$

$\quad$$x_{t+1}=\Pi_{\dom}(x_{t}-\eta_{t}g_{t}).$

\textbf{end for}
\end{algorithm}

The projected clipped SGD algorithm is shown in Algorithm \ref{alg:algo}.
The algorithm itself is simple to understand. Compared with SGD, the
only difference is to clip the stochastic gradient $\hp F(x_{t})$
with a threshold $M_{t}$. In the next two sections, we will show
that properly picked $M_{t}$ and $\eta_{t}$ guarantee both high-probability
and in-expectation convergence for Algorithm \ref{alg:algo}.

\subsection{Convergence Theorems When $\mu=0$}

In this section, we present the convergence theorems of Algorithm
\ref{alg:algo} for convex functions, i.e., $\mu=0$. 

First, when $T$ is not assumed to be known, Theorem \ref{thm:lip-prob}
gives any-time high-probability convergence bounds for two cases,
i.e., whether the noise level $\sigma$ is known or not. As far as
we know, Theorem \ref{thm:lip-prob} is the first to describe an any-time
high-probability convergence rate for nonsmooth convex optimization
problems when the noise is assumed to be heavy-tailed.
\begin{thm}
\label{thm:lip-prob}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$.
\begin{itemize}
\item If $\sigma$ is known, under the choices of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
where $\alpha>0$ can be any real number, for any $T\geq1$ and $\delta\in(0,1)$,
the following bound holds with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\left(\alpha\left(\log^{2}T+\log^{2}\left(\frac{\log T}{\delta}\right)\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G+\sigma}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\item If $\sigma$ is unknown, under the choices of $M_{t}=2Gt^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$ where $\alpha>0$
can be any real number, for any $T\geq1$ and $\delta\in(0,1)$, the
following bound holds with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\left(\alpha\left(\log T+(\sigma/G)^{2p}\log^{2}T+\log^{2}\left(\frac{\log T}{\delta}\right)\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\frac{G}{T^{\frac{p-1}{p}}}\right).
\]
\end{itemize}
\end{thm}
%
\begin{rem}
It is possible to choose $\eta_{t}=\frac{\alpha_{1}}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha_{2}}{\sigma t^{\frac{1}{p}}}$
for different $\alpha_{1},\alpha_{2}>0$ when $\sigma$ is known.
However, we keep the same $\alpha$ for simplicity in Theorem \ref{thm:lip-prob}
and the following Theorems \ref{thm:lip-prob-fix}, \ref{thm:lip-exp}
and \ref{thm:lip-exp-fix}.
\end{rem}
%
We note that whenever $\sigma$ is known or not, both choices will
lead to the (nearly) optimal rate $\widetilde{O}(T^{\frac{1-p}{p}})$
in $T$. When $\sigma=0$, in other words, the deterministic case,
the choices of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
will be $M_{t}=2G$ and $\eta_{t}=\frac{\alpha}{G\sqrt{t}}$. Recall
that the norm of any subgradient is bounded by $G$, which implies
$M_{t}=2G$ won't have any effect now. Hence, the algorithm will be
the totally same as the traditional Projected SGD. The corresponding
bound will also be the (nearly) optimal rate $\widetilde{O}(G/\sqrt{T})$. 

Next, in Theorem \ref{thm:lip-prob-fix}, we state the fixed time
bound, i.e., the case of known $T$. Besides changing $t$ in $M_{t}$
and $\eta_{t}$ to $T$, a time-dependent choice of $\alpha$ is provided.
We would like to point out that Theorem \ref{thm:lip-prob-fix} is
not a simple extension of Theorem \ref{thm:lip-prob}. For example,
when considering the choices of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$,
following the same steps of proving Theorem \ref{thm:lip-prob}, one
can only obtain the rate of $O((\alpha\log^{2}(\log(T)/\delta)+\|x_{1}-x_{*}\|^{2}/\alpha))$
(ignoring the other dependence on $T$ here since it will keep the
same), which implies the optimized $\alpha$ should be $\Theta(\beta/\log(\log(T)/\delta))$
and will only give an $O(\log(\log(T)/\delta))$ dependence on $\delta$.
However, we further improve the dependence to $O(\sqrt{\log(\log(T)/\delta)\log(1/\delta)})$
as shown in Theorem \ref{thm:lip-prob-fix}.
\begin{thm}
\label{thm:lip-prob-fix}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Additionally,
assume $T$ is known.
\begin{itemize}
\item If $\sigma$ is known, under the choices of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$
where $\alpha=\frac{\beta}{\sqrt{\log\left(8\log_{2}(4T)/\delta\right)\log(8/\delta)}}$
and $\beta>0$ can be any real number, for any $T\geq1$ and $\delta\in(0,1)$,
the following bound holds with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\sqrt{\log\left(\frac{\log T}{\delta}\right)\log\frac{1}{\delta}}\left(\beta+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\beta}\right)\left(\frac{G+\sigma}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\item If $\sigma$ is unknown, under the choices of $M_{t}=2GT^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$ where $\alpha=\frac{\beta}{\sqrt{\log\left(8\log_{2}(4T)/\delta\right)\log(8/\delta)}}$
and $\beta>0$ can be any real number, for any $T\geq1$ and $\delta\in(0,1)$,
the following bound holds with probability at least $1-\delta$,
\[
F(\bar{x}_{T})-F(x_{*})\leq O\left(\sqrt{\log\left(\frac{\log T}{\delta}\right)\log\frac{1}{\delta}}\left(\beta\left(1+(\sigma/G)^{2p}\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\beta}\right)\frac{G}{T^{\frac{p-1}{p}}}\right).
\]
\end{itemize}
\end{thm}
%
To finish the high-probability bounds, we would like to make a comprehensive
comparison with \cite{zhang2022parameter}, which is the only existing
work showing a high-probability bound of $\widetilde{O}(\epsilon\log(1/\delta)T^{-1}+(\sigma+G)\log(T/\delta)\log(\|x_{1}-x_{*}\|T/\epsilon)\|x_{1}-x_{*}\|T^{\frac{1-p}{p}})$
(where $\epsilon>0$ is any user-specified parameter) in in the related
literature. We need to emphasize our work is different in several
aspects.
\begin{enumerate}
\item The algorithm in \cite{zhang2022parameter} is much more complicated
than ours. To be more precise, their main algorithm needs to call
several outer algorithms. The outer algorithms themselves are even
very involved. This difference is because we only focus on convex
optimization, in contrast, their algorithm is designed for online
convex optimization, which is known to be more general. Hence, when
only considering solving the heavy-tailed nonsmooth convex optimization,
we believe our algorithm is much easier to be implemented.
\item When choosing $M_{t}$ and $\eta_{t}$, \cite{zhang2022parameter}
requires not only the time horizon $T$ but also the noise $\sigma$,
which means their result is neither an any-time bound nor parameter-free
with respect to $\sigma$. In comparison, our Theorem \ref{thm:lip-prob}
doesn't require $T$. We also show how to set $M_{t}$ and $\eta_{t}$
when $\sigma$ is unknown. Besides, If $T$ is assumed to be known,
the dependence on $\delta$ in our Theorem \ref{thm:lip-prob-fix}
is only $O(\sqrt{\log(\log T/\delta)\log(1/\delta)})$, which is significantly
better than $O(\log(T/\delta))$ in \cite{zhang2022parameter}.
\item However, our result is not as good as \cite{zhang2022parameter} for
the dependence on the initial distance $\|x_{1}-x_{*}\|$. As one
can see, our obtained bound is always in the form of $O(\alpha+\|x_{1}-x_{*}\|^{2}/\alpha)$
($\alpha$ can be replaced by $\beta$ for Theorem \ref{thm:lip-prob-fix}),
which is worse than $\|x_{1}-x_{*}\|\log(\|x_{1}-x_{*}\|)$ in \cite{zhang2022parameter}.
\item Finally, it is worth pointing out that our proof techniques are very
different from \cite{zhang2022parameter}. Our analysis is done in
a direct way compared to the reduction-based manner in \cite{zhang2022parameter}.
\end{enumerate}
Now, we turn to provide the first (nearly) optimal in-expectation
convergence rate of clipping algorithms in Theorems \ref{thm:lip-exp}
(any-time bound) and \ref{thm:lip-exp-fix} (fixed time bound), which
correspond to the cases of unknown $T$ and known $T$ respectively.
\begin{thm}
\label{thm:lip-exp}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$.
\begin{itemize}
\item If $\sigma$ is known, under the choices of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
where $\alpha>0$ can be any real number, for any $T\geq1$, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha\log^{2}T+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G+\sigma}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\item If $\sigma$ is unknown, under the choices of $M_{t}=2Gt^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$ where $\alpha>0$
can be any real number, for any $T\geq1$, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha\left(\log T+(\sigma/G)^{2p}\log^{2}T\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\frac{G}{T^{\frac{p-1}{p}}}\right).
\]
\end{itemize}
\end{thm}
%
\begin{thm}
\label{thm:lip-exp-fix}Suppose Assumptions (1)-(5) hold with $\mu=0$
and let $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Additionally,
assume $T$ is known.
\begin{itemize}
\item If $\sigma$ is known, under the choices of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$
where $\alpha>0$ can be any real number, for any $T\geq1$, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\left(\frac{G+\sigma}{\sqrt{T}}\lor\frac{\sigma}{T^{\frac{p-1}{p}}}\right)\right).
\]
\item If $\sigma$ is unknown, under the choices of $M_{t}=2GT^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$ where $\alpha>0$
can be any real number, for any $T\geq1$, we have
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq O\left(\left(\alpha\left(1+(\sigma/G)^{2p}\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{\alpha}\right)\frac{G}{T^{\frac{p-1}{p}}}\right).
\]
\end{itemize}
\end{thm}
%
We first remark that the choices of $M_{t}$ and $\eta_{t}$ in Theorems
\ref{thm:lip-exp} and \ref{thm:lip-exp-fix} are the same as them
in Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix}. Hence,
our $M_{t}$ and $\eta_{t}$ guarantee both high-probability and in-expectation
convergence. Next, compared with the any-time bounds in Theorem \ref{thm:lip-exp},
the extra logarithmic factors are removed in the fixed time bounds
in Theorem \ref{thm:lip-exp-fix}. Additionally, the rates for the
case of known $\sigma$ are always adaptive to the noise. In particular,
when $T$ is known and $p=2$, our result matches the traditional
bound of SGD perfectly. 

Lastly, let us talk about the differences with the prior work \cite{vural2022mirror}
providing the only in-expectation bound but for a different algorithm.
\begin{enumerate}
\item The algorithm in \cite{vural2022mirror} is based on MD, but more
importantly, requires the property of uniform convexity (see Definition
1 in \cite{vural2022mirror}) for the mirror map. However, our in-expectation
bounds are for the algorithm employing the clipping method, which
is widely used to deal with heavy-tailed problems in several areas
but lacks theoretical justifications in nonsmooth convex optimization. 
\item \cite{vural2022mirror} only assumes $\E[\|\widehat{\pa}F(x)\|^{p}\vert x]\leq\sigma^{p}$
for some $p\in(1,2]$ and $\sigma>0$ (strictly speaking, the norm
in \cite{vural2022mirror} is $\ell_{q}$ norm for some $q\in[1,\infty]$,
however, our method can be extended to an arbitrary norm including
$\ell_{q}$ norm as a subcase). This assumption is equivalent to
$\E[\|\widehat{\pa}F(x)\|^{p}\vert x]\leq O((\sigma+G)^{p})$ under
our assumptions, which means \cite{vural2022mirror} needs both $G$
and $\sigma$ as input but their final rate doesn't adapt to $\sigma$.
In contrast, we not only give a rate being adaptive to the noise when
$\sigma$ is known but also show how to run our algorithm without
any prior knowledge of $\sigma$.
\item Besides, our parameter settings not only guarantee in-expectation
convergence but also admit provable high-probability bounds as shown
in Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix}. But \cite{vural2022mirror}
only provides the in-expectation result for their algorithm.
\item Finally, our proof strategy is completely different from \cite{vural2022mirror}
as there is no clipping step. We believe that our techniques in the
proof will lead to a better understanding of the clipping method. 
\end{enumerate}

\subsection{Convergence Theorems When $\mu>0$}

In this section, we focus on establishing the convergence rate of
Algorithm \ref{alg:algo} for strongly convex objectives, i.e., $\mu>0$.
In this case, even when $T$ is assumed to be known, we no longer
consider using $T$ to set $M_{t}$ and $\eta_{t}$ since a step size
$\eta_{t}$ depending on $T$ is rarely used under strong convexity
assumption.

The first result, Theorem \ref{thm:str-prob}, describes the high-probability
behavior of Algorithm \ref{alg:algo}. To our best knowledge, this
is the first high-probability bound for nonsmooth strongly convex
optimization with heavy-tailed noises matching the in-expectation
lower bound of $\Omega(T^{\frac{2(1-p)}{p}})$ up to logarithmic factors.
\begin{thm}
\label{thm:str-prob}Suppose Assumptions (1)-(5) hold with $\mu>0$
let $\bar{x}_{T}=\frac{2}{T(T+1)}\sum_{t=1}^{T}tx_{t}$. Under the
choices of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{4}{\mu(t+1)}$,
for any $T\geq1$ and $\delta\in(0,1)$, the following two bounds
hold simultaneously with probability at least $1-\delta$,
\begin{align*}
F(\bar{x}_{T})-F(x_{*}) & \leq O\left(\frac{G^{2}+\sigma^{2}+\sigma^{2p}G^{2-2p}}{\mu}\cdot\frac{\log^{2}\frac{T}{\delta}}{T^{\frac{2(p-1)}{p}}}\right);\\
\left\Vert x_{T+1}-x_{*}\right\Vert ^{2} & \leq O\left(\frac{G^{2}+\sigma^{2}+\sigma^{2p}G^{2-2p}}{\mu^{2}}\cdot\frac{\log^{2}\frac{T}{\delta}}{T^{\frac{2(p-1)}{p}}}\right).
\end{align*}
\end{thm}
%
Our choices of $M_{t}$ and $\eta_{t}$ are inspired by \cite{zhang2020adaptive}
but $M_{t}$ is very different at the same time. We need to emphasize
that the parameter $G$ in $M_{t}=Gt^{\frac{1}{p}}$ used in \cite{zhang2020adaptive}
is not the same as our definition of $G$ since \cite{zhang2020adaptive}
only assumes $\E[\|\hp F(x)\|^{p}\vert x]\leq G^{p}$, $\forall x\in\dom$
rather than separates the assumption on noises independently. Under
our assumptions, there is only $\E[\|\hp F(x)\|^{p}\vert x]\leq O((\sigma+G)^{p})$,
which implies $M_{t}=Gt^{\frac{1}{p}}$ in \cite{zhang2020adaptive}
is equivalent to $M_{t}=\Theta((\sigma+G)t^{\frac{1}{p}})$. But this
diverges from our choice of $M_{t}=2Gt^{\frac{1}{p}}$.

As one can see, $M_{t}$ in our settings doesn't rely on $\sigma$,
this property makes our choices more practical. Also, we believe that,
under more careful calculations, the bound can be improved to $\widetilde{O}(G^{2}/T+\sigma^{p}G^{2-p}\mathrm{poly}((\sigma/G)^{p})/T^{\frac{2(p-1)}{p}})$
(the dependence on $\mu$ is still $1/\mu$ for the function value
gap and $1/\mu^{2}$ for the distance $\|x_{T+1}-x_{*}\|^{2}$). The
reason is that our choices of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{4}{\mu(t+1)}$
will lead to a rate of $O(G^{2}/T)$ when $\sigma=0$ which means
the final bound should be adaptive to $\sigma$.

Finally, though an in-expectation bound of clipping algorithms for
the strongly convex case has been established in \cite{zhang2020adaptive},
we provide a refined rate in Theorem \ref{thm:str-exp}.
\begin{thm}
\label{thm:str-exp}Suppose Assumptions (1)-(5) hold with $\mu>0$
let $\bar{x}_{T}=\frac{2}{T(T+1)}\sum_{t=1}^{T}tx_{t}$. Under the
choices of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{4}{\mu(t+1)}$,
for any $T\geq1$, we have
\begin{align*}
\E\left[F(\bar{x}_{T})-F(x_{*})\right] & \leq O\left(\frac{G^{2}}{\mu T}+\frac{\sigma^{p}G^{2-p}+\sigma^{2p}G^{2-2p}}{\mu T^{\frac{2(p-1)}{p}}}\right);\\
\E\left[\left\Vert x_{T+1}-x_{*}\right\Vert ^{2}\right] & \leq O\left(\frac{G^{2}}{\mu^{2}T}+\frac{\sigma^{p}G^{2-p}+\sigma^{2p}G^{2-2p}}{\mu^{2}T^{\frac{2(p-1)}{p}}}\right).
\end{align*}
\end{thm}
%
We briefly discuss Theorem \ref{thm:str-exp} here before finishing
this section. First, we remark that the clipping magnitude $M_{t}$
and step size $\eta_{t}$ are the same as Theorem \ref{thm:str-prob}
without any extra modifications. Additionally, these two in-expectation
bounds are both optimal as they attain the best possible in-expectation
rate $\Omega(T^{\frac{2(1-p)}{p}})$. It is also worth pointing out
that our results have a more explicit dependence on the noise level
$\sigma$ and the Lipschitz constant $G$ compared with \cite{zhang2020adaptive}
as in which there is no explicit assumption on noises as mentioned
before. Notably, the two rates are both adaptive to the noise $\sigma$
while not requiring any prior knowledge on $\sigma$ to set $M_{t}$
and $\eta_{t}$. In other words, when $\sigma=0$, we obtain the optimal
rate $O(T^{-1})$ automatically even not knowing $\sigma$.

\section{Theoretical Analysis\label{sec: analysis}}

We show the ideas for proving our theorems in this section. In Section
\ref{subsec:Three-lemmas}, three of the most fundamental lemmas in
the proof are presented. In Sections \ref{subsec:lip-prob} and \ref{subsec:lip-exp},
we focus on the high-probability rate and in-expectation bound respectively
for the case $\mu=0$. Several lemmas used to prove these two results
will be given, the omitted proofs of which are delivered in Section
\ref{sec:app-missing-proofs}. To the end, the proofs of Theorems
\ref{thm:lip-prob}, \ref{thm:lip-prob-fix} and \ref{thm:lip-exp},
\ref{thm:lip-exp-fix} are provided. The analysis for the strongly
convex case (i.e., $\mu>0$) is deferred into Section \ref{sec:app-str}
in the appendix.

Before going through the proof, we introduce some notations used in
the analysis. Let $\F_{t}=\sigma(\hp F(x_{1}),\cdots,\hp F(x_{t}))$
be the natural filtration. Under this definition, $x_{t}$ is $\F_{t-1}$
measurable. $\E_{t}[\cdot]$ denotes $\E[\cdot\mid\F_{t-1}]$ for
brevity. We also employ the following definitions:
\begin{align*}
\Delta_{t}\coloneqq & F(x_{t})-F(x_{*});\quad\pa_{t}\coloneqq\E_{t}\left[\hp F(x_{t})\right]\in\pa F(x_{t});\\
\xi_{t}\coloneqq & g_{t}-\pa_{t};\quad\xi_{t}^{u}\coloneqq g_{t}-\E_{t}\left[g_{t}\right];\quad\xi_{t}^{b}\coloneqq\E_{t}\left[g_{t}\right]-\pa_{t};\\
d_{t}\coloneqq & \left\Vert x_{t}-x_{*}\right\Vert ;\quad D_{t}\coloneqq\max_{s\in\left[t\right]}d_{s};\quad\mathfrak{D}_{t}\coloneqq D_{t}\lor2\alpha;
\end{align*}
where $\alpha>0$ is the parameter used to set the step size $\eta_{t}$.
We remark that $\xi_{t},\xi_{t}^{u}\in\F_{t}$ and $\pa_{t},\xi_{t}^{b},d_{t},D_{t},\mathfrak{D}_{t}\in\F_{t-1}$.

\subsection{Three Fundamental Lemmas\label{subsec:Three-lemmas}}

To start with the analysis, we first introduce Lemmas \ref{lem:err-bound},
\ref{lem:basic} and \ref{lem:normalize}, which serve as foundations
in our proof. 
\begin{lem}
\label{lem:err-bound}For any $t\in\left[T\right]$, if $M_{t}\geq2G$,
we have
\begin{align*}
\|\xi_{t}^{u}\| & \le2M_{t};\quad\E_{t}[\|\xi_{t}^{u}\|^{2}]\le10\sigma^{p}M_{t}^{2-p};\\
\|\xi_{t}^{b}\| & \le2\sigma^{p}M_{t}^{1-p};\quad\|\xi_{t}^{b}\|^{2}\leq10\sigma^{p}M_{t}^{2-p}.
\end{align*}
\end{lem}
%
Several similar results (except the bound on $\|\xi_{t}^{b}\|^{2}$)
to Lemma \ref{lem:err-bound} appear in \cite{zhang2020adaptive,gorbunov2020stochastic,gorbunov2021near,zhang2022parameter,sadiev2023high,nguyen2023high,liu2023breaking}
before. However, every existing analysis only considers $\ell_{2}$
norm. When a general norm is used, we provide an extended version
of Lemma \ref{lem:err-bound}, Lemma \ref{lem:err-bound-general}
in Section \ref{sec:general-norm} in the appendix along with the
proof. We refer the interested reader to Section \ref{sec:general-norm}
for details.

From a high-level overview, Lemma \ref{lem:err-bound} tells us how
small the errors $\xi_{t}^{u}$ and $\xi_{t}^{b}$ can be when $M_{t}\geq2G$.
Note that the part of $\xi_{t}^{u}$ will get larger as $M_{t}$ becomes
bigger, in contrast, the error $\xi_{t}^{b}$ (consider the bound
of $\|\xi_{t}^{b}\|\leq2\sigma^{p}M_{t}^{1-p}$) will decrease since
$1-p<0$ given $p\in(1,2]$. So $M_{t}$ should be chosen appropriately
to balance the order between $\|\xi_{t}^{u}\|$ and $\|\xi_{t}^{b}\|$.
Besides, note that our choice of $M_{t}$ always satisfies the condition
$M_{t}\geq2G$. Hereinafter, we will apply Lemma \ref{lem:err-bound}
directly in the analysis.
\begin{lem}
\label{lem:basic}For any $t\in\left[T\right]$, we have
\[
\Delta_{t}+\frac{\eta_{t}^{-1}}{2}\left\Vert x_{t+1}-x_{*}\right\Vert ^{2}-\frac{\eta_{t}^{-1}-\mu}{2}\left\Vert x_{t}-x_{*}\right\Vert ^{2}\leq\langle\xi_{t},x_{*}-x_{t}\rangle+\eta_{t}\left(2\left\Vert \xi_{t}^{u}\right\Vert ^{2}+2\left\Vert \xi_{t}^{b}\right\Vert ^{2}+G^{2}\right).
\]
\end{lem}
%
Lemma \ref{lem:basic} is the basic inequality used to bound both
the function value gap $\Delta_{t}$ and the distance term $\|x_{t}-x_{*}\|^{2}$.
The same as Lemma \ref{lem:err-bound}, we will present a generalized
version (Lemma \ref{lem:basic-general} in Section \ref{sec:general-norm})
for the general norm with its proof in the appendix.

Now let us explain Lemma \ref{lem:basic} a bit more here. If $\xi_{t}=\bzero$,
then one can view Lemma \ref{lem:basic} as a one-step descent lemma.
However, even after taking the expectation on both sides, the term
$\E[\langle\xi_{t},x_{*}-x_{t}\rangle]$ won't vanish since $\xi_{t}=g_{t}-\pa_{t}$
but $g_{t}$ is not an unbiased gradient estimator of $\pa_{t}$ due
to the clipping. So one of the hard parts of the analysis is how to
deal with the term $\langle\xi_{t},x_{*}-x_{t}\rangle$ both in expectation
and in a high-probability way. 

Another challenge is to deal with $\|\xi_{t}^{u}\|^{2}$ and $\|\xi_{t}^{b}\|^{2}$.
For $\|\xi_{t}^{b}\|^{2}$, Lemma \ref{lem:err-bound} already tells
us how to bound it. As for $\|\xi_{t}^{u}\|^{2}$, though it can be
bounded by $4M_{t}^{2}$ (by Lemma \ref{lem:err-bound} again). However,
this simple bound is not enough to obtain the correct order. For the
in-expectation analysis, after taking expectations on both sides of
Lemma \ref{lem:basic}, we can instead use the bound $\E_{t}[\|\xi_{t}^{u}\|^{2}]\leq10\sigma^{p}M_{t}^{2-p}$
in Lemma \ref{lem:err-bound}, which turns to be in a strictly smaller
order compared with $4M_{t}^{2}$ under our choice of $M_{t}$. Hence,
for the more complicated high-probability analysis, a hint of using
the bound on $\E_{t}[\|\xi_{t}^{u}\|^{2}]$ arises from analyzing
the in-expectation bound. As such, a natrual decomposition, $\|\xi_{t}^{u}\|^{2}=\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}]+\E_{t}[\|\xi_{t}^{u}\|^{2}]$,
shows up. We make this thought formally and show a high-probability
bound of $\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}]$ in Lemma
\ref{lem:xi-u-lip} in next section.
\begin{lem}
\label{lem:normalize}When $\mu=0$, under our choices of $M_{t}$
and $\eta_{t}$ whenever $T$ is known or not, for any $t\in\left[T\right]$,
we have
\[
\eta_{t}M_{t}\leq2\alpha\text{ and }d_{t}\leq d_{1}+2\alpha(t-1).
\]
\end{lem}
%
Lemma \ref{lem:normalize} is a powerful tool when specialized to
the case $\mu=0$. Again, this result also holds for the general norm,
Lemma \ref{lem:normalize-general} in Section \ref{sec:general-norm},
the statement and the proof of which can be found in the appendix.

We note that such a property is named \textit{Bounded Step-Size Property
}in \cite{faw2022power}, which is used in proving the convergence
rate of an adaptive algorithm, AdaGrad-Norm \cite{McMahanS10,duchi2011adaptive}.
\cite{carmon2022making} also employs this crucial fact to prove a
high-probability bound of parameter-free AdaGrad-Norm. It is known
that AdaGrad-Norm can be viewed as incorporating the clipping method
implicitly due to its special step size. From such a view, we expect
this property still holds as the clipping method is explicitly used
in Algorithm \ref{alg:algo-md}. Finally, Lemma \ref{lem:normalize}
confirms our guess. This lemma is the key to proving Lemma \ref{lem:lip-prob-loglog}
which is able to give an $O(\log(\log(T/\delta)))$ dependence rather
than $O(\log(T/\delta))$. We refer the reader to the proof of Lemma
\ref{lem:lip-prob-loglog} for details.

\subsection{High-Probability Analysis when $\mu=0$\label{subsec:lip-prob}}

In this section, our ultimate goal is to prove the high-probability
convergence rate, i.e., Theorems \ref{thm:lip-prob} and \ref{thm:lip-prob-fix}.
To save space, only the lemmas used for the case of unknown $T$ will
be stated formally. We will describe how the lemmas will change for
known $T$ accordingly in the remarks.

First, we introduce Lemma \ref{lem:basic-lip-prob}, which can be
viewed as a finer result of Lemma \ref{lem:basic} for $\mu=0$. 
\begin{lem}
\label{lem:basic-lip-prob}When $\mu=0$, for any $\tau\in\left[T\right]$,
we have
\[
\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}-\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}\leq h(\tau)+\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t},x_{*}-x_{t}\rangle+2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)
\]
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$:
\[
h(\tau)=95\alpha^{2}\log(e\tau).
\]
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$:
\[
h(\tau)=80\left(1+(\sigma/G)^{p}\right)\alpha^{2}\log(e\tau).
\]
\end{itemize}
\end{lem}
%
\begin{rem}
\label{rem:basic-lip-prob}For the case of known $T$, under the choices
of $M_{t}$ and $\eta_{t}$ in Theorem \ref{thm:lip-prob-fix} (or
the same, Theorem \ref{thm:lip-exp-fix}), $h(\tau)$ in Lemma \ref{lem:basic-lip-prob}
will be $95\alpha^{2}$ and $80\left(1+(\sigma/G)^{p}\right)\alpha^{2}$
respectively when $\sigma$ is known or not.
\end{rem}
%
As mentioned above, to use the conditional expectation bound on $\|\xi_{t}^{u}\|^{2}$,
the term $\eta_{t}^{2}(\|\xi_{t}^{u}\|^{2}-\E_{t}[\|\xi_{t}^{u}\|^{2}])$
appears, which can be bounded by Freedman's inequality (Lemma \ref{lem:freedman}).
So we only need to deal with the other hard part $\eta_{t}\langle\xi_{t},x_{*}-x_{t}\rangle$,
to use the bound on $\|\xi_{t}^{u}\|$ and $\|\xi_{t}^{b}\|$, it
is natural to consider $\eta_{t}\langle\xi_{t},x_{*}-x_{t}\rangle=\eta_{t}\langle\xi_{t}^{b}+\xi_{t}^{u},x_{*}-x_{t}\rangle$.
For the term $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},x_{*}-x_{t}\rangle$,
it will be bounded by $\widetilde{O}(D_{\tau})$ in the proof of Theorem
\ref{thm:lip-prob}. The other term, $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$,
is more interesting. A key observation is that $\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$
is a martingale difference sequence, which may allow us to use the
concentration inequality to bound the summation again. However, $\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$
doesn't admit an almost surely time uniform bound since the existence
of $x_{*}-x_{t}$, which implies we can not apply Freedman's inequality
directly. Besides, the conditional variance $\E_{t}[\eta_{t}^{2}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle^{2}]$
involves the distance term $\|x_{*}-x_{t}\|^{2}=d_{t}^{2}$ , which
should be somehow deal with.

A key insight here is that we can use $\mathfrak{D}_{\tau}$ to bound
$d_{t}$ when $t\leq\tau$ and expect $\mathfrak{D}_{\tau}$ to have
a bound. The careful reader may think that we are proving a stronger
result because a bound on $\mathfrak{D}_{\tau}$ implies that $d_{t}$
should be uniformly bounded. However, from the traditional in-expectation
analysis when $p=2$, $d_{t}$ indeed obeys s a uniform bound. Hence,
we can hope for the same thing to happen here. It turns out that we
are on the right way. But we can not to simply bound $\eta_{t}\langle\xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{\tau}}\rangle$
since $\eta_{t}\langle\xi_{t}^{u},\frac{x_{*}-x_{t}}{\mathfrak{D}_{\tau}}\rangle\in\F_{\tau}$.
To solve this problem, we employ the strategy used in the proof of
Proposition 4 in \cite{carmon2022making}. Lastly, in Lemma \ref{lem:lip-prob-loglog},
a desired bound is obtained. We refer the reader to Section \ref{sec:app-missing-proofs}
for more details of our proof of Lemma \ref{lem:lip-prob-loglog}.
\begin{lem}
\label{lem:lip-prob-loglog}When $\mu=0$, we have that
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$
\[
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle\leq14\mathfrak{D}_{\tau}\left(\log\frac{4\log_{2}(4T)}{\delta}+\sqrt{\log(eT)\log\frac{4\log_{2}(4T)}{\delta}}\right)\alpha
\]
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$
\[
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle\leq14\mathfrak{D}_{\tau}\left(\log\frac{4\log_{2}(4T)}{\delta}+\sqrt{(\sigma/G)^{p}\log(eT)\log\frac{4\log_{2}(4T)}{\delta}}\right)\alpha
\]
\end{itemize}
\end{lem}
%
\begin{rem}
For the case of known $T$, under the choices of $M_{t}$ and $\eta_{t}$
in Theorem \ref{thm:lip-prob-fix} (or the same, Theorem \ref{thm:lip-exp-fix}),
the term $\log(eT)$ in both bounds in Lemma \ref{lem:lip-prob-loglog}
will be removed (or replaced by $1$ equivalently).
\end{rem}
%
Next, in Lemma \ref{lem:xi-u-lip}, we provide an any time high-probability
bound of the term $\sum_{t=1}^{\tau}\eta_{t}^{2}(\|\xi_{t}^{u}\|^{2}-\E_{t}[||\xi_{t}^{u}\|^{2}])$
by using Freedman's inequality.
\begin{lem}
\label{lem:xi-u-lip}When $\mu=0$, we have that
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$
\[
\sum_{t=1}^{\tau}\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq26\left(\log\frac{4}{\delta}+\sqrt{\log(eT)\log\frac{4}{\delta}}\right)\alpha^{2}.
\]
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$
\[
\sum_{t=1}^{\tau}\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq26\left(\log\frac{4}{\delta}+\sqrt{(\sigma/G)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha^{2}.
\]
\end{itemize}
\end{lem}
%
\begin{rem}
\label{rem:xi-u-lip}For the case of known $T$, under the choices
of $M_{t}$ and $\eta_{t}$ in Theorem \ref{thm:lip-prob-fix} (or
the same, Theorem \ref{thm:lip-exp-fix}), the term $\log(eT)$ in
both bounds in Lemma \ref{lem:xi-u-lip} will be removed (or replaced
by $1$ equivalently).
\end{rem}
%
Equipped with the above lemmas, we are finally able to prove Theorems
\ref{thm:lip-prob} and \ref{thm:lip-prob-fix}.

\subsubsection{Proof of Theorem \ref{thm:lip-prob} }

\begin{proof}[Proof of Theorem \ref{thm:lip-prob}]
We first define a constant $K$
\begin{itemize}
\item under the choices of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
\begin{align}
K= & 4704\alpha^{2}\left(2\log^{2}(eT)+\log^{2}\frac{4\log_{2}(4T)}{\delta}+\log\frac{4}{\delta}\right)+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\label{eq:lip-prob-def-k-known}\\
= & O\left(\alpha^{2}\left(\log^{2}T+\log^{2}\left(\frac{\log T}{\delta}\right)\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)\nonumber 
\end{align}
\item under the choices of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$
\begin{align}
K= & 4704\alpha^{2}\left(\frac{3}{2}\log(eT)+\frac{3}{2}(\sigma/G)^{2p}\log^{2}(eT)+\log^{2}\frac{4\log_{2}(4T)}{\delta}+\log\frac{4}{\delta}\right)+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\label{eq:lip-prob-def-k-unknown}\\
= & O\left(\alpha^{2}\left(\log T+(\sigma/G)^{2p}\log^{2}T+\log^{2}\left(\frac{\log T}{\delta}\right)\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)\nonumber 
\end{align}
\end{itemize}
We sart with Lemma \ref{lem:basic-lip-prob} to get for any $\tau\in\left[T\right]$,
there is
\begin{align}
\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}-\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}\leq & h(\tau)+\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t},x_{*}-x_{t}\rangle+2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\nonumber \\
\Rightarrow\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}= & \sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},x_{*}-x_{t}\rangle+\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle+2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\nonumber \\
 & +h(\tau)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\label{eq:lip-1}
\end{align}
\begin{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},x_{*}-x_{t}\rangle$:
We know
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$:
\begin{align}
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},x_{*}-x_{t}\rangle\leq & \sum_{t=1}^{\tau}\eta_{t}\left\Vert \xi_{t}^{b}\right\Vert \left\Vert x_{*}-x_{t}\right\Vert \overset{(a)}{\leq}\sum_{t=1}^{\tau}\left(\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}\right)\cdot2\sigma^{p}M_{t}^{1-p}\cdot D_{\tau}\nonumber \\
\leq & \sum_{t=1}^{\tau}2D_{\tau}\cdot\frac{\alpha}{\sigma t^{\frac{1}{p}}}\cdot\sigma t^{\frac{1}{p}-1}=\sum_{t=1}^{\tau}\frac{2\alpha}{t}D_{\tau}\leq2\alpha\log(eT)D_{\tau}.\label{eq:lip-known-1}
\end{align}
where $(a)$ is due to$\left\Vert \xi_{t}^{b}\right\Vert \leq2\sigma^{p}M_{t}^{1-p}$
by Lemma \ref{lem:err-bound} and $\left\Vert x_{*}-x_{t}\right\Vert =d_{t}\leq\max_{t\in\left[\tau\right]}d_{t}=D_{\tau}$
for $t\leq\tau$.
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$:
\begin{align}
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},x_{*}-x_{t}\rangle\leq & \sum_{t=1}^{\tau}\eta_{t}\left\Vert \xi_{t}^{b}\right\Vert \left\Vert x_{*}-x_{t}\right\Vert \overset{(b)}{\leq}\sum_{t=1}^{\tau}\frac{\alpha}{Gt^{\frac{1}{p}}}\cdot2\sigma^{p}M_{t}^{1-p}\cdot D_{\tau}\nonumber \\
= & \sum_{t=1}^{\tau}\frac{2(\sigma/G)^{p}\alpha}{t}D_{\tau}\leq2(\sigma/G)^{p}\alpha\log(eT)D_{\tau}.\label{eq:lip-unknown-1}
\end{align}
where $(b)$ is due to$\left\Vert \xi_{t}^{b}\right\Vert \leq2\sigma^{p}M_{t}^{1-p}$
by Lemma \ref{lem:err-bound} and $\left\Vert x_{*}-x_{t}\right\Vert =d_{t}\leq\max_{t\in\left[\tau\right]}d_{t}=D_{\tau}$
for $t\leq\tau$.
\end{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle$:
By Lemma \ref{lem:lip-prob-loglog}. we have
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle\leq14\mathfrak{D}_{\tau}\left(\log\frac{4\log_{2}(4T)}{\delta}+\sqrt{\log(eT)\log\frac{4\log_{2}(4T)}{\delta}}\right)\alpha\label{eq:lip-known-2}
\end{equation}
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle\leq14\mathfrak{D}_{\tau}\left(\log\frac{4\log_{2}(4T)}{\delta}+\sqrt{(\sigma/G)^{p}\log(eT)\log\frac{4\log_{2}(4T)}{\delta}}\right)\alpha\label{eq:lip-unkown-2}
\end{equation}
\end{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)$:
By Lemma \ref{lem:xi-u-lip}, we have
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq52\left(\log\frac{4}{\delta}+\sqrt{\log(eT)\log\frac{4}{\delta}}\right)\alpha^{2}.\label{eq:lip-known-3}
\end{equation}
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq52\left(\log\frac{4}{\delta}+\sqrt{(\sigma/G)^{p}\log(eT)\log\frac{4}{\delta}}\right)\alpha^{2}.\label{eq:lip-unknown-3}
\end{equation}
\end{itemize}
\end{itemize}
Now we use the case of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
as an example to finish the proof. A similar argument can be applied
to the case of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$
as well.

Combining (\ref{eq:lip-1}), (\ref{eq:lip-known-1}), (\ref{eq:lip-known-2})
and (\ref{eq:lip-known-3}), we have with probability at least $1-\delta$,
for any $\tau\in\left[T\right]$:
\begin{align*}
\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}\leq & 2\alpha\log(eT)D_{\tau}+14\mathfrak{D}_{\tau}\left(\log\frac{4\log_{2}(4T)}{\delta}+\sqrt{\log(eT)\log\frac{4\log_{2}(4T)}{\delta}}\right)\alpha\\
 & +52\left(\log\frac{4}{\delta}+\sqrt{\log(eT)\log\frac{4}{\delta}}\right)\alpha^{2}+h(\tau)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\overset{(c)}{\leq} & \frac{D_{\tau}^{2}+\mathfrak{D}_{\tau}^{2}}{8}+8\alpha^{2}\log^{2}(eT)+784\alpha^{2}\left(\log^{2}\frac{4\log_{2}(4T)}{\delta}+\log(eT)\log\frac{4\log_{2}(4T)}{\delta}\right)\\
 & +52\left(\log\frac{4}{\delta}+\sqrt{\log(eT)\log\frac{4}{\delta}}\right)\alpha^{2}+95\alpha^{2}\log(eT)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\overset{(d)}{\leq} & \frac{D_{\tau}^{2}}{4}+1176\alpha^{2}\left(2\log^{2}(eT)+\log^{2}\frac{4\log_{2}(4T)}{\delta}+\log\frac{4}{\delta}\right)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\overset{(e)}{\leq} & \frac{D_{\tau}^{2}}{4}+\frac{K}{4}
\end{align*}
where $(c)$ is by AM-GM inequality and plugging in $h(\tau)=95\alpha^{2}\log(e\tau)\leq95\alpha^{2}\log(eT)$;
$(d)$ is by $\mathfrak{D}_{\tau}^{2}=(D_{\tau}\lor2\alpha)^{2}\leq D_{\tau}^{2}+4\alpha^{2}$
and some other omitted tedious calculations; $(e)$ is due to the
definition of $K$ (see (\ref{eq:lip-prob-def-k-known})). Hence,
we have with probability at least $1-\delta$, for any $\tau\in\left[T\right]$,
\[
\frac{d_{\tau+1}^{2}}{2}\leq\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}\leq\frac{D_{\tau}^{2}}{4}+\frac{K}{4}\Rightarrow d_{\tau+1}^{2}\leq\frac{D_{\tau}^{2}}{2}+\frac{K}{2},
\]
which implies $d_{t}^{2}\leq D_{t}^{2}\leq K$ for any $t\in\left[T+1\right]$
by simple induction.

Finally, we consider time $T$ to get with probability at least $1-\delta$
\[
\sum_{t=1}^{T}\eta_{t}\Delta_{t}\leq\frac{\left\Vert x_{T+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{T}\eta_{t}\Delta_{t}\leq\frac{D_{T}^{2}}{4}+\frac{K}{4}\leq\frac{K}{2}.
\]
Note that $\eta_{t}$ is non-increasing and $F(\bar{x}_{T})-F(x_{*})\leq\frac{\sum_{t=1}^{T}\Delta_{t}}{T}$
by the convexity of $F$ where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$,
we conclude that
\[
F(\bar{x}_{T})-F(x_{*})\leq\frac{K}{2\eta_{T}T}.
\]
Plugging $K$ and $\eta_{t}$ for two cases respectively, we get the
desired result.
\end{proof}


\subsubsection{Proof of Theorem \ref{thm:lip-prob-fix}}

\begin{proof}[Proof of Theorem \ref{thm:lip-prob-fix}]
Following a similar proof of Theorem \ref{thm:lip-prob}, we know
that with probability at least $1-\delta$, for any $t\in\left[T+1\right]$,
there is
\[
\left\Vert x_{t}-x_{*}\right\Vert ^{2}\leq K
\]
where $K$ is defined as
\begin{itemize}
\item under the choices of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$
\begin{align}
K= & 4704\alpha^{2}\left(2+\log^{2}\frac{4\log_{2}(4T)}{\delta}+\log\frac{4}{\delta}\right)+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\nonumber \\
= & O\left(\alpha^{2}\log^{2}\left(\frac{4\log_{2}(4T)}{\delta}\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)\label{eq:lip-prob-fix-def-k-known}
\end{align}
\item under the choices of $M_{t}=2GT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$
\begin{align}
K= & 4704\alpha^{2}\left(\frac{3}{2}+\frac{3}{2}(\sigma/G)^{2p}+\log^{2}\frac{4\log_{2}(4T)}{\delta}+\log\frac{4}{\delta}\right)+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\nonumber \\
= & O\left(\alpha^{2}\left((\sigma/G)^{2p}+\log^{2}\left(\frac{4\log_{2}(4T)}{\delta}\right)\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)\label{eq:lip-prob-fix-def-k-unknown}
\end{align}
\end{itemize}
Now considering the following $\F_{t-1}$-measurable random variable
\[
Z_{t}=\left(x_{*}-x_{t}\right)\mathds{1}_{\left\Vert x_{t}-x_{*}\right\Vert ^{2}\leq K},\forall t\in\left[T\right],
\]
which implies $\Pr\left[E\right]\geq1-\delta$ where $E\coloneqq\left\{ Z_{t}=x_{*}-x_{t},\forall t\in\left[T\right]\right\} $.
We remark that $\left\Vert Z_{t}\right\Vert \leq\sqrt{K}$ almost
surely.

Next, assuming that $E$ happens, we use (\ref{thm:lip-prob-fix})
here to get for any $\tau\in\left[T\right]$
\begin{align}
\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}= & \sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},x_{*}-x_{t}\rangle+\eta_{t}\langle\xi_{t}^{u},x_{*}-x_{t}\rangle+2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\nonumber \\
 & +h(\tau)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\nonumber \\
= & \sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},Z_{t}\rangle+\eta_{t}\langle\xi_{t}^{u},Z_{t}\rangle+2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\nonumber \\
 & +h(\tau)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}.\label{eq:lip-fix-1}
\end{align}
Note that $h(\tau)$ should be changed as stated in Remark \ref{rem:basic-lip-prob}
accordingly.
\begin{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},Z_{t}\rangle$:
We know
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$,
following the similar steps in (\ref{eq:lip-known-1}), we obtain
\begin{equation}
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},Z_{t}\rangle\leq2\alpha\sqrt{K}.\label{eq:lip-known-fix-1}
\end{equation}
\item under the choice of $M_{t}=2GT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$,
following the similar steps in (\ref{eq:lip-unknown-1}), we obtain
\begin{equation}
\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{b},Z_{t}\rangle\leq2\alpha(\sigma/G)^{p}\sqrt{K}.\label{eq:lip-unknown-fix-1}
\end{equation}
\end{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},Z_{t}\rangle$:
Observe that 
\begin{align*}
\E_{t}\left[\eta_{t}\langle\xi_{t}^{u},Z_{t}\rangle\right]= & 0;\\
\left|\eta_{t}\langle\xi_{t}^{u},Z_{t}\rangle\right|\leq & \eta_{t}\left\Vert \xi_{t}^{u}\right\Vert \left\Vert Z_{t}\right\Vert \overset{(a)}{\leq}\eta_{t}\cdot2M_{t}\cdot\sqrt{K}\overset{(b)}{\leq}4\alpha\sqrt{K};\\
\E_{t}\left[\eta_{t}^{2}\langle\xi_{t}^{u},Z_{t}\rangle^{2}\right]\leq & \eta_{t}^{2}\cdot\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\cdot K\overset{(c)}{\leq}10\eta_{t}^{2}\sigma^{p}M_{t}^{2-p}K;
\end{align*}
where $(a)$ and $(c)$ are both due to Lemma \ref{lem:err-bound};
$(b)$ is by Lemma \ref{lem:normalize}.
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$:
\begin{align*}
10\eta_{t}^{2}\sigma^{p}M_{t}^{2-p}K\leq & 10K\cdot\left(\frac{\alpha^{2}}{\left(G^{2}+\sigma^{2}\right)T}\land\frac{\alpha^{2}}{\sigma^{2}T^{\frac{2}{p}}}\right)\cdot\left(2^{2-p}\sigma^{p}G^{2-p}\lor\sigma^{2}T^{\frac{2}{p}-1}\right)\leq\frac{20\alpha^{2}K}{T}\\
\Rightarrow\sum_{t=1}^{T}\E_{t}\left[\eta_{t}^{2}\langle\xi_{t}^{u},Z_{t}\rangle^{2}\right]\leq & 20\alpha^{2}K.
\end{align*}
Let $R=4\alpha\sqrt{K}$, $F=20\alpha^{2}$. By Freedman's inequality
(Corollary \ref{cor:ez-any-freedman}), we know with probability at
least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$
\begin{align}
\left|\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},Z_{t}\rangle\right|\leq & \frac{2R}{3}\log\frac{4}{\delta}+\sqrt{2F\log\frac{4}{\delta}}=\frac{8\alpha}{3}\sqrt{K}\log\frac{4}{\delta}+\sqrt{40\alpha^{2}K\log\frac{4}{\delta}}\nonumber \\
\leq & 7\left(\log\frac{4}{\delta}+\sqrt{\log\frac{4}{\delta}}\right)\alpha\sqrt{K}.\label{eq:lip-known-fix-2}
\end{align}
\item under the choice of $M_{t}=2GT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$:
\begin{align*}
10\eta_{t}^{2}\sigma^{p}M_{t}^{2-p}K\leq & 10K\cdot\frac{\alpha^{2}}{G^{2}T^{\frac{2}{p}}}\cdot2^{2-p}\sigma^{p}G^{2-p}T^{\frac{2}{p}-1}\leq\frac{20(\sigma/G)^{p}\alpha^{2}K}{T}\\
\Rightarrow\sum_{t=1}^{T}\E_{t}\left[\eta_{t}^{2}\langle\xi_{t}^{u},Z_{t}\rangle^{2}\right]\leq & 20(\sigma/G)^{p}\alpha^{2}K.
\end{align*}
Let $R=4\alpha\sqrt{K}$, $F=20(\sigma/G)^{p}\alpha^{2}K$. By Freedman's
inequality (Corollary \ref{cor:ez-any-freedman}), we know with probability
at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$
\begin{align}
\left|\sum_{t=1}^{\tau}\eta_{t}\langle\xi_{t}^{u},Z_{t}\rangle\right|\leq & \frac{2R}{3}\log\frac{4}{\delta}+\sqrt{2F\log\frac{4}{\delta}}=\frac{8\alpha}{3}\sqrt{K}\log\frac{4}{\delta}+\sqrt{40\alpha^{2}(\sigma/G)^{p}K\log\frac{4}{\delta}}\nonumber \\
\leq & 7\left(\log\frac{4}{\delta}+\sqrt{(\sigma/G)^{p}\log\frac{4}{\delta}}\right)\alpha\sqrt{K}.\label{eq:lip-unknown-fix-2}
\end{align}
\end{itemize}
\item Bounding the term $\sum_{t=1}^{\tau}2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)$:
By Remark \ref{rem:xi-u-lip}, we have
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq52\left(\log\frac{4}{\delta}+\sqrt{\log\frac{4}{\delta}}\right)\alpha^{2}.\label{eq:lip-known-fix-3}
\end{equation}
\item under the choice of $M_{t}=2GT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$,
with probability at least $1-\frac{\delta}{2}$, for any $\tau\in\left[T\right]$:
\begin{equation}
\sum_{t=1}^{\tau}2\eta_{t}^{2}\left(\left\Vert \xi_{t}^{u}\right\Vert ^{2}-\E_{t}\left[\left\Vert \xi_{t}^{u}\right\Vert ^{2}\right]\right)\leq52\left(\log\frac{4}{\delta}+\sqrt{(\sigma/G)^{p}\log\frac{4}{\delta}}\right)\alpha^{2}.\label{eq:lip-unknown-fix-3}
\end{equation}
\end{itemize}
\end{itemize}
Now we use the case of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$
as an example to finish the proof. A similar argument can be applied
to the case of $M_{t}=2GT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$
as well.

Combining (\ref{eq:lip-fix-1}), (\ref{eq:lip-known-fix-1}), (\ref{eq:lip-known-fix-2})
and (\ref{eq:lip-known-fix-3}), we have with probability at least
$1-2\delta$, for any $\tau\in\left[T\right]$:
\begin{align*}
\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}\leq & 2\alpha\sqrt{K}+7\left(\log\frac{4}{\delta}+\sqrt{\log\frac{4}{\delta}}\right)\alpha\sqrt{K}+52\left(\log\frac{4}{\delta}+\sqrt{\log\frac{4}{\delta}}\right)\alpha^{2}+h(\tau)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\leq & 16\log\left(\frac{4}{\delta}\right)\alpha\sqrt{K}+199\log\left(\frac{4}{\delta}\right)\alpha^{2}+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\leq & 16\log\left(\frac{4}{\delta}\right)\alpha\sqrt{O\left(\alpha^{2}\log^{2}\left(\frac{4\log_{2}(4T)}{\delta}\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)}+199\log\left(\frac{4}{\delta}\right)\alpha^{2}+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\leq & 16\log\left(\frac{4}{\delta}\right)O\left(\log\left(\frac{4\log_{2}(4T)}{\delta}\right)\right)\alpha^{2}+128\log^{2}\left(\frac{4}{\delta}\right)\alpha^{2}+199\log\left(\frac{4}{\delta}\right)\alpha^{2}+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\\
= & O\left(\log\frac{4}{\delta}\log\left(\frac{4\log_{2}(4T)}{\delta}\right)\alpha^{2}+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).
\end{align*}
Replacing $\delta$ by $\delta/2$, we know with probability at least
$1-\delta$, for any $\tau\in\left[T\right]$:
\[
\frac{\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}}{2}+\sum_{t=1}^{\tau}\eta_{t}\Delta_{t}\leq O\left(\log\frac{8}{\delta}\log\left(\frac{8\log_{2}(4T)}{\delta}\right)\alpha^{2}+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)
\]
which implies
\[
F(\bar{x}_{T})-F(x_{*})\leq\frac{1}{T}\sum_{t=1}^{T}\Delta_{t}\leq\frac{O\left(\log\frac{8}{\delta}\log\left(\frac{8\log_{2}(4T)}{\delta}\right)\alpha^{2}+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right)}{\eta_{T}T}
\]
where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Finally, plugging
in $\eta_{T}$, the proof is finished.
\end{proof}


\subsection{In-Expectation Analysis when $\mu=0$\label{subsec:lip-exp}}

Now we turn to the in-expectation bound of Algorithm \ref{alg:algo}
for the general convex case. We introduce Lemma \ref{lem:basic-lip-exp}.
the in-expectation version of Lemma \ref{lem:basic-lip-prob}. This
lemma is enough to let us prove Theorems \ref{thm:lip-exp} and \ref{thm:lip-exp-fix}.
\begin{lem}
\label{lem:basic-lip-exp}When $\mu=0$, for any $\tau\in\left[T\right]$,
we have
\begin{itemize}
\item under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$:
\[
\frac{\E\left[\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}\right]-\E\left[\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq h(\tau)+2\alpha\sum_{t=1}^{\tau}\frac{\sqrt{\E\left[\left\Vert x_{t}-x_{*}\right\Vert ^{2}\right]}}{t}.
\]
\item under the choice of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$:
\[
\frac{\E\left[\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}\right]-\E\left[\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq h(\tau)+2\alpha(\sigma/G)^{p}\sum_{t=1}^{\tau}\frac{\sqrt{\E\left[\left\Vert x_{t}-x_{*}\right\Vert ^{2}\right]}}{t}.
\]
\end{itemize}
where $h(\tau)$ is defined in Lemma \ref{lem:basic-lip-prob}.
\end{lem}
%
\begin{rem}
For the case of known $T$, under the choices of $M_{t}$ and $\eta_{t}$
in Theorem \ref{thm:lip-prob-fix} (or the same, Theorem \ref{thm:lip-exp-fix}),
in Lemma \ref{lem:basic-lip-exp}, $\sqrt{\E[\|x_{t}-x_{*}\|^{2}]}/t$
will be replaced by $\sqrt{\E[\|x_{t}-x_{*}\|^{2}]}/T$ and $h(\tau)$
will be changed according to Remark \ref{rem:basic-lip-prob}.
\end{rem}
%
Note that Lemma \ref{lem:basic-lip-prob} is a very interesting result.
On the L.H.S., there is $\E[\|x_{\tau+1}-x_{*}\|^{2}]$, however,
the R.H.S. only has $\sqrt{\E[\|x_{t}-x_{*}\|^{2}]}$ for any $t\le\tau$.
If we assume $\E[\|x_{t}-x_{*}\|^{2}]$ can be uniformly bounded by
some $K$ for any $t\leq\tau$, we know $\sum_{t=1}^{\tau}\sqrt{\E[\|x_{t}-x_{*}\|^{2}]}/t=O(\sqrt{K}\log T)$,
which implies $\E[\|x_{\tau+1}-x_{*}\|^{2}]=O(h(\tau)+\sqrt{K}\log T+\E[\|x_{1}-x_{*}\|^{2}])$.
Such a result tells us $\E[\|x_{\tau+1}-x_{*}\|^{2}]$ can still be
bounded by $K$ once $K$ is picked properly. So we can expect a uniform
bound on $\max_{s\in\left[t\right]}\E[\|x_{s}-x_{*}\|^{2}]$ for any
$t\leq T$. We will show how to demonstrate this idea formally in
the proof of Theorems \ref{thm:lip-exp} and \ref{thm:lip-exp-fix}.

\subsubsection{Proof of Theorem \ref{thm:lip-exp}}

\begin{proof}[Proof of Theorem \ref{thm:lip-exp}]
We first define the notation $\mathcal{D}_{t}=\max_{s\in\left[t\right]}\sqrt{\E\left[d_{s}^{2}\right]}$.
Next, consider the following constant $K$:
\begin{itemize}
\item under the choices of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
\begin{align}
K= & 396\alpha^{2}\log^{2}(eT)+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\label{eq:lip-exp-def-k-known}\\
= & O\left(\alpha^{2}\log^{2}T+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).\nonumber 
\end{align}
\item under the choices of $M_{t}=2Gt^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{Gt^{\frac{1}{p}}}$
\begin{align}
K= & 16(\sigma/G)^{2p}\alpha^{2}\log^{2}(eT)+320\left(1+(\sigma/G)^{p}\right)\alpha^{2}\log(eT)+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\label{eq:lip-exp-def-k-unknown}\\
= & O\left(\alpha^{2}\left(\log T+(\sigma/G)^{2p}\log^{2}T\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).\nonumber 
\end{align}
\end{itemize}
Here, we use the case $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
as an example. A similar proof can be made for the other setting.

We invoke Lemma \ref{lem:basic-lip-exp} under the choice of $M_{t}=2G\lor\sigma t^{\frac{1}{p}}$
and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)t}}\land\frac{\alpha}{\sigma t^{\frac{1}{p}}}$
for any time $\tau\in\left[T\right]$ to get
\begin{align*}
\frac{\E\left[\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}\right]-\E\left[\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq & h(\tau)+2\alpha\sum_{t=1}^{\tau}\frac{\sqrt{\E\left[\left\Vert x_{t}-x_{*}\right\Vert ^{2}\right]}}{t}\\
\overset{(a)}{\leq} & h(\tau)+2\alpha\sum_{t=1}^{\tau}\frac{\mathcal{D}_{\tau}}{t}\leq h(\tau)+2\alpha\left(\log T+1\right)\mathcal{D}_{\tau}\\
\Rightarrow\frac{\E\left[\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\overset{(b)}{\leq} & \frac{\mathcal{D}_{\tau}^{2}}{4}+4\alpha^{2}\log^{2}(eT)+95\alpha^{2}\log^{2}(eT)+\frac{\left\Vert x_{1}-x_{*}\right\Vert ^{2}}{2}\\
\overset{(c)}{=} & \frac{\mathcal{D}_{\tau}^{2}}{4}+\frac{K}{4}
\end{align*}
where $(a)$ is by the definition of $\mathcal{D}_{\tau}$; $(b)$
is by using due to AM-GM inequality for the term $2\alpha\log\left(T+1\right)\mathcal{D}_{\tau}$
and plugging in $h(\tau)=95\alpha^{2}\log(e\tau)\leq95\alpha^{2}\log^{2}(eT)$;
$(c)$ is from the definition of $K$ (see (\ref{eq:lip-exp-def-k-known})).
Thus, for any $\tau\in\left[T\right]$, there is
\[
\frac{\E\left[d_{\tau+1}^{2}\right]}{2}\leq\frac{\E\left[\left\Vert x_{\tau+1}-x_{*}\right\Vert ^{2}\right]}{2}+\sum_{t=1}^{\tau}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\mathcal{D}_{\tau}^{2}}{4}+\frac{K}{4}\Rightarrow d_{\tau+1}^{2}\leq\frac{\mathcal{D}_{\tau}^{2}}{2}+\frac{K}{2},
\]
which implies $\E\left[d_{t}^{2}\right]\leq\mathcal{D}_{t}^{2}\leq K$
for any $t\in\left[T+1\right]$ by simple induction.

Finally, for time $T$, we know
\[
\sum_{t=1}^{T}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\E\left[\left\Vert x_{T+1}-x_{*}\right\Vert ^{2}\right]}{2}+\sum_{t=1}^{T}\eta_{t}\E\left[\Delta_{t}\right]\leq\frac{\mathcal{D}_{T}^{2}}{4}+\frac{K}{4}\leq\frac{K}{2}.
\]
Note that is $\eta_{t}$ non-increasing and $\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq\frac{\sum_{t=1}^{T}\E\left[\Delta_{t}\right]}{T}$
by the convexity of $F$ where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$,
we conclude that
\[
\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq\frac{K}{2\eta_{T}T}.
\]
Plugging $K$ and $\eta_{t}$ for two cases respectively, we get the
desired result.
\end{proof}


\subsubsection{Proof of Theorem \ref{thm:lip-exp-fix}.}

\begin{proof}[Proof of Theorem \ref{thm:lip-exp-fix}]
Following the same line in the proof of Theorem \ref{thm:lip-exp},
we only need to notice that the constant $K$ now is defined as
\begin{itemize}
\item under the choices of $M_{t}=2G\lor\sigma T^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{\sqrt{\left(G^{2}+\sigma^{2}\right)T}}\land\frac{\alpha}{\sigma T^{\frac{1}{p}}}$
\begin{align*}
K= & 396\alpha^{2}+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\\
= & O\left(\alpha^{2}+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).
\end{align*}
\item under the choices of $M_{t}=2GT^{\frac{1}{p}}$ and $\eta_{t}=\frac{\alpha}{GT^{\frac{1}{p}}}$
\begin{align*}
K= & 16(\sigma/G)^{2p}\alpha^{2}+320\left(1+(\sigma/G)^{p}\right)\alpha^{2}+2\left\Vert x_{1}-x_{*}\right\Vert ^{2}\\
= & O\left(\alpha^{2}\left(1+(\sigma/G)^{2p}\right)+\left\Vert x_{1}-x_{*}\right\Vert ^{2}\right).
\end{align*}
\end{itemize}
By similar steps, we will reach $\E\left[F(\bar{x}_{T})-F(x_{*})\right]\leq\frac{K}{2\eta_{T}T}$
again where $\bar{x}_{T}=\frac{1}{T}\sum_{t=1}^{T}x_{t}$. Plugging
$K$ and $\eta_{t}$ for two cases respectively, we get the desired
result.
\end{proof}

