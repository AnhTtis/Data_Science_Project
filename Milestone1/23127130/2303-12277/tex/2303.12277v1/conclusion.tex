
\section{Conclusion\label{sec:conclusion}}

In this paper, we present a comprehensive analysis of stochastic
nonsmooth optimization with heavy-tailed noises and obtain several
new results. More specifically, under properly picked parameters,
we show a simple clipping algorithm provably converges both in expectation
and probability for convex or strongly convex objectives. Furthermore,
no matter whether the time horizon $T$ or noise level $\sigma$ is
known or not, our choices of clipping magnitude and step size still
guarantee (nearly) optimal in-expectation and high-probability rates.

However, there still remains an interesting direction worth exploring.
The same as the previous works, our results heavily rely on the prior
knowledge of $p$, $G$ and $\mu$ (when considering strongly convex
functions), all of which may be hard to estimate in practice. Hence,
finding an algorithm without requiring any parameters is very important
for both theoretical and practical sides. We leave this important
question as future work and expect it to be addressed.
