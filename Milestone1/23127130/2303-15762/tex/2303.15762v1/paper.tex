
% \documentclass[acmtog, authorversion, review=false]{acmart}
\documentclass[acmtog, authorversion, review=false, nonacm, timestamp]{acmart}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}\acmJournal{TOG}

\citestyle{acmauthoryear}

\input{preamble.tex}

\def\teasersize{0.96}
\def\teasercaptionoffset{-2mm}
\setcopyright{none}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\def\imagesdir/{images/}


% ---------------------------------------------------------------------
% ---------------------------------------------------------------------
% ---------------------------------------------------------------------


\input{fig_teaser.tex}

% ---------------------------------------------------------------------

\begin{document}

% ---------------------------------------------------------------------

\externaldocument{supplemental}

% ---------------------------------------------------------------------

\title[A Generalized Ray Formulation For Wave-Optics Rendering]%
      {A Generalized Ray Formulation For Wave-Optics Rendering}

\author{Shlomi Steinberg}
\email{p@shlomisteinberg.com}
\orcid{0000-0003-2748-4036}

\author{Ravi Ramamoorthi}
\email{ravir@cs.ucsd.edu}
\orcid{0000-0003-3993-5789}

\author{Benedikt Bitterli}
\email{benedikt.bitterli@gmail.com}
\orcid{0000-0002-8799-7119}

\affiliation{%
    \institution{NVIDIA}
    \city{San Francisco}
    \country{United States}
}

\author{Eugene d'Eon}
\email{ejdeon@gmail.com}
\orcid{0000-0002-3761-2989}

\affiliation{%
    \institution{NVIDIA}
    \city{Wellington}
    \country{New Zealand}
}

\author{Ling-Qi Yan}
\email{lingqi@cs.ucsb.edu}

\affiliation{%
    \institution{University of California, Santa Barbara}
    \city{Santa Barbara}
    \state{California}
    \postcode{93106}
    \country{United States}
}

\author{Matt Pharr}
\email{matt@pharr.org}
\orcid{0000-0002-0566-8291}

\affiliation{%
    \institution{NVIDIA}
    \city{San Francisco}
    \country{United States}
}

%-------------------------------------------------------------------------
\begin{abstract}
    Under ray-optical light transport, the classical ray serves as a local and linear ``point query'' of light's behaviour.
    Such point queries are useful, and sophisticated path tracing and sampling techniques enable efficiently computing solutions to light transport problems in complex, real-world settings and environments.
    However, such formulations are firmly confined to the realm of ray optics, while many applications of interest, in computer graphics and computational optics, demand a more precise understanding of light.
    We rigorously formulate the \emph{generalized ray}, which enables local and linear point queries of the wave-optical phase space.
    Furthermore, we present \emph{sample-solve}: a simple method that serves as a novel link between path tracing and computational optics.
    We will show that this link enables the application of modern path tracing techniques for wave-optical rendering, improving upon the state-of-the-art in terms of the generality and accuracy of the formalism, ease of application, as well as performance.
    Sampling using generalized rays enables interactive rendering under rigorous wave optics, with orders-of-magnitude faster performance compared to existing techniques.
\end{abstract}

%-------------------------------------------------------------------------
%  ACM CCS 1998
%  (see http://www.acm.org/about/class/1998)
% \begin{classification} % according to http:http://www.acm.org/about/class/1998
% \CCScat{Computer Graphics}{I.3.3}{Picture/Image Generation}{Line and curve generation}
% \end{classification}
%-------------------------------------------------------------------------
%  ACM CCS 2012
%The tool at \url{http://dl.acm.org/ccs.cfm} can be used to generate
% CCS codes.
\begin{CCSXML}
    <ccs2012>
    <concept>
        <concept_id>10010147.10010371.10010372</concept_id>
        <concept_desc>Computing methodologies~Rendering</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010147.10010371</concept_id>
        <concept_desc>Computing methodologies~Computer graphics</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010405.10010432.10010441</concept_id>
        <concept_desc>Applied computing~Physics</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
    </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computing methodologies~Rendering}
\ccsdesc[500]{Computing methodologies~Computer graphics}
\ccsdesc[300]{Applied computing~Physics}

\keywords{}



%-------------------------------------------------------------------------
\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}
%-------------------------------------------------------------------------



\input{fig_ray_wave_plt.tex}


%-------------------------------------------------------------------------

\section{Introduction} \label{section_introduction_paper}

Rendering and light transport often employ a simplistic understanding of light: a collection of rays.
This \emph{ray optical} view (see \cref{fig_ray_wave_plt}) is convenient, as a light ray is perfectly localized in space---it has precise position and direction of propagation---and rays add up linearly in terms of their observable radiometric properties (e.g., radiance).
These traits, \emph{locality} and \emph{linearity}, are essential for rendering: 
they allow formulating a linear rendering equation;
and, enable the application of powerful sampling techniques in order to sample paths through the scene, connecting a light source to the sensor.
In addition, dedicated ray-tracing hardware today facilitates applying these path tracing techniques in real-time \cite{Burgess2020-eq}.

The above comes at a price: ray optics ignores the wave nature of light, meaning wave-interference effects cannot be reproduced.
Such effects include the colourful glints that appear when light is scattered by scratches in metal, the colour of the wings and scales of some species of insects, snakes and fish, stress birefringence in glass and plastics, and the appearance of a layer of oil or metallic oxide on a surface.
A wave-optical formulation of light, in sharp contrast to the ray-optical view, admits neither a clear locality nor linearity:
Under wave optics, the \emph{wave function} 
replaces the ray as the descriptor of light (\cref{fig_ray_wave_plt_b_wave}).
However, it is well-known that the wave function admits intrinsic \emph{uncertainty}: its extent over spatial position and frequency (direction of propagation) may not be arbitrarily small, implying light may no longer be perfectly localized.
Unlike the ray, which permits local sampling of light's behaviour, the wave function serves as a \emph{global} descriptor of light.
Furthermore, superposition of wave functions is not linear in terms of their intensity and radiometry, but is \emph{bilinear}: it admits an interference term.
The loss of locality and linearity greatly frustrates applications of path tracing under wave optics.
While path-tracing techniques power most of the complex computer-generated content in films and movies, wave solvers struggle with scenes that would be considered exceedingly simple by the rendering community.

Faithful reproduction of wave effects, as observed in the physical world, is further complicated by the fact that virtually all the light that we encounter in daily life and render with is \emph{partially coherent}.
That is, this light's underlying field contains seemingly random fluctuations and its ability to produce observable wave-interference effects is spatially and temporally limited.
Therefore, even though interference happens all the time and all around us, many of these processes produce interference effects with very poor observable contrast, and are not visible; while others may produce observable interference effects, with varying contrast that depends both on the material and light properties.

Recognizing that the partial coherence of light plays a crucial role in the observable appearance of materials, \emph{physical light transport} (PLT) was introduced \cite{Steinberg_practical_plt_2022,Steinberg_lt_framework_2021} as a framework that enables practical wave-optical light transport. 
PLT understands light as a collection of partially coherent, but mutually-incoherent, beams of light (\cref{fig_ray_wave_plt_c_plt}).
% \begin{enuminline}
    % \item 
        PLT quantifies the optical coherence properties of these beams, and propagates these properties around the scene. 
        The BSDFs (\emph{bidirectional scattering distribution functions}) that quantify the interaction of a beam with matter are now explicitly functions of the coherence of light.
    % \item 
        Distinct beams are mutually incoherent, meaning that no interference effects arise on superposition of beams, thus regaining \emph{linearity}.
        To be able to interpret distinct beams as mutually incoherent, the spatial extent of a beam must be greater than its spatial optical coherence.
    % \item 
        The most coherent light that we would typically use---sunlight on earth---admits spatial coherence length of about \SI{150}{\micro\metre}.
        As long as geometrical scene features are greater than that coherence length, approximating the propagation of a beam via ray tracing is a good approximation, resulting in practical \emph{locality}.
% \end{enuminline}
That is, PLT draws upon optical coherence theory in order to both reproduce the observable (partially-coherent) wave-interference effects, as well as take advantage of light's limited coherence in order to regain linearity and a form of locality.

However, the moment our descriptor of light quantifies the coherence of light and BSDFs are functions of these coherence properties, a \textbf{sampling problem} comes into effect (see \cref{fig_sampling_problem}):
These coherence properties depend on the source, and change on propagation and on interaction with matter.
Hence, they are only known when tracing light beams from a light source, but for a variety of practical reasons, path tracing is often done \emph{backwards}---from the sensor---or bidirectionally---backward and forward.
Because BSDFs depend on the coherence properties of light, which are unknown when tracing paths backward, it is unclear how to importance sample these BSDFs: it would require predicting in some fashion what the coherence of light would be at the interaction region, a difficult endeavour.
One approach \cite{Steinberg_practical_plt_2022} is to fix a global lower limit on the coherence of light.
However that limit needs to be conservative, and while this approach works for some materials, it yields poor sampling for materials that admit strong wave-interference effects, which precisely are the materials of interest.
Good sampling is critical for efficient path tracing, and solving the sampling problem is the core motivation for this paper.

\input{fig_sampling_problem.tex}

Evidently, if we sample partially-coherent interactions, the coherence properties of light need to be known.
We choose to side-step the issue by avoiding partial coherence altogether in the formulation of path sampling.
We take inspiration from the significant effort made in quantum mechanics in attempts to regain the ``grainy'' view of classical mechanics (with intrinsic locality and linearity).
In \cref{section_theory}, we draw upon such formulations in order to introduce a \emph{generalized ray}: a form of a coherent state that is the closest wave-optical analogue of the classical ray.
Light can always be understood as a collection of such mutually-incoherent generalized rays, thereby regaining linearity and locality (as physically allowed by the uncertainty relation).
This decomposition of light into generalized rays is a form of coherent-modes decomposition \cite{mandel1995optical}.
In \cref{section_sample_solve}, we then present a simple, but powerful, two-step \emph{sample-solve} algorithm, which is our primary contribution:
\begin{enumerate}
    \item 
        \textbf{Sample} ---
        Because each generalized ray is perfectly coherent, these generalized rays can be used to sample paths in a coherence-agnostic manner, solving the sampling problem.
        The generalized ray plays a similar role to the classical ray under ray optics: it allows \emph{point sampling} of light's behaviour.
    \item 
        \textbf{Solve} ---
        Once a path connecting a light source to the sensor has been sampled, we apply PLT tools in order to \emph{solve} for the partially-coherent transport over that path.
        This is possible due to the exact, formal relation between generalized rays and optical coherence that we show.
\end{enumerate}

This sample-solve approach enables efficient sampling, and, as a consequence, we show in \cref{section_interactive_rendering} that we are able to do wave-optical rendering with complex scenes (as in \cref{fig_teaser}), and at interactive performance.
As a matter of fact, the convergence performance of our approach is multiple orders-of-magnitude greater compared to the state-of-the-art.
Moreover, as our generalized rays extend classical rays to wave optics (in the sense that locality and linearity is regained), such wave-optical rendering can be achieved with relatively minor modifications of an existing, classical path tracer: only the BSDFs need to be redesigned.
More generally, sample-solve serves as a link between path tracing techniques and computational optics methods:
The sample step, as formulated, applies in highly-general settings, and allows the application of a wide class of sampling techniques; while the solve step is unconstrained by sampling concerns.
This link may serve to extend the applicability of some computational optics tools to complex scenes and environments, as well as to additional computer graphics applications.

%-------------------------------------------------------------------------


\section{Related Work} \label{section_related_work}

\paragraph{Partially-coherent light transport}

Physical light transport (PLT) was introduced over the recent years by \citet{Steinberg_lt_framework_2021,Steinberg_practical_plt_2022}. 
In contrast to work that aims to reproduce the appearance of a particular wave-interference effect by simulating the wave-optical interaction locally, at the BSDF level, PLT takes a global approach: it propagates the optical coherence of light throughout the scene.
This allows PLT to accurately, and efficiently, reproduce partially-coherent effects.
Similar to PLT, \citet{Oh_Kashyap_Garg_Chandran_Raskar_2010,Cuypers_Haber_Bekaert_Oh_Raskar_2012} also aim to simulate the global evolution of light's optical coherence, by using a form of a ``generalized radiance'' (the Wigner distribution function in its optical context \cite{testorf2010phase}), acting as a descriptor of partially-coherent light.
Further comparison between this line of work and PLT is given by \citet{Steinberg_hg_2021}.
We stress that the sampling problem inevitably arises in these approaches, as they quantify and propagate partially-coherent light.


\paragraph{Additional related computer graphics work}

\citet{Kajiya_1986} introduced \emph{Path tracing} to the computer graphics community in their seminal paper.
Path tracing refers to a family of techniques that aim to solve for the light transport via Monte-Carlo integration.
A wide body of work has extended path tracing or proposed important sampling techniques and approaches.
The most notable are bidirectional path tracing \cite{bdpt1998}, multiple importance sampling \cite{veach1995optimally}, ``photon'' mapping \cite{Jensen96}, vertex merging \cite{georgiev2011bidirectional}, and Metropolis light transport \cite{veach1997metropolis}.

Also related is work that aims to reproduce the appearance of diffractive materials.
Amongst the first to consider wave effects in rendering are \citet{Moravec1981waverendering,Stam_1999}.
Later work includes the rendering of iridescent and pearlescent materials \cite{Guillen:2020:Pearlescence}; diffractive scratches \cite{Velinov2018scratches,Werner2017Scratch}; diffractive surface models, with explicit microgeometry \cite{Yan2018,Falster2020} or statistical surface profiles \cite{Holzschuch2017,Steinberg_speckle_2021}; and, thin-film interference at a soap bubbles \cite{Huang_Iseringhausen_Kneiphof_Qu_Jiang_Hullin_2020} or due to a dielectric layer over a conductor \cite{Belcour:17,Kneiphof2019}.
Synthesis of BSDFs that account for wave interference was discussed by \citet{Toisoul2017diffractions}.


\paragraph{Computational optics}

Of some relevance is a hybrid method often referred to as ``shooting-bouncing ray'' (SBR).
SBR is a high-frequency electromagnetic scattering solver, and uses ray optics and ray tracing to track field propagation to a target aperture (i.e., a ray captures the behaviour of the field as it propagates into some solid angle).
Then, the electromagnetic field is reconstructed at that aperture and physical optics methods are applied to compute the light-matter interaction.
Applications of SBR methods are extensive, and include:
simulation of radar for imaging \cite{feng2021multiview}; driving-assistive technology \cite{Castro_Singh_Arora_Louie_Senic_2019}; urban structures verification \cite{weijie2016sar}; analysis of stealth aircraft scattering cross-section \cite{bilal2019comparison}; ground-penetrating radar \cite{Warren_Giannopoulos_Giannakis_2016}; radio propagation \cite{gunduzalp2019radio}; analysis of WiFi antenna characteristics \cite{chen2019analysis}; and, indoor positioning using WiFi \cite{hossain2018indoor}.

As very many rays need to be traced, attempts to accelerate SBR methods have taken a research trajectory not dissimilar to early computer graphics work: 
Employing multi-resolution grids \cite{Suk2001} and spatial-subdivision data structures to accelerate ray-facet intersections \cite{Jin2006,Tao2008}; accelerating on GPUs \cite{YuboTao2010,gao2015}; and, sampling the initial ray directions using Halton sequences \cite{Key2018}.
Nevertheless, SBR research greatly trails computer graphics in its ability to apply sophisticated sampling techniques, like importance sampling light-matter interactions or path guiding.
Therefore, scene complexity remains a very limiting factor.
While beyond the scope of this paper, our sample-solve approach may allow leveraging these powerful path tracing tools in order to accelerate SBR (and similar) methods, allowing high-frequency electrodynamics solvers in more complicated environments. 


%-------------------------------------------------------------------------


\section{Theoretical Foundations} \label{section_theory}

We start with an overview of wave optics and its ``phase-space'' formulation, then we proceed to define the \emph{generalized ray}.
We also formally show how partial coherence arises when we trace a collection of such generalized rays.


\subsection{Background: Wave Optics} \label{section_wave_optics}

The basic descriptor of light under wave optics is the \emph{wave function}, denoted $\wavefunc(\va{r} \argsep t)$, where $\va{r}$ is position and $t$ is time.
The wave function can be understood as describing the spatial excitations of the associated electric field.
Henceforth, we will fix time and, for brevity, drop $t$ from the argument lists.
Any physically-realizable wave function can be written in terms of its spectral decomposition into plane waves \cite{zangwill2013modern}, viz.
\begin{align}
    \wavefunc\qty(\va{r})
        =&
            \frac{1}{\qty(2\mpi)^{\frac{3}{2}}}
            \int \dd[3]{\va{k}}
                \tilde{\wavefunc}\qty(\va{k})
                \ee^{\ii \va{r}\cdot\va{k}}
    \label{wavefunc_spectral_decomposition}
    ~.
\end{align}
When we omit the bounds of integration, it implies that we integrate over the entire domain.
The above integral formalises a superposition of plane waves, where each plane wave ${\exp}(\ii \va{r}\cdot\va{k})$ propagates in the direction of the wavevector $\va{k}$, and with amplitude $\tilde{\wavefunc}(\va{k})$.
Also recall that $\abs*{\va{k}}=\eta\tfrac{2\mpi}{\lambda}$, with $\eta$ being the refractive index of the medium in which light propagates, is the wavenumber and $\lambda$ is the light's wavelength.

Clearly, \cref{wavefunc_spectral_decomposition} is simply an inverse Fourier transform, hence $\tilde{\wavefunc}$ and the wave function $\wavefunc$ are Fourier-pairs and contain the same information.
However, while $\wavefunc$ quantifies the spatial distribution of light, its Fourier-conjugate $\tilde{\wavefunc}$ quantifies its distribution in momentum, i.e. direction of propagation.
This Fourier relation between position and momentum gives rise to the important \emph{uncertainty relation} \cite{torre2005linear}:
\begin{align}
    \sigma_r \sigma_k 
        &\geq 
        \frac{1}{2}
    \label{eq_uncertainty_relation}
\end{align}
(in each dimension), with $\sigma_r^2$ and $\sigma_k^2$ being the variances of $\wavefunc$ and $\tilde{\wavefunc}$, respectively.
Intuitively, \cref{eq_uncertainty_relation} arises because the Fourier transform of a ``wide'' function is narrow, and vice versa.

For example, consider an idealised plane wave 
\begin{align}
    \wavefunc\qty(\va{r}) = \ee^{\ii \va{r}\cdot\va{k}^\prime}
    &&
    \longrightarrow
    &&
    \tilde{\wavefunc}\qty(\va{k}) \propto \dirac\qty(\va{k}-\va{k}^\prime)
\end{align}
for some fixed $\va{k}^\prime$. 
This wave function describes excitations that exist throughout space,
but its momentum distribution is proportional to a Dirac delta, i.e. the field propagates strictly in direction $\va{k}^\prime$.
Conversely, we may try to fix position, viz. 
\begin{align}
    \wavefunc\qty(\va{r}) = \dirac\qty(\va{r}-\va{r}^\prime)
    &&
    \longrightarrow
    &&
    \tilde{\wavefunc}(\va{k}) \propto \ee^{-\ii \va{k}\cdot\va{r}^\prime}
\end{align}
for some fixed $\va{r}^\prime$, but now this field propagates into all directions.
A wave function that is a Dirac delta both in position and wavevector is prohibited by the uncertainty relation.
This is in sharp contrast to the ray-optical formulation where a ray's position and direction of propagation are exactly known, and hence while a ray constitutes a \emph{local} sample of light's behaviour, the wave function is a \emph{global} descriptor.


\paragraph{An ensemble of waves}

To describe observable phenomena (averaged over the sources of randomness), it is common to consider statistical ensembles of waves, instead of a deterministic wave function.
Under this context, the wave function $\wavefunc$ becomes a realization from this ensemble.
An important quantity is then the \emph{cross-spectral density} (CSD) of light:
\begin{align}
    \csd\qty(\va{r}_1,\va{r}_2)
        \triangleq&
            \ev{\wavefunc\qty(\va{r}_1)\wavefunc^\star\qty(\va{r}_2)}
    \label{csd}
    ~,
\end{align}
where $\ev{\cdot}$ denotes the ensemble-averaging operator, and $\star$ complex conjugation.

The CSD describes the \emph{mutual coherence} of the ensemble over the spatial points $\va{r}_{1,2}$.
Mutual coherence is the statistical similarity of the wavefronts at these positions, and the ability of light that arrives to these points to produce observable interference effects.
For example, assume that monochromatic light is scattered at two points, $\va{r}_1,\va{r}_2$, and then the scattered light from both points superposes and is observed.
The observable, ensemble-averaged \emph{optical intensity} (magnitude of the Poynting vector, i.e. the power flow of the electromagnetic field with units of power per unit area) of the superposition is
\begin{align}
    I 
        =& \ev{\qty[\wavefunc\qty(\va{r}_1)+\wavefunc\qty(\va{r}_2)]\qty[\wavefunc\qty(\va{r}_1)+\wavefunc\qty(\va{r}_2)]^\star}
    \nonumber \\
        &= I_1 + I_2 + 2\Re\csd\qty(\va{r}_1,\va{r}_2)
    % \nonumber
    \label{interference_csd}
    ~,
\end{align}
with the shorthand $I_1=\csd(\va{r}_1,\va{r}_1)$, i.e. the observed optical intensity at $\va{r}_1$, and similarly for $I_2$.
Henceforth, we will refer to optical intensity simply as ``intensity''.
The intensity of the superposition above is then the sum of the contributing waves' intensities, plus an interference term modulated by light's mutual coherence (highlighting the non-linearity of wave optics).

At optical frequencies the ensemble-averaging operator above can often be understood simply as \emph{time averaging}.
This is because the observation periods of our sensors (the eye or a camera) are very long compared to the light's (reciprocal) frequency, hence the instantaneous behaviour of the field ceases to matter.
If the wavefronts that arrive at points $\va{r}_1,\va{r}_2$ are not statistically similar, then constructive and destructive interference happens with similar probability and cancels out, thus $\csd(\va{r}_1,\va{r}_2)=0$ and no interference is observed.
Only when light is able to maintain similarity over the course of an observation, constructive or destructive interference dominates the interaction and observable interference may arise.
This highlights the difficulty of rendering with partially-coherent light: 
rapidly-fluctuating instantaneous interference happens all the time.
However, more often than not, these effects are averaged out and disappear during observation.
The CSD $\csd$ then quantifies the observable properties of light, and the quantities used by PLT for rendering are linearly related to the CSD \cite{Steinberg_practical_plt_2022}.
See \citet{wolf2007introduction,mandel1995optical} for additional discussions about optical coherence.


\paragraph{``Phase-space'' formulation}

Ever since Heisenberg's seminal work on uncertainty, published in 1927, significant effort has been made to restore to quantum mechanics and optics the classical pictorial view, under which position and momentum can be simultaneously considered on equal footing.
In 1932 Wigner formulated the \emph{Wigner distribution function} (WDF) \cite{Wigner1932}, which serves to define a position-momentum space (known as a ``phase space'') for quantum mechanics, a formulation that later found its natural way to optics \cite{Bastiaans:79}.
The WDF is a bilinear distribution of the wave function, and relates linearly to the CSD via a Fourier-like transform:
\begin{align}
    \wvd\qty(\va{r},\va{k}) 
    &\triangleq
        \frac{1}{\qty(2\mpi)^3}
        \int\dd[3]{\va{r}^\prime} 
        \csd\qty(\va{r} - \frac{\va{r}^\prime}{2}, \va{r} + \frac{\va{r}^\prime}{2})
        \ee^{-\ii \va{r}^\prime\cdot\va{k}}
    \label{def_WDF_csd}
    ~.
\end{align}
Clearly, this transform is invertible and the CSD may always be recovered from the WDF.

Because the WDF is a function of position and wavevector, by its very definition it gives rise to a \emph{position-momentum} space (the WDF's domain), and the dynamics in this space are governed by the WDF.
In the context of Hamiltonian dynamics, the pairs $(\va{r},\va{k})$ are often called canonical coordinates, and the position-momentum space, in which these coordinates live, is referred to as \emph{phase space}.
It is well known that the wave-optical phase space, induced by the WDF, fulfils most of the postulates expected of a classical (ray optical) phase-space density function \cite{testorf2010phase}.
The WDF then resembles a form of a classical ray.

For example, let $\wvd_\mathrm{i}$ be the WDF of light, and let $\wvd_\mathrm{o}$ be the WDF of that light after free-space propagation over a time period $t$. 
Then,
\begin{align}
    \wvd_\mathrm{o}\qty(\va{r},\va{k}) = \wvd_\mathrm{i}\qty(\va{r}-\frac{ct}{k}\va{k},\va{k})
    \label{WDF_freespace}
    ~,
\end{align}
where $c$ is the speed of light, hence $ct$ is the propagation distance, and $k=\frac{2\mpi}{\lambda}$ is the wavenumber.
That is, the dynamics of the WDF under free-space propagation echo the dynamics of a classical system of rays: a ray at position $\va{r}$ with direction of propagation proportional to $\va{k}$ (this ray is the point $(\va{r},\va{k})$ in phase space) indeed propagates in direction $\va{k}$ and its direction remains unchanged.
The WDF also reproduces the expected marginal distributions: for example, the observable intensity at a point $\va{r}$ is
\begin{align}
    I\qty(\va{r}) 
        &= \csd\qty(\va{r},\va{r})
         =
            \int\dd[3]{\va{k}} \wvd\qty(\va{r},\va{k})
    \label{marginal_distribution_position}
    ~,
\end{align}
i.e. we integrate in phase space over the frequency subspace, i.e.
over all possible $\va{k}$.

As an aside, note that the free-space transformation implied by \cref{WDF_freespace} is a horizontal shearing of the phase space, see \cref{fig_phase_space_transport} for an illustration.
Because a shearing transformation belongs to the special linear group (has unit determinant), the phase-space volume occupied by light on free-space propagation is unchanged, a direct of consequence of which is the well-known \emph{conservation of radiance} (see \cref{section_Liouville_supplemental} in our supplemental material).
Refer to our supplemental material for a more in-depth discussion of wave optics, the WDF and its properties, and the dynamics of the induced wave-optical phase space.


\subsection{Generalized Rays} \label{section_generalized_rays}

It has been shown that a phase-space distribution that fulfils all classical postulates (of a ray-optical phase-space distribution) does not exist \cite{Friberg_1979}.
Specifically, the WDF is not non-negative, restricting its interpretation as an energy density.
Using the WDF directly to describe light 
leaves one to contend with such negative energy densities.
Instead of using the WDF directly, we will study why these negative values arise, and use the WDF to identify the closest analogue to the classical ray, i.e. a Dirac delta in phase space.

The source of the WDF's negativity can be directly traced to the uncertainty relation \cite{Dragoman2000-bw}: 
we may not measure light at a perfectly localized point, but over the extent of a sensor, and over a physically-realizable sensor the WDF is indeed non-negative.
That is, an observable will always be sensitive to light over a positive extent in phase space---dictated by the uncertainty relation.
As uncertainty is the source of negativity, it is also well-known that an arbitrary WDF convolved with a minimum uncertainty Gaussian (a Gaussian in phase space that fulfils the equality in \cref{eq_uncertainty_relation}) does indeed become non-negative \cite{torre2005linear}.

\input{fig_gen_ray_phase_space.tex}

Then, we may ask: what is the closest, physical analogue to an idealized ray with position $\va{r}$ and a direction equivalent to wavevector $\va{k}$?
That would be this ray (i.e. a Dirac delta in phase space) convolved with a minimum uncertainty Gaussian, which yields a Gaussian centred at $(\va{r},\va{k})$ in phase space, and we term that Gaussian a \emph{generalized ray} (illustrated in \cref{fig_phase_space_ray}).
We provide a formal expression for that generalized ray in \cref{generalized_ray} in our supplemental material, however we use a simpler expression in our implementation (viz. \cref{def_generalized_ray}), for reasons that we will explain later.

The discretization process---known as ``quantization''---that promotes ray optics to wave optics (see our supplemental material) is the source of uncertainty, as well as of the negativity of the WDF.
It is helpful to imagine the wave-optical phase space not as a continuous space (like the classical ray-optical phase space), but as a space that is discretized into overlapping cells, each with a minimum-uncertainty Gaussian footprint.
Resolving light beyond a cell is not possible (known as the ``diffraction limit'' in imaging), due to the uncertainty relation.
A generalized ray then occupies a footprint of a single cell, and is thus the ``most local'' construct that is allowed under wave optics (similar to the quantum-optical \emph{squeezed coherent states} \cite{Leonhardt1997-xz}).
Because the WDF convolved with a minimum uncertainty Gaussian must be non-negative, we are formally guaranteed that over the support of a generalized ray (said equivalently: over the support of a single phase-space cell), the WDF integrates to a non-negative quantity.
We show so in \cref{subsection_rendering_Gaussian_beams} in our supplemental material.


\paragraph{On the nature of the wave function}

As an aside, we note that under quantum mechanics the wave function is understood as a probability amplitude, hence $\abs*{\wavefunc(\va{r})}^2$ is the spatial probability density of the represented particle.
Classical wave optics understands the wave function as the amplitude of the electric field disturbances over space, and $\abs*{\wavefunc(\va{r})}^2$ is proportional to the (instantaneous) optical intensity at $\va{r}$.
Because we time average, only the behaviour of the system under expectation is of interest to us (we only observe the averaged stimulus to a ``stream'' of photons, i.e. ensemble-averaged quantities as in \cref{csd}), hence the two interpretations are formally equivalent for our purposes.

Reproduction of quantum-optical effects is not the aim of this paper.
Nevertheless, ``quantum-like'' phenomena, such as the uncertainty between position and momentum, arise naturally in the context of classical wave optics (as discussed in \cref{section_wave_optics}), and this uncertainty is fundamental to our discussion: it is both the reason to why a classical-like point sample in phase space is prohibited, as well as the means by which we find the physical construct closest to such a point sample---the generalized ray.
While the term ``uncertainty'' refers to the impossibility of simultaneously specifying a photon's position and momentum with arbitrary precision, under our context we should understand the term simply as the inability to localize light energy both in its position and momentum (i.e. wavevector).


\paragraph{As photoelectric detection eigenstates}

In quantum mechanics, a minimum uncertainty Gaussian in phase space is known as a \emph{coherent state} (not to be confused with optical coherence and coherent light).
It is well known that such coherent states are the (only) eigenstates of the photon creation and annihilation operators, therefore coherent states correspond to observables of photoelectric detection \cite{mandel1995optical}.
Therefore, \emph{generalized rays should be understood as corresponding to the measurable (ensemble-averaged) response of photoelectric detectors}.
As essentially all of our sensors operate via the process of photoelectric detection, generalized rays are the natural construct to sample a sensor's sensitivity function, and hence the measurable wave-optical distribution of light.
We expand upon this discussion in \cref{subsection_coherent_states_genrays} in our supplemental.


\paragraph{As a coherent-modes decomposition}

We will now briefly show that our generalized rays can be understood as being mutually incoherent.
Let $\{\va{r}_n,\va{k}_n\}$ be a collection of generalized rays (each centred at $(\va{r}_n,\va{k}_n)$ in phase space), and let $\wvd_{\mathzapf{ray}}^{(n)}$ and $\csd_{\mathzapf{ray}}^{(n)}$ be the WDF and CSD of each ray, respectively.
In our supplemental material, we formally show that any measurable WDF (an arbitrary WDF convolved with a minimum uncertainty Gaussian, as discussed) can be written as a superposition of generalized rays:
\begin{align}
    \wvd
        =
            \sum_n 
            I_n
            \wvd_{\mathzapf{ray}}^{(n)}
    \label{wvd_decomposition}
    ~,
\end{align}
where $I_n>0$ is the intensity of each generalized ray.
That is, the phase-space distribution of light is decomposed into generalized rays.
Intuitively: this holds because Gaussians form an overcomplete functional basis, therefore any non-negative distribution can be written to arbitrary precision as a sum of finite Gaussians.

We may now recover the CSD of the WDF above, by applying the inverse of \cref{def_WDF_csd} (explicitly given in \cref{WDF_csd_inverse} in our supplemental material) and using its linearity (up to a constant):
\begin{align}
    \csd
        =
            \sum_n
            I_n
            \csd_{\mathzapf{ray}}^{(n)}
    \label{csd_decomposition}
    ~.
\end{align}
By linearity, each $\csd_{\mathzapf{ray}}^{(n)}$ is the CSD that corresponds to the generalized ray $\wvd_{\mathzapf{ray}}^{(n)}$.
Recall that CSDs superpose linearly if, and only if, the wave ensembles they describe are mutually incoherent, hence we may understand the decomposition of the WDF into generalized rays (\cref{wvd_decomposition}) as a decomposition of light into mutually-incoherent generalized rays, where each ray is perfectly coherent, i.e. a form of a \emph{coherent-modes decomposition} \cite{mandel1995optical}.

To summarise: 
A measurable WDF can always be written as an arbitrary function convolved with a minimum uncertainty Gaussian, because resolving details beyond the uncertainty relation (and thus, beyond a generalized ray) is prohibited.
Such a WDF must be non-negative, therefore may be written as a superposition of generalized rays, viz. \cref{wvd_decomposition}.
\cref{csd_decomposition} follows immediately from \cref{wvd_decomposition} via the definition of the WDF, and formally describes a coherent-modes decomposition of light into generalized rays. 
This process is not unique: there are infinitely many ways to write the decomposition in \cref{wvd_decomposition}, because Gaussians are an \emph{overcomplete} basis, and there are infinitely many wave ensembles that yield the same CSD.
However, because the CSD quantifies the observable properties of light (as discussed in \cref{section_wave_optics}), all these possible wave ensembles, as well as all possible ways to sample their measurable WDF with generalized rays, are identical in terms of their observable properties.


\paragraph{At optical frequencies}

A generalized ray is a very narrow Gaussian beam.
At optical frequencies and far from the source, the wavefront curvature of this beam is negligible, meaning that in such a setting we may readily treat it as a plane wave, and define:
\begin{align}
    \wavefunc_{\mathzapf{ray}}(\va{r}^\prime \argsep \va{r}, \va{k})
        &\approx
            \ee^{\ii \vartheta_0}
            \ee^{\ii \va{r}^\prime\cdot\va{k}}
    \label{def_generalized_ray}
    ~,
\end{align}
where $\vartheta_0$ is an irrelevant initial phase term. 
The above is the generalized ray wave function that we use in this work.

It is worth noting that while \cref{wavefunc_spectral_decomposition} describes a decomposition of an arbitrary wave function into plane waves, our formulation is starkly different: 
The plane waves that enter \cref{wavefunc_spectral_decomposition}:
\begin{enuminline}
    \item 
        cannot be assumed to be mutually-incoherent; and,
    \item 
        they extend throughout all space.
\end{enuminline}
That is, \cref{wavefunc_spectral_decomposition} does not imply a coherent-modes decomposition, and it serves to regain neither linearity nor locality.
A generalized ray admits a well-defined spatial extent, and facilitates a rigorous coherent-modes decomposition.
Understanding the wavefront of a generalized ray, at a local spatial region, as a plane wave is merely an excellent analytical approximation at optical frequencies.
In general, it should be remembered that a generalized ray is in-fact a narrow Gaussian beam.



\input{fig_generalized_rays_coherence.tex}

\subsection{A Bundle of Generalized Rays} \label{section_bunle_of_rays}

We will now explain the relation between optical coherence and a collection of generalized rays.
By inverting the transform that defines the WDF (given in \cref{WDF_csd_inverse_diff} in our supplemental material), it is easy to see that $\csd(\va{r}-\tfrac{1}{2}\va{x},\va{r}+\tfrac{1}{2}\va{x})$, as a function of $\va{x}$, is proportional to $\frft*{\wvd(\va{r},\va{k})}(\va{x})$, i.e. the Fourier transform of the WDF with respect to the wavevector $\va{k}$, while position $\va{r}$ is held fixed.
We may immediately conclude that optical coherence only depends on the light's momentum distribution in phase space.

More concretely, consider monochromatic light of wavelength $\lambda$, that propagates roughly in a preferred (mean) direction, say $\va{z}$.
We define a \emph{ray bundle} to be a collection of many generalized rays that compose this light (as discussed in \cref{section_generalized_rays}, a decomposition into generalized rays is always possible).
We term the \emph{diffusivity} of this bundle to be the variance of the geometric angular deviations of the generalized rays from the mean axis of propagation $\va{z}$.
Diffusivity, and its relation to optical coherence, is illustrated in \cref{fig_generalized_rays_coherence}.
The diffusivity is quantified via a $2\times 2$ positive-definite matrix $\matb{\Omega}$, allowing for anisotropy.
The variance in the solid angle into which the bundle propagates is then $\varOmega=\abs*{\matb{\Omega}}$.
Then, in our supplemental material we formally derive the following result:
\begin{align}
    \matb{\Theta} = \lambda^2 \matb{\Omega}^{-1}
    \label{coherence_diffusivity}
    ~,
\end{align}
where $\matb{\Theta}$ is the \emph{shape matrix} from PLT theory \cite{Steinberg_practical_plt_2022}.

A consequence of the above relation between optical coherence and the diffusivity of light is the well-known connection between the coherence area of light, i.e. $\abs*{\matb{\Theta}}$, and the solid angle subtended by a thermal source \cite{mandel1995optical}: $\abs*{\matb{\Theta}}=\tfrac{\lambda^2}{\varOmega}$.
Though note that the relation \cref{coherence_diffusivity} is more general, and serves as a direct connection between optical coherence and light's distribution in the wave-optical phase space, with no assumptions on the light sourcing process or its state-of-coherence at other regions in space.


%-------------------------------------------------------------------------

\input{fig_BSDFs.tex}

\section{Sample-Solve} \label{section_sample_solve}

To summarise:
It is useful to depict and discuss the distribution of light in a position-momentum ``phase space''.
For example, a sensor (a pixel in a camera) is sensitive to light arriving to a specific region in a space and from a specific range of directions; and, light-matter interactions (and the BSDFs that quantify them) are formulated in terms of position and light's incident and exitant directions.
Under the context of classical ray optics, the phase-space distribution is strictly non-negative, and we use perfectly localized, linear rays (points in phase space) in order to sample the behaviour of light.

As we transition to wave optics, the phase space becomes discretized, due to the uncertainty relation, and may only be queried at a limited resolution---the ``diffraction limit''.
Then, \emph{generalized rays}, which are thin beams that quantify the greatest possible resolution of measurement, replace the classical ray as the means to perform point sampling in phase-space.
We have explained why these generalized rays superpose linearly (as they are mutually incoherent), and that, while the wave-optical phase-space distribution may take negative values, generalized rays will always carry non-negative intensities.


\input{fig_sample_solve.tex}

\paragraph{Interaction of generalized rays with matter}

Interactions are partitioned into a pair of types:
\begin{enumerate}
    \item \textbf{Simple} ---
            These arise with \emph{linear optical systems}: propagation through a homogeneous medium with a slowly-varying refractive index (including free space), and reflection or refraction at a smooth interface.
            In our supplemental material, we explain that under simple interactions the dynamics of generalized rays are identical to ray-optical dynamics. 
            That is, the generalized ray mean $(\va{r},\va{k})$ behaves just as a classical ray with position $\va{r}$ and direction proportional to $\va{k}$ would.
    \item \textbf{Diffractive} ---
            These are all other interactions, e.g., scattering by a rough surface.
            A \emph{diffractive BSDF} $\mat{f}(\va{r},\va{k}_\mathrm{i},\va{k}_\mathrm{o})$ of a diffractive interaction quantifies the coherent, wave-optical light-matter interaction, at a position $\va{r}$ and with wavevectors $\va{k}_\mathrm{i},\va{k}_\mathrm{o}$ of the incident and scattered generalized ray, respectively.
\end{enumerate}
As we use the simplified expression in \cref{def_generalized_ray} for the wave function of a generalized ray, i.e. a plane wave, a diffractive BSDF $\mat{f}$ solves the electromagnetic problem of a plane wave scattered by matter.
Note: the wavelength-dependence of the BSDF is implicitly captured by the wavevectors (recall $\abs*{\va{k}}=\tfrac{2\mpi}{\lambda}$).
See \cref{section_interactive_rendering} for more details about the diffractive BSDF.

The diffractive BSDF describes perfectly-coherent interactions.
We may formulate partially-coherent interactions by considering the interaction of a ray bundle with matter, because
the bundle's diffusivity is the (inverse) spatial coherence, as formalised in \cref{section_bunle_of_rays}.
This interaction of a bundle with matter is quantified by $\mat{f}$ summed over all $\va{k}_\mathrm{i}$ in the ray bundle (illustrated in \cref{fig_BSDFs}).
When there are very many generalized rays in the bundle, this summation becomes integration, and we understand a partially-coherent BSDF as the diffractive BSDF $\mat{f}$ convolved with the probability density of the wavevectors in the bundle.
Such a convolved BSDF takes a very similar form to the coherence-aware BSDF presented by \citet[Section 3.5]{Steinberg_practical_plt_2022} (our derivations serve as an alternative, and more general, proof of their result).


\input{fig_transport_phase_space.tex}

\input{fig_MS.tex}
\input{fig_FC.tex}

\paragraph{Sample}

We may now present the first stage of our \emph{sample-solve} algorithm, see \cref{fig_sample_solve} for an overview.
A measurable distribution of light (the convolved WDF discussed in \cref{section_generalized_rays}) may always be sampled via generalized rays.
The observable irradiance measured by a sensor is then the sum of all generalized rays that fall upon the sensor's sensitivity region in phase space (e.g., the Cartesian product of a pixel's spatial extent times its acceptance solid angle).
From these generalized rays, we may sample a ray using classical techniques, for example uniformly, or importance sample with respect to the sensor's sensitivity function.
We may do so because generalized rays are mutually incoherent, and hence superpose linearly in terms of their radiometric units.

Generalized rays traverse free space and homogeneous media in an identical fashion to classical, ray-optical rays (as these constitute simple interactions).
Therefore, tracing generalized rays through space, tracing Eikonals through media with a slowly-varying refractive index, and reflecting or refracting at a smooth interface between media remains unchanged.
We make the assumption that the spatial extent of a generalized ray is smaller than the scene's geometric details, therefore we may use ray tracing to propagate generalized rays.
A similar assumption is made by PLT, though note that a generalized ray will typically have a smaller spatial extent compared to PLT's partially-coherent beams.

Finally, sampling diffractive interactions is done by importance sampling the coherent diffractive BSDFs that quantify these interactions.
While classical BSDFs and diffractive BSDFs describe different physics, both fulfil similar postulates: they are non-negative density functions, energy is conserved over all scattering directions, and Helmholtz reciprocity is obeyed.
Once an expression for a particular diffractive interaction is derived, seeking an importance sampling strategy for the resulting diffractive BSDF is no different from the classical case.

We stress that we are able to seamlessly and accurately transition between propagating a generalized ray through simple interactions to importance sampling diffractive interactions, because the generalized ray admits a well-defined wave function (unlike the classical ray, for which there is no clear way to define diffractive interactions).
The above highlights again how generalized rays serve as the wave-optical analogue of the classical rays: sampling remains mostly unchanged, compared with classical formulations, with the primary difference being that we replace classical BSDFs with coherent diffractive BSDFs.
To demonstrate that we are able to apply advanced sampling techniques for our wave-optical path tracing, we have implemented manifold sampling, see \cref{fig_MS}.


\paragraph{Solve}

The sample stage above, as described, enables by itself accurate rendering.
Though, at a price: we sample perfectly-coherent interactions.
Coherent interactions can be imagined as illumination by a laser, giving rise to high-contrast, coherent optical speckle with rapidly-fluctuating intensities. 
As we sample many generalized rays propagating into different directions, we sample the angular spectrum of light---thus its optical coherence---correctly reproducing the \emph{observable}, partially-coherent light transport through the scene.
Hence, we may say that rendering via generalized rays is unbiased, but admits higher variance, see \cref{fig_FC}.

To combat this high variance, we \emph{solve} for the partially-coherent transport over a path that was sampled in the sample stage.
To do so we apply PLT, essentially unmodified.
That is, while in the sample stage we trace a single generalized ray, the solve stage should be understood as tracing a ray bundle consisting of many generalized rays, all originating from the sampled light source, and diverging only a little in their direction.
Our exact relation between the bundle's diffusivity and optical coherence then serves as the formal link between sampling and solving for the partially-coherent transport, and ensures that PLT can be accurately and rigorously applied.
See \cref{fig_phase_space_transport} for an illustration.

In our domain of interest---rendering with partially-coherent optical light---applying PLT makes sense:
The optical coherence of light serves as the primary factor that limits our ability to resolve diffraction effects, because partial coherence induces blurring (i.e. a convolution) of light-matter interactions.
PLT then acts as a \emph{variance-reduction technique}, by computing the observable optical phenomena over a sampled path, thereby eliminating the variance that arises due to the coherent sampling.

Other applications might find a different \emph{solve} stage to be more appropriate:
As an example, an SBR method (see \cref{section_related_work}) might apply the \emph{sample} stage, as described, to sample multiple paths that connect a source to a particular object, or an exit aperture.
For the \emph{solve}, the SBR method would remain virtually unchanged, and use these sampled paths in order to determine the induced electric currents on the object's surface, or compute the exit field's wavefront.
Such applications are beyond the scope of this paper, however they serve to highlight the generality and strength of our simple sample-solve approach: it bridges the gap between classical path tracing tools and wave optics, via the generalized ray concept, and enables the application of powerful sampling techniques in a much wider context.


\subsection{Validity domain and limitations} \label{section_validity_domain}

We now briefly discuss the validity domain and enumerate the assumptions we make in this work.
\begin{enumerate}
    \item \textbf{Ensemble averaging}.
            This is the core assumption that enables us to achieve mutual incoherence (formalised in \cref{csd_decomposition}), and thus linearity, between generalized rays.
            For wave-optical rendering, we are only interested in ensemble-averaged---i.e. \emph{observable}---effects.
            In other contexts, where sensors may no longer be understood to ensemble average, the instantaneous values of the fields come into play, and no linearity, at least in terms of radiometric units, can be regained.
    \item \textbf{Spatial extent of a generalized ray}.
            Unlike classical rays, generalized rays have a positive spatial extent.
            We approximate the propagation of a generalized ray via ray tracing, i.e., ignore its spatial extent, and point sample the wave-optical phase space.
    \item \textbf{Optical far field}.
            We make the far field assumption in order to simplify the expression for a generalized ray's wave function (\cref{def_generalized_ray}).
            This is also a good approximation at optical frequencies, and it serves to significantly simplify the analysis of deriving analytic expressions for diffractive BSDFs, as well as importance sampling these BSDFs.
            Nevertheless, sample-solve, per s\=e, does not require any far-field assumptions, and can be formulated using the exact expression for the generalized ray's wave function (\cref{generalized_ray}).
\end{enumerate}

The second assumption above is worth further discussion. 
When a generalized ray is partially occluded by matter, or falls upon the boundary between different matter (i.e., objects with different materials), interference between the different interactions arises.
For example, on partial occlusion, interference between the matter interaction (of the partially occluded energy) and free-space propagation (of the unoccluded energy) results in free-space diffraction: diffracted lobes that ``bend'' around the matter.
We ignore these \emph{edge effects} in this work, and leave these for future work.

Recall that, as a generalized ray occupies a single wave-optical phase-space cell, resolving features smaller than a generalized ray is not possible.
That is to say, geometrical reasoning does not apply beyond the limit of uncertainty, and a generalized ray cannot be broken up into smaller ``parts''.
Hence, on partial occlusion, part of the entire generalized ray's energy, proportional to the cross-sectional area masked by matter, interacts with that matter (and not only the occluded geometric region).
Therefore, as we sample many such generalized rays, we correctly reproduce the interactions of these rays with distinct materials.
Only the interference between different interactions is neglected.
Also note, that in contrast to partially-coherent beams (employed by previous work), whose geometric and coherence properties change on partial occlusion, generalized rays always remain perfectly coherent.

To conclude, making the assumption that we ignore the spatial extent of generalized rays is equivalent to ignoring edge effects: the diffracted lobes of free-space diffracted generalized rays; or the interference that arises between different materials.
At optical frequencies, where the spatial extent of a generalized ray is very small, these edge interactions constitute only a small fraction of the interactions of light with matter (indeed, we very rarely observe free-space diffraction, but we do observe wave effects on interaction with matter).
We provide additional formal discussion on the propagation of generalized rays in \cref{subsection_propagation_genrays} in our supplemental.


\input{fig_PC.tex}

\paragraph{Validation}

We stress that sample-solve is derived under a highly general validity domain.
Up to the assumptions listed above, correctness is formally ensured (see our supplemental material):
The measurable effects of any physically-realizable wave-optical phase space distribution can always be written, to arbitrary precision, as a finite sum of generalized rays (because Gaussians form an overcomplete functional basis).
These generalized rays then act as ``point queries'' of light's behaviour, in a manner very similar to the classical case.
These assumptions above are also less restrictive than the state-of-the-art.
As a matter of fact, we extend PLT's applicability to light of any state-of-coherence: PLT assumes linearity due to light's partial coherence, while our generalized rays always remain linear, and hence sampling is unconstrained by light's coherence properties.
We understand partially-coherent light simply as a ray bundle, and ray bundles composed of distinct generalized rays must be mutually incoherent as well.

As a numeric validation of our implementation, we note that both perfectly-coherent transport, viz. \cref{fig_FC}, and partially-coherent sampling, viz. \cref{fig_PC}, produce identical results to the presented sample-solve, as expected.


\paragraph{Comparison with the state-of-the-art}

As discussed, the state-of-the-art \cite{Steinberg_practical_plt_2022} samples partially-coherent BSDFs.
Because the coherence of light cannot be known a priori when tracing paths backwards, a practical global lower limit on the coherence of light is set.
Partially-coherent BSDFs are then sampled with respect to that limit.
We emulate such partially-coherent sampling in our renderer, and compare the performance in a simple scene, see \cref{fig_PC}.
The difference is dramatic: we observe over a thousand-fold increase in convergence performance when sampling with generalized rays compared to partially-coherent sampling.

We also compare directly to \citet{Steinberg_practical_plt_2022} using the same scene, see \cref{fig_mitsuba}.
An increase in performance is expected, as our implementation is GPU accelerated, nevertheless, we record up to a several thousand speed gain for a similar-quality rendering (for image areas that admit diffractive materials).
Our equal-sample convergence performance is increased by a factor of 1 to 8.
This is despite the fact that their path tracer is bidirectional (ours is unidirectional) and each sample propagates 64 uniformly sampled spectral samples (compared to the up to 4 per sample with our renderer), therefore they do a considerably greater amount of work per sample.
\cref{fig_PC,fig_mitsuba} highlight the sampling problem and emphasize the need for better solutions: the state-of-the-art is able to achieve decent results when diffractive materials are directly illuminated, making good use of a bidirectional path-tracer.
However, it struggles greatly when it isn't entirely trivial to connect paths from a light source to these materials.


%-------------------------------------------------------------------------

\input{fig_mitsuba.tex}

\section{Wave-Optical Rendering} \label{section_interactive_rendering}

We note a consequence of our contributions: 
Given a classical path tracer, the minimal change required to make that path tracer compatible with wave optics is replacing classical rays with generalized rays as the ``point queries'' of light's behaviour.
In practice, doing so requires essentially only two changes:
\begin{enuminline}
    \item transitioning to a spectral, vectorized (polarization-aware) path tracer; and 
    \item reformulating all the BSDFs in terms of generalized rays.
\end{enuminline}
The former is simply a consequence of the fact that generalized rays and wave-optical BSDFs depend on light's wavelength and polarization.

Sample-solve takes a step beyond the bare minimum, and adds an additional pass that serves the role of a variance reduction method.
This forward pass applies PLT in order to solve for the partially-coherent transport over the sampled path, and, as long as closed-form expressions for the partially-coherent BSDFs have been formulated, is very cheap.
All images and videos in this paper and supplemental material were rendered using our unidirectional path tracer that implements our sample-solve approach.
Our implementation is available with our supplemental material.
In this section we will give a brief overview of our implementation.

As generalized rays are defined in (paraxial) phase space---the space spanned by the position and wavevector variables---their energy density is described in terms of flux per area per solid angle, i.e. units of the radiometric radiance.
We therefore continue to use radiance as the primary quantity that describes energy transport throughout the scene.
To formalize a generalized ray's polarimetric properties, we use Stokes parameters vectors (carrying units of radiance).


\paragraph{Initiating paths}

Consider a sensor.
The sensor's sensitivity function $\mathcal{S}(\va{r},\va{k})$ is a phase-space function that quantifies the sensor's observable response to radiation.
As discussed in \cref{section_generalized_rays}, for this function to be measurable, it must satisfy the uncertainty relation (\cref{eq_uncertainty_relation}), in the sense that it is possible to write it formally as an arbitrary function convolved with some minimum-uncertainty Gaussian.
Then, it is always possible to write this sensitivity function as some superposition of generalized rays, viz.
\begin{align}
    \mathcal{S}\qty(\va{r},\va{k})
        =&
            \sum_n 
            s^{(n)}
            \wvd_{\mathzapf{ray}}^{(n)}
    ~,
\end{align}
where $s$ is sensitivity, and $\wvd_{\mathzapf{ray}}$ is the WDF of a generalized ray.
That is, the sensor's extent in phase space is covered with generalized rays, and this is always possible, because a sensor must adhere to the uncertainty relation.

Each such generalized ray will serve as a single \emph{sample} of the transport that light undergoes prior to arriving at the sensor.
Denote $\va{r}^{(n)}_0,\va{k}_0^{(n)}$ as the mean positions and incident wavevectors (incident direction scaled by the wavenumber), at the sensor, of these generalized rays, where we use superscripts to indicate sample indices and subscripts for indices along a sampled path.
In practice, positions and incidence directions are sampled with respect to the sensitivity function $\mathcal{S}$, in an identical manner to the classical setting.
A wavelength for each sample is also needed (our implementation draws a random wavelength uniformly from the visible spectrum).
This wavelength drives the path sampling process, hence we refer to it as the ``Hero wavelength'' \cite{Wilkie2014-yg}.


\paragraph{Sample stage}

This stage applies standard unidirectional path tracing from the sensor (i.e., backward path tracing).
Once a generalized ray's terminal phase-space position has been sampled, we proceed with retracing the steps light took before arriving to the sensor, i.e. backward path tracing.
The temporal dynamics of a generalized ray (see \cref{section_wave_optics_supplemental} in our supplemental) are entirely deterministic, hence time reversal and backward propagation are well defined.
For backward propagation through free space (and other simple interactions, as discussed in \cref{section_sample_solve}), a generalized ray retains the classical dynamics, and we simply ray trace the generalized ray's mean position through space.

Once a generalized ray encounters matter, we attempt to connect the path to a light source via next-event-estimation (NEE).
We also perform ``Russian roulette'' early path termination.
To continue the path, we importance sample the diffractive BSDF that governs the diffractive interaction at the interaction region, yielding the next pair $\va{r}^{(n)}_j,\va{k}_j^{(n)}$, with $j\geq 1$ being the interaction index from the sensor.
Sampling then proceeds recursively.
Diffractive BSDFs will be discussed later.

As a design choice, our implementation ignores polarization during the sample stage.
This is done for simplicity and performance: decomposing and inverting Mueller matrices, and other polarimetric interactions, can be cumbersome and expensive.
No error is introduced, as the solve stage is fully vectorized.


\input{fig_snake.tex}

\paragraph{Solve stage}

Every time a path has been connected to a light source, either organically or via NEE, the solve stage kicks in to compute the partially-coherent transport over the sampled path.
If the sampled path does not contain dispersive delta segments (e.g., refraction through a smooth dielectric interface), in addition to the Hero wavelength, we also importance sample 3 additional wavelengths from the light's emission spectrum.
We therefore transport one to four spectral samples per sampled path.
Because the solve stage applies PLT \cite{Steinberg_practical_plt_2022} in an essentially unchanged manner, we only provide a brief overview here.

To source partially-coherent light, i.e. a ray bundle, from a light source, we need to quantify its coherence properties (for each spectral sample).
This depends on the light source:
\begin{enumerate}
    \item Distant light sources ---
        These include analytic distant sources, with a predefined solid angle that they subtend from the scene, as well as environment maps, in which case light is sourced from a small cluster of a few pixels, which define the solid angle.
        The coherence shape matrix $\matb{\Theta}$ is then immediately defined via \cref{coherence_diffusivity}.
        Because these are distant emitters, the coherence shape matrix does not depend on the distance of propagation from the source, and only undergoes transformations due to interactions.
    \item Emissive geometry light sources ---
        We source light from a small area on the emitting triangle.
        In our implementation we set this area to a constant $a=\SI{10}{\milli\metre^2}$.
        Then, the solid angle subtended by the sourcing area at the first interaction region is $\varOmega={a}/{r_1^2}$, where $r_1$ is the distance from the source to that first interaction point.
        The coherence shape matrix, for the entire path, can then be written as
        \begin{align}
            \matb{\Theta} = \frac{r^2}{\lambda^2} \matb{\Omega}^{-1}
            ~,
        \end{align}
        where $r$ is the total distance of propagation from the source to each interaction region, and $\lambda$ is the wavelength of light, as before.
        $\matb{\Omega}$ is the $2\times 2$ matrix that quantifies the partially-coherent ray bundle's angular extent.
        Initially, we set $\matb{\Omega}=a\mat{I}$ and afterwards $\matb{\Omega}$ transforms on interactions.
\end{enumerate}

Let $\va{r}^{(n)}_l,\va{k}^{(n)}_l$ be the mean position and wavevector, at the light source, of the sourced ray bundle, with $l>0$ being the total path length and $l-1$ the number of interactions along the path.
We propagate this ray bundle over the sampled path, evaluate the partially-coherent BSDFs at each interaction in forward order, i.e. over interaction indices $l-1,l-2,\ldots,1$.
At each interaction we transform the coherence shape matrix, as described in \citet[Section 3.5]{Steinberg_practical_plt_2022}.
Finally, the ray bundle arrives at the sensor, and contributes to the observed intensities with respect to the (wavelength-dependant) sensor's sensitivities $s^{(n)}$.
We also perform multiple importance sampling.

We make a small adjustment compared to PLT: we assume the coherence shape matrix for the entire ray bundle (i.e. both the s- and p-polarized transverse components) is the same.
This is accurate (because, as discussed, the ``correct'' coherence of light will be sampled naturally as we sample the diffusivity of light), and slightly simplifies the implementation.


\paragraph{Diffractive BSDFs}

A generalized rendering equation has been formulated by \citet[Definition 4.1]{Steinberg_practical_plt_2022} for PLT.
This equation acts upon partially-coherent beams via a \emph{diffraction operator}.
However, because generalized rays are perfectly coherent, and always remain so, by construction, and because we ignore a generalized ray's spatial extent, our formalism enables making a major simplification: the diffraction operator is no longer coherence-dependant, and its only affect on a generalized ray is modulating its energy density.
That is, the diffraction operator reduces to a multiplication by the diffractive BSDF, and the rendering equation for generalized rays takes a form starkly similar to its classical counterpart:
\begin{align}
    \va{L}_\mathrm{o}\qty(\va{r},\va{k}_\mathrm{o}) 
        =& 
            \va{L}_\mathrm{e}\qty(\va{r},\va{k}_\mathrm{o}) 
            +
            \int \dd[3]{\va{k}_\mathrm{i}}
                \mat{f}\qty(\va{r},\va{k}_\mathrm{i},\va{k}_\mathrm{o})
                \va{L}_\mathrm{i}\qty(\va{r},\va{k}_\mathrm{i})
    \label{rendering_equation}
    ~,
\end{align}
where $\va{L}$ is the (scattered, emitted or incident) Stokes parameters vector (with units of radiance) quantifying a generalized ray, at position $\va{r}$, with scattered wavevector $\va{k}_\mathrm{o}$.
$\mat{f}$ is the diffractive BSDF: a Mueller matrix that acts upon the Stokes parameters vectors.

In the rendering equation above we integrate over the entire frequency subspace of phase space, i.e. over all incident wavevectors $\va{k}_\mathrm{i}$.
As a wavevector quantifies both propagation direction and wavenumber $\abs*{\va{k}}=\tfrac{2\mpi}{\lambda}$, the diffractive BSDF $\mat{f}$ becomes a function of incident and scattered directions, as well as incident and scattered wavelengths.
Cross-wavelength scattering, i.e. when $\abs*{\va{k}_\mathrm{o}}\neq\abs*{\va{k}_\mathrm{i}}$, e.g., due to fluorescence or phosphorescence, can be formulated then, however we ignore such effects in our implementation.
When restricted to same-wavelength scattering, the integral above reduces to a classical-like integral over a (scaled) unit sphere.

The discussion above serves to highlight how the diffractive BSDF is the wave-optical analogue of the classical BSDF, and aims to solve the electrodynamics problem that answers the question: 
given a generalized ray, with wavevector $\va{k}_\mathrm{i}$, impinging upon the matter, how much energy is scattered into a generalized ray with wavevector $\va{k}_\mathrm{o}$?
The problem is simplified when we make the discussed far-field assumption, where the wavefront of the generalized ray becomes a plane wave.

Because the diffractive BSDF is reduced to a well understood and studied problem, we may apply well-known electromagnetism to devise a diffractive BSDF.
As a few examples, our implementation supports the following diffractive BSDFs:
\begin{itemize}
    \item 
        Thin-film and structured multi-layered interference, used for the Bornite ore and beetle in \cref{fig_teaser}, employ well-known solutions to electromagnetic scattering from layered structures \cite{orfanidiselectromagnetic}.
    \item 
        Diffraction gratings \cite{Born1999principles}.
        We implement one- or two-dimensional, sinusoidal, rectangular or triangular gratings of a dielectric or conductive surface. Used for \cref{fig_teaser,fig_MS,fig_PC,fig_bike,fig_snake}.
    \item 
        Dielectric or conductive statistical surfaces of arbitrary roughness (all scenes).
        We employ existing surface scatter theories, specifically the Harvey-Shack \cite{Krywonos2006} model, which relates the surface power spectral density to the BSDF (see \citet{Banon2019-ow} for a derivation that connects such models to electromagnetism).
        This model was also investigated by \citet{Holzschuch2017}, and we use their importance sampling strategy.
        As we deal with vectorized quantities, the reflectivity coefficients in the model are replaced with the Fresnel coefficients in Mueller matrix form.

        For conductors, the refractive indices of such surfaces are measured, spectrally-tabulated values.
        For dielectrics, we use the simple Cauchy dispersion equation \cite{Gooch2011}:
        \begin{align}
            \eta\qty(\lambda) =& A + \tfrac{1}{\lambda^2}B
            ~.
        \end{align}
        For example, the prism in \cref{fig_MS} admits a dispersion of $B=0.005$ with a (mean) refractive index of $\eta(\SI{550}{\nano\metre})=1.2$.
    \item 
        Idealised diffuse surfaces.
        Used in the CD scene, \cref{fig_PC,fig_mitsuba} (for backward compatibility with \citet{Steinberg_practical_plt_2022}), and some of the materials in \cref{fig_bike}.
        Lambertian surfaces that are assumed to act as perfect depolarizers.
\end{itemize}

Finally, the partially-coherent form of the BSDF (used for the solve stage) is the convolution of the diffractive BSDF with light's diffusivity (i.e., its coherence properties).
This was rigorously shown by \citet{Steinberg_practical_plt_2022}, however our relation between coherence and diffusivity (viz. \cref{coherence_diffusivity}) is both more general and simpler to understand: the partially-coherent BSDF is the (perfectly-coherent) diffractive BSDF integrated over all incident directions of the ray bundle, i.e., its diffusivity.
Unlike previous work, we do not require exact expressions for a partially-coherent BSDF: because partial coherence is naturally reproduced as we sample light's diffusivity.


\input{fig_bike.tex}

\subsection{Results} \label{section_results}

Our results are comprised of three main scenes: 
\begin{enumerate}
    \item 
        \textbf{Snake enclosure}, \cref{fig_teaser,fig_snake}.
        This scene is illuminated by multiple light sources: the sun, the sky (diffused sunlight), as well as a pair of industrial \SI{4100}{\kelvin} fluorescent lamps with a decent colour rendering index of 82 located at the back of the enclosure. 
        Sunlight and skylight arrive from the opening at the top.
        As most of the light that arrives at the different diffractive materials is indirect, this is a difficult scene to render.
    \item 
        \textbf{Manifold sampling}, \cref{fig_MS}.
        A highly-detailed scene that we use as our manifold-sampling playground.
    \item 
        \textbf{Bike}, \cref{fig_bike}.
        Adapted from \citet{Steinberg_practical_plt_2022}, however appearance is not expected to match, as our materials are different.
\end{enumerate}
Our supplemental material contains additional renderings, as well as animated videos of all these scenes, showcasing the performance of our method.
In addition to the above, the CD scene is used for analysis of partially-coherent sampling, \cref{fig_PC}, and comparison to the state-of-the-art, \cref{fig_mitsuba}.

Performance metrics, i.e. rendering resolution and samples-per-pixel (spp) count are given in each figure, and summarised in \cref{table_performance}.
For the paper figures we used high-quality, converged (i.e., very high samples-per-pixel) renders.
Nevertheless, our method and implementation enable interactive wave-optical rendering at 1 spp, and the frame times for interactive rendering are also given in the figures and in \cref{table_performance}.
Similar to other modern GPU-accelerated path tracer, we use a denoiser for interactive rendering.
This enables generating acceptable images at 1 spp, and allows the user to interact with and edit the scene in real-time.
The videos in our supplemental material were also rendered using a denoiser.

Low-spp images of the bike scene, with and without the denoiser, are shown in \cref{fig_bike} (bottom).
Low sample count is sufficient for the vast majority of the materials, including the diffractive birefringent dielectrics.
An exception is the dispersive diffraction lobes, which are visible on the floor, and arise due to diffraction grated wheel brake surface.
This is due to a couple of reasons: 
\begin{enuminline}
    \item 
        A diffraction grating scatters into many wavelength-dependant lobes, hence requires a moderate amount of spectral samples.
    \item 
        These are rendered via manifold sampling (MS), as discussed in \cref{fig_MS}.
        However, MS is only initiated during the sample stage when a path is scattered from the floor into the grating.
        The probability of finding such a connection organically is rather low (roughly about 1 in a few dozen), as the grated surface is quite small.
\end{enuminline}
The difficulty of rendering the diffracted lobes of a diffraction grating is also analysed in \cref{fig_MS} (bottom).
This problem is similar to the classical problem of rendering dispersive caustics, e.g., on the bottom of a pool with a disturbed water surface; a problem where classical unidirectional path tracers struggle as well.
We highlight again that our formalism reduces wave-optical rendering problem to classical sampling problem, to which classical tools apply.


%-------------------------------------------------------------------------

\section{Conclusion} \label{section_conclusion}

The generalized ray---being a local and linear descriptor of light---serves as the tool that allows us to apply classical sampling and path tracing techniques, in order to sample the wave-optical phase space. 
As that generalized ray admits a well-defined wave function, its electrodynamics and diffractive interactions with matter are also well-defined.
Thereby, the generalized ray spans a link between path tracing and computational optics, formalised via our sample-solve method.
This serves as our primary contribution.
We have also shown that generalized rays sample the partial coherence of light naturally: as we sample the angular spectrum of light.
Therefore, the partially-coherent analysis that happens in our solve step does not need to be exact, and useful simplifications can be made.
This is important, because designing partially-coherent BSDFs, as mandated by PLT, can be analytically difficult, and being able to make analytic approximations is of real practical value. 

\input{fig_performance.tex}

We showcase the power of our formalism, by applying sample-solve for wave-optical rendering.
For our use case, the solve stage is used as a variance-reduction technique that applies PLT in order to solve for the partially-coherent transport (i.e., a ray bundle) over a sampled path.
As a consequence of our generalized ray formalism, the challenges faced by designing such a wave-optical path tracer, e.g., sampling, importance sampling strategies for diffractive BSDFs, challenges imposed by spectral rendering, and effects like fluorescence, are essentially all classical in nature.
That is, electromagnetism needs to be explicitly considered only when devising diffractive BSDFs.
Furthermore, we show that we are able to achieve interactive wave-optical rendering of complex scenes, at a performance that is orders-of-magnitude faster than the state-of-the-art.
Our formalism is highly general, with a validity domain that subsumes the state-of-the-art.



%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
% Bibliography

\bibliographystyle{ACM-Reference-Format}
\bibliography{paper}






%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
% Appendix

\appendix


%-------------------------------------------------------------------------


\end{document}
