\section{Unambiguous Proof Trees}\label{sec:unambiguous-trees}
%

Although non-recursive and minimal-depth proof trees form central classes that deserve our attention, there are still proof trees from those classes that can be classified as counterintuitive. More precisely, we can devise proof trees that are both non-recursive and minimal-depth, but they are ambiguous concerning the way some facts are derived.
%Here is an example that illustrates this phenomenon.

\begin{example}\label{exa:unambiguous-trees}
	Let $Q = (\dep,A)$, where $\dep$ is the Datalog program that encodes the path accessibility problem as in Example~\ref{exa:proof-tree}. Consider also the database
	\[
	D\ =\ \{S(a),S(b),T(a,a,c),T(b,b,c),T(c,c,d)\}.
	\]
	The following is a proof tree of the fact $A(d)$ w.r.t.~$D$ and $\dep$ that is both non-recursive and minimal-depth, but suffers from the ambiguity issue mentioned above:
	
	\centerline{
	\includegraphics[width=.48\textwidth]{proof-tree-3.pdf}}

	\noindent Indeed, there are two nodes labeled with the fact $A(c)$, but their subtrees differ, and thus, it is ambiguous how $A(c)$ is derived. Hence, the database $D$, which belongs to the why-provenance of $(d)$ w.r.t.~$D$ and $Q$ relative to non-recursive and minimal-depth proof trees
	%$\nrwhy{(d)}{D}{Q}$ and $\mdwhy{(d)}{D}{Q}$ 
	due to the above proof tree, might be classified as a counterintuitive explanation since it does not correspond to an intuitive derivation process where each fact is derived once due to an unambiguous  reason. \hfill\markfull
	%Observe also that in whatever way we try to convert the above proof tree into an unambiguous one, the resulting tree will have a smaller support. In fact, $\unwhy{(d)}{D}{Q}$ consists of the sets of facts $\{S(a),T(a,a,c),T(c,c,d)\}$ and $\{S(b),T(b,b,c),T(c,c,d)\}$, which is what one expects as conceptually meaningful explanations for the tuple $(d)$. \hfill\markfull
\end{example}


The above discussion leads to the novel class of unambiguous proof trees, where all occurrences of a fact in such a tree must be proved via the same derivation.


%Recall that, for two labeled rooted trees $T$ and $T'$, $T \eqtree T'$ means that $T$ and $T'$ are isomorphic (respecting also the node-labels). Recall also that, for a node $v$ in $T$, $T[v]$ is the subtree of $T$ rooted at $v$.


\begin{definition}[\textbf{Unambiguous Proof Tree}]\label{def:unambiguous-proof-tree}
	Consider a Datalog program $\dep$, a database $D$ over $\esch{\dep}$, and a fact $\alpha$ over $\sch{\dep}$. An {\em unambiguous proof tree of $\alpha$ w.r.t.~$D$ and $\dep$} is a proof tree $T = (V,E,\lambda)$ of $\alpha$ w.r.t.~$D$ and $\dep$ such that, for all $v,u \in V$, $\lambda(v) = \lambda(u)$ implies $T[v] \eqtree T[u]$. \hfill\markfull
\end{definition}


Considering again Example~\ref{exa:unambiguous-trees}, we can construct an unambiguous proof tree of $A(d)$ w.r.t.~$D$ and $\dep$ by simply replacing the subtree of the second child of $A(d)$ with the subtree of its first child (or vice versa).
%
Now, why-provenance relative to unambiguous proof trees is defined as expected: for a Datalog query $Q = (\dep,R)$, a database $D$ over $\esch{\dep}$, and a tuple $\bar t \in \adom{D}^{\arity{R}}$, the {\em why-provenance of $\bar t$ w.r.t.~$D$ and $Q$ relative to unambiguous proof trees} is the family
\begin{multline*}
\{\support{T} \mid T \text{ is an unambiguous proof tree of }\\
R(\bar t) \text{ w.r.t. } D \text{ and } \dep\}
\end{multline*}
denoted $\unwhy{\bar t}{D}{Q}$.
%
Considering again Example~\ref{exa:unambiguous-trees}, $\unwhy{(d)}{D}{Q}$ consists of $\{S(a),T(a,a,c),T(c,c,d)\}$ and $\{S(b),T(b,b,c),T(c,c,d)\}$, which is what one expects as conceptually intuitive explanations for the tuple $(d)$, unlike the whole database $D$.
%
The algorithmic problems
\[
\mathsf{Why\text {-}Provenance_{UN}[C]} \quad \text{and} \quad  \mathsf{Why\text {-}Provenance_{UN}}[Q]
\] 
are defined in the expected way. We can show that the data complexity of why-provenance remains unchanged.

\def\theunambiguouscomplexity{
	The following hold:
	\begin{enumerate}
		\item $\mathsf{Why\text {-}Provenance_{UN}[C]}$ is \NP-complete in data complexity, for each class $\class{C} \in \{\DAT,\LDAT\}$.
		\item $\mathsf{Why\text {-}Provenance_{UN}[\NRDAT]}$ is in $\ACZ$ in data compl.
	\end{enumerate}
}
\begin{theorem}\label{the:complexity-unambiguous-proof-trees}
	\theunambiguouscomplexity
\end{theorem}

%The proof of Theorem~\ref{the:complexity-unambiguous-proof-trees} mimics that of Theorem~\ref{the:complexity-non-recursive-proof-trees} for non-recursive proof trees. In fact, the \NP-hardness is inherited from the proof that $\mathsf{Why\text {-}Provenance_{NR}[\LDAT]}$ is \NP-hard as for linear Datalog programs, non-recursive and unambiguous proof trees coincide. 

For item (1), we show that $\mathsf{Why\text {-}Provenance_{UN}[\DAT]}$ is in \NP~and $\mathsf{Why\text {-}Provenance_{NR}[\LDAT]}$ is \NP-hard.
%
The latter is established via a reduction from the problem of deciding whether a directed graph has a Hamiltonian cycle.
%
The \NP~upper bound relies on a characterization of the existence of an unambiguous proof tree of a fact $\alpha$ w.r.t.~a database $D$ and a Datalog program $\dep$ with $\support{T} = D' \subseteq D$ via the existence of a so-called {\em unambiguous proof DAG} $G$ of $\alpha$ w.r.t.~$D$ and $\dep$ with $\support{G} = D'$ of polynomial size. 
%
Interestingly, unlike arbitrary
%, non-recursive, and minimal-depth 
proof trees, we can directly go from an unambiguous proof tree $T$ to a polynomially-sized unambiguous proof DAG with the same support as $T$, without applying any intermediate steps for reducing the depth or the subtree count of $T$. This is because an unambiguous proof tree has, by definition, ``small'' depth and subtree count (in fact, the subtree count is one).
%
The $\ACZ$ upper bound in item (2) is shown via FO rewritability. The target FO query is obtained as in the proof of Theorem~\ref{the:non-recursive-complexity}, but considering only unambiguous proof trees in the definition of $\cq{Q}$.


\subsection{Computing Why-Provenance via SAT Solvers}\label{sec:reduction-to-sat}
%


We proceed to discuss how off-the-shelf SAT solvers can be used to efficiently compute the why-provenance of a tuple relative to unambiguous proof trees. 
%In particular, the why-provenance of a tuple can be extracted from the satisfying truth assignments of a Boolean formula. 
We then discuss a proof-of-concept implementation and report encouraging results of a preliminary experimental evaluation. 
%
Let us stress that focusing on unambiguous proof trees was crucial towards these encouraging results as it is unclear how a SAT-based implementation can be made practical for proof trees that are not unambiguous. This is mainly because unambiguous proof trees, unlike other classes of proof trees, have always subtree count one, which is crucial for keeping the size of the Boolean formula manageable.



%; this is discussed further below.


%We proceed to discuss a data-efficient reduction of the problem $\mathsf{Why\text {-}Provenance_{UN}[DAT]}$ to $\mathsf{SAT}$, that is, the problem of deciding whether a Boolean formula is satisfiable, which opens up the possibility of employing efficient SAT solvers for explaining query answers to Datalog queries.
%
%We are also going to discuss in the next subsection a proof-of-concept implementation of this reduction, and report encouraging results of a preliminary experimental evaluation. Note that the choice of focusing on the class of unambiguous proof trees was crucial towards these promising results, as it is currently unclear how such a reduction can be made practical for arbitrary, non-recursive, or minimal-depth proof trees.


Consider a Datalog query $Q = (\dep,R)$, a database $D$ over $\esch{\dep}$, and a tuple $\bar t \in \adom{D}^{\arity{R}}$. We construct in polynomial time in $D$ a Boolean formula $\phi_{(\bar t,D,Q)}$ such that the why-provenance of $\bar t$ w.r.t.~$D$ and $Q$ relative to unambiguous proof trees can be computed from the truth assignments that make $\phi_{(\bar t,D,Q)}$ true.
This relies on the characterization mentioned above of the existence of an unambiguous proof tree of $R(\bar t)$ w.r.t.~$D$ and $\dep$ with $\support{T} = D' \subseteq D$ via the existence of an unambiguous proof DAG $G$ of $R(\bar t)$ w.r.t.~$D$ and $\dep$ with $\support{G} = D'$.
%
The formula $\phi_{(\bar t,D,Q)}$ is of the form
$\phi_{\mi{graph}} \wedge \phi_{\mi{acyclic}} \wedge \phi_{\mi{root}} \wedge \phi_{\mi{proof}}$, where $\phi_{\mi{graph}}$ verifies that a truth assignment corresponds to a syntactically correct labeled directed graph $G$, $\phi_{\mi{acyclic}}$ verifies that $G$ is acyclic, $\phi_{\mi{root}}$ verifies that $R(\bar t)$ is the unique root of $G$, and $\phi_{\mi{proof}}$ verifies that $G$ is an unambiguous proof DAG.
%
\begin{comment}
\begin{itemize}
	\item $\phi_{\mi{graph}}$ verifies that a truth assignment corresponds to a syntactically correct labeled directed graph $G$,
	
	\item $\phi_{\mi{acyclic}}$ verifies that $G$ is acyclic,
	
	\item $\phi_{\mi{root}}$ verifies that $R(\bar t)$ is the unique root of $G$, and
	
	\item $\phi_{\mi{proof}}$ verifies that $G$ is an unambiguous proof DAG.
\end{itemize}
\end{comment}

The key ingredient in the construction of $\phi_{(\bar t,D,Q)}$ is the so-called  {\em downward closure of $R(\bar t)$ w.r.t.~$D$ and $\dep$}, taken from~\cite{ElKM22}, which, intuitively speaking, is a hypergraph that encodes all possible proof DAGs of $R(\bar t)$ w.r.t.~$D$ and $\dep$. 
%
We first construct this hypergraph $H$, which can be done in polynomial time in the size of $D$, and then guided by $H$ we build the formula $\phi_{(\bar t,D,Q)}$, which essentially searches for an unambiguous proof DAG inside the hypergraph $H$.
%The formula $\phi_{(\bar t,D,Q)}$ essentially searches for a proof DAG inside this hypergraph.
%
Now, a truth assignment $\tau$ to the variables of $\phi_{(\bar t,D,Q)}$ naturally gives rise to a database denoted $\db{\tau}$. Let $\sem{\phi_{(\bar t,D,Q)}}$ be the family
\[
\left\{\db{\tau} \mid \tau \text{ is a satisfying assignment of } \phi_{(\bar t,D,Q)}\right\}.
\]
We can then show the next technical result:

\def\prowhyprovenancesat{
	Consider a Datalog query $Q = (\dep,R)$, a database $D$ over $\esch{\dep}$, and a tuple $\bar t \in \adom{D}^{\arity{R}}$. It holds that $\unwhy{\bar t}{D}{Q} = \sem{\phi_{(\bar t,D,Q)}}$.
}

\begin{proposition}\label{pro:why-provenance-sat}
\prowhyprovenancesat
\end{proposition}

The above proposition provides a way for computing the why-provenance of a tuple relative to unambiguous proof trees via off-the-shelf SAT solvers.
%: construct the Boolean formula, use a SAT solver to compute its satisfying assignments, and convert those assignments into databases. 
But how does this machinery behave when applied in a practical context? In particular, we are interested in the incremental computation of the why-provenance by enumerating its members instead of computing the whole set at once. The rest of the section is devoted to providing a preliminary answer to this question.


\subsection{Some Implementation Details}
%

Before presenting our experimental results, let us first briefly discuss some interesting aspects of the implementation. In what follows, fix a Datalog query $Q = (\dep,R)$, a database $D$ over $\esch{\dep}$, and a tuple $\bar t \in \adom{D}^{\arity{R}}$.

\medskip

\noindent \textbf{Constructing the Downward Closure.} Recall that the construction of $\phi_{(\bar t,D,Q)}$ relies on the downward closure of $R(\bar t)$ w.r.t.~$D$ and $\dep$. It turns out that the hyperedges of the downward closure can be computed by executing a slightly modified Datalog query $Q_{\downarrow}$ over a slightly modified database $D_{\downarrow}$. In other words, the answers to $Q_{\downarrow}$ over $D_{\downarrow}$ coincide with the hyperedges of the downward closure. Hence, to construct the downward closure we exploit a state-of-the-art Datalog engine, that is, version 2.1.1 of DLV~\cite{AACC+18}.
%
Note that our approach based on evaluating a Datalog query differs form the one in~\cite{ElKM22}, which uses an extension of Datalog with set terms.
%The latter can be simulated by terminating existential rules, which let the authors of~\cite{ElKM22} to use VLog

%a Datalog engine differs from the one used in~\cite{ElKM22}, which relies on an engine called VLog that supports a more expressive language than Datalog, in particualr, existentiallanguages

\medskip
\noindent \textbf{Constructing the Formula.} Recall that $\phi_{(\bar t,D,Q)}$ consists of four conjuncts, where each one is responsible for a certain task. As it might be expected, the heavy task is to verify that the graph in question is acyclic (performed by the formula $\phi_{\mi{acyclic}}$).
%
Checking the acyclicity of a directed graph via a Boolean formula is a well-studied problem in the SAT literature.
%with several different encodings. 
%
For our purposes, we employ the technique of {\em vertex elimination}~\cite{RankoohR22}.
%
The advantage of this approach is that the number of Boolean variables needed for the encoding of $\phi_{\mi{acyclic}}$ is of the order $O(n \cdot \delta)$, where $n$ is the number of nodes of the graph, and $\delta$ is the so-called \emph{elimination width} of the graph, which, intuitively speaking, is related to how connected the graph is.


\medskip
\noindent \textbf{Incrementally Constructing the Why-Provenance.} Recall that we are interested in the incremental computation of the why-provenance, which is more useful in practice than computing the whole set at once. To this end, we need a way to enumerate all the members of the why-provenance without repetitions. This is achieved by adapting a standard technique from the SAT literature for enumerating the satisfying assignments of a Boolean formula, called {\em blocking clause}.
%
We initially collect in a set $S$ all the facts of $D$ occurring in the downward closure of $R(\bar t)$ w.r.t.~$D$ and $\dep$. Then, after asking the SAT solver for an arbitrary satisfying assignment $\tau$ of $\phi_{(\bar t,D,Q)}$, we output the database $\db{\tau}$, and then construct the ``blocking'' clause
$
\vee_{\alpha \in S} \ell_\alpha,
$
where $\ell_\alpha = \neg x_\alpha$ if $\alpha \in \db{\tau}$, and $\ell_\alpha = x_\alpha$ otherwise. We then add this clause to the formula, which expresses that no other satisfying assignment $\tau'$ should give rise to the same member of the why-provenance.
%, i.e., $\db{\tau'}=\db{\tau}$. 
This will exclude the previously computed explanations from the computation. We keep adding such blocking clauses each time we get a new member of the why-provenance until the formula is unsatisfiable.
%, in which case the whole why-provenance has been computed.

\subsection{Experimental Evaluation}
%


We now proceed to experimentally evaluate the SAT-based approach discussed above. To this end, we consider a variety of scenarios from the literature consisting of a Datalog query $Q = (\dep,R)$ and a family of databases $\mathcal{D}$ over $\esch{\dep}$.


{\footnotesize 
	\begingroup
	\setlength{\tabcolsep}{5pt} % Default value: 6pt
	\renewcommand{\arraystretch}{1.3} % Default value: 1
	\begin{table*}[t]
		\centering
		\begin{tabular}{|c||c|c|c|}
			\hline
			\textbf{Scenario} & \textbf{Databases} & \textbf{Query Type}  & \textbf{Number of Rules}\\ \hline
			\hline
			$\mathsf{TransClosure}$ & $D_\mathsf{bitcoin}$ (235K), $D_\mathsf{facebook}$ (88.2K) & linear, recursive & 2 \\ \hline
			$\mathsf{Doctors\text{-}}i$, $i \in [7]$ & $D_1$ (100K) & linear, non-recursive & 6\\ \hline
			$\mathsf{Galen}$ & $D_1$ (26.5K), $D_2$ (30.5K), $D_3$ (67K), $D_4$ (82K) & non-linear, recursive & 14\\ \hline
			$\mathsf{Andersen}$ & $D_1$ (68K), $D_2$ (340K), $D_3$ (680K), $D_4$ (3.4M), $D_5$ (6.8M) & non-linear, recursive & 4 \\ \hline
			$\mathsf{CSDA}$ & $D_\mathsf{httpd}$ (10M), $D_\mathsf{postgresql}$ (34.8M), $D_\mathsf{linux}$ (44M) & linear, recursive & 2\\ \hline
		\end{tabular}
		\caption{Experimental scenarios.}
		\label{tab:scenarios}
	\end{table*}
	\endgroup
}


\medskip
\noindent \textbf{Experimental Scenarios.} All the considered scenarios are summarized in Table~\ref{tab:scenarios}.
%, where we give the name of the scenario, the various databases with their size (number of facts), the type of the query, and the number of Datalog rules occurring in the underlying program. 
Here is brief description:


\begin{description}
	\item[$\mathsf{TransClosure}$.] This scenario computes the transitive closure of a graph and asks for connected nodes. The database $D_\mathsf{bitcoin}$ stores a portion of the Bicoin network~\cite{Weber19}, whereas
	%with nodes representing transactions and directed edges representing flow of money from one transaction to the other~\cite{Weber19}. 
	$D_\mathsf{facebook}$ stores different ``social circles'' from Facebook~\cite{McAuley12}.
	%, where nodes represent users and an edge from $u$ to $v$ denotes that $u$ has chosen $v$ to be part of her social circle~\cite{McAuley12}.
	%
	%The scenarios $\mathsf{DOCTORS\text{-}1},\ldots,\mathsf{DOCTORS\text{-}7}$, and $\mathsf{GALEN}$ from~\cite{ElhalawatiKM22}, which have been used to experimentally evaluate the computation of ondemand why-provenance for standard proof trees, and the scenarios $\mathsf{ANDERSEN}$ and $\mathsf{CSDA}$ from~\cite{FanMK22}.
	
	
	\item[$\mathsf{Doctors}$.] The scenarios $\mathsf{Doctors\text{-}}i$, for $i \in [7]$, were used in~\cite{ElKM22} and represent queries obtained from a well-known data-exchange benchmark involving existential rules (the existential variables have been replaced with fresh constants). All such scenarios share the same database with 100K facts. 
	
	
	\item[$\mathsf{Galen}$.] This scenario used in~\cite{ElKM22} implements the ELK calculus~\cite{KazakovKS14} and asks for all pairs of concepts that are related with the $\mathsf{subClassOf}$ relation. The various databases contain different portions of the Galen ontology~\cite{Galen}.
	
	\item[$\mathsf{Andersen}$.] This scenario used in~\cite{FanMK22} implements the classical Andersen ``points-to'' algorithm for determining the flow of data in procedural programs and asks for all the pairs of a pointer $p$ and a variable $v$ such that $p$ points to $v$. The databases are encodings of program statements of different length.
	
	
	\item[$\mathsf{CSDA}$.] This scenario (Context-Sensitive Dataflow Analysis) used in~\cite{FanMK22} is similar to $\mathsf{Andersen}$ but asks for null references in a program. The databases $D_\mathsf{httpd}$, $D_{\mathsf{postgresql}}$, and $D_{\mathsf{linux}}$ store the statements of the httpd web server, the PostgreSQL DBMS, and the Linux kernel, respectively. 
\end{description}


\begin{comment}
\begin{description}
\item[$\mathsf{TransClosure}$.] This scenario computes the transitive closure of a graph and asks for all pairs of connected nodes. The database $D_\mathsf{bitcoin}$ stores a portion of the Bicoin network with nodes representing transactions and directed edges representing flow of money from one transaction to the other~\cite{Weber19}. $D_\mathsf{facebook}$ stores different ``social circles'' from facebook, where nodes represent users and an edge from $u$ to $v$ denotes that $u$ has chosen $v$ to be part of her social circle~\cite{McAuley12}.
%
%The scenarios $\mathsf{DOCTORS\text{-}1},\ldots,\mathsf{DOCTORS\text{-}7}$, and $\mathsf{GALEN}$ from~\cite{ElhalawatiKM22}, which have been used to experimentally evaluate the computation of ondemand why-provenance for standard proof trees, and the scenarios $\mathsf{ANDERSEN}$ and $\mathsf{CSDA}$ from~\cite{FanMK22}.


\item[$\mathsf{Doctors}$.] The scenarios $\mathsf{Doctors\text{-}}i$, where $i \in [7]$, used in~\cite{ElKM22} represent seven queries obtained from a well-known data-exchange benchmark involving existential rules (existential variables have been replaced with fresh constants). All such scenarios share the same database with 100K facts. 


\item[$\mathsf{Galen}$.] This scenario used in~\cite{ElKM22} implements the ELK calculus~\cite{KazakovKS14} and asks for all pairs of concepts that are related with the $\mathsf{subClassOf}$ relation. The various databases contain different portions of the Galen ontology~\cite{Galen}.

\item[$\mathsf{Andersen}$.] This scenario used in~\cite{FanMK22} implements the classical Andersen ``points-to'' algorithm for determining the flow of data in procedural programs and asks for all the pairs of a pointer $p$ and a variable $v$ such that $p$ points to $v$. The databases are encodings of program statements of different length.


\item[$\mathsf{CSDA}$.] This scenario (Context-Sensitive Dataflow Analysis) used in~\cite{FanMK22} is similar to $\mathsf{Andersen}$ but asks for null references in a program. The databases $D_\mathsf{httpd}$, $D_{\mathsf{postgresql}}$, and $D_{\mathsf{linux}}$ store the statements of the httpd web server, the PostgreSQL DBMS, and the Linux kernel, respectively. 
\end{description}
\end{comment}


%Regarding $\mathsf{ANDERSEN}$, and $\mathsf{CSDA}$, let us clarify that these scenarios were meant to stress test Datalog engines, and thus are quite demanding for our hardware setup. Indeed, the authors of~\cite{FanMK22} tested these scenarios on a server with 160GB of RAM and a Xeon CPU. For this reason, we had to exclude the two largest databases from the $\mathsf{ANDERSEN}$ scenario, as for these databases, even simply collecting the answers to the $\mathsf{ANDERSEN}$'s query, which is necessary in order to obtain tuples to explain, goes out of memory on our machine.


\medskip
\noindent \textbf{Experimental Setup.} For each scenario $s$ consisting of the query $Q = (\dep,R)$ and the family of databases $\mathcal{D}$, and for each $D \in \mathcal{D}$, we have computed $Q(D)$ using DLV, and then selected five tuples $\bar t^1_{s,D},\ldots,\bar t^5_{s,D}$ from $Q(D)$ uniformly at random. 
%
Then, for each $i \in [5]$, we constructed the downward closure of $R(\bar t^i_{s,D})$ w.r.t.~$D$ and $\dep$ by first computing the adapted query $Q_{\downarrow}$ and database $D_{\downarrow}$ via a Python 3 implementation and then using DLV for the actual computation of the downward closure, then
%
we constructed the Boolean formula $\phi_{(\bar t^i_{s,D},D,Q)}$ via a C++ implementation, and finally 
%
we ran the state-of-the-art SAT solver Glucose (see, e.g.,~\cite{Audemard18}), version 4.2.1, with input the above formula to enumerate the members of $\unwhy{\bar t^i_{s,D}}{D}{Q}$.
%
%Note that the size of the why-provenance in some cases can be extremely large (in the order of millions), and thus, we executed the enumeration until either 10K explanations have been constructed, or a timeout of 5 minutes has been reached (whatever comes first). We did not set any timeout for building the downward closure and the Boolean formula.
%
All the experiments have been conducted on a laptop with an Intel(R) Core(TM) i7-10750H CPU @ 2.60GHz, and 32GB of RAM, running Fedora Linux 37. The Python code is executed with Python 3.11.2, and the C++ code has been compiled with g++ 12.2.1, using the -O3 optimization flag.


\begin{figure}[t]
	\centering
	\includegraphics[width=.464\textwidth]{ground_formula_ANDERSEN.pdf}
	\caption{Building the downward closure and the Boolean formula.}
	\label{fig:andersen-task1}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=.47\textwidth]{exptimes_ANDERSEN.pdf}
	\caption{Incremental computation of the why-provenance.}
	\label{fig:andersen-task2}
\end{figure}


\medskip
\noindent \textbf{Experimental Results.} Due to space constraints, we are going to present only the results based on the $\mathsf{Andersen}$ scenario. Nevertheless, the final outcome is aligned with what we have observed based on all the other scenarios.
%
%Recall that we are dealing with two main tasks, which we are going to discuss separately, namely (1) the construction of the downward closure and the Boolean formula, and (2) the incremental computation of the why-provenance.


Concerning the construction of the downward closure and the Boolean formula, we report in Figure~\ref{fig:andersen-task1} the total running time for each database of the $\mathsf{Andersen}$ scenario (recall that there are five databases of varying size, and thus we have five plots). Furthermore, each plot consists of five bars that correspond to the five randomly chosen tuples. Each such bar shows the time for building the downward closure plus the time for constructing the Boolean formula. 
%
%The two contributions are presented with two different colors, although in some cases the contribution of building the formula is invisible since it is too small.
%
%It is evident 
We have observed that almost all the time is spent for computing the downward closure, whereas the time for building the formula is negligible. Hence, our efforts should concentrate on improving the computation of the downward closure.
%
Moreover, for the reasonably sized databases (68K, 340K, and 680K facts) the total time is in the order of seconds, which is quite encouraging. Now, for the very large databases that we consider (3.4M and 6.8M facts), the total time is between half a minute and a minute, which is also encouraging taking into account the complexity of the query, the large size of the databases, and the limited power of our machine.
%that has been used for performing the experiments.



For the incremental computation of the why-provenance, we give in Figure~\ref{fig:andersen-task2}, for each database of the $\mathsf{Andersen}$ scenario, the times required to build an explanation, that is, the time between the current member of the why-provenance and the next one (this time is also known as the delay).
%
Each of the five plots collects the delays of constructing the members of the why-provenance (up to a limit of 10K members or 5 minutes timeout) for each of the five randomly chosen tuples. We use box plots, where the bottom and the top borders of the box represent the first and third quartile, i.e., the delay under which 25\% and 75\% of all delays occur, respectively, and the orange line represents the median delay. Moreover, the bottom and the top whisker represent the minimum and maximum delay, respectively. All times are expressed in milliseconds and we use logarithmic scale.
%
As we can see, most of the delays are below 1 millisecond, with the median in the order of microseconds. Therefore, once we have the Boolean formula in place, incrementally computing the members of the why-provenance is extremely fast.


\begin{comment}
\medskip
\noindent \textbf{Comparative Evaluation.}
We conclude this section by performing a preliminary comparison with the approach of~\cite{ElKM22}. 
%
Let us first clarify that our work deals with a different problem. For a Datalog query $Q = (\dep,R)$, a database $D$ over $\esch{\dep}$, and a tuple $\bar t \in \adom{D}^{\arity{R}}$, the approach from~\cite{ElKM22} has been designed and evaluated for building the whole set $\why{\bar t}{D}{Q}$, whereas our approach has been designed and evaluated for incrementally computing $\unwhy{\bar t}{D}{Q}$. However, there is a setting where a reasonable comparison can be performed, which will provide some insights for the two approaches.
%
This is when the Datalog query $Q$ is both linear and non-recursive in which case the sets $\why{\bar t}{D}{Q}$ and $\unwhy{\bar t}{D}{Q}$ coincide since a proof tree of $R(\bar t)$ w.r.t.~$D$ and $\dep$ is trivially unambiguous.
%
Therefore, towards a fair comparison, we are going to consider the scenarios $\mathsf{Doctors}\text{-}i$, for $i \in [7]$, which consist of a Datalog query that is linear and non-recursive, and consider the end-to-end runtime of our approach (not the delays) without, of course, setting a limit on the number of members of why-provenance to build, or on the total runtime.




%We point out that providing a reasonable comparison between our approach and the one above is tricky, since they basically solve different problems. Indeed, while in our case we might have the advantage of needing to construct much less supports, since our proof trees are more refined, in the all-trees case there is no need to check for the unambiguity of the underlying trees. For these reasons, we believe the only reasonable comparison that can be made is over scenarios for which the two notions of trees coincide. This is only the case with the DOCTORS-based scenarios, which employ queries that are both linear and non-recursive. It is easy to verify that for such queries, unmabiguous trees and arbitrary trees coincide.\footnote{In any case, even if we wanted to make a comparison over all scenarios, this would not be possible, since the authors of~\cite{ElhalawatiKM22} did not provide the tools needed to construct the set of existential rules that are able to build the why-provenance. Hence, we have to rely on the pregenerated ones for the scenarios they also consider (i.e., $\mathsf{DOCTORS}$ and $\mathsf{GALEN}$).}

\begin{figure}[t]
	\centering
	\includegraphics[width=.477\textwidth]{comparison.pdf}
	\caption{Comparative Evaluation Using the Doctors Scenarios.}
	\label{fig:comparison-time}
\end{figure}

%Since the approach of~\cite{ElhalawatiKM22} can only construct the whole why-provenance, without enumeration, we compare the end-to-end running time of our apprach (i.e., by combining tasks \emph{1)} and \emph{2)}) with the running time of their approach. In this case, of course, we did not specify any limit on the number of supports to build, or on the total running time. Finally, although not strictly necessary for enumerating the supports, our implementation keeps in main memory each support that is produced, so to be fair with the approach of~\cite{ElhalawatiKM22} which keeps all supports in memory by design. 


The comparison is shown in Figure~\ref{fig:comparison-time}. For each scenario, we present the runtime for all five randomly chosen tuples for our approach (in blue) and the approach of~\cite{ElKM22} (in red); if a bar is missing for a certain tuple, then the execution ran out of memory.
%
We observe that for the simple scenarios the two approaches are comparable in the order of a second.
%
Now, concerning the demanding scenarios, i.e., $\mathsf{Doctors}\text{-}i$ for $i \in \{1,5,7\}$, we observe that our approach is, in general, faster. Observe also that for some of the most demanding cases, the approach of~\cite{ElKM22} runs out of memory.
%
We believe that the latter is due to the use of the rule engine VLog, which is intended for materialization-based reasoning with existential rules, whereas our approach relies on a Datalog engine (in particular, DLV), and thus, exploiting all the optimizations that are typically employed for evaluating a Datalog query. For example, the technique of {\em magic-set rewriting}, implemented by DLV, can greatly reduce the memory usage by building much fewer facts during the evaluation of the rules; see, e.g.,~\cite{LAAC+19}.
\end{comment}