@article{skarbez2019iat,
	author		= {Richard Skarbez and Nicholas F. Polys and J. Todd Oggle and Chris North and Doug A. Bowman},
	title		= {{{Immersive Analytics}: Theory and Research Agenda}},
	journal		= {Frontiers in Robotics and AI},
	pages		= {82:1--15},
	volume		= {6},
	number		= {},
	publisher	= {{Frontiers Media S.A.}},
	year		= {2019},
	month		= "10~" # sep,
	language	= {english},
	url			= {https://doi.org/10.3389/frobt.2019.00082},
	doi 		= {10.3389/frobt.2019.00082},
	keywords	= {immersive analytics, visual analytics, immersion, virtual reality, visualization, sensemaking, knowledge generation},
	abstract	= {Advances in a variety of computing fields, including “big data,” machine learning, visualization, and augmented/mixed/virtual reality, have combined to give rise to the emerging field of immersive analytics, which investigates how these new technologies support analysis and decision making. Thus far, we feel that immersive analytics research has been somewhat ad hoc, possibly owing to the fact that there is not yet an organizing framework for immersive analytics research. In this paper, we address this lack by proposing a definition for immersive analytics and identifying some general research areas and specific research questions that will be important for the development of this field. We also present three case studies that, while all being examples of what we would consider immersive analytics, present different challenges, and opportunities. These serve to demonstrate the breadth of immersive analytics and illustrate how the framework proposed in this paper applies to practical research.}
}

@incollection{dwyer2018iai,
	author 		= {Tim Dwyer and Kim Marriott and Tobias Isenberg and Karsten Klein and Nathalie Henry Riche and Flak Schreiber and Wolfgang Stuerzlinger},
	title 		= {{{Immersive Analytics}: An Introduction}},
	booktitle 	= {Immersive Analytics},
	series 		= {Lecture Notes in Computer Science (LNCS, volume 11190)},
	editor 		= {Kim Marriott and Falk Schreiber and Tim Dwyer and Karsten Klein and Nathalie Henry Riche and Takayuki Itoh and Wolfgang Stuerzlinger and Bruce H. Thomas},
	pages		= {1--23},
	edition 	= {First Online},
	publisher	= {Springer, Cham},
	year		= {2018},
	month		= "16~" # oct,
	language	= {english},
	url			= {https://doi.org/10.1007/978-3-030-01388-2_1},
	doi 		= {10.1007/978-3-030-01388-2_1},
	isbn		= {978-3-030-01388-2},
	keywords	= {},
	abstract	= {Immersive Analytics is a new research initiative that aims to remove barriers between people, their data and the tools they use for analysis and decision making. Here we clarify the aims of immersive analytics research, its opportunities and historical context, as well as providing a broad research agenda for the field. In addition, we review how the term immersion has been used to refer to both technological and psychological immersion, both of which are central to immersive analytics research.}
}

@incollection{buschel2018ifi,
	author 		= {Wolfgang Büschel and Jian Chen and Raimund Dachselt and Steven Drucker and Tim Dwyer and Carsten Görg and Tobias Isenberg and Andreas Kerren and Chris North and Wolfgang Stuerzlinger},
	title 		= {{Interaction for {Immersive Analytics}}},
	booktitle 	= {Immersive Analytics},
	series 		= {Lecture Notes in Computer Science (LNCS, volume 11190)},
	editor 		= {Kim Marriott and Falk Schreiber and Tim Dwyer and Karsten Klein and Nathalie Henry Riche and Takayuki Itoh and Wolfgang Stuerzlinger and Bruce H. Thomas},
	pages		= {95--138},
	edition 	= {First Online},
	publisher	= {Springer, Cham},
	year		= {2018},
	month		= "16~" # oct,
	language	= {english},
	url			= {https://doi.org/10.1007/978-3-030-01388-2_4},
	doi 		= {10.1007/978-3-030-01388-2_4},
	isbn		= {978-3-030-01388-2},
	keywords	= {},
	abstract	= {In this chapter, we briefly review the development of natural user interfaces and discuss their role in providing human-computer interaction that is immersive in various ways. Then we examine some opportunities for how these technologies might be used to better support data analysis tasks. Specifically, we review and suggest some interaction design guidelines for immersive analytics. We also review some hardware setups for data visualization that are already archetypal. Finally, we look at some emerging system designs that suggest future directions.}
}

@inproceedings{fruchard2019lbi,
	author		= {Bruno Fruchard and Arnaud Prouzeau and Olivier Chapuis and Eric Lecolinet},
	title		= {{Leveraging Body Interactions to Support {Immersive Analytics}}},
	booktitle	= {Proceedings of the 2019 ACM Conference on Human Factors in Computing Systems (CHI) - Workshop on Interaction Design \& Prototyping for Immersive Analytics},
	pages		= {11},
	address		= {Glasgow, Scotland, UK},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {2019},
	month		= "4--9~" # may,
	language	= {english},
	url			= {https://hal.archives-ouvertes.fr/hal-02095993},
	doi 		= {},
	isbn		= {},
	keywords	= {On-body interaction, Gestural interaction, Memorization, Command selection, Data manipulation},
	abstract	= {New immersive devices (e.g., virtual or augmented reality) enable displaying large amounts of data in space to better support data analysis. Manipulating this data efficiently is crucial, but challenging because the user must be able to activate various commands or adjust various values while remaining free to move. Using the whole body offers several valuable advantages: 1) The body provides a physical support as an interactive surface, which improves accuracy and makes it less tiring to interact; 2) Using the body does not impair mobility and avoids handling devices; 3) Proprioception makes it possible to interact eyes-free, including for choosing values in a range; 4) By leveraging spatial memory, the body helps memorizing commands, thus interacting in expert mode (i.e., perform quick actions without visual feedback). In this position paper, we analyze various ways of interacting with the body and discuss their advantages and challenges for immersive analytics.}
}

@article{roberts2014vbt,
	author		= {Jonathan C. Roberts and Panagiotis D. Ritsos and Sriram Karthik Badam and Dominique Brodbeck and Jessie Kennedy and Niklas Elmqvist},
	title		= {{Visualization beyond the Desktop--the Next Big Thing}},
	journal		= {IEEE Computer Graphics and Applications},
	pages		= {26--34},
	volume		= {34},
	number		= {6},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2014},
	month		= "" # nov # "--" # "" # dec,
	language	= {english},
	url			= {https://doi.org/10.1109/MCG.2014.82},
	doi 		= {10.1109/MCG.2014.82},
	keywords	= {},
	abstract	= {Visualization is coming of age. With visual depictions being seamlessly integrated into documents, and data visualization techniques being used to understand increasingly large and complex datasets, the term "visualization"' is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today's new devices and tomorrow's technology. Today, people interact with visual depictions through a mouse. Tomorrow, they'll be touching, swiping, grasping, feeling, hearing, smelling, and even tasting data. The next big thing is multisensory visualization that goes beyond the desktop.}
}

@inproceedings{hackathorn2016iab,
	author		= {Richard Hackathorn and Todd Margolis},
	title		= {{{Immersive Analytics}: Building Virtual Data Worlds for Collaborative Decision Support}},
	series 		= {},
	booktitle	= {2016 Workshop on Immersive Analytics (IA)},
	editor		= {},
	volume		= {},
	pages		= {44--47},
	address		= {Greenville, South Carolina, USA},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2016},
	month		= "20~" # mar,
	language	= {english},
	url			= {https://doi.org/10.1109/IMMERSIVE.2016.7932382},
	doi 		= {10.1109/IMMERSIVE.2016.7932382},
	isbn		= {},
	keywords	= {},
	abstract	= {Immersive analytics is an emerging research area that blends analytical reasoning with immersive virtual space to enhance collaborative decision support. The intent of this position paper is to stimulate discussion and cooperation toward maturing immersive analytics. An open innovation community to build immersive data worlds has been established at ImmersiveAnalytics.com, to serve as a bridge and catalyst between academia and corporate communities. The paper outlines the objectives for analytical reasoning and immersive data spaces, followed by suggestions for the design and architecture of data worlds. Finally, current work for building data worlds is described.}
}

@article{fonnet2021soi,
	author		= {Adrien Fonnet and Yannick Prié},
	title		= {{Survey of {Immersive Analytics}}},
	journal		= {IEEE Transactions on Visualization and Computer Graphics},
	pages		= {2101--2122},
	volume		= {27},
	number		= {3},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2021},
	month		= "1~" # mar,
	language	= {english},
	url			= {https://doi.org/10.1109/TVCG.2019.2929033},
	doi 		= {10.1109/TVCG.2019.2929033},
	keywords	= {Immersive analytics, survey, virtual environments, immersive environments, data visualization, information visualization, scientific visualization, visual data mining},
	abstract	= {Immersive analytics (IA) is a new term referring to the use of immersive technologies for data analysis. Yet such applications are not new, and numerous contributions have been made in the last three decades. However, no survey reviewing all these contributions is available. Here we propose a survey of IA from the early nineties until the present day, describing how rendering technologies, data, sensory mapping, and interaction means have been used to build IA systems, as well as how these systems have been evaluated. The conclusions that emerge from our analysis are that: multi-sensory aspects of IA are under-exploited, the 3DUI and VR community knowledge regarding immersive interaction is not sufficiently utilised, the IA community should focus on converging towards best practices, as well as aim for real life IA systems.}
}

@inproceedings{ens2021gci,
	author		= {Barrett Ens and Benjamin Bach and Maxime Cordeil and Ulrich Engelke and Marcos Serrano and Wesley Willett and Arnaud Prouzeau and Christoph Anthes and Wolfgang Büschel and Cody Dunne and Tim Dwyer and Jens Grubert and Jason H. Haga and Nurit Kirshenbaum and Dylan Kobayashi and Tica Lin and Monsurat Olaosebikan and Fabian Pointecker and David Saffo and Nazmus Saquib and Dieter Schmalstieg and Danielle {Albers Szafir} and Matt Whitlock and Yalong Yang},
	title		= {{Grand Challenges in {Immersive Analytics}}},
	booktitle	= {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI 2021)},
	pages		= {459:1--17},
	address		= {Yokohama, Japan},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {2021},
	month		= "8--13~" # may,
	language	= {english},
	url			= {https://doi.org/10.1145/3411764.3446866},
	doi 		= {10.1145/3411764.3446866},
	isbn		= {},
	keywords	= {Immersive analytics, grand research challenges, data visualisation, augmented reality, virtual reality},
	abstract	= {Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.}
}

@inproceedings{huang2017ags,
	author		= {Yi-Jheng Huang and Takanori Fujiwara and Yun-Xuan Lin and Wen-Chieh Lin and Kwan-Liu Ma},
	title		= {{A Gesture System for Graph Visualization in {Virtual Reality Environments}}},
	series 		= {},
	booktitle	= {2017 IEEE Pacific Visualization Symposium (PacificVis)},
	editor		= {},
	volume		= {},
	pages		= {41--45},
	address		= {Seoul, Korea},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2017},
	month		= "18--21~" # apr,
	language	= {english},
	url			= {https://doi.org/10.1109/PACIFICVIS.2017.8031577},
	doi 		= {10.1109/PACIFICVIS.2017.8031577},
	isbn		= {},
	keywords	= {},
	abstract	= {As virtual reality (VR) hardware technology becomes more mature and affordable, it is timely to develop visualization applications making use of such technology. How to interact with data in an immersive 3D space is both an interesting and challenging problem, demanding more research investigations. In this paper, we present a gesture input system for graph visualization in a stereoscopic 3D space. We compare desktop mouse input with gesture input with bare hands for performing a set of tasks on graphs. Our study results indicate that users are able to effortlessly manipulate and analyze graphs using gesture input. Furthermore, the results also show that using gestures is more efficient when exploring the complicated graph.}
}

@article{wagner2020eai,
	author		= {Jorge A. {Wagner Filho} and Wolfgang Stuerzlinger and Luciana Nedel},
	title		= {{Evaluating an Immersive {Space-Time Cube} Geovisualization for Intuitive Trajectory Data Exploration}},
	journal		= {IEEE Transactions on Visualization and Computer Graphics},
	pages		= {514--524},
	volume		= {26},
	number		= {1},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2020},
	month		= "1~" # jan,
	language	= {english},
	url			= {https://doi.org/10.1109/TVCG.2019.2934415},
	doi 		= {10.1109/TVCG.2019.2934415},
	keywords	= {},
	abstract	= {A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.}
}

@inproceedings{reski2020eot,
	author		= {Nico Reski and Aris Alissandrakis and Andreas Kerren},
	title		= {{Exploration of Time-Oriented Data in Immersive {Virtual Reality} Using a {3D Radar Chart} Approach}},
	booktitle	= {Proceedings of the 11th Nordic Conference on Human-Computer Interaction: Shaping Experiences, Shaping Society (NordiCHI 2020)},
	pages		= {33:1--11},
	address		= {Tallinn, Estonia},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {2020},
	month		= "25--29~" # oct,
	language	= {english},
	url			= {https://doi.org/10.1145/3419249.3420171},
	doi 		= {10.1145/3419249.3420171},
	diva 		= {http://urn.kb.se/resolve?urn=urn:nbn:se:lnu:diva-98671},
	isbn		= {9781450375795},
	keywords	= {immersive analytics, radar chart, time-oriented data, virtual reality, 3D gestural input},
	abstract	= {In this paper, we present an approach to interact with time-oriented data in Virtual Reality within the context of Immersive Analytics. We implemented a Virtual Reality application that enables its user to explore data in an immersive environment (head-mounted display, 3D gestural input), utilizing potential advantages of immersive technologies, for instance, depth cues for better spatial understanding, natural interaction, and user engagement. The visualization design is inspired by the overall concept of a radar chart, and using the third dimension to represent time-series related data. We conducted a user study with 15 participants, encouraging them to examine a representative dataset within an explorative analysis scenario with no time constraints. Based on the results of usability and user engagement scores, task completion analysis, observations, and interviews, we were able to empirically validate the approach in general, and gain insights in the users’ interaction and data analysis strategies.}
}

@inproceedings{fittkau2015esc,
	author		= {Florian Fittkau and Alexander Krause and Wilhelm Hasselbring},
	title		= {{Exploring Software Cities in {Virtual Reality}}},
	booktitle	= {Proceedings of the 3rd Working Conference on Software Visualization (VISSOFT 2015)},
	pages		= {130--134},
	address		= {Bremen, Germany},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2015},
	month		= "23~" # nov,
	language	= {english},
	url			= {https://doi.org/10.1109/VISSOFT.2015.7332423},
	doi 		= {10.1109/VISSOFT.2015.7332423},
	isbn		= {978-1-4673-7526-9},
	issn		= {},
	keywords	= {},
	abstract	= {Software visualizations, such as the software city metaphor, are usually displayed on 2D screens and controlled by means of a mouse and thus often do not take advantage of more natural interaction techniques. Virtual reality (VR) approaches aim to improve the user experience. Emerging new technologies, like the Oculus Rift, dramatically enhance the VR experience at an affordable price. Therefore, new technologies have the potential to provide even higher immersion - and thus benefits - than previous VR approaches.}
}

@inproceedings{streppel2018iiv,
	author		= {Barbara Streppel and Dorothea Pantförder and Birgit Vogel-Heuser},
	title		= {{{Interaction in Virtual Environments} -- How to Control the Environment by Using {VR}-Glasses in the Most Immersive Way}},
	series 		= {Lecture Notes in Computer Science (LNCS, volume 10909)},
	booktitle	= {Virtual, Augmented and Mixed Reality: Interaction, Navigation, Visualization, Embodiment, and Simulation (VAMR 2018)},
	editor		= {Jessie Y. C. Chen and Gino Fragomeni},
	pages		= {183--201},
	address		= {},
	publisher	= {Springer, Cham},
	year		= {2018},
	month		= "2~" # jun,
	language	= {english},
	url			= {https://doi.org/10.1007/978-3-319-91581-4_14},
	doi 		= {10.1007/978-3-319-91581-4_14},
	isbn		= {978-3-319-91580-7},
	keywords	= {Virtual reality, Interaction, Virtual environment },
	abstract	= {Not only in the gaming industry is Virtual Reality (VR) the new way to give users a new experience – in engineering or production plant operation we also see first attempts at finding innovative ways of visualizing data or training plant staff. This is necessary because processes are getting more and more complex thanks to higher interconnection and flexibility. This paper presents actual possibilities of interacting with a virtual environment and provides three concepts for immersive interaction. We also show the results of an evaluation of these concepts at the end of the paper.}
}

@article{austin2020esi,
	author		= {Christopher R. Austin and Barrett Ens and Kadek Ananta Satriadi and Bernhard Jenny},
	title		= {{Elicitation study investigating hand and foot gesture interaction for immersive maps in augmented reality}},
	journal		= {Cartography and Geographic Information Science},
	pages		= {214--228},
	volume		= {47},
	number		= {3},
	publisher	= {Taylor \& Francis},
	year		= {2020},
	month		= "7~" # jan,
	language	= {english},
	url			= {https://doi.org/10.1080/15230406.2019.1696232},
	doi 		= {10.1080/15230406.2019.1696232},
	keywords	= {Mid-air interaction, user-defined gestures, foot gesture, elicitation study, augmented reality},
	abstract	= {Immersive maps in augmented reality (AR) are virtual maps that plausibly blend with the physical environment, such that the user perceives them as a part of the real world. While immersive maps can offer unprecedented engaging experiences, the way to perform panning, zooming and other basic map interaction is not obvious. This limitation may hamper widespread adoption of immersive maps. We therefore conducted an elicitation study to identify commonly suggested hand and foot gestures for interacting with large immersive AR maps placed on the floor. Study participants were shown simulations of interaction operations, and asked to design gestures to trigger these operations. We selected interaction with hand gestures because they are natural and familiar from interacting with touchscreens. Foot gestures were included because the users’ feet touch immersive maps placed on the floor. Eighteen participants designed hand and foot gestures for panning, rotating, zooming, changing the height of the map, creating a point marker, and selecting a point marker. The most agreed-on hand gesture was for zooming, consisting of grabbing with both hands, then separating or bringing the hands together. Our study participants suggested numerous other inspirational hand and foot gestures, which can guide the further development of interactive immersive maps. Because user preference does not necessarily align with performance, it is important to evaluate the efficiency, accuracy, ease of learning, and physical fatigue for these gestures in follow-up studies.}
}

@book{laviola20173ui,
	title		= {{{3D User Interfaces}: Theory and Practice}},
	author 		= {Joseph J. {LaViola, Jr.} and Ernst Kruijff and Ryan P. McMahan and Doug Bowman and Ivan P. Poupyrev},
	editor 		= {},
	edition 	= {2nd},
	publisher	= {Addison-Wesley Professional},
	year		= {2017},
	month		= "",
	language	= {english},
	url			= {https://www.pearson.com/us/higher-education/program/La-Viola-3-D-User-Interfaces-Theory-and-Practice-2nd-Edition/PGM101825.html},
	isbn		= {9780134034324},
	keywords	= {},
	abstract	= {From video games to mobile augmented reality, 3D interaction is everywhere. But simply choosing to use 3D input or 3D displays isn't enough: 3D user interfaces (3D UIs) must be carefully designed for optimal user experience. 3D User Interfaces: Theory and Practice, Second Edition is today's most comprehensive primary reference to building outstanding 3D UIs. Four pioneers in 3D user interface research and practice have extensively expanded and updated this book, making it today's definitive source for all things related to state-of-the-art 3D interaction.


	This edition goes far beyond VR, covering the full spectrum of emerging applications for 3D UIs, and presenting an extraordinary array of pioneering techniques and technologies. The authors combine theoretical foundations, analysis of devices and techniques, empirically validated design guidelines, and much more. Throughout each chapter, they illustrate key concepts with running case studies on gaming, mobile AR, and robot teleoperation.}
}

@article{norman2010nui,
	author		= {Donald A. Norman},
	title		= {{{Natural User Interfaces} Are Not Natural}},
	journal		= {Interactions},
	pages		= {6--10},
	volume		= {17},
	number		= {3},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {2010},
	month		= "" # may # "--" # "" # jun,
	language	= {english},
	url			= {https://doi.org/10.1145/1744161.1744163},
	doi 		= {10.1145/1744161.1744163},
	keywords	= {},
	abstract	= {}
}

@article{bachmann2018rot,
	author		= {Daniel Bachmann and Frank Weichert and Gerhard Rinkenauer},
	title		= {{Review of Three-Dimensional {Human-Computer Interaction} with Focus on the {Leap Motion} Controller}},
	journal		= {Sensors},
	pages		= {2194:1--39},
	volume		= {18},
	number		= {7},
	publisher	= {Multidisciplinary Digital Publishing Institute (MDPI)},
	year		= {2018},
	month		= "",
	language	= {english},
	url			= {https://doi.org/10.3390/s18072194},
	doi 		= {10.3390/s18072194},
	keywords	= {human-computer interaction, contact-free input devices, three-dimensional interaction, natural user interfaces, leap motion controller},
	abstract	= {Modern hardware and software development has led to an evolution of user interfaces from command-line to natural user interfaces for virtual immersive environments. Gestures imitating real-world interaction tasks increasingly replace classical two-dimensional interfaces based on Windows/Icons/Menus/Pointers (WIMP) or touch metaphors. Thus, the purpose of this paper is to survey the state-of-the-art Human-Computer Interaction (HCI) techniques with a focus on the special field of three-dimensional interaction. This includes an overview of currently available interaction devices, their applications of usage and underlying methods for gesture design and recognition. Focus is on interfaces based on the Leap Motion Controller (LMC) and corresponding methods of gesture design and recognition. Further, a review of evaluation methods for the proposed natural user interfaces is given.}
}

@inproceedings{ulinski2009spb,
	author		= {Amy C. Ulinski and Zachary Wartell and Paula Goolkasian and Evan A. Suma and Larry F. Hodges},
	title		= {{Selection Performance Based on Classes of Bimanual Actions}},
	booktitle	= {Proceedings of the IEEE Symposium on 3D User Interfaces (3DUI 2009)},
	pages		= {51--58},
	address		= {Lafayette, Louisiana, USA},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2009},
	month		= "14--25~" # mar,
	language	= {english},
	url			= {https://doi.org/10.1109/3DUI.2009.4811205},
	doi 		= {10.1109/3DUI.2009.4811205},
	isbn		= {978-1-4244-3965-2},
	keywords	= {3D selection, bimanual interaction, volumetric data, splat-based rendering, polygonal objects, visualization},
	abstract	= {We evaluated four selection techniques for volumetric data based on the four classes of bimanual action: symmetric-synchronous, asymmetric-synchronous, symmetric-asynchronous, and asymmetric-asynchronous. The purpose of this study was to determine the relative performance characteristics of each of these classes. In addition, we compared two types of data representations to determine whether these selection techniques were suitable for interaction in different environments. The techniques were evaluated in terms of accuracy, completion times, TLX overall workload, TLX physical demand, and TLX cognitive demand. Our results suggest that symmetric and synchronous selection strategies both contribute to faster task completion. Our results also indicate that no class of bimanual selection was a significant contributor to reducing or increasing physical demand, while asynchronous action significantly increased cognitive demand in asymmetric techniques and decreased ease of use in symmetric techniques. However, for users with greater computer usage experience, accuracy performance differences diminished between the classes of bimanual action. No significant differences were found between the two types of data representations.}
}

@article{pavlovic1997vio,
	author		= {Vladimir I. Pavlovic and Rajeev Sharma and Thomas S. Huang},
	title		= {{Visual Interpretation of Hand Gestures for {Human-Computer Interaction}: A Review}},
	journal		= {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages		= {677--695},
	volume		= {19},
	number		= {7},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {1997},
	month		= "" # jul,
	language	= {english},
	url			= {https://doi.org/10.1109/34.598226},
	doi 		= {10.1109/34.598226},
	keywords	= {Vision-based gesture recognition, gesture analysis, hand tracking, nonrigid motion analysis, human-computer interaction},
	abstract	= {The use of hand gestures provides an attractive alternative to cumbersome interface devices for human-computer interaction (HCI). In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for HCI. This has motivated a very active research area concerned with computer vision-based analysis and interpretation of hand gestures. We survey the literature on visual interpretation of hand gestures in the context of its role in HCI. This discussion is organized on the basis of the method used for modeling, analyzing, and recognizing gestures. Important differences in the gesture interpretation approaches arise depending on whether a 3D model of the human hand or an image appearance model of the human hand is used. 3D hand models offer a way of more elaborate modeling of hand gestures but lead to computational hurdles that have not been overcome given the real-time requirements of HCI. Appearance-based models lead to computationally efficient "purposive" approaches that work well under constrained situations but seem to lack the generality desirable for HCI. We also discuss implemented gestural systems as well as other potential applications of vision-based gesture recognition. Although the current progress is encouraging, further theoretical as well as computational advances are needed before gestures can be widely used for HCI. We discuss directions of future research in gesture recognition, including its integration with other natural modes of human-computer interaction.}
}

@inproceedings{nehaniv2005ama,
	author		= {Chrystopher L. Nehaniv and Kerstin Dautenhahn and Jens Kubacki and Martin Haegele and Christopher Parlitz and Rachid Alami},
	title		= {{A Methodological Approach relating the Classification of Gesture to Identification of Human Intent in the Context of {Human-Robot Interaction}}},
	booktitle	= {Proceedings of the 2005 IEEE International Workshop on Robot and Human Interactive Communication (ROMAN 2005)},
	pages		= {371--377},
	address		= {Nashville, Tennessee, USA},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2005},
	month		= "13--15~" # mar,
	language	= {english},
	url			= {https://doi.org/10.1109/ROMAN.2005.1513807},
	doi			= {10.1109/ROMAN.2005.1513807}, 
	isbn		= {0-7803-9274-4},
	keywords 	= {interaction context, classification of gestures, human-robot activity and interaction},
	abstract	= {In order to infer intent from gesture, a broad classification of types of gestures into five main classes is introduced. The classification is intended as a generally applicable basis for incorporating the understanding of gesture into human-robot interaction (HRI). Examples from human-robot interaction show the need to take into account not only the kinematics of gesture, but also the interactional context. Requirements for the operational classification of gesture by a robot interacting with humans are suggested and initial steps in its deployment are discussed.}
}

@article{rempel2014tdo,
	author		= {David Rempel and Matt J. Camilleri and David L. Lee},
	title		= {{The design of hand gestures for {Human–Computer Interaction}: Lessons from sign language interpreters}},
	journal		= {International Journal of Human-Computer Studies},
	pages		= {728--735},
	volume		= {72},
	number		= {10--11},
	publisher	= {Elsevier Science Inc.},
	year		= {2014},
	month		= "" # oct # "--" # "" # nov,
	language	= {english},
	url			= {https://doi.org/10.1016/j.ijhcs.2014.05.003},
	doi 		= {10.1016/j.ijhcs.2014.05.003},
	keywords	= {Gesture-based interaction, Computer interface, Hand postures, Multi-touch, Computer input},
	abstract	= {The design and selection of 3D modeled hand gestures for human–computer interaction should follow principles of natural language combined with the need to optimize gesture contrast and recognition. The selection should also consider the discomfort and fatigue associated with distinct hand postures and motions, especially for common commands. Sign language interpreters have extensive and unique experience forming hand gestures and many suffer from hand pain while gesturing. Professional sign language interpreters (N=24) rated discomfort for hand gestures associated with 47 characters and words and 33 hand postures. Clear associations of discomfort with hand postures were identified. In a nominal logistic regression model, high discomfort was associated with gestures requiring a flexed wrist, discordant adjacent fingers, or extended fingers. These and other findings should be considered in the design of hand gestures to optimize the relationship between human cognitive and physical processes and computer gesture recognition systems for human–computer input.}
}

@book{aigner2011vot,
	title		= {{Visualization of Time-Oriented Data}},
	series		= {Human-Computer Interaction Series (HCIS)},
	author 		= {Wolfgang Aigner and Silvia Miksch and Heidrun Schumann and Christian Tominski},
	editor 		= {},
	edition 	= {1st},
	publisher	= {Springer London},
	year		= {2011},
	month		= "",
	language	= {english},
	url			= {https://doi.org/10.1007/978-0-85729-079-3},
	doi 		= {10.1007/978-0-85729-079-3},
	isbn		= {9780857290786},
	keywords	= {},
	abstract	= {Time is an exceptional dimension that is common to many application domains such as medicine, engineering, business, science, biography, history, planning, or project management. Understanding time-oriented data enables us to learn from the past in order to predict, plan, and build the future. Due to the distinct characteristics of time, appropriate visual and analytical methods are required to explore and analyze them.

	This book starts with an introduction to visualization and a number of historical examples of visual representations. At its core, the book presents and discusses a systematic view of the visualization of time-oriented data. This view is structured along three key questions. While the aspects of time and associated data describe what is being visualized, user tasks are related to the question why something is visualized. These characteristics and tasks determine how the visualization is to be designed. To support visual exploration, interaction techniques and analytical methods are required as well, which are discussed in separate chapters. The concepts explained in this book are illustrated with numerous examples.

	A large part of this book is devoted to a structured survey of existing techniques for visualizing time and time-oriented data. Overall, 101 different visualization techniques are presented on a per-page basis; each of these self-contained descriptions is accompanied by an illustration and corresponding references.  This survey serves as a reference for scientists conducting related research as well as for practitioners seeking information on how their time-oriented data can best be visualized in order to gain valuable insights.}
}

@book{ward2015idv,
	title		= {{{Interactive Data Visualization}: Foundations, Techniques, and Applications}},
	series		= {},
	author 		= {Matthew O. Ward and Georges Grinstein and Daniel Keim},
	editor 		= {},
	edition 	= {2nd},
	publisher	= {A K Peters/CRC Press},
	year		= {2015},
	month		= "",
	language	= {english},
	url			= {https://www.crcpress.com/Interactive-Data-Visualization-Foundations-Techniques-and-Applications/Ward-Grinstein-Keim/p/book/9781482257373},
	doi 		= {},
	isbn		= {9781482257373},
	keywords	= {},
	abstract	= {An Updated Guide to the Visualization of Data for Designers, Users, and Researchers

	Interactive Data Visualization: Foundations, Techniques, and Applications, Second Edition provides all the theory, details, and tools necessary to build visualizations and systems involving the visualization of data. In color throughout, it explains basic terminology and concepts, algorithmic and software engineering issues, and commonly used techniques and high-level algorithms. Full source code is provided for completing implementations.

	New to the Second Edition: New related readings, exercises, and programming projects; Better quality figures and numerous new figures; New chapter on techniques for time-oriented data

	This popular book continues to explore the fundamental components of the visualization process, from the data to the human viewer. For developers, the book offers guidance on designing effective visualizations using methods derived from human perception, graphical design, art, and usability analysis. For practitioners, it shows how various public and commercial visualization systems are used to solve specific problems in diverse domains. For researchers, the text describes emerging technology and hot topics in development at academic and industrial centers today.

	Each chapter presents several types of exercises, including review questions and problems that motivate readers to build on the material covered and design alternate approaches to solving a problem. In addition, programming projects encourage readers to perform a range of tasks, from the simple implementation of algorithms to the extension of algorithms and programming techniques.

	Web Resource: A supplementary website includes downloadable software tools and example data sets, enabling hands-on experience with the techniques covered in the text. The site also offers links to useful data repositories and data file formats, an up-to-date listing of software packages and vendors, and instructional tools, such as reading lists, lecture slides, and demonstration programs.}
}

@inproceedings{shneiderman1996teh,
	author		= {Ben Shneiderman},
	title		= {{{The Eyes Have It}: A Task by Data Type Taxonomy for Information Visualizations}},
	booktitle	= {Proceedings 1996 IEEE Symposium on Visual Languages},
	pages		= {336--343},
	address		= {Boulder, Colorado, USA},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {1996},
	month		= "3--6~" # sep,
	language	= {english},
	url			= {https://doi.org/10.1109/VL.1996.545307},
	doi 		= {10.1109/VL.1996.545307},
	keywords	= {Eyes, Taxonomy, Data visualization, Displays, Information filtering, Information filters, Data mining, Art, Multimedia databases, Visual databases},
	abstract	= {A useful starting point for designing advanced graphical user interfaces is the visual information seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. The paper offers a task by data type taxonomy with seven data types (one, two, three dimensional data, temporal and multi dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts).}
}

@book{munzner2014vad,
	title		= {{Visualization Analysis \& Design}},
	series		= {A K Peters Visualization Series},
	author 		= {Tamara Munzner},
	editor 		= {},
	edition 	= {1st},
	publisher	= {A K Peters/CRC Press},
	year		= {2014},
	month		= "26~" # nov,
	language	= {english},
	url			= {https://www.routledge.com/Visualization-Analysis-and-Design/Munzner/p/book/9781466508910},
	doi 		= {},
	isbn		= {9781466508910},
	keywords	= {},
	abstract	= {}
}

@article{brehmer2013aml,
	author		= {Matthew Brehmer and Tamara Munzner},
	title		= {{A Multi-Level Typology of Abstract Visualization Tasks}},
	journal		= {IEEE Transactions on Visualization and Computer Graphics},
	pages		= {2376--2885},
	volume		= {19},
	number		= {12},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2013},
	month		= "16~" # oct,
	language	= {english},
	url			= {https://doi.org/10.1109/TVCG.2013.124},
	doi 		= {10.1109/TVCG.2013.124},
	keywords	= {Augmented Reality, Virtual Reality, Immersive Visualization, Immersive Analytics, Visualization Toolkit},
	abstract	= {The considerable previous work characterizing visualization usage has focused on low-level tasks or interactions and high-level tasks, leaving a gap between them that is not addressed. This gap leads to a lack of distinction between the ends and means of a task, limiting the potential for rigorous analysis. We contribute a multi-level typology of visualization tasks to address this gap, distinguishing why and how a visualization task is performed, as well as what the task inputs and outputs are. Our typology allows complex tasks to be expressed as sequences of interdependent simpler tasks, resulting in concise and flexible descriptions for tasks of varying complexity and scope. It provides abstract rather than domain-specific descriptions of tasks, so that useful comparisons can be made between visualization systems targeted at different application domains. This descriptive power supports a level of analysis required for the generation of new designs, by guiding the translation of domain-specific problems into abstract tasks, and for the qualitative evaluation of visualization usage. We demonstrate the benefits of our approach in a detailed case study, comparing task descriptions from our typology to those derived from related work. We also discuss the similarities and differences between our typology and over two dozen extant classification systems and theoretical frameworks from the literatures of visualization, human-computer interaction, information retrieval, communications, and cartography.}
}

@article{yi2007tad,
	author		= {Ji Soo Yi and Youn ah Kang and John T. Stasko and Julie A. Jacko},
	title		= {{Toward a Deeper Understanding of the Role of Interaction in {Information Visualization}}},
	journal		= {IEEE Transactions on Visualization and Computer Graphics},
	pages		= {1224--1231},
	volume		= {13},
	number		= {6},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2007},
	month		= "" # nov # "--" # "" # dec,
	language	= {english},
	url			= {https://doi.org/10.1109/TVCG.2007.70515},
	doi 		= {10.1109/TVCG.2007.70515},
	keywords	= {Information visualization, interaction, interaction techniques, taxonomy, visual analytics},
	abstract	= {Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user's intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction.}
}

@inproceedings{laviola2000mav,
	author		= {Joseph J. {LaViola, Jr.}},
	title		= {{{MSVT}: A {Virtual Reality}-Based Multimodal Scientific Visualization Tool}},
	booktitle	= {Proceedings of the Third IASTED International Conference on Computer Graphics and Imaging},
	pages		= {7},
	address		= {},
	publisher	= {International Association of Science and Technology for Development (IASTED)},
	year		= {2000},
	month		= "" # nov,
	language	= {english},
	url			= {http://cs.brown.edu/people/jlaviola/pubs/msvt.pdf},
	doi 		= {},
	keywords	= {Multimodal Interaction, Virtual Environ- ments, Scientific Visualization, Speech Recognition},
	abstract	= {Recent approaches to providing users with more natural methods of interacting with virtual environment applica- tions have shown that more than one mode of input can be both beneficial and intuitive as a communication medium between humans and computer applications. Although there are many different modes that could be used in these applications, hand gestures and speech appear to be two of the most logical since users will typically be in environ- ments that will have them immersed in a virtual world with limited access to traditional input devices such as the key- board or mouse. In this paper, we describe a prototype application, MSVT (Multimodal Scientific Visualization Tool), for visualizing fluid flow around a dataset. MSVT uses a multimodal interface which combines whole-hand and voice input to allow users to visualize and interact with the dataset in a natural manner. A discussion of the various interaction techniques, and the results of an informal user evaluation are presented.}
}

@inproceedings{osawa2000ign,
	author		= {Noritaka Osawa and Kikuo Asai and Yuji Y. Sugimoto},
	title		= {{Immersive Graph Navigation Using Direct Manipulation and Gestures}},
	booktitle	= {Proceedings of the ACM Symposium on Virtual Reality Software and Technology (VRST 2000)},
	pages		= {147--152},
	address		= {Seoul, Korea},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {2000},
	month		= "22--25~" # oct,
	language	= {english},
	url			= {https://doi.org/10.1145/502390.502418},
	doi 		= {10.1145/502390.502418},
	isbn		= {978-1-58113-316-5},
	keywords	= {graph visualization, graph navigation, heat models, direct manipulation, hand gestures},
	abstract	= {An immersive graph visualization and navigation system is proposed. Its visualization is based on a multiple-focus layout technique using heat models. Virtual temperatures influence the graph layout. The navigation uses a combination of the layout technique and hand gestures. The system allows one to have multiple-focus nodes and to move a focus by direct manipulation and hand gestures dynamically. Direct manipulation by hand can arrange nodes and choose focus nodes. Hand gestures can control a focus area using a spotlight-like heat radiation, A forefinger points the direction of the spotlight and an angle between the forefinger and the thumb controls a spread angle of the spotlight. This technique enables one to navigate a graph in an immersive virtual space.}
}

@inproceedings{betella2014uln,
	author		= {Alberto Betella and Enrique {Martínez Bueno} and Wipawee Kongsantad and Riccardo Zucca and Xerxes D. Arsiwalla and Pedro Omedas and Paul F. M. J. Verschure},
	title		= {{Understanding Large Network Datasets through Embodied Interaction in {Virtual Reality}}},
	booktitle	= {Proceedings of the 2014 Virtual Reality International Conference (VRIC 2014)},
	pages		= {1--7},
	address		= {Laval, France},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {2014},
	month		= "9--11~" # apr,
	language	= {english},
	url			= {https://doi.org/10.1145/2617841.2620711},
	doi 		= {10.1145/2617841.2620711},
	isbn		= {978-1-4503-2626-1},
	keywords	= {immersive technologies, interaction, visualization},
	abstract	= {The intricate web of information we generate nowadays is more massive than ever in the history of mankind. The sheer enormity of big data makes the task of extracting semantic associations out of complex networks more complicated. Stemming this "data deluge" calls for novel unprecedented technologies. In this work, we engineered a system that enhances a user's understanding of large datasets through embodied navigation and natural gestures. This system constitutes an immersive virtual reality environment called the "eXperience Induction Machine" (XIM). One of the applications that we tested using our system is the exploration of the human connectome: the network of nodes and connections that underlie the anatomical architecture of the human brain. As a comparative validation of our technology, we then exposed participants to a connectome dataset using both our system and a state-of-the-art software for visualization and analysis of the same network. We systematically measured participants' understanding and visual memory of the connectomic structure. Our results showed that participants retained more information about the structure of the network when using our system. Overall, our system constitutes a novel approach in the exploration and understanding of large complex networks.}
}

@article{kolence1973sup,
	author		= {Kenneth W. Kolence and Philip J. Kiviat},
	title		= {{{Software Unit Profiles} \& {Kiviat Figures}}},
	journal		= {ACM SIGMETRICS Performance Evaluation Review},
	pages		= {2--12},
	volume		= {2},
	number		= {3},
	publisher	= {Association for Computing Machinery (ACM)},
	year		= {1973},
	month		= "" # sep,
	language	= {english},
	url			= {https://doi.org/10.1145/1041613.1041614},
	doi 		= {10.1145/1041613.1041614},
	issn 		= {0163-5999},
	keywords	= {},
	abstract	= {In the June, 1973 issue of the Performance Evaluation Review, the concept of using circular graphs (called Kiviat graphs by Kolence) to present system performance data was introduced in the column The Software Empiricist. In this article we wish to report on some recent work in using such graphs to present system and program profiles in a strikingly visual way of potential use to all practitioners of computer measurement. In discussing this data, we find it necessary to comment on the meaning of the variables used for such profiles in a way which also should be of interest to practitioners.}
}

@article{ivanov2019awa,
	author		= {Alexander Ivanov and Kurtis Danyluk and Christian Jacob and Wesley Willett},
	title		= {{{A Walk Among the Data}: Exploration and Anthropomorphism in Immersive Unit Visualizations}},
	journal		= {IEEE Computer Graphics and Applications},
	pages		= {19--28},
	volume		= {39},
	number		= {},
	publisher	= {Institute of Electrical and Electronics Engineers (IEEE)},
	year		= {2019},
	month		= "" # may # "--" # "" # jun,
	language	= {english},
	url			= {https://doi.org/10.1109/MCG.2019.2898941},
	doi 		= {10.1109/MCG.2019.2898941},
	keywords	= {Data Visualisation, Image Representation, Virtual Reality, Individual Items, Immersive Exploration Support, Objects Represention, Affective Personal Experiences Creation, Multiple Scales, Immersive Unit Visualizations Interactive Virtual Environments, Virtual Reality Prototype, Data Visualization, Visualization, Art, Tools, Virtual Environments, Two Dimensional Displays, Anthropomorphism},
	abstract	= {We examine the potential for immersive unit visualizations-interactive virtual environments populated with objects representing individual items in a dataset. Our virtual reality prototype highlights how immersive unit visualizations can allow viewers to examine data at multiple scales, support immersive exploration, and create affective personal experiences with data.}
}

@article{brooke2013sus,
	author		= {John Brooke},
	title		= {{{SUS}: A Retrospective}},
	journal		= {Journal of Usability Studies},
	pages		= {29--40},
	volume		= {8},
	number		= {2},
	publisher	= {User Experience Professional Association},
	year		= {2013},
	month		= "" # feb,
	language	= {english},
	url			= {http://uxpajournal.org/sus-a-retrospective/},
	doi 		= {},
	keywords	= {},
	abstract	= {Rather more than 25 years ago, as part of a usability engineering program, I developed a questionnaire—the System Usability Scale (SUS)—that could be used to take a quick measurement of how people perceived the usability of computer systems on which they were working. This proved to be an extremely simple and reliable tool for use when doing usability evaluations, and I decided, with the blessing of engineering management at Digital Equipment Co. Ltd (DEC; where I developed SUS), that it was probably something that could be used by other organizations (the benefit for us being that if they did use it, we potentially had something we could use to compare their systems against ours). So, in 1986, I made SUS freely available to a number of colleagues, with permission to pass it on to anybody else who might find it useful, and over the next few years occasionally heard of evaluations of systems where researchers and usability engineers had used it with some success.


	Eventually, about a decade after I first created it, I contributed a chapter describing SUS to a book on usability engineering in industry (Brooke, 1996). Since then its use has increased exponentially. It has now been cited in (at the time of writing) more than 1,200 publications and has probably been used in many more evaluations that have not been published. It has been incorporated into commercial usability evaluation toolkits such as Morae, and I have recently seen several publications refer to it as an ``industry standard''—although it has never been through any formal standardization process.


	What is SUS, how did it get developed, and how has it reached this status of a de facto standard?}
}

@article{bangor2009dwi,
	author		= {Aaron Bangor and Philip Kortum and James Miller},
	title		= {{Determining What Individual {SUS} Scores Mean: Adding an Adjective Rating Scale}},
	journal		= {Journal of Usability Studies},
	pages		= {114--123},
	volume		= {4},
	number		= {3},
	publisher	= {User Experience Professional Association},
	year		= {2009},
	month		= "" # may,
	language	= {english},
	url			= {http://uxpajournal.org/determining-what-individual-sus-scores-mean-adding-an-adjective-rating-scale/},
	doi 		= {},
	keywords	= {},
	abstract	= {The System Usability Scale (SUS) is an inexpensive, yet effective tool for assessing the usability of a product, including Web sites, cell phones, interactive voice response systems, TV applications, and more. It provides an easy-to-understand score from 0 (negative) to 100 (positive). While a 100-point scale is intuitive in many respects and allows for relative judgments, information describing how the numeric score translates into an absolute judgment of usability is not known. To help answer that question, a seven-point adjective-anchored Likert scale was added as an eleventh question to nearly 1,000 SUS surveys. Results show that the Likert scale scores correlate extremely well with the SUS scores (r=0.822). The addition of the adjective rating scale to the SUS may help practitioners interpret individual SUS scores and aid in explaining the results to non-human factors professionals.}
}

@article{obrien2018apa,
	author		= {Heather L. {O'Brien} and Paul Cairns and Mark Hall},
	title		= {{A practical approach to measuring user engagement with the refined {User Engagement Scale} ({UES}) and new {UES} short form}},
	journal		= {International Journal of Human-Computer Studies},
	pages		= {28--39},
	volume		= {112},
	number		= {},
	publisher	= {Elsevier Inc.},
	year		= {2018},
	month		= "" # apr,
	language	= {english},
	url			= {https://doi.org/10.1016/j.ijhcs.2018.01.004},
	doi 		= {10.1016/j.ijhcs.2018.01.004},
	keywords	= {User engagement, Questionnaires, Measurement, Reliability, Validity},
	abstract	= {User engagement (UE) and its measurement have been of increasing interest in human-computer interaction (HCI). The User Engagement Scale (UES) is one tool developed to measure UE, and has been used in a variety of digital domains. The original UES consisted of 31-items and purported to measure six dimensions of engagement: aesthetic appeal, focused attention, novelty, perceived usability, felt involvement, and endurability. A recent synthesis of the literature questioned the original six-factors. Further, the ways in which the UES has been implemented in studies suggests there may be a need for a briefer version of the questionnaire and more effective documentation to guide its use and analysis. This research investigated and verified a four-factor structure of the UES and proposed a Short Form (SF). We employed contemporary statistical tools that were unavailable during the UES’ development to re-analyze the original data, consisting of 427 and 779 valid responses across two studies, and examined new data (N=344) gathered as part of a three-year digital library project. In this paper we detail our analyses, present a revised long and short form (SF) version of the UES, and offer guidance for researchers interested in adopting the UES and UES-SF in their own studies.}
}

@book{nent2016gfr,
	title		= {{Guidelines For Research Ethics in Science and Technology}},
	series		= {},
	author 		= {{Norwegian National Committee For Research Ethics in Science and Technology}},
	editor 		= {},
	edition 	= {2nd},
	publisher	= {The Norwegian National Research Ethics Committees},
	year		= {2016},
	month		= "" # jun,
	language	= {english},
	url			= {https://www.forskningsetikk.no/en/guidelines/science-and-technology/guidelines-for-research-ethics-in-science-and-technology/},
	doi 		= {},
	isbn		= {9788276820751},
	keywords	= {},
	abstract	= {These guidelines for research ethics were prepared by the National Committee for Research Ethics in Science and Technology (NENT) in 2007 and revised in 2015. The guidelines supplement existing international guidelines on research ethics.[1] In interdisciplinary projects that include, for example, human medicine or social sciences, the research ethics guidelines applying to these disciplines must also be observed.

	Research institutions are responsible for ensuring that the guidelines are implemented and observed in their research communities and that they are routinely communicated to staff and students. The institutions should also establish procedures for preventing and dealing with scientific misconduct. They should moreover have mechanisms for addressing and resolving potential conflicts and cases of doubt relating to research ethics.}
}

@book{swedishresearchcouncil2017grp,
	title		= {{Good Research Practice}},
	series		= {},
	author 		= {{Swedish Research Council}},
	editor 		= {},
	edition 	= {2nd},
	publisher	= {Swedish Research Council, Vetenskapsrådet, Stockholm, Sweden},
	year		= {2017},
	month		= "12~" # jun,
	language	= {english},
	url			= {https://www.vr.se/english/analysis/reports/our-reports/2017-08-31-good-research-practice.html},
	doi 		= {},
	isbn		= {9789173073547},
	keywords	= {},
	abstract	= {This is a partially revised version of Good Research Practice, published in 2011. The revision covers areas such as changes in legislation.

	Research ethics is not static, neither as a discipline nor as a practice. When the scientific landscape changes, sometimes the debate about research ethics shifts as well. New principles may be added, and old ones may need to be reinterpreted or applied differently.

	Ethical considerations in research are largely a matter of finding a reasonable balance between various interests that are all legitimate. The quest for knowledge is one such interest. Individual privacy interests as well as protection against various forms of harm or risk of harm are other legitimate interests. Issues like the handling of integrity-sensitive material raise questions about the interests of the researcher, the study participants and other researchers, but also about what a researcher is able to promise participants and who owns research material.

	This book addresses relevant legislation and ethical requirements and recommendations against the background of questions that may arise in research work. The aim is to provide an orientation among the issues and problems, stimulate thought and contribute to the debate on responsibility and challenges. The book primarily addresses researchers, not least the younger generation, to help them make well-reasoned research ethical decisions.}
}

@article{reski2022aee,
	author		= {Nico Reski and Aris Alissandrakis and Andreas Kerren},
	title		= {{An Empirical Evaluation of Asymmetric Synchronous Collaboration Combining Immersive and Non-Immersive Interfaces Within the Context of {Immersive Analytics}}},
	journal		= {Frontiers in Virtual Reality},
	pages		= {743445:1--29},
	volume		= {2},
	number		= {},
	publisher	= {{Frontiers Media S.A.}},
	month		= "17~" # jan,
	year		= {2022},
	language	= {english},
	url			= {https://doi.org/10.3389/frvir.2021.743445},
	doi 		= {10.3389/frvir.2021.743445},
	diva 		= {http://urn.kb.se/resolve?urn=urn:nbn:se:lnu:diva-109309},
	issn		= {2673-4192},
	keywords	= {asymmetric user roles, computer-supported cooperative work, heterogeneous display and interaction technologies, immersive analytics, empirical evaluation, spatio-temporal data exploration, synchronous remote collaboration, virtual reality},
	abstract	= {Collaboration is an essential part of data analysis, allowing multiple users to combine their expertise and to debate about the interpretation of data discoveries using their contextual knowledge. The design of collaborative interfaces within the context of Immersive Analytics remains challenging, particularly due to the various user-centered characteristics of immersive technologies. In this article, we present the use case of a system that enables multiple users to synchronously explore the same data in a collaborative scenario that combines immersive and non-immersive interfaces in an asymmetric role setup. Such a setup allows for bridging the gap when applying heterogeneous display and interaction technologies, enabling each analyst to have an independent and different view of the data, while maintaining important collaborative aspects during the joint data exploration. We developed an immersive VR environment (head- mounted display, 3D gestural input) and a non-immersive desktop terminal (monitor, keyboard and mouse) centered around spatio-temporal data exploration. Supported through a real-time communication interface, synchronous collaborative features are integrated in both interfaces, facilitating the users in their ability to establish a shared context and to make spatio-temporal references. We conducted an empirical evaluation with five participant pairs (within-subject design) to investigate aspects of usability, user engagement, and collaboration during a confirmative analysis task. Synthesis of questionnaire results in combination with additional log file analysis, audio activity analysis, and observations, revealed good usability scores, high user engagement, as well as overall close and balanced collaboration of enthusiastic pairs during the task completion independent of their interface type, validating our system approach in general. Further supported through the self-constructed Spatio-Temporal Collaboration Questionnaire, we are able to contribute with discussion and considerations of the presented scenario and the synchronous collaborative features for the design of similar applications.}
}
