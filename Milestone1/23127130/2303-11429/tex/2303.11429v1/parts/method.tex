\subsection{Data}
The data was collected from two challenges PhysioNet/CinC Challenge 2017 and 2020~\cite{cinc2017, cinc2020}. The disclosure data was split into train/validation/test subsets with the ratio 60/20/20.

\begin{table}[h]
\caption{Descriptive statistics for CinC 2017 and CinC 2020 datasets}
\centering
\begin{tabular}{llrrrrrr}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Samples}}} & \multicolumn{1}{l}{\multirow{2}{*}{\textbf{Rate}}} & \multicolumn{4}{c}{\textbf{Length (second)}} \\ \cline{5-8} 
 &  & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{\textbf{Mean}} & \multicolumn{1}{l}{\textbf{Min}} & \multicolumn{1}{l}{\textbf{Median}} & \multicolumn{1}{l}{\textbf{Max}} \\ \hline
 & Train & 5116 & 300 & 32.4 & 9.1 & 30.0 & 61.0 \\
CinC 2017 & Val & 1706 & 300 & 32.8 & 9.8 & 30.0 & 60.6 \\
 & Test & 1706 & 300 & 32.5 & 9.0 & 30.0 & 60.8 \\ \hline
 & Train & 25860 & 257 - 1000 & 15.4 & 5.0 & 10.0 & 1800.0 \\
CinC 2020 & Val & 8620 & 257 - 1000 & 16.1 & 5.0 & 10.0 & 1800.0 \\
 & Test & 8621 & 257 - 1000 & 16.1 & 5.0 & 10.0 & 1800.0 \\ \hline
\end{tabular}
\end{table}


The CinC 2017 dataset was recorded by AliveCor device and contains 8528 single lead signals. The length of recordings is from 9 to 60 seconds, the average length is about 32 seconds. Every ECG signal was recorded at 300 Hz and already filtered by the recorder. The host provided the data in WFDB format with a .mat file containing signal data and a .hea file containing headers for basic information including ID, recording parameters, and patient information. % The distribution of classes and Poincaré diagram of CinC 2017 was presented in Figure \ref{fig:cinc2017_class_dist} and \ref{fig:cinc2017_classes} 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/classes_distribution.pdf}
    \caption{The classes distribution in datasets CinC 2017 (left side) and CinC 2020 (right side)}
    \label{fig:classes_distribution}
\end{figure}

The CinC 2020 dataset contains 12-lead signals which come from five different sources: CPSC Database and CPSC-Extra Database, INCART Database, PTB and PTB-XL Database, The Georgia 12-lead ECG Challenge (G12EC) Database, and the Private Database.

The dataset CPSC is the collection of the ECG signals of Chinese patients which were recorded at 500 Hz. The patient's gender and age were disclosed in this dataset, however, the age of over-89-year-old patients is masked as 92 due to the HIPAA guidelines.
%
The INCART database contains 30-min recordings at 257 Hz while the PTB and Georgia datasets consist of 10-second recordings only.
%
The private data is not public so this source was not     included in our work.
%
The remaining dataset was split into train/test/split with the ratio 60/20/20.
%
Like the CinC 2017 dataset, the data from CinC 2020 is also WFDB-compliant. The header files embedded the demographics information and diagnosis labels. 
%The Figure \ref{fig:classes_distribution} and \ref{fig:cinc2020_classes} show an overview of the class distribution and an example from the CinC 2020 dataset.

\subsection{Model architecture and training pipeline}

This section describes the detail of the configuration of each model as well as the flow of data when training the model. The overview of the training pipeline is given in Figure \ref{fig:pipeline} and interpreted in the following.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/training_pipeline.pdf}
    \caption{The high-level scheme of training pipeline.}
    \label{fig:pipeline}
\end{figure}

\subsubsection{Learning over Poincaré representation}

For the methods based on the Poincaré diagram, the input ECG signals were preprocessed by \texttt{biosppy}~\cite{biosppypaper} to extract the R-peak positions from the signal \cite{zong2003open}. This library filters the ECG signal in the frequency range from 3 to 45 before using Hamilton algorithm \cite{hamilton2002open} to detect the R-peak. The distance between R-peaks (or RR intervals) was evaluated from the R-peak location. Furthermore, in our study, we only used the NN intervals which are the distances between normal R-peaks after removing the noise and artifacts. The Poincaré diagram was constructed by plotting the scatter charts for $NN_i$ and $NN_{i+1}$ intervals. Figure \ref{fig:poincare_diagram} shows the examples of Poincaré diagram of a short and long recording.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/short_poincare.pdf}
        \caption{}
        \label{fig:short_poincare}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/long_poincare.pdf}
        \caption{}
        \label{fig:long_poincare}
    \end{subfigure}
    \caption{The Poincaré diagrams of the short-term (a) and long-term (b) ECG. The diagrams plot the normal R-peak intervals (or NN~intervals).}
    \label{fig:poincare_diagram}
\end{figure}

To predict the heart disease over the Poincaré diagram, the default architecture of \texttt{ResNet50}~\cite{he2016deep} and \texttt{DenseNet121}~\cite{huang2017densely} were used to train from the scratch (without pre-trained weights). The last layers of these models were also tailored to match the number of classes of each dataset. The Gradient-weighted Class Activation Mapping (GradCAM) \cite{Selvaraju2017} was constructed to explore the mechanism behind the model decision.

\subsubsection{Learning over 1D signal}

The 1D CNN model comprises twelve base blocks. Each base block consists of a 1D Convolutional layer, 1D Batch Normalization, Activation function, Pooling layer, and Drop-out layer.  In the 1D Convolutional layer, the padding is always \texttt{'valid'} while the stride size is always 1. The output channel starts at 256 and decreases gradually to 32 in the last convolutional layer, and the kernel size starts at 20 followed by 5 layers with a kernel size of 5, and then 3 for the remaining layers. The Batch Normalization layers have the number of weights the same as the number of output channels of the prior convolutional layer. The momentum of normalization is 0.99 for every block. The Pooling of base block is \texttt{MaxPooling1d} of which the kernel size and stride size are 2. The dropout probability is set to 0.3 in every place. Before flattening the tensor and feeding to the last fully connected layer for the logit outputs, there is an Average pooling layer with a kernel size of 1 and stride size of 2.

The structure of the base block in 1D ResNet includes a 1D Convolutional layer, 1D Batch Normalization, ReLU activation function, a Drop-out layer, another 1D Convolutional layer, and 1D Batch Normalization. In a base block, the input would go through these layers before adding the residual which is also the input tensor. This summation is activated by the ReLu function after leaving the block.

In our work, the 1D ResNet starts with a 1D Convolutional layer with a kernel size of 15 and the number of output channels is 64 followed by a 1D Batch Normalization, ReLU Activation function, and Max Pooling layer. After that, there are four base blocks with kernel sizes increasing from 65 to 256. The output of the last base block goes through two pooling layers: an Average Pooling layer and a Max Pooling layer. These outputs are concatenated before feeding to the final fully connected layer to compute the output logits.

In both 1D CNN and 1D ResNet models, the signals are converted to first-order difference and scaled to zero mean and unit variance before transferring to the models. We also adapted the GradCAM \cite{Selvaraju2017} to figure out which regions in ECG recordings contribute to the model results.

\subsubsection{Learning over XGBoost feature space}

In the pipeline of the XGBoost model, the processed ECG signals need to feed to module \texttt{tsfresh} to extract the features before training model. The features extraction used the default setting in the subset \texttt{EfficientFCParameters} but filtered out the time-consuming features including: \texttt{approximate\_entropy}, \texttt{sample\_entropy}, \texttt{matrix\_profile}, \texttt{number\_cwt\_peaks}, \texttt{partial\_autocorrelation}, \texttt{agg\_linear\_trend}, \texttt{augmented\_dickey\_fuller}. In the feature matrix, the pipeline filled the missing data with $-999$ and removed the low-variance features.

The hyperparameters of \texttt{XGBoost} were optimized by searching within the predefined space (Figure \ref{tab:xgb_hp}). The optimum collection was found by Bayesian optimization implemented in the library \texttt{scikit-optimize}.~\cite{skopt2021} The number of search trials was limited to 100 because of time constraints.

\begin{table}[!ht]
   \caption{The hyperparameters searching space of XGBoost.} 
   \label{tab:xgb_hp}
   \small
   \centering
   \begin{tabular}{p{0.15\linewidth} | p{0.22\linewidth}  p{0.15\linewidth}  p{0.11\linewidth}}
   \toprule\toprule
   \textbf{Component} & \textbf{Hyperparameter} & \textbf{Range} & \textbf{Distribution} \\ 
   \midrule

   Feature Elimination & \texttt{min\_features\_to\_select} & $[10, \text{\# features}]$ & uniform \\ 
   
    XGBClassifier & \texttt{max\_depth} & $[2, 100]$ & uniform \\ 
    XGBClassifier & \texttt{gamma} & $[10^{-3} , 10^3]$ & log-uniform \\ 
    XGBClassifier & \texttt{eta} & $[10^{-3} , 10^3]$ & log-uniform \\ 
    XGBClassifier & \texttt{scale\_pos\_weight} & $[10^{-3} , 10^3]$ & log-uniform \\ 
    XGBClassifier & \texttt{reg\_lambda} & $[10^{-3} , 10^3]$ & log-uniform \\ 
    XGBClassifier & \texttt{reg\_alpha} & $[10^{-3} , 10^3]$ & log-uniform \\ 
   
   \bottomrule
  \end{tabular}
  
\end{table}