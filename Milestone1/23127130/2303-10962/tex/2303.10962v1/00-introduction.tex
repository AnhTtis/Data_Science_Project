
A key component of building intelligent robots capable of operating in unstructured and cluttered human environments is the representation used to model the robot's surroundings.
Often times representations have to trade-off properties which depend on the usage scenario. These properties include the quality of the reconstruction, the ability to integrate sensor data continuously, and the computational complexity to query the representation. The importance of these aspects differs based on what components of a robotic system needs to use the representation, dictating the requirements for available capabilities, sensor data throughput, or query latency. For instance, an obstacle avoidance system needs to query for occupancy at high frequency, while a high-level planning system needs access to semantic knowledge, and finally a grasp planning system requires fine-grained segmentation information.

While in the past occupancy was the main information of interest, robotics has moved towards richer representations using semantics in recent years. A challenge is that most semantic approaches use a fixed, closed set, of pre-determined semantic labels. However, real environments contain more than a few dozen classes, and thus methods capable of handling arbitrary semantic classes, i.e. open set, are desirable. Additionally, objects in an environment do not necessarily belong to distinct, mutually exclusive classes. Certain objects might belong to several classes. A bookshelf is also a piece of furniture, for example. For high-level planning purposes, being able to reason about relations between their semantics might also be useful.

An environment representation that has wide applicability has several desirable properties, including: (1) can be built incrementally as the robot explores the environment, (2) enables real-time integration of new measurements, (3) has a compact memory footprint, (4) represents geometry at a high-level of detail, (5) is differentiable, (6) supports open set semantic queries, and (7) allows fast querying by downstream modules. Previously introduced 3D semantic scene representations are either built from global scene information \cite{peng2022openscene}, use closed set semantics \cite{grinvald2019volumetric, rosinol2020kimera, zhi2021ilabel, mazur2022feature}, operate on a fixed level of detail \cite{grinvald2019volumetric, rosinol2020kimera}, or are not differentiable \cite{grinvald2019volumetric, rosinol2020kimera}. In this paper, we take a step towards a representation which has the above-mentioned properties.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/hero_image.jpg}
    \caption{Our method enables real-time segmentation of scenes into arbitrary text classes provided at run-time.}
    \label{fig:main}
    \vspace{-0.2cm}
\end{figure}

Vision-language models (VLM) have shown remarkable performance on open vocabulary object detection \cite{zareian2021open, gu2021open}. Recently, these results have been extended to dense semantic segmentation \cite{ghiasi2022scaling, li2022language, zhang2022glipv2, zou2022generalized}. Some of these methods \cite{ghiasi2022scaling, li2022language} associate each pixel with a semantically meaningful vector, which is embedded in the same high-dimensional vector space as natural language prompts through a text encoder. This allows direct computation of the similarity between text prompts and image features at run-time.

As vision-language models can be trained on massive web-scale datasets that can be collected automatically without human supervision, they often show better generalization capabilities than models trained on smaller closed-set manually curated datasets. Additionally, VLMs can capture the long tail of scenarios and classes that are so rare that they are unlikely to be included in curated datasets. These properties offer great promise for applications in robotics, where we might want our robots to be able to perform new tasks in never-before-seen environments. 

In this paper, we present a method for grounding dense vision-language features into a 3D implicit neural representation that can be built up incrementally, in real-time, as new observations come in. We jointly model radiance, vision-language model features, and density in the scene using an implicit neural representation. Our representation can be incrementally built up given posed images of the scene and a pre-trained language model. We can directly compute the similarity between natural language text prompts and either 3D points or 2D image coordinates for any given viewpoint of the scene through volumetric rendering. This enables semantically segmenting a scene zero-shot into text categories provided at run-time, without having to fine-tune the system on any domain specific semantics.

In experiments, we showcase results in real-world experiments where we build up our scene representation in real-time on a real system, and demonstrate the ability to segment the scene into different classes provided as natural language prompts at run-time. We additionally present quantitative segmentation results on the large and diverse ScanNet dataset. To the best of our knowledge, our method is the first real-time capable 3D vision-language neural implicit representation. Our implementation will be made available through the Autolabel project \footnote{https://github.com/ethz-asl/autolabel}.

% Need to check if the fused features are actually better than the raw 2D features. If that is the case, this would be a nice result to showcase.
