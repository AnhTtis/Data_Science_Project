
\subsection*{Open Vocabulary Semantic Segmentation and Vision-Language Models}

CLIP \cite{radford2021learning} introduced a visual-language model capable of mapping images into the same vector space as natural language queries by correlating images to their text descriptions mined from the open web.
Open vocabulary segmentation methods typically learn dense features which are compared to text queries given at run-time \cite{li2022language, ghiasi2022scaling}. Others take a multi-task learning approach, fusing a task prompt with the architecture \cite{zhang2022glipv2, zou2022generalized}.
Other methods such as Clippy \cite{ranasinghe2022perceptual} explored learning pixel-aligned visual-language models from large scale web datasets without requiring segmentation labels, potentially enabling large-scale open set training, if the results can be extended to full semantic segmentation. 

\subsection*{Language Models in Robotics}

Large language models have been explored as an approach to high-level planning \cite{ahn2022can, huang2022visual, song2022llm, chen2022open, raman2022planning} and scene understanding \cite{chen2022leveraging, ha2022semantic}. Vision-language models embedding image features into the same space as text have been applied to open vocabulary object detection \cite{song2022llm, chen2022open}, natural language maps \cite{blukis2021few, chen2022open, shafiullah2022clip, huang2022visual, tan2022self}, and for language-informed navigation \cite{shah2022lm, wang2022find, majumdar2022zson}. 

Recent methods have explored fusing global CLIP features \cite{shafiullah2022clip}, image caption embeddings \cite{ding2022language}, or dense pixel-aligned \cite{peng2022openscene} visual-language model features into a point cloud representation for scene understanding. Concurrent work ConceptFusion \cite{jatavallabhula2023conceptfusion} explores building multi-modal semantic maps by fusing features from vision-language models as well as audio into a reconstructed 3D point cloud. Similar to these, we also fuse VLM features into a 3D representation. Unlike \cite{shafiullah2022clip, peng2022openscene, jatavallabhula2023conceptfusion}, we use a continuous neural representation of geometry and semantics which we learn jointly through volumetric rendering. \cite{peng2022openscene, ding2022language} fuse image features from a pre-built point cloud using a multi-view fusion method and learn a 3D convolutional network to map scene points to dense features. Our representation can be built incrementally as measurements are collected and does not require global scene geometry upfront. 


% note: Should we somehow specify what we mean by map. I feel it's quite vague and in this case I consider both NLmaps (2D), Clip fields (sparse 3D point cloud), and our method (dense volumetric 3D) to be "maps", even though they are quite different. A SLAM person might also have a heavy preconception of a map being a representation of sparse 3D landmarks.  

% Clip-Fields \cite{shafiullah2022clip} introduced the first system capable of associating natural language text queries to coarse spatial locations within a scene. They do this by fusing CLIP features into a neural representation which maps points from a point cloud to CLIP feature vectors computed from images observing each point. While this results in a natural language queriable 3D representation, the representation is not fine enough to enable dense scene understanding, as only a single global feature vector is extracted from each image. 

% \cite{huang2022visual} 

% \cite{peng2022openscene} presented a system enabling dense 3D scene understanding by fusing dense, pixel-aligned LSeg or OpenSeg features using multi-view fusion and a 3D convolutinal neural network. 

% Our approach similarly leverages dense pixel-aligned visual-language model features, but we directly fuse extracted features into a neural field, outputting features, color and density for any point in a volume. As our approach uses a continuous feature field, we are able to render fine-grained feature maps for any given viewpoint in the scene. Our approach also doesn't require a point cloud of the scene, as our neural field representation implicitly and jointly learns the geometry, radiance and feature field from the posed input images, enabling the use of regular RGB cameras. % Let's see if BitF feature fusion actually works better than their approach.

\subsection*{Semantic Scene Representations}

Voxel-based map representations have been proposed to store semantic information about a scene \cite{strecke2019fusion, grinvald2019volumetric, narita2019panopticfusion, rosinol2020kimera, schmid2022panoptic}. These methods assign a semantic class to each individual voxel in the scene. Voxel-based dense semantic representations typically operate on static scenes, but some have explored modeling dynamic objects \cite{xu2019mid, grinvald2021tsdf++}. 

Scene graphs \cite{armeni20193d, hughes2022hydra, wu2021scenegraphfusion} have also been proposed as a candidate for a semantic scene representation that can be built-up online. Such methods decompose the scene into a graph where edges model relations between parts of the scene. The geometry of the parts are typically represented as a signed distance functions stored in a voxel grid \cite{huang2022visual}.

Neural implicit representations infer scene semantics \cite{zhi2021place, zhi2021ilabel, blomqvist2022baking, mazur2022feature, fu2022panoptic, siddiqui2022panoptic, liu2022unsupervised, kundu2022panoptic} jointly with geometry using a multi-layer perceptron or similar parametric model. These have been extended to dynamic scenes \cite{kong2023vmap}. Neural feature fields \cite{kobayashi2022d3f, tschernezkineural, blomqvist2022baking, mazur2022feature} are neural implicit representations which map continuous 3D coordinates to vector-valued features. Such representations have shown remarkable ability at scene segmentation and editing. \cite{kobayashi2022d3f} also presented some initial results on combining feature fields with vision-language features, motivating their use for language driven semantic segmentation and scene composition. 

