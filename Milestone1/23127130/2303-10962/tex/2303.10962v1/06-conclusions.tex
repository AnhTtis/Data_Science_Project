To conclude, we proposed a volumetric neural representation which is able to jointly learn geometry, radiance, and semantic feature information of a scene. We have shown that by using dense pixel-aligned vision-language features, our resulting learned representation can be used to volumetrically segment scenes into, at run-time, user defined categories. We have also shown how the representation can be used to produce dense 2D segmentation maps for queried viewpoints. Experiments on the ScanNet dataset showed competitive performance and our real-world experiments demonstrate that the method could be run onboard a robotic system.