\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{images/zs_seg_grid.png}
\caption{Randomly sampled 2D segmentation examples from the ScanNet validation set. Top row shows the original RGB images, second row shows our segmentation and the bottom row shows the ground truth segmentation from the ScanNet dataset. Black pixels in the ground truth segmentation correspond to classes not included in the 20 ScanNet evaluation classes.}
\label{fig:scannet}
\end{figure*}

In the following experiments we provide quantitative results on the ScanNet dataset. We compare our method to the OpenScene \cite{peng2022openscene} work in terms of mean intersection over union ($\mathrm{mIoU}$) and mean accuracy ($\mathrm{mAcc}$). Then, to highlight the utility of our approach in robotics application we integrate our approach with a SLAM framework. Finally, we report run-time information to demonstrate the feasibility of running our algorithm on a real robotic system.

In all our experiments, we use LSeg features \cite{li2022language} trained on the ADE20k dataset \cite{zhou2017scene}. For the loss function, we use $\lambda_d = 0.1$ and $\lambda_f = 0.5$ throughout all experiments. Having tried a range of different values, we found that they perform similarly and settled on these values in the middle of the range. In case less noisy and more accurate depth measurements are available, a higher $\lambda_d$ value might yield better results.

\subsection{ScanNet}

On the ScanNet dataset we perform evaluation both in 3D, by segmenting the provided ground truth point cloud, as well as in 2D by comparing our rendered segmentation maps to the ones provided in the dataset. We use the $20$ classes from the ScanNet benchmark. Points or pixels that do not belong to these classes are ignored.

We first fit our representation using the given RGB, depth frames and camera poses using $\num{20 000}$ optimization iterations. For 3D point cloud segmentation, we look up the feature vector for each point in the point cloud and assign it to the nearest text class using the ScanNet class label names as the text prompts. For 2D segmentation, we segment feature maps from each viewpoint in each scan and compare against the reference segmentation map.

\begin{table}[h]
\centering
\begin{tabular}{l c c}
\toprule
                         & ScanNet $\mathrm{mIoU}$ & ScanNet $\mathrm{mAcc}$\\ \midrule
OpenScene - LSeg (3D)    & 54.2         & 66.6 \\
OpenScene - OpenSeg (3D) & 47.5         & 70.7 \\
Ours - LSeg (3D)         & 47.4         & 55.8 \\ 
Ours - LSeg (2D)         & 62.5         & 80.2 \\ \bottomrule
\end{tabular}
\caption{Mean intersection-over-union agreement with the ScanNet validation set.}
\label{table:scannet-overall}
\end{table}

Table \ref{table:scannet-overall} shows mean intersection-over-union ($\mathrm{mIoU}$) results on the ScanNet validation set, averaging over scenes and classes. LSeg\cite{li2022language}/OpenSeg \cite{ghiasi2022scaling} denotes the 2D image features used. 3D denotes segmentation agreement on the given ground truth point cloud whereas 2D shows agreement against the semantic segmentation maps.

OpenScene \cite{peng2022openscene} performs better overall, but it should be noted that it makes use of the ground truth scene point cloud, whereas we only use the color and depth frames and implicitly reconstruct the geometry. We only use the scene point cloud for evaluation. We additionally show 2D segmentation results compared with the ground truth segmentation frames in the dataset. As OpenScene only segments the point cloud, only 3D segmentation accuracy is shown.

Figure \ref{fig:scannet} shows qualitative 2D segmentation masks. Our method mostly performs well, but often struggles to distinguish between semantically similar classes such as ``desk" and ``table" or ``curtain" and ``shower curtain" in the ScanNet evaluation, as we do not make use of any tuning to align the semantics of the dataset with the semantics of the vision-language vector space. The ScanNet label quality is also not perfect and our method often gets details correct which are missed by the ScanNet ground-truth labels, such as legs of tables and chairs or other thin structures.


\subsection{Real-time SLAM Experiment}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/zs_seg_timeline.jpg}
    \vspace{-0.2cm}
    \caption{Snapshots from real-time zero shot volumetric segmentations from a fixed viewpoint at given intervals. Our representation is able to learn in real-time and is already useful after a dozen seconds. Each image shows RGB rendering output for the viewpoint, overlayed with the semantic segmentation given the $6$ class prompts shown.}
    \label{fig:timeline}
\end{figure*}

To test our scene representation in a real-world robotics scenario, we integrate our system with a SLAM pipeline \footnote{Specifically the SpectacularAI SDK available here: https://github.com/SpectacularAI/sdk} using a Luxonis OAK-D Pro stereo camera. While the system is running, we integrate color, depth, and features extracted using LSeg from keyframes at $\SI{5}{Hz}$ with poses obtained from the SLAM system. In experiments, we use either the left (grayscale) camera image or RGB camera. Depth is computed using stereo matching and aligned to the keyframe camera's frame. 

To test our system we give it classes in the form of text prompts while it is running and inspect the quality of the segmentation. Using the odometry poses provided by the SLAM system, we render color, depth maps and segmentation maps from the current camera viewpoint in real-time, segmenting the camera image into the given classes. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/real_time.jpg}
    \caption{RGB renderings and semantic segmentation maps of our representation from our real-time experiment in an office environment given the prompts shown below the images.}
    \label{fig:real-time}
    \vspace{-1.5em}
\end{figure}

Figure \ref{fig:real-time} shows snapshots of a real-time experiment performed with a handheld camera in a regular office environment. The prompts used to produce the segmentation map are shown, but note that these can be changed at run-time to re-segment the scene. Figure \ref{fig:timeline} shows how quickly our representation is able to fit to a new scene when learned from scratch and integrating frames in real-time. After a dozen seconds, our method is able to produce good segmentation maps and scene reconstructions.

\subsection{Query Performance}

We time the latency and throughput of queries performed with our implementation on an Nvidia RTX $3070$ GPU. 3D semantic and density point queries can be performed at over $7$ million lookups per second with a latency of less than $10$ milliseconds. 2D ray queries can be rendered and segmented at roughly $\num{30 000}$ pixels per second using $256$ samples per ray, but this can be adjusted to to suit the desired fidelity.

