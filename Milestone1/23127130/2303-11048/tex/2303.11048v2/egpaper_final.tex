\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{booktabs}    %导言区	
\usepackage{float}
\usepackage[table]{xcolor}
\usepackage{authblk}


% \bibliographystyle{unsrt}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation}
\author[1,2]{Changsheng Lv}
\author[1,2]{Mengshi Qi \thanks{Corresponding author.}}
\author[2]{Xia Li}
\author[3]{Zhengyuan Yang}
\author[1,2]{Huadong Ma}
\affil[1]{Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia}
\affil[2]{Beijing University of Posts and Telecommunications}
\affil[3]{University of Rochester \authorcr \tt \small \{lvchangsheng,qms,mhd\}@bupt.edu.cn; lixia@bupt.cn; zhengyuan.yang13@gmail.com}
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
%%%%%%%%% ABSTRACT
\begin{abstract}
   %  \zyang{In this paper, we propose \textbf{S}emantic-\textbf{G}raph \textbf{T}ransformer~\textbf{(SGT)} for the 3D scene graph generation task.}
   % 3D scene graph generation aims at automatically parsing an observed cloud point-based 3D scene into a semantic structural graph, which can be applied in robot navigation, and AR/VR, etc. Despite recent great progress achieved in 3D object detection and localization with deep learning methods, inferring the complex spatial contextual relationships and bridging the semantic gap in the 3D domain still remains challenging and difficult. \zyang{task motivation too long, no enough intro of previous methods and method motivation. Will modify later after our discussion on analyses and key insights to sell.}  In this study, we revisit Transformer to approach this task by exploring the importance of edges in the graph embedding module and transferring knowledge from language models to enhance the visual feature with the semantic injection module. In the experiments, we evaluate our proposed model on the widely-adopted 3DSSG benchmark, and the results show the effectiveness and superiority of our method, even in the long-tail and zero-shot settings. \zyang{still quite high level, should be more concrete and empirical sentence w.r.t. some metrics/results/details}.\clv{We demonstrate the efficacy of our proposed approach in achieving remarkable advancements over the state-of-the-art technique in objects, predicates, and relationship triplet prediction, by 3.9\%, 4.0\%, and 35.9\% absolute improvements, respectively. Notably, our model showcases superior performances in complex scenes, long-tailed scenarios, and zero-shot settings, thereby affirming its effectiveness and generalizability.}Our code will be released upon the acceptance of the paper.
% In the experiments, we evaluate our proposed model on the widely-adopted 3DSSG benchmark, and the results show the effectiveness and superiority of our method, even in the long-tail and zero-shot settings. \zyang{still quite high level, should be more concrete and empirical sentence w.r.t. some metrics/results/details}.\clv{We demonstrate the efficacy of our proposed approach in achieving remarkable advancements over the state-of-the-art technique in objects, predicates, and relationship triplet prediction, by 3.9\%, 4.0\%, and 35.9\% absolute improvements, respectively. Notably, our model showcases superior performances in complex scenes, long-tailed scenarios, and zero-shot settings, thereby affirming its effectiveness and generalizability.}Our code will be released upon the acceptance of the paper.


% [zyang's previous abstract] 
In this paper, we propose the semantic graph Transformer~(SGT) for the 3D scene graph generation. The task aims to parse a cloud point-based scene into a semantic structural graph, with the core challenge of modeling the complex global structure. Existing methods based on graph convolutional networks (GCNs) suffer from the over-smoothing dilemma and could only propagate information from limited neighboring nodes. In contrast, our SGT uses Transformer layers as the base building block to allow global information passing, with two types of proposed Transformer layers tailored for the 3D scene graph generation task. Specifically, we introduce the graph embedding layer to best utilize the global information in graph edges while maintaining comparable computation costs. Additionally, we propose the semantic injection layer to leverage categorical text labels and visual object knowledge. We benchmark our SGT on the established 3DSSG benchmark and achieve a 35.9\% absolute improvement in relationship prediction's R@50 and an 80.4\% boost on the subset with complex scenes over the state-of-the-art. Our analyses further show SGT's superiority in the long-tailed and zero-shot scenarios. We will release the code and model.



% [Xia Li]
% The generation of 3D scene graphs, which involves the parsing of point cloud-based scenes into semantic structural graphs, is a challenging task due to the complex global structure. Existing methods based on graph convolutional networks~(GCNs) suffer from the over-smoothing dilemma and weakness in their ability to propagate information from limited neighboring nodes. To resolve the above dilemma, we propose the Semantic Graph Transformer~(SGT) because Transformer layers allow for global information passing. In detail, we present two types of proposed Transformer layers designed specifically for the 3D scene graph generation task. The proposed graph embedding layer effectively utilizes global information in graph edges while maintaining computational efficiency. Furthermore, the semantic injection layer leverages categorical text and visual object knowledge to enhance the performance of the SGT. We benchmark our SGT on the 3DSSG benchmark and demonstrate a significant 35.9\% improvement in relationship triplet prediction's R@50 metric over the state of the art. Additionally, the SGT performs effectively in modeling complex global scene structures, as demonstrated by the 80.40\% improvement on the subset with complex scenes. Our analyses show the SGT's superiority in long-tailed and zero-shot scenarios. We will release the code and model.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\label{sec:intro}
% How to understand a 3D scene is the intrinsic essence of human vision, which need to accurately recognize the category and localization of each object, and the complex intrinsic structural and semantic information. Hence 3D Scene Graph Generation \textbf{(SGG)} is recently introduced to imitate the human visual system to comprehensively understand the 3D scene, where the scene graph is a visually-grounded graph over the object instances and the edges depict their pairwise relationships~\cite{yu2017visual} (as depicted in Figure~\ref{fig: an overview of an example of SGG}). The significant importance of 3D scene graph representation makes it has already been applied in a wide range of visual tasks such as VR/AR scene interactions~\cite{tahara2020retargetable}, 3D scene synthesis~\cite{dhamo2021graph}, and robot navigation~\cite{gomez2020hybrid}.
% Conventional 3D scene understanding tasks focus on 3D semantic 
% segmentation~\cite{qi2017pointnet,rethage2018fully,engelmann2017exploring,hou20193d,vu2022softgroup}, object detection and classification~\cite{zheng2022hyperdet3d,qi2017pointnet,zhao20193d,qi2016volumetric}, most of which devote to the detection and classification of the individual object, and often ignore the scene context and existing inter-object relationships. Afterward, other methods~\cite{song2015sun,armeni20193d} are proposed to capture scene context as a hierarchical tree, in which the leaves refer to each object and the intermediate nodes group individual object as the scene component or functional entity. \textcolor{blue}{However, most of them can't predict inter-object relationships. Our approach builds on a higher-order problem that predicts the inter-object relationships based on the prediction of object classes and uses the relationships to improve the object prediction results.} \zyang{this paragraph on other 3D tasks may be compressed and merged into the first paragraph as the motivation of studying 3D SGG. Since SGG over traditional 3D tasks is not our claimed contribution (a method paper for better SGG), the goal is more about making the setup (e.g., input-output and motivation) clear, rather than talking too much about the task motivation.}
% %
% % predict the objects and their relationships independently regardless of the global scene structure.

%% MERGE as task intro and motivation paragraph; highlight complex scene modeling is the core challenge
Understanding a 3D scene is the essence of human vision, requiring accurate recognition of each object's category and localization, as well as the complex intrinsic structural and semantic relationship. 
Conventional 3D scene understanding tasks, such as 3D semantic segmentation~\cite{qi2017pointnet,rethage2018fully,engelmann2017exploring,hou20193d,vu2022softgroup}, object detection and classification~\cite{zheng2022hyperdet3d,qi2017pointnet,zhao20193d,qi2016volumetric}, focus primarily on the single object localization and recognition but miss higher-order object relationship information, making it challenging to deploy such 3D understanding models. To close this gap, the 3D Scene Graph Generation task~\cite{3dssg} is recently proposed, as a visually-grounded graph over the detected object instances with edges depicting their pairwise relationships~\cite{yu2017visual} (as shown in Figure~\ref{fig: an overview of an example of SGG}). The significant importance of 3D scene graph is evident as it has already been applied in a wide range of 3D vision tasks, such as VR/AR~\cite{tahara2020retargetable}, 3D scene synthesis~\cite{dhamo2021graph}, and robot navigation~\cite{gomez2020hybrid}.
% Conventional 3D scene understanding tasks focus on 3D semantic segmentation~\cite{qi2017pointnet,rethage2018fully,engelmann2017exploring,hou20193d,vu2022softgroup}, object detection and classification~\cite{zheng2022hyperdet3d,qi2017pointnet,zhao20193d,qi2016volumetric}, most of which devote to the detection and classification of the individual object, and often ignore the scene context and existing inter-object relationships. Afterward, other methods~\cite{song2015sun,armeni20193d} are proposed to capture scene context as a hierarchical tree, in which the leaves refer to each object and the intermediate nodes group individual object as the scene component or functional entity. \textcolor{blue}{However, most of them can't predict inter-object relationships. Our approach builds on a higher-order problem that predicts the inter-object relationships based on the prediction of object classes and uses the relationships to improve the object prediction results.}


%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{latex/fig-1.pdf}
    \caption{An example of 3D scene graph generation (SGG). (a) an input 3D scene, (b) global aggregation using Transformer on the right \emph{versus} local aggregation using GCN on the left, (c) result using EdgeGCN~\cite{edge-gcn}, and (d) result using our Semantic Graph Transformer (SGT). The incorrect results are highlighted in red font, whereas the correct ones are shown in black. SGT uses global information aggregation to correct predictions on objects with less information about their neighbors.
    % \zyang{the current intro figure is used to show the task setup. It may be doable to also include our own major contribution into the figure for illustration, e.g., reference the intro figure 1 in https://arxiv.org/pdf/1908.06354.pdf or https://arxiv.org/pdf/2104.08541.pdf. }
    }
    \label{fig: an overview of an example of SGG}
    \vspace{-3mm}
\end{figure} %%%%%%%%%%



% With the huge development of deep learning, increasing works~\cite{xu2017scene,herzig2018mapping,qi2019attentive,zellers2018neural,li2017scene,yang2018graph,li2018factorizable,newell2017pixels} adopt Convolutional Neural Networks~(CNNs) or Recurrent Neural Networks~(RNNs) to handle SGG task in 2D images, which explicitly model the scene graph by message passing. However, 3D scene graph generation is more challenging because the input point cloud-based object and the complex global 3D scene structure are more difficult to recognize and model than RGB images. \textcolor{blue}{Most existing methods~\cite{3dssg,edge-gcn,zhang2021knowledge,chen2022exploring} use nodes to represent objects in the scene and edges to describe relationships between objects.} Utilizing Graph Convolution Networks (GCNs) to model the objects and relationships. For example, 3DSSG method~\cite{3dssg} regards the edge as another kind of node in the graph, while~EdgeGCN~\cite{edge-gcn} exploits the edge-assisted reasoning capability of GCN for modeling. However, the main limitations of GCN-based methods lie in the over-smoothing dilemma~\cite{li2018deeper}, which makes the GCNs able to model the neighborhoods of nodes with few layers but overlooks the global-level structure of the whole scene. \zyang{only adjacent; more jump or stack layer theoretically help, but difficult to train as we will show. Point to Figure 1 for strong support.} In contrast, Transformer~\cite{vaswani2017attention} has demonstrated a strong global modeling capability in various tasks, and we try to revisit Transformer in this specific task.
% % \zyang{This paragraph tries to introduce the prior methods on 3D SSG. 1. I'm not sure why we need to mention 2D SSG, as method-wise they are also not that relevant. Probably can move to related works. 2. The second half on the limitation of GCN is nice, we can further strengthen the limitation, e.g., by providing some qualitative or quantitative examples (e.g., Fig 2 in https://arxiv.org/pdf/2008.01059.pdf), and better link them to the motivation of using Transformer. Saying ``a strong global modeling capability'' is nice, but would be better to make it more concrete.}
% %% previous methods and method motivation section; remove 2D SG part
One core challenge for 3D scene graph generation is to accurately predict complex 3D scene structures from sparse and noisy point cloud inputs. Most existing methods~\cite{3dssg,edge-gcn,zhang2021knowledge,chen2022exploring} are built based on Graph Convolution Networks (GCNs)~\cite{kipf2016semi}, where nodes represent objects in the scene and edges indicate relationships between objects. 
%Among them, SPGN~\cite{3dssg} regards the edge as another kind of node in the graph, while~EdgeGCN~\cite{edge-gcn} exploits the edge-assisted reasoning capability of GCN for modeling. 
Despite the recent success, GCN suffers from its inherent limitation of the over-smoothing dilemma~\cite{li2018deeper}. That is, GCN is effective in modeling neighboring nodes with a controlled number of layers but could struggle with learning the global structure and high-order relationships of the entire scene. %%Quantitatively, Figure~\ref{fig: an overview of an example of SGG} (c) shows that the GCN-based methods consistently deteriorate in relationship accuracy when the scene becomes more complicated. For example, there is no direct connection between ``sink'', ``heater'', ``kettle'', and ``towel'', but they are all part of a complex kitchen scene. 
As shown in Figure~\ref{fig: an overview of an example of SGG}, the GCN-based method deteriorates in relationship accuracy when the scene becomes more complicated. 
%%Transformer method Figure~\ref{fig: an overview of an example of SGG} (d) successfully predicts `towel' instead of `lamp' based on `sink', and predicts `kettle' and `heater', which should also be present in the kitchen, and GCN, which only aggregates neighbor information, cannot model the relationship of these items simultaneously.
Increasing the GCN layers or node radius could be one attempt to improve the global modeling capability, but often make the network difficult to converge and result in worse performance, as discussed in previous studies~\cite{ying2021Transformers} and our later analyses. 
In contrast, we explore a more fundamental change, \ie replacing the base building block from GCN to Transformer layers~\cite{vaswani2017attention}, which has shown a strong global context modeling capability and therefore could overcome the limitations of GCNs. % in various tasks, and we try to revisit Transformer in this specific task.


With the above analysis, we propose \textbf{S}emantic \textbf{G}raph \textbf{T}ransformer~\textbf{(SGT)} to generate a scene graph for a 3D point clouds scene. SGT takes both node and edge proposal features obtained from PointNet~\cite{qi2017pointnet} as inputs, and stacks Transformer layers to model the higher-order inter-object relationships. The node and edge classifiers then predict each node and edge's categories. However, one technical challenge is the number of edges will grow quadratically to the number of nodes, leading to a quadratically-growing input sequence length to the Transformer. To address this challenge, we propose the Graph Embedding Layer that injects the edge information only to the relevant nodes via edge-aware self-attention. In this way, SGT preserves the global context with a comparable computation cost.

Furthermore, we propose to exploit external semantic information via the Semantic Injection Layer. Previous GCN-based methods~\cite{zhang2021knowledge,chen2022exploring} have shown the benefit of learning a categorical object and relationship semantic embedding. However, these works need to train an extra module to obtain such knowledge. Moreover, the prior knowledge learned from the same training set may only work well for common categories in training set distribution but may fail to help with the long-tail object and relationship categories. Alternatively, we design Semantic Injection Layer to enhance the visual feature in object nodes. Specifically, we inject natural language knowledge (\ie word embeddings) from large-scale pre-trained language~\cite{devlin2018bert,radford2021learning,pennington2014glove} to corresponding object visual features. Note that our approach requires no extra module and significantly improves SGT's performance in long-tail and zero-shot scenarios.

% On the other hand, semantic information as prior knowledge has been shown to bring an improvement in the prediction of objects and inter-object relationships. For instance, Zellers~\etal~\cite{zellers2018neural} analyzes the regularly appearing substructures in scene graphs and predicts the relationships using the co-occurrence relationship between objects. Zhang~\etal~\cite{zhang2021knowledge} learn the prior knowledge from the dataset using GNN, and incorporate the prior knowledge with visual information to improve the prediction of the scene graph. Moreover, Chen~\etal~\cite{chen2022exploring} further propose a semantic clue with GCN to extract the objects' semantic feature from the dataset. However, most of them need to train an extra model or redundant computation to obtain such knowledge. \textcolor{blue}{Moreover, the  prior knowledge from the training set shows good performance only for the common categories but drops for the uncommon objects or unseen relationships in the training set.} Inspired by the large-scale pre-trained models~\cite{devlin2018bert, pennington2014glove,radford2021learning} to obtain rich semantic knowledge, we will explore the effectiveness of such semantic embeddings extracted from the pre-trained model, \eg, BERT~\cite{devlin2018bert}, by injecting semantic knowledge into the objects visual features. 
% \zyang{present motivation of each module. Stopped here, will resume} which mainly contains two specially designed modules: Graph Embedding Module and Semantic Injection Module. Specifically, Graph Embedding Module is introduced to simultaneously encode global-level dependencies among various nodes in a graph through edge-aware self-attention, which can avoid the over-smoothing dilemma caused by multi-layer GCNs. Furthermore, the Semantic Injection Module is designed to enhance the visual feature space of nodes by augmenting natural language knowledge from the large-scale pre-trained models, \eg, BERT~\cite{devlin2018bert}. Finally, our model classifies each node and edge through two Multi-layer Perceptrons~(MLPs) and then generates an accurate 3D scene graph.

% To address aforementioned challenges, we inherit the advantages of Transformer~\cite{vaswani2017attention} and propose a novel \textbf{S}emantic-\textbf{G}raph \textbf{T}ransformer~\textbf{(SGT)} to generate the scene graph for the 3D point clouds scene, which mainly contains two specially designed modules: Graph Embedding Module and Semantic Injection Module. Specifically, Graph Embedding Module is introduced to simultaneously encode global-level dependencies among various nodes in a graph through edge-aware self-attention, which can avoid the over-smoothing dilemma caused by multi-layer GCNs. Furthermore, the Semantic Injection Module is designed to enhance the visual feature space of nodes by augmenting natural language knowledge from the large-scale pre-trained models, \eg, BERT~\cite{devlin2018bert}. Finally, our model classifies each node and edge through two Multi-layer Perceptrons~(MLPs) and then generates an accurate 3D scene graph.
%%However, few layers of GCNs could only model the neighborhoods of nodes and can not model the scene from a global level due to over-smoothing issues caused by multi-layer GCNs~\cite{li2018deeper}. The 3D scene contains richer spatial location information than the 2D scene, we use Transformer to capture the global-level structure information in them. 
% As illustrated in Figure\ref{fig:overview of exmaple of SGG}. \textbf{Also above the floor, because of the relationship between the pillow and the chair, the relationship between the chair and the floor is not the same as the relationship between the wall and the floor}. 

% \zyang{People would expect the method paragraph ``To address aforementioned challenges, we inherit the advantages of Transformer ...'' after the previous paragraph, rather than another motivation paragraph on a different topic (semantic info). Maybe we can merge this paragraph to the next method paragraph, or move after it. Instead of two separate contributions (Transformer and semantic info), it may be more coherent to present as a semantic-enhanced Transformer, i.e., first saying using Transformer, then strengthening with injecting semantic info (which means we need to finish introducing the Transformer part first).}
% On the other hand, semantic information as prior knowledge has been shown to bring an improvement in the prediction of objects and inter-object relationships. For instance, Zellers~\etal~\cite{zellers2018neural} analyzes the regularly appearing substructures in scene graphs and predicts the relationships using the co-occurrence relationship between objects. Zhang~\etal~\cite{zhang2021knowledge} learn the prior knowledge from the dataset using GNN, and incorporate the prior knowledge with visual information to improve the prediction of the scene graph. Moreover, Chen~\etal~\cite{chen2022exploring} further propose a semantic clue with GCN to extract the objects' semantic feature from the dataset. However, most of them need to train an extra model or redundant computation to obtain such knowledge. \textcolor{blue}{Moreover, the  prior knowledge from the training set shows good performance only for the common categories but drops for the uncommon objects or unseen relationships in the training set.} Inspired by the large-scale pre-trained models~\cite{devlin2018bert, pennington2014glove,radford2021learning} to obtain rich semantic knowledge, we will explore the effectiveness of such semantic embeddings extracted from the pre-trained model, \eg, BERT~\cite{devlin2018bert}, by injecting semantic knowledge into the objects visual features. 

%%However, all of them this prior knowledge need to be analyzed or trained to get, and for scenes with a different distribution, this prior knowledge is instead noise, and both methods have good results for common categories, while for uncommon object and unseen relationships in the train data, no good feature representation can be obtained. In large-scale pre-trained models~\cite{devlin2018bert, pennington2014glove,radford2021learning}, semantic knowledge has good spatial properties and does not require retraining before being used, which is useful for improving the spatial properties of features for uncommon classes in the dataset. In this paper, we explore the effectiveness of the semantic knowledge extracted from the pre-trained model, by injecting semantic knowledge into the features of the object.

%%To address the aforementioned issues, we design an novel structure to model the relationship between nodes and edges and introduce outer knowledge to build up a better semantic space. Specifically, we propose a novel \textbf{S}emantic-\textbf{G}raph \textbf{T}ransformer network \textbf{(SGT)} that contains two modules: the Graph Embedding Module and the Semantic Injection Module. The Graph Embedding Module simultaneously models the global-level dependencies between different nodes and edges in graph and uses a Transformer structure to avoid the over-smoothing problem caused by multi-layer GCNs. To the best of our knowledge, our method is the first to use Transformer for 3D scene graph estimation. Then a Semantic Injection Module is introduced to augment the feature space of nodes by injecting outer knowledge from pre-trained language models. Finally, the object and relationship prediction module is leveraged to classify each node and edge by Multi-Layer Perceptron (MLP) and to generate an accurate scene graph. 


Our main contributions can be summarized as follows:

\par\textbf{(1)} We propose Semantic Graph Transformer (SGT) for 3D scene graph generation, which captures global dependencies between objects and models inter-object relationships with the Graph Embedding Layer. To the best of our knowledge, SGT is the first Transformer-based framework for this specific task.

\par\textbf{(2)} We introduce a Semantic Injection Layer to enhance object features with natural language knowledge and show its effectiveness in the long-tail and zero-shot scenarios. % from pre-trained BERT~\cite{devlin2018bert}.

\par\textbf{(3)} We benchmark our SGT on the established 3DSSG dataset~\cite{3dssg}, with 35.9\% absolute improvements over the state-of-the-art GCN-based method. More importantly, we achieve an 80.4\% improvement on the subset with complex scenes. 
    %%Extensive experiments on the 3DSSG benchmark demonstrate the effectiveness and superiority of our proposed model, especially in complex scenes thanks to SGT's strong global context modeling capability. %, especially under long-tail and zero-shot settings.\zyang{may consider mentioning numbers, if good}



%-------------------------------------------------------------------------

\section{Related Work}



\noindent{\bf Scene Graph Generation.}~Scene graph was first introduced into image retrieval~\cite{johnson2015image} to capture more semantic information about objects and their inter-relationships. Afterward, the first large-scale dataset, Visual Genome~\cite{krishna2017visual}, with scene graph annotations on 2D images gave rise to a line of deep learning-based advances~\cite{xu2017scene,herzig2018mapping,qi2019attentive,zellers2018neural,li2017scene,yang2018graph,li2018factorizable,newell2017pixels}. These methods explored the inter-object relationship by the massage passing strategy to improve the quality of scene graph generation. Nowadays, in order to understand the complex 3D indoor structure, 3DSSG dataset~\cite{3dssg} was first presented to tackle 3D scene graph~\cite{edge-gcn,zhang2021knowledge} from point clouds. Zhang~\etal~\cite{edge-gcn} introduced an edge-oriented GCN named EdgeGCN to learn a pair of twinning interactions between nodes and edges. Zhang~\etal~\cite{zhang2021knowledge} divide the generation task into two stages, the prior knowledge learning stage and the scene graph generation with a prior knowledge intervention. 
% Xu~\etal~\cite{xu2017scene} proposed an end-to-end framework with gated recurrent units (GRUs)~\cite{cho2014properties} which incorporate informative contextual cues, and  MotifNet~\cite{zellers2018neural} captured the high-order structure and global interactions in scene graphs, and Qi~\etal~\cite{qi2019attentive} proposed a graph self-attention module for joint graph representation in scene graph generation, projecting visual features and linguistic knowledge into a common space. 
% However, the above methods all use GCNs to model nodes and edges in scenes,
%and the methods of increasing the number of GCN layers in order to obtain more information about neighbors suffer from over-smoothing problems and therefore cannot model the global-level structure of scenes. 
However, the aforementioned methods fall short in modeling the global-level structure of scenes and increasing the number of GCN layers easily results in over-smoothing problems. We address the task differently by utilizing the edge-aware self-attention in the Graph Embedding Layer to capture adaptable global representation among nodes.
%%However, most of them ignore the global-level structure. Different from them, we address the task by utilizing the edge-aware self-attention in Graph Embedding Module to capture adaptable global representation among nodes.


\noindent{\bf Knowledge Representation.}~There has been growing interest in improving data-driven models with external semantic knowledge in natural language processing~\cite{hinton2015distilling,bao2014knowledge} and computer vision~\cite{li2017incorporating,deng2014large}, by incorporating semantic cues, (\eg language priors of object class names) into visual contents (\eg object proposals) could significantly improve the generation capability~\cite{lu2016visual,liang2018visual,pennington2014glove,speer2017conceptnet,sharifzadeh2021classification}. Early method~\cite{zellers2018neural} uses statistical co-occurrence as extra knowledge for scene graph generation by introducing a pre-computed bias into the final prediction. Chen~\etal~\cite{chen2019knowledge} presented a knowledge-embedded routing network to implicitly fuse co-occurrence probabilities as weights in the message pass step to neutralize the long-tail phenomenon. In this work, we propose a simple yet effective Semantic Injection Layer to enhance visual features with pre-trained text embedding by cross-attention. 
%%Nevertheless, using this extra knowledge helps the feature learning in the deep learning models is nontrivial. 


%%Another researches leverage the semantic knowledge by automatically extracting vector arithmetic\cite{pennington2014glove} from unstructured or semi-structured data\cite{speer2017conceptnet}, and then encoded inductive biases into the class-level prototypical representations~\cite{sharifzadeh2021classification}. 

\noindent{\bf Transformer.}~Transformer~\cite{vaswani2017attention} was first applied to natural language processing (NLP) tasks and achieved significant improvements~\cite{devlin2018bert,brown2020language}. Recently, increasing researchers have used it for computer vision~\cite{chen2020generative, dosovitskiy2020image}. Dhingra~\etal~\cite{dhingra2021bgt} utilized Transformer with bidirectional GRU (BiGRU) for 2D scene graph generation, and Dong~\etal~\cite{dong2022stacked} proposed Stacked Hybrid-Attention network to facilitate the intra-modal refinement as well as the inter-modal interaction. However, existing work has not applied Transformer to capture 3D scene graph generation so far. In this paper, we are the first to revisit Transformer's strong global modeling capability to combine the visual feature and semantic knowledge into the 3D field.

%-------------------------------------------------------------------------

%-------------------------------------------------------------------------
%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}
\subsection{Overview}

\noindent\textbf{Problem Definition.}~We define a \emph{3D scene graph} as~$G=(V, E)$, which describes the category of each object and the corresponding semantic inter-object relationships. In the graph, nodes $V$
refer to the object set, while edges $E$ mean inter-object relationships. Meanwhile, we define the output object class labels as $O = \left\{o_1,...,o_N\right\}, o_i\in \mathcal{C}_{\text{node}} $, where $\mathcal{C}_{\text{node}}$ represents all possible object categories, $N$ is the number of nodes. And the set of inter-object relationships can be defined as $R=\left\{r_1,...,r_M\right\}$, $r_j \in \mathcal{C}_{\text{edge}}$, where $\mathcal{C}_{\text{edge}}$ is the set of all pre-defined relationships classes, $M$ denotes the number of valid edges.

As illustrated in Figure~\ref{fig: an overview of model}, the overall framework of our proposed method follows a typical Transformer architecture but consists of two carefully designed components: \emph{Graph Embedding Layer}~\textbf{(GEL)} encodes the global-level features of nodes simultaneously with edge-aware self-attention; \emph{Semantic Injection Layer}~\textbf{(SIL)} extracts semantic knowledge from the pre-trained BERT and fuses the node information with the corresponding semantic features via cross-attention. For our SGT, we empirically set the $L$ layers, \ie a GEL layer and a SIL layer followed by another $L-2$ layers of GEL. We detail the analyses on $L$ and layer set up in Section~\ref{Ablation Studies} and supplementary materials.
% The analysis on $L$ we show in section~\ref{Ablation Studies}, and the analysis on the location of SIL we will show in the supplementary material.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8\linewidth]{latex/model-change.png}
    \caption{Overview of the proposed~\emph{Semantic Graph Transformer (SGT)} with $L$ layers, which mainly consists of two carefully designed components: a Graph Embedding Layer and a Semantic Injection Layer.}
    \label{fig: an overview of model}
    \vspace{-2mm}
\end{figure}
\subsection{Scene Graph Initialization}


Different from 3D instance segmentation~\cite{vu2022softgroup,Chu_2022_CVPR}, we study the higher-order problem of object relationship prediction in the scene. %%In order to explore the mutual promotion between inter-object relations and object prediction and to avoid the influence of different segmentation methods on the results, 
Note that we use point cloud data with real instance indexes but without category labels. We follow~\cite{edge-gcn} adopt the PointNet~\cite{qi2017pointnet} backbone to capture the point-wise feature~$\mathbf{X}_{P} \in \mathbb{R}^{P \times C_{\text{point}}}$ from the input point cloud~$\mathbf{P} \in \mathbb{R}^{P \times C_{\text{input}}}$ that forms 3D scene~$S$, where~$C_{\text{point}}$ and~$C_{\text{input}}$ 
denote the channel numbers of point clouds and their extracted point-wise feature, respectively, and~$P$ denote the number of sampling points~\footnote{In our experiments, we set $P = 4096$.}.

{\bf Node and edge feature generation.} Following~\cite{edge-gcn}, for the input scene~$S$, we use the average pooling function~\cite{qi2017pointnet} to aggregate the points in~$\mathbf{X}_{P}$ with the same instance index to obtain the corresponding node visual features~$\mathbf{X}_{V} \in \mathbb{R}^{N \times d_{\text{node}}}$, where $N$ indicates the number of instances in scene~$S$, and $d_{\text{node}}$ means node feature dimensions.
% we use the pooling function to aggregate X with the same instance index to obtain the corresponding in-scene node features VFollowing~\cite{edge-gcn}, for the input scene $S$, we use  point cloud~$\mathbf{P}$ with the objects' corresponding instance indexes, we use the average pooling function~\cite{qi2017pointnet} to aggregate node visual feature~$\mathbf{X}_{V_i} \in \mathbb{R}^{d_{\text{node}}}$ from the unordered points set. To generate the visual feature $\mathbf{X}_{V_i} \in \mathbb{R}^{d_{\text{node}}}$  
% for each object $V_i$ in $S$, we define a class-agnostic point-to-instance indicator like~\cite{edge-gcn}.}
% $M \in \left\{1,...,m\right\}^N$, where $d_{\text{node}}$ denotes the dimension of node feature.
%, and $\mathbf{X}_{V_i}$ is the feature for the $i$-th node. Then the operation can be formally described as:
%\begin{equation}\label{eq1}
%    \mathbf{X}_{V_i} = \math{\frac{\delta(M_i,i) \cdot \mathbf{X}_P}{\sum_{k=0}^N\delta(M_k,i)}}
%\end{equation}
%\text{Average}}(\left\{\delta(M_i,i) \cdot \mathbf{X}_{P_n}\right\})_{n=1,...,N},
%\textcolor{blue}{where $\delta(\cdot) $ denotes the Kronecker Delta, $\mathbf{X}_{P}$ denote the point-wise feature of input scene $S$.}

Unlike~\cite{3dssg} uses independent PointNet to extract inter-object relationships, we assume that all objects are connected to each other, and thus we can obtain multi-dimensional edge features based on node features. For each $\mathbf{X}_{E_{(i,j)}} \in \mathbb{R}^{d_{\text{edge}}}$, it denotes the feature for the edge $E_{(i,j)}$ that connects two points from subject $V_i$ toward object $V_j$, and the feature can be initialized as the following formula using the concatenation scheme introduced in~\cite{wang2019dynamic}:
\begin{equation}\label{eq2}
    \mathbf{X}_{E(i,j)} = (\mathbf{X}_{V_i} \parallel (\mathbf{X}_{V_j} - \mathbf{X}_{V_i})),
\end{equation}
where $\parallel$ denotes the concatenation operation.
 




\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{latex/GT-2.png}
    \caption{Illustration of the proposed Multi-Head Edge-aware Self-Attention layer, of which attention maps are obtained by dot products between queries and keys generated by nodes, where the node feature $\mathbf{V}_i^l$ used as queries and the global node features $\mathbf{V}_j^l$ adopted as keys. After fusing the edge feature $\mathbf{E}_{(i,j)}^l$ by Hadamard product, the attention map is then used to update edge features and node features.}
    \label{fig:GT}
    \vspace{-2mm}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Embedding Layer}
\label{Graph Encoding M}

Following~\cite{dwivedi2020generalization}, we feed the input node and edge features into the Graph Embedding Layer. The input node features $\mathbf{X}_{V_i} \in \mathbb{R}^{d_{\text{node}}}$ and edge features $\mathbf{X}_{E_{(i,j)}} \in \mathbb{R}^{d_{\text{edge}}}$ are passed via linear projections to embed into $d$-dimensional hidden features $\mathbf{V}_i^0$ and $\mathbf{E}_{(i,j)}^0$, respectively, as the following:
\begin{equation}
     \mathbf{V}_i^0 = \mathbf{A} \mathbf{X}_{V_i} + \mathbf{a}^0, 
\end{equation}
\begin{equation}
    \mathbf{E}_{(i,j)}^0 = \mathbf{B} \mathbf{X}_{E_{(i,j)}} + \mathbf{b}^0,
\end{equation}
where $\mathbf{A} \in \mathbb{R}^{d \times d_{\text{node}}}$, $\mathbf{B} \in \mathbb{R}^{d \times d_{\text{edge}}}$ and $\mathbf{a}^0$, $\mathbf{b}^0 \in \mathbb{R}^d $ are weights and bias of linear projection layers, respectively.

{\bf Multi-Head Edge-aware Self-Attention} is proposed in the layer for the message passing in the graph, which is different from the conventional self-attention described in~\cite{vaswani2017attention}, as shown in Figure~\ref{fig:GT}. For the $l$-th layer, we use the node feature $\mathbf{V}_i^l$ as query and the nodes feature $\mathbf{V}_j^l$ ($j=1,2,\cdots, N$) as keys and values. Moreover, the updated edge features between nodes are obtained by point-wise multiplying with the corresponding self-attention maps.
The node feature $\mathbf{V}_i^{l+1}$ in the $l+1$ layer is calculated as the concatenation of the self-attention results from $H$ heads. Besides, the edge feature $\mathbf{E}_{(i,j)}^{l+1}$ can be calculated by concatenating the edge-aware self-attention maps ${\mathbf{M}_{ij}^{l,h}} \in \mathbb{R}^{d_h}, 1 \leq h \leq\ H$, $h$ denotes the number of attention heads and $d_h$ denotes the dimension corresponding to each head. Note that, $\mathbf{M}_{ij}^{l,h}$ in our method is a vector instead of a scalar as in the standard Transformer. The information propagating from node $\mathbf{V}_j$ to $\mathbf{V}_i$ can be formulated as the following in the layer $l$:
\begin{equation}
    \hat{\mathbf{V}}_i^{l+1} =  \mathbf{O}_v^l \cdot \left[ \parallel_{h=1}^H \left( \sum_j^N \mathbf{M}_{ij}^{l,h} \circ \mathbf{W}_V^{l,h} \mathbf{V}_j^{l,h} \right) \right],
    % \hat{\mathbf{V}}_i^{l+1} =  \mathbf{O}_v^l \cdot \left( \sum_j^N \mathbf{M}_{ij}^l \circ \mathbf{W}_V^l \mathbf{V}_j^l \right),
\end{equation}
\begin{equation}
    \hat{\mathbf{E}}_{(i,j)}^{l+1} = \mathbf{O}_e^l \cdot \left[\parallel_{h=1}^H \mathbf{M}_{ij}^{l,h} \right],
    % \hat{\mathbf{E}}_{(i,j)}^{l+1} = \mathbf{O}_e^l \cdot \mathbf{M}_{ij}^l,
\end{equation}
where
\begin{equation}
    \mathbf{M}_{ij}^{l,h} = \text{softmax}_j\left(\hat{\mathbf{M}}_{ij}^{l,h}\right),
    % \mathbf{M}_{ij}^l = \text{softmax}_j\left(\hat{\mathbf{M}}_{ij}^l\right),
\end{equation}
\begin{equation}
    \hat{\mathbf{M}}_{ij}^{l,h} = \left(\frac{ (\mathbf{W}_Q^{l,h} \mathbf{V}_i^{l,h})^T \cdot \mathbf{W}_K^{l,h} \mathbf{V}_j^{l,h}}{\sqrt{d_h}}\right) \cdot \mathbf{W}_E^{l,h} \mathbf{E}_{i,j}^{l,h},
    % \hat{\mathbf{M}}_{ij}^l = \left(\frac{ (\mathbf{W}_Q^l \mathbf{V}_i^l)^T \cdot \mathbf{W}_K^l \mathbf{V}_j^l}{\sqrt{d/h}}\right) \cdot \mathbf{W}_E^l \mathbf{E}_{(i,j)}^l,
\end{equation}
where $\mathbf{W}_Q^{l,h}, \mathbf{W}_K^{l,h}, \mathbf{W}_V^{l,h}, \mathbf{W}_E^{l,h}\in \mathbb{R}^{d_h \times d_h}, \mathbf{O}_v^l, \mathbf{O}_e^l\in \mathbb{R}^{d \times d}, $ $\mathbf{O}_v^l, \mathbf{O}_e^l$ are the weights of linear layers, $\circ$ denotes the Hadamard product, $\cdot$ denotes the Hadamard product
% where $\mathbf{W}_Q^l, \mathbf{W}_K^l, \mathbf{W}_V^l, \mathbf{W}_E^l, \mathbf{O}_v^l$ , and $\mathbf{O}_e^l$ are the weights of linear layers,
and $\parallel$ denotes the concatenation operation. 
% We leave the head index $h$ for simplicity.

In order to keep the numerical stability, the outputs after taking exponents of the terms inside softmax will be clamped to a value in $[-5, 5]$. The outputs $\hat{\mathbf{V}}_i^{l+1}$ and $\hat{\mathbf{E}}_{ij}^{l+1}$ are then separately passed into Feed Forward Networks~(FFN) preceded and succeeded by residual connections and normalization layers, formulated as the following:
\begin{equation}
\begin{aligned}
    \mathbf{V}_i^{l+1} &=  \mathrm{Norm} \left( \mathrm{FFN}_V^l \left( \mathrm{Norm} \left( \mathbf{V}_i^l + \hat{\mathbf{V}}_i^{l+1} \right) \right)\right) \\
    &+ \mathrm{Norm} \left( \mathbf{V}_i^l + \hat{\mathbf{V}}_i^{l+1} \right),
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
    \mathbf{E}_{(i,j)}^{l+1} &=  \mathrm{Norm}\left(\mathrm{FFN}_E^l\left(\mathrm{Norm}\left(\mathbf{E}_{(i,j)}^l + \hat{\mathbf{E}}_{(i,j)}^{l+1}\right)\right)\right) \\
    &+ \mathrm{Norm}\left(\mathbf{E}_{(i,j)}^l + \hat{\mathbf{E}}_{(i,j)}^{l+1}\right),
\end{aligned}
\end{equation}
where $\mathrm{Norm}$ denotes layer normalization.


%%%%%%%%%%%%%

%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Semantic Injection Layer}\label{SIM}
For most known objects in a dataset, we extract their semantic knowledge by directly getting their word embedding $\mathbf{Z}_K \in \mathbb{R}^{K\times d_{\text{emb}}}$ from pre-trained word2vec models (\eg BERT~\cite{devlin2018bert} or GloVe~\cite{pennington2014glove}), where $K$ is the number of object classes. And for the unknown object categories which don't have corresponding embeddings, we set learnable parameters $\mathbf{Z}_U \in \mathbb{R}^{U\times d_{\text{emb}}}$ to represent their word embeddings, where $K + U =|\mathcal{C}_{\text{node}}|$ is the category number and $d_{\text{emb}}$ denotes the word embedding dimension. The semantic knowledge set can be formed as follows:
\begin{equation}
    \mathbf{Z} = \mathrm{Concat}(\mathbf{Z}_K, \mathbf{Z}_U).
\end{equation}

\noindent where $\mathrm{Concat}$ refers to concatenation in the column dimension, with the $\mathbf{Z} \in \mathbb{R}^{|\mathcal{C}_{\text{node}}| \times d_{\text{emb}}}$ means the semantic embedding of all object categories.

{\bf Cross-Attention} is designed in the layer to aggregate the visual features and the semantic embeddings followed by two feed-forward layers, as shown in Figure~\ref{fig: an overview of model}.
Since the node visual features and the semantic knowledge embeddings are in different latent spaces, we transform node features through a layer of a feed-forward network before fusing them. Then for the node $i$ in $l$-th layer, we use the node feature$\mathbf{V}_i^l$ as query and each object semantic embedding $\mathbf{Z}_i$ as keys and values. We calculate the cross-attention scores $\mathbf{S}_i$ between $\mathbf{V}_i^l$ and each object semantic embedding $\mathbf{Z}_j$ as following:
\begin{equation}
    \mathbf{H}_i^l = \mathrm{Norm}\left(\mathrm{FFN}_H\left(\mathbf{V}_i^l\right) + \mathbf{V}_i^l\right),
\end{equation}
\begin{equation}
    \mathbf{S}_{i,j}^l = \text{softmax}_j\left(
    \frac{(\mathbf{W}_h \mathbf{H}_i^{l})^T \cdot \mathbf{W}_z\mathbf{Z}_j}{\sqrt{d}}
    \right),
    \label{eq:sim}
\end{equation}
where $\mathbf{W}_h \in \mathbb{R}^{d \times d}$, and~$\mathbf{W}_z \in \mathbb{R}^{d \times d_{\text{emb}}}$. 
Therefore the newly aggregated node features with semantic knowledge injection can be calculated as follows:
\begin{equation}
    \hat{\mathbf{U}}_i^l = \sum_j \mathbf{S}_{i,j}^l \cdot \mathbf{W}_u \mathbf{Z}_j^{l} ,
\end{equation}
\begin{equation}
    \mathbf{U}_i^l = \mathrm{Norm}\left(\mathrm{FFN}_U\left(\hat{\mathbf{U}}_i^l\right) + \hat{\mathbf{U}}_i^l\right),
\end{equation}
where $\mathbf{W}_u \in \mathbb{R}^{d \times d_{\text{emb}}}$.

\subsection{Training and Inference}


As shown in Figure~\ref{fig: an overview of model}, we first initialize the nodes and edges feature and extract word embeddings from the pre-trained language model. Then Graph Embedding Layer is used to encode and propagate the global node-edge information, while the Semantic Injection Layer is utilized after the first Graph Embedding Layer. The final result is output from the last Graph Embedding Layer, by two MLPs named NodeMLP and EdgeMLP to recognize objects and their structural relationships, respectively.

Specifically, we adopt the focal loss~\cite{lin2017focal} for objects and relationships classification due to the data distribution imbalance problem~\cite{zhang2021knowledge,3dssg}. Hence we formulate the object classification focal loss as the following:
\begin{equation}
    L_{\text{focal}}^{\text{obj}} = \alpha\left(1-p\right)^\gamma \log\left( p \right),
\end{equation}
where $p$ is the logits of object prediction, $\alpha$ is the normalized inverse frequency of objects, and the $\gamma$ is a hyper-parameter. Similarly, we can formulate $L_{\text{focal}}^{\text{edge}}$ as relationship classification focal loss.

Moreover, in order to make the injected semantic embedding close to the corresponding object visual feature, we formulate semantic similarity loss for Semantic Injection Layer training based on Eq.~(\ref{eq:sim}) as the following:
\begin{equation}
    L_\text{SIL} =  \alpha\left(1-\mathbf{S}_i\right)^\gamma log\left( \mathbf{S}_i \right).
\end{equation}

Therefore, the final loss $L_{\text{SG}}$ can be calculated as the sum of the aforementioned three loss functions:
\begin{equation}
    L_{\text{SG}} = L_{\text{focal}}^{\text{obj}} + L_{\text{focal}}^{\text{edge}} + L_{\text{SIL}}.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
\begin{table*}[!t]
    \footnotesize
    \centering
    \caption{\small blabla}
    \setlength\tabcolsep{1.0mm}
    \subfloat[Comparisons of our model and existing state-of-the-art methods on 3DSSG~\cite{3dssg}. $\text{EdgeGCN}^*$ denotes using EdgeGCN to model global information, {\bf Ours w/o SIM} and {\bf Ours Full} denote our model without the Semantic Injection Layer and our full model, respectively. The best performances are shown in bold.]{
        \label{table1}
        \begin{tabularx}{0.608\linewidth}{lcccccc}
        \hline 
        \rowcolor{gray!20} &
            \multicolumn{2}{c}{Object Classification} &
            \multicolumn{2}{c}{Predicate Classification} &
            \multicolumn{2}{c}{Relationship Prediction} \\
            \rowcolor{gray!20}                 \multirow{-2}{*}{Approach}       & R@5    & R@10   & F1@3  & F1@5  & R@50   & R@100 \\    \hline 
        $\text{PointNet alone}$                  & 87.40   & 96.26  & 68.55 & 82.79 & 34.97  & 45.86 \\
        $+$ $\text{SGPN}$~\cite{3dssg}                           & 89.61  & 96.98  & 63.38 & 77.79 & 32.45  & 41.65 \\
        $+$ $\text{GloRe}_{\text{PC}}$~\cite{ma2020global}                       & 84.06  & 95.17  & 69.23 & 80.01 & 31.87  & 42.21 \\
        $+$ $\text{GloRe}_{\text{SG}}$~\cite{chen2019graph}                       & 85.27  & 96.62  & 72.57 & \underline{83.42} & 29.58  & 38.64 \\
        $+$ $\text{EdgeGCN}^*$~\cite{edge-gcn}                 & 64.46   & 84.88  & 33.34 & 35.90 & 12.19  & 19.69 \\ 
        $+$ $\text{EdgeGCN}$~\cite{edge-gcn}                 & 90.70   & 97.58  & \underline{78.88} & \textbf{90.86} & 39.91  & 48.68 \\ \hline
        $+$ $\textbf{Ours w/o SIM}$ & \underline{92.71} & \underline{97.67} & 76.64   & 77.82   & \underline{49.66} & \underline{55.21}   \\
        $+$ $\textbf{Ours Full}$  & \textbf{94.24} & \textbf{98.24} & \textbf{82.01}   & 83.13   & \textbf{54.25} & \textbf{59.14}   \\ \hline
        \end{tabularx}
    } \hfill
    \subfloat[Ablation studies on edge feature and layer numbers. We show that the integration of edge features leads to notable improvements in both node and edge prediction performance.]{
        \label{GEM}
        \begin{tabularx}{0.344\linewidth}{ccccc}
        
        \hline \rowcolor{gray!20} 
        Model & Edge                 & Layer & Node R@1                  & Edge R@1                  \\ \hline
        Ours full A                     &$\times$              & 3     &66.68                      &87.72                           \\
        Ours full B                     & \checkmark           & 3     & 70.71                     & 90.97                     \\
        Ours full C                     & $\times$             & 6     & 68.56                          &88.22                           \\
        Ours full D                     & \checkmark           & 6     & 69.33                     & 91.72                     \\
        Ours full E                     & $\times$             & 9     & 69.35                          & 88.42                          \\
        Ours full F                     & \checkmark           & 9     & 69.32                     & 91.86                     \\
        Ours full G                     & $\times$             & 12    & 67.94                          & 87.67      \\
        Ours full H                     & \checkmark           & 12    & \bf{72.32}                     & \bf{92.05}\\ \hline 
        \end{tabularx}
    }
\end{table*}
\fi

% \iffalse
\begin{table*}[!t]
\centering
\footnotesize
\setlength\tabcolsep{6.5mm}
\begin{tabular}{lcccccc}
\hline 
\rowcolor{gray!20} 
&
  \multicolumn{2}{c}{Object Class Prediction} &
  \multicolumn{2}{c}{Predicate Class Prediction} &
  \multicolumn{2}{c}{Relationship Prediction} \\
          \rowcolor{gray!20}                 \multirow{-2}{*}{Graph Reasoning Approach}       & R@5    & R@10   & F1@3  & F1@5  & R@50   & R@100 \\    \hline 
$\text{PointNet alone}$                  & 87.40   & 96.26  & 68.55 & 82.79 & 34.97  & 45.86 \\
$+$ $\text{SGPN}$~\cite{3dssg}                           & 89.61  & 96.98  & 63.38 & 77.79 & 32.45  & 41.65 \\
$+$ $\text{GloRe}_{\text{PC}}$~\cite{ma2020global}                       & 84.06  & 95.17  & 69.23 & 80.01 & 31.87  & 42.21 \\
$+$ $\text{GloRe}_{\text{SG}}$~\cite{chen2019graph}                       & 85.27  & 96.62  & 72.57 & \underline{83.42} & 29.58  & 38.64 \\
$+$ $\text{EdgeGCN}^*$~\cite{edge-gcn}                 & 64.46   & 84.88  & 33.34 & 35.90 & 12.19  & 19.69 \\ 
$+$ $\text{EdgeGCN}$~\cite{edge-gcn}                 & 90.70   & 97.58  & \underline{78.88} & \textbf{90.86} & 39.91  & 48.68 \\ \hline
$+$ $\textbf{Ours w/o SIL}$ & \underline{92.71} & \underline{97.67} & 76.64   & 77.82   & \underline{49.66} & \underline{55.21}   \\
$+$ $\textbf{Ours Full}$  & \textbf{94.24} & \textbf{98.24} & \textbf{82.01}   & 83.13   & \textbf{54.25} & \textbf{59.14}   \\ \hline
\end{tabular}
\vspace{-3mm}
\caption{Comparisons of our model and existing state-of-the-art methods on 3DSSG~\cite{3dssg}. $\text{EdgeGCN}^*$ denotes aggregates information with all nodes, while~$\text{EdgeGCN}$ aggregates information only with nodes that have a known relationship. {\bf Ours w/o SIL} and {\bf Ours Full} denote our model without the Semantic Injection Layer and our full model, respectively. The best performances are shown in bold.%\zyang{other than performance gain, will Transformer be faster or saving parameters or have other advantages? Simple is one but not easy to claim.}\clv{We will report back if there is space available}
}
\label{table1}
%%\vspace{-2mm}
\end{table*}
% \fi

% \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7} 
\section{Experiments}


To validate our proposed model, extensive experiments are conducted on the public widely-adopted 3DSSG dataset~\cite{3dssg} with its RIO27 annotation set.
% ~\footnote{\url{https://docs.google.com/spreadsheets/d/1eRTJ2M9OHz7ypXfYD-KTR1AIT-CrVLmhJf8mxgVZWnI/edit#gid=0}}.


\subsection{Experimental Settings}

\noindent\textbf{3DSSG Dataset~\cite{3dssg}.} 3DSSG provides annotated 3D semantic scene graphs for 3RScan~\cite{wald2019rio}, which includes 1,482 3D reconstruction models of $478$ naturally changing indoor environments. There are 48k object nodes and 544k edges in total. We follow their RIO27 annotation to evaluate $27$ class objects and $16$ class relationships in our experiments. For the fair comparison, we follow the same experimental settings in~\cite{edge-gcn} and split the dataset into $1084/113/113$ scenes as train/validation/test sets, respectively.

\noindent\textbf{Metrics.} Following~\cite{3dssg,edge-gcn}, we evaluate our model in terms of object classification, predicate classification, and relationship classification. We adopt \emph{Top-K Recall}~\cite{lu2016visual} for object classification, and we compute the \emph{macro-F1 score} and \emph{Top-K Recall} for predicate classification due to the imbalanced distribution. Besides, we multiply the classification scores of each subject, predicate, and object, and then use~\emph{Recall@K} to evaluate the obtained ordered list of relationship classification. Regarding the long-tail and zero-shot tasks, we use \emph{mean Recall@K(mR@K)}~\cite{tang2020unbiased} for object and predicate classification, and \emph{Zero-Shot Recall@K}~\cite{lu2016visual} for never been observed relationships evaluation.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1\linewidth]{latex/per-label-analysis.pdf}
    \vspace{-3mm}
    \caption{Performance comparisons of our full model and EdgeGCN w.r.t the per-label accuracy of each object and predicate.}
    \label{fig: per-label-analysis}
    %%\vspace{-3mm}
\end{figure*}

\noindent\textbf{Compared Methods.} We compare our approach with the following methods on 3DSSG benchmarks: only using PointNet~\cite{qi2017pointnet}, SGPN~\cite{3dssg}, $\text{GloRe}_\text{PC}$ with the point cloud~\cite{ma2020global}, $\text{GloRe}_\text{SG}$ with the scene graph~\cite{chen2019graph}, and Edge-GCN~\cite{edge-gcn}~\footnote{In all experiments, the settings of the above-mentioned methods are adopted from the corresponding papers.}.




% \noindent\textbf{Tasks.}
% Following~\cite{sharifzadeh2021classification}, we report experimental results under scene graph classification \textbf{(SGCls)}: to classify object and predicate labels, given the edges without labels between nodes. Another task is scene graph detection~\textbf{(SGDet)}, where the network should also detect the objects and edges, and we report them in the supplementary material. Note that we report all results under constrained setting~\cite{yu2017visual} because the 3DSSG dataset has only one kind of label for each relationship between the given two objects.
%%Since the focus of our method is not to improve the backbone of the object detector, and the improvement in SGDet is similar to the improvement in SGCls,



\subsection{Implementation Details}





% \begin{table*}[t]
% \setlength{\tabcolsep}{1.7mm}{
% \begin{tabular}{lccccccccccccc}
% \cline{1-10}
% & \multicolumn{5}{c}{Object} & \multicolumn{4}{c}{Predicate}  \\ \cmidrule(lr){2-6} \cmidrule(lr){7-10}
% \multirow{-2}{*}{Mothod}   & cabinet & table & wall & curtain & toilet  & close by  & cover & lying in & belonging to \\ \cline{1-10}
% PoineNet                   &         &       &      &         &         &         &           &       &          &    \\
% EdgeGCN                    & 21.72   & 18.64  & 90.71  & 29.11  & 0.00  & 51.49   & 0.00    & 0.00     & 66.67   \\
%                            & 54.92   & 54.34  & 89.29  & 33.73  & 14.29 & 62.18   & 50.00   & 0.00     & 74.17    \\
% \multirow{-2}{*}{Ours w/o SIM} & {\textcolor{red}{152.90\%$\uparrow$}} & {\textcolor{red}{191.43\%$\uparrow$}} & {\textcolor{blue}{-1.57\%$\downarrow$}} & {\textcolor{red}{15.87\%$\uparrow$}} & { }     & {\textcolor{red}{20.77\%$\uparrow$}} & { }        & { } & {\textcolor{red}{11.25\%$\uparrow$}} \\ \cline{2-10}
%                                     & 62.12    & 62.29  & 88.98   & 60.98   & 57.14   & 88.50 & 75.00 & 26.32 & 60.00 \\
% \multirow{-2}{*}{Ours Full}    & {\textcolor{red}{13.11\%$\uparrow$}}  & {\textcolor{red}{14.63\%$\uparrow$}}  & {\textcolor{blue}{-0.34\%$\downarrow$}} & {\textcolor{red}{80.75\%$\uparrow$}} & {\textcolor{red}{300.00\%$\uparrow$}} & {\textcolor{red}{42.32\%$\uparrow$}} & {\textcolor{red}{50.00\%$\uparrow$}}  &                         & {\textcolor{blue}{-19.10\%$\downarrow$}}\\ \cline{1-10}
% \end{tabular}}
% \caption{The per-label accuracy includes some objects and predicates. The percentages below the numbers indicate the improvement over the previous method, where red indicates an improvement and blue indicates a decrease\zyang{``PointNet EdgeGCN''use multirow to center the numbers, or remove ``PointNet''?} The per-label accuracy includes some objects and predicates. The percentages below the numbers indicate the improvement over the previous method, where red indicates an improvement and blue indicates a decrease \zyang{I'm not sure we want to have the red and blue relative improvements. As 1. the number comparison seems already clear, 2. it is unclear how the relative number is computed, 3. some entries are empty, making the table looks a bit messy.} }
% \label{table2}
% \end{table*}

We implement our model based on Pytorch~\cite{paszke2019pytorch} on a single NVIDIA RTX 3090 GPU. Similar to prior works in 3D scene graph generation~\cite{3dssg,edge-gcn}, we choose PointNet~\cite{qi2017pointnet} as the backbone. Specifically, we set $C_{\text{input}}$ to $9$ which including 3-dim coordinates, $3$-dim RGB colors and $3$-dim normal vectors. while $C_{point}$ is set to $256$ for unified point-wise feature extraction. In the Graph Embedding Layer, we set $d$=$d_{\text{node}} = 256$, $d_{\text{edge}} = 512$, and $H = 8$. In the Semantic Injection Layer, we directly extract word embedding from GloVe~\cite{pennington2014glove} based on the corresponding object category and use the prompt ``A photo of [class]'' with BERT~\cite{devlin2018bert} and CLIP~\cite{radford2021learning} to extract another type of word embedding. We set the $d_{\text{emb}} = 300$ for GloVe, $d_{\text{emb}} = 768$ for BERT and $d_{\text{emb}} = 512$ for CLIP. 
% We empirically utilize one layer of Graph Embedding Layer~(GEM) and one layer of Semantic Injection Module after the generation of nodes and edges features, followed by $L=12$ layer of GEM to update features.\clv{In order to correspond with table 2 afterward, I would like to express a total of 12 layers here, but I do not know how to express it.} 
We set the default layer number $L$ to be $12$, consisting of a pair of GEL and SIL layers followed by $L-2=10$ extra GEL layers.
Regarding the nodes and edges focal loss, we set $\alpha = 0.25$ and  $\gamma = 2$. We employ Adam as the optimizer with an initial learning rate of $1 \times 10^{-3}$, weight decay of $1 \times 10^{-4}$, and the mini-batch training with batch size 8.


% \begin{table}[!t]
% \setlength\tabcolsep{2.7mm}
% \begin{tabular}{lccc}
% \hline
% {Model}            & {Blocks} &  NodeR@1                 & EdgeR@1   \\ \hline
% PointNet    & 1      & \multicolumn{1}{c}{61.61}    &         87.84  \\ 
% GEM w/o Edge & 1      & \multicolumn{1}{c}{67.54}   &88.92           \\ 
% GEM w/ Edge   & 1      & \multicolumn{1}{c}{67.82}   &87.83           \\ 
% GEM w/o Edge & 10     & \multicolumn{1}{c}{68.83}   & 91.07          \\
% GEM w/ Edge   & 10     & \multicolumn{1}{c}{\textbf{69.59}}   & \textbf{91.79}          \\ \hline
% \end{tabular}
% \caption{Comparison with different variants of Graph Embedding Module~(GEM) and the different numbers of GEM blocks. \zyang{1 block means 1 GEM followed by 1 semantic inject module, 10 blocks means 10*(GEM+SIM)?, or 1*(GEM+SIM)+9*GEM? A bit confused about the architecture. Maybe we can have another table of different layers of different modules, performance, and the number of parameters.}} 
% \label{GEM}
% \end{table}

% \begin{table}[!t]
% \setlength\tabcolsep{1mm}
% \begin{tabular}{cccccc}
% \hline
% ID & Edge & GEM Layer & Parameter & Node R@1 & Edge R@1 \\ \hline
% A  & $\times$ & 1         & 10.0M     & 67.54    & 88.92    \\
% B  & \checkmark     & 1         &           & 67.82    & 87.83    \\
% C  &      & 5         &           &          &          \\
% D  &      & 5         &           &          &          \\
% E  &      & 10        &           &          &          \\
% F  &      & 10        &           &          &          \\ \hline
% \end{tabular}
% \label{GEM}
% \end{table}


% \iffalse

% \fi

    
\iffalse
\begin{table}[!t]
\setlength\tabcolsep{2mm}
\begin{tabular}{ccccc}
\hline \rowcolor{gray!20} 
Model & Edge                 & Layer & Node R@1                  & Edge R@1                  \\ \hline
Ours full A                     &$\times$              & 3     &66.68                      &87.72                           \\
\rowcolor{gray!10} Ours full B                     & \checkmark           & 3     & 70.71                     & 90.97                     \\
Ours full C                     & $\times$             & 6     & 68.56                          &88.22                           \\
\rowcolor{gray!10} Ours full D                     & \checkmark           & 6     & 69.33                     & 91.72                     \\
Ours full E                     & $\times$             & 9     & 69.35                          & 88.42                          \\
\rowcolor{gray!10} Ours full F                     & \checkmark           & 9     & 69.32                     & 91.86                     \\
Ours full G                     & $\times$             & 12    & 67.94                          & 87.67      \\
\rowcolor{gray!10} Ours full H                     & \checkmark           & 12    & \bf{72.32}                     & \bf{92.05}\\ \hline 
\end{tabular}
% \caption{The presented results showcase the model's performance on node and edge prediction across varying layer configurations, denoted by the number of layers and the whether or not to use edge features during node update. Our findings reveal that the integration of edge features into the model leads to notable improvements in both node and edge prediction. Furthermore, our results demonstrate the efficacy of increasing the model's depth in achieving better prediction accuracy.}
\caption{Ablation studies on edge feature and layer numbers. We show that the integration of edge features leads to notable improvements in both node and edge prediction performance. \zyang{1. not sure if we need to repeat ``Ours full'' 8 times; 2. do we with have a background color to differentiate with and w/o edge results?}
% The presented results showcase the model's performance on node and edge prediction across varying layer configurations, denoted by the number of layers and the whether or not to use edge features during node update. Our findings reveal that the integration of edge features into the model leads to notable improvements in both node and edge prediction. Furthermore, our results demonstrate the efficacy of increasing the model's depth in achieving better prediction accuracy.
}
\label{GEM}
\end{table}
\fi

\subsection{Results}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\noindent\textbf{Quantitative results.} We report the quantitative performance of our proposed model compared with other existing methods in Table~\ref{table1}. For \emph{PointNet alone}, we only use PointNet without any reasoning model for evaluation. SPGN~\cite{3dssg} adopts the edge between nodes as a kind of node to conduct message propagation with GCN. As shown in Table~\ref{table1}, using GCN for scene graph generation could improve the object classification but harm the predicate or relationship classification, which confirms the empirical findings reported in~\cite{3dssg,zhang2021knowledge} about the over-smoothing issue caused by multi-layer GCNs. Instead, our model captures the global scene structure with the Transformer architecture, alleviating the over-smoothing in multi-layer GCN and surpassing the state-of-the-art Edge-GCN~\cite{edge-gcn}. Furthermore, compared with the global-level model $\text{GloRe}_\text{SG}$, our proposed model obtains better results by utilizing the information passing between inter-object relationships. Attributing to the introduced Semantic Injection Layer, our model achieves the best result in terms of most evaluation metrics. Additionally, Figure~\ref{fig: per-label-analysis} shows the per-category performance comparison with the state-of-the-art EdgeGCN model and our model. Our SGT surpasses EdgeGCN across the majority of objects and predicates, except the ``chair'' and ``wall'' object categories. This issue may attribute to the inadequacy of fine-grained annotation for ``chair'' and ``wall'' within the dataset.
% The GloRe module could reach reasoning effects in two different levels, namely the point cloud level ($\text{GloRe}_\text{PC}$~\cite{ma2020global}) and scene graph level ($\text{GloRe}_\text{SG}$~\cite{chen2019graph}). Compared with SGPN~\cite{3dssg}, both of them could improve the predicate prediction but negatively influence the object prediction, and $\text{GloRe}_\text{PC}$ performs better than $\text{GloRe}_\text{SG}$ in terms of relationship triplet prediction and worse in object and predicate prediction. Different from Edge-GCN~\cite{edge-gcn}, 


%%indicating the superiority of our Transformer architecture and the semantic injection strategy. 


% However, our method encounters limitations when processing the chair and wall objects. 
%%We conduct a comprehensive analysis of this issue and attribute it to the inadequacy of fine-grained annotation for the chair object within the dataset. Consequently, the incorporation of semantic information results in a decrease in prediction effectiveness, highlighting the need for additional fine-grained annotation to improve our method's accuracy.

\begin{figure*}[!t]
    \centering
    \includegraphics[width = 1\linewidth]{latex/example-2.pdf}
    \vspace{-6mm}
    \caption{The qualitative results of our model. Given a 3D scene with class-agnostic instance segmentation labels, our Transformer-based model infers a semantic graph $G$ from the point cloud. For visualization purposes, misclassified objects or relationship predictions are indicated in red, while the correct ones are shown in black with the GT value omitted.}
    \label{exp of scene}
\end{figure*} 

\iffalse
\begin{table}[!t]
\centering
\setlength\tabcolsep{6.2mm}
\begin{tabular}{lcc}
\hline \rowcolor{gray!20} 
{Model}              &  NodeR@1          & EdgeR@1         \\ \hline
$\text{w/o SIL}$     & 68.80             & 91.07         \\
{$\text{Concat}_{\text{GloVe}}$} &   61.25               & 86.57                \\
{$\text{Concat}_{\text{CLIP}}$} &   61.15               & 88.86                \\
{$\text{Concat}_{\text{BERT}}$}  &  60.32                & 85.56                \\ 
{$ \text{SIL}_{\text{GloVe}}$}     & 71.62             & 91.70         \\
{$ \text{SIL}_{\text{CLIP}}$}     & 67.58             & 91.60         \\
{$\text{SIL}_{\text{BERT}}$}      & \textbf{72.32}                 &\textbf{92.05}                 \\\hline
\end{tabular}
% \caption{The experimental results obtained for different semantic knowledge types and fusion strategies, demonstrate their impact on the performance of the model. Notably, the SIM approach employing BERT outperformed the other methods, showcasing its potential to enhance the predictive performance of the model for uncommon classes.
\caption{Ablation study for different semantic knowledge types and fusion strategies. 
% SIL variants consistently outperform the ``w/o SIL'' and feature concatenation baselines.
}
\label{table:SIM}
\end{table}


\begin{table}[!t]
\centering
\setlength\tabcolsep{2.4mm}
\begin{tabular}{lcccc}
\hline \rowcolor{gray!20} 
\multicolumn{1}{c}{Pre-Trained} & \multicolumn{2}{c}{Node} & \multicolumn{2}{c}{Edge} \\ \rowcolor{gray!20} 
% \cmidrule(lr){2-3}\cmidrule(lr){4-5}
\multicolumn{1}{c}{Embedding} & mR@5   & mR@10  & mR@1   & mR@3  \\ \hline
None                 & 84.16 & 94.73 & 53.77 & 85.26 \\
GloVe~\cite{pennington2014glove}                & 85.17 &94.72 &59.71  &84.43    \\ 
CLIP~\cite{radford2021learning} & 85.35 & 91.91  & 57.66 & 85.63 \\
BERT~\cite{devlin2018bert} & \textbf{90.81} & \textbf{96.58}  & \textbf{61.67} & \textbf{87.93} \\\hline
\end{tabular}
\caption{Quantitative result of the nodes and edges prediction task, as evaluated by the mean Recall metric~\cite{tang2020unbiased}. The ``Pre-trained Embedding'' denotes the type of pre-trained textual embedding used in the Semantic Injection Layer. 
%Experiments validate the effectiveness of having textual knowledge $\mathbf{Z}_K$ alongside the learnable visual knowledge $\mathbf{Z}_U$.
}
% Notably, the results show that the method utilizing BERT as the pre-trained model outperforms the other models. Moreover, it is worth noting that not all pre-trained models yield a significant improvement in the task's outcomes.} 
\label{meanrecall}
\end{table}
\fi

\begin{table*}[!t]
    \footnotesize
    \setlength\tabcolsep{1mm}
    \begin{subtable}[!t]{0.35\textwidth}
        % \setlength\tabcolsep{2mm}
        \vspace{-1mm}
        \begin{tabular}{ccccc}
        \hline \rowcolor{gray!20} 
        Model & Edge                 & Layer & Node R@1                  & Edge R@1                  \\ \hline
        Ours full A                     &$\times$              & 3     &66.68                      &87.72                           \\
        \rowcolor{gray!10} Ours full B                     & \checkmark           & 3     &\underline{ 70.71}                     & 90.97                     \\
        Ours full C                     & $\times$             & 6     & 68.56                          &88.22                           \\
        \rowcolor{gray!10} Ours full D                     & \checkmark           & 6     & 69.33                     & 91.72                     \\
        Ours full E                     & $\times$             & 9     & 69.35                          & 88.42                          \\
        \rowcolor{gray!10} Ours full F                     & \checkmark           & 9     & 69.32                     & \underline{91.86}                     \\
        Ours full G                     & $\times$             & 12    & 67.94                          & 87.67      \\
        \rowcolor{gray!10} Ours full H                     & \checkmark           & 12    & \bf{72.32}                     & \bf{92.05}\\ \hline 
        \end{tabular}
        \caption{Ablation study on edge feature and layer numbers. 
        % We show that the integration of edge features leads to notable improvements in both node and edge prediction performance.
        }
        \label{GEM}
    \end{subtable}
    \hspace{\fill}
    \begin{subtable}[!t]{0.25\textwidth}
        \centering
        \vspace{-0.5mm}
        % \setlength\tabcolsep{6.2mm}
        \begin{tabular}{lcc}
        \hline \rowcolor{gray!20} 
        {Model}              &  NodeR@1          & EdgeR@1         \\ \hline
        $\text{w/o SIL}$     & 68.80             & 91.07         \\ \hline
        {$\text{Concat}_{\text{GloVe}}$} &   61.25               & 86.57                \\
        {$\text{Concat}_{\text{CLIP}}$} &   61.15               & 88.86                \\
        {$\text{Concat}_{\text{BERT}}$}  &  60.32                & 85.56                \\ \hline
        {$ \text{SIL}_{\text{GloVe}}$}     & \underline{71.62}             & \underline{91.70}         \\
        {$ \text{SIL}_{\text{CLIP}}$}     & 67.58             & 91.60         \\
        {$\text{SIL}_{\text{BERT}}$}      & \textbf{72.32}                 &\textbf{92.05}                 \\\hline
        \end{tabular}
        \caption{Ablation study for different semantic knowledge types and fusion strategies. 
        % SIL variants consistently outperform the ``w/o SIL'' and feature concatenation baselines.
        }
        \label{table:SIM}
    \end{subtable}
    \hspace{\fill}
    \begin{subtable}[!t]{0.33\textwidth}
        \centering
        % \vspace{-2mm}
        % \setlength\tabcolsep{2.4mm}
        \begin{tabular}{lcccc}
        \hline \rowcolor{gray!20} 
        \multicolumn{1}{c}{Pre-Trained} & \multicolumn{2}{c}{Node} & \multicolumn{2}{c}{Edge} \\ \rowcolor{gray!20} 
    % \cmidrule(lr){2-3}\cmidrule(lr){4-5}
        \multicolumn{1}{c}{Embedding} & mR@5   & mR@10  & mR@1   & mR@3  \\ \hline
        None                 & 84.16 & \underline{94.73} & 53.77 & 85.26 \\
        GloVe~\cite{pennington2014glove}                & 85.17 &94.72 & \underline{59.71}  &84.43    \\ 
        CLIP~\cite{radford2021learning} & \underline{85.35} & 91.91  & 57.66 & \underline{85.63} \\
        BERT~\cite{devlin2018bert} & \textbf{90.81} & \textbf{96.58}  & \textbf{61.67} & \textbf{87.93} \\\hline
        \end{tabular}
        \caption{Quantitative result of the nodes and edges prediction task, as evaluated by the mean Recall metric~\cite{tang2020unbiased}. ``Pre-trained Embedding'' denotes the type of pre-trained textual embedding used in the Semantic Injection Layer. 
        % Experiments validate the effectiveness of having textual knowledge $\mathbf{Z}_K$ alongside the learnable visual knowledge $\mathbf{Z}_U$.
        }
        \label{meanrecall}
    \end{subtable}
    \vspace{-2mm}
    \caption{Ablation studies and analysis.}
    %%\vspace{-2mm}
\end{table*}


\noindent\textbf{Qualitative Results.}~Figures~\ref{exp of scene} (a) and (b) depict two visualized qualitative results of the state-of-the-art method and our proposed model, and our approach showcases noteworthy advancements in both node and edge prediction. For example, Figure~\ref{exp of scene} (b) depicts a situation when ``sink'' and ``object'' have only one neighbor, which makes EdgeGCN's predictive performance falls short while our proposed SGT leverages global information to overcome this challenge and accurately predicting the labels of these nodes. Moreover, EdgeGCN exhibits a cascading effect of erroneous predictions after it hits an error ``ceiling'', while SGT delivers more robust results despite encountering the similar problem. These findings highlight the significance of global information aggregation in SGT for enhancing the recognition of objects and relationships in complex scenes. Additionally, when two objects have a similar appearance such as ``cabinet'' and ``wall'', EdgeGCN fails to classify them solely based on visual cues. However, our approach is adept at avoiding such errors by incorporating semantic information, indicating the efficacy of our Semantic Injection Layer to provide much-needed semantic assistance.
% The qualitative visualization can be seen in Figure~\ref{exp of scene}. Figure \ref{exp of scene} (a) is the visualization result of EdgeGCN, while Figure \ref{exp of scene} (b) shows the result of our proposed SGT model. For the same input scene, the performance of our proposed model in terms of node and edge classification is better than EdgeGCN. For example, our model achieves better results for the prediction of ``spatial proximity'' and ``close by'', which demonstrates our model could capture the spatial location information well by using the Graph Embedding Module. The accurate prediction result of ``door hanging on wall'' illustrates the effectiveness of the proposed Semantic Injection Module in our model, because, in a realistic scene, the door cannot behave with the wall in spatial proximity. We will present more qualitative examples and analyses in the supplementary material. 


\subsection{Ablation Studies}
\label{Ablation Studies}

%%Here we perform comprehensive ablation studies to further examine the efficacy of the proposed layers.
\noindent\textbf{Graph Embedding Layer~(GEL).}
In Table~\ref{GEM}, we design variants of our models as A to H, to examine whether or not to use edge features in the node updates, and the varying layer numbers. Specifically, by comparing the performance between G and H, which have the same depth but differ in whether they use edge features, we can observe that incorporating edge features during graph encoding can significantly enhance the model's predictive capabilities. Additionally, we find that increasing the layer number effectively improves performance, as evidenced by comparing B, D, F, and H, demonstrating the proposed GEL can avoid the over-smoothing problem commonly encountered by GCN when attempting to stack deeper.

%%To demonstrate the effectiveness of Edge-aware Self-Attention in our GEL  and the superiority of Transformer encoding.
 
% Following~\cite{edge-gcn}, to reveal the precise performance gain of each proposed module, we report \emph{R@1} to evaluate the respective classification results of objects and predicates. We use different variants of GEM in experiments: {\bf GEM W/o Edge} denotes the GEM without updating edge features but with message propagation, {\bf GEM w/ Edge} denotes the full module proposed in~\ref{Graph Encoding M}. In addition, we examine the GEM with different {\bf Blocks} numbers used in our proposed Transformer. As shown in Table~\ref{GEM}, all GEM variants can improve the performance of nodes/edges classification compared with PointNet, demonstrating the Transformer can benefit the global modeling in scene graph generation. Note that the Recall@1 of edge classification performance will increase along with the increased block number. Additionally, we compare the results of {\bf GEM W/o Edge} and {\bf GEM w/ Edge}, and find the message propagation with the edge feature could not only improve the node classification performance but also the edges classification accuracy, showing incorporating edge information via edge-aware self-attention in GEM is very effective.


\begin{figure}[!t]
    \centering
    \includegraphics[width = 1\linewidth]{latex/t-sne.pdf}
    %%\vspace{-5mm}
    \caption{The t-SNE visualization of objects latent space of the w/o SIL(a) and {$ \text{SIL}_{\text{BERT}}$}(b).
    % It shows clearly that our Semantic Injection Layer could make the model learn a more distinguishable latent space.
    }
    % \vspace{-8mm}
    \label{t-SNE}
\end{figure} 


\noindent\textbf{Semantic Injection Layer~(SIL).} We explore various semantic knowledge types and knowledge fusion strategies here. To be specific, we implement SIL with different types of semantic knowledge extracted from GloVe~\cite{pennington2014glove}, CLIP text encoder~\cite{radford2021learning}, and BERT~\cite{devlin2018bert}, denoted as $\text{SIL}_{\text{Glove}}$, $\text{SIL}_{\text{CLIP}}$, and $\text{SIL}_{\text{BERT}}$, respectively. For semantic fusion strategies, we adopt direct concatenation to fuse semantic features with node features to compare with our introduced cross-attention in SIL, denoted as $\mathrm{Concat}_{\text{Glove}}$, $\mathrm{Concat}_{\text{CLIP}}$, and $\text{Concat}_{\text{BERT}}$. The results of our experiments are shown in Table \ref{table:SIM}. Notably, direct concatenation resulted in inferior performance compared to Transformer without SIL and our cross-attention-based SIL, suggesting that our proposed cross-attention is more appropriate and effective. Furthermore, we also find that the injected knowledge from BERT can help our proposed model achieve the best performance than GloVe and CLIP from the table. Besides, Figure~\ref{t-SNE} demonstrates that the output latent features through semantic injection, and we can see our proposed model with SIL using BERT ($\text{SIL}_{\text{BERT}}$) achieves a more distinguishable latent space compared with the Transformer without SIL ($\text{w/o SIL}$), demonstrating that the proposed SIL can enlarge the distance between different object categories, thereby enhancing object classification accuracy. 

% \textbf{Semantic Injection Module~(SIM).}~To explore the effectiveness of the proposed SIM for scene graph generation, we propose various types of semantic knowledge and knowledge fusion strategies. {$\text{SIM}_{\text{Glove}}$}, {$\text{SIM}_{\text{CLIP}}$}, and {$\text{SIM}_{\text{BERT}}$} denote SIM with semantic knowledge extracted from Glove\cite{pennington2014glove}, CLIP~\cite{radford2021learning} text encoder and BERT\cite{devlin2018bert}, respectively. $\text{w/o SIM}$ denote our propose Transformer without SIM. The {$\mathrm{Concat}_{\text{Glove}}$}, {$\mathrm{Concat}_{\text{Glove}}$}, and {$\text{Concat}_{\text{BERT}}$} denote that the fusion of semantic knowledge is achieved by directly concatenating the semantic features with node features in SIM. The results are shown in Table~\ref{table:SIM}. We can clearly observer the result of {$\text{Concat}_{\text{BERT}}$} and {$\text{Concat}_{\text{Glove}}$} are both worse than Transformer without SIM. While {$\text{SIM}_{\text{Glove}}$} and {$\text{SIM}_{\text{BERT}}$} achieved better performance, indicating that using different semantic fusion strategies can affect the final classification results. The direct concatenation operation is not applicable to our task, and the cross self-attention we proposed in the SIM approach is more appropriate and better. To analyze the effectiveness of semantic injection for object features classification, we illustrate the t-SNE visualization of the output objects features of our proposed model in Figure~\ref{t-SNE}. From the figure, we can see that the output latent features through semantic injection are close to the corresponding BERT embeddings of object labels in the latent space, which verifies the enhancement and semantic consistency of SIM for each node feature. Moreover, compared with the Transformer w/o SIM ($\text{w/o SIM}$), our proposed model with SIM with BERT ({$\text{SIM}_{\text{BERT}}$}) can obtain more distinguishable latent space demonstrating the proposed SIM can enlarge the distance between different object categories.

\begin{figure}[!t]
    \centering
    \includegraphics[width = 1\linewidth]{latex/relation-accuacy-analysis.pdf}
    \vspace{-5mm}
    \caption{Performance comparison of our full model and EdgeGCN w.r.t relationship classification.}
    %%\vspace{-2mm}
    \label{Relationship_acc_analysis}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{latex/long-tail-2.pdf}
    \caption{The blue bars indicate the ratio of the label to the total data, the green dots indicate the improvement by comparing our SGT with EdgeGCN~\cite{edge-gcn}, and the red line indicates the trend line fitted by the scatter dots.}
    \label{fig: long-tail-analysis}
    %%\vspace{-2mm}
\end{figure} 

\begin{figure}[t]
    \centering
    \includegraphics[width = 1\linewidth]{latex/zero-shot.pdf}
    \caption{The R@50 results of  EdgeGCN\cite{edge-gcn} and our method SGT in terms of the zero-shot relationships}
    \label{zeroshot}
\end{figure} 
%%\vspace{-2mm}
\section{How does SGT help?}

\noindent\textbf{Scene size analysis.}~
To demonstrate the exceptional global modeling capacity of Transformer, we examine the performance of our proposed method in complex 3D scenes. Note that scene complexity is defined as the summation of the number of nodes and relationships in the whole scene. We evaluate the performance of our approach across varying scene scales and validate its accuracy in modeling complex node relationships compared with EdgeGCN as shown in Figure~\ref{Relationship_acc_analysis}, by observing an obviously growing relative gain along with the increased scene complexity.

\noindent\textbf{Long-tail issue.}~The long-tail phenomenon is very common yet challenging in scene graph generation, where the model usually misleads an uncommon or rare category in the dataset as a common label. As shown in Figure~\ref{fig: long-tail-analysis}, the distribution of each label in the train set is very unbalanced. Following~\cite{tang2020unbiased}, we use \textbf{mean Recall@K(mR@K)} in terms of node and edge prediction for long-tail settings. The results are reported in Table~\ref{meanrecall}, with the semantic knowledge extracted from Glove, CLIP, or BERT, our proposed model performs better than the model with \emph{None} knowledge. Especially, the model with BERT achieves the best result, which improves 7.8\% and 8.6\% than baseline w.r.t Node mR@5 and Edge mR@1, indicating that our proposed SIL can improve the predictions performance in marginally sampled object and predicate categories, and successfully alleviate the long-tail challenge. We illustrate the analysis of per-label improvement with data ratio in the training set in Figure~\ref{fig: long-tail-analysis}, and the trend lines show the improvement is increasingly greater for very``tail'' items no matter objects or predicates.
%Furthermore, we illustrate the performance of individual object prediction w.r.t R@1 with semantic injection and the distribution of each object category in the test set in Figure~\ref{zhifangtu}. From the figure, we can see that most of the uncommon categories in the test set achieve a great performance boost, especially ``towel" and ``tv" get 10 times and 4 times improvements, respectively. However, the prediction results of common categories are decreased, such as ``box", ``shelf" and ``bed", which confirms the findings reported in~\cite{zhang2021knowledge} that the semantic knowledge could add noises for common categories. Fortunately, the decrease in the recall value is acceptable due to the large proportion of common classes in the dataset, and we will show more qualitative analysis later in the supplementary material.

\noindent\textbf{Zero-shot scenario.}~Furthermore, under the zero-shot scenarios, we report the results of the unseen relationships in the test set~\footnote{In our test set, the unseen relationships are $<$clothes-lying on-table$>$, $<$cabinet-supported by-table$>$, $<$shelf-close by-pillow$>$, $<$toilet-spatial proximity-curtain$>$, $<$window-close by-box$>$, $<$plant-standing in-cabinet$>$} in Figure~\ref{zeroshot}. We can observe that our proposed model can improve by a large margin compared with EdgeGCN\cite{edge-gcn}, such as $<$cabinet-supported by-table$>$ and $<$window-close by-box$>$. These findings consistently demonstrate the efficacy and superiority of the proposed GEL and SIL in our SGT. However, both methods perform poorly in $<$plant-standing in-cabinet$>$ due to this relationship is uncommon in real life, and more training data can resolve this case.  



\section{Conclusion}

% In this paper, we revisit Transformer for 3D scene graph generation, by proposing newly Graph Embedding Layer to capture the global-level structure and pass messages between nodes and edges for the input 3D scene, and introducing a novel Semantic Injection Layer to enhance the visual object features with extracted word embedding knowledge from BERT. Extensive experimental results performed on the 3DSSG benchmark demonstrate our proposed model outperforms other state-of-the-art methods across all common tasks, especially under challenging long-tail and zero-shot settings.
In this paper, we revisit Transformer for 3D scene graph generation, including the Graph Embedding Layer to capture the global-level structure and pass messages between nodes and edges, and the Semantic Injection Layer to enhance the visual object features with pre-trained textual knowledge. Extensive experiments on the 3DSSG benchmark demonstrate the effectiveness and state-of-the-art performance of our proposed model. Notably, our SGT performs exceptionally well on complicated scenes, and under challenging long-tail and zero-shot scenarios.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

This supplementary material contains implementation and training details, more visualization results of our model, analysis of Semantic Injection Layer~(SIL) location, confusion matrix of node/edge classification, and the whole algorithm description of our proposed method.

\section{Implementation and Training Details}

Following~\cite{edge-gcn}, we use the RIO27 annotation set~(27 object class\footnote{$\mathcal{C}_{\text{node}}$ = \{wall, floor, cabinet, bed, chair, sofa, table, door, window, counter, shelf, curtain, pillow, clothes, ceiling, fridge, tv, towel, plant, box, nightstand, toilet, sink, lamp, bathhub, object, blanket. \}}) in our study, instead of the initially published annotations~(160 object class)~\cite{3dssg}. The adopted RIO27 is a subset of the raw 160-class set according to mapping rules in its official repository. In our experiments, we utilize only a subset of the relationships~(16 structural relationships\footnote{$\mathcal{C}_{\text{edge}}$ = \{supported by, attached to, standing on, lying on, hanging on, connected to, leaning against, part of, belonging to, build in, standing in, cover, lying in, hanging in, spatial proximity, close by.\}}) to formulate the scene graph edge classification as multi-class classification problems. As for the sampled point cloud representation, we follow the sampling strategy in 3DSSG benchmarks~\cite{edge-gcn}.
% \footnote{\url{https://docs.google.com/spreadsheets/d/1eRTJ2M9OHz7ypXfYD-KTR1AIT-CrVLmhJf8mxgVZWnI/edit#gid=0}}

During the training of PointNet~\cite{qi2017pointnet} and our proposed Semantic Graph Transformer, we chose Adam~\cite{kingma2015adam} as the optimizer, and the learning rate and weight decay are set to 1e-3 and 1e-4, respectively. Additionally, we train our model for 50 epochs with early stopping applied on the held-out validation set and set the batch size to 8. In order to accelerate training, we merged 8 separate scenes by re-mapping the instance index of the scene in the whole batch, and also changed the index of the corresponding edges. In order to ensure the point cloud can be sampled balanced under the varying object sizes, we follow~\cite{edge-gcn} and randomly cropped 4,096 points by maintaining the same sampling ratio between all object instances in the scene. Our model is trained and tested with a single 24GB NVIDIA RTX 3090 GPU.
% \begin{table}[t]
% \begin{tabular}{cccc}
% \hline
% \rowcolor{gray!20} 
% Location of SIL & Layer & Node R@1 & Edge R@1 \\ \rowcolor{gray!10}  \hline
% 1st             & 6     & \underline{71.45}         & 91.64         \\\rowcolor{gray!10} 
% 3rd             & 6     & 68.99         & 91.70    \\\rowcolor{gray!10} 
% 5th             & 6     & 69.63         & 91.65         \\
% 1st             & 9     & \bf{74.47}    & \bf{92.49}    \\
% 3rd             & 9     & 67.01         & \underline{91.72}         \\
% 5th             & 9     & 68.52    & 91.29    \\
% 7th             & 9     & 67.56         & 91.21        \\
% 9th             & 9     & 68.44         & 91.15        \\\hline
% \end{tabular}
% \caption{Analysis of SIL locations in our SGT. The best results are highlighted in bold.}
% \label{SIL position}
% \end{table}
% \begin{table}[t]
%  \setlength\tabcolsep{5mm}
% \begin{tabular}{lcc}
% \hline
% \rowcolor{gray!20} 
% Object     & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}EdgeGCN\\ R@1\end{tabular}} & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Our full model \\ R@1\end{tabular}} \\ \hline
% floor      & 94.62                                                                                             & 99.23                                                                                                     \\
% pillow     & 87.94                                                                                             & 91.37                                                                                                     \\
% ceiling    & 86.60                                                                                             & 91.09                                                                                                     \\
% wall       & 90.71                                                                                             & 88.98                                                                                                     \\
% window     & 56.86                                                                                             & 85.69                                                                                                     \\
% chair      & 85.20                                                                                             & 78.36                                                                                                     \\
% object     & 57.19                                                                                             & 73.03                                                                                                     \\
% door       & 35.59                                                                                             & 70.19                                                                                                     \\
% sofa       & 32.50                                                                                             & 70.00                                                                                                     \\
% blanket    & 0.00                                                                                              & 65.38                                                                                                     \\
% plant      & 29.57                                                                                             & 63.21                                                                                                     \\
% table      & 18.64                                                                                             & 62.29                                                                                                     \\
% cabinet    & 21.72                                                                                             & 62.12                                                                                                     \\
% curtain    & 29.11                                                                                             & 60.98                                                                                                     \\
% shelf      & 36.60                                                                                             & 59.38                                                                                                     \\
% counter    & 39.39                                                                                             & 58.82                                                                                                     \\
% bed        & 21.05                                                                                             & 57.14                                                                                                     \\
% toilet     & 0.00                                                                                              & 58.64                                                                                                     \\
% box        & 40.38                                                                                             & 54.43                                                                                                     \\
% lamp       & 40.28                                                                                             & 52.63                                                                                                     \\
% bathtub    & 0.00                                                                                              & 50.00                                                                                                     \\
% nightstand & 0.00                                                                                              & 53.20                                                                                                     \\
% tv         & 0.00                                                                                              & 42.86                                                                                                     \\
% sink       & 11.76                                                                                             & 41.18                                                                                                     \\
% clothes    & 11.11                                                                                             & 32.14                                                                                                     \\
% towel      & 4.00                                                                                              & 32.13                                                                                                     \\
% fridge     & 0.00                                                                                              & 12.50                                                                                                     \\ \hline
% \end{tabular}
% \caption{Object's per-label accuracy}
% \label{table-obj-acc}
% \end{table}

% \begin{table}[t]
% \setlength\tabcolsep{3.5mm}
% \begin{tabular}{lcc}

% \hline
% \rowcolor{gray!20}  
% Predicate         & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}EdgeGCN\\ R@1\end{tabular}} & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Our full model \\ R@1\end{tabular}} \\ \hline
% spatial proximity & 84.61                                                                                             & 98.57                                                                                                     \\
% standing on       & 84.39                                                                                             & 95.15                                                                                                     \\
% attached to       & 88.57                                                                                             & 94.22                                                                                                     \\
% hanging on        & 77.31                                                                                             & 91.82                                                                                                     \\
% lying on          & 78.42                                                                                             & 90.82                                                                                                     \\
% close by          & 51.49                                                                                             & 88.50                                                                                                     \\
% part of           & 63.16                                                                                             & 77.85                                                                                                     \\
% connected to      & 66.67                                                                                             & 75.64                                                                                                     \\
% cover             & 0.00                                                                                              & 75.00                                                                                                     \\
% belonging to      & 66.67                                                                                             & 74.17                                                                                                     \\
% leaning against   & 41.67                                                                                             & 72.50                                                                                                     \\
% standing in       & 54.84                                                                                             & 70.00                                                                                                     \\
% build in          & 46.43                                                                                             & 68.28                                                                                                     \\
% supported by      & 45.45                                                                                             & 58.20                                                                                                     \\
% lying in          & 0.00                                                                                              & 26.32                                                                                                     \\
% hanging in        & 0.00                                                                                              & 0.00                                                                                                      \\ \hline
% \end{tabular}
% \caption{Predicate's per-label accuracy}
% \label{table-pred-acc}
% \end{table}
% \section{Results under SGDet Setting}

% We evaluate our model under Scene Graph Detection~(SGDet) setting, where the network should detect whether the edge exists or not between two nodes. Note that we label `none' meaning there is no edge between two objects, and hence we have 17 class of edges in total. We report the results in Table~\ref{table:main}, in which $\emph{PointNet alone} $ refers to we only adopt PointNet~\cite{qi2017pointnet} without any reasoning module, $\emph{EdgeGCN\cite{edge-gcn}}$ denotes that we use EdgeGCN~\cite{edge-gcn} for reasoning, $\emph{Ours w/o SIM}$ and $\emph{Ours Full}$ denote our model without the Semantic Injection Module and our full model, respectively. As shown in the table, we can draw the same conclusion under SGCls setting that $\emph{Ours Full}$ still achieves the best results in SGDet task.



\section{More Visualization Results}
As shown in Figure~\ref{fig:visual} and Figure~\ref{fig:visual2}, we present the visualization results of input scenes and the corresponding generated 3D scene graphs, of which the ids of the chosen scenes are ``0cac761f-8d6f-2d13-8c29-90f19c5f65c6'' (top), ``0958222d-e2c2-2de1-9732-e2fb990692ef'' (middle), and ``2e4a3964-d452-21a0-9de5-4f7895499143'' (bottom) in Figure~\ref{fig:visual} and ``0cac761f-8d6f-2d13-8c29-90f19c5f65c6'' (top), ``0958222f-e2c2-2de1-94fd-986561908cee'' (middle), and ``c7895f61-339c-2d13-8004-3cbebf317f21'' (bottom) in Figure~\ref{fig:visual2}. From the figures, we can see that our model can accurately classify most of the categories in terms of objects and relationships.
\begin{table}[t]
\begin{tabular}{cccc}
\hline
\rowcolor{gray!20} 
Location of SIL & Layer & Node R@1 & Edge R@1 \\ \rowcolor{gray!10}  \hline
1st             & 6     & \underline{71.45}         & 91.64         \\\rowcolor{gray!10} 
3rd             & 6     & 68.99         & 91.70    \\\rowcolor{gray!10} 
5th             & 6     & 69.63         & 91.65         \\
1st             & 9     & \bf{74.47}    & \bf{92.49}    \\
3rd             & 9     & 67.01         & \underline{91.72}         \\
5th             & 9     & 68.52    & 91.29    \\
7th             & 9     & 67.56         & 91.21        \\
9th             & 9     & 68.44         & 91.15        \\\hline
\end{tabular}
\caption{Analysis of SIL locations in our SGT. The best results are highlighted in bold.}
\label{SIL position}
\end{table}
\begin{table}[t]
 \setlength\tabcolsep{5mm}
\begin{tabular}{lcc}
\hline
\rowcolor{gray!20} 
Object     & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}EdgeGCN\\ R@1\end{tabular}} & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Our full model \\ R@1\end{tabular}} \\ \hline
floor      & 94.62                                                                                             & 99.23                                                                                                     \\
pillow     & 87.94                                                                                             & 91.37                                                                                                     \\
ceiling    & 86.60                                                                                             & 91.09                                                                                                     \\
wall       & 90.71                                                                                             & 88.98                                                                                                     \\
window     & 56.86                                                                                             & 85.69                                                                                                     \\
chair      & 85.20                                                                                             & 78.36                                                                                                     \\
object     & 57.19                                                                                             & 73.03                                                                                                     \\
door       & 35.59                                                                                             & 70.19                                                                                                     \\
sofa       & 32.50                                                                                             & 70.00                                                                                                     \\
blanket    & 0.00                                                                                              & 65.38                                                                                                     \\
plant      & 29.57                                                                                             & 63.21                                                                                                     \\
table      & 18.64                                                                                             & 62.29                                                                                                     \\
cabinet    & 21.72                                                                                             & 62.12                                                                                                     \\
curtain    & 29.11                                                                                             & 60.98                                                                                                     \\
shelf      & 36.60                                                                                             & 59.38                                                                                                     \\
counter    & 39.39                                                                                             & 58.82                                                                                                     \\
bed        & 21.05                                                                                             & 57.14                                                                                                     \\
toilet     & 0.00                                                                                              & 58.64                                                                                                     \\
box        & 40.38                                                                                             & 54.43                                                                                                     \\
lamp       & 40.28                                                                                             & 52.63                                                                                                     \\
bathtub    & 0.00                                                                                              & 50.00                                                                                                     \\
nightstand & 0.00                                                                                              & 53.20                                                                                                     \\
tv         & 0.00                                                                                              & 42.86                                                                                                     \\
sink       & 11.76                                                                                             & 41.18                                                                                                     \\
clothes    & 11.11                                                                                             & 32.14                                                                                                     \\
towel      & 4.00                                                                                              & 32.13                                                                                                     \\
fridge     & 0.00                                                                                              & 12.50                                                                                                     \\ \hline
\end{tabular}
\caption{Object's per-label accuracy}
\label{table-obj-acc}
\end{table}

\begin{table}[t]
\setlength\tabcolsep{3.5mm}
\begin{tabular}{lcc}

\hline
\rowcolor{gray!20}  
Predicate         & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}EdgeGCN\\ R@1\end{tabular}} & \multicolumn{1}{c}{\cellcolor[HTML]{C0C0C0}\begin{tabular}[c]{@{}c@{}}Our full model \\ R@1\end{tabular}} \\ \hline
spatial proximity & 84.61                                                                                             & 98.57                                                                                                     \\
standing on       & 84.39                                                                                             & 95.15                                                                                                     \\
attached to       & 88.57                                                                                             & 94.22                                                                                                     \\
hanging on        & 77.31                                                                                             & 91.82                                                                                                     \\
lying on          & 78.42                                                                                             & 90.82                                                                                                     \\
close by          & 51.49                                                                                             & 88.50                                                                                                     \\
part of           & 63.16                                                                                             & 77.85                                                                                                     \\
connected to      & 66.67                                                                                             & 75.64                                                                                                     \\
cover             & 0.00                                                                                              & 75.00                                                                                                     \\
belonging to      & 66.67                                                                                             & 74.17                                                                                                     \\
leaning against   & 41.67                                                                                             & 72.50                                                                                                     \\
standing in       & 54.84                                                                                             & 70.00                                                                                                     \\
build in          & 46.43                                                                                             & 68.28                                                                                                     \\
supported by      & 45.45                                                                                             & 58.20                                                                                                     \\
lying in          & 0.00                                                                                              & 26.32                                                                                                     \\
hanging in        & 0.00                                                                                              & 0.00                                                                                                      \\ \hline
\end{tabular}
\caption{Predicate's per-label accuracy}
\label{table-pred-acc}
\end{table}
\section{Analysis of SIL Location}

In this part, we conduct the ablation study about different locations of Semantic Injection Layer~(SIL) and report R@1 results w.r.t node/edge classification in Table~\ref{SIL position}. Layer denotes the Transformer Layer described in our paper. We can clearly see that adding SIL to the backward layers does not work as well as adding it to the forward layers.
\section{Confusion Matrix of Node/Edge Classification}

% In this part, we show the confusion matrix to measure the recall of node and edge classification after normalization for each category in Figure~\ref{fig:cm_node} and Figure~\ref{fig:cm_edge}, respectively. Moreover, we report R@1 results w.r.t node/edge classification in Table~\ref{table-obj-acc} and Table~\ref{table-pred-acc}.


\section{Algorithm of Our Model}
As shown in Algorithm~\ref{alg:1}, we describe the details of our whole training algorithm, where each equation number corresponds to the number in our paper.


% \begin{table}[t]
% \centering
% \setlength\tabcolsep{3.0mm}
% \begin{tabular}{lcc}
% \hline
% \multirow{2}{*}{Model}              & \multicolumn{2}{c}{\textbf{SGDet}} \\\cmidrule(lr){2-3}
%                    & NodeR@1          & EdgeR@1         \\ \hline
% $\text{PointNet alone\cite{qi2017pointnet}}$     & 68.84             & 84.50         \\
% + $\text{EdgeGCN\cite{edge-gcn}}$     & 16.52             & 58.51         \\\hline
% + $\textbf{Ours w/o SIL}$ &   69.98               & 87.98                \\
% + $\textbf{Ours Full}$  & \bf{70.22}                 & \bf{88.68}                \\\hline
% \end{tabular}
% \caption{Comparisons of our proposed model and EdgeGCN~\cite{edge-gcn}, PonitNet~\cite{qi2017pointnet} on 3DSSG~\cite{3dssg}. $\textbf{Ours w/o SIL}$ and $\textbf{Ours Full}$ denote our model without the Semantic Injection Module and our full model, respectively. The best performances are shown in bold.}
% \label{table:main}
% \end{table}

% \begin{table}[t]
% \centering
% \setlength\tabcolsep{3.0mm}
% \begin{tabular}{lcc}
% \hline
% \multirow{2}{*}{Location of SIM}              & \multicolumn{2}{c}{\textbf{SGCls}} \\\cmidrule(lr){2-3}
%                    & NodeR@1          & EdgeR@1         \\ \hline
% after 1 block GEM    & \textbf{69.59} & \textbf{91.79}         \\
% after 5 block GEM     & 67.69             & 91.59         \\
% after 9 block GEM &   45.28               & 89.73                \\
% after every block GEM  & 65.91            & 92.08                \\\hline
% \end{tabular}
% \caption{Comparisons of our proposed model with different location of Semantic Injection Module. \emph{After $l$ block GEM} denotes that our model use Semantic Injection Module after $l$-th Graph Embedding Modules. \emph{After every block GEM} denotes our model use SIM after every GEM.}
% \label{table:Location of SIM}
% \end{table}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{latex/CVPR2023/Figure/统计recall表.png}
%     \caption{The Recall@1 results w.r.t node classification (a) and edge classification (b).}
%     \label{fig:recall}
% \end{figure}


\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{latex/sm-exp1.pdf}
    \caption{Visualization results of EdgeGCN~\cite{edge-gcn} and our proposed model, where the misclassified objects or relationships are highlighted in red, and the correct ones are shown in black.}
    \label{fig:visual}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\linewidth]{latex/sm-exp2.pdf}
    \caption{Visualization results of EdgeGCN~\cite{edge-gcn} and our proposed model, where the misclassified objects or relationships are  highlighted in red, and the correct ones are shown in black.}
    \label{fig:visual2}
\end{figure*}

\begin{algorithm*}[h]
\caption{Semantic Graph Transformer Batch-Wise Training}
\label{alg:1}
\SetAlgoLined
\KwIn{Point cloud $\mathbf{P} \in \mathbb{R}^{N \times C_{\text{input}}}$; Point-to-instance index $\mathrm{M} \in \left\{1, ..., m\right\}^{N}$; Edge-index $E_{(i,j)}$; Pre-trained BERT; Layers number $L$}%输入参数
\KwOut{3D Scene Graph $G = (V,E) $}%输出
% \KwResult{Write here the result}
 initialize node features $\mathbf{X}_{V_i}$ and edge features $\mathbf{X}_{E_{(i,j)}}$ using Eq. (1, 2)
 
 initialize semantic knowledge $\mathbf{Z}_{\text{emb}}$ from BERT

 \For {\text{each node} $\mathbf{X}_{V_i} \in  \mathbf{X}_V$}{update $\mathbf{X}_{V_i}$ and $\mathbf{X}_{E_{(i,j)}}$ using Eq. (5, 6, 7, 8, 9, 10)}

 update $\mathbf{X}$ with semantic knowledge using Eq. (13, 14, 15)

 \For{$l$ in L}{update $\mathbf{X}_{V_i}$ and $\mathbf{X}_{E_{(i,j)}}$ using Eq. (5, 6, 7, 8, 9, 10)}
 
 classify nodes $V$ and edges $E$ in graph $G$ based on the updated feature
 
\end{algorithm*}



% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{latex/CVPR2023/Figure/节点混淆矩阵.pdf}
%     \caption{The confusion matrix of nodes after normalization.}
%     \label{fig:cm_node}
% \end{figure*}

% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{latex/CVPR2023/Figure/边缘混淆矩阵.pdf}
%     \caption{The confusion matrix of edges after normalization.}
%     \label{fig:cm_edge}
% \end{figure*}



\end{document}