% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[pagenumbers]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{caption}
\usepackage{subcaption}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5986} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}

\renewcommand{\thesection}{A\arabic{section}}
\setcounter{section}{0}
% \renewcommand{\thesubsection}{\Alph{subsection}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Supplementary Materials for CVPR'23 Paper Titled \\
``Conditional Image-to-Video Generation with Latent Flow Diffusion Models''}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\maketitle

% %%%%%%%%% ABSTRACT
% \begin{abstract}
%   The ABSTRACT is to be in fully justified italicized text, at the top of the left-hand column, below the author and affiliation information.
%   Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized.
%   The abstract is to be in 10-point, single-spaced type.
%   Leave two blank lines after the Abstract, then begin the main text.
%   Look at previous CVPR abstracts to get a feel for style and length.
% \end{abstract}

%%%%%%%%% BODY TEXT
\section{Potential Negative Social Impact} Conditional image-to-video models can be used for unethical purposes \cite{yu2022generating}, \eg, creating videos of celebrities for fake news spreading. We will restrict the usage of our models to research purposes only. We also plan to investigate some fake video detection techniques \cite{amerini2019deepfake} that may be effective in detecting fake videos like the ones generated by our methods.

\section{Additional Experiments}
\subsection{Additional Ablation Study on Network Architecture}

% \begin{table}[b]
% \begin{minipage}{0.56\linewidth}
% %\centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{c|c|c}
% \hline
% \# Residual Blocks  & $L_1$ error$\downarrow$ & FVD$\downarrow$   \\ \hline
% 6  & 0.418       & \textbf{32.09} \\
% 10 & \textbf{0.371}       & 32.83     \\ \hline
% \end{tabular}%
% }
% \subcaption{Comparison of stage-one LFAE using different numbers of residual blocks in image decoder $\Omega$.}
% \label{tab:aba_decoder}
% \end{minipage}
% \begin{minipage}{0.43\linewidth}
% %\centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{c|c}
% \hline
% Channel Multipliers & FVD$\downarrow$   \\ \hline
% (1, 2, 4, 8)     & \textbf{32.09} \\
% (1, 2, 4, 8, 16) & 68.07 \\ \hline
% \end{tabular}%
% }
% \subcaption{Comparison of stage-two DM using different channel multipliers in network $\epsilon_\theta$.}
% \label{tab:aba_dm}
% \end{minipage}
% \caption{Ablation study comparing stage-one LFAE and stage-two DM using different architectures on MUG dataset under $128\times128$ resolution.}
% \label{tab:aba}
% \end{table}
To evaluate the performance difference of our proposed LFDM with different architectures, we change the depth of the image decoder $\Omega$ in stage-one LFAE (Table~\ref{tab:aba_decoder}) and the 3D U-Net $\epsilon_\theta$ in stage-two DM (Table~\ref{tab:aba_dm}). We experiment with different settings on MUG dataset to generate videos of $128\times128$ frame resolution. 

In our default setting, the image decoder $\Omega$ in stage-one LFAE is implemented with a network including 6 residual blocks and 2 up-sampling blocks. In Table~\ref{tab:aba_decoder}, we compare using different network depths for the image decoder $\Omega$ in stage-one LFAE . We add four extra residual blocks to the decoder $\Omega$. So the number of residual blocks is increased from 6 to 10. Then we only retrain this deeper decoder in stage one, while keeping all the remaining modules unchanged. As Table~\ref{tab:aba_decoder} shows, using a deeper image decoder shows slightly better self-reconstruction performance (as measured by $L_1$ error) but fails to generate higher-quality videos (as measured by FVD).
%This demonstrates that using more residual blocks in the image decoder may not help a lot for video generation. So
Therefore, we keep using 6 residual blocks in our experiments. 


In our default setting, the denoising network $\epsilon_\theta$ employs a 3D U-Net architecture including 4 down-sampling and 4 up-sampling 3D convolutional blocks, where the \textit{channel multipliers} are (1, 2, 4, 8) with a base channel of 64. That is, from highest to lowest resolution,  the 4 down- or up-sampling blocks in $\epsilon_\theta$ use ($1\times64$, $2\times64$, $4\times64$, $8\times64$) channels, respectively. In Table~\ref{tab:aba_dm}, we compare using different channel multipliers in stage-two DM . We add one more layer to the down-sampling and up-sampling blocks of the 3D U-Net and the {channel multipliers} are (1, 2, 4, 8, 16) with a base channel of 64. We retrain this deeper DM in stage two with 1,200 training epochs as in our previous simpler DM training. We keep using the same stage-one LFAE. From Table~\ref{tab:aba_dm}, one can observe that using more layers in DM led to decreased performance. %drops the performance. %We suspect that the reason is this deeper DM requires more training epochs to converge. 
Therefore, we adopt the simpler (1, 2, 4, 8) as the default setting of channel multipliers in our stage-two DM.

\begin{table}[t]
    \centering
    %\vspace{-6pt}
\resizebox{0.6\linewidth}{!}{%
\begin{tabular}{c|c|c}
\hline
\# Residual Blocks  & $L_1$ error$\downarrow$ & FVD$\downarrow$   \\ \hline
6  & 0.418       & \textbf{32.09} \\
10 & \textbf{0.371}       & 32.83     \\ \hline
\end{tabular}%
}
\caption{Comparison using different numbers of residual blocks in the image decoder $\Omega$ of stage-one LFAE.}
\label{tab:aba_decoder}
\end{table}

\begin{table}[t]
%\vspace{-6pt}
    \centering
    \resizebox{0.5\linewidth}{!}{%
\begin{tabular}{c|c}
\hline
Channel Multipliers & FVD$\downarrow$   \\ \hline
(1, 2, 4, 8)     & \textbf{32.09} \\
(1, 2, 4, 8, 16) & 68.07 \\ \hline
\end{tabular}%
}
\caption{Comparison using different channel multipliers in the network $\epsilon_\theta$ of stage-two DM.}
\label{tab:aba_dm}
\end{table}

\subsection{Additional Analysis of Flow and Occlusion Maps}
\begin{figure}[h]
    \centering
\includegraphics[width=\linewidth]{visualization.pdf}
    \caption{Visualization of flow and occlusion map. [Ref. + Flow] is generated by applying the flow to reference image; note the change in head pose and the shape of eyes and mouth after applying flow. Occlusion map further masks the eyes and mouth to help decoder generate novel pixels for these parts in output image. }
    \label{fig:vis}
\end{figure}
Figure~\ref{fig:vis} shows the visualization of flow and occlusion map of one example video frame from MUG dataset. As illustrated in the caption of Fig.~\ref{fig:vis}, without using occlusion map, decoder may need to learn which regions should be kept and which regions should be masked and repainted. Our additional experiments show that retraining LFAE without occlusion maps increases the $L_1$ error of self-reconstruction from 0.418 to 0.450 on MUG dataset. 

\subsection{Comparison of Inference Time among Different Models}
Table~\ref{tab:time} shows the average inference time of each method to generate one video when using batch size 10 on one NVIDIA A100 GPU on MUG dataset. Note that VDM uses 200-step DDIM while both LDM and LFDM employ 1000-step DDPM.
\begin{table}[h]
\resizebox{\linewidth}{!}
{%
\begin{tabular}{l|cccc|cc}
\hline
Model    & ImaGINator & VDM  & LDM$_\text{64}$ & LFDM$_\text{64}$ & LDM$_\text{128}$  & LFDM$_\text{128}$ \\
\hline
Time(s) & 0.9        & 23.1 & 8.0 & 8.8  & 25.5 & 36.0 \\
\hline
\end{tabular}%
}
\caption{Inference time comparison among different methods.}
\label{tab:time}
\end{table}

\section{More Discussion about Future Work}
%In Sec.~5, we list several limitations of our proposed LFDM. Here we discuss several possible strategies to mitigate these limitations. 1. To generalize LFDM to the multi-subject setting, one can first employ LFDM to separately synthesize a motion flow sequence for each subject and then design a fusion module to combine the results. 2. To extend the current condition from class labels to natural text descriptions for describing more complex motion, we plan to investigate how to enable the diffusion model in our LFDM to learn text-to-flow generation in the future. 3. To reduce the video generation time, we can apply some fast sampling works \cite{lu2022dpm,kong2021fast} to accelerate our LFDM.
Several limitations and some future work are discussed in Section~5 of the paper. Here we elaborate more on future work about LFDM. One future direction is to enable the generation of a video with changing background (or context). We plan to first utilize our LFDM to generate a video describing the motion of foreground subject, and then design another generative network conditioned on each generated foreground frame to synthesize the changing background for each frame. %To enable video generation conditioned on more categories and enhance its capacity on generating more diverse motion, 
In addition, to enhance the generalization ability of LFDM on generating diverse motions of more categories, 
we plan to collect more labeled training video datasets and apply some continual/incremental learning techniques such as  \cite{volpi2021continual,wang2022learning,liang2022balancing,wang2022dualprompt} to train our LFDM. 
%for improving the generalization ability. 
Finally, in our experiments (Table~6), we noticed that 10-step DDIM can achieve acceptable generation performance with faster sampling speed, suggesting it may have greater potential with better hyperparameter settings. To explore these settings, including diffusion sampling steps, we plan to employ some recent hyperparameter optimization techniques such as \cite{bergstra2012random,li2017hyperband,zhang2023targeted}. 
%We also plan to further investigate better settings of hyperparameters such as diffusion sampling steps with some hyperparameter optimization techniques such as \cite{bergstra2012random,li2017hyperband,zhang2023targeted}.

\section{Information about Attached Videos}
% diversity on three datasets
We attach seven MP4 files of example video clips generated by our proposed method in Supp. materials\footnote{These videos are also available in \url{https://github.com/nihaomiao/CVPR23_LFDM}.}. All the given images are testing (\textit{unseen}) images. 
\begin{itemize}
    \item \textbf{mug.mp4} shows the synthesized video clips displaying all 7 expressions of one subject from MUG dataset.
    \item \textbf{mhad1.mp4} and \textbf{mhad2.mp4} include the generated video clips for 26 actions of one subject from MHAD dataset. We exclude the action \textit{sit to stand} because the subject in the given image is standing.
    \item \textbf{natops.mp4} shows the synthesized video clips containing all 24 gestures of one subject from NATOPS dataset.
    \item \textbf{new\_domain.mp4} shows the synthesized video clips including 4 expressions of four subjects from FaceForensics dataset. ``Original'' means directly applying our LFDM pretrained on MUG dataset. ``Finetuned'' means that the image decoder is finetuned with the \textit{training} videos from FaceForensics dataset. Note that other modules including stage-two DM are still unchanged during finetuning. From this video, one can observe that our original LFDM can generate acceptable results for given subject images from a new domain and achieve better performance when the decoder is finetuned with training videos from the new domain. 
    \item \textbf{mug\_ddim.mp4} shows the synthesized video clips containing 4 expressions of four subjects from MUG dataset.
    ``DDIM-10'' means using 10-step DDIM for diffusion sampling while ``DDPM-1000'' is our default 1000-step DDPM sampling strategy. From this video, one can observe that 10-step DDIM can generate visually-acceptable videos with faster sampling speed (0.3s per video \vs 36s per video when using DDPM-1000). But note that the FVD score  of DDPM-1000 is still noticeably better than DDIM-10 (32.09 \vs 50.18) so we keep DDPM-1000 as our default setting.
    
    \item \textbf{sota.mp4} is a video for comparison between our proposed LFDM and several other models including ImaGINator, VDM, and LDM.  We show synthesized video clips by each model on 3 subjects from MUG, MHAD, and NATOPS datasets. The video frames of ground truth (GT) and results of LDM and our LFDM have $128\times128$ resolution while results of ImaGINator and VDM are $64\times64$. The original video clips generated by ImaGINator only contain 32 frames. So we repeat the first frame and the last frame four times to make all the displaying videos have 40 frames. 
\end{itemize}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
