% \documentclass{amsart}
% \usepackage{fullpage}
% \usepackage{xcolor}
% \usepackage{tikz}
% \usepackage{nicefrac}
% \usetikzlibrary{arrows}
% \usetikzlibrary{decorations.markings}
% \usepackage{subfig}

% \newcommand{\norm}[1]{\left\| #1 \right\|}
% \newcommand{\abs}[1]{\left| #1 \right|}
% \newcommand{\set}[1]{\left\{ #1 \right\}}
% \newcommand{\R}{\mathbb{R}}
% \newcommand{\N}{\mathbb{N}}
% \newcommand{\hide}[1]{}

% \theoremstyle{plain}
% \newtheorem{lemma}{Lemma}
% \newtheorem*{problem}{Open Problem}

% \begin{document}

\section{Price of Asynchrony}\label{sec:ratfish}
\begin{flushright}
{\it Carlos Ortiz Marrero, Stephen J.\ Young}
\end{flushright}
% \author{Carlos M. Ortiz Marerro}
% \author{Stephen J.\ Young}
%\maketitle
%{\color{red} Between two and five pages.}
\mySub{Introduction}
Finding the solution $x^*$ of a large-scale system of linear equations $Ax = b$ is one of the most common computation primitives in scientific computation, forming the backbone of numerical approaches  to a variety of applications including computational fluid dynamics, optimization, and atmospheric modeling.  While there are many different approaches to solve such systems (for example, Gaussian elimination, Krylov subspace methods, or conjugate gradient methods), iterative approaches are popular because of their straight-forward nature and ease of implementation.  Broadly speaking, iterative methods proceed by cleverly selecting an auxiliary matrix $M$ and using $M$ to generate a sequence of approximate solutions $x^{(0)}, x^{(1)}, x^{(2)}, \ldots$ with the recurrence $Mx^{(k+1)} = (M-A)x^{(k)} + b$.  Thus if systems of the form $My = c$ can be easily solved, then the approximation can be refined for essentially the cost of one matrix-vector multiplication.  For example, the (damped) Jacobi iteration is when $M$ is chosen to be the (scaled) diagonal of $A$ and Gauss-Seidel iteration is when $M$ chosen to be the lower-triangular portion of $M$.

Typically, the analysis of these methods work with the sequence of residuals $r^{(k)} = x^{(k)} - x^*$, rather than directly with the sequence of approximate solutions.  It is easy to see that the residual solutions satisfy a homogeneous version of the recurrence which defines the approximate solutions, i.e. $Mr^{(k+1)} = (M-A)r^{(k)}.$  Indeed, if $M$ is invertible, then the sequence of residuals is given by $r^{(k)} = C^kr^{(0)}$ where $C = I - M^{-1}A$ is known as the iteration matrix.  As a consequence, the iterative approaches to solving $Ax = b$ can be understood in terms of repeated matrix multiplication.  Indeed, an iterative method converges independent of the initial approximate solution if and of if the spectral radius of $C$, $\rho(C)$, is strictly less than 1.  Even more is true, specifically, if $\rho(C) < 1$ then $-\log_{10}(\rho(C))$ gives an lower bound on the rate of increase (in terms of number of iterations) of the approximate solutions, i.e. the number decimals of precision of $x^{(k)}$ is approximately $-\log_{10}(\rho(C)) k + \log_{10}(\norm{r^{(0)}})$. 

While iterative methods are quite effective and practical for solving linear systems, there are significant challenges that arise when considering large scale systems $Ax = b$ such as those that arise in many applications, such as the modeling of wind turbines, the modeling of reactor processes, computational chemistry for catalysis design, and many others. In particular, the matrix $A$ can be so large that it can not be effectively stored in memory (RAM) on a single compute node -- this necessitates either  taking a significant performance hit by repeatedly transferring portions of $A$ in and out of memory or dividing the system into parts and using many compute nodes to build the solution.  

More concretely, suppose that $A \in \real^{n \times n}$ and $b \in \real^{n}$ and we wish to use compute nodes labeled $1,2,\ldots,\ell$ to solve the system $Ax = b$.  One approach is to partition the rows of $A$ into $\ell$ sets resulting in rectangular matrices $A_i \in \real^{k_i \times n}$ and have a matching partition for $M$, $x$ and $b$.  In cases such as Jacobi iteration it is easy to extend the iterative scheme by considering the same partition on the diagonal matrix $M$, i.e. $M_ix_i^{(k+1)} = (M_i - A_i)x^{(k)} + b_i$, where $x^{(k)}$ is the approximate solution at iteration $k$ reconstructed from the local solutions $x_1^{(k)}, \ldots, x_{\ell}^{(k)}$. For non-diagonal $M$, such as with Gauss-Seidel iteration, the na\"ive approach would be to iteratively solve for the $x^{(k+1)}_i$ using the lower-triangular form and then ``pass" the information on $x^{(k+1)}_i$ onto the subsequent compute nodes so that the lower-triangular form can be used to solve $x_{i+1}^{(k+1)}, x_{i+2}^{(k+1)},\ldots,x_{\ell}^{(k+1)}.$  However, both of these approaches result in computational delays resulting from the time need to communicate between different compute nodes.  

The problem is especially acute in large high-performance computing (HPC) systems such as Aurora, Fugaku, and LUMI, where size of the system and over all network congestion can cause significant inter-node communication delays.
%where the inter-node communication delays can be as large as $1.21 \texttt{\textmu s}$.
Indeed, for some scientific computations it is estimated that over 50\% of the computational time is taken up by communication requirements~\cite{jain2016optimization}.  In other contexts it is known that by performing work \emph{asynchronously}, that is, without waiting for the results of computation or work on remote nodes, can significantly increase the performance of HPC systems \cite{Suetterlein:introspector,Suetterlein:AMT,Suetterlein:roofline}.  However, little is known about how iterative methods for solving linear systems are effected by asynchrony.  More generally, we wish to understand the \emph{price of asyncrhony}, that is, the effect of asynchronous updates on the convergence rate of iterative methods.

% Summit performance from https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_2019-02-SummitNodePerformance-WJ.pdf page 16

\mySub{Problem Statement}

As a first step towards understanding the price of asynchrony, we propose a simple model of asynchronous updates.  To begin, we assume that the iterative method can be viewed through the recurrence $x^{(k+1)} = M x^{(k)}$ where $M$ is a $n \times n$ real matrix with $\rho(M) < 1$.  To build a simple model of asynchronous updates for $M$, we partition the matrix $M$ into an $\ell \times \ell$ array of blocks and for each $i \in [\ell]$ define the update scheme 
\[ \tilde{x}_i^{(k+1)} = \sum_j M_{ij}\tilde{x}_j^{(k - \delta_{ji})}, \] where $\delta_{ji}$ represents the ``delay" in information from compute node $j$ reaching compute node $i$ (and so we assume that $\delta_{ii}=0$).  The \emph{price of asynchrony} can be viewed as the difference between the rates at which $\norm{x^{(k)}}$ and $\norm{\tilde{x}^{(k)}}$ converge to 0.  With this framing, for any $\kappa \geq \max_{i,j} \delta_{ij}$ the asychronous updates can be viewed as a linear operator on $\real^{(\kappa+1)n}$ defined by  $(\tilde{x}^{(\kappa)}, \ldots, \tilde{x}^{(0)})$ is mapped to $(\tilde{x}^{(\kappa+1)}, \ldots, \tilde{x}^{(1)})$ where $\tilde{x}_i^{(\kappa+1)} = \sum_j M_{ij}\tilde{x}^{(\kappa - \delta_{ji})}_j$ for all blocks $i$.   We denote the matrix for this linear operator by $M^{(\delta,\kappa)}$ and note that $M^{(0,\kappa)}$ corresponds to the synchronous update which maintains a history of $\kappa$ steps.  

\begin{lemma}\label{L:spectra}
Let $M$ be a $n \times n$ real matrix with $\rho(M) < 1$ partitioned into an $\ell \times \ell$ array of blocks.  Let $\delta$ be an element of $\N^{\ell \times \ell}$ such that $\delta_{ii} = 0$ for all $i$.  Suppose that $\max_{ij} \delta_{ij} \leq \kappa \leq \kappa'$, then $\rho(M^{(\delta,\kappa)}) = \rho(M^{(\delta,\kappa')}).$ 
\end{lemma}

\begin{proof}
Suppose that $(\mu,v)$ is an eigenpair of $M^{(\delta,\kappa)}$.  Since $M^{(\delta,\kappa)}$ takes $(\tilde{x}^{(\kappa)}, \ldots, \tilde{x}^{(0)})$ to $(\tilde{x}^{(\kappa+1)}, \ldots, \tilde{x}^{(1)})$,
there is some vector $v_0 \in \real^n$ such that $v = (\mu^{\kappa}v_0, \mu^{\kappa-1}v_0, \ldots, \mu v_0, v_0).$  Further, since $v$ is an eigenvector of $M^{(\delta,\kappa)}$ we have that $\mu^{\kappa+1}(v_0)_i = \sum_j \mu^{\kappa - \delta^{ji}}v_0$.
Now define the vector $v' = (\mu^{\kappa}v_0, \mu^{\kappa - 1}v_0, \ldots, \mu v_0, v_0, \mu^{-1}v_0,\ldots, \mu^{\kappa - \kappa'} v_0)$ and consider $M^{(\delta,\kappa')}v'.$  By construction, we have that construction $M^{(\delta,\kappa')}v' =  (w', \mu^{\kappa}v_0,\ldots, \mu^{\kappa -\kappa'+1}v_0)$ for some $w' \in \real^n$.  Furthermore, $w'_i = \sum_j \mu^{\kappa - \delta_{ji}}v_0 = \mu^{\kappa+1}(v_0)_i$.  As a consequence $(\mu,v')$ is a eigenpair for $M^{(\delta,\kappa')}$. A similar argument shows that any eigenpair for $M^{(\delta,\kappa')}$ has a corresponding eigenpair for $M^{(\delta,\kappa)}$. In particular, the two matrices are co-spectral and hence have the same spectral radius.
\end{proof}

As an immediate consequence of Lemma \ref{L:spectra}, we have that $\rho(M^{(0,\kappa)}) = \rho(M)$ for any $\kappa.$
Furthermore, it suffices to restrict our attention to the spectral radius of $M^{(\delta,\kappa)}$ for any system of delays $\delta$ and $\kappa = \max \delta$. Thus, we will abuse notation slightly denote by $M^{(\delta)}$ any member of the family $\set{ M^{(\delta, k)} \mid k \geq \max \delta}$. 

Intuitively, it seems that if we fix a maximum delay $k$, then $\rho(M^{(\delta)})$ is maximized when the delay between any pair of nodes achieves the maximum delay.  In this case, uniformity of the delays allows for an explicit calculation of $\rho(M^{(\delta)})$.

\begin{lemma}\label{L:uniform}
Let $M$ be an $n \times n$ real matrix with $\rho(M) < 1$ partitioned into an $\ell \times \ell$ array of blocks. Let $\delta$ be an element of $\N^{\ell \times \ell}$ such that $\delta_{ij} = k$ for all non-zero blocks of $M$, then $\rho(M^{(\delta)}) = \rho(M)^{\nicefrac{1}{k+1}}$.
\end{lemma}

\begin{proof}
    As noted in Lemma \ref{L:spectra}, any eigenpair $(\mu,v)$ of $M^{(\delta,\kappa)}$ has the form $v = (w, \mu^{-1},\ldots,\mu^{-\kappa}w)$, where $\mu w_i = \sum_j \mu^{-\delta_{ij}}M_{ij} w_j$.  However, since $\delta_{ij} = k$ for any non-zero block of $M$, this implies that $\mu w = \mu^{-k} M w$ and $(\mu^{k+1},w)$ is an eigenpair for $M$.  It is easy to see that a similar construction will take an eigenpair for $M$ to an eigenpair for $M^{(\delta,\kappa)}$.  The desired result on the spectral radius follows immediately.
\end{proof}

This leads naturally to the following open questions:
\begin{problem}
Let $M$ be a $n \times n$ real matrix with $p(M)= \lambda < 1$ partitioned into an $\ell \times \ell$ array of blocks.  Is it the case that for all elements $\delta$ of $[k]^{\ell \times \ell}$ with zero diagonals, $\rho(M^{(\delta)}) \leq \lambda^{\nicefrac{1}{k+1}}?$  If not, is there some function $f \colon (0,1) \times \N \rightarrow (0,1)$ such that $\rho(M^{(\delta)}) \leq f(\lambda,k)$?
\end{problem}
Intuitively, it seems as if this is the worst possible case for the convergence rate, that is, if the all the delays are at most $k$. 
\mySub{Experimental Results}

As a first pass towards resolving this problem, consider a random matrix $M$ selected from one of three different ensembles derived from a matrix $X$ with independent normally distributed entries with mean zero;  $M = X$,  the Gaussian Orthonormal Ensemble where $M = X + X^T$, and the Wishart ensemble where $M = XX^T$.  For notational convenience, we assume that $M$ is re-scaled to having unit spectral radius.   We also consider, for each of these matrix ensembles, the iteration matrix associated with a block Jacobi iteration (again, normalized to have norm 1), that is, the diagonal blocks are zero and the off-diagonal blocks are given by $M_{ii}^{-1}M_{ij}$ where $M_{xy}$ denotes the $x,y$ block in $M$.  In each of these six, cases we also consider two different different delay patterns with 4 compute nodes;  a single node with a delay of 5 and all others having a delay of 0 (Figure \ref{F:single}) and randomly generated Poisson delays with a mean of 3 (Figure \ref{F:poisson}).  In Figure \ref{F:spectral_plots}, we plot the relationship between $\rho((cM)^{(\delta)})$ versus $c \in (0,1)$ for each of these graphs over 50 different trials. 

\begin{figure}
    \centering
    \hfill
        \subfloat[Single Delays\label{F:single}]{
    \begin{tikzpicture}[decoration={
    markings,
    mark=at position 0.5 with {\arrow{>}}}]
    \node[circle,draw] (4) at (-2,-1.155) {4};
    \node[circle,draw] (3) at (0,2.31) {3};
    \node[circle,draw] (2) at (2,-1.155) {2};
    \node[circle,draw] (1) at (0,0)  {1};
    \draw[postaction = {decorate}] (1) to [bend right=10] node[midway,left] {\tiny 5} (2);
    \draw[postaction = {decorate}] (1) to [bend right=10] node[midway,below right] {\tiny 5} (3);
    \draw[postaction = {decorate}] (1) to [bend right=10] node[midway,above] {\tiny 5} (4);
    \draw[white,postaction = {decorate}] (4) to [bend right=10] node[midway,below] {\tiny 1} (2);
    \end{tikzpicture}
    }
    \hfill
        \subfloat[Poisson Delays\label{F:poisson}]{
    \mbox{
    \begin{tikzpicture}[decoration={
    markings,
    mark=at position 0.5 with {\arrow{>}}}]
    \node[circle,draw] (4) at (-2,-1.155) {4};
    \node[circle,draw] (3) at (0,2.31) {3};
    \node[circle,draw] (2) at (2,-1.155) {2};
    \node[circle,draw] (1) at (0,0)  {1};
    %\draw[postaction = {decorate}] (1) to [bend right=10] (2) node[midway,above] {5};
    \draw[postaction = {decorate}] (1) to [bend right=10] node[midway,left] {\tiny 1} (2);
    \draw[postaction = {decorate}] (1) to [bend right=10] node[midway,below right] {\tiny 3} (3);
    \draw[postaction = {decorate}] (1) to [bend right=10] node[midway,above] {\tiny 2} (4);
    \draw[postaction = {decorate}] (2) to [bend right=10] node[midway,above] {\tiny 4} (1);
    \draw[postaction = {decorate}] (2) to [bend right=10] node[midway,above right] {\tiny 2} (3);
    \draw[postaction = {decorate}] (2) to [bend right=10] node[midway,above] {\tiny 5} (4);
    \draw[postaction = {decorate}] (3) to [bend right=10] node[midway,below left] {\tiny 2} (1);
    \draw[postaction = {decorate}] (3) to [bend right=10] node[midway,below] {\tiny 1} (2);
    \draw[postaction = {decorate}] (3) to [bend right=10] node[midway,above left] {\tiny 2} (4);
    \draw[postaction = {decorate}] (4) to [bend right=10] node[midway,right] {\tiny 4} (1);
    \draw[postaction = {decorate}] (4) to [bend right=10] node[midway,below] {\tiny 2} (2);
    \draw[postaction = {decorate}] (4) to [bend right=10] node[midway,below] {\tiny 5} (3);
    \end{tikzpicture}
    }
    }
    \hfill\phantom{}
    \caption{Delay patterns for asynchronous computation.  A weight $d$ edge directed from $j$ to $i$ represents a delay of $d$ time steps in information propagating from $j$ to $i$, i.e. $\delta_{ji} = d$.}\label{F}
\end{figure}




\begin{figure}[h!]
    \centering
    \hfill
    \subfloat[Single Delay\label{F:single_plot}]{\includegraphics[trim = 25 15 45 35, clip, width=.40\textwidth]{Figures/fix_full_ensamble.png}} \hfill
    \subfloat[Poisson Delays\label{F:poisson_plot}]{\includegraphics[trim = 25 15 45 35, clip, width=.40\textwidth]{Figures/poisson_full_ensambles.png}} \hfill \phantom{} \\
    \hfill
    \subfloat[Single Delay, Block Jacobi\label{F:jac_single_plot}]{\includegraphics[trim = 25 15 45 35, clip, width=.40\textwidth]{Figures/fix_jacobi_ensambles.png}} \hfill
    \subfloat[Poisson Delays, Block Jacobi\label{F:jac_poissoin_plot}]{\includegraphics[trim = 25 15 45 35, clip, width=.40\textwidth]{Figures/poisson_jacobi_ensambles.png}} \hfill \phantom{}
    \caption{{Price of Asynchrony.}
    For each of the three matrix ensembles, the solid line represents the average spectral radius over 50 different samples from the ensemble, while the shaded area represents a range of two standard deviations.  The dashed line is the conjectured upper bound in terms of the spectral radius of the synchronous update matrix.}
    \label{F:spectral_plots}
\end{figure}



These experiments would seem to provide at least some support for the conjecture that $\rho(M^{(\delta)}) \leq \rho(M)^{\nicefrac{1}{\max \delta}},$ at least when $\rho(M)$ is sufficiently small.  In the case where $\rho(M)$ is close to one, we expect that at least part of the violation of the conjecture may be as a result of numerical instabilities in the spectral radius calculation.  In particular, from the proofs of Lemma \ref{L:spectra} and \ref{L:uniform}, it easy to see that despite $M^{(\delta,\kappa)}$ being an $(\kappa +1)n \times (\kappa+1)n$ matrix, the sum of the geometric multiplicities is at most $n$.  To further complicate matters, $M^{(\delta)}$ is non-Hermitian and has entries of both positive and negative signs, increasing the likelihood that the eigenvalue which yields the spectral radius is complex.

It is also interesting to note that there is relatively little difference between the behavior of the full matrix iteration and the block Jacobi iteration when there is a single constant delay, while for Poisson delays the block Jacobi iteration has a significantly higher spectral radius.  One possible explanation is to consider the average effective delays (define as the mean delay over non-zero blocks of $M$) which is $\nicefrac{15}{16}$ for a single delayed node, $\nicefrac{15}{12}$ for Jacobi iteration with a single delayed node, $\nicefrac{33}{16}$ for Poisson delays, and $\nicefrac{33}{12}$ for Jacobi iteration with Poisson delays.  As a result, one might speculate that the important parameter for understanding $\rho(M^{(\delta)})$ is the mean delay, perhaps weighted by the spectral radius of the individual blocks.  

\mySub{Acknowledgements} The authors gratefully acknowledge the funding support from the Applied Mathematics Program within the U.S. Department of Energyâ€™s Office of Advanced Scientific Computing Research as part of RAndomized Techniques For Iterative Solvers in Heterogeneous environments (RATFISH). Pacific Northwest National Laboratory is operated by Battelle for the DOE under Contract DE-AC05-76RL01830.

% \bibliographystyle{siam}
% \bibliography{references}


% \end{document}