@article{80percentrule,
  title = {What Happened in Hazelwood: {{Statistics}}, Employment Discrimination, and the 80\% Rule},
  author = {Meier, Paul and Sacks, Jerome and Zabell, Sandy L.},
  year = {1984},
  journal = {American Bar Foundation Research Journal},
  volume = {9},
  number = {1},
  pages = {139--186},
  publisher = {{Cambridge University Press}}
}

@inproceedings{abernethyActiveSamplingMinMax2022,
  title = {Active {{Sampling}} for {{Min-Max Fairness}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Abernethy, Jacob D. and Awasthi, Pranjal and Kleindessner, Matth{\"a}us and Morgenstern, Jamie and Russell, Chris and Zhang, Jie},
  year = {2022},
  month = jun,
  pages = {53--65},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose simple active sampling and reweighting strategies for optimizing min-max fairness that can be applied to any classification or regression model learned via loss minimization. The key intuition behind our approach is to use at each timestep a datapoint from the group that is worst off under the current model for updating the model. The ease of implementation and the generality of our robust formulation make it an attractive option for improving model performance on disadvantaged groups. For convex learning problems, such as linear or logistic regression, we provide a fine-grained analysis, proving the rate of convergence to a min-max fair solution.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WMIBVUJ7/Abernethy et al. - 2022 - Active Sampling for Min-Max Fairness.pdf}
}

@inproceedings{saleiro2020dealing,
  title={Dealing with bias and fairness in data science systems: A practical hands-on tutorial},
  author={Saleiro, Pedro and Rodolfa, Kit T and Ghani, Rayid},
  booktitle={Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={3513--3514},
  year={2020}
}

@incollection{rodolfa2020bias,
  title={Bias and fairness},
  author={Rodolfa, Kit T and Saleiro, Pedro and Ghani, Rayid},
  booktitle={Big data and social science},
  pages={281--312},
  year={2020},
  publisher={Chapman and Hall/CRC}
}

@inproceedings{agarwalDoesDataRepair2022,
  title = {Does {{Data Repair Lead}} to {{Fair Models}}? {{Curating Contextually Fair Data To Reduce Model Bias}}},
  shorttitle = {Does {{Data Repair Lead}} to {{Fair Models}}?},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Agarwal, Sharat and Muku, Sumanyu and Anand, Saket and Arora, Chetan},
  year = {2022},
  pages = {3298--3307},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/N8B95L2I/Agarwal et al. - 2022 - Does Data Repair Lead to Fair Models Curating Con.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/B7WEBGMM/Agarwal_Does_Data_Repair_Lead_to_Fair_Models_Curating_Contextually_Fair_WACV_2022_paper.html}
}

@inproceedings{agarwalFairRegressionQuantitative2019,
  title = {Fair {{Regression}}: {{Quantitative Definitions}} and {{Reduction-Based Algorithms}}},
  shorttitle = {Fair {{Regression}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Agarwal, Alekh and Dudik, Miroslav and Wu, Zhiwei Steven},
  year = {2019},
  month = may,
  pages = {120--129},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In this paper, we study the prediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with respect to a protected attribute such as gender or race. We call this class of problems fair regression. We propose general schemes for fair regression under two notions of fairness: (1) statistical parity, which asks that the prediction be statistically independent of the protected attribute, and (2) bounded group loss, which asks that the prediction error restricted to any protected group remain below some pre-determined level. While we only study these two notions of fairness, our schemes are applicable to arbitrary Lipschitz-continuous losses, and so they encompass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while providing theoretical guarantees on the optimality and fairness of the obtained solutions. In addition to analyzing theoretical properties of our schemes, we empirically demonstrate their ability to uncover fairness\textendash accuracy frontiers on several standard datasets.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CFS5I2JY/Agarwal et al. - 2019 - Fair Regression Quantitative Definitions and Redu.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ND6V4ASC/Agarwal et al. - 2019 - Fair Regression Quantitative Definitions and Redu.pdf}
}

@inproceedings{agarwalReductionsApproachFair2018,
  title = {A {{Reductions Approach}} to {{Fair Classification}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Agarwal, Alekh and Beygelzimer, Alina and Dudik, Miroslav and Langford, John and Wallach, Hanna},
  year = {2018},
  month = jul,
  pages = {60--69},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BM4H7M6D/Agarwal et al. - 2018 - A Reductions Approach to Fair Classification.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/L4CXWGY4/Agarwal et al. - 2018 - A Reductions Approach to Fair Classification.pdf}
}

@inproceedings{agarwalSimultaneousImprovementML,
  title = {Simultaneous {{Improvement}} of {{ML Model Fairness}} and {{Performance}} by {{Identifying Bias}} in {{Data}}},
  booktitle = {{{NeurIPS Data-Centric AI Workshop}} 2021},
  author = {Agarwal, Aakash and Chaudhari, Bhushan and Bhowmik, Dr Tanmoy},
  pages = {6},
  abstract = {Machine learning models built on datasets containing discriminative instances attributed to various underlying factors result in biased and unfair outcomes. It's a well founded and intuitive fact that existing bias mitigation strategies often sacrifice accuracy in order to ensure fairness. But when AI engine's prediction is used for decision making which reflects on revenue or operational efficiency such as credit risk modeling, it would be desirable by the business if accuracy can be somehow reasonably preserved. This conflicting requirement of maintaining accuracy and fairness in AI motivates our research. In this paper, we propose a fresh approach for simultaneous improvement of fairness and accuracy of ML models within a realistic paradigm. The essence of our work is a data preprocessing technique that can detect instances ascribing a specific kind of bias that should be removed from the dataset before training and we further show that such instance removal will have no adverse impact on model accuracy. In particular, we claim that in the problem settings where instances exist with similar feature but different labels caused by variation in protected attributes, an inherent bias gets induced in the dataset, which can be identified and mitigated through our novel scheme. Our experimental evaluation on two open-source datasets demonstrates how the proposed method can mitigate bias along with improving rather than degrading accuracy, while offering certain set of control for end user.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PYU9GPKV/Agarwal et al. - Simultaneous Improvement of ML Model Fairness and .pdf}
}

@misc{akpinarSandboxToolBias2022,
  title = {A {{Sandbox Tool}} to {{Bias}}({{Stress}})-{{Test Fairness Algorithms}}},
  author = {Akpinar, Nil-Jana and Nagireddy, Manish and Stapleton, Logan and Cheng, Hao-Fei and Zhu, Haiyi and Wu, Steven and Heidari, Hoda},
  year = {2022},
  month = apr,
  number = {arXiv:2204.10233},
  eprint = {2204.10233},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.10233},
  abstract = {Motivated by the growing importance of reducing unfairness in ML predictions, Fair-ML researchers have presented an extensive suite of algorithmic "fairness-enhancing" remedies. Most existing algorithms, however, are agnostic to the sources of the observed unfairness. As a result, the literature currently lacks guiding frameworks to specify conditions under which each algorithmic intervention can potentially alleviate the underpinning cause of unfairness. To close this gap, we scrutinize the underlying biases (e.g., in the training data or design choices) that cause observational unfairness. We present a bias-injection sandbox tool to investigate fairness consequences of various biases and assess the effectiveness of algorithmic remedies in the presence of specific types of bias. We call this process the bias(stress)-testing of algorithmic interventions. Unlike existing toolkits, ours provides a controlled environment to counterfactually inject biases in the ML pipeline. This stylized setup offers the distinct capability of testing fairness interventions beyond observational data and against an unbiased benchmark. In particular, we can test whether a given remedy can alleviate the injected bias by comparing the predictions resulting after the intervention in the biased setting with true labels in the unbiased regime -- that is, before any bias injection. We illustrate the utility of our toolkit via a proof-of-concept case study on synthetic data. Our empirical analysis showcases the type of insights that can be obtained through our simulations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7AJY499F/Akpinar et al. - 2022 - A Sandbox Tool to Bias(Stress)-Test Fairness Algor.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KT9VZCTM/2204.html}
}

@article{alghamdiAdultCOMPASFairness2022,
  title = {Beyond {{Adult}} and {{COMPAS}}: {{Fairness}} in {{Multi-Class Prediction}}},
  shorttitle = {Beyond {{Adult}} and {{COMPAS}}},
  author = {Alghamdi, Wael and Hsu, Hsiang and Jeong, Haewon and Wang, Hao and Michalak, P. Winston and Asoodeh, Shahab and Calmon, Flavio P.},
  year = {2022},
  month = jun,
  eprint = {2206.07801},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.2206.07801},
  abstract = {We consider the problem of producing fair probabilistic classifiers for multi-class classification tasks. We formulate this problem in terms of "projecting" a pre-trained (and potentially unfair) classifier onto the set of models that satisfy target group-fairness requirements. The new, projected model is given by post-processing the outputs of the pre-trained classifier by a multiplicative factor. We provide a parallelizable iterative algorithm for computing the projected classifier and derive both sample complexity and convergence guarantees. Comprehensive numerical comparisons with state-of-the-art benchmarks demonstrate that our approach maintains competitive performance in terms of accuracy-fairness trade-off curves, while achieving favorable runtime on large datasets. We also evaluate our method at scale on an open dataset with multiple classes, multiple intersectional protected groups, and over 1M samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FP989U7B/Alghamdi et al. - Beyond Adult and COMPAS Fairness in Multi-Class P.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RW73HZ6V/Alghamdi et al. - 2022 - Beyond Adult and COMPAS Fairness in Multi-Class P.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NZ65VRR9/2206.html}
}

@article{anahidehFairActiveLearning2022,
  title = {Fair Active Learning},
  author = {Anahideh, Hadis and Asudeh, Abolfazl and Thirumuruganathan, Saravanan},
  year = {2022},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {199},
  pages = {116981},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.116981},
  abstract = {Machine learning (ML) is increasingly being used in high-stakes applications impacting society. Therefore, it is of critical importance that ML models do not propagate discrimination. Collecting accurate labeled data in societal applications is challenging and costly. Active learning is a promising approach to build an accurate classifier by interactively querying an oracle within a labeling budget. We introduce the fair active learning framework to carefully select data points to be labeled so as to balance model accuracy and fairness. To incorporate the notion of fairness in the active learning sampling core, it is required to measure the fairness of the model after adding each unlabeled sample. Since their labels are unknown in advance, we propose an expected fairness metric to probabilistically measure the impact of each sample if added for each possible class label. Next, we propose multiple optimizations to balance the trade-off between accuracy and fairness. Our first optimization linearly aggregate the expected fairness with entropy using a control parameter. To avoid erroneous estimation of the expected fairness, we propose a nested approach to maintain the accuracy of the model, limiting the search space to the top bucket of sample points with large entropy. Finally, to ensure the unfairness reduction of the model after labeling, we propose to replicate the points that truly reduce the unfairness after labeling. We demonstrate the effectiveness and efficiency of our proposed algorithms over widely used benchmark datasets using demographic parity and equalized odds notions of fairness.},
  langid = {english},
  keywords = {Active learning,Algorithmic fairness,Limited labeled data},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RS45QXMP/S0957417422004055.html}
}

@article{andersonUsingBayesianNetworks2019a,
  title = {Using {{Bayesian}} Networks to Perform Reject Inference},
  author = {Anderson, Billie},
  year = {2019},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {137},
  pages = {349--356},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.07.011},
  abstract = {Credit scoring is an automatic credit assessment tool that has been used by different types of financial institutions for years. When a financial institution wants to create a credit scoring model for all applicants, the institution only has the known good/bad loan outcome for the accepted applicants; this causes an inherent bias in the model. Reject inference is the process of inferring a good/bad loan outcome to the applicants that were rejected for a loan so that the updated credit scoring model will be representative of all loan applicants, accepted and rejected. This paper presents an empirical reject inference technique using a Bayesian network. The proposed method has an advantage over traditional reject inference methods since there is no functional form that will be estimated with the accepted applicants' data and extrapolated to the rejected applicants to infer their good/bad loan outcome status.},
  langid = {english},
  keywords = {Bayesian networks,Credit scoring,Predictive accuracy,Reject inference},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WF4SVD85/S0957417419304907.html}
}

@book{appiceDiscoveryScience23rd2020,
  title = {Discovery {{Science}}: 23rd {{International Conference}}, {{DS}} 2020, {{Thessaloniki}}, {{Greece}}, {{October}} 19\textendash 21, 2020, {{Proceedings}}},
  shorttitle = {Discovery {{Science}}},
  author = {Appice, Annalisa and Tsoumakas, Grigorios and Manolopoulos, Yannis and Matwin, Stan},
  year = {2020},
  month = oct,
  publisher = {{Springer Nature}},
  abstract = {This book constitutes the proceedings of the 23rd International Conference on Discovery Science, DS 2020, which took place during October 19-21, 2020. The conference was planned to take place in Thessaloniki, Greece, but had to change to an online format due to the COVID-19 pandemic.  The 26 full and 19 short papers presented in this volume were carefully reviewed and selected from 76 submissions. The contributions were organized in topical sections named: classification; clustering; data and knowledge representation; data streams; distributed processing; ensembles; explainable and interpretable machine learning; graph and network mining; multi-target models; neural networks and deep learning; and spatial, temporal and spatiotemporal data.},
  googlebooks = {IisDEAAAQBAJ},
  isbn = {978-3-030-61527-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Data Science / Data Analytics,Computers / Data Science / General,Computers / General,Computers / Information Technology,Computers / System Administration / Storage \& Retrieval,Education / Computers \& Technology}
}

@article{arconesBootstrapStatistics1992,
  title = {On the {{Bootstrap}} of \${{U}}\$ and \${{V}}\$ {{Statistics}}},
  author = {Arcones, Miguel A. and Gine, Evarist},
  year = {1992},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {20},
  number = {2},
  pages = {655--674},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176348650},
  abstract = {Bootstrap distributional limit theorems for \$U\$ and \$V\$ statistics are proved. They hold a.s., under weak moment conditions and without restrictions on the bootstrap sample size (as long as it tends to \$\textbackslash infty\$), regardless of the degree of degeneracy of \$U\$ and \$V\$. A testing procedure based on these results is outlined.},
  keywords = {$U$-statistics,60F05,62E20,62F05,bootstrap,von Mises functionals},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/U55VAIXY/Arcones and Gine - 1992 - On the Bootstrap of $U$ and $V$ Statistics.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DEHL63IC/1176348650.html}
}

@article{baoItCOMPASlicatedMessy2021,
  title = {It's {{COMPASlicated}}: {{The Messy Relationship}} between {{RAI Datasets}} and {{Algorithmic Fairness Benchmarks}}},
  shorttitle = {It's {{COMPASlicated}}},
  author = {Bao, Michelle and Zhou, Angela and Zottola, Samantha and Brubach, Brian and Brubach, Brian and Desmarais, Sarah and Horowitz, Aaron and Lum, Kristian and Venkatasubramanian, Suresh},
  year = {2021},
  month = dec,
  journal = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume = {1},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6MRM8D2S/Bao et al. - 2021 - It's COMPASlicated The Messy Relationship between.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NBB3YWXF/92cc227532d17e56e07902b254dfad10-Abstract-round1.html}
}

@inproceedings{barataActiveLearningImbalanced2021,
  title = {Active Learning for Imbalanced Data under Cold Start},
  booktitle = {Proceedings of the {{Second ACM International Conference}} on {{AI}} in {{Finance}}},
  author = {Barata, Ricardo and Leite, Miguel and Pacheco, Ricardo and Sampaio, Marco O. P. and Ascens{\~a}o, Jo{\~a}o Tiago and Bizarro, Pedro},
  year = {2021},
  month = nov,
  series = {{{ICAIF}} '21},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3490354.3494423},
  abstract = {Modern systems that rely on Machine Learning (ML) for predictive modelling, may suffer from the cold-start problem: supervised models work well but, initially, there are no labels, which are costly or slow to obtain. This problem is even worse in imbalanced data scenarios, where labels of the positive class take longer to accumulate. We propose an Active Learning (AL) system for datasets with orders of magnitude of class imbalance, in a cold start streaming scenario. We present a computationally efficient Outlier-based Discriminative AL approach (ODAL) and design a novel 3-stage sequence of AL labeling policies where ODAL is used as warm-up. Then, we perform empirical studies in four real world datasets, with various magnitudes of class imbalance. The results show that our method can more quickly reach a high performance model than standard AL policies without ODAL warm-up. Its observed gains over random sampling can reach 80\% and be competitive with policies with an unlimited annotation budget or additional historical data (using just 2\% to 10\% of the labels).},
  isbn = {978-1-4503-9148-1},
  keywords = {active learning,cold start,data streams,high class imbalance},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WAVNBGPQ/Barata et al. - 2021 - Active learning for imbalanced data under cold sta.pdf}
}

@inproceedings{barbosaLightweightPrivacySmart2014,
  title = {Lightweight Privacy for Smart Metering Data by Adding Noise},
  booktitle = {Proceedings of the 29th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Barbosa, Pedro and Brito, Andrey and Almeida, Hyggo and Clau{\ss}, Sebastian},
  year = {2014},
  month = mar,
  series = {{{SAC}} '14},
  pages = {531--538},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2554850.2554982},
  abstract = {With a Smart Metering infrastructure, there are many motivations for power providers to collect high-resolution data of energy usage from consumers. However, this collection implies very detailed information about the energy consumption of consumers being monitored. Consequently, a serious issue needs to be addressed: how to preserve the privacy of consumers but making the provision of certain services still possible? Clearly, this is a tradeoff between privacy and utility. There are approaches for preserving privacy in various ways, but many of them affect the data usefulness or are computationally expensive. In this paper, we propose and evaluate a lightweight approach for privacy and utility based on the addition of noise. Furthermore, using real consumers' data, we discuss the influence of the technique in various Smart Grid scenarios. Finally, we also design and evaluate possible attacks to our solution.},
  isbn = {978-1-4503-2469-4},
  keywords = {data masking,noise addition,smart grid systems,smart metering},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6MLI5Z6B/Barbosa et al. - 2014 - Lightweight privacy for smart metering data by add.pdf}
}

@article{barocasBigDataDisparate2016,
  title = {Big {{Data}}'s {{Disparate Impact}}},
  author = {Barocas, Solon and Selbst, Andrew D.},
  year = {2016},
  journal = {California Law Review},
  volume = {104},
  number = {3},
  pages = {671--732},
  publisher = {{California Law Review, Inc.}},
  issn = {0008-1221},
  abstract = {Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm's use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court. This Essay examines these concerns through the lens of American antidiscrimination law\textemdash more particularly, through Title VII's prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining's victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission's Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others' discrimination against members of protected groups, or flaws in the underlying data. Addressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data's disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of "discrimination" and "fairness."}
}

@article{bartlettConsumerlendingDiscriminationFinTech2022,
  title = {Consumer-Lending Discrimination in the {{FinTech Era}}},
  author = {Bartlett, Robert and Morse, Adair and Stanton, Richard and Wallace, Nancy},
  year = {2022},
  month = jan,
  journal = {Journal of Financial Economics},
  volume = {143},
  number = {1},
  pages = {30--56},
  issn = {0304-405X},
  doi = {10.1016/j.jfineco.2021.05.047},
  abstract = {U.S. fair-lending law prohibits lenders from making credit determinations that disparately affect minority borrowers if those determinations are based on characteristics unrelated to creditworthiness. Using an identification under this rule, we show risk-equivalent Latinx/Black borrowers pay significantly higher interest rates on GSE-securitized and FHA-insured loans, particularly in high-minority-share neighborhoods. We estimate these rate differences cost minority borrowers over \$450 million yearly. FinTech lenders' rate disparities were similar to those of non-Fintech lenders for GSE mortgages, but lower for FHA mortgages issued in 2009\textendash 2015 and for FHA refi mortgages issued in 2018\textendash 2019.},
  langid = {english},
  keywords = {Algorithmic underwriting,Big-data lending,Credit scoring,Discrimination,FinTech,GSE mortgages,Legitimate business necessity,Platform loans,Statistical discrimination},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XSS9BFRK/Bartlett et al. - 2022 - Consumer-lending discrimination in the FinTech Era.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PIWWMAWY/S0304405X21002403.html}
}

@article{basic_account_EU,
  title = {Directive 2014/92/{{EU}} of the {{European Parliament}} and of the {{Council}} of 23 {{July}} 2014 on the Comparability of Fees Related to Payment Accounts, Payment Account Switching and Access to Payment Accounts with Basic Features},
  author = {{European Parliament and Council}},
  year = {2014},
  journal = {Official Journal of the European Union (OJ)},
  volume = {57},
  pages = {214}
}

@misc{begleyExplainabilityFairMachine2020a,
  title = {Explainability for Fair Machine Learning},
  author = {Begley, Tom and Schwedes, Tobias and Frye, Christopher and Feige, Ilya},
  year = {2020},
  month = oct,
  number = {arXiv:2010.07389},
  eprint = {2010.07389},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.07389},
  abstract = {As the decisions made or influenced by machine learning models increasingly impact our lives, it is crucial to detect, understand, and mitigate unfairness. But even simply determining what "unfairness" should mean in a given context is non-trivial: there are many competing definitions, and choosing between them often requires a deep understanding of the underlying task. It is thus tempting to use model explainability to gain insights into model fairness, however existing explainability tools do not reliably indicate whether a model is indeed fair. In this work we present a new approach to explaining fairness in machine learning, based on the Shapley value paradigm. Our fairness explanations attribute a model's overall unfairness to individual input features, even in cases where the model does not operate on sensitive attributes directly. Moreover, motivated by the linearity of Shapley explainability, we propose a meta algorithm for applying existing training-time fairness interventions, wherein one trains a perturbation to the original model, rather than a new model entirely. By explaining the original model, the perturbation, and the fair-corrected model, we gain insight into the accuracy-fairness trade-off that is being made by the intervention. We further show that this meta algorithm enjoys both flexibility and stability benefits with no loss in performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NGBECI3Y/Begley et al. - 2020 - Explainability for fair machine learning.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CRE4KLQ9/2010.html}
}

@article{bellamy2019ai,
  title = {{{AI Fairness}} 360: {{An}} Extensible Toolkit for Detecting and Mitigating Algorithmic Bias},
  author = {Bellamy, Rachel KE and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovi{\'c}, Aleksandra and others},
  year = {2019},
  journal = {IBM Journal of Research and Development},
  volume = {63},
  number = {4/5},
  pages = {4--1},
  publisher = {{IBM}}
}

@inproceedings{bengioMetaTransferObjectiveLearning2020,
  title = {A {{Meta-Transfer Objective}} for {{Learning}} to {{Disentangle Causal Mechanisms}}},
  booktitle = {8th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2020, {{Addis Ababa}}, {{Ethiopia}}, {{April}} 26-30, 2020},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Nan Rosemary and Lachapelle, S{\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher J.},
  year = {2020},
  publisher = {{OpenReview.net}},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JHSES5ZZ/Bengio et al. - 2019 - A Meta-Transfer Objective for Learning to Disentan.pdf}
}

@inproceedings{bergstraAlgorithmsHyperParameterOptimization2011,
  title = {Algorithms for {{Hyper-Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [Larochelle et al., 2007] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4GQ3M32Q/Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf}
}

@article{berk2021fairness,
  title = {Fairness in {{Criminal Justice Risk Assessments}}: {{The State}} of the {{Art}}},
  author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Kearns, Michael and Roth, Aaron},
  year = {2021},
  journal = {Sociological Methods \& Research},
  volume = {50},
  number = {1},
  pages = {3--44},
  publisher = {{Sage Publications Sage CA: Los Angeles, CA}}
}

@misc{berkConvexFrameworkFair2017a,
  title = {A {{Convex Framework}} for {{Fair Regression}}},
  author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
  year = {2017},
  month = jun,
  number = {arXiv:1706.02409},
  eprint = {1706.02409},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.02409},
  abstract = {We introduce a flexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and they span the rang from notions of group fairness to strong individual fairness. By varying the weight on the fairness regularizer, we can compute the efficient frontier of the accuracy-fairness trade-off on any given dataset, and we measure the severity of this trade-off via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six different datasets in which fairness is a primary consideration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/X3Y4ATXC/Berk et al. - 2017 - A Convex Framework for Fair Regression.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VAXB4VU7/1706.html}
}

@article{berkeley-admissions,
  title = {Sex Bias in Graduate Admissions: {{Data}} from {{Berkeley}}},
  author = {Bickel, Peter J and Hammel, Eugene A and O'Connell, J William},
  year = {1975},
  journal = {Science (New York, N.Y.)},
  volume = {187},
  number = {4175},
  pages = {398--404},
  publisher = {{American Association for the Advancement of Science}}
}

@article{bertrandAreEmilyGreg2004,
  title = {Are {{Emily}} and {{Greg More Employable Than Lakisha}} and {{Jamal}}? {{A Field Experiment}} on {{Labor Market Discrimination}}},
  shorttitle = {Are {{Emily}} and {{Greg More Employable Than Lakisha}} and {{Jamal}}?},
  author = {Bertrand, Marianne and Mullainathan, Sendhil},
  year = {2004},
  month = sep,
  journal = {American Economic Review},
  volume = {94},
  number = {4},
  pages = {991--1013},
  issn = {0002-8282},
  doi = {10.1257/0002828042002561},
  abstract = {We study race in the labor market by sending fictitious resumes to help-wanted ads in Boston and Chicago newspapers. To manipulate perceived race, resumes are randomly assigned African-American- or White-sounding names. White names receive 50 percent more callbacks for interviews. Callbacks are also more responsive to resume quality for White names than for African-American ones. The racial gap is uniform across occupation, industry, and employer size. We also find little evidence that employers are inferring social class from the names. Differential treatment by race still appears to still be prominent in the U. S. labor market.},
  langid = {english},
  keywords = {Economics of Minorities; Races; Indigenous Peoples; and Immigrants,Non-labor Discrimination; Labor Discrimination},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I9MB9RMI/Bertrand and Mullainathan - 2004 - Are Emily and Greg More Employable Than Lakisha an.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ECA26W85/articles.html}
}

@inproceedings{binnsApparentConflictIndividual2020,
  title = {On the Apparent Conflict between Individual and Group Fairness},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Binns, Reuben},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {514--524},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372864},
  abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
  isbn = {978-1-4503-6936-7},
  keywords = {discrimination,fairness,individual fairness,justice,machine learning,statistical parity},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BPV9QMM6/Binns - 2020 - On the apparent conflict between individual and gr.pdf}
}

@techreport{birdFairlearnToolkitAssessing,
  title = {Fairlearn: {{A}} Toolkit for Assessing and Improving Fairness in {{AI}}},
  author = {Bird, Sarah and Dud{\'i}k, Miroslav and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen and Design, Allovus},
  pages = {7},
  abstract = {We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness\textemdash some societal and some technical\textemdash it is not possible to fully ``debias'' a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9V64BTAN/Bird et al. - Fairlearn A toolkit for assessing and improving f.pdf}
}

@inproceedings{blanzeiskyAlgorithmicFactorsInfluencing2021,
  title = {Algorithmic {{Factors Influencing Bias}} in {{Machine Learning}}},
  booktitle = {Machine {{Learning}} and {{Principles}} and {{Practice}} of {{Knowledge Discovery}} in {{Databases}}},
  author = {Blanzeisky, William and Cunningham, P{\'a}draig},
  editor = {Kamp, Michael and Koprinska, Irena and Bibal, Adrien and Bouadi, Tassadit and Fr{\'e}nay, Beno{\^i}t and Gal{\'a}rraga, Luis and Oramas, Jos{\'e} and Adilova, Linara and Krishnamurthy, Yamuna and Kang, Bo and Largeron, Christine and Lijffijt, Jefrey and Viard, Tiphaine and Welke, Pascal and Ruocco, Massimiliano and Aune, Erlend and Gallicchio, Claudio and Schiele, Gregor and Pernkopf, Franz and Blott, Michaela and Fr{\"o}ning, Holger and Schindler, G{\"u}nther and Guidotti, Riccardo and Monreale, Anna and Rinzivillo, Salvatore and Biecek, Przemyslaw and Ntoutsi, Eirini and Pechenizkiy, Mykola and Rosenhahn, Bodo and Buckley, Christopher and Cialfi, Daniela and Lanillos, Pablo and Ramstead, Maxwell and Verbelen, Tim and Ferreira, Pedro M. and Andresini, Giuseppina and Malerba, Donato and Medeiros, Ib{\'e}ria and {Fournier-Viger}, Philippe and Nawaz, M. Saqib and Ventura, Sebastian and Sun, Meng and Zhou, Min and Bitetta, Valerio and Bordino, Ilaria and Ferretti, Andrea and Gullo, Francesco and Ponti, Giovanni and Severini, Lorenzo and Ribeiro, Rita and Gama, Jo{\~a}o and Gavald{\`a}, Ricard and Cooper, Lee and Ghazaleh, Naghmeh and Richiardi, Jonas and Roqueiro, Damian and Saldana Miranda, Diego and Sechidis, Konstantinos and Gra{\c c}a, Guilherme},
  year = {2021},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {559--574},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-93736-2_41},
  abstract = {It is fair to say that many of the prominent examples of bias in Machine Learning (ML) arise from bias in the training data. In fact, some would argue that supervised ML algorithms cannot be biased, they reflect the data on which they are trained. In this paper, we demonstrate how ML algorithms can misrepresent the training data through underestimation. We show how irreducible error, regularization, and feature and class imbalance can contribute to this underestimation. The paper concludes with a demonstration of how the careful management of synthetic counterfactuals can ameliorate the impact of this underestimation bias.},
  isbn = {978-3-030-93736-2},
  langid = {english},
  keywords = {Bias,Fairness,Model capacity,Regularisation},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XKHIAPSP/Blanzeisky and Cunningham - 2021 - Algorithmic Factors Influencing Bias in Machine Le.pdf}
}

@misc{BlueprintAIBill,
  title = {Blueprint for an {{AI Bill}} of {{Rights}}},
  journal = {The White House},
  abstract = {Among the great challenges posed to democracy today is the use of technology, data, and automated systems in ways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities and prevent our access to critical resources or services. These problems are well documented. In America and around\ldots},
  howpublished = {https://www.whitehouse.gov/ostp/ai-bill-of-rights/},
  langid = {american},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GT65TBDP/ai-bill-of-rights.html}
}

@inproceedings{blumRecoveringBiasedData2020,
  title = {Recovering from {{Biased Data}}: {{Can Fairness Constraints Improve Accuracy}}?},
  shorttitle = {Recovering from {{Biased Data}}},
  booktitle = {1st {{Symposium}} on {{Foundations}} of {{Responsible Computing}} ({{FORC}} 2020)},
  author = {Blum, Avrim and Stangl, Kevin},
  editor = {Roth, Aaron},
  year = {2020},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {156},
  pages = {3:1--3:20},
  publisher = {{Schloss Dagstuhl\textendash Leibniz-Zentrum f\"ur Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.FORC.2020.3},
  isbn = {978-3-95977-142-9},
  keywords = {bias,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,equal opportunity,fairness in machine learning,machine learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9AVGFA3X/Blum and Stangl - 2019 - Recovering from Biased Data Can Fairness Constrai.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/N89TN3RU/Blum and Stangl - 2020 - Recovering from Biased Data Can Fairness Constrai.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GRPBDMNX/12019.html}
}

@misc{borisovDeepNeuralNetworks2022,
  title = {Deep {{Neural Networks}} and {{Tabular Data}}: {{A Survey}}},
  shorttitle = {Deep {{Neural Networks}} and {{Tabular Data}}},
  author = {Borisov, Vadim and Leemann, Tobias and Se{\ss}ler, Kathrin and Haug, Johannes and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2022},
  month = jun,
  number = {arXiv:2110.01889},
  eprint = {2110.01889},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.01889},
  abstract = {Heterogeneous tabular data are the most commonly used form of data and are essential for numerous critical and computationally demanding applications. On homogeneous data sets, deep neural networks have repeatedly shown excellent performance and have therefore been widely adopted. However, their adaptation to tabular data for inference or data generation tasks remains challenging. To facilitate further progress in the field, this work provides an overview of state-of-the-art deep learning methods for tabular data. We categorize these methods into three groups: data transformations, specialized architectures, and regularization models. For each of these groups, our work offers a comprehensive overview of the main approaches. Moreover, we discuss deep learning approaches for generating tabular data, and we also provide an overview over strategies for explaining deep models on tabular data. Thus, our first contribution is to address the main research streams and existing methodologies in the mentioned areas, while highlighting relevant challenges and open research questions. Our second contribution is to provide an empirical comparison of traditional machine learning methods with eleven deep learning approaches across five popular real-world tabular data sets of different sizes and with different learning objectives. Our results, which we have made publicly available as competitive benchmarks, indicate that algorithms based on gradient-boosted tree ensembles still mostly outperform deep learning models on supervised learning tasks, suggesting that the research progress on competitive deep learning models for tabular data is stagnating. To the best of our knowledge, this is the first in-depth overview of deep learning approaches for tabular data; as such, this work can serve as a valuable starting point to guide researchers and practitioners interested in deep learning with tabular data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HXIDQFPS/Borisov et al. - 2022 - Deep Neural Networks and Tabular Data A Survey.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YX2HLLUN/2110.html}
}

@misc{branchaud-charronCanActiveLearning2021a,
  title = {Can {{Active Learning Preemptively Mitigate Fairness Issues}}?},
  author = {{Branchaud-Charron}, Fr{\'e}d{\'e}ric and Atighehchian, Parmida and Rodr{\'i}guez, Pau and Abuhamad, Grace and Lacoste, Alexandre},
  year = {2021},
  month = apr,
  number = {arXiv:2104.06879},
  eprint = {2104.06879},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.06879},
  abstract = {Dataset bias is one of the prevailing causes of unfairness in machine learning. Addressing fairness at the data collection and dataset preparation stages therefore becomes an essential part of training fairer algorithms. In particular, active learning (AL) algorithms show promise for the task by drawing importance to the most informative training samples. However, the effect and interaction between existing AL algorithms and algorithmic fairness remain under-explored. In this paper, we study whether models trained with uncertainty-based AL heuristics such as BALD are fairer in their decisions with respect to a protected class than those trained with identically independently distributed (i.i.d.) sampling. We found a significant improvement on predictive parity when using BALD, while also improving accuracy compared to i.i.d. sampling. We also explore the interaction of algorithmic fairness methods such as gradient reversal (GRAD) and BALD. We found that, while addressing different fairness issues, their interaction further improves the results on most benchmarks and metrics we explored.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/M3DPD4P5/Branchaud-Charron et al. - 2021 - Can Active Learning Preemptively Mitigate Fairness.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KCQZVY8D/2104.html}
}

@inproceedings{caiAdaptiveSamplingStrategies2022a,
  title = {Adaptive {{Sampling Strategies}} to {{Construct Equitable Training Datasets}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Cai, William and Encarnacion, Ro and Chern, Bobbie and {Corbett-Davies}, Sam and Bogen, Miranda and Bergman, Stevie and Goel, Sharad},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {1467--1478},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3531146.3533203},
  abstract = {In domains ranging from computer vision to natural language processing, machine learning models have been shown to exhibit stark disparities, often performing worse for members of traditionally underserved groups. One factor contributing to these performance gaps is a lack of representation in the data the models are trained on. It is often unclear, however, how to operationalize representativeness in specific applications. Here we formalize the problem of creating equitable training datasets, and propose a statistical framework for addressing this problem. We consider a setting where a model builder must decide how to allocate a fixed data collection budget to gather training data from different subgroups. We then frame dataset creation as a constrained optimization problem, in which one maximizes a function of group-specific performance metrics based on (estimated) group-specific learning rates and costs per sample. This flexible approach incorporates preferences of model-builders and other stakeholders, as well as the statistical properties of the learning task. When data collection decisions are made sequentially, we show that under certain conditions this optimization problem can be efficiently solved even without prior knowledge of the learning rates. To illustrate our approach, we conduct a simulation study of polygenic risk scores on synthetic genomic data\textemdash an application domain that often suffers from non-representative data collection. When optimizing policies for overall or group-specific average health, we find that our adaptive approach outperforms heuristic strategies, including equal and representative sampling. In this sense, equal treatment with respect to sampling decisions does not guarantee equal or equitable outcomes.},
  isbn = {978-1-4503-9352-2},
  keywords = {Active learning,artificial intelligence,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,computer vision,F.2.0,fairness,I.2.0,machine learning,polygenic risk scores,representative data,Statistics - Methodology},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9B7442I6/Cai et al. - 2022 - Adaptive Sampling Strategies to Construct Equitabl.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JKEP3NKL/Cai et al. - 2022 - Adaptive Sampling Strategies to Construct Equitabl.pdf}
}

@article{caiGenerativeAdversarialNetworks2021a,
  title = {Generative {{Adversarial Networks}}: {{A Survey Toward Private}} and {{Secure Applications}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Cai, Zhipeng and Xiong, Zuobin and Xu, Honghui and Wang, Peng and Li, Wei and Pan, Yi},
  year = {2021},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {132:1--132:38},
  issn = {0360-0300},
  doi = {10.1145/3459992},
  abstract = {Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model's compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.},
  keywords = {deep learning,Generative adversarial networks,privacy and security},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VC4ABMB3/Cai et al. - 2021 - Generative Adversarial Networks A Survey Toward P.pdf}
}

@article{caldersThreeNaiveBayes2010,
  title = {Three Naive {{Bayes}} Approaches for Discrimination-Free Classification},
  author = {Calders, Toon and Verwer, Sicco},
  year = {2010},
  month = sep,
  journal = {Data Mining and Knowledge Discovery},
  volume = {21},
  number = {2},
  pages = {277--292},
  issn = {1573-756X},
  doi = {10.1007/s10618-010-0190-x},
  abstract = {In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data.},
  langid = {english},
  keywords = {Discrimination-aware classification,Expectation maximization,Naive Bayes},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SFI937JG/Calders and Verwer - 2010 - Three naive Bayes approaches for discrimination-fr.pdf}
}

@inproceedings{calmonOptimizedPreProcessingDiscrimination2017,
  title = {Optimized {{Pre-Processing}} for {{Discrimination Prevention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Calmon, Flavio and Wei, Dennis and Vinzamuri, Bhanukiran and Natesan Ramamurthy, Karthikeyan and Varshney, Kush R},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Y33CA2JY/Calmon et al. - 2017 - Optimized Pre-Processing for Discrimination Preven.pdf}
}

@inproceedings{caoActiveApproximatelyMetricFair2022,
  title = {Active {{Approximately Metric-Fair Learning}}},
  booktitle = {The 38th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Cao, Yiting and Lan, Chao},
  year = {2022},
  month = jun,
  abstract = {Existing studies on individual fairness focus on the passive setting and typically require \$O(\textbackslash frac\{1\}\{\textbackslash varepsilon\^2\})\$ labeled instances to achieve an \$\textbackslash varepsilon\$ bias budget. In this paper, we...},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JUAZ5CK7/forum.html}
}

@article{cardMinimumWagesEmployment1994,
  title = {Minimum {{Wages}} and {{Employment}}: {{A Case Study}} of the {{Fast-Food Industry}} in {{New Jersey}} and {{Pennsylvania}}},
  shorttitle = {Minimum {{Wages}} and {{Employment}}},
  author = {Card, David and Krueger, Alan B.},
  year = {1994},
  journal = {The American Economic Review},
  volume = {84},
  number = {4},
  pages = {772--793},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {On April 1, 1992, New Jersey's minimum wage rose from \$4.25 to \$5.05 per hour. To evaluate the impact of the law we surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage. We also compare employment changes at stores in New Jersey that were initially paying high wages (above \$5) to the changes at lower-wage stores. We find no indication that the rise in the minimum wage reduced employment.}
}

@misc{cardosoBRSNISBiasReduced2022,
  title = {{{BR-SNIS}}: {{Bias Reduced Self-Normalized Importance Sampling}}},
  shorttitle = {{{BR-SNIS}}},
  author = {Cardoso, Gabriel and Samsonov, Sergey and Thin, Achille and Moulines, Eric and Olsson, Jimmy},
  year = {2022},
  month = sep,
  number = {arXiv:2207.06364},
  eprint = {2207.06364},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.06364},
  abstract = {Importance Sampling (IS) is a method for approximating expectations under a target distribution using independent samples from a proposal distribution and the associated importance weights. In many applications, the target distribution is known only up to a normalization constant, in which case self-normalized IS (SNIS) can be used. While the use of self-normalization can have a positive effect on the dispersion of the estimator, it introduces bias. In this work, we propose a new method, BR-SNIS, whose complexity is essentially the same as that of SNIS and which significantly reduces bias without increasing the variance. This method is a wrapper in the sense that it uses the same proposal samples and importance weights as SNIS, but makes clever use of iterated sampling--importance resampling (ISIR) to form a bias-reduced version of the estimator. We furnish the proposed algorithm with rigorous theoretical results, including new bias, variance and high-probability bounds, and these are illustrated by numerical examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/S3AJ9C8V/Cardoso et al. - 2022 - BR-SNIS Bias Reduced Self-Normalized Importance S.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VSA6LKQ5/2207.html}
}

@misc{catonFairnessMachineLearning2020,
  title = {Fairness in {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Fairness in {{Machine Learning}}},
  author = {Caton, Simon and Haas, Christian},
  year = {2020},
  month = oct,
  number = {arXiv:2010.04053},
  eprint = {2010.04053},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DP2U7YG5/Caton and Haas - 2020 - Fairness in Machine Learning A Survey.pdf}
}

@inproceedings{chakrabortyBiasMachineLearning2021,
  title = {Bias in {{Machine Learning Software}}: {{Why}}? {{How}}? {{What}} to {{Do}}?},
  shorttitle = {Bias in Machine Learning Software},
  booktitle = {Proceedings of the 29th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Chakraborty, Joymallya and Majumder, Suvodeep and Menzies, Tim},
  year = {2021},
  month = aug,
  series = {{{ESEC}}/{{FSE}} 2021},
  pages = {429--440},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3468264.3468537},
  abstract = {Increasingly, software is making autonomous decisions in case of criminal sentencing, approving credit cards, hiring employees, and so on. Some of these decisions show bias and adversely affect certain social groups (e.g. those defined by sex, race, age, marital status). Many prior works on bias mitigation take the following form: change the data or learners in multiple ways, then see if any of that improves fairness. Perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy. This paper postulates that the root causes of bias are the prior decisions that affect- (a) what data was selected and (b) the labels assigned to those examples. Our Fair-SMOTE algorithm removes biased labels; and rebalances internal distributions such that based on sensitive attribute, examples are equal in both positive and negative classes. On testing, it was seen that this method was just as effective at reducing bias as prior approaches. Further, models generated via Fair-SMOTE achieve higher performance (measured in terms of recall and F1) than other state-of-the-art fairness improvement algorithms. To the best of our knowledge, measured in terms of number of analyzed learners and datasets, this study is one of the largest studies on bias mitigation yet presented in the literature.},
  isbn = {978-1-4503-8562-6},
  keywords = {Bias Mitigation,Fairness Metrics,Software Fairness},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LZ2R7AVM/Chakraborty et al. - 2021 - Bias in machine learning software why how what .pdf}
}

@inproceedings{chenWhyMyClassifier2018,
  title = {Why {{Is My Classifier Discriminatory}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Irene and Johansson, Fredrik D and Sontag, David},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CHVMXP8M/Chen et al. - 2018 - Why Is My Classifier Discriminatory.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2},
  keywords = {large-scale machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VWKZKT3F/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@article{chouldechovaFairPredictionDisparate2017,
  title = {Fair {{Prediction}} with {{Disparate Impact}}: {{A Study}} of {{Bias}} in {{Recidivism Prediction Instruments}}},
  shorttitle = {Fair {{Prediction}} with {{Disparate Impact}}},
  author = {Chouldechova, Alexandra},
  year = {2017},
  month = jun,
  journal = {Big Data},
  volume = {5},
  number = {2},
  pages = {153--163},
  publisher = {{Mary Ann Liebert, Inc., publishers}},
  issn = {2167-6461},
  doi = {10.1089/big.2016.0047},
  abstract = {Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance.},
  keywords = {bias,disparate impact,fair machine learning,recidivism prediction,risk assessment},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IPWA2W5Q/Chouldechova - 2017 - Fair Prediction with Disparate Impact A Study of .pdf}
}

@inproceedings{chuangFairMixupFairness2021,
  title = {Fair {{Mixup}}: {{Fairness}} via {{Interpolation}}},
  shorttitle = {Fair {{Mixup}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chuang, Ching-Yao and Mroueh, Youssef},
  year = {2021},
  month = may,
  abstract = {Publication for ICLR 2021 by Ching-Yao chuang et al.},
  copyright = {\textcopyright{} Copyright IBM Corp. 2021},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6RREDGVK/Chuang and Mroueh - 2021 - Fair Mixup Fairness via Interpolation.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GQYXPL2K/fair-mixup-fairness-via-interpolation.html}
}

@article{cloggCommonProblemsLogLinear1987,
  title = {Some {{Common Problems}} in {{Log-Linear Analysis}}},
  author = {CLOGG, CLIFFORD C. and ELIASON, SCOTT R.},
  year = {1987},
  month = aug,
  journal = {Sociological Methods \& Research},
  volume = {16},
  number = {1},
  pages = {8--44},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124187016001002},
  abstract = {Several problems often encountered in research using log-linear models for categorical response variables are discussed. The issues covered are (a) determining the degrees of freedom for a model, (b) analyzing sparse data, (c) analyzing weighted data, (d) modeling rates, and (e) interpreting results.},
  langid = {english}
}

@misc{COMPASRecidivismRisk,
  title = {{{COMPAS Recidivism Risk Score Data}} and {{Analysis}}},
  journal = {ProPublica Data Store},
  abstract = {The  data, code, and documentation behind our analysis of Northpointe, Inc.'s  COMPAS risk-assessment algorithm for the story, "Machine Bias," by Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner.},
  howpublished = {https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NAZGWC64/compas-recidivism-risk-score-data-and-analysis.html}
}

@article{cook1977detection,
  title = {Detection of Influential Observation in Linear Regression},
  author = {Cook, R Dennis},
  year = {1977},
  journal = {Technometrics : a journal of statistics for the physical, chemical, and engineering sciences},
  volume = {19},
  number = {1},
  pages = {15--18},
  publisher = {{Taylor \& Francis}}
}

@article{cookDetectionInfluentialObservation2000,
  title = {Detection of {{Influential Observation}} in {{Linear Regression}}},
  author = {Cook, R. Dennis},
  year = {2000},
  month = feb,
  journal = {Technometrics},
  volume = {42},
  number = {1},
  pages = {65--68},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2000.10485981},
  abstract = {A new measure based on confidence ellipsoids is developed for judging the contribution of each data point to the determination of the least squares estimate of the parameter vector in full rank linear regression models. It is shown that the measure combines information from the studentized residuals and the variances of the residuals and predicted values. Two examples are presented.},
  keywords = {Confidence ellipsoids,Influential observations,Outliers,Variances of residuals},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.2000.10485981},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IC7U2T7H/00401706.2000.html}
}

@inproceedings{corbett-daviesAlgorithmicDecisionMaking2017,
  title = {Algorithmic {{Decision Making}} and the {{Cost}} of {{Fairness}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {{Corbett-Davies}, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
  year = {2017},
  month = aug,
  series = {{{KDD}} '17},
  pages = {797--806},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098095},
  abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
  isbn = {978-1-4503-4887-4},
  keywords = {algorithmic fairness,discrimination,disparate impact,pretrial detention,risk assessment},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FQU6SPE4/Corbett-Davies et al. - 2017 - Algorithmic Decision Making and the Cost of Fairne.pdf}
}

@inproceedings{costonCharacterizingFairnessSet2021,
  title = {Characterizing {{Fairness Over}} the {{Set}} of {{Good Models Under Selective Labels}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Coston, Amanda and Rambachan, Ashesh and Chouldechova, Alexandra},
  year = {2021},
  month = jul,
  pages = {2144--2155},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the ``Rashomon Effect.'' These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or ``the set of good models.'' Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CBLNM728/Coston et al. - 2021 - Characterizing Fairness Over the Set of Good Model.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CVFMJXDR/Coston et al. - 2021 - Characterizing Fairness Over the Set of Good Model.pdf}
}

@inproceedings{costonCounterfactualRiskAssessments2020a,
  title = {Counterfactual Risk Assessments, Evaluation, and Fairness},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {582--593},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372851},
  abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may---and as we show empirically, do---induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
  isbn = {978-1-4503-6936-7},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9AN9RBMA/Coston et al. - 2020 - Counterfactual risk assessments, evaluation, and f.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KSDD3889/Coston et al. - 2020 - Counterfactual Risk Assessments, Evaluation, and F.pdf}
}

@inproceedings{cotterTwoPlayerGamesEfficient2019,
  title = {Two-{{Player Games}} for {{Efficient Non-Convex Constrained Optimization}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Algorithmic Learning Theory}}},
  author = {Cotter, Andrew and Jiang, Heinrich and Sridharan, Karthik},
  year = {2019},
  month = mar,
  pages = {300--332},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In recent years, constrained optimization has become increasingly relevant to the machine learning community, with applications including Neyman-Pearson classification, robust optimization, and fair machine learning. A natural approach to constrained optimization is to optimize the Lagrangian, but this is not guaranteed to work in the non-convex setting, and, if using a first-order method, cannot cope with non-differentiable constraints (e.g. constraints on rates or proportions).  The Lagrangian can be interpreted as a two-player game played between a player who seeks to optimize over the model parameters, and a player who wishes to maximize over the Lagrange multipliers. We propose a non-zero-sum variant of the Lagrangian formulation that can cope with non-differentiable\textemdash even discontinuous\textemdash constraints, which we call the ``proxy-Lagrangian''. The first player minimizes external regret in terms of easy-to-optimize ``proxy constraints'', while the second player enforces the \textbackslash emph\{original\} constraints by minimizing swap regret.  For this new formulation, as for the Lagrangian in the non-convex setting, the result is a stochastic classifier. For both the proxy-Lagrangian and Lagrangian formulations, however, we prove that this classifier, instead of having unbounded size, can be taken to be a distribution over no more than m+1m+1m+1 models (where mmm is the number of constraints). This is a significant improvement in practical terms.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3T6VSN2K/Cotter et al. - 2019 - Two-Player Games for Efficient Non-Convex Constrai.pdf}
}

@article{crookDoesRejectInference2004a,
  title = {Does Reject Inference Really Improve the Performance of Application Scoring Models?},
  author = {Crook, Jonathan and Banasik, John},
  year = {2004},
  month = apr,
  journal = {Journal of Banking \& Finance},
  series = {Retail {{Credit Risk Management}} and {{Measurement}}},
  volume = {28},
  number = {4},
  pages = {857--874},
  issn = {0378-4266},
  doi = {10.1016/j.jbankfin.2003.10.010},
  abstract = {The parameters of application scorecards are usually estimated using a sample that excludes rejected applicants which may prove biased when applied to all applicants. This paper uses a rare sample that includes those who would normally be rejected to examine the extent to which (1) the exclusion of rejected applicants undermines the predictive performance of a scorecard based only on accepted applicants, and (2) reject inference techniques can remedy the influence of this exclusion.},
  langid = {english},
  keywords = {Augmentation,Credit scoring,Extrapolation,Reject inference,Risk management},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/39U6LMYR/Crook and Banasik - 2004 - Does reject inference really improve the performan.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9ZAX5STE/Crook and Banasik - 2004 - Does reject inference really improve the performan.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YTCHQLQE/S0378426603002036.html}
}

@incollection{crowther2013over,
  title = {Over-Policing and under-Policing Social Exclusion},
  booktitle = {Hard Cop, Soft Cop},
  author = {Crowther, Chris},
  year = {2013},
  pages = {63--77},
  publisher = {{Willan}}
}

@misc{cruzFairGBMGradientBoosting2022,
  title = {{{FairGBM}}: {{Gradient Boosting}} with {{Fairness Constraints}}},
  shorttitle = {{{FairGBM}}},
  author = {Cruz, Andr{\'e} F. and Bel{\'e}m, Catarina and Bravo, Jo{\~a}o and Saleiro, Pedro and Bizarro, Pedro},
  year = {2022},
  month = sep,
  number = {arXiv:2209.07850},
  eprint = {2209.07850},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.07850},
  abstract = {Machine Learning (ML) algorithms based on gradient boosted decision trees (GBDT) are still favored on many tabular data tasks across various mission critical applications, from healthcare to finance. However, GBDT algorithms are not free of the risk of bias and discriminatory decision-making. Despite GBDT's popularity and the rapid pace of research in fair ML, existing in-processing fair ML methods are either inapplicable to GBDT, incur in significant train time overhead, or are inadequate for problems with high class imbalance. We present FairGBM, a learning framework for training GBDT under fairness constraints with little to no impact on predictive performance when compared to unconstrained LightGBM. Since common fairness metrics are non-differentiable, we employ a "proxy-Lagrangian" formulation using smooth convex error rate proxies to enable gradient-based optimization. Additionally, our open-source implementation shows an order of magnitude speedup in training time when compared with related work, a pivotal aspect to foster the widespread adoption of FairGBM by real-world practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PH2B73N3/Cruz et al. - 2022 - FairGBM Gradient Boosting with Fairness Constrain.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I2CU9IL6/2209.html}
}

@inproceedings{dalpozzoloCalibratingProbabilityUndersampling2015,
  title = {Calibrating {{Probability}} with {{Undersampling}} for {{Unbalanced Classification}}},
  booktitle = {2015 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Dal Pozzolo, Andrea and Caelen, Olivier and Johnson, Reid and Bontempi, Gianluca},
  year = {2015},
  month = dec,
  address = {{Cape Town, South Africa}},
  doi = {10.1109/SSCI.2015.33},
  abstract = {Undersampling is a popular technique for unbalanced datasets to reduce the skew in class distributions. However, it is well-known that undersampling one class modifies the priors of the training set and consequently biases the posterior probabilities of a classifier [9]. In this paper, we study analytically and experimentally how undersampling affects the posterior probability of a machine learning model. We formalize the problem of undersampling and explore the relationship between conditional probability in the presence and absence of undersampling. Although the bias due to undersampling does not affect the ranking order returned by the posterior probability, it significantly impacts the classification accuracy and probability calibration. We use Bayes Minimum Risk theory to find the correct classification threshold and show how to adjust it after undersampling. Experiments on several real-world unbalanced datasets validate our results.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LZXPHU5Q/Dal Pozzolo et al. - 2015 - Calibrating Probability with Undersampling for Unb.pdf}
}

@inproceedings{dalviAdversarialClassification2004,
  title = {Adversarial {{Classification}}},
  booktitle = {Proceedings of the Tenth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Dalvi, Nilesh and Domingos, Pedro and Mausam and Sanghai, Sumit and Verma, Deepak},
  year = {2004},
  month = aug,
  series = {{{KDD}} '04},
  pages = {99--108},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1014052.1014066},
  abstract = {Essentially all data mining algorithms assume that the data-generating process is independent of the data miner's activities. However, in many domains, including spam detection, intrusion detection, fraud detection, surveillance and counter-terrorism, this is far from the case: the data is actively manipulated by an adversary seeking to make the classifier produce false negatives. In these domains, the performance of a classifier can degrade rapidly after it is deployed, as the adversary learns to defeat it. Currently the only solution to this is repeated, manual, ad hoc reconstruction of the classifier. In this paper we develop a formal framework and algorithms for this problem. We view classification as a game between the classifier and the adversary, and produce a classifier that is optimal given the adversary's optimal strategy. Experiments in a spam detection domain show that this approach can greatly outperform a classifier learned in the standard way, and (within the parameters of the problem) automatically adapt the classifier to the adversary's evolving manipulations.},
  isbn = {978-1-58113-888-7},
  keywords = {cost-sensitive learning,game theory,integer linear programming,naive Bayes,spam detection},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/M7VY9VDY/Dalvi et al. - 2004 - Adversarial classification.pdf}
}

@article{dasFairnessMeasuresMachine2021,
  title = {Fairness {{Measures}} for {{Machine Learning}} in {{Finance}}},
  author = {Das, Sanjiv and Donini, Michele and Gelman, Jason and Haas, Kevin and Hardt, Mila and Katzman, Jared and Kenthapadi, Krishnaram and Larroy, Pedro and Yilmaz, Pinar and Zafar, Muhammad Bilal},
  year = {2021},
  month = oct,
  journal = {The Journal of Financial Data Science},
  volume = {3},
  number = {4},
  pages = {33--64},
  issn = {2640-3943},
  doi = {10.3905/jfds.2021.1.075},
  abstract = {We present a machine learning pipeline for fairness-aware machine learning (FAML) in finance that encompasses metrics for fairness (and accuracy). Whereas accuracy metrics are well understood and the principal ones used frequently, there is no consensus as to which of several available measures for fairness should be used in a generic manner in the financial services industry. We explore these measures and discuss which ones to focus on, at various stages in the ML pipeline, pre-training and post-training, and we also examine simple bias mitigation approaches. Using a standard dataset we show that the sequencing in our FAML pipeline offers a cogent approach to arriving at a fair and accurate ML model. We discuss the intersection of bias metrics with legal considerations in the US, and the entanglement of explainability and fairness is exemplified in the case study. We discuss possible approaches for training ML models while satisfying constraints imposed from various fairness metrics, and the role of causality in assessing fairness.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5IHEXZPR/Das et al. - 2021 - Fairness Measures for Machine Learning in Finance.pdf}
}

@inproceedings{dasFindingHighValueTraining2021,
  title = {Finding {{High-Value Training Data Subset Through Differentiable Convex Programming}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}. {{Research Track}}},
  author = {Das, Soumi and Singh, Arshdeep and Chatterjee, Saptarshi and Bhattacharya, Suparna and Bhattacharya, Sourangshu},
  editor = {Oliver, Nuria and {P{\'e}rez-Cruz}, Fernando and Kramer, Stefan and Read, Jesse and Lozano, Jose A.},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {666--681},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-86520-7_41},
  abstract = {Finding valuable training data points for deep neural networks has been a core research challenge with many applications. In recent years, various techniques for calculating the ``value'' of individual training datapoints have been proposed for explaining trained models. However, the value of a training datapoint also depends on other selected training datapoints - a notion which is not explicitly captured by existing methods. In this paper, we study the problem of selecting high-value subsets of training data. The key idea is to design a learnable framework for online subset selection, which can be learned using mini-batches of training data, thus making our method scalable. This results in a parameterised convex subset selection problem that is amenable to a differentiable convex programming paradigm, thus allowing us to learn the parameters of the selection model in an end-to-end training. Using this framework, we design an online alternating minimization based algorithm for jointly learning the parameters of the selection model and ML model. Extensive evaluation on a synthetic dataset, and three standard datasets, show that our algorithm finds consistently higher value subsets of training data, compared to the recent state of the art methods, sometimes \$\$\textbackslash sim 20\textbackslash\%\$\${$\sim$}20\%higher value than existing methods. The subsets are also useful in finding mislabelled training data. Our algorithm takes running time comparable to the existing valuation functions.},
  isbn = {978-3-030-86520-7},
  langid = {english},
  keywords = {Convex optimisation,Data valuation,Explainability,Subset selection},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NGCBMMGV/Das et al. - 2021 - Finding High-Value Training Data Subset Through Di.pdf}
}

@misc{DataAugmentationDiscrimination,
  title = {Data {{Augmentation}} for {{Discrimination Prevention}} and {{Bias Disambiguation}} | {{Proceedings}} of the {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  howpublished = {https://dl.acm.org/doi/10.1145/3375627.3375865},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4X7585L4/3375627.html}
}

@inproceedings{databias-counter-chiappa2019path,
  title = {Path-{{Specific Counterfactual Fairness}}},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Chiappa, Silvia},
  year = {2019},
  volume = {33},
  pages = {7801--7808}
}

@inproceedings{datasource-noisylabelsfair,
  title = {The {{Impacts}} of {{Labeling Biases}} on {{Fairness Criteria}}},
  booktitle = {10th International Conference on Learning Representations, {{ICLR}} 2022, Virtual Event, April 25-29, 2022},
  author = {Liao, Yiqiao and Naghizadeh, Parinaz},
  year = {2022}
}

@inproceedings{dawidBewareDAG2010,
  title = {Beware of the {{DAG}}!},
  booktitle = {Proceedings of {{Workshop}} on {{Causality}}: {{Objectives}} and {{Assessment}} at {{NIPS}} 2008},
  author = {Dawid, A. Philip},
  year = {2010},
  month = feb,
  pages = {59--86},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Directed acyclic graph (DAG) models are popular tools for describing causal relationships and for guiding attempts to learn them from data.  They appear to supply a means of extracting causal conclusions from probabilistic conditional independence properties inferred from purely observational data.  I take a critical look at  this enterprise, and suggest that it is in need of more, and more explicit, methodological and philosophical justification than it typically receives. In particular, I argue for the value of a clean separation between formal causal language and intuitive causal assumptions.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/58Z5RY8W/Dawid - 2010 - Beware of the DAG!.pdf}
}

@misc{de-arteagaLearningSelectiveLabels2018,
  title = {Learning under Selective Labels in the Presence of Expert Consistency},
  author = {{De-Arteaga}, Maria and Dubrawski, Artur and Chouldechova, Alexandra},
  year = {2018},
  month = jul,
  number = {arXiv:1807.00905},
  eprint = {1807.00905},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We explore the problem of learning under selective labels in the context of algorithm-assisted decision making. Selective labels is a pervasive selection bias problem that arises when historical decision making blinds us to the true outcome for certain instances. Examples of this are common in many applications, ranging from predicting recidivism using pre-trial release data to diagnosing patients. In this paper we discuss why selective labels often cannot be effectively tackled by standard methods for adjusting for sample selection bias, even if there are no unobservables. We propose a data augmentation approach that can be used to either leverage expert consistency to mitigate the partial blindness that results from selective labels, or to empirically validate whether learning under such framework may lead to unreliable models prone to systemic discrimination.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/67PDB66F/De-Arteaga et al. - 2018 - Learning under selective labels in the presence of.pdf}
}

@misc{delbarrioReviewMathematicalFrameworks2020,
  title = {Review of {{Mathematical}} Frameworks for {{Fairness}} in {{Machine Learning}}},
  author = {{del Barrio}, Eustasio and Gordaliza, Paula and Loubes, Jean-Michel},
  year = {2020},
  month = may,
  number = {arXiv:2005.13755},
  eprint = {2005.13755},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.13755},
  abstract = {A review of the main fairness definitions and fair learning methodologies proposed in the literature over the last years is presented from a mathematical point of view. Following our independence-based approach, we consider how to build fair algorithms and the consequences on the degradation of their performance compared to the possibly unfair case. This corresponds to the price for fairness given by the criteria \$\textbackslash textit\{statistical parity\}\$ or \$\textbackslash textit\{equality of odds\}\$. Novel results giving the expressions of the optimal fair classifier and the optimal fair predictor (under a linear regression gaussian model) in the sense of \$\textbackslash textit\{equality of odds\}\$ are presented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I9TAEJFM/del Barrio et al. - 2020 - Review of Mathematical frameworks for Fairness in .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7FMA24EW/2005.html}
}

@inproceedings{dengExploringHowMachine2022a,
  title = {Exploring {{How Machine Learning Practitioners}} ({{Try To}}) {{Use Fairness Toolkits}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Deng, Wesley Hanwen and Nagireddy, Manish and Lee, Michelle Seng Ah and Singh, Jatinder and Wu, Zhiwei Steven and Holstein, Kenneth and Zhu, Haiyi},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {473--484},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3531146.3533113},
  abstract = {Recent years have seen the development of many open-source ML fairness toolkits aimed at helping ML practitioners assess and address unfairness in their systems. However, there has been little research investigating how ML practitioners actually use these toolkits in practice. In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts.},
  isbn = {978-1-4503-9352-2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/47TBVCYL/Deng et al. - 2022 - Exploring How Machine Learning Practitioners (Try .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/D2LPLU8B/Deng et al. - 2022 - Exploring How Machine Learning Practitioners (Try .pdf}
}

@article{dengMNISTDatabaseHandwritten2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  abstract = {In this issue, ``Best of the Web'' presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  keywords = {Machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/45WMR8R7/6296535.html}
}

@inproceedings{dingRetiringAdultNew2021,
  title = {Retiring {{Adult}}: {{New Datasets}} for {{Fair Machine Learning}}},
  shorttitle = {Retiring {{Adult}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
  year = {2021},
  volume = {34},
  pages = {6478--6490},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/K26MSI5J/Ding et al. - 2021 - Retiring Adult New Datasets for Fair Machine Lear.pdf}
}

@inproceedings{dixonMeasuringMitigatingUnintended2018,
  title = {Measuring and {{Mitigating Unintended Bias}} in {{Text Classification}}},
  booktitle = {Proceedings of the 2018 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  year = {2018},
  month = dec,
  series = {{{AIES}} '18},
  pages = {67--73},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3278721.3278729},
  abstract = {We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our definition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classifiers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.},
  isbn = {978-1-4503-6012-8},
  keywords = {algorithmic bias,fairness,machine learning,natural language processing,text classification},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VEVE5VEV/Dixon et al. - 2018 - Measuring and Mitigating Unintended Bias in Text C.pdf}
}

@inproceedings{doniniEmpiricalRiskMinimization2018,
  title = {Empirical {{Risk Minimization Under Fairness Constraints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Donini, Michele and Oneto, Luca and {Ben-David}, Shai and {Shawe-Taylor}, John S and Pontil, Massimiliano},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We address the problem of algorithmic fairness: ensuring that sensitive information does not unfairly influence the outcome of a classifier. We present an approach based on empirical risk minimization, which incorporates a fairness constraint into the learning problem. It encourages the conditional risk of the learned classifier to be approximately constant with respect to the sensitive variable. We derive both risk and fairness bounds that support the statistical consistency of our methodology. We specify our approach to kernel methods and observe that the fairness requirement implies an orthogonality constraint which can be easily added to these methods. We further observe that for linear models the constraint translates into a simple data preprocessing step. Experiments indicate that the method is empirically effective and performs favorably against state-of-the-art approaches.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BVZQQUTR/Donini et al. - 2018 - Empirical Risk Minimization Under Fairness Constra.pdf}
}

@inproceedings{dorleonFeatureSelectionFairness2022,
  title = {Feature {{Selection Under Fairness}} and~{{Performance Constraints}}},
  booktitle = {Big {{Data Analytics}} and {{Knowledge Discovery}}},
  author = {Dorleon, Ginel and Megdiche, Imen and {Bricon-Souf}, Nathalie and Teste, Olivier},
  editor = {Wrembel, Robert and Gamper, Johann and Kotsis, Gabriele and Tjoa, A. Min and Khalil, Ismail},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {125--130},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-12670-3_11},
  abstract = {Feature selection is an essential preprocessing procedure in data analysis. The process refers to selecting a subset of relevant features to improve prediction performance and better understand the data. However, we notice that traditional feature selection methods have limited ability to deal with data distribution over protected features due to data imbalance and indeed protected features are selected. Two problems can occur with current feature selection methods when protected features are considered: the presence of protected features among the selected ones which often lead to unfair results and the presence of redundant features which carry potentially the same information with the protected ones. To address these issues, we introduce in this paper a fair feature selection method that takes into account the existence of protected features and their redundant. Our new method finds a set of relevant features with no protected features and with the least possible redundancy under prediction quality constraint. This constraint consists of a trade-off between fairness and prediction performance. Our experiments on well-known biased datasets from the literature demonstrated that our proposed method outperformed the traditional feature selection methods under comparison in terms of performance and fairness.},
  isbn = {978-3-031-12670-3},
  langid = {english},
  keywords = {Bias,Fairness,Feature selection,Machine learning,Protected features}
}

@book{dt,
  title = {Classification and Regression Trees},
  author = {Breiman, Leo and Friedman, Jerome H and Olshen, Richard A and Stone, Charles J},
  year = {1984},
  publisher = {{CRC press}}
}

@article{duttaInformationTheoreticQuantificationDiscrimination2020,
  title = {An {{Information-Theoretic Quantification}} of {{Discrimination}} with {{Exempt Features}}},
  author = {Dutta, Sanghamitra and Venkatesh, Praveen and Mardziel, Piotr and Datta, Anupam and Grover, Pulkit},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {3825--3833},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.5794},
  abstract = {The needs of a business (e.g., hiring) may require the use of certain features that are critical in a way that any discrimination arising due to them should be exempted. In this work, we propose a novel information-theoretic decomposition of the total discrimination (in a counterfactual sense) into a non-exempt component, which quantifies the part of the discrimination that cannot be accounted for by the critical features, and an exempt component, which quantifies the remaining discrimination. Our decomposition enables selective removal of the non-exempt component if desired. We arrive at this decomposition through examples and counterexamples that enable us to first obtain a set of desirable properties that any measure of non-exempt discrimination should satisfy. We then demonstrate that our proposed quantification of non-exempt discrimination satisfies all of them. This decomposition leverages a body of work from information theory called Partial Information Decomposition (PID). We also obtain an impossibility result showing that no observational measure of non-exempt discrimination can satisfy all of the desired properties, which leads us to relax our goals and examine alternative observational measures that satisfy only some of these properties. We then perform a case study using one observational measure to show how one might train a model allowing for exemption of discrimination due to critical features.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/429UVFT9/Dutta et al. - 2020 - An Information-Theoretic Quantification of Discrim.pdf}
}

@misc{duttaQuantifyingFeatureContributions2022,
  title = {Quantifying {{Feature Contributions}} to {{Overall Disparity Using Information Theory}}},
  author = {Dutta, Sanghamitra and Venkatesh, Praveen and Grover, Pulkit},
  year = {2022},
  month = jun,
  number = {arXiv:2206.08454},
  eprint = {2206.08454},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.08454},
  abstract = {When a machine-learning algorithm makes biased decisions, it can be helpful to understand the sources of disparity to explain why the bias exists. Towards this, we examine the problem of quantifying the contribution of each individual feature to the observed disparity. If we have access to the decision-making model, one potential approach (inspired from intervention-based approaches in explainability literature) is to vary each individual feature (while keeping the others fixed) and use the resulting change in disparity to quantify its contribution. However, we may not have access to the model or be able to test/audit its outputs for individually varying features. Furthermore, the decision may not always be a deterministic function of the input features (e.g., with human-in-the-loop). For these situations, we might need to explain contributions using purely distributional (i.e., observational) techniques, rather than interventional. We ask the question: what is the "potential" contribution of each individual feature to the observed disparity in the decisions when the exact decision-making mechanism is not accessible? We first provide canonical examples (thought experiments) that help illustrate the difference between distributional and interventional approaches to explaining contributions, and when either is better suited. When unable to intervene on the inputs, we quantify the "redundant" statistical dependency about the protected attribute that is present in both the final decision and an individual feature, by leveraging a body of work in information theory called Partial Information Decomposition. We also perform a simple case study to show how this technique could be applied to quantify contributions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CFWPV53D/Dutta et al. - Quantifying Feature Contributions to Overall Dispa.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I9CJ62GF/Dutta et al. - 2022 - Quantifying Feature Contributions to Overall Dispa.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/C43RB368/2206.html}
}

@inproceedings{dworkDifferentialPrivacySurvey2008,
  title = {Differential {{Privacy}}: {{A Survey}} of {{Results}}},
  shorttitle = {Differential {{Privacy}}},
  booktitle = {Theory and {{Applications}} of {{Models}} of {{Computation}}},
  author = {Dwork, Cynthia},
  editor = {Agrawal, Manindra and Du, Dingzhu and Duan, Zhenhua and Li, Angsheng},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--19},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-79228-4_1},
  abstract = {Over the past five years a new approach to privacy-preserving data analysis has born fruit [13, 18, 7, 19, 5, 37, 35, 8, 32]. This approach differs from much (but not all!) of the related literature in the statistics, databases, theory, and cryptography communities, in that a formal and ad omnia privacy guarantee is defined, and the data analysis techniques presented are rigorously proved to satisfy the guarantee. The key privacy guarantee that has emerged is differential privacy. Roughly speaking, this ensures that (almost, and quantifiably) no risk is incurred by joining a statistical database.},
  isbn = {978-3-540-79228-4},
  langid = {english},
  keywords = {Differential Privacy,Privacy Mechanism,Statistical Database,Statistical Query,True Answer}
}

@inproceedings{dworkFairnessAwareness2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = {2012},
  month = jan,
  series = {{{ITCS}} '12},
  pages = {214--226},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2090236.2090255},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of "fair affirmative action," which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VRWDVC96/Dwork et al. - 2012 - Fairness through awareness.pdf}
}

@misc{elieOverviewActiveLearning2022,
  title = {An Overview of Active Learning Methods for Insurance with Fairness Appreciation},
  author = {Elie, Romuald and Hillairet, Caroline and Hu, Fran{\c c}ois and Juillard, Marc},
  year = {2022},
  month = mar,
  number = {arXiv:2112.09466},
  eprint = {2112.09466},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {This paper addresses and solves some challenges in the adoption of machine learning in insurance with the democratization of model deployment. The first challenge is reducing the labelling effort (hence focusing on the data quality) with the help of active learning, a feedback loop between the model inference and an oracle: as in insurance the unlabeled data is usually abundant, active learning can become a significant asset in reducing the labelling cost. For that purpose, this paper sketches out various classical active learning methodologies before studying their empirical impact on both synthetic and real datasets. Another key challenge in insurance is the fairness issue in model inferences. We will introduce and integrate a post-processing fairness for multi-class tasks in this active learning framework to solve these two issues. Finally numerical experiments on unfair datasets highlight that the proposed setup presents a good compromise between model precision and fairness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8GUBR2S3/Elie et al. - 2022 - An overview of active learning methods for insuran.pdf}
}

@misc{estornellUnfairnessAwarenessGroupFair2021,
  title = {Unfairness {{Despite Awareness}}: {{Group-Fair Classification}} with {{Strategic Agents}}},
  shorttitle = {Unfairness {{Despite Awareness}}},
  author = {Estornell, Andrew and Das, Sanmay and Liu, Yang and Vorobeychik, Yevgeniy},
  year = {2021},
  month = dec,
  number = {arXiv:2112.02746},
  eprint = {2112.02746},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.02746},
  abstract = {The use of algorithmic decision making systems in domains which impact the financial, social, and political well-being of people has created a demand for these decision making systems to be "fair" under some accepted notion of equity. This demand has in turn inspired a large body of work focused on the development of fair learning algorithms which are then used in lieu of their conventional counterparts. Most analysis of such fair algorithms proceeds from the assumption that the people affected by the algorithmic decisions are represented as immutable feature vectors. However, strategic agents may possess both the ability and the incentive to manipulate this observed feature vector in order to attain a more favorable outcome. We explore the impact that strategic agent behavior could have on fair classifiers and derive conditions under which this behavior leads to fair classifiers becoming less fair than their conventional counterparts under the same measure of fairness that the fair classifier takes into account. These conditions are related to the the way in which the fair classifier remedies unfairness on the original unmanipulated data: fair classifiers which remedy unfairness by becoming more selective than their conventional counterparts are the ones that become less fair than their counterparts when agents are strategic. We further demonstrate that both the increased selectiveness of the fair classifier, and consequently the loss of fairness, arises when performing fair learning on domains in which the advantaged group is overrepresented in the region near (and on the beneficial side of) the decision boundary of conventional classifiers. Finally, we observe experimentally, using several datasets and learning methods, that this fairness reversal is common, and that our theoretical characterization of the fairness reversal conditions indeed holds in most such cases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3GAQYZY7/Estornell et al. - 2021 - Unfairness Despite Awareness Group-Fair Classific.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/C9CBGPBH/2112.html}
}

@article{exacerbate-inequities-howard2018ugly,
  title = {The Ugly Truth about Ourselves and Our Robot Creations: The Problem of Bias and Social Inequity},
  author = {Howard, Ayanna and Borenstein, Jason},
  year = {2018},
  journal = {Science and engineering ethics},
  volume = {24},
  number = {5},
  pages = {1521--1536},
  publisher = {{Springer}}
}

@misc{exacerbate-inequities-kirchner2016machine,
  title = {Machine {{Bias}}: {{There}}'s Software Used across the Country to Predict Future Criminals. {{And}} It's Biased against Blacks},
  author = {Angwin, Julia and Larson, Jeff and Kirchner, Lauren and Mattu, Surya},
  year = {2016},
  month = may
}

@book{exacerbate-inequities-o2016weapons,
  title = {Weapons of Math Destruction: {{How}} Big Data Increases Inequality and Threatens Democracy},
  author = {O'Neil, Cathy},
  year = {2016},
  publisher = {{Crown}}
}

@book{exacerbate-inequities-osoba2017intelligence,
  title = {An Intelligence in Our Image: {{The}} Risks of Bias and Errors in Artificial Intelligence},
  author = {Osoba, Osonde A and Welser IV, William},
  year = {2017},
  publisher = {{Rand Corporation}}
}

@misc{ExplainableFairnessRecommendation,
  title = {Explainable {{Fairness}} in {{Recommendation}} | {{Proceedings}} of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  howpublished = {https://dl.acm.org/doi/10.1145/3477495.3531973},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/TGXJX883/3477495.html}
}

@inproceedings{f.cruzPromotingFairnessHyperparameter2021,
  title = {Promoting {{Fairness}} through {{Hyperparameter Optimization}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Cruz, Andr{\'e} F. and Saleiro, Pedro and Bel{\'e}m, Catarina and Soares, Carlos and Bizarro, Pedro},
  year = {2021},
  month = dec,
  pages = {1036--1041},
  issn = {2374-8486},
  doi = {10.1109/ICDM51629.2021.00119},
  abstract = {Considerable research effort has been guided towards algorithmic fairness but real-world adoption of bias reduction techniques is still scarce. Existing methods are either metric-or model-specific, require access to sensitive attributes at inference time, or carry high development or deployment costs. This work explores the unfairness that emerges when optimizing ML models solely for predictive performance, and how to mitigate it with a simple and easily deployed intervention: fairness-aware hyperparameter optimization (HO). We propose and evaluate fairness-aware variants of three popular HO algorithms: Fair Random Search, Fair TPE, and Fairband. We validate our approach on a real-world bank account opening fraud case-study, as well as on three datasets from the fairness literature. Results show that, without extra training cost, it is feasible to find models with 111\% mean fairness increase and just 6\% decrease in performance when compared with fairness-blind HO.1},
  keywords = {Costs,fairness,hyperparameter optimization,Inference algorithms,machine learning,Navigation,Pipelines,Prediction algorithms,Predictive models,Training,trustworthy AI},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CSEZ8ZQW/F.Cruz et al. - 2021 - Promoting Fairness through Hyperparameter Optimiza.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RQ9GDD9V/9679036.html}
}

@misc{fabbrizziSurveyBiasVisual2022,
  title = {A {{Survey}} on {{Bias}} in {{Visual Datasets}}},
  author = {Fabbrizzi, Simone and Papadopoulos, Symeon and Ntoutsi, Eirini and Kompatsiaris, Ioannis},
  year = {2022},
  month = jun,
  number = {arXiv:2107.07919},
  eprint = {2107.07919},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07919},
  abstract = {Computer Vision (CV) has achieved remarkable results, outperforming humans in several tasks. Nonetheless, it may result in significant discrimination if not handled properly as CV systems highly depend on the data they are fed with and can learn and amplify biases within such data. Thus, the problems of understanding and discovering biases are of utmost importance. Yet, there is no comprehensive survey on bias in visual datasets. Hence, this work aims to: i) describe the biases that might manifest in visual datasets; ii) review the literature on methods for bias discovery and quantification in visual datasets; iii) discuss existing attempts to collect bias-aware visual datasets. A key conclusion of our study is that the problem of bias discovery and quantification in visual datasets is still open, and there is room for improvement in terms of both methods and the range of biases that can be addressed. Moreover, there is no such thing as a bias-free dataset, so scientists and practitioners must become aware of the biases in their datasets and make them explicit. To this end, we propose a checklist to spot different types of bias during visual dataset collection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VQE2RA7M/Fabbrizzi et al. - 2022 - A Survey on Bias in Visual Datasets.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ECFVJ87P/2107.html}
}

@book{fairmlbook,
  title = {Fairness and Machine Learning},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2019},
  publisher = {{fairmlbook.org}}
}

@incollection{fangDPCTGANDifferentiallyPrivate2022,
  title = {{{DP-CTGAN}}: {{Differentially Private Medical Data Generation Using CTGANs}}},
  shorttitle = {{{DP-CTGAN}}},
  booktitle = {Artificial {{Intelligence}} in {{Medicine}}},
  author = {Fang, Mei Ling and Dhami, Devendra Singh and Kersting, Kristian},
  editor = {Michalowski, Martin and Abidi, Syed Sibte Raza and Abidi, Samina},
  year = {2022},
  volume = {13263},
  pages = {178--188},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-09342-5_17},
  abstract = {Generative Adversarial Networks (GANs) are an important tool to generate synthetic medical data, in order to combat the limited and difficult access to the real data sets and accelerate the innovation in the healthcare domain. Despite their promising capability, they are vulnerable to various privacy attacks that might reveal information of individuals from the training data. Preserving privacy while keeping the quality of the generated data still remains a challenging problem. We propose DP-CTGAN, which incorporates differential privacy into a conditional tabular generative model. Our experiments demonstrate that our model outperforms existing state-of-the-art models under the same privacy budget on several benchmark data sets. In addition, we combine our method with federated learning, enabling a more secure way of synthetic data generation without the need of uploading locally collected data to a central repository.},
  isbn = {978-3-031-09341-8 978-3-031-09342-5},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7PRWE5IN/Fang et al. - 2022 - DP-CTGAN Differentially Private Medical Data Gene.pdf}
}

@article{fazelpourAlgorithmicBiasSenses2021,
  title = {Algorithmic Bias: {{Senses}}, Sources, Solutions},
  shorttitle = {Algorithmic Bias},
  author = {Fazelpour, Sina and Danks, David},
  year = {2021},
  journal = {Philosophy Compass},
  volume = {16},
  number = {8},
  pages = {e12760},
  issn = {1747-9991},
  doi = {10.1111/phc3.12760},
  abstract = {Data-driven algorithms are widely used to make or assist decisions in sensitive domains, including healthcare, social services, education, hiring, and criminal justice. In various cases, such algorithms have preserved or even exacerbated biases against vulnerable communities, sparking a vibrant field of research focused on so-called algorithmic biases. This research includes work on identification, diagnosis, and response to biases in algorithm-based decision-making. This paper aims to facilitate the application of philosophical analysis to these contested issues by providing an overview of three key topics: What is algorithmic bias? Why and how can it occur? What can and should be done about it? Throughout, we highlight connections\textemdash both actual and potential\textemdash with philosophical ideas and concerns.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/phc3.12760},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4SJ47ZAZ/Fazelpour and Danks - 2021 - Algorithmic bias Senses, sources, solutions.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NNYFBNP6/phc3.html}
}

@inproceedings{feldmanCertifyingRemovingDisparate2015,
  title = {Certifying and {{Removing Disparate Impact}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Feldman, Michael and Friedler, Sorelle A. and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year = {2015},
  month = aug,
  series = {{{KDD}} '15},
  pages = {259--268},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2783258.2783311},
  abstract = {What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process. When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses. We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.},
  isbn = {978-1-4503-3664-2},
  keywords = {disparate impact,fairness,machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WQIZCSKB/Feldman et al. - 2015 - Certifying and Removing Disparate Impact.pdf}
}

@article{finbias-4,
  title = {Artificial Intelligence and Machine Learning in Financial Services: {{Market}} Developments and Financial Stability Implications},
  author = {Board, Financial Stability},
  year = {2017},
  journal = {Financial Stability Board},
  volume = {45}
}

@article{fleuretFastBinaryFeature2004,
  title = {Fast {{Binary Feature Selection}} with {{Conditional Mutual Information}}},
  author = {Fleuret, Fran{\c c}ois},
  year = {2004},
  journal = {Journal of Machine Learning Research},
  volume = {5},
  number = {Nov},
  pages = {1531--1555},
  issn = {ISSN 1533-7928},
  abstract = {We propose in this paper a very fast feature selection technique based on conditional mutual information. By picking features which maximize their mutual information with the class to predict conditional to any feature already picked, it ensures the selection of features which are both individually informative and two-by-two weakly dependant. We show that this feature selection method outperforms other classical algorithms, and that a naive Bayesian classifier built with features selected that way achieves error rates similar to those of state-of-the-art methods such as boosting or SVMs. The implementation we propose selects 50 features among 40,000, based on a training set of 500 examples in a tenth of a second on a standard 1Ghz PC.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9QZMHJFL/Fleuret - 2004 - Fast Binary Feature Selection with Conditional Mut.pdf}
}

@inproceedings{fogliatoFairnessEvaluationPresence2020,
  title = {Fairness {{Evaluation}} in {{Presence}} of {{Biased Noisy Labels}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Fogliato, Riccardo and Chouldechova, Alexandra and G'Sell, Max},
  year = {2020},
  month = jun,
  pages = {2325--2336},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Risk assessment tools are widely used around the country to inform decision making within the criminal justice system. Recently, considerable attention has been devoted to the question of whether such tools may suffer from racial bias. In this type of assessment, a fundamental issue is that the training and evaluation of the model is based on a variable (arrest) that may represent a noisy version of an unobserved outcome of more central interest (offense). We propose a sensitivity analysis framework for assessing how assumptions on the noise across groups affect the predictive bias properties of the risk assessment model as a predictor of reoffense. Our experimental results on two real world criminal justice data sets demonstrate how even small biases in the observed labels may call into question the conclusions of an analysis based on the noisy outcome.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/92S8GUJI/Fogliato et al. - 2020 - Fairness Evaluation in Presence of Biased Noisy La.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/H9H9XEYG/Fogliato et al. - 2020 - Fairness Evaluation in Presence of Biased Noisy La.pdf}
}

@inproceedings{fouldsIntersectionalDefinitionFairness2020,
  title = {An {{Intersectional Definition}} of {{Fairness}}},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Foulds, James R. and Islam, Rashidul and Keya, Kamrun Naher and Pan, Shimei},
  year = {2020},
  month = apr,
  pages = {1918--1921},
  issn = {2375-026X},
  doi = {10.1109/ICDE48307.2020.00203},
  abstract = {We propose differential fairness, a multi-attribute definition of fairness in machine learning which is informed by intersectionality, a critical lens arising from the humanities literature, leveraging connections between differential privacy and legal notions of fairness. We show that our criterion behaves sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. We provide a learning algorithm which respects our differential fairness criterion. Experiments on the COMPAS criminal recidivism dataset and census data demonstrate the utility of our methods.},
  keywords = {80\% rule,AI and society,Computer Science - Computers and Society,Computer Science - Machine Learning,fairness in AI,Law,Lenses,Machine learning,privacy,Privacy,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IFGH3SUZ/Foulds et al. - 2020 - An Intersectional Definition of Fairness.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/L8IUC5SM/9101635.html}
}

@article{FraudSurvey2016,
  title = {Fraud Detection System: {{A}} Survey},
  author = {Abdallah, Aisha and Maarof, Mohd Aizaini and Zainal, Anazida},
  year = {2016},
  journal = {Journal of Network and Computer Applications},
  volume = {68},
  pages = {90--113},
  issn = {1084-8045}
}

@inproceedings{friedlerComparativeStudyFairnessenhancing2019a,
  title = {A Comparative Study of Fairness-Enhancing Interventions in Machine Learning},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {329--338},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287589},
  abstract = {Computers are increasingly used to make decisions that have significant impact on people's lives. Often, these predictions can affect different population subgroups disproportionately. As a result, the issue of fairness has received much recent interest, and a number of fairness-enhanced classifiers have appeared in the literature. This paper seeks to study the following questions: how do these different techniques fundamentally compare to one another, and what accounts for the differences? Specifically, we seek to bring attention to many under-appreciated aspects of such fairness-enhancing interventions that require investigation for these algorithms to receive broad adoption. We present the results of an open benchmark we have developed that lets us compare a number of different algorithms under a variety of fairness measures and existing datasets. We find that although different algorithms tend to prefer specific formulations of fairness preservations, many of these measures strongly correlate with one another. In addition, we find that fairness-preserving algorithms tend to be sensitive to fluctuations in dataset composition (simulated in our benchmark by varying training-test splits) and to different forms of preprocessing, indicating that fairness interventions might be more brittle than previously thought.},
  isbn = {978-1-4503-6125-5},
  keywords = {benchmarks,Fairness-aware machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HHFKKVWI/Friedler et al. - 2019 - A comparative study of fairness-enhancing interven.pdf}
}

@misc{friedlerImPossibilityFairness2016,
  title = {On the (Im)Possibility of Fairness},
  author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  year = {2016},
  month = sep,
  number = {arXiv:1609.07236},
  eprint = {1609.07236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.07236},
  abstract = {What does it mean for an algorithm to be fair? Different papers use different notions of algorithmic fairness, and although these appear internally consistent, they also seem mutually incompatible. We present a mathematical setting in which the distinctions in previous papers can be made formal. In addition to characterizing the spaces of inputs (the "observed" space) and outputs (the "decision" space), we introduce the notion of a construct space: a space that captures unobservable, but meaningful variables for the prediction. We show that in order to prove desirable properties of the entire decision-making process, different mechanisms for fairness require different assumptions about the nature of the mapping from construct space to decision space. The results in this paper imply that future treatments of algorithmic fairness should more explicitly state assumptions about the relationship between constructs and observations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NM958VA2/Friedler et al. - 2016 - On the (im)possibility of fairness.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HLMK8VFM/1609.html}
}

@inproceedings{galhotraCausalFeatureSelection2022,
  title = {Causal {{Feature Selection}} for {{Algorithmic Fairness}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Galhotra, Sainyam and Shanmugam, Karthikeyan and Sattigeri, Prasanna and Varshney, Kush R.},
  year = {2022},
  month = jun,
  pages = {276--285},
  publisher = {{ACM}},
  address = {{Philadelphia PA USA}},
  doi = {10.1145/3514221.3517909},
  abstract = {The use of machine learning (ML) in high-stakes societal decisions has encouraged the consideration of fairness throughout the ML lifecycle. Although data integration is one of the primary steps to generate high-quality training data, most of the fairness literature ignores this stage. In this work, we consider fairness in the integration component of data management, aiming to identify features that improve prediction without adding any bias to the dataset. We work under the causal fairness paradigm [45]. Without requiring the underlying structural causal model a priori, we propose an approach to identify a sub-collection of features that ensure fairness of the dataset by performing conditional independence tests between different subsets of features. We use group testing to improve the complexity of the approach. We theoretically prove the correctness of the proposed algorithm and show that sublinear conditional independence tests are sufficient to identify these variables. A detailed empirical evaluation is performed on real-world datasets to demonstrate the efficacy and efficiency of our technique.},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/88RRICH3/Galhotra et al. - 2022 - Causal Feature Selection for Algorithmic Fairness.pdf}
}

@misc{geExplainableFairnessRecommendation2022,
  title = {Explainable {{Fairness}} in {{Recommendation}}},
  author = {Ge, Yingqiang and Tan, Juntao and Zhu, Yan and Xia, Yinglong and Luo, Jiebo and Liu, Shuchang and Fu, Zuohui and Geng, Shijie and Li, Zelong and Zhang, Yongfeng},
  year = {2022},
  month = jun,
  eprint = {2204.11159},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3477495.3531973},
  abstract = {Existing research on fairness-aware recommendation has mainly focused on the quantification of fairness and the development of fair recommendation models, neither of which studies a more substantial problem\textendash identifying the underlying reason of model disparity in recommendation. This information is critical for recommender system designers to understand the intrinsic recommendation mechanism and provides insights on how to improve model fairness to decision makers. Fortunately, with the rapid development of Explainable AI, we can use model explainability to gain insights into model (un)fairness. In this paper, we study the problem of explainable fairness, which helps to gain insights about why a system is fair or unfair, and guides the design of fair recommender systems with a more informed and unified methodology. Particularly, we focus on a common setting with feature-aware recommendation and exposure unfairness, but the proposed explainable fairness framework is general and can be applied to other recommendation settings and fairness definitions. We propose a Counterfactual Explainable Fairness framework, called CEF, which generates explanations about model fairness that can improve the fairness without significantly hurting the performance. The CEF framework formulates an optimization problem to learn the ``minimal'' change of the input features that changes the recommendation results to a certain level of fairness. Based on the counterfactual recommendation result of each feature, we calculate an explainability score in terms of the fairness-utility trade-off to rank all the feature-based explanations, and select the top ones as fairness explanations. Experimental results on several real-world datasets validate that our method is able to effectively provide explanations to the model disparities and these explanations can achieve better fairness-utility trade-off when using them for recommendation than all the baselines.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/F62NCKCF/Ge et al. - 2022 - Explainable Fairness in Recommendation.pdf}
}

@inproceedings{ghorbaniDataShapleyEquitable2019,
  title = {Data {{Shapley}}: {{Equitable Valuation}} of {{Data}} for {{Machine Learning}}},
  shorttitle = {Data {{Shapley}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Ghorbani, Amirata and Zou, James},
  year = {2019},
  month = may,
  pages = {2242--2251},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on nnn data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/G7QERWMU/Ghorbani and Zou - 2019 - Data Shapley Equitable Valuation of Data for Mach.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KNVI5CTU/Ghorbani and Zou - 2019 - Data Shapley Equitable Valuation of Data for Mach.pdf}
}

@misc{ghorbaniDataShapleyValuation2021,
  title = {Data {{Shapley Valuation}} for {{Efficient Batch Active Learning}}},
  author = {Ghorbani, Amirata and Zou, James and Esteva, Andre},
  year = {2021},
  month = apr,
  number = {arXiv:2104.08312},
  eprint = {2104.08312},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08312},
  abstract = {Annotating the right set of data amongst all available data points is a key challenge in many machine learning applications. Batch active learning is a popular approach to address this, in which batches of unlabeled data points are selected for annotation, while an underlying learning algorithm gets subsequently updated. Increasingly larger batches are particularly appealing in settings where data can be annotated in parallel, and model training is computationally expensive. A key challenge here is scale - typical active learning methods rely on diversity techniques, which select a diverse set of data points to annotate, from an unlabeled pool. In this work, we introduce Active Data Shapley (ADS) -- a filtering layer for batch active learning that significantly increases the efficiency of active learning by pre-selecting, using a linear time computation, the highest-value points from an unlabeled dataset. Using the notion of the Shapley value of data, our method estimates the value of unlabeled data points with regards to the prediction task at hand. We show that ADS is particularly effective when the pool of unlabeled data exhibits real-world caveats: noise, heterogeneity, and domain shift. We run experiments demonstrating that when ADS is used to pre-select the highest-ranking portion of an unlabeled dataset, the efficiency of state-of-the-art batch active learning methods increases by an average factor of 6x, while preserving performance effectiveness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/P95NN6DY/Ghorbani et al. - 2021 - Data Shapley Valuation for Efficient Batch Active .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LNAQH2Z3/2104.html}
}

@inproceedings{ghorbaniDistributionalFrameworkData2020a,
  title = {A {{Distributional Framework For Data Valuation}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Ghorbani, Amirata and Kim, Michael and Zou, James},
  year = {2020},
  month = nov,
  pages = {3535--3544},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Shapley value is a classic notion from game theory, historically used to quantify the contributions of individuals within groups, and more recently applied to assign values to data points when training machine learning models. Despite its foundational role, a key limitation of the data Shapley framework is that it only provides valuations for points within a fixed data set. It does not account for statistical aspects of the data and does not give a way to reason about points outside the data set. To address these limitations, we propose a novel framework \textendash{} distributional Shapley\textendash{} where the value of a point is defined in the context of an underlying data distribution. We prove that distributional Shapley has several desirable statistical properties; for example, the values are stable under perturbations to the data points themselves and to the underlying data distribution. We leverage these properties to develop a new algorithm for estimating values from data, which comes with formal guarantees and runs two orders of magnitude faster than state-of-the-art algorithms for computing the (non distributional) data Shapley values. We apply distributional Shapley to diverse data sets and demonstrate its utility in a data market setting.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/25QDWJ4V/Ghorbani et al. - 2020 - A Distributional Framework For Data Valuation.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/C9DI2TKC/Ghorbani et al. - 2020 - A Distributional Framework For Data Valuation.pdf}
}

@misc{ghoshHowBiasedYour2022,
  title = {How {{Biased}} Is {{Your Feature}}?: {{Computing Fairness Influence Functions}} with {{Global Sensitivity Analysis}}},
  shorttitle = {How {{Biased}} Is {{Your Feature}}?},
  author = {Ghosh, Bishwamittra and Basu, Debabrota and Meel, Kuldeep S.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.00667},
  eprint = {2206.00667},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Fairness in machine learning has attained significant focus due to the widespread application of machine learning in high-stake decision-making tasks. Unless regulated with a fairness objective, machine learning classifiers might demonstrate unfairness/bias towards certain demographic populations in the data. Thus, the quantification and mitigation of the bias induced by classifiers have become a central concern. In this paper, we aim to quantify the influence of different features on the bias of a classifier. To this end, we propose a framework of Fairness Influence Function (FIF), and compute it as a scaled difference of conditional variances in the classifier's prediction. We also instantiate an algorithm, FairXplainer, that uses variance decomposition among the subset of features and a local regressor to compute FIFs accurately, while also capturing the intersectional effects of the features. Our experimental analysis validates that FairXplainer captures the influences of both individual features and higher-order feature interactions, estimates the bias more accurately than existing local explanation methods, and detects the increase/decrease in bias due to affirmative/punitive actions in the classifier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6Y5L9FWH/Ghosh et al. - 2022 - How Biased is Your Feature Computing Fairness In.pdf}
}

@inproceedings{gorishniyRevisitingDeepLearning2021,
  title = {Revisiting {{Deep Learning Models}} for {{Tabular Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gorishniy, Yury and Rubachev, Ivan and Khrulkov, Valentin and Babenko, Artem},
  year = {2021},
  volume = {34},
  pages = {18932--18943},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks. Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols. We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution. The source code is available at https://github.com/yandex-research/rtdl.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/G3HFBKVG/Gorishniy et al. - 2021 - Revisiting Deep Learning Models for Tabular Data.pdf}
}

@article{grettonConsistentNonparametricTests2010,
  title = {Consistent {{Nonparametric Tests}} of {{Independence}}},
  author = {Gretton, Arthur and Gy{\"o}rfi, L{\'a}szl{\'o}},
  year = {2010},
  month = aug,
  journal = {The Journal of Machine Learning Research},
  volume = {11},
  pages = {1391--1423},
  issn = {1532-4435},
  abstract = {Three simple and explicit procedures for testing the independence of two multi-dimensional random variables are described. Two of the associated test statistics (L1, log-likelihood) are defined when the empirical distribution of the variables is restricted to finite partitions. A third test statistic is defined as a kernel-based independence measure. Two kinds of tests are provided. Distribution-free strong consistent tests are derived on the basis of large deviation bounds on the test statistics: these tests make almost surely no Type I or Type II error after a random sample size. Asymptotically {$\alpha$}-level tests are obtained from the limiting distribution of the test statistics. For the latter tests, the Type I error converges to a fixed non-zero value {$\alpha$}, and the Type II error drops to zero, for increasing sample size. All tests reject the null hypothesis of independence if the test statistics become large. The performance of the tests is evaluated experimentally on benchmark data.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/J6BBNWR9/Gretton and Gyrfi - 2010 - Consistent Nonparametric Tests of Independence.pdf}
}

@inproceedings{grettonKernelStatisticalTest2007,
  title = {A {{Kernel Statistical Test}} of {{Independence}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch{\"o}lkopf, Bernhard and Smola, Alex},
  year = {2007},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Although kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically significant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O(m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GXEQMDAW/Gretton et al. - 2007 - A Kernel Statistical Test of Independence.pdf}
}

@article{grgic-hlacaDistributiveFairnessAlgorithmic2018,
  title = {Beyond {{Distributive Fairness}} in {{Algorithmic Decision Making}}: {{Feature Selection}} for {{Procedurally Fair Learning}}},
  shorttitle = {Beyond {{Distributive Fairness}} in {{Algorithmic Decision Making}}},
  author = {{Grgi{\'c}-Hla{\v c}a}, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P. and Weller, Adrian},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11296},
  abstract = {With widespread use of machine learning methods in numerous domains involving humans, several studies have raised questions about the potential for unfairness towards certain individuals or groups. A number of recent works have proposed methods to measure and eliminate unfairness from machine learning models. However, most of this work has focused on only one dimension of fair decision making: distributive fairness, i.e., the fairness of the decision outcomes. In this work, we leverage the rich literature on organizational justice and focus on another dimension of fair decision making: procedural fairness, i.e., the fairness of the decision making process. We propose measures for procedural fairness that consider the input features used in the decision process, and evaluate the moral judgments of humans regarding the use of these features. We operationalize these measures on two real world datasets using human surveys on the Amazon Mechanical Turk (AMT) platform, demonstrating that our measures capture important properties of procedurally fair decision making. We provide fast submodular mechanisms to optimize the tradeoff between procedural fairness and prediction accuracy. On our datasets, we observe empirically that procedural fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Discrimination in classification},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BTF7AM87/Grgi-Hlaa et al. - 2018 - Beyond Distributive Fairness in Algorithmic Decisi.pdf}
}

@misc{grinsztajnWhyTreebasedModels2022,
  title = {Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?},
  author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  year = {2022},
  month = jul,
  number = {arXiv:2207.08815},
  eprint = {2207.08815},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.08815},
  abstract = {While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data (\$\textbackslash sim\$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and Neural Networks (NNs). This leads to a series of challenges which should guide researchers aiming to build tabular-specific NNs: 1. be robust to uninformative features, 2. preserve the orientation of the data, and 3. be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20 000 compute hours hyperparameter search for each learner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/A84JGX8Q/Grinsztajn et al. - 2022 - Why do tree-based models still outperform deep lea.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JQQA7JT4/2207.html}
}

@misc{groushap-gshap,
  title = {Generalized Shapley Additive Explanations},
  author = {Bowen, Dillon},
  year = {2020},
  month = may
}

@misc{gurobi,
  title = {Gurobi Optimizer Reference Manual},
  author = {{Gurobi Optimization, LLC}},
  year = {2022}
}

@inproceedings{haasPriceFairnessFramework2019,
  title = {The {{Price}} of {{Fairness}} - {{A Framework}} to {{Explore Trade-Offs}} in {{Algorithmic Fairness}}},
  booktitle = {{{ICIS}} 2019 {{Proceedings}}},
  author = {Haas, Christian},
  year = {2019},
  month = nov,
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZULQY999/19.html}
}

@inproceedings{haraDataCleansingModels2019,
  title = {Data {{Cleansing}} for {{Models Trained}} with {{SGD}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hara, Satoshi and Nitanda, Atsushi and Maehara, Takanori},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Data cleansing is a typical approach used to improve the accuracy of machine learning models, which, however, requires extensive domain knowledge to identify the influential instances that affect the models. In this paper, we propose an algorithm that can identify influential instances without using any domain knowledge. The proposed algorithm automatically cleans the data, which does not require any of the users' knowledge. Hence, even non-experts can improve the models. The existing methods require the loss function to be convex and an optimal model to be obtained, which is not always the case in modern machine learning. To overcome these limitations, we propose a novel approach specifically designed for the models trained with stochastic gradient descent (SGD). The proposed method infers the influential instances by retracing the steps of the SGD while incorporating intermediate models computed in each step. Through experiments, we demonstrate that the proposed method can accurately infer the influential instances. Moreover, we used MNIST and CIFAR10 to show that the models can be effectively improved by removing the influential instances suggested by the proposed method.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9SUKZLPD/Hara et al. - 2019 - Data Cleansing for Models Trained with SGD.pdf}
}

@inproceedings{hardtAmazonSageMakerClarify2021,
  title = {Amazon {{SageMaker Clarify}}: {{Machine Learning Bias Detection}} and {{Explainability}} in the {{Cloud}}},
  shorttitle = {Amazon {{SageMaker Clarify}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Hardt, Michaela and Chen, Xiaoguang and Cheng, Xiaoyi and Donini, Michele and Gelman, Jason and Gollaprolu, Satish and He, John and Larroy, Pedro and Liu, Xinyu and McCarthy, Nick and Rathi, Ashish and Rees, Scott and Siva, Ankit and Tsai, ErhYuan and Vasist, Keerthan and Yilmaz, Pinar and Zafar, Muhammad Bilal and Das, Sanjiv and Haas, Kevin and Hill, Tyler and Kenthapadi, Krishnaram},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {2974--2983},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467177},
  abstract = {Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explainability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.},
  isbn = {978-1-4503-8332-5},
  keywords = {explainability,fairness,machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZTCG5ZD6/Hardt et al. - 2021 - Amazon SageMaker Clarify Machine Learning Bias De.pdf}
}

@inproceedings{hardtEqualityOpportunitySupervised2016,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/93V6T6N9/Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf}
}

@inproceedings{harvard-workshop-relating,
  title = {An {{Empirical Study}} of the {{Trade-Offs Between Interpretability}} and {{Fairness}}},
  booktitle = {{{ICML}} 2020 Workshop on Human Interpretability in Machine Learning, Preliminary Version},
  author = {Jabbari, Shahin and Ou, Han-Ching and Lakkaraju, Himabindu and Tambe, Milind},
  year = {2020},
  abstract = {As machine learning models are increasingly being deployed in critical domains such as criminal justice and healthcare, there has been a growing interest in developing algorithms that are interpretable and fair. While there has been a lot of research on each of these topics in isolation, there has been little work on their intersection. In this paper, we present an empirical study for understanding the relationship between model interpretability and fairness. To this end, we propose a novel evaluation framework and outline appropriate evaluation metrics to determine this relationship across various classes of models in both synthetic and real world datasets.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/EXZBWHH9/Jabbari et al. - An Empirical Study of the Trade-Offs Between Inter.pdf}
}

@article{heLearningImbalancedData2009,
  title = {Learning from {{Imbalanced Data}}},
  author = {He, Haibo and Garcia, Edwardo A.},
  year = {2009},
  month = sep,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {21},
  number = {9},
  pages = {1263--1284},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2008.239},
  abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
  keywords = {active learning,assessment metrics.,Availability,classification,cost-sensitive learning,Data analysis,Data engineering,Data security,Decision making,Finance,Imbalanced learning,IP networks,kernel-based learning,Knowledge representation,Large-scale systems,sampling methods,Surveillance},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WHSXSNW4/5128907.html}
}

@misc{hintonDistillingKnowledgeNeural2015a,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02531},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1503.02531},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Z9SDWDXR/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/H2HJNWYN/1503.html}
}

@inproceedings{holsteinImprovingFairnessMachine2019,
  title = {Improving {{Fairness}} in {{Machine Learning Systems}}: {{What Do Industry Practitioners Need}}?},
  shorttitle = {Improving {{Fairness}} in {{Machine Learning Systems}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Holstein, Kenneth and Wortman Vaughan, Jennifer and Daum{\'e}, Hal and Dudik, Miro and Wallach, Hanna},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--16},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300830},
  abstract = {The potential for machine learning (ML) systems to amplify social inequities and unfairness is receiving increasing popular and academic attention. A surge of recent work has focused on the development of algorithmic tools to assess and mitigate such unfairness. If these tools are to have a positive impact on industry practice, however, it is crucial that their design be informed by an understanding of real-world needs. Through 35 semi-structured interviews and an anonymous survey of 267 ML practitioners, we conduct the first systematic investigation of commercial product teams' challenges and needs for support in developing fairer ML systems. We identify areas of alignment and disconnect between the challenges faced by teams in practice and the solutions proposed in the fair ML research literature. Based on these findings, we highlight directions for future ML and HCI research that will better address practitioners' needs.},
  isbn = {978-1-4503-5970-2},
  keywords = {algorithmic bias,empirical study,fair machine learning,needfinding,product teams,ux of machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4YBLLW2V/Holstein et al. - 2019 - Improving Fairness in Machine Learning Systems Wh.pdf}
}

@article{hookerMovingAlgorithmicBias2021,
  title = {Moving beyond ``Algorithmic Bias Is a Data Problem''},
  author = {Hooker, Sara},
  year = {2021},
  month = apr,
  journal = {Patterns},
  volume = {2},
  number = {4},
  publisher = {{Elsevier}},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2021.100241},
  langid = {english},
  pmid = {33982031},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KH2VLMK8/Hooker - 2021 - Moving beyond algorithmic bias is a data problem.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CIECTY5D/S2666-3899(21)00061-1.html}
}

@article{horaAleatoryEpistemicUncertainty1996,
  title = {Aleatory and Epistemic Uncertainty in Probability Elicitation with an Example from Hazardous Waste Management},
  author = {Hora, Stephen C.},
  year = {1996},
  month = nov,
  journal = {Reliability Engineering \& System Safety},
  series = {Treatment of {{Aleatory}} and {{Epistemic Uncertainty}}},
  volume = {54},
  number = {2},
  pages = {217--223},
  issn = {0951-8320},
  doi = {10.1016/S0951-8320(96)00077-4},
  abstract = {The quantification of a risk assessment model often requires the elicitation of expert judgments about quantities that cannot be precisely measured. The aims of the model being quantified provide important guidance as to the types of questions that should be asked of the experts. The uncertainties underlying a quantity may be classified as aleatory or epistemic according to the goals of the risk process. This paper discusses the nature of such a classification and how it affects the probability elicitation process and implementation of the resulting judgments. Examples from various areas of risk assessment are used to show the practical implications of how uncertainties are treated. An extended example from hazardous waste disposal is given.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/93S53JEK/S0951832096000774.html}
}

@misc{hospedalesMetaLearningNeuralNetworks2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2020},
  month = nov,
  number = {arXiv:2004.05439},
  eprint = {2004.05439},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/N77VJFW7/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf}
}

@inproceedings{huanFairnessEqualityEffort2020,
  title = {Fairness through {{Equality}} of {{Effort}}},
  booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2020},
  author = {Huan, Wen and Wu, Yongkai and Zhang, Lu and Wu, Xintao},
  year = {2020},
  month = apr,
  series = {{{WWW}} '20},
  pages = {743--751},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3366424.3383558},
  abstract = {Fair machine learning is receiving an increasing attention in machine learning fields. Researchers in fair learning have developed correlation or association-based measures such as demographic disparity, mistreatment disparity, calibration, causal-based measures such as total effect, direct and indirect discrimination, and counterfactual fairness, and fairness notions such as equality of opportunity and equalized odds that consider both decisions in the training data and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions like to what extend a legitimate variable should change to make a particular individual achieve a certain outcome level and addresses the concerns whether the efforts made to achieve the same outcome level for individuals from the protected group and that from the unprotected group are different. We develop algorithms for determining whether an individual or a group of individuals is discriminated in terms of equality of effort. We also develop an optimization-based method for removing discriminatory effects from the data if discrimination is detected. We conduct empirical evaluations to compare the equality of effort and existing fairness notion and show the effectiveness of our proposed algorithms.},
  isbn = {978-1-4503-7024-0},
  keywords = {Causality,Equality of Effort,Fairness},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DMCN7JQP/Huan et al. - 2020 - Fairness through Equality of Effort.pdf}
}

@misc{HUDArchivesHUD2002,
  title = {{{HUD Archives}}: {{HUD RELEASES REPORT}}: {{DISCRIMINATION IN METROPOLITAN HOUSING MARKETS}} 1989-2000},
  year = {2002},
  howpublished = {https://archives.hud.gov/news/2002/pr02-138.cfm},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XI5YNKZY/pr02-138.html}
}

@article{iosifidisDealingBiasDataa,
  title = {Dealing with {{Bias}} via {{Data Augmentation}} in {{Supervised Learning Scenarios}}},
  author = {Iosifidis, Vasileios and Ntoutsi, Eirini},
  pages = {6},
  abstract = {There is an increasing amount of work from different communities in data mining, machine learning, information retrieval, semantic web, and databases on bias discovery and discrimination-aware learning with the goal of developing not only good quality models but also models that account for fairness. In this work, we focus on supervised learning where biases towards certain attributes like race or gender might exist. We propose data augmentation techniques to correct for bias at the input/data layer. Our experiments with real world datasets show the potential of augmentation techniques for dealing with bias.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2PBN8BGA/Iosifidis and Ntoutsi - Dealing with Bias via Data Augmentation in Supervi.pdf}
}

@misc{ittnerFeatureSynergyRedundancy2021a,
  title = {Feature {{Synergy}}, {{Redundancy}}, and {{Independence}} in {{Global Model Explanations}} Using {{SHAP Vector Decomposition}}},
  author = {Ittner, Jan and Bolikowski, Lukasz and Hemker, Konstantin and Kennedy, Ricardo},
  year = {2021},
  month = jul,
  number = {arXiv:2107.12436},
  eprint = {2107.12436},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.12436},
  abstract = {We offer a new formalism for global explanations of pairwise feature dependencies and interactions in supervised models. Building upon SHAP values and SHAP interaction values, our approach decomposes feature contributions into synergistic, redundant and independent components (S-R-I decomposition of SHAP vectors). We propose a geometric interpretation of the components and formally prove its basic properties. Finally, we demonstrate the utility of synergy, redundancy and independence by applying them to a constructed data set and model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I97QBPTD/Ittner et al. - 2021 - Feature Synergy, Redundancy, and Independence in G.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QZM24877/2107.html}
}

@inproceedings{jainOverviewImportanceData2020,
  title = {Overview and {{Importance}} of {{Data Quality}} for {{Machine Learning Tasks}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Jain, Abhinav and Patel, Hima and Nagalapatti, Lokesh and Gupta, Nitin and Mehta, Sameep and Guttula, Shanmukha and Mujumdar, Shashank and Afzal, Shazia and Sharma Mittal, Ruhi and Munigala, Vitobha},
  year = {2020},
  month = aug,
  series = {{{KDD}} '20},
  pages = {3561--3562},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3394486.3406477},
  abstract = {It is well understood from literature that the performance of a machine learning (ML) model is upper bounded by the quality of the data. While researchers and practitioners have focused on improving the quality of models (such as neural architecture search and automated feature selection), there are limited efforts towards improving the data quality. One of the crucial requirements before consuming datasets for any application is to understand the dataset at hand and failure to do so can result in inaccurate analytics and unreliable decisions. Assessing the quality of the data across intelligently designed metrics and developing corresponding transformation operations to address the quality gaps helps to reduce the effort of a data scientist for iterative debugging of the ML pipeline to improve model performance. This tutorial highlights the importance of analysing data quality in terms of its value for machine learning applications. This tutorial surveys all the important data quality related approaches discussed in literature, focusing on the intuition behind them, highlighting their strengths and similarities, and illustrates their applicability to real-world problems. Finally we will discuss the interesting work IBM Research is doing in this space.},
  isbn = {978-1-4503-7998-4},
  keywords = {data quality,machine learning,quality metrics}
}

@article{jesus2022turning,
  title={Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation},
  author={Jesus, S{\'e}rgio and Pombal, Jos{\'e} and Alves, Duarte and Cruz, Andr{\'e} and Saleiro, Pedro and Ribeiro, Rita and Gama, Jo{\~a}o and Bizarro, Pedro},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={33563--33575},
  year={2022}
}

@inproceedings{jiaEfficientDataValuation2019,
  title = {Towards {{Efficient Data Valuation Based}} on the {{Shapley Value}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and G{\"u}rel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J.},
  year = {2019},
  month = apr,
  pages = {1167--1176},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {\{\textbackslash em ``How much is my data worth?''\} is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits among multiple data contributors and determining prospective compensation when data breaches happen. In this paper, we study the problem of \textbackslash emph\{data valuation\} by utilizing the Shapley value, a popular notion of value which originated in coopoerative game theory. The Shapley value defines a unique payoff scheme that satisfies many desiderata for the notion of data value. However, the Shapley value often requires \textbackslash emph\{exponential\} time to compute. To meet this challenge, we propose a repertoire of efficient algorithms for approximating the Shapley value. We also demonstrate the value of each training instance for various benchmark datasets.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9RNJ25AK/Jia et al. - 2019 - Towards Efficient Data Valuation Based on the Shap.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GJXZK3B3/Jia et al. - 2019 - Towards Efficient Data Valuation Based on the Shap.pdf}
}

@article{jiaEfficientTaskspecificData2019,
  title = {Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms},
  author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Gurel, Nezihe Merve and Li, Bo and Zhang, Ce and Spanos, Costas and Song, Dawn},
  year = {2019},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {11},
  pages = {1610--1623},
  issn = {2150-8097},
  doi = {10.14778/3342263.3342637},
  abstract = {Given a data set D containing millions of data points and a data consumer who is willing to pay for \$X to train a machine learning (ML) model over D, how should we distribute this \$X to each data point to reflect its ``value''? In this paper, we define the ``relative value of data'' via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all N data points, it requires O(2N ) model evaluations for exact computation and O(N log N ) for ( , {$\delta$})-approximation.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5S2FTAPW/Jia et al. - 2020 - Efficient Task-Specific Data Valuation for Nearest.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/B4Z3FTT2/Jia et al. - 2019 - Efficient task-specific data valuation for nearest.pdf}
}

@inproceedings{jiangIdentifyingCorrectingLabel2020,
  title = {Identifying and {{Correcting Label Bias}} in {{Machine Learning}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Jiang, Heinrich and Nachum, Ofir},
  year = {2020},
  month = jun,
  pages = {702--712},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CAIT6P8Z/Jiang and Nachum - 2020 - Identifying and Correcting Label Bias in Machine L.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/E83SQY77/Jiang and Nachum - 2020 - Identifying and Correcting Label Bias in Machine L.pdf}
}

@inproceedings{jitkrittumAdaptiveTestIndependence2017,
  title = {An {{Adaptive Test}} of {{Independence}} with {{Analytic Kernel Embeddings}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Jitkrittum, Wittawat and Szab{\'o}, Zolt{\'a}n and Gretton, Arthur},
  year = {2017},
  month = jul,
  pages = {1742--1751},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HMZEBL9T/Jitkrittum et al. - 2017 - An Adaptive Test of Independence with Analytic Ker.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SPNC56TY/Jitkrittum et al. - 2017 - An Adaptive Test of Independence with Analytic Ker.pdf}
}

@inproceedings{kallusResidualUnfairnessFair2018a,
  title = {Residual {{Unfairness}} in {{Fair Machine Learning}} from {{Prejudiced Data}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kallus, Nathan and Zhou, Angela},
  year = {2018},
  month = jul,
  pages = {2439--2448},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a "bias in, bias out" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6Q7CE643/Kallus and Zhou - 2018 - Residual Unfairness in Fair Machine Learning from .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7XH9FWGA/Kallus and Zhou - 2018 - Residual Unfairness in Fair Machine Learning from .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GILH6HBU/Kallus and Zhou - 2018 - Residual Unfairness in Fair Machine Learning from .pdf}
}

@inproceedings{kamiranClassifyingDiscriminating2009,
  title = {Classifying without Discriminating},
  booktitle = {Control and {{Communication}} 2009 2nd {{International Conference}} on {{Computer}}},
  author = {Kamiran, Faisal and Calders, Toon},
  year = {2009},
  month = feb,
  pages = {1--6},
  doi = {10.1109/IC4.2009.4909197},
  abstract = {Classification models usually make predictions on the basis of training data. If the training data is biased towards certain groups or classes of objects, e.g., there is racial discrimination towards black people, the learned model will also show discriminatory behavior towards that particular community. This partial attitude of the learned model may lead to biased outcomes when labeling future unlabeled data objects. Often, however, impartial classification results are desired or even required by law for future data objects in spite of having biased training data. In this paper, we tackle this problem by introducing a new classification scheme for learning unbiased models on biased training data. Our method is based on massaging the dataset by making the least intrusive modifications which lead to an unbiased dataset. On this modified dataset we then learn a non-discriminating classifier. The proposed method has been implemented and experimental results on a credit approval dataset show promising results: in all experiments our method is able to reduce the prejudicial behavior for future classification significantly without loosing too much predictive accuracy.},
  keywords = {Accuracy,Computer science,Employment,Labeling,Mathematical model,Mathematics,Predictive models,Recruitment,Remuneration,Training data},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GNEAXTSU/4909197.html}
}

@article{kamiranDataPreprocessingTechniques2012,
  title = {Data Preprocessing Techniques for Classification without Discrimination},
  author = {Kamiran, Faisal and Calders, Toon},
  year = {2012},
  month = oct,
  journal = {Knowledge and Information Systems},
  volume = {33},
  number = {1},
  pages = {1--33},
  issn = {0219-3116},
  doi = {10.1007/s10115-011-0463-8},
  abstract = {Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.},
  langid = {english},
  keywords = {Classification,Discrimination-aware data mining,Preprocessing},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LKGZGUXQ/Kamiran and Calders - 2012 - Data preprocessing techniques for classification w.pdf}
}

@inproceedings{kamiranDiscriminationAwareDecision2010,
  title = {Discrimination {{Aware Decision Tree Learning}}},
  booktitle = {2010 {{IEEE International Conference}} on {{Data Mining}}},
  author = {Kamiran, Faisal and Calders, Toon and Pechenizkiy, Mykola},
  year = {2010},
  month = dec,
  pages = {869--874},
  issn = {2374-8486},
  doi = {10.1109/ICDM.2010.50},
  abstract = {Recently, the following discrimination aware classification problem was introduced: given a labeled dataset and an attribute B, find a classifier with high predictive accuracy that at the same time does not discriminate on the basis of the given attribute B. This problem is motivated by the fact that often available historic data is biased due to discrimination, e.g., when B denotes ethnicity. Using the standard learners on this data may lead to wrongfully biased classifiers, even if the attribute B is removed from training data. Existing solutions for this problem consist in ``cleaning away'' the discrimination from the dataset before a classifier is learned. In this paper we study an alternative approach in which the non-discrimination constraint is pushed deeply into a decision tree learner by changing its splitting criterion and pruning strategy. Experimental evaluation shows that the proposed approach advances the state-of-the-art in the sense that the learned decision trees have a lower discrimination than models provided by previous methods, with little loss in accuracy.},
  keywords = {Accuracy,Biological system modeling,Classification,Cleaning,Data mining,Data Mining,Decision trees,Discrimination Aware Data Mining,Economics,Training data},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/TT4RN4XW/Kamiran et al. - 2010 - Discrimination Aware Decision Tree Learning.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ETRKMR2Y/5694053.html}
}

@inproceedings{kamishimaFairnessAwareClassifierPrejudice2012,
  title = {Fairness-{{Aware Classifier}} with {{Prejudice Remover Regularizer}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kamishima, Toshihiro and Akaho, Shotaro and Asoh, Hideki and Sakuma, Jun},
  editor = {Flach, Peter A. and De Bie, Tijl and Cristianini, Nello},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {35--50},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33486-3_3},
  abstract = {With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.},
  isbn = {978-3-642-33486-3},
  langid = {english},
  keywords = {classification,discrimination,fairness,information theory,logistic regression,social responsibility},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LD2NAKZM/Kamishima et al. - 2012 - Fairness-Aware Classifier with Prejudice Remover R.pdf}
}

@inproceedings{kangNovelCreditScoring2019b,
  title = {A Novel Credit Scoring Framework for Auto Loan Using an Imbalanced-Learning-Based Reject Inference},
  booktitle = {2019 {{IEEE Conference}} on {{Computational Intelligence}} for {{Financial Engineering}} \& {{Economics}} ({{CIFEr}})},
  author = {Kang, Yanzhe and Cui, Runbang and Deng, Jiang and Jia, Ning},
  year = {2019},
  month = may,
  pages = {1--8},
  issn = {2640-7701},
  doi = {10.1109/CIFEr.2019.8759110},
  abstract = {Along with the booming consumer credit market, credit scoring has received an increasing concern in auto financial companies. However, the modeling without rejected applicants and the imbalanced distribution of accepted examples affect the predictive performance. In this paper, we propose a novel framework for credit scoring using an imbalanced-learning-based reject inference. First, we employ an imbalanced learning for the accepted applicant data using Synthetic Minority Over-sampling Technique for reject inference. Second, we conduct reject inference for rejected applicants based on a graph-based semi-supervised learning algorithm, which is called label propagation. Third, we use tree-based ensemble learning models as base classifiers to train the combined training data. Finally, we give an exact experiment for assessment using data from a Chinese auto loan company. The results indicate that the proposed novel framework performs better than comparative models, which represents a progressive method for auto loan.},
  keywords = {Biological system modeling,Classification algorithms,Companies,credit scoring,Data models,financial technology,graph-based semi-supervised learning,imbalanced learning,Inference algorithms,label propagation,reject inference,Semisupervised learning,Training},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NXFUKXC4/8759110.html}
}

@misc{karlasDataDebuggingShapley2022,
  title = {Data {{Debugging}} with {{Shapley Importance}} over {{End-to-End Machine Learning Pipelines}}},
  author = {Karla{\v s}, Bojan and Dao, David and Interlandi, Matteo and Li, Bo and Schelter, Sebastian and Wu, Wentao and Zhang, Ce},
  year = {2022},
  month = apr,
  number = {arXiv:2204.11131},
  eprint = {2204.11131},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.11131},
  abstract = {Developing modern machine learning (ML) applications is data-centric, of which one fundamental challenge is to understand the influence of data quality to ML training -- "Which training examples are 'guilty' in making the trained ML model predictions inaccurate or unfair?" Modeling data influence for ML training has attracted intensive interest over the last decade, and one popular framework is to compute the Shapley value of each training example with respect to utilities such as validation accuracy and fairness of the trained ML model. Unfortunately, despite recent intensive interest and research, existing methods only consider a single ML model "in isolation" and do not consider an end-to-end ML pipeline that consists of data transformations, feature extractors, and ML training. We present DataScope (ease.ml/datascope), the first system that efficiently computes Shapley values of training examples over an end-to-end ML pipeline, and illustrate its applications in data debugging for ML training. To this end, we first develop a novel algorithmic framework that computes Shapley value over a specific family of ML pipelines that we call canonical pipelines: a positive relational algebra query followed by a K-nearest-neighbor (KNN) classifier. We show that, for many subfamilies of canonical pipelines, computing Shapley value is in PTIME, contrasting the exponential complexity of computing Shapley value in general. We then put this to practice -- given an sklearn pipeline, we approximate it with a canonical pipeline to use as a proxy. We conduct extensive experiments illustrating different use cases and utilities. Our results show that DataScope is up to four orders of magnitude faster over state-of-the-art Monte Carlo-based methods, while being comparably, and often even more, effective in data debugging.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/X225AN6J/Karla et al. - 2022 - Data Debugging with Shapley Importance over End-to.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HF38YCDC/2204.html}
}

@inproceedings{kearnsEmpiricalStudyRich2019a,
  title = {An {{Empirical Study}} of {{Rich Subgroup Fairness}} for {{Machine Learning}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {100--109},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287592},
  abstract = {Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.},
  isbn = {978-1-4503-6125-5},
  keywords = {Algorithmic Bias,Fair Classification,Fairness Auditing,Subgroup Fairness},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VPM8RPN2/Kearns et al. - 2019 - An Empirical Study of Rich Subgroup Fairness for M.pdf}
}

@inproceedings{kearnsPreventingFairnessGerrymandering2018,
  title = {Preventing {{Fairness Gerrymandering}}: {{Auditing}} and {{Learning}} for {{Subgroup Fairness}}},
  shorttitle = {Preventing {{Fairness Gerrymandering}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kearns, Michael and Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  year = {2018},
  month = jul,
  pages = {2564--2572},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The most prevalent notions of fairness in machine learning fix a small collection of pre-defined groups (such as race or gender), and then ask for approximate parity of some statistic of the classifier (such as false positive rate) across these groups. Constraints of this form are susceptible to fairness gerrymandering, in which a classifier is fair on each individual group, but badly violates the fairness constraint on structured subgroups, such as certain combinations of protected attribute values. We thus consider fairness across exponentially or infinitely many subgroups, defined by a structured class of functions over the protected attributes. We first prove that the problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is computationally equivalent to the problem of weak agnostic learning \textemdash{} which means it is hard in the worst case, even for simple structured subclasses. However, it also suggests that common heuristics for learning can be applied to successfully solve the auditing problem in practice. We then derive an algorithm that provably converges in a polynomial number of steps to the best subgroup-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning problem. The algorithm is based on a formulation of subgroup fairness as a zero-sum game between a Learner (the primal player) and an Auditor (the dual player). We implement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DZ4JWAYZ/Kearns et al. - 2018 - Preventing Fairness Gerrymandering Auditing and L.pdf}
}

@inproceedings{keLightGBMHighlyEfficient2017,
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  shorttitle = {{{LightGBM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \textbackslash emph\{Gradient-based One-Side Sampling\} (GOSS) and \textbackslash emph\{Exclusive Feature Bundling\} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \textbackslash emph\{LightGBM\}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PYP4QKAJ/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Dec.pdf}
}

@misc{khodadadianInformationTheoreticMeasures2021,
  title = {Information {{Theoretic Measures}} for {{Fairness-aware Feature Selection}}},
  author = {Khodadadian, Sajad and Nafea, Mohamed and Ghassami, AmirEmad and Kiyavash, Negar},
  year = {2021},
  month = jun,
  number = {arXiv:2106.00772},
  eprint = {2106.00772},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.00772},
  abstract = {Machine learning algorithms are increasingly used for consequential decision making regarding individuals based on their relevant features. Features that are relevant for accurate decisions may however lead to either explicit or implicit forms of discrimination against unprivileged groups, such as those of certain race or gender. This happens due to existing biases in the training data, which are often replicated or even exacerbated by the learning algorithm. Identifying and measuring these biases at the data level is a challenging problem due to the interdependence among the features, and the decision outcome. In this work, we develop a framework for fairness-aware feature selection which takes into account the correlation among the features and the decision outcome, and is based on information theoretic measures for the accuracy and discriminatory impacts of features. In particular, we first propose information theoretic measures which quantify the impact of different subsets of features on the accuracy and discrimination of the decision outcomes. We then deduce the marginal impact of each feature using Shapley value function; a solution concept in cooperative game theory used to estimate marginal contributions of players in a coalitional game. Finally, we design a fairness utility score for each feature (for feature selection) which quantifies how this feature influences accurate as well as nondiscriminatory decisions. Our framework depends on the joint statistics of the data rather than a particular classifier design. We examine our proposed framework on real and synthetic data to evaluate its performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/MDU23CVD/Khodadadian et al. - 2021 - Information Theoretic Measures for Fairness-aware .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BHRELW6T/2106.html}
}

@inproceedings{kilbertusSensitivityCounterfactualFairness2020,
  title = {The {{Sensitivity}} of {{Counterfactual Fairness}} to {{Unmeasured Confounding}}},
  booktitle = {Proceedings of {{The}} 35th {{Uncertainty}} in {{Artificial Intelligence Conference}}},
  author = {Kilbertus, Niki and Ball, Philip J. and Kusner, Matt J. and Weller, Adrian and Silva, Ricardo},
  year = {2020},
  month = aug,
  pages = {616--626},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Causal approaches to fairness have seen substantial recent interest, both from the machine learning community and from wider parties interested in ethical prediction algorithms. In no small part, this has been due to the fact that causal models allow one to simultaneously leverage data and expert knowledge to remove discriminatory effects from predictions. However, one of the primary assumptions in causal modeling is that you know the causal graph. This introduces a new opportunity for bias, caused by misspecifying the causal model. One common way for misspecification to occur is via unmeasured confounding: the true causal effect between variables is partially described by unobserved quantities. In this work we design tools to assess the sensitivity of fairness measures to this confounding for the popular class of non-linear additive noise models (ANMs). Specifically, we give a procedure for computing the maximum difference between two counterfactually fair predictors, where one has become biased due to confounding. For the case of bivariate confounding our technique can be swiftly computed via a sequence of closed-form updates. For multivariate confounding we give an algorithm that can be efficiently solved via automatic differentiation. We demonstrate our new sensitivity analysis tools in real-world fairness scenarios to assess the bias arising from confounding.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HPTX5REQ/Kilbertus et al. - 2020 - The Sensitivity of Counterfactual Fairness to Unme.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JMXSCGBN/Kilbertus et al. - 2020 - The Sensitivity of Counterfactual Fairness to Unme.pdf}
}

@inproceedings{kimDisentanglingFactorising2018,
  title = {Disentangling by {{Factorising}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = {2018},
  month = jul,
  pages = {2649--2658},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5LQSA86E/Kim and Mnih - 2018 - Disentangling by Factorising.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CQ25LKE3/Kim and Mnih - 2018 - Disentangling by Factorising.pdf}
}

@inproceedings{kimMultiaccuracyBlackBoxPostProcessing2019,
  title = {Multiaccuracy: {{Black-Box Post-Processing}} for {{Fairness}} in {{Classification}}},
  shorttitle = {Multiaccuracy},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
  year = {2019},
  month = jan,
  series = {{{AIES}} '19},
  pages = {247--254},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3306618.3314287},
  abstract = {Prediction systems are successfully deployed in applications ranging from disease diagnosis, to predicting credit worthiness, to image recognition. Even when the overall accuracy is high, these systems may exhibit systematic biases that harm specific subpopulations; such biases may arise inadvertently due to underrepresentation in the data used to train a machine-learning model, or as the result of intentional malicious discrimination. We develop a rigorous framework of *multiaccuracy* auditing and post-processing to ensure accurate predictions across *identifiable subgroups*. Our algorithm, MULTIACCURACY-BOOST, works in any setting where we have black-box access to a predictor and a relatively small set of labeled data for auditing; importantly, this black-box framework allows for improved fairness and accountability of predictions, even when the predictor is minimally transparent. We prove that MULTIACCURACY-BOOST converges efficiently and show that if the initial model is accurate on an identifiable subgroup, then the post-processed model will be also. We experimentally demonstrate the effectiveness of the approach to improve the accuracy among minority subgroups in diverse applications (image classification, finance, population health). Interestingly, MULTIACCURACY-BOOST can improve subpopulation accuracy (e.g. for "black women") even when the sensitive features (e.g. "race", "gender") are not given to the algorithm explicitly.},
  isbn = {978-1-4503-6324-2},
  keywords = {Computer Science - Machine Learning,discrimination,fairness,machine learning,post-processing,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6CM2ESPD/Kim et al. - 2018 - Multiaccuracy Black-Box Post-Processing for Fairn.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SPI7N8LD/Kim et al. - 2019 - Multiaccuracy Black-Box Post-Processing for Fairn.pdf}
}

@inproceedings{kimSOSScorebasedOversampling2022,
  title = {{{SOS}}: {{Score-based Oversampling}} for {{Tabular Data}}},
  shorttitle = {{{SOS}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Kim, Jayoung and Lee, Chaejeong and Shin, Yehjin and Park, Sewon and Kim, Minjung and Park, Noseong and Cho, Jihoon},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {762--772},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539454},
  abstract = {Score-based generative models (SGMs) are a recent breakthrough in generating fake images. SGMs are known to surpass other generative models, e.g., generative adversarial networks (GANs) and variational autoencoders (VAEs). Being inspired by their big success, in this work, we fully customize them for generating fake tabular data. In particular, we are interested in oversampling minor classes since imbalanced classes frequently lead to sub-optimal training outcomes. To our knowledge, we are the first presenting a score-based tabular data oversampling method. Firstly, we re-design our own score network since we have to process tabular data. Secondly, we propose two options for our generation method: the former is equivalent to a style transfer for tabular data and the latter uses the standard generative policy of SGMs. Lastly, we define a fine-tuning method, which further enhances the oversampling quality. In our experiments with 6 datasets and 10 baselines, our method outperforms other oversampling methods in all cases.},
  isbn = {978-1-4503-9385-0},
  keywords = {oversampling,score-based generative model,tabular data synthesis},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KSV7PJ43/Kim et al. - 2022 - SOS Score-based Oversampling for Tabular Data.pdf}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {International {{Conference}} on {{Learning Represantations}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2015},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/P6XKPVST/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LDPDGH37/1412.html}
}

@article{kleinbergAlgorithmsDiscriminationDetectors2020,
  title = {Algorithms as Discrimination Detectors},
  author = {Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil and Sunstein, Cass R.},
  year = {2020},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30096--30100},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1912790117},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4AYKVVP5/Kleinberg et al. - 2020 - Algorithms as discrimination detectors.pdf}
}

@inproceedings{kleinbergInherentTradeOffsFair2017,
  title = {Inherent {{Trade-Offs}} in the {{Fair Determination}} of {{Risk Scores}}},
  booktitle = {8th {{Innovations}} in {{Theoretical Computer Science Conference}} ({{ITCS}} 2017)},
  author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  editor = {Papadimitriou, Christos H.},
  year = {2017},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {67},
  pages = {43:1--43:23},
  publisher = {{Schloss Dagstuhl\textendash Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.ITCS.2017.43},
  isbn = {978-3-95977-029-3},
  keywords = {algorithmic fairness,calibration,risk tools},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IL8MFKVZ/Kleinberg et al. - 2017 - Inherent Trade-Offs in the Fair Determination of R.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IJLXV8HV/8156.html}
}

@book{knokeLogLinearModels1980,
  title = {Log-{{Linear Models}}},
  author = {Knoke, David and Burke, Peter J. and Burke, Peter John},
  year = {1980},
  month = aug,
  publisher = {{SAGE}},
  abstract = {Discusses the innovative log-linear model of statistical analysis. This model makes no distinction between independent and dependent variables, but is used to examine relationships among categoric variables by analyzing expected cell frequencies.},
  googlebooks = {6ugyf5oBxCoC},
  isbn = {978-0-8039-1492-6},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Social Science / Research}
}

@article{kocijanGapGapTackling2021,
  title = {The {{Gap}} on {{Gap}}: {{Tackling}} the {{Problem}} of {{Differing Data Distributions}} in {{Bias-Measuring Datasets}}},
  shorttitle = {The {{Gap}} on {{Gap}}},
  author = {Kocijan, Vid and Camburu, Oana-Maria and Lukasiewicz, Thomas},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {14},
  pages = {13180--13188},
  issn = {2374-3468},
  abstract = {Diagnostic datasets that can detect biased models are an important prerequisite for bias reduction within natural language processing. However, undesired patterns in the collected data can make such tests incorrect. For example, if the feminine subset of a gender-bias-measuring coreference resolution dataset contains sentences with a longer average distance between the pronoun and the correct candidate, an RNN-based model may perform worse on this subset due to long-term dependencies. In this work, we introduce a theoretically grounded method for weighting test samples to cope with such patterns in the test data. We demonstrate the method on the GAP dataset for coreference resolution. We annotate GAP with spans of all personal names and show that examples in the female subset contain more personal names and a longer distance between pronouns and their referents, potentially affecting the bias score in an undesired way. Using our weighting method, we find the set of weights on the test instances that should be used for coping with these correlations,   and we re-evaluate 16 recently released coreference models.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Ethics -- Bias,Fairness,Transparency \& Privac},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/J7BIQIRV/Kocijan et al. - 2021 - The Gap on Gap Tackling the Problem of Differing .pdf}
}

@inproceedings{kohavi1996scaling,
  title = {Scaling up the Accuracy of Naive-Bayes Classifiers: {{A}} Decision-Tree Hybrid},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  author = {Kohavi, Ron},
  year = {1996},
  series = {{{KDD}}'96},
  pages = {202--207},
  publisher = {{AAAI Press}},
  address = {{Portland, Oregon}}
}

@inproceedings{kohUnderstandingBlackboxPredictions2017,
  title = {Understanding {{Black-box Predictions}} via {{Influence Functions}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Koh, Pang Wei and Liang, Percy},
  year = {2017},
  month = jul,
  pages = {1885--1894},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions \textemdash{} a classic technique from robust statistics \textemdash{} to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/428DP55J/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/52J6Q8ZN/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf}
}

@inproceedings{koutsovitikoumeriBiasQuantificationProtected2021,
  title = {Bias {{Quantification}} for~{{Protected Features}} in {{Pattern Classification Problems}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Koutsoviti Koumeri, Lisa and N{\'a}poles, Gonzalo},
  editor = {Tavares, Jo{\~a}o Manuel R. S. and Papa, Jo{\~a}o Paulo and Gonz{\'a}lez Hidalgo, Manuel},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {351--360},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-93420-0_33},
  abstract = {The need to measure and mitigate bias in machine learning data sets has gained wide recognition in the field of Artificial Intelligence (AI) during the past decade. The academic and business communities call for new general-purpose measures to quantify bias. In this paper, we propose a new measure that relies on the fuzzy-rough set theory. The intuition of our measure is that protected features should not change the fuzzy-rough set boundary regions significantly. The extent to which this happens can be understood as a proxy for bias quantification. Our measure can be categorized as an individual fairness measure since the fuzzy-rough regions are computed using instance-based information pieces. The main advantage of our measure is that it does not depend on any prediction model but on a distance function. At the same time, our measure offers an intuitive rationale for the bias concept. The results using a proof-of-concept show that our measure can capture the bias issues better than other state-of-the-art measures.},
  isbn = {978-3-030-93420-0},
  langid = {english},
  keywords = {Bias,Fairness-aware AI,Fuzzy-rough sets}
}

@article{kozodoiFairnessCreditScoring2022,
  title = {Fairness in Credit Scoring: {{Assessment}}, Implementation and Profit Implications},
  shorttitle = {Fairness in Credit Scoring},
  author = {Kozodoi, Nikita and Jacob, Johannes and Lessmann, Stefan},
  year = {2022},
  month = mar,
  journal = {European Journal of Operational Research},
  volume = {297},
  number = {3},
  pages = {1083--1094},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2021.06.023},
  abstract = {The rise of algorithmic decision-making has spawned much research on fair machine learning (ML). Financial institutions use ML for building risk scorecards that support a range of credit-related decisions. Yet, the literature on fair ML in credit scoring is scarce. The paper makes three contributions. First, we revisit statistical fairness criteria and examine their adequacy for credit scoring. Second, we catalog algorithmic options for incorporating fairness goals in the ML model development pipeline. Last, we empirically compare different fairness processors in a profit-oriented credit scoring context using real-world data. The empirical results substantiate the evaluation of fairness measures, identify suitable options to implement fair credit scoring, and clarify the profit-fairness trade-off in lending decisions. We find that multiple fairness criteria can be approximately satisfied at once and recommend separation as a proper criterion for measuring the fairness of a scorecard. We also find fair in-processors to deliver a good balance between profit and fairness and show that algorithmic discrimination can be reduced to a reasonable level at a relatively low cost. The codes corresponding to the paper are available on GitHub.},
  langid = {english},
  keywords = {Algorithmic fairness,Credit scoring,Machine learning,OR in banking},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/TIFPFKMQ/Kozodoi et al. - 2022 - Fairness in credit scoring Assessment, implementa.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DXDNLAL9/S0377221721005385.html}
}

@inproceedings{kozodoiShallowSelflearningReject2020,
  title = {Shallow {{Self-learning}} for {{Reject Inference}} in {{Credit Scoring}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kozodoi, Nikita and Katsas, Panagiotis and Lessmann, Stefan and {Moreira-Matias}, Luis and Papakonstantinou, Konstantinos},
  editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {516--532},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-46133-1_31},
  abstract = {Credit scoring models support loan approval decisions in the financial services industry. Lenders train these models on data from previously granted credit applications, where the borrowers' repayment behavior has been observed. This approach creates sample bias. The scoring model is trained on accepted cases only. Applying the model to screen applications from the population of all borrowers degrades its performance. Reject inference comprises techniques to overcome sampling bias through assigning labels to rejected cases. This paper makes two contributions. First, we propose a self-learning framework for reject inference. The framework is geared toward real-world credit scoring requirements through considering distinct training regimes for labeling and model training. Second, we introduce a new measure to assess the effectiveness of reject inference strategies. Our measure leverages domain knowledge to avoid artificial labeling of rejected cases during evaluation. We demonstrate this approach to offer a robust and operational assessment of reject inference. Experiments on a real-world credit scoring data set confirm the superiority of the suggested self-learning framework over previous reject inference strategies. We also find strong evidence in favor of the proposed evaluation measure assessing reject inference strategies more reliably, raising the performance of the eventual scoring model.},
  isbn = {978-3-030-46133-1},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Credit scoring,Evaluation,Quantitative Finance - Risk Management,Reject inference,Self-learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/38D7UHDB/Kozodoi et al. - 2019 - Shallow Self-Learning for Reject Inference in Cred.pdf}
}

@misc{kRevisitingMethodsFinding2021,
  title = {Revisiting {{Methods}} for {{Finding Influential Examples}}},
  author = {K, Karthikeyan and S{\o}gaard, Anders},
  year = {2021},
  month = nov,
  number = {arXiv:2111.04683},
  eprint = {2111.04683},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.04683},
  abstract = {Several instance-based explainability methods for finding influential training examples for test-time decisions have been proposed recently, including Influence Functions, TraceIn, Representer Point Selection, Grad-Dot, and Grad-Cos. Typically these methods are evaluated using LOO influence (Cook's distance) as a gold standard, or using various heuristics. In this paper, we show that all of the above methods are unstable, i.e., extremely sensitive to initialization, ordering of the training data, and batch size. We suggest that this is a natural consequence of how in the literature, the influence of examples is assumed to be independent of model state and other examples -- and argue it is not. We show that LOO influence and heuristics are, as a result, poor metrics to measure the quality of instance-based explanations, and instead propose to evaluate such explanations by their ability to detect poisoning attacks. Further, we provide a simple, yet effective baseline to improve all of the above methods and show how it leads to very significant improvements on downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/TPWQAQ8F/K and Sgaard - 2021 - Revisiting Methods for Finding Influential Example.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Y8AAPSSC/2111.html}
}

@article{kruskalUseRanksOneCriterion1952,
  title = {Use of {{Ranks}} in {{One-Criterion Variance Analysis}}},
  author = {Kruskal, William H. and Wallis, W. Allen},
  year = {1952},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {47},
  number = {260},
  pages = {583--621},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1952.10483441},
  abstract = {Given C samples, with n i observations in the ith sample, a test of the hypothesis that the samples are from the same population may be made by ranking the observations from from 1 to {$\Sigma$}n i (giving each observation in a group of ties the mean of the ranks tied for), finding the C sums of ranks, and computing a statistic H. Under the stated hypothesis, H is distributed approximately as {$\chi$}2(C \textendash{} 1), unless the samples are too small, in which case special approximations or exact tables are provided. One of the most important applications of the test is in detecting differences among the population means.* * Based in part on research supported by the Office of Naval Research at the Statistical Research Center, University of Chicago.},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483441}
}

@article{kugelgenFairnessCausalAlgorithmic2022,
  title = {On the {{Fairness}} of {{Causal Algorithmic Recourse}}},
  author = {von K{\"u}gelgen, Julius and Karimi, Amir-Hossein and Bhatt, Umang and Valera, Isabel and Weller, Adrian and Sch{\"o}lkopf, Bernhard},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {9},
  pages = {9584--9594},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i9.21192},
  abstract = {Algorithmic fairness is typically studied from the perspective of predictions. Instead, here we investigate fairness from the perspective of recourse actions suggested to individuals to remedy an unfavourable classification. We propose two new fair-ness criteria at the group and individual level, which\textemdash unlike prior work on equalising the average group-wise distance from the decision boundary\textemdash explicitly account for causal relationships between features, thereby capturing downstream effects of recourse actions performed in the physical world. We explore how our criteria relate to others, such as counterfactual fairness, and show that fairness of recourse is complementary to fairness of prediction. We study theoretically and empirically how to enforce fair causal recourse by altering the classifier and perform a case study on the Adult dataset. Finally, we discuss whether fairness violations in the data generating process revealed by our criteria may be better addressed by societal interventions as opposed to constraints on the classifier.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Humans And AI (HAI)},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/65PG9JZJ/Kgelgen et al. - 2022 - On the Fairness of Causal Algorithmic Recourse.pdf}
}

@inproceedings{kusnerCounterfactualFairness2017,
  title = {Counterfactual {{Fairness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing.  In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation.  Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it  the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5L3MQZSE/Kusner et al. - 2017 - Counterfactual Fairness.pdf}
}

@inproceedings{kwonBetaShapleyUnified2022,
  title = {Beta {{Shapley}}: A {{Unified}} and {{Noise-reduced Data Valuation Framework}} for {{Machine Learning}}},
  shorttitle = {Beta {{Shapley}}},
  booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kwon, Yongchan and Zou, James},
  year = {2022},
  month = may,
  pages = {8780--8802},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Data Shapley has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. It can effectively identify helpful or harmful data points for a learning algorithm. In this paper, we propose Beta Shapley, which is a substantial generalization of Data Shapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the Shapley value, which is not critical for machine learning settings. Beta Shapley unifies several popular data valuation methods and includes data Shapley as a special case. Moreover, we prove that Beta Shapley has several desirable statistical properties and propose efficient algorithms to estimate it. We demonstrate that Beta Shapley outperforms state-of-the-art data valuation methods on several downstream ML tasks such as: 1) detecting mislabeled training data; 2) learning with subsamples; and 3) identifying points whose addition or removal have the largest positive or negative impact on the model.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3CTJVMEJ/Kwon and Zou - 2022 - Beta Shapley a Unified and Noise-reduced Data Val.pdf}
}

@inproceedings{label-shift,
  title = {Label {{Bias}}, {{Label Shift}}: {{Fair Machine Learning}} with {{Unreliable Labels}}},
  booktitle = {{{NeurIPS}} 2020 {{Workshop}} on {{Consequential Decision Making}} in {{Dynamic Environments}}},
  author = {Dai, Jessica and Brown, Sarah M},
  year = {2020},
  series = {Proceedings of Machine Learning Research},
  publisher = {{PMLR}}
}

@inproceedings{lakkarajuSelectiveLabelsProblem2017a,
  title = {The Selective Labels Problem: {{Evaluating}} Algorithmic Predictions in the Presence of Unobservables},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Lakkaraju, Himabindu and Kleinberg, Jon and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
  year = {2017},
  series = {{{KDD}} '17},
  pages = {275--284},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098066},
  abstract = {Evaluating whether machines improve on human performance is one of the central questions of machine learning. However, there are many domains where the data is selectively labeled, in the sense that the observed outcomes are themselves a consequence of the existing choices of the human decision-makers. For instance, in the context of judicial bail decisions, we observe the outcome of whether a defendant fails to return for their court appearance only if the human judge decides to release the defendant on bail. This selective labeling makes it harder to evaluate predictive models as the instances for which outcomes are observed do not represent a random sample of the population. Here we propose a novel framework for evaluating the performance of predictive models on selectively labeled data. We develop an approach called contraction which allows us to compare the performance of predictive models and human decision-makers without resorting to counterfactual inference. Our methodology harnesses the heterogeneity of human decision-makers and facilitates effective evaluation of predictive models even in the presence of unmeasured confounders (unobservables) which influence both human decisions and the resulting outcomes. Experimental results on real world datasets spanning diverse domains such as health care, insurance, and criminal justice demonstrate the utility of our evaluation metric in comparing human decisions and machine predictions.},
  isbn = {978-1-4503-4887-4},
  keywords = {evaluating machine learning algorithms,selective labels,unmeasured confounders,unobservables},
  annotation = {90 citations (Semantic Scholar/DOI) [2022-05-26]},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/U2KVWS86/Lakkaraju et al. - 2017 - The Selective Labels Problem Evaluating Algorithm.pdf}
}

@article{lambaEmpiricalComparisonBias2021a,
  title = {An {{Empirical Comparison}} of {{Bias Reduction Methods}} on {{Real-World Problems}} in {{High-Stakes Policy Settings}}},
  author = {Lamba, Hemank and Rodolfa, Kit T. and Ghani, Rayid},
  year = {2021},
  month = may,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {23},
  number = {1},
  pages = {69--85},
  issn = {1931-0145},
  doi = {10.1145/3468507.3468518},
  abstract = {Applications of machine learning (ML) to high-stakes policy settings - such as education, criminal justice, healthcare, and social service delivery - have grown rapidly in recent years, sparking important conversations about how to ensure fair outcomes from these systems. The machine learning research community has responded to this challenge with a wide array of proposed fairness-enhancing strategies for ML models, but despite the large number of methods that have been developed, little empirical work exists evaluating these methods in real-world settings. Here, we seek to fill this research gap by investigating the performance of several methods that operate at different points in the ML pipeline across four real-world public policy and social good problems. Across these problems, we find a wide degree of variability and inconsistency in the ability of many of these methods to improve model fairness, but postprocessing by choosing group-specific score thresholds consistently removes disparities, with important implications for both the ML research community and practitioners deploying machine learning to inform consequential policy decisions.},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/48XCSS5U/Lamba et al. - 2021 - An Empirical Comparison of Bias Reduction Methods .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8MKFKN2A/2105.06442.pdf}
}

@inproceedings{lamyNoisetolerantFairClassification2019,
  title = {Noise-Tolerant Fair Classification},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lamy, Alex and Zhong, Ziyuan and Menon, Aditya K and Verma, Nakul},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Fairness-aware learning involves designing algorithms that do not discriminate with respect to some sensitive feature (e.g., race or gender). Existing work on the problem operates under the assumption that the sensitive feature available in one's training sample is perfectly reliable. This assumption may be violated in many real-world cases: for example, respondents to a survey may choose to conceal or obfuscate their group identity out of fear of potential discrimination. This poses the question of whether one can still learn fair classifiers given noisy sensitive features. In this paper, we answer the question in the affirmative: we show that if one measures fairness using the mean-difference score, and sensitive features are subject to noise from the mutually contaminated learning model, then owing to a simple identity we only need to change the desired fairness-tolerance. The requisite tolerance can be estimated by leveraging existing noise-rate estimators from the label noise literature. We finally show that our procedure is empirically effective on two case-studies involving sensitive feature censoring.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Z9X8FDYD/Lamy et al. - 2019 - Noise-tolerant fair classification.pdf}
}

@article{lauferEndtoendAuditingDecision,
  title = {End-to-End {{Auditing}} of {{Decision Pipelines}}},
  author = {Laufer, Benjamin and Pierson, Emma and Garg, Nikhil},
  pages = {7},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GZV3WVQA/Laufer et al. - End-to-end Auditing of Decision Pipelines.pdf}
}

@article{lequySurveyDatasetsFairnessaware2022,
  title = {A Survey on Datasets for Fairness-Aware Machine Learning},
  author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
  year = {2022},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {12},
  number = {3},
  pages = {e1452},
  issn = {1942-4795},
  doi = {10.1002/widm.1452},
  abstract = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues {$>$} Fairness in Data Mining Fundamental Concepts of Data and Knowledge {$>$} Data Concepts Technologies {$>$} Data Preprocessing},
  langid = {english},
  keywords = {benchmark datasets,bias,datasets for fairness,discrimination,fairness-aware machine learning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1452},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FKLZSR2Z/Le Quy et al. - 2022 - A survey on datasets for fairness-aware machine le.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/L6X9YGR2/widm.html}
}

@inproceedings{lewis1994sequential,
  title = {A Sequential Algorithmfor Training Text Classifiers},
  booktitle = {Proc. {{ofSIGIR-94}}, 17th {{ACM}} International Conference on Research and Development in Information Retrieval},
  author = {Lewis, DD and Gale, WA},
  year = {1994},
  pages = {3--12}
}

@article{lewisSequentialAlgorithmTraining,
  title = {A {{Sequential Algorithm}} for {{Training Text Classifiers}}: {{Corrigendum}} and {{Additional Data}}},
  author = {Lewis, David D and Laboratories, T Bell and Hill, Murray},
  pages = {7},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RT72GKBX/Lewis et al. - A Sequential Algorithm for Training Text Classifie.pdf}
}

@inproceedings{liAchievingFairnessNo2022,
  title = {Achieving {{Fairness}} at {{No Utility Cost}} via {{Data Reweighing}} with {{Influence}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Peizhao and Liu, Hongfu},
  year = {2022},
  month = jun,
  pages = {12917--12930},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {With the fast development of algorithmic governance, fairness has become a compulsory property for machine learning models to suppress unintentional discrimination. In this paper, we focus on the pre-processing aspect for achieving fairness, and propose a data reweighing approach that only adjusts the weight for samples in the training phase. Different from most previous reweighing methods which usually assign a uniform weight for each (sub)group, we granularly model the influence of each training sample with regard to fairness-related quantity and predictive utility, and compute individual weights based on influence under the constraints from both fairness and utility. Experimental results reveal that previous methods achieve fairness at a non-negligible cost of utility, while as a significant advantage, our approach can empirically release the tradeoff and obtain cost-free fairness for equal opportunity. We demonstrate the cost-free fairness through vanilla classifiers and standard training processes, compared to baseline methods on multiple real-world tabular datasets. Code available at https://github.com/brandeis-machine-learning/influence-fairness.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RJMQXSJJ/Li and Liu - 2022 - Achieving Fairness at No Utility Cost via Data Rew.pdf}
}

@article{liInferringOutcomesRejected2020,
  title = {Inferring the Outcomes of Rejected Loans: An Application of Semisupervised Clustering},
  shorttitle = {Inferring the Outcomes of Rejected Loans},
  author = {Li, Zhiyong and Hu, Xinyi and Li, Ke and Zhou, Fanyin and Shen, Feng},
  year = {2020},
  month = feb,
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {183},
  number = {2},
  pages = {631--654},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/rssa.12534},
  abstract = {Rejection inference aims to reduce sample bias and to improve model performance in credit scoring.We propose a semisupervised clustering approach as a new rejection inference technique. K -prototype clustering can deal with mixed types of numeric and categorical characteristics, which are common in consumer credit data. We identify homogeneous acceptances and rejections and assign labels to part of the rejections according to the label of acceptances. We test the performance of various rejection inference methods in logit, support vector machine and random-forests models based on data sets of real consumer loans. The predictions of clustering rejection inference show advantages over other traditional rejection inference methods. Inferring the label of the rejection from semisupervised clustering is found to help to mitigate the sample bias problem and to improve the predictive accuracy.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Z7UMLT7A/Li et al. - 2020 - Inferring the outcomes of rejected loans an appli.pdf}
}

@misc{liMoreDataCan2022,
  title = {More {{Data Can Lead Us Astray}}: {{Active Data Acquisition}} in the {{Presence}} of {{Label Bias}}},
  shorttitle = {More {{Data Can Lead Us Astray}}},
  author = {Li, Yunyi and {De-Arteaga}, Maria and {Saar-Tsechansky}, Maytal},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07723},
  eprint = {2207.07723},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {An increased awareness concerning risks of algorithmic bias has driven a surge of efforts around bias mitigation strategies. A vast majority of the proposed approaches fall under one of two categories: (1) imposing algorithmic fairness constraints on predictive models, and (2) collecting additional training samples. Most recently and at the intersection of these two categories, methods that propose active learning under fairness constraints have been developed. However, proposed bias mitigation strategies typically overlook the bias presented in the observed labels. In this work, we study fairness considerations of active data collection strategies in the presence of label bias. We first present an overview of different types of label bias in the context of supervised learning systems. We then empirically show that, when overlooking label bias, collecting more data can aggravate bias, and imposing fairness constraints that rely on the observed labels in the data collection process may not address the problem. Our results illustrate the unintended consequences of deploying a model that attempts to mitigate a single type of bias while neglecting others, emphasizing the importance of explicitly differentiating between the types of bias that fairness-aware algorithms aim to address, and highlighting the risks of neglecting label bias during data collection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LMRPKA3J/Li et al. - 2022 - More Data Can Lead Us Astray Active Data Acquisit.pdf}
}

@inproceedings{linMeasuringEffectTraining2022,
  title = {Measuring the {{Effect}} of {{Training Data}} on {{Deep Learning Predictions}} via {{Randomized Experiments}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Lin, Jinkun and Zhang, Anqi and L{\'e}cuyer, Mathias and Li, Jinyang and Panda, Aurojit and Sen, Siddhartha},
  year = {2022},
  month = jun,
  pages = {13468--13504},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We develop a new, principled algorithm for estimating the contribution of training data points to the behavior of a deep learning model, such as a specific prediction it makes. Our algorithm estimates the AME, a quantity that measures the expected (average) marginal effect of adding a data point to a subset of the training data, sampled from a given distribution. When subsets are sampled from the uniform distribution, the AME reduces to the well-known Shapley value. Our approach is inspired by causal inference and randomized experiments: we sample different subsets of the training data to train multiple submodels, and evaluate each submodel's behavior. We then use a LASSO regression to jointly estimate the AME of each data point, based on the subset compositions. Under sparsity assumptions (k{$\ll$}Nk{$\ll$}Nk \textbackslash ll N datapoints have large AME), our estimator requires only O(klogN)O(klogN)O(k\textbackslash log N) randomized submodel trainings, improving upon the best prior Shapley value estimators.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2FYQFKT8/Lin et al. - 2022 - Measuring the Effect of Training Data on Deep Lear.pdf}
}

@article{liRejectInferenceCredit2017a,
  title = {Reject Inference in Credit Scoring Using {{Semi-supervised Support Vector Machines}}},
  author = {Li, Zhiyong and Tian, Ye and Li, Ke and Zhou, Fanyin and Yang, Wei},
  year = {2017},
  month = may,
  journal = {Expert Systems with Applications},
  volume = {74},
  pages = {105--114},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2017.01.011},
  abstract = {Credit scoring models are commonly built on a sample of accepted applicants whose repayment and behaviour information is observable once the loan has been issued. However in practice these models are regularly applied to new applicants, which may cause sample bias. This bias is even more pronounced in online lending, where over 90\% of total loan requests are rejected. Reject inference is a technique to infer the outcomes for rejected applicants and incorporate them in the scoring system, with the expectation that predictive accuracy is improved. This paper extends previous studies in two main ways: firstly, we propose a new method involving machine learning to solve the reject inference problem; secondly, the Semi-supervised Support Vector Machines model is found to improve the performance of scoring models compared to the industrial benchmark of logistic regression, based on 56,626 accepted and 563,215 rejected online consumer loans.},
  langid = {english},
  keywords = {Credit scoring,Online lending,Predictive accuracy,Reject inference,Semi-supervised Support Vector Machines},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6HD3YZJF/Li et al. - 2017 - Reject inference in credit scoring using Semi-supe.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QZ6TTXFD/S095741741730012X.html}
}

@inproceedings{liTrainingDataDebugging2022a,
  title = {Training Data Debugging for the Fairness of Machine Learning Software},
  booktitle = {Proceedings of the 44th {{International Conference}} on {{Software Engineering}}},
  author = {Li, Yanhui and Meng, Linghan and Chen, Lin and Yu, Li and Wu, Di and Zhou, Yuming and Xu, Baowen},
  year = {2022},
  month = may,
  series = {{{ICSE}} '22},
  pages = {2215--2227},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3510003.3510091},
  abstract = {With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community. According to the "data-driven" programming paradigm of ML software, we consider the root cause of the unfairness as biased features in training data. Inspired by software debugging, we propose a novel method, Linear-regression based Training Data Debugging (LTDD), to debug feature values in training data, i.e., (a) identify which features and which parts of them are biased, and (b) exclude the biased parts of such features to recover as much valuable and unbiased information as possible to build fair ML software. We conduct an extensive study on nine data sets and three classifiers to evaluate the effect of our method LTDD compared with four baseline methods. Experimental results show that (a) LTDD can better improve the fairness of ML software with less or comparable damage to the performance, and (b) LTDD is more actionable for fairness improvement in realistic scenarios.},
  isbn = {978-1-4503-9221-1},
  keywords = {debugging,Debugging,fairness,Fairness,Linear regression,Machine learning,ML software,ML Software,Programming,Software,Training,training data,Training data,Training Data},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FW8USR5C/Li et al. - 2022 - Training data debugging for the fairness of machin.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JNRK6UME/Li et al. - 2022 - Training Data Debugging for the Fairness of Machin.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/AIBEPGE3/figures.html}
}

@inproceedings{liuFairRepresentationLearning2022,
  title = {Fair {{Representation Learning}}: {{An Alternative}} to {{Mutual Information}}},
  shorttitle = {Fair {{Representation Learning}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Liu, Ji and Li, Zenan and Yao, Yuan and Xu, Feng and Ma, Xiaoxing and Xu, Miao and Tong, Hanghang},
  year = {2022},
  month = aug,
  pages = {1088--1097},
  publisher = {{ACM}},
  address = {{Washington DC USA}},
  doi = {10.1145/3534678.3539302},
  abstract = {Learning fair representations is an essential task to reduce bias in data-oriented decision making. It protects minority subgroups by requiring the learned representations to be independent of sensitive attributes. To achieve independence, the vast majority of the existing work primarily relaxes it to the minimization of the mutual information between sensitive attributes and learned representations. However, direct computation of mutual information is computationally intractable, and various upper bounds currently used either are still intractable or contradict the utility of the learned representations. In this paper, we introduce distance covariance as a new dependence measure into fair representation learning. By observing that sensitive attributes (e.g., gender, race, and age group) are typically categorical, the distance covariance can be converted to a tractable penalty term without contradicting the utility desideratum. Based on the tractable penalty, we propose FairDisCo, a variational method to learn fair representations. Experiments demonstrate that FairDisCo outperforms existing competitors for fair representation learning.},
  isbn = {978-1-4503-9385-0},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YJE9MJRW/Liu et al. - 2022 - Fair Representation Learning An Alternative to Mu.pdf}
}

@article{liuNewApproachReject2020b,
  title = {A New Approach in Reject Inference of Using Ensemble Learning Based on Global Semi-Supervised Framework},
  author = {Liu, Yan and Li, Xiner and Zhang, Zaimei},
  year = {2020},
  month = aug,
  journal = {Future Generation Computer Systems},
  volume = {109},
  pages = {382--391},
  issn = {0167-739X},
  doi = {10.1016/j.future.2020.03.047},
  abstract = {Credit scoring in online Peer-to-Peer (P2P) lending faces a huge challenge, which is the credit scoring models discard rejected applicants. This selective discarding leads to bias in the parameters of the models and ultimately affects the performance of credit evaluation. One approach for handling this problem is to adopt reject inference, which is a technique that infer the status of rejected samples and incorporate the results into credit scoring models. The most popular practice of reject inference is to use a credit scoring model that is only built on accepted samples to directly predict the status of rejected samples. However, the distribution of accepted samples in online P2P lending is different from rejected samples. We propose SSL-EC3, a global semi-supervised framework that merges multiple classifiers and clustering algorithms together to make better use of the information of rejected samples. It uses multiple unsupervised models (clustering algorithms) to explore the internal relationships of all samples, and then incorporates the information into the ensemble of supervised models (classifiers) to help correct initial classification results of rejected samples. In addition, we try to use a dynamic ensemble selection (DES) to select the appropriate ensemble of classifiers for each sample to be classified. Experimental results on the real data sets demonstrate the benefits of the proposed methods over conventional methods based on the reject inference.},
  langid = {english},
  keywords = {DES,Ensemble learning,Online P2P lending,Reject inference,Supervised model,Unsupervised model},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RH4UKJED/S0167739X19323635.html}
}

@article{liuSurveyActiveDeep2022,
  title = {A {{Survey}} on {{Active Deep Learning}}: {{From Model Driven}} to {{Data Driven}}},
  shorttitle = {A {{Survey}} on {{Active Deep Learning}}},
  author = {Liu, Peng and Wang, Lizhe and Ranjan, Rajiv and He, Guojin and Zhao, Lei},
  year = {2022},
  month = sep,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {10s},
  pages = {221:1--221:34},
  issn = {0360-0300},
  doi = {10.1145/3510414},
  abstract = {Which samples should be labelled in a large dataset is one of the most important problems for the training of deep learning. So far, a variety of active sample selection strategies related to deep learning have been proposed in the literature. We defined them as Active Deep Learning (ADL) only if their predictor or selector is a deep model, where the basic learner is called the predictor and the labeling schemes are called the selector. In this survey, we categorize ADL into model-driven ADL and data-driven ADL by whether its selector is model driven or data driven. We also introduce the different characteristics of the two major types of ADL, respectively. We summarized three fundamental factors in the designation of a selector. We pointed out that, with the development of deep learning, the selector in ADL also is experiencing the stage from model driven to data driven. The advantages and disadvantages between data-driven ADL and model-driven ADL are thoroughly analyzed. Furthermore, different sub-classes of data-drive or model-driven ADL are also summarized and discussed emphatically. Finally, we survey the trend of ADL from model driven to data driven.},
  keywords = {Active learning,data-driven,labelling samples,model-driven}
}

@misc{loftusCausalReasoningAlgorithmic2018,
  title = {Causal {{Reasoning}} for {{Algorithmic Fairness}}},
  author = {Loftus, Joshua R. and Russell, Chris and Kusner, Matt J. and Silva, Ricardo},
  year = {2018},
  month = may,
  number = {arXiv:1805.05859},
  eprint = {1805.05859},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.05859},
  abstract = {In this work, we argue for the importance of causal reasoning in creating fair algorithms for decision making. We give a review of existing approaches to fairness, describe work in causality necessary for the understanding of causal approaches, argue why causality is necessary for any approach that wishes to be fair, and give a detailed analysis of the many recent approaches to causality-based fairness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VG2N58J9/Loftus et al. - 2018 - Causal Reasoning for Algorithmic Fairness.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9EK6Q62U/1805.html}
}

@inproceedings{lopez-rojasPAYSIMFINANCIALMOBILE2016,
  title = {{{PAYSIM}}: {{A FINANCIAL MOBILE MONEY SIMULATOR FOR FRAUD DETECTION}}},
  shorttitle = {{{PAYSIM}}},
  booktitle = {28th {{European Modeling}} and {{Simulation Symposium}} 2016 ({{EMSS}} 2016)},
  author = {{Lopez-Rojas}, Edgar Alonso and Elmir, Ahmad and Axelsson, Stefan},
  year = {2016},
  month = sep,
  address = {{Larnaca, Cyprus}},
  abstract = {The lack of legitimate datasets on mobile money transactions to perform research on in the domain of fraud detection is a big problem today in the scientific community. Part of the problem is the intrinsic private nature of financial transactions, that leads to no public available data sets. This will leave the researchers with the burden of first harnessing the dataset before performing the actual research on it. This paper propose an approach to such a problem that we named the PaySim simulator. PaySim is a financial simulator that simulates mobile money transactions based on an original dataset. In this paper, we present a solution to ultimately yield the possibility to simulate mobile money transactions in such a way that they become similar to the original dataset. With technology frameworks such as Agent-Based simulation techniques, and the application of mathematical statistics, we show in this paper that the simulated data can be as prudent as the original dataset for research.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QD2BU2UG/Lopez-Rojas et al. - 2016 - PAYSIM A FINANCIAL MOBILE MONEY SIMULATOR FOR FRA.pdf}
}



@inproceedings{louizosVariationalFairAutoencoder2016,
  title = {The {{Variational Fair Autoencoder}}},
  booktitle = {4th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2016, {{San Juan}}, {{Puerto Rico}}, {{May}} 2-4, 2016, {{Conference Track Proceedings}}},
  author = {Louizos, Christos and Swersky, Kevin and Li, Yujia and Welling, Max and Zemel, Richard S.},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016}
}

@misc{lundbergConsistentIndividualizedFeature2019,
  title = {Consistent {{Individualized Feature Attribution}} for {{Tree Ensembles}}},
  author = {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  year = {2019},
  month = mar,
  number = {arXiv:1802.03888},
  eprint = {1802.03888},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.03888},
  abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3SNE25HT/Lundberg et al. - 2019 - Consistent Individualized Feature Attribution for .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/X686MKUB/1802.html}
}

@misc{lundbergExplainingMeasuresFairness2021,
  title = {Explaining {{Measures}} of {{Fairness}}},
  author = {Lundberg, Scott},
  year = {2021},
  month = jan,
  journal = {Medium},
  abstract = {Avoid the black-box use of fairness metrics in machine learning by applying modern explainable AI methods to measures of fairness.},
  howpublished = {https://towardsdatascience.com/explaining-measures-of-fairness-f0e419d4e0d7},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KJKNFXCG/explaining-measures-of-fairness-f0e419d4e0d7.html}
}

@article{lundbergLocalExplanationsGlobal2020,
  title = {From Local Explanations to Global Understanding with Explainable {{AI}} for Trees},
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  year = {2020},
  month = jan,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {1},
  pages = {56--67},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0138-9},
  abstract = {Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Medical research,Software},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ND8FBXC3/Lundberg et al. - 2020 - From local explanations to global understanding wi.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Z9FBUZNZ/s42256-019-0138-9.html}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KUCGTD5V/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@inproceedings{luongKNNImplementationSituation2011,
  title = {K-{{NN}} as an Implementation of Situation Testing for Discrimination Discovery and Prevention},
  booktitle = {Proceedings of the 17th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Luong, Binh Thanh and Ruggieri, Salvatore and Turini, Franco},
  year = {2011},
  month = aug,
  series = {{{KDD}} '11},
  pages = {502--510},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2020408.2020488},
  abstract = {With the support of the legally-grounded methodology of situation testing, we tackle the problems of discrimination discovery and prevention from a dataset of historical decisions by adopting a variant of k-NN classification. A tuple is labeled as discriminated if we can observe a significant difference of treatment among its neighbors belonging to a protected-by-law group and its neighbors not belonging to it. Discrimination discovery boils down to extracting a classification model from the labeled tuples. Discrimination prevention is tackled by changing the decision value for tuples labeled as discriminated before training a classifier. The approach of this paper overcomes legal weaknesses and technical limitations of existing proposals.},
  isbn = {978-1-4503-0813-7},
  keywords = {discrimination discovery and prevention,k-nn classification},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/M7LL43L4/Luong et al. - 2011 - k-NN as an implementation of situation testing for.pdf}
}

@inproceedings{madrasFairnessCausalAwareness2019,
  title = {Fairness through {{Causal Awareness}}: {{Learning Causal Latent-Variable Models}} for {{Biased Data}}},
  shorttitle = {Fairness through {{Causal Awareness}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Madras, David and Creager, Elliot and Pitassi, Toniann and Zemel, Richard},
  year = {2019},
  month = jan,
  series = {{{FAT}}* '19},
  pages = {349--358},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3287560.3287564},
  abstract = {How do we learn from biased data? Historical datasets often reflect historical prejudices; sensitive or protected attributes may affect the observed treatments and outcomes. Classification algorithms tasked with predicting outcomes accurately from these datasets tend to replicate these biases. We advocate a causal modeling approach to learning from biased data, exploring the relationship between fair classification and intervention. We propose a causal model in which the sensitive attribute confounds both the treatment and the outcome. Building on prior work in deep learning and generative modeling, we describe how to learn the parameters of this causal model from observational data alone, even in the presence of unobserved confounders. We show experimentally that fairness-aware causal modeling provides better estimates of the causal effects between the sensitive attribute, the treatment, and the outcome. We further present evidence that estimating these causal effects can help learn policies that are both more accurate and fair, when presented with a historically biased dataset.},
  isbn = {978-1-4503-6125-5},
  keywords = {causal inference,fairness in machine learning,variational inference},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Q8WC5QRY/Madras et al. - 2019 - Fairness through Causal Awareness Learning Causal.pdf}
}

@article{makhloufApplicabilityMachineLearning2021,
  title = {On the {{Applicability}} of {{Machine Learning Fairness Notions}}},
  author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
  year = {2021},
  month = may,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {23},
  number = {1},
  pages = {14--23},
  issn = {1931-0145},
  doi = {10.1145/3468507.3468511},
  abstract = {Machine Learning (ML) based predictive systems are increasingly used to support decisions with a critical impact on individuals' lives such as college admission, job hiring, child custody, criminal risk assessment, etc. As a result, fairness emerged as an important requirement to guarantee that ML predictive systems do not discriminate against specific individuals or entire sub-populations, in particular, minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions of fairness have been introduced in the literature. This paper is a survey of fairness notions that, unlike other surveys in the literature, addresses the question of "which notion of fairness is most suited to a given real-world scenario and why?". Our attempt to answer this question consists in (1) identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzing the behavior of each fairness notion, and then (3) fitting these two elements to recommend the most suitable fairness notion in every specific setup. The results are summarized in a decision diagram that can be used by practitioners and policy makers to navigate the relatively large catalogue of ML fairness notions.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XTS3S624/Makhlouf et al. - 2021 - On the Applicability of Machine Learning Fairness .pdf}
}

@misc{makhloufSurveyCausalbasedMachine2022a,
  title = {Survey on {{Causal-based Machine Learning Fairness Notions}}},
  author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
  year = {2022},
  month = jun,
  number = {arXiv:2010.09553},
  eprint = {2010.09553},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.09553},
  abstract = {Addressing the problem of fairness is crucial to safely use machine learning algorithms to support decisions with a critical impact on people's lives such as job hiring, child maltreatment, disease diagnosis, loan granting, etc. Several notions of fairness have been defined and examined in the past decade, such as statistical parity and equalized odds. The most recent fairness notions, however, are causal-based and reflect the now widely accepted idea that using causality is necessary to appropriately address the problem of fairness. This paper examines an exhaustive list of causal-based fairness notions and study their applicability in real-world scenarios. As the majority of causal-based fairness notions are defined in terms of non-observable quantities (e.g., interventions and counterfactuals), their deployment in practice requires to compute or estimate those quantities using observational data. This paper offers a comprehensive report of the different approaches to infer causal quantities from observational data including identifiability (Pearl's SCM framework) and estimation (potential outcome framework). The main contributions of this survey paper are (1) a guideline to help selecting a suitable fairness notion given a specific real-world scenario, and (2) a ranking of the fairness notions according to Pearl's causation ladder indicating how difficult it is to deploy each notion in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SHAGLIAK/Makhlouf et al. - 2022 - Survey on Causal-based Machine Learning Fairness N.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RLISIMWU/2010.html}
}

@article{mancisidorDeepGenerativeModels2020,
  title = {Deep Generative Models for Reject Inference in Credit Scoring},
  author = {Mancisidor, Rogelio A. and Kampffmeyer, Michael and Aas, Kjersti and Jenssen, Robert},
  year = {2020},
  month = may,
  journal = {Knowledge-Based Systems},
  volume = {196},
  pages = {105758},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2020.105758},
  abstract = {Credit scoring models based on accepted applications may be biased and their consequences can have a statistical and economic impact. Reject inference is the process of attempting to infer the creditworthiness status of the rejected applications. Inspired by the promising results of semi-supervised deep generative models, this research develops two novel Bayesian models for reject inference in credit scoring combining Gaussian mixtures and auxiliary variables in a semi-supervised framework with generative models. To the best of our knowledge this is the first study coupling these concepts together. The goal is to improve the classification accuracy in credit scoring models by adding reject applications. Further, our proposed models infer the unknown creditworthiness of the rejected applications by exact enumeration of the two possible outcomes of the loan (default or non-default). The efficient stochastic gradient optimization technique used in deep generative models makes our models suitable for large data sets. Finally, the experiments in this research show that our proposed models perform better than classical and alternative machine learning models for reject inference in credit scoring, and that model performance increases with the amount of data used for model training.},
  langid = {english},
  keywords = {Credit scoring,Deep generative models,Quantitative Finance - Computational Finance,Quantitative Finance - Risk Management,Reject inference,Semi-supervised learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8J46S26F/Mancisidor et al. - 2020 - Deep generative models for reject inference in cre.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/K99YVWVI/S0950705120301660.html}
}

@article{mannTestWhetherOne1947,
  title = {On a {{Test}} of {{Whether}} One of {{Two Random Variables}} Is {{Stochastically Larger}} than the {{Other}}},
  author = {Mann, H. B. and Whitney, D. R.},
  year = {1947},
  journal = {The Annals of Mathematical Statistics},
  volume = {18},
  number = {1},
  pages = {50--60},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {Let x and y be two random variables with continuous cumulative distribution functions f and g. A statistic U depending on the relative ranks of the x's and y's is proposed for testing the hypothesis f = g. Wilcoxon proposed an equivalent test in the Biometrics Bulletin, December, 1945, but gave only a few points of the distribution of his statistic. Under the hypothesis f = g the probability of obtaining a given U in a sample of n x's and m y's is the solution of a certain recurrence relation involving n and m. Using this recurrence relation tables have been computed giving the probability of U for samples up to n = m = 8. At this point the distribution is almost normal. From the recurrence relation explicit expressions for the mean, variance, and fourth moment are obtained. The 2rth moment is shown to have a certain form which enabled us to prove that the limit distribution is normal if m, n go to infinity in any arbitrary manner. The test is shown to be consistent with respect to the class of alternatives \$f(x) {$>$} g(x)\$ for every x.}
}

@article{masonAreasRelativeOperating2002,
  title = {Areas beneath the Relative Operating Characteristics ({{ROC}}) and Relative Operating Levels ({{ROL}}) Curves: {{Statistical}} Significance and Interpretation},
  shorttitle = {Areas beneath the Relative Operating Characteristics ({{ROC}}) and Relative Operating Levels ({{ROL}}) Curves},
  author = {Mason, S. J. and Graham, N. E.},
  year = {2002},
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {128},
  number = {584},
  pages = {2145--2166},
  issn = {1477-870X},
  doi = {10.1256/003590002320603584},
  abstract = {The areas beneath the relative (or receiver) operating characteristics (ROC) and relative operating levels (ROL) curves can be used as summary measures of forecast quality, but statistical significance tests for these areas are conducted infrequently in the atmospheric sciences. A development of signal-detection theory, the ROC curve has been widely applied in the medical and psychology fields where significance tests and relationships to other common statistical methods have been established and described. This valuable literature appears to be largely unknown to the atmospheric sciences where applications of ROC and related techniques are becoming more common. This paper presents a survey of that literature with a focus on the interpretation of the ROC area in the field of forecast verification. We extend these foundations to demonstrate that similar principles can be applied to the interpretation and significance testing of the ROL area. It is shown that the ROC area is equivalent to the Mann\textendash Whitney U-statistic testing the significance of forecast event probabilities for cases where events actually occurred with those where events did not occur. A similar derivation shows that the ROL area is equivalent to the Mann\textendash Whitney U-statistic testing the magnitude of events with respect to whether or not an event has been forecast. Because the Mann\textendash Whitney U-statistic follows a known probability distribution, under certain assumptions it can be used to define the statistical significance of ROC and ROL areas and for comparing the areas of competing forecasts. For large samples the significance of either measure can be accurately assessed using a normal-distribution approximation. Copyright \textcopyright{} 2002 Royal Meteorological Society},
  langid = {english},
  keywords = {Forecast verification,MannWhitney U-test,Probabilistic forecasts,Signal-detection theory,Student's t-test},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1256/003590002320603584},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IYLPAJ56/003590002320603584.html}
}

@article{mchughChisquareTestIndependence2013,
  title = {The {{Chi-square}} Test of Independence},
  author = {McHugh, Mary L.},
  year = {2013},
  month = jun,
  journal = {Biochemia Medica},
  volume = {23},
  number = {2},
  pages = {143--149},
  publisher = {{Medicinska naklada}},
  issn = {1330-0962, 1846-7482},
  doi = {10.11613/BM.2013.018},
  abstract = {The Chi-square statistic is a non-parametric (distribution free) tool designed to analyze group diffe-rences when the dependent variable is measured at a nominal level. Like all non-parametric statistics, the Chi-square is robust with respect to the...},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9AA8NP2U/McHugh - 2013 - The Chi-square test of independence.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SKII9LXU/152608.html}
}

@article{measure-barocas,
  title = {Fairness in Machine Learning},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  year = {2017},
  journal = {NIPS tutorial},
  volume = {1},
  pages = {2017}
}

@article{mehrabiSurveyBiasFairness2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2021},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3457607},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NGLWLMN9/Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@book{mendenhallIntroductionProbabilityStatistics2012,
  title = {Introduction to {{Probability}} and {{Statistics}}},
  author = {Mendenhall, William and Beaver, Robert J. and Beaver, Barbara M.},
  year = {2012},
  month = jan,
  publisher = {{Cengage Learning}},
  abstract = {Used by hundreds of thousands of students, INTRODUCTION TO PROBABILITY AND STATISTICS, Fourteenth Edition, blends proven coverage with new innovations to ensure you gain a solid understanding of statistical concepts--and see their relevance to your everyday life. The new edition retains the text's straightforward presentation and traditional outline for descriptive and inferential statistics while incorporating modern technology--including computational software and interactive visual tools--to help you master statistical reasoning and skillfully interpret statistical results. Drawing from decades of classroom teaching experience, the authors clearly illustrate how to apply statistical procedures as they explain how to describe real sets of data, what statistical tests mean in terms of practical application, how to evaluate the validity of the assumptions behind statistical tests, and what to do when statistical assumptions have been violated. Statistics can be an intimidating course, but with this text you will be well prepared. With its thorough explanations, insightful examples, practical exercises, and innovative technology features, this text equips you with a firm foundation in statistical concepts, as well as the tools to apply them to the world around you.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
  googlebooks = {fQsKAAAAQBAJ},
  isbn = {978-1-133-71167-4},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@inproceedings{merrickExplanationGameExplaining2020a,
  title = {The {{Explanation Game}}: {{Explaining Machine Learning Models Using Shapley Values}}},
  shorttitle = {The {{Explanation Game}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Merrick, Luke and Taly, Ankur},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {17--38},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-57321-8_2},
  abstract = {A number of techniques have been proposed to explain a machine learning model's prediction by attributing it to the corresponding input features. Popular among these are techniques that apply the Shapley value method from cooperative game theory. While existing papers focus on the axiomatic motivation of Shapley values, and efficient techniques for computing them, they offer little justification for the game formulations used, and do not address the uncertainty implicit in their methods' outputs. For instance, the popular SHAP algorithm's formulation may give substantial attributions to features that play no role in the model. In this work, we illustrate how subtle differences in the underlying game formulations of existing methods can cause large differences in the attributions for a prediction. We then present a general game formulation that unifies existing methods, and enables straightforward confidence intervals on their attributions. Furthermore, it allows us to interpret the attributions as contrastive explanations of an input relative to a distribution of reference inputs. We tie this idea to classic research in cognitive psychology on contrastive explanations, and propose a conceptual framework for generating and interpreting explanations for ML models, called formulate, approximate, explain (FAE). We apply this framework to explain black-box models trained on two UCI datasets and a Lending Club dataset.},
  isbn = {978-3-030-57321-8},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/D9Q7EKW3/Merrick and Taly - 2020 - The Explanation Game Explaining Machine Learning .pdf}
}

@article{miroshnikovWassersteinbasedFairnessInterpretability2022a,
  title = {Wasserstein-Based Fairness Interpretability Framework for Machine Learning Models},
  author = {Miroshnikov, Alexey and Kotsiopoulos, Konstandinos and Franks, Ryan and Kannan, Arjun Ravi},
  year = {2022},
  month = jul,
  journal = {Machine Learning},
  eprint = {2011.03156},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-022-06213-9},
  abstract = {The objective of this article is to introduce a fairness interpretability framework for measuring and explaining the bias in classification and regression models at the level of a distribution. In our work, we measure the model bias across sub-population distributions in the model output using the Wasserstein metric. To properly quantify the contributions of predictors, we take into account the favorability of both the model and predictors with respect to the non-protected class. The quantification is accomplished by the use of transport theory, which gives rise to the decomposition of the model bias and bias explanations to positive and negative contributions. To gain more insight into the role of favorability and allow for additivity of bias explanations, we adapt techniques from cooperative game theory.},
  archiveprefix = {arXiv},
  keywords = {49Q22; 91A12; 68T01; 90C08,Computer Science - Machine Learning,Mathematics - Probability},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BFVLIA8S/Miroshnikov et al. - 2022 - Wasserstein-based fairness interpretability framew.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6AFIKP8D/2011.html}
}

@misc{mishlerFairWhenTrained2022,
  title = {Fair {{When Trained}}, {{Unfair When Deployed}}: {{Observable Fairness Measures}} Are {{Unstable}} in {{Performative Prediction Settings}}},
  shorttitle = {Fair {{When Trained}}, {{Unfair When Deployed}}},
  author = {Mishler, Alan and Dalmasso, Niccol{\`o}},
  year = {2022},
  month = feb,
  number = {arXiv:2202.05049},
  eprint = {2202.05049},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.05049},
  abstract = {Many popular algorithmic fairness measures depend on the joint distribution of predictions, outcomes, and a sensitive feature like race or gender. These measures are sensitive to distribution shift: a predictor which is trained to satisfy one of these fairness definitions may become unfair if the distribution changes. In performative prediction settings, however, predictors are precisely intended to induce distribution shift. For example, in many applications in criminal justice, healthcare, and consumer finance, the purpose of building a predictor is to reduce the rate of adverse outcomes such as recidivism, hospitalization, or default on a loan. We formalize the effect of such predictors as a type of concept shift-a particular variety of distribution shift-and show both theoretically and via simulated examples how this causes predictors which are fair when they are trained to become unfair when they are deployed. We further show how many of these issues can be avoided by using fairness definitions that depend on counterfactual rather than observable outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PLBBCRN8/Mishler and Dalmasso - 2022 - Fair When Trained, Unfair When Deployed Observabl.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QKXVL4GP/2202.html}
}

@misc{mivuleUtilizingNoiseAddition2013,
  title = {Utilizing {{Noise Addition}} for {{Data Privacy}}, an {{Overview}}},
  author = {Mivule, Kato},
  year = {2013},
  month = sep,
  number = {arXiv:1309.3958},
  eprint = {1309.3958},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1309.3958},
  abstract = {The internet is increasingly becoming a standard for both the production and consumption of data while at the same time cyber-crime involving the theft of private data is growing. Therefore in efforts to securely transact in data, privacy and security concerns must be taken into account to ensure that the confidentiality of individuals and entities involved is not compromised, and that the data published is compliant to privacy laws. In this paper, we take a look at noise addition as one of the data privacy providing techniques. Our endeavor in this overview is to give a foundational perspective on noise addition data privacy techniques, provide statistical consideration for noise addition techniques and look at the current state of the art in the field, while outlining future areas of research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9NF2S32T/Mivule - 2013 - Utilizing Noise Addition for Data Privacy, an Over.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5JTLLSJV/1309.html}
}

@article{mivuleUtilizingNoiseAddition2013a,
  title = {Utilizing {{Noise Addition}} for {{Data Privacy}}, an {{Overview}}},
  author = {Mivule, Kato},
  year = {2013},
  month = sep,
  abstract = {The internet is increasingly becoming a standard for both the production and consumption of data while at the same time cyber-crime involving the theft of private data is growing. Therefore in efforts to securely transact in data, privacy and security concerns must be taken into account to ensure that the confidentiality of individuals and entities involved is not compromised, and that the data published is compliant to privacy laws. In this paper, we take a look at noise addition as one of the data privacy providing techniques. Our endeavor in this overview is to give a foundational perspective on noise addition data privacy techniques, provide statistical consideration for noise addition techniques and look at the current state of the art in the field, while outlining future areas of research.}
}

@inproceedings{mohammedMachineLearningOversampling2020a,
  title = {Machine {{Learning}} with {{Oversampling}} and {{Undersampling Techniques}}: {{Overview Study}} and {{Experimental Results}}},
  shorttitle = {Machine {{Learning}} with {{Oversampling}} and {{Undersampling Techniques}}},
  booktitle = {2020 11th {{International Conference}} on {{Information}} and {{Communication Systems}} ({{ICICS}})},
  author = {Mohammed, Roweida and Rawashdeh, Jumanah and Abdullah, Malak},
  year = {2020},
  month = apr,
  pages = {243--248},
  issn = {2573-3346},
  doi = {10.1109/ICICS49469.2020.239556},
  abstract = {Data imbalance in Machine Learning refers to an unequal distribution of classes within a dataset. This issue is encountered mostly in classification tasks in which the distribution of classes or labels in a given dataset is not uniform. The straightforward method to solve this problem is the resampling method by adding records to the minority class or deleting ones from the majority class. In this paper, we have experimented with the two resampling widely adopted techniques: oversampling and undersampling. In order to explore both techniques, we have chosen a public imbalanced dataset from kaggle website Santander Customer Transaction Prediction and have applied a group of well-known machine learning algorithms with different hyperparamters that give best results for both resampling techniques. One of the key findings of this paper is noticing that oversampling performs better than undersampling for different classifiers and obtains higher scores in different evaluation metrics.},
  keywords = {Accuracy,Class Imbalance,Communication systems,Decision trees,Kernel,Machine Learning,Measurement,Naive Bayes,Oversampling,Precision,Random Forest,Random forests,Recall,Support vector machines,SVM,Undersampling},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YHCFBVFW/Mohammed et al. - 2020 - Machine Learning with Oversampling and Undersampli.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/C7MK5JDD/9078901.html}
}

@book{molnarShapleyValuesInterpretable,
  title = {9.5 {{Shapley Values}} | {{Interpretable Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XCG65I6L/shapley.html}
}

@misc{mullainathanMarketFinancialAdvice2012,
  type = {Working {{Paper}}},
  title = {The {{Market}} for {{Financial Advice}}: {{An Audit Study}}},
  shorttitle = {The {{Market}} for {{Financial Advice}}},
  author = {Mullainathan, Sendhil and Noeth, Markus and Schoar, Antoinette},
  year = {2012},
  month = mar,
  series = {Working {{Paper Series}}},
  number = {17929},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w17929},
  abstract = {Do financial advisers undo or reinforce the behavioral biases and misconceptions of their clients? We use an audit methodology where trained auditors meet with financial advisers and present different types of portfolios. These portfolios reflect either biases that are in line with the financial interests of the advisers (e.g., returns-chasing portfolio) or run counter to their interests (e.g., a portfolio with company stock or very low-fee index funds). We document that advisers fail to de-bias their clients and often reinforce biases that are in their interests. Advisers encourage returns-chasing behavior and push for actively managed funds that have higher fees, even if the client starts with a well-diversified, low-fee portfolio.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/G4HTQTM4/Mullainathan et al. - 2012 - The Market for Financial Advice An Audit Study.pdf}
}

@inproceedings{nguyenEpistemicUncertaintySampling2019,
  title = {Epistemic {{Uncertainty Sampling}}},
  booktitle = {Discovery {{Science}}},
  author = {Nguyen, Vu-Linh and Destercke, S{\'e}bastien and H{\"u}llermeier, Eyke},
  editor = {Kralj Novak, Petra and {\v S}muc, Tomislav and D{\v z}eroski, Sa{\v s}o},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {72--86},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-33778-0_7},
  abstract = {Various strategies for active learning have been proposed in the machine learning literature. In uncertainty sampling, which is among the most popular approaches, the active learner sequentially queries the label of those instances for which its current prediction is maximally uncertain. The predictions as well as the measures used to quantify the degree of uncertainty, such as entropy, are almost exclusively of a probabilistic nature. In this paper, we advocate a distinction between two different types of uncertainty, referred to as epistemic and aleatoric, in the context of active learning. Roughly speaking, these notions capture the reducible and the irreducible part of the total uncertainty in a prediction, respectively. We conjecture that, in uncertainty sampling, the usefulness of an instance is better reflected by its epistemic than by its aleatoric uncertainty. This leads us to suggest the principle of ``epistemic uncertainty sampling'', which we instantiate by means of a concrete approach for measuring epistemic and aleatoric uncertainty. In experimental studies, epistemic uncertainty sampling does indeed show promising performance.},
  isbn = {978-3-030-33778-0},
  langid = {english},
  keywords = {Active learning,Aleatoric uncertainty,Epistemic uncertainty,Uncertainty sampling},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/M8X4N5BZ/Nguyen et al. - 2019 - Epistemic Uncertainty Sampling.pdf}
}

@techreport{nguyenRejectInferenceApplication2016,
  title = {Reject Inference in Application Scorecards: Evidence from {{France}}},
  shorttitle = {Reject Inference in Application Scorecards},
  author = {Nguyen, Ha-Thu},
  year = {2016},
  journal = {EconomiX Working Papers},
  number = {2016-10},
  institution = {{University of Paris Nanterre, EconomiX}},
  abstract = {Credit scoring models are commonly developed using only accepted Known Good/Bad (G/B) applications, called KGB model, because we only know the performance of those accepted in the past. Obviously, the KGB model is not indicative of the entire through-the-door population, and reject inference precisely attempts to address the bias by assigning an inferred G/B status to rejected applications. In this paper, we discuss the pros and cons of various reject inference techniques, and pitfalls to avoid when using them. We consider a real dataset of a major French consumer finance bank to assess the effectiveness of the practice of using reject inference. To do that, we rely on the logistic regression framework to model probabilities to become good/bad, and then validate the model performance with and without sample selection bias correction. Our main results can be summarized as follows. First, we show that the best reject inference technique is not necessarily the most complicated one: reweighting and parceling provide more accurate and relevant results than fuzzy augmentation and Heckmans two-stage correction. Second, disregarding rejected applications significantly impacts the forecast accuracy of the scorecard. Third, as the sum of standard errors dramatically reduces when the sample size increases, reject inference turns out to produce an improved representation of the population. Finally, reject inference appears to be an effective way to reduce overfitting in model selection.},
  langid = {english},
  keywords = {fuzzy augmentation,Heckmans two-stage correction.,logistic regression,parceling,Reject inference,reweighting,sample selection,selection bias},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FTTB3LWZ/Nguyen - 2016 - Reject inference in application scorecards eviden.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6FYGGQJZ/2016-10.html}
}

@article{noise_maths,
  title = {New Trends in Gender and Mathematics Performance: A Meta-Analysis.},
  author = {Lindberg, Sara M and Hyde, Janet Shibley and Petersen, Jennifer L and Linn, Marcia C},
  year = {2010},
  journal = {Psychological bulletin},
  volume = {136},
  number = {6},
  pages = {1123},
  publisher = {{American Psychological Association}}
}

@article{noise_medical_trials,
  title = {The Accuracy, Fairness, and Limits of Predicting Recidivism},
  author = {Dressel, Julia and {Hany Farid}},
  year = {2018},
  journal = {Science Advances},
  volume = {4},
  number = {1}
}

@article{pageRacialEthnicDiscrimination1995,
  title = {Racial and {{Ethnic Discrimination}} in {{Urban Housing Markets}}: {{Evidence}} from a {{Recent Audit Study}}},
  shorttitle = {Racial and {{Ethnic Discrimination}} in {{Urban Housing Markets}}},
  author = {Page, Marianne},
  year = {1995},
  month = sep,
  journal = {Journal of Urban Economics},
  volume = {38},
  number = {2},
  pages = {183--206},
  issn = {0094-1190},
  doi = {10.1006/juec.1995.1028},
  abstract = {Using audit data from HUD{${'}$}s 1989 Housing Discrimination Study of 25 American cities, this paper analyzes the level and causes of one type of discrimination encountered by blacks and Hispanics during housing searches. A statistical model is developed that recognizes features of audit data not properly accounted for in previous studies. Estimates based on this model indicate that real estate agents show blacks and Hispanics 80-90\% of the housing units inspected by their Anglo customers. A detailed analysis suggests that customer prejudice and racial stereotyping influence agent behavior, particularly toward black customers. City-specific estimates indicate that discrimination levels vary across cities.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RZ3CH7KR/S0094119085710285.html}
}

@inproceedings{pandlTrustworthyMachineLearning2021,
  title = {Trustworthy Machine Learning for Health Care: Scalable Data Valuation with the Shapley Value},
  shorttitle = {Trustworthy Machine Learning for Health Care},
  booktitle = {Proceedings of the {{Conference}} on {{Health}}, {{Inference}}, and {{Learning}}},
  author = {Pandl, Konstantin D. and Feiland, Fabian and Thiebes, Scott and Sunyaev, Ali},
  year = {2021},
  month = apr,
  series = {{{CHIL}} '21},
  pages = {47--57},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3450439.3451861},
  abstract = {Collecting data from many sources is an essential approach to generate large data sets required for the training of machine learning models. Trustworthy machine learning requires incentives, guarantees of data quality, and information privacy. Applying recent advancements in data valuation methods for machine learning can help to enable these. In this work, we analyze the suitability of three different data valuation methods for medical image classification tasks, specifically pleural effusion, on an extensive data set of chest X-ray scans. Our results reveal that a heuristic for calculating the Shapley valuation scheme based on a k-nearest neighbor classifier can successfully value large quantities of data instances. We also demonstrate possible applications for incentivizing data sharing, the efficient detection of mislabeled data, and summarizing data sets to exclude private information. Thereby, this work contributes to developing modern data infrastructures for trustworthy machine learning in health care.},
  isbn = {978-1-4503-8359-2},
  keywords = {computer vision,data valuation,machine learning,medical imaging,shapley value},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6APVXZNX/Pandl et al. - 2021 - Trustworthy machine learning for health care scal.pdf}
}

@inproceedings{panExplainingAlgorithmicFairness2021a,
  title = {Explaining {{Algorithmic Fairness Through Fairness-Aware Causal Path Decomposition}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Pan, Weishen and Cui, Sen and Bian, Jiang and Zhang, Changshui and Wang, Fei},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1287--1297},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467258},
  abstract = {Algorithmic fairness has aroused considerable interests in data mining and machine learning communities recently. So far the existing research has been mostly focusing on the development of quantitative metrics to measure algorithm disparities across different protected groups, and approaches for adjusting the algorithm output to reduce such disparities. In this paper, we propose to study the problem of identification of the source of model disparities. Unlike existing interpretation methods which typically learn feature importance, we consider the causal relationships among feature variables and propose a novel framework to decompose the disparity into the sum of contributions from fairness-aware causal paths, which are paths linking the sensitive attribute and the final predictions, on the graph. We also consider the scenario when the directions on certain edges within those paths cannot be determined. Our framework is also model agnostic and applicable to a variety of quantitative disparity measures. Empirical evaluations on both synthetic and real-world data sets are provided to show that our method can provide precise and comprehensive explanations to the model disparities.},
  isbn = {978-1-4503-8332-5},
  keywords = {causal graph,explanation,fairness},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YQJZCG73/Pan et al. - 2021 - Explaining Algorithmic Fairness Through Fairness-A.pdf}
}

@book{pareto1919manuale,
  title = {Manuale Di Economia Politica: Con Una Introduzione Alla Scienza Sociale},
  author = {Pareto, Vilfredo},
  year = {1919},
  volume = {13},
  publisher = {{Societ\`a editrice libraria}}
}

@inproceedings{pastaltzidisDataAugmentationFairnessaware2022,
  title = {Data Augmentation for Fairness-Aware Machine Learning: {{Preventing}} Algorithmic Bias in Law Enforcement Systems},
  shorttitle = {Data Augmentation for Fairness-Aware Machine Learning},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Pastaltzidis, Ioannis and Dimitriou, Nikolaos and {Quezada-Tavarez}, Katherine and Aidinlis, Stergios and Marquenie, Thomas and Gurzawska, Agata and Tzovaras, Dimitrios},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {2302--2314},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3531146.3534644},
  abstract = {Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.},
  isbn = {978-1-4503-9352-2},
  keywords = {AI ethics,algorithmic bias,computer vision,fairness,law enforcement technology,violence detection},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3PUS9A8Q/Pastaltzidis et al. - 2022 - Data augmentation for fairness-aware machine learn.pdf}
}

@article{peet-pareLongTermFairness,
  title = {Long {{Term Fairness}} for {{Minority Groups}} via {{Performative Distributionally Robust Optimization}}},
  author = {{Peet-Pare}, Liam and Hegde, Nidhi and Fyshe, Alona},
  pages = {26},
  abstract = {Fairness researchers in machine learning (ML) have coalesced around several fairness criteria which provide formal definitions of what it means for an ML model to be fair. However, these criteria have some serious limitations. We identify four key shortcomings of these formal fairness criteria, and aim to help to address them by extending performative prediction to include a distributionally robust objective.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8JKPXMAP/Peet-Pare et al. - Long Term Fairness for Minority Groups via Perform.pdf}
}

@misc{pengXFAIRBetterFairness2022a,
  title = {{{xFAIR}}: {{Better Fairness}} via {{Model-based Rebalancing}} of {{Protected Attributes}}},
  shorttitle = {{{xFAIR}}},
  author = {Peng, Kewen and Chakraborty, Joymallya and Menzies, Tim},
  year = {2022},
  month = mar,
  number = {arXiv:2110.01109},
  eprint = {2110.01109},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.01109},
  abstract = {Context: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. Objective: We aim at better detection and mitigation of algorithmic discrimination in machine learning software problems. Method: Here we propose xFAIR, a model-based extrapolation method, that is capable of both mitigating bias and explaining the cause. In our xFAIR approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model via rebalancing the distribution of protected attributes. Results: The experiments of this paper show that, without compromising (original) model performance, xFAIR can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, when compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. Conclusion: Algorithmic decision bias can be removed via extrapolation that smooths away outlier points. As evidence for this, our proposed xFAIR is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,D.2},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/F6DDGPYR/Peng et al. - 2022 - xFAIR Better Fairness via Model-based Rebalancing.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WBRM4WGM/2110.html}
}

@inproceedings{perdomoPerformativePrediction2020,
  title = {Performative {{Prediction}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Perdomo, Juan and Zrnic, Tijana and {Mendler-D{\"u}nner}, Celestine and Hardt, Moritz},
  year = {2020},
  month = nov,
  pages = {7599--7609},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining. We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss. In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/A9ASL9LU/Perdomo et al. - 2020 - Performative Prediction.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SKZHGWJ5/Perdomo et al. - 2020 - Performative Prediction.pdf}
}

@article{pessachReviewFairnessMachine2022,
  title = {A {{Review}} on {{Fairness}} in {{Machine Learning}}},
  author = {Pessach, Dana and Shmueli, Erez},
  year = {2022},
  month = feb,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {3},
  pages = {51:1--51:44},
  issn = {0360-0300},
  doi = {10.1145/3494672},
  abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.},
  keywords = {Algorithmic bias,algorithmic fairness,fairness in machine learning,fairness-aware machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7TBI82EI/Pessach and Shmueli - 2022 - A Review on Fairness in Machine Learning.pdf}
}

@article{pfisterKernelbasedTestsJoint2018,
  title = {Kernel-Based Tests for Joint Independence},
  author = {Pfister, Niklas and B{\"u}hlmann, Peter and Sch{\"o}lkopf, Bernhard and Peters, Jonas},
  year = {2018},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {80},
  number = {1},
  pages = {5--31},
  issn = {1467-9868},
  doi = {10.1111/rssb.12235},
  abstract = {We investigate the problem of testing whether d possibly multivariate random variables, which may or may not be continuous, are jointly (or mutually) independent. Our method builds on ideas of the two-variable Hilbert\textendash Schmidt independence criterion but allows for an arbitrary number of variables. We embed the joint distribution and the product of the marginals in a reproducing kernel Hilbert space and define the d-variable Hilbert\textendash Schmidt independence criterion dHSIC as the squared distance between the embeddings. In the population case, the value of dHSIC is 0 if and only if the d variables are jointly independent, as long as the kernel is characteristic. On the basis of an empirical estimate of dHSIC, we investigate three non-parametric hypothesis tests: a permutation test, a bootstrap analogue and a procedure based on a gamma approximation. We apply non-parametric independence testing to a problem in causal discovery and illustrate the new methods on simulated and real data sets.},
  langid = {english},
  keywords = {Causal inference,Independence test,Kernel methods,V-statistics},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12235},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/682CCR4F/Pfister et al. - 2018 - Kernel-based tests for joint independence.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HVWI6H88/rssb.html}
}

@inproceedings{pleissFairnessCalibration2017,
  title = {On {{Fairness}} and {{Calibration}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pleiss, Geoff and Raghavan, Manish and Wu, Felix and Kleinberg, Jon and Weinberger, Kilian Q},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be "fair." In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NCFUSXFB/Pleiss et al. - 2017 - On Fairness and Calibration.pdf}
}

@inproceedings{pombalPrisonersTheirOwn2022,
  title = {Prisoners of {{Their Own Devices}}: {{How Models Induce Data Bias}} in {{Performative Prediction}}},
  booktitle = {{{ICML}} 2022 {{Workshop}} on {{Responsible Decision Making}} in {{Dynamic Environments}}},
  author = {Pombal, Jos{\'e} and Saleiro, Pedro and Figueiredo, M{\'a}rio A. T. and Bizarro, Pedro},
  year = {2022},
  month = aug,
  eprint = {2206.13183},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.13183},
  abstract = {The unparalleled ability of machine learning algorithms to learn patterns from data also enables them to incorporate biases embedded within. A biased model can then make decisions that disproportionately harm certain groups in society. Much work has been devoted to measuring unfairness in static ML environments, but not in dynamic, performative prediction ones, in which most real-world use cases operate. In the latter, the predictive model itself plays a pivotal role in shaping the distribution of the data. However, little attention has been heeded to relating unfairness to these interactions. Thus, to further the understanding of unfairness in these settings, we propose a taxonomy to characterize bias in the data, and study cases where it is shaped by model behaviour. Using a real-world account opening fraud detection case study as an example, we study the dangers to both performance and fairness of two typical biases in performative prediction: distribution shifts, and the problem of selective labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SU7W5IBR/Pombal et al. - 2022 - Prisoners of Their Own Devices How Models Induce .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GNSRID5A/2206.html}
}

@inproceedings{pombalUnderstandingUnfairnessFraud2022,
  title = {Understanding {{Unfairness}} in {{Fraud Detection}} through {{Model}} and {{Data Bias Interactions}}},
  booktitle = {{{KDD}} 2022 {{Workshop}} on {{Machine Learning}} in {{Finance}}},
  author = {Pombal, Jos{\'e} and Cruz, Andr{\'e} F. and Bravo, Jo{\~a}o and Saleiro, Pedro and Figueiredo, M{\'a}rio A. T. and Bizarro, Pedro},
  year = {2022},
  month = jul,
  eprint = {2207.06273},
  eprinttype = {arxiv},
  primaryclass = {cs, q-fin},
  doi = {10.48550/arXiv.2207.06273},
  abstract = {In recent years, machine learning algorithms have become ubiquitous in a multitude of high-stakes decision-making applications. The unparalleled ability of machine learning algorithms to learn patterns from data also enables them to incorporate biases embedded within. A biased model can then make decisions that disproportionately harm certain groups in society -- limiting their access to financial services, for example. The awareness of this problem has given rise to the field of Fair ML, which focuses on studying, measuring, and mitigating unfairness in algorithmic prediction, with respect to a set of protected groups (e.g., race or gender). However, the underlying causes for algorithmic unfairness still remain elusive, with researchers divided between blaming either the ML algorithms or the data they are trained on. In this work, we maintain that algorithmic unfairness stems from interactions between models and biases in the data, rather than from isolated contributions of either of them. To this end, we propose a taxonomy to characterize data bias and we study a set of hypotheses regarding the fairness-accuracy trade-offs that fairness-blind ML algorithms exhibit under different data bias settings. On our real-world account-opening fraud use case, we find that each setting entails specific trade-offs, affecting fairness in expected value and variance -- the latter often going unnoticed. Moreover, we show how algorithms compare differently in terms of accuracy and fairness, depending on the biases affecting the data. Finally, we note that under specific data bias conditions, simple pre-processing interventions can successfully balance group-wise error rates, while the same techniques fail in more complex settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Finance - Statistical Finance},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XXFUGN4B/Pombal et al. - 2022 - Understanding Unfairness in Fraud Detection throug.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JQZGR2RY/2207.html}
}

@inproceedings{pruthiEstimatingTrainingData2020,
  title = {Estimating {{Training Data Influence}} by {{Tracing Gradient Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pruthi, Garima and Liu, Frederick and Kale, Satyen and Sundararajan, Mukund},
  year = {2020},
  volume = {33},
  pages = {19920--19930},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/A95DAHAF/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracing Grad.pdf}
}

@inproceedings{puyol-antonFairnessCardiacMR2021,
  title = {Fairness in {{Cardiac MR Image Analysis}}: {{An Investigation}} of {{Bias Due}} to {{Data Imbalance}} in {{Deep Learning Based Segmentation}}},
  shorttitle = {Fairness in {{Cardiac MR Image Analysis}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2021},
  author = {{Puyol-Ant{\'o}n}, Esther and Ruijsink, Bram and Piechnik, Stefan K. and Neubauer, Stefan and Petersen, Steffen E. and Razavi, Reza and King, Andrew P.},
  editor = {{de Bruijne}, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {413--423},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-87199-4_39},
  abstract = {The subject of `fairness' in artificial intelligence (AI) refers to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most applications to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive results in recent years, and such techniques are starting to be translated into clinical practice. However, no work has yet investigated the fairness of such models. In this work, we perform such an analysis for racial/gender groups, focusing on the problem of training data imbalance, using a nnU-Net model trained and evaluated on cine short axis cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from 6 different racial groups. We find statistically significant differences in Dice performance between different racial groups. To reduce the racial bias, we investigated three strategies: (1) stratified batch sampling, in which batch sampling is stratified to ensure balance between racial groups; (2) fair meta-learning for segmentation, in which a DL classifier is trained to classify race and jointly optimized with the segmentation model; and (3) protected group models, in which a different segmentation model is trained for each racial group. We also compared the results to the scenario where we have a perfectly balanced database. To assess fairness we used the standard deviation (SD) and skewed error ratio (SER) of the average Dice values. Our results demonstrate that the racial bias results from the use of imbalanced training data, and that all proposed bias mitigation strategies improved fairness, with the best SD and SER resulting from the use of protected group models.},
  isbn = {978-3-030-87199-4},
  langid = {english},
  keywords = {Cardiac MRI,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Fair AI,Inequality,Segmentation},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QDN4JB3H/Puyol-Antn et al. - 2021 - Fairness in Cardiac MR Image Analysis An Investig.pdf}
}

@misc{quinzanFastFeatureSelection2022,
  title = {Fast {{Feature Selection}} with {{Fairness Constraints}}},
  author = {Quinzan, Francesco and Khanna, Rajiv and Hershcovitch, Moshik and Cohen, Sarel and Waddington, Daniel G. and Friedrich, Tobias and Mahoney, Michael W.},
  year = {2022},
  month = feb,
  number = {arXiv:2202.13718},
  eprint = {2202.13718},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.13718},
  abstract = {We study the fundamental problem of selecting optimal features for model construction. This problem is computationally challenging on large datasets, even with the use of greedy algorithm variants. To address this challenge, we extend the adaptive query model, recently proposed for the greedy forward selection for submodular functions, to the faster paradigm of Orthogonal Matching Pursuit for non-submodular functions. Our extension also allows the use of downward-closed constraints, which can be used to encode certain fairness criteria into the feature selection process. The proposed algorithm achieves exponentially fast parallel run time in the adaptive query model, scaling much better than prior work. The proposed algorithm also handles certain fairness constraints by design. We prove strong approximation guarantees for the algorithm based on standard assumptions. These guarantees are applicable to many parametric models, including Generalized Linear Models. Finally, we demonstrate empirically that the proposed algorithm competes favorably with state-of-the-art techniques for feature selection, on real-world and synthetic datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3KPDYWBR/Quinzan et al. - 2022 - Fast Feature Selection with Fairness Constraints.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2W2TEGEV/2202.html}
}

@article{rajkomarMachineLearningMedicine2019,
  title = {Machine {{Learning}} in {{Medicine}}},
  author = {Rajkomar, Alvin and Dean, Jeffrey and Kohane, Isaac},
  year = {2019},
  month = apr,
  journal = {New England Journal of Medicine},
  volume = {380},
  number = {14},
  pages = {1347--1358},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJMra1814259},
  pmid = {30943338},
  annotation = {\_eprint: https://www.nejm.org/doi/pdf/10.1056/NEJMra1814259}
}

@article{rambachanEconomicPerspectiveAlgorithmic2020a,
  title = {An {{Economic Perspective}} on {{Algorithmic Fairness}}},
  author = {Rambachan, Ashesh and Kleinberg, Jon and Ludwig, Jens and Mullainathan, Sendhil},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {91--95},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.20201036},
  abstract = {There are widespread concerns that the growing use of machine learning algorithms in important decisions may reproduce and reinforce existing discrimination against legally protected groups. Most of the attention to date on issues of ``algorithmic bias'' or ``algorithmic fairness'' has come from computer scientists and machine learning researchers. We argue that concerns about algorithmic fairness are at least as much about questions of how discrimination manifests itself in data, decision-making under uncertainty, and optimal regulation. To fully answer these questions, an economic framework is necessary--and as a result, economists have much to contribute.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DHGH6HCV/Rambachan et al. - 2020 - An Economic Perspective on Algorithmic Fairness.pdf}
}

@inproceedings{reddyBenchmarkingBiasMitigation2021,
  title = {Benchmarking {{Bias Mitigation Algorithms}} in {{Representation Learning}} through {{Fairness Metrics}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems Track}} on {{Datasets}} and {{Benchmarks}}},
  author = {Reddy, Charan and Sharma, Deepak and Mehri, Soroush and Romero Soriano, Adriana and Shabanian, Samira and Honari, Sina},
  year = {2021},
  month = dec,
  volume = {1},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4KFJZJDJ/Reddy et al. - 2021 - Benchmarking Bias Mitigation Algorithms in Represe.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VTAYVVT2/2723d092b63885e0d7c260cc007e8b9d-Abstract-round1.html}
}

@inproceedings{renLearningReweightExamples2018,
  title = {Learning to {{Reweight Examples}} for {{Robust Deep Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  year = {2018},
  month = jul,
  pages = {4334--4343},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/T5KJU8AC/Ren et al. - 2018 - Learning to Reweight Examples for Robust Deep Lear.pdf}
}

@article{renSurveyDeepActive2021,
  title = {A {{Survey}} of {{Deep Active Learning}}},
  author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
  year = {2021},
  month = oct,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {9},
  pages = {180:1--180:40},
  issn = {0360-0300},
  doi = {10.1145/3472291},
  abstract = {Active learning (AL) attempts to maximize a model's performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have a relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely according the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to a large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.). Therefore, AL is gradually coming to receive the attention it is due. It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotation while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DeepAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DeepAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DeepAL from an application perspective. Finally, we discuss the confusion and problems associated with DeepAL and provide some possible development directions.},
  keywords = {active learning,deep active learning,Deep learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YVL5M4JA/Ren et al. - 2021 - A Survey of Deep Active Learning.pdf}
}

@article{rf,
  title = {Random Forests},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  publisher = {{Springer}}
}

@misc{rosenblattDifferentiallyPrivateSynthetic2020,
  title = {Differentially {{Private Synthetic Data}}: {{Applied Evaluations}} and {{Enhancements}}},
  shorttitle = {Differentially {{Private Synthetic Data}}},
  author = {Rosenblatt, Lucas and Liu, Xiaoyan and Pouyanfar, Samira and {de Leon}, Eduardo and Desai, Anuj and Allen, Joshua},
  year = {2020},
  month = nov,
  number = {arXiv:2011.05537},
  eprint = {2011.05537},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.05537},
  abstract = {Machine learning practitioners frequently seek to leverage the most informative available data, without violating the data owner's privacy, when building predictive models. Differentially private data synthesis protects personal details from exposure, and allows for the training of differentially private machine learning models on privately generated datasets. But how can we effectively assess the efficacy of differentially private synthetic data? In this paper, we survey four differentially private generative adversarial networks for data synthesis. We evaluate each of them at scale on five standard tabular datasets, and in two applied industry scenarios. We benchmark with novel metrics from recent literature and other standard machine learning tools. Our results suggest some synthesizers are more applicable for different privacy budgets, and we further demonstrate complicating domain-based tradeoffs in selecting an approach. We offer experimental learning on applied machine learning scenarios with private internal data to researchers and practioners alike. In addition, we propose QUAIL, an ensemble-based modeling approach to generating synthetic data. We examine QUAIL's tradeoffs, and note circumstances in which it outperforms baseline differentially private supervised learning models under the same budget constraint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QI2DVSCP/Rosenblatt et al. - 2020 - Differentially Private Synthetic Data Applied Eva.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JKPHVKRK/2011.html}
}

@inproceedings{rothMarginBasedActiveLearning2006,
  title = {Margin-{{Based Active Learning}} for {{Structured Output Spaces}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Roth, Dan and Small, Kevin},
  editor = {F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {413--424},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11871842_40},
  abstract = {In many complex machine learning applications there is a need to learn multiple interdependent output variables, where knowledge of these interdependencies can be exploited to improve the global performance. Typically, these structured output scenarios are also characterized by a high cost associated with obtaining supervised training data, motivating the study of active learning for these situations. Starting with active learning approaches for multiclass classification, we first design querying functions for selecting entire structured instances, exploring the tradeoff between selecting instances based on a global margin or a combination of the margin of local classifiers. We then look at the setting where subcomponents of the structured instance can be queried independently and examine the benefit of incorporating structural information in such scenarios. Empirical results on both synthetic data and the semantic role labeling task demonstrate a significant reduction in the need for supervised training data when using the proposed methods.},
  isbn = {978-3-540-46056-5},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XG4E66ZL/Roth and Small - 2006 - Margin-Based Active Learning for Structured Output.pdf}
}

@article{Saleiro2018,
  title = {Aequitas: {{A}} Bias and Fairness Audit Toolkit},
  author = {Saleiro, Pedro and Kuester, Benedict and Stevens, Abby and Anisfeld, Ari and Hinkson, Loren and London, Jesse and Ghani, Rayid},
  year = {2018},
  journal = {arXiv preprint arXiv:1811.05577},
  eprint = {1811.05577},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@inproceedings{saleiroDealingBiasFairness2020,
  title = {Dealing with {{Bias}} and {{Fairness}} in {{Data Science Systems}}: {{A Practical Hands-on Tutorial}}},
  shorttitle = {Dealing with {{Bias}} and {{Fairness}} in {{Data Science Systems}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Saleiro, Pedro and Rodolfa, Kit T. and Ghani, Rayid},
  year = {2020},
  month = aug,
  series = {{{KDD}} '20},
  pages = {3513--3514},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3394486.3406708},
  abstract = {Tackling issues of bias and fairness when building and deploying data science systems has received increased attention from the research community in recent years, yet a lot of the research has focused on theoretical aspects and very limited set of application areas and data sets. There is a lack of 1) practical training materials, 2) methodologies, and 3) tools for researchers and developers working on real-world algorithmic decision making system to deal with issues of bias and fairness. Today, treating bias and fairness as primary metrics of interest, and building, selecting, and validating models using those metrics is not standard practice for data scientists. In this hands-on tutorial we will try to bridge the gap between research and practice, by deep diving into algorithmic fairness, from metrics and definitions to practical case studies, including bias audits using the Aequitas toolkit (http://github.com/dssg/aequitas). By the end of this hands-on tutorial, the audience will be familiar with bias mitigation frameworks and tools to help them making decisions during a project based on intervention and deployment contexts in which their system will be used.},
  isbn = {978-1-4503-7998-4},
  keywords = {ai ethics,algorithmic fairness,bias},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2EJCZPKX/Saleiro et al. - 2020 - Dealing with Bias and Fairness in Data Science Sys.pdf}
}

@misc{salimiCapuchinCausalDatabase2019a,
  title = {Capuchin: {{Causal Database Repair}} for {{Algorithmic Fairness}}},
  shorttitle = {Capuchin},
  author = {Salimi, Babak and Rodriguez, Luke and Howe, Bill and Suciu, Dan},
  year = {2019},
  month = oct,
  number = {arXiv:1902.08283},
  eprint = {1902.08283},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.08283},
  abstract = {Fairness is increasingly recognized as a critical component of machine learning systems. However, it is the underlying data on which these systems are trained that often reflect discrimination, suggesting a database repair problem. Existing treatments of fairness rely on statistical correlations that can be fooled by statistical anomalies, such as Simpson's paradox. Proposals for causality-based definitions of fairness can correctly model some of these situations, but they require specification of the underlying causal models. In this paper, we formalize the situation as a database repair problem, proving sufficient conditions for fair classifiers in terms of admissible variables as opposed to a complete causal model. We show that these conditions correctly capture subtle fairness violations. We then use these conditions as the basis for database repair algorithms that provide provable fairness guarantees about classifiers trained on their training labels. We evaluate our algorithms on real data, demonstrating improvement over the state of the art on multiple fairness metrics proposed in the literature while retaining high utility.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VDH7PHGS/Salimi et al. - 2019 - Capuchin Causal Database Repair for Algorithmic F.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5PL9MG54/1902.html}
}

@misc{SelectiveLabelsProblem,
  title = {The {{Selective Labels Problem}} | {{Proceedings}} of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  howpublished = {https://dl.acm.org/doi/10.1145/3097983.3098066},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WGMDATU8/3097983.html}
}

@phdthesis{senfumaMetaLearningSelection2011,
  type = {Thesis},
  title = {Meta Learning for Selection of Best Causal Discovery Algorithms.},
  author = {Senfuma, William},
  year = {2011},
  month = nov,
  abstract = {Selection of the best causal discovery algorithm for any new dataset is a difficult and time consuming process as it requires a researcher to have prior knowledge about a number of existing standard structure learning algorithms. During this research, we proposed a novel meta-learning approach to this problem. Meta-learning refers to learning about learning algorithms where different kinds of meta-data, such as properties of the learning problem, performance measures of different algorithms and patterns previously derived from the data are used to select the best or combine different learning algorithms to effectively solve a given learning problem. Several Bayesian networks in literature were manipulated, sampled to generate thousands of datasets, and specific features were extracted from each for meta-learning. Three standard structure learning algorithms were run on each of the generated datasets to discover the underlying causal networks and their performance was evaluated.  With our new techniques, we were able to implement a tool for generating of many causal models and sampling many datasets from each model. We were able to determine the best algorithm or a combination of algorithms for specific datasets based on features extracted from them.},
  langid = {english},
  school = {Makerere University},
  annotation = {Accepted: 2014-05-12T07:03:07Z}
}

@techreport{settlesActiveLearningLiterature2009,
  type = {Technical {{Report}}},
  title = {Active {{Learning Literature Survey}}},
  author = {Settles, Burr},
  year = {2009},
  institution = {{University of Wisconsin-Madison Department of Computer Sciences}},
  abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain.    This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
  langid = {english},
  annotation = {Accepted: 2012-03-15T17:23:56Z},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2H6JAS9Y/Settles - 2009 - Active Learning Literature Survey.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PW55QM86/60660.html}
}

@inproceedings{shakerAleatoricEpistemicUncertainty2020,
  title = {Aleatoric and {{Epistemic Uncertainty}} with {{Random Forests}}},
  booktitle = {Advances in {{Intelligent Data Analysis XVIII}}},
  author = {Shaker, Mohammad Hossein and H{\"u}llermeier, Eyke},
  editor = {Berthold, Michael R. and Feelders, Ad and Krempl, Georg},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {444--456},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-44584-3_35},
  abstract = {Due to the steadily increasing relevance of machine learning for practical applications, many of which are coming with safety requirements, the notion of uncertainty has received increasing attention in machine learning research in the last couple of years. In particular, the idea of distinguishing between two important types of uncertainty, often refereed to as aleatoric and epistemic, has recently been studied in the setting of supervised learning. In this paper, we propose to quantify these uncertainties, referring, respectively, to inherent randomness and a lack of knowledge, with random forests. More specifically, we show how two general approaches for measuring the learner's aleatoric and epistemic uncertainty in a prediction can be instantiated with decision trees and random forests as learning algorithms in a classification setting. In this regard, we also compare random forests with deep neural networks, which have been used for a similar purpose.},
  isbn = {978-3-030-44584-3},
  langid = {english},
  keywords = {Machine learning,Random forest,Uncertainty},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CCTFGBAX/Shaker and Hllermeier - 2020 - Aleatoric and Epistemic Uncertainty with Random Fo.pdf}
}

@misc{shanbhagUnifiedShapleyFramework2021a,
  title = {Unified {{Shapley Framework}} to {{Explain Prediction Drift}}},
  author = {Shanbhag, Aalok and Ghosh, Avijit and Rubin, Josh},
  year = {2021},
  month = feb,
  number = {arXiv:2102.07862},
  eprint = {2102.07862},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.07862},
  abstract = {Predictions are the currency of a machine learning model, and to understand the model's behavior over segments of a dataset, or over time, is an important problem in machine learning research and practice. There currently is no systematic framework to understand this drift in prediction distributions over time or between two semantically meaningful slices of data, in terms of the input features and points. We propose GroupShapley and GroupIG (Integrated Gradients), as axiomatically justified methods to tackle this problem. In doing so, we re-frame all current feature/data importance measures based on the Shapley value as essentially problems of distributional comparisons, and unify them under a common umbrella. We axiomatize certain desirable properties of distributional difference, and study the implications of choosing them empirically.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Q3HRLSJT/Shanbhag et al. - 2021 - Unified Shapley Framework to Explain Prediction Dr.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/U4Q8Y6G8/2102.html}
}

@inproceedings{sharafPromotingFairnessLearned2022,
  title = {Promoting {{Fairness}} in {{Learned Models}} by {{Learning}} to {{Active Learn}} under {{Parity Constraints}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Sharaf, Amr and Daume III, Hal and Ni, Renkun},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {2149--2156},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3531146.3534632},
  abstract = {Machine learning models can have consequential effects when used to automate decisions, and disparities between groups of people in the error rates of those decisions can lead to harms suffered more by some groups than others. Past algorithmic approaches aim to enforce parity across groups given a fixed set of training data; instead, we ask: what if we can gather more data to mitigate disparities? We develop a meta-learning algorithm for parity-constrained active learning that learns a policy to decide which labels to query so as to maximize accuracy subject to parity constraints. To optimize the active learning policy, our proposed algorithm formulates the parity-constrained active learning task as a bi-level optimization problem. The inner level corresponds to training a classifier on a subset of labeled examples. The outer level corresponds to updating the selection policy choosing this subset to achieve a desired fairness and accuracy behavior on the trained classifier. To solve this constrained bi-level optimization problem, we employ the Forward-Backward Splitting optimization method. Empirically, across several parity metrics and classification tasks, our approach outperforms alternatives by a large margin.},
  isbn = {978-1-4503-9352-2},
  keywords = {active learning,meta-learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/UYHH84ER/Sharaf et al. - 2022 - Promoting Fairness in Learned Models by Learning t.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XXSDGWUG/Sharaf - 2020 - Promoting Fairness in Learned Models by Learning t}
}

@inproceedings{sharchilevFindingInfluentialTraining2018,
  title = {Finding {{Influential Training Samples}} for {{Gradient Boosted Decision Trees}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Sharchilev, Boris and Ustinovskiy, Yury and Serdyukov, Pavel and Rijke, Maarten},
  year = {2018},
  month = jul,
  pages = {4577--4585},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model's predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/P79CE7WN/Sharchilev et al. - 2018 - Finding Influential Training Samples for Gradient .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/W69Q9GDA/Sharchilev et al. - 2018 - Finding Influential Training Samples for Gradient .pdf}
}

@article{sharmaDataAugmentationDiscrimination2020,
  title = {Data {{Augmentation}} for {{Discrimination Prevention}} and {{Bias Disambiguation}}},
  author = {Sharma, Shubham and Zhang, Yunfeng and Aliaga, Jes{\'u}s M R{\'i}os and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R},
  year = {2020},
  journal = {New York},
  pages = {7},
  abstract = {Machine learning models are prone to biased decisions due to biases in the datasets they are trained on. In this paper, we introduce a novel data augmentation technique to create a fairer dataset for model training that could also lend itself to understanding the type of bias existing in the dataset i.e. if bias arises from a lack of representation for a particular group (sampling bias) or if it arises because of human bias reflected in the labels (prejudice based bias). Given a dataset involving a protected attribute with a privileged and unprivileged group, we create an ``ideal world'' dataset: for every data sample, we create a new sample having the same features (except the protected attribute(s)) and label as the original sample but with the opposite protected attribute value. The synthetic data points are sorted in order of their proximity to the original training distribution and added successively to the real dataset to create intermediate datasets. We theoretically show that two different notions of fairness: statistical parity difference (independence) and average odds difference (separation) always change in the same direction using such an augmentation. We also show submodularity of the proposed fairness-aware augmentation approach that enables an efficient greedy algorithm. We empirically study the effect of training models on the intermediate datasets and show that this technique reduces the two bias measures while keeping the accuracy nearly constant for three datasets. We then discuss the implications of this study on the disambiguation of sample bias and prejudice based bias and discuss how pre-processing techniques should be evaluated in general. The proposed method can be used by policy makers\textemdash who want to use unbiased datasets to train machine learning models for their applications\textemdash to add a subset of synthetic points to an extent that they are comfortable with to mitigate unwanted bias.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HHZQZKJM/Sharma et al. - 2020 - Data Augmentation for Discrimination Prevention an.pdf}
}

@misc{sharmaMetaCIMetaLearningCausal2021,
  title = {{{MetaCI}}: {{Meta-Learning}} for {{Causal Inference}} in a {{Heterogeneous Population}}},
  shorttitle = {{{MetaCI}}},
  author = {Sharma, Ankit and Gupta, Garima and Prasad, Ranjitha and Chatterjee, Arnab and Vig, Lovekesh and Shroff, Gautam},
  year = {2021},
  month = feb,
  number = {arXiv:1912.03960},
  eprint = {1912.03960},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Performing inference on data obtained through observational studies is becoming extremely relevant due to the widespread availability of data in fields such as healthcare, education, retail, etc. Furthermore, this data is accrued from multiple homogeneous subgroups of a heterogeneous population, and hence, generalizing the inference mechanism over such data is essential. We propose the MetaCI framework with the goal of answering counterfactual questions in the context of causal inference (CI), where the factual observations are obtained from several homogeneous subgroups. While the CI network is designed to generalize from factual to counterfactual distribution in order to tackle covariate shift, MetaCI employs the meta-learning paradigm to tackle the shift in data distributions between training and test phase due to the presence of heterogeneity in the population, and due to drifts in the target distribution, also known as concept shift. We benchmark the performance of the MetaCI algorithm using the mean absolute percentage error over the average treatment effect as the metric, and demonstrate that meta initialization has significant gains compared to randomly initialized networks, and other methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4CN6L2PX/Sharma et al. - 2021 - MetaCI Meta-Learning for Causal Inference in a He.pdf}
}

@inproceedings{sheikhalishahiPrivacyUtilityFeatureSelection2018,
  title = {Privacy-{{Utility Feature Selection}} as a Tool in {{Private Data Classification}}},
  booktitle = {Distributed {{Computing}} and {{Artificial Intelligence}}, 14th {{International Conference}}},
  author = {Sheikhalishahi, Mina and Martinelli, Fabio},
  editor = {Omatu, Sigeru and Rodr{\'i}guez, Sara and Villarrubia, Gabriel and Faria, Pedro and Sitek, Pawe{\l} and Prieto, Javier},
  year = {2018},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {254--261},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-62410-5_31},
  abstract = {This paper presents a novel framework for privacy aware collaborative information sharing for data classification. Two data holders participating in this information sharing system, for global benefits are interested to model a classifier on whole dataset, if a certain amount of privacy is guaranteed. To address this issue, we propose a privacy mechanism approach based on privacy-utility feature selection, which by eliminating the most irrelevant set of features in terms of accuracy and privacy, guarantees the privacy requirements of data providers, whilst the data remain practically useful for classification. Due to the fact that the proposed trade-off metric is required to be exploited on whole dataset, secure weighted average protocol is utilized to protect information leakage in each site.},
  isbn = {978-3-319-62410-5},
  langid = {english}
}

@inproceedings{shekharAdaptiveSamplingMinimax2021,
  title = {Adaptive {{Sampling}} for {{Minimax Fair Classification}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shekhar, Shubhanshu and Fields, Greg and Ghavamzadeh, Mohammad and Javidi, Tara},
  year = {2021},
  volume = {34},
  pages = {24535--24544},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Machine learning models trained on uncurated datasets can often end up adversely affecting inputs belonging to underrepresented groups. To address this issue, we consider the problem of adaptively constructing training sets which allow us to learn classifiers that are fair in a \{\textbackslash em minimax\} sense. We first propose an adaptive sampling algorithm based on the principle of \textbackslash emph\{optimism\}, and derive theoretical bounds on its performance. We also propose heuristic extensions of this algorithm suitable for application to large scale, practical problems. Next, by deriving algorithm independent lower-bounds for a specific class of problems, we show that the performance achieved by our adaptive scheme cannot be improved in general. We then validate the benefits of adaptively constructing training sets via experiments on synthetic tasks with logistic regression classifiers, as well as on several real-world tasks using convolutional neural networks (CNNs).},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/59YNP5W7/Shekhar et al. - 2021 - Adaptive Sampling for Minimax Fair Classification.pdf}
}

@inproceedings{shenMetricFairActiveLearning2022,
  title = {Metric-{{Fair Active Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Shen, Jie and Cui, Nan and Wang, Jing},
  year = {2022},
  month = jun,
  pages = {19809--19826},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Active learning has become a prevalent technique for designing label-efficient algorithms, where the central principle is to only query and fit ``informative'' labeled instances. It is, however, known that an active learning algorithm may incur unfairness due to such instance selection procedure. In this paper, we henceforth study metric-fair active learning of homogeneous halfspaces, and show that under the distribution-dependent PAC learning model, fairness and label efficiency can be achieved simultaneously. We further propose two extensions of our main results: 1) we show that it is possible to make the algorithm robust to the adversarial noise~\textendash ~one of the most challenging noise models in learning theory; and 2) it is possible to significantly improve the label complexity when the underlying halfspace is sparse.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/KV93Y6K7/Shen et al. - 2022 - Metric-Fair Active Learning.pdf}
}

@article{shwartz-zivTabularDataDeep2022,
  title = {Tabular Data: {{Deep}} Learning Is Not All You Need},
  shorttitle = {Tabular Data},
  author = {{Shwartz-Ziv}, Ravid and Armon, Amitai},
  year = {2022},
  month = may,
  journal = {Information Fusion},
  volume = {81},
  pages = {84--90},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.11.011},
  abstract = {A key element in solving real-life data science problems is selecting the types of models to use. Tree ensemble models (such as XGBoost) are usually recommended for classification and regression problems with tabular data. However, several deep learning models for tabular data have recently been proposed, claiming to outperform XGBoost for some use cases. This paper explores whether these deep models should be a recommended option for tabular data by rigorously comparing the new deep models to XGBoost on various datasets. In addition to systematically comparing their performance, we consider the tuning and computation they require. Our study shows that XGBoost outperforms these deep models across the datasets, including the datasets used in the papers that proposed the deep models. We also demonstrate that XGBoost requires much less tuning. On the positive side, we show that an ensemble of deep models and XGBoost performs better on these datasets than XGBoost alone.},
  langid = {english},
  keywords = {Deep neural networks,Hyperparameter optimization,Tabular data,Tree-based models},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NXK4TEDB/Shwartz-Ziv and Armon - 2022 - Tabular data Deep learning is not all you need.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BRRRUY7E/S1566253521002360.html}
}

@inproceedings{simDataValuationMachine2022,
  title = {Data {{Valuation}} in {{Machine Learning}}: \&quot;{{Ingredients}}\&quot;, {{Strategies}}, and {{Open Challenges}}},
  shorttitle = {Data {{Valuation}} in {{Machine Learning}}},
  booktitle = {Thirty-{{First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Sim, Rachael Hwee Ling and Xu, Xinyi and Low, Bryan Kian Hsiang},
  year = {2022},
  month = jul,
  volume = {6},
  pages = {5607--5614},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2022/782},
  abstract = {Electronic proceedings of IJCAI 2022},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FIQP4YXQ/Sim et al. - 2022 - Data Valuation in Machine Learning &quot\;Ingredie.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2HL2KTZE/782.html}
}

@article{simpsons-paradox,
  title = {Simpson's {{Paradox}} in {{Real Life}}},
  author = {Wagner, Clifford H},
  year = {1982},
  journal = {The American Statistician},
  volume = {36},
  number = {1},
  pages = {46--48},
  publisher = {{Taylor \& Francis}}
}

@misc{slackFairMetaLearningLearning2019,
  title = {Fair {{Meta-Learning}}: {{Learning How}} to {{Learn Fairly}}},
  shorttitle = {Fair {{Meta-Learning}}},
  author = {Slack, Dylan and Friedler, Sorelle and Givental, Emile},
  year = {2019},
  month = nov,
  number = {arXiv:1911.04336},
  eprint = {1911.04336},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.04336},
  abstract = {Data sets for fairness relevant tasks can lack examples or be biased according to a specific label in a sensitive attribute. We demonstrate the usefulness of weight based meta-learning approaches in such situations. For models that can be trained through gradient descent, we demonstrate that there are some parameter configurations that allow models to be optimized from a few number of gradient steps and with minimal data which are both fair and accurate. To learn such weight sets, we adapt the popular MAML algorithm to Fair-MAML by the inclusion of a fairness regularization term. In practice, Fair-MAML allows practitioners to train fair machine learning models from only a few examples when data from related tasks is available. We empirically exhibit the value of this technique by comparing to relevant baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JPL9FYZN/Slack et al. - 2019 - Fair Meta-Learning Learning How to Learn Fairly.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FCS7QR8L/1911.html}
}

@inproceedings{slackFairnessWarningsFairMAML2020,
  title = {Fairness Warnings and Fair-{{MAML}}: Learning Fairly with Minimal Data},
  shorttitle = {Fairness Warnings and Fair-{{MAML}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Slack, Dylan and Friedler, Sorelle A. and Givental, Emile},
  year = {2020},
  month = jan,
  series = {{{FAT}}* '20},
  pages = {200--209},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372839},
  abstract = {Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a model-agnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.},
  isbn = {978-1-4503-6936-7},
  keywords = {covariate shift,fairness,machine learning,meta-learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/EHXMKFR7/Slack et al. - 2020 - Fairness warnings and fair-MAML learning fairly w.pdf}
}

@misc{slowikAlgorithmicBiasData2021,
  title = {Algorithmic {{Bias}} and {{Data Bias}}: {{Understanding}} the {{Relation}} between {{Distributionally Robust Optimization}} and {{Data Curation}}},
  shorttitle = {Algorithmic {{Bias}} and {{Data Bias}}},
  author = {S{\l}owik, Agnieszka and Bottou, L{\'e}on},
  year = {2021},
  month = jun,
  number = {arXiv:2106.09467},
  eprint = {2106.09467},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09467},
  abstract = {Machine learning systems based on minimizing average error have been shown to perform inconsistently across notable subsets of the data, which is not exposed by a low average error for the entire dataset. In consequential social and economic applications, where data represent people, this can lead to discrimination of underrepresented gender and ethnic groups. Given the importance of bias mitigation in machine learning, the topic leads to contentious debates on how to ensure fairness in practice (data bias versus algorithmic bias). Distributionally Robust Optimization (DRO) seemingly addresses this problem by minimizing the worst expected risk across subpopulations. We establish theoretical results that clarify the relation between DRO and the optimization of the same loss averaged on an adequately weighted training dataset. The results cover finite and infinite number of training distributions, as well as convex and non-convex loss functions. We show that neither DRO nor curating the training set should be construed as a complete solution for bias mitigation: in the same way that there is no universally robust training set, there is no universal way to setup a DRO problem and ensure a socially acceptable set of results. We then leverage these insights to provide a mininal set of practical recommendations for addressing bias with DRO. Finally, we discuss ramifications of our results in other related applications of DRO, using an example of adversarial robustness. Our results show that there is merit to both the algorithm-focused and the data-focused side of the bias debate, as long as arguments in favor of these positions are precisely qualified and backed by relevant mathematics known today.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QLKCGQA8/Sowik and Bottou - 2021 - Algorithmic Bias and Data Bias Understanding the .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZBGTLPIM/2106.html}
}

@inproceedings{speicherUnifiedApproachQuantifying2018,
  title = {A {{Unified Approach}} to {{Quantifying Algorithmic Unfairness}}: {{Measuring Individual}} \&{{Group Unfairness}} via {{Inequality Indices}}},
  shorttitle = {A {{Unified Approach}} to {{Quantifying Algorithmic Unfairness}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Speicher, Till and Heidari, Hoda and {Grgic-Hlaca}, Nina and Gummadi, Krishna P. and Singla, Adish and Weller, Adrian and Zafar, Muhammad Bilal},
  year = {2018},
  month = jul,
  series = {{{KDD}} '18},
  pages = {2239--2248},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3219819.3220046},
  abstract = {Discrimination via algorithmic decision making has received considerable attention. Prior work largely focuses on defining conditions for fairness, but does not define satisfactory measures of algorithmic unfairness. In this paper, we focus on the following question: Given two unfair algorithms, how should we determine which of the two is more unfair? Our core idea is to use existing inequality indices from economics to measure how unequally the outcomes of an algorithm benefit different individuals or groups in a population. Our work offers a justified and general framework to compare and contrast the (un)fairness of algorithmic predictors. This unifying approach enables us to quantify unfairness both at the individual and the group level. Further, our work reveals overlooked tradeoffs between different fairness notions: using our proposed measures, the overall individual-level unfairness of an algorithm can be decomposed into a between-group and a within-group component. Earlier methods are typically designed to tackle only between-group un- fairness, which may be justified for legal or other reasons. However, we demonstrate that minimizing exclusively the between-group component may, in fact, increase the within-group, and hence the overall unfairness. We characterize and illustrate the tradeoffs between our measures of (un)fairness and the prediction accuracy.},
  isbn = {978-1-4503-5552-0},
  keywords = {algorithmic decision making,fairness in machine learning,fairness measures,generalized entropy,group fairness,individual fairness,inequality indices,subgroup decomposability},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/M3HJ7C5V/Speicher et al. - 2018 - A Unified Approach to Quantifying Algorithmic Unfa.pdf}
}

@article{stevensonAssessingRiskAssessment2018,
  title = {Assessing {{Risk Assessment}} in {{Action}}},
  author = {Stevenson, Megan},
  year = {2018},
  journal = {Minnesota Law Review},
  volume = {103},
  pages = {303},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/K99WTN7K/LandingPage.html}
}

@article{stiglerEconomicsMinimumWage1946,
  title = {The {{Economics}} of {{Minimum Wage Legislation}}},
  author = {Stigler, George J.},
  year = {1946},
  journal = {The American Economic Review},
  volume = {36},
  number = {3},
  pages = {358--365},
  publisher = {{American Economic Association}},
  issn = {0002-8282}
}

@article{sunBackfireEffectsFairness,
  title = {The {{Backfire Effects}} of {{Fairness Constraints}}},
  author = {Sun, Yi and {Cuesta-Infante}, Alfredo and Veeramachaneni, Kalyan},
  pages = {9},
  abstract = {Recently the fairness community has shifted from achieving one-shot fair decisions to striving for long-term fairness. In this work, we propose a metric to measure the long-term impact of a policy on the target variable distributions. We theoretically characterize the conditions under which threshold policies could lead to a backfire effect. We conduct experiments with a set of well-used fairness constraints on both synthetic and realworld datasets.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8ZFTTSZH/Sun et al. - The Backre Effects of Fairness Constraints.pdf}
}

@inproceedings{sureshFrameworkUnderstandingSources2021,
  title = {A {{Framework}} for {{Understanding Sources}} of {{Harm}} throughout the {{Machine Learning Life Cycle}}},
  booktitle = {Equity and {{Access}} in {{Algorithms}}, {{Mechanisms}}, and {{Optimization}}},
  author = {Suresh, Harini and Guttag, John V.},
  year = {2021},
  month = oct,
  eprint = {1901.10002},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {1--9},
  doi = {10.1145/3465416.3483305},
  abstract = {As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/UN5ZPZQA/Suresh and Guttag - 2021 - A Framework for Understanding Sources of Harm thro.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/UJFGB636/1901.html}
}

@inproceedings{suteraGlobalLocalMDI2021a,
  title = {From Global to Local {{MDI}} Variable Importances for Random Forests and When They Are {{Shapley}} Values},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutera, Antonio and Louppe, Gilles and {Huynh-Thu}, Van Anh and Wehenkel, Louis and Geurts, Pierre},
  year = {2021},
  volume = {34},
  pages = {3533--3543},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Random forests have been widely used for their ability to provide so-called importance measures, which give insight at a global (per dataset) level on the relevance of input variables to predict a certain output. On the other hand, methods based on Shapley values have been introduced to refine the analysis of feature relevance in tree-based models to a local (per instance) level. In this context, we first show that the global Mean Decrease of Impurity (MDI) variable importance scores correspond to Shapley values under some conditions. Then, we derive a local MDI importance measure of variable relevance, which has a very natural connection with the global MDI measure and can be related to a new notion of local feature relevance. We further link local MDI importances with Shapley values and discuss them in the light of related measures from the literature. The measures are illustrated through experiments on several classification and regression problems.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/HEQ7IFBZ/Sutera et al. - 2021 - From global to local MDI variable importances for .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZWIY4ISE/Sutera et al. - 2021 - From global to local MDI variable importances for .pdf}
}

@misc{taoBenchmarkingDifferentiallyPrivate2022,
  title = {Benchmarking {{Differentially Private Synthetic Data Generation Algorithms}}},
  author = {Tao, Yuchao and McKenna, Ryan and Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome},
  year = {2022},
  month = feb,
  number = {arXiv:2112.09238},
  eprint = {2112.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.09238},
  abstract = {This work presents a systematic benchmark of differentially private synthetic data generation algorithms that can generate tabular data. Utility of the synthetic data is evaluated by measuring whether the synthetic data preserve the distribution of individual and pairs of attributes, pairwise correlation as well as on the accuracy of an ML classification model. In a comprehensive empirical evaluation we identify the top performing algorithms and those that consistently fail to beat baseline approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2QYQWV22/Tao et al. - 2022 - Benchmarking Differentially Private Synthetic Data.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6H35TVPT/2112.html}
}

@article{tarekegnReviewMethodsImbalanced2021a,
  title = {A Review of Methods for Imbalanced Multi-Label Classification},
  author = {Tarekegn, Adane Nega and Giacobini, Mario and Michalak, Krzysztof},
  year = {2021},
  month = oct,
  journal = {Pattern Recognition},
  volume = {118},
  pages = {107965},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2021.107965},
  abstract = {Multi-Label Classification (MLC) is an extension of the standard single-label classification where each data instance is associated with several labels simultaneously. MLC has gained much importance in recent years due to its wide range of application domains. However, the class imbalance problem has become an inherent characteristic of many multi-label datasets, where the samples and their corresponding labels are non-uniformly distributed over the data space. The imbalanced problem in MLC imposes challenges to multi-label data analytics which can be viewed from three perspectives: imbalance within labels, among labels, and label-sets. In this paper, we provide a review of the approaches for handling the imbalance problem in multi-label data by collecting the existing research work. As the first systematic study of approaches addressing an imbalanced problem in MLC, this paper provides a comprehensive survey of the state-of-the-art methods for imbalanced MLC, including the characteristics of imbalanced multi-label datasets, evaluation measures and comparative analysis of the proposed methods. The study also discusses important results reported so far in the literature and highlights some of their strengths and limitations to guide future research.},
  langid = {english},
  keywords = {Imbalanced Approaches,Imbalanced Classification,Imbalanced Data,Machine learning,Multi-label Classification,Review on Imbalanced Classification},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VE37C2K4/Tarekegn et al. - 2021 - A review of methods for imbalanced multi-label cla.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DSXW8MZ8/S0031320321001527.html}
}

@article{thistlethwaiteRegressiondiscontinuityAnalysisAlternative1960,
  title = {Regression-Discontinuity Analysis: {{An}} Alternative to the Ex Post Facto Experiment},
  shorttitle = {Regression-Discontinuity Analysis},
  author = {Thistlethwaite, Donald L. and Campbell, Donald T.},
  year = {1960},
  journal = {Journal of Educational Psychology},
  volume = {51},
  number = {6},
  pages = {309--317},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2176},
  doi = {10.1037/h0044319},
  abstract = {This study presents a method of testing casual hypotheses, called regression-discontinuity analysis, in situations where the investigator is unable to randomly assign Ss to experimental and control groups. The Ss were selected from near winners\textemdash 5126 students who received certificates of merit and 2848 students who merely received letters of commendation. Comparison of the results obtained from the new mode of analysis with those obtained when the ex post facto design was applied to the same data. The new analysis suggested that public recognition for achievement tends to increase the likelihood that the recipient will receive a scholarship but did not support the inference that recognition affects the student's attitudes and career plans. From Psyc Abstracts 36:01:1AF09T. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Academic Achievement,Causal Analysis,Elementary School Students,Experimentation,Occupational Choice,Statistical Regression,Student Attitudes,Testing},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/CGSSRSNN/1962-00061-001.html}
}

@article{tianNewApproachReject2018b,
  title = {A New Approach for Reject Inference in Credit Scoring Using Kernel-Free Fuzzy Quadratic Surface Support Vector Machines},
  author = {Tian, Ye and Yong, Ziyang and Luo, Jian},
  year = {2018},
  month = dec,
  journal = {Applied Soft Computing},
  volume = {73},
  pages = {96--105},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2018.08.021},
  abstract = {Credit scoring models have offered benefits to lenders and borrowers for many years. However, in practice these models are normally built on a sample of accepted applicants and fail to consider the remaining rejected applicants. This may cause a sample bias which is an important statistical issue, especially in the online lending situation where a large proportion of requests are rejected. Reject inference is a method for inferring how rejected applicants would have behaved if they had been granted and incorporating this information in rebuilding a more accurate credit scoring system. Due to the good performances of SVM models in this area, this paper proposes a new approach based on the state-of-the-art kernel-free fuzzy quadratic surface SVM model. It is worth pointing out that our method not only performs very well in classification as some latest works, but also handles some big issues in the classical SVM models, such as searching proper kernel functions and solving complex models. Besides, this paper is the first one to eliminate the bad effect of outliers in credit scoring. Moreover, we use two real-world loan data sets to compare our method with some benchmark methods. Particularly, one of the data set is very valuable for the study of reject inference, because the outcomes of rejected applicants are partially known. Finally, the numerical results strongly demonstrate the superiority of the proposed method in applicability, accuracy and efficiency.},
  langid = {english},
  keywords = {Credit scoring,Kernel-free quadratic surface SVM,Online lending,Outlier detection,Reject inference},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VDRU2KMQ/S1568494618304812.html}
}

@inproceedings{tomasevFairnessUnobservedCharacteristics2021,
  title = {Fairness for {{Unobserved Characteristics}}: {{Insights}} from {{Technological Impacts}} on {{Queer Communities}}},
  shorttitle = {Fairness for {{Unobserved Characteristics}}},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Tomasev, Nenad and McKee, Kevin R. and Kay, Jackie and Mohamed, Shakir},
  year = {2021},
  month = jul,
  pages = {254--265},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3461702.3462540},
  abstract = {Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness\textemdash frequently, race and legal gender\textemdash can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics.},
  isbn = {978-1-4503-8473-5},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SAZU96Q5/Tomasev et al. - 2021 - Fairness for Unobserved Characteristics Insights .pdf}
}

@article{tonMetaLearningCausal2021a,
  title = {Meta {{Learning}} for {{Causal Direction}}},
  author = {Ton, Jean-Fran{\c c}ois and Sejdinovic, Dino and Fukumizu, Kenji},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {11},
  pages = {9897--9905},
  issn = {2374-3468},
  abstract = {The inaccessibility of controlled randomized trials due to inherent constraints in many fields of science has been a fundamental issue in causal inference. In this paper, we focus on distinguishing the cause from effect in the bivariate setting under limited observational data. Based on recent developments in meta learning as well as in causal inference, we introduce a novel generative model that allows distinguishing cause and effect in the small data setting. Using a learnt task variable that contains distributional information of each dataset, we propose an end-to-end algorithm that makes use of similar training datasets at test time. We demonstrate our method on various synthetic as well as real-world data and show that it is able to maintain high accuracy in detecting directions across varying dataset sizes.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Kernel Methods,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6MXWIRMY/Ton et al. - 2021 - Meta Learning for Causal Direction.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/X7XN2IH9/Ton et al. - 2021 - Meta Learning for Causal Direction.pdf}
}

@article{tradeoffs-corbett2018measure,
  title = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}: {{A Critical Review}} of {{Fair Machine Learning}}},
  author = {{Corbett-Davies}, Sam and Goel, Sharad},
  year = {2018},
  journal = {arXiv preprint arXiv:1808.00023},
  eprint = {1808.00023},
  eprinttype = {arxiv},
  archiveprefix = {arXiv}
}

@article{tradeoffs-feller2016computer,
  title = {A Computer Program Used for Bail and Sentencing Decisions Was Labeled Biased against Blacks. {{It}}'s Actually Not That Clear},
  author = {Feller, Avi and Pierson, Emma and {Corbett-Davies}, Sam and Goel, Sharad},
  year = {2016},
  journal = {The Washington Post},
  volume = {17}
}

@inproceedings{tramerFairTestDiscoveringUnwarranted2017a,
  title = {{{FairTest}}: {{Discovering Unwarranted Associations}} in {{Data-Driven Applications}}},
  shorttitle = {{{FairTest}}},
  booktitle = {2017 {{IEEE European Symposium}} on {{Security}} and {{Privacy}} ({{EuroS}}\&{{P}})},
  author = {Tram{\`e}r, Florian and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Hubaux, Jean-Pierre and Humbert, Mathias and Juels, Ari and Lin, Huang},
  year = {2017},
  month = apr,
  pages = {401--416},
  doi = {10.1109/EuroSP.2017.29},
  abstract = {In a world where traditional notions of privacy are increasingly challenged by the myriad companies that collect and analyze our data, it is important that decision-making entities are held accountable for unfair treatments arising from irresponsible data usage. Unfortunately, a lack of appropriate methodologies and tools means that even identifying unfair or discriminatory effects can be a challenge in practice. We introduce the unwarranted associations (UA) framework, a principled methodology for the discovery of unfair, discriminatory, or offensive user treatment in data-driven applications. The UA framework unifies and rationalizes a number of prior attempts at formalizing algorithmic fairness. It uniquely combines multiple investigative primitives and fairness metrics with broad applicability, granular exploration of unfair treatment in user subgroups, and incorporation of natural notions of utility that may account for observed disparities. We instantiate the UA framework in FairTest, the first comprehensive tool that helps developers check data-driven applications for unfair user treatment. It enables scalable and statistically rigorous investigation of associations between application outcomes (such as prices or premiums) and sensitive user attributes (such as race or gender). Furthermore, FairTest provides debugging capabilities that let programmers rule out potential confounders for observed unfair effects. We report on use of FairTest to investigate and in some cases address disparate impact, offensive labeling, and uneven rates of algorithmic error in four data-driven applications. As examples, our results reveal subtle biases against older populations in the distribution of error in a predictive health application and offensive racial labeling in an image tagger.},
  keywords = {Algorithmic Fairness,Computer bugs,Google,Machine learning algorithms,Measurement,Medical services,Statistics,Systems,Testing,Tools},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I5E8PWV4/Tramr et al. - 2017 - FairTest Discovering Unwarranted Associations in .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/R462BZWK/7961993.html}
}

@misc{UCIMachineLearning,
  title = {{{UCI Machine Learning Repository}}: {{Adult Data Set}}},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/adult},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DDW4UGN4/adult.html}
}

@inproceedings{ustunFairnessHarmDecoupled2019,
  title = {Fairness without {{Harm}}: {{Decoupled Classifiers}} with {{Preference Guarantees}}},
  shorttitle = {Fairness without {{Harm}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Ustun, Berk and Liu, Yang and Parkes, David},
  year = {2019},
  month = may,
  pages = {6373--6382},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In domains such as medicine, it can be acceptable for machine learning models to include sensitive attributes such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disparity, then it should be in the best interest of each group. Drawing on ethical principles such as beneficence ("do the best") and non-maleficence ("do no harm"), we show how to use sensitive attributes to train decoupled classifiers that satisfy preference guarantees. These guarantees ensure the majority of individuals in each group prefer their assigned classifier to (i) a pooled model that ignores group membership (rationality), and (ii) the model assigned to any other group (envy-freeness). We introduce a recursive procedure that adaptively selects group attributes for decoupling, and present formal conditions to ensure preference guarantees in terms of generalization error. We validate the effectiveness of the procedure on real-world datasets, showing that it improves accuracy without violating preference guarantees on test data.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9U5PW78X/Ustun et al. - 2019 - Fairness without Harm Decoupled Classifiers with .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/DQGECRFR/Ustun et al. - 2019 - Fairness without Harm Decoupled Classifiers with .pdf}
}

@misc{vadhanOpenDPOpenSourceSuite2019a,
  title = {{{OpenDP}} : {{An Open-Source Suite}} of {{Differential Privacy Tools}}},
  shorttitle = {{{OpenDP}}},
  author = {Vadhan, S. and Crosas, M. and Honaker, James},
  year = {2019},
  abstract = {This platform, OpenDP, aims for this platform to become the standard body of trusted and open-source implementations of differentially private algorithms for statistical analysis and machine learning on sensitive data, and a pathway that rapidly brings the newest algorithmic developments to a wide array of practitioners. Project Goal We propose to lead a community effort to build a system of tools for enabling privacy-protective analysis of sensitive personal data, focused on an open-source library of algorithms for generating differentially private statistical releases. We aim for this platform, OpenDP, to become the standard body of trusted and open-source implementations of differentially private algorithms for statistical analysis and machine learning on sensitive data, and a pathway that rapidly brings the newest algorithmic developments to a wide array of practitioners.},
  howpublished = {https://www.semanticscholar.org/paper/OpenDP-\%3A-An-Open-Source-Suite-of-Differential-Tools-Vadhan-Crosas/853f3d2673078c0fe7d16b04c7eae307905eb22b},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5JTBV38X/853f3d2673078c0fe7d16b04c7eae307905eb22b.html}
}

@misc{vanschorenMetaLearningSurvey2018,
  title = {Meta-{{Learning}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}}},
  author = {Vanschoren, Joaquin},
  year = {2018},
  month = oct,
  number = {arXiv:1810.03548},
  eprint = {1810.03548},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4SWXEY5N/Vanschoren - 2018 - Meta-Learning A Survey.pdf}
}

@inproceedings{vealeFairnessAccountabilityDesign2018,
  title = {Fairness and {{Accountability Design Needs}} for {{Algorithmic Support}} in {{High-Stakes Public Sector Decision-Making}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Veale, Michael and Van Kleek, Max and Binns, Reuben},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3173574.3174014},
  abstract = {Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications.},
  isbn = {978-1-4503-5620-6},
  keywords = {algorithmic accountability,algorithmic bias,decision-support,predictive policing,public administration},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZB5KQU9Q/Veale et al. - 2018 - Fairness and Accountability Design Needs for Algor.pdf}
}

@article{vergaraReviewFeatureSelection2014,
  title = {A Review of Feature Selection Methods Based on Mutual Information},
  author = {Vergara, Jorge R. and Est{\'e}vez, Pablo A.},
  year = {2014},
  month = jan,
  journal = {Neural Computing and Applications},
  volume = {24},
  number = {1},
  pages = {175--186},
  issn = {1433-3058},
  doi = {10.1007/s00521-013-1368-0},
  abstract = {In this work, we present a review of the state of the art of information-theoretic feature selection methods. The concepts of feature relevance, redundance, and complementarity (synergy) are clearly defined, as well as Markov blanket. The problem of optimal feature selection is defined. A unifying theoretical framework is described, which can retrofit successful heuristic criteria, indicating the approximations made by each method. A number of open problems in the field are presented.},
  langid = {english},
  keywords = {Complementarity,Feature selection,Markov blanket,Mutual information,Redundancy,Relevance,Sinergy},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3ZFGC7IH/Vergara and Estvez - 2014 - A review of feature selection methods based on mut.pdf}
}

@inproceedings{vermaFairnessDefinitionsExplained2018,
  title = {Fairness Definitions Explained},
  booktitle = {Proceedings of the {{International Workshop}} on {{Software Fairness}}},
  author = {Verma, Sahil and Rubin, Julia},
  year = {2018},
  month = may,
  series = {{{FairWare}} '18},
  pages = {1--7},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3194770.3194776},
  abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
  isbn = {978-1-4503-5746-3},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IGR3EQ3U/Verma and Rubin - 2018 - Fairness definitions explained.pdf}
}

@misc{vermaRemovingBiasedData2021,
  title = {Removing Biased Data to Improve Fairness and Accuracy},
  author = {Verma, Sahil and Ernst, Michael and Just, Rene},
  year = {2021},
  month = feb,
  number = {arXiv:2102.03054},
  eprint = {2102.03054},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.03054},
  abstract = {Machine learning systems are often trained using data collected from historical decisions. If past decisions were biased, then automated systems that learn from historical data will also be biased. We propose a black-box approach to identify and remove biased training data. Machine learning models trained on such debiased data (a subset of the original training data) have low individual discrimination, often 0\%. These models also have greater accuracy and lower statistical disparity than models trained on the full historical data. We evaluated our methodology in experiments using 6 real-world datasets. Our approach outperformed seven previous approaches in terms of individual discrimination and accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/UM3PL4ZA/Verma et al. - 2021 - Removing biased data to improve fairness and accur.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/3RBQFBNM/2102.html}
}

@article{vowelsYaDAGsSurvey2022,
  title = {D'ya {{Like DAGs}}? {{A Survey}} on {{Structure Learning}} and {{Causal Discovery}}},
  shorttitle = {D'ya {{Like DAGs}}?},
  author = {Vowels, Matthew J. and Camgoz, Necati Cihan and Bowden, Richard},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  issn = {0360-0300},
  doi = {10.1145/3527154},
  abstract = {Causal reasoning is a crucial part of science and human intelligence. In order to discover causal relationships from data, we need structure discovery methods. We provide a review of background theory and a survey of methods for structure discovery. We primarily focus on modern, continuous optimization methods, and provide reference to further resources such as benchmark datasets and software packages. Finally, we discuss the assumptive leap required to take us from structure to causality.},
  annotation = {Just Accepted},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RHI6XK8K/Vowels et al. - 2022 - Dya Like DAGs A Survey on Structure Learning and.pdf}
}

@article{walkerEstimationProbabilityEvent1967a,
  title = {Estimation of the Probability of an Event as a Function of Several Independent Variables},
  author = {Walker, Strother H and Duncan, David B},
  year = {1967},
  journal = {Biometrika},
  volume = {54},
  number = {1-2},
  pages = {167--179},
  publisher = {{Oxford University Press}},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/V6NFMBHZ/331528.html}
}

@inproceedings{wangFairClassificationGroupDependent2021,
  title = {Fair {{Classification}} with {{Group-Dependent Label Noise}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Wang, Jialu and Liu, Yang and Levy, Caleb},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {526--536},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442188.3445915},
  abstract = {This work examines how to train fair classifiers in settings where training labels are corrupted with random noise, and where the error rates of corruption depend both on the label class and on the membership function for a protected subgroup. Heterogeneous label noise models systematic biases towards particular groups when generating annotations. We begin by presenting analytical results which show that naively imposing parity constraints on demographic disparity measures, without accounting for heterogeneous and group-dependent error rates, can decrease both the accuracy and the fairness of the resulting classifier. Our experiments demonstrate these issues arise in practice as well. We address these problems by performing empirical risk minimization with carefully defined surrogate loss functions and surrogate constraints that help avoid the pitfalls introduced by heterogeneous label noise. We provide both theoretical and empirical justifications for the efficacy of our methods. We view our results as an important example of how imposing fairness on biased data sets without proper care can do at least as much harm as it does good.},
  isbn = {978-1-4503-8309-7},
  keywords = {algorithmic fairness,learning with noisy and biased labels,machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7W7RDTJ5/Wang et al. - 2021 - Fair Classification with Group-Dependent Label Noi.pdf}
}

@misc{wangImprovingCooperativeGame2022,
  title = {Improving {{Cooperative Game Theory-based Data Valuation}} via {{Data Utility Learning}}},
  author = {Wang, Tianhao and Yang, Yu and Jia, Ruoxi},
  year = {2022},
  month = apr,
  number = {arXiv:2107.06336},
  eprint = {2107.06336},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.06336},
  abstract = {The Shapley value (SV) and Least core (LC) are classic methods in cooperative game theory for cost/profit sharing problems. Both methods have recently been proposed as a principled solution for data valuation tasks, i.e., quantifying the contribution of individual datum in machine learning. However, both SV and LC suffer computational challenges due to the need for retraining models on combinatorially many data subsets. In this work, we propose to boost the efficiency in computing Shapley value or Least core by learning to estimate the performance of a learning algorithm on unseen data combinations. Theoretically, we derive bounds relating the error in the predicted learning performance to the approximation error in SV and LC. Empirically, we show that the proposed method can significantly improve the accuracy of SV and LC estimation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/EZIRTNE4/Wang et al. - 2022 - Improving Cooperative Game Theory-based Data Valua.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/P7Z7XBUW/2107.html}
}

@inproceedings{wangNewActiveLabeling2014,
  title = {A New Active Labeling Method for Deep Learning},
  booktitle = {2014 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Wang, Dan and Shang, Yi},
  year = {2014},
  month = jul,
  pages = {112--119},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2014.6889457},
  abstract = {Deep learning has been shown to achieve outstanding performance in a number of challenging real-world applications. However, most of the existing works assume a fixed set of labeled data, which is not necessarily true in real-world applications. Getting labeled data is usually expensive and time consuming. Active labelling in deep learning aims at achieving the best learning result with a limited labeled data set, i.e., choosing the most appropriate unlabeled data to get labeled. This paper presents a new active labeling method, AL-DL, for cost-effective selection of data to be labeled. AL-DL uses one of three metrics for data selection: least confidence, margin sampling, and entropy. The method is applied to deep learning networks based on stacked restricted Boltzmann machines, as well as stacked autoencoders. In experiments on the MNIST benchmark dataset, the method outperforms random labeling consistently by a significant margin.},
  keywords = {Classification algorithms,Entropy,Labeling,Measurement,Neural networks,Training,Uncertainty},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/VTAZZTPR/6889457.html}
}

@inproceedings{wangRepairingRetrainingAvoiding2019,
  title = {Repairing without {{Retraining}}: {{Avoiding Disparate Impact}} with {{Counterfactual Distributions}}},
  shorttitle = {Repairing without {{Retraining}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Hao and Ustun, Berk and Calmon, Flavio},
  year = {2019},
  month = may,
  pages = {6618--6627},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {When the performance of a machine learning model varies over groups defined by sensitive attributes (e.g., gender or ethnicity), the performance disparity can be expressed in terms of the probability distributions of the input and output variables over each group. In this paper, we exploit this fact to reduce the disparate impact of a fixed classification model over a population of interest. Given a black-box classifier, we aim to eliminate the performance gap by perturbing the distribution of input variables for the disadvantaged group. We refer to the perturbed distribution as a counterfactual distribution, and characterize its properties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data preprocessor that can reduce disparate impact without training a new model. We validate our approach through experiments on real-world datasets, showing that it can repair different forms of disparity without a significant drop in accuracy.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/F6CCCNFE/Wang et al. - 2019 - Repairing without Retraining Avoiding Disparate I.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/IL58P9UF/Wang et al. - 2019 - Repairing without Retraining Avoiding Disparate I.pdf}
}

@inproceedings{wangUnderstandingInstanceLevelImpact2022a,
  title = {Understanding {{Instance-Level Impact}} of {{Fairness Constraints}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Jialu and Wang, Xin Eric and Liu, Yang},
  year = {2022},
  month = jun,
  pages = {23114--23130},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {A variety of fairness constraints have been proposed in the literature to mitigate group-level statistical bias. Their impacts have been largely evaluated for different groups of populations corresponding to a set of sensitive attributes, such as race or gender. Nonetheless, the community has not observed sufficient explorations for how imposing fairness constraints fare at an instance level. Building on the concept of influence function, a measure that characterizes the impact of a training example on the target model and its predictive performance, this work studies the influence of training examples when fairness constraints are imposed. We find out that under certain assumptions, the influence function with respect to fairness constraints can be decomposed into a kernelized combination of training examples. One promising application of the proposed fairness influence function is to identify suspicious training examples that may cause model discrimination by ranking their influence scores. We demonstrate with extensive experiments that training on a subset of weighty data examples leads to lower fairness violations with a trade-off of accuracy.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8BT8GCIF/Wang et al. - 2022 - Understanding Instance-Level Impact of Fairness Co.pdf}
}

@article{watanabeInformationTheoreticalAnalysis1960,
  title = {Information {{Theoretical Analysis}} of {{Multivariate Correlation}}},
  author = {Watanabe, Satosi},
  year = {1960},
  month = jan,
  journal = {IBM Journal of Research and Development},
  volume = {4},
  number = {1},
  pages = {66--82},
  issn = {0018-8646},
  doi = {10.1147/rd.41.0066},
  abstract = {A set {$\lambda$} of stochastic variables, y1 ,y2, \ldots, yn, is grouped into subsets, \textmu 1, \textmu 2, ..., \textmu k. The correlation existing in {$\lambda$} with respect to the \textmu 's is adequately expressed by an equation where S({$\nu$}) is the entropy function defined with reference to the variables y in subset {$\nu$}. For a given {$\lambda$}, C becomes maximum when each \textmu i consists of only one variable, (n = k). The value C is then called the total correlation in {$\lambda$}, Ctot({$\lambda$}). The present paper gives various theorems, according to which Ctot({$\lambda$}) can be decomposed in terms of the partial correlations existing in subsets of {$\lambda$}, and of quantities derivable therefrom. The information-theoretical meaning of each decomposition is carefully explained. As illustrations, two problems are discussed at the end of the paper: (1) redundancy in geometrical figures in pattern recognition, and (2) randomization effect of shuffling cards marked ``zero'' or ``one.''},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5TKRLHYG/5392532.html}
}

@inproceedings{weiBigDatasetsResearch2016,
  title = {Big {{Datasets}} for {{Research}}: {{A Survey}} on {{Flagship Conferences}}},
  shorttitle = {Big {{Datasets}} for {{Research}}},
  booktitle = {2016 {{IEEE International Congress}} on {{Big Data}} ({{BigData Congress}})},
  author = {Wei, Yi and Liu, Shijun and Sun, Jiao and Cui, Lizhen and Pan, Li and Wu, Lei},
  year = {2016},
  month = jun,
  pages = {394--401},
  doi = {10.1109/BigDataCongress.2016.62},
  abstract = {It is obvious that big data can bring us new opportunities to discover valuable information. Apparently, corresponding big datasets are powerful tools for scholars, which connect theoretical studies to reality. They can help scholars to evaluate their achievements and find new problems. In recent years, there has been a significant growth in research data repositories and registries. However, these infrastructures are fragmented across institutions, countries and research domains. As such, finding research datasets is not a trivial task for many researchers. Thus we investigated 195 papers regarding big data on some notable international conferences in recent 3 years, and also gathered 285 datasets mentioned in them. In this paper, we present and analyze our survey results in terms of the status quo of big data research and datasets from different aspects. In particular, we propose two different taxonomies of big datasets and classify our surveyed datasets into them. In addition, we also give a brief introduction about 7 widely accepted data collections online. Finally, some basic principles for scholars in choosing and using big datasets are given.},
  keywords = {big data,Big data,Conferences,datasets,Internet topology,survey},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/66MR2NLT/Wei et al. - 2016 - Big Datasets for Research A Survey on Flagship Co.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/4I9AMMLX/7584968.html}
}

@inproceedings{weiDecisionMakingSelectiveLabels2021,
  title = {Decision-{{Making Under Selective Labels}}: {{Optimal Finite-Domain Policies}} and {{Beyond}}},
  shorttitle = {Decision-{{Making Under Selective Labels}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Wei, Dennis},
  year = {2021},
  month = jul,
  pages = {11035--11046},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Selective labels are a common feature of high-stakes decision-making applications, referring to the lack of observed outcomes under one of the possible decisions. This paper studies the learning of decision policies in the face of selective labels, in an online setting that balances learning costs against future utility. In the homogeneous case in which individuals' features are disregarded, the optimal decision policy is shown to be a threshold policy. The threshold becomes more stringent as more labels are collected; the rate at which this occurs is characterized. In the case of features drawn from a finite domain, the optimal policy consists of multiple homogeneous policies in parallel. For the general infinite-domain case, the homogeneous policy is extended by using a probabilistic classifier and bootstrapping to provide its inputs. In experiments on synthetic and real data, the proposed policies achieve consistently superior utility with no parameter tuning in the finite-domain case and lower parameter sensitivity in the general case.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/B6HJCVNH/Wei - 2021 - Decision-Making Under Selective Labels Optimal Fi.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/MTHNP285/Wei - 2021 - Decision-Making Under Selective Labels Optimal Fi.pdf}
}

@misc{WhatMLfairnessgym2022,
  title = {What Is {{ML-fairness-gym}}?},
  year = {2022},
  month = jun,
  copyright = {Apache-2.0},
  howpublished = {Google}
}

@inproceedings{wuDAVINZDataValuation2022,
  title = {{{DAVINZ}}: {{Data Valuation}} Using {{Deep Neural Networks}} at {{Initialization}}},
  shorttitle = {{{DAVINZ}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wu, Zhaoxuan and Shu, Yao and Low, Bryan Kian Hsiang},
  year = {2022},
  month = jun,
  pages = {24150--24176},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Recent years have witnessed a surge of interest in developing trustworthy methods to evaluate the value of data in many real-world applications (e.g., collaborative machine learning, data marketplaces). Existing data valuation methods typically valuate data using the generalization performance of converged machine learning models after their long-term model training, hence making data valuation on large complex deep neural networks (DNNs) unaffordable. To this end, we theoretically derive a domain-aware generalization bound to estimate the generalization performance of DNNs without model training. We then exploit this theoretically derived generalization bound to develop a novel training-free data valuation method named data valuation at initialization (DAVINZ) on DNNs, which consistently achieves remarkable effectiveness and efficiency in practice. Moreover, our training-free DAVINZ, surprisingly, can even theoretically and empirically enjoy the desirable properties that training-based data valuation methods usually attain, thus making it more trustworthy in practice.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/RZ32CBJP/Wu et al. - 2022 - DAVINZ Data Valuation using Deep Neural Networks .pdf}
}

@inproceedings{wuGeneralizationGenerativeAdversarial2019a,
  title = {Generalization in {{Generative Adversarial Networks}}: {{A Novel Perspective}} from {{Privacy Protection}}},
  shorttitle = {Generalization in {{Generative Adversarial Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Bingzhe and Zhao, Shiwan and Chen, Chaochao and Xu, Haoyang and Wang, Li and Zhang, Xiaolu and Sun, Guangyu and Zhou, Jun},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In this paper, we aim to understand the generalization properties of generative adversarial networks (GANs) from a new perspective of privacy protection. Theoretically, we prove that a differentially private learning algorithm used for training the GAN does not overfit to a certain degree, i.e., the generalization gap can be bounded. Moreover, some recent works, such as the Bayesian GAN, can be re-interpreted based on our theoretical insight from privacy protection. Quantitatively, to evaluate the information leakage of well-trained GAN models, we perform various membership attacks on these models. The results show that previous Lipschitz regularization techniques are effective in not only reducing the generalization gap but also alleviating the information leakage of the training dataset.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9MFFFHBI/Wu et al. - 2019 - Generalization in Generative Adversarial Networks.pdf}
}

@article{wuMultiLabelActiveLearning2020,
  title = {Multi-{{Label Active Learning Algorithms}} for {{Image Classification}}: {{Overview}} and {{Future Promise}}},
  shorttitle = {Multi-{{Label Active Learning Algorithms}} for {{Image Classification}}},
  author = {Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng},
  year = {2020},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {2},
  pages = {28:1--28:35},
  issn = {0360-0300},
  doi = {10.1145/3379504},
  abstract = {Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.},
  keywords = {active learning,annotation,Image classification,multi-label image,sampling strategy},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5XAWHJVC/Wu et al. - 2020 - Multi-Label Active Learning Algorithms for Image C.pdf}
}

@misc{xieDifferentiallyPrivateGenerative2018,
  title = {Differentially {{Private Generative Adversarial Network}}},
  author = {Xie, Liyang and Lin, Kaixiang and Wang, Shu and Wang, Fei and Zhou, Jiayu},
  year = {2018},
  month = feb,
  number = {arXiv:1802.06739},
  eprint = {1802.06739},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.06739},
  abstract = {Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JG32WYH6/Xie et al. - 2018 - Differentially Private Generative Adversarial Netw.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8A26RJTU/1802.html}
}

@inproceedings{xingFairnessAwareUnsupervisedFeature2021,
  title = {Fairness-{{Aware Unsupervised Feature Selection}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Xing, Xiaoying and Liu, Hongfu and Chen, Chen and Li, Jundong},
  year = {2021},
  month = oct,
  series = {{{CIKM}} '21},
  pages = {3548--3552},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3459637.3482106},
  abstract = {Feature selection is a prevalent data preprocessing paradigm for various learning tasks. Due to the expensive cost of acquiring supervision information, unsupervised feature selection sparks great interests recently. However, existing unsupervised feature selection algorithms do not have fairness considerations and suffer from a high risk of amplifying discrimination by selecting features that are over associated with protected attributes such as gender, race, and ethnicity. In this paper, we make an initial investigation of the fairness-aware unsupervised feature selection problem and develop a principled framework, which leverages kernel alignment to find a subset of high-quality features that can best preserve the information in the original feature space while being minimally correlated with protected attributes. Specifically, different from the mainstream in-processing debiasing methods, our proposed framework can be regarded as a model-agnostic debiasing strategy that eliminates biases and discrimination before downstream learning algorithms are involved. Experimental results on real-world datasets demonstrate that our framework achieves a good trade-off between feature utility and promoting feature fairness.},
  isbn = {978-1-4503-8446-9},
  keywords = {fairness,feature selection,unsupervised learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZAIGR3T3/Xing et al. - 2021 - Fairness-Aware Unsupervised Feature Selection.pdf}
}

@inproceedings{xuFairGANAchievingFair2019a,
  title = {{{FairGAN}}+: {{Achieving Fair Data Generation}} and {{Classification}} through {{Generative Adversarial Nets}}},
  shorttitle = {{{FairGAN}}+},
  booktitle = {2019 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Xu, Depeng and Yuan, Shuhan and Zhang, Lu and Wu, Xintao},
  year = {2019},
  month = dec,
  pages = {1401--1406},
  doi = {10.1109/BigData47090.2019.9006322},
  abstract = {How to achieve fairness is important for next generation machine learning. Two tasks that are equally important in fair machine learning are how to obtain fair datasets and how to build fair classifiers. In this work, we propose a new generative adversarial network (GAN) model for fair machine learning, named FairGAN+. FairGAN+ contains a generator to generate close-to-real samples, a classifier to predict class labels and three discriminators to assist adversarial learning. FairGAN+ simultaneously achieves fair data generation and classification by co-training the generative model and the classifier through joint adversarial games with the discriminators. Evaluations on real world data show the effectiveness of FairGAN+ on both fair data generation and fair classification.},
  keywords = {Data models,fair classification,fair data generation,fairness-aware learning,Gallium nitride,Games,generative adversarial networks,Generative adversarial networks,Generators,Machine learning,Task analysis},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/BP28CCZN/Xu et al. - 2019 - FairGAN +  Achieving Fair Data Generat.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/Q847DPWJ/9006322.html}
}

@misc{yanFORMLLearningReweight2022,
  title = {{{FORML}}: {{Learning}} to {{Reweight Data}} for {{Fairness}}},
  shorttitle = {{{FORML}}},
  author = {Yan, Bobby and Seto, Skyler and Apostoloff, Nicholas},
  year = {2022},
  month = feb,
  number = {arXiv:2202.01719},
  eprint = {2202.01719},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Deployed machine learning models are evaluated by multiple metrics beyond accuracy, such as fairness and robustness. However, such models are typically trained to minimize the average loss for a single metric, which is typically a proxy for accuracy. Training to optimize a single metric leaves these models prone to fairness violations, especially when the population of sub-groups in the training data are imbalanced. This work addresses the challenge of jointly optimizing fairness and predictive performance in the multi-class classification setting by introducing Fairness Optimized Reweighting via Meta-Learning (FORML), a training algorithm that balances fairness constraints and accuracy by jointly optimizing training sample weights and a neural network's parameters. The approach increases fairness by learning to weight each training datum's contribution to the loss according to its impact on reducing fairness violations, balancing the contributions from both over- and under-represented sub-groups. We empirically validate FORML on a range of benchmark and real-world classification datasets and show that our approach improves equality of opportunity fairness criteria over existing state-of-the-art reweighting methods by approximately 1\% on image classification tasks and by approximately 5\% on a face attribute prediction task. This improvement is achieved without pre-processing data or post-processing model outputs, without learning an additional weighting function, and while maintaining accuracy on the original predictive metric.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/PPLN79S2/Yan et al. - 2022 - FORML Learning to Reweight Data for Fairness.pdf}
}

@article{yangBenchmarkComparisonActive2018,
  title = {A Benchmark and Comparison of Active Learning for Logistic Regression},
  author = {Yang, Yazhou and Loog, Marco},
  year = {2018},
  month = nov,
  journal = {Pattern Recognition},
  volume = {83},
  pages = {401--415},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2018.06.004},
  abstract = {Logistic regression is by far the most widely used classifier in real-world applications. In this paper, we benchmark the state-of-the-art active learning methods for logistic regression and discuss and illustrate their underlying characteristics. Experiments are carried out on three synthetic datasets and 44 real-world datasets, providing insight into the behaviors of these active learning methods with respect to the area of the learning curve (which plots classification accuracy as a function of the number of queried examples) and their computational costs. Surprisingly, one of the earliest and simplest suggested active learning methods, i.e., uncertainty sampling, performs exceptionally well overall. Another remarkable finding is that random sampling, which is the rudimentary baseline to improve upon, is not overwhelmed by individual active learning techniques in many cases.},
  langid = {english},
  keywords = {Active learning,Benchmark,Experimental design,Logistic regression,Preference maps},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LTS2SAPS/Yang and Loog - 2018 - A benchmark and comparison of active learning for .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/9EBPRB8P/S0031320318302140.html}
}

@inproceedings{yonaWhoResponsibleJointly2021,
  title = {Who's {{Responsible}}? {{Jointly Quantifying}} the {{Contribution}} of the {{Learning Algorithm}} and {{Data}}},
  shorttitle = {Who's {{Responsible}}?},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Yona, Gal and Ghorbani, Amirata and Zou, James},
  year = {2021},
  month = jul,
  series = {{{AIES}} '21},
  pages = {1034--1041},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3461702.3462574},
  abstract = {A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D' would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A' on the same dataset D would have improved performance. As ML becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of ML accountability.},
  isbn = {978-1-4503-8473-5},
  keywords = {accountability,data valuation,fairness,machine learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7ZPETNJS/Yona et al. - 2021 - Who's Responsible Jointly Quantifying the Contrib.pdf}
}

@inproceedings{yoonDataValuationUsing2020,
  title = {Data {{Valuation}} Using {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Yoon, Jinsung and Arik, Sercan and Pfister, Tomas},
  year = {2020},
  month = nov,
  pages = {10842--10851},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Quantifying the value of data is a fundamental problem in machine learning and has multiple important use cases: (1) building insights about the dataset and task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. We propose Data Valuation using Reinforcement Learning (DVRL), to adaptively learn data values jointly with the predictor model. DVRL uses a data value estimator (DVE) to learn how likely each datum is used in training of the predictor model. DVE is trained using a reinforcement signal that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across numerous datasets and application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6\% and 10.8\%, respectively.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/57PNGVFP/Yoon et al. - 2020 - Data Valuation using Reinforcement Learning.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YB92QNTH/Yoon et al. - 2020 - Data Valuation using Reinforcement Learning.pdf}
}

@inproceedings{zafarFairnessConstraintsMechanisms2017,
  title = {Fairness {{Constraints}}: {{Mechanisms}} for {{Fair Classification}}},
  shorttitle = {Fairness {{Constraints}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and Rogriguez, Manuel Gomez and Gummadi, Krishna P.},
  year = {2017},
  month = apr,
  pages = {962--970},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Algorithmic decision making systems are ubiquitous across a wide variety of online as well as offline services. These systems rely on complex learning methods and vast amounts of data to optimize the service functionality, satisfaction of the end user and profitability. However, there is a growing concern that these automated decisions can lead, even in the absence of intent, to a lack of fairness, i.e., their outcomes can disproportionately hurt (or, benefit) particular groups of people sharing one or more sensitive attributes (e.g., race, sex). In this paper, we introduce a flexible mechanism to design fair classifiers by leveraging a novel intuitive measure of decision boundary (un)fairness. We instantiate this mechanism with two well-known classifiers, logistic regression and support vector machines, and show on real-world data that our mechanism allows for a fine-grained control on the degree of fairness, often at a small cost in terms of accuracy.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5DDJYQ4J/Zafar et al. - 2017 - Fairness Constraints Mechanisms for Fair Classifi.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FPABXP74/Zafar et al. - 2017 - Fairness Constraints Mechanisms for Fair Classifi.pdf}
}

@inproceedings{zemelLearningFairRepresentations2013,
  title = {Learning {{Fair Representations}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  year = {2013},
  month = may,
  pages = {325--333},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We propose a learning algorithm for fair classification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is identical to the proportion in the population as a  whole), and individual fairness (similar individuals should be treated similarly).  We formulate fairness as an optimization problem of finding a  good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about membership in the protected group.  We show positive results of our algorithm relative to other known techniques, on three datasets.  Moreover, we demonstrate several advantages to our approach.  First, our intermediate representation can be used for other classification tasks (i.e., transfer  learning is possible); secondly, we take a step toward learning a distance metric which can find important dimensions of the data for classification.},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/L9EL72GV/Zemel et al. - 2013 - Learning Fair Representations.pdf}
}

@article{zhangCorrelatedDifferentialPrivacy2020,
  title = {Correlated {{Differential Privacy}}: {{Feature Selection}} in {{Machine Learning}}},
  shorttitle = {Correlated {{Differential Privacy}}},
  author = {Zhang, Tao and Zhu, Tianqing and Xiong, Ping and Huo, Huan and Tari, Zahir and Zhou, Wanlei},
  year = {2020},
  month = mar,
  journal = {IEEE Transactions on Industrial Informatics},
  volume = {16},
  number = {3},
  pages = {2115--2124},
  issn = {1941-0050},
  doi = {10.1109/TII.2019.2936825},
  abstract = {Privacy preserving in machine learning is a crucial issue in industry informatics since data used for training in industries usually contain sensitive information. Existing differentially private machine learning algorithms have not considered the impact of data correlation, which may lead to more privacy leakage than expected in industrial applications. For example, data collected for traffic monitoring may contain some correlated records due to temporal correlation or user correlation. To fill this gap, in this article, we propose a correlation reduction scheme with differentially private feature selection considering the issue of privacy loss when data have correlation in machine learning tasks. The proposed scheme involves five steps with the goal of managing the extent of data correlation, preserving the privacy, and supporting accuracy in the prediction results. In this way, the impact of data correlation is relieved with the proposed feature selection scheme, and moreover the privacy issue of data correlation in learning is guaranteed. The proposed method can be widely used in machine learning algorithms, which provide services in industrial areas. Experiments show that the proposed scheme can produce better prediction results with machine learning tasks and fewer mean square errors for data queries compared to existing schemes.},
  keywords = {Correlation,data correlation,Differential privacy,Feature extraction,feature selection,machine learning,Machine learning,Machine learning algorithms,Sensitivity},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/G6SGGAKR/Zhang et al. - 2020 - Correlated Differential Privacy Feature Selection.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6KQ6SQHI/8809721.html}
}

@inproceedings{zhangEqualityOpportunityClassification2018,
  title = {Equality of {{Opportunity}} in {{Classification}}: {{A Causal Approach}}},
  shorttitle = {Equality of {{Opportunity}} in {{Classification}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Junzhe and Bareinboim, Elias},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The Equalized Odds (for short, EO)  is one of the most popular measures of discrimination used in the supervised learning setting. It ascertains fairness through the balance of the misclassification rates (false positive and negative) across the protected groups -- e.g., in the context of law enforcement, an African-American defendant who would not commit a future crime will have an equal opportunity of being released, compared to a non-recidivating Caucasian defendant. Despite this noble goal, it has been acknowledged in the literature that statistical tests based on the EO are oblivious to the underlying causal mechanisms that generated the disparity in the first place (Hardt et al. 2016). This leads to a critical disconnect between statistical measures readable from the data and the meaning of discrimination in the legal system, where compelling evidence that the observed disparity is tied to a specific causal process deemed unfair by society is required to characterize discrimination. The goal of this paper is to develop a principled approach to connect the statistical disparities characterized by the EO  and the underlying, elusive, and frequently unobserved, causal mechanisms that generated such inequality. We start by introducing a new family of counterfactual measures that allows one to explain the misclassification disparities in terms of the underlying mechanisms in an arbitrary, non-parametric structural causal model. This will, in turn, allow legal and data analysts to interpret currently deployed classifiers through causal lens, linking the statistical disparities found in the data to the corresponding causal processes. Leveraging the new family of counterfactual measures, we develop a learning procedure to construct a classifier that is statistically efficient, interpretable, and compatible with the basic human intuition of fairness. We demonstrate our results through experiments in both real (COMPAS) and synthetic datasets.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SZHYHSW7/Zhang and Bareinboim - 2018 - Equality of Opportunity in Classification A Causa.pdf}
}

@inproceedings{zhangHowFairDecisions2020,
  title = {How Do Fair Decisions Fare in Long-Term Qualification?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Xueru and Tu, Ruibo and Liu, Yang and Liu, Mingyan and Kjellstrom, Hedvig and Zhang, Kun and Zhang, Cheng},
  year = {2020},
  volume = {33},
  pages = {18457--18469},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Although many fairness criteria have been proposed for decision making, their long-term impact on the well-being of a population remains unclear. In this work, we study the dynamics of population qualification and algorithmic decisions under a partially observed Markov decision problem setting. By characterizing the equilibrium of such dynamics, we analyze the long-term impact of static fairness constraints on the equality and improvement of group well-being. Our results show that static fairness constraints can either promote equality or exacerbate  disparity depending on the driving factor of qualification transitions and the effect of sensitive attributes on feature distributions. We also consider possible interventions that can effectively improve group qualification or promote equality of group qualification. Our theoretical results and experiments on static real-world datasets with simulated dynamics show that our framework can be used to facilitate social science studies.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ZCVVY8CW/Zhang et al. - 2020 - How do fair decisions fare in long-term qualificat.pdf}
}

@article{zhangLargescaleKernelMethods2018,
  title = {Large-Scale Kernel Methods for Independence Testing},
  author = {Zhang, Qinyi and Filippi, Sarah and Gretton, Arthur and Sejdinovic, Dino},
  year = {2018},
  month = jan,
  journal = {Statistics and Computing},
  volume = {28},
  number = {1},
  pages = {113--130},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9721-7},
  abstract = {Representations of probability measures in reproducing kernel Hilbert spaces provide a flexible framework for fully nonparametric hypothesis tests of independence, which can capture any type of departure from independence, including nonlinear associations and multivariate interactions. However, these approaches come with an at least quadratic computational cost in the number of observations, which can be prohibitive in many applications. Arguably, it is exactly in such large-scale datasets that capturing any type of dependence is of interest, so striking a favourable trade-off between computational efficiency and test performance for kernel independence tests would have a direct impact on their applicability in practice. In this contribution, we provide an extensive study of the use of large-scale kernel approximations in the context of independence testing, contrasting block-based, Nystr\"om and random Fourier feature approaches. Through a variety of synthetic data experiments, it is demonstrated that our large-scale methods give comparable performance with existing methods while using significantly less computation time and memory.},
  langid = {english},
  keywords = {HilbertSchmidt independence criteria,Independence testing,Large-scale kernel method,Nystrm method,Random Fourier features},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/E2R9W536/Zhang et al. - 2018 - Large-scale kernel methods for independence testin.pdf}
}

@inproceedings{zhangLearningFastSample2021,
  title = {Learning {{Fast Sample Re-Weighting Without Reward Data}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Zhang, Zizhao and Pfister, Tomas},
  year = {2021},
  pages = {725--734},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GKEWVYDJ/Zhang and Pfister - 2021 - Learning Fast Sample Re-Weighting Without Reward D.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/STH4C4IN/Zhang_Learning_Fast_Sample_Re-Weighting_Without_Reward_Data_ICCV_2021_paper.html}
}

@article{zhangReviewMultiLabelLearning2014a,
  title = {A {{Review}} on {{Multi-Label Learning Algorithms}}},
  author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
  year = {2014},
  month = aug,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {8},
  pages = {1819--1837},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2013.39},
  abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
  keywords = {algorithm adaptation,Algorithm design and analysis,Artificial Intelligence,Computing Methodologies,Correlation,Data mining,Database Applications,Database Management,Information Technology and Systems,label correlations,Learning,Machine learning algorithms,Multi-label learning,problem transformation,Semantics,Supervised learning,Training,Vectors},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YT43VXTC/Zhang and Zhou - 2014 - A Review on Multi-Label Learning Algorithms;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/WPBTL3T4/6471714.html}
}

@misc{zhaoAdaptiveFairnessAwareOnline2022,
  title = {Adaptive {{Fairness-Aware Online Meta-Learning}} for {{Changing Environments}}},
  author = {Zhao, Chen and Mi, Feng and Wu, Xintao and Jiang, Kai and Khan, Latifur and Chen, Feng},
  year = {2022},
  month = may,
  number = {arXiv:2205.11264},
  eprint = {2205.11264},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11264},
  abstract = {The fairness-aware online learning framework has arisen as a powerful tool for the continual lifelong learning setting. The goal for the learner is to sequentially learn new tasks where they come one after another over time and the learner ensures the statistic parity of the new coming task across different protected sub-populations (e.g. race and gender). A major drawback of existing methods is that they make heavy use of the i.i.d assumption for data and hence provide static regret analysis for the framework. However, low static regret cannot imply a good performance in changing environments where tasks are sampled from heterogeneous distributions. To address the fairness-aware online learning problem in changing environments, in this paper, we first construct a novel regret metric FairSAR by adding long-term fairness constraints onto a strongly adapted loss regret. Furthermore, to determine a good model parameter at each round, we propose a novel adaptive fairness-aware online meta-learning algorithm, namely FairSAOML, which is able to adapt to changing environments in both bias control and model precision. The problem is formulated in the form of a bi-level convex-concave optimization with respect to the model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The theoretic analysis provides sub-linear upper bounds for both loss regret and violation of cumulative fairness constraints. Our experimental evaluation on different real-world datasets with settings of changing environments suggests that the proposed FairSAOML significantly outperforms alternatives based on the best prior online learning approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7WK9VPLR/Zhao et al. - 2022 - Adaptive Fairness-Aware Online Meta-Learning for C.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/MCCB68KS/2205.html}
}

@inproceedings{zhaoFairClassifiersSensitive2022a,
  title = {Towards {{Fair Classifiers Without Sensitive Attributes}}: {{Exploring Biases}} in {{Related Features}}},
  shorttitle = {Towards {{Fair Classifiers Without Sensitive Attributes}}},
  booktitle = {Proceedings of the {{Fifteenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Zhao, Tianxiang and Dai, Enyan and Shu, Kai and Wang, Suhang},
  year = {2022},
  month = feb,
  series = {{{WSDM}} '22},
  pages = {1433--1442},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3488560.3498493},
  abstract = {Despite the rapid development and great success of machine learning models, extensive studies have exposed their disadvantage of inheriting latent discrimination and societal bias from the training data. This phenomenon hinders their adoption on high-stake applications. Thus, many efforts have been taken for developing fair machine learning models. Most of them require that sensitive attributes are available during training to learn fair models. However, in many real-world applications, it is usually infeasible to obtain the sensitive attributes due to privacy or legal issues, which challenges existing fair-ensuring strategies. Though the sensitive attribute of each data sample is unknown, we observe that there are usually some non-sensitive features in the training data that are highly correlated with sensitive attributes, which can be used to alleviate the bias. Therefore, in this paper, we study a novel problem of exploring features that are highly correlated with sensitive attributes for learning fair and accurate classifiers. We theoretically show that by minimizing the correlation between these related features and model prediction, we can learn a fair classifier. Based on this motivation, we propose a novel framework which simultaneously uses these related features for accurate prediction and enforces fairness. In addition, the model can dynamically adjust the regularization weight of each related feature to balance its contribution on model classification and fairness. Experimental results on real-world datasets demonstrate the effectiveness of the proposed model for learning fair models with high classification accuracy.},
  isbn = {978-1-4503-9132-0},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,data learning,fairness,social mining},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/ASH2Y2PU/Zhao et al. - 2022 - Towards Fair Classifiers Without Sensitive Attribu.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QLKA3Q7L/Zhao et al. - 2022 - Towards Fair Classifiers Without Sensitive Attribu.pdf}
}

@inproceedings{zhaoFairMetaLearningFewShot2020,
  title = {Fair {{Meta-Learning For Few-Shot Classification}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Knowledge Graph}} ({{ICKG}})},
  author = {Zhao, Chen and Li, Changbin and Li, Jincheng and Chen, Feng},
  year = {2020},
  month = aug,
  pages = {275--282},
  doi = {10.1109/ICBK50248.2020.00047},
  abstract = {Artificial intelligence nowadays plays an increasingly prominent role in our life since decisions that were once made by humans are now delegated to automated systems. A machine learning algorithm trained based on biased data, however, tends to make unfair predictions. Developing classification algorithms that are fair with respect to protected attributes of the data thus becomes an important problem. Motivated by concerns surrounding the fairness effects of sharing and few-shot machine learning tools, such as the Model Agnostic Meta-Learning [1] framework, we propose a novel fair fast-adapted few-shot meta-learning approach that efficiently mitigates biases during meta-train by ensuring controlling the decision boundary covariance that between the protected variable and the signed distance from the feature vectors to the decision boundary. Through extensive experiments on two real-world image benchmarks over three state-of-the-art meta-learning algorithms, we empirically demonstrate that our proposed approach efficiently mitigates biases on model output and generalizes both accuracy and fairness to unseen tasks with a limited amount of training samples.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,decision boundary covariance,Decision making,Dogs,Extraterrestrial measurements,few-shot,Machine learning algorithms,meta-learning,Process control,statistical parity,Task analysis,Training},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/H9WFADT2/Zhao et al. - 2020 - Fair Meta-Learning For Few-Shot Classification.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/B9U9H9WC/9194529.html}
}

@phdthesis{zhaoFairnessAwareMultiTaskMeta2021,
  type = {Thesis},
  title = {Fairness-{{Aware Multi-Task}} and {{Meta Learning}}},
  author = {Zhao, Chen},
  year = {2021},
  month = jul,
  abstract = {Nowadays, machine learning plays an increasingly prominent role in our life since decisions that humans once made are now delegated to automated systems. In recent years, an increasing number of reports stated that human bias is revealed in an artificial intelligence system applied by high-tech companies. For example, Amazon was exposed a secret that its AI recruiting tool is biased against women. A critical component of developing responsible machine learning models is ensuring that such models are not unfairly harming any population subgroups. However, most of the existing fairness-aware algorithms focus on solving machine learning problems limited to a single task. How to jointly learn a fair model with multiple biased tasks is barely touched. This dissertation presents fairness-ware optimization algorithms for learning models with multiple tasks under three problem settings. Firstly, under the multi-task learning setting, we propose a multi-task regression model based on a popular non-parametric independence statistic test, i.e., Mann Whitney U statistic. We formulate and analyze the problem as a new non-convex optimization problem. A non-convex constraint is defined based on the group-wise ranking functions of an individual object. Secondly, under the meta-learning setting, we efficiently control unfairness by learning a good pair of parameters. Iteratively adapts and updates such parameter pairs across each task. Meta-learning is also known as learning to lean. It leverages the transferable knowledge learned from previous tasks and then adapted to new environments rapidly. Our proposed framework ensures the generalization capability of both accuracy and fairness onto new tasks. Thirdly, we formulate the problem as a constrained bi-level convex-concave optimization for the online learning setting that involves a primal-dual parameter pair for each level. The parameter pair is updated in an online version to adjust accuracy and fairness notion adaptively. Theoretical analysis justifies the efficiency and effectiveness of the proposed method by demonstrating sub-linear bound in time for both loss regret and violation of fairness constraints. We both theoretically discuss and empirically show that our designed methods are more effective than state-of-the-art counterparts on benchmark datasets.},
  langid = {english},
  annotation = {Accepted: 2022-03-30T20:58:15Z},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/I3CBQQ9V/Zhao - 2021 - Fairness-Aware Multi-Task and Meta Learning.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/NPS6F5NT/9388.html}
}

@inproceedings{zhaoFairnessAwareOnlineMetalearning2021,
  title = {Fairness-{{Aware Online Meta-learning}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zhao, Chen and Chen, Feng and Thuraisingham, Bhavani},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {2294--2304},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467389},
  abstract = {In contrast to offline working fashions, two research paradigms are devised for online learning: (1) Online Meta-Learning (OML)[6, 20, 26] learns good priors over model parameters (or learning to learn) in a sequential setting where tasks are revealed one after another. Although it provides a sub-linear regret bound, such techniques completely ignore the importance of learning with fairness which is a significant hallmark of human intelligence. (2) Online Fairness-Aware Learning [1, 8, 21]. This setting captures many classification problems for which fairness is a concern. But it aims to attain zero-shot generalization without any task-specific adaptation. This therefore limits the capability of a model to adapt onto newly arrived data. To overcome such issues and bridge the gap, in this paper for the first time we proposed a novel online meta-learning algorithm, namely FFML, which is under the setting of unfairness prevention. The key part of FFML is to learn good priors of an online fair classification model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The problem is formulated in the form of a bi-level convex-concave optimization. The theoretic analysis provides sub-linear upper bounds O(log T)for loss regret and O({$\surd$}log T)violation of cumulative fairness constraints. Our experiments demonstrate the versatility of FFML by applying it to classification on three real-world datasets and show substantial improvements over the best prior work on the tradeoff between fairness and classification accuracy.},
  isbn = {978-1-4503-8332-5},
  keywords = {bi-level optimization,bias control,long-term constraints,online learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/T2KLISPG/Zhao et al. - 2021 - Fairness-Aware Online Meta-learning.pdf}
}

@inproceedings{zhaoPrimalDualSubgradientApproach2020a,
  title = {A {{Primal-Dual Subgradient Approach}} for {{Fair Meta Learning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Zhao, Chen and Chen, Feng and Wang, Zhuoyi and Khan, Latifur},
  year = {2020},
  month = nov,
  pages = {821--830},
  issn = {2374-8486},
  doi = {10.1109/ICDM50108.2020.00091},
  abstract = {The problem of learning to generalize on unseen classes during the training step, also known as few-shot classification, has attracted considerable attention. Initialization based methods, such as the gradient-based model agnostic meta-learning (MAML) [1], tackle the few-shot learning problem by ``learning to fine-tune''. The goal of these approaches is to learn proper model initialization, so that the classifiers for new classes can be learned from a few labeled examples with a small number of gradient update steps. Few shot meta-learning is well-known with its fast-adapted capability and accuracy generalization onto unseen tasks [2]. Learning fairly with unbiased outcomes is another significant hallmark of human intelligence, which is rarely touched in few-shot meta-learning. In this work, we propose a Primal-Dual Fair Meta-learning framework, namely PDFM, which learns to train fair machine learning models using only a few examples based on data from related tasks. The key idea is to learn a good initialization of a fair model's primal and dual parameters so that it can adapt to a new fair learning task via a few gradient update steps. Instead of manually tuning the dual parameters as hyperparameters via a grid search, PDFM optimizes the initialization of the primal and dual parameters jointly for fair meta-learning via a subgradient primal-dual approach. We further instantiate an example of bias controlling using decision boundary covariance (DBC) [3] as the fairness constraint for each task, and demonstrate the versatility of our proposed approach by applying it to classification on a variety of three realworld datasets. Our experiments show substantial improvements over the best prior work for this setting. Our code and datasets are available at https://github.com/charliezhaoyinpeng/PDFM.git.},
  keywords = {Adaptation models,Data models,dual decompositon,dual subgradient,fairness,few shot,Human intelligence,Machine learning,meta-learning,Task analysis,Training,Tuning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SJ4Z9IEH/Zhao et al. - 2021 - A Primal-Dual Subgradient Approachfor Fair Meta Le.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/7LB5C7VP/9338398.html}
}

@inproceedings{zhaoUnfairnessDiscoveryPrevention2020,
  title = {Unfairness {{Discovery}} and {{Prevention For Few-Shot Regression}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Knowledge Graph}} ({{ICKG}})},
  author = {Zhao, Chen and Chen, Feng},
  year = {2020},
  month = aug,
  pages = {137--144},
  doi = {10.1109/ICBK50248.2020.00029},
  abstract = {We study fairness in supervised few-shot meta-learning models that are sensitive to discrimination (or bias) in historical data. A machine learning model trained based on biased data tends to make unfair predictions for users from minority groups. Although this problem has been studied before, existing methods mainly aim to detect and control the dependency effect of the protected variables (e.g. race, gender) on target prediction based on a large amount of training data. These approaches carry two major drawbacks that (1) lacking showing a global cause-effect visualization for all variables; (2) lacking generalization of both accuracy and fairness to unseen tasks. In this work, we first discover discrimination from data using a causal Bayesian knowledge graph which not only demonstrates the dependency of the protected variable on target but also indicates causal effects between all variables. Next, we develop a novel algorithm based on risk difference in order to quantify the discriminatory influence for each protected variable in the graph. Furthermore, to protect prediction from unfairness, a fast-adapted bias-control approach in meta-learning is proposed, which efficiently mitigates statistical disparity for each task and it thus ensures independence of protected attributes on predictions based on biased and few-shot data samples. Distinct from existing meta-learning models, group unfairness of tasks are efficiently reduced by leveraging the mean difference between (un)protected groups for regression problems. Through extensive experiments on both synthetic and real-world data sets, we demonstrate that our proposed unfairness discovery and prevention approaches efficiently detect discrimination and mitigate biases on model output as well as generalize both accuracy and fairness to unseen tasks with a limited amount of training samples.},
  keywords = {Bayes methods,bias discovery and prevention,causal Bayesian network,Computer Science - Machine Learning,Data mining,Data models,fairness generalization,fewshot meta-learning,Machine learning,Machine learning algorithms,Predictive models,statistic parity,Statistics - Machine Learning,Task analysis},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/V3MUJXBS/Zhao and Chen - 2020 - Unfairness Discovery and Prevention For Few-Shot R.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/H4JJPBG3/9194502.html}
}

@article{greenbergTaxonomyOrganizationalJustice1987,
  title = {A {{Taxonomy}} of {{Organizational Justice Theories}}},
  author = {Greenberg, Jerald},
  year = {1987},
  month = jan,
  journal = {Academy of Management Review},
  volume = {12},
  number = {1},
  pages = {9--22},
  publisher = {{Academy of Management}},
  issn = {0363-7425},
  doi = {10.5465/amr.1987.4306437},
  abstract = {A taxonomy is presented that categorizes theories of organizational justice with respect to two independent dimensions: a reactive-proactive dimension and a process-content dimension. Various theories within each of the four resulting categories are identified. The implications of the taxonomy are discussed with respect to clarifying theoretical interrelationships, tracking research trends, and identifying needed areas of research.},
  keywords = {CONFLICT management,INDUSTRIAL management,INDUSTRIAL relations,INTERPERSONAL relations,MANAGEMENT research,ORGANIZATIONAL justice,ORGANIZATIONAL research,ORGANIZATIONAL sociology,PERSONNEL management,TAXONOMY},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/U982IEQH/Greenberg - 1987 - A Taxonomy of Organizational Justice Theories.pdf}
}


%%%%%%
% DV DATA QUALITY MOTIVATION
%%%%%%

@misc{hestnessDeepLearningScaling2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  number = {arXiv:1712.00409},
  eprint = {1712.00409},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1712.00409},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/W3A64QQG/Hestness et al. - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/K6PRE8B6/1712.html}
}

@article{najafabadiDeepLearningApplications2015,
  title = {Deep Learning Applications and Challenges in Big Data Analytics},
  author = {Najafabadi, Maryam M. and Villanustre, Flavio and Khoshgoftaar, Taghi M. and Seliya, Naeem and Wald, Randall and Muharemagic, Edin},
  year = {2015},
  month = feb,
  journal = {Journal of Big Data},
  volume = {2},
  number = {1},
  pages = {1},
  issn = {2196-1115},
  doi = {10.1186/s40537-014-0007-7},
  abstract = {Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.},
  langid = {english},
  keywords = {Big data,Deep learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/A6VLK5N7/Najafabadi et al. - 2015 - Deep learning applications and challenges in big d.pdf}
}



%%%%%%
% DV MOTIVATION
%%%%%%


@misc{ngiamDomainAdaptiveTransfer2018,
  title = {Domain {{Adaptive Transfer Learning}} with {{Specialist Models}}},
  author = {Ngiam, Jiquan and Peng, Daiyi and Vasudevan, Vijay and Kornblith, Simon and Le, Quoc V. and Pang, Ruoming},
  year = {2018},
  month = dec,
  number = {arXiv:1811.07056},
  eprint = {1811.07056},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.07056},
  abstract = {Transfer learning is a widely used method to build high performing computer vision models. In this paper, we study the efficacy of transfer learning by examining how the choice of data impacts performance. We find that more pre-training data does not always help, and transfer performance depends on a judicious choice of pre-training data. These findings are important given the continued increase in dataset sizes. We further propose domain adaptive transfer learning, a simple and effective pre-training method using importance weights computed based on the target dataset. Our method to compute importance weights follow from ideas in domain adaptation, and we show a novel application to transfer learning. Our methods achieve state-of-the-art results on multiple fine-grained classification datasets and are well-suited for use in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/C9UATRWX/Ngiam et al. - 2018 - Domain Adaptive Transfer Learning with Specialist .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/YIFIP979/1811.html}
}

@inproceedings{tonevaEmpiricalStudyExample2019,
  title = {An {{Empirical Study}} of {{Example Forgetting}} during {{Deep Neural Network Learning}}},
  booktitle = {7th {{International Conference}} on {{Learning Representations}}, \{\vphantom\}{{ICLR}}\vphantom\{\} 2019,                {{New Orleans}}, {{LA}}, {{USA}}, {{May}} 6-9, 2019},
  author = {Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
  year = {2019},
  eprint = {1812.05159},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{OpenReview.net}},
  doi = {10.48550/arXiv.1812.05159},
  abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a `forgetting event' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/8JGJNLPR/Toneva et al. - 2019 - An Empirical Study of Example Forgetting during De.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XSJDQJ2A/1812.html}
}

@inproceedings{zhuLearningTransferLearn2020,
  title = {Learning to {{Transfer Learn}}: {{Reinforcement Learning-Based Selection}} for {{Adaptive Transfer Learning}}},
  shorttitle = {Learning to {{Transfer Learn}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2020},
  author = {Zhu, Linchao and Ar{\i}k, Sercan {\"O}. and Yang, Yi and Pfister, Tomas},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {342--358},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {We propose a novel adaptive transfer learning framework, learning to transfer learn (L2TL), to improve performance on a target dataset by careful extraction of the related information from a source dataset. Our framework considers cooperative optimization of shared weights between models for source and target tasks, and adjusts the constituent loss weights adaptively. The adaptation of the weights is based on a reinforcement learning (RL) selection policy, guided with a performance metric on the target validation set. We demonstrate that L2TL outperforms fine-tuning baselines and other adaptive transfer learning methods on eight datasets. In the regimes of small-scale target datasets and significant label mismatch between source and target datasets, L2TL shows particularly large benefits.},
  isbn = {978-3-030-58583-9},
  langid = {english},
  keywords = {Reinforcement learning,Transfer learning,Visual understanding},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/N55M8LQ8/Zhu et al. - 2020 - Learning to Transfer Learn Reinforcement Learning.pdf}
}

@inproceedings{xuOnlineDataValuation2022,
  title = {Online {{Data Valuation}} and {{Pricing}} for {{Machine Learning Tasks}} in {{Mobile Health}}},
  booktitle = {{{IEEE INFOCOM}} 2022 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Xu, Anran and Zheng, Zhenzhe and Wu, Fan and Chen, Guihai},
  year = {2022},
  month = may,
  pages = {850--859},
  issn = {2641-9874},
  doi = {10.1109/INFOCOM48880.2022.9796669},
  abstract = {Mobile health (mHealth) applications, benefiting from mobile computing, have emerged rapidly in recent years, and generated a large volume of mHealth data. However, these valuable data are dispersed across isolated devices or organizations, which hinders discovering insights underlying the aggregated data. Considering the online characteristics of mHealth tasks, there is an urgent need for online data acquisition. In this paper, we present the first online data Valuation And Pricing mechanism, namely VAP, to incentive users to contribute mHealth data for machine learning (ML) tasks in mHealth systems. Under the framework of Bayesian ML, we propose a new metric based on the concept of entropy, to evaluate data valuation during model training in an online manner. In proportion to the data valuation, we then determine payments as compensations for users to contribute their data. We formulate this pricing problem as a contextual multi-armed bandit with the goal of profit maximization and propose a new algorithm based on the characteristics of pricing. We also extend VAP to general ML models. Finally, we have evaluated VAP on two real-world mHealth data sets. Evaluation results show that VAP outperforms the state-of-the-art valuation and pricing mechanisms in terms of computational complexity and extracted profit.},
  keywords = {Bayes methods,Computational modeling,Data acquisition,Data Valuation,Machine learning,Mobile Health,Online Pricing,Organizations,Pricing,Training},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XEKR7YP2/9796669.html}
}

@article{douglasCobbDouglasProductionFunction1976,
  title = {The {{Cobb-Douglas Production Function Once Again}}: {{Its History}}, {{Its Testing}}, and {{Some New Empirical Values}}},
  shorttitle = {The {{Cobb-Douglas Production Function Once Again}}},
  author = {Douglas, Paul H.},
  year = {1976},
  month = oct,
  journal = {Journal of Political Economy},
  volume = {84},
  number = {5},
  pages = {903--915},
  publisher = {{The University of Chicago Press}},
  issn = {0022-3808},
  doi = {10.1086/260489},
  abstract = {Research into the production function has a long history. Since the first work, in 1928, many studies have tended to support the hypothesis that production processes are well described by a linear homogeneous function with an elasticity of substitution of one between factors. New results are presented here, using 7 years of observations on Australian manufacturing industries during the 1950s and 1960s. In all seven cases, constant returns to scale are very closely approximated, and the coefficient for labor hovers near 0.6. The appropriate coincidence of the estimated coefficients with the shares received strengthens the competitive theory of distribution.}
}