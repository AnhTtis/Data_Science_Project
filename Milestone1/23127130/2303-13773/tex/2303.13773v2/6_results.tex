\section{Experiments and Results}\label{sec:exps}

The experiments for this paper were conducted in Python, using PyTorch, the DGL libraries, and the SCIP solver, on a server with an Intel i7-12700  16-Core (12 cores, 20 threads), an NVIDIA RTX A4000, 16 GB of memory, and Ubuntu 22.04.1 LTS 64 bits.
Implementation details are available in the paper's accompanying repository\footnote{\url{https://github.com/brunompacheco/sat-gnn}}.

In the following sections, we present three experiments addressing the two research questions raised in the introduction (Sec. \ref{sec:intro}).
In the first experiment, we train the proposed SatGNN model to classify the feasibility of candidate solutions for problem instances.
We consider three scenarios of increasing difficulty with respect to the generalization performance.
In the second experiment we train SatGNN models to predict the bias for the binary variables of problem instances, effectively learning the probability that each binary variable has to assume a positive value in an optimal solution.
We compare two training approaches, considering either the optimal solution as the target or multiple feasible solutions as targets for each instance.
Finally, we use the models trained for optimality prediction to generate candidate solutions, which are used to construct heuristics to the ONTS problem.
More specifically, we use the trained models to construct three approaches (as presented in Sec. \ref{sec:meth-heuristics}) to improve the SCIP solver in its off-the-shelf setting: warm-starting, early-fixing, and trust region.

\subsection{Dataset}\label{sec:exp-datasets}

The data for the following experiments is generated according to the procedure described in Sec. \ref{sec:data}.
The dataset construction requires verifying feasibility solving to (quasi-)optimality instances of the ONTS problem.
In other words, the dataset construction imposes a high computational cost, which scales with the size of the problem.

To circumvent the cost of dataset generation, we build a training set with many instances of smaller size, and validation and test sets with fewer-but-larger instances.
This way, we can afford a sufficient volume of data for training while evaluating on the most relevant instances, which are the ones with a higher solving cost.
At the same time, we test the model's generalization capability by evaluating its performance on instances harder than those seen during training.

We base our data generation in the instances proposed by Rigo et al.~\cite{rigo_instance_2023}, which take the FloripaSat-I mission as a reference~\cite{marcelino_critical_2020} (see \ref{appx:random-instance}).
More precisely, we focus on instances with scheduling horizon $T=125$.
%which accounts for a little more than a full orbit.

We generate 200 instances for each $J\in\{9,13,18\}$ jobs, and 40 instances for each $J\in\{20,22,24\}$ jobs, in a total of 720 instances.

The dataset is generated through Algorithm \ref{alg:dataset-generation}.
We solve each instance and gather 500 of the best solutions possible, limited to a time budget of 5 minutes.
More precisely, every instance $(I, Z^\star)\in \mathcal{D}$ is such that $|Z^\star| = 500$.
An example of the optimal schedule for one of the generated instances with $J=9$ can be seen in Figure \ref{fig:example-scheduling}.
All instances and solutions are made publicly available~\cite{pacheco_bruno_m_2023_8356798}.

\subsection{Feasibility Classification}\label{sec:exp-feas}

As described in Sec. \ref{sec:meth-feas-classification}, we build the dataset for feasibility classification $\mathcal{D}_{\rm feas}$ by adding random candidate solutions to $Z^\star$ for every $(I,Z^\star) \in \mathcal{D}$.
More precisely, for every instance of the ONTS problem, we generate $Z_R$ and $Z_N$ such that $|Z_R|=250$ and $|Z_N| = 250$, in a total of 1000 solutions for every instance.

SatGNN is modified for graph classification as described in Sec. \ref{sec:meth-feas-classification}.
We use the graph convolution proposed by Kipf and Welling~\cite{kipf_semi-supervised_2017}, as presented in equation \eqref{eq:graph-conv}, and a single layer ($L=1$).
The hidden features all have a fixed size $d=8$.
Both ${\rm NN}_{\rm con}$ and ${\rm NN}_{\rm var}$ are neural networks with a single layer and ReLU activation, while ${\rm NN}_{\rm out}$ is an artificial neural network with 3 layers and ReLU activation in the hidden layers and sigmoid activation at the last layer.
For all feasibility classification experiments, SatGNN is trained using Adam~\cite{kingma_adam_2015} with a learning rate of $10^{-3}$ to minimize the binary cross-entropy between the prediction and the feasibility of the candidate solution.
The model is trained until the training loss becomes smaller than $10^{-2}$, limited to a maximum of 200 epochs.

First, SatGNN is trained for one particular instance.
Given an instance $I$, we build $\mathcal{D}_{\rm feas}^{I} = \{ (\bm{\hat{z}},y)\in Z\times \{0,1\} : (I,\bm{\hat{z}},y)\in\mathcal{D}_{\rm feas}\}$.
This dataset is randomly divided into a training and a test set in an 80-20 split, so 800 instances are used to train the model.
For each number of jobs $J\in\{9,13,18,20,22,24\}$, we selected 5 different instances randomly.
In all experiments (all instances of all sizes), \emph{SatGNN achieved 100\%  accuracy}, that is, the model could perfectly distinguish between feasible and infeasible candidate solutions.

Then, SatGNN is trained to generalize across instances with the same number of jobs.
For each number of jobs $J$, we build the dataset by selecting only instances (and the respective candidate solutions) with the same number of jobs, i.e.,
\begin{equation}\label{eq:D-feas-jobs}
    \mathcal{D}_{\rm feas}^{J} = \{(I,\hat{\bm{z}},y)\in \mathcal{D}_{\rm feas} : J_I = J \}
,\end{equation}
where $J_I$ represents the number of jobs of instance $I$.
20 of the generated instances are randomly selected for training, in a total of 20,000 training samples, while 10 different instances are randomly selected for testing.
SatGNN had a very high performance, with over 90\% accuracy on all settings.
The complete test set performance for the different number of jobs can be seen in Figure \ref{fig:across-instances-accuracy}.

\begin{figure}
    \centering
    \includegraphics{accuracy_classification_across_instances.pdf}
    \caption{Test set performance of SatGNN trained for feasibility classification of candidate solutions given a fixed number of jobs. All instances used for testing were not seen by the model beforehand.}
    \label{fig:across-instances-accuracy}
\end{figure}

Finally, SatGNN is trained solely on candidate solutions from small instances (i.e., those with $J\in\{9,13,18\}$) and tested on candidate solutions from large instances (i.e., those with $J\in\{20,22,24\}$).
Following the notation of equation \eqref{eq:D-feas-jobs}, we use $\mathcal{D}_{\rm feas}^9\cup \mathcal{D}_{\rm feas}^{13}\cup\mathcal{D}_{\rm feas}^{18}$ for training and $\mathcal{D}_{\rm feas}^{20}\cup \mathcal{D}_{\rm feas}^{22}\cup\mathcal{D}_{\rm feas}^{24}$ for testing.
In total, 60,000 samples are used for training and 30,000 samples are used for testing.
SatGNN achieved 94.15\% accuracy in the test set.
The performance across the different sizes of instances and the groups of candidate solutions is presented in Table \ref{tab:across-sizes-accuracy}.
We see that the model had more difficulty in the candidate solutions from the $Z_N$ sets, the candidate solutions in the edge of the feasible region.
It is also true that the performance decreased with problem size (number of jobs), indicating an increasing difficulty, as expected.

\begin{table}[h]
    \centering
    \caption{Feasibility classification performance of SatGNN on instances larger than those used for training. The test set is discriminated into the three sets of candidate solutions that compose it (see Sec. \ref{sec:meth-feas-classification}). Each row indicates the set of 10 instances with the same size (number of jobs).}
    \label{tab:across-sizes-accuracy}
    \begin{tabular}{c | cccc}
    \toprule
               & \multicolumn{4}{c}{Accuracy}          \\
    \# of jobs & $Z^\star$ & $Z_N$ & $Z_R$   & Total   \\
    \midrule
    20         & 100\%     & 90\%  & 100\%   & 97.5\%  \\
    22         & 100\%     & 80\%  & 100\%   & 95\%    \\
    24         & 90\%      & 90\%  & 89.84\% & 89.96\% \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Optimal Solution Prediction}\label{sec:exp-opt-pred}

Following the methodology presented in Sec. \ref{sec:meth-sol-pred}, we first generate the datasets $\mathcal{D}_{\rm opt\text{-}b}$ and $\mathcal{D}_{\rm opt\text{-}m}$ from the dataset $\mathcal{D}$ (described in Sec. \ref{sec:exp-datasets}).
Having both datasets allows us to compare the two different training strategies: training with a single target (quasi-optimal solution), and training with the best solutions found (which include the quasi-optimal solutions).
% Note that $\mathcal{D}_{\rm opt}$ is easily generated from the $\mathcal{D}$ by just taking the best candidate solution from each $Z^\star$, while $\mathcal{D}_{\rm mopt}$ is identical to $\mathcal{D}$.
The data is divided into training, validation, and test sets.
The training sets contain the 200 small instances from $\mathcal{D}$, with 9, 13 or 18 jobs, while the validation and test sets contain, each, 20 large instances from $\mathcal{D}$, with 20, 22 or 24 jobs, such that no instance is used in both validation and test (empty intersection).

The models are trained according to the description provided in Sec. \ref{sec:meth-sol-pred}.
Both when training with the best solution for each instance and when training with multiple solutions, the models are trained using Adam to minimize the BCE between the prediction and the targets.
The validation set is used to perform hyperparameter tuning.
More specifically, learning rate, size, number of hidden features ($d$), graph convolution (from the two presented in Sec. \ref{sec:gnns}), whether the convolutions would share parameters or not, and number of layers ($L$) are determined through random search.
In our experiments with training with the best solution, SatGNN performed best on the validation set with 2 layers, 64 hidden features at each layer, and a learning rate of $10^{-2}$.
When training with multiple solutions, SatGNN performed best with 3 layers, 256 hidden features, and a $10^{-3}$ learning rate.
On both scenarios, sharing the parameters across graph convolutions and using the SAGE graph convolution was the best choice.

We use the validation set to perform early-stopping of the training, i.e., during the training budget (100 epochs), we pick the model that performed the best on the validation set.
The training curve for both training approaches can be seen in Figure \ref{fig:opt-training-curves}.
After training with the best solution, the average BCE on the validation set was 0.2887 and on the test set 0.2873.
After training with multiple solutions, the average BCE on the validation set was 0.2451 and on the test set 0.2482.
Although the BCE values are not comparable across training approaches, the small difference between the validation and the test sets for both approaches indicates no sign of overfitting.

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{training_curve_optimal.pdf}
        \caption{Best Solution}\label{fig:training-bs}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{training_curve_multi.pdf}
        \caption{Multiple Solutions}\label{fig:training-ms}
    \end{subfigure}
    \caption{Training curves for SatGNN models trained with (a) the best solution or (b) multiple solutions. The best average BCE on the validation set (highlighted in red) is used for early-stopping the training.}
    \label{fig:opt-training-curves}
\end{figure}

A deeper analysis is performed on the models' confidence when predicting variable assignments $\bm{\kappa}(\bm{z}=1|I)$, as presented in Sec. \ref{sec:meth-heuristics-ws}.
The average confidence of both models on instances of the test set is depicted in Figure \ref{fig:prediction-confidences} with respect to the binary variables of the optimization problem.
Training with multiple solutions seems to result in a more confident model overall.
Furthermore, both models provide significantly more confident predictions for the $\bm{\phi}$ variables.

% \begin{figure}
%     \centering
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics{figures/optimals_confidence.pdf}
%         \caption{Model trained with the best solution.}
%     \end{subfigure}
%     \begin{subfigure}{\textwidth}
%         \centering
%         \includegraphics{figures/multiple_confidence.pdf}
%         \caption{Model trained with multiple solutions.}
%     \end{subfigure}
%     \caption{Average confidence (probability of the predicted class) of predicted values for $\bm{x}$ and $\bm{\phi}$ variables. The average is taken over all jobs of all instances in the test set.}
%     \label{fig:prediction-confidences}
% \end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{X_confidences.pdf}
        \caption{$x_{j,t}$}\label{fig:conf-x}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Phi_confidences.pdf}
        \caption{$\phi_{j,t}$}\label{fig:conf-phi}
    \end{subfigure}
    \caption{Average confidence (probability of the predicted class) of predicted values for (a) $\bm{x}$ and (b) $\bm{\phi}$ variables of the model trained only with the best solution and the model trained with multiple solutions. The average is taken over all jobs of all instances in the test set.}
    \label{fig:prediction-confidences}
\end{figure}

\subsection{SatGNN-based Heuristic}\label{sec:exp-heuristics}

The two models presented in Sec. \ref{sec:exp-opt-pred} are used (each) to build three matheuristics, as described in Sec. \ref{sec:meth-sol-pred}.
Namely, the two models are the SatGNN trained with the best solution available and the SatGNN trained with multiple solutions (see Sec. \ref{sec:meth-sol-pred}).
The three matheuristics evaluated are warm-starting, early-fixing, and trust region.
We use SCIP as our solver both for the baseline results and the optimization within the matheuristics.

Although all model parameters and hyperparameters are already defined (and will not change) by the time the heuristics are assembled, the heuristics have their own hyperparameters, which require adjustment.
More specifically, all three heuristic approaches have the hyperparameter $N\in\mathbb{N}$, representing the size of the partial solution extracted from SatGNN.
Besides, the trust region matheuristic has the size of the trust region $\Delta\in\mathbb{N}$.

We use the validation set to select the best values for the hyperparameters, aiming to optimize two performance metrics.
For once, we select values for the hyperparameters that maximize the expected objective value.
The objective values are normalized, i.e., we divide them by the known optimal for each instance, resulting in a value between $0$ (no QoS) and $1$ (max. QoS possible for the instance).
In parallel, we select the hyperparameters that minimize the time required to find a feasible solution to each problem.
The two tunings are performed for the SatGNN model trained with the best solution available and the SatGNN trained with multiple solutions.
Table \ref{tab:best-N-delta} summarizes the best hyperparameters found for each model and each heuristic approach, in both evaluations.

\begin{table}[h]
    \centering
    \caption{Best values for partial solution size ($N$) and trust region size ($\Delta$) for each heuristic approach and each SatGNN model, as evaluated on the validation set. \emph{Objective} corresponds to the values that maximize the objective value of the best solution found. \emph{Feasibility} indicates the values that minimize the amount of time to find a feasible solution.}
    \label{tab:best-N-delta}
    \begin{tabular}{ll|cc|cc}
    \toprule
                                        &              & \multicolumn{2}{c|}{Objective} & \multicolumn{2}{c}{Feasibility} \\
    SatGNN model                        & Heuristic    & $N$         & $\Delta$        & $N$          & $\Delta$         \\
    \midrule
    \multirow{3}{*}{Best Solution}      & Warm-start   & 750         & -               & 1000         & -                \\
                                        & Early-fix    & 500         & -               & 750          & -                \\
                                        & Trust region & 1000        & 5               & 1000         & 1                \\
    \midrule
    \multirow{3}{*}{Multiple Solutions} & Warm-start   & 1750        & -               & 1500         & -                \\
                                        & Early-fix    & 1000        & -               & 1250         & -                \\
                                        & Trust region & 1250        & 1               & 1750         & 1             \\
    \bottomrule
    \end{tabular}
\end{table}

The heuristics are then evaluated on the test set, which is not used for tuning neither the deep learning models' hyperparameters nor the heuristics' hyperparameters.
The final performance is measured through the expected relative objective value given the time budget, and the time it takes the heuristics to find a feasible solution.
Furthermore, we also analyze the progress of the lower bound over time, indicating the expected performance under more restricted budgets.
Figure \ref{fig:heuristics-test-results} illustrates the results.

\begin{figure}
    \centering
    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics{heuristic_test_obj.pdf}
        \caption{Best solution found within the time budget.}
        \label{fig:heuristics-test-results-obj}
    \end{subfigure}
    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics{heuristic_test_feas.pdf}
        \caption{Time to find a feasible solution.}
        \label{fig:heuristics-test-results-feas}
    \end{subfigure}
    \caption{Test set performance of the SatGNN-based matheuristics. On the left, we have the distribution of the evaluation metric of interest over the instances of the test set for the multiple approaches, in which the triangle indicates the mean value and the circles indicate outliers. \emph{MS} indicates that the SatGNN model trained with multiple solutions was used, whereas \emph{OS} indicates that the model trained solely with the optimal solution was used instead. On the right is the average progress of the lower bound on all test set instances. The objective value is considered relative to the known optimal value; thus it always lies in the unit interval. The heuristics' hyperparameters $N$ and $\Delta$ are defined upon experiments on the validation set, as presented in Table \ref{tab:best-N-delta}.}
    \label{fig:heuristics-test-results}
\end{figure}

To assess the significance of the results presented in Figure \ref{fig:heuristics-test-results}, we apply the Wilcoxon signed-rank test~\cite{wilcoxon_1945}, which is a non-parametric version of the t-test for matched pairs.
We apply the test pair-wise, comparing each matheuristic approach to every other matheuristic approach, and to the baseline.
The results are summarized in the critical difference diagram of Figure \ref{fig:cdds}.
Through early-fixing, both SatGNN models provided statistically significant ($p$-value$>0.05$) improvements over the baseline in both performance metrics.
However, the results show a significant advantage of the model trained with multiple solutions, providing better solutions (objective value) through early-fixing and trust region, and speeding up the feasibility definition (time to find a feasible solution) through all heuristic approaches.

\begin{figure}
    \centering
    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics{cdd_obj.pdf}
        \caption{Best solution found within the time budget.}
        \label{fig:cdd-obj}
    \end{subfigure}
    \begin{subfigure}{0.99\textwidth}
        \centering
        \includegraphics{cdd_feas.pdf}
        \caption{Time to find a feasible solution.}
        \label{fig:cdd-feas}
    \end{subfigure}
    \caption{Critical difference diagram presenting the average test set performance of the SatGNN-based matheuristics (round marker in the axis). A crossbar between two (or more) approaches indicates that their performance (distribution on the test set) was not deemed significantly different ($p$-value$>0.05$) through the paired Wilcoxon signed-rank test~\cite{wilcoxon_1945}.}
    \label{fig:cdds}
\end{figure}

Through the SatGNN model trained with multiple solutions, a 43\% increase in the expected objective value within the time budget was achieved by early-fixing the partial solution, while defining a trust region around that same partial solution resulted in an expected 45\% increase.
Although the results are close, we see from the progress of the lower bound (in Fig. \ref{fig:heuristics-test-results-obj}, the plot on the right) that the early-fix heuristic was able to find better solutions more quickly than the trust region method during the time budget.
In terms of feasibility, the early-fixing strategy using the same SatGNN model reduced in 35\% the expected time to find a feasible solution, having a significant advantage over all other heuristic approaches.
Surprisingly, even when optimized (through hyperparameter tuning) for reducing the time to find a feasible solution, the early-fix heuristic still improved the expected objective value of the best solution found in 41\%, as the lower bound progress illustrates (in Fig. \ref{fig:heuristics-test-results-feas}, the plot on the right).

\subsubsection{Partial Solution Size}\label{sec:exp-N}

To evaluate the impact of the value of $N$, the number of variables in the partial solution being fed to the matheuristics, we further evaluate the SatGNN model on the test.
More specifically, we vary the value of $N$ for all matheuristics and measure the impact on the test set considering both perspectives (objective value and time to find a feasible solution).
For completeness, we also vary $\Delta$, the size of the trust region, to evaluate the full potential of the matheuristic.
For conciseness, we consider only the SatGNN model trained with multiple solutions, as it was overall superior to the model trained solely with the best solution in our experiments.
The results are illustrated in Figure \ref{fig:N-impact}.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.59\textwidth}
        % \centering
        \includegraphics[width=\textwidth]{N_impact_obj.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.40\textwidth}
        % \centering
        \includegraphics[width=\textwidth]{N_impact_feas.pdf}
        \label{fig:cdd-feas}
    \end{subfigure}
    \caption{Performance of the matheuristics for varying values of $N$, the number of binary variables in the partial solution. \emph{EF} indicates the early-fixing matheuristic, \emph{TR} the trust region, and \emph{WS} warm-start. The SatGNN model trained with multiple solutions is used. The evaluation is performed on the test set.}
    \label{fig:N-impact}
\end{figure}

As expected, the number of variables that compose the partial solution is impactful for all approaches, with early-fixing being the one most sensitive to it, and warm-start the less sensitive one.
Furthermore, we note a relation between $N$ and $\Delta$ through the peak performance of the trust region method (including early-fixing, which is equivalent to trust region with $\Delta=0$), that is, larger partial solutions seem to require larger trust regions.
Intuitively, this result is expected since the partial solutions are based on confidence, and, thus, the larger the partial solution is, the higher the expected probability of it including wrongly predicted variables.
Therefore, this results suggests that the SatGNN model is properly trained (its confidence correlates to its performance).
