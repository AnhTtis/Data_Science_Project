{
    "arxiv_id": "2303.15669",
    "paper_title": "Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages",
    "authors": [
        "Seongyeon Park",
        "Myungseo Song",
        "Bohyung Kim",
        "Tae-Hyun Oh"
    ],
    "submission_date": "2023-03-28",
    "revised_dates": [
        "2023-03-29"
    ],
    "latest_version": 1,
    "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG"
    ],
    "abstract": "Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are available at: https://github.com/cnaigithub/SpeechDewarping",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15669v1"
    ],
    "publication_venue": "ICASSP 2023"
}