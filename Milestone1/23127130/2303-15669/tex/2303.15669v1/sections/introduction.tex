% !TEX root = ../main.tex

\section{Introduction}
\label{sec:introduction}
Recent advance in deep neural networks enables us to build end-to-end text-to-speech (TTS) models~\cite{wang2017tacotron, shen2018natural} to synthesize plausible speech.
Recent research~\cite{chung2019semi, zhang2020unsupervised} attributes natural and plausible speech generation of TTS models to the following capabilities to be learned: 1) \emph{attention alignment} between the input and output sequences, and 2) \emph{autoregressive prediction} of acoustic features.
The supervised learning or pre-training methods~\cite{chen2018sample,moss2020boffin} directly inject the necessary capabilities for TTS through supervision using large-scale transcribed speech. 
However, such models require a large amount of transcribed speech data for training, which is not annotation efficient.
Constructing such large-scale text-annotated speech is time-consuming, costly, and even infeasible for low-resource languages.


To mitigate the labeled data deficiency, pre-training methods for TTS systems have been investigated~\cite{chen2018sample,moss2020boffin,chung2019semi,zhang2020unsupervised}.
Among them, Chung~\etal and Zhang~\etal\cite{chung2019semi, zhang2020unsupervised} specifically designed to induce either of such capabilities in unsupervised ways by leveraging large-scale untranscribed speech data.
In \cite{chung2019semi}, the decoder of Tacotron~\cite{wang2017tacotron} is pre-trained as an autoregressive speech generator.
In \cite{zhang2020unsupervised}, the whole model of Tacotron~2~\cite{shen2018natural} is pre-trained to predict speech from unsupervised linguistic units extracted by an external Vector-quantization Variational-Autoencoder (VQ-VAE)~\cite{chorowski2019unsupervised}.
It would be desirable to pre-train the full TTS model without any external model.

The goal of this paper is to further reduce the amount of transcribed speech required for TTS training.
To this end, we propose an unsupervised pre-training method for Tacotron~2, \emph{Speech De-warping}.
By utilizing large-scale \emph{untranscribed} speech, our key idea is to make the TTS model learn to reconstruct original spectrograms from warped ones, \ie, learn to \emph{de-warp}.
This method does not require annotation, as we synthesize the warped spectrograms by a simple random temporal warping technique.
We sample random segment boundaries and resize
each segment along the temporal axis to be a fixed size.
Learning to de-warp as a pre-training step encourages the model to acquire both preliminary knowledge of attention alignment and autoregressive prediction.
After the pre-training, we fine-tune the model using small-scale transcribed speech data of a target speaker, possibly in a low-resource language.
In addition, we extend our simple random warping technique to a data augmentation method for the fine-tuning step, which further improves performance.

Compared to the previous studies, our pre-training method does not suffer from the model mismatch problem between pre-training and fine-tuning~\cite{chung2019semi} and does not require training an external model for data preparation~\cite{zhang2020unsupervised}.
It is also worth noting that our data augmentation does not require any external data or pre-trained models unlike other data augmentation approaches for TTS~\cite{hwang2021tts,song2022tts,oh2022effective,comini2022low,terashima2022cross,huybrechts2021low};
they typically leverage a large amount of transcribed speech to generate synthetic data with pre-trained TTS models or voice conversion models.

Our main contributions are summarized as follows:
1) proposing an unsupervised pre-training method for TTS models, \emph{Speech De-warping},
2) proposing a simple yet effective data augmentation method, \emph{SegAug},
3) demonstrating improved data efficiency,
and 4) showing the cross-language effectiveness of our methods.