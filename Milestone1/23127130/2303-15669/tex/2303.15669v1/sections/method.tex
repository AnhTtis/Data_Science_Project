% !TEX root = ../main.tex

\section{Proposed Method}
\label{sec:method}

\subsection{Segment-based Speech Warping}
Our pre-training and data augmentation methods include the procedure of warping speech.
To warp the speech, we segment the mel-spectrograms of the speech along the time axis and apply a transformation per segment.
To clarify the procedure, we describe the general form of the segment-based speech warping $f$.


Given a mel-spectrogram $\m$ of timesteps $N$, we warp $\m$ to generate a warped mel-spectrogram $\hat{\m}$, which is given by
\begin{equation}
    \label{eqn:warping}
    \hat{\m} = f(\m; S, T),
\end{equation}
where $S$ is a segmentation method, and $T$ is a transformation.
The segmentation method $S$ segments $\m$ into $k$ different spectrogram segments $\m_{1}, \m_{2}, ..., \m_{k}$ such that $N = \sum_{i=1}^{k}N_{i}$, where $N_{i}$ is the number of timesteps of $\m_{i}$.
Then, for each segment $\m_{i}$, the transformation $T$ transforms $\m_{i}$ to a warped segment $\hat{\m_{i}} = T(\m_{i})$.
We concatenate the warped segments along the time axis to generate the warped spectrogram $\hat{\m} = concat(\hat{\m_{1}}, \hat{\m_{2}}, ..., \hat{\m_{k}})$.

Theoretically, any segmentation method and transformation can be used as $S$ and $T$, \eg, phoneme segmentation for $S$.
We present our specific configuration for $S$ and $T$ in the following subsections.


\subsection{Pre-training: Unsupervised Speech De-warping}
\begin{figure}[t]
% \vskip 0.2in
\centering
% \begin{center}
% \centerline{
\includegraphics[width=1\columnwidth]{figures/icassp_ours.pdf}
% }
% \vspace{-3mm}
\caption{Overview of our
% Our unsupervised pre-training method, 
\emph{Speech De-warping}.
We randomly segment the spectrogram and warp it by resizing each segment to have equal unit timesteps.
% ,  \ie, 1.
The model is learned to reconstruct the original spectrogram from the warped one.
}
\label{fig:our_framework}
% \end{center}
% \vspace{-4mm}
\end{figure}


We aim to reduce the amount of transcribed speech required for TTS training.
To this end, we propose an unsupervised pre-training method, \emph{Speech De-warping} which leverages large-scale \emph{untranscribed} speech, which is much cheaper to obtain than transcribed one.
The main idea is to pre-train a TTS model to recover original spectrograms from warped ones, which is illustrated in Figure~\ref{fig:our_framework}.


To generate pairs of input and expected output for unsupervised learning, we first generate warped spectrograms from the original spectrograms converted from the untranscribed speech by the segment-based speech warping (see Equation~\ref{eqn:warping}).
Specifically, we use random segmentation as $S$, which randomly selects $k-1$ number of boundary timesteps; thus, 
$k$ segments are obtained.
The segment boundaries are independently sampled for each training step.
We set $k = \lfloor \frac{N}{6} \rfloor$ for each spectrogram.
For the transformation $T$, we use linear interpolation to make each segment have an equal unit timestep (\ie, length 1).

We adopt Tacotron 2~\cite{shen2018natural} as our backbone TTS model and denote it as Tacotron for simplicity.
Tacotron has the input format of text embedding; thus, the spectrogram inputs are not directly applicable.
To feed the warped spectrograms to the model's encoder as input, we replace the text embedding look-up table of Tacotron with a simple 1D convolutional layer.
It maps the mel-dimension to the embedding dimension of the Tacotron encoder during unsupervised training.


Other segmentation methods can be used as $S$ to generate segments instead of our proposed random segmentation, \eg, more semantically aligned segments like exact phonemes.
For example, one can adopt the Montreal Forced Alignment (MFA)~\cite{mcauliffe2017montreal} tool to extract phoneme segments using text annotations. 
However, it is not applicable in our unsupervised pre-training setting, where no text annotation is available.
Instead, one can use unsupervised pseudo phoneme segmentation~\cite{kreuk2020self}.
We empirically show that these semantic segmentation methods can improve the performance of \emph{Speech De-warping}, but the simple random segmentation is powerful enough to outperform other baselines.

\subsection{Fine-tuning: Transferring Knowledge to TTS}
After pre-training Tacotron with the pretext task, we fine-tune the model with the downstream TTS task of a target speaker.
We use a few transcribed speeches, \ie, text-audio pairs, of the target speaker to fine-tune the model.
Before starting fine-tuning, to feed texts to the model as in the original Tacotron, the 1D convolutional layer preceding the encoder is discarded, and a learnable text embedding look-up table for the target speaker's language is randomly initialized. 
With this reconfiguration, we fine-tune the TTS model for the small target speaker data.

\vspace{2mm}
\paragraph{Data Augmentation}
To further improve data efficiency during fine-tuning, we propose a simple data augmentation method called \emph{SegAug}.
During fine-tuning, we augment the training data by applying the segment-based speech warping (Equation~\ref{eqn:warping}) to the target spectrograms.
Specifically, for $S$, we use random segmentation as in the pre-training stage.
For the transformation $T$, we use linear interpolation to resize each segment of the input spectrogram along the time axis by a factor uniformly sampled from $[\frac{1}{3}, \frac{5}{3}]$.
The resulting warped spectrograms are used as the target spectrograms for training loss.
After training the model with this augmentation, we additionally train the model for a few steps without the augmentation to adapt the model to the ground truth prosody of the target speaker, \ie, a cool-down step.
Note that this augmentation in the fine-tuning stage is optional.
While our pre-training alone empirically demonstrates favorable performance, we can further improve the performance with this augmentation during fine-tuning.
