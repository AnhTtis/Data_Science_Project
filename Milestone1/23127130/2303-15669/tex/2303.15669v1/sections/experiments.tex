% !TEX root = ../main.tex


\section{Experiments}
\label{sec:experiments}
\subsection{Experiment Setup}
\label{sec:exp_setup}

\paragraph{Dataset and Evaluation}
We use the \textit{train-clean-100} subset of the LibriTTS~\cite{zen2019libritts} dataset as the untranscribed pre-training set, which consists of 47.6 hours of speech from 247 English speakers.
We set Korean as a low-resource language and use the Korean Single speaker Speech (KSS)~\cite{park2018kss} dataset 
as our transcribed fine-tuning set.
Following \cite{chung2019semi,zhang2020unsupervised}, we define 24 minutes of speech as 1~shard of data.
Then, we construct fine-tuning datasets by randomly sampling 0.5, 1, 2, 3, 5, 8~shards from the KSS dataset.
For evaluation, we conduct both objective and subjective tests.
For the objective evaluation, we use Mel-cepstral Distortion with Dynamic time-warping (MCD-DTW) \cite{kubichek1993mel}, simply denoted as MCD. 
The objective results are reported as an average over the test set containing 571 utterances (about 22.7 minutes in total).
For the subjective evaluation, we conduct AB preference tests on 20 utterances randomly sampled from the test set.
We ask 15 native Korean raters to choose the more preferred one among two synthesized audios given the text, in terms of pronunciation, recognizability, and naturalness.


\paragraph{Implementation details}
For pre-training, we use the Adam \cite{kingma2015adam} optimizer with a learning rate of $10^{-3}$.
The models are pre-trained for 100K steps with batch size 16.
For fine-tuning, we gradually decrease the learning rate from $10^{-3}$ to $10^{-4}$ for 50K training steps with batch size 32.
Audio waveforms are down-sampled to 16~kHz, and Griffin-Lim~\cite{griffin1984signal} algorithm is used as a vocoder for fast experiment cycles. 


\paragraph{Compared methods}
We use Tacotron~2~\cite{shen2018natural} as the TTS model in our experiments.
Following the naming conventions of Zhang~\etal\cite{zhang2020unsupervised} with T(acotron), we denote the model only trained with the fine-tuning data without pre-training by Tac. 
We denote two recent unsupervised pre-training methods, decoder pre-training~\cite{chung2019semi} and VQ-VAE-based pre-training~\cite{zhang2020unsupervised}, by T-Dec and T-VQ, respectively.
The model pre-trained with our \emph{Speech De-warping} is denoted by T-SD, \ie, ours.
As an upper bound of performance for the unsupervised pre-training methods, we employ the model pre-trained in a supervised manner with text annotations and denote it by T-Pho, as suggested by Zhang~\etal\cite{zhang2020unsupervised}.
In addition to the pre-training methods, we compare our data augmentation with other data augmentation methods in the fine-tuning stage.
We denote 
additive Gaussian noise-based augmentation~\cite{moell2022speech} by Gaussian,
mixup-based augmentation~\cite{guo22e_interspeech} by Mixup,
SpecAugment~\cite{park2019specaugment} by SpecAug.


\subsection{Results on Small Amount of Fine-tuning Data}
\begin{table}[]
\caption{
MCD results of several pre-training and data augmentation methods when being fine-tuned on 0.5 or 1 shard (12 or 24 minutes) of paired speech of the target speaker.
Note that T-Pho leverages text annotations in pre-training.\vspace{2mm}
}
\label{tab:main_mcd_0.5_shards}
\resizebox{\linewidth}{!}{%
\begin{tabular}{clccc}
\toprule
Augmentation
& \multicolumn{1}{l}{\multirow{2}{*}{Model}} & Supervised & \multicolumn{2}{c}{Paired data (in shards)}  \\ 
\cline{4-5}
in fine-tuning & \multicolumn{1}{l}{}                       &       pre-training              & 0.5            & 1                           \\ 
\hline
\multirow{5}{*}{No aug.}       & Tac                                        & $\times$                           & 11.98          & 12.41                       \\
                                        & T-Dec                                      & $\times$                           & 12.07          & 12.18                       \\
                                        & T-VQ                                       & $\times$                           & 11.11          & 10.41                       \\
                                        & T-SD (Ours)                  & $\times$                           & \textbf{10.79} & \textbf{10.40}              \\
                                        & T-Pho                                      & $\bigcirc$                           & 10.40          & 10.28                       \\ 
\hline
\multirow{9}{*}{With aug.}        & \multicolumn{2}{l}{Tac + Gaussian}                                      & 12.59          & 12.33                       \\
                                        & \multicolumn{2}{l}{Tac + Mixup}                                         & 12.06 & 12.04                       \\
                                        & \multicolumn{2}{l}{Tac + SpecAug}                                       & 12.29          & 10.60              \\
                                        & \multicolumn{2}{l}{Tac + SegAug}                                      & 12.19          & 10.68                       \\ 
\cdashline{2-5}[1pt/1pt]
                                        & \multicolumn{2}{l}{T-VQ + Gaussian}                                     & 10.63          & 10.40                       \\
                                        & \multicolumn{2}{l}{T-VQ + Mixup}                                        & 11.12          & 10.48                       \\
                                        & \multicolumn{2}{l}{T-VQ + SpecAug}                                       & 10.46          & 10.33                       \\
                                        & \multicolumn{2}{l}{T-VQ + SegAug}                                     & 10.41 & 10.27              \\ 
\cdashline{2-5}[1pt/1pt]
                                        & \multicolumn{2}{l}{T-SD + SegAug (Ours)}                                     & \textbf{10.28} & \textbf{10.24}              \\
\bottomrule
% \hline
\end{tabular}
}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph {Objective Evaluation}
Table~\ref{tab:main_mcd_0.5_shards} presents the superior performance of the proposed methods compared to competing methods on small amounts of fine-tuning data.
Without data augmentation during fine-tuning, T-SD outperforms all unsupervised pre-training methods and Tac.
Both T-Dec~\cite{chung2019semi} and Tac, which do not have the opportunity to pre-learn a sufficient capability of attention alignment in pre-training, show similarly lower performance than the others.
In contrast, the proposed de-warping task encourages the model to learn both preliminary knowledge of attention alignment and autoregressive prediction.
When data augmentation is applied during fine-tuning, T-SD with \emph{SegAug} outperforms other combinations of pre-training and augmentation methods.
\emph{SegAug} even effectively improves the performance of other pre-training baselines and shows competitive performance compared to other augmentation methods.


\paragraph{Subjective Evaluation}
Table~\ref{tab:main_ab_test} shows the preference test results with competitive methods using 0.5 shards of fine-tuning data.
Consistent with the objective results, our methods outperform the prior art of unsupervised methods (T-VQ).
Interestingly, with our proposed data augmentation applied during fine-tuning, our whole transfer learning scheme even outperforms the supervised pre-training baseline (T-Pho).


\begin{table}[t]
\caption{
AB test results of our method over competitive baselines.
All methods use 0.5 shards (12 minutes) of fine-tuning data.\vspace{2mm}
}\label{tab:main_ab_test}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{Model pair} & \multicolumn{3}{c}{Preference (\%)} \\
\cline{2-4}
    & Former & Latter & Neutral \\
\midrule
T-VQ vs. T-SD    & 15.7 & \textbf{54.7} & 29.6 \\
T-VQ vs. T-SD + SegAug    & 3.7 & \textbf{76.0} & 20.3 \\
T-Pho vs. T-SD + SegAug   & 23.3 & \textbf{44.0} & 32.7 \\
\bottomrule
\end{tabular}
}
\end{table}



\begin{figure}[t]
% \vspace{-2mm}
  \centering
  \includegraphics[width=\linewidth]{figures/main_mcd_final.pdf}
%   \vspace{-6mm}
  \caption{
MCD results according to varying 
% for several 
amounts of paired data.
The dashed line denotes the MCD of T-Pho, which is a supervised method; thus, it can be considered a near-upper-bound performance of unsupervised pre-training methods.
  }
  \label{fig:main_mcd}
\end{figure}





\begin{table}[t]
\caption{
MCD results for the ablation study comparing our de-warping to the up-sampling pre-training.
\emph{Naive} indicates the model pre-trained with the up-sampling task.
}
\label{tab:ablation_mcd_naive}
\vspace{2mm}
% \vskip 0.15in
% \begin{center}
\centering
\resizebox{0.65\linewidth}{!}{%
\begin{tabular}{ccc} 
\toprule
% \hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{Paired data (in shards)}  \\ 
\cline{2-3}
                        & 0.5            & 1                           \\ 
\hline
Naive~                  & 11.37          & 10.88                       \\
T-SD                    & \textbf{10.79} & \textbf{10.40}              \\
\bottomrule
\end{tabular}}
\end{table}

\begin{table}
% \vspace{3mm}
\caption{
MCD results of T-VQ and our \emph{Speech De-warping} according to different segmentation methods on two fine-tuning languages.
Different and Same denote that the fine-tuning language is different or the same as the pre-training language. 
Note that phoneme segmentation requires text supervision.
}\vspace{2mm}
\label{tab:ablation_english}
% \vskip 0.15in
% \begin{center}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc} 
\toprule
% \hline
\multirow{3}{*}{Method}      & \multicolumn{4}{c}{Paired data (in shards)}                        \\ 
\cline{2-5}
                             & \multicolumn{2}{c}{Different}   & \multicolumn{2}{c}{Same}         \\ 
\cline{2-5}
                             & 0.5            & 1              & 0.5            & 1               \\ 
\hline
T-VQ~                        & 11.11          & 10.41          & 11.85          & 10.51           \\
T-SD (Random segment)      & 10.79          & 10.40          & 11.57          & 10.63           \\
Pseudo phoneme segment& {10.56} & 
{10.38} & 11.71          & 10.69           \\
Phoneme segment             & 11.35          & 10.48          & {11.19} & {10.46}  \\
\bottomrule
% \hline
\end{tabular}
}
\end{table}

\subsection{Effects of 
% Results on Other
Different Amounts of Fine-tuning Data}

Figure~\ref{fig:main_mcd} presents the MCD evaluation results of the competing
% various
methods according to
% on
different 
% several
amounts of fine-tuning data.
Our method shows the best performance overall and is particularly better on small amounts of data.
As the amount of fine-tuning data increases, the models tend to show better performance, and the performance gaps between the methods gradually decrease.

\begin{figure*}[t]
  \centering
  \setlength\tabcolsep{0.02\linewidth}
  \begin{tabular}{ccc}
  \includegraphics[width=0.3\linewidth]{figures/obvious_alignment.pdf} &
  \includegraphics[width=0.3\linewidth]{figures/random2.pdf} &
  \includegraphics[width=0.3\linewidth]{figures/tphone.pdf}  
 \\
  (a) \emph{Naive} & (b) T-SD & (c) T-Pho\\

  \end{tabular}
  \caption{
  Examples of the learned attention alignments between input and output timesteps of the decoders of the models during pre-training.
  While \emph{Naive} induces the model to learn a linear alignment, our T-SD encourages the model to learn a non-linear alignment whose form is similar to the alignment between text and speech as in T-Pho.
  }
  \label{fig:obvious/non-obvious alignment}
\end{figure*}


\subsection{Additional Results}
\paragraph{Comparison to an upsampling pre-text task}
In our \emph{Speech De-warping}, we resize segments of different lengths into the same timestep 1 to warp the input spectrograms.
As a result, the alignment between the warped spectrogram and the original one becomes non-linear, which is analogous to 
% like 
the alignment characteristics between text and speech.
We argue that \emph{learning this monotonic yet non-linear alignment in pre-training is one of the critical factors} of our method.
To validate this argument, we introduce a control experiment with simple upsampling pre-training as a pre-text task, called \emph{Naive}, and compare it with our \emph{Speech De-warping} in Table~\ref{tab:ablation_mcd_naive}.
Specifically, in \emph{Naive}, 
instead of using the segment-wise warping to warp the spectrogram, we downsample the whole spectrogram by a single scale factor of $\tfrac{1}{6}$
using linear interpolation along the time axis.
Thereby, the model with \emph{Naive} learns a linear alignment between the uniformly downsampled spectrograms and the original spectrograms.
Figure~\ref{fig:obvious/non-obvious alignment} presents examples of attention alignments learned during pre-training.
The superior performance of T-SD compared to \emph{Naive} in Table~\ref{tab:ablation_mcd_naive} verifies that learning a \emph{monotonic and non-linear alignment} benefits our \emph{Speech De-warping}.
Note that \emph{Naive} performs better than Tac and T-Dec in Table~\ref{tab:main_mcd_0.5_shards}, which demonstrates the effectiveness of learning \emph{monotonic alignment} through the upsampling task itself.


\paragraph{Effect of Heterogeneous Languages in Fine-tuning}
We investigate the effect of using different or the same languages between pre-training and fine-tuning steps, which is the main scope of this work.
As described, we use English as pre-training data.
For the same language scenario, called \emph{Same}, we use the LJspeech~\cite{ljspeech17} dataset (English) for fine-tuning data.
The different language scenario follows the same setup described in Sec.~\ref{sec:exp_setup}, called \emph{Different}.
We compare our T-SD with T-VQ to show the algorithmic behavioral differences.
Table~\ref{tab:ablation_english} shows the performance of T-SD is overall similar to T-VQ when the language between pre-training and fine-tuning is unchanged, \ie, \emph{Same}.
However, as shown in the \emph{Different} columns, T-SD is more robust against overfitting to the pre-training language than T-VQ.
We conjecture this is because the burden to memorize acoustic features of the pre-training language is less for our method since some language-specific acoustic information is already given as input for de-warping.



\paragraph{Effect of Segmentation Methods}
We investigate the effect of different segmentation methods for the segment-based speech warping in \emph{Speech De-warping}.
In addition to the random segmentation used in our T-SD, we compare with the phoneme segmentation by using the MFA tool~\cite{mcauliffe2017montreal}, which requires text supervision, and the pseudo phoneme segmentation by using the unsupervised phoneme segmentation model~\cite{kreuk2020self}.
As shown in Table~\ref{tab:ablation_english}, the performance of \emph{Speech De-warping} can be boosted by using semantically meaningful segmentation obtained from external models.
The phoneme segmentation shows the best performance when the fine-tuning language is the same as the pre-training language and the worst when the fine-tuning language is unseen during pre-training.
The phoneme segmentation of a specific language induces the alignment of the warped spectrograms and original spectrograms to be very similar to the alignment between text and speech of that language in pre-training.
This behavior can lead to overfitting to the specific language used in pre-training.
