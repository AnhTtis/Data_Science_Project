% !TEX root = ../main.tex


\section{Conclusion}
\label{sec:conclusion}
We propose an unsupervised pre-training method and a data augmentation method for training TTS models with limited amounts of text-annotated speech data.
Our pre-training method enables us to build a TTS system for a low-resource language by leveraging a large-scale and untranscribed speech dataset that can be easily collected.
The proposed data augmentation technique can be used to further improve such data efficiency.
Our comprehensive experiments show the superior
performance of the proposed methods compared to various competing pre-training and data augmentation methods.
We empirically demonstrate that learning a non-linear alignment during pre-training of the model is beneficial in TTS compared to learning a linear alignment.
We show that our pre-training method can achieve better performance by using external models for segmentation.
