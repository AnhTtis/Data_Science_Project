\subsection{Results}

\noindent\textbf{Feature-level matching \& model ablations.}
Figure~\ref{fig:gso_pr_curves} presents precision/recall curves for LFM-3D on the Google Scanned Objects dataset for two object classes: \emph{shoe} (Fig.~\ref{fig:gso_pr_curves_a}) and \emph{camera} (Fig.~\ref{fig:gso_pr_curves_b}). 
We train class-specific matchers on only \textit{shoe} and \textit{camera} data and use NOCS and MDE as the 3D signals, respectively.
We compare LFM-3D against model variants that allow us to ablate the importance of the different components in our system: finetuning, usage of 3D signals, and improved positional encoding - all with a maximum of 1024 features.
To generate the curves, we sweep over confidence values in $[0, 1]$ to filter match predictions.
We use the following terminology for variants:

\begin{itemize}
    \item \textit{SG-MegaDepth} - Default SuperPoint + SuperGlue model.
    \item \textit{SG-Finetuned} - Default SuperPoint + SuperGlue model, initialized with \textit{SG-MegaDepth} and finetuned on Google Scanned Objects rendered pairs.
    \item \textit{SG + NOCS/MDE} - Finetuned SuperPoint + SuperGlue model, with an additional input encoding layer for 3D signals per keypoint (Eq.~\eqref{eq:keypoint-encoder-3d}). We use NOCS for \emph{shoe} and inverse depth estimates for \emph{camera}.
\end{itemize}

We observe that LFM-3D outperforms all baseline methods, with the most significant improvements being in maximum recall. 
The NOCS-based LFM-3D outperforms \textit{SG-Finetuned} on recall by $6.2\%$ and at maximum recall for \textit{SG-Finetuned} ($44.3\%$), it sees a $28.9\%$ increase in precision.
The significant gap between \textit{SG + MDE} and \textit{LFM-3D (MDE)} indicates that 
an effective positional encoding can enhance the model's capability to integrate low-dimensional 3D information.
Lastly, we note a large performance gap between the baseline \textit{SG-MegaDepth} method and all methods finetuned on synthetic renderings, indicating the importance of training learnable matchers on domain-specific data.

\noindent\textbf{Class-specific relative pose estimation.}
We evaluate relative pose estimation on the Objectron~\cite{objectron2021} dataset, by sampling pairs of video frames with camera axes angle distances between $90^{\circ}$-$120^{\circ}$. 
We limit keypoint extraction to a segmentation mask to minimize the domain gap.
We compare LFM-3D against conventional and state-of-the-art feature matching techniques; results are presented in Table~\ref{tab:rel_pose_results}.

Focusing on the first two columns of Table~\ref{tab:rel_pose_results}, we compare LFM-3D to all relevant benchmarks.
LFM-3D \textit{(class-specific)} and SuperPoint+SG-Finetuned \textit{(class-specific)} are both finetuned only on rendered pairs for their respective object classes.
As before, the \emph{shoe} LFM-3D model uses estimated NOCS maps and the \emph{camera} LFM-3D model uses estimated depth maps.
The LFM-3D model based on NOCS outperforms all other methods for the \emph{shoe} case.
Here, we observe a large gap between our method (row 11) and the finetuned 2D-only SuperPoint+SuperGlue (row 9): by $8.6$\% for Acc$@15^{\circ}$. 
More interestingly, we also outperform NOCS-based PnP variants by large margins (up to $10.4\%$ 
against the dense version (row 4)), which shows the limitations of directly using estimated NOCS maps for relative pose prediction.
Similarly, our depth-based \emph{camera} LFM-3D model outperforms all other class-specific methods. Although we observe a less drastic improvement over the 2D-only techniques, we note that our method can directly use an off-the-shelf MDE model to provide up to $1.5\%$ improvement.

\noindent\textbf{Relative pose estimation on novel classes.}
The \textit{generic} variants of SuperPoint+SG-Finetuned and LFM-3D (MDE) are trained on the entire GSO catalog of objects, including \textit{shoes} and \textit{cameras}. 
Results on the five other Objectron categories are aggregated into a single set of \textit{unseen} results, but per-class results are included in the appendix.
We note that the generic LFM-3D MDE model (row 13) outperforms the generic SuperPoint+SG-Finetuned (row 10) on \emph{all seen and unseen object classes}, by an average of $1.4\%$ for Acc@5$^{\circ}$ 
for unseen objects. Even further, generic LFM-3D outperforms (row 13) the class-specific LFM-3D camera model (row 12).
From these results, we observe that LFM-3D demonstrates an ability to improve from large amounts of data. The strong performance on the \textit{unseen} category shows that LFM-3D can also generalize well.

\noindent\textbf{Qualitative results and limitations}
Figure~\ref{fig:qual} shows how LFM-3D proposes improved correspondences between images with relatively small overlapping visible regions. 
\textit{SG-MegaDepth} suffers from low confidence predictions under very-wide baselines, but still correctly predicts many correspondences for minor viewpoint changes (Figure~\ref{fig:qual_gso}, bottom left). Finetuning SuperGlue results in higher-confidence predictions, but adding 3D signals with positional encoding leads to more correct matches and a wider distribution of proposed correspondences across the object's geometry (Figure~\ref{fig:qual_objectron}), which simplifies essential matrix estimation.

The disparity between LFM-3D performance on NOCS and MDE shows that the model can be sensitive to the choice of 3D signal. Figure~\ref{fig:qual_failure} shows examples where LFM-3D still struggles to match reliably. In particular, we observe that feature-less objects (first column) negatively affect local feature extractors and 3D signal estimators alike. Out-of-distribution geometries (heel shoes in the second column, camera strap in the third column) can also confuse the geometry estimation model and match poorly at inference time.