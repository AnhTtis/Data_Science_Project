\section{Experiments}
\label{sec:experiments}
\subsection{Datasets}

We focus our experiments on object-centric image pairs taken from varying angles. We use synthetic renderings of 3D models for training, and evaluate on both synthetic and real-world image pairs. 

\noindent\textbf{Google Scanned Objects~\cite{downs2022google}.}
We use the high-quality scanned models with realistic textures provided by the Google Scanned Objects (GSO) dataset to train the learnable matcher components of LFM-3D. 
The GSO catalog features over 1,000 3D models of a diverse set of object categories, including \textit{shoes}, \textit{cameras}, \textit{toys}, \textit{bags}, \textit{car seats}, \textit{keyboards}, etc.
To define the canonical coordinate space for a category, we simply scale all the models so they have the same length (about $30cm$). Note that the 3D models were also prealigned during the capture process.

We render pairs of images with random poses, image dimensions, aspect ratios, focal lengths, and crops, and each rendered frame also provides ground truth camera matrices and depth maps.
To generate ground truth correspondence data, we extract local features from each image and use the camera/depth information to project a keypoint set from one image onto the other.
Ground truth matches are assigned via mutual nearest neighbors in reprojected pixel coordinate space, given a 3 pixel reprojection error threshold.

\noindent\textbf{Objectron~\cite{objectron2021}.}
The Objectron dataset contains object-centric videos with wide viewpoint 
coverage for nine object categories. Each video frame is 
labeled with camera intrinsics and extrinsics, and shows the instance in a 
real-world scene. To overcome the domain gap between synthetic and real-world 
images, we filter out all frames with $> 1$ object instance and use an off-the-shelf segmentation model to limit extracted features to the instance segmentation mask. The lack of metric depth data prevents us from training on Objectron frame pairs, but we leverage the freely available camera matrices for pose estimation evaluation purposes.



\noindent\textbf{Evaluation pairs.}
We sample image pairs at wide baselines to evaluate matcher performance in challenging scenarios. 
Specifically, we select two frames from the same 
scene and use the angular distance between the cameras' optical axes to 
filter for camera baselines between 90-120$^{\circ}$.


\subsection{Acquiring estimated 3D signals}

Given the flexible nature of LFM-3D, we provide experiments on two types of 3D estimates: NOCS maps and MDE inverse depth maps. The following section details how we retrieve these predicted 3D signals for each image pair.

\begin{figure}[t]
\begin{center}
 \begin{table}[H]
 \centering
 \vspace{-8mm}
 \resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{cccccc}
    \parbox[t]{2mm}{\rotatebox[origin=c]{90}{\small  Input \quad\quad\quad}}&%
    \hspace{-2mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe1.png}}&%
    \hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe2.png}}&%
    \hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe3.png}}&%
    \hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe4.png}}&%
    \hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe7.png}}
    \cr
    \parbox[t]{2mm}{\rotatebox[origin=c]{90}{\small Alpha \quad\quad\enskip}}\\
    \parbox[t]{2mm}{\rotatebox[origin=c]{90}{\small NOCS \quad\quad}}\\
    \end{tabular}
}
\end{table}%
\end{center}
\vspace{-25pt}
   \caption{
\textbf{Qualitative results of our trained NOCS model on Objectron.} Top: input image, middle: segmentation from off-the-shelf instance segmenter, bottom: NOCS rendering with axis aligned grid overlaid. Note that the NOCS model was trained only on synthetic renderings from Google Scanned Objects.
   }
\label{fig:nocs}
\end{figure}


\noindent\textbf{Normalized object coordinates.}
We opt to train a new NOCS map regression network for the \emph{shoe} object class, which contains enough 3D models for learning class-specific object coordinates.
As training data, we use the $327$ shoe models to produce 500 renderings each at resolutions of $256\times256$.
We randomly sample cameras while oversampling images with the object's up-vector aligned to the image's up-vector, and sample various crops, distances to the object and focal lengths, all while maintaining the object extent constant.
The network is a reduced-capacity version of Imagen's U-Net~\cite{saharia2022photorealistic} with fewer attention heads and features per layer, for a total of $28$M parameters.
Our model regresses the NOCS coordinates directly, in contrast to the classification approach~\cite{guler2018densepose} used in~\cite{wang2019normalized}.
Unlike~\cite{wang2019normalized}, we output NOCS coordinates at the same resolution as the input image.
We train the model with batch size $96$ for $300$k iterations on 8 NVIDIA Tesla V100 GPUs.

At inference time, given an image, we first apply an off-the-shelf instance segmenter for the target object category. Using the mask, we produce a tight crop around the object and pass it to the feed-forward network that estimates NOCS coordinates per pixel. 
On the synthetic dataset, the network is able to achieve a regression error of $1.4cm$ on a test set of unseen shoe models.
We show qualitative results of running NOCS on Objectron in Figure~\ref{fig:nocs}. Note that NOCS is only trained on synthetic renders from Google Scanned Objects, but is able to generalize to real images. We also note that the network fails for less common shoes (fifth column), as the dataset contains mostly sneaker type shoes.







\noindent\textbf{Monocular depth.}
We use an off-the-shelf, class-agnostic monocular depth estimation model to predict depth maps when studying long-tailed object categories where a large repository of 3D models is not available.
We use the publicly available DPT~\cite{ranftl2021vision} model, which is based on a Vision Transformer~\cite{dosovitskiy2020vit} backbone and trained on a large-scale meta-dataset across many image domains~\cite{Ranftl2020}. Following convention, we directly use the output inverse-depth maps to better handle depth values at large distances from the camera~\cite{civera2008inverse}.
Please see the appendix for qualitative depth estimate examples.


\subsection{Experimental setup}

\noindent\textbf{Evaluation scenarios and object classes.}
We focus our evaluation on three separate use cases of LFM-3D: a known object class with class-specific NOCS estimates, a known object class with class-agnostic MDE estimates, and unseen object classes with class-agnostic MDE estimates. 
In the `known' test object cases, we select the overlapping object classes between GSO and Objectron (\textit{shoe} and \textit{camera}) for evaluation, limiting training data to a single class in each case.
For the `unseen' test object evaluation, we use the entire GSO catalog to train generic (\ie, class-agnostic) model variants and evaluate on $5$ classes from Objectron: \textit{bike, book, cereal box, chair, laptop} (note that these classes do not overlap with those in the GSO catalog).

\noindent\textbf{Metrics.}
We report results on match-level statistics for the Google Scanned Objects dataset. We calculate precision/recall at the match level as the ratio of correct matches (precision) and ratio of ground truth matches recovered (recall) for a given correspondence prediction set.

Additionally, we evaluate correspondences on the downstream task of relative pose estimation, which is critical to many 3D reconstruction tasks. We estimate the essential matrix $E'$ that relates the two cameras using known camera intrinsics and predicted correspondences, and recover a relative rotation prediction. We calculate the relative rotation error between the predicted and ground truth rotation matrices using Rodrigues' formula and report accuracy within $5^{\circ}, 10^{\circ},$ and $15^{\circ}$ of error.

\begin{figure}[]
    \centering
            \vspace{-0.5em}

    \begin{subfigure}[b]{0.70\linewidth}
        \centering        

        \includegraphics[trim={1.2cm 0 2.5cm 1cm},clip,width=\textwidth]{images/shoe_gso_extreme_pr.pdf}

        \caption{Shoe ($90^{\circ}$- $120^{\circ}$)}
        \label{fig:gso_pr_curves_a}
    \end{subfigure}
    
    
    \begin{subfigure}[b]{0.70\linewidth}
        \centering

        \includegraphics[trim={1.2cm 0 2.5cm 1cm},clip,width=\textwidth]{images/camera_gso_extreme_pr.pdf}
        \caption{Camera ($90^{\circ}$- $120^{\circ}$)}
        \vspace{-0.5em}
        \label{fig:gso_pr_curves_b}

    \end{subfigure}
    \caption{\textbf{Correspondence-level precision/recall curves} for ablated version of class-specific LFM-3D on the Google Scanned Objects evaluation datasets.}
    \label{fig:gso_pr_curves}
\end{figure}

\input{sections/table_relpose}

\noindent\textbf{Compared techniques.}
We compare LFM-3D to popular 2D-based sparse matching techniques, such as SIFT~\cite{Lowe2004} features with heuristics-based matchers and SuperPoint~\cite{detone18superpoint} features with SuperGlue correspondence prediction.
For \emph{shoe} relative pose estimation, we can also compare against sparse matching between 2D keypoints and their 3D NOCS estimates (\ie Sparse PnP), and with dense matching between all 2D pixel coordinates and their 3D NOCS estimates (\ie Dense PnP).
Additionally, we compare our method to the popular dense learnable matching technique, LoFTR~\cite{sun2021loftr}, for relative pose estimation.

\noindent\textbf{Architecture and training details.}
Following \cite{sarlin2020superglue}, we use SuperPoint local features \cite{detone18superpoint} in our learnable matching framework.
We initialize the 2D keypoint encoder, graph neural network, and optimal transport layers from a 2D-only matching model pretrained on synthetic homographies and Megadepth~\cite{li2018megadepth} image pairs, following the recipe described in \cite{sarlin2020superglue}.
The model extracting 3D signals (\ie, NOCS or MDE) is frozen when training the LFM-3D matching layers.
We mirror the $\text{MLP}_{3D}$ architecture after $\text{MLP}_{2D}$, but increase the number of parameters in each layer as follows: $[64, 64, 128, 128, D]$, where $D$ is the local feature descriptor ($\mathbf d_i$) dimension.
An initial learning rate of \num{8e-5} is used until 65k iterations, after which the learning rate decays by a factor of $0.999995$ per iteration. The model was 
trained for 750,000 iterations with a batch size of 56 on the same set of GPUs as stated above. LFM-3D is implemented natively in Tensorflow 2~\cite{tensorflow2015-whitepaper} from scratch, and we use a public Tensorflow reproduction of Superpoint.

\begin{figure*}[t]

\begin{center}
    \begin{subfigure}[t]{0.78\textwidth}
        \hspace{-5pt}
        \includegraphics[width=\linewidth]{images/qual/fig5a.pdf}
        \caption{Google Scanned Objects wide-baseline pairs.\vspace{5pt}}
        \label{fig:qual_gso}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.78\textwidth}
        \hspace{-5pt}
        \includegraphics[width=\linewidth]{images/qual/fig5b.pdf}
        \caption{ Objectron wide-baseline image pairs. }
        \label{fig:qual_objectron}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.45\textwidth}
        \vspace{7pt}
        \begin{tabular}{ccc}
            \includegraphics[width=0.32\linewidth]{images/qual/failures/fail3.png} & 
            \includegraphics[width=0.32\linewidth]{images/qual/failures/fail4.png} &
            \includegraphics[width=0.32\linewidth]{images/qual/failures/fail5.png}
        \end{tabular}
        \caption{Failure cases for LFM-3D.}
        \label{fig:qual_failure}
    \end{subfigure}
    
    \end{center}
    \vspace{-5pt}
    \caption{\textbf{Qualitative results for our LFM-3D method.} We show predicted correspondences with confidence threshold $0.2$. An absence of correspondence lines means that the model found no matches. \textbf{(a)} Correct matches are shown in green and incorrect matches ($>3$ pixel error) are shown in red. \textbf{(b)} \& \textbf{(c)} Ground truth isn't available, so we show matches in randomized colors.}
    \label{fig:qual}
\end{figure*}


