\section{LFM-3D}
\label{sec:method}

\subsection{Background}

We start by introducing the necessary background on learnable sparse feature matching, normalized object coordinates, and monocular 
depth estimation.

\noindent\textbf{Learnable sparse feature matching.}
Traditional matching methods rely solely on image region descriptors 
to propose correspondences. Learnable matchers improve upon these techniques 
by using additional information during the matching process, such as keypoint 
locations and global image context.
Our method builds on top of SuperGlue~\cite{sarlin2020superglue}, a landmark learnable sparse matching network, so we briefly introduce it here along with the required notation.

SuperGlue~\cite{sarlin2020superglue} is a graph neural network (GNN) model architecture that predicts correspondences between a pair of sparse keypoint sets.
Denote local features extracted from an image as $({\bf p},{\bf d})$, where $\mathbf p_i := (x, y, c)_i$ corresponds to the 2D position $x,y$ and confidence score $c$ for the $i$-th local feature, and $\mathbf d_i \in \mathbb R^D$ is the $i$-th local descriptor of dimension $D$.
A keypoint encoder combines $\mathbf p_i$ and $\mathbf d_i$ to produce a new matching representation $\mathbf x_i$ as follows:

\begin{equation}
    \label{eq:keypoint-encoder}
        \mathbf x_i = \mathbf d_i + \text{MLP}\left(\mathbf{p}_i\right)
\end{equation}

\noindent where MLP denotes a multi-layer perceptron that performs learnable positional encoding. Alternating layers of self-attention (within a 
single keypoint set) and cross-attention (between keypoint sets of two images) iteratively 
refine $\mathbf x_i$ by providing global context for both images to 
the local region.

Training uses ground truth correspondence supervision at the local feature level, 
and an optimal transport optimization problem is leveraged to solve for a partial assignment 
between the two feature sets. Ground truth correspondences between pairs of images are recovered using either known homographies or complete camera and depth information.

\noindent\textbf{Normalized object coordinates.}
The Normalized Object Coordinate Space (NOCS)~\cite{wang2019normalized} is a 3D space defined over the unit cube that can be used to represent the geometry of all instances of an object class in a canonical orientation.
Given an image of an unseen object instance, a NOCS map can be computed that predicts 3D normalized coordinates $\mathbf n_i \in \mathbb R^3$ per pixel with respect to the canonical reference frame.
Note that a prerequisite to exploit NOCS is to register all objects in the category in the same coordinate space.
From the NOCS estimates, one can solve for the 6D object pose of the instance with respect to the canonical orientation by using the 2D-3D correspondences between image pixels and NOCS points to solve the Perspective-n-Point (PnP) problem \cite{kneip2011novel,haralick1994review}. However, we observe in our experiments that fine-grained pose estimation accuracy is highly sensitive to the quality of the predicted NOCS map.

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.7\linewidth]{images/fig2_collapsed.png}
\end{center}
   \caption{
\textbf{Block diagram of an instance of the proposed LFM-3D system, which uses NOCS~\cite{wang2019normalized} maps for 3D signals.}
We extract local features and normalized object coordinates (NOCS) from each image.
The NOCS maps are visualized by mapping XYZ to RGB.
The NOCS 3D coordinates undergo positional encoding and are combined with local features in order to generate 3D-infused local feature embeddings.
A graph neural network is then applied on these to propose correspondences.
Our method can find correspondences across images under very wide baselines, thanks to the 3D information leveraged from NOCS.
Besides NOCS, in this work we also instantiate the LFM-3D model with monocular depth estimates (MDE), which would follow the same process by changing NOCS maps to MDE maps.
   }
\label{fig:method}
\end{figure*}

\noindent\textbf{Monocular depth estimation.}
Monocular Depth Estimation (MDE)~\cite{eigen2014depth, godard2017unsupervised} is the task of predicting the depth value of each pixel given a single RGB image as the input. 
In most modern applications, the output is a dense inverse depth map of the same resolution as the input image. 
Compared to depth obtained from metric-based depth sensors (stereo cameras, LIDAR, etc.), MDE suffers from scale ambiguity, which makes it hard to be used directly for pose estimation. 
Despite the scale ambiguity problem, MDE models can be trained on massive amounts of data from varying image domains to produce robust relative depth estimates~\cite{Ranftl2020, ranftl2021vision}.
For our purpose, we found that these relative depth estimates provides sufficient 3D signals to boost correspondence estimation. 


\subsection{Learning to match using 3D signals}

In the following, we present our method to integrate 3D signals in a learnable manner for improved feature matching, which is illustrated in \figref{fig:method}.

\noindent\textbf{Notation for 3D signals.}
Since many relevant 3D signals can be obtained as a dense feature map, to simplify the presentation, we assume that estimated information about the structure of the underlying object is extracted in this form.
Note, though, that sparse 3D signals could also naturally be incorporated into our framework.
Given an image $\textit{I} \in \mathbb R^{H \times W \times 3}$, the corresponding feature map $N \in \mathbb R^{H \times W \times V}, V \geq 1$ relates each image pixel to some vector $\mathbf{n}_i$ that encodes local information about the object's geometry. 
Given sufficient training data on the object class (i.e. 3D models), we can learn a strong prior over the object's geometry.
For example, a known object class with an accurate NOCS regression model available allows us to associate each pixel with an (estimated) 3D point on the object in the canonical orientation ($V = 3$). Under weaker assumptions, such as an unknown object class or limited 3D model data, monocular depth estimates may be used to relate each pixel to an estimated distance from the capturing camera ($V = 1$).

\noindent\textbf{Enhancing local features with 3D information.}
Given the feature map $N$, we use bilinear interpolation to look up the prediction for a local feature coordinate.
Thus, each local feature $i$  has information about its 2D pixel coordinates $\mathbf{p}_i$, local image content $\mathbf{d}_i$, and predicted 3D signal $\mathbf{n}_i \in \mathbb R^V$. 

\noindent\textbf{Extending SuperGlue to handle 3D signals.}
The 3D signals for each local feature can be directly used to improve each keypoint's matching representation:
\begin{equation}
    \label{eq:keypoint-encoder-3d}
        \mathbf x'_i = \mathbf d_i + \text{MLP}_{\text{2D}}\left(\mathbf{p}_i\right) + \text{MLP}_{\text{3D}}\left(\mathbf{n}_i\right)
\end{equation}

\noindent where $\text{MLP}_{\text{2D}}$ and $\text{MLP}_{\text{3D}}$ denote separate multi-layer 
perceptrons that act on the 2D keypoint information and the 3D signals, respectively. This 
3D-infused representation $\mathbf x'_i$ can be directly used in the GNN with the same architecture and loss functions to improve feature matching. Note that this design of summing up the features instead of concatenating allows for pre-training on datasets where no 3D information is available, as described below.

\noindent\textbf{Improved positional encoding.}
However, MLPs are know to have a spectral bias to learn overly smooth functions~\cite{tancik2020fourier}. This is particularly prevalent in models that learn functions over a distribution of low frequency, low dimensional values, such as from NOCS (3-dim) or inverse depth (1-dim). An alternative positional encoding formulation~\cite{tancik2020fourier, vaswani2017attention} can lift the input representation to a higher dimension space using periodic functions of increasing frequency. In our model, we use them to encode $\mathbf{n_i}$ before passing them onto the MLP. 
Our final representation for each local feature is then:

\begin{equation}
    \label{eq:keypoint-encoder-3d-pe}
        \hat{\mathbf x}_i = \mathbf d_i + \text{MLP}_{\text{2D}}\left(\mathbf{p}_i\right) + \text{MLP}_{\text{3D}}(\text{PE}\left(\mathbf{n}_i\right))
\end{equation}

\noindent where $\text{PE}$ stands for positional encoding. We use 10 frequencies with an implementation following the one of NeRF~\cite{mildenhall2020nerf}.
We refer to the new keypoint representations $\hat{\mathbf x}$ as \textit{3D-infused local 
feature embeddings}, or \textit{3D embeddings} for short.

\noindent\textbf{Multi-stage training.}
Limitations in object-centric data with the necessary match-level annotations~\cite{jampani2023navi} may cause the model 
to generalize poorly to different domains (e.g. from synthetic to real-world images). However, we 
note that there exists many multi-view and Structure-from-Motion datasets for outdoor scenes that 
can be used to initialize some layers of LFM-3D~\cite{li2018megadepth}. We bootstrap the LFM-3D model by training the 
$\text{MLP}_{\text{2D}}$ and $\text{GNN}$ layers of the matcher model from scratch for many 
iterations on synthetic homography and outdoor scene multi-view datasets.
Once the model has a strong understanding of the 2D 
correspondence estimation task, we introduce the $\text{MLP}_\text{3D}$ encoding layer and 
finetune all matcher-model layers jointly on object-centric data augmented with 3D estimates.