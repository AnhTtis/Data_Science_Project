\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.7\linewidth]{images/teaser_v5.pdf}
\end{center}
   \caption{
   We propose \textbf{LFM-3D}, a novel learnable method for local feature matching leveraging 3D information.
   Infusing local feature matching with 3D signals enables accurate estimation of correspondences across very wide baselines, where conventional methods (SIFT + ratio test \cite{Lowe2004}) and even recent ones (SuperPoint + SuperGlue \cite{sarlin2020superglue}) fail -- we represent correct matches with green lines, and incorrect ones with red lines.
   Here, our method incorporates 3D normalized object coordinates as part of a graph neural network matcher, which significantly boosts the feature association process.
   }
\label{fig:key_fig}
\end{figure}

Matching images of the same object or scene is a core task in computer vision, since it enables the recovery of 3D structure and camera poses.
Many applications depend on these capabilities, ranging from augmented/virtual reality to novel view synthesis.
Central to image matching are local image features and associated algorithms.

\blfootnote{$^*$This work was completed while Guilherme was an intern at Google.}

Traditionally, hand-crafted techniques such as SIFT \cite{Lowe2004} or SURF \cite{bay2008speeded} were used to detect keypoints and describe them via local gradient statistics.
Putative feature matches were then obtained via simple nearest neighbor search, or its variants (\eg, Lowe's ratio test \cite{Lowe2004}), and fed into robust estimation methods such as RANSAC \cite{Fischler1981}.

In the past few years, deep learning based techniques have been proposed to replace hand-crafted methods in this area.
Several papers initially proposed to learn keypoint detectors \cite{moo2016learning,barroso2019key} or local descriptors \cite{mishchuk2017working,tian2017l2net,tian2019sosnet}.
Later, techniques were introduced to jointly model these two stages \cite{detone18superpoint,revaud2019r2d2,tyszkiewicz2020disk}, improving performance with end-to-end learning.
More recently, learnable feature matcher models were proposed to replace hand-crafted matching heuristics and obtain improved putative correspondences \cite{yi2018learning,sarlin2020superglue,sun2021loftr}.

Unfortunately, these methods still encounter difficulties in many cases. 
Wide baseline scenarios present a significant challenge due to small co-visibility regions and vastly different visual descriptors between the two keypoint sets.
Although learnable matchers integrate keypoint locations and global context to account for these factors, performance still suffers, as illustrated in~\figref{fig:key_fig}.
In this paper, we propose to address such challenging situations by integrating estimated 3D signals to help guide local feature matching even further. 
We propose using prior learned knowledge to infer rough estimates of the 3D structure of the underlying object. Then, along with the 2D keypoint locations, these 3D signals can help disambiguate matches when pixel-based descriptors are uninformative. Additionally, we propose using a positional encoding to upsample the 3D signal such that the matcher is more sensitive to the low-frequency signals.
Motivated by the increasing importance of object reconstruction, posing, and view synthesis~\cite{mildenhall2020nerf, truong2023sparf}, our experiments focus on improving object-centric image matching.

Depending on the availability of images and 3D models for given objects of interest, different types of 3D signals can be used.
In the case where a large-scale 3D model repository is available, we learn a class-specific Normalized Object Coordinate Space \cite{wang2019normalized} to predict 
3D coordinates for points on the object in a canonical orientation.
On the other hand, in the long-tailed case where no such data is available, we resort to class-agnostic monocular depth estimates \cite{Ranftl2020, ranftl2021vision} which can still provide helpful 3D hints for matching.
Thus, to evaluate our method, we perform an array of experiments, varying: 3D signals, object classes, controlled/in-the-wild datasets, and class-specific/generic variants of our model.
In more detail, our contributions are as follows.

\noindent\textbf{Contributions.}
\textbf{(1)} A novel method for local feature matching that can integrate different types of 3D information to help guide correspondence estimation, within a learnable framework that is a natural and simple extension to SuperGlue \cite{sarlin2020superglue}.
3D signals are associated with each local feature and input alongside descriptors and 2D positions into a graph neural network model that predicts correspondences. Single class variants of this method improves feature matching recall by more than 6\% and precision by up to 28\%. However, we find that our method can be scaled by jointly training on a mix of object classes, and that this generalized model can outperform both generic 2D matchers and class-specific LFM-3D.
Example results are highlighted in \figref{fig:key_fig}.
\textbf{(2)} We demonstrate that suitable positional encodings are required for this task in order to have 3D information actually help; simple encodings used in the learnable feature matching literature do not help much.
\textbf{(3)} As an example application of the improved correspondences, we present significant improvements in the downstream task of object relative pose estimation. In the large-scale Objectron dataset \cite{objectron2021} of in-the-wild images, relative pose estimates improve by up to $8.6$\% compared to the 2D-only approach.
We also outperform PnP-methods based on 3D coordinates by up to $10.4$\%. 
