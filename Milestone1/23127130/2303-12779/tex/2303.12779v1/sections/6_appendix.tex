\appendix
\section*{Appendix}

\begin{figure*}[t]
    \begin{center}
    \vspace{7pt}
    \begin{tabular}{cc|cc|cc}
    \includegraphics[width=0.14\linewidth]{images/mde/objectron1_rgb.png} & 
    \includegraphics[width=0.14\linewidth]{images/mde/objectron1_mde.png} &
    \includegraphics[width=0.14\linewidth]{images/mde/objectron2_rgb.png} & 
    \includegraphics[width=0.14\linewidth]{images/mde/objectron2_mde.png} &
    \includegraphics[width=0.14\linewidth]{images/mde/objectron3_rgb.png} & 
    \includegraphics[width=0.14\linewidth]{images/mde/objectron3_mde.png}
    \end{tabular}
    \end{center}
    \vspace{-5pt}
    \caption{\textbf{Visualizations of monocular depth estimates on camera Objectron images.} Inverse depth values are generated using the general-purpose DPT~\cite{ranftl2021vision} monocular depth estimation model.}
    \label{fig:mde_examples}
\end{figure*}

\section{Examples of monocular depth estimates}
Figure~\ref{fig:mde_examples} presents qualitative examples from the general-purpose, pre-trained monocular depth estimation model, DPT~\cite{ranftl2021vision}. We use the published model weights available on their \href{https://github.com/isl-org/DPT}{github}.

\section{Comparison to additional baselines}

\begin{table*}
    \small
    \centering
    \begin{tabular}{lccc}
        \toprule
        & \multicolumn{3}{c}{\emph{Shoe}} \\
        Method & Acc@$5^\circ$ & Acc@$10^\circ$ & Acc@$15^\circ$ \\ %
        \midrule
        
        Official SP+SG release & 2.3 & 4.8 & 7.2 \\ %
        SuperPoint + SuperGlue (ours) & 2.6 & 5.6 & 8.6 \\ %
        \midrule
        
        SP+SG w/ NOCS filtering ($d = 0.03$) & 8.4 & 18.5 & 25.3 \\ %
        SP+SG w/ NOCS filtering ($d = 0.05$) & 9.6 & 19.7 & 27.6 \\ %
        SP+SG w/ NOCS filtering ($d = 0.1$) & 10.5 & 21.5 & 29.7 \\ %
        SP+SG w/ NOCS filtering ($d = 0.2$) & 10.8 & 22.1 & 30.6 \\ %
        SP+SG w/ NOCS filtering ($d = 0.5$) & 10.8 & 22.2 & 30.6 \\ %
        \midrule
        
        LoFTR~\cite{sun2021loftr}  & 1.9 & 4.0 & 5.7 \\ %
        \midrule
        
        
        SP + LFM-3D (copied from Table~\ref{tab:rel_pose_results}) & \textbf{14.3} & \textbf{29.3} & \textbf{40.2} \\ %
        \bottomrule
    \end{tabular}
    
    \caption{\textbf{Additional baselines for relative pose estimation on the \emph{shoe} Objectron sampled video frame pairs.} We present results for relative pose estimation in terms of Accuracy@$N$. At the bottom, we copy the LFM-3D results from Table~\ref{tab:rel_pose_results}.}
    \label{tab:rel_pose_baselines_appendix}
\end{table*}

 In this section, we compare our method to some other baseline methods for relative pose estimation. Table~\ref{tab:rel_pose_baselines_appendix} reports the same relative pose metrics as Table~\ref{tab:rel_pose_results} for four additional baselines:
 
 \begin{itemize}
     \item \textit{Official SP+SG release} - We use the official SuperGlue weights provided by the authors on \href{https://github.com/magicleap/SuperGluePretrainedNetwork}{github}. 
     \item \textit{SP+SG w/ NOCS filtering} - We use the SuperPoint+SuperGlue (finetuned) model to propose candidates, and only keep correspondences whose NOCS map values are within distance $d$ of one another.
     \item \textit{LoFTR}~\cite{sun2021loftr} - Dense feature matchers offer an alternative solution for matching pairs with small co-visibility regions. We use the highly popular LoFTR model (trained on outdoor scenes) provided by the Kornia~\cite{eriba2019kornia} library to propose correspondences between all image pair pixels and estimate the fundamental matrix.
 \end{itemize}

\noindent LFM-3D outperforms all of the additional baselines for the three challenging Acc@N error thresholds.

\paragraph{Comparison to Official SuperGlue baseline.}
Note that these results (\textit{Official SP+SG}) are roughly equivalent to our own SuperPoint+SuperGlue model trained on Megadepth, without finetuning (i.e. SuperPoint + SuperGlue (ours)). This indicates that we have reproduced SuperGlue in Tensorflow successfully.
 
\paragraph{Comparison to NOCS-based heuristics baseline.} Rather than integrating NOCS 3D signals in a learned manner, we experimented with using a heuristics-based approach for filtering correspondences proposed by the 2D-only method by NOCS distance (\textit{SP + SG w/ NOCS filtering}). This naive method of integrating NOCS coordinates underperforms LFM-3D, predictably. We note that LFM-3D's greatest improvements are in increased recall, as shown by Figure~\ref{fig:gso_pr_curves}. This filtering method cannot introduce new correspondences to the limited set produced by the 2D-only baseline model.
 
\paragraph{Comparison to dense learnable matcher baseline.} The general-purpose dense matcher LoFTR~\cite{sun2021loftr} underperforms LFM-3D significantly for the relative pose estimation task. We argue that this is due to significant noise in the image background, and due to a relatively small co-visibility region in which the dense learnable matcher can gain hints from.
 

\section{Ablation on number of keypoints}
\begin{table}[]
    \small
    \centering
    \begin{tabular}{lccc}
        \toprule
        & \multicolumn{3}{c}{\emph{Shoe}} \\
        Method & Acc@$5^\circ$ & Acc@$10^\circ$ & Acc@$15^\circ$ \\
        \midrule
        SP (1024) + SG & 11.0 & 22.2 & 31.6 \\
        SP (2048) + SG & 11.4 & 22.6 & 31.3 \\
        SP (4096) + SG & 10.7 & 21.6 & 30.6 \\
        \midrule
        SP (1024) + LFM-3D & 14.3 & 29.3 & 40.2 \\
        SP (2048) + LFM-3D & \textbf{15.5} & \textbf{30.6} & \textbf{42.3} \\
        SP (4096) + LFM-3D & 13.9 & 28.7 & 39.6 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Ablation study over number of extracted keypoints.} We compare the finetuned 2D-only baseline (SuperPoint~\cite{detone18superpoint} + SuperGlue~\cite{sarlin2020superglue}) to our LFM-3D model from NOCS estimates on the \emph{shoe} Objectron evaluation set.}
    \label{tab:num_keypoints_ablation}
\end{table}

Table~\ref{tab:num_keypoints_ablation} compares LFM-3D with the finetuned 2D-only baseline with varying numbers of keypoints (2048 and 4096, vs baseline 1024 from the original paper) during relative pose estimation on Objectron \emph{shoe} image pairs. Note that we do not retrain models for each new keypoint input size. Our results show that LFM-3D benefits from additional keypoints more than the 2D-only baselines for 2048 keypoints, but performance drops with 4096 keypoints. We hypothesize that our method can produce better features due to augmented 3D inputs, while in the 2D-only case, having more features can introduce noise and false matches.

\section{Additional qualitative examples}
We present additional qualitative results on the Objectron dataset in Figure~\ref{fig:qual_appendix}. These results highlight that LFM-3D performs well across a variety of shapes, textures, and lighting conditions.

\begin{figure*}[t]
    \begin{center}
    \hspace{-5pt}
    \includegraphics[width=\linewidth]{images/qual/appendix_qual.pdf}
    
    \end{center}
    \vspace{-5pt}
    \caption{\textbf{Additional qualitative examples from sampled Objectron frames.} For each triplet, we compare SP+SG-MegaDepth, SP+SG-Finetuned, and LFM-3D.}
    \label{fig:qual_appendix}
\end{figure*}



    
