\section{Experiments}
\label{sec:experiments}
\subsection{Datasets}
We focus our experiments on object-centric image pairs taken from varying angles. We use synthetic renderings of 3D models for training, and evaluate on both synthetic and real-world image pairs. Table~\ref{tab:datasets} contains our dataset statistics.

\begin{table}[]
    \small
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccc}
        \bf{Object class} & \bf{Dataset variant} & \bf{Angles ($^{\circ}$)} & \bf{\# Scenes / Pairs} \\ %
        \hline
        \multirow{3}{*}{\shortstack[c]{\emph{Shoe}}}
        & GSO-Train & $15$-$75$ & $327$ / $490$k \\
        & GSO-Eval& $90$-$120$ & $30$ / $4479$ \\
        & Objectron & $90$-$120$ & $423$ / $6840$ \\
        \hline
        
        \multirow{3}{*}{\shortstack[c]{\emph{Camera}}}
        & GSO-Train & $15$-$75$ & $40$ / $60$k \\
        & GSO-Eval& $90$-$120$ & $7$ / $1050$ \\
        & Objectron & $90$-$120$ & $163$ / $5087$ \\
        \hline

    \end{tabular}
    }
    \caption{\textbf{Training and evaluation dataset statistics.} We leverage the Google Scanned Objects (GSO) \cite{downs2022google} and Objectron \cite{objectron2021} datasets.}
    \label{tab:datasets}
\end{table}



\noindent\textbf{Google Scanned Objects~\cite{downs2022google}.}
We use the high-quality scanned models with realistic textures 
provided by the Google Scanned Objects (GSO) dataset to train the learnable matcher components of LFM-3D. 
Within GSO, we focus on the \emph{shoe} and \emph{camera} object categories, of which there are about 360 and 47 entries, respectively, after filtering bad quality and duplicated models. Note that the 3D models were also prealigned during the capture process. To define the canonical coordinate space, we simply scale all the models so they have the same length (about $30cm$).
We render each object in 500 random poses, with a distribution that tries to match the one of real photos (instances are likely upright, etc). Additionally, we randomly modify the focal length and distance to the object and generate partial views by applying random crops.
Each rendered frame also provides ground truth camera matrices and depth maps. 
To generate ground truth correspondence data, we extract local features from each image and use the camera/depth information to project a keypoint set from one image onto the other. 
Following~\cite{sarlin2020superglue}, we select mutual nearest neighbors in reprojected pixel coordinate space and consider keypoints within 3 pixels as ground truth matches. 

\noindent\textbf{Objectron~\cite{objectron2021}.}
The Objectron dataset contains object-centric videos with wide viewpoint 
coverage for nine object categories, including \emph{shoe} and \emph{camera}. Each video frame is 
labeled with camera intrinsics and extrinsics, and shows the instance in a 
real-world scene. To overcome the domain gap between synthetic and real-world 
images, we filter out all frames with $> 1$ object instance and use an off-the-shelf segmentation model to limit extracted features to the instance segmentation mask. The lack of metric depth data prevents us from training on Objectron frame pairs, but we leverage the freely available camera matrices for evaluation purposes.

\noindent\textbf{Evaluation pairs.} 
For both source datasets, we sample image pairs at wide baselines to evaluate 
matcher performance in challenging scenarios; Table~\ref{tab:datasets} 
details each variant and its statistics. Specifically, we select two frames from the same 
scene and use the angular distance between the cameras' optical axes to 
filter for a specific camera baseline.


\subsection{Acquiring estimated 3D signals}

Given the flexible nature of LFM-3D, we provide experiments on two types of 3D estimates: NOCS maps and MDE inverse depth maps. The following section details how we retrieve these predicted 3D signals for each image pair.

\noindent\textbf{Normalized object coordinates.}
We opt to train a new NOCS map regression network for the \emph{shoe} object class, which contains enough 3D models for learning class-specific object coordinates.
As training data, we use the $327$ shoe models to produce renderings at resolutions of $256\times256$.
We randomly sample cameras while oversampling images with the object's up-vector aligned to the image's up-vector, and sample various crops, distances to the object and focal lengths, all while maintaining the object extent constant.
The network is a reduced-capacity version of Imagen's U-Net~\cite{saharia2022photorealistic}, with a total of $28$M parameters, with fewer attention heads and features per layer.
Our model regresses the NOCS coordinates directly, in contrast to the classification approach~\cite{guler2018densepose} used in~\cite{wang2019normalized}.
Unlike~\cite{wang2019normalized}, we output NOCS coordinates at the same resolution as the input image.
We train the model with batch size $96$ for $300$k iterations on 8 NVIDIA Tesla V100 GPUs.

At inference time, given an image, we first apply an off-the-shelf instance segmenter for the target object category. Using the mask, we produce a tight crop around the object and pass it to the feed-forward network that estimates NOCS coordinates per pixel. 
On the synthetic dataset, the network is able to achieve a regression error of $1.4cm$ on a test set of unseen shoe models.
We show qualitative results of running NOCS on Objectron in Figure~\ref{fig:nocs}. Note that NOCS is only trained on synthetic renders from Google Scanned Objects, but is able to generalize to real images. We also note that the network fails for less common shoes (fifth column), as the dataset contains mostly sneaker type shoes.






\begin{figure}[t]
\begin{center}
 \begin{table}[H]
 \centering
 \vspace{-8mm}
\begin{tabular}{cccccc}
\parbox[t]{2mm}{\rotatebox[origin=c]{90}{\small  Input \quad\quad\quad}}&%
\hspace{-2mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe1.png}}&%
\hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe2.png}}&%
\hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe3.png}}&%
\hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe4.png}}&%
\hspace{-4mm}\multirow{3}{*}{\includegraphics[width=0.18\linewidth]{images/nocs/shoe7.png}}
\cr
\parbox[t]{2mm}{\rotatebox[origin=c]{90}{\small Alpha \quad\quad\enskip}}\\
\parbox[t]{2mm}{\rotatebox[origin=c]{90}{\small NOCS \quad\quad}}\\
\end{tabular}
\end{table}%
\end{center}
\vspace{-25pt}
   \caption{
\textbf{Qualitative results of our trained NOCS model on Objectron.} Top: input image, middle: segmentation from off-the-shelf instance segmenter, bottom: NOCS rendering with axis aligned grid overlaid. Note that the NOCS model was trained only on synthetic renderings from Google Scanned Objects.
   }
\label{fig:nocs}
\end{figure}

\noindent\textbf{Monocular depth.}
We use an off-the-shelf, class-agnostic monocular depth estimation model to predict depth maps for the \emph{camera} object class, to study the long-tailed case where a large repository of 3D models is not available.
We use the publicly available DPT~\cite{ranftl2021vision} model, which is based on a Vision Transformer~\cite{dosovitskiy2020vit} backbone and trained on a large-scale meta-dataset across many image domains~\cite{Ranftl2020}. Following convention, we directly use the output inverse-depth maps to better handle depth values at large distances from the camera~\cite{civera2008inverse}.
Please see the appendix for qualitative depth estimate examples.


\subsection{Experimental setup}

\noindent\textbf{Object classes.}
As previously mentioned, we focus our experiments on two object classes that represent two separate use cases of LFM-3D: \emph{shoes} with learned class-specific NOCS estimates, and \emph{cameras} with class-agnostic monocular depth estimates.


\noindent\textbf{Metrics.}
We report results on match-level statistics for the Google Scanned Objects dataset. From a soft assignment matrix representing correspondence scores between each keypoint pair, we can threshold these proposals to recover a predicted correspondence set. We calculate precision/recall at the match level as the ratio of correct matches (precision) and ratio of ground truth matches recovered (recall) for a given correspondence prediction set. We utilize fixed keypoint sets with known camera and depth to define recall.

Additionally, we evaluate correspondences on the downstream task of relative pose estimation, which is critical to many 3D reconstruction tasks. We estimate the essential matrix $E'$ that relates the two cameras using known camera intrinsics and predicted correspondences, and recover a relative rotation prediction. We calculate the relative rotation error between the predicted and ground truth rotation matrices using Rodrigues' formula and report accuracy within $5^{\circ}, 10^{\circ},$ and $15^{\circ}$ of error.

\begin{figure}[]
    \centering
            \vspace{-0.5em}

    \begin{subfigure}[b]{0.72\linewidth}
        \centering        

        \includegraphics[trim={1.2cm 0 2.5cm 1cm},clip,width=\textwidth]{images/shoe_gso_extreme_pr.pdf}

        \caption{Shoe ($90^{\circ}$- $120^{\circ}$)}
    \end{subfigure}
    
    
    \begin{subfigure}[b]{0.72\linewidth}
        \centering

        \includegraphics[trim={1.2cm 0 2.5cm 1cm},clip,width=\textwidth]{images/camera_gso_extreme_pr.pdf}
        \caption{Camera ($90^{\circ}$- $120^{\circ}$)}
        \vspace{-0.5em}

    \end{subfigure}
    \caption{\textbf{Correspondence-level precision/recall curves for the Google Scanned Objects evaluation datasets.} We compare ablated versions of our LFM-3D method to highlight each component.}
    \label{fig:gso_pr_curves}
\end{figure}



\noindent\textbf{Compared techniques.}
We compare LFM-3D to popular 2D-based sparse matching techniques, such as SIFT~\cite{Lowe2004} features with heuristics-based matchers and SuperPoint~\cite{detone18superpoint} features with SuperGlue correspondence prediction.
For \emph{shoe} relative pose estimation, we can also compare against sparse matching between 2D keypoints and their 3D NOCS estimates (\ie Sparse PnP), and with dense matching between all 2D pixel coordinates and their 3D NOCS estimates (\ie Dense PnP).
We compare against additional techniques in the appendix.



\begin{table*}
    \small
    \centering
    \begin{tabular}{lccc|ccc}
        \toprule
        & \multicolumn{3}{c}{\emph{Shoe}} & \multicolumn{3}{c}{\emph{Camera}} \\
        Method & Acc@$5^{\circ}$ & Acc@$10^{\circ}$ & Acc@$15^{\circ}$ & Acc@$5^{\circ}$ & Acc@$10^{\circ}$ & Acc@$15^{\circ}$ \\
        \midrule
        
        SIFT + MNN & 4.0 & 7.8 & 11.8 & 4.6 & 9.0 & 13.2 \\
        SIFT + NN/Ratio & 4.4 & 8.7 & 13.2 & 4.3 & 9.0 & 13.0 \\
        \midrule
        
        NOCS + Sparse-PnP & 4.8 & 18.4 & 33.4 & - & - & - \\
        NOCS + Dense-PnP & 4.2 & 18.9 & 34.9 & - & - & - \\
        \midrule
        
        SuperPoint + MNN & 3.4 & 7.1 & 11.1 & 2.7 & 5.9 & 8.8 \\
        SuperPoint + NN/Ratio & 4.0 & 7.8 & 11.9 & 3.0 & 6.6 & 10.2 \\
        SuperPoint + SG-MegaDepth & 2.6 & 5.6 & 8.6 & 2.7 & 5.5 & 8.6 \\
        SuperPoint + SG-Finetuned & 11.0 & 22.2 & 31.6 & 10.6 & 21.4 & 29.2 \\
        \midrule
        
        SuperPoint + LFM-3D (Ours) & \textbf{14.3} & \textbf{29.3} & \textbf{40.2} & \textbf{11.8} & \textbf{22.4} & \textbf{30.7} \\
        \bottomrule
    \end{tabular}
    
    \caption{\textbf{Relative pose estimation results on Objectron sampled video frames.} We calculate the error between the ground truth rotation matrix and recovered relative rotation matrix from essential matrix estimation. We report accuracy within a given error threshold (in degrees), as defined by Rodrigues' formula. Note that we use estimated NOCS~\cite{wang2019normalized} maps as inputs to LFM-3D for the \emph{shoe} object category and estimated inverse depth maps from DPT~\cite{ranftl2021vision} for \emph{camera}. We use all predictions (threshold $0.0$) when estimating the essential matrix.}
    \label{tab:rel_pose_results}
\end{table*}

\noindent\textbf{Architecture and training details.}
Following \cite{sarlin2020superglue}, we use SuperPoint local features \cite{detone18superpoint} in our learnable matching framework.
We initialize the 2D keypoint encoder, graph neural network, and optimal transport layers from a 2D-only matching model pretrained on synthetic homographies and Megadepth~\cite{li2018megadepth} image pairs, following the recipe described in \cite{sarlin2020superglue}.
The model extracting 3D signals (\ie, NOCS or MDE) is frozen when training the LFM-3D matching layers.
We mirror the $\text{MLP}_{3D}$ architecture after $\text{MLP}_{2D}$, but increase the number of parameters in each layer as follows: $[64, 64, 128, 128, D]$, where $D$ is the local feature descriptor ($\mathbf d_i$) dimension.
An initial learning rate of \num{8e-5} is used until 65k iterations, after which the learning rate decays by a factor of $0.999995$ per iteration. The model was 
trained for $750000$ iterations with a batch size of 56 on the same set of GPUs as stated above. LFM-3D is implemented natively in Tensorflow 2 from scratch, and we use a public Tensorflow reproduction of Superpoint.

\begin{figure*}[t]

\begin{center}
    \begin{subfigure}[t]{0.87\textwidth}


    \hspace{-5pt}
    \includegraphics[width=\linewidth]{images/qual/fig5a.pdf}
    \caption{Google Scanned Objects wide-baseline pairs.\vspace{5pt}}
    \label{fig:qual_gso}
    \end{subfigure}
    \begin{subfigure}[t]{0.87\textwidth}
    \hspace{-5pt}
    \includegraphics[width=\linewidth]{images/qual/fig5b.pdf}
        
    \caption{ Objectron wide-baseline image pairs. }
    \label{fig:qual_objectron}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.5\textwidth}
    \vspace{7pt}
    \begin{tabular}{ccc}
    
    \includegraphics[width=0.32\linewidth]{images/qual/failures/fail3.png} & 
    \includegraphics[width=0.32\linewidth]{images/qual/failures/fail4.png} &
    \includegraphics[width=0.32\linewidth]{images/qual/failures/fail5.png}
    
    \end{tabular}
    \caption{Failure cases for LFM-3D.}
    
    \label{fig:qual_failure}
    \end{subfigure}
    
    \end{center}
    \vspace{-5pt}
    \caption{\textbf{Qualitative results for our LFM-3D method.} For all image pairs, we show predicted correspondences with confidence threshold $0.2$. Note that an absence of correspondence lines means that the model found no matches. (a) Correct matches are shown in green and incorrect matches ($>3$ pixel error) are shown in red. (b) \& (c) Ground truth isn't available, so we show matches in randomized colors.}
    \label{fig:qual}
\end{figure*}


\subsection{Results}

\noindent\textbf{Feature-level matching \& model ablations.}
Figure~\ref{fig:gso_pr_curves} presents precision/recall curves for LFM-3D on the Google Scanned Objects dataset for the \emph{shoe} and \emph{camera} object classes. 
We compare LFM-3D against model variants that allow us to ablate the importance of the different components in our system: finetuning, usage of 3D signals, and improved positional encoding. 
For each method, we extract a maximum of 1024 features and use ground truth camera and depth information to label known correspondences. To generate the curves, we sweep over confidence values in $[0, 1]$ and filter match predictions from the soft assignment output.
We use the following terminology for variants:

\begin{itemize}
    \item \textit{SG-MegaDepth} - Default SuperPoint + SuperGlue method trained on synthetic homography pairs and MegaDepth~\cite{li2018megadepth} outdoor scene pairs.
    \item \textit{SG-Finetuned} - Default SuperPoint + SuperGlue model, initialized with \textit{SG-MegaDepth} and finetuned on Google Scanned Objects rendered pairs.
    \item \textit{SG + NOCS/MDE} - Finetuned SuperPoint + SuperGlue model, with an additional input encoding layer for 3D signals per keypoint (Eq.~\eqref{eq:keypoint-encoder-3d}). As described above, we use points in NOCS space for \emph{shoe} and inverse depth estimates for \emph{camera}.
\end{itemize}

We observe that LFM-3D outperforms all baseline methods, with the most significant improvements being in maximum recall (no confidence thresholding). 
The NOCS-based LFM-3D outperforms \textit{SG-Finetuned} on recall by $6.2\%$ and at maximum recall for \textit{SG-Finetuned} ($44.3\%$), it sees a $28.9\%$ increase in precision.
The significant gap between \textit{SG + MDE} and \textit{LFM-3D (MDE)} indicates the importance of properly encoding low-dimensional 3D signals.
An effective positional encoding can enhance the model's capability to integrate 3D information.
Lastly, we note a large performance gap between the baseline \textit{SG-MegaDepth} method and all methods finetuned on synthetic renderings.

\noindent\textbf{Relative pose estimation.}
We evaluate relative pose estimation on the Objectron~\cite{objectron2021} dataset, by sampling pairs of video frames with viewpoint changes (\ie, camera axes angle distances) between $90^{\circ}$-$120^{\circ}$. 
We limit keypoint extraction to a segmentation mask (instance segmentation for shoes, and foreground segmentation for cameras) to minimize the domain gap.
Results are presented in Table~\ref{tab:rel_pose_results}; as before, the \emph{shoe} LFM-3D model uses estimated NOCS maps and the \emph{camera} LFM-3D model uses estimated depth maps.
We compare LFM-3D against conventional and state-of-the-art sparse feature matching techniques.
In the \emph{shoe} case, where a NOCS model is available, we also compare to sparse and dense PnP methods based on NOCS.


The LFM-3D model based on NOCS outperforms all other methods for the \emph{shoe} case.
We observe a large gap between our method and the finetuned 2D-only SuperPoint+SuperGlue: by $8.6$\% for Acc$@15^{\circ}$. 
More interestingly, we also outperform NOCS-based PnP variants by large margins (up to $10.4\%$ against the dense version), which shows the limitations of directly using estimated NOCS maps for relative pose prediction.


Similarly, our depth-based LFM-3D model outperforms all methods in the \emph{camera} case. Although we observe a less drastic improvement over the 2D-only techniques, we note that our method makes no further assumptions about the query image pair and uses predictions from an off-the-shelf MDE model to provide up to $1.5\%$ improvement, compared to the finetuned 2D-only SuperPoint+SuperGlue.




\noindent\textbf{Qualitative results.}
Figure~\ref{fig:qual} shows how LFM-3D proposes improved correspondences between images with relatively small overlapping visible regions. 
\textit{SG-MegaDepth} suffers from low confidence predictions under very-wide baselines, but still correctly predicts many correspondences for minor viewpoint changes (Figure~\ref{fig:qual_gso}, bottom left). Finetuning SuperGlue on domain-specific data results in more high-confidence predictions, but adding 3D signals with positional encoding leads to even better results, with more correct matches and a wider distribution of proposed correspondences across the image and the object's geometry (Figure~\ref{fig:qual_objectron}), which simplifies essential matrix estimation.

\noindent\textbf{Limitations and failure scenarios.}
The disparity between LFM-3D performance on the two object classes shows that the model can be sensitive to the choice of 3D signal. For long-tail objects that lack large-scale 3D data, one must rely on a less informative 3D signal. Figure~\ref{fig:qual_failure} shows examples where LFM-3D still struggles to match reliably. In particular, we observe that feature-less objects (first column) negatively affect local feature extractors and 3D signal estimators alike. Irregular geometries (second column) can also confuse the NOCS estimation model. And out-of-distribution geometries not shown in the training set (camera strap - third column) can match poorly at inference time.