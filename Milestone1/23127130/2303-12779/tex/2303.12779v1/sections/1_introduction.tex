\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{images/teaser_v5.pdf}
\end{center}
   \caption{
   We propose \textbf{LFM-3D}, a novel learnable method for local feature matching leveraging 3D information.
   Infusing local feature matching with 3D signals enables accurate estimation of correspondences across very wide baselines, where conventional methods (SIFT + ratio test \cite{Lowe2004}) and even recent ones (SuperPoint + SuperGlue \cite{sarlin2020superglue}) fail -- we represent correct matches with green lines, and incorrect ones with red lines.
   Here, our method incorporates 3D normalized object coordinates as part of a graph neural network matcher, which significantly boosts the feature association process.
   }
\label{fig:key_fig}
\end{figure}

Matching images of the same object or scene is a core task in computer vision, since it enables the recovery of 3D structure and camera poses.
Many applications depend on these capabilities, ranging from augmented/virtual reality to novel view synthesis from in-the-wild image collections.
Central to image matching are local image features and associated algorithms.
\blfootnote{$^*$This work was completed while Guilherme was an intern at Google.}
Traditionally, hand-crafted techniques such as SIFT \cite{Lowe2004} or SURF \cite{bay2008speeded} were used to detect keypoints and describe them via local gradient statistics.
Putative feature matches were then obtained via simple nearest neighbor search, or its variants (\eg, Lowe's ratio test \cite{Lowe2004}), and fed into robust estimation methods such as RANSAC \cite{Fischler1981}.

In the past few years, deep learning based techniques have been proposed to replace hand-crafted methods in this area.
Several papers initially proposed to learn keypoint detectors \cite{moo2016learning,barroso2019key} or local descriptors \cite{mishchuk2017working,tian2017l2net,tian2019sosnet}.
Later, techniques were introduced to jointly model these two stages \cite{detone18superpoint,revaud2019r2d2,tyszkiewicz2020disk}, improving performance with end-to-end learning.
More recently, learnable feature matcher models were proposed to replace hand-crafted matching heuristics and obtain improved putative correspondences \cite{yi2018learning,sarlin2020superglue,sun2021loftr}.

Unfortunately, these methods still encounter difficulties in many cases. 
Although learnable matchers integrate keypoint locations and global context into correspondence estimation, wide baselines, where small co-visibility regions appear, often hinder performance, as illustrated in~\figref{fig:key_fig}.
In this paper, we propose to address such challenging situations by integrating estimated 3D signals to help guide local feature matching even further. 
We propose using prior learned knowledge to infer rough estimates of the 3D structure of the underlying object. Then, along with the 2D keypoint locations, these 3D signals can help disambiguate matches when pixel-based descriptors are uninformative.

Depending on the availability of images and 3D models for given objects of interest, different types of 3D signals can be used.
In the case where a large-scale 3D model repository is available, we learn a class-specific Normalized Object Coordinate Space \cite{wang2019normalized} to predict 
3D coordinates for points on the object in a canonical orientation.
On the other hand, in the long-tailed case where no such data is available, we resort to class-agnostic monocular depth estimates \cite{Ranftl2020, ranftl2021vision} which can still provide helpful 3D hints that improve matching.
We perform experiments on large-scale datasets for the \emph{shoe} and \emph{camera} object classes, both in controlled and in-the-wild capture scenarios.
In more detail, our contributions are as follows.



\noindent\textbf{Contributions.}
\textbf{(1)} A novel method for local feature matching that can integrate different types of 3D information to help guide correspondence estimation, within a learnable framework that is a natural and simple extension to SuperGlue \cite{sarlin2020superglue}.
3D signals are associated with each local feature and input alongside descriptors and 2D positions into a graph neural network model that predicts correspondences. This method improves feature matching recall by more than 6\%, and precision by up to 28\%. %
Example results are highlighted in \figref{fig:key_fig}.
\textbf{(2)} We demonstrate that suitable positional encodings are required for this task in order to have 3D information actually help; simple encodings used in the learnable feature matching literature do not help much.
\textbf{(3)} As an example application of the improved correspondences, we present significant improvements in the downstream task of object relative pose estimation based on the improved matches. In the large-scale Objectron dataset \cite{objectron2021} containing in-the-wild images, relative pose estimation for shoes improves significantly, by up to $8.6$\% compared to the 2D-only approach.
We also outperform a direct dense PnP method based on 3D coordinates, by up to $10.4$\%. 