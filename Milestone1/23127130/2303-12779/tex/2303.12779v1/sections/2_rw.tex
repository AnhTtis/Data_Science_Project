\section{Related Work}
\label{sec:rw}

\noindent\textbf{Traditional local feature extraction and matching.} Traditionally, hand-crafted methods, such as SIFT \cite{Lowe2004}, SURF \cite{bay2008speeded} or others \cite{tola2010daisy,chandrasekhar2009chog,calonder2012brief}, have been used to find correspondences between images.
Such keypoint detection approaches generally rely on finding salient low-level patterns such as blobs or corners, with representative methods being Difference-of-Gaussians \cite{Lowe2004}, Harris-Laplace or Hessian-Affine \cite{Mikolajczyk2004scale}.
Local description in this case is mainly based on gradient orientation statistics for small image patches around the keypoint.
In this conventional pipeline, associating local features across images generally builds on top of nearest neighbor matching, using Lowe's ratio test \cite{Lowe2004} or mutual consistency checks, and possibly additionally leveraging consensus methods \cite{tuytelaars2000wide,sattler2009scramsac}.

\noindent\textbf{Deep learning based local features.}
As the interest in deep learning methods for computer vision grew, hand-crafted feature extraction gave way to learned techniques.
As a first step, researchers proposed to independently learn either the keypoint detection \cite{verdie2015tilde,moo2016learning,mishkin2018repeatability,barroso2019key} or the keypoint description \cite{mishchuk2017working,luo2019contextdesc,tian2017l2net,tian2019sosnet,Simo2015discriminative,wang2020learning} stages.
This led to systems which were partially learned, but still relied on hand-crafted components.
A more recent trend is the unified modeling of detection and description, which enables end-to-end learning \cite{yi2016lift,noh2017large,detone18superpoint,ono2018lfnet,revaud2019r2d2,Dusmanu2019CVPR,tyszkiewicz2020disk}, leading to stronger results in image matching related tasks.
These methods may be trained using different levels of supervision: supervised at the feature level where the correspondence labels are computed from Structure-from-Motion pipelines \cite{yi2016lift,ono2018lfnet,revaud2019r2d2,Dusmanu2019CVPR,tyszkiewicz2020disk}, weaker supervision at the level of relative camera poses \cite{wang2020learning} or image-level labels \cite{noh2017large}, or finally fully self-supervised with synthetic image transformations \cite{detone18superpoint}.
Learned local features improve upon hand-crafted methods in several respects, \eg: the detection process extracts much more discriminative keypoints instead of hand-crafted low-level corners or blobs; the descriptor achieves more robust illumination and viewpoint invariance due to deep semantic representations.
Still, wide baseline matching and the presence of repeated patterns (\eg, building windows) may confuse these techniques and lead to poor correspondences.

\noindent\textbf{Learnable feature matching.}
One of the latest trends in the area of local image features is to additionally enable the learning of the feature matching process.
This goes beyond learned outlier filtering approaches \cite{yi2018learning,zhang2019learning}.
SuperGlue \cite{sarlin2020superglue} proposed graph neural network models in conjunction with optimal transport optimization in order to effectively associate sparse local features and reject outliers.
This method takes in sets of local features from two images, comprising local descriptors and 2D positions, and learns feature matching patterns with a data-driven approach.
A follow-up method \cite{chen2021learning} proposed to improve matching efficiency with reliable seed matches to initialize the process.
The learnable matching framework has also evolved to perform matching in a dense manner \cite{chen2022aspanformer, sun2021loftr}, which avoids issues with potentially noisy keypoint detections at the cost of higher memory requirements.
In this work, we focus on the sparse case and extend the learnable feature matching model to consider 3D information at the keypoint level, allowing the framework to reason beyond 2D signals.

\noindent\textbf{Feature matching leveraging 3D information.}
Most local feature methods do not consider 3D information in the matching process.
Closest in spirit to our work is the method of Toft \etal \cite{toft2020single}, where monocular depth estimation is used to rectify perspective distortions, improving the quality of extracted local features.
In contrast, our work can leverage different types of 3D information in a learnable framework, which enables more precise matching. 
Another recent work in this area by Ma \etal \cite{ma2022virtual} relies on human poses as 3D priors in order to disambiguate occlusions during matching.
Our method shares some commonalities with it in the sense that both rely on some prior about the 3D shape of an object depicted in a 2D image.
However, we differ from them as our goal is to improve local feature representations for matching of visible parts, as opposed to their focus on associating invisible points for bundle adjustment.