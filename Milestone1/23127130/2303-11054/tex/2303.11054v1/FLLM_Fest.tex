%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

% choose options for [] as required from the list
% in the Reference Guide

% choose options for [] as required from the list
% in the Reference Guide
\usepackage{amsmath}
\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
\usepackage{subcaption}                             % when including figure files
\captionsetup{compatibility=false}
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage{bigstrut}
\usepackage{multirow}
\usepackage{newtxtext}       % 
\usepackage[varvw]{newtxmath}       % selects Times Roman as basic font
\usepackage{float}
\def\0{{\bf 0}}
\def\x{{\bf x}}
\def\y{{\bf y}}

\newcommand{\n}{^{(n)}}

\newcommand{\F}{\mathbf{F}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\Ind}[1]{I \left(#1\right)}
\newcommand{\rmE}{{\rm E}}

\def\pms{\mspace{-1mu}{\scriptscriptstyle\pm}}


\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Some novel aspects of quantile regression: local stationarity, random forests and optimal transportation}
  \titlerunning{Novel aspects of quantile regression} %for an abbreviated version of
% your contribution title if the original one is too long
\author{Manon Felix, Davide La Vecchia, Hang Liu and Yiming Ma}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Manon Felix and Davide La Vecchia  \at Research Center for Statistics and Geneva School of Economics and Management, University of Geneva, \email{manon.felix@unige.ch and davide.lavecchia@unige.ch}
%\and \at Research Center for Statistics and Geneva School of Economics and Management, University of Geneva, \email{davide.lavecchia@unige.ch}
\and Hang Liu  \at  International Institute of Finance, School of Management, University of Science and Technology of China \email{hliu01@ustc.edu.cn}
\and Yiming Ma \at Department of Statistics and Finance, School of Management, University of Science and Technology of China  \email{mayiming@mail.ustc.edu.cn}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle


\abstract{This paper is written for a Festschrift in honour of Professor Marc Hallin and it proposes some developments on quantile regression. We connect our  investigation to Marc's scientific production and we  present some theoretical and methodological advances for quantiles estimation in non standard settings.  We split our contributions in two parts. The first part is about conditional quantiles estimation for nonstationary time series: our approach combines local stationarity for Markov processes with quantile regression. The second part is about conditional quantiles estimation  for the analysis of multivariate independent data in the presence of possibly large dimensional covariates: our procedure  combines optimal transport theory with quantile regression forests. Monte Carlo studies illustrate numerically the performance of our methods and compare them to extant methods. The codes needed to replicate our results are available on our \texttt{GitHub} pages.}

\section{Introduction}
\label{sec:1}
A decision-theoretical approach to quantile-oriented and  rank-based  inference has been a  \textit{fil rouge}  running through Professor Marc Hallin’s entire scientific life. It provides coherence to his otherwise very broad, long, and diverse list of 
contributions to mathematical statistics and econometrics.

Following the \textit{fil rouge}, we build on some (old and recent) Marc's research  outcomes: we propose some theoretical and methodological developments for conditional quantile estimation. Our investigation is motivated by the need for modeling and estimating conditional quantiles in two non standard and challenging settings: nonstationary time series  (section \ref{sec:2}) and multivariate independent data (section \ref{sec:3}). 
%Due to the diverse nature of the inference tasks that we consider, we split our contributions in two parts. 
%The first part deals with the problem of conditional quantiles estimation in a nonstationary time series, whilst %our approach combines local stationarity for Markov processes with quantile regression. 
%the second part is about conditional quantiles estimation  for multivariate independent  data. %: our novel inference procedures  combine optimal transport theory with quantile regression forests. 
To introduce our contributions, we review the extant literature. Rather than embarking in a pointless attempt at being comprehensive, we decided to put the emphasis on the key results and on Marc's research outcomes  connected  to our developments.

\subsection{Quantile estimation for nonstationary AR processes}

Quantile estimation for time series and related autoregression rank scores have been discussed in many Marc's publications. For instance, \cite{EH02} propose
estimators of the quantile density function associated with the innovation density of an autoregressive model of order $p$ (AR($p$)) and their estimators are based on 
autoregression quantiles. % \cite{HW06} comment on the quantile autoregression models introduced by \cite{KZ06} and discuss temporal aggregation issues.
\cite{HJ99} construct locally asymptotically optimal tests based on autoregression rank scores, whose concept  
was introduced by \cite{GJ92} %(from a duality argument applied to Koenker and Bassett’s regression quantiles) 
and further developed by \cite{KS95}
 in the univariate time series context.
 
 The assumption of stationarity lies at the heart of all these developments. 
%the vast majority of models and inferential procedures for time series data. 
%This is easy enough to explain. The existence of a time-invariant probability distribution is a powerful tool for identification and estimation. 
However, empirical studies suggest that  stationarity appears to be doubtful (to say the least). %; see e.g. \cite{O01,O05} and related papers. 
In some cases, the nonstationarity of the process can be dealt with using simple devices. For instance, for an AR process with a unit root, taking the first difference is sufficient to obtain a stationary series for which standard methods apply. However, in other cases, it is not so simple to find an appropriate transformation, which may not even exist. Therefore, it is crucial to devise a theoretical framework addressing the issue of nonstationarity in the statistical analysis of time series data. 

%One of the reasons for the absence of a unified theory for nonstationary processes is that nonstationarity can be thought of in various ways. 
%In the context of  Markov processes, an important part of the literature has focused on the idea of recurrence, a less stringent requirement than stationarity; see \cite{BP}  and the references therein. A different 
One possible approach to nonstationarity is related to the concept of local stationarity; see \cite{D12} for a review. In that framework, the object of interest is a stochastic process whose parameters are changing smoothly over time, in such a way that it can be locally approximated by a stationary process. Importantly, by rescaling the observation period to the unit interval, estimators for the time-varying parameters can be  obtained using windowed (in time) estimating equations. This concept has been successfully applied to various types of processes, including AR processes \cite{D97,DNV99}, ARCH processes \cite{DSR06,FSSR08}, nonlinear AR processes \cite{V12}, scalar diffusion processes \cite{KL12},  Markov processes \cite{Tr19}  and \cite{DLV19} (multivariate diffusions). Some recent developments of local stationarity for quantile spectral analysis are available in \cite{BVKDH17}, while \cite{XSZ22} (and in some sense also \cite{ZW09}) study conditional quantile estimation. 

 The approach that we adopt in this paper has a spirit similar to the one of
%In the setting of discretely observed diffusions, 
\cite{DLV19}, who explain how to conduct inference on the time-varying parameters of Markov processes and how to derive the asymptotics of the corresponding estimators. %The theory developed in \cite{DLV19}  includes as a special case the AR  processes considered in this paper. 
However,  \cite{DLV19} %Del\'eamont and La Vecchia
 focus on the first two (infinitesimal) moments of the process, whilst here we study the problem of quantile regression and we look at the entire conditional distribution. To this end, we consider  time-varying autoregression  quantile  estimation for a process  that we observe over a time span $[0,n]$. 
%Similarly to the approach of \cite{DLV19}, the 
The central idea of our estimation method relies on solving a sequence of localized in time quantile regression optimization problems, where the localization is achieved using a kernel with compact support.  
%Essentially, our method involves two steps. First, at each time point, we approximate the nonstationary process (having time-varying autoregression quantiles) by a stationary process (having time constant autoregression quantiles).  Second,  
We make use of  the local polynomial quantile regression technique to estimate the model parameters and
we derive the asymptotic properties  of proposed estimators, studying their bias and their asymptotic distribution.  The resulting inference procedure 
%is related to %the quantile estimation problems and techniques discussed in \cite{KS95,HJ99}, combined with 
 %quantiles estimation studied in \cite{ZW09} and our derivation 
 complements the results already available in \cite{XSZ22}. 
 %We consider also computational and numerical aspects, studying via Monte Carlo exercises the behaviour of three autoregression quantiles estimators, which are special cases of the proposed local polynomial estimators: the  local constant, local linear and local quadratic estimator.  %Similarly to the numerical analysis for standard quantile regression available in  \cite{YJ97}, we consider the performance of these two estimators for time points which are in the middle or at the boundary of the time span.

\subsection{Nonparametric multiple-output center-outward quantile regression and random forests for independent data}

The topic of {multivariate nonparametric quantile regression} has been attracting the interests of the research community for a long time; see  \cite{HM17} for a review.
 Let us summarize its key inference issues. 
 
 The problem of quantile regression is well-understood for the case of univariate random variables. However, an extension to the multivariate case (also called multiple-output case) is not straightforward because most of the extant definitions of regression quantiles (namely, the traditional definition, the $L_1$ definition and  the definition based on regression quantile hyperplane) exploit the canonical ordering of the real line. Such an ordering no longer exists in $\mathbb{R}^d$, $d \geq 2$. This entails that notions like quantiles, check function, distribution function, signs, and ranks do not clearly extend to higher dimensions. Some solutions to this problem are already available, like the directional and direct approaches; see \cite{HM17} p.187-193. % for a discussion.

Making use of statistical concepts related to optimal transportation theory (see \cite{H22} and \cite{LVRI23}  for a discussion on the use of Monge-Kantorovich results in statistics),
\cite{dBSH22} define nested conditional center-outward quantile regression contours and regions, with given conditional probability content irrespective of the underlying 
distribution. Their  graphs constitute nested center-outward quantile regression tubes.  \cite{dBSH22} illustrate how to construct empirical counterparts of these population 
concepts, yielding interpretable empirical regions and contours. Their construction is based on two steps: in Step 1, one specifies a set of weights and uses them to construct an 
empirical distribution of the multivariate response variable conditional on some values of the multivariate covariates;  in Step 2, one computes the corresponding empirical center-
outward quantile map, resorting on Monge-Kantorovich's results.

Building on that approach, we combine the theory of random forests with the novel concepts of center-outward quantiles. The proposed 
inference method merges some results rooted in the machine learning literature (random forests \cite{B01}, quantile regression forests \cite{MR06}, and generalized random forests \cite{ATW19}),  in mathematics (optimal transportation, \cite{V08}) 
and in statistics (multivariate quantile, \cite{Hallin2021}).  

At a high level, our procedure makes use of random forests as an adaptive neighbourhood classification tool, which we combine with the multiple-output center-outward quantile regression of  \cite{dBSH22}. We grow the trees mimicking the logic of standard random forests, obtaining, for every regressors value, a set of weights for the original multivariate response. This idea has the same spirit as \cite{MR06} but it deals with a multivariate response variable and needs a novel approach to specify and compute the weights. To this end, we build on the theory of multivariate random forests, where the key tool is the Mahalanobis distance; see  \cite{SX11}.  We apply the resulting weights to estimate the conditional distribution, which is defined as the weighted distribution of observed response variables, as in Step 1 of \cite{dBSH22}. Then, moving along the same lines as Step 2 in \cite{dBSH22}, conditional center-outward quantile maps are obtained. This construction yields a novel inference tool which is be able, by design, to alleviate the curse of dimensionality for the analysis of multi-output variables in the presence of large dimensional covariates.

%\subsection{Structure of the paper}
%
%In section \ref{sec:2}, we develop the theory for local polynomial autoregressive quantiles estimation for locally stationary processes. %We motivate our investigation using a Monte Carlo example about conditional quantile estimation for an AR(p) process having time-varying parameters; see section \ref{Mot1}. 
%In section \ref{LS_theory} we introduce the estimation theory\footnote{A lengthy proof  about consistency and asymptotic normality of the proposed estimators
%is available in the Supplementary Material. }, %proving consistency and asymptotic normality of the estimators, 
%while in section \ref{Sec_MC1} we illustrate numerically the performance of %different polynomial estimators: 
%the local constant, local linear and local quadratic estimators. In section \ref{sec:3}, we study the problem multiple-output center-outward nonparametric quantile regression. In section \ref{Bas2} we review the concepts of center-outward quantities (quantiles, regions and contours) at the population and at the sample level. Then in section \ref{Meth2}, we explain how these notions can be combined with random forests to obtain empirical conditional quantiles. Section \ref{experiments} contains Monte Carlo studies which illustrate numerically the performance of our novel method.% and compare it to the extant methods. 
%  


\section{Local stationarity}\label{sec:2}

\subsection{A motivating example} \label{Mot1}

To illustrate the impact that ignoring the nonstationarity of a time series may have on the estimation of the conditional quantiles (and hence on the whole conditional distribution), let us consider the following simple motivating example. 

To begin with, let us fix the notation and consider the AR(1) process having dynamics
\newcommand{\fin}{i/n}
\newcommand{\fiin}{\frac{i-1}{n}}
 $X_i = \phi_1 X_{i-1} + e_i$,
where $\{e_i, i=1,...,n\}$ are i.i.d., zero mean and unit variance, with cumulative distribution function (cdf) $F$ and $X_0=0$.  Let $\mathcal{F}_{i-1}$ be the $\sigma$-field containing the
 past values of the process. The theoretical conditional quantile, at a probability $\tau\in(0,1)$, of $X_i$ given $\mathcal{F}_{i-1}$ is 
 \begin{equation}
 Q_\tau (X_i  \vert \mathcal{F}_{i-1}) = \phi_1 X_{i-1} + F^{-1}(\tau)  = \phi_1X_{i-1} + \alpha (\tau) = \boldsymbol{U}_i^{\top} \boldsymbol{\theta}(\tau) ,
 \label{QAR_stat_case}
 \end{equation}
with $\boldsymbol{\theta}(\tau) = (\alpha(\tau), \phi_1)^{\top}$ and $\boldsymbol{U}_i = (1, X_{i-1})^{\top}$. To estimate the model parameters we use the standard quantile regression approach and obtain
\begin{equation}
\hat{\boldsymbol{\theta}}(\tau)=\underset{\boldsymbol{\theta} \in \Theta}{\operatorname{argmin}} \sum_{i = 1}^n \rho_\tau\left(X_i- \boldsymbol{U}_i^{\top} \boldsymbol{\theta}(\tau) \right),
\label{Eq: est_constQAR}
\end{equation}
where  $\rho_\tau$ is the check-function defined as $\rho_\tau(u)=u(\tau-I(u<0))$ and $I(\cdot)$ is the indicator function.  A simple plug-in of $\hat{\boldsymbol{\theta}}(\tau)$ (called the autoregression quantile) into the expression of $Q_\tau( X_i | \mathcal{F}_{i-1})$ yields $\hat Q_\tau (X_i | \mathcal{F}_{i-1})= \boldsymbol{U}_i^{\top} \hat{\boldsymbol{\theta}}(\tau)$, which is an estimate of the conditional quantile.

Now assume that the underlying AR(1) has a time-varying parameter 
$\phi_1(i/n)$. %$Y_{\fin} = \boldsymbol{\theta}_1(\fin)Y_{\fiin} + e_{\fin}$. 
The resulting process is nonstationary, with non constant true conditional quantiles (and more generally, with time-changing conditional density) having expression as
in (\ref{QAR_stat_case}), 
 %$$
%\mathcal{Q}_Y(i/n, \tau| \mathcal{F}_{i-1}) = \boldsymbol{\theta}_1(i/n) Y_{\fiin} + F^{-1}(\tau) = \boldsymbol{\theta}_1(\fin)Y_{\fiin} + \boldsymbol{\theta}_0(\tau) = 
%\boldsymbol{\theta}(\fin, \tau)^{\top}X_\fin,
%$$
with the time constant $\boldsymbol{\theta}(\tau)$ replaced by the time-varying  $\boldsymbol{\theta}(i/n \mid \tau) = (\alpha(\tau), \phi_1(i/n))^{\top}$. Assume that we conduct inference ignoring the fact that the  process is nonstationary:  % As a result, we pretend that the process is stationary, we make use of the observations drawn from the nonstationary process %$\{Y_\fin\}$ and 
we estimate $\boldsymbol{\theta}(\tau)$ using (\ref{Eq: est_constQAR}) and we define the conditional quantile via the plug-in of resulting estimates into the expression of $Q_\tau (X_i | \mathcal{F}_{i-1})$. 
It is easy to conjecture that the time varying nature of the model parameter entails that the estimated conditional quantiles %(obtained using a constant estimated parameter) 
are not reliable estimates of the true time-varying conditional quantiles. %(obtained using $\boldsymbol{\theta}(i/n, \tau)$). 
To illustrate numerically this aspect, we conduct a Monte Carlo study. 
 Specifically, we consider a stationary process with parameter $\phi_1=0.5$ and a nonstationary 
one with time-varying parameter  $\phi_1({i}/{n})= c ({i}/{n}) + d ({i}/{n})^{2.5}$,  where we set $c = 0.1$, $d = 0.85$, $n=4000$ and the innovations are i.i.d. with standard Gaussian distribution. % and $\boldsymbol{\theta}_1(0) = \theta_1(u)$ if $u < 0$ and $\theta_1(1) = \theta_1(u)$ if $u > 1$. 
In Figure \ref{Fig.1}, we display  the estimated and true conditional quantiles via scatter plots, for the stationary and nonstationary case, in one Monte Carlo run. For both $\tau=0.15$ (left plot) and $\tau=0.5$ (right plot), when the underlying process is stationary, the true and the estimates conditional quantiles are very similar (the $\times$ symbols are overlapping with the 45 degrees line). In contrast, when the underlying process is nonstationary, the estimated conditional quantiles are biased (the  $+$ symbols are scattered around the  45 degrees line). This bias is due to the fact that the estimation procedure in (\ref{Eq: est_constQAR}) does not take into account the time-varying nature of the process. %: it estimates one parameter which remains constant over time, rather than allowing for  time-varying dynamics. 
In the next subsection we explain how to derive a class of estimators, which is able, by design, to cope with this issue. % and we study their asymptotics.

\begin{figure}
\begin{center}
 \includegraphics[width = 0.85\textwidth, height = 0.275\textheight]{Figures/Plot3.eps}
 \end{center}
 \caption{Conditional quantiles (true versus estimated)  scatter plots for an AR(1). Left plot: probability level $0.15$; Right plot: probability level $0.5$. %The parameter is estimated by the standard quantile regression method.  
 In each plot: the (blue) $+$'s are for the case of time-varying  parameter ; the (pink) $\times$'s are for the case of time constant parameter; the bold continuous line is for the 45 degrees line.}
\label{Fig.1}
 \end{figure}


\subsection{Autoregression quantile estimation} \label{LS_theory}

\subsubsection{Stationary case}

\textit{Key notions.} Let us start by a review of the key ingredients of quantile regression estimation   %For a complete review, see e.g. \cite{Xia017}. 
for a stationary AR($p$), $p \in \mathbb{N}$:
%\begin{equation*}
$X_i=\phi_0+\phi_1 X_{i-1}+\cdots+\phi_p X_{i-p}+e_i$, with $p\geq 1$,
%\end{equation*}
where $\{e_i\}$ is an i.i.d. mean-zero sequence, with variance $\sigma_e^2<\infty$ and $e_i \sim F$, for all $i$. 

The conditional distribution of $X_i$ given $\mathcal{F}_{i-1}=\sigma(X_0,X_1,\ldots,X_{i-1}$) is simply a location shift of the cdf, with conditional mean ${\rm E}[X_i \vert \mathcal{F}_{t-1}]=\phi_0+\phi_1 X_{i-1}+\cdots+\phi_p X_{i-p}$. Thus, the conditional quantile function of $X_i$ is 
%\begin{eqnarray*}
$Q_{\tau} \left(X_i \mid \mathcal{F}_{t-1}\right)=\phi_0+ \sum_{j=1}^{p} \phi_j X_{i-j}+F^{-1}(\tau) = \alpha(\tau) + \sum_{j=1}^{p} \phi_j X_{i-j}
= \boldsymbol{U}_i^{\top} \boldsymbol{\theta}(\tau),$ 
%\end{eqnarray*}
with $\boldsymbol{\theta}(\tau) = (\alpha(\tau), \phi_1,\ldots,\phi_p)^{\top}$,  $\alpha(\tau)= \phi_0 + F^{-1}(\tau)$ and $\boldsymbol{U}_i = (1, X_{i-1},\ldots,X_{i-p})^{\top}$.
%Let $\boldsymbol{\theta}_0(\tau)=\boldsymbol{\theta}_0+F_u^{-1}(\tau), \boldsymbol{\theta}(\tau)=\left(\boldsymbol{\theta}_0(\tau), \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_p\right)^{\top}$, and $X_t=\left(1, Y_{t-1}, \ldots, Y_{t-p}\right)^{\top}$.
  Generalizing (\ref{QAR_stat_case}) to the AR($p$) case, we  write
%\begin{equation}
$Q_{\tau}\left(X_i \mid \mathcal{F}_{t-1}\right)=\boldsymbol{U}_i^{\top} \boldsymbol{\theta}(\tau).$ 

While the conditional mean ${\rm E}[X_i \mid \mathcal{F}_{t-1}]$ provides a model for the average only, the conditional quantile $Q_{\tau}\left(X_i \mid \mathcal{F}_{t-1}\right)$ can 
capture different characteristics of the distribution of $X_i$ by specifying different probability levels (quantiles) $\tau$. For example, setting $\tau = 0.5$ we study the median,  while $\tau = 0.15$ concerns the left tail of the distribution.
Adding noise to  $Q_{\tau}\left(X_i \mid \mathcal{F}_{t-1}\right)$, we obtain 
\begin{equation}
X_i= Q_{\tau}\left(X_i \mid \mathcal{F}_{t-1}\right) + e_i(\tau) =  \alpha(\tau) + \sum_{j=1}^{p} \phi_j X_{i-j}  + e_i(\tau)
= \boldsymbol{U}_i^{\top} \boldsymbol{\theta}(\tau)  + e_i(\tau), 
\label{Eq. QReg}
\end{equation}
where, for identifiability of  $\alpha(\tau)$, we set that $e_i(\tau)$ has zero $\tau$th quantile. At the cost of a more cumbersome notation, our modeling can be extended to the case where the AR($p$) process includes also some exogenous covariates. However, for the ease of exposition, we do not pursue that model.\\

%\label{Eq.QstatGener}
%\end{equation}

\textit{Estimation method.} Given observations $\left\{X_i\right\}_{i=1}^n$ of the stationary process,  the vector $\boldsymbol{\theta}(\tau)$ can be estimated by the quantile regression and the autoregression quantile is
%\begin{equation*}
$$
\hat{\boldsymbol{\theta}}(\tau)=\underset{\boldsymbol{\theta} \in \Theta}{\operatorname{argmin}} \sum_i \rho_\tau\left(X_i-\boldsymbol{U}_i^{\top} \boldsymbol{\theta}(\tau)\right).
$$
%\end{equation*}
%  Given $\hat{\boldsymbol{\theta}}(\tau)$, the $\tau$th conditional quantile of 
%$X_i$  is estimated by $\hat{Q}_{\tau}\left(X_i \mid \mathcal{F}_{t-1}\right)=\boldsymbol{U}_i^{\top} \hat{\boldsymbol{\theta}}(\tau)$. 
%Moreover, one can
%estimate the conditional density of $X_i$ at $y=Q_{\tau}\left(X_i  \mid \mathcal{F}_{t-1}\right)$ using, for some appropriately chosen sequence  $h=h(n) \rightarrow 0$,
%\begin{equation*}
%\hat{f}_{X_i}\left(y \mid \mathcal{F}_{t-1} \right)=\frac{2 h}{\hat{Q}_{\tau + h}\left(X_i \mid \mathcal{F}_{t-1} \right)-\hat{Q}_{\tau-h}\left(X_i \mid \mathcal{F}_{t-1} \right)}.
%\end{equation*}
If the sequence $\left\{e_i\right\}$ contains i.i.d. random variables such, for that each $e_i$, $F$ has a continuous density $f$ with $f(e)>0$ on $\mathcal{E}=\{e: 0<F(e)<1\}$, then  $\hat{\boldsymbol{\theta}}(\tau)$ satisfies 
%%\begin{equation*}
%$f\left[F^{-1}(\tau)\right] \Omega_0^{1 / 2} \sqrt{n}(\hat{\boldsymbol{\theta}}(\tau)-\boldsymbol{\theta}(\tau)) \Rightarrow B_m(\tau)$,
%%\end{equation*}
%where
%\begin{equation*}
%\Omega_0= \rmE \left(\boldsymbol{U}_{i} \boldsymbol{U}_{i}^{\top}\right)=\left[\begin{array}{cc}
%1 & {\mu}_x^{\top} \\
%{\mu}_x & \Omega_x
%\end{array}\right], \quad \Omega_x=\left[\begin{array}{ccc}
%\rmE \left(X_i^2\right) & \cdots & \rmE \left(X_i X_{i-p+1}\right) \\
%\vdots & \ddots & \vdots \\
%\rmE \left(X_i X_{i-p+1}\right) & \cdots & \rmE \left(X_i^2\right)
%\end{array}\right] .
%\end{equation*}
%Here ${\mu}_x=\rmE\left(X_i\right) \cdot 1_{p \times 1}$, $\rmE (\cdot)$ denotes the expected value, and $B_m(\tau)$ represents a $m$-dimensional standard Brownian bridge, 
%with $m=p+1$.
%For any fixed $\tau \in [0,1]$,  $B_m(\tau) \sim \mathcal N\left(0, \tau(1-\tau) I_k\right)$, with $I_m$ being the $m$-dimensional identity matrix. Thus,  
%\begin{equation*}
$\sqrt{n}(\hat{\boldsymbol{\theta}}(\tau)-\boldsymbol{\theta}(\tau)) \rightarrow^{\mathcal{D}} \mathcal N\left(0, \boldsymbol{v}(\tau) \right)$,  
%\label{Asy_S}
%\end{equation*}
with asymptotic variance given by
\begin{equation}
	\boldsymbol{v}(\tau) =  \boldsymbol{\Gamma}^{-1}  \frac{\tau(1-\tau)}{f\left[F^{-1}(\tau)\right]^2},
\label{Asy_S}
\end{equation}
 where $\boldsymbol{\Gamma}= \rmE \left(\boldsymbol{U}_{i} \boldsymbol{U}_{i}^{\top}\right)$ exists and it is non singular. 
 %, and $ \rightarrow^{\mathcal{D}}$ denotes the convergence in distribution.

\subsubsection{Locally stationary case}

\textit{Key notions}. In what follows, for a generic random variable $Z$, we write $Z\in L_q$  for $q>0$, if its $L_q$ norm $\Vert Z \Vert_q = [{\rm E}(|Z|^q)]^{1/q} < \infty$. 
We  denote by $C^k[0,1]$ the set of functions on $[0, 1]$ with $k$th order continuous derivatives. 

To discuss the nonstationary AR($p$) case, we proceed as in \cite{XSZ22}: we assume that the model is similar to (\ref{Eq. QReg}), but  the parameters are quantile-varying and time-varying functions and
 $\varepsilon_i(\tau) \in L_2$  with zero $\tau$th quantile. Thus, we set % and $t=i/n \in [0, 1]$:
\begin{equation}
    X_i= \alpha(i/n\mid\tau) + \sum_{j=1}^{p}\phi_j(i/n\mid\tau) X_{i-1} + \varepsilon_i(\tau) = \boldsymbol{\theta}(i/n\mid \tau )^\top \boldsymbol{U}_i + \varepsilon_i(\tau).
    \label{eq:tvmodel}
\end{equation}

Looking at (\ref{eq:tvmodel}), four comments  are in order. First,  the coefficients and hence the quantile dependence structure vary with time, leading to nonstationarity, with  time-varying conditional quantiles $Q_{\tau}\left(X_i \mid \mathcal{F}_{t-1}\right)=  \boldsymbol{\theta}({i}/{n} \mid \tau )^\top \boldsymbol{U}_i $.  Second, the functional form of the time-varying coefficients is not specified: this feature allows the quantile-specific dependence structure to change over time in a nonparametric way. Third,   the model implies that the conditional quantile of $X_i$ depends on its $p$ most recent values: intuitively, as in the stationary case, distant (in time) data do not influence the current observation. 
Fourth, we emphasize the dependence on $n$ of the time-varying coefficients $\phi_j$s: the use of the rescaled time $i/n$ allows us to develop an inference procedure having a meaningful asymptotic theory.
%in fact, we are dealing with a triangyular array of processes $\{X_i, 0 \leq i \leq n \}$  satisfying equation~\eqref{eq:tvmodel}. \\

As it is customary in the literature on local stationarity (see \cite{D12}),
we introduce a stationary Markov process which we use to approximate the nonstationary $\{X_i, 0 \leq i \leq n \}$. Thus, we 
define a process $\{X_i(u), i\geq 1\}$, indexed by $u\in[0,1]$, that 
satisfies %the quantile regression model
\begin{equation}
X_i(u)= \alpha(u \mid\tau) + \sum_{j=1}^{p}\phi_j(u \mid \tau) X_{i-1}(u) + \varepsilon_i(\tau) = \boldsymbol{\theta}(u  \mid \tau )^\top \boldsymbol{U}_i(u) + \varepsilon_i(\tau).
\label{Xtilde}
\end{equation}
where $\boldsymbol{U}_i(u)=(1, X_{i-1}(u),\ldots,X_{i-p}(u))^{\top}$.  Comparing (\ref{eq:tvmodel}) with (\ref{Xtilde}), it seems intuitively clear that if $i/n$ is near $u$, then $X_{i}$ and ${X}_{i}(u)$ should be close in some norm, like e.g. the
$L_2$ norm. The degree of closeness should depend on both the rescaling factor $n$ and the deviation $\vert i/n - u\vert
$. Thus, the family of processes defined through~\eqref{Xtilde} should provide, in some sense, a reasonable approximation to the  process~\eqref{eq:tvmodel}. The following definition, (see~\cite{XSZ22})  formalizes these heuristics.  \\

\textbf{Definition}. The process $\{X_i, 1 \leq i \leq n\}$ is said to be locally stationary, if for each  $ u \in [0, 1]$, there exist a stationary process indexed by 
$\{X_i(u), 1 \leq i \leq n \}$ 
and a constant $C > 0$ such that in the local time window about $i/n$, $X_i$ can be approximated by $X_i(u)$ in the sense that 
\begin{eqnarray}
\sup_{u\in[0,1]} \Vert X_i(u) \Vert_2 < \infty, \quad \Vert X_i - X_i(u)  \Vert_2 \leq C \left(\Big\vert i/n-u \Big\vert + \frac{1}{n} \right), \ i=1,...,n. 
\label{Def_LS}
\end{eqnarray}
Occasionally, when (\ref{Def_LS}) holds, we say that $\{X_i, 1 \leq i \leq n\}$ has a locally stationary approximation $\{X_i(u), 1 \leq i \leq n \}$ in the $L_2$-norm. With this definition in mind,  the model in (\ref{eq:tvmodel}) is called locally stationary quantile regression autoregressive model. \\

To prove that the nonstationary process satisfying (\ref{eq:tvmodel}) 
admits a locally stationary approximation  satisfying  (\ref{Xtilde}), we introduce the following\\

\textbf{Assumption 1}  %For each $\tau\in (0,1)$ 
We have that: (i) the $\varepsilon_i(\tau) \in L_2$ are i.i.d.; (ii) the coefficients $\alpha(\cdot \vert \tau),\phi_1(\cdot \vert \tau),...,\phi_p(\cdot \vert \tau) \in 
C^2[0,1]$, $\sup_{u} \vert \alpha(u \vert \tau)\vert< 1 $ and $\sup_{u} \sum_j  \vert \phi_{j} (u \vert \tau) \vert < 1$.\\

Assumption 1 essentially imposes some regularity conditions (continuity and smoothness) on the model coefficients and it is standard in the literature on
locally stationary processes. Then, we can prove the following
 
\begin{theorem} 
    Suppose Assumption 1 holds. The process $\{X_i, 1 \leq i \leq n\}$ has a locally stationary approximation $\{X_i(u), 1 \leq i \leq n \}$ in $L_2$ norm.
    \label{Th1}
 \end{theorem}

The proof follows along the lines of the proof of Th. 1 in \cite{XSZ22} and it is omitted.  The theorem provides a key theoretical tool: 
it allows us to define a class of nonparametric estimators for the time-varying model parameters, that in the next pages we are going to apply for the estimation of the model parameters and for the  derivation of their asymptotic theory. \\ %In the next subsection we elaborate further on this point. \\

\textit{Estimation method}. To estimate the time-varying model parameters in (\ref{eq:tvmodel}), %we start considering $Q_\tau(X_i \mid \mathcal{F}_{i-1})$ and introducing\\
we introduce the following\\

 \textbf{Assumption 2}
    $\boldsymbol{\theta}(\cdot|\tau)$ is in $C^{k+1}[0,1]$, for $k\geq 1$. \\
  
Thanks to this assumption we write a $k$th order of Taylor's approximation of $\boldsymbol{\theta}({i}/{n} \mid \tau )$ around $u$
\begin{equation}
    \boldsymbol{U}_i^{\top} \boldsymbol{\theta}(i/n \mid \tau ) \approx \boldsymbol{U}_i^{\top} \left\{\boldsymbol{\theta}(u \mid \tau ) +  \left(i/n -  u \right) \boldsymbol{\theta}'(u \mid \tau ) + 
    \dots + \left(i/n - u\right)^k \boldsymbol{\theta}^{(k)}(u \mid \tau )/k!\right\}.
    \label{Exp.Taylor}
\end{equation}
Then, we consider a sequence of localized (in time) polynomial quantile regressions, where the 
time localization is achieved by the kernel $K$,  having bandwidth $b_n$  and satisfying \\

\textbf{Assumption 3}
    (i) $K(\cdot)$, with bounded support, is symmetric and continuously differentiable, and $\int_{\mathbb{R}} K(u) d u=1$.
    (ii) $n b_n \rightarrow \infty$ and $n b_n^{2(k + 2)} \rightarrow 0$. \\
 
Setting for convenience $\boldsymbol{\theta}_0({i}/{n} \mid \tau )=\boldsymbol{\theta}({i}/{n} \mid \tau )$, $K_i(u)=K\left\{(i / n-u) / b_n\right\}$, our locally stationary
autoregression quantile 
is the solution to the following time localized  optimization problem ($k$-order polynomial quantile regression)
\begin{equation}
\begin{gathered}
\left(\hat{\boldsymbol{\theta}}_0(u \mid \tau), \dots , \hat{\boldsymbol{\theta}}^{(k)}(u \mid \tau)\right)=
\underset{\boldsymbol{\theta}_0, \dots, \boldsymbol{\theta}_k}{\operatorname{argmin}} \sum_{i=1}^n \rho_\tau\left\{X_i- \sum_{m = 0}^k\left(i/n-u\right)^k  
\frac{\boldsymbol{U}_i^{\top}\boldsymbol{\theta}_k}{k!}\right\} K_{i}(u).
\end{gathered}
\label{eq:argmin1}
\end{equation}
% The local linear estimator is defined setting $k=1$, while the local constant estimator is obtained setting $k=0$. 
The derivation of the asymptotics of local polynomial quantile estimator requires some restrictions on
higher-order moments of the innovation terms (needed for the existence of the asymptotic distribution). Thus we introduce the following\\
  
 \textbf{Assumption 4} 
   (i)  $\left\{\varepsilon_i(\tau)\right\}_i$ are i.i.d., and for each $i, \varepsilon_i(\tau)$ is independent of the historical information 
     $\left\{\left(\boldsymbol{U}_j, \boldsymbol{U}_j(u)\right)\right\}_{j \leq i}$. (ii) $ \varepsilon_i(\tau) \in {L}_{4+2 \epsilon}$ for some $\epsilon>0$. (iii) $\left\{\left(X_i(u), \varepsilon_i(\tau)\right)\right\}_i$ is $\alpha$-mixing with mixing 
     coefficients $\alpha_k$ satisfying $\sum_{k=1}^{\infty} \alpha_k^{\epsilon /(2+\epsilon)}<\infty.$ (iv) The density $f_\tau(\cdot)$ of $\varepsilon_i(\tau)$ is bounded and has bounded derivative. (v) The matrix $\boldsymbol{\Gamma}(u)=\rmE\left[\boldsymbol{U}_i(u) \boldsymbol{U}_i(u)^{\top}\right]$ is nonsingular. \\

Looking at Assumption 1-4, we remark that our assumptions are similar to the ones in \cite{XSZ22} (who consider the local linear case),
but we need to include some additional conditions on the higher-order derivatives (see Assumption 3) of the time-varying coefficients, which are needed
to define the local polynomial estimators and their asymptotics. Then, we complement the results available in Th. 2 of \cite{XSZ22} and we state

\begin{theorem} \label{Th2}
    Under the Assumptions 1-4, for the $\hat{\boldsymbol{\theta}}_0(u|\tau)$ parameter in (\ref{eq:argmin}), we have

    \begin{equation}
        \sqrt{nb_n} \left\{\hat{\boldsymbol{\theta}}_0(u|\tau) - \boldsymbol{\theta}_0(u|\tau) -  b_n^{k+1}  \frac{\boldsymbol{\theta}^{(k+1)}(u\mid \tau)}{(k+1)!} \int_{\mathbb{R}} v^{k+1} K(v) dv\right\}  
        \rightarrow^{\mathcal{D}} \mathcal{N}(0,\boldsymbol{s}(u \mid \tau)), 
        \label{AsyN}
    \end{equation}
with, for $\kappa_2=  \int_{\mathbb{R}} K^2(v) d v$,
    \begin{equation}
        %\begin{aligned}
			\boldsymbol{s}(u \mid \tau)  =\boldsymbol{\Gamma}(u)^{-1} \frac{\tau(1-\tau)}{f_\tau^2(0)}\kappa_2.
        %\boldsymbol{\Gamma}(u) & =\rmE\left[U_1(u) U_1(u)^{\top}\right].
        %\end{aligned}
        \label{Asy_LS}
        \end{equation}
    \end{theorem}
    
The proof  makes use of Th. \ref{Th1} and it requires an adaptation of the arguments in \cite{XSZ22}. %---which are tailored to the local linear case. 
We refer to the  Supplementary Material for the mathematical details. Here we focus on  some theoretical and methodological aspects
that we express in the following remark.\\

\textbf{Remark.} First, the results in \cite{XSZ22} follow as a special case, setting $k=1$ (local linear estimator) 
and without the need for Assumption 2 on higher-order derivatives. Moreover, comparing  (\ref{Asy_LS}) to the asymptotic variance in Eq. (22) of  \cite{XSZ22}, we notice that 
the $k$th order local polynomial estimator has the same asymptotic variability as the local linear and local constant estimator, for $u\in(0,1)$. Second, as in the stationary case, 
Th. \ref{Th2} proves that the asymptotic distribution of the local polynomial estimators is Gaussian and, comparing (\ref{Asy_LS}) to (\ref{Asy_S}), we notice 
that the expression of the asymptotic variance in the locally stationary case is similar to one obtained in the stationary case, but it contains two important differences: the factor  $\kappa_2$ does not exist in (\ref{Asy_S}), since it is related to time localization;  the matrix $\boldsymbol{\Gamma}$ of (\ref{Asy_S}) is replaced by its time localized version $\boldsymbol{\Gamma}(u)$ in  (\ref{Asy_LS}).   Third, (\ref{AsyN}) gives an explicit form 
for the estimator's bias, which is due to nonstationarity. Specifically, 
 the term $\boldsymbol{\theta}^{(k+1)}(u\mid \tau)$ illustrates that the bias depends on the degree of time-variability of the model parameters. Thus, if one believes that the 
 model parameters have a complex time-varying structure, (\ref{AsyN}) indicates that the use of a higher-order polynomial regression can be helpful to control the asymptotic estimation bias in the
 conditional quantiles, without affecting the estimator's precision. % (interpreted as the inverse of the asymptotic variance).  
 
% Specifically, the local linear estimator ($k=1$) is able to capture the linear behaviour of  $\boldsymbol{\theta}(u\mid \tau)$ with a bias of order $O(b^2_n)$, whilst the local constant ($k=0$) cannot achieve the same target and it entails a bias of order $O(b_n)$. However, it is possible that for some $u \approx i/n$ where  $\boldsymbol{\theta}^{(k+1)}(u\mid \tau)=0$, both a local polynomial estimator of order $k$ and one of order $(k-1)$ may yield an asymptotic bias of the same order $O(b^k_n)$. For instance,  a local constant estimator and a local linear one both yield an asymptotic bias of order $O(b^2_n)$ for all those $u$s where the first derivative $\boldsymbol{\theta}^{(1)}(u\mid \tau)$ is identically zero and the (norm of the) second derivative is bounded, namely for flat portions, minima, maxima, points of inflection of $\boldsymbol{\theta}(u\mid \tau)$. Since the asymptotic variance of the two estimators is also the same (see (\ref{Asy_LS})) at these $u$s, then we argue that the two estimators have an asymptotic localized Mean Squared Error (MSE, as obtained adding the squared asymptotic bias and variance locallized at these $u$s), that is of the same order. We refer to \cite{YJ97} for similar considerations in 
% a comparison between  local constant fitting and local linear fitting for  conditional quantiles estimation, in the setting of independent data.
 
 \subsubsection{Monte Carlo studies} \label{Sec_MC1}

Theorem \ref{Th2} opens the door to the use of polynomial estimators, with the possibility for the ultimate user of selecting the degree of the polynomial via the specification of $k$
in (\ref{eq:argmin1}). An interesting question is related to the performance that different estimators may have in the AR case with time-varying coefficients. To investigate numerically this aspect, we consider the median ($\tau=0.5$) of an AR(3) with  dynamics %as in (\ref{eq:tvmodel})
	\begin{equation*}
		X_i= \phi_1(i/n \mid \tau) X_{i-1}+ \phi_2(i/n\mid \tau) X_{i-2}+ \phi_3(i/n\mid \tau) X_{i-3} + \varepsilon_{i}(\tau), \quad \varepsilon_{i}(\tau) \sim \mathcal{N}(0,1),
		%\label{eq:modAR3}
	\end{equation*}

\noindent where $\phi_2 (i/n\mid \tau) =  0.2 + 0.2 \sin(18  i/n) +  0.608(i/n) - 0.032((i/n)+1)^3)$
%with $d = 0.2, c = 18$ 
and $\phi_1(\cdot \mid \tau) = \phi_2(\cdot \mid \tau)/10, \phi_3(\cdot \mid \tau) = \phi_2(\cdot \mid \tau)/3$. 

The selected functional form is flexible and it creates a challenging estimation problem: it combines periodic oscillatory behaviour (encoded in the $\sin$ function) with a polynomial growth in time (encoded in the powers of $({i}/{n})$).  Moreover, it ensures that the local approximating process $\{X_i(u)\}$ is stationary, at every $u$. We set the sample size $n=3000$ and $X_0=0$. We estimate the functional parameter $\boldsymbol{\theta}(u)=(\phi_1(u),\phi_2(u),\phi_3(u))$ as in (\ref{eq:argmin1}), setting $k = 0,1,2$, a choice which yields the local constant ($k=0$), the local linear ($k=1$) and the local quadratic ($k=2$) estimators. For the sake of comparison, we use the same bandwidth $b_n=0.05$ for the three estimators and set the same grid of 100 equidistant points for $u$. 

To comment on the results, we focus on the estimation of the function $\phi_2$---identical comments apply to the other functional parameters $\phi_1$ and $\phi_3$, which are just 
rescaled versions of $\phi_2$. In Figure \ref{Fig.2}, we display the estimated values in the form of functional boxplots, as obtained with 100 Monte Carlo runs; see {\url{https://
github.com/manonflx} for the code. The plots illustrate that the three estimators yield very similar results. Indeed, for each value of $k$, the median's functional curve follows the true 
underlying dynamics of the time-varying parameter (middle smooth line). Moreover, the interquartile ranges are essentially identical for the three choices of $k$. %This can be remarked, for example, if we look around $u = 0.3$: we see that three estimators suffer from a bias with very similar size. 
The main difference that can be noticed is that the local linear and local quadratic estimators have slightly larger variance than the local constant estimator at the
boundaries of the time span (in particular, for $u$ close to zero). We conjecture that this may be due to the sample size; see \cite{YJ97} p. 162 for a similar comment.
 

\begin{figure}
\begin{center}
 \includegraphics[width = 0.85\textwidth, height = 0.3\textheight]{Figures/Plot4pdf.pdf}
 \end{center}
 \caption{Functional boxplots of the estimated $\phi_2(u\mid \tau)$, for $\tau=0.5$, with $u\in[0,1]$ (x-axis) as obtained using the local constant, local linear and local quadratic estimation (from top to bottom). In each plot, the middle smooth line represents the true time-varying parameter $\phi_2(u\mid \tau)$}
\label{Fig.2}
 \end{figure}

To have a measure of each estimator performance, we compute the (estimated) MSE at each $u$. In Figure \ref{Fig.3}, we display the MSE curve for each estimator. Remarkably, the three MSEs have very similar values: the three MSE curves are almost overlapping.  Moreover, as it is common to nonparametric kernel estimation procedures, we observe a boundary effect: the MSEs tend to increase when $u$ is close to $0$ or $1$. %This effect is due to the fact that as one approaches the boundaries, fewer observations are available for estimation. 
Figure \ref{Fig.2} illustrates that the MSE curves behaviour is principally driven by an increase in the estimators' variance. As far as the computation time is concerned,
we ran our Monte Carlo simulations using and \texttt{Intel(R) core i5-8500 CPU \@ 3.00GHz} and the local constant estimator requires a computation time which is about one and half times smaller than the one requested by the local linear and by the local quadratic estimator, while the local linear is almost as fast as the the local quadratic estimator.  %We do not report the bias and variance as their aspect are quite identical to Figure (\ref{Fig.4}). Due to the comparison, the recommendation would be to use a local constant estimator.
We performed other simulations (unreported)  with different functional forms of the parameters and our conclusions remain essentially the same. 

In light of this numerical experience, our recommendation is to use the local constant estimator, which offers a performance that is comparable to the performance yielded by the other considered polynomial estimators, but it requires less computational efforts. 

%On the contrary, when raising the order $k$ the model gains in complexity which has the effect to increase the computation time as well. On the top of that, one should not forget the fact that the variance related to the model also depends on the number of covariates which is also an argument for a low $k$'s choice. 



 
 
 \begin{figure} 
\begin{center}
 \includegraphics[width = 0.85\textwidth, height = 0.275\textheight]{Figures/Plot5.eps}
 \end{center}
 \caption{Estimated MSE at different rescaled time points $u \in [0,1]$ (x-axis) for the local constant, local linear and local quadratic estimator.} %The dashed line is for the local constant, the dotted line is for the local linear and the solid line is for the local quadratic. %The two key messages are that the three curves follow each other and that all three are susceptible to the boundary effect.
 \label{Fig.3}
 \end{figure}



\section{Optimal transportation and quantile regression forests}\label{sec:3}


Throughout this section, we consider the setting where $(\X, \Y)$ is an $\mathbb{R}^{m+d}$ random vector with joint distribution ${\rm P}_{\X\Y}$ and marginal distributions ${\rm P}_{\X}$ and ${\rm P}_{\Y}$ for $\X \in \mathbb{R}^{m}$ and $\Y \in \mathbb{R}^{d}$, respectively. Denote by ${\rm P}_{\Y \vert \X = \x}$ the distribution of $\Y$ conditional on $\X = \x$.  Let $(\X\n, \Y\n) := ((\X_1\n, \Y_1\n), \ldots, (\X_n\n, \Y_n\n))$ be a sample of $n$ i.i.d. copies of $(\X, \Y) \sim  {\rm P}_{\X\Y}$. 
%The aim of this section is to propose an estimator of  ${\rm P}_{\Y \vert \X = \x}$ based on the weights generated by random forests, so that one can perform quantile regression for multiple-output data through implementing the algorithm in \cite{dBSH22} for computing their {\it empirical conditional  center-outward quantile}.





\subsection{Basic notions} \label{Bas2}

\subsubsection{Conditional  center-outward quantile, region and contour} \label{Pop}

Based on the measure transportation theory, \cite{Hallin2021} proposed the concept of { center-outward quantile function}, which is the key tool for performing multiple-output quantile regression. To introduce the method, let us denote by $\mathbb S _d$ and ${\bar{\mathbb S}}_d$ the open and closed unit ball, respectively, and by~${\mathcal S}_{d-1}$ the unit hypersphere   in $\mathbb R ^d$. 
Let $\mathcal{P}_d^\pm$ denote the family of all distributions~$\rm P$ with nonvanishing densities $f$, that is,  for all positive $r\in\mathbb{R}$, there exist constants~$L^-_r >0$ and $L^+_r <\infty$ for which   
$L^-_r\leq f({\bf y}) \leq L_r^+$  for all ${\bf y}\in r\,{\bar{\mathbb S}}_d.$
%with modulus less than or equal to $R$. 
 For $\rm P$ in  this family,  the center-outward distribution and quantile functions defined below  are continuous: see \cite{Figalli2018}.  Denote by~${\rm U}_d$ the spherical uniform distribution over~${\mathbb S}_d$, that is, the product of a uniform measure over the hypersphere ${\mathcal S}_{d-1}$ and a uniform over the unit interval of distances to the origin. 
 

The {center-outward distribution function}~$\F_{{\pms}}$ of $\rm P$ is defined as the a.e.\ unique gradient of convex function mapping $\mathbb{R}^d$ to $\mathbb{S}_{d}$  
and  { pushing~$\rm P$ forward} to    ${\rm U}_d$ (that is, such that $\F_{{\pms}}({\bf X})\sim{\rm U}_d$ if ${\bf X}\sim{\rm P}$). For ${\rm P}\in{\mathcal P}_d^\pm$, such mapping is  a homeomorphism between~${\mathbb S}_d\setminus\{{\bf 0}\}$ and~$\mathbb{R}^d\setminus \F_{{\pms}}^{-1}(\{{\bf 0}\})$ (\cite{Figalli2018}) and   the corresponding {center-outward quantile function} is defined  as~$\Q_{\pms} \coloneqq \F_{\pms}^{-1}$ (letting, with a small abuse of notation, $\Q_{\pms} ({\bf 0}) \coloneqq \F_{\pms}^{-1}(\{{\bf 0}\})$). For any given distribution~$\rm P$, the quantile function $\Q_{{\pms}}$ induces a collection of continuous, connected, and nested quantile  contours $\Q_{\pms}(r{\mathcal S}_{d-1})$ and regions $\Q_{\pms}(r{\mathbb S}_{d})$ of order $r\in[0,1)$;   the { center-outward median}  $\Q_{\pms}(\0)$ is a uniquely defined  compact set of Lebesgue measure zero. We refer to \cite{Hallin2021} for details.  These notions allow us to introduce the following \\  %the concepts of conditional  center-outward quantile, contour and region proposed by 


\textbf{Definition} (\cite{dBSH22}) The {conditional  center-outward quantile function} of $\Y$ given $\X$ is the center-outward quantile map ${\bf u} \mapsto \Q_{\pms}({\bf u} \vert \X = {\bf x})$ of ${\rm P}_{\Y \vert \X = \x}, \x \in \mathbb{R}^m$. The corresponding { conditional  center-outward quantile region} and { contour} of order $\tau \in (0, 1)$ are the sets
%\begin{equation*}
$\mathbb{C}_{\pms} (\tau \vert \x) := \Q_{\pms}(\tau {\bar{\mathbb S}}_d \vert \X = \x)$  %\qquad \text{and} \qquad 
$\mathcal{C}_{\pms} (\tau \vert \x) := \Q_{\pms}(\tau {{\mathcal S}}_{d-1} \vert \X = \x)$,
%\end{equation*}
respectively.


\subsubsection{Empirical  conditional center-outward quantile}\label{sec.EmpQuantile}

The measure transportation quantities introduced in section \ref{Pop} are at the population level.
Given the sample $(\X\n, \Y\n)$, the empirical version of $\Q_{\pms}({\bf u} \vert \X = {\bf x})$ can be constructed via two following steps; we refer to \cite{dBSH22}.\\ 
%for the consistency result of the empirical center-outward quantile.

{\it Step 1.} Compute the { empirical conditional distribution} of $\Y$ given $\X = \x$ using the formula
${\rm P}\n_{w(\x)} := \sum_{j = 1}^n w_j\n(\x; \X\n) \delta_{\Y_j},$
where  $\delta_{\Y_j}$ is the Dirac function computed at $\Y_j$ and the sequence of weights $w_j\n, j =1, \ldots, n$ satisfies
\begin{equation}\label{eq.wj.condition}
w_j\n(\x; \X\n)  \geq 0   \qquad \text{and} \qquad \sum_{j = 1}^n w_j\n(\x; \X\n) = 1.
\end{equation}


{\it Step 2.} Compute the empirical conditional quantiles based on the empirical conditional distribution in Step 1. In order to do this, one needs to first construct a regular grid $\mathfrak{G}^{(N)}$ consisting of $N$ points $\mathfrak{g}_1^{(N)}, \ldots, \mathfrak{g}^{(N)}_N$. To this end\footnote{Other heuristic criteria are possible, see e.g. \cite{HM22}.}, let $N$ factorize into $N=N_R N_S +N_0,$ for~$N_R, N_S, N_0 \in \mathbb{N}$ and~$0\leq N_0 < \min \{ N_R, N_S \}$, where~$N_R \rightarrow \infty$ and $N_S \rightarrow \infty$ as~$N \rightarrow \infty$, and consider a sequence $\mathfrak{G}^{(N)}$ of grids, where each grid consists of the $N_R N_S$ intersections between an $N_S$-tuple $(\boldsymbol{u}_1,\ldots \boldsymbol{u}_{N_S})$ of unit vectors, and the~$N_R$ hyperspheres  with radii $1/(N_R+1),\ldots ,N_R/(N_R+1)$ centered at the origin, along with~$N_0$ copies of the origin. The only requirement is that  the discrete distribution 
$${\rm U}_d^{(N)} := \frac{1}{N} \sum_{i=1}^N \delta_{\mathfrak{g}_i^{(N)}}, \quad N\in \mathbb{N},$$
 converges weakly to the uniform~${\rm U}_d$ over the ball $\mathbb{S}_d$. Estimation of the conditional center-outward quantile is based on the optimal transport pushing ${\rm U}_d^{(N)}$ forward to ${\rm P}\n_{w(\x)}$, and it is achieved by solving the linear program
\begin{equation}\label{eq.OT}
\begin{split}
& \quad \min_{\pi := \{\pi_{i, j}\}} \sum_{i=1}^N \sum_{j=1}^n \frac{1}{2} |\Y_j - \mathfrak{g}_i^{(N)} |^2 \pi_{i, j}, \\
& {\rm s.t.} \, \sum_{j=1}^n \pi_{i, j} = \frac{1}{N}, \, i = 1, 2, \ldots, N,\\
& \quad \sum_{i=1}^N \pi_{i, j} =  w_j\n(\x; \X\n), \, j = 1, 2, \ldots, n, \\
& \quad \pi_{i, j} \geq 0,  \,  i = 1, 2, \ldots, N, j = 1, 2, \ldots, n
\end{split},
\end{equation}
with $\vert \cdot \vert$ applied to a vector denoting its Euclidean norm---if applied to a real number, it denotes its absolute value. 

Now, let us denote by $\pi^*(\x) = \{\pi^*_{i, j}(\x), i = 1, 2, \ldots, N, j = 1, 2, \ldots, n\}$ the solution of \eqref{eq.OT}. For any gridpoint $\mathfrak{g}^{(N)}_i, i = 1, \ldots, N$, there exists at least one $j \in \{1, 2, \ldots, n\}$ such that $(\mathfrak{g}^{(N)}_i, \Y_j) \in {\rm supp}(\pi^*(\x))$. Since more than one such $j$ may exist, we choose the one which gets the highest mass from $\mathfrak{g}^{(N)}_i$, and in case of ties, we choose the smallest one by letting
$$\Q\n_{w, \pms}(\mathfrak{g}^{(N)}_i \vert \x) := \arg \inf \left\lbrace \vert \y \vert: \y \in {\rm conv}\left( \{Y_J: J \in \arg \max_j  \pi^*_{i, j}(\x)\}\right)\right\rbrace,$$
where ${\rm conv}(\mathcal{A})$ denotes the convex hull of a set $\mathcal{A}$. Supposing the gridpoint $\mathfrak{g}^{(N)}_i$ is on the hypersphere with radius $j/(N_R+1)$, then $\Q\n_{w, \pms}(\mathfrak{g}^{(N)}_i \vert \x)$ is the empirical conditional center-outward quantile of $\Y$ given $\X = \x$ at the level $j/(N_R+1)$.  The corresponding empirical quantile  region $\mathbb{C}\n_{w, \pms} (j/(N_R+1) \vert \x)$ is the set of empirical conditional center-outward quantiles at the level $\tau \leq j/(N_R+1)$, and the empirical quantile contour $\mathcal{C}\n_{w, \pms} (j/(N_R+1) \vert \x)$ is the set of  the quantiles at the level $j/(N_R+1)$. \\


 
%\textbf{Remark.}  The empirical conditional center-outward quantiles introduced here are defined at gridpoints and are hence discrete sample points. Smooth interpolation is possible by adopting the  Moreau-Yosida regularization procedure in \cite{Hallin2021}. Since the main aim of this paper is to propose a novel methodology for computing  the empirical conditional distribution of $\Y$ in the step 1, we do not consider the interpolation approach here.



\subsection{Methodology} \label{Meth2}

%We consider the two-step approach described in Section~\ref{sec.EmpQuantile} for computing the empirical  center-outward quantile. 
 

\subsubsection{A motivating example}

In Step 1 of section~\ref{sec.EmpQuantile}, one needs to specify the weight function %$w\n: (\x; \X\n) \mapsto w\n(\x; \X\n) := \left(w_1\n(\x; \X\n), \ldots, w_n\n(\x; \X\n)\right)$ 
$$w\n: (\x; \X\n) \mapsto  \left(w_1\n(\x; \X\n), \ldots, w_n\n(\x; \X\n)\right),$$ which is needed to compute ${\rm P}\n_{w(\x)}$. In this section we propose a novel approach for generating weights based on random forests.  The code is available at 
{\url{https://github.com/mayiming24/Empirical-conditional-center-outward-quantile}.

To begin with, we use a Monte Carlo simulation to illustrate some issues in the procedure of \cite{dBSH22}, who propose to specify the weights  in Step 1 using a kernel or a $k$-nearest neighbors (kNN) method. In practice, both approaches may encounter some problems; see \cite{bengio2005curse} and \cite{pestov2013k}.  For instance: (i) they may perform poorly when the data is sparse; (ii) kernel method may not perform well for multidimensional data. To illustrate these points, in Figure~\ref{fig:contour_d}, we plot the empirical conditional center-outward quantile contours at levels $\tau=0.2,0.4,0.6$ with the Gaussian kernel weights (first row) and kNN weights (second row) for $n = 1000$ and $\X \in \mathbb{R}^m$, $m=1, 2, 5$ (the dimension of $\Y$ is $d=2$). We refer to section~\ref{experiments} for details about the data generating process (DGP).  For $m = 1$ (the first column), both types of weights perform reasonably well, with the empirical contours staying close to their population counterparts (the dashed lines).  However, for $m=2$, their performance becomes much worse than the one observed in the $m=1$ case and when $m=5$, we see that the empirical contours yielded by the kernel weights are concentrating at a single point, and those of the kNN at $\tau = 0.4,0.6$ also deviate severely from the population ones.
%In addition to the disadvantages mentioned above, both types of weights have additional parameters to choose from: kernel method requires the choice of bandwidth and KNN method requires the choice of the number of nearest neighbors. Since there is no universal rules on how to choose these parameters for a real data, their implementation in practice is more complicated than the random forests method introduced below.
In the next subsection we illustrate how one can use the random forests to solve these issues.




\begin{figure}[htb]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_k_1.eps}
		\caption{$\mathbf{X} = 0.5 $}
		\label{fig:ck_1}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_k_2.eps}
		\caption{$ \mathbf{X} = (0.5,0) ^{\top}$}
		\label{fig:ck_2}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_k_5.eps}
		\caption{$\mathbf{X}=(0.5,0,0,0,0)^\top$}
		\label{fig:ck_5}
	\end{subfigure}
	\\
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_kn_1.eps}
		\caption{$\mathbf{X} = 0.5 $}
		\label{fig:ckn_1}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_kn_2.eps}
		\caption{$ \mathbf{X} = (0.5,0)^\top$}
		\label{fig:ckn_2}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_kn_5.eps}
		\caption{$\mathbf{X}=(0.5,0,0,0,0)^\top$}
		\label{fig:ckn_5}
	\end{subfigure}
	\caption{Plots of the empirical conditional center-outward quantile contours with the kernel weights (first row) and kNN weights (second row). The first, second and third columns are for $m=1, 2, 5$, respectively, and $n=1000$. The  (green,  red  and black) dashed lines represent the   conditional center-outward quantile contours of $\tau=0.2,0.4,0.6$, respectively, and the solid lines are the corresponding empirical contours.} % Note that in panel (c), $\mathcal{C}_{w, \pm}^{(n)}(\tau \mid \textbf{X})$ concentrates on a single point. See Section~\ref{experiments} for the DGP where the sample size $n=1000$)
	\label{fig:contour_d}
\end{figure}

\subsubsection{Random forests based weights}

To circumvent the problems mentioned above for the kernel and kNN weights, we propose to obtain weights using random forests (henceforth, RF). Following \cite{ATW19}, the weights are generated by averaging neighborhoods produced by different trees. More precisely, we grow a set of $B$ trees indexed by $b = 1, \ldots, B$ and, for each tree, let $L_b(\x)$ denote the set of training examples falling in the same leaf as $\x$. Then the weight $w_j\n(\x; \X\n), j = 1, \ldots, n$ is the averaged (over $B$ trees) frequency that the training sample falls into the same leaf as $\x$, that is,
$$w_j\n(\x; \X\n) = \frac{1}{B} \sum_{b=1}^B w_{bj}\n(\x; \X\n),$$
where 
$$w_{bj}\n(\x; \X\n) := \frac{I(\X_j\n \in L_b(\x))}{{\rm card} \{L_b(\x)\}},$$
with ${\rm card} \{L_b(\x)\} $ denoting the number of elements in $L_b(\x)$. Clearly, $w_j\n(\x; \X\n)$ satisfies condition \eqref{eq.wj.condition}  by construction. 

As usual,  we need a criterion for splitting when growing a tree. This criterion is typically based on the minimization of a pre-specified loss function. In the RF literature, the  response variable is one-dimensional and the loss function is usually the mean square error of the prediction. Since the response variable is a random vector in our setting, naively taking Euclidean distance between the prediction and test sample in the loss function is not advisable since it ignores the underlying correlation structure between components of the response variable. Therefore, we consider minimizing the Mahalanobis distance between the predictions and observations as in \cite{SX11}. This type of multivariate RF has been implemented in R package {\tt MultivariateRandomForest}: the availability of some routines simplifies the implementation of our method. Clearly, other distances (e.g. the Wasserstein distance) may be considered. %Alternatively, one may consider replacing the Mahalanobis distance with the Wasserstein distance, but we leave it for further research.






\subsection{Monte Carlo studies}\label{experiments}

%\cite{dBSH22} shows that the kernel method and KNN method already perform well when $\textbf{X}$ is one-dimensional. So we consider the case where $\textbf{X}$ is higher dimensional. 
In this section, we investigate, via Monte Carlo experiments, finite sample performance of the kernel, kNN and RF weights. The DGP is 
\begin{equation}
	\textbf{Y}\n_{i}=  \binom{Y\n_{i_1}}{Y\n_{i_2}} =  (|X^{(n)}_{i_1}|+\ldots+|X^{(n)}_{i_m}| )  \binom{e\n_{i_1}}{e\n_{i_2}}, i = 1, \ldots, n,
\end{equation}
where  $(e\n_{i_1}, e\n_{i_2})^{\top} \sim \mathcal{N}\left((0,  0)^{\top}, \mathbf{I}_2 \right)$ and $ \textbf{X}^{(n)}_i= (X^{(n)}_{i_1},\ldots,X^{(n)}_{i_m} )^{\top}   \sim \mathcal{U}( [-1,1]^m )$. 

To evaluate the performance of different methods, we propose to compare the radius of the population contours with the length of the corresponding empirical quantiles. Note that since $ \textbf{Y}^{(n)}_i$ is sampled from standard normal distribution after scaling, the population conditional quantile contour  $\mathcal{C}_{w, \pm}(\tau \mid \x)$ at each $\x \in \mathbb{R}^m$ is a sphere. Let $R_{\tau}(\x)$  represent the radius of $\mathcal{C}_{w, \pm}(\tau \mid \x)$. So we can evaluate the performance of the method by calculating the distance between each point on the empirical regression quantile contour $ \mathcal{C}_{w, \pm}^{(n)}(\tau \mid \x)$ and the origin (the closer the distance to $ R_{\tau}(\textbf{X}) $, the better the performance). Specifically, assuming that $\mathcal{C}_{w, \pm}^{(n)}(\tau \mid \x)$ has $N_S$ elements, denoted by $\textbf{Y}_1^{\x},\textbf{Y}_2^{\x},\dots,\textbf{Y}_{N_S}^{\x}$,  we define a quantity called MSREC (mean square  radius error of regression quantile contour): %similar to the MSE:
%\[ 
%\mathrm{MSREC}_{\tau}(\x) := \frac{1}{N_S} \sum_{j=1}^{N_S} (\vert \textbf{Y}_{j}^{\x} \vert - R_{\tau}(\x) )^2.
%\]
$$
\mathrm{MSREC}_{\tau}(\x) := \frac{1}{N_S} \sum_{j=1}^{N_S} (\vert \textbf{Y}_{j}^{\x} \vert - R_{\tau}(\x) )^2.
$$ 
Based on the MSREC, we can define the MSRET (mean square  radius error of regression quantile tube) whose intuitive idea is to average MSREC again for the different contours. Note that we should take into account that different $\x$ correspond to different radius, so we consider rescaling by $R^{-2}_{\tau}(\x)$. Suppose there are $N_{\textbf{x}}$ contours, %we define
\[  
\mathrm{MSRET}_{\tau} := \frac{1}{N_{\textbf{x}}} \sum_{k=1}^{N_{\textbf{X}}} \frac{1}{R^2_{\tau}(\textbf{x}_k)} \mathrm{MSREC}_{\tau}(\textbf{x}_k).
\]



In the following experiments, we set: the bandwidth $b_n=0.1$ in the kernel method; the number of nearest neighbours $k=50$ in the kNN method; the number of  trees  $B= 200$ in RF method---in fact, we have tried $B = 100, 200, 500, 1000$, and for $B \geq 200$, the results are similar. We apply the Gaussian kernel %$K(\x) = e^{-\vert \x \vert^2}$ 
to generate 
 %the kernel weights
$$w_i\n(\x; \X\n) = K\left(\frac{\X_i\n -\x}{b_n}\right) / \sum_{j=1}^n  K\left(\frac{\X_j\n -\x}{b_n}\right), \quad i= 1, \ldots, n.$$



First, we investigate the performance of the three methods from the contour perspective. We set $n=3000$ and $m=2$.
The cross-section plot at $\x = (0.7, 0.7)^\top$ is shown in Figure~\ref{fig:contour}. We find that the empirical conditional center-outward contours of kernel and kNN methods differ significantly from the population contours---e.g. the kNN contours are quite jagged and erratic. On top of that, the estimated contours of different $\tau$-levels overlap, which means that a point is at multiple quantile levels:  this is obviously  undesirable. In contrast, the RF yields better results: the estimated contours (which are discrete by construction) have a shape which conforms nicely to the theoretical contours.  Moreover, we find that the MSRECs of the RF method are significantly smaller than the others.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_k.eps}
		\caption{Kernel: $\textrm{MSREC} = 0.0757, 0.0710 ,0.1526$. }
		\label{fig:ck}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_kn.eps}
		\caption{kNN: \textrm{MSREC} = 0.0966, 0.1645, 0.2318.}
		\label{fig:ckn}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/c_f.eps}
		\caption{RF: \textrm{MSREC} = 0.0282, 0.0565, 0.0563.}
		\label{fig:cf}
	\end{subfigure}
	\caption{Cross-sectional plot at $\x = (0.7, 0.7)^\top$. The  green,  red  and black dashed lines represent the quantile contours of $\tau=0.2,0.4,0.6$, respectively, and the solid lines are the corresponding empirical results.}
	\label{fig:contour}
\end{figure}

Second, we compute regression quantile tubes to compare the performance of the three methods in a broader context. For the sake of visualization, let us consider 
(we drop the superscripts for the ease of notation)  
%$ \left\lbrace \x_{j}^{(N_{\x})} \right\rbrace _{j =1}^{N_{\x}} $ where  $N_{\x}=20$,  
%$ \x_{j}^{(N_{\x})}   = (x_{j_1}^{(N_{\x})},0.5)^{\top} $,
%and  $\left\lbrace x^{(N_{\x})}_{j}\right\rbrace_{j_1=1}^{N_{\x} }$ is an equally spaced sequence from  from -0.9 to 0.9.
%\textcolor{red}{I do not understand the business of the index. Can we use: " 
 $ \left\lbrace \x_{j}, j =1,2,...,N_{\x}  \right\rbrace $, where  $N_{\x}=20$,  
$ \x_{j}   = (x_{j_1},0.5)^{\top} $,
and  $\left\lbrace x_{j_1}, j_1=1,2..., N_{\x}\right\rbrace$ is an equally spaced sequence from  from -0.9 to 0.9.
%" ? Otherwise we have $\left\lbrace \x_{j}\right\rbrace$ whose first component has index $j_1$ and the indices $j$ and $j_1$ do not match. Or for you $j$ is a bivariate index: $j=(j_1,j_2)$? But then it should be bold?}
Then we  obtain a set of contours, and by considering projection to the three dimensional space containing vectors like  $(x_1, y_1, y_2)^\top$, we get an empirical regression quantile tube. In Figure~\ref{fig:tube} we display the tubes at different $\tau$-levels.  The plots illustrate that over a wide range of $x_1$, the RF method performs best, yielding the smoothest and approximately spherical shaped contours. Also, the \textrm{MSRET} yielded by the RF method is much smaller than the one of the other competitors. 
%These show that the forest approach has better performance. 



\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.965\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/t_k.eps}
		\caption{Kernel weights: \textrm{MSRET} = 0.0405, 0.0360, 0.0614 for $\tau=0.2,0.4,0.6$. }
		\label{fig:tk}
	\end{subfigure}
	\begin{subfigure}[b]{0.965\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/t_kn.eps}
		\caption{kNN weights: \textrm{MSRET} = 0.0601, 0.0718, 0.0929 for $\tau=0.2,0.4,0.6$.}
		\label{fig:tkn}
	\end{subfigure}
	\begin{subfigure}[b]{0.965\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/t_f.eps}
		\caption{RF weights: \textrm{MSRET} = 0.0216, 0.0236, 0.0386 for $\tau=0.2,0.4,0.6$.}
		\label{fig:tf}
	\end{subfigure}
	\caption{The red line represents the   regression median. The black, the blue  and magenta  lines represent the  center-outward regression quantile tubes of $\tau=0.2,0.4,0.6$ respectively.  }
	\label{fig:tube}
\end{figure}

Thanks to the adaptive and data-driven property of RF \cite{ATW19, lin2006random}, our method tends to perform better than the kernel and kNN methods in the case of sparse sample distribution. This is often the case when one has to deal with a small number sample size and/or when $\mathbf{X}$ has large dimension. We illustrate this aspect through the following experiments.


We still keep the above experimental setup and try different sample sizes $n=500,1000,2000,3000$.  $\textrm{MSRET}_{\tau}$ of the kernel, kNN and RF methods at $\tau=0.2,0.4,0.6$  are shown in Table~\ref{tab:samplesize}. For all sample sizes, the RF method outperforms the others in terms of $\textrm{MSRET}_{\tau}$.  Notice that the kernel and kNN are significantly affected by the sample size: when $n=500$, they produce very large $\textrm{MSRET}_{\tau}$ compared to the RF. Quite on the contrary, $\textrm{MSRET}_{\tau}$ of the RF remains stable when $n$ changes.




\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}[4]{*}{} & \multicolumn{3}{c|}{$n=500$} & \multicolumn{3}{c|}{$n=1000$} & \multicolumn{3}{c|}{$n=2000$} & \multicolumn{3}{c|}{$n=3000$} \bigstrut\\
		\cline{2-13}          & Kernel & kNN   & RF & Kernel & kNN   & RF & Kernel & kNN   & RF & Kernel & kNN   & RF \bigstrut\\
		\hline
		$\tau$=0.2 & 0.1937 & 0.0748 & 0.0301 & 0.0783 & 0.0552 & 0.0398 & 0.0569 & 0.0630 & 0.0394 & 0.0405 & 0.0601 & 0.0216 \bigstrut\\
		\hline
	$	\tau$=0.4 & 0.2779 & 0.1287 & 0.0561 & 0.0954 & 0.0608 & 0.0496 & 0.0625 & 0.0723 & 0.0454 & 0.0360 & 0.0718 & 0.0236 \bigstrut\\
		\hline
		$\tau$=0.6 & 0.3808 & 0.1643 & 0.0888 & 0.2237 & 0.1090 & 0.1171 & 0.0828 & 0.1202 & 0.0472 & 0.0614 & 0.0929 & 0.0386 \bigstrut\\
		\hline
	\end{tabular}%
	\caption{\textrm{MSRET} for different sample sizes. }
	\label{tab:samplesize}%
\end{table}%

%Now we consider the case of different dimensions. Define  $ \left\lbrace \x_{j}^{(N_{\x})} \right\rbrace _{j =1}^{N_{\x}} $ where  $N_{\x}=20$,  
%$ \x_{j}^{(N_{\textbf{X}})}  = (x_{j_1}^{(N_{\x})},\ldots, x_{j_m}^{(N_{\x})} )^{\top} = (x_{j_1}^{(N_{\x})},0.5, \ldots, 0.5)^{\top} $,
%and  $\left\lbrace x^{(N_{\x})}_{j_1}\right\rbrace_{j=1}^{N_{\x} }$ is an equally spaced sequence from -0.9 to 0.9. \textcolor{red}{Same comment as before: change notation?} 
Now, we consider the case of several dimensions $m$. To this end, we define the $m$-dimensional  $ \left\lbrace \x_{j}, j =1,2,..., N_{\x}\right\rbrace$, where  $N_{\x}=20$,  
$ \x_{j}  = (x_{j_1},0.5, \ldots, 0.5)^{\top} $, and  $\left\lbrace x_{j_1}, j_1=1,2,...,N_{\x}\right\rbrace$ is an equally spaced sequence from -0.9 to 0.9. 
We project to the three dimensional space containing vectors like $(x_1, y_1,y_2)^\top$, and calculate the \textrm{MSRET} of the projection empirical regression quantile tubes. In Table~\ref{tab:dimensions}, we display the results.  In the one-dimensional case ($m=1$),  the numbers in the first three columns  provide evidence of the fact that both the kernel and the kNN method perform well, with a the kernel method yielding the smaller \textrm{MSRET}. The RF method performs similarly to the kNN method, giving similar \textrm{MSRET}, for all choices of $\tau$  and for both sample sizes ($n=500,1000$). In contrast, in the multidimensional cases ($m=2,5$), the RF method provides a clear advantage: its \textrm{MSRET} is smaller than the one entailed by the other methods. To this regard, we emphasize that the RF weights are somewhat similar to the the kNN method weights, in the sense that they are a weighted average of the samples of the nearest neighbours. However, the kNN method fixes  the number of nearest neighbors and their weights to $k$ and ${1}/{k}$, whereas random forests update them adaptively and are hence more flexible. This improves  the \textrm{MSRET} of RF method over the \textrm{MSRET} of the kNN method.


Based on the above numerical evidence, we recommend the use of the RF method for small samples and multidimensional situations.


\begin{table}[htbp]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multicolumn{2}{|c|}{\multirow{2}[4]{*}{}} & \multicolumn{3}{c|}{$m=1$} & \multicolumn{3}{c|}{$m=2$} & \multicolumn{3}{c|}{$m=5$} \bigstrut\\
		\cline{3-11}    \multicolumn{2}{|c|}{} & Kernel & kNN   & RF & Kernel & kNN   & RF & Kernel & kNN   & RF \bigstrut\\
		\hline
		\multirow{3}[6]{*}{$n=500$} & $\tau=0.2$ & 0.0373 & 0.0591 & 0.0705 & 0.1937 & 0.0748 & 0.0301 & 0.2445 & 0.0804 & 0.0549 \bigstrut\\
		\cline{2-11}          & $\tau=0.4$ & 0.0438 & 0.0669 & 0.0903 & 0.2779 & 0.1287 & 0.0561 & 0.1426 & 0.0569 & 0.0403 \bigstrut\\
		\cline{2-11}          & $\tau=0.6$ & 0.0779 & 0.1112 & 0.1065 & 0.3808 & 0.1643 & 0.0888 & 0.2666 & 0.1124 & 0.0904 \bigstrut\\
		\hline
		\multirow{3}[6]{*}{$n=1000$} & $\tau=0.2$ & 0.0260 & 0.0656 & 0.0462 & 0.0783 & 0.0552 & 0.0398 & 0.4490 & 0.0500  & 0.0200 \bigstrut\\
		\cline{2-11}          & $\tau=0.4$ & 0.0305 & 0.0633 & 0.0502 & 0.0954 & 0.0608 & 0.0496 & 0.2245 & 0.0894 & 0.0246 \bigstrut\\
		\cline{2-11}          & $\tau=0.6$ & 0.0443 & 0.1053 & 0.0761 & 0.2237 & 0.1090 & 0.1171 & 0.2240 & 0.1384 & 0.0336 \bigstrut\\
		\hline
	\end{tabular}%
	\caption{\textrm{MSRET} for different dimensions of $\x$. }
	\label{tab:dimensions}
\end{table}%

\section{Conclusion}

We consider some novel aspects of quantile regression in non standard settings. 

In the first part of the paper, we focus on the theory and methods for autoregression quantile estimation for locally stationary time series. We introduce the class of local polynomial estimators and derive their asymptotics. Our Monte Carlo studies illustrate that the local constant estimator strikes a good balance between computational complexity and finite sample performance. Some open questions remain. For instance, to complete our theoretical and numerical investigation, we are planning to study the behaviour of the proposed estimators when $u$ is close to its boundaries, namely $u \rightarrow 0$ and $u \rightarrow 1$; see \cite{ZW09} for a similar analysis. Moreover, similarly to \cite{KS95, HJ99}, we are planning to investigate if and how the notion of autoregression quantiles for locally stationary processes of section \ref{sec:2} can yield autoregressive rank scores for testing in locally stationary processes. 

In the second part of the paper, we explain how to merge the theory of generalized random forests with the optimal transportation theory to derive a forest-based multiple-output center-outward quantile regression. The investigation carried in this paper is mainly at the methodological level. We provide numerical evidence of the good performance of the proposed inference procedure in  small/moderate samples or when the dimension of the covariates is large. The theoretical analysis of the properties of the estimation procedure defined in section \ref{sec:3} remains the topic for future research. %Our educated guess is that the asymptotics can be derived moving along the lines as \cite{ATW19}.




\bibliographystyle{apalike}
\bibliography{HallinPaper}

\newpage 

\section{Appendix}

\subsection{Proof of Theorem 2}

In this Appendix, we use the notation $|\boldsymbol z |=\left|z_1\right|+\cdots+\left|z_k\right|$ for a vector $\boldsymbol z=\left(z_1, \ldots, z_k\right)^{\top}$.
For simplicity and readability of the proof, we derive our arguments focusing on an AR(1)---the AR($p$) case follows with some notational changes. Thus,
we consider 
\begin{equation}
    X_i= \alpha(i/n\mid\tau) + \phi(i/n\mid\tau) X_{i-1} + \varepsilon_i(\tau) = \boldsymbol{\theta}_0(i/n \mid \tau )^{\top} \boldsymbol{U}_i + \varepsilon_i(\tau),
    \label{eq:model}
\end{equation}
where $ \boldsymbol{\theta}_0(i/n \mid \tau ) = [\alpha(i/n\mid\tau) ,\phi(i/n\mid\tau)]^{\top}$.  We denote by $Q_\tau(X_i \mid \mathcal{F}_{i-1})$ the conditional $\tau$th quantile of $X_i$ given its filtration. A $k$th order of Taylor's approximation of $\boldsymbol{\theta}(i/n \mid \tau )$ around $i/n$ yields
\begin{equation}
    \begin{aligned}
    Q_\tau(X_i \mid \mathcal{F}_{i-1}) &= \boldsymbol{U}_i^{\top} \boldsymbol{\theta}_0(i/n \mid \tau ) \\ 
    & \approx \boldsymbol{U}_i^{\top} \left\{\boldsymbol{\theta}_0(u \mid \tau ) +  \left(i/n - u\right)^1 \boldsymbol{\theta}'(u \mid \tau )/1! + \dots + \left(i/n - u\right)^k \boldsymbol{\theta}^{(k)}(u \mid \tau )/k!\right\}.
\end{aligned}
\end{equation}

We consider the local polynomial quantile regression
\begin{equation}
\begin{gathered}
\left(\hat{\boldsymbol{\theta}}_0(u \mid \tau), \hat{\boldsymbol{\theta}}^{\prime}(u \mid \tau), \dots , \hat{\boldsymbol{\boldsymbol{\theta}}}^{(k)}(u \mid \tau)\right)= \\ 
\underset{\boldsymbol{\theta}_0, \dots, \boldsymbol{\theta}_k}{\operatorname{argmin}} \sum_{i=1}^n \rho_\tau\left\{X_i- \sum_{m = 0}^k\left(i/n-u\right)^k \boldsymbol{U}_i^{\top} \boldsymbol{\theta}_k/k!\right\}K\left(\frac{i/n - u}{b_n}\right) .
\end{gathered}
\label{eq:argmin}
\end{equation}


%\begin{theorem}
%    Suppose Assumption 1 holds. Then $\left\{X_i\right\}_i$ in model \ref{eq:model}  has the local stationarity approximation $\left\{X_i(u)\right\}_i$ in $L_2$ norm: 
%    \begin{equation}
%        X_i(u)= \alpha(u \mid \tau)+\phi(u \mid \tau) X_{i-1}(u) +\varepsilon_i(\tau)
%    \end{equation}
%\end{theorem}
%
To devise the proof for the asymptotics of the resulting estimators, we recall the following two Lemmas from \cite{XSZ22}.
\begin{lemma}

    Let $g(\cdot) \in \mathcal{C}^1[0,1]$ be any function. Under Assumption 3, for given $t \in(0,1)$,
    $$
    \sum_{i=1}^n g(i / n)\left(\frac{i/n - u}{b_n}\right)^r K_i(u)=n b_n g(u) \int_{\mathbb{R}} u^r K(u) d u+O\left(n b_n^2+1\right) . \quad r=0,1, \ldots
    $$
\end{lemma}
%\begin{proof}
 In particular, by simple calculus, we can show
    $$
    \sum_{i=1}^n\left(\frac{i/n - u}{b_n}\right)^r K_i(u)=n b_n \int_{\mathbb{R}} u^r K(u) d u+O(1), \quad r=0,1, \ldots
    $$
    The result of Lemma 1 then easily follows from $g(i / n)=g(u)+O\left(b_n\right)$ for $i/n - u=O\left(b_n\right)$. We will need these calculations for
    the proof of Theorem 2. Moreover, we will need also the following
\begin{lemma}
    For each $i$, let $D_i \in {L}^\delta, \delta>2$, be any measurable function of a stationary $\alpha$-mixing process with mixing coefficients $\alpha_k$ satisfying $\sum_{k=1}^{\infty} \alpha_k^{1-2 / \delta}<\infty$. Then
    $$
    \operatorname{var}\left\{\sum_{i=1}^n D_i K_i(u)\right\}=O\left(n b_n \Lambda_n^2\right), \quad \Lambda_n=\max _{1 \leq i \leq n}\left\|D_i\right\|_\delta .
    $$
\end{lemma}

\begin{lemma}
Consider $\boldsymbol{U}_i$ and its local stationary approximation $\boldsymbol{U}_i(u)$ (see (\ref{Def_LS})). For any non-random column vectors $\boldsymbol{z}_i \in \mathbb{R}^{p+1}$ that may depend on $n$, define
\begin{equation*}
H_n=\sum_{i=1}^n \rho_\tau\left(\boldsymbol{z}_i^{\top} \boldsymbol{U}_i+\varepsilon_i(\tau)\right) K_i(u),
\end{equation*}
\begin{equation*}
\tilde{H}_n=\sum_{i=1}^n \rho_\tau\left(\boldsymbol{z}_i^{\top} \boldsymbol{U}_i(u)+\varepsilon_i(\tau)\right) K_i(u) .
\end{equation*}
Suppose Assumption 3 and Assumption 4(i) and (iv) hold. Then
\begin{equation*}
H_n-\tilde{H}_n=O_{\rm p}\left[b_n\left(\chi_n+\sqrt{\chi_n}\right)\right] \quad \text { with } \quad \chi_n=\sum_{i=1}^n \boldsymbol{z}_i^{\top} \boldsymbol{z}_i K_i(u) .
\end{equation*}

\end{lemma}
The proofs of these lemmas can be found in the Supplementary Material of \cite{XSZ22}.\\


Now, we can prove the Theorem 2. To start with, let us recall that $\boldsymbol{U}_i = [1, X_{i-1}]^{\top}$ and that  $\boldsymbol{U}_i(u)$ is $\boldsymbol{U}_i$ where we replace the $X_i$ by its stationary approximation $X_i(u)$. The following bounds are valid since the vector $\boldsymbol{U}_i$ is the same as in the proof of \cite{XSZ22}, thus we have

\begin{equation}
    \begin{aligned}
    \rmE\left|\boldsymbol{U}_i-\boldsymbol{U}_i(u)\right| & =O(|i/n - u|+1 / n), \\
    \rmE\left|\boldsymbol{U}_i \left[\boldsymbol{U}_i\right]^{\top}-\boldsymbol{U}_i(u)\left[\boldsymbol{U}_i\right]^{\top}(u)\right| & =O(|i/n - u|+1 / n), \\
    \rmE\left|\left[\boldsymbol{U}_i(u)-\boldsymbol{U}_i\right]\left[\boldsymbol{U}_i(u)-\boldsymbol{U}_i\right]^{\top}\right| & =O\left[(|i/n - u|+1 / n)^2\right] .
    \end{aligned}
\end{equation}
The loss function of the optimization problem is defined by

$$
\mathcal{L}(\boldsymbol{\theta}_0, \dots,\boldsymbol{\theta}_k ) = \sum_{i = 1}^n \rho_\tau \left\{X_i - \sum_{m = 0}^k \boldsymbol{U}_i^{\top} (i/n - u)^m \boldsymbol{\theta}_m\right\} K_i(u).
$$
The vectors $\left(\hat{\boldsymbol{\theta}}_0(u \mid \tau), \hat{\boldsymbol{\theta}^{\prime}}(u \mid \tau), \dots , \hat{\boldsymbol{\theta}}^{(k)}(u \mid \tau)\right)$ are the minimizer of $\mathcal{L}(\boldsymbol{\theta}_0, \boldsymbol{\theta}_1, \dots,\boldsymbol{\theta}_k )$. Define

$$
    \boldsymbol{\Delta}_k =\sqrt{n b_n}\left[\begin{array}{c}
    b_n^0 \left\{ \boldsymbol{\theta}_0-\boldsymbol{\theta}_0(u \mid \tau)\right\} \\
    b_n^1\left\{\boldsymbol{\theta}_1-\boldsymbol{\theta}^{\prime}(u \mid \tau)\right\}  \\
    \dots  \\
    b_n^k\left\{\boldsymbol{\theta}_k-\boldsymbol{\theta}^{(k)}(u \mid \tau)\right\} \\
    \end{array}\right], \quad  
    \boldsymbol{Y}_{i,k}=\left[\begin{array}{c}
    (\frac{i/n - u}{b_n})^0 \boldsymbol{U}_i \\
    (\frac{(i/n - u)}{b_n})^1 \boldsymbol{U}_i \\
    \dots \\
    (\frac{(i/n - u)}{b_n})^k \boldsymbol{U}_i \\
\end{array}\right],
$$
both ${(k+1) \times 1}$ vectors. Note that the covariance matrix of $\boldsymbol{Y}_{i,k}(u)$ has the following form

$$
\rmE\left[\boldsymbol{Y}_{i,k}(u) \boldsymbol{Y}_{i,k}(u)^{\top}\right] = 
\begin{bmatrix}
    \boldsymbol{\Gamma}(u) & (i/n - u) \boldsymbol{\Gamma}(u) & \dots & (i/n - u)^{k} \boldsymbol{\Gamma}(u) \\
    (i/n - u) \boldsymbol{\Gamma}(u) & \dots & \dots & \dots \\
    \dots & \dots & \dots & \dots \\
    (i/n - u)^{k} \boldsymbol{\Gamma}(u) & \dots & \dots & (i/n - u)^{2k} \boldsymbol{\Gamma}(u)
    \end{bmatrix} .
$$
We can rewrite 

$$
\begin{aligned}
    \boldsymbol{U}_i^{\top} \delta_{i,k} +\varepsilon_i(\tau)-\frac{\boldsymbol{Y}_{i,k}^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}} = X_i - \sum_{m = 0}^k \boldsymbol{U}_i^{\top} (i/n - u)^m \boldsymbol{\theta}_m
\end{aligned}
$$
as a function of $\boldsymbol{\Delta}_k$ where 

\begin{equation}
    \boldsymbol{\delta}_{i,k} = 
  \begin{cases} 
    \boldsymbol{\theta}_0\left(i/n \mid \tau\right)-\boldsymbol{\theta}_0(u \mid \tau) = O(b_n) & \text{if } k = 0  \\
    \boldsymbol{\theta}_0\left(i/n \mid \tau\right)-\boldsymbol{\theta}_0(u \mid \tau)- \sum_{m = 1}^k \left(i/n-u\right)^m \boldsymbol{\theta}^{(m)}(u \mid \tau)/m! = O(b_n^{k+1})      & \text{if } k \geq 1
  \end{cases}
\end{equation}
and since $\boldsymbol{\theta}(t|\tau) \in C^{k+1}[0,1], \boldsymbol{\delta}_{i,k} = O(b_n^{k+1})$. 
We write the re-parameterized vector
$$
\hat{\boldsymbol{\Delta}}_k=\sqrt{n b_n}\left[\begin{array}{c}
b_n^0 \left\{\hat{\boldsymbol{\theta}}_0(u \mid \tau)-\boldsymbol{\theta}_0(u \mid \tau) \right\} \\
\cdots \\
b_n^k\left\{\hat{\boldsymbol{\theta}}^{(k)}(u \mid \tau)-\boldsymbol{\theta}^{(k)}(u \mid \tau)\right\}
\end{array}\right],
$$
which minimizes the re-parameterized loss

$$\mathcal{L}(\boldsymbol{\Delta}_k):=\sum_{i=1}^n\left(\rho_\tau\left\{\boldsymbol{U}_i^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)-\frac{\boldsymbol{Y}_{i,k}^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}\right\}-\rho_\tau \left\{\boldsymbol{U}_i^{\top} \boldsymbol{\delta}_{i,k} +\varepsilon_i(\tau)\right\}\right) K_i(u).$$
We compute a quadratic approximation for $\mathcal{L}(\boldsymbol{\Delta}_k)$ by means of the two following steps. \\

\textbf{1st step.}  The first step shows that the difference between $\mathcal{L}(\boldsymbol{\Delta}_k)$ and its version in which we replace $\boldsymbol{U}_{i}$ by its stationary approximation $\boldsymbol{U}(u)_{i}$ is $\rm o_p(1)$. To develop further, define

$$
\tilde{\mathcal{L}}(\boldsymbol{\Delta}_k):=\sum_{i=1}^n\left(\rho_\tau\left\{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)-\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}\right\}-\rho_\tau\left\{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)\right\}\right) K_i(u).
$$
We want to prove that $\mathcal{L}(\boldsymbol{\Delta}_k)-\tilde{\mathcal{L}}(\boldsymbol{\Delta}_k) = o_{\rm p}(1)$ using Lemma 3. Note that 

$$
\begin{aligned}
    \tilde{\mathcal{L}}(\boldsymbol{\Delta}_k)&:=\sum_{i=1}^n\left(\rho_\tau\left\{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)-\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}\right\}-\rho_\tau\left\{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)\right\}\right) K_i(u) \\
    & = \sum_{i=1}^n\rho_\tau\left\{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)-\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}\right\}K_i(u) - \sum_{i = 1}^n\rho_\tau\left\{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)\right\} K_i(u)
\end{aligned}
$$
where we define the first term of the right-hand side as $\tilde{\mathcal{L}_1}(\boldsymbol{\Delta}_k)$ and the seconde one as $\tilde{\mathcal{L}}_2$. Now, we know that $\tilde{\mathcal{L}}_2 - \mathcal{L}_2 = O_{\rm p}\left[b_n\left(\chi_{2,n}+\sqrt{\chi_{2,n}}\right)\right]$, where 
$$\chi_{2,n}=\sum_{i=1}^n\left(\boldsymbol{\delta}_{i,k}^{\top} \boldsymbol{\delta}_{i,k}\right) K_i(u) = O(nb_n b_n^{2(k+1)}),$$ 
thus 
$$\tilde{\mathcal{L}}_2 - \mathcal{L}_2 = O_{\rm p}\left[b_n\left(\sqrt{O(nb_nb_n^{2(k+1)})}\right)\right] = o_{\rm p}(1).
$$ 
Moreover, $\tilde{\mathcal{L}_1}(\boldsymbol{\Delta}_k) - \mathcal{L}_1(\boldsymbol{\Delta}_k) = O_{\rm p}\left[b_n\left(\chi_{1,n}+\sqrt{\chi_{1,n}}\right)\right]$ where 
$$
\chi_{1,n}=\sum_{i=1}^n\left(\boldsymbol{\delta}_{i,k}^{\top} \boldsymbol{\delta}_{i,k}+O([nb_n]^{-1})\right) K_i(u) = O(b_n^{2(k+1)} + O([nb_n]^{-1}))O(nb_n).
$$ 
Thus $\mathcal{L}(\boldsymbol{\Delta}_k)=\tilde{\mathcal{L}}(\boldsymbol{\Delta}_k)+o_{\rm p}(1)$ under the assumption that $nb_n^{2(k+1)+1} \rightarrow 0$. \\

\textbf{2nd step.} The second step derives a quadratic approximation to the loss function. By the identity 

\begin{equation}
    \rho_\tau(u-\boldsymbol{\delta})-\rho_\tau(u)=-\boldsymbol{\delta}\left(\tau-\Ind{u<0}\right)+\int_0^{\boldsymbol{\delta}}\left(\Ind{u \leq s}-\Ind{u \leq 0}\right) d s,
\end{equation}
we write

\begin{equation}
    \tilde{\mathcal{L}}(\boldsymbol{\Delta}_k)=-\boldsymbol{A}_{n,k}^{\top} \boldsymbol{\Delta}_k+I_{n,k},
\end{equation}
where
\begin{equation}
    \begin{aligned}
        & \boldsymbol{A}_{n,k}=\frac{1}{\sqrt{n b_n}} \sum_{i=1}^n\left[\tau-\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)<0}\right] K_i(u) Y_i(u), \\
        & I_{n,k}=\sum_{i=1}^n \eta_i K_i(u), \quad \\
        & \eta_i=\int_0^{\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}}\left[\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau) \leq s}-\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau) \leq 0}\right] d s . \\
        \end{aligned}
\end{equation}
We start by focusing on $I_{n,k}$. Using the inequality $\left|\int_0^{\boldsymbol{\delta}_k} \left(\Ind{u \leq s}-\Ind{u \leq 0}\right) d s\right| \leq|\boldsymbol{\delta}_k| \Ind{|u| \leq|\boldsymbol{\Delta}_k|}$, we have

$$
\left|\eta_i\right| \leq \frac{\left|\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right|}{\sqrt{n b_n}} \Ind{\left|\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)\right| \leq\left|\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right| / \sqrt{n b_n}} .
$$
Note that there exists a constant $c_1<\infty$ such that $\left|\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right| \leq c_1\left|\boldsymbol{U}_i(u)\right|$ for a fixed $\boldsymbol{\delta}_k$ which implies that $\left|\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right| = O(\left|\boldsymbol{U}_i(u)\right|)$ for $i/n - u=O\left(b_n\right)$. 
So the bound becomes, 

$$
\left|\eta_i\right| \leq \frac{c_1 |\boldsymbol{U}_i(u)|}{\sqrt{n b_n}} \Ind{\left|\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)\right| \leq\left|\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right| / \sqrt{n b_n}} .
$$
Also, $\boldsymbol{\delta}_{i,k}=O\left(b_n^{k+1}\right)$ uniformly for $i/n - u=$ $O\left(b_n\right)$. Thus, there exists a constant $c_2$ such that
$$
\left|\eta_i\right| \leq \frac{c_1\left|\boldsymbol{U}_i(u)\right|}{\sqrt{n b_n}} \Ind{\left|\varepsilon_i(\tau)\right| \leq c_2\left|\boldsymbol{U}_i(u)\right|\left(1 / \sqrt{n b_n}+b_n^{k+1}\right)} .
$$
Next we can compute the variance of $I_{n,k}$ by Lemma 2 and the stationarity of $\left\{\left(\boldsymbol{U}_i(u), \varepsilon_i(\tau)\right)\right\}_i$
$$
\begin{aligned}
\operatorname{var}\left(I_{n,k}\right) &= \operatorname{var}\left(\sum_{i = 1}^n \eta_i K_i\right) \\& \leq n b_n \max _{1 \leq i \leq n}\left\|\frac{c_1\left|\boldsymbol{U}_i(u)\right|}{\sqrt{n b_n}} \Ind{\left|\varepsilon_i(\tau)\right| \leq c_2\left|\boldsymbol{U}_i(u)\right|\left(1 / \sqrt{n b_n}+b_n^{k+1}\right)}\right\|_{2+\epsilon}^2 \\
& =c_1^2\left\|\left|\boldsymbol{U}_1(u)\right| \Ind{\left|\varepsilon_1(\tau)\right| \leq c_2\left|\boldsymbol{U}_1(u)\right|\left(1 / \sqrt{n b_n}+b_n^{k+1}\right)}\right\|_{2+\epsilon}^2 \rightarrow 0,
\end{aligned}
$$
where the last equality comes from the dominated convergence theorem because of $\left|\boldsymbol{U}_i(u)\right| \in {L}^{2(2+\epsilon)}$ and $\left(n b_n\right)^{-1 / 2}+b_n^{k+1} \rightarrow 0$.

Using the Taylor's expansion 
$$
F_\tau(v)=F_\tau(0)+v f_\tau(0)+O\left(v^2\right),
$$ 
and $\boldsymbol{\delta}_{i,k}=O\left(b_n^{k+1}\right)$ for $i/n - u=$ $O\left(b_n\right)$, with the notation $\kappa_i = \eta_i \mid \boldsymbol{U}_i(u)$, we derive

$$
\begin{aligned}
\rmE\left[\kappa_i\right]  & =\int_0^{\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}}\left[F_\tau\left\{s-\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}\right\}-F_\tau\left\{-\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}\right\}\right] d s \\
& =\frac{f_\tau(0)\left[\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right]^2}{2 n b_n}+O(1)\left[\frac{\left|\boldsymbol{U}_i(u)\right|^3}{\left(n b_n\right)^{3 / 2}}+\frac{\left|\boldsymbol{U}_i(u)\right| b_n^{2(k+1)}}{\sqrt{n b_n}}\right] \\
& = \int_0^{\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}}  [sf_\tau(0) + O((s-\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2) - O((\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2)] ds \\ 
% & = \frac{f_\tau(0)\left[\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right]^2}{2 n b_n} + \int_0^{\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}} O((s-\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2) - O((\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2)] ds \\
% & =  \frac{f_\tau(0)\left[\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right]^2}{2 n b_n} + \int_0^{\frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}} O((s-\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2) ds + \frac{\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k}{\sqrt{n b_n}}O((\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2)] \\
% & =  \frac{f_\tau(0)\left[\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right]^2}{2 n b_n} + O(1)[\frac{O(|\boldsymbol{U}_i(u)|^3)}{\sqrt{nb_n}^3} + \frac{O(|\boldsymbol{U}_i(u)|)O((\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2)}{\sqrt{nb_n}} \\ 
& = \frac{f_\tau(0)\left[\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right]^2}{2 n b_n}+O(1)\left[\frac{\left|\boldsymbol{U}_i(u)\right|^3}{\left(n b_n\right)^{3 / 2}}+\frac{\left|\boldsymbol{U}_i(u)\right|^3 b_n^{2(k+1)}}{\sqrt{n b_n}}\right]
\end{aligned}
$$
and 

$$
\begin{aligned}
    \rmE\left\{\rmE\left[\eta_i \mid \boldsymbol{U}_i(u)\right]\right\} &= \frac{f_\tau(0)}{2 n b_n}\rmE[\left[\boldsymbol{Y}_{i,k}(u)^{\top} \boldsymbol{\Delta}_k\right]^2]  +  O(1)\left[\frac{\rmE\left|\boldsymbol{U}_i(u)\right|^3}{\left(n b_n\right)^{3 / 2}}+\frac{\rmE\left|\boldsymbol{U}_i(u)\right|^3 b_n^{2(k+1)}}{\sqrt{n b_n}}\right] \\
     & =\frac{f_\tau(0)}{2 n b_n} \boldsymbol{\Delta}_k^{\top}\rmE[Y_i(u)\boldsymbol{Y}_{i,k}(u)^{\top}]\boldsymbol{\Delta}_k+  O(1)\left[\frac{1}{\left(n b_n\right)^{3 / 2}}+\frac{b_n^{2(k+1)}}{\sqrt{n b_n}}\right].
\end{aligned}
$$
Thus, using $\left|\boldsymbol{U}_i(u)\right| \in {L}^{2(2+\epsilon)}$ and by Lemma 1, we have


\begin{equation}
\begin{aligned}
\rmE\left(I_{n,k}\right) & =\sum_{i=1}^n \rmE\left\{\rmE\left[\eta_i \mid \boldsymbol{U}_i(u)\right]\right\} K_i(u) \\ 
% & = \sum_{i =1}^n K_i(u) \boldsymbol{\Delta}_k^{\top} \rmE\left[Y_i(u) \boldsymbol{Y}_{i,k}(u)^{\top}\right] \boldsymbol{\Delta}_k  +  \sum_{i =1}^n K_i(u) O(1)\left[\frac{\left|\boldsymbol{U}_i(u)\right|^3}{\left(n b_n\right)^{3 / 2}}+\frac{\left|\boldsymbol{U}_i(u)\right| b_n^{2(k+1)}}{\sqrt{n b_n}}\right]  \\
& =\frac{f_\tau(0)}{2 n b_n} \sum_{i=1}^n \boldsymbol{\Delta}_k^{\top} \rmE\left[Y_i(u) \boldsymbol{Y}_{i,k}(u)^{\top}\right] \boldsymbol{\Delta}_k K_i(u)+O\left(n b_n\right)\left[\frac{1}{\left(n b_n\right)^{3 / 2}}+\frac{b_n^{2(k+1)}}{\sqrt{n b_n}}\right] \\
& = \frac{f_\tau(0)}{2} \boldsymbol{\Delta}_k^{\top} \boldsymbol{\Omega}(u) \boldsymbol{\Delta}_k + O(1 / \sqrt{nb_n}) + O([n^{-1/2} b_n^{2(k+1)-1/2}) \\
& \rightarrow \frac{f_\tau(0)}{2} \boldsymbol{\Delta}_k^{\top} \boldsymbol{\Omega}(u) \boldsymbol{\Delta}_k
\end{aligned}
\end{equation}
where $\boldsymbol{\Omega}(u)=\operatorname{diag}\left\{\boldsymbol{\Gamma}(u), \boldsymbol{\Gamma}(u) \int_{\mathbb{R}} u^{k+1} K(u) d u\right\}$ is a block diagonal matrix. We are therefore able to write the following quadratic approximation 
\begin{equation}
    %\tilde{\mathcal{L}}(\boldsymbol{\Delta}_k)=  
    \tilde{\mathcal{L}}(\boldsymbol{\Delta}_k)=-\boldsymbol{A}_{n,k}^{\top} \boldsymbol{\Delta}_k+I_{n,k} =  -\boldsymbol{A}_{n,k}^{\top} \boldsymbol{\Delta}_k+\frac{f_\tau(0)}{2} \boldsymbol{\Delta}_k^{\top} \boldsymbol{\Omega}(u) \boldsymbol{\Delta}_k+o_{\rm p}(1),
\end{equation}
since $$I_{n,k} - \frac{f_\tau(0)}{2} \boldsymbol{\Delta}_k^{\top} \boldsymbol{\Omega}(u) \boldsymbol{\Delta}_k = o_{\rm p}(1).$$  
The same quadratic approximation also holds for $\mathcal{L}(\boldsymbol{\Delta}_k)$. Therefore by applying  the convexity lemma [Pollard (1991)], $\hat{\boldsymbol{\Delta}}_k$ has the Bahadur representation
$$
\begin{aligned}
\hat{\boldsymbol{\Delta}}_k & =\underset{\boldsymbol{\Delta}_k}{\operatorname{argmin}}\left\{-\boldsymbol{A}_{n,k}^{\top} \boldsymbol{\Delta}_k+\frac{f_\tau(0)}{2} \boldsymbol{\Delta}_k^{\top} \boldsymbol{\Omega}(u) \boldsymbol{\Delta}_k\right\}+o_{\rm p}(1) \\
& =\frac{1}{f_\tau(0)} \boldsymbol{\Omega}(u)^{-1} \boldsymbol{A}_{n,k}+o_{\rm p}(1) .
\end{aligned}
$$
From the first component of $\hat{\boldsymbol{\Delta}}_k$, we have the asymptotic Bahadur representation for $\sqrt{n b_n}[\hat{\boldsymbol{\theta}}_0(u \mid \tau)-\boldsymbol{\theta}_0(u \mid \tau)]$

\begin{equation}
\frac{\boldsymbol{\Gamma}(u)^{-1}}{\sqrt{n b_n} f_\tau(0)} \sum_{i=1}^n\left[\tau-\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)<0}\right] K_i(u) \boldsymbol{U}_i(u)+o_{\rm p}(1)
\end{equation}
Therefore, we have the bias term and stochastic term decomposition:

\begin{equation}
\sqrt{n b_n}\left[\hat{\boldsymbol{\theta}}_0(u \mid \tau)-\boldsymbol{\theta}_0(u \mid \tau)-\frac{\boldsymbol{\Gamma}(u)^{-1}}{f_\tau(0)} \boldsymbol{B}_{n,k}\right]=\frac{\boldsymbol{\Gamma}(u)^{-1}}{f_\tau(0)} \boldsymbol{W}_n+o_{\rm p}(1),
\end{equation}
where

\begin{equation*}
\begin{aligned}
\boldsymbol{W}_n & =\frac{1}{\sqrt{n b_n}} \sum_{i=1}^n\left[\tau-\Ind{\varepsilon_i(\tau)<0}\right] K_i(u) \boldsymbol{U}_i(u), \\
\boldsymbol{B}_{n,k} & =\frac{1}{n b_n} \sum_{i=1}^n K_i(u) \boldsymbol{\zeta}_i, 
\end{aligned}
\end{equation*}
with $\boldsymbol{\zeta}_i=\left[\Ind{\varepsilon_i(\tau)<0}-\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)<0}\right] \boldsymbol{U}_i(u) .$ Using the same Taylor's expansion argument as before [remember that $\boldsymbol{\delta}_{i,k}=O\left(b_n^{k+1}\right)$ for $i/n - u=$ $\left.O\left(b_n\right)\right]$, we have 

\begin{equation}
\begin{aligned}
    \rmE\left[\boldsymbol{\zeta}_i \mid \boldsymbol{U}_i(u)\right] & = \rmE\left[\left[\Ind{\varepsilon_i(\tau)<0}-\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)<0}\right] \boldsymbol{U}_i(u) \mid \boldsymbol{U}_i(u)\right] \\
    & = \boldsymbol{U}_i(u)  \rmE\left[\left[\Ind{\varepsilon_i(\tau)<0}-\Ind{\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+\varepsilon_i(\tau)<0}\right] \mid \boldsymbol{U}_i(u)\right] \\
    & = \boldsymbol{U}_i(u)  [F_\tau(0) - F_\tau(-\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})] \\ 
    & = \boldsymbol{U}_i(u) \left[F_\tau(0)  - F_\tau(0) + \boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k} f_\tau(0) + O((\boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k})^2)\right] \\
    & = f_\tau(0) \boldsymbol{U}_i(u) \boldsymbol{U}_i(u)^{\top} \boldsymbol{\delta}_{i,k}+O\left(b_n^{2(k+1)}\right)\left|\boldsymbol{U}_i(u)\right|^3
\end{aligned}  
\end{equation} 
and $\rmE\left(\boldsymbol{\zeta}_i\right)=\rmE\left\{\rmE\left[\boldsymbol{\zeta}_i \mid \boldsymbol{U}_i(u)\right]\right\}= f_\tau(0) \boldsymbol{\Gamma}(u) \boldsymbol{\delta}_{i,k}+O\left(b_n^{2(k+1)}\right)$. Now, we recall that 

$$
\boldsymbol{\delta}_{i,k} = 
  \begin{cases} 
    (i/n - u)\boldsymbol{\theta}'(u \mid \tau) + O(b_n^2) & \text{if } k = 0  \\
    \frac{\left(i/n-u\right)^{k+1}}{(k+1)!} \boldsymbol{\theta}^{(k+1)}(u \mid \tau) + O(b_n^{k+2})    & \text{if } k \geq 1
  \end{cases},
$$
thus
\begin{equation*}
\begin{aligned}
\frac{\boldsymbol{\Gamma}(u)^{-1}}{f_\tau(0)} \rmE\left(\boldsymbol{B}_{n,k}\right) & = \frac{\boldsymbol{\Gamma}(u)^{-1}}{f_\tau(0)} \frac{1}{nb_n} \sum_{i = 1}^n K_i(u)f_\tau(0) \boldsymbol{\Gamma}(u) \boldsymbol{\delta}_{i,k}+  \frac{1}{nb_n}\sum_{i = 1}^n K_i(u)O\left(b_n^{2(k+1)}\right) \\
& = \frac{1}{nb_n}\sum_{i = 1}^n K_i(u) \boldsymbol{\delta}_{i,k}+ O(1)O\left(b_n^{2(k+1)}\right) \\
%  &= \frac{1}{nb_n}\sum_{i = 1}^n K_i(u) \left\{(i/n - u)^{k+1} \boldsymbol{\theta}^{(k+1)}(u \mid \tau) / (k+1)!+O\left(b_n^{k+2}\right)\right\}+ O\left(b_n^{2(k+1)}\right) \\
& = b_n^{k+1}  \boldsymbol{\theta}^{(k+1)}(u \mid \tau) /(k+1)!  \left\{\frac{1}{nb_n}\sum_{i = 1}^n (\frac{i/n - u}{b_n})^{k+1} K_i(u) \right\} + O(b_n^{k+2}) \\
& =  b_n^{k+1}  \boldsymbol{\theta}^{(k+1)}(u \mid \tau) /(k+1)! \left\{\int u^{k+1} K(u) du + O((nb_n)^{-1})\right\} + O(b_n^{k+2}) \\ 
& =  b_n^{k+1}  \boldsymbol{\theta}^{(k+1)}(u \mid \tau) /(k+1)! \int u^{k+1} K(u) du + o(([nb_n]^{-1/2})).\\
\end{aligned}
\end{equation*}

Then, we compute the variance of $\boldsymbol{B}_{n,k}$. Since $\boldsymbol{\delta}_{i,k} = O(b_n^{k+1})$ there exists a constant $c_3$ such that

\begin{equation}
    \rmE\left(\left|\boldsymbol{\zeta}_{i,k}\right|^{2+\epsilon}\right) \leq \rmE\left[\Ind{\left|\varepsilon_i(\tau)\right| \leq c_3 b_n^{k+1}\left|\boldsymbol{U}_i(u)\right|}\left|\boldsymbol{U}_i(u)\right|^{2+\epsilon}\right]=O\left(b_n^{k+1}\right) \rmE\left[\left|\boldsymbol{U}_i(u)\right|^{3+\epsilon}\right],
\end{equation}
which leads to $\max _{1 \leq i \leq n} \rmE\left(\left|\boldsymbol{\zeta}_{i,k}\right|^{2+\epsilon}\right)=$ $O\left(b_n^{k+1}\right)$. Thus, applying Lemma 2 with $\delta=2+\epsilon$, we have that the variance is
\begin{equation}
    \begin{aligned}
       \operatorname{var}[\boldsymbol{B}_{n,k}] & = \frac{1}{n^2b_n^2}\operatorname{var}[\sum_{i=1}^{n}K_i(u)\boldsymbol{\zeta}_i] \\
        & = \frac{1}{n^2b_n^2}O(nb_n\left[\operatorname{max} || \boldsymbol{\zeta}_i ||_{2 + \epsilon}\right]^2) \quad \textit{(by Lemma 3)} \\
        & = O(\frac{1}{n^2b_n^2})O(nb_n\cdot b_n^{2(k+1)}) = o([1/(nb_n)]).
    \end{aligned}
\end{equation}
Therefore, we obtain the following expression for the asymptotic bias 

$$
\frac{\boldsymbol{\Gamma}(u)^{-1}}{f_\tau(0)} \boldsymbol{B}_{n,k} = b_n^{k+1}  \boldsymbol{\theta}^{(k+1)}(u \mid \tau) /(k+1)! \int u^{k+1} K(u) du + o_{\rm p}(([nb_n]^{-1/2})).
$$
Now, regarding the last part of the proof, i.e. a CLT for $\boldsymbol{W}_n$, it remains the same as in \cite{XSZ22} since this term does not depend on $\boldsymbol{\delta}_{i,k}$. This completes the proof.



\end{document}
