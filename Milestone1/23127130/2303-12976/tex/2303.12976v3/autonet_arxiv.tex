\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pbox}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{NVAutoNet}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}


% New commands
\newcommand{\loss}{L}
\newcommand\normx[1]{\left\Vert#1\right\Vert}

  
%% Title
\title{NVAutoNet: Fast and Accurate 360$^{\circ}$ 3D Visual Perception For Self Driving
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Corresponding author}}: 
%\textbf{Trung Pham (trungp@nvidia.com)}} 
}

\author{
\small{Trung Pham\thanks{Corresponding author: Trung Pham (trungp@nvidia.com)} , Mehran Maghoumi, Wanli Jiang, \break Bala Siva Sashank Jujjavarapu, Mehdi Sajjadi,}\\
\small{Xin Liu, Hsuan-Chu Lin, Bor-Jeng Chen, Giang Truong, Chao Fang, Junghyun Kwon, Minwoo Park}\\
NVIDIA \\
}


\begin{document}
\maketitle

\begin{figure}[h!]
\centering
\includegraphics[width=0.99\textwidth]{figures/autonet.png}
\caption{NVAutoNet overview. Surround view images are input to CNN backbones to extract 2D features, which are uplifted and fused into a unified BEV feature map. The generated BEV features are then encoded by a BEV backbone. Finally 3D signals are predicted by various 3D perception heads.}
\label{fig:nvautonet}  
\end{figure}
\begin{table}[h!]
	\begin{tabular}{lllll}
	\hline
	Inputs & Outputs & Latency & Detection Range & 	Training data \\
	8 cameras & 3D signals  & 18ms (53fps) & 200 meters & 2.2M scenes\\
	\hline \hline 
    Architecture & Fusion & Modular design & End-to-end training & In-car tested \\ 
	 CNN + MLP & Mid-level & \checkmark  & \checkmark  & \checkmark  \\
	\hline
	\end{tabular}
	\centering
	\caption{NVAutoNet highlights.}
	\label{tab:nvautonet_highlights}
\end{table}

%%%%%%%%% ABSTRACT
\begin{abstract}
Robust, real-time perception of 3D world is essential to the autonomous vehicle. We introduce an end-to-end surround camera perception system, named NVAutoNet, for self-driving. NVAutoNet is a multi-task, multi-camera network which takes a variable set of time-synced camera images as input and produces a rich collection of 3D signals such as sizes, orientations, locations of obstacles, parking spaces and free-spaces, etc. NVAutoNet is modular and end-to-end: 1) the outputs can be consumed directly by downstream modules without any post-processing such as clustering and fusion --- improving speed of model deployment and in-car testing 2) the whole network training is done in one single stage --- improving speed of model improvement and iterations. The network is carefully designed to have high accuracy while running at 53 fps on NVIDIA Orin SoC (system-on-a-chip). The network is robust to sensor mounting variations (within some tolerances) and can be quickly customized for different vehicle types via efficient model fine-tuning.
\end{abstract}
% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Precise and comprehensive understanding of 3D surrounding environments is a critical task for autonomous vehicles. In early days, monocular camera perception module is firstly run for each camera individually, then the outputs from multiple cameras are fused to produce a single set of unique 3D signals. One of the challenges of this approach is that prediction errors (e.g., under or over estimation) caused by independent monocular perception modules are different and their error/noise models are usually unknown. As a result, the association/fusion task becomes nontrivial, often leading to many false positive detections. Nonetheless, such camera independence is desirable for product safety in that it can avoid a certain level of common cause failures.

Production scalability is another crucial factor. Car manufacturers often have many different car lines such as SUV, sedan, low-profile sport car, and pick-up truck. Their sizes are all different and so are the camera mounting positions. Therefore, a camera perception system tolerant to different camera mounting angles, positions, and different radial distortions, focal lengths is a must for scalability. Last, but not the least, real-time capability in low powered, shared compute budget in system on chip (SoC) is required because other redundant but independent radar or ultrasonic perception module can run concurrently.

\textbf{Design principles:} We aim to design a perception network with the following principles:
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{Accurate 3D perception}. The projections of 3D predictions to image views well align with 2D image content.
	\item \textbf{Far range perception}. The network provides long-range object detection (upto 200 meter range).
	\item \textbf{Efficient}. The network runs real-time on edge devices, e.g., Nvidia Orin SoC.
	\item \textbf{Camera independence}. The network must be tolerant to camera dropouts.
	\item \textbf{End-to-end data-driven machine learning}. No post-processing is required. Performance is scaled with data.
	\item \textbf{Scalability}. The network can be easily and quickly customized for different vehicle platforms. 
\end{itemize}

\textbf{Key components and features:} With the above design principles, we develop a camera perception network, termed NVAutoNet, with the following key components and features. (1) The image feature extractor networks are well-customized using hardware-aware neural architecture search (NAS) to have high accuracy and low latency. (2) The multi-camera fusion is done at the Bird's-eye-view (BEV) feature level which combines the best of early and late fusion approaches. (3) The perspective to BEV view transformation is done using column-wise MultiLayer Perceptron (MLP) layers which is very efficient. (4) All perception tasks including freespace perception are formulated as set predictions tasks so that expensive and adhoc post-processing such as clustering, boundary extraction, curve fitting can be avoided. Figure \ref{fig:nvautonet} shows an overview of NVAutoNet and Table \ref{tab:nvautonet_highlights} summarizes key NVAutoNet's features.


The remainder of the paper is organized as follows. Section 2 discusses related work. Sections 3 explains NVAutoNet with details. Section 4 provides extensive experimental results. Lastly, section 5 concludes the paper and discusses a couple of future directions for autonomous driving (AV) perception.

\section{Related Work}
To the best of our knowledge, there was no previously published work that has achieved all the goals listed above. In this section, we summarize methods which are related to one or more components of our work.

\textbf{Multi-camera Fusion and BEV Perception.} While monocular camera perception tasks have been well established in computer vision, multi-camera perception has only attracted much of interests recently \cite{Saha2021,Saha2022, Wang2021, Can2021StructuredBT,Reiher2020,Philion2020,Ng2020BEVSegBE}. Modern methods, including ours, have moved from a late fusion to a mid-level fusion approach, where information from different cameras are fused at the feature level. To that end, a shared representation independent of cameras is needed. A common choice is the BEV representation as it offers a physically interpretable way to fuse information from multiple sensors and multiple timestamps. The BEV representation can be also utilized directly by downstream consumers such as prediction, planning and control.

\textbf{Perspective to 3D/BEV View Transformation.} Transforming perspective (image) view to 3D/BEV view is an ill-posed problem. Existing methods can be grouped into four categories: homography based, depth based, MLP based and attention based approaches. Homography based methods (e.g., \cite{Reiher2020}) often assume a flat-world model to transform pixels from a perspective view to the BEV view. The flat-world assumption is hardly valid in the autonomous driving applications. Depth based methods (e.g., \cite{Schulter2018, Philion2020, Reading_2021_CVPR}) require per-pixel (absolute or probabilistic) depth information for view transformation. There are two issues with the depth-based approach. First, inaccurate predicted depth will be harmful for 3d uplifting, thus potentially leading to low-quality 3D feature maps. Second is the inefficiency due to the high-dimensional 3D voxel feature map resulted from per-pixel uplifting, which makes these methods less favorable for far-range detection. MLP based methods (e.g., \cite{Chitta2021NEATNA, Yang2021, Mani2020MonoLO, Pan2020CrossViewSS, Hendy2020FISHINGNF}) is a popular choice for view transformation thanks to a strong capability of MLP of transforming data from one space to another space. While simple, these MLP based methods are camera-dependent, completely ignore geometric priors such as camera intrinsic and extrinsic, thus can not be applied to sensor configurations different from the ones used for training. These methods are also very computationally expensive because these methods often stretch the 2D feature map into 1D vector, then perform a fully connected operation on it. Recent works \cite{Roddick2020PredictingSM,Saha2021,Saha2022} improve the computational hurdle by applying MLP operations on image columns independently. In contrast to the MLP based methods, where the view transformation is done in a forward manner (i.e., 2D to 3D), attentions based methods (e.g., \cite{Saha2022, Can2021StructuredBT,Wang2021}) employs a backward manner (i.e., 3D to 2D). More specifically, 3D or BEV queries are constructed and cross-attended to image features to build 3D or BEV features. While achieving impressive performance, these attentions based methods are often too expensive for cost-effective autonomous driving systems, especially when dense queries are used. A more comprehensive review of view transformation methods can be found in \cite{ma2023visioncentric}.

To achieve a real-time perception system for self-driving cars, our 2D-to-BEV view transformation method adopts a MLP approach, but individual image columns are `uplifted` independently similar to \cite{Roddick2020PredictingSM,Saha2021,Saha2022}. Compared to \cite{Roddick2020PredictingSM,Saha2021,Saha2022}, our method has several differences and improvements. First, our method does not assume that each image column corresponds to a BEV ray, which is often invalid for wide-field-of-view cameras such as fish-eye cameras. Second, we extend these methods to fuse BEV features from multiple cameras using camera intrinsic and extrinsic parameters. Lastly, our method does not use attentions based layers for high-efficiency. Instead small MLP layers are used.

%\subsection{Perception for Autonomous Driving}
\textbf{Perception for Autonomous Driving.} For autonomous driving, the perception module needs to detect and recognize all static and dynamic objects from the surrounding environment. The list includes obstacles, road markings, lanes, road boundaries, traffic lights, traffic signs, parking spaces, free spaces and many more. Below we review object detection and freespace detection methods which have attracted more attention and interest from the literature.

%\subsubsection{3D Object Detection}
\textbf{3D Object Detection.} 3D object detection using 2D images is challenging due to the lack of depth information. Prior methods often relied on external sub-networks, e.g., depth estimation \cite{wang2019pseudo, ma2019color}, to assist 3D detection. Recently, end-to-end methods have become more popular \cite{roddick2018orthographic,smoke_2020,Wang2021,Wang2021FCOS3DFC}. Unlike methods like FCOS3D \cite{Wang2021FCOS3DFC} which detect 3D objects on the single-view image space, modern methods \cite{ Wang2021, Li2022BEVDepthAO, LiuWZS22} detect 3D objects directly on the BEV space from surround-view cameras. Our 3D detection approach follows the same trend, there are, however, a number of differences and improvements. First, our network is fully convolutional and light-weight, thus  much more efficient for both training and inference than previous methods based on Transformers architectures. Second, our network not only predicts object parameters, but also returns uncertainty for those parameters. These uncertainty estimates are very crucial for downstream consumers. Third, our method detects full object orientation instead of yaw angles only. And lastly, similar to PolarFormer \cite{jiang2023polarformer} and PolarDERT \cite{chen2022polar}, we adopt a BEV Polar representation to enable far range detection (e.g., upto 200 meters) without much additional computation cost. 

%\subsubsection{Free space detection}
\textbf{Free space detection.} Most existing methods such as \cite{Zidong_BMVC2021,BMVC2015_109,Sanberg2016,Scheck2020} focus on detecting 2D freespace from monocular cameras. The method in \cite{Zidong_BMVC2021} operates on a stitched BEV image. Our method consumes multiple-view inputs, and outputs top-view freespace region as a polygon instead of pixel segmentation, thus does not require expensive post-processing such as pixel clustering.

\section{NVAutoNet}
\subsection{Overview}
The inputs to NVAutoNet is a collections of $N_{view}$ camera images $\{I_i\}_{i=1}^{N_{view}}$, capturing a 360-degree field of view of the surrounding environment, together with camera intrinsic and extrinsic parameters. The input images are passed through 2D image encoders to extract 2D image features, which are then uplifted to BEV features. The camera specific BEV features from multiple images are fused into a single BEV feature map, before being passed to a common BEV encoder to extract high-level BEV features. Finally, the encoded BEV feature map will be input to a series of task encoders to output 3D signals such as obstacles, parking spaces, free spaces, etc. Figure \ref{fig:nvautonet} displays the overview of NVAutoNet.

\subsection{2D Image Feature Extractors}
Each input image $\mathbf{I}$ of dimensions $W\times  H \times 3$ is fed to a feature extractor to compute a set of proportionally sized feature maps at multiple levels $\{F^k\}$ where each $F^k$ is of shape $C \times \frac{H}{2^{k+1}} \times \frac{W}{2^{k+1}}$. Our image feature extractors are underpinned by convolutional architectures which have been carefully customized to have real time performance. A CNN backbone composes of a series of CNN blocks. Each CNN block requires kernel size, stride, number of channels and repeat parameters. Table \ref{table:cam_encoders} describes the network configurations for different camera groups. These parameters are optimized via hardware-aware neural architectural search \cite{wang2022gpunet} to have a good trade-off between latency and accuracy. Residual connections are removed for higher latency. Coarser feature maps are up-sampled and merged with finer feature maps to form multi-level semantically rich feature maps. It's worth noting that that input images from different cameras need not to be the same size, and different input images can share the same feature extractor. For example, all images from fisheye cameras share the same 2D image encoder. Next we will detail our approach to transform these 2D image feature maps to BEV.
\begin{table}[t]
	\begin{tabular}{llllllll}
	\hline
	& Input size & Blocks & Kernel sizes & Strides & Repeats & Channels \\
	\hline
        Front cam encoder & 480x960 & 5  &  7-3-3-3-3 & 4-1-2-2-2 & 1-0-2-5-3 & 32-32-128-256-512\\
        Side cam encoder  & 480x960 & 5  &  7-3-3-3-3 & 4-1-2-2-2 & 1-0-2-3-3 & 32-32-128-192-512\\
        Fisheye cam encoder & 480x960 & 5  &  7-3-3-3-3 & 4-1-2-2-2 & 1-0-2-3-3 & 32-32-64-96-512\\
        BEV encoder & 64x360 & 3  &  3-3-3 & 1-2-2 & 4-4-4 & 64-128-256\\
	\hline
	\end{tabular}
	\centering
	\caption{Lightweight camera and BEV CNN backbones. All camera backbones have 5 CNN blocks while BEV backbone has only 3 CNN blocks. In all convolution layers, 3x3 kernels are used, except the first convolution layers in the camera backbones where 7x7 kernels are used.}
	\label{table:cam_encoders}
\end{table}

\subsection{Image-to-BEV Transformation and Fusion}
\subsubsection{BEV Plane and BEV Grid}
The BEV representation is commonly used in self-driving due to its efficiency to represent object positions and sizes, and its suitability for behavior prediction and planing. The BEV plane passes through the vehicle's rig center and is orthogonal to the Z axis. We discretize the BEV plane using a BEV grid $\mathbf{G}^{bev}$ of dimensions $M \times N$. Each cell in the grid $\mathbf{G}^{bev}$ corresponds to a region in the real-world space, and the ego vehicle's position is at the BEV grid's center.

\subsubsection{2D to BEV View Transformation}
Given a set of 2D image feature maps from different cameras, we want to compute a single BEV feature map $F_{bev}$. In this work, we take a data-driven approach in which we train a camera-to-bev transformation function using a multilayer perceptron (MLP) network. Unlike the previous MLP based view transformation methods, our method explicitly consumes camera intrinsic and extrinsic parameters during training and inference, therefore the model, once trained, will be able to generalize well to different sensor configurations. Figure \ref{fig:bev_transformer} displays an overview of our image to BEV view transformation module.
\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{figures/bev_points.png} \hspace{1cm}
\includegraphics[width=0.45\textwidth]{figures/bev_view_transformation.png}
\caption{\label{fig:bev_transformer} An overview of perspective to BEV view transformation. Left: Camera pixels are projected onto the BEV plane using camera intrinsic and extrinsic parameters. The resulting polar BEV points are then used to fit polynomial functions (one for each image column). These polynomial functions accept BEV radial distances as inputs and output corresponding BEV angular positions. Right: Image features are transformed into pseudo BEV features which are then transformed to the BEV features using BEV indices. The BEV indices are pre-computed using the fitted polynomial functions and a pre-defined BEV grid.} 
\end{figure}

A common observation is that there is a strong geometric relationship between row and column positions in the image plane and radial and angular positions in the BEV plane. In particular, each image column will correspond to a \textbf{curve} passing through the camera center on the BEV plane (see Figure \ref{fig:bev_transformer} left). If camera images are rectified, these curves become polar BEV rays as utilized in the previous works \cite{Saha2022, Saha2021}. Our goal is to learn a function to transform each image column features from the image plane to the BEV plane.

Formally, let $\mathbf{I}^c$ be a column from an image $\mathbf{I}$ (for notation simplicity we dropped the camera index), we first project every pixel location $\mathbf{p}_i=[u_i, v_i] \in \mathbf{I}^c$ onto the BEV plane (using camera intrinsic and extrinsic parameters), and convert them to polar coordinates (angles and distances), resulting in a list of polar BEV points $\{\mathbf{b}_i = [a_i, d_i]\}$. Here it is worth noting that we are not merely projecting pixels to the BEV plane for view transformation as we do not assume the world is flat. Instead, these polar BEV points are used to fit a polynomial curve $a=f^c(d)$, which will be used to infer a BEV angular position given a BEV radial position. Figure \ref{fig:bev_transformer} (left) shows an examples of projected BEV points and fitted polynomial curves. Intuitively, those fitted curves encode camera intrinsic and extrinsic information. 

Let $r$ (meters) be the maximum detection range for this particular camera, we discretize the range $[0, r]$ into $D$ (logarithmically spaced) bins. Let $\mathbf{c} \in \mathbb{R}^{H \times C}$ represent the encoded features corresponding to the image column $\mathbf{I}^c$ (again we dropped the feature level for notation simplicity), we define a BEV transformation network $\mathcal{B}(.)$ s.t. $\mathbf{b} = \mathcal{B}(\mathbf{c}), \mathbf{b} \in \mathbb{R}^{D \times C}$. At this point, the features $\mathbf{b}$ are not yet real BEV features. Instead they are pseudo BEV features which need to be aligned and aggregated into the global BEV feature map $F_{bev}$.

Observe that the features $\mathbf{b}$ has $D$ feature vectors, each is associated with a radial distance bin. As a result, for each cell in $\mathbf{b}$, its location in the BEV plane can be computed using its associated distance (i.e., center of the distance bin) and the fitted polynomial function $f^c$. Other cells in $\mathbf{b}$ are projected into the BEV plane similarly. The same process is applied to all other image columns and other images to transform image features from the image plane to the BEV plane. To reduce the computation cost, for each image, we transform only one feature map level at stride 8 to the BEV plane.

\textbf{BEV Pooling.} Given a pre-defined BEV grid $\mathbf{G}^{bev}$, we compute a final BEV feature map $F_{bev}$ by aggregating all the BEV feature points computed above. In particular, all the BEV feature points belonging to the same BEV grid cell are simply summed up.\\
\textbf{BEV Indexing.} Assuming that camera calibration parameters are accurate and we can treat them as constant parameters. We could pre-compute BEV indices to map pseudo BEV features to the BEV grid cells. With this BEV look-up-table, the whole image to BEV transformation operation is super efficient for both training and inference (i.e., only costs 0.3ms per each image). 

\subsubsection{BEV Transformation using MLP}
The BEV transformation function $\mathcal{B}(.)$ is modeled using an MLP block with only one hidden layer, and the MLP parameters are not shared by different image columns. Unlike convolutional layers, MLP layers have a capability to encode global contextual information (aka global attention), which we found very crucial for assigning image features into correct BEV positions as depth information has been lost and objects appears at different heights. %Note that there is no explicit supervision for the BEV features, which are instead implicitly supervised by the downstream perception losses.

\subsubsection{Polar vs Cartesian Coordinates} The resolution of the BEV grid $\mathbf{G}^{bev}$ plays an important role in detection range and detection accuracy. For autonomous driving on highways, the detection range should be up to 200 meters for driving safety and comfortableness. A regular Cartesian grid with 0.25 meter per cell results in a $1600 \times 1600$ grid, which will be both memory intensive and computational expensive, prohibiting us training deep neural networks, and deploying them on the on-vehicle chips such as NVIDIA Orin. In practice, it is more desired to have a higher resolution for close range and a coarser resolution for far range. Therefore, we design an irregular BEV grid using Polar coordinates, in which $M$ is the number of angular samples ranging from 0 to 360, and $N$ is the number of depth samples. For example, with 1 degree angular resolution and logarithmic radial distance spacing, we can represent the BEV plane efficiently using a small $360 \times 64$ grid.

\subsection{BEV Feature Extractor}
Similar to 2D image feature extractors, we adopt a CNN backbone to extract high-level features from the fused BEV feature map. See Table \ref{table:cam_encoders} for details. The output of the BEV feature extractor, dubbed $\hat{F}_{bev}$, will be consumed by different 3D detection heads.

\section{Perception Tasks}
\subsection{3D Object Detection}
3D object detection is a key capability for autonomous driving. The goal is to localize, classify and estimate dimensions and orientations of objects in 3D space. Each object is represented by its category and 3D cuboid. In particular, each 3D cuboid has 9 degree-of-freedom (DOF) representing position, dimension, and orientation. In this work, we adopt a set prediction approach to remove the need for a non-maximum suppression (NMS) post-processing. 

The 3D object detection network includes five lightweight heads (implemented by a couple of convolutional layers), which takes the bottleneck feature map $\hat{F}_{bev}$ as input and predicts object class distributions and 3D cuboid parameters. Formally, let us denote $C \times M \times N$ be the dimension of $\hat{F}_{bev}$, where $M \times N$ is the spatial dimension and $C$ is the number of channels, the 3D detection network will output $\hat{K} = M \times N$ objects --- one object per grid cell. The model employs one head to predict the object classification scores, and three other heads for 3D cuboid parameters (position, dimensions and orientation) regression. There is an additional head for predicting uncertainty of cuboid parameters.\\
\textbf{Classification.} For $k$ number of objects, the network outputs $k+1$ classification channels, where the first channel represents object existence and the other $k$ channels represent a categorical distribution over $k$ classes. \\
\textbf{Position.} The network predicts a tuple $[r, a, e]$, where $r$ is radial distance, $a$ is azimuth angle, and $e$ is elevation. Note that the network actually predicts radial and angular offset values, which are then added to grid cell positions to form final radial and angular positions.\\
\textbf{Dimensions.} The network predicts three scalars $[d_x, d_y, d_z]$ which are absolute values in meters. \\
\textbf{Orientation.} We represent object orientation using a full rotation matrix $\mathbf{R} \in \mathbb{R}^{3 \times 3}$ , as opposed to previous works only estimating yaw angle. However, rotation matrix prediction is nontrivial because not every $3 \times 3$ matrix is a valid rotation matrix. Here we propose to train the network to predict sine and cosine values of yaw ($\psi$), pitch ($\theta$), and roll ($\phi$) angles respectively, which are later used to construct a rotation matrix by applying matrix multiplication of three axis rotation matrices together.
\begin{equation}
R = R_{z}(\psi)R_{y}(\theta)R_{x}(\phi),
\end{equation}
\begin{equation}
\begin{aligned}
R_{z} = \begin{bmatrix}
\cos \psi & - \sin \psi & 0\\
\sin \psi & \cos \psi & 0\\
0 & 0 & 1
\end{bmatrix},
R_{y}(\theta) = \begin{bmatrix}
\cos \theta & 0 & \sin \theta\\
0 & 1 & 0\\
-\sin \theta & 0 & \cos \theta
\end{bmatrix},
R_{x}(\phi)=\begin{bmatrix}
1 & 0 & 0\\
0 & \cos \phi & - \sin \phi\\
0 & \sin \phi & \cos \phi
\end{bmatrix}.
\end{aligned}
\end{equation}

\textbf{Training Losses.} For each scene, let $\mathcal{G} = \{\mathbf{g}_i\}_{i=1}^K$
be  a set of $K$ ground-truth objects, and let $\mathcal{P} =\{\mathbf{d}_i\}_{i=1}^{\hat{K}}$ be a set of $\hat{K}$ predicted objects. The training loss is computed in a two-step fashion where in the first step, we found the best one-to-one matching between $\mathcal{G}$ and $\mathcal{P}$. In the second step, we compute the final loss based on the matching result. Although the idea sounds very simple, the training process will be highly affected by the matching quality --- if the matching algorithm returns wrong or sub-optimal matches, the network training won't be successful. 

To ensure high quality matches, we constructed a matching cost function considering classification, position, dimension, and orientation costs between each candidate-ground truth pair. In the early phase of training, however the network prediction is quite noisy, often leading to bad assignments. This issue can be mitigated by setting a higher weight for the position cost. Alternatively, inspired by \cite{peize2020onenet}, we proposed to limit the matching candidates for each ground truth object by its corresponding coverage on the BEV grid. This will prohibit matching pairs in which the ground truth object and the candidate are far apart. Moreover, the matching optimization problem is now simplified,  we adopt a greedy matching algorithm instead of the well-known Hungarian algorithm without losing accuracy, but more efficient.

Let $\mathcal{P}_{pos}$ be the set of positive object candidates matched to the ground truth objects, and $\mathcal{P}_{neg} = \mathcal{P} \setminus \mathcal{P}_{pos}$ be a set of negative objects, the training loss is as below:
\begin{equation}
\begin{aligned}
L_{obs}(\mathcal{G}, \mathcal{P}) = L^{pos}_{obs}(\mathcal{G}, \mathcal{P}_{pos}) + L^{neg}_{obs}(\mathcal{P}_{neg}) \\ = \sum_{\mathbf{d}_i \in \mathcal{P}_{pos}} L(\mathbf{g}_i, \mathbf{d}_i) + \sum_{\mathbf{d}_i \in \mathcal{P}_{neg}}L(\mathbf{d}_i),
\end{aligned}
\end{equation}
where $L(\mathbf{g}_i, \mathbf{d}_i)$, and  $L(\mathbf{d}_i)$ are loss functions per each predicted candidate. % $w_{\mathbf{g}_i}$ is a weighting parameter pre-computed for each ground-truth object $\mathbf{g}_i$. In this work, $w_{\mathbf{g}_i}$ is simply equal to the number of views in which the object $\mathbf{g}_i$ appear. Effectively, objects appearing in multiple views are expected to have higher detection accuracy.
If $\mathbf{d}_i$ is a negative candidate,  the loss function $L(\mathbf{d}_i)$ is simply a (focal) binary cross entropy loss --- pulling its objectness score to zero. If $\mathbf{d}_i$ is a positive candidate, the loss function $L(\mathbf{g}_i, \mathbf{d}_i)$ composes of classification and regression losses. While the classification loss is still a (focal) cross-entropy loss, the regression loss is more complex as explained below.

%Note that each ground-truth and prediction 3D cuboid has 15 parameters (3 for position, 3 for dimensions, and 9 for orientation), 
To regress 3D cuboid parameters, ideally we should compute a global loss such as intersection-over-union (IoU) score which considers all parameters together as there is a strong correlation between cuboid parameters. However there is no closed-form solution to compute IoU between two 3D cuboids. Here, we propose to decompose the 3D cuboid regression loss into position (location) loss, shape (size) loss and orientation (rotation) loss, as below:
\begin{equation}\label{eq:obs_reg}
L^{reg}(\mathbf{g}_i, \mathbf{d}_i) = L^{loc}(\mathbf{g}_i, \mathbf{d}_i) + L^{size}(\mathbf{g}_i, \mathbf{d}_i)  + L^{rot}(\mathbf{g}_i, \mathbf{d}_i) 
\end{equation}
\begin{equation}
\begin{aligned}
L^{loc}(\mathbf{g}_i, \mathbf{d}_i) =\frac{| \mathbf{g}_i^r -  \mathbf{d}_i^r|}{\sigma_r} + \frac{| \mathbf{g}_i^a -  \mathbf{d}_i^a|}{\sigma_a} + \frac{| \mathbf{g}_i^e -  \mathbf{d}_i^e|}{\sigma_e} \\+ \log(2\sigma_r) + \log(2\sigma_a) + \log(2\sigma_e)
\end{aligned}
\end{equation}
\begin{equation}
L^{size}(\mathbf{g}_i, \mathbf{d}_i) =\frac{1}{\sigma_s} (1 - \prod_{d \in \{d_x, d_y, d_z\}} \frac{\min (\mathbf{g}_i^{d}, \mathbf{d}_i^{d})}{\max (\mathbf{g}_i^{d}, \mathbf{d}_i^{d})}) + \log(2\sigma^s)
\end{equation}
\begin{equation}
L^{rot}(\mathbf{g}_i, \mathbf{d}_i) = \frac{1}{\sigma_o} \sum_{r \in R} | \mathbf{g}_i^{r} - \mathbf{d}_i^{r}| + \log(2\sigma^o)
\end{equation} 
where $\sigma_r$,$\sigma_a$, $\sigma_e$, $\sigma_s$, $\sigma_o$ are  uncertainty values for position, shape and orientation respectively. These values are predicted by the network.

\subsection{3D Freespace} 
3D obstacle detection generally covers category-wise classifiable vehicles and vulnerable road users (VRU). In a driving scenario, there is a lot more information which is relevant for safe driving beyond the predictions of 3D obstacle detection. For example, there can be random hazard obstacles like tyres, traffic cones lying on the road. Additionally, there are a lot of static obstacles like road-divider, road-side curb, and guard rails which are not covered by 3D obstacle detection. An autonomous vehicle has to drive safely within the boundaries of the road by avoiding all kinds of obstacles. The region within the boundaries of the road which is not occupied by any obstacle could be carved out as a driveable region. The driveable region is used interchangeably as the freespace region. The down-stream behaviour planner would consume the freespace region information to plan a safe trajectory for the AV. So it is essential to have a perception component which predicts this freespace region. The freespace region is represented as a radial distance map (RDM). The representation is explained in more details in the next section.

\textbf{Radial Distance Map.} While polygons are used for labeling of 3D freespace, we use the RDM representation due to its higher efficiency. RDM is composed of equiangular bins and radial distance values for each angular bin to denote spatial locations. For autonomous driving applications, the distance to the closest freespace boundary is the most important one and thus we use a single scalar for each angular bin to represent the closest freespace boundary. In order to create ground-truth RDM for 3D freespace, we simply shoot a ray from the center of BEV plane at each angular bin direction and compute the intersection between the ray and the ground-truth polygon. The most important benefit of using RDM to represent 3D freespace is it can be directly used to create 3D boundary points without additional post-processing. 
In addition to the radial distance, we also have, for each angular bin, boundary semantic labels such as vehicle, VRU and others. 
Each freespace ground truth label becomes $(\mathbf{r}, \mathbf{c})$, where $\mathbf{r}$ is a radial distance vector, and $\mathbf{c}$ is a boundary semantic vector.

\textbf{Model.} The 3D freespace detection network consists of a shared neck and two separate heads. The shared neck includes a couple of convolutional layers on top of the bottleneck feature map $\hat{F}_{bev}$. It is later extended into two heads which predict radial distance and classification maps.

\textbf{Training Losses.} For each scene, let $\mathcal{G} = (\mathbf{r}, \mathbf{c})$ be the ground-truth, let $\mathcal{P} = (\hat{\mathbf{r}}, \hat{\mathbf{c}})$ be the prediction.
%Here, ˆd={^di}i=1,…,N\hat{d} = \{\hat{d_i}\}_{i=1,\ldots,N} is the prediction RDM from the radius head and d={di}i=1,…,Nd = \{d_i\}_{i=1,\ldots,N} is the label RDM. Assuming we do KK-class classification in each of the angular bins, classification label is defined as c={ci}i=1,…,Nc = \{c_i\}_{i=1,\ldots,N} where ci∈[0,K)c_i \in [0, K) is the corresponding label class index. If pijp_{ij} is the probability from the classification head corresponding to ithi^{th} angular bin and the jthj^{th} class, the classification prediction is defined as ˆc={{pij}j=1,…,K}i=1,…,N\hat{c} = \{\{{p_{ij}\}_{j=1,\ldots,K}}\}_{i=1,\ldots,N}.
 The overall loss function for the 3D freespace detection task is defined as:
\begin{equation}
L_{fsp}(\mathcal{G}, \mathcal{P}) = L^{reg}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) +  L^{cls}_{fsp}(\hat{\mathbf{c}}, \mathbf{c})
\end{equation}
\begin{equation}
L^{reg}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) = L^{iou}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) + L^{sim}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})
\end{equation}
where $L^{reg}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})$ is the regression loss which is a combination of radius loss $L^{iou}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})$ and the similarity loss $L^{sim}(\hat{\mathbf{r}}, \mathbf{r})$; $L^{cls}_{fsp}(\hat{\mathbf{c}}, \mathbf{c})$ is the classification loss.
The radius loss is a polar intersection-over-union loss (IoU) computed between the prediction and ground truth labels, defined as below:
\begin{equation}
L^{iou}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})  = 1.0 - \prod_{i=1}^{N_{bins}} \frac{\min(r_i, \hat{r_i})}{\max(r_i, \hat{r_i})}.
\end{equation}
The similarity loss is computed between the line segments formed by joining the end points from the consecutive angular bins in the prediction and ground truth labels. This loss helps in reducing the noise in the predicted RDM.
\begin{equation}
L^{sim}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) = \sum_{i=1}^{N_{bins}} (1.0 - \frac{{\hat{l}}_{i}^{i+1} \cdot {l}_{i}^{i+1}}{\left\lVert {\hat{l}}_{i}^{i+1} \right\rVert \left\lVert {l}_{i}^{i+1} \right\rVert}),
\end{equation}
where ${\hat{l}}_{i}^{i+1}$ is the line segment formed by joining the end points of $\hat{r}_{i}$ and $\hat{r}_{i+1}$. ${{l}}_{i}^{i+1}$ is defined similarly.
Finally, the classification loss $L^{cls}_{fsp}(\hat{\mathbf{c}}, \mathbf{c})$ is the standard focal loss, i.e., 
\begin{equation}
L^{clc}_{fsp}(\hat{\mathbf{c}}, \mathbf{c}) = \sum_{i=1}^{N_{bins}} \sum_{j=1}^{C} (1 - p_{ij})^{\gamma} \log(p_{ij}),
\end{equation}
where $\gamma$ is the focal loss parameter.
\subsection{3D Parking Space}
Another important aspect of autonomous driving is the ability to localize and classify parking spaces. For simplicity, we assume a flat-world model wherein the ground plane is at a height of zero with respect to the ego-car. Therefore, each parking space is represented as an oriented rectangle, parameterized by $[cx, cy, l, w, \theta]$, where $cx$ and $cy$ are the center coordinates of the box, $l$ and $w$ are the length and the width of the box in meters respectively, and $\theta$ is the orientation of the box (yaw angle in radians) in the range $[0, \pi)$. Note that an oriented box angled at $\pi$ visually appears the same as a box oriented at 0, thus the orientation value $\theta$ need not cover the entire angular range of $[0, 2\pi)$.
Knowing the \textit{profile} of every parking space is important for planning and control purposes. As such, every prediction output by our model will be assigned a parking profile. In the current system we support three different parking profiles: \textit{angled, parallel and perpendicular}. As their name suggests, the profiles denote the types of planning and control maneuvering required to successfully park the car in the parking space. \textit{Parallel} parking spaces are ones that typically appear on the side of the street and require a parallel parking maneuver. Conversely, \textit{angled} and \textit{perpendicular} parking spots are ones where the car can be parked straight in (or backed in). 

\textbf{Model.} The parking space detection task follows the same design (head, training strategy and losses) of the obstacle detection task. The parking detection network consists of classification and regression heads. The classification head predicts per-profile confidence scores. We rely on each parking spot's profile to implicitly encode its existence score. The regression head predicts the parking space oriented bounding boxes as discussed above.

\textbf{Training Losses.} The training loss is similar to that of the obstacle detection task, but its regression loss function is much simpler. Let $\mathbf{g}_i$ and $\mathbf{d}_i$ be a matched pair of ground truth and detection, the regression loss is defined as:
\begin{align}
    \loss^{reg}_{prk}(\mathbf{g}_i, \mathbf{d}_i) = \sum_{s \in \{cx, cy, l, w, \theta\}} ({\mathbf{g}_i^{s} - \mathbf{d}_i^{s}})^2.
\end{align}

\section{Multi-task Learning and Loss Balancing}\label{sec:loss_balancing}
NVAutoNet is a multi-task network which learns multiple tasks at the same time. These tasks are inherently different in nature and the same training recipe is not necessarily optimal for all of them. In addition, different task losses have different landscapes, in which case linear combination of various loss components with equal weights leads to under representation of certain tasks or a subset of tasks dominating the total loss. Let $T$ be the number of tasks, the total training loss, for a given batch $b$, is $L_{total} = \sum_{t=1}^{T} w_t L_{t}^b$, where $w_t$ is a loss weight assigned to task $t$, and $L_t^b$ is the loss for task $t$ given a training batch $b$. Finding the optimal set of weighting parameters $\{w_t\}_{t=1}^{T}$ is a challenging task. 
Here we develop a simple, but effective weighting algorithm to dynamically update the loss weights $\{w_t\}_{t=1}^{T}$ that are assigned to each loss component. 
Given a dataset with $S$ samples, let $L_{t,s}$ be the loss of task $t$ evaluated on training sample $s$. Note that not all training samples in the dataset necessarily contain ground-truth labels for all the tasks. The task loss for such cases will be set to 0 (its gradient is ignored during back-propagation). At every epoch, we calculate a loss sum, for every task $t$, over all the training samples, and the loss weight $w_t$ is an inverse of the loss sum scaled by a task loss prior $c_t$, i.e., 
\begin{equation}
L_{t} = \sum_{s=1}^{S} L_{t,s}, \quad w_t = \frac{c_t}{L_t}, \quad w_t = \frac{w_t}{\sum_{t=1}^{T} w_t},
\end{equation}
where $c_t$ is a configurable loss multiplier for task $t$ that can be used to boost or reduce a task's loss. Intuitively, scaling losses by their inverses of loss sums will help making all the losses to be in the similar scale, thus reducing the gaps between gradient magnitudes. And the loss multiplier $\{c_t\}$ will be helpful when certain tasks are more important than others or harder to learn. For the first epoch we set all the task loss weights $\{w_t\}_{t=1}^{T}$ manually (e.g., $w_t =1$ for all $t \in [1, T]$) since we don't have the loss sum statistics from the previous epoch, and we update $w_t$ is after each epoch. 

In practice, we carry out a two-stage approach where in the first training trial, all $\{c_t\}$ are set to 1. We then compare the multi-task results versus the single task results, and infer $\{c_t\}$ accordingly based on task KPI improvements or regressions. With the found $\{c_t\}$, we retrain the network in the subsequent training trials. Although $\{c_t\}$ are tuned manually, we found that searching for $\{c_t\}$ is much easier than searching for $\{w_t\}$ directly.


\section{Experimental Evaluation}
In this section, we demonstrate latency and accuracy performance of NVAutoNet. Ideally, we should compare NVAutoNet against the state-of-the-art methods. Unfortunately, we found a number of difficulties. First, we could not find any methods (and datasets) which jointly detect obstacles, freespaces and parking spaces like NVAutoNet. Second, most existing BEV perception methods (e.g., 3D object detection) focus primarily on pushing their accuracy, unlike NVAutoNet being optimized for both accuracy and latency. Third, NVAutoNet is designed for real self-driving application, where the required detection range is up to 200 meters. Many of our design choices (such as image to BEV view transformation, polar BEV representation) become disadvantageous when testing on public datasets such as nuScenes where only short range ($<70$ meters) is considered. Therefore, we mainly evaluate NVAuoNet on our in-house dataset and NVIDIA Orin platform.
\subsection{Datasets and Evaluation Metrics}
\subsubsection{Datasets}\label{sec:datasets}
Our in-house datasets consist of real, simulated reality and augmented reality data. In total, there are 2.2M training scenes, 400K validation scenes and 177K testing scenes. Table \ref{table:datasets} summarizes our datasets. Lidar data was used to generate ground truth labels. Our data contains a fair amount of noisy labels due to view point differences between Lidar and camera sensors. For example, Lidar is mounted at a higher position than cameras, thus there are obstacles, which may be visible by Lidar, are hardly visible by cameras. These issue not only affect model training but also model evaluation (e.g., low recall rates).
\begin{table}
	\begin{tabular}{lcc}
	\hline
	& Amount & Details\\
	\hline
	Number of cameras per scene & 8 & 2 front, 2 rear, and 4 surround fisheye cameras\\
        \hline
        Real training samples & 2M \\
        Sim training samples & 200k \\
        Validation samples & 400K \\
        Test samples & 177K \\
        \hline
        Number of countries & 20 & US, EU and New Zealand \\
        \hline
        Number of obstacle classes & 5 \\
        Number of freespace classes & 3 \\
        Number of parking classes & 3 \\
        \hline
        Percentage of dry roads & 95.17$\%$ \\
        Percentage of wet roads & 4.83$\%$  & rain, fog, snow, ice\\
        \hline
        Bright light condition & 49.5$\%$  & \\
        Diffused light condition & 31.4$\%$  & \\
        Poor light condition & 19.1$\%$  & \\
	\hline
	\end{tabular}
	\centering
	\caption{In-house dataset summary.}
	\label{table:datasets}
\end{table}
\begin{table}[h!]
	\begin{tabular}{llccl}
	\hline
	Components & Latency & Multiplier &Total\\
	\hline
	Front Cam Encoder  &  1.80  & 2 & 3.61\\
	Side Cam Encoder   &  1.60  & 2 & 3.21\\
    Fisheye Cam Encoder&  1.39  & 4 & 5.58\\
    3D Uplifting + Fusion   &  0.30      & 8 & 2.40 \\
    3D Encoder + Heads &  3.91  & 1 & 3.91\\
        &   &  &  18.72 \\
	\hline
	\end{tabular}
	\centering
	\caption{NVAutoNet latency (ms) measured on NVIDIA Orin embedded GPU. The model runs at 53 FPS.}
	\label{table:latency}
\end{table}
\subsubsection{Evaluation Metrics}
\textbf{3D Obstacles.} 
We calculate obstacle detection metrics based on identifying true positives (TP), false positives (FP), and false negatives (FN) from detection outputs and ground truth labels. For each class, we find one-to-one matching between detection output and ground truth using greedy algorithm with the Euclidean distance between their centroids. A match is valid if the relative radial distance between a prediction and ground truth objects is less than 10$\%$, and their absolute azimuth error is less than 2 degrees.  
All unmatched detections become FP while all unmatched ground truth becomes FN. Once TP, FP, and FN have been identified, we compute precision, recall, F1-score, AP and mAP KPIs. Moreover, we search for the best confidence threshold that maximizes F1-score, and compute regression errors for all true positive detections. \emph{Position error} measures relative radius error ($\%$), absolute azimuth error (degrees) and absolute elevation error (meter). \emph{Orientation error} is defined as $|| \textrm{log}(R^{-1}\hat{R})||$, where $R$ and $\hat{R}$ are ground truth and prediction rotation matrices. \emph{Shape error} measures relative error for length, width, and height ($\%$). We also define the safety mAP based on a safety zone. The safety zone is defined as a rectangular region around the ego vehicle, i.e., 100 meters ahead and behind the ego vehicle and 10 meters left and right of the vehicle. 

\textbf{3D Freespaces.}
%As freespace regression is represented as radial distance maps, evaluating 3D freespace output accuracy simply computes the errors between ground truth and prediction radial distances for each angular bin. Similarly freespace classification related errors are computed between ground truth and prediction classifications.\\
Given a pair of ground truth and prediction freespace RDMs, we compute the following metrics (averaged over angular bins and frames). \emph{Relative gap} measures the relative radial distance errors ($\%$). 
\emph{Absolute gap} measures the absolute radial distance errors (meters). \emph{Success rate} measures the percentages of successfully estimated angular bins. Angular bins are considered as successfully estimated when the relative gap is less than $10\%$. \emph{Smoothness} measures the total variation of radial distance maps defined as $\sum_{i=1}^{N_{bins}} |r_i - r_{i-1}|$. \emph{Classification error} measures precision and recall for each label class.

\textbf{3D Parking Spaces.} Similar to object detection, we compute precision, recall, F1 and AP metrics.  Intersection over union (IoU) scores are used to match predictions to ground truth labels. A match is valid if the IoU $\ge$ 70\%. This strict criteria is necessary for real-world applications of autonomous parking as small misalignment between the detection and the actual parking space position can lead to imperfect parking. We also compute mean IoU values for all true positive detections.

\subsection{Latency Performance}
We export NVAutoNet using NVIDIA TensorRT and time it on NVIDIA Orin. Table \ref{table:latency} reports the total NVAutoNet latency as well as the breakdown latency numbers for different NVAutoNet components. It can be seen that our network only costs \textbf{18.7 ms} (8 cameras input), leading to a very high frame-rate at 53 FPS. Compared to BEVDET \cite{huang2021bevdet}, one of the most efficient BEV object detection methods, NVAutoNet is 7.5x times faster even though BEVDET's latency was measured on NVIDIA GeForce RTX 3090 GPU, which is much more powerful than NVIDIA Orin SoC. Compared to Fast-BEV \cite{fastbev}, a method that is highly optimized for real-time applications, NVAutoNet is still a lot faster (53 vs 45 FPS). Both NVAutoNet and Fast-BEV are measured on the same NVIDIA Orin SoC. It is also worth mentioning that NVAutoNet handles multiple tasks jointly and its BEV perception range is much higher than those of the competitors (e.g., 200 vs 50 meters).

\subsection{Quantitative Performance}
\subsubsection{3D Obstacles} \label{sec:results_3d_obs}
Table \ref{table:obstacle_map} reports the obstacle detection accuracy results. The overall mAP score is 0.465. The network is best at vehicle detection (with 0.648 AP score), and worst at person/pedestrian detection (with 0.351 AP score). This result makes sense because persons are usually much smaller than vehicles. When considering a safety region only (i.e., 100 meters ahead and behind the ego vehicle and 10 meters left and right of the vehicle), the mAP score increases significantly to 0.595. 

The network estimates orientations of vehicles and trucks reasonably well with average errors less than 7 degrees. However, the orientation errors of bikes and persons on average are very high (12.3 and 53.4 degrees respectively). Table \ref{table:obstacle_f1_range} shows more granular detection results. Generally, the detection accuracy drops considerably with respect to distances.

%It is also important to compare our method against the state-of-the-art (SOTA) methods. For example, BEVFormer \cite{bevformer}, one of the state-of-the-art methods, achieves 0.375 and 0.435 mAP scores on nuScenes eval and test sets respectively. Although it is a not apple-to-apple comparison, NVAutoNet's detection accuracy (0.465 mAP) is competitive to the SOTA methods. Keep in mind that SOTA methods are often heavily optimized for accuracy, but latency. Nonetheless, we found a real-time model FastBEV \cite{fastbev}, but its accuracy is quite low (i.e., 0.277 mAP score on nuScene eval set, see Tab. \ref{tab:autonet_vs_fastbev} for more details). Based on these results, we believe that our model's performance is strong.
\begin{table}[t]
	\begin{tabular}{lccccccc}
	\hline
	 & Vehicle & Truck & Person  & Bike-rider & mAP & (safety) mAP \\
	\hline
	AP & 0.638 & 0.388 & 0.351 & 0.483 & 0.465 & 0.595 \\
        Position error & 0.989 & 1.932 & 0.614 & 0.7 & - & - \\
        Orientation error & 6.542 & 5.295 & 53.4 & 12.334 & - & - \\
	\hline
	\end{tabular}
	\centering
	\caption{Obstacle detection accuracy for different classes.}
	\label{table:obstacle_map}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{cccccc}
	\hline
	Class & Range & F1-score & Position errors & Orientation errors & Shape errors \\
	\hline
	Vehicle & 0-50   m & 0.715 &	0.741 &	7.213 & 0.173 \\
        Vehicle & 50-100 m & 0.481	& 2.231 &	11.295 & 0.246 \\
        Vehicle & 100-150   m & 0.371 &	4.098 &	12.304 & 0.306 \\
        Vehicle & 150-200 m & 0.250	& 6.647	 & 9.193  & 0.339\\
        \hline
        Truck & 0-50   m & 0.481 &	1.099	&  8.463 & 0.362 \\
        Truck & 50-100 m & 0.430 &	2.269	& 7.064 & 0.453\\
        Truck & 100-150   m & 0.419	& 3.554	& 6.854 & 0.420 \\
        Truck & 150-200 m & 0.386	& 5.334	 & 6.778 & 0.411\\
        \hline
        Pedestrian & 0-30   m & 0.491 &	0.500 &	37.772 & 0.435  \\
        Pedestrian & 30-50 m & 0.413	& 1.259 &	51.483 & 0.480 \\
        Pedestrian & 50-100   m & 0.313	& 2.283	& 60.708 & 0.528 \\
        \hline
        Bike-with-rider  & 0-30   m & 0.574	&	0.437	&	11.876 & 0.297 \\
        Bike-with-rider & 30-50 m & 0.482	&	1.152	&	16.516 & 0.352 \\
        Bike-with-rider & 50-100   m & 0.387	&  2.055 &	19.197 & 0.428  \\
	\hline
	\end{tabular}
	\caption{Obstacle detection accuracy at different radial distance ranges.}
	\label{table:obstacle_f1_range}
\end{table}
%\begin{table}[hbt!]
%	\begin{tabular}{llcccc}
%	\hline
%	Model & Datasets & Range (m) & mAP & FPS & Latency\\
%	\hline
%	NVAutoNet  &  In-house & 200  & 0.46 & 53.4 & 18.7\\
%        Fast-BEV-tiny   &  nuScenes eval & 50 & 0.277 & 44.8 & 22.3\\
%	Fast-BEV-small   &  nuScenes eval & 50 & 0.369 & 16.6 & 60.3\\
%	\hline
%	\end{tabular}
%	\centering
%	\caption{NVAutoNet versus Fast-BEV \cite{fastbev}. This is not a apple-to-apple comparison but a good reference.}
%	\label{tab:autonet_vs_fastbev}
%\end{table}
\subsubsection{3D Freespace}
Table \ref{table:freespace_result_solo} summarizes the overall freespace results. Additionally, Table \ref{tab:freespace_result_sector} reports bucketized regression metrics per sector divided into different combinations of angular and radial sectors. Overall, close range regions have higher accuracy than far range regions. Also front region is more accurate than rear region. This is because the front region is covered by three different cameras: 120FOV, 30FOV and fisheye 200FOV. Also it can seen that precision and recall for the Other and Vehicle categories are much better than the VRU category. VRU classification is more difficult as VRU is often covered by few angular bins as compared to the other categories.
\begin{table}[hbt!]
\begin{tabular}{cccc}
\hline
Relative gap & Absolute gap & Success rate  & Smoothness \\ \hline
44.14 & 1.95 & 77.59 & 0.77 \\
\hline
\\
\hline
\end{tabular}
\quad
\begin{tabular}{cccc}
\hline
& Vehicle & VRU & Other\\
\hline
Precision  & 0.92 & 0.73  & 0.98\\
\hline
Recall & 0.92 & 0.66 & 0.98\\
\hline
\end{tabular}
\centering
\caption{3D freespace regression metrics (left) and classification metrics (right).}
\label{table:freespace_result_solo}
\end{table}
\begin{table}[t]
	\begin{tabular}{llcccc}
	\hline
	Radial (meters) & Angular (degrees)  & Success Rate (\%) & Absolute gap (m)\\
	\hline
	0-10  &  [-45, +45] & 86.43  & 0.91\\
        10-20   &  [-45, +45] & 83.02 & 1.40\\
        20-30   &  [-45, +45] & 73.74 & 2.70\\
        30-50   &  [-45, +45] & 64.52 & 4.72\\
        50-80   &  [-45, +45] & 57.05 & 8.21\\
        80-120  &  [-45, +45] & 42.39 & 18.96\\
        120-200 &  [-45, +45] & 1.81 & 54.38\\
	\hline
        0-10  &  [-135, +135] & 82.68  & 0.93\\
        10-20   & [-135, +135] & 76.54 & 1.67\\
	  20-30   &  [-135, +135] & 67.85 & 3.11\\
        30-50   &  [-135, +135] & 58.96 & 5.31\\
        50-80   &  [-135, +135] & 51.46 & 9.31\\
        80-120   &  [-135, +135] & 38.84 & 15.39\\
        120-200   &  [-135, +135] & 0.04 & 61.2\\
	\hline
	\end{tabular}
	\centering
	\caption{Freespace regression metrics for different radial ranges and field-of-views.}
	\label{tab:freespace_result_sector}
\end{table}
\subsubsection{3D Parking space}
\begin{table}
    \begin{tabular}{c|ccc}
    \hline
    & AP & mean IoU & F1\\
    \hline
    Angled & 0.68 & 0.86 & 0.75 \\
    Parallel & 0.19 & 0.82 & 0.37 \\
    Perpendicular & 0.57 & 0.85 & 0.67 \\ 
    All & 0.58 & 0.85 & 0.68\\
    \hline
    \end{tabular}
    \centering
    \caption{Parking space detection performances.}
    \label{table:parking_results_solo}
\end{table}
Table \ref{table:parking_results_solo} reports parking space detection results. We see that the mean IoU of all the detections that were counted as true positives is close to 86\%, which indicates that the majority of the detections highly overlap with the ground truth labels. We also observe that the lowest performing class is the parallel parking space, which can be attributed to challenges in labeling these parking spaces. The dimensions of parallel parking spaces are highly variable. In addition, many parking spaces on the side of the street do not have strictly enforced widths. As such, labelers have to rely on their judgment to determine how wide each parking space should be, which in turn makes the labeling of such spaces highly inconsistent.
\subsection{Single task vs Multi-task Learning}
%Having a model that performs multiple tasks is beneficial as it help reducing the computation cost, as compared to multiple single-task models. However, it is widely known that it is challenging to train multi-task models properly, which often leads to significant accuracy drops.

In this experiment, we set the $c_t$ parameters (defined in Sec. \ref{sec:loss_balancing}) be $[5,3,1]$ for obstacle, parking space and freespace tasks separately. 
Intuitively, we want the network pay more attention to the obstacle detection task, and less to the freespace task as obstacle detection is much more difficult than others. Table \ref{table:single_vs_multi_tasks} compares the performances of the multi-task model against the single task models. We observe that obstacle and parking detection accuracy are comparable to those of single task models. The freespace task drops $9.5\%$, but its accuracy still remains high. This result indicates the impressive effectiveness of our proposed multi-task loss balancing algorithm.
\begin{table}[hbt!]
    \begin{tabular}{c|c|c|c}
    \hline
     & Obstacle (mAP) & Freespace (Success rate) & Parking space (F1) \\ \hline
    Single task & 0.46 & 77.59 & 0.68  \\
    Multi-task & 0.46 & 70.19 & 0.68\\ 
    \hline
    \end{tabular}
    \centering
    \caption{Single task vs. multi-task.}
    \label{table:single_vs_multi_tasks}
\end{table}
\subsection{IPM vs MLP based 2D-to-BEV View Transformation}
In this study, we want to demonstrate the benefits of using a learning based approach to transform 2D image features to BEV features as compared to using a homograph transformation with a flat-world assumption (also called as the IPM method). We pick obstacle detection task for this experiment. Table \ref{tab:ipm_vs_mlp} reports comparison results. It can be seen that our proposed 2D-to-BEV method significantly outperforms the classical IPM method with a very large margin. 
\begin{table}[hbt!]
	\begin{tabular}{lccccc}
	\hline
	 & Vehicle & Truck & Person  & Bike-rider & \\
	 Method & AP & AP & AP  & AP & mAP\\
	\hline
	IPM & 0.49 & 0.32 & 0.27 & 0.33 & 0.30 \\
	MLP & 0.63 & 0.38 & 0.35 & 0.48 & 0.46 \\
	\hline
	\end{tabular}
	\centering
	\caption{IPM versus MLP based 2D-to-BEV view transformation for obstacle detection.}
	\label{tab:ipm_vs_mlp}
\end{table} 

\subsection{Generalization to Different Vehicle Lines}
To verify the robustness and scalability of our proposed architect, we test NVAutoNet, which was originally developed for a car platform, on a truck platform. Although the differences in the intrinsic parameters between the two platforms are small, the differences in the extrinsic parameters are very significant (see Figure \ref{fig:cam_configs}). Nonetheless, we will show that model redesign and large-scale data collection are not required when deploying our perception model on a different platform. In particular, only model finetuing using a small training dataset is sufficient.
\begin{figure}
\centering
\begin{subfigure}[b]{.3\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/car_sensors.png}
    \caption{Car platform.}
\end{subfigure}
\begin{subfigure}[b]{.4\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/truck_sensors.png}
    \caption{Truck platform.}
\end{subfigure}
\caption{Different camera sensor setups for cars and trucks.}
\label{fig:cam_configs}  
\end{figure}
\begin{table}
    \begin{tabular}{*{8}{c}}
    \hline
    & Pre-trained & Trained/Finetuned & \multicolumn{5}{c}{Dataset size (Truck Platform)}  \\ \hline
    &  &  & 50K & 75K & 100K & 125K & 150K \\ \hline
    Model-A & No & Yes & 0.146 & 0.180 & 0.192 & 0.209 & 0.210 \\
    Model-B & Yes & No & 0.168 & 0.168 & 0.168 & 0.168 &0.168  \\
    Model-C & Yes & Yes &0.286 & 0.292 & 0.294 & 0.294 & 0.300  \\ \hline
    \end{tabular}
    \centering
    \caption{Transfer learning experiments for NVAutoNet. mAP scores for obstacle detection are reported. Model-A trained from scratch using different dataset sizes collected from the truck platform; Model-B previously trained using data collected from \emph{\textbf{the car platform}}; Model-C fine-tuned from model-B using data collected from the truck platform.}
    \label{table:truck_dataset_size}
\end{table}

In this study, 3D obstacle detection task is considered. Using the car platform, we collected 850K scenes for training, denoted as car-dataset-train. Using the truck platform, we collected 150K scenes for training, denoted as truck-dataset-train, and 26K scenes for validations, denoted as truck-dataset-val. The label distribution between the two platforms is roughly similar. To see the impact of the training set size, we sub-sample the 150K truck-dataset-train into multiple smaller ones of increasing different sizes (i.e., 50K, 75K, ... 150K).

We compare the following models: Model-A trained from scratch using truck-dataset-train, Model-B previously trained using car-dataset-train and Model-C fine-tuned from Model-B using truck-dataset-train. All three models are tested on the truck-dataset-val. Table \ref{table:truck_dataset_size} reports the comparison results. 

As expected, the performance of Model-A increases with respect to the dataset sizes. Interestingly, Model-A, when trained with 50K scenes, is worse than Model-B although Model-B has never seen the data from the truck platform. This is due to the network's generalization capability. Unsurprisingly, Model-C is the best and significantly outperforms Model-A and Model-B with very large margins. Also notice that the performance of Model-C does not increase considerably with larger finetuning dataset sizes as opposed to the model A. This result indicates that we only need to collect a small amount of data for finetuning if we want to deploy NVAutoNet model (previously trained for one platform) on a different platform. These results confirm again the robustness, scalability of our proposed NVAutoNet --- a must-have feature for production.

\subsection{Qualitative Performance}
Figures \ref{fig:autonet_visual_parking} and \ref{fig:autonet_visual_obs_fsp} showcase qualitative detection results, which are the actual in-car-testing results. Notice that the detections of obstacles, freespaces and parking spaces (when back-projected onto the images) are very well aligned with the image contents. This indicates that the predicted 3D signals (i.e., object locations, shapes and orientations) are highly accurate. Also it can be observed that both short range and long range objects are well detected.


\begin{figure}
\centering
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/parking_01.png}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/parking_04.png}
\end{subfigure}
\caption{Parking space detection by NVAutoNet. For visual clarity, obstacles and freespaces are not visualized.}
\label{fig:autonet_visual_parking}  
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/autonet_highway.png}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/autonet_urban_01.png}
\end{subfigure}
\caption{Obstacles and freespaces detection by NVAutoNet. Bottom: Green cuboids are obstacles. Yellow lines are freespace regions.}
\label{fig:autonet_visual_obs_fsp}  
\end{figure}

\section{Conclusion and Future Work}
Most existing BEV perception works are highly optimized for detection accuracy, which often require huge computational resources. Therefore, these models are impractical to real-world applications such as self-driving where computational budgets are very limited. Moreover, most existing BEV perception datasets and benchmarks are very far from reality. For example, the popular nuScenes dataset  
has ground-truth labels within 70 meters range, while self-driving requires detection range up to 200-300 meters. This further makes the existing BEV perception models not suitable for a self-driving car. In this work, we introduce NVAutoNet in which every component is very well optimized to offer the highest accuracy and latency balance. The technical contributions include, to name a few, small but strong camera backbones, efficient 2D to BEV uplifting, multi-task learning, different platform adaptation, and various task specific training recipes. As a result, NVAutoNet is able to run faster than real-time at 53 FPS on NVIDIA Orin SoC while attaining sufficiently high accuracy.

Extending BEV perception to truly 3D perception (e.g., 3D volumetric occupancy perception) will enable higher levels of autonomy such as L4/L5 self-driving. But that transition is seen to be very challenging due to its high memory and computation consumption. Far range perception (e.g., up to 300 meters) will be necessary to increase driving safety and comfortableness. Holistic scene understanding, where not only dynamic objects but also static objects such as lane graphs, and relations/associations between them are predicted, becomes more relevant to unlock autonomous mapless driving.

\section*{Acknowledgments}
We would like to thank the NVIDIA Maglev and ML foundation teams for the machine learning infrastructure support, thank the NAS (neural architectural search) team for the network optimization, thank Tero Kuosmanen for the initial idea of the loss balancing algorithm and special thank to the whole DRIVEAV team for data collection, network deployment, iteration and on-road testing. 

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
