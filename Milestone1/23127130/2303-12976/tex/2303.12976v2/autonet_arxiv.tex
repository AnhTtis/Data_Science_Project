\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pbox}

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{NVAutoNet}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}


% New commands
\newcommand{\loss}{L}
\newcommand\normx[1]{\left\Vert#1\right\Vert}

  
%% Title
\title{NVAutoNet: Fast and Accurate 360$^{\circ}$ 3D Visual Perception For Self Driving
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Corresponding author}}: 
%\textbf{Trung Pham (trungp@nvidia.com)}} 
}

\author{
\small{Trung Pham\thanks{Corresponding author: Trung Pham (trungp@nvidia.com)} , Mehran Maghoumi, Wanli Jiang, \break Bala Siva Sashank Jujjavarapu, Mehdi Sajjadi}\\
\small{Xin Liu, Hsuan-Chu Lin, Bor-Jeng Chen, Giang Truong, Chao Fang, Junghyun Kwon, Minwoo Park}\\
NVIDIA \\
}


\begin{document}
\maketitle

\begin{figure}[h!]
\centering
\includegraphics[width=0.99\textwidth]{figures/nvautonet.png}
\caption{NVAutoNet overview. Surround view images are input to CNN backbones to extract 2D features, which are uplifted and fused into an unified BEV feature map. The BEV features are then used for 3D perception heads.}
\label{fig:nvautonet}  
\end{figure}
\begin{table}[h!]
	\begin{tabular}{lllll}
	\hline
	Inputs & Outputs & Latency & Detection Range & 	Training data \\
	8 cameras & 3D signals  & 18ms (53fps) & 200 meters & 2.2M scenes\\
	\hline \hline 
    Architecture & Fusion & Modular design & End-to-end training & In-car tested \\ 
	 CNN + MLP & Mid-level & \checkmark  & \checkmark  & \checkmark  \\
	\hline
	\end{tabular}
	\centering
	\caption{NVAutoNet highlights.}
	\label{tab:nvautonet_highlights}
\end{table}

%%%%%%%%% ABSTRACT
\begin{abstract}
Robust real-time perception of 3D world is essential to the autonomous vehicle. We introduce an end-to-end surround camera perception system for self-driving. Our perception system is a novel multi-task, multi-camera network which takes a variable set of time-synced camera images as input and produces a rich collection of 3D signals such as sizes, orientations, locations of obstacles, parking spaces and free-spaces, etc. Our perception network is modular and end-to-end: 1) the outputs can be consumed directly by downstream modules without any post-processing such as clustering and fusion --- improving speed of model deployment and in-car testing 2) the whole network training is done in one single stage --- improving speed of model improvement and iterations. The network is well designed to have high accuracy while running at 53 fps on NVIDIA Orin SoC (system-on-a-chip). The network is robust to sensor mounting variations (within some tolerances) and can be quickly customized for different vehicle types via efficient model fine-tuning thanks of its capability of taking calibration parameters as additional inputs during training and testing. Most importantly, our network has been successfully deployed and being tested on real roads.
\end{abstract}
% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
%Reconstructing precise 3D world from a single camera (2D sensor) is an ill-posed, long-standing problem in computer vision. However, perception of precise 3D world is critical for the consumers of camera perception modules such as planning and control for autonomous vehicles. In parallel, in autonomous driving the world has been migrated from a single camera or front cameras (stereo or three frontal cameras) perception to the full surround camera perception. This makes the 3D surround perception problem much harder because the multi-camera fusion problem needs to be solved on top of the 3D perception.
Reconstructing a precise 3D world from a single camera (2D sensor) is an ill-posed, long-standing problem in computer vision. However, perception of a precise 3D world is a critical task for autonomous vehicles. For a long time, the consumer vehicle industry has relied on a single camera and radar based perception to enable advanced driver assistance systems (ADAS). Then the number of use cases for the consumer vehicles with ADAS has increased dramatically over recent years where full surround perception became essential. Therefore, multi-camera and multi-radar systems are becoming a norm for most of the major consumer vehicle manufactures.

In early days, single camera perception module is developed independently for each camera in multi-camera configuration. Then perception outputs from multiple cameras are combined to produce an unique set of fused perception outputs. One of the challenges of this approach is that prediction errors (e.g., under or over estimation) caused by independent monocular perception modules are different and their error/noise models are usually unknown. As a result, the association/fusion task becomes nontrivial, often leading to many false positive detections. Nonetheless, such \emph{\textbf{camera independence}} is desirable for product safety in that it can avoid a certain level of common cause failures. 

Production scalability is another crucial factor. Car manufacturers often have many different car lines such as SUV, sedan, low-profile sport car, and pick-up truck. Their sizes are all different and so are the camera mounting positions. Therefore, a camera perception system tolerant to different camera mounting angles, positions, and different radial distortion, focal length is a must for scalability. Last, but not the least, real-time capability in low powered, shared compute budget in system on chip (SoC) is required because other redundant but independent radar or ultrasonic perception module can run concurrently.

Here, we aim to achieve (1) accurate 3D estimation (2) \emph{\textbf{camera independence}} (3) end-to-end data-driven machine learning (4) scalability (5) efficiency. Although the ideas of the end-to-end machine learning and camera-independence are conflicting but it is important to address them together. Specifically, we design a neural network, termed NVAutoNet\footnote{NVAutoNet has been released as parts of NVIDIA DRIVE PERCEPTION https://developer.nvidia.com/drive/drive-perception}, with the following principles:
\begin{itemize}%[noitemsep,nolistsep]
    \item NVAutoNet accepts a variable number of camera inputs and produces an unique set of 3D perception outputs for the union field-of-view.
    %\item NVAutoNet produces a unique set of 3D perception output for union of field of each camera view even when some of the cameras drop out.
    \item NVAutoNet is able to tolerate different mounting angles and positions of cameras given nominal intrinsic and extrinsic camera parameters.
    \item NVAutoNet produces full 3D signals. Their projections to image views align with 2D image content.
    %\item NVAutoNet produces 3D signal beyond GT sensor's distance range in a way that projection of it to each image view aligns with 2D image content.
    \item NVAutoNet outputs are directly consumable by downstream modules without any post-processing such as clustering, boundary extraction, etc. 
    \item NVAutoNet runs real-time on Nvidia Orin SoC.
\end{itemize}

To achieve the above goals, we propose several novel technical contributions. (1) The image and bird-eye-view (BEV) feature extractor networks are well-customized using hardware-aware neural architecture search (NAS) to have high accuracy and low latency. (2) The multi-camera fusion is done at the feature level which combines the best of early and late fusion approaches. That means the network still functions properly if one or more cameras are dropped out during inference and the fusion is done within the network. Our machine learned fusion strategy not only avoids handcrafted parameter tuning but also takes advantage of triangulation cues --- offering higher detection accuracy when objects are observed from more than one cameras. (3)  We propose a novel MultiLayer Perceptron (MLP) based 2d-to-3d uplifting module without relying on  depth prediction, and  explicitly taking as input camera intrinsic and extrinsic parameters. Once trained, the network can be applied for different camera configurations given vehicle's rig parameters. (4) All perception tasks including freespace perception are formulated as detection tasks so that expensive and adhoc post-processing such as clustering, boundary extraction, curve fitting can be avoided. Table  \ref{tab:nvautonet_highlights} shows highlights of NVAutoNet. 

The remainder of the paper is organized as follows. Section 2 discusses related work. Sections 3 explains the proposed NVAutoNet. Section 4 provides extensive experimental results. Lastly, section 5 concludes the paper and discuss a couple of future directions for AV perception.

%We adopt a set prediction approach based on fully convolutional layers to enable end-to-end detection and especially allow efficient training and inference using NVIDIA TensorRT engine.
%(5) Our ground-truth labels come from Lidar sensors, which admit shorter ranges compared to camera sensors. We offer a novel weakly-supervised learning approach so that the network can detect objects beyond the range of Lidar sensors (e.g., upto 300 meters).
%\footnote{It's worth mentioning that many novel ideas in this work have been initiated and developed since early 2020, and as of June 2022 the developed network has been released as parts of NVIDIA DRIVE PERCEPTION https://developer.nvidia.com/drive/drive-perception}

\section{Related Work}
To the best of our knowledge, there was no previously published work that has achieved all the goals listed above. In this section, we summarize methods which are related to one or more components of our work.
%\subsection{Multi-camera Fusion and BEV Perception}
%\textbf{Multi-camera Fusion and BEV Perception.} In autonomous driving, inputs to perception models is a set of multiple cameras capturing 360 degree view of the surround environment. While monocular camera perception tasks have been well established in computer vision, multi-camera perception has attracted a lot of interest  recently \cite{Saha2021,Saha2022, wang2021detrd, Can2021StructuredBT, Reiher2020,Philion2020,Ng2020BEVSegBE}. Instead of applying monocular perception models to individual cameras and fusing prediction outputs using camera parameters, modern methods, including ours, adopt an early fusion approach using machine learning. All input images from multiple cameras are injected into a single perception model which will produce a single set of output signals. In order to (early) fuse multiple cameras, a shared representation independent of cameras is needed. One common choice is the BEV representation of which the center is at the vehicle rig's coordinates. The BEV representation is also preferable by downstream modules such as prediction, planning and control.

\subsection{Multi-camera Fusion and BEV Perception.} 
While monocular camera perception tasks have been well established in computer vision, multi-camera perception has attracted a lot of interest recently \cite{Saha2021,Saha2022, wang2021detrd, Can2021StructuredBT, Reiher2020,Philion2020,Ng2020BEVSegBE}. Modern methods, including ours, have moved from a late fusion to an early fusion approach, where all input images from multiple cameras are injected into a single perception model to produce a single set of output signals. In the late fusion approach, a shared representation independent of cameras is needed. A common choice is the BEV representation, which is also preferable by downstream modules such as prediction, planning and control.

%\subsection{Perspective to 3D View Transformation}
%\textbf{Perspective to 3D View Transformation.} Transforming perspective (image) view to 3D view is an ill-posed problem as depth information has been lost during image formation. One common approach (e.g., \cite{Reiher2020}) is using the inverse perspective mapping (IPM) method to transform a perspective image into a BEV image, assuming a flat-world which is usually not true in real world applications. Another group of methods \cite{Schulter2018, Philion2020, Reading_2021_CVPR} rely on per-pixel (absolute or probabilistic) depth information for 3D uplifting. While simple, one downside of the depth-based approaches is that inaccurate predicted depth will be harmful for 3d uplifting, thus potentially leading to low-quality 3D feature maps. Another disadvantage is the inefficiency due to the high-dimensional 3D voxel feature map resulted from per-pixel uplifting, which makes these methods less favorable for far-range detection.

\subsection{Perspective to 3D View Transformation.}
Transforming perspective (image) view to 3D view is an ill-posed problem. Classical methods either assume a flat-world model (e.g., \cite{Reiher2020}) or require per-pixel (absolute or probabilistic) depth information for view transformation \cite{Schulter2018, Philion2020, Reading_2021_CVPR}. There are two issues with the depth-based approach. First, inaccurate predicted depth will be harmful for 3d uplifting, thus potentially leading to low-quality 3D feature maps. Second is the inefficiency due to the high-dimensional 3D voxel feature map resulted from per-pixel uplifting, which makes these methods less favorable for far-range detection.

Data driven and machine learning based view transformation approaches \cite{Chitta2021NEATNA, Yang2021, Mani2020MonoLO, Pan2020CrossViewSS, Hendy2020FISHINGNF} are becoming more popular. For example, the View Parsing Network (VPN) \cite{Pan2020CrossViewSS} and FISHNET \cite{Hendy2020FISHINGNF} methods employ MLP layers to model the view transformation. However, these methods are camera-dependent, which completely ignores geometric priors such as camera intrinsic and extrinsic, thus can not be applied to sensor configurations different from the ones used for training. These MLP based methods are also very computationally expensive because these methods often stretch the 2D feature map into 1D vector, then perform a fully connected operation on it. Recent works \cite{Roddick2020PredictingSM,Saha2021,Saha2022} improve the computational hurdle by applying MLP operations on columns independently.% Nonetheless, we have made a couple of improvements on top of \cite{Roddick2020PredictingSM,Saha2021,Saha2022}. First, our method does not assume that each image column corresponds to a ray on the BEV plane, which is often invalid in practice. Instead, we compute, for each image column, a corresponding polynomial function to accurately reflects camera geometry. This improvement is beneficial for wide-field-of-view cameras such as fish-eye cameras. Second, the existing methods use a fully-connected layer to compress each image column feature vector into a single scalar which are then expanded along the radial distance axis. In contrast, our method projects each image column feature vector into a higher dimensional space to avoid information loss before projecting it to the BEV plane.

%As a result, these methods are not suitable for high-resolution input images and high-resolution BEV feature map (a necessary requirement for detecting far distant objects). 
%Our view transformation module also relies on a MLP, however it performs MLP operations on columns independently for high efficiency similar to recent works \cite{Roddick2020PredictingSM,Saha2021,Saha2022}. Nonetheless, we have made a couple of improvements on top of \cite{Roddick2020PredictingSM,Saha2021,Saha2022}. First, our method does not assume that each image column corresponds to a ray on the BEV plane, which is often invalid in practice. Instead, we compute, for each image column, a corresponding polynomial function to accurately reflects camera geometry. This improvement is beneficial for wide-field-of-view cameras such as fish-eye cameras. Second, the existing methods use a fully-connected layer to compress each image column feature vector into a single scalar which are then expanded along the radial distance axis. In contrast, our method projects each image column feature vector into a higher dimensional space to avoid information loss before projecting it to the BEV plane.

Since mid-2020, Transformers architectures has attracted much attentions from the computer vision community due to their favorable performance on image classification and object detection tasks. Unsurprisingly, there have been a number of works \cite{Saha2022, Can2021StructuredBT,wang2021detrd} which have attempted to use Transformers architectures for the view transformation task. Unfortunately, these methods face two main disadvantages. First, these methods use a sparse set of BEV queries, thus not suitable for dense prediction tasks such as occupancy, freespace or semantic segmentation. Second, Transformers based methods often require considerable resources to execute on-vehicle inference, especially when a dense set of queries is used. A more comprehensive review of perspective view to BEV view transformation methods can be found in \cite{bev_survey}.

%These methods skip computing the dense BEV feature map completely, but query a sparse set of BEV features from a set of object priors. 

%These methods are computational attractive, but not suitable for tasks requiring dense BEV feature maps such as freespace or occupancy perception.

\subsection{Perception for Autonomous Driving}
For autonomous driving, the perception module needs to detect and recognize all static and dynamic objects from the surrounding environment. The list includes obstacles, road markings, lanes, road boundaries, traffic lights, traffic signs, parking spaces, free spaces and many more. In this work, we select three main tasks (obstacles, frees paces and parking spaces) to demonstrate the performance of our perception system. Below we review object detection and freespace detection methods which have attracted more attention and interest from the literature.

\subsubsection{3D object detection}
3D object detection using 2D images is challenging due to the lack of depth information. Prior methods often relied on external sub-networks, e.g., depth estimation \cite{wang2019pseudo, ma2019color}, to assist 3D detection. Notably, Pseudo-LiDAR pipelines \cite{wang2019pseudo, ma2019color} first convert the estimated dense depth map to a 3D point cloud, then adopt a LiDAR-based detection algorithms \cite{frustum2018, pixor2018, pointrcnn2019} to perform 3D object detection. Recently, end-to-end methods have become more popular \cite{roddick2018orthographic,smoke_2020,Wang2021,Wang2021FCOS3DFC}. FCOS3D \cite{Wang2021FCOS3DFC} adopt an anchor-free detection approach, in which objects are represented as single points marked with additional parameters such as size and orientation. Rather than working on the image space, the method introduced by Roddick et al. \cite{roddick2018orthographic} detects 3D objects directly on BEV space. More recently, the DERT3D \cite{Wang2021} method detects 3D objects from multiple cameras in one shot. Our 3D detection approach is mostly similar to DERT3D, there are, however, a number of differences and improvements. First, our network is fully convolutional and light-weight, thus  much more efficient for both training and inference than DERT3D which is based on a Transformers based architecture. Second, our network not only predicts object parameters, but also returns uncertainty for those parameters. Lastly, our method detects full object orientation instead of yaw angles only.

%In contrast, our network is designed to be modular (no dependency on others) and end-to-end (post-processing free). 

\subsubsection{Free space detection}
Accurate detection of free, driveable space is vital for improving autonomous driving safety. Most existing methods such as \cite{Zidong_BMVC2021,BMVC2015_109,Sanberg2016,Scheck2020} focus on detecting 2D freespace from monocular cameras. The method in \cite{Zidong_BMVC2021} operates on a stitched BEV image. Our method consumes multiple-view inputs, and outputs top-view freespace region as a polygon instead of pixel segmentation, thus does not require expensive post-processing such as pixel clustering.

\section{NVAutoNet}
\subsection{Overview}
The inputs to NVAutoNet is a collections of $N_{view}$ camera images $\{I_i\}_{i=1}^{N_{view}}$, capturing a 360-degree field of view of the surrounding environment, together with camera intrinsic and extrinsic parameters. The input images are passed through 2D image encoders to extract 2D image features, which are then uplifted to BEV features. The BEV features from multiple images are fused into a single BEV feature map, before being passed to a common BEV encoder to extract high-level BEV features. Finally, the network will output a collection of 3D signals such as positions, sizes and orientations of obstacles, parking spaces, free spaces. Figure \ref{fig:nvautonet} displays the overview of NVAutoNet.

\subsection{2D Image Feature Extractors}
Each input image $\mathbf{I}$ of dimensions $W\times  H \times 3$ is fed to a feature extractor to compute a set of proportionally sized feature maps at multiple levels $\{F^k\}$ where each $F^k$ is of shape $\frac{H}{2^{k+1}} \times \frac{W}{2^{k+1}} \times C$. Our image feature extractors are underpinned by convolutional architectures which have been carefully customized to have real time performance. A CNN backbone composes of a series of CNN blocks. Each CNN block requires kernel size, stride, number of channels and repeat parameters. Tab. \ref{table:cam_encoders} describes the network configurations for different camera groups. These parameters are optimized via hardware-aware neural architectural search to have a good trade-off between latency and accuracy. Residual connections are removed for higher latency. Coarser feature maps are up-sampled and merged with finer feature maps to form multi-level semantically rich feature maps. It's worth noting that that input images from different cameras need not to be the same size, and different input images can share the same feature extractor. For example, all images from fisheye cameras share the same 2D image encoder. Next we will detail our approach to transform these 2D image feature maps to 3D.

\begin{table}[t]
	\begin{tabular}{llllllll}
	\hline
	& Input size & Blocks & Kernel sizes & Strides & Repeats & Channels \\
	\hline
        Front cam encoder & 480x960 & 5  &  7-3-3-3-3 & 4-1-2-2-2 & 1-0-2-5-3 & 32-32-128-256-512\\
        Side cam encoder  & 480x960 & 5  &  7-3-3-3-3 & 4-1-2-2-2 & 1-0-2-3-3 & 32-32-128-192-512\\
        Fisheye cam encoder & 480x960 & 5  &  7-3-3-3-3 & 4-1-2-2-2 & 1-0-2-3-3 & 32-32-64-96-512\\
        BEV encoder & 64x360 & 3  &  3-3-3 & 1-2-2 & 4-4-4 & 64-128-256\\
	\hline
	\end{tabular}
	\centering
	\caption{Lightweight camera and BEV CNN backbones. All camera backbones have 5 CNN blocks while BEV backbone has only 3 CNN blocks. In all convolution layers, 3x3 kernels are used, except the first convolution layers in the camera backbones where 7x7 kernels are used.}
	\label{table:cam_encoders}
\end{table}

\subsection{Image-to-BEV Transformation and Fusion}
\subsubsection{BEV Plane and BEV Grid}
The BEV representation is commonly used in self-driving due to its efficiency to represent object positions and sizes, and its suitability for behavior prediction and planing. The BEV plane passes through the vehicle's rig center and is orthogonal to the Z axis. We discretize the BEV plane using a BEV grid $G^{bev}$ of dimensions ${W^{bev}\times H^{bev}}$. Each cell in the grid $G^{bev}$ corresponds to a region in the real-world space, and the ego vehicle's position is at the BEV grid's center.

\subsubsection{2D to BEV Uplifting}
Given a set of 2D image feature maps from different cameras, we want to compute a single BEV feature map $F_{bev}$. In this work, we take a data-driven approach in which we train a camera-to-bev transformation function using a multilayer perceptron (MLP) network. Unlike the previous MLP based BEV view transformation methods, our method explicitly consumes camera intrinsic and extrinsic parameters via Look-up-Table (LUT) during training and inference, therefore the model, once trained, will be able to generalize well to different sensor configurations. 
%It's known that the problem of lifting 2D perspective observations into 3D is an inherently ill-posed problem because depth information has been lost. 
%$\{F_{cam}^i\}_{i=1}^{N_{view}}$ 
\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/bev_transformer_v3.png}
\caption{\label{fig:bev_transformer} Left: Pixel locations from each image column are projected onto the BEV plane, resulting in BEV points which are used to fit a polynomial function of which input variable is depth and output is angular position. Right: Each image column features will be transformed into pseudo BEV features which are then scattered onto the BEV plane using the fitted polynomial functions.}
\end{figure}

Our idea is based on the observation that there is a strong geometric relationship between row and column positions in the image plane and  radial and angular positions in the BEV plane. In particular, each image column will correspond to a \textbf{curve} passing through the camera center on the BEV plane. Figure \ref{fig:bev_transformer} (left) shows polynomial curves from projecting individual image columns onto the BEV plane. Note that this observation is different from previous works \cite{Saha2022, Saha2021} which incorrectly assumes each image column will correspond to a BEV \textbf{ray}. Our goal is to learn a function to transform each image column features into pseudo BEV features, which are then scattered onto the BEV plane using the fitted polynomial curves.

Formally, let $\mathbf{I}^c$ be an column from the image $\mathbf{I}$ (for notation simplicity we dropped the camera index), we first project every pixel location $\mathbf{d}_i=[u_i, v_i] \in \mathbf{I}^c$ onto the BEV plane (using camera intrinsic and extrinsic parameters), and convert them to Polar coordinates, resulting in a list of BEV points $\{\mathbf{b}_i = [a_i, d_i]\}$. We then fit a polynomial curve $a=f^c(d)$ to those BEV points, which will be used to infer an angular position given any radial position per each image column. 

Let $r$ (meters) be the maximum detection range for this particular camera, we discretize the range $[0, r]$ into $\hat{h}$ (logarithmically spaced) bins. Let $F_{cam}^c$ with dimension $h \times C$ represent the corresponding features of the image column $\mathbf{I}^c$, we define a BEV transformation network $\mathcal{B}(.)$ s.t. $F^c_{polar} = \mathcal{B}(F_{cam}^c)$. $F^c_{polar}$ with dimensions  $\hat{h} \times C$ will be aggregated into the global BEV feature map $F_{bev}$.

Notice that the features $F^c_{polar}$ has the same number of radial distance bins $\hat{h}$, which means each cell in $F^c_{polar}$ corresponds to a radial distance bin, from where we can sample multiple depth values and compute the corresponding angular values using the fitted polynomial function $f^c(.)$. For computational efficiency, one depth value per each radial distance bin is sampled. Thus, for each cell in the $F^c_{polar}$, its exact location in the BEV grid can be pre-computed and stored in a Look-Up-Table, which is super efficient for training and inference. Figure \ref{fig:bev_transformer} (right) illustrates how image features are transformed into the BEV plane. We perform the same procedure for every other columns and every other images. Once all the image features from multiple cameras are transformed into the BEV plane, those are summed up to produce a fused BEV feature map $F^{bev}$. It is worth noting that by summing up instead of concatenating BEV feature maps from different cameras, the network will be independent of the number of camera inputs.


\subsubsection{BEV Transformation using MLP}
The BEV transformation function $\mathcal{B}(.)$ is modeled using an MLP block with only one hidden layer, and the MLP parameters are not shared by different image columns. Unlike convolutional layers, MLP layers have a capability to encode global contextual information (aka global attention), which we found very crucial for assigning image features into correct BEV positions as depth information has been lost and objects appears at different heights. Note that there is no explicit supervision for the BEV features, which are instead implicitly supervised by the downstream perception losses.

%\subsubsection{Polar coordinates vs Cartesian coordinates}
\subsubsection{Polar vs Cartesian Coordinates} The resolution of BEV grid $G^{bev}$ plays an important role in detection range and detection accuracy. For autonomous driving on highways, we need to detect obstacles up to 200 meters. A regular Cartesian grid with 0.25 meter per cell results in a $1600 \times 1600$ grid, which will be both memory intensive and computational expensive, prohibiting us training deep neural networks, and deploying them on the on-vehicle chips such as NVIDIA Orin. In practice, it is more desired to have a higher resolution for close ranges and a coarser resolution for far range. Therefore, we design an irregular BEV grid using Polar coordinates, in which $W^{bev}$ is the number of angular samples ranging from 0 to 360, and $H^{bev}$ is the number of depth samples. For example, with 1 degree angular resolution and log depth space, we can represent the BEV plane efficiently using a small $360 \times 64$ grid.

\subsection{BEV Feature Extractor}
Similar to 2D image feature extractors, we adopt a CNN backbone to extract high-level features from the fused BEV feature map. See Table \ref{table:cam_encoders} for details. The output of the BEV feature extractor, dubbed $\hat{F}_{bev}$, will be consumed by different 3D detection heads.

\section{Perception Tasks}
\subsection{3D Object Detection}
3D object detection is a key capability for autonomous driving. The goal is to localize, classify and estimate dimensions and orientations of the objects in 3D space. Each object is represented by its category and 3D cuboid. In particular, each 3D cuboid has 9 degree-of-freedom (DOF) and is defined by a position $[r, a, e]$, dimensions $[d_x, d_y, d_z]$, and orientation matrix $\mathbf{R} \in \mathbb{R}^{3 \times 3}$ , as opposed to previous works only estimating yaw angle. In this work, we adopt a set prediction approach to remove the need for a non-maximum suppression (NMS) post-processing. 

The 3D object detection network includes four lightweight heads (implemented by a couple of convolutional layers), which takes the bottleneck feature map $\hat{F}_{bev}$ as input and predict class distributions and 3D cuboid parameters. Formally, let us denote $M\times N\times C$ be the dimension of $\hat{F}_{bev}$, where $M \times N$ is the spatial dimension and $C$ is the number of channels, the 3D detection network will output $\hat{K} = M \times N$ objects --- one object per grid cell. The model employs one head to predict the object classification scores, and three other heads for 3D cuboid parameters (position, dimensions and orientation) regression. There is an additional head used for predicting uncertainty of cuboid parameters.\\
\textbf{Classification.} For $k$ number of objects, the network outputs $k+1$ classification channels, where the first channel represents object existence and the other $k$ channels represent a categorical distribution over $k$ classes. \\
\textbf{Position.} The network predicts a tuple $[r, a, e]$, where $r$ is radial distance, $a$ is azimuth angle, and $e$ is elevation. Note that the network actually predicts radial and angular offset values, which are then added to grid cell positions to form real radial and angular positions.\\
\textbf{Dimensions.} The network predicts three scalars $[d_x, d_y, d_z]$ which are absolute values in meters. \\
\textbf{Orientation.} Unlike other parameters, regressing object orientation (i.e., $3 \times 3$ rotation matrix) is more challenging. This is because not every $3 \times 3$ matrix is a valid rotation matrix. In this work, we propose to train the network to predict sine and cosine values of yaw ($\psi$), pitch ($\theta$), and roll ($\phi$) angles respectively, which are later used to construct a rotation matrix by applying matrix multiplication of three axis rotation matrices together.
\begin{equation}
R = R_{z}(\psi)R_{y}(\theta)R_{x}(\phi),
\end{equation}
\begin{equation}
\begin{aligned}
R_{z} = \begin{bmatrix}
\cos \psi & - \sin \psi & 0\\
\sin \psi & \cos \psi & 0\\
0 & 0 & 1
\end{bmatrix},
R_{y}(\theta) = \begin{bmatrix}
\cos \theta & 0 & \sin \theta\\
0 & 1 & 0\\
-\sin \theta & 0 & \cos \theta
\end{bmatrix},
R_{x}(\phi)=\begin{bmatrix}
1 & 0 & 0\\
0 & \cos \phi & - \sin \phi\\
0 & \sin \phi & \cos \phi
\end{bmatrix}
\end{aligned}
\end{equation}

\textbf{Training Losses.} For each scene, let $\mathcal{G} = \{\mathbf{g}_i\}_{i=1}^K$
be  a set of $K$ ground-truth objects, and let $\mathcal{P} =\{\mathbf{d}_i\}_{i=1}^{\hat{K}}$ be a set of $\hat{K}$ predicted objects. The training loss is computed in a two-step fashion where in the first step, we found the best one-to-one matching between $\mathcal{G}$ and $\mathcal{P}$. In the second step, we compute the final loss based on the matching result. Although the idea sounds very simple, the training process will be highly affected by the matching quality --- if the matching algorithm returns wrong or sub-optimal matches, the network training won't be successful. 

To ensure high quality matches, we constructed a matching cost function considering classification, position, dimension, and orientation costs between each candidate-ground truth pair. In the early phase of training, however the network prediction is quite noisy, often leading to bad assignments. This issue can be mitigated by setting a higher weight for the position cost. Alternatively, inspired by \cite{peize2020onenet}, we proposed to limit the matching candidates for each ground truth object by its corresponding coverage on the BEV grid. This will prohibit matching pairs in which the ground truth object and the candidate are far apart. Moreover, the matching optimization problem is now simplified,  we adopt a greedy matching algorithm instead of the well-known Hungarian algorithm without losing accuracy, but more efficient.

Let $\mathcal{P}_{pos}$ be the set of positive object candidates matched to the ground truth objects, and $\mathcal{P}_{neg} = \mathcal{P} \setminus \mathcal{P}_{pos}$ be a set of negative objects, the training loss is as below:
\begin{equation}
\begin{aligned}
L_{obs}(\mathcal{G}, \mathcal{P}) = L^{pos}_{obs}(\mathcal{G}, \mathcal{P}_{pos}) + L^{neg}_{obs}(\mathcal{P}_{neg}) \\ = \sum_{\mathbf{d}_i \in \mathcal{P}_{pos}} L(\mathbf{g}_i, \mathbf{d}_i) + \sum_{\mathbf{d}_i \in \mathcal{P}_{neg}}L(\mathbf{d}_i),
\end{aligned}
\end{equation}
where $L(\mathbf{g}_i, \mathbf{d}_i)$, and  $L(\mathbf{d}_i)$ are loss functions per each predicted candidate. % $w_{\mathbf{g}_i}$ is a weighting parameter pre-computed for each ground-truth object $\mathbf{g}_i$. In this work, $w_{\mathbf{g}_i}$ is simply equal to the number of views in which the object $\mathbf{g}_i$ appear. Effectively, objects appearing in multiple views are expected to have higher detection accuracy.
If $\mathbf{d}_i$ is a negative candidate,  the loss function $L(\mathbf{d}_i)$ is simply a (focal) binary cross entropy loss --- pulling its objectness score to zero. If $\mathbf{d}_i$ is a positive candidate, the loss function $L(\mathbf{g}_i, \mathbf{d}_i)$ composes of classification and regression losses. While the classification loss is still a (focal) cross-entropy loss, the regression loss is more complex as explained below.

%Note that each ground-truth and prediction 3D cuboid has 15 parameters (3 for position, 3 for dimensions, and 9 for orientation), 
To regress 3D cuboid parameters, ideally we should compute a global loss such as intersection-over-union (IoU) score which considers all parameters together as there is a strong correlation between cuboid parameters. However there is no closed-form solution to compute IoU between two 3D cuboids. Here, we propose to decompose the 3D cuboid regression loss into position (location) loss, shape (size) loss and orientation (rotation) loss, as below:
\begin{equation}\label{eq:obs_reg}
L^{reg}(\mathbf{g}_i, \mathbf{d}_i) = L^{loc}(\mathbf{g}_i, \mathbf{d}_i) + L^{size}(\mathbf{g}_i, \mathbf{d}_i)  + L^{rot}(\mathbf{g}_i, \mathbf{d}_i) 
\end{equation}
\begin{equation}
\begin{aligned}
L^{loc}(\mathbf{g}_i, \mathbf{d}_i) =\frac{| \mathbf{g}_i^r -  \mathbf{d}_i^r|}{\sigma_r} + \frac{| \mathbf{g}_i^a -  \mathbf{d}_i^a|}{\sigma_a} + \frac{| \mathbf{g}_i^e -  \mathbf{d}_i^e|}{\sigma_e} \\+ \log(2\sigma_r) + \log(2\sigma_a) + \log(2\sigma_e)
\end{aligned}
\end{equation}
\begin{equation}
L^{size}(\mathbf{g}_i, \mathbf{d}_i) =\frac{1}{\sigma_s} \prod_{d \in \{d_x, d_y, d_z\}} \frac{\min (\mathbf{g}_i^{d}, \mathbf{d}_i^{d})}{\max (\mathbf{g}_i^{d}, \mathbf{d}_i^{d})} + \log(2\sigma^s)
\end{equation}
\begin{equation}
L^{rot}(\mathbf{g}_i, \mathbf{d}_i) = \frac{1}{\sigma_o} \sum_{r \in R} | \mathbf{g}_i^{r} - \mathbf{d}_i^{r}| + \log(2\sigma^o)
\end{equation} 
where $\sigma_r$,$\sigma_a$, $\sigma_e$, $\sigma_s$, $\sigma_o$ are  uncertainty values for position, shape and orientation respectively. These values are predicted by the network.

\subsection{3D Freespace} 
3D obstacle detection generally covers category-wise classifiable vehicles and vulnerable road users (VRU). In a driving scenario, there is a lot more information which is relevant for safe driving beyond the predictions of 3D obstacle detection. For example, there can be random hazard obstacles like tyres, traffic cones lying on the road. Additionally, there are a lot of static obstacles like road-divider, road-side curb, and guard rails which are not covered by 3D obstacle detection. An autonomous vehicle (AV) has to drive safely within the boundaries of the road by avoiding all kinds of obstacles. The region within the boundaries of the road which is not occupied by any obstacle could be carved out as a driveable region. The driveable region is used interchangeably as the freespace region. The down-stream behaviour planner would consume the freespace region information to plan a safe trajectory for the AV. So it is essential to have a perception component which predicts this freespace region. The freespace region is represented as a radial distance map (RDM). The representation is explained in more details in the next section.

%The goal is to detect a driveable region so as for the down-stream behavior planner to plan a safe trajectory for the self-driving vehicle.

%\subsubsection{Radial Distance Map Representation}

%\begin{figure}
%\centering
%\includegraphics[width=0.4\textwidth]{figures/RDM_freespace.png}
%\caption{\label{fig:RDM_freespace} 3D freespace ground-truth labels are originally in the form of polygons as shown in (a), but for model training, they are converted to a radial distance map as shown in (b). It allows us to get 3D freespace output direcly consumable by downstream modules without complicated post-processing from NVAutoNet.}
%\end{figure}
%As Figure \ref{fig:RDM_freespace}(a) shows, polygons are used for labeling of 3D freespace. 
%Polygons are used for labeling of 3D freespace. Therefore, binary segmentation in the BEV space (i.e., freespace vs non-freespace) might seem to be the most intuitive approach to the 3D freespace detection task. However, there is an important limitation in the segmentation approach: its raw output (binary segmentation map) is not directly consumable by downstream modules such as planning and control and thus it needs additional post-processing such as boundary extraction. Therefore, instead, we encode the 3D freespace ground-truth labels using the radial distance map (RDM). 
\textbf{Radial Distance Map Representation.} While polygons are used for labeling of 3D freespace, we used the RDM representation due to its higher efficiency. RDM is composed of equiangular bins and radial distance values for each angular bin to denote spatial locations. For autonomous driving applications, the distance to the closest freespace boundary is the most important and thus we use a single scalar for each angular bin to represent the closest freespace boundary. In order to create ground-truth RDM for 3D freespace, we simply shoot a ray from the center of BEV plane at each angular bin direction and compute the intersection between the ray and the ground-truth polygon. The most important benefit of using RDM to represent 3D freespace is it can be directly used to create 3D boundary points without additional post-processing. 
%Figure \ref{fig:RDM_freespace}(b) shows the ground-truth RDM created from 3D freespace polygons in Figure \ref{fig:RDM_freespace}(a). 
In addition to the radial distance, we also have, for each angular bin, boundary semantic labels such as vehicle, VRU and others. 
Each freespace ground truth label becomes $(\mathbf{r}, \mathbf{c})$, where $\mathbf{r}$ is a radial distance vector, and $\mathbf{c}$ is a boundary semantic vector. %Each $c_i$ takes a value in $[0, C)$ (C is the number of classes).
%Therefore, 3D freespace ground-truth labels are finally represented as a combination of RDM and the corresponding semantic class labels per angular bin. 

\textbf{Model.} The 3D freespace detection network consists of a shared `neck' and two separate heads. The shared neck includes a couple of convolutional layers on top of the bottleneck feature map $\hat{F}_{bev}$. It is later extended into two heads which predict radial distance and classification maps. %denoted as $\hat{\mathbf{d}}$ and $\hat{\mathbf{c}}$ respectively. 


\textbf{Training Losses.} For each scene, let $\mathcal{G} = (\mathbf{r}, \mathbf{c})$ be the ground-truth, let $\mathcal{P} = (\hat{\mathbf{r}}, \hat{\mathbf{c}})$ be the prediction.
%Here, ˆd={^di}i=1,…,N\hat{d} = \{\hat{d_i}\}_{i=1,\ldots,N} is the prediction RDM from the radius head and d={di}i=1,…,Nd = \{d_i\}_{i=1,\ldots,N} is the label RDM. Assuming we do KK-class classification in each of the angular bins, classification label is defined as c={ci}i=1,…,Nc = \{c_i\}_{i=1,\ldots,N} where ci∈[0,K)c_i \in [0, K) is the corresponding label class index. If pijp_{ij} is the probability from the classification head corresponding to ithi^{th} angular bin and the jthj^{th} class, the classification prediction is defined as ˆc={{pij}j=1,…,K}i=1,…,N\hat{c} = \{\{{p_{ij}\}_{j=1,\ldots,K}}\}_{i=1,\ldots,N}.
 The overall loss function for the 3D freespace detection task is defined as:
\begin{equation}
L_{fsp}(\mathcal{G}, \mathcal{P}) = L^{reg}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) +  L^{cls}_{fsp}(\hat{\mathbf{c}}, \mathbf{c})
\end{equation}
\begin{equation}
L^{reg}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) = L^{iou}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) + L^{sim}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})
\end{equation}
where $L^{reg}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})$ is the regression loss which is a combination of radius loss $L^{iou}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})$ and the similarity loss $L^{sim}(\hat{\mathbf{r}}, \mathbf{r})$; $L^{cls}_{fsp}(\hat{\mathbf{c}}, \mathbf{c})$ is the classification loss.
The radius loss is a polar intersection-over-union loss (IoU) computed between the prediction and ground truth labels, defined as below:
\begin{equation}
L^{iou}_{fsp}(\hat{\mathbf{r}}, \mathbf{r})  =  \sum_{i=1}^{N_{bins}} \frac{\min(r_i, \hat{r_i})}{\max(r_i, \hat{r_i})}.
\end{equation}
%\begin{equation}
%L^{iou}(\hat{d}, d)  =  \sum_{i=1}^{N_{bins}} (1.0 - \frac{{\hat{\triangle}}_{i}^{i+1} \cap {\triangle}_{i}^{i+1}}{{\hat{\triangle}}_{i}^{i+1} \cup {\triangle}_{i}^{i+1}}).
%\end{equation}
%For each angular bin $i$, $(x_i, y_i)$ and $(\hat{x}_i, \hat{y}_i)$ are the ground truth and prediction Cartesian points respectively. ${\hat{\triangle}}_{i}^{i+1}$ is the triangle formed by $(0, 0)$,  
%$(\hat{x}_i, \hat{y}_i)$ and $(\hat{x}_{i+1}, \hat{y}_{i+1})$, ${\triangle}_{i}^{i+1}$ is the triangle formed by $(0, 0)$,  $(x_i, y_i)$ 
%and $(x_{i+1}, y_{i+1})$.
The similarity loss is computed between the line segments formed by joining the end points from the consecutive angular bins in the prediction and ground truth labels. This loss helps in reducing the noise in the predicted RDM.
\begin{equation}
L^{sim}_{fsp}(\hat{\mathbf{r}}, \mathbf{r}) = \sum_{i=1}^{N_{bins}} (1.0 - \frac{{\hat{l}}_{i}^{i+1} \cdot {l}_{i}^{i+1}}{\left\lVert {\hat{l}}_{i}^{i+1} \right\rVert \left\lVert {l}_{i}^{i+1} \right\rVert}),
\end{equation}
where ${\hat{l}}_{i}^{i+1}$ is the line segments formed by joining the end points of $\hat{r}_{i}$ and $\hat{r}_{i+1}$. ${{l}}_{i}^{i+1}$ is defined similarly.
Finally, the classification loss $L^{cls}_{fsp}(\hat{\mathbf{c}}, \mathbf{c})$ is the standard focal loss, i.e., 
\begin{equation}
L^{clc}_{fsp}(\hat{\mathbf{c}}, \mathbf{c}) = \sum_{i=1}^{N_{bins}} \sum_{j=1}^{C} (1 - p_{ij})^{\gamma} \log(p_{ij}),
\end{equation}
where $\gamma$ is the focal loss parameter.
\subsection{3D Parking Space}
Another important aspect of autonomous driving is the ability to localize and classify parking spaces. For simplicity, we assume a flat-world model wherein the ground plane is at a height of zero with respect to the ego-car. Therefore, each parking space is represented as an oriented rectangle, parameterized by $[cx, cy, l, w, \theta]$, where $cx$ and $cy$ are the center coordinates of the box, $l$ and $w$ are the length and the width of the box in meters respectively, and $\theta$ is the orientation of the box (yaw angle in radians) in the range $[0, \pi)$. Note that an oriented box angled at $\pi$ visually appears the same as a box oriented at 0, thus the orientation value $\theta$ need not cover the entire angular range of $[0, 2\pi)$.
Knowing the \textit{profile} of every parking space is important for planning and control purposes. As such, every prediction output by our model will be assigned a profile. In the current system we support three different parking profiles: \textit{angled, parallel and perpendicular}. As their name suggests, the profile denotes the type of planning and control maneuvering required to successfully park the car in the parking space. \textit{Parallel} parking spaces are ones that typically appear on the side of the street and require a parallel parking maneuver. Conversely, \textit{angled} and \textit{perpendicular} parking spots are ones where the car can be parked straight in (or backed in). 

\textbf{Model.} The parking space detection task follows the same design (head, training strategy and losses) of the obstacle detection task. The parking detection network consists of classification and regression heads. The classification head predicts per-profile confidence scores. We rely on each parking spot's profile to implicitly encode its existence score. The regression head predicts the parking space oriented bounding boxes as discussed above.

\textbf{Training Losses.} The training loss is similar to that of the obstacle detection task, but its regression loss function is much simpler. Let $\mathbf{g}_i$ and $\mathbf{d}_i$ be a matched pair of ground truth and detection, the regression loss is defined as:
\begin{align}
    \loss^{reg}_{prk}(\mathbf{g}_i, \mathbf{d}_i) = \sum_{s \in \{cx, cy, l, w, \theta\}} ({\mathbf{g}_i^{s} - \mathbf{d}_i^{s}})^2.
\end{align}

%For each scene, let $\mathcal{G} =\{\mathbf{g}_i\}_{i=1}^{K}$ be a set of $K$ ground-truth parking spots, and let $\mathcal{P} =\{\mathbf{d}_i\}_{i=1}^{\hat{K}}$ be a set of predicted boxes. We first determine the matching between each prediction and ground truth label using the Hungarian matching algorithm. Once the matching is found, we separate the losses into \textit{positive} and \textit{negative} categories. Let $\mathcal{P}_{pos}$ be the set of positive candidates matched to the ground truth labels, and $\mathcal{P}_{neg} = \mathcal{P} \setminus \mathcal{P}_{pos}$ be a set of negative (unmatched) boxes. Our training loss is defined as:
%
%\begin{equation}
%\begin{aligned}
%\loss_{psp}(\mathcal{G}, \mathcal{P}) = \loss^{pos}(\mathcal{G}, \mathcal{P}_{pos}) + \loss^{neg}(\mathcal{P}_{neg}) \\ = \sum_{i=1}^{K} \loss(\mathbf{g}_i, \mathbf{d}_i) + \sum_{i=1}^{\hat{K}-K}\loss(\mathbf{d}_i),
%\end{aligned}
%\end{equation}
%where $\loss(\mathbf{g}_i, \mathbf{d}_i)$, and  $\loss(\mathbf{d}_i)$ are positive and negative loss functions respectively composed of classification and regression loss terms. The classification loss is simply the (focal) binary cross entropy loss between each profile output channel and its corresponding element in the ground truth label. The regression loss is the sum of squared errors between each regression output channel and its corresponding element in the ground truth label. Note that for positive samples we calculate both classification and regression loss terms, but we only calculate the classification loss for negative samples, as the regressed box values for negative samples is irrelevant and need not be optimized. 

%Formally, let $\mathbf{p}^{\text{cls}}$ and $\mathbf{p}^{\text{reg}}$ respectively denote the classification and regression components of an encoded parking space. Our positive and negative losses are defined as:
%\begin{align}
%    \loss(\mathbf{g}_i, \mathbf{d}_i) = &
%    % Classification
%    \loss_{\text{BCE}}(\mathbf{g}_i^{\text{cls}}, \mathbf{d}_i^{\text{cls}}) + 
%    % Regression
%    \normx{\mathbf{g}_i^{\text{reg}} - \mathbf{d}_i^{\text{reg}}}_2\\
%    \loss(\mathbf{d}_i) = &
%    % Classification
%    \loss_{\text{BCE}}(0, \mathbf{d}_i^{\text{cls}}).
%\end{align}

% \noindent
% where w_{\text{cls}}w_{\text{cls}} and w_{\text{reg}}w_{\text{reg}} are hyperparameters that control the weighting of each loss term.

\section{Multi-task Learning and Loss Balancing}\label{sec:loss_balancing}
NVAutoNet is a multi-task network which learns multiple tasks at the same time. These tasks are inherently different in nature and the same training recipe is not necessarily optimal for all of them. In addition, different task loss have different landscapes, in which case linear combination of various loss components with equal weights leads to under representation of certain tasks or a subset of tasks dominating the loss. Let $T$ be the number of tasks, the total training loss, for a given batch $b$, is $L_{total} = \sum_{t=1}^{T} w_t L_{t}^b$, where $w_t$ is a loss weight assigned to task $t$, and $L_t^b$ is the loss for task $t$ given a training batch $b$. Finding the optimal set of weighting parameters $\{w_t\}_{t=1}^{T}$ is a challenging task. 
Here we develop a simple, but effective weighting algorithm to dynamically update the loss weights $\{w_t\}_{t=1}^{T}$ that are assigned to each loss component. 
Given a dataset with $S$ samples, let $L_{t,s}$ be the loss of task $t$ evaluated on training sample $s$. Note that not all training samples in the dataset necessarily contain ground-truth labels for all the tasks. The task loss for such cases will be set to 0 (but its gradient is ignored during back-propagation). At every epoch, we calculate a loss sum, for every task $t$, over all the training samples, and the loss weight $w_t$ is an inverse of the loss sum scaled by a task loss prior $c_t$, i.e., 
\begin{equation}
L_{t} = \sum_{s=1}^{S} L_{t,s}, \quad w_t = \frac{c_t}{L_t}, \quad w_t = \frac{w_t}{\sum_{t=1}^{T} w_t},
\end{equation}
where $c_t$ is a configurable loss multiplier for task $t$ that can be used to boost or reduce a task's loss. Intuitively, scaling losses by their inverses of loss sums will help making all the losses to be in the similar scale, thus reducing the gaps between gradient magnitudes. And the loss multiplier $\{c_t\}$ will be helpful when certain tasks are more important than others or harder to learn. For the first epoch we set all the task loss weights $\{w_t\}_{t=1}^{T}$ manually (e.g., $w_t =1$ for all $t \in [1, T]$) since we don't have the loss sum statistics from the previous epoch, and we update $w_t$ is after each epoch. 

In practice, we carry out a two-stage approach where in the first training trial, all $\{c_t\}$ are set to 1. We then compare the multi-task results versus the single task results, and infer $\{c_t\}$ accordingly based on task KPI improvements or regressions. With the found $\{c_t\}$, we retrain the network in the subsequent training trials. Although $\{c_t\}$ are tuned manually, we found that searching for $\{c_t\}$ is much easier than searching for $\{w_t\}$ directly.


\section{Experimental Evaluation}
In this section, we demonstrate latency and accuracy performance of NVAutoNet. Ideally we should compare NVAutoNet against the state-of-the-art methods. Unfortunately, we found a number of difficulties. First, we could not find any methods (and datasets) which jointly detect obstacles, freespaces and parking spaces like NVAutoNet. Second, most existing BEV perception methods (e.g., 3D object detection) focus primarily on pushing their accuracy, unlike NVAutoNet optimized for both accuracy and latency. Third, NVAutoNet is designed for real self-driving application, where the required detection range is up to 200 meters. Many of our design choices (such as polar BEV representation) become disadvantageous when testing on public datasets such as nuScenes where only short range ($<70$ meters) is considered. Therefore, we mainly evaluate NVAuoNet on our in-house dataset and NVIDIA Orin platform.
\subsection{Datasets and Evaluation Metrics}
\subsubsection{Datasets}\label{sec:datasets}
Our in-house datasets consist of real, simulated reality and augmented reality data. In total, there are 2.2M training scenes, 400K validation scenes and 177K testing scenes. Table \ref{table:datasets} summarizes our datasets. Lidar data was used to generate ground truth labels. Our data contains a fair amount of noisy labels due to view point differences between Lidar and camera sensors. For example, Lidar is mounted at a higher position than cameras, thus there are obstacles, which may be visible by Lidar, are hardly visible by cameras. These issue not only affect model training but also model evaluation (e.g., low recall rates).
\begin{table}
	\begin{tabular}{lcc}
	\hline
	& Amount & Details\\
	\hline
	Number of cameras per scene & 8 & 2 front, 2 rear, and 4 surround fisheye cameras\\
        \hline
        Real training samples & 2M \\
        Sim training samples & 200k \\
        Validation samples & 400K \\
        Test samples & 177K \\
        \hline
        Number of countries & 20 & US, EU and New Zealand \\
        \hline
        Number of obstacle classes & 5 \\
        Number of freespace classes & 3 \\
        Number of parking classes & 3 \\
        \hline
        Percentage of dry roads & 95.17$\%$ \\
        Percentage of wet roads & 4.83$\%$  & rain, fog, snow, ice\\
        \hline
        Bright light condition & 49.5$\%$  & \\
        Diffused light condition & 31.4$\%$  & \\
        Poor light condition & 19.1$\%$  & \\
	\hline
	\end{tabular}
	\centering
	\caption{In-house dataset summary.}
	\label{table:datasets}
\end{table}
\begin{table}[h!]
	\begin{tabular}{llccl}
	\hline
	Components & Latency & Multiplier &Total\\
	\hline
	Front Cam Encoder  &  1.80  & 2 & 3.61\\
	Side Cam Encoder   &  1.60  & 2 & 3.21\\
    Fisheye Cam Encoder&  1.39  & 4 & 5.58\\
    3D Uplifting + Fusion   &  0.30      & 8 & 2.40 \\
    3D Encoder + Heads &  3.91  & 1 & 3.91\\
        &   &  &  18.72 \\
	\hline
	\end{tabular}
	\centering
	\caption{NVAutoNet latency (ms) measured on NVIDIA Orin embedded GPU. The model runs at 53 FPS.}
	\label{table:latency}
\end{table}
\subsubsection{Evaluation Metrics}
\textbf{3D Obstacles.} 
We calculate obstacle detection metrics based on identifying true positives (TP), false positives (FP), and false negatives (FN) from detection outputs and ground truth labels. For each class, we find one-to-one matching between detection output and ground truth using greedy algorithm with the Euclidean distance between their centroids. A match is valid if the relative radial distance between a prediction and ground truth objects is less than 10$\%$, and their absolute azimuth error is less than 2 degrees.  
All unmatched detections become FP while all unmatched ground truth becomes FN. Once TP, FP, and FN have been identified, we compute precision, recall, F1-score, AP and mAP KPIs. Moreover, we search for the best confidence threshold that maximizes F1-score, and compute regression errors for all true positive detections. \emph{Position error} measures relative radius error ($\%$), absolute azimuth error (degrees) and absolute elevation error (meter). \emph{Orientation error} is defined as $|| \textrm{log}(R^{-1}\hat{R})||$, where $R$ and $\hat{R}$ are ground truth and prediction rotation matrices. \emph{Shape error} measures relative error for length, width, and height ($\%$). We also define the safety mAP based on a safety zone. The safety zone is defined as a rectangular region around the ego vehicle, i.e., 100 meters ahead and behind the ego vehicle and 10 meters left and right of the vehicle. 

\textbf{3D Freespaces.}
%As freespace regression is represented as radial distance maps, evaluating 3D freespace output accuracy simply computes the errors between ground truth and prediction radial distances for each angular bin. Similarly freespace classification related errors are computed between ground truth and prediction classifications.\\
Given a pair of ground truth and prediction freespace RDMs, we compute the following metrics (averaged over angular bins and frames). \emph{Relative gap} measures the relative radial distance errors ($\%$). 
\emph{Absolute gap} measures the absolute radial distance errors (meters). \emph{Success rate} measures the percentages of successfully estimated angular bins. Angular bins are considered as successfully estimated when the relative gap is less than $10\%$. \emph{Smoothness} measures the total variation of radial distance maps defined as $\sum_{i=1}^{N_{bins}} |r_i - r_{i-1}|$. \emph{Classification error} measures precision and recall for each label class.

\textbf{3D Parking Spaces.} Similar to object detection, we compute precision, recall, F1 and AP metrics.  Intersection over union (IoU) scores are used to match predictions to ground truth labels. A match is valid if the IoU $\ge$ 70\%. This strict criteria is necessary for real-world applications of autonomous parking as small misalignment between the detection and the actual parking space position can lead to imperfect parking. We also compute mean IoU values for all true positive detections.

\subsection{Latency Performance}
We export NVAutoNet using NVIDIA TensorRT and time it on NVIDIA Orin. Table \ref{table:latency} reports the total NVAutoNet latency as well as the breakdown latency for different NVAutoNet components. It can be seen that our network only costs \textbf{18.7 ms} (8 cameras input), leading to a very high frame-rate at 53 FPS. This is a lot faster than Fast-BEV \cite{fastbev} (6 cameras input) running at 44.8 FPS on the same NVIDIA Orin SoC. %Note that while our model detects obstacles, freespaces and parking spaces, Fast-BEV only detects obstacles. 

\subsection{Quantitative Performance}
\subsubsection{3D Obstacles} \label{sec:results_3d_obs}
Table \ref{table:obstacle_map} reports the obstacle detection accuracy results. It is also important to compare our method against the state-of-the-art methods. Looking at NuScenes leader board, we found that the top-1 mAP for obstacle detection is about 0.58 (achieved by BEVFormer \cite{bevformer}), which is better than ours (0.46). However, bear in mind that in the NuScenes evaluation, the detection range is limited by 70 meters, whereas our detection range is up to 200 meters. The detection accuracy often drops significantly with respect to distances (see Table \ref{table:obstacle_f1_range}). More importantly, almost all of the methods participated in the NuScenes benchmark are heavily optimized for accuracy (giant models, ensemble models), unlike ours. Nonetheless, we found a real-time model FastBEV \cite{fastbev}, but its accuracy is quite low (see Tab. \ref{tab:autonet_vs_fastbev}). Based on those facts, we strongly believe that our model's performance is strong.
\begin{table}[t]
	\begin{tabular}{lccccccc}
	\hline
	 & Vehicle & Truck & Person  & Bike-rider & mAP & (safety) mAP \\
	\hline
	AP & 0.638 & 0.388 & 0.351 & 0.483 & 0.465 & 0.595 \\
        Position error & 0.989 & 1.932 & 0.614 & 0.7 & - & - \\
        Orientation error & 6.542 & 5.295 & 53.4 & 12.334 & - & - \\
	\hline
	\end{tabular}
	\centering
	\caption{Obstacle detection accuracy for different classes.}
	\label{table:obstacle_map}
\end{table}
\begin{table}
	\centering
	\begin{tabular}{cccccc}
	\hline
	Class & Range & F1-score & Position errors & Orientation errors & Shape errors \\
	\hline
	Vehicle & 0-50   m & 0.715 &	0.741 &	7.213 & 0.173 \\
        Vehicle & 50-100 m & 0.481	& 2.231 &	11.295 & 0.246 \\
        Vehicle & 100-150   m & 0.371 &	4.098 &	12.304 & 0.306 \\
        Vehicle & 150-200 m & 0.250	& 6.647	 & 9.193  & 0.339\\
        \hline
        Truck & 0-50   m & 0.481 &	1.099	&  8.463 & 0.362 \\
        Truck & 50-100 m & 0.430 &	2.269	& 7.064 & 0.453\\
        Truck & 100-150   m & 0.419	& 3.554	& 6.854 & 0.420 \\
        Truck & 150-200 m & 0.386	& 5.334	 & 6.778 & 0.411\\
        \hline
        Pedestrian & 0-30   m & 0.491 &	0.500 &	37.772 & 0.435  \\
        Pedestrian & 30-50 m & 0.413	& 1.259 &	51.483 & 0.480 \\
        Pedestrian & 50-100   m & 0.313	& 2.283	& 60.708 & 0.528 \\
        \hline
        Bike-with-rider  & 0-30   m & 0.574	&	0.437	&	11.876 & 0.297 \\
        Bike-with-rider & 30-50 m & 0.482	&	1.152	&	16.516 & 0.352 \\
        Bike-with-rider & 50-100   m & 0.387	&  2.055 &	19.197 & 0.428  \\
	\hline
	\end{tabular}
	\caption{Obstacle detection accuracy at different ranges.}
	\label{table:obstacle_f1_range}
\end{table}
\begin{table}[hbt!]
	\begin{tabular}{llcccc}
	\hline
	Model & Datasets & Range (m) & mAP & FPS & Latency\\
	\hline
	NVAutoNet  &  In-house & 0-200  & 0.46 & 53.4 & 18.7\\
        Fast-BEV-tiny   &  NuScenes & 0-70 & 0.277 & 44.8 & 22.3\\
	Fast-BEV-small   &  NuScenes & 0-70 & 0.369 & 16.6 & 60.3\\
	\hline
	\end{tabular}
	\centering
	\caption{NVAutoNet versus Fast-BEV \cite{fastbev}. This is not a apple-to-apple comparison but a good reference.}
	\label{tab:autonet_vs_fastbev}
\end{table}
\subsubsection{3D Freespace}
Table \ref{table:freespace_result_solo} summarizes the overall freespace results. Additionally, Table \ref{tab:freespace_result_sector} reports bucketized regression metrics per sector divided into different combinations of angular and radial sectors. Overall, close range regions have higher accuracy than far range regions. Also front region is more accurate than rear region. This is because the front region is covered by three different cameras: 120FOV, 30FOV and fisheye 200FOV. Also it can seen that precision and recall for the Other and Vehicle categories are much better than the VRU category. VRU classification is more difficult as VRU is often covered by few angular bins as compared to the other categories.
\begin{table}[hbt!]
    \begin{tabular}{llcccc}
    \hline
     Relative gap & Absolute gap & Success rate  & Smoothness \\ \hline
    44.14 & 1.95 & 77.59 & 0.77 \\
    \hline
    \hline
    \hline
    \hline
	   & Vehicle & VRU & Other\\
	\hline
	Precision  & 0.92 & 0.73  & 0.98\\
        Recall & 0.92 & 0.66 & 0.98\\
	\hline
    \end{tabular}
    \centering
    \caption{3D freespace regression and classification metrics.}
    \label{table:freespace_result_solo}
\end{table}
\begin{table}[t]
	\begin{tabular}{llcccc}
	\hline
	Radial (meters) & Angular (degrees)  & Success Rate (\%) & Absolute gap (m)\\
	\hline
	0-10  &  [-45, +45] & 86.43  & 0.91\\
        10-20   &  [-45, +45] & 83.02 & 1.40\\
        20-30   &  [-45, +45] & 73.74 & 2.70\\
        30-50   &  [-45, +45] & 64.52 & 4.72\\
        50-80   &  [-45, +45] & 57.05 & 8.21\\
        80-120  &  [-45, +45] & 42.39 & 18.96\\
        120-200 &  [-45, +45] & 1.81 & 54.38\\
	\hline
        0-10  &  [-135, +135] & 82.68  & 0.93\\
        10-20   & [-135, +135] & 76.54 & 1.67\\
	  20-30   &  [-135, +135] & 67.85 & 3.11\\
        30-50   &  [-135, +135] & 58.96 & 5.31\\
        50-80   &  [-135, +135] & 51.46 & 9.31\\
        80-120   &  [-135, +135] & 38.84 & 15.39\\
        120-200   &  [-135, +135] & 0.04 & 61.2\\
	\hline
	\end{tabular}
	\centering
	\caption{Freespace regression metrics for different radial ranges and field-of-views.}
	\label{tab:freespace_result_sector}
\end{table}
\subsubsection{3D Parking space}
\begin{table}
    \begin{tabular}{c|ccc}
    \hline
    % Results are from this workflow: https://redash.nvda.ai/queries/16499/source?p_workflow=mmaghoumi-parknet3d-v202304-tot-01-26-baseline-with-awlsome-cl
    & AP & mean IoU & F1\\
    \hline
    Angled & 0.68 & 0.86 & 0.75 \\
    Parallel & 0.19 & 0.82 & 0.37 \\
    Perpendicular & 0.57 & 0.85 & 0.67 \\ 
    All & 0.58 & 0.85 & 0.68\\
    \hline
    \end{tabular}
    \centering
    \caption{Parking space detection performances.}
    \label{table:parking_results_solo}
\end{table}
Table \ref{table:parking_results_solo} reports parking space detection results. We see that the mean IoU of all the detections that were counted as true positives is close to 86\%, which indicates that the majority of the detections highly overlap with the ground truth labels. We also observe that the lowest performing class is the parallel parking spaces, which can be attributed to challenges in labeling these parking spaces. The dimensions of parallel parking spaces are highly variable. In addition, many parking spaces on the side of the street do not have strictly enforced widths. As such, labelers have to rely on their judgment to determine how wide each parking space should be, which in turn makes the labeling of such spaces highly inconsistent.
\subsection{Single task vs multi-task learning}
Having a model that performs multiple tasks is beneficial as it help reducing the computation cost, as compared to multiple single-task models. However, it is widely known that multi-task models are are challenging to train properly, and often lead to significant accuracy drops.

In this experiment, we set the $c_t$ parameters (defined in Sec. \ref{sec:loss_balancing}) be $[5,3,1]$ for obstacle, parking space and freespace tasks separately. 
Intuitively, we want the network pay more attention to the obstacle detection task, and less to the freespace task as obstacle detection is much more difficult than others. Tab. \ref{table:single_vs_multi_tasks} compares the performances of NVAutoNet against the single task models. We observe that obstacle and parking detection accuracy are comparable to those of single task models. The freespace task drops $9.5\%$, but its accuracy still remains high. 
\begin{table}[hbt!]
    \begin{tabular}{c|c|c|c}
    \hline
     & Obstacle (mAP) & Freespace (Success rate) & Parking space (F1) \\ \hline
    Single task & 0.46 & 77.59 & 0.68  \\
    Multi-task & 0.46 & 70.19 & 0.68\\ 
    % For ParkNet, obtained from https://redash.nvda.ai/queries/16499/source?p_workflow=chaof-v40-bs48-finetune-mlmcf4d-3task-v40d-v04d-ncap-sim-aug-oversample-patch7
    % TODO: The numbers don't match with PRC1 v1 model KPIs.
    \hline
    \end{tabular}
    \centering
    \caption{Single task vs. multi-task.}
    \label{table:single_vs_multi_tasks}
\end{table}
\subsection{IPM vs MLP based 2D-to-BEV View Transformation}
In this study, we want to demonstrate the benefits of using a learning based approach to transform 2D image features onto the BEV plane as compared to using a homograph transformation with a flat-world assumption. We pick obstacle detection task for this experiment. Table \ref{tab:ipm_vs_mlp} report comparison results. It can be seen that our proposed 2D-to-BEV approach significantly outperforms the classical IPM approach with a very large margin. 
\begin{table}[hbt!]
	\begin{tabular}{lccccc}
	\hline
	 & Vehicle & Truck & Person  & Bike-rider & \\
	 Method & AP & AP & AP  & AP & mAP\\
	\hline
	IPM & 0.49 & 0.32 & 0.27 & 0.33 & 0.30 \\
	MLP & 0.63 & 0.38 & 0.35 & 0.48 & 0.46 \\
	\hline
	\end{tabular}
	\centering
	\caption{IPM versus MLP based 2D-to-BEV view transformation for obstacle detection.}
	\label{tab:ipm_vs_mlp}
\end{table} 

\subsection{Generalization to Different Vehicle Lines}
To verify the robustness and scalability of our proposed architect, we test NVAutoNet, which was originally developed for cars, on trucks. Although the differences in the intrinsic parameters between the two platforms are small, those of the extrinsic parameters are very significant (see Fig. \ref{fig:cam_configs}). Nonetheless, we will show that model redesign and large-scale data collection is not required when deploying our perception model on different platforms. In particular, only model finetuing using a small training dataset is sufficient.
\begin{figure}
\centering
\begin{subfigure}[b]{.3\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/car_sensors.png}
    \caption{Car platform.}
\end{subfigure}
\begin{subfigure}[b]{.4\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/truck_sensors.png}
    \caption{Truck platform.}
\end{subfigure}
\caption{Different camera sensor setups for cars and trucks.}
\label{fig:cam_configs}  
\end{figure}
\begin{table}
    \begin{tabular}{*{8}{c}}
    \hline
    & Pre-train & Train/Finetune & \multicolumn{5}{c}{Dataset size (Truck Platform)}  \\ \hline
    &  &  & 50K & 75K & 100K & 125K & 150K \\ \hline
    Model-A & No & Yes & 0.146 & 0.180 & 0.192 & 0.209 & 0.210 \\
    Model-B & Yes & No & 0.168 & 0.168 & 0.168 & 0.168 &0.168  \\
    Model-C & Yes & Yes &0.286 & 0.292 & 0.294 & 0.294 & 0.300  \\ \hline
    \end{tabular}
    \centering
    \caption{Transfer learning experiments for NVAutoNet. mAP scores for obstacle detection are reported. Model-A trained from scratch using different dataset sizes collected from the truck platform; Model-B previously trained using data collected from \emph{\textbf{the car platform}}; Model-C fine-tuned from model-B using data collected from the truck platform.}
    \label{table:truck_dataset_size}
\end{table}

In this study, 3D obstacle detection task is considered. Using the car platform, we collected 850K scenes for training, denoted as car-dataset-train. Using the truck platform, we collected 150K scenes for training, denoted as truck-dataset-train, and 26K scenes for validations, denoted as truck-dataset-val. The label distribution between the two platforms is similar. To see the impact of the training set size, we sub-sample the 150K truck-dataset-train into multiple smaller ones of increasing different sizes (i.e., 50K, 75K, ... 150K).

We compare the following models: Model-A trained from scratch using truck-dataset-train, Model-B previously trained using car-dataset-train and Model-C fine-tuned from Model-B using truck-dataset-train. All three models are tested on the truck-dataset-val. Tab. \ref{table:truck_dataset_size} reports the comparison results. 

As expected, the performance of Model-A increases with respect to the dataset sizes. Interestingly, Model-A, when trained with 50K scenes, is worse than Model-B although Model-B has never seen the data from the truck platform. This is due to the network's generalization capability. Unsurprisingly, Model-C is the best and significantly outperforms Model-A and Model-B with very large margins. Also notice that the performance of Model-C does not increase considerably with larger finetuning dataset sizes as opposed to the model A. This result indicates that we only need to collect a small amount of data for finetuning if we want to deploy NVAutoNet model (previously trained for one platform) on a different platform. These results confirm again the robustness, scalability of our proposed NVAutoNet --- a must-have feature for production.

\subsection{Qualitative Performance}
Figures \ref{fig:autonet_visual_parking} and \ref{fig:autonet_visual_obs_fsp} showcase qualitative detection results. Note that these results reflect real in-car-testing performance.
\begin{figure}
\centering
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/parking_01.png}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/parking_04.png}
\end{subfigure}
\caption{Parking space detection by NVAutoNet. For visual clarity, obstacles and freespaces are not visualized.}
\label{fig:autonet_visual_parking}  
\end{figure}

\begin{figure}
\centering
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/autonet_highway.png}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
    \includegraphics[width=1.0\textwidth]{figures/autonet_urban_01.png}
\end{subfigure}
\caption{Obstacles and freespaces detection by NVAutoNet. Bottom: Green cuboids are obstacles. Yellow lines are freespace regions.}
\label{fig:autonet_visual_obs_fsp}  
\end{figure}

\section{Conclusion and Future Work}
Most existing BEV perception works are highly optimized for detection accuracy, which often require huge computational resources. Therefore, these models are impractical to real-world applications such as self-driving where computational budgets are very limited. Moreover, most existing BEV perception datasets and benchmarks are very far from reality. For example, the popular nuScenes dataset  
has ground-truth labels within 70 meters range, while self-driving requires detection range up to 200-300 meters. This further makes the existing BEV perception models not suitable for a self-driving car. In this work, we introduce NVAutoNet in which every component is very well optimized to offer the highest accuracy and latency balance. The technical contributions include, to name a few, small but strong camera backbones, efficient 2D to 3D uplifting, multi-task learning, different platform adaptation, and various task specific training recipes. As a result, NVAutoNet is able to run faster than real-time at 53 FPS on NVIDIA Orin SoC while attaining sufficiently high accuracy.

Extending BEV perception to truly 3D perception (e.g., 3D volumetric occupancy perception) will enable higher levels of autonomy such as L4/L5 self-driving. But that transition is seen to be very challenging due to its high memory and computation consumption. Far range perception (e.g., up to 300 meters) will be necessary to increase driving safety and comfortableness. Holistic scene understanding where not only objects but also their relations are predicted becomes more relevant to unlock driving without HD maps.

\section*{Acknowledgments}
We would like to thank the NVIDIA Maglev and ML foundation teams for the machine learning infrastructure support, thank the NAS (neural architectural search) team for the network optimization and special thank to the whole DRIVEAV team for data collection, network deployment, iteration and on-road testing. 

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
