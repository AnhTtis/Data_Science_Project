@misc{bev_survey,
  doi = {10.48550/ARXIV.2208.02797},
  
  url = {https://arxiv.org/abs/2208.02797},
  
  author = {Ma, Yuexin and Wang, Tai and Bai, Xuyang and Yang, Huitong and Hou, Yuenan and Wang, Yaming and Qiao, Yu and Yang, Ruigang and Manocha, Dinesh and Zhu, Xinge},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Vision-Centric BEV Perception: A Survey},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@misc{bevformer,
  doi = {10.48550/ARXIV.2203.17270},
  
  url = {https://arxiv.org/abs/2203.17270},
  
  author = {Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Yu, Qiao and Dai, Jifeng},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{fastbev,
  doi = {10.48550/ARXIV.2301.12511},
  
  url = {https://arxiv.org/abs/2301.12511},
  
  author = {Li, Yangguang and Huang, Bin and Chen, Zeren and Cui, Yufeng and Liang, Feng and Shen, Mingzhu and Liu, Fenggang and Xie, Enze and Sheng, Lu and Ouyang, Wanli and Shao, Jing},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Fast-BEV: A Fast and Strong Bird's-Eye View Perception Baseline},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Wang2021FCOS3DFC,
  title={FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection},
  author={Tai Wang and Xinge Zhu and Jiangmiao Pang and Dahua Lin},
  journal={2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  year={2021},
  pages={913-922}
}
@article{Wang2021,
  author    = {Yue Wang and
               Vitor Guizilini and
               Tianyuan Zhang and
               Yilun Wang and
               Hang Zhao and
               Justin Solomon},
  title     = {{DETR3D:} 3D Object Detection from Multi-view Images via 3D-to-2D
               Queries},
  journal   = {CoRR},
  volume    = {abs/2110.06922},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.06922},
  eprinttype = {arXiv},
  eprint    = {2110.06922},
  timestamp = {Thu, 05 May 2022 08:42:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-06922.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Zidong_BMVC2021,
	title={Surround-view Free Space Boundary Detection with Polar Representation},
	author={Zidong Cao, Ang Li, Zhiliang Xiong, Zejian Yuan},
	year={2021},
	month={November},
	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
	publisher={BMVA Press},
}
@inproceedings{BMVC2015_109,
	title={StixelNet: A Deep Convolutional Network for Obstacle Detection and Road Segmentation},
	author={Dan Levi and Noa Garnett and Ethan Fetaya},
	year={2015},
	month={September},
	pages={109.1-109.12},
	articleno={109},
	numpages={12},
	booktitle={Proceedings of the British Machine Vision Conference (BMVC)},
	publisher={BMVA Press},
	editor={Xianghua Xie, Mark W. Jones, and Gary K. L. Tam},
	doi={10.5244/C.29.109},
	isbn={1-901725-53-7},
	url={https://dx.doi.org/10.5244/C.29.109}
}
@article{Sanberg2016,
  author    = {Willem P. Sanberg and
               Gijs Dubbelman and
               Peter H. N. de With},
  title     = {Free-Space Detection with Self-Supervised and Online Trained Fully
               Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1604.02316},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.02316},
  eprinttype = {arXiv},
  eprint    = {1604.02316},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SanbergDW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Scheck2020,
  author    = {Tobias Scheck and
               Adarsh Mallandur and
               Christian Wiede and
               Gangolf Hirtz},
  title     = {Where to drive: free space detection with one fisheye camera},
  journal   = {CoRR},
  volume    = {abs/2011.05822},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.05822},
  eprinttype = {arXiv},
  eprint    = {2011.05822},
  timestamp = {Thu, 12 Nov 2020 15:14:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-05822.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{Saha2021,
  author={Saha, Avishkar and Mendez, Oscar and Russell, Chris and Bowden, Richard},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Enabling spatio-temporal aggregation in Birds-Eye-View Vehicle Estimation}, 
  year={2021},
  volume={},
  number={},
  pages={5133-5139},
  doi={10.1109/ICRA48506.2021.9561169}}
@article{Roddick2020PredictingSM,
  title={Predicting Semantic Map Representations From Images Using Pyramid Occupancy Networks},
  author={Thomas Roddick and Roberto Cipolla},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={11135-11144}
}
@inproceedings{
wang2021detrd,
title={{DETR}3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries},
author={Yue Wang and Vitor Campagnolo Guizilini and Tianyuan Zhang and Yilun Wang and Hang Zhao and Justin Solomon},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=xHnJS2GYFDz}
}
@Inproceedings{Saha2022,
 author = {Avishkar Saha and Oscar Mendez and Chris Russell and Richard Bowden},
 title = {Translating images into maps},
 year = {2022},
 url = {https://www.amazon.science/publications/translating-images-into-maps},
 booktitle = {ICRA 2022},
}
@article{Can2021StructuredBT,
  title={Structured Bird’s-Eye-View Traffic Scene Understanding from Onboard Images},
  author={Yigit Baran Can and Alexander Liniger and Danda Pani Paudel and Luc Van Gool},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={15641-15650}
}
@article{Chitta2021NEATNA,
  title={NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
  author={Kashyap Chitta and Aditya Prakash and Andreas Geiger},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={15773-15783}
}
@InProceedings{Yang_2021_CVPR,
    author    = {Yang, Weixiang and Li, Qi and Liu, Wenxi and Yu, Yuanlong and Ma, Yuexin and He, Shengfeng and Pan, Jia},
    title     = {Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-View Transformation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {15536-15545}
}

@InProceedings{Reading_2021_CVPR,
    author    = {Reading, Cody and Harakeh, Ali and Chae, Julia and Waslander, Steven L.},
    title     = {Categorical Depth Distribution Network for Monocular 3D Object Detection},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8555-8564}
}
@article{Pan2020CrossViewSS,
  title={Cross-View Semantic Segmentation for Sensing Surroundings},
  author={Bowen Pan and Jiankai Sun and Ho Yin Tiga Leung and Alex Andonian and Bolei Zhou},
  journal={IEEE Robotics and Automation Letters},
  year={2020},
  volume={5},
  pages={4867-4873}
}

@article{Hendy2020FISHINGNF,
  title={FISHING Net: Future Inference of Semantic Heatmaps In Grids},
  author={Noureldin Hendy and Cooper Sloan and Fengshi Tian and Pengfei Duan and Nick Charchut and Yuxing Xie and Chuan Wang and James Philbin},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.09917}
}
@InProceedings{Schulter2018,
author="Schulter, Samuel
and Zhai, Menghua
and Jacobs, Nathan
and Chandraker, Manmohan",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Learning to Look around Objects for Top-View Representations of Outdoor Scenes",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="815--831",
abstract="Given a single RGB image of a complex outdoor road scene in the perspective view, we address the novel problem of estimating an occlusion-reasoned semantic scene layout in the top-view. This challenging problem not only requires an accurate understanding of both the 3D geometry and the semantics of the visible scene, but also of occluded areas. We propose a convolutional neural network that learns to predict occluded portions of the scene layout by looking around foreground objects like cars or pedestrians. But instead of hallucinating RGB values, we show that directly predicting the semantics and depths in the occluded areas enables a better transformation into the top-view. We further show that this initial top-view representation can be significantly enhanced by learning priors and rules about typical road layouts from simulated or, if available, map data. Crucially, training our model does not require costly or subjective human annotations for occluded areas or the top-view, but rather uses readily available annotations for standard semantic segmentation in the perspective view. We extensively evaluate and analyze our approach on the KITTI and Cityscapes data sets.",
isbn="978-3-030-01267-0"
}
@inproceedings{Reiher2020,
author = {Reiher, Lennart and Lampe, Bastian and Eckstein, Lutz},
title = {A Sim2Real Deep Learning Approach for the Transformation of Images from Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird’s Eye View},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ITSC45102.2020.9294462},
doi = {10.1109/ITSC45102.2020.9294462},
abstract = {Accurate environment perception is essential for automated driving. When using monocular cameras, the distance estimation of elements in the environment poses a major challenge. Distances can be more easily estimated when the camera perspective is transformed to a bird’s eye view (BEV). For flat surfaces, Inverse Perspective Mapping (IPM) can accurately transform images to a BEV. Three-dimensional objects such as vehicles and vulnerable road users are distorted by this transformation making it difficult to estimate their position relative to the sensor. This paper describes a methodology to obtain a corrected 360° BEV image given images from multiple vehicle-mounted cameras. The corrected BEV image is segmented into semantic classes and includes a prediction of occluded areas. The neural network approach does not rely on manually labeled data, but is trained on a synthetic dataset in such a way that it generalizes well to real-world data. By using semantically segmented images as input, we reduce the reality gap between simulated and real-world data and are able to show that our method can be successfully applied in the real world. Extensive experiments conducted on the synthetic data demonstrate the superiority of our approach compared to IPM. Source code and datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV.},
booktitle = {2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)},
pages = {1–7},
numpages = {7},
location = {Rhodes}
}
@inproceedings{Philion2020,
author = {Philion, Jonah and Fidler, Sanja},
title = {Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D},
year = {2020},
isbn = {978-3-030-58567-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58568-6_12},
doi = {10.1007/978-3-030-58568-6_12},
abstract = {The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: .},
booktitle = {Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV},
pages = {194–210},
numpages = {17},
location = {Glasgow, United Kingdom}
}
@INPROCEEDINGS{Yang2021,
  author={Yang, Weixiang and Li, Qi and Liu, Wenxi and Yu, Yuanlong and Ma, Yuexin and He, Shengfeng and Pan, Jia},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Projecting Your View Attentively: Monocular Road Scene Layout Estimation via Cross-view Transformation}, 
  year={2021},
  volume={},
  number={},
  pages={15531-15540},
  doi={10.1109/CVPR46437.2021.01528}}
  
  @article{Mani2020MonoLO,
  title={Mono Lay out: Amodal scene layout from a single image},
  author={Kaustubh Mani and Swapnil Daga and Shubhika Garg and N. Sai Shankar and Krishna Murthy Jatavallabhula and K. Madhava Krishna},
  journal={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2020},
  pages={1678-1686}
}
@article{Ng2020BEVSegBE,
  title={BEV-Seg: Bird's Eye View Semantic Segmentation Using Geometry and Semantic Point Cloud},
  author={Mong H. Ng and Kaahan Radia and Jianfei Chen and Dequan Wang and Ionel Gog and Joseph Gonzalez},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.11436}
}

% Image-based 3D Object detection 
@inproceedings{wang2019pseudo,
  title={Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving},
  author={Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ma2019color,
    author = {Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Zhang, Pengbo and Ouyang, Wanli and Fan, Xin},
    year = {2019},
    pages = {6850-6859},
    title = {Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving},
    booktitle={ICCV},
    doi = {10.1109/ICCV.2019.00695}
}

@INPROCEEDINGS{unsup_depth2017,
  author={Godard, Clément and Aodha, Oisin Mac and Brostow, Gabriel J.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Monocular Depth Estimation with Left-Right Consistency}, 
  year={2017},
  pages={6602-6611},
  doi={10.1109/CVPR.2017.699}
}

@INPROCEEDINGS{deepord2018,
  author={Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Deep Ordinal Regression Network for Monocular Depth Estimation}, 
  year={2018},
  pages={2002-2011},
  doi={10.1109/CVPR.2018.00214}
}

@INPROCEEDINGS{frustum2018,
  author={Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Frustum PointNets for 3D Object Detection from RGB-D Data}, 
  year={2018},
  pages={918-927},
  doi={10.1109/CVPR.2018.00102}
}

@INPROCEEDINGS{pixor2018,
  author={Yang, Bin and Luo, Wenjie and Urtasun, Raquel},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={PIXOR: Real-time 3D Object Detection from Point Clouds}, 
  year={2018},
  pages={7652-7660},
  doi={10.1109/CVPR.2018.00798}
}

@INPROCEEDINGS{pointrcnn2019,
  author={Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud}, 
  year={2019},
  pages={770-779},
  doi={10.1109/CVPR.2019.00086}
}

@INPROCEEDINGS{mono_2016,
  author={Chen, Xiaozhi and Kundu, Kaustav and Zhang, Ziyu and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Monocular 3D Object Detection for Autonomous Driving}, 
  year={2016},
  pages={2147-2156},
  doi={10.1109/CVPR.2016.236}}
  
@INPROCEEDINGS{Deep3DBox,
  author={Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Košecká, Jana},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={3D Bounding Box Estimation Using Deep Learning and Geometry}, 
  year={2017},
  pages={5632-5640},
  doi={10.1109/CVPR.2017.597}}
  
@INPROCEEDINGS{m3drpn,
  author={Brazil, Garrick and Liu, Xiaoming},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={M3D-RPN: Monocular 3D Region Proposal Network for Object Detection}, 
  year={2019},
  pages={9286-9295},
  doi={10.1109/ICCV.2019.00938}}

@inproceedings{zhou2019objects,
  title={Objects as Points},
  author={Zhou, Xingyi and Wang, Dequan and Kr{\"a}henb{\"u}hl, Philipp},
  booktitle={arXiv preprint arXiv:1904.07850},
  year={2019}
}

@article{roddick2018orthographic,  
  title={Orthographic feature transform for monocular 3d object detection},  
  author={Roddick, Thomas and Kendall, Alex and Cipolla, Roberto},  
  journal={British Machine Vision Conference},  
  year={2019}  
}

@INPROCEEDINGS{smoke_2020,
  author={Liu, Zechen and Wu, Zizhang and Toth, Roland},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation}, 
  year={2020},
  pages={4289-4298},
  doi={10.1109/CVPRW50498.2020.00506}
}

@InProceedings{peize2020onenet,
  title = 	 {What Makes for End-to-End Object Detection?},
  author =       {Sun, Peize and Jiang, Yi and Xie, Enze and Shao, Wenqi and Yuan, Zehuan and Wang, Changhu and Luo, Ping},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9934--9944},
  year = 	 {2021},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
}

@InProceedings{cp_cluster_2022,
    author    = {Shen, Yichun and Jiang, Wanli and Xu, Zhen and Li, Rundong and Kwon, Junghyun and Li, Siyi},
    title     = {Confidence Propagation Cluster: Unleash Full Potential of Object Detectors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {1151-1161}
}
