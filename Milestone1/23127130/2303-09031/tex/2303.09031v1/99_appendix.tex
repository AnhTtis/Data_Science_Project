\appendix
\section{Appendix}



\subsection{Experimental Setup Details}
\paragraph{Visual Observations, Text Actions.}
\vizalfworld uses the same action set defined in \alfworld with the following difference. 
\alfworld is a text-based environment and it references multiple instances of an object type with object identifiers (cabinet 1, cabinet 2, etc.).
These object identifiers are no longer meaningful in the visual setting as the grounding of cabinet 1 to the physical cabinet is unknown to the agent.
We thus removed these numeric identifiers in the action and ground-truth text observation (\eg, ``cabinet 1'' becomes ``cabinet'').
The set of valid actions and objects is defined in~\citep{shridhar2021alfworld}.
We use the same procedure for removing object numeric identifiers from \vhome. 

\paragraph{Ground Truth Captions.}
Ground truth captions are needed to train the captions baseline and needed for the captioning auxiliary task. In \vizalfworld, we use the ground truth captions provided in ALFWorld~\citep{shridhar2021alfworld}, which are generated from visible objects and a pre-defined template.
We define ground truth captions for \vhome using a similar template on the environment's list of interactable objects for each observation.

\paragraph{Ground Truth Affordability.}
Ground truth labels for affordability are needed to train the SayCan trained affordance model. As the trained affordance is a binary classifier, we create a training set of observation-action-affordability pairs. To do this, for each observation, we sampled all affordable actions, and a subset of non-affordable actions (as there are vastly more non-affordable actions in each state).
We tested two methods for sampling non-affordable actions: 1) We used a hand-coded heuristic that selects non-affordable actions based on actions that are likely to be predicted by the LM but are not affordable.
2) We use actions that are predicted with high likelihood from the Ignore baseline but are not affordable.
We did not find a significant difference in non-affordable examples on performance.

\subsection{SayCan Baseline Details}
In the original SayCan~\citep{ahn2022can}, the PLM gives a score for every possible action at each timestep.
When the oracle action affordances are known (around 15 actions affordable) at a given time,
we can evaluate the PLM score on all affordable actions.
However, when oracle affordances are not available, in our trained affordance setting, each step has \textit{thousands} of possible actions (actions are a combinatorial product on action type and objects). Evaluating the LM on each action is infeasible.
Instead, we use a beam search on the PLM to generate the top-$k$ most likely actions, rank
these likely actions using the SayCan score. Specifically, for a given observation $o$, predict the next action $a$ by:
\begin{enumerate}[leftmargin=*,itemsep=0em]
\item Sample top-$k$ actions $a_1, \dots, a_k$ from PLM.
\item Compute
    \begin{align*}
        \mathrm{Score}(a_t) = p_\mathrm{LM}(a_t | g, a_{t-1}, \dots a_1) \cdot p_\mathrm{aff}(a_t | o_t)
        \label{eq:saycan}
    \end{align*}
\item Return $\mathrm{argmax}(\text{Score}(a))$
\end{enumerate}

In addition, the original SayCan does not perform any finetuning on the PLM. 
However, we found that SayCan improved in our environment tasks when finetuning a smaller model.

\subsection{Code Implementation Details}

Our approaches and baselines were implemented using the Huggingface Transformers Python library~\cite{wolf2019huggingface}.
Code for the \vizalfworld environment was modified from the ALFWorld codebase~\cite{shridhar2021alfworld}.
Code for the \vhome environment was modified from the LID codebase~\cite{li2022pre}.

\subsection{Computational Budget}
Experiments were conducted on A100 GPUs. Most approaches used a \gptmed as a base LM, totalling 400M parameters. Prompt tuning ablations with larger frozen models had 1.5B and 6B parameters in total.
For the \flan-SayCan experiments (using an 11B model) we did no training.

\subsection{Hyperparameters}

In general we used similar hyperparameters for the PLM and visual prompts across approaches, but adjusted the epochs to account for learning visual prompts vs. learning text only and number of training samples.

\input{tables/hyperparameters}
\paragraph{\ours.} We show relevant hyperparameters for \ours in Table~\ref{tab:vp2hyperparameters}.

\paragraph{Ignore Baseline.}
We used the same hyperparameters as in Table~\ref{tab:vp2hyperparameters} but remove visual prompt parameters.

\paragraph{Caption Baseline.}
To train a captioning model, we use the same hyperparameters as Table~\ref{tab:vp2hyperparameters} but set \{epochs = 10\}.
We train a separate PLM for the action prediction model (which uses predicted captions as input) using the same hyperparameters as Table~\ref{tab:vp2hyperparameters} but set \{epochs = 20\}.
For the context of the captions, we add goal text to encourage the PLM to produce captions: 
``Your task is to: caption the following observation''.
We also tested finetuning a CLIPCap captioning model for this baseline, but found this decreased captioning performance.

\paragraph{SayCan Baseline.}
For the frozen \flan-SayCan model, we performed prompt engineering similar to~\citet{ahn2022can}. For every task, we sample $k$ examples for few shot prompting (as many can fit in the context length) in the following format:

{\small
\begin{verbatim}
Here are some step by step instructions 
for example tasks.
Example: <g1>. 1. <a1> 2. [...] [...]
Example: <gk>. 1. <a1> 2. [...]
Give step by step instructions for the 
following task. 
<goal>
\end{verbatim}
}

To retrieve examples, we sampled $k$ samples from the training dataset that are similar to the current goal.
For \vizalfworld, we took samples that are the same task type (pick-place, heat, etc.\ ) and
for \vhome, we took samples where the goals were most similar according to a simple bag of words heuristic.
We found the best value for $k = 30$ from a grid search on $k \in \{1, 5, 10, 15, 30\}$.

For the fine-tuned action prediction model \gptmed-SayCan, we use the same hyperparameters as Table~\ref{tab:vp2hyperparameters} but set \{epochs = 20\}.
For the trained affordance model, we use the same hyperparameters as
Table~\ref{tab:vp2hyperparameters} but set \{epochs = 2\}.
For the context of the trained affordance, we add goal text to encourage the PLM to predict affordance: 
``Your task is to: predict whether the following action is valid.''
The PLM must either output the token ``valid'' or ``invalid'', given the observations and action contexts.

\subsection{Full Ablations}

\input{tables/ablations_full}

We present more detailed ablations in 
\Cref{tab:vizalf-ablations-full} presents additional ablations for prompt size, training samples, and LM pretraining (more comprehensive version of  \Cref{tab:vizalf-ablations} in the main text).

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\textwidth]{figures/vp2-example-v3}
\caption{Example of \ours with the captioning auxiliary task in \vizalfworld. Each observation is encoded into a visual prompt VP$_t$ and used to predict the next action.
The LM is also trained to predict a caption for each VP$_t$.}
\label{fig:aug-arch}
\end{figure*}

\paragraph{Prompt Size.}
We tested visual prompt sizes \{1, 5, 10, 20\}, where 10 is the prompt size used in \ours.
Lowering the prompt size to 1 and 5 lowers the success rate in \vizalfworld, as less visual information can be contained in each prompt.
However, raising the prompt size to 20 may allow more information in a visual prompt, but also causes the LM's context to increase. This can harm the LM as the context needs to be trimmed to fit within the limited context window.
From Table~\ref{tab:vizalf-ablations-full}, this seems to harm the success rate.

\subsection{Auxiliary Tasks}
In \ours, the LM is only trained on the action prediction loss function $\mathcal{L}_\mathcal{D}$.
We hypothesized that auxiliary tasks that train visual prompts in additional ways can help ground and improve the visual prompts for planning.
To do this, we trained the LM concurrently on $\mathcal{L}_\mathcal{D}$ and a loss derived for each auxiliary task.
1) Inverse Dynamics (inv-dyn.\ ). The LM must predict the action that is executed between two observations:
$\mathcal{L}_\text{inv-dyn} = -\frac{1}{N} \sum_{i,t} \log p_\text{LM}(a_t^{(i)} | o_t^{(i)}, o_{t+1}^{(i)})$.
2) Captions. The LM must predict the ground truth caption text for each observation.
This is the same training objective used in CLIPCap~\citep{mokady2021clipcap}:
$\mathcal{L}_\text{cap} = -\frac{1}{N} \sum_{i,t} \log p_\text{LM}(\text{caption}_t^{(i)} | o_t^{(i)})$.
3) Goal Prediction (goal-pred). The LM must predict the goal text given action-observation context.
$\mathcal{L}_\text{goal-pred} = -\frac{1}{N} \sum_{i} \log p_\text{LM}(g^{(i)} | o_1^{(i)}, a_1^{(i)}, \dots, o_T^{(i)}, a_T^{(i)})$.

\paragraph{Results.}
We show an example of the captioning auxiliary task in Figure~\ref{fig:aug-arch}.
We find auxiliary tasks (inv-dyn.\ and captions) can help ground visual prompts and improve success rate for in distribution tasks. However they also cause \ours to overfit and perform worse on out of distribution tasks.
The last auxiliary task goal pred.\ seems to decrease performance in both ID and OD.

\paragraph{Hyperparameters.}
For auxiliary tasks, we add a tunable task embedding to each context, where each task (action prediction vs.\ auxiliary task) has a separate task embedding. This embedding helps the LM to learn multiple tasks. We used a task embedding length = 10.
To compute the loss, we add a weight parameter to control the loss between action prediction and auxiliary task: $\mathcal{L} = \mathcal{L}_\mathcal{D} + \alpha \mathcal{L}_\text{aux}$.
In our experiments, we tested $\alpha = \{0.1, 1.0\}$ and found $\alpha = 0.1$ works the best.
Otherwise, we used the same hyperparameters as Table~\ref{tab:vp2hyperparameters}.