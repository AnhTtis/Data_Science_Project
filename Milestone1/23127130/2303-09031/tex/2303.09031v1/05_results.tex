\section{Experiments}

\subsection{Environments}

We experiment with embodied agent tasks that involve navigating and manipulating objects in a simulated household environment.
The agent acts by feeding text commands to a low-level controller that executes various pretrained skills (such as \textit{go to cabinet} or \textit{take apple from cabinet}).

\paragraph{\vizalfworld.}
This environment is based on \alfworld~\citep{shridhar2021alfworld} and contains 6 types of tasks that are compositional and contain multiple subgoals that must be completed.
In contrast to ALFWorld which is a purely text based environment, we consider the same set of tasks but with \textbf{only} visual observations from the AI2-Thor simulator \citep{ai2thor}.
We used the training and evaluation task split provided in ALFWorld which consists of 4620 training tasks, 187 in distribution evaluation tasks, and 192 out of distribution evaluation tasks.
However, we found 64/187 and 52/190 of the ID and OD evaluation tasks respectively were impossible to complete, due to errors in the ALFWorld low level action implementations. So, we normalized the success rate of all agents by the oracle agent's success rate.

\paragraph{VirtualHome.}
We experiment with tasks from LID \citep{li2022pre} which are based on the VirtualHome simulator \citep{puig2018virtualhome}.
Each task is specified using a set of goal conditions that must be met at the end of the episode (e.g., There must be two pancakes in the fridge and the stove must be turned on).
We use the in distibution and novel scene splits from LID. 2000 in distribution tasks were used for training and 200 novel scene tasks were used for evaluation.

\subsection{Models}


\input{tables/main}

We use the \gptmed language model (355M parameters) in all our experiments.
We consider the following baselines for comparison.
    \paragraph{Ignore.} A simple baseline inspired by \citet{jansen-2020-visually} that ignores the visual observations and predicts the entire text action sequence only from the goal text description. This baseline is finetuned with the same objective as in \Cref{eq:trainobj} but without observations in the context.
    \paragraph{Captions.} Instead of feeding visual observations to the planner language model, we use text captions predicted by a captioning model as a proxy \citep{shridhar2021alfworld}. We train a ClipCap \citep{mokady2021clipcap} model on ground-truth captions from the respective environment's training demonstrations and use them for captioning. The captioning model is trained on 70k and 60k captions on \vizalfworld and \vhome respectively.
    \paragraph{SayCan.}
    The SayCan~\citep{ahn2022can} architecture has two components: a) A PLM that ranks actions and b) An affordance function that predicts what actions are affordable from a given state.
    SayCan evaluates a given action by combining its likelihood under the PLM (ignoring visual observations) with its affordance score as shown in \Cref{eq:saycan}.
    \begin{align}
        \mathrm{Score}(a_t) = p_\mathrm{LM}(a_t | g, a_{t-1}, \dots, a_1) \cdot p_\mathrm{aff}(a_t | o_t)
        \label{eq:saycan}
    \end{align}
    We consider different design choices for each of these components.
    We consider two variations for the PLM: 
    1) A frozen \flan (11B) model that is few-shot prompted similar to the original work.
    2) A \gptmed model fine-tuned for next action prediction (details in the appendix).
 

        
    We also consider two variations for the affordance model: 
    1) An \emph{oracle affordance} function which assumes knowledge about ground-truth affordable actions. %
    2) A \emph{trained affordance} function trained to predict whether an action is affordable from a given visual observation using supervised learning on training demonstrations annotated with affordance information.
        In both versions, we use a PLM to predict action sequences from the goal text and select an action based on the SayCan score in \Cref{eq:saycan}.






\subsection{Results}

Table~\ref{tab:vizalf-results} compares the performance of our method against baselines.
Our simple approach (\ours) performs better than all baselines, despite not using external data (caption and affordance information).
\ours benefits from direct coupling between the planner language model and environment observations.

The Ignore baseline performs worse compared to other methods that make use of observations.
However, on out of distribution tasks it suffers less from domain-shift compared to some of the other grounding baselines such as Captions.
The Captions baseline performs better than Ignore, but suffers from information loss in the captioning process.

In comparison, SayCan with oracle affordance is comparable to or better than Ignore and Captions (slightly worse than Captions on \vizalfworld ID) in spite of incorporating observation information only through the affordance function (both \flan and \gptmed).
SayCan using the trained affordance only performs slightly better than Ignore on in distribution, and similarly suffers from domain-shift on out of distribution tasks.
\flan performs well on \vizalfworld despite no training.
However, it performs poorly on \vhome due to demonstration trajectories in \vhome being relatively long, which inhibits SayCan from using many examples for few-shot prompting.







\subsection{Ablations}

\input{tables/ablations}

Table~\ref{tab:vizalf-ablations} presents ablations we perform to identify the importance of each component of our approach.

\paragraph{Visual Encoder.}
Replacing the CLIP visual encoder with a Resnet50 \citep{he2016deep} significantly degrades the performance.
This suggests that the image-text alignment pre-training of CLIP helps produce observation features that are more easily interpreted by the language model.
In contrast to prior methods that consider auxiliary alignment objectives to match the distribution of inputs \citep{reid2022can}, it could be more beneficial to use powerful encoders such as CLIP pre-trained for image-text alignment.

\paragraph{Pretrained Prompt Model.}
We also test how using pre-trained visual prompts can affect performance.
We used visual prompts pre-trained with the CLIPCap captioning objective on the Conceptual Captions dataset~\citep{conceptual}.
However, using this pre-trained visual prompt hurts the success rate.
We hypothesize that the knowledge aquired by the LM during captioning isn't directly useful for action prediction.

\paragraph{Prompt Tuning on frozen language models.}
Finally, we consider an ablation where the language model backbone is held fixed and only visual prompts and task prompt tokens are tuned (similar to Frozen \citep{tsimpoukelli2021multimodal}).
We found that freezing the language model generally performs worse than fine-tuning it. 
The drop in performance can be attributed to the limited influence of prompt tokens alone in controlling the language model's behavior. 



\paragraph{Effect of LM pre-training.}
\begin{figure}[!t]
\centering
\includegraphics[trim=5 10 0 20,width=0.48\textwidth]{figures/sample-pt-ablation-v2}
\caption{Ablation showing the learning efficiency (a) with and without language model pre-training and (b) varying the number of training demonstrations.}
\label{fig:samples-pt}
\end{figure}
We examine the benefit of LM pre-training by training a model from scratch on varying amounts of training data in Figure~\ref{fig:samples-pt}.
We find that the pre-trained model converges faster and is more sample efficient compared to the model trained from scratch.
This confirms findings from prior work about how language model pre-training benefits learning sequential decision making tasks \citep{reid2022can}.


